{"cells":[{"metadata":{},"cell_type":"markdown","source":"Thanks to all the contributors with kernels to this competition!\n\nSpecially, to the ones that provided code and comments that inspired us making this and other kernels we developed in the context of this competition: \n\nhttps://www.kaggle.com/dmdm02/complete-eda-voting-lightgbm \n\nhttps://www.kaggle.com/chocozzz/two-sigma-news-simple-eda-prophet-nlp \n\nhttps://www.kaggle.com/ashishpatel26/bird-eye-view-of-two-sigma-nn-approach \n    \nhttps://www.kaggle.com/jsaguiar/baseline-with-news \n    \nhttps://www.kaggle.com/artgor/eda-feature-engineering-and-everything \n    \nhttps://www.kaggle.com/christofhenkel/market-data-nn-baseline \n    \nhttps://www.kaggle.com/smasar/tutorial-timeseriesapproach \n    \nhttps://www.kaggle.com/rabaman/0-64-in-100-lines \n    \nhttps://www.kaggle.com/guowenrui/market-nn-if-you-like-you-can-use-it-and-upvote/notebook \n    \nhttps://www.kaggle.com/smasar/eda-preprocessing-processing-evaluation"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5"},"cell_type":"markdown","source":"Importar librerías"},{"metadata":{"_uuid":"e2524e66838667e802b1fe2a999502b6554f4b8f","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport time\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport datetime as dt\nfrom datetime import datetime, date, time, timedelta\nimport calendar\n\nplt.style.use('seaborn')\nsns.set(font_scale=2)\n\nimport warnings \nwarnings.filterwarnings('ignore')\nimport os","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fdd83f326e1787df94e1a11e7a3bfa3e23dd32ac"},"cell_type":"markdown","source":"Crear entorno"},{"metadata":{"_uuid":"a99c0e803afb4d179d0c33f7bb73b4458fe3b43d","trusted":true},"cell_type":"code","source":"from kaggle.competitions import twosigmanews\n# You can only call make_env() once, so don't lose it!\nenv = twosigmanews.make_env()\nprint('Done!')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e08403c881b7963f4ad31a9a1ac31daca547125b"},"cell_type":"markdown","source":"Cargar datos de training"},{"metadata":{"_uuid":"048583ec640b32ca30a238ca81b5660fe0dbff59","trusted":true},"cell_type":"code","source":"(market_train_df, news_train_df) = env.get_training_data()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Cargamos datos de test, para tener a mano la plantilla que en principio habría que rellenar para submitir la competicion...\nNosotros no submitiremos, pero... vamos a basarnos en ella para realizar el cálculo de los rendimientos que se obtendrían aplicando el modelo"},{"metadata":{"trusted":true},"cell_type":"code","source":"days = env.get_prediction_days()\n(market_test_df, news_test_df, predictions_template_df) = next(days)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d9aff50dda8a73c529c3001f42cc0d508ec0f7aa"},"cell_type":"markdown","source":"Dadas las características del proyecto, de cara a poder medir la accuracy, simplemente consultamos las características de los ficheros de test... pero no vamos a usarlos en lo sucesivo, sino que vamos a coger un subconjunto de los ficheros de train, que tengan las mismas características: los datos de mercados del último día y los datos del fichero de noticias posteriores a las 22h del penúltimo día (más todos los del último día).\n\nPara poder simular un seguimiento durante varios días en lugar del último día, cogeremos los días de la última semana para reservarlos como test."},{"metadata":{"_uuid":"08c536a8702144f8b7cd44ac0ca45586e340d072","trusted":true},"cell_type":"code","source":"del market_test_df, news_test_df\n\nstart = '2016-06-30 22:00:00+00:00'\n\nmarket_test_df = market_train_df.loc[market_train_df['time'] > start]\nnews_test_df = news_train_df.loc[news_train_df['time'] > start]\n\nmarket_train_df = market_train_df.loc[market_train_df['time'] <= start].reset_index(drop=True)\nnews_train_df = news_train_df.loc[news_train_df['time'] <= start].reset_index(drop=True)\n\nmarket_train_df = market_train_df.loc[market_train_df['time'] >= '2010-01-01 22:00:00+0000']\nnews_train_df = news_train_df.loc[news_train_df['time'] >= '2009-12-31 22:00:00+0000']\n#to make it fast\n#market_train_df = market_train_df.loc[market_train_df['time'] >= '2016-01-01 22:00:00+0000']\n#news_train_df = news_train_df.loc[news_train_df['time'] >= '2015-12-31 22:00:00+0000']\n\nmarket_train_df['close_to_open'] =  np.abs(market_train_df['close'] / market_train_df['open'])\nmarket_train_df = market_train_df.loc[market_train_df['close_to_open'] > 0.5]\nmarket_train_df = market_train_df.loc[market_train_df['close_to_open'] < 2]\n\nmarket_test_df['close_to_open'] =  np.abs(market_test_df['close'] / market_test_df['open'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Para reproducir las condiciones reales, en las que el fichero de mercados de test no tendría las variables de los rendimientos en los siguientes 10 días y la de 'universe' (si la acción entraría en cotización), vamos a eliminar estas dos variables del fichero de test...\npero, para poder medir al final el resultado de la predicción, guardaremos estos datos en un fichero auxiliar."},{"metadata":{"trusted":true},"cell_type":"code","source":"aux_columns = ['assetCode', 'time', 'returnsOpenNextMktres10', 'universe']\nmarket_test_aux = market_test_df[aux_columns]\nmarket_test_aux.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"drop_test = ['returnsOpenNextMktres10', 'universe']\nmarket_test_df.drop(drop_test, axis=1, inplace=True)\nmarket_test_df.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"market_train_df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"De cara a ahorrar pasos, una vez preparado el fichero de mercados de test, volvemos a juntar los ficheros de training y test en uno sólo, para hacer las transformaciones una única vez"},{"metadata":{"trusted":true},"cell_type":"code","source":"market_dfs = [market_train_df, market_test_df]\n\nmarket_train_df = pd.concat(market_dfs)\n\nmarket_train_df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Y hacemos los mismo con los ficheros de noticias"},{"metadata":{"trusted":true},"cell_type":"code","source":"news_dfs = [news_train_df, news_test_df]\n\nnews_train_df = pd.concat(news_dfs)\n\nnews_train_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del market_test_df, news_test_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Preparamos los datos para hacer el merge que nos permitirá comenzar a ejecutar modelos"},{"metadata":{"_uuid":"431da71f2607c3b604f63435e9c4275ebaf618a1","trusted":true},"cell_type":"code","source":"def preprocess_news(news_train):\n    drop_list = [\n        'audiences', 'subjects', 'assetName',\n        'headline', 'firstCreated', 'sourceTimestamp',\n    ]\n    news_train.drop(drop_list, axis=1, inplace=True)\n    \n    # Factorize categorical columns\n    for col in ['headlineTag', 'provider', 'sourceId']:\n        news_train[col], uniques = pd.factorize(news_train[col])\n        del uniques\n    \n    # Remove {} and '' from assetCodes column\n    news_train['assetCodes'] = news_train['assetCodes'].apply(lambda x: x[1:-1].replace(\"'\", \"\"))\n    return news_train\n\nnews_train_df = preprocess_news(news_train_df)\n#news_test_df = preprocess_news(news_test_df)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6a260ae107acfbcbb332e0879243ebeab6e4d973","trusted":true},"cell_type":"code","source":"def unstack_asset_codes(news_train_df):\n    codes = []\n    indexes = []\n    for i, values in news_train_df['assetCodes'].iteritems():\n        explode = values.split(\", \")\n        codes.extend(explode)\n        repeat_index = [int(i)]*len(explode)\n        indexes.extend(repeat_index)\n    index_df = pd.DataFrame({'news_index': indexes, 'assetCode': codes})\n    del codes, indexes\n#    gc.collect()\n    return index_df\n\nindex_df = unstack_asset_codes(news_train_df)\n#index_df2 = unstack_asset_codes(news_test_df)\n\ndef merge_news_on_index(news_train_df, index_df):\n    news_train_df['news_index'] = news_train_df.index.copy()\n\n    # Merge news on unstacked assets\n    news_unstack = index_df.merge(news_train_df, how='left', on='news_index')\n    news_unstack.drop(['news_index', 'assetCodes'], axis=1, inplace=True)\n    return news_unstack\n\nnews_unstack = merge_news_on_index(news_train_df, index_df)\n#news_unstack2 = merge_news_on_index(news_test_df, index_df2)\n#del news_train_df, index_df, news_test_df, index_df2\ndel news_train_df, index_df\n#gc.collect()\n#news_unstack.head(3)\n\n\ndef group_news(news_frame):\n#    news_frame['date'] = news_frame.time.dt.date  # Add date column\n    news_frame['date'] = np.where(news_frame.time.dt.time < dt.time(22, 0, 0), news_frame.time.dt.date, news_frame.time.dt.date + timedelta(days=1))   \n#    news_frame['date'] = np.where(datetime.weekday(news_frame['date'])==5, news_frame['date'] + timedelta(days=2), news_frame['date'])\n#    news_frame['date'] = np.where(datetime.weekday(news_frame['date'])==6, news_frame['date'] + timedelta(days=1), news_frame['date'])\n    news_frame['weekday'] = np.where(news_frame.time.dt.time < dt.time(22, 0, 0), news_frame.time.dt.dayofweek, news_frame.time.dt.dayofweek + 1)\n    aggregations = ['mean']\n    gp = news_frame.groupby(['assetCode', 'date', 'weekday']).agg(aggregations)\n#    gp.columns = pd.Index([\"{}_{}\".format(e[0], e[1]) for e in gp.columns.tolist()])\n    gp.columns = pd.Index([e[0] for e in gp.columns.tolist()])\n    gp.reset_index(inplace=True)\n    # Set datatype to float32\n    float_cols = {c: 'float32' for c in gp.columns if c not in ['assetCode', 'date', 'weekday']}\n    return gp.astype(float_cols)\n\nnews_agg = group_news(news_unstack)\ndel news_unstack\n#news_agg2 = group_news(news_unstack2)\n#del news_unstack2\n#; gc.collect()\n#news_agg.head(3)\n\nnews_agg['weekday'][news_agg.weekday == 7] = 0\n#news_agg2['weekday'][news_agg2.weekday == 7] = 0","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Tratamiento sobre los festivos y sobre fines de semana"},{"metadata":{"trusted":true},"cell_type":"code","source":"news_agg['date'][news_agg.weekday == 5] = news_agg['date'][news_agg.weekday == 5] + timedelta(days=2)\nnews_agg['date'][news_agg.weekday == 6] = news_agg['date'][news_agg.weekday == 6] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.weekday == 5] = 0\nnews_agg['weekday'][news_agg.weekday == 6] = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"news_agg2['date'][news_agg2.weekday == 5] = news_agg2['date'][news_agg2.weekday == 5] + timedelta(days=2)\nnews_agg2['date'][news_agg2.weekday == 6] = news_agg2['date'][news_agg2.weekday == 6] + timedelta(days=1)\n\nnews_agg2['weekday'][news_agg2.weekday == 5] = 0\nnews_agg2['weekday'][news_agg2.weekday == 6] = 0"},{"metadata":{"_uuid":"ee4d34cd51a2990af3db9552f507a11c80314bda","trusted":true},"cell_type":"code","source":"news_agg['weekday'][news_agg.date == date(2010,1,1)] = news_agg['weekday'][news_agg.date == date(2010,1,1)] + 1\nnews_agg['date'][news_agg.date == date(2010,1,1)] = news_agg['date'][news_agg.date == date(2010,1,1)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2012,1,2)] = news_agg['weekday'][news_agg.date == date(2012,1,2)] + 1\nnews_agg['date'][news_agg.date == date(2012,1,2)] = news_agg['date'][news_agg.date == date(2012,1,2)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2013,1,1)] = news_agg['weekday'][news_agg.date == date(2013,1,1)] + 1\nnews_agg['date'][news_agg.date == date(2013,1,1)] = news_agg['date'][news_agg.date == date(2013,1,1)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2014,1,1)] = news_agg['weekday'][news_agg.date == date(2014,1,1)] + 1\nnews_agg['date'][news_agg.date == date(2014,1,1)] = news_agg['date'][news_agg.date == date(2014,1,1)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2015,1,1)] = news_agg['weekday'][news_agg.date == date(2015,1,1)] + 1\nnews_agg['date'][news_agg.date == date(2015,1,1)] = news_agg['date'][news_agg.date == date(2015,1,1)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2016,1,1)] = news_agg['weekday'][news_agg.date == date(2016,1,1)] + 1\nnews_agg['date'][news_agg.date == date(2016,1,1)] = news_agg['date'][news_agg.date == date(2016,1,1)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2010,1,18)] = news_agg['weekday'][news_agg.date == date(2010,1,18)] + 1\nnews_agg['date'][news_agg.date == date(2010,1,18)] = news_agg['date'][news_agg.date == date(2010,1,18)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2011,1,17)] = news_agg['weekday'][news_agg.date == date(2011,1,17)] + 1\nnews_agg['date'][news_agg.date == date(2011,1,17)] = news_agg['date'][news_agg.date == date(2011,1,17)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2012,1,16)] = news_agg['weekday'][news_agg.date == date(2012,1,16)] + 1\nnews_agg['date'][news_agg.date == date(2012,1,16)] = news_agg['date'][news_agg.date == date(2012,1,16)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2013,1,21)] = news_agg['weekday'][news_agg.date == date(2013,1,21)] + 1\nnews_agg['date'][news_agg.date == date(2013,1,21)] = news_agg['date'][news_agg.date == date(2013,1,21)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2014,1,20)] = news_agg['weekday'][news_agg.date == date(2014,1,20)] + 1\nnews_agg['date'][news_agg.date == date(2014,1,20)] = news_agg['date'][news_agg.date == date(2014,1,20)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2015,1,19)] = news_agg['weekday'][news_agg.date == date(2015,1,19)] + 1\nnews_agg['date'][news_agg.date == date(2015,1,19)] = news_agg['date'][news_agg.date == date(2015,1,19)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2016,1,18)] = news_agg['weekday'][news_agg.date == date(2016,1,18)] + 1\nnews_agg['date'][news_agg.date == date(2016,1,18)] = news_agg['date'][news_agg.date == date(2016,1,18)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2010,2,15)] = news_agg['weekday'][news_agg.date == date(2010,2,15)] + 1\nnews_agg['date'][news_agg.date == date(2010,2,15)] = news_agg['date'][news_agg.date == date(2010,2,15)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2011,2,21)] = news_agg['weekday'][news_agg.date == date(2011,2,21)] + 1\nnews_agg['date'][news_agg.date == date(2011,2,21)] = news_agg['date'][news_agg.date == date(2011,2,21)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2012,2,20)] = news_agg['weekday'][news_agg.date == date(2012,2,20)] + 1\nnews_agg['date'][news_agg.date == date(2012,2,20)] = news_agg['date'][news_agg.date == date(2012,2,20)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2013,2,18)] = news_agg['weekday'][news_agg.date == date(2013,2,18)] + 1\nnews_agg['date'][news_agg.date == date(2013,2,18)] = news_agg['date'][news_agg.date == date(2013,2,18)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2014,2,17)] = news_agg['weekday'][news_agg.date == date(2014,2,17)] + 1\nnews_agg['date'][news_agg.date == date(2014,2,17)] = news_agg['date'][news_agg.date == date(2014,2,17)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2015,2,16)] = news_agg['weekday'][news_agg.date == date(2015,2,16)] + 1\nnews_agg['date'][news_agg.date == date(2015,2,16)] = news_agg['date'][news_agg.date == date(2015,2,16)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2016,2,15)] = news_agg['weekday'][news_agg.date == date(2016,2,15)] + 1\nnews_agg['date'][news_agg.date == date(2016,2,15)] = news_agg['date'][news_agg.date == date(2016,2,15)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2010,4,2)] = news_agg['weekday'][news_agg.date == date(2010,4,2)] + 1\nnews_agg['date'][news_agg.date == date(2010,4,2)] = news_agg['date'][news_agg.date == date(2010,4,2)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2011,4,22)] = news_agg['weekday'][news_agg.date == date(2011,4,22)] + 1\nnews_agg['date'][news_agg.date == date(2011,4,22)] = news_agg['date'][news_agg.date == date(2011,4,22)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2012,4,6)] = news_agg['weekday'][news_agg.date == date(2012,4,6)] + 1\nnews_agg['date'][news_agg.date == date(2012,4,6)] = news_agg['date'][news_agg.date == date(2012,4,6)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2013,3,29)] = news_agg['weekday'][news_agg.date == date(2013,3,29)] + 1\nnews_agg['date'][news_agg.date == date(2013,3,29)] = news_agg['date'][news_agg.date == date(2013,3,29)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2014,4,18)] = news_agg['weekday'][news_agg.date == date(2014,4,18)] + 1\nnews_agg['date'][news_agg.date == date(2014,4,18)] = news_agg['date'][news_agg.date == date(2014,4,18)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2015,4,3)] = news_agg['weekday'][news_agg.date == date(2015,4,3)] + 1\nnews_agg['date'][news_agg.date == date(2015,4,3)] = news_agg['date'][news_agg.date == date(2015,4,3)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2016,3,25)] = news_agg['weekday'][news_agg.date == date(2016,3,25)] + 1\nnews_agg['date'][news_agg.date == date(2016,3,25)] = news_agg['date'][news_agg.date == date(2016,3,25)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2010,5,31)] = news_agg['weekday'][news_agg.date == date(2010,5,31)] + 1\nnews_agg['date'][news_agg.date == date(2010,5,31)] = news_agg['date'][news_agg.date == date(2010,5,31)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2011,5,30)] = news_agg['weekday'][news_agg.date == date(2011,5,30)] + 1\nnews_agg['date'][news_agg.date == date(2011,5,30)] = news_agg['date'][news_agg.date == date(2011,5,30)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2012,5,28)] = news_agg['weekday'][news_agg.date == date(2012,5,28)] + 1\nnews_agg['date'][news_agg.date == date(2012,5,28)] = news_agg['date'][news_agg.date == date(2012,5,28)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2013,5,27)] = news_agg['weekday'][news_agg.date == date(2013,5,27)] + 1\nnews_agg['date'][news_agg.date == date(2013,5,27)] = news_agg['date'][news_agg.date == date(2013,5,27)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2014,5,26)] = news_agg['weekday'][news_agg.date == date(2014,5,26)] + 1\nnews_agg['date'][news_agg.date == date(2014,5,26)] = news_agg['date'][news_agg.date == date(2014,5,26)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2015,5,25)] = news_agg['weekday'][news_agg.date == date(2015,5,25)] + 1\nnews_agg['date'][news_agg.date == date(2015,5,25)] = news_agg['date'][news_agg.date == date(2015,5,25)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2016,5,30)] = news_agg['weekday'][news_agg.date == date(2016,5,30)] + 1\nnews_agg['date'][news_agg.date == date(2016,5,30)] = news_agg['date'][news_agg.date == date(2016,5,30)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2010,7,5)] = news_agg['weekday'][news_agg.date == date(2010,7,5)] + 1\nnews_agg['date'][news_agg.date == date(2010,7,5)] = news_agg['date'][news_agg.date == date(2010,7,5)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2011,7,4)] = news_agg['weekday'][news_agg.date == date(2011,7,4)] + 1\nnews_agg['date'][news_agg.date == date(2011,7,4)] = news_agg['date'][news_agg.date == date(2011,7,4)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2012,7,4)] = news_agg['weekday'][news_agg.date == date(2012,7,4)] + 1\nnews_agg['date'][news_agg.date == date(2012,7,4)] = news_agg['date'][news_agg.date == date(2012,7,4)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2013,7,4)] = news_agg['weekday'][news_agg.date == date(2013,7,4)] + 1\nnews_agg['date'][news_agg.date == date(2013,7,4)] = news_agg['date'][news_agg.date == date(2013,7,4)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2014,7,4)] = news_agg['weekday'][news_agg.date == date(2014,7,4)] + 1\nnews_agg['date'][news_agg.date == date(2014,7,4)] = news_agg['date'][news_agg.date == date(2014,7,4)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2015,7,4)] = news_agg['weekday'][news_agg.date == date(2015,7,4)] + 1\nnews_agg['date'][news_agg.date == date(2015,7,4)] = news_agg['date'][news_agg.date == date(2015,7,4)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2016,7,4)] = news_agg['weekday'][news_agg.date == date(2016,7,4)] + 1\nnews_agg['date'][news_agg.date == date(2016,7,4)] = news_agg['date'][news_agg.date == date(2016,7,4)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2010,9,6)] = news_agg['weekday'][news_agg.date == date(2010,9,6)] + 1\nnews_agg['date'][news_agg.date == date(2010,9,6)] = news_agg['date'][news_agg.date == date(2010,9,6)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2011,9,5)] = news_agg['weekday'][news_agg.date == date(2011,9,5)] + 1\nnews_agg['date'][news_agg.date == date(2011,9,5)] = news_agg['date'][news_agg.date == date(2011,9,5)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2012,9,3)] = news_agg['weekday'][news_agg.date == date(2012,9,3)] + 1\nnews_agg['date'][news_agg.date == date(2012,9,3)] = news_agg['date'][news_agg.date == date(2012,9,3)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2013,9,2)] = news_agg['weekday'][news_agg.date == date(2013,9,2)] + 1\nnews_agg['date'][news_agg.date == date(2013,9,2)] = news_agg['date'][news_agg.date == date(2013,9,2)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2014,9,1)] = news_agg['weekday'][news_agg.date == date(2014,9,1)] + 1\nnews_agg['date'][news_agg.date == date(2014,9,1)] = news_agg['date'][news_agg.date == date(2014,9,1)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2015,9,7)] = news_agg['weekday'][news_agg.date == date(2015,9,7)] + 1\nnews_agg['date'][news_agg.date == date(2015,9,7)] = news_agg['date'][news_agg.date == date(2015,9,7)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2016,9,5)] = news_agg['weekday'][news_agg.date == date(2016,9,5)] + 1\nnews_agg['date'][news_agg.date == date(2016,9,5)] = news_agg['date'][news_agg.date == date(2016,9,5)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2010,11,25)] = news_agg['weekday'][news_agg.date == date(2010,11,25)] + 1\nnews_agg['date'][news_agg.date == date(2010,11,25)] = news_agg['date'][news_agg.date == date(2010,11,25)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2011,11,24)] = news_agg['weekday'][news_agg.date == date(2011,11,24)] + 1\nnews_agg['date'][news_agg.date == date(2011,11,24)] = news_agg['date'][news_agg.date == date(2011,11,24)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2012,11,22)] = news_agg['weekday'][news_agg.date == date(2012,11,22)] + 1\nnews_agg['date'][news_agg.date == date(2012,11,22)] = news_agg['date'][news_agg.date == date(2012,11,22)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2013,11,28)] = news_agg['weekday'][news_agg.date == date(2013,11,28)] + 1\nnews_agg['date'][news_agg.date == date(2013,11,28)] = news_agg['date'][news_agg.date == date(2013,11,28)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2014,11,27)] = news_agg['weekday'][news_agg.date == date(2014,11,27)] + 1\nnews_agg['date'][news_agg.date == date(2014,11,27)] = news_agg['date'][news_agg.date == date(2014,11,27)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2015,11,26)] = news_agg['weekday'][news_agg.date == date(2015,11,26)] + 1\nnews_agg['date'][news_agg.date == date(2015,11,26)] = news_agg['date'][news_agg.date == date(2015,11,26)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2016,11,24)] = news_agg['weekday'][news_agg.date == date(2016,11,24)] + 1\nnews_agg['date'][news_agg.date == date(2016,11,24)] = news_agg['date'][news_agg.date == date(2016,11,24)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2010,12,24)] = news_agg['weekday'][news_agg.date == date(2010,12,24)] + 1\nnews_agg['date'][news_agg.date == date(2010,12,24)] = news_agg['date'][news_agg.date == date(2010,12,24)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2011,12,26)] = news_agg['weekday'][news_agg.date == date(2011,12,26)] + 1\nnews_agg['date'][news_agg.date == date(2011,12,26)] = news_agg['date'][news_agg.date == date(2011,12,26)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2012,12,25)] = news_agg['weekday'][news_agg.date == date(2012,12,25)] + 1\nnews_agg['date'][news_agg.date == date(2012,12,25)] = news_agg['date'][news_agg.date == date(2012,12,25)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2013,12,25)] = news_agg['weekday'][news_agg.date == date(2013,12,25)] + 1\nnews_agg['date'][news_agg.date == date(2013,12,25)] = news_agg['date'][news_agg.date == date(2013,12,25)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2014,12,25)] = news_agg['weekday'][news_agg.date == date(2014,12,25)] + 1\nnews_agg['date'][news_agg.date == date(2014,12,25)] = news_agg['date'][news_agg.date == date(2014,12,25)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2015,12,25)] = news_agg['weekday'][news_agg.date == date(2015,12,25)] + 1\nnews_agg['date'][news_agg.date == date(2015,12,25)] = news_agg['date'][news_agg.date == date(2015,12,25)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2016,12,26)] = news_agg['weekday'][news_agg.date == date(2016,12,26)] + 1\nnews_agg['date'][news_agg.date == date(2016,12,26)] = news_agg['date'][news_agg.date == date(2016,12,26)] + timedelta(days=1)\n\n#news_agg['weekday'][news_agg.weekday == 7] = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"news_agg2['weekday'][news_agg2.date == date(2016,12,26)] = news_agg2['weekday'][news_agg2.date == date(2016,12,26)] + 1\nnews_agg2['date'][news_agg2.date == date(2016,12,26)] = news_agg2['date'][news_agg2.date == date(2016,12,26)] + timedelta(days=1)\n\n#news_agg2['weekday'][news_agg2.weekday == 7] = 0"},{"metadata":{"_uuid":"4fce005f4edf5f343ab37632dce6eab4d65c66e5","trusted":true},"cell_type":"code","source":"news_agg['date'][news_agg.weekday == 5] = news_agg['date'][news_agg.weekday == 5] + timedelta(days=2)\nnews_agg['date'][news_agg.weekday == 6] = news_agg['date'][news_agg.weekday == 6] + timedelta(days=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"news_agg2['date'][news_agg2.weekday == 5] = news_agg2['date'][news_agg2.weekday == 5] + timedelta(days=2)\nnews_agg2['date'][news_agg2.weekday == 6] = news_agg2['date'][news_agg2.weekday == 6] + timedelta(days=1)"},{"metadata":{"_uuid":"ea32c7ba7a64c589f4b3db94a430a8418af5c998","trusted":true},"cell_type":"code","source":"def group_news(news_frame):\n    aggregations = ['mean']\n    gp = news_frame.groupby(['assetCode', 'date']).agg(aggregations)\n#    gp.columns = pd.Index([\"{}_{}\".format(e[0], e[1]) for e in gp.columns.tolist()])\n    gp.columns = pd.Index([e[0] for e in gp.columns.tolist()])\n    gp.reset_index(inplace=True)\n    # Set datatype to float32\n    float_cols = {c: 'float32' for c in gp.columns if c not in ['assetCode', 'date']}\n    return gp.astype(float_cols)\n\nnews_aggr = group_news(news_agg)\ndel news_agg\n#news_aggr2 = group_news(news_agg2)\n#del news_agg2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Generamos el fichero con el que vamos a trabajar que resulta del merge de los ficheros de mercados y de noticias"},{"metadata":{"trusted":true},"cell_type":"code","source":"market_train_df['date'] = market_train_df.time.dt.date\nfull_train_df = market_train_df.merge(news_aggr, how='left', on=['assetCode', 'date'])\ndel market_train_df, news_aggr\nfull_train_df.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"market_test_df['date'] = market_test_df.time.dt.date\nfull_test_df = market_test_df.merge(news_aggr2, how='left', on=['assetCode', 'date'])\ndel market_test_df, news_aggr2\nfull_test_df.head(5)"},{"metadata":{"_uuid":"320b63b310cda36592324ac9d6453a71c16e84b9"},"cell_type":"markdown","source":"## 5.2 Preparación de los datos"},{"metadata":{},"cell_type":"markdown","source":"## pensar justificación en elección de variables... basada en el EDA... Y ver si quitar las variables calculadas y poner las que hay a pelo\n"},{"metadata":{},"cell_type":"markdown","source":"Antes de nada, vamos a hacer encoding de los asset codes para poder incluirlos en nuestro modelo"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\nfull_train_df[\"assetToken\"] = LabelEncoder().fit_transform(full_train_df[\"assetCode\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A continuación, dividimos el fichero en train y test"},{"metadata":{"trusted":true},"cell_type":"code","source":"start = '2016-06-30 22:00:00+00:00'\n\nfull_test_df = full_train_df.loc[full_train_df['time'] > start]\nfull_train_df = full_train_df.loc[full_train_df['time'] <= start].reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Eliminamos del fichero de test las columnas de los rendimientos en los siguientes 10 días y de la variable 'universe' (que, por otro lado, están rellenas por nulos, ya que antes de juntar los ficheros, habíamos eliminado esas variables del fichero de test)"},{"metadata":{"trusted":true},"cell_type":"code","source":"drop_test = ['returnsOpenNextMktres10', 'universe']\nfull_test_df.drop(drop_test, axis=1, inplace=True)\nfull_test_df.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"full_train_df = full_train_df[pd.notnull(full_train_df['urgency'])]\nfull_train_df['returnsClosePrevMktres1'] = np.where(np.isnan(full_train_df['returnsClosePrevMktres1']), full_train_df['returnsClosePrevRaw1'], full_train_df['returnsClosePrevMktres1'])\nfull_train_df['returnsOpenPrevMktres1'] = np.where(np.isnan(full_train_df['returnsOpenPrevMktres1']), full_train_df['returnsOpenPrevRaw1'], full_train_df['returnsOpenPrevMktres1'])\nfull_train_df['returnsClosePrevMktres10'] = np.where(np.isnan(full_train_df['returnsClosePrevMktres10']), full_train_df['returnsClosePrevRaw10'], full_train_df['returnsClosePrevMktres10']) \nfull_train_df['returnsOpenPrevMktres10'] = np.where(np.isnan(full_train_df['returnsOpenPrevMktres10']), full_train_df['returnsOpenPrevRaw10'], full_train_df['returnsOpenPrevMktres10'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot correlation\ncolumns_corr = ['returnsOpenNextMktres10', 'volume', 'close', 'open', 'returnsClosePrevRaw1', 'returnsOpenPrevRaw1', 'returnsClosePrevRaw10', 'returnsOpenPrevRaw10',\n           'returnsClosePrevMktres1', 'returnsOpenPrevMktres1', 'returnsClosePrevMktres10', 'returnsOpenPrevMktres10',\n               'assetToken']\ncolormap = plt.cm.RdBu\nplt.figure(figsize=(18,15))\nsns.heatmap(full_train_df[columns_corr].astype(float).corr(), linewidths=0.1, vmax=1.0, vmin=-1., square=True, cmap=colormap, linecolor='white', annot=True)\nplt.title('Pair-wise correlation')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot correlation\ncolumns_corr = ['returnsOpenNextMktres10', 'sourceId', 'urgency', 'takeSequence', \n                'provider', 'bodySize', 'companyCount', 'headlineTag',\n                'marketCommentary','sentenceCount', 'wordCount',\n                'firstMentionSentence','relevance','sentimentClass',\n                'sentimentNegative', 'sentimentNeutral', 'sentimentPositive',\n                'sentimentWordCount','noveltyCount12H','noveltyCount24H',\n                'noveltyCount3D','noveltyCount5D', 'noveltyCount7D','volumeCounts12H',\n                'volumeCounts24H','volumeCounts3D','volumeCounts5D','volumeCounts7D']\ncolormap = plt.cm.RdBu\nplt.figure(figsize=(18,15))\nsns.heatmap(full_train_df[columns_corr].astype(float).corr(), linewidths=0.1, vmax=1.0, vmin=-1., square=True, cmap=colormap, linecolor='white', annot=True)\nplt.title('Pair-wise correlation')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"full_train_df['average'] = (full_train_df['close'] + full_train_df['open'])/2\nfull_train_df['pricevolume'] = full_train_df['volume'] * full_train_df['close']\nfull_train_df['position'] = full_train_df['firstMentionSentence'] / full_train_df['sentenceCount']\nfull_train_df['coverage'] = full_train_df['sentimentWordCount'] / full_train_df['wordCount']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"columns_corr = ['returnsOpenNextMktres10', 'volume', 'close', 'open', 'returnsClosePrevRaw1', 'returnsOpenPrevRaw1', 'returnsClosePrevRaw10', 'returnsOpenPrevRaw10',\n           'returnsClosePrevMktres1', 'returnsOpenPrevMktres1', 'returnsClosePrevMktres10', 'returnsOpenPrevMktres10',\n               'assetToken', 'average', 'pricevolume', 'close_to_open']\ncolormap = plt.cm.RdBu\nplt.figure(figsize=(18,15))\nsns.heatmap(full_train_df[columns_corr].astype(float).corr(), linewidths=0.1, vmax=1.0, vmin=-1., square=True, cmap=colormap, linecolor='white', annot=True)\nplt.title('Pair-wise correlation')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"columns_corr = ['returnsOpenNextMktres10', 'sourceId', 'urgency', 'takeSequence', \n                'provider', 'bodySize', 'companyCount', 'headlineTag',\n                'marketCommentary','sentenceCount', 'wordCount',\n                'firstMentionSentence','relevance','sentimentClass',\n                'sentimentNegative', 'sentimentNeutral', 'sentimentPositive',\n                'sentimentWordCount','noveltyCount12H','noveltyCount24H',\n                'noveltyCount3D','noveltyCount5D', 'noveltyCount7D','volumeCounts12H',\n                'volumeCounts24H','volumeCounts3D','volumeCounts5D','volumeCounts7D', \n                'position', 'coverage']\ncolormap = plt.cm.RdBu\nplt.figure(figsize=(18,15))\nsns.heatmap(full_train_df[columns_corr].astype(float).corr(), linewidths=0.1, vmax=1.0, vmin=-1., square=True, cmap=colormap, linecolor='white', annot=True)\nplt.title('Pair-wise correlation')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"columns_corr = ['returnsOpenNextMktres10', 'volume', 'close', 'open', 'returnsClosePrevRaw1', 'returnsOpenPrevRaw1', 'returnsClosePrevRaw10', 'returnsOpenPrevRaw10',\n           'returnsClosePrevMktres1', 'returnsOpenPrevMktres1', 'returnsClosePrevMktres10', 'returnsOpenPrevMktres10',\n               'assetToken', 'average', 'pricevolume', 'close_to_open', 'sourceId', 'urgency', 'takeSequence', \n                'provider', 'bodySize', 'companyCount', 'headlineTag',\n                'marketCommentary','sentenceCount', 'wordCount',\n                'firstMentionSentence','relevance','sentimentClass',\n                'sentimentNegative', 'sentimentNeutral', 'sentimentPositive',\n                'sentimentWordCount','noveltyCount12H','noveltyCount24H',\n                'noveltyCount3D','noveltyCount5D', 'noveltyCount7D','volumeCounts12H',\n                'volumeCounts24H','volumeCounts3D','volumeCounts5D','volumeCounts7D', \n                'position', 'coverage']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeRegressor\nimport matplotlib.pyplot as plt\n#from sklearn.preprocessing import MinMaxScaler\n\ndata = full_train_df\n#data[columns_corr] = MinMaxScaler().fit_transform(data[columns_corr])\nnot_target = [col for col in columns_corr if col not in ['returnsOpenNextMktres10', 'sourceId']]\nX = data[not_target]  #independent columns\ny = data['returnsOpenNextMktres10']    #target column i.e price range\n\nmodel = DecisionTreeRegressor()\nmodel.fit(X,y)\nprint(model.feature_importances_) #use inbuilt class feature_importances of tree based classifiers\n#plot graph of feature importances for better visualization\nfeat_importances = pd.Series(model.feature_importances_, index=X.columns)\nfeat_importances.nlargest(19).plot(kind='barh')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeRegressor\nimport matplotlib.pyplot as plt\n#from sklearn.preprocessing import MinMaxScaler\n\ndata = full_train_df\n#data[columns_corr] = MinMaxScaler().fit_transform(data[columns_corr])\nnot_target = [col for col in columns_corr if col not in ['returnsOpenNextMktres10', 'sourceId']]\nX = data[not_target]  #independent columns\ny = data['returnsOpenNextMktres10']    #target column i.e price range\n\nmodel = DecisionTreeRegressor()\nmodel.fit(X,y)\nprint(model.feature_importances_) #use inbuilt class feature_importances of tree based classifiers\n#plot graph of feature importances for better visualization\nfeat_importances = pd.Series(model.feature_importances_, index=X.columns)\nfeat_importances.nlargest(18).plot(kind='barh')\nplt.yticks(size = 10)\nplt.xticks(size = 10)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"from sklearn.tree import DecisionTreeRegressor\nimport matplotlib.pyplot as plt\n#from sklearn.preprocessing import MinMaxScaler\n\ndata = full_train_df\n#data[columns_corr] = MinMaxScaler().fit_transform(data[columns_corr])\nnot_target = [col for col in columns_corr if col not in ['returnsOpenNextMktres10', 'sourceId']]\nX = data[not_target]  #independent columns\ny = data['returnsOpenNextMktres10']    #target column i.e price range\n\nmodel = DecisionTreeRegressor()\nmodel.fit(X,y)\nprint(model.feature_importances_) #use inbuilt class feature_importances of tree based classifiers\n#plot graph of feature importances for better visualization\nfeat_importances = pd.Series(model.feature_importances_, index=X.columns)\nfeat_importances.nlargest(17).plot(kind='barh')\nplt.show()"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"from sklearn.tree import DecisionTreeRegressor\nimport matplotlib.pyplot as plt\n#from sklearn.preprocessing import MinMaxScaler\n\ndata = full_train_df\n#data[columns_corr] = MinMaxScaler().fit_transform(data[columns_corr])\nnot_target = [col for col in columns_corr if col not in ['returnsOpenNextMktres10', 'sourceId']]\nX = data[not_target]  #independent columns\ny = data['returnsOpenNextMktres10']    #target column i.e price range\n\nmodel = DecisionTreeRegressor()\nmodel.fit(X,y)\nprint(model.feature_importances_) #use inbuilt class feature_importances of tree based classifiers\n#plot graph of feature importances for better visualization\nfeat_importances = pd.Series(model.feature_importances_, index=X.columns)\nfeat_importances.nlargest(16).plot(kind='barh')\nplt.show()"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"from sklearn.tree import DecisionTreeRegressor\nimport matplotlib.pyplot as plt\n#from sklearn.preprocessing import MinMaxScaler\n\ndata = full_train_df\n#data[columns_corr] = MinMaxScaler().fit_transform(data[columns_corr])\nnot_target = [col for col in columns_corr if col not in ['returnsOpenNextMktres10', 'sourceId']]\nX = data[not_target]  #independent columns\ny = data['returnsOpenNextMktres10']    #target column i.e price range\n\nmodel = DecisionTreeRegressor()\nmodel.fit(X,y)\nprint(model.feature_importances_) #use inbuilt class feature_importances of tree based classifiers\n#plot graph of feature importances for better visualization\nfeat_importances = pd.Series(model.feature_importances_, index=X.columns)\nfeat_importances.nlargest(15).plot(kind='barh')\nplt.show()"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"from sklearn.tree import DecisionTreeRegressor\nimport matplotlib.pyplot as plt\n#from sklearn.preprocessing import MinMaxScaler\n\ndata = full_train_df\n#data[columns_corr] = MinMaxScaler().fit_transform(data[columns_corr])\nnot_target = [col for col in columns_corr if col not in ['returnsOpenNextMktres10', 'sourceId']]\nX = data[not_target]  #independent columns\ny = data['returnsOpenNextMktres10']    #target column i.e price range\n\nmodel = DecisionTreeRegressor()\nmodel.fit(X,y)\nprint(model.feature_importances_) #use inbuilt class feature_importances of tree based classifiers\n#plot graph of feature importances for better visualization\nfeat_importances = pd.Series(model.feature_importances_, index=X.columns)\nfeat_importances.nlargest(14).plot(kind='barh')\nplt.show()"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"from sklearn.tree import DecisionTreeRegressor\nimport matplotlib.pyplot as plt\n#from sklearn.preprocessing import MinMaxScaler\n\ndata = full_train_df\n#data[columns_corr] = MinMaxScaler().fit_transform(data[columns_corr])\nnot_target = [col for col in columns_corr if col not in ['returnsOpenNextMktres10', 'sourceId']]\nX = data[not_target]  #independent columns\ny = data['returnsOpenNextMktres10']    #target column i.e price range\n\nmodel = DecisionTreeRegressor()\nmodel.fit(X,y)\nprint(model.feature_importances_) #use inbuilt class feature_importances of tree based classifiers\n#plot graph of feature importances for better visualization\nfeat_importances = pd.Series(model.feature_importances_, index=X.columns)\nfeat_importances.nlargest(13).plot(kind='barh')\nplt.show()"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"from sklearn.tree import DecisionTreeRegressor\nimport matplotlib.pyplot as plt\n#from sklearn.preprocessing import MinMaxScaler\n\ndata = full_train_df\n#data[columns_corr] = MinMaxScaler().fit_transform(data[columns_corr])\nnot_target = [col for col in columns_corr if col not in ['returnsOpenNextMktres10', 'sourceId']]\nX = data[not_target]  #independent columns\ny = data['returnsOpenNextMktres10']    #target column i.e price range\n\nmodel = DecisionTreeRegressor()\nmodel.fit(X,y)\nprint(model.feature_importances_) #use inbuilt class feature_importances of tree based classifiers\n#plot graph of feature importances for better visualization\nfeat_importances = pd.Series(model.feature_importances_, index=X.columns)\nfeat_importances.nlargest(12).plot(kind='barh')\nplt.show()"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"from sklearn.tree import DecisionTreeRegressor\nimport matplotlib.pyplot as plt\n#from sklearn.preprocessing import MinMaxScaler\n\ndata = full_train_df\n#data[columns_corr] = MinMaxScaler().fit_transform(data[columns_corr])\nnot_target = [col for col in columns_corr if col not in ['returnsOpenNextMktres10', 'sourceId']]\nX = data[not_target]  #independent columns\ny = data['returnsOpenNextMktres10']    #target column i.e price range\n\nmodel = DecisionTreeRegressor()\nmodel.fit(X,y)\nprint(model.feature_importances_) #use inbuilt class feature_importances of tree based classifiers\n#plot graph of feature importances for better visualization\nfeat_importances = pd.Series(model.feature_importances_, index=X.columns)\nfeat_importances.nlargest(11).plot(kind='barh')\nplt.show()"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"from sklearn.tree import DecisionTreeRegressor\nimport matplotlib.pyplot as plt\n#from sklearn.preprocessing import MinMaxScaler\n\ndata = full_train_df\n#data[columns_corr] = MinMaxScaler().fit_transform(data[columns_corr])\nnot_target = [col for col in columns_corr if col not in ['returnsOpenNextMktres10', 'sourceId']]\nX = data[not_target]  #independent columns\ny = data['returnsOpenNextMktres10']    #target column i.e price range\n\nmodel = DecisionTreeRegressor()\nmodel.fit(X,y)\nprint(model.feature_importances_) #use inbuilt class feature_importances of tree based classifiers\n#plot graph of feature importances for better visualization\nfeat_importances = pd.Series(model.feature_importances_, index=X.columns)\nfeat_importances.nlargest(10).plot(kind='barh')\nplt.show()"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"from sklearn.tree import DecisionTreeRegressor\nimport matplotlib.pyplot as plt\n#from sklearn.preprocessing import MinMaxScaler\n\ndata = full_train_df\n#data[columns_corr] = MinMaxScaler().fit_transform(data[columns_corr])\nnot_target = [col for col in columns_corr if col not in ['returnsOpenNextMktres10', 'sourceId']]\nX = data[not_target]  #independent columns\ny = data['returnsOpenNextMktres10']    #target column i.e price range\n\nmodel = DecisionTreeRegressor()\nmodel.fit(X,y)\nprint(model.feature_importances_) #use inbuilt class feature_importances of tree based classifiers\n#plot graph of feature importances for better visualization\nfeat_importances = pd.Series(model.feature_importances_, index=X.columns)\nfeat_importances.nlargest(10).plot(kind='barh')\nplt.show()"},{"metadata":{"trusted":true},"cell_type":"code","source":"data['return'] = np.where(data.returnsOpenNextMktres10 > np.percentile(y,87.5), 4, \n                          np.where(data.returnsOpenNextMktres10 > np.percentile(y,75), 3, \n                                   np.where(data.returnsOpenNextMktres10 > np.percentile(y,62.5), 2, \n                                            np.where(data.returnsOpenNextMktres10 > np.percentile(y,50), 1, \n                                                     np.where(data.returnsOpenNextMktres10 > np.percentile(y,37.5), 0, \n                                                              np.where(data.returnsOpenNextMktres10 > np.percentile(y,25), -1, \n                                                                       np.where(data.returnsOpenNextMktres10 > np.percentile(y,12.5), -2, -3)))))))\n\ny = data['return']    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.feature_selection import SelectFromModel\nclf = ExtraTreesClassifier(n_estimators=50)\nclf = clf.fit(X, y)\nclf.feature_importances_  \n#model = SelectFromModel(clf, prefit=True)\n#X_new = model.transform(X)\n#X_new.shape          ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"variables = np.asarray(not_target)\n\nvarimp_df = pd.DataFrame({'variable': variables, 'importance': clf.feature_importances_})\nvarimp_df.sort_values(by=['importance'], ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib \nmatplotlib.rc('xtick', labelsize=15) \nmatplotlib.rc('ytick', labelsize=15) \nplt.figure(figsize=(10,5))\nfeat_imp = pd.Series(clf.feature_importances_, variables).sort_values(ascending=False)\nfeat_imp.plot(kind='bar', title='Feature Importances')\nplt.ylabel('Feature Importance Score')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\nfrom sklearn.preprocessing import MinMaxScaler\n\ndata = full_train_df\ndata[columns_corr + 'return'] = MinMaxScaler().fit_transform(data[columns_corr + 'return'])\nnot_target = [col for col in columns_corr if col not in ['returnsOpenNextMktres10']]\nX = data[not_target]  #independent columns\ny = data['return']    #target column i.e price range\n#apply SelectKBest class to extract top 10 best features\nbestfeatures = SelectKBest(score_func=chi2, k=2)\nfit = bestfeatures.fit(X,y)\ndfscores = pd.DataFrame(fit.scores_)\ndfcolumns = pd.DataFrame(X.columns)\n#concat two dataframes for better visualization \nfeatureScores = pd.concat([dfcolumns,dfscores],axis=1)\nfeatureScores.columns = ['Specs','Score']  #naming the dataframe columns\nprint(featureScores.nlargest(2,'Score'))  #print 10 best features"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\nfrom sklearn.preprocessing import MinMaxScaler\n\ndata = full_train_df\ndata[columns_corr + 'return'] = MinMaxScaler().fit_transform(data[columns_corr + 'return'])\nnot_target = [col for col in columns_corr if col not in ['returnsOpenNextMktres10']]\nX = data[not_target]  #independent columns\ny = data['return']    #target column i.e price range\n#apply SelectKBest class to extract top 10 best features\nbestfeatures = SelectKBest(score_func=chi2, k=2)\nfit = bestfeatures.fit(X,y)\ndfscores = pd.DataFrame(fit.scores_)\ndfcolumns = pd.DataFrame(X.columns)\n#concat two dataframes for better visualization \nfeatureScores = pd.concat([dfcolumns,dfscores],axis=1)\nfeatureScores.columns = ['Specs','Score']  #naming the dataframe columns\nprint(featureScores.nlargest(2,'Score'))  #print 10 best features"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Recursive Feature Elimination\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LogisticRegression\n# create a base classifier used to evaluate a subset of attributes\nmodel = LogisticRegression()\n# create the RFE model and select 3 attributes\nrfe = RFE(model, 2)\nrfe = rfe.fit(X, y)\n# summarize the selection of the attributes\nprint(rfe.support_)\nprint(rfe.ranking_)"},{"metadata":{},"cell_type":"markdown","source":"import matplotlib.pyplot as plt\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.feature_selection import RFECV\n# Create the RFE object and compute a cross-validated score.\nsvc = SVC(kernel=\"linear\")\n# The \"accuracy\" scoring is proportional to the number of correct\n# classifications\nrfecv = RFECV(estimator=svc, step=1, cv=StratifiedKFold(2),\n              scoring='accuracy')\nrfecv.fit(X, y)\n\nprint(\"Optimal number of features : %d\" % rfecv.n_features_)\n\n# Plot number of features VS. cross-validation scores\nplt.figure()\nplt.xlabel(\"Number of features selected\")\nplt.ylabel(\"Cross validation score (nb of correct classifications)\")\nplt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\nplt.show()"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":1}