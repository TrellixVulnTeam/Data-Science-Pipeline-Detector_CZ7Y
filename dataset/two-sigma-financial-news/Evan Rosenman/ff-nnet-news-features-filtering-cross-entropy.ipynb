{"cells":[{"metadata":{"trusted":true,"_uuid":"469cb5d07ef0e42543e4468c6b6af1d4c88220de"},"cell_type":"code","source":"# define constants\nsplitYear = 2014\nfirstYear = 2011\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"78e4c696215ba1d03463364e10a4631bce7599e8"},"cell_type":"markdown","source":"# Make the TensorFlow Graph"},{"metadata":{"trusted":true,"_uuid":"122a957d285fcfce1d962f0cc07eb5e09cda381a"},"cell_type":"code","source":"# constants\ntrainDim = 63\nnum_epochs = 25\nminibatch_size = 64\nseed = 0\nhidden_size = 32\nlr = 0.001","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1cf40def0f70291f0095bbd62a440d7649c37e68"},"cell_type":"code","source":"# okay let's actually set up a tensor flow graph\nimport tensorflow as tf\n\n# placeholders\ntf.reset_default_graph()\ninputs = tf.placeholder(tf.float32, shape=(None, trainDim), name='inputs')\nlabels = tf.placeholder(tf.float32, shape=(None, 1), name='labels')\n\n# First layer\nW1 = tf.get_variable(\"W1\", shape=[trainDim, hidden_size],\\\n           initializer=tf.contrib.layers.xavier_initializer())\nb1 = tf.get_variable(\"b1\", shape=[1, hidden_size],\\\n           initializer=tf.zeros_initializer())\nW2 = tf.get_variable(\"W2\", [hidden_size, 1],\\\n            initializer = tf.contrib.layers.xavier_initializer())\nb2 = tf.get_variable(\"b2\", [1, 1], initializer = tf.zeros_initializer())\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"caced4c9aa6a74fec107641b671e0247bf1d0731"},"cell_type":"code","source":"# set up the relationships\nZ1 = tf.add(tf.matmul(inputs, W1), b1)\nA1 = tf.nn.sigmoid(Z1)\nZ2 = tf.add(tf.matmul(A1, W2), b2) \ncost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels = labels, logits = Z2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"83c3d43b3c985b6c21c496a1377f8d6af732bd10"},"cell_type":"code","source":"# optimizer and cost\noptimizer = tf.train.AdamOptimizer(learning_rate = lr).minimize(cost)\ninit = tf.global_variables_initializer()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f01f498729662689c63b3689f87493cc8303e7a4"},"cell_type":"markdown","source":"# Load in the Data"},{"metadata":{"trusted":true,"_uuid":"cfa8129f6d380afb33107c10a0bedc781398db9f"},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom itertools import chain","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"from kaggle.competitions import twosigmanews\n# You can only call make_env() once, so don't lose it!\nenv = twosigmanews.make_env()\nprint('Done!')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c20fa6deeac9d374c98774abd90bdc76b023ee63"},"cell_type":"code","source":"(market_train_df, news_train_df) = env.get_training_data()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cc0acfc203edff687d451b30644d15575a35a5ff"},"cell_type":"code","source":"# add outcome and year data\nmarket_train_df['y'] = ((market_train_df.returnsOpenNextMktres10 > 0).values).astype(int)\nmarket_train_df['year'] = pd.to_datetime(market_train_df.time).dt.year\nnews_train_df['year'] = pd.to_datetime(news_train_df.time).dt.year","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"256c2cdf29f5e02f55402d76bc62a461eba40d21"},"cell_type":"code","source":"# drop years we won't need\nmarket_train_df = market_train_df.drop(market_train_df[market_train_df['year'] < firstYear].index)\nnews_train_df = news_train_df.drop(news_train_df[news_train_df['year'] < firstYear].index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6cd2d03e8a07019bde78c021899496901ef2bf34"},"cell_type":"code","source":"news_cols_agg = {\n#    'urgency': ['min', 'count'],\n#    'takeSequence': ['max'],\n#    'bodySize': ['min', 'max', 'mean', 'std'],\n    'wordCount': ['min', 'max', 'mean', 'std'],\n    'sentenceCount': ['min', 'max', 'mean', 'std'],\n    'companyCount': ['min', 'max', 'mean', 'std'],\n    'marketCommentary': ['min', 'max', 'mean', 'std'],\n    'relevance': ['min', 'max', 'mean', 'std'],\n    'sentimentNegative': ['min', 'max', 'mean', 'std'],\n    'sentimentNeutral': ['min', 'max', 'mean', 'std'],\n    'sentimentPositive': ['min', 'max', 'mean', 'std'],\n    'sentimentWordCount': ['min', 'max', 'mean', 'std'],\n    'noveltyCount12H': ['min', 'max', 'mean', 'std'],\n#    'noveltyCount24H': ['min', 'max', 'mean', 'std'],\n#    'noveltyCount3D': ['min', 'max', 'mean', 'std'],\n#    'noveltyCount5D': ['min', 'max', 'mean', 'std'],\n    'noveltyCount7D': ['min', 'max', 'mean', 'std'],\n    'volumeCounts12H': ['min', 'max', 'mean', 'std'],\n#    'volumeCounts24H': ['min', 'max', 'mean', 'std'],\n#    'volumeCounts3D': ['min', 'max', 'mean', 'std'],\n#    'volumeCounts5D': ['min', 'max', 'mean', 'std'],\n    'volumeCounts7D': ['min', 'max', 'mean', 'std']\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"136393d504dce97d86863b8720f03999d6c54ad4"},"cell_type":"code","source":"def join_market_news(market_train_df, news_train_df):\n    # Fix asset codes (str -> list)\n    news_train_df['assetCodes'] = news_train_df['assetCodes'].str.findall(f\"'([\\w\\./]+)'\")    \n    \n    # Expand assetCodes\n    assetCodes_expanded = list(chain(*news_train_df['assetCodes']))\n    assetCodes_index = news_train_df.index.repeat( news_train_df['assetCodes'].apply(len) )\n\n    assert len(assetCodes_index) == len(assetCodes_expanded)\n    df_assetCodes = pd.DataFrame({'level_0': assetCodes_index, 'assetCode': assetCodes_expanded})\n\n    # Create expandaded news (will repeat every assetCodes' row)\n    news_cols = ['time', 'assetCodes'] + sorted(news_cols_agg.keys())\n    news_train_df_expanded = pd.merge(df_assetCodes, news_train_df[news_cols], left_on='level_0', right_index=True, suffixes=(['','_old']))\n\n    # Free memory\n    del news_train_df, df_assetCodes\n\n    # Aggregate numerical news features\n    news_train_df_aggregated = news_train_df_expanded.groupby(['time', 'assetCode']).agg(news_cols_agg)\n    \n    # Free memory\n    del news_train_df_expanded\n\n    # Convert to float32 to save memory\n    news_train_df_aggregated = news_train_df_aggregated.apply(np.float32)\n\n    # Flat columns\n    news_train_df_aggregated.columns = ['_'.join(col).strip() for col in news_train_df_aggregated.columns.values]\n\n    # Join with train\n    market_train_df = market_train_df.join(news_train_df_aggregated, on=['time', 'assetCode'])\n\n    # Free memory\n    del news_train_df_aggregated\n    \n    return market_train_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2a2a132908ef5c848a90061ac4fc942c29130d12"},"cell_type":"code","source":"def get_xy(market_train_df, news_train_df, le=None):\n    x, le = get_x(market_train_df, news_train_df)\n    y = market_train_df['returnsOpenNextMktres10'].clip(-1, 1)\n    return x, y, le\n\n\ndef label_encode(series, min_count):\n    vc = series.value_counts()\n    le = {c:i for i, c in enumerate(vc.index[vc >= min_count])}\n    return le\n\n\ndef get_x(market_train_df, news_train_df, le=None):\n    # Split date into before and after 22h (the time used in train data)\n    # E.g: 2007-03-07 23:26:39+00:00 -> 2007-03-08 00:00:00+00:00 (next day)\n    #      2009-02-25 21:00:50+00:00 -> 2009-02-25 00:00:00+00:00 (current day)\n    news_train_df['time'] = (news_train_df['time'] - np.timedelta64(22,'h')).dt.ceil('1D')\n\n    # Round time of market_train_df to 0h of curret day\n    market_train_df['time'] = market_train_df['time'].dt.floor('1D')\n\n    # Join market and news\n    x = join_market_news(market_train_df, news_train_df)\n    \n    # If not label-encoder... encode assetCode\n    if le is None:\n        le_assetCode = label_encode(x['assetCode'], min_count=10)\n        le_assetName = label_encode(x['assetName'], min_count=5)\n    else:\n        # 'unpack' label encoders\n        le_assetCode, le_assetName = le\n        \n    x['assetCode'] = x['assetCode'].map(le_assetCode).fillna(-1).astype(int)\n    x['assetName'] = x['assetName'].map(le_assetName).fillna(-1).astype(int)\n    \n    try:\n        x.drop(columns=['returnsOpenNextMktres10'], inplace=True)\n    except:\n        pass\n    try:\n        x.drop(columns=['universe'], inplace=True)\n    except:\n        pass\n    x['dayofweek'], x['month'] = x.time.dt.dayofweek, x.time.dt.month\n    x.drop(columns='time', inplace=True)\n#    x.fillna(-1000,inplace=True)\n\n    # Fix some mixed-type columns\n#    for bogus_col in ['marketCommentary_min', 'marketCommentary_max']:\n#        x[bogus_col] = x[bogus_col].astype(float)\n    \n    return x, (le_assetCode, le_assetName)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bb7d86e1e1055c9bcd58f515ed9168621db0ec3e"},"cell_type":"code","source":"# This will take some time...\nX, y, le = get_xy(market_train_df, news_train_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bb70d1c931b802e93e9bf077e1d4cca97260fbc1"},"cell_type":"code","source":"# filter out the examples that do not have news features  \nX_filtered = X[~np.isnan(X.wordCount_min)]\ny_filtered = y[~np.isnan(X.wordCount_min)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"908c394a764123d13a2060dacd6f635141b91a14"},"cell_type":"code","source":"def get_input(market_train, indices):\n    y = (market_train.loc[indices,'returnsOpenNextMktres10'] >= 0).values\n    r = market_train.loc[indices,'returnsOpenNextMktres10'].values\n    u = market_train.loc[indices, 'universe']\n    d = market_train.loc[indices, 'time'].dt.date\n    return y,r,u,d\n\n# r, u and d are used to calculate the scoring metric\ntrain_indices = market_train_df[market_train_df['year'] <= splitYear].index\ntest_indices = market_train_df[market_train_df['year'] > splitYear].index\ny_train,r_train,u_train,d_train = get_input(market_train_df, train_indices)\ny_test, r_test, u_test, d_test = get_input(market_train_df, test_indices)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dd6db208a81ad419d748e32c9a175c6523d6e7d4"},"cell_type":"code","source":"y_train = y_train[~np.isnan(X[X.year <= splitYear].wordCount_min)]\nr_train = r_train[~np.isnan(X[X.year <= splitYear].wordCount_min)]\nu_train = u_train[~np.isnan(X[X.year <= splitYear].wordCount_min)]\nd_train = d_train[~np.isnan(X[X.year <= splitYear].wordCount_min)]\n\ny_test = y_test[~np.isnan(X[X.year > splitYear].wordCount_min)]\nr_test = r_test[~np.isnan(X[X.year > splitYear].wordCount_min)]\nu_test = u_test[~np.isnan(X[X.year > splitYear].wordCount_min)]\nd_test = d_test[~np.isnan(X[X.year > splitYear].wordCount_min)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1c9663718fe0a6238af570ac68c65713428e64cf"},"cell_type":"code","source":"# split into treatment and control\ny = (y >= 0).astype(int)\n\nnum_cols = X.columns[(X.dtypes == \"float64\") | (X.dtypes == \"float32\")]\nX_train = X_filtered[X_filtered.year <= splitYear][num_cols].values\ny_train = y_train.reshape(-1, 1)\nX_test = X_filtered[X_filtered.year > splitYear][num_cols].values\ny_test = y_test.reshape(-1, 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2027f13ea40f6aca7a555f1f1b7271092d0d19f7"},"cell_type":"code","source":"# scale the variables\nfrom sklearn.preprocessing import StandardScaler\nprint('scaling numerical columns')\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_train = np.nan_to_num(X_train)\nX_train = scaler.fit_transform(X_train)\n\nX_test = scaler.fit_transform(X_test)\nX_test = np.nan_to_num(X_test)\nX_test = scaler.fit_transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c03d4976fdc32a14f291000213b13c8caee587fc"},"cell_type":"markdown","source":"# Train neural net model"},{"metadata":{"trusted":true,"_uuid":"12c361052cf570ee38a1a0df652afcb03f2754e5"},"cell_type":"code","source":"# mini-batch generation function\nimport math\ndef random_mini_batches(X, Y, mini_batch_size = 64, seed = 0):\n    \"\"\"\n    Creates a list of random minibatches from (X, Y)\n    \n    Arguments:\n    X -- input data, of shape (input size, number of examples)\n    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n    mini_batch_size - size of the mini-batches, integer\n    seed -- this is only for the purpose of grading, so that you're \"random minibatches are the same as ours.\n    \n    Returns:\n    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)\n    \"\"\"\n    \n    m = X.shape[0]                  # number of training examples\n    mini_batches = []\n    np.random.seed(seed)\n    \n    # Step 1: Shuffle (X, Y)\n    permutation = list(np.random.permutation(m))\n    shuffled_X = X[permutation, :]\n    shuffled_Y = Y[permutation, :].reshape((m, Y.shape[1]))\n\n    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n    num_complete_minibatches = math.floor(m/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n    for k in range(0, num_complete_minibatches):\n        mini_batch_X = shuffled_X[k * mini_batch_size : k * mini_batch_size + mini_batch_size, :]\n        mini_batch_Y = shuffled_Y[k * mini_batch_size : k * mini_batch_size + mini_batch_size, :]\n        mini_batch = (mini_batch_X, mini_batch_Y)\n        mini_batches.append(mini_batch)\n    \n    # Handling the end case (last mini-batch < mini_batch_size)\n    if m % mini_batch_size != 0:\n        mini_batch_X = shuffled_X[num_complete_minibatches * mini_batch_size : m, :]\n        mini_batch_Y = shuffled_Y[num_complete_minibatches * mini_batch_size : m, :]\n        mini_batch = (mini_batch_X, mini_batch_Y)\n        mini_batches.append(mini_batch)\n    \n    return mini_batches","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c224cc6a7a8661dceddfa0592721cfca60935a14"},"cell_type":"code","source":"# make the mini-batches\nminibatches = random_mini_batches(X_train, y_train.astype(int).reshape(-1, 1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e43f5d623409e5d543b9397e8ed00b8b9cd1aeb6"},"cell_type":"code","source":"# Start the session to compute the tensorflow graph\nwith tf.Session() as sess:\n\n    # Run the initialization\n    sess.run(init)\n        \n    # Do the training loop\n    for epoch in range(num_epochs):\n        \n        epoch_cost = 0.\n                \n        # iterate through the minibatches\n        for minibatch in minibatches:\n            \n            # Select a minibatch\n            (minibatch_X, minibatch_Y) = minibatch\n            \n            # IMPORTANT: The line that runs the graph on a minibatch.\n            # Run the session to execute the \"optimizer\" and the \"cost\", the feedict should contain a minibatch for (X,Y).\n            _ , minibatch_cost = sess.run([optimizer, cost], feed_dict={inputs: minibatch_X, labels: minibatch_Y})\n            epoch_cost += minibatch_cost \n\n        # Print the cost every epoch\n        print (\"Cost after epoch %i: %f\" % (epoch, epoch_cost))\n\n    # make the predictions on the outcome\n    trainPreds = sess.run([Z2], feed_dict={inputs: X_train})\n    testPreds = sess.run([Z2], feed_dict={inputs: X_test})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"08f21e37b105282f615fd3509fc67363b26f72df"},"cell_type":"code","source":"# get the test and train predictions\ntemp_train = trainPreds[0].flatten()\nprobs_train = np.exp(temp_train)/(1 + np.exp(temp_train))\nconfidence_train = 2*(probs_train - 0.5)\n\ntemp_test = testPreds[0].flatten()\nprobs_test = np.exp(temp_test)/(1 + np.exp(temp_test))\nconfidence_test = 2*(probs_test - 0.5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7d0a2140ccefce1d8f39f996295c7e14ed0d521f"},"cell_type":"code","source":"def computeSigmaScore(preds, r, u, d):\n    x_t_i = preds * r * u\n    data = {'day' : d, 'x_t_i' : x_t_i}\n    df = pd.DataFrame(data)\n    x_t = df.groupby('day').sum().values.flatten()\n    mean = np.mean(x_t)\n    std = np.std(x_t)\n    score_valid = mean / std\n    return(score_valid)\n    \ndef computeCrossEntropyLoss(probs, r, eps = 1e-7):\n    labels = (r >= 0).astype(int)\n    probs_clipped = np.clip(probs, eps, 1.0-eps)\n    return(np.mean(labels*np.log(probs_clipped) + (1-labels)*np.log(1-probs_clipped)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ca04391ca989dda4c22b2a56810f5ba7b2014dcf"},"cell_type":"code","source":"[computeSigmaScore(confidence_test, r_test, u_test, d_test), \n -computeCrossEntropyLoss(probs_test, r_test)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1595de93e2a5ba4ba66607433cd1f65095a2cb59"},"cell_type":"code","source":"[computeSigmaScore(confidence_train, r_train, u_train, d_train), \n -computeCrossEntropyLoss(probs_train, r_train)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b5e80f971ac180cc1672c95f1ec68d7da408c572"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}