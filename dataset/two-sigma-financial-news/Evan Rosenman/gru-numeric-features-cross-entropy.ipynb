{"cells":[{"metadata":{"trusted":true,"_uuid":"469cb5d07ef0e42543e4468c6b6af1d4c88220de"},"cell_type":"code","source":"# define global constants\nsplitYear = 2014\nfirstYear = 2011","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"78e4c696215ba1d03463364e10a4631bce7599e8"},"cell_type":"markdown","source":"# Make the TensorFlow Graph"},{"metadata":{"trusted":true,"_uuid":"122a957d285fcfce1d962f0cc07eb5e09cda381a"},"cell_type":"code","source":"# constants pertaining to the graph\ntrainDim = 11\nnum_epochs = 25\nminibatch_size = 64\nnum_hidden = 32\nseed = 0\nwindowLen = 50\nlr = 0.001","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1cf40def0f70291f0095bbd62a440d7649c37e68"},"cell_type":"code","source":"# okay let's actually set up a tensor flow graph\nimport tensorflow as tf\n\n# placeholders\ntf.reset_default_graph()\ninputs = tf.placeholder(tf.float32, shape=(None, windowLen, trainDim), name='inputs')\nlabels = tf.placeholder(tf.float32, shape=(None, 1), name='labels')\nlengths = tf.placeholder(tf.int64, shape = (None, ), name='lengths')\n\n# create the cells\n#cell = tf.nn.rnn_cell.LSTMCell(num_hidden, state_is_tuple=True)\ncell = tf.nn.rnn_cell.BasicRNNCell(num_hidden)\noutput, states = tf.nn.dynamic_rnn(cell, inputs, dtype=tf.float32, sequence_length = lengths)\n\n# output layer weight matrices\nW1 = tf.get_variable(\"W1\", shape=[num_hidden, 1],\\\n           initializer=tf.contrib.layers.xavier_initializer())\nb1 = tf.get_variable(\"b1\", shape=[1, 1],\\\n           initializer=tf.zeros_initializer())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"caced4c9aa6a74fec107641b671e0247bf1d0731"},"cell_type":"code","source":"# set up the relationships\noutput = tf.add(tf.matmul(states, W1), b1)\ncost = tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(labels = labels, logits = output))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"83c3d43b3c985b6c21c496a1377f8d6af732bd10"},"cell_type":"code","source":"# optimizer and cost\noptimizer = tf.train.AdamOptimizer(learning_rate = lr).minimize(cost)\ninit = tf.global_variables_initializer()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f01f498729662689c63b3689f87493cc8303e7a4"},"cell_type":"markdown","source":"# Load in the Data"},{"metadata":{"trusted":true,"_uuid":"cfa8129f6d380afb33107c10a0bedc781398db9f"},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom itertools import chain","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"from kaggle.competitions import twosigmanews\n# You can only call make_env() once, so don't lose it!\nenv = twosigmanews.make_env()\nprint('Done!')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c20fa6deeac9d374c98774abd90bdc76b023ee63"},"cell_type":"code","source":"(market_train_df, _) = env.get_training_data()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bb7d86e1e1055c9bcd58f515ed9168621db0ec3e"},"cell_type":"code","source":"# pre-process and scale\ncat_cols = ['assetCode']\nnum_cols = ['volume', 'close', 'open', 'returnsClosePrevRaw1', 'returnsOpenPrevRaw1', 'returnsClosePrevMktres1',\n                    'returnsOpenPrevMktres1', 'returnsClosePrevRaw10', 'returnsOpenPrevRaw10', 'returnsClosePrevMktres10',\n                    'returnsOpenPrevMktres10']\n\nfrom sklearn.preprocessing import StandardScaler\nprint('scaling numerical columns')\nscaler = StandardScaler()\n\nmarket_train_df[num_cols] = scaler.fit_transform(market_train_df[num_cols])\n\nmarket_train_df[num_cols] = market_train_df[num_cols].fillna(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8a172258a46c7db96e85964d6d56c24f0ec56f0c"},"cell_type":"code","source":"# data formatting and splitting\nmarket_train_df['y'] = ((market_train_df.returnsOpenNextMktres10 > 0).values).astype(int)\nmarket_train_df['year'] = pd.to_datetime(market_train_df.time).dt.year","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4b062ca125c06b89c1dacaf3c637848c8715bc70"},"cell_type":"code","source":"# split the data into train and test sets\ntrain = market_train_df[(market_train_df.year >= firstYear) & (market_train_df.year <= splitYear)]\ntest = market_train_df[(market_train_df.year > splitYear)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e6437d15c36a41cbd6576e72214d2fb49117cc1d"},"cell_type":"code","source":"# function extract all the sequences\nimport datetime\ndef sequenceExtraction(df): \n    \n    # prepare to store the data\n    X = [] \n    y = [] \n    r = [] \n    u = [] \n    d = [] \n    \n    # for each asset code \n    cntr = 0\n    for assetCode in df.assetCode.unique():\n        if cntr % 100 == 0:\n            print(cntr, datetime.datetime.now())\n        cntr += 1\n\n        # get the whole sequence\n        if not df[df.assetCode == assetCode].time.is_monotonic:\n            print('not sequential time series data!')\n        data = df[df.assetCode == assetCode]\n        Xvalues = data[num_cols].values\n        outcome = data['y'].values\n        rvalues = data['returnsOpenNextMktres10'].values\n        uvalues = data['universe'].values\n        dvalues = data['time'].values\n\n        # generate a sliding window of data \n        for i in range(data.shape[0] - windowLen):\n            X += [Xvalues[i:(i + windowLen)]]\n            y += [outcome[(i + windowLen)]]\n            r += [rvalues[(i + windowLen)]]\n            u += [uvalues[(i + windowLen)]]\n            d += [dvalues[(i + windowLen)]]\n            \n    return((X, y, r, u, d))\n\nX_train, y_train, r_train, u_train, d_train = sequenceExtraction(train)\nX_test, y_test, r_test, u_test, d_test = sequenceExtraction(test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c03d4976fdc32a14f291000213b13c8caee587fc"},"cell_type":"markdown","source":"# Train neural net model"},{"metadata":{"trusted":true,"_uuid":"12c361052cf570ee38a1a0df652afcb03f2754e5"},"cell_type":"code","source":"# mini-batch generation function\nfrom pdb import set_trace as t\nimport math\ndef random_mini_batches(X, Y, mini_batch_size = 64, seed = 0, random = True):\n    \"\"\"\n    Creates a list of random minibatches from (X, Y)\n\"\"\"\n    \n    m = len(X)                  # number of training examples\n    mini_batches = []\n    np.random.seed(seed)\n    \n    # Step 1: Shuffle (X, Y)\n    if random:\n        permutation = list(np.random.permutation(m))\n    else:\n        permutation = range(m)\n    shuffled_X = [X[i] for i in permutation]\n    shuffled_Y = [Y[i] for i in permutation]\n\n    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n    num_complete_minibatches = math.floor(m/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n    for k in range(0, num_complete_minibatches):\n        mini_batch_X = shuffled_X[k * mini_batch_size : k * mini_batch_size + mini_batch_size]\n        mini_batch_Y = shuffled_Y[k * mini_batch_size : k * mini_batch_size + mini_batch_size]\n        mini_batch_X = np.dstack(mini_batch_X).transpose([2, 0, 1])\n        mini_batch_Y = np.asarray(mini_batch_Y).reshape(-1,1)\n        mini_batch = (mini_batch_X, mini_batch_Y)\n        mini_batches.append(mini_batch)\n    \n    # Handling the end case (last mini-batch < mini_batch_size)\n    if m % mini_batch_size != 0:\n        mini_batch_X = shuffled_X[num_complete_minibatches * mini_batch_size : m]\n        mini_batch_Y = shuffled_Y[num_complete_minibatches * mini_batch_size : m]\n        mini_batch_X = np.dstack(mini_batch_X).transpose([2, 0, 1])\n        mini_batch_Y = np.asarray(mini_batch_Y).reshape(-1,1)\n        mini_batch = (mini_batch_X, mini_batch_Y)\n        mini_batches.append(mini_batch)\n    \n    return mini_batches","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c224cc6a7a8661dceddfa0592721cfca60935a14","scrolled":false},"cell_type":"code","source":"# make the mini-batches\nminibatches = random_mini_batches(X_train, y_train)\ntrainMiniBatches = random_mini_batches(X_train, y_train, random = False)\ntestMiniBatches = random_mini_batches(X_test, y_test, random = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d25e07ff277deb817004d05d6d2f380cc38534cc"},"cell_type":"code","source":"y_train = np.asarray(y_train).reshape(-1,1)\nr_train = np.asarray(r_train).reshape(-1,1)\nu_train = np.asarray(u_train).reshape(-1,1)\nd_train = np.asarray(d_train).reshape(-1,1)\n\ny_test = np.asarray(y_test).reshape(-1,1)\nr_test = np.asarray(r_test).reshape(-1,1)\nu_test = np.asarray(u_test).reshape(-1,1)\nd_test = np.asarray(d_test).reshape(-1,1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e43f5d623409e5d543b9397e8ed00b8b9cd1aeb6"},"cell_type":"code","source":"# Start the session to compute the tensorflow graph\ntestPreds = [] \ntrainPreds = [] \n\nwith tf.Session() as sess:\n\n    # Run the initialization\n    sess.run(init)\n\n    # Do the training loop\n    for epoch in range(num_epochs):\n        \n        epoch_cost = 0.\n                \n        # iterate through the minibatches\n        for minibatch in minibatches:\n            \n            # Select a minibatch\n            (minibatch_X, minibatch_Y) = minibatch\n            \n            # IMPORTANT: The line that runs the graph on a minibatch.\n            # Run the session to execute the \"optimizer\" and the \"cost\", the feedict should contain a minibatch for (X,Y).\n            seqLens = np.repeat(windowLen, minibatch_Y.shape[0])\n            _ , minibatch_cost = sess.run([optimizer, cost], feed_dict={inputs: minibatch_X, labels: minibatch_Y, lengths: seqLens})\n            \n            epoch_cost += minibatch_cost \n\n        # Print the cost every epoch\n        print (\"Cost after epoch %i: %f\" % (epoch, epoch_cost))\n\n    # make the predictions on the test and train data\n    for minibatch in trainMiniBatches:\n        \n            # Select a minibatch\n            (minibatch_X, minibatch_Y) = minibatch\n            \n            # Run the session to execute the \"optimizer\" and the \"cost\", the feedict should contain a minibatch for (X,Y).\n            seqLens = np.repeat(windowLen, minibatch_Y.shape[0])\n            preds = sess.run([output], feed_dict={inputs: minibatch_X, lengths: seqLens})\n            trainPreds += preds[0].flatten().tolist()\n            \n    for minibatch in testMiniBatches:\n        \n            # Select a minibatch\n            (minibatch_X, minibatch_Y) = minibatch\n            \n            # Run the session to execute the \"optimizer\" and the \"cost\", the feedict should contain a minibatch for (X,Y).\n            seqLens = np.repeat(windowLen, minibatch_Y.shape[0])\n            preds = sess.run([output], feed_dict={inputs: minibatch_X, lengths: seqLens})\n            testPreds += preds[0].flatten().tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"08f21e37b105282f615fd3509fc67363b26f72df"},"cell_type":"code","source":"# get the test and train predictions\ntemp_train = np.asarray(trainPreds)\nprobs_train = np.exp(temp_train)/(1 + np.exp(temp_train))\nconfidence_train = 2*(probs_train - 0.5)\n\ntemp_test = np.asarray(testPreds)\nprobs_test = np.exp(temp_test)/(1 + np.exp(temp_test))\nconfidence_test = 2*(probs_test - 0.5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7d0a2140ccefce1d8f39f996295c7e14ed0d521f"},"cell_type":"code","source":"def computeSigmaScore(preds, r, u, d):\n    x_t_i = preds * r * u\n    data = {'day' : d, 'x_t_i' : x_t_i}\n    df = pd.DataFrame(data)\n    x_t = df.groupby('day').sum().values.flatten()\n    mean = np.mean(x_t)\n    std = np.std(x_t)\n    score_valid = mean / std\n    return(score_valid)\n    \ndef computeCrossEntropyLoss(probs, r, eps = 1e-7):\n    labels = (r >= 0).astype(int)\n    probs_clipped = np.clip(probs, eps, 1.0-eps)\n    return(np.mean(labels*np.log(probs_clipped) + (1-labels)*np.log(1-probs_clipped)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ca04391ca989dda4c22b2a56810f5ba7b2014dcf"},"cell_type":"code","source":"[computeSigmaScore(confidence_test, r_test.flatten(), u_test.flatten(), d_test.flatten()), \n -computeCrossEntropyLoss(probs_test, r_test.flatten())]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1595de93e2a5ba4ba66607433cd1f65095a2cb59"},"cell_type":"code","source":"[computeSigmaScore(confidence_train, r_train.flatten(), u_train.flatten(), d_train.flatten()), \n -computeCrossEntropyLoss(probs_train, r_train.flatten())]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3db49b991d621351df3dc221f9194cbbae3e0385"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}