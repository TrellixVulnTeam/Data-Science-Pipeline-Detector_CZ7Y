{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"scrolled":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.neural_network import MLPRegressor\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.\n\n# Import environment\nfrom kaggle.competitions import twosigmanews\nenv = twosigmanews.make_env()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c3e9c4f2fb15f811d9fe05159884a273fdc3500c"},"cell_type":"code","source":"from sklearn.model_selection import KFold # import KFold to split data set into K splits\nfrom sklearn.utils import shuffle\nfrom sklearn.metrics import accuracy_score\nimport matplotlib.pyplot as plt\n# distribution of confidence as a sanity check: they should be distributed as above\nimport time\nimport warnings\nwarnings.simplefilter(action='ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6cba554c4d0aaf6bb7a9636bcabda483a542b875"},"cell_type":"code","source":"(mt_df, nt_df) = env.get_training_data() # load test dataset","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f6de6d35d78b381ce9d2979eec5620fe36facd9"},"cell_type":"markdown","source":"# Processing Market Data\nRemoving rows with NaN/Null  and Outliers"},{"metadata":{"trusted":true,"_uuid":"8fe44321c5d5e080ad1343c2723deb3b45869332","scrolled":false},"cell_type":"code","source":"##### Analyze the null and NaN values in the dataset - they are the same\ndef dropNull(mt_df):\n    total_null = mt_df.isnull().sum().sort_values(ascending = False)\n    #total_NaN = mt_df.isna().sum().sort_values(ascending = False)\n    total = mt_df.shape[0]\n    percent_Null = round(mt_df.isnull().sum().sort_values(ascending = False)/total*100, 2)\n    #percent_NaN = round(mt_df.isna().sum().sort_values(ascending = False)/mt_df.shape[0]*100, 2)\n    mt_df_null = pd.concat([total_null, percent_Null], axis = 1,\\\n                            keys= ['Number of Null Entries', 'Percentage of Null Entries'])\n    return mt_df_null\n\nmt_df_null = dropNull(mt_df) # Analyzing Null values\nprint(mt_df_null)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bbcb492d920860cb87ee5761ee01bb19d1be35cf"},"cell_type":"code","source":"# drop rows with Null values\nmt_df = mt_df.dropna()\n\n# sanity check that it did drop Null rows\nmt_df_null = dropNull(mt_df) # Analyzing Null values\nprint(mt_df_null)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f6118f017eee1e701936944606efc19f353b4e2c"},"cell_type":"markdown","source":"# Putting all functions in this next cell"},{"metadata":{"trusted":true,"_uuid":"14166b9ee13def4515313a67979c44be8a31c61b"},"cell_type":"code","source":"\nfrom sklearn.preprocessing import StandardScaler\n\n# Function to remove outliers in defined columns\ndef remove_outlier(df,column_list,lower_percentile,upper_percentile):\n    for i in range(len(column_list)):\n        #upper_bound = np.percentile(df[column_list[i]],upper_percentile)\n        #lower_bound = np.percentile(df[column_list[i]],lower_percentile)\n        df = (df[(df[column_list[i]]<np.percentile(df[column_list[i]],upper_percentile)) & (df[column_list[i]]>np.percentile(df[column_list[i]],lower_percentile))])\n    return df\n\n# split data into features and target values\ndef get_target_data(df, features):\n#     features = ['returnsClosePrevRaw1',\n#          'returnsOpenPrevRaw1',\n#          'returnsClosePrevMktres1',\n#          'returnsOpenPrevMktres1',\n#          'returnsClosePrevRaw10',\n#          'returnsOpenPrevRaw10',\n#          'returnsClosePrevMktres10',\n#          'returnsOpenPrevMktres10']  \n    ft = ['time'] + features\n    x = df[ft]\n    y = df[['time','returnsOpenNextMktres10']]\n    return x,y\n\n# Standardizing/Scaling the Data so that mean ~= 0 and standard deviation is 1\ndef scale_data(X_train, X_test, features):\n    scaler = StandardScaler()\n    scaler.fit(X_train[features]) # do not want to fit to the test data\n    X_train[features] = scaler.transform(X_train[features])\n    X_test[features] = scaler.transform(X_test[features])\n    return X_train, X_test\n\ndef make_my_prediction(x):\n#     my_pred = (mlpR.predict(x)).reshape(1,-1)[0]\n#     my_pred[my_pred>0]=1\n#     my_pred[my_pred<0]=-1\n    start_time = time.time()\n    my_pred = (mlpR.predict(x)).reshape(1,-1)[0]\n    positive_pred = my_pred[my_pred>=0]\n    negative_pred = my_pred[my_pred<0]\n    pos_min = positive_pred.min()\n    pos_max = positive_pred.max()\n    neg_min = negative_pred.min()\n    neg_max = negative_pred.max()\n\n    for i in range(len(positive_pred)):\n        positive_pred[i] = (positive_pred[i]-pos_min)/(pos_max - pos_min)\n    for m in range(len(negative_pred)):\n        negative_pred[m] = -1 + (negative_pred[m]-neg_min)/(neg_max-neg_min)\n    elapsed_time = time.time() - start_time\n    print('It took', elapsed_time/60, 'minutes make predictions and scale them to confidence interval')\n\n    return my_pred\n\n# sigma_score function is considered as a custom evaluation metric for xgboost\n# example of how custom evaluation function is incorporated into xgboost's training can be found here : https://github.com/dmlc/xgboost/blob/master/demo/guide-python/custom_objective.py\ndef sigma_score(preds,dval,df):\n    \n    # get y_target values\n    labels = dval\n    # call time parameter to be used for grouping, so that we can add x_t values for each day\n    df_time = df\n    \n    #calculate x_t and score as specified by the competition\n    x_t = pd.Series(preds*labels)\n    x_t_sum = x_t.groupby(df_time).sum()    \n    score = (x_t_sum.mean())/(x_t_sum.std())\n    return 'sigma_score', round(score,5)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8aef4313a8aa53994da9b5969900aea4e4ea822f"},"cell_type":"code","source":"# Removing the market residual outliers because of trial and error\n# raw outliers did not seem to impact some residual outliers but vice versa was true\noutlier_removal_list = [#'returnsClosePrevRaw1',\n         #'returnsOpenPrevRaw1',\n         'returnsClosePrevMktres1',\n         'returnsOpenPrevMktres1',\n         #'returnsClosePrevRaw10',\n         #'returnsOpenPrevRaw10',\n         'returnsClosePrevMktres10',\n         'returnsOpenPrevMktres10',   \n        'returnsOpenNextMktres10']\n# Defining features that will be fed into the Neural Network\nfeatures = ['returnsClosePrevRaw1',\n         'returnsOpenPrevRaw1',\n         'returnsClosePrevMktres1',\n         'returnsOpenPrevMktres1',\n         'returnsClosePrevRaw10',\n         'returnsOpenPrevRaw10',\n         'returnsClosePrevMktres10',\n         'returnsOpenPrevMktres10']  \n# Orgininal feature values\nmt_df[features].describe().transpose()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e634ba7b3b3648096261693c67e7c82b378f2d9c"},"cell_type":"code","source":"mt_df_no_outlier = remove_outlier(mt_df,outlier_removal_list,2,98)\n\n# feature values without outliers\nmt_df_no_outlier[features].describe().transpose()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6af1801a16caefb47124556568dcbd693510e771"},"cell_type":"code","source":"# # Split data into n sets for cross validation\nfrom sklearn.model_selection import KFold # import KFold\n\n# Getting feature data and target data\nmt_df_features,mt_df_target = \\\n                        get_target_data(mt_df_no_outlier, features)\n\ndef split_data_Kfold(X, y, n_splits, NNfeatures):\n    kf = KFold(n_splits)\n    NNfeatures = [#'returnsClosePrevRaw1',\n#          'returnsOpenPrevRaw1',\n#          'returnsClosePrevMktres1',\n#          'returnsOpenPrevMktres1',\n         'returnsClosePrevRaw10',\n         'returnsOpenPrevRaw10',\n#         'returnsClosePrevMktres10',\n         'returnsOpenPrevMktres10']  \n    for train_index, test_index in kf.split(X, y): # passes y since surpervised learning is desired\n        X_train, X_test = X.iloc[train_index,:], X.iloc[test_index,:]\n        y_train, y_test = y.iloc[train_index,:], y.iloc[test_index,:]\n        train_t = y_train['time']\n        test_t = y_test['time']\n        return  X_train[NNfeatures], X_test[NNfeatures], y_train.iloc[:,1], y_test.iloc[:,1],\\\n                                                                train_t, test_t\n\nn_splits = 5\nmt_features_train, mt_features_test, mt_target_train, mt_target_test, train_time, test_time \\\n            = split_data_Kfold(mt_df_features,mt_df_target, n_splits, features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"16c2a9ad854daae4716bd20c350534c11ad93d89"},"cell_type":"code","source":"# Before cross validation\n# Multi-layer Perceptron (MLP) Regressor\nfrom sklearn.neural_network import MLPRegressor\n# Suppose there are n training samples, m features, k hidden layers, \n# each containing h neurons - for simplicity, and o output neurons. \n# The time complexity of backpropagation is O(n⋅m⋅h^k⋅o⋅i), where i is the number of iterations\n\n# Creating the model\nmlpR = MLPRegressor(hidden_layer_sizes = (5,5,5,5),\n                   max_iter=300) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"013117edf6ae01e2ad41a6062d6e38e82d23f05e"},"cell_type":"code","source":"import time\n# Training the model\nstart_time = time.time()\nmlpR.fit(mt_features_train,mt_target_train)\nelapsed_time = time.time() - start_time\nprint('It took', elapsed_time/60, 'minutes to train the neural network')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"82d398315626f5f323d109065aa66e05e22a12aa"},"cell_type":"code","source":"mlpR.n_layers_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c644974705f04b192695b8e1ed062852a8105bd9"},"cell_type":"code","source":"# testing and evaluating the model\nfrom sklearn.metrics import classification_report,confusion_matrix\nmt_target_pred = mlpR.predict(mt_features_test)\nmt_target_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d59ca4d9f22650c2d9d4b59fb2356af4b7a647fd"},"cell_type":"code","source":"import matplotlib.pyplot as plt\ndata = pd.DataFrame({'y_real':mt_target_test,'y_pred':mt_target_pred})\ndiff = data.iloc[:,0]-(data.iloc[:,1])\nt = range(0,100)\nplt.plot(t,data.iloc[0:100, 0])\nplt.plot(t,10*data.iloc[0:100, 1])\n#plt.plot(diff.iloc[0:100])\n# plt.plot(mt_target_train.iloc[0:100])\nplt.legend()\nplt.title('Real Return Values vs. Predicted\\nReturn Values for Keras Neural Network')\nplt.xlabel('Time (Days)')\nplt.ylabel('10 Day Leading Market Adjusted Return')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f1692e42d995ace658ec2d8fb0f80a4eb2728410"},"cell_type":"markdown","source":"# Getting Regression metrics"},{"metadata":{"trusted":true,"_uuid":"e0771c34e771956f5139189c342fbe7d45367aa5"},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error\n\nprint('mean_absolute_error is', mean_absolute_error(mt_target_test, mt_target_pred))\nprint('mean_squared_error is', mean_squared_error(mt_target_test, mt_target_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d94240892ff28e229dbb7eb12bc31ba88e24f910"},"cell_type":"code","source":"mt_pred_test = make_my_prediction(mt_features_test)\nprint(\"test : \",sigma_score(mt_pred_test,mt_target_test,test_time))\nmt_pred_train = make_my_prediction(mt_features_train)\nprint(\"train : \",sigma_score(mt_pred_train,mt_target_train,train_time))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dfbc2fb5468530b0d8ba0a7c8e747ebd2065808d"},"cell_type":"code","source":"mlpR.get_params(deep=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"81f84656231d81df5758ef72d9718446ce820fca"},"cell_type":"code","source":"# for (market_obs_df, _, predictions_template_df) in env.get_prediction_days():  \n#     features = [#'returnsClosePrevRaw1',\n# #          'returnsOpenPrevRaw1',\n# #          'returnsClosePrevMktres1',\n# #          'returnsOpenPrevMktres1',\n#          'returnsClosePrevRaw10',\n#          'returnsOpenPrevRaw10',\n#          'returnsClosePrevMktres10',\n#          'returnsOpenPrevMktres10']  \n#     market_obs_df_scaled = scale_data(market_obs_df,features)    \n#     x_submission = market_obs_df_scaled[features].copy()\n#     # fill in NaN values with mean of rest of the values\n#     for i in range(len(features)):\n#          x_submission[features[i]]= x_submission[features[i]].fillna(x_submission[features[i]].mean())\n#     predictions_template_df['confidenceValue'] = make_my_prediction(x_submission)\n#     env.predict(predictions_template_df)\n#     del x_submission\n# print('Done!')\n# # Write submission file    \n# env.write_submission_file()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}