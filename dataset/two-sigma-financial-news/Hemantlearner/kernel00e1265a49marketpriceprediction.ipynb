{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nfrom kaggle.competitions import twosigmanews\n#import os\nfrom sklearn.linear_model import LogisticRegression\n#from sklearn import datasets\nimport nltk\nimport re\nfrom stop_words import get_stop_words\nfrom datetime import datetime\nfrom nltk.corpus import stopwords\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\n#print(os.listdir(\"../input\"))\nimport datetime,time\nenv = twosigmanews.make_env()\n%matplotlib inline\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"scrolled":true},"cell_type":"code","source":"(market_train_df, news_train_df) = env.get_training_data()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"19df0d29c0cc7d8e7aeeead98e62af49487f66d3"},"cell_type":"code","source":"news_train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"92f828dcca4157a01aff2cf8d0966a498a95d5b6","scrolled":false},"cell_type":"code","source":"market_train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a9f4ba3bb6a95d6a0613507c4a4a96e86ab57f56"},"cell_type":"code","source":"def intersection(lst1, lst2): \n    return list(set(lst1) & set(lst2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0df51ab89bbe715396f919c922d5187d7abec71e"},"cell_type":"code","source":"def headlines(value,asset):\n    #print('processing headlines for ', asset)\n    #get company name\n    assetsplit=str(asset).lower().split()\n    #print(assetsplit[0])\n    if len(assetsplit[0])==1 & len(assetsplit) > 2:\n        companyname=assetsplit[0]+assetsplit[1]\n    else:\n        companyname=assetsplit[0]\n    print(companyname)\n    nameposition=[]\n    print(len(value))\n    for i in range(len(value)):\n        score=0\n        sentence=value[i]\n        #print(sentence)\n        tokens = nltk.word_tokenize(sentence)\n        #print(tokens,'\\n')\n        #print(tokens)\n        #check polarity score for news (postive, negtive, neutral)\n        for word in tokens:\n            if str(word).lower()==companyname:\n                nameposition.append(i)\n                break\n    print(len(nameposition))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"74a8647c546a20d0c574ce62a207139a0c921189"},"cell_type":"code","source":"def text_process(title,text_values):\n    #print('Processing Text for counts for ',title)\n    regex = r'\\w+'\n    text=[]\n    text_count=[]\n    for i in range(len(text_values)):\n        list1=re.findall(regex, text_values[i])\n        #count of related topics\n        text.append(list1)\n        text_count.append(len(list1))    \n    return text_count","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4d4ae1890907e99da5c5825eea04a4aac9940914","scrolled":true},"cell_type":"code","source":"def replace_txt_w_num(values):\n    values_temp=[]\n    \n    for (i, item) in enumerate(values):\n        if item == True:\n            values_temp.append(1.0)\n        else:\n            values_temp.append(0.0)\n    return values_temp","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4ed289b1729b7220a85d1d6dd157c0868ff4f2fe"},"cell_type":"code","source":"def sync_to_date_range(added_dates,row_insert,asset_matrix,time_values,values,max_time_buckets, title):\n    #print('Syncing date to values for ',title)\n    \n    upd_max_time_buckets= [datetime.datetime.utcfromtimestamp(i/(1e+9)).date() for i in max_time_buckets]\n    #print(upd_max_time_buckets)\n    start_row=row_insert\n    indexes_v=[]\n    checks=0\n    #print(time_values)\n    for i in time_values:\n        if i>upd_max_time_buckets[len(upd_max_time_buckets)-1]:\n            break\n        if i in upd_max_time_buckets:\n            asset_matrix[start_row,upd_max_time_buckets.index(i)+checks]=values[time_values.index(i)]\n            indexes_v.append(upd_max_time_buckets.index(i)+checks)\n        else:\n            x=1\n            word='damn'\n            \n            while word=='damn':\n                date=str(i)\n                \n                date2=date[0:4]+date[5:7]+date[8:10]\n                \n                if date2 in set(added_dates):\n                    asset_matrix[start_row,np.where(asset_matrix == int(date2))[1]]=values[time_values.index(i)]\n                    #print('Done new', date2, '   ',values[upd_time_values.index(i)] )\n                    word='yay'\n                if date2 not in set(added_dates):\n                    #print(date2)\n                    if i+datetime.timedelta(days=x) in upd_max_time_buckets:       \n                        asset_matrix=np.insert(asset_matrix,upd_max_time_buckets.index(i+datetime.timedelta(days=x))+checks,int(date2),axis=1)\n                        asset_matrix[1:,upd_max_time_buckets.index(i+datetime.timedelta(days=x))+checks]=0.0\n                        asset_matrix[start_row,upd_max_time_buckets.index(i+datetime.timedelta(days=x))+checks]=values[time_values.index(i)]\n                        indexes_v.append(upd_max_time_buckets.index(i+datetime.timedelta(days=x))+checks)\n                        added_dates.append(date2)\n                        checks+=1\n                        word='yay'\n                    x+=1  \n                                 \n    #print('Nett new Date Columns added = ',checks)\n    \n    return asset_matrix, added_dates","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d2a97292fd3522d6cd3327808e3e1e95dcd52b74"},"cell_type":"code","source":"def category_replace(list_of_values, title):\n    #print ('Replacing with category count for ',title)\n    new_list=[]\n    try:\n        x = list(set(list_of_values.categories))\n    except:\n        x = list(set(list_of_values))\n    dic = dict(zip(x, list(range(1,len(x)+1))))\n    for y in list_of_values:\n        new_list.append(dic[y])\n    return new_list\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7e5bcc5b7c10cfd5619b362fa888634c64c804fd"},"cell_type":"code","source":"def normalize(input_array):\n    input_array[np.isnan(input_array)]=0\n    divisor=np.amax(input_array, axis=0)\n    #print(divisor)\n    divisor[divisor==0]=1\n    #print(divisor)\n    input_array2=input_array/divisor\n    return input_array2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9cb145eacce55421921bd347eb1911bc7751860d"},"cell_type":"code","source":"def find_traded_assets(market_train_df,news_train_df):    \n    traded_assets=market_train_df[market_train_df.universe==float(1.0)]['assetName'].unique().astype(str)\n    news_asset=news_train_df['assetName'].unique().astype(str)\n    trade_w_news=intersection(traded_assets, news_asset)\n    \n    return trade_w_news","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"42a3482f991e7b3c8773bb91cbb599c1b6b717b9"},"cell_type":"code","source":"def values_date_avg(news_sorted,field):\n    j=0\n    dict_values={}\n    for i in range(len(news_sorted)-1):\n        if news_sorted['time'][i].date() != news_sorted['time'][i+1].date():\n            mean=news_sorted[field][j:i].mean()\n            j=i\n            dict_values[news_sorted['time'][i].date()]=mean\n    #print(dict_values)\n    return dict_values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a72428fcc0464edcbc0c36ee4163a42ebe093548"},"cell_type":"code","source":"def values_date_avg_mod(news_sorted,list_of_values,field):\n    j=0\n    dict_values={}\n    for i in range(len(news_sorted)-1):\n        if news_sorted['time'][i].date() != news_sorted['time'][i+1].date():\n            try:\n                mean=sum(list_of_values[j:i])/(i-j)\n            except:\n                mean=sum(list_of_values[j:i])/1\n            j=i\n            dict_values[news_sorted['time'][i].date()]=mean\n    #print(dict_values)\n    return dict_values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"83cb5528003958830eec15c802ced5c367dad6be","scrolled":true},"cell_type":"code","source":"def build_matrix(market_train_df, news_train_df,traded_asset):\n    \n    asset=traded_asset\n    print(asset)\n    market_sorted = market_train_df[(market_train_df.assetName == str(asset))]['time'].values.tolist()\n    print('Asset time bucket count',len(market_sorted))\n    asset_matrix=np.zeros(shape=(41,len(market_sorted)))\n    max_time_buckets_upd=[datetime.datetime.utcfromtimestamp(i/(1e+9)).date() for i in market_sorted]\n    date_list=[]\n    for i in max_time_buckets_upd:\n        date=str(i)\n        date2=date[0:4]+date[5:7]+date[8:10]\n        date_list.append(int(date2))\n    asset_matrix[0,:]=date_list\n    #print(asset_matrix)\n    column_list=market_train_df.columns.values\n    #print(asset_matrix.shape)\n\n    #Build feature matrix for a random asset (except asset name and asset code in the matrix) for market information\n    \n    for j in range(3, 16):\n        #print(\"Processing Column \",column_list[j])\n        #quick way of creating max_length list\n        column_values=market_train_df[(market_train_df.assetName == str(asset))][column_list[j]].values.tolist()\n    \n        if len(column_values) < len(market_sorted):\n            #create_empty list\n            empty_column_list=[0.0]*len(max_time_buckets)\n            #print('Check1 - less than max time buckets')\n            #update values from actual into empty list\n            i=0\n            while i<len(positions):\n                #print('Replacing at position ',positions[i], 'with value ', column_values[i])\n                empty_column_list[int(positions[i])]=column_values[i]\n                i+=1\n        else:\n            empty_column_list=column_values\n        \n        asset_matrix[(j-2),:]=empty_column_list\n        #print('Check 2 - Deleting list')\n        del column_values[:],empty_column_list[:]\n    asset_matrix[np.isnan(asset_matrix)] = 0\n    #print(asset_matrix)\n    ## Add news to the matrix\n    #Asset data till row 13\n    \n    row_insert=14\n    added_dates=[]\n    news_sorted = news_train_df[(news_train_df.assetName == str(asset))].sort_values(by='time')\n    print('Total rows count ',len(news_sorted))\n    #time_values=news_train_df[(news_train_df.assetName == str(asset))]['time'].values.tolist()\n    #4 headline - name appears in headline, positive, negative, factual, earnings\n    '''headline_value=news_train_df[(news_train_df.assetName == str(asset))]['headline'].values.tolist()\n    '''\n    \n    #Name occurence, positive, negative, neutral, earnings report\n\n    #5 urgency - as is\n    urgency_values=values_date_avg(news_sorted,'urgency')\n    time_values=[*urgency_values.keys()]\n    urgency_values=[*urgency_values.values()]\n    asset_matrix,added_dates=sync_to_date_range(added_dates,row_insert,asset_matrix,time_values,urgency_values,market_sorted,'urgency')\n    row_insert+=1\n\n    '''#6 takesequence - as - is ignore\n    takeSequence_values=news_train_df[(news_train_df.assetName == str(asset))]['takeSequence'].values\n    '''\n    #7 provider\n    provider_values=news_train_df[(news_train_df.assetName == str(asset))]['provider'].values\n    provider_values_replace=category_replace(provider_values,'provider')  \n    provider_values=values_date_avg_mod(news_sorted,provider_values_replace,'provider')\n    provider_values=[*provider_values.values()]\n    asset_matrix,added_dates=sync_to_date_range(added_dates,row_insert,asset_matrix,time_values,provider_values,market_sorted,'provider')\n    print('Reduced row Count', len(provider_values))\n    row_insert+=1\n\n    #8 subjects\n    subject_values=news_train_df[(news_train_df.assetName == str(asset))]['subjects'].values\n    subject_count=text_process('subjects',subject_values)\n    subject_values=values_date_avg_mod(news_sorted,subject_count,'subjects')\n    subject_values=[*subject_values.values()]\n    asset_matrix,added_dates=sync_to_date_range(added_dates,row_insert,asset_matrix,time_values,subject_values,market_sorted,'subjects')\n    #print(len(subject_values))\n    row_insert+=1\n\n    #9 'audiences'\n    audiences_value=news_train_df[(news_train_df.assetName == str(asset))]['audiences'].values\n    audiences_count=text_process('audiences',audiences_value)\n    audiences_value=values_date_avg_mod(news_sorted,audiences_count,'audiences')\n    audiences_value=[*audiences_value.values()]\n    asset_matrix,added_dates=sync_to_date_range(added_dates,row_insert,asset_matrix,time_values,audiences_value,market_sorted,'audiences')\n    #print(len(audiences_value))\n    row_insert+=1\n\n\n    #10 'bodySize' - create intervals and map\n    bodySize_value=values_date_avg(news_sorted,'bodySize')\n    bodySize_value=[*bodySize_value.values()]\n    asset_matrix,added_dates=sync_to_date_range(added_dates,row_insert,asset_matrix,time_values,bodySize_value,market_sorted,'bodySize')\n    row_insert+=1\n\n    #11 'companyCount' \n    companyCount_value=values_date_avg(news_sorted,'companyCount')\n    companyCount_value=[*companyCount_value.values()]\n    asset_matrix,added_dates=sync_to_date_range(added_dates,row_insert,asset_matrix,time_values,companyCount_value,market_sorted,'companyCount')\n    #print(len(companyCount_value))\n    row_insert+=1\n\n    #12 'headlineTag'- assign numbers\n    headlineTag_value=news_train_df[(news_train_df.assetName == str(asset))]['headlineTag'].values\n    headlineTag_value_replace=category_replace(provider_values,'headlineTag')  \n    headlineTag_value=values_date_avg_mod(news_sorted,headlineTag_value_replace,'headlineTag')\n    headlineTag_value=[*headlineTag_value.values()]\n    asset_matrix,added_dates=sync_to_date_range(added_dates,row_insert,asset_matrix,time_values,headlineTag_value,market_sorted,'headlineTag')\n    #print(len(headlineTag_value))\n    #print(len(provider_values))\n\n\n    #13 'marketCommentary' -0/1\n    marketCommentary_value=news_train_df[(news_train_df.assetName == str(asset))]['marketCommentary'].values\n    marketCommentrary_repl_value=replace_txt_w_num(marketCommentary_value)\n    marketCommentary_value=values_date_avg_mod(news_sorted,marketCommentrary_repl_value,'marketCommentary')\n    marketCommentary_value=[*marketCommentary_value.values()]\n    asset_matrix,added_dates=sync_to_date_range(added_dates,row_insert,asset_matrix,time_values,marketCommentary_value,market_sorted,'marketCommentary')\n    #print(len(marketCommentary_value))\n    #print(len(provider_values))\n\n    #14 'sentenceCount' - create ranges\n    sentenceCount_value=values_date_avg(news_sorted,'sentenceCount')\n    sentenceCount_value=[*sentenceCount_value.values()]\n    asset_matrix,added_dates=sync_to_date_range(added_dates,row_insert,asset_matrix,time_values,sentenceCount_value,market_sorted,'sentenceCount')\n    #print(len(sentenceCount_value))\n    row_insert+=1\n    \n    #15 'wordCount' - create ranges\n    wordCount_value=values_date_avg(news_sorted,'wordCount')\n    wordCount_value=[*wordCount_value.values()]\n    asset_matrix,added_dates=sync_to_date_range(added_dates,row_insert,asset_matrix,time_values,wordCount_value,market_sorted,'wordCount')\n    #print(len(wordCount_value))\n    row_insert+=1\n    \n    #16 'assetCodes' - blank\n    #17 'assetName' - blank\n\n    #18 'firstMentionSentence' -create ranges\n    firstMentionSentence_value=values_date_avg(news_sorted,'firstMentionSentence')\n    firstMentionSentence_value=[*firstMentionSentence_value.values()]\n    asset_matrix,added_dates=sync_to_date_range(added_dates,row_insert,asset_matrix,time_values,firstMentionSentence_value,market_sorted,'firstMentionSentence')\n    #print(len(firstMentionSentence_value))\n    row_insert+=1\n    \n    #19 'relevance'- score between 0 and 1\n    relevance_value=values_date_avg(news_sorted,'relevance')\n    relevance_value=[*relevance_value.values()]\n    asset_matrix,added_dates=sync_to_date_range(added_dates,row_insert,asset_matrix,time_values,relevance_value,market_sorted,'relevance')\n    #print(len(relevance_value))\n    row_insert+=1\n    \n    #20 'sentimentClass' - 1,-1,0\n    sentimentClass_value=values_date_avg(news_sorted,'sentimentClass')\n    sentimentClass_value=[*sentimentClass_value.values()]\n    asset_matrix,added_dates=sync_to_date_range(added_dates,row_insert,asset_matrix,time_values,sentimentClass_value,market_sorted,'sentimentClass')\n    #print(len(sentimentClass_value))\n    row_insert+=1\n    \n    #21 'sentimentNegative' -  score between 0 and 1\n    sentimentNegative_value=values_date_avg(news_sorted,'sentimentNegative')\n    sentimentNegative_value=[*sentimentNegative_value.values()]\n    asset_matrix,added_dates=sync_to_date_range(added_dates,row_insert,asset_matrix,time_values,sentimentNegative_value,market_sorted,'sentimentNegative')\n    #print(len(sentimentNegative_value))\n    row_insert+=1\n    \n    #22 'sentimentNeutral' - score between 0 and 1\n    sentimentNeutral_value=values_date_avg(news_sorted,'sentimentNeutral')\n    sentimentNeutral_value=[*sentimentNeutral_value.values()]\n    asset_matrix,added_dates=sync_to_date_range(added_dates,row_insert,asset_matrix,time_values,sentimentNeutral_value,market_sorted,'sentimentNeutral')\n    #print(len(sentimentNeutral_value))\n    row_insert+=1\n    \n    #23 'sentimentPositive' - score between 0 and 1\n    sentimentPositive_value=values_date_avg(news_sorted,'sentimentPositive')\n    sentimentPositive_value=[*sentimentPositive_value.values()]\n    asset_matrix,added_dates=sync_to_date_range(added_dates,row_insert,asset_matrix,time_values,sentimentPositive_value,market_sorted,'sentimentPositive')\n    #print(len(sentimentPositive_value))\n    row_insert+=1\n    \n    #24 'sentimentWordCount' - create ranges\n    sentimentWordCount_value=values_date_avg(news_sorted,'sentimentWordCount')\n    sentimentWordCount_value=[*sentimentWordCount_value.values()]\n    asset_matrix,added_dates=sync_to_date_range(added_dates,row_insert,asset_matrix,time_values,sentimentWordCount_value,market_sorted,'sentimentWordCount')\n    #print(len(sentimentWordCount_value))\n    row_insert+=1\n\n\n    #25 'noveltyCount12H' - 0 to 6\n    noveltyCount12H_value=values_date_avg(news_sorted,'noveltyCount12H')\n    noveltyCount12H_value=[*noveltyCount12H_value.values()]\n    asset_matrix,added_dates=sync_to_date_range(added_dates,row_insert,asset_matrix,time_values,noveltyCount12H_value,market_sorted,'noveltyCount12H')\n    #print(len(noveltyCount12H_value))\n    row_insert+=1\n    \n    #26 'noveltyCount24H'- 0 to 6\n    noveltyCount24H_value=values_date_avg(news_sorted,'noveltyCount24H')\n    noveltyCount24H_value=[*noveltyCount24H_value.values()]\n    asset_matrix,added_dates=sync_to_date_range(added_dates,row_insert,asset_matrix,time_values,noveltyCount24H_value,market_sorted,'noveltyCount24H')\n    #print(len(noveltyCount24H_value))\n    row_insert+=1\n    \n    #27 'noveltyCount3D' - 0 to 6\n    noveltyCount3D_value=values_date_avg(news_sorted,'noveltyCount3D')\n    noveltyCount3D_value=[*noveltyCount3D_value.values()]\n    asset_matrix,added_dates=sync_to_date_range(added_dates,row_insert,asset_matrix,time_values,noveltyCount3D_value,market_sorted,'noveltyCount3D')\n    #print(len(noveltyCount3D_value))\n    row_insert+=1\n    \n    #28 'noveltyCount5D' - 0 to 6\n    noveltyCount5D_value=values_date_avg(news_sorted,'noveltyCount5D')\n    noveltyCount5D_value=[*noveltyCount5D_value.values()]\n    asset_matrix,added_dates=sync_to_date_range(added_dates,row_insert,asset_matrix,time_values,noveltyCount5D_value,market_sorted,'noveltyCount5D')\n    #print(len(noveltyCount5D_value))\n    row_insert+=1\n    \n    #29 'noveltyCount7D' - 0 to 6\n    noveltyCount7D_value=values_date_avg(news_sorted,'noveltyCount7D')\n    noveltyCount7D_value=[*noveltyCount7D_value.values()]\n    asset_matrix,added_dates=sync_to_date_range(added_dates,row_insert,asset_matrix,time_values,noveltyCount7D_value,market_sorted,'noveltyCount7D')\n    #print(len(noveltyCount7D_value))\n    row_insert+=1\n    \n    #30 'volumeCounts12H' - range till 21\n    volumeCounts12H_value=values_date_avg(news_sorted,'volumeCounts12H')\n    volumeCounts12H_value=[*volumeCounts12H_value.values()]\n    asset_matrix,added_dates=sync_to_date_range(added_dates,row_insert,asset_matrix,time_values,volumeCounts12H_value,market_sorted,'volumeCounts12H')\n    #print(len(volumeCounts12H_value))\n    row_insert+=1\n    \n    #31 'volumeCounts24H' - range till 21\n    volumeCounts24H_value=values_date_avg(news_sorted,'volumeCounts24H')\n    volumeCounts24H_value=[*volumeCounts24H_value.values()]\n    asset_matrix,added_dates=sync_to_date_range(added_dates,row_insert,asset_matrix,time_values,volumeCounts24H_value,market_sorted,'volumeCounts24H')\n    #print(len(volumeCounts24H_value))\n    row_insert+=1\n    \n    #32 'volumeCounts3D' - range till 21\n    volumeCounts3D_value=values_date_avg(news_sorted,'volumeCounts3D')\n    volumeCounts3D_value=[*volumeCounts3D_value.values()]\n    asset_matrix,added_dates=sync_to_date_range(added_dates,row_insert,asset_matrix,time_values,volumeCounts3D_value,market_sorted,'volumeCounts3D')\n    #print(len(volumeCounts3D_value))\n    row_insert+=1\n    \n    #33 'volumeCounts5D' - range till 27\n    volumeCounts5D_value=values_date_avg(news_sorted,'volumeCounts5D')\n    volumeCounts5D_value=[*volumeCounts5D_value.values()]\n    asset_matrix,added_dates=sync_to_date_range(added_dates,row_insert,asset_matrix,time_values,volumeCounts5D_value,market_sorted,'volumeCounts5D')\n    #print(len(volumeCounts5D_value))\n    row_insert+=1\n    \n    #34 'volumeCounts7D' - range till 37\n    volumeCounts7D_value=values_date_avg(news_sorted,'volumeCounts7D')\n    volumeCounts7D_value=[*volumeCounts7D_value.values()]\n    asset_matrix,added_dates=sync_to_date_range(added_dates,row_insert,asset_matrix,time_values,volumeCounts7D_value,market_sorted,'volumeCounts7D')\n    #print(len(volumeCounts7D_value))\n    row_insert+=1\n    \n    return asset_matrix\n   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0369673ca6966ff7ce0ad7560f0ea2ef7c39bfc5"},"cell_type":"code","source":"def prep_data(asset_matrix):\n    cause2=np.delete(asset_matrix,0,0)\n    cause=np.delete(cause2,[1,2,3,4,5,6,7,8,9,10],0)\n    \n    effect=asset_matrix[12,:]\n\n    for i in range(asset_matrix.shape[1]):\n        if cause[8,i]==0 and cause[2,i]==0:\n            np.delete(cause,i,1)\n            np.delete(effect,i,0)\n    cause=cause.transpose()\n    cause=normalize(cause)\n    effect=effect.transpose()\n    effect=np.delete(effect,0)\n    effectout=np.append(effect,[0,])\n    effectout[effectout>0]=1\n    effectout[effectout<0]=-1\n    #consider a column only if news exists and universe = 1\n    #print('Classes ',np.unique(effectout))\n    X=cause\n    y=effectout\n    \n    return X,y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f30d70929091c595b7a6c057b9f2e25de31f1a9f"},"cell_type":"code","source":"def train_my_model(X,y):\n    clf = LogisticRegression(random_state=0, solver='liblinear',multi_class='ovr',verbose=1).fit(X, y)\n    return clf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f7462782e3548914e7e9337c9c8def10f0c48423"},"cell_type":"code","source":"def append_matrix(asset,asset_matrix,market_obs_df,news_obs_df):\n    #print(asset_matrix.shape)\n    asset_matrix=np.insert(asset_matrix, asset_matrix.shape[1], 0, axis=1)\n    #print(asset_matrix.shape)\n    volume=market_obs_df[(market_obs_df.assetName == str(asset))]['volume'].values.tolist()\n    #print(volume)\n    asset_matrix[1,asset_matrix.shape[1]-1]= volume[0]\n    \n    returnsClosePrevRaw1=market_obs_df[(market_obs_df.assetName == str(asset))]['returnsClosePrevRaw1'].values.tolist()\n    asset_matrix[11,asset_matrix.shape[1]-1]= returnsClosePrevRaw1[0]\n    \n    time=market_obs_df[(market_obs_df.assetName == str(asset))]['time'].values.tolist()\n    time_upd=[datetime.datetime.utcfromtimestamp(i/(1e+9)).date() for i in time]\n    if time_upd[0].weekday()<5:\n        asset_matrix[13,asset_matrix.shape[1]-1]= 1\n    else:\n        asset_matrix[13,asset_matrix.shape[1]-1]= 0\n    \n    urgency=news_obs_df[(news_obs_df.assetName == str(asset))]['urgency']\n    asset_matrix[14,asset_matrix.shape[1]-1]= urgency[:].mean()\n\n    provider=news_obs_df[(news_obs_df.assetName == str(asset))]['provider']\n    provider_values_replace=category_replace(provider,'provider')  \n    try:\n        asset_matrix[15,asset_matrix.shape[1]-1]= sum(provider_values_replace[:])/len(provider_values_replace)\n    except:\n        asset_matrix[15,asset_matrix.shape[1]-1]=0\n        \n    subjects=news_obs_df[(news_obs_df.assetName == str(asset))]['subjects']\n    subject_count=text_process('subjects',subjects)\n    try:\n        asset_matrix[16,asset_matrix.shape[1]-1]= sum(subject_count[:])/len(subject_count)\n    except:\n        asset_matrix[16,asset_matrix.shape[1]-1]=0\n        \n    audiences=news_obs_df[(news_obs_df.assetName == str(asset))]['audiences']\n    audiences_count=text_process('audiences',audiences)\n    try:\n        asset_matrix[17,asset_matrix.shape[1]-1]= sum(audiences_count[:])/len(audiences_count)\n    except:\n        asset_matrix[17,asset_matrix.shape[1]-1]=0\n        \n    bodySize=news_obs_df[(news_obs_df.assetName == str(asset))]['bodySize']\n    asset_matrix[18,asset_matrix.shape[1]-1]= bodySize[:].mean()\n    \n    companyCount=news_obs_df[(news_obs_df.assetName == str(asset))]['companyCount']\n    asset_matrix[19,asset_matrix.shape[1]-1]= companyCount[:].mean()\n    \n    headlineTag=news_obs_df[(news_obs_df.assetName == str(asset))]['headlineTag']\n    headlineTag_value_replace=category_replace(headlineTag,'headlineTag')  \n    try:\n        asset_matrix[20,asset_matrix.shape[1]-1]= sum(headlineTag_value_replace[:])/len(headlineTag_value_replace)\n    except:\n        asset_matrix[20,asset_matrix.shape[1]-1]=0\n        \n    marketCommentary=news_obs_df[(news_obs_df.assetName == str(asset))]['marketCommentary']\n    marketCommentrary_repl_value=replace_txt_w_num(marketCommentary)\n    try:\n        asset_matrix[21,asset_matrix.shape[1]-1]= sum(headlineTag_value_replace[:])/len(headlineTag_value_replace)\n    except:\n        asset_matrix[21,asset_matrix.shape[1]-1]=0\n        \n    sentenceCount=news_obs_df[(news_obs_df.assetName == str(asset))]['sentenceCount']\n    asset_matrix[22,asset_matrix.shape[1]-1]= sentenceCount[:].mean()\n    \n    wordCount=news_obs_df[(news_obs_df.assetName == str(asset))]['wordCount']\n    asset_matrix[23,asset_matrix.shape[1]-1]= wordCount[:].mean()\n    \n    firstMentionSentence=news_obs_df[(news_obs_df.assetName == str(asset))]['firstMentionSentence']\n    asset_matrix[24,asset_matrix.shape[1]-1]= firstMentionSentence[:].mean()\n    \n    relevance=news_obs_df[(news_obs_df.assetName == str(asset))]['relevance']\n    asset_matrix[25,asset_matrix.shape[1]-1]= relevance[:].mean()\n    \n    sentimentClass=news_obs_df[(news_obs_df.assetName == str(asset))]['sentimentClass']\n    asset_matrix[26,asset_matrix.shape[1]-1]= sentimentClass[:].mean()\n    \n    sentimentNegative=news_obs_df[(news_obs_df.assetName == str(asset))]['sentimentNegative']\n    asset_matrix[27,asset_matrix.shape[1]-1]= sentimentNegative[:].mean()\n    \n    sentimentNeutral=news_obs_df[(news_obs_df.assetName == str(asset))]['sentimentNeutral']\n    asset_matrix[28,asset_matrix.shape[1]-1]= sentimentNeutral[:].mean()\n    \n    sentimentPositive=news_obs_df[(news_obs_df.assetName == str(asset))]['sentimentPositive']\n    asset_matrix[29,asset_matrix.shape[1]-1]= sentimentPositive[:].mean()\n    \n    sentimentWordCount=news_obs_df[(news_obs_df.assetName == str(asset))]['sentimentWordCount']\n    asset_matrix[30,asset_matrix.shape[1]-1]= sentimentWordCount[:].mean()\n    \n    noveltyCount12H=news_obs_df[(news_obs_df.assetName == str(asset))]['noveltyCount12H']\n    asset_matrix[31,asset_matrix.shape[1]-1]= noveltyCount12H[:].mean()\n    \n    noveltyCount24H=news_obs_df[(news_obs_df.assetName == str(asset))]['noveltyCount24H']\n    asset_matrix[32,asset_matrix.shape[1]-1]= noveltyCount24H[:].mean()\n    \n    noveltyCount3D=news_obs_df[(news_obs_df.assetName == str(asset))]['noveltyCount3D']\n    asset_matrix[33,asset_matrix.shape[1]-1]= noveltyCount3D[:].mean()\n    \n    noveltyCount5D=news_obs_df[(news_obs_df.assetName == str(asset))]['noveltyCount5D']\n    asset_matrix[34,asset_matrix.shape[1]-1]= noveltyCount5D[:].mean()\n    \n    noveltyCount7D=news_obs_df[(news_obs_df.assetName == str(asset))]['noveltyCount7D']\n    asset_matrix[35,asset_matrix.shape[1]-1]= noveltyCount7D[:].mean()\n    \n    volumeCounts12H=news_obs_df[(news_obs_df.assetName == str(asset))]['volumeCounts12H']\n    asset_matrix[36,asset_matrix.shape[1]-1]= volumeCounts12H[:].mean()\n    \n    volumeCounts24H=news_obs_df[(news_obs_df.assetName == str(asset))]['volumeCounts24H']\n    asset_matrix[37,asset_matrix.shape[1]-1]= volumeCounts24H[:].mean()\n    \n    volumeCounts3D=news_obs_df[(news_obs_df.assetName == str(asset))]['volumeCounts3D']\n    asset_matrix[38,asset_matrix.shape[1]-1]= volumeCounts3D[:].mean()\n    \n    volumeCounts5D=news_obs_df[(news_obs_df.assetName == str(asset))]['volumeCounts5D']\n    asset_matrix[39,asset_matrix.shape[1]-1]= volumeCounts5D[:].mean()\n    \n    volumeCounts7D=news_obs_df[(news_obs_df.assetName == str(asset))]['volumeCounts7D']\n    asset_matrix[40,asset_matrix.shape[1]-1]= volumeCounts7D[:].mean()\n    \n    X,y=prep_data(asset_matrix)\n    #print(X[-1,:])\n    return X","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"062699b4f4adbe0daa0454c859f9177e97fb62e8","scrolled":false},"cell_type":"code","source":"dict_assets={}\ntraded_w_news=find_traded_assets(market_train_df,news_train_df)\ncount=1\nfor i in traded_w_news:\n    #if i == 'Agilent Technologies Inc':\n    asset_matrix=build_matrix(market_train_df, news_train_df,i)\n    X,y=prep_data(asset_matrix)\n    model=train_my_model(X,y)\n    dict_assets[i]=[model,X,asset_matrix]\n    print ('\\nModel done for ',i, 'which is ',count,'/',len(traded_w_news),'\\n')\n    count+=1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f2a0672d2526129472fbfdcc3aa535579ba4bf8f","scrolled":false},"cell_type":"code","source":"for (market_obs_df, news_obs_df, predictions_template_df) in env.get_prediction_days():\n    predictions_df=predictions_template_df.copy()\n    count=0\n    for i in range(len(predictions_template_df)):\n        asset=market_obs_df['assetName'][i]\n        assetcode=market_obs_df['assetCode'][i]\n        #print(asset)\n        if asset in traded_w_news:\n            model=dict_assets[asset][0]\n            matrix=dict_assets[asset][2]\n            X_upd=append_matrix(asset,matrix,market_obs_df, news_obs_df)\n            X1=X_upd[X_upd[:,0]>0]\n            X2=X1[0,:]\n            Xm=np.vstack([X2, X_upd[-1,:]])\n            prediction1=model.predict_proba(Xm)\n            prediction2=model.predict(Xm)\n            prediction=np.amax(prediction1[1,:])*prediction2[1]\n            predictions_df.confidenceValue[i] = prediction\n        else:\n            predictions_df.confidenceValue[i] = 0\n        count+=1\n        print('completed ',count,' out of ',len(predictions_df))\n    env.predict(predictions_df)\nprint('Done')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b79a211b30e0aaf4d1c5b5df249ff3a12917bfdd"},"cell_type":"code","source":"env.write_submission_file()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}