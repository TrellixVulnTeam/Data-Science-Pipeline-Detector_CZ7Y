{"cells":[{"metadata":{"_uuid":"9d3252ef8a4f2df75e5df13f4c3ad848666dbd3a"},"cell_type":"markdown","source":"**Importing used libraries**"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom matplotlib import pylab as plt\nfrom collections import Counter\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler \nfrom sklearn import preprocessing\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom kaggle.competitions import twosigmanews\nfrom itertools import chain\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.linear_model import LinearRegression \nfrom sklearn.model_selection import train_test_split\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"64c3e87404f91eea916a86846ee5df925a6e05e7"},"cell_type":"markdown","source":"**Loading training dataset**"},{"metadata":{"trusted":true,"_uuid":"2130ec36b60786f8d29eec6723380fc66e25fb01"},"cell_type":"code","source":"env = twosigmanews.make_env()\n(market_df, news_df) = env.get_training_data()\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"255f20aecc8a87b864d9b6995dbd18246f32a5c7"},"cell_type":"markdown","source":"**Define Global Variables**"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"\nlr = LinearRegression()\nMLPR = MLPRegressor(hidden_layer_sizes=(50, ), activation= 'relu', solver='adam', alpha=0.0001, batch_size='auto')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5069d8a6bc826d15650723b554d45b5ccc88792a"},"cell_type":"markdown","source":"**Starting Data exploration and Analysis**"},{"metadata":{"_uuid":"640fd2c4c9a4c436d5a3731b87f989d85f966b3d"},"cell_type":"markdown","source":"*EDA on market data*"},{"metadata":{"trusted":true,"_uuid":"5d8c223685b7a2c9aa001b5a3e511beee1007775"},"cell_type":"code","source":"def merge(market_df, news_df,istrain):\n    \n    sc = StandardScaler()\n    pca = PCA()\n    max_abs_scale = preprocessing.MaxAbsScaler()\n    \n    \n    #*Change date to more simple way to use*\n    market_df['time']= pd.to_datetime(market_df['time'], format = '%Y%m%d', utc = None)\n    market_df['time'] = (market_df['time'] - np.timedelta64(22,'h')).dt.ceil('1D')\n    \n    #*Dropping down the data rows that has zero value as universe*\n    \n    if 'universe' in market_df.columns:\n        market_df = market_df[market_df['universe']==1]\n        market_df = market_df.drop(['universe'], axis =1)\n    \n    #*Dropping the rows that have null values inside*\n    \n    market_df = market_df.fillna(market_df.mode().iloc[0])\n       \n    \n    #Dropping unnecessary columns\n    news_df = news_df.drop(['sourceTimestamp','firstCreated','sourceId','headline','headlineTag','assetName'],axis=1)\n    \n    #Date-time conversion,\n    if istrain == True:\n        news_df['time']= pd.to_datetime(news_df['time'], format = '%Y%m%d', utc = None)\n        news_df['time'] = (news_df['time'] - np.timedelta64(22,'h')).dt.ceil('1D')\n    else:\n        news_df['time'] = market_df['time'][0]\n        news_df['time']= pd.to_datetime(news_df['time'], format = '%Y%m%d', utc = None)\n        news_df['time'] = (news_df['time'] - np.timedelta64(22,'h')).dt.ceil('1D')\n        \n        \n        \n    \n    #Using PCA on volumes\n    news_volume = news_df.loc[:,\"noveltyCount12H\":\"volumeCounts7D\"]\n    news_volume = sc.fit_transform(news_volume)\n    news_volume = pca.fit_transform(news_volume)\n    explained_variance = pca.explained_variance_ratio_\n    \n    pca = PCA(n_components = 3)\n    news_volume =pca.fit_transform(news_volume)\n    \n    news_df = news_df.drop(['noveltyCount12H','noveltyCount24H','noveltyCount3D','noveltyCount5D','noveltyCount7D','volumeCounts12H','volumeCounts24H','volumeCounts3D','volumeCounts5D','volumeCounts7D'], axis =1)\n    \n    news_volume_df = pd.DataFrame(news_volume,columns = ['PCA1','PCA2','PCA3'])\n    \n    news_df['vol1'] = news_volume_df['PCA1']\n    news_df['vol2'] = news_volume_df['PCA2']\n    news_df['vol3'] = news_volume_df['PCA3']\n    \n    #using LDA to convert sentiment data\n    news_sentiment_X = news_df.loc[:,\"sentimentNegative\":\"sentimentPositive\"].values\n    news_sentiment_Y = news_df.loc[:,\"sentimentClass\"].values\n    news_sentiment_X = sc.fit_transform(news_sentiment_X)\n    \n    lda = LinearDiscriminantAnalysis(n_components = 1)\n    news_sentiment = lda.fit_transform(news_sentiment_X,news_sentiment_Y)\n    news_sentiment = max_abs_scale.fit_transform(news_sentiment)\n    \n    news_sentiment_df = pd.DataFrame(news_sentiment,columns = ['sentiment'])\n    news_df['sentiment'] = news_sentiment\n    \n    news_df = news_df.drop(['sentimentNegative','sentimentNeutral','sentimentPositive','sentimentClass'], axis = 1)\n    \n    #Define sentiment proportion by using word counts\n    sentiment_proportion = news_df['sentimentWordCount'].values / news_df['wordCount'].values\n    news_df['sentiment_prop'] = sentiment_proportion\n    news_df = news_df.drop(['sentimentWordCount','wordCount'], axis = 1)\n    \n    #Define sentence proportion by using sentence counts\n    sentence_proportion = news_df['firstMentionSentence'].values / news_df['sentenceCount'].values\n    news_df = news_df.drop(['firstMentionSentence','sentenceCount'], axis = 1)\n    news_df['sentence_prop'] = sentence_proportion\n    \n    #Using subjects and audiences as their lenghts\n    news_df['subjects']=news_df.subjects.apply(len)\n    news_df['audiences']=news_df.audiences.apply(len)\n    \n    #Seperating asset codes  -----  MAKE NEWS TABLE MERGABLE\n    news_df['assetCodes'] = news_df['assetCodes'].str.findall(f\"'([\\w\\./]+)'\")\n    \n    assetCodes_expanded = list(chain(*news_df['assetCodes']))\n    assetCodes_index = news_df.index.repeat( news_df['assetCodes'].apply(len) )\n\n    assert len(assetCodes_index) == len(assetCodes_expanded)\n    df_assetCodes = pd.DataFrame({'level_0': assetCodes_index, 'assetCode': assetCodes_expanded})\n    \n    news_train_df_expanded = pd.merge(df_assetCodes, news_df,left_on='level_0', right_index=True, suffixes=(['','_old']))\n    \n    #DROP UNNECESSARY TABLES AFTER MERGING\n    del news_df\n    del df_assetCodes\n    del assetCodes_index\n    del assetCodes_expanded\n    \n    #DROP UNNECESSARY COLUMNS\n    news_train_df_expanded = news_train_df_expanded.drop(['level_0','assetCodes'], axis = 1)\n    if istrain == False:\n        news_train_df_expanded = news_train_df_expanded.drop_duplicates(subset = 'assetCode') \n    \n    #MERGE TWO TABLES\n    X_df =  news_train_df_expanded.merge(market_df, how = 'right', on = ['time', 'assetCode'])\n    \n    if 'returnsOpenNextMktres10' in X_df.columns:\n        Y_df = X_df['returnsOpenNextMktres10']\n        X_df = X_df.drop('returnsOpenNextMktres10', axis = 1)\n    else:\n        Y_df = 0\n\n    del news_train_df_expanded\n    del market_df\n    \n    X_df = X_df.drop(['assetCode','time','provider','marketCommentary','assetName'], axis = 1)\n    X_df.fillna(0,inplace=True)\n    \n    X_df = X_df.sort_index(axis = 1)\n    \n    return X_df, Y_df\n\n    \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4602f148b9edfe3718eaac19fa53bbf9fa698f0d"},"cell_type":"code","source":"X_df, Y_df = merge(market_df, news_df,True)\ndays = env.get_prediction_days()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"01741add0eea240c86c6ae66974249f90b4154bc"},"cell_type":"code","source":"\"\"\"#****VALIDATION SET DATA******\n    X_train, X_valid, y_train, y_valid = train_test_split(X_df, Y_df, test_size=0.2, random_state=0)\n\n    del X_df\n    del Y_df\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d341b1f8282cfc341f082b50cea2133aa60a6f86"},"cell_type":"markdown","source":"TRAIN DATA"},{"metadata":{"trusted":true,"_uuid":"13d5390eca286c144a588e7349899bac802f196c"},"cell_type":"code","source":"MLPR.fit(X_df, Y_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"da303f01d64c275637315170411a10fde66c8d73"},"cell_type":"code","source":"for (market_test_df, news_test_df, predictions_template_df) in days:\n    X_test_df, Y_test_df = merge(market_test_df, news_test_df,False)\n    predictions_template_df['confidenceValue'] = MLPR.predict(X_test_df)\n    env.predict(predictions_template_df)\nprint('Done')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"145b7940d9ddd05c58f078c4ea681bed8d3519da"},"cell_type":"code","source":"env.write_submission_file()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fc37a2d30eaeba18a938c52f07de34045e63dfa7"},"cell_type":"code","source":"import os\nprint([filename for filename in os.listdir('.') if '.csv' in filename])","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}