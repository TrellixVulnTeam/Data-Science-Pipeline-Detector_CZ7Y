{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport tensorflow as tf\nimport re\nfrom nltk.corpus import stopwords\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import median_absolute_error as mae\nfrom sklearn.metrics import mean_squared_error as mse\nfrom sklearn.metrics import accuracy_score as acc\nimport matplotlib.pyplot as plt\n\nfrom keras.models import Sequential\nfrom keras import initializers\nfrom keras.layers import Dropout, Activation, Embedding, Convolution1D, MaxPooling1D, Input, Dense, BatchNormalization, Flatten, Reshape, Concatenate\nfrom keras.layers.recurrent import LSTM, GRU\nfrom keras.callbacks import Callback, ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\nfrom keras.models import Model\nfrom keras.optimizers import Adam, SGD, RMSprop\nfrom keras import regularizers","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# This competition settings\nfrom kaggle.competitions import twosigmanews\nenv = twosigmanews.make_env()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"94fb71538b832ec01b9489fa7aac13e21d57c49d"},"cell_type":"code","source":"import lightgbm as lgb\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom itertools import chain\nfrom time import time\nimport seaborn as sns\nimport os\nimport gc\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import QuantileTransformer,StandardScaler, MinMaxScaler,OneHotEncoder, LabelEncoder, RobustScaler\nfrom sklearn.metrics import accuracy_score\nfrom keras import optimizers\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Conv2D, Flatten, MaxPool2D, Dropout, BatchNormalization, Activation\nfrom keras.callbacks import ModelCheckpoint, Callback, EarlyStopping, ReduceLROnPlateau\nfrom keras.utils import to_categorical\nimport tensorflow\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b10d3c0142b43ddcadc434aa76c157f5dc66af6f"},"cell_type":"code","source":"toy=False\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"003777f4ac1aef3b298a1b40032417bb17498db1"},"cell_type":"code","source":"(market_train_df, news_train_df) = env.get_training_data()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f77b01bc05ddfd5264df36ea706511afffee8f54"},"cell_type":"code","source":"market_train_df.shape\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"34d30cb0c3a2519dfa183be4eaea04c8469e9586"},"cell_type":"code","source":"news_train_df.shape\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e31f4cc4cf3eb44f4e2f51b8cf61b0e20437bcdd"},"cell_type":"code","source":"# We will reduce the number of samples for memory reasons\nif toy:\n    market_train_df = market_train_df.tail(100_000)\n    news_train_df = news_train_df.tail(300_000)\nelse:\n    market_train_df = market_train_df.tail(750_000)\n    news_train_df = news_train_df.tail(1500_000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2944fbdf4488ab88391d8390563443742e73ecb8"},"cell_type":"code","source":"contractions = { \n\"ain't\": \"am not\",\n\"aren't\": \"are not\",\n\"can't\": \"cannot\",\n\"can't've\": \"cannot have\",\n\"'cause\": \"because\",\n\"could've\": \"could have\",\n\"couldn't\": \"could not\",\n\"couldn't've\": \"could not have\",\n\"didn't\": \"did not\",\n\"doesn't\": \"does not\",\n\"don't\": \"do not\",\n\"hadn't\": \"had not\",\n\"hadn't've\": \"had not have\",\n\"hasn't\": \"has not\",\n\"haven't\": \"have not\",\n\"he'd\": \"he would\",\n\"he'd've\": \"he would have\",\n\"he'll\": \"he will\",\n\"he's\": \"he is\",\n\"how'd\": \"how did\",\n\"how'll\": \"how will\",\n\"how's\": \"how is\",\n\"i'd\": \"i would\",\n\"i'll\": \"i will\",\n\"i'm\": \"i am\",\n\"i've\": \"i have\",\n\"isn't\": \"is not\",\n\"it'd\": \"it would\",\n\"it'll\": \"it will\",\n\"it's\": \"it is\",\n\"let's\": \"let us\",\n\"ma'am\": \"madam\",\n\"mayn't\": \"may not\",\n\"might've\": \"might have\",\n\"mightn't\": \"might not\",\n\"must've\": \"must have\",\n\"mustn't\": \"must not\",\n\"needn't\": \"need not\",\n\"oughtn't\": \"ought not\",\n\"shan't\": \"shall not\",\n\"sha'n't\": \"shall not\",\n\"she'd\": \"she would\",\n\"she'll\": \"she will\",\n\"she's\": \"she is\",\n\"should've\": \"should have\",\n\"shouldn't\": \"should not\",\n\"that'd\": \"that would\",\n\"that's\": \"that is\",\n\"there'd\": \"there had\",\n\"there's\": \"there is\",\n\"they'd\": \"they would\",\n\"they'll\": \"they will\",\n\"they're\": \"they are\",\n\"they've\": \"they have\",\n\"wasn't\": \"was not\",\n\"we'd\": \"we would\",\n\"we'll\": \"we will\",\n\"we're\": \"we are\",\n\"we've\": \"we have\",\n\"weren't\": \"were not\",\n\"what'll\": \"what will\",\n\"what're\": \"what are\",\n\"what's\": \"what is\",\n\"what've\": \"what have\",\n\"where'd\": \"where did\",\n\"where's\": \"where is\",\n\"who'll\": \"who will\",\n\"who's\": \"who is\",\n\"won't\": \"will not\",\n\"wouldn't\": \"would not\",\n\"you'd\": \"you would\",\n\"you'll\": \"you will\",\n\"you're\": \"you are\"\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5fea584fb35f5532ef3f57c96b26516a755203d2"},"cell_type":"code","source":"def clean_text(text, remove_stopwords = True):\n    '''Remove unwanted characters and format the text to create fewer nulls word embeddings'''\n    \n    # Convert words to lower case\n    text = text.lower()\n    \n    # Replace contractions with their longer forms \n    if True:\n        text = text.split()\n        new_text = []\n        for word in text:\n            if word in contractions:\n                new_text.append(contractions[word])\n            else:\n                new_text.append(word)\n        text = \" \".join(new_text)\n    \n    # Format words and remove unwanted characters\n    text = re.sub(r'&amp;', '', text) \n    text = re.sub(r'0,0', '00', text) \n    text = re.sub(r'[_\"\\-;%()|.,+&=*%.,!?:#@\\[\\]]', ' ', text)\n    text = re.sub(r'\\'', ' ', text)\n    text = re.sub(r'\\$', ' $ ', text)\n    text = re.sub(r'u s ', ' united states ', text)\n    text = re.sub(r'u n ', ' united nations ', text)\n    text = re.sub(r'u k ', ' united kingdom ', text)\n    text = re.sub(r'j k ', ' jk ', text)\n    text = re.sub(r' s ', ' ', text)\n    text = re.sub(r' yr ', ' year ', text)\n    text = re.sub(r' l g b t ', ' lgbt ', text)\n    text = re.sub(r'0km ', '0 km ', text)\n    \n    # Optionally, remove stop words\n    if remove_stopwords:\n        text = text.split()\n        stops = set(stopwords.words(\"english\"))\n        text = [w for w in text if not w in stops]\n        text = \" \".join(text)\n\n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f14f84857bcfc178255983b34e0a0b161d3b9f48"},"cell_type":"code","source":"news_cols_agg = {\n    'urgency': ['min', 'count'],\n    'takeSequence': ['max'],\n    'bodySize': ['min', 'max', 'mean', 'std'],\n    'wordCount': ['min', 'max', 'mean', 'std'],\n    'sentenceCount': ['min', 'max', 'mean', 'std'],\n    'companyCount': ['min', 'max', 'mean', 'std'],\n    'marketCommentary': ['min', 'max', 'mean', 'std'],\n    'relevance': ['min', 'max', 'mean', 'std'],\n    'sentimentNegative': ['min', 'max', 'mean', 'std'],\n    'sentimentNeutral': ['min', 'max', 'mean', 'std'],\n    'sentimentPositive': ['min', 'max', 'mean', 'std'],\n    'sentimentWordCount': ['min', 'max', 'mean', 'std'],\n    'noveltyCount12H': ['min', 'max', 'mean', 'std'],\n    'noveltyCount24H': ['min', 'max', 'mean', 'std'],\n    'noveltyCount3D': ['min', 'max', 'mean', 'std'],\n    'noveltyCount5D': ['min', 'max', 'mean', 'std'],\n    'noveltyCount7D': ['min', 'max', 'mean', 'std'],\n    'volumeCounts12H': ['min', 'max', 'mean', 'std'],\n    'volumeCounts24H': ['min', 'max', 'mean', 'std'],\n    'volumeCounts3D': ['min', 'max', 'mean', 'std'],\n    'volumeCounts5D': ['min', 'max', 'mean', 'std'],\n    'volumeCounts7D': ['min', 'max', 'mean', 'std']\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dbd4f6c9fe6a7cdb1a97be3773a0b105072833e0"},"cell_type":"code","source":"def join_market_news(market_train_df, news_train_df):\n    # Fix asset codes (str -> list)\n    news_train_df['assetCodes'] = news_train_df['assetCodes'].str.findall(f\"'([\\w\\./]+)'\")    \n    \n    # Expand assetCodes\n    assetCodes_expanded = list(chain(*news_train_df['assetCodes']))\n    assetCodes_index = news_train_df.index.repeat( news_train_df['assetCodes'].apply(len) )\n\n    assert len(assetCodes_index) == len(assetCodes_expanded)\n    df_assetCodes = pd.DataFrame({'level_0': assetCodes_index, 'assetCode': assetCodes_expanded})\n\n    # Create expandaded news (will repeat every assetCodes' row)\n    news_cols = ['time', 'assetCodes'] + sorted(news_cols_agg.keys())\n    news_train_df_expanded = pd.merge(df_assetCodes, news_train_df[news_cols], left_on='level_0', right_index=True, suffixes=(['','_old']))\n\n    # Free memory\n    del news_train_df, df_assetCodes\n    gc.collect()\n\n    # Aggregate numerical news features\n    news_train_df_aggregated = news_train_df_expanded.groupby(['time', 'assetCode']).agg(news_cols_agg)\n    \n    # Free memory\n    del news_train_df_expanded\n    gc.collect()\n\n    # Convert to float32 to save memory\n    news_train_df_aggregated = news_train_df_aggregated.apply(np.float32)\n\n    # Flat columns\n    news_train_df_aggregated.columns = ['_'.join(col).strip() for col in news_train_df_aggregated.columns.values]\n\n    # Join with train\n    market_train_df = market_train_df.join(news_train_df_aggregated, on=['time', 'assetCode'])\n\n    # Free memory\n    del news_train_df_aggregated\n    gc.collect()\n    \n    return market_train_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2cd22f8ea8f860f581b49f87630eca997aa82a97"},"cell_type":"code","source":"market_df = join_market_news(market_train_df, news_train_df)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d5acec6fbcaeda56410b8e8d51b2c4dd47eeaa43"},"cell_type":"code","source":"# Split to train, validation and test.\n# ToDo: remove shuffle, use generator.\n#market_train_df, market_test_df = train_test_split(market[market.time > '2009'].sample(100000, random_state=42), shuffle=True, random_state=24)\nmarket_train_df, market_test_df = train_test_split(market_df, shuffle=True, random_state=24)\nmarket_train_df, market_val_df = train_test_split(market_train_df, shuffle=True, random_state=24)\n\n# Look at min/max and quantiles\nmarket_train_df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8c1d1d48152434b21bb5fc2e784ff9a2bb94f5d2"},"cell_type":"code","source":"class Prepro:\n    \"\"\"\n    Bring all preprocessing here: scale, encoding\n    Should be fit on train data and called on train, validation or test data\n    \"\"\"\n    \n    def __init__(self, feature_cols, cat_cols, time_cols, numeric_cols, label_cols):\n        self.feature_cols = feature_cols\n        self.cat_cols = cat_cols\n        self.time_cols = time_cols\n        self.numeric_cols = numeric_cols\n        self.label_cols = label_cols\n        self.cats={}\n    \n    def transformXy(self, df):\n        \"\"\"\n        Preprocess and return X,y\n        \"\"\"\n        df = df.copy()\n        # Scale, encode etc. features\n        X = self.transform(df)\n        # Scale labels\n        df[self.label_cols] = self.y_scaler.transform(df[self.label_cols])\n        y = df[self.label_cols]\n        return(X,y)\n    \n    def transform(self, df):\n        \"\"\"\n        Preprocess and return X\n        \"\"\"\n        # Add day, week, year\n        df = self.prepare_time_cols(df)\n        # Fill nans\n        df[self.numeric_cols] = df[self.numeric_cols].fillna(0)\n        # Preprocess categorical features\n        for col in cat_cols:\n            df[col] = df[col].apply(lambda cat_name: self.prepare_cat_cols(cat_name, col))\n        # Scale numeric features and labels\n        df[self.numeric_cols+self.time_cols] = self.numeric_scaler.transform(df[self.numeric_cols+self.time_cols])\n        # Return X\n        return df[self.feature_cols]\n    \n    def fit(self, df):\n        \"\"\"\n        Fit preprocessing scalers, encoders on given train df\n        To be called on train df only\n        \"\"\"\n        # Extract day, week, year from time\n        df = df.copy()\n        df = self.prepare_time_cols(df)\n        # Handle strange cases, impute etc.\n        df = self.prepare_train_df(df)\n        # Use QuantileTransformer to handle outliers\n        # Fit for labels\n        self.y_scaler = QuantileTransformer()\n        self.y_scaler.fit(df[self.label_cols])\n        # Fit for numeric and time\n        self.numeric_scaler = QuantileTransformer()\n        self.numeric_scaler.fit(df[self.numeric_cols + self.time_cols])\n        # Fit for categories\n        # Organize dictionary, each category column has list with values\n        self.cats=dict()\n        for col in cat_cols:\n            self.cats[col] = list(df[col].unique())\n\n    def prepare_train_df(self, train_df):\n        \"\"\"\n        Clean na, remove strange cases.\n        For train dataset only. \n        \"\"\"\n        # Handle nans\n        train_df = train_df.copy()\n        # Need better imputer\n        # for col in numeric_cols:\n        #     market_train_df[col] = market_train_df[col].fillna(market_train_df[col].mean())\n        train_df.tail()\n        train_df[self.numeric_cols] = train_df[self.numeric_cols].fillna(0)\n\n        # # Remove strange cases with close/open ratio > 2\n        max_ratio  = 2\n        train_df = train_df[np.abs(train_df['close'] / train_df['open']) <= max_ratio]\n        return(train_df)\n\n    def prepare_time_cols(self, df):\n        \"\"\" \n        Extract time parts, they are important for time series \n        \"\"\"\n        df = df.copy()\n        df['year'] = df['time'].dt.year\n        # Maybe remove month because week of year can handle the same info\n        df['day'] = df['time'].dt.day\n        # Week of year\n        df['week'] = df['time'].dt.week\n        df['dayofweek'] = df['time'].dt.dayofweek\n        return(df)\n        \n    def prepare_cat_cols(self, cat_name, col):\n        \"\"\"\n        Encode categorical features to numbers\n        \"\"\"\n        try:\n            # Transform to index of name in stored names list\n            index_value = self.cats[col].index(cat_name)\n        except ValueError:\n            # If new value, add it to the list and return new index\n            self.cats[col].append(cat_name)\n            index_value = len(self.cats[col])\n        index_value = 1.0/(index_value+1.0)\n        return(index_value)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ba751ab140e6d7f0f49f4505cfa5bc210d3f55b1"},"cell_type":"code","source":"    # Features\n    cat_cols = ['assetCode']\n    time_cols=['year', 'week', 'day', 'dayofweek']\n    mkt_numeric_cols = ['volume', 'close', 'open', 'returnsClosePrevRaw1', 'returnsOpenPrevRaw1', 'returnsClosePrevMktres1',\n                    'returnsOpenPrevMktres1', 'returnsClosePrevRaw10', 'returnsOpenPrevRaw10', 'returnsClosePrevMktres10',\n                    'returnsOpenPrevMktres10']\n    \n    news_numeric_cols = [\n#        'returnsClosePrevRaw1', 'returnsOpenPrevRaw1',\n#        'returnsClosePrevMktres1', 'returnsOpenPrevMktres1',\n#        'returnsClosePrevRaw10', 'returnsOpenPrevRaw10',\n#        'returnsClosePrevMktres10', 'returnsOpenPrevMktres10',\n#        'returnsOpenNextMktres10', 'universe', 'urgency_min', 'urgency_count',\n#        'takeSequence_max', 'bodySize_min', 'bodySize_max', 'bodySize_mean',\n#        'bodySize_std', 'wordCount_min', 'wordCount_max', 'wordCount_mean',\n#        'wordCount_std', 'sentenceCount_min', 'sentenceCount_max',\n#        'sentenceCount_mean', 'sentenceCount_std', 'companyCount_min',\n#        'companyCount_max', 'companyCount_mean', 'companyCount_std',\n#        'marketCommentary_min', 'marketCommentary_max', 'marketCommentary_mean',\n#        'marketCommentary_std', \n        'relevance_min', 'relevance_max',\n        'relevance_mean', 'relevance_std', 'sentimentNegative_min',\n        'sentimentNegative_max', 'sentimentNegative_mean',\n        'sentimentNegative_std', 'sentimentNeutral_min', 'sentimentNeutral_max',\n        'sentimentNeutral_mean', 'sentimentNeutral_std',\n        'sentimentPositive_min', 'sentimentPositive_max',\n        'sentimentPositive_mean', 'sentimentPositive_std',\n        'sentimentWordCount_min', 'sentimentWordCount_max',\n        'sentimentWordCount_mean', 'sentimentWordCount_std',\n        'noveltyCount12H_min', 'noveltyCount12H_max', 'noveltyCount12H_mean',\n        'noveltyCount12H_std', 'noveltyCount24H_min', 'noveltyCount24H_max',\n        'noveltyCount24H_mean', 'noveltyCount24H_std', 'noveltyCount3D_min',\n        'noveltyCount3D_max', 'noveltyCount3D_mean', 'noveltyCount3D_std',\n        'noveltyCount5D_min', 'noveltyCount5D_max', 'noveltyCount5D_mean',\n        'noveltyCount5D_std', 'noveltyCount7D_min', 'noveltyCount7D_max',\n        'noveltyCount7D_mean', 'noveltyCount7D_std']\n#         'volumeCounts12H_min',\n#        'volumeCounts12H_max', 'volumeCounts12H_mean', 'volumeCounts12H_std',\n#        'volumeCounts24H_min', 'volumeCounts24H_max', 'volumeCounts24H_mean',\n#        'volumeCounts24H_std', 'volumeCounts3D_min', 'volumeCounts3D_max',\n#        'volumeCounts3D_mean', 'volumeCounts3D_std', 'volumeCounts5D_min',\n#        'volumeCounts5D_max', 'volumeCounts5D_mean', 'volumeCounts5D_std',\n#        'volumeCounts7D_min', 'volumeCounts7D_max', 'volumeCounts7D_mean',\n#        'volumeCounts7D_std']\n    \n    numeric_cols = mkt_numeric_cols + news_numeric_cols\n    \n    feature_cols = cat_cols + time_cols + numeric_cols\n    \n    # Labels\n    label_cols = ['returnsOpenNextMktres10']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"45ea48afb9e9de3347fd03cc51588f17df12d899"},"cell_type":"code","source":"print (np.unique(feature_cols).shape)\nprint (len(feature_cols))\nprint (numeric_cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6ac73038577ba8e9594feb9964a7c7d9e889aae5"},"cell_type":"code","source":"class Prepro:\n    \"\"\"\n    Bring all preprocessing here: scale, encoding\n    Should be fit on train data and called on train, validation or test data\n    \"\"\"\n    \n    def __init__(self, feature_cols, cat_cols, time_cols, numeric_cols, label_cols):\n        self.feature_cols = feature_cols\n        self.cat_cols = cat_cols\n        self.time_cols = time_cols\n        self.numeric_cols = numeric_cols\n        self.label_cols = label_cols\n        self.cats={}\n    \n    def transformXy(self, df):\n        \"\"\"\n        Preprocess and return X,y\n        \"\"\"\n        df = df.copy()\n        # Scale, encode etc. features\n        X = self.transform(df)\n        # Scale labels\n        df[self.label_cols] = self.y_scaler.transform(df[self.label_cols])\n        y = df[self.label_cols]\n        return(X,y)\n    \n    def transform(self, df):\n        \"\"\"\n        Preprocess and return X\n        \"\"\"\n        # Add day, week, year\n        df = self.prepare_time_cols(df)\n        # Fill nans\n        df[self.numeric_cols] = df[self.numeric_cols].fillna(0)\n        # Preprocess categorical features\n        for col in cat_cols:\n            df[col] = df[col].apply(lambda cat_name: self.prepare_cat_cols(cat_name, col))\n        # Scale numeric features and labels\n        df[self.numeric_cols+self.time_cols] = self.numeric_scaler.transform(df[self.numeric_cols+self.time_cols])\n        # Return X\n        return df[self.feature_cols]\n    \n    def fit(self, df):\n        \"\"\"\n        Fit preprocessing scalers, encoders on given train df\n        To be called on train df only\n        \"\"\"\n        # Extract day, week, year from time\n        df = df.copy()\n        df = self.prepare_time_cols(df)\n        # Handle strange cases, impute etc.\n        df = self.prepare_train_df(df)\n        # Use QuantileTransformer to handle outliers\n        # Fit for labels\n        self.y_scaler = QuantileTransformer()\n        self.y_scaler.fit(df[self.label_cols])\n        # Fit for numeric and time\n        self.numeric_scaler = QuantileTransformer()\n        self.numeric_scaler.fit(df[self.numeric_cols + self.time_cols])\n        # Fit for categories\n        # Organize dictionary, each category column has list with values\n        self.cats=dict()\n        for col in cat_cols:\n            self.cats[col] = list(df[col].unique())\n\n    def prepare_train_df(self, train_df):\n        \"\"\"\n        Clean na, remove strange cases.\n        For train dataset only. \n        \"\"\"\n        # Handle nans\n        train_df = train_df.copy()\n        # Need better imputer\n        # for col in numeric_cols:\n        #     market_train_df[col] = market_train_df[col].fillna(market_train_df[col].mean())\n        train_df.tail()\n        train_df[self.numeric_cols] = train_df[self.numeric_cols].fillna(0)\n\n        # # Remove strange cases with close/open ratio > 2\n        max_ratio  = 2\n        train_df = train_df[np.abs(train_df['close'] / train_df['open']) <= max_ratio]\n        return(train_df)\n\n    def prepare_time_cols(self, df):\n        \"\"\" \n        Extract time parts, they are important for time series \n        \"\"\"\n        df = df.copy()\n        df['year'] = df['time'].dt.year\n        # Maybe remove month because week of year can handle the same info\n        df['day'] = df['time'].dt.day\n        # Week of year\n        df['week'] = df['time'].dt.week\n        df['dayofweek'] = df['time'].dt.dayofweek\n        return(df)\n        \n    def prepare_cat_cols(self, cat_name, col):\n        \"\"\"\n        Encode categorical features to numbers\n        \"\"\"\n        try:\n            # Transform to index of name in stored names list\n            index_value = self.cats[col].index(cat_name)\n        except ValueError:\n            # If new value, add it to the list and return new index\n            self.cats[col].append(cat_name)\n            index_value = len(self.cats[col])\n        index_value = 1.0/(index_value+1.0)\n        return(index_value)\n# Preprocess and split to X_train, X_val, X_test, y_train ...\nprepro = Prepro(feature_cols, cat_cols, time_cols, numeric_cols, label_cols)\nprepro.fit(market_train_df)\n\n# Clean train df,handle strange cases\nmarket_train_df = prepro.prepare_train_df(market_train_df)\n\nX_train, y_train = prepro.transformXy(market_train_df)\nX_val, y_val = prepro.transformXy(market_val_df)\nX_test, y_test = prepro.transformXy(market_test_df)\n\n# Display for visual check. \npd.concat([X_train,y_train], axis=1).describe()\n#X_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1203f67234099fe0d3224467905b7ec1f63184d2"},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import LSTM, Dense, Dropout, Masking, Embedding\n\nmodel = Sequential()\n\ntraining_length = X_train.shape[1]\nnum_words = 1\nembedding_matrix = np.zeros((10, X_train.shape[1]))\n\n# Embedding layer\nmodel.add(\n    Embedding(input_dim=10,\n              input_length = training_length,\n              output_dim=56,\n              weights=[embedding_matrix],\n              trainable=False,\n              mask_zero=True))\n\n# Masking layer for pre-trained embeddings\nmodel.add(Masking(mask_value=0.0))\n\n# Recurrent layer\nmodel.add(LSTM(256, return_sequences=False, \n               dropout=0.1, recurrent_dropout=0.1))\n\n# Fully connected layer\nmodel.add(Dense(64, activation='relu'))\n\n# Dropout for regularization\nmodel.add(Dropout(0.5))\n\n# Output layer\nmodel.add(Dense(1, activation='softmax'))\n\n# Compile the model\nmodel.compile(\n    optimizer='adam', loss='binary_crossentropy', metrics=['mse'])\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3ea00fb2c060f75c65764ff9ac1b3f3e52f36539"},"cell_type":"code","source":"embedding_matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d32d2de9b379cf3044637cf7e2945c74280340b5"},"cell_type":"code","source":"weights_file='best_weights.h5'\n\n# We'll stop training if no improvement after some epochs\nearlystopper = EarlyStopping(patience=5, verbose=1)\n \n# Low, avg and high scor training will be saved here\n# Save the best model during the traning\ncheckpointer = ModelCheckpoint(weights_file\n    ,verbose=1\n    ,save_best_only=True\n    ,save_weights_only=True)\n\nreduce_lr = ReduceLROnPlateau(factor=0.2,\n                              patience=5, min_lr=0.001)\n\n# Train\ntraining = model.fit(X_train,y_train\n                                ,batch_size=512\n                                ,epochs=5\n                                ,validation_data=[X_val, y_val]\n                                #,steps_per_epoch=100\n                                 #, validation_steps=100\n                                ,callbacks=[earlystopper, checkpointer, reduce_lr])\n# Load best weights saved\nmodel.load_weights(weights_file)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"02fe00fc14c18fa01cf5fb2d0b1681e76bf1618e"},"cell_type":"code","source":"# f, axs = plt.subplots(1,2, figsize=(12,4))\n# axs[0].plot(training.history['loss'])\n# axs[0].set_xlabel(\"Epoch\")\n# axs[0].set_ylabel(\"Loss\")\n# axs[0].set_title(\"Loss\")\n# axs[1].plot(training.history['val_loss'])\n# axs[1].set_xlabel(\"Epoch\")\n# axs[1].set_ylabel(\"val_loss\")\n# axs[1].set_title(\"Validation loss\")\n# plt.tight_layout()\n# plt.show()\nplt.plot(training.history['loss'])\nplt.plot(training.history['val_loss'])\nplt.title(\"Loss and validation loss\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Value\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"15cc251ddf42ad01fb19f44a3d3e35811b7a9085"},"cell_type":"code","source":"pred_size=100\nX_test2 = X_test.values[:pred_size]\ny_pred2 = model.predict(X_test2) [:,0]*2-1\ny_test2 = y_test.values[:pred_size][:,0]*2-1\n\nax1 = plt.subplot2grid((2, 2), (0, 0), rowspan=2)\nax1.plot(y_test2, linestyle='none', marker='.', color='darkblue')\nax1.plot(y_pred2, linestyle='none', marker='.', color='darkred')\nax1.legend([\"Ground truth\",\"Predicted\"])\nax1.set_title(\"Both\")\nax1.set_xlabel(\"Epoch\")\nax2 = plt.subplot2grid((2, 2), (0, 1), colspan=1,rowspan=1)\nax2.plot(y_test2, linestyle='none', marker='.', color='darkblue')\nax2.set_title(\"Ground truth\")\nax3 = plt.subplot2grid((2, 2), (1, 1), colspan=1,rowspan=1)\nax3.plot(y_pred2, linestyle='none', marker='.', color='darkred')\nax3.set_title(\"Predicted\")\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d236372bb0bebd0538924047bf18328bc6c00154"},"cell_type":"code","source":"def predict_random_asset():\n    \"\"\"\n    Get random asset from test set, predict on it, plot ground truth and predicted value\n    \"\"\"\n    # Get any asset\n    ass = market_test_df.assetName.sample(1, random_state=24).iloc[0]\n    test_ass_df = market_test_df[market_test_df['assetName'] == ass]\n    # Preprocess\n    X,y = prepro.transformXy(test_ass_df)\n    y.index = test_ass_df.time\n    # Predict\n    pred = pd.DataFrame(model.predict(X)*2 -1)\n    pred.index = test_ass_df.time\n    # Plot\n    plt.plot(y*2-1, linestyle='none', marker='.', color='darkblue')\n    plt.plot(pred, linestyle='none', marker='.', color='orange')\n    plt.title(ass)\n    plt.legend([\"Ground truth\", \"predicted\"])\n    plt.show()\n    \npredict_random_asset()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"47591f31770338e8a3cb512b3ee32685a1a39ad9"},"cell_type":"code","source":"# Accuracy metric\nconfidence = model.predict(X_test)*2 -1\nprint(\"Accuracy: %f\" % accuracy_score(y_test > 0, confidence > 0))\n\n# Show distribution of confidence that will be used as submission\nplt.hist(confidence, bins='auto')\nplt.xlabel(\"Confidence\")\nplt.ylabel(\"Count\")\nplt.title(\"predicted confidence\")\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3731fd47ff4015bea9ea621edb5d14813c3c8445"},"cell_type":"code","source":"def make_predictions(market_obs_df, news_obs_df, predictions_template_df):\n    \"\"\"\n    Predict confidence for one day and update predictions_template_df['confidenceValue']\n    @param market_obs_df: market_obs_df returned from env\n    @param predictions_template_df: predictions_template_df returned from env.\n    @return: None. prediction_template_df updated instead. \n    \"\"\"\n    # Preprocess the data\n    market_obs_df = join_market_news(market_obs_df, news_obs_df)\n    X = prepro.transform(market_obs_df)\n    # Predict\n    y_pred = model.predict(X)\n    confidence_df=pd.DataFrame(y_pred*2-1, columns=['confidence'])\n\n    # Merge predicted confidence to predictions template\n    pred_df = pd.concat([predictions_template_df, confidence_df], axis=1).fillna(0)\n    predictions_template_df.confidenceValue = pred_df.confidence\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2460e02db7b336d198299b5a7f8c0b6ed40499d0"},"cell_type":"code","source":"##########################\n# Submission code\n\n# Save data here for later debugging on it\ndays_saved_data = []\n\n# Store execution info for plotting later\npredicted_days=[]\npredicted_times=[]\nlast_predictions_template_df = None\n\n# Predict day by day\ndays = env.get_prediction_days()\n\nfor (market_obs_df, news_obs_df, predictions_template_df) in days:\n    # Store the data for later debugging on it\n    days_saved_data.append((market_obs_df, news_obs_df, predictions_template_df))\n    # For later plotting\n    predicted_days.append(market_obs_df.iloc[0].time.strftime('%Y-%m-%d'))\n    time_start = time()\n\n    # Call prediction func\n    make_predictions(market_obs_df, news_obs_df, predictions_template_df)\n    #!!!\n    env.predict(predictions_template_df)\n    \n    # For later plotting\n    last_predictions_template_df = predictions_template_df\n    predicted_times.append(time()-time_start)\n    #print(\"Prediction completed for \", predicted_days[-1])","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}