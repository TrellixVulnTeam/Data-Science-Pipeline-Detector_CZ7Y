{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom kaggle.competitions import twosigmanews\n\nenv = twosigmanews.make_env()\n(marketdf, newsdf) = env.get_training_data()\n\nprint('preparing data...')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ndef prepare_data(marketdf, newsdf):\n    # a bit of feature engineering\n    marketdf['time'] = marketdf.time.dt.strftime(\"%Y%m%d\").astype(int)\n    marketdf['bartrend'] = marketdf['close'] / marketdf['open']\n    marketdf['average'] = (marketdf['close'] + marketdf['open'])/2\n    marketdf['pricevolume'] = marketdf['volume'] * marketdf['close']\n    \n    newsdf['time'] = newsdf.time.dt.strftime(\"%Y%m%d\").astype(int)\n    newsdf['assetCode'] = newsdf['assetCodes'].map(lambda x: list(eval(x))[0])\n    newsdf['position'] = newsdf['firstMentionSentence'] / newsdf['sentenceCount']\n    newsdf['coverage'] = newsdf['sentimentWordCount'] / newsdf['wordCount']\n\n    # filter pre-2012 data, no particular reason\n    marketdf = marketdf.loc[marketdf['time'] > 20120000]\n    \n    # get rid of extra junk from news data\n    droplist = ['sourceTimestamp','firstCreated','sourceId','headline','takeSequence','provider','firstMentionSentence',\n                'sentenceCount','bodySize','headlineTag','marketCommentary','subjects','audiences','sentimentClass',\n                'assetName', 'assetCodes','urgency','wordCount','sentimentWordCount']\n    newsdf.drop(droplist, axis=1, inplace=True)\n    marketdf.drop(['assetName', 'volume'], axis=1, inplace=True)\n    \n    # combine multiple news reports for same assets on same day\n    newsgp = newsdf.groupby(['time','assetCode'], sort=False).aggregate(np.mean).reset_index()\n    \n    # join news reports to market data, note many assets will have many days without news data\n    return pd.merge(marketdf, newsgp, how='left', on=['time', 'assetCode'], copy=False) #, right_on=['time', 'assetCodes'])\n\n\ncdf = prepare_data(marketdf, newsdf)    \ndel marketdf, newsdf  # save the precious memory","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"830b985822b8f081d6a7c6e3766f5cf66453f6ac"},"cell_type":"code","source":"print('building training set...')\ntargetcols = ['returnsOpenNextMktres10']\ntraincols = [col for col in cdf.columns if col not in ['time', 'assetCode', 'universe'] + targetcols]\n\ndates = cdf['time'].unique()\ntrain = range(len(dates))[:int(0.85*len(dates))]\nval = range(len(dates))[int(0.85*len(dates)):]\n\n# we be classifyin\ncdf[targetcols[0]] = (cdf[targetcols[0]] > 0).astype(int)\n\n# train data\nXt = cdf[traincols].fillna(0).loc[cdf['time'].isin(dates[train])].values\nYt = cdf[targetcols].fillna(0).loc[cdf['time'].isin(dates[train])].values\n\n# validation data\nXv = cdf[traincols].fillna(0).loc[cdf['time'].isin(dates[val])].values\nYv = cdf[targetcols].fillna(0).loc[cdf['time'].isin(dates[val])].values\n\nprint(Xt.shape, Xv.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f5139a925b855d70a2a245163797cd80c63f9733"},"cell_type":"code","source":"import lightgbm as lgb\nprint ('Training lightgbm')\n\n# money\nparams = {\"objective\" : \"binary\",\n          \"metric\" : \"binary_logloss\",\n          \"num_leaves\" : 125, # originally 60\n          \"max_depth\": -1,\n          \"learning_rate\" : 0.0005,   # originally .01\n          \"bagging_fraction\" : 0.9,  # subsample\n          \"feature_fraction\" : 0.9,  # colsample_bytree\n          \"bagging_freq\" : 5,        # subsample_freq\n          \"bagging_seed\" : 2018,\n          \"verbosity\" : -1 }\n\n# We can introduce other boosting algos, default is traditional gradient boosting decision tree\n# Other options include random forest (rf), dropouts meet multiple additive regression trees (dart), \n# or gradient-based on one-side sampling \nlgtrain, lgval = lgb.Dataset(Xt, Yt[:,0]), lgb.Dataset(Xv, Yv[:,0])\nlgbmodel = lgb.train(params, lgtrain, 2000, valid_sets=[lgtrain, lgval], early_stopping_rounds=100, verbose_eval=200)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2a4fa57487652d868b7f13165fac3e87b5c07884"},"cell_type":"code","source":"def post_scaling(df):\n    mean, std = np.mean(df), np.std(df)\n    df = (df - mean)/ (std * 8)\n    return np.clip(df,-1,1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"8748a361edd8b802891850c0bd2addf0f14d6483"},"cell_type":"code","source":"print(\"generating predictions...\")\npreddays = env.get_prediction_days()\nfor marketdf, newsdf, predtemplatedf in preddays:\n    cdf = prepare_data(marketdf, newsdf)\n    Xp = cdf[traincols].fillna(0).values\n    preds = lgbmodel.predict(Xp, num_iteration=lgbmodel.best_iteration) * 2 - 1\n    predsdf = pd.DataFrame({'ast':cdf['assetCode'],'conf':post_scaling(preds)})\n    predtemplatedf['confidenceValue'][predtemplatedf['assetCode'].isin(predsdf.ast)] = predsdf['conf'].values\n    env.predict(predtemplatedf)\n\nenv.write_submission_file()\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}