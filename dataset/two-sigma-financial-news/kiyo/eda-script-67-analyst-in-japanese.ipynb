{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"#モジュールインストール\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\nfrom kaggle.competitions import twosigmanews\nimport matplotlib.pyplot as plt\nimport random\nfrom datetime import datetime, date\nfrom xgboost import XGBClassifier\nfrom sklearn import model_selection\nfrom sklearn.metrics import mean_squared_error\nimport time","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"#データ取得コマンド\nfrom kaggle.competitions import twosigmanews\nenv = twosigmanews.make_env()\nprint('Done!')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4ff149df14250f7e68cbde3e3c9f44fdd6956a0f"},"cell_type":"code","source":"#株価とニュースのデータをそれぞれ振り分けて格納\n(market_train_df, news_train_df) = env.get_training_data()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a6bd9fc9a59db29e02c8d0ad5f85cf7d0e7ab01b"},"cell_type":"code","source":"market_train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"404f9729417c7d54ea31fc693e8a9d6907be1453"},"cell_type":"code","source":"#timeを日付のみに整形\nmarket_train_df['time'] = market_train_df['time'].dt.date\n#2010年～のデータだけ保存\nmarket_train_df = market_train_df.loc[market_train_df['time']>=date(2010, 1, 1)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"302f2f9696d7d7c6c4e0e4efe136fa4c7500c532"},"cell_type":"code","source":"market_train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"06db696df59db8381b4518124a95ddf6c3a18941"},"cell_type":"code","source":"#平行作業のモジュールのインポート\nfrom multiprocessing import Pool\n\n#create_lag(移動平均の平均最大最小標準偏差作成メソッド)\n#メソッド定義カッコの中にデフォルト値いれてる\ndef create_lag(df_code,n_lag=[3,7,14,],shift_size=1):\n\n    #df_codeの資産コードを重複なしでcodeに入れる\n    code = df_code['assetCode'].unique()\n    \n    #return_features(収益機能)からcol変数に\n    for col in return_features:\n        #n_lagをwindow変数にループ処理\n        for window in n_lag:\n            #window変数の値の分、現在から以前の移動平均の値を出して値をずらしている\n            rolled = df_code[col].shift(shift_size).rolling(window=window)\n            #移動平均の平均\n            lag_mean = rolled.mean()\n            #最大値\n            lag_max = rolled.max()\n            #最小値\n            lag_min = rolled.min()\n            #標準偏差\n            lag_std = rolled.std()\n            #colの値_lag_windowの値_meanのカラム名でdf_codeに格納される\n            df_code['%s_lag_%s_mean'%(col,window)] = lag_mean\n            df_code['%s_lag_%s_max'%(col,window)] = lag_max\n            df_code['%s_lag_%s_min'%(col,window)] = lag_min\n            #標準偏差はやらない？\n#             df_code['%s_lag_%s_std'%(col,window)] = lag_std\n    #df_codeに空値があったら-1にして返す\n    return df_code.fillna(-1)\n\n\n#generate_lag_features(時系列での特徴抽出)\ndef generate_lag_features(df,n_lag = [3,7,14]):\n    #features(特徴変数)に株価データ全部入れてる（項目全書き出ししている理由は不明）\n    features = ['time', 'assetCode', 'assetName', 'volume', 'close', 'open',\n       'returnsClosePrevRaw1', 'returnsOpenPrevRaw1',\n       'returnsClosePrevMktres1', 'returnsOpenPrevMktres1',\n       'returnsClosePrevRaw10', 'returnsOpenPrevRaw10',\n       'returnsClosePrevMktres10', 'returnsOpenPrevMktres10',\n       'returnsOpenNextMktres10', 'universe']\n    #assetCode(銘柄コード)の重複削除\n    assetCodes = df['assetCode'].unique()\n    #表示\n    print(assetCodes)\n    #all_dfとりあえず作ってる？\n    all_df = []\n    #df_codesにassetCodeで集計(SQLグループバイ)で格納\n    df_codes = df.groupby('assetCode')\n    #df_codes(全体データ)からdf_code(一日データ)に時間と銘柄コードと将来予測を先頭に放り込んでいるが\n    #不明\n    df_codes = [df_code[1][['time','assetCode']+return_features] for df_code in df_codes]\n    #df_codesの長さを表示\n    print('total %s df'%len(df_codes))\n    \n    #cpu４個で並列処理開始\n    pool = Pool(4)\n    #create_lagメソッドにdf_codesの引数を代入して出た結果をall_dfに格納している\n    all_df = pool.map(create_lag, df_codes)\n    \n    #new_dfにall_dfをテーブル結合\n    new_df = pd.concat(all_df)\n    #return_featuresが空値の行を削除\n    new_df.drop(return_features,axis=1,inplace=True)\n    #並列処理終了\n    pool.close()\n    #new_dfを返す\n    return new_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5b5c80a63310bb9f370a10cda02e0b4d01d18b36"},"cell_type":"code","source":"#ここはコメントのみだったので設定変更用の控えでは？\n\n# return_features(収益機能)に終値を入れる\n# return_features = ['close']\n#　new_dfは時系列での特徴抽出のメソッドに株価訓練データを放り込む　移動平均の設定は当日から前５日に設定\n# new_df = generate_lag_features(market_train_df,n_lag = 5)\n# 株価訓練データにnew_dfをtime assetCodeで外部結合\n# market_train_df = pd.merge(market_train_df,new_df,how='left',on=['time','assetCode'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"34630df14f3e514becabd95623fa2456c48f7759"},"cell_type":"code","source":"#return_features(収益機能)の設定（結局終値だけではない）\nreturn_features = ['returnsClosePrevMktres10','returnsClosePrevRaw10','open','close']\n#移動平均の日数設定\nn_lag = [3,7,14]\n#株価訓練データに時系列の特徴抽出メソッドを3,7,14日の移動平均を放り込む\nnew_df = generate_lag_features(market_train_df,n_lag=n_lag)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b3a49a3f3a02de7bbe7c65197011f237129bf1d9","scrolled":false},"cell_type":"code","source":"new_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b0796f6ca4020110312b828d21cef80bf79269a4"},"cell_type":"code","source":"#株価訓練データに移動平均のデータをtime,assetCodeカラムで外部結合\nmarket_train_df = pd.merge(market_train_df,new_df,how='left',on=['time','assetCode'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"92f6558993f3a3b932a7bfaa94ee2e7a575d366a"},"cell_type":"code","source":"#移動平均がちゃんと追加されているかチェック\nprint(market_train_df.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"302a5d2900f6c1b5a64d957a736247f068b6c426"},"cell_type":"code","source":"market_train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a65eaf741fc937ea1828368aa0d7ba56a82dd54b"},"cell_type":"code","source":"#mis_imputeメソッド(空値置換)\ndef mis_impute(data):\n    #カラムを一つずつ引っ張ってくる\n    for i in data.columns:\n        #型が文字列なら空値をotherに置き換え\n        if data[i].dtype == \"object\":\n            data[i] = data[i].fillna(\"other\")\n        #型が数字なら空値は平均値に置き換え\n        elif (data[i].dtype == \"int64\" or data[i].dtype == \"float64\"):\n            data[i] = data[i].fillna(data[i].mean())\n        else:\n            pass\n    return data\n#株価訓練データを整形したデータに置換\nmarket_train_df = mis_impute(market_train_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6ed6313b2510f55dd160cf56461fa9f76f5d8aed"},"cell_type":"code","source":"#data_prep(株価データを入れたら整形してくれるメソッド)\ndef data_prep(market_train):\n    #銘柄コードの重複削除しながら、行番号を加えつつ、カラム一つずつ引っ張る\n    lbl = {k: v for v, k in enumerate(market_train['assetCode'].unique())}\n    #銘柄コードを上記で処理(多分行番号をassetCodeTを追加してそこに入れてるだけ)\n    market_train['assetCodeT'] = market_train['assetCode'].map(lbl)\n    #空値の行を削除\n    market_train = market_train.dropna(axis=0)\n    return market_train\n#上記のメソッドを実行\nmarket_train_df = data_prep(market_train_df)\n# # shapeはデータが縦何列あって横何行あるかを表記\nprint(market_train_df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7b125f168143a2c1c963dc49b9315d1ecf129981"},"cell_type":"code","source":"#assetCodeTがバラバラになっているのが分かる\nmarket_train_df.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"28a319be7fd82500778ef562f5c851ea96c2e4cd"},"cell_type":"code","source":"#データを数字に変更するモジュールをインポート\nfrom sklearn.preprocessing import LabelEncoder\n#10日後に株価が停滞したか上昇したものだけup変数に放り込む\nup = market_train_df['returnsOpenNextMktres10'] >= 0\n\n#universe(これが１以外は訓練データとして使えない)の値をuniverse変数に放り込む\n#でもなんで値？\nuniverse = market_train_df['universe'].values\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"013965849984e1637f397e62b166c69142e1bbc6"},"cell_type":"code","source":"universe","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2c0d9a765484efce4f9e8c68659ef4d9f9650eb9"},"cell_type":"code","source":"#d変数に時間放り込む\nd = market_train_df['time']\n#最終的に訓練データとして使用するカラムを選択\n#カラムを一つずつ引っ張り出してきてはいるが\n#下記のカラムは除外する様にしている\nfcol = [c for c in market_train_df if c not in ['assetCode', 'assetCodes', 'assetCodesLen', 'assetName', 'audiences', \n                                             'firstCreated', 'headline', 'headlineTag', 'marketCommentary', 'time_x','provider', \n                                             'returnsOpenNextMktres10', 'sourceId', 'subjects', 'time', 'universe','sourceTimestamp']]\nfcol","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7f4972b56f864940ecb0551dc855a8bac3913302"},"cell_type":"code","source":"#上記で最終的に選択したカラムの値を放り込む\nX = market_train_df[fcol].values\nX","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9e371d4c812e5b3ef3c0d9df76723bec12e598ad"},"cell_type":"code","source":"#returnsOpenNextMktres10のTrueFalseの値を放り込む\nup = up.values\nup","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"248311406fdfeb12c1921939cd3a2ea9ce02d8d4"},"cell_type":"code","source":"#rはTrueFalseじゃなくて実際の値を放り込んでいる\nr = market_train_df.returnsOpenNextMktres10.values\nr","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6ad7076989b79b3ae6caaa6791b61fad173be09c"},"cell_type":"code","source":"# X値の範囲　(この範囲を後々保持しておくと良いと書いてます)\n#それぞれのカラム毎の最小値\nmins = np.min(X, axis=0)\nmins","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6dc7fdac5e1dcb93971084748bbf9030ba3156c3"},"cell_type":"code","source":"#それぞれのカラムの最大値\nmaxs = np.max(X, axis=0)\nmaxs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cf2a6abc91d2655da95cdc71ec6a3def86238f14"},"cell_type":"code","source":"#最大値ー最小値で範囲算出\nrng = maxs - mins\nrng","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b19d29e62af5a9640026b204a27171fd82c6956a"},"cell_type":"code","source":"#最低値からどれ位の位置に存在しているかをXに挿入\n#平均値だったら0.5 上位10%だったら0.9\nX = 1 - ((maxs - X) / rng)\nX","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6f5aff4e150761217548d9486a0568bdc3e556f1"},"cell_type":"code","source":"# 値の整合性チェック 簡単に\nassert X.shape[0] == up.shape[0] == r.shape[0]\nprint(X.shape[0])\nprint(up.shape[0])\nprint(r.shape[0])\n#それぞれのリストの数が一致している事を確認","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f850a0d1eaa095bd7b4eb3ae8f4633f6f0c02349"},"cell_type":"code","source":"#ここからXgboost入れてる\nfrom xgboost import XGBClassifier\nfrom sklearn import model_selection\nfrom sklearn.metrics import mean_squared_error\nimport time","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"483a754de67e23475d4c4f1d4ffc87eaf4d4fe79"},"cell_type":"code","source":"# X_train, X_test, up_train, up_test, r_train, r_test,u_train,u_test,d_train,d_test = model_selection.train_test_split(X, up, r,universe,d, test_size=0.25, random_state=99)\n#訓練データとテストデータを分ける作業\n#2015年以後のデータはTrue、以前はFalseで処理\n#te = market_train_df['time']>date(2015, 1, 1)\n#te\n#今後のコメントアウトをこのversionでは使わずに\n#サイキットラーンのtrain_test_splitを使って訓練データ75%テストデータ25%でそれぞれ放り込んでいる\n# X = 生データ \n# up = returnsOpenNextMktres10の正負TrueFalse \n#　r = returnsOpenNextMktres10の値\n# u = universeの値　１のデータだけ使う為\n# d = 日付\nX_train, X_test, up_train, up_test, r_train, r_test,u_train,u_test,d_train,d_test = \\\nmodel_selection.train_test_split(X, up, r,universe,d, test_size=0.25, random_state=99)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3b5ffaaf52e8c84861c3ef9f94f30791f7c4cbf8"},"cell_type":"code","source":"#2015年以後のデータは2946738件数中2054539件だという事が分かるだけ\n#tt = 0\n#for tt,i in enumerate(te.values):\n#    if i:\n#        idx = tt\n#        print(i,tt)\n#        break\n#print(idx)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3f3cafc884990478034d46db46ce5aa8e24f1d89"},"cell_type":"code","source":"#訓練データとテストデータに分ける実際の作業(スライジング)\n#生データ\n#X_train, X_test = X[:idx],X[idx:]\n#returnsOpenNextMktres10の正負TrueFalse\n#up_train, up_test = up[:idx],up[idx:]\n#returnsOpenNextMktres10の値\n#r_train, r_test = r[:idx],r[idx:]\n#１のデータだけ使う為\n#u_train,u_test = universe[:idx],universe[idx:]\n#日付\n#d_train,d_test = d[:idx],d[idx:]\n\n#LightGBMの訓練データの設定\ntrain_data = lgb.Dataset(X, label=up.astype(int))\n#LightGBMのテストデータの設定\ntest_data = lgb.Dataset(X_test, label=up_test.astype(int))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"136f82932cda9d2508a6f15eefdc80f80d1f4039"},"cell_type":"code","source":"# LightBGMの機械学習の設定　param_1 param_2に使ってます\nx_1 = [0.19000424246380565, 2452, 212, 328, 202]\nx_2 = [0.19016805202090095, 2583, 213, 312, 220]\nprint(up_train)\n\n####################わかりませーん ってかメソッド定義しているのに使ってない\n\ndef exp_loss(p,y):\n    #yにグラフの凡例を入れてる？\n    y = y.get_label()\n#     p = p.get_label()\n    #expは対数関数だけど分かりません\n    grad = -y*(1.0-1.0/(1.0+np.exp(-y*p)))\n    hess = -(np.exp(y*p)*(y*p-1)-1)/((np.exp(y*p)+1)**2)\n    \n    return grad,hess\n###########################################\n#実際の機械学習のパラメーターチューニング\nparams_1 = {\n        #精度をあげる為の設定も一緒に書いておきます\n        #テンプレ\n        'task': 'train',\n        #テンプレ\n        'boosting_type': 'gbdt',\n        #regression(分類)だと思われるがなぜかbinary\n        'objective': 'binary',\n#         'objective': 'regression',\n        #学習度合い(値が小さいほうが良い)\n        'learning_rate': x_1[0],\n        #葉っぱの数(値が大きいほうが良いが、大きすぎると詰まる事がある)\n        'num_leaves': x_1[1],\n        #葉っぱの最低数\n        'min_data_in_leaf': x_1[2],\n#         'num_iteration': x_1[3],\n        #反復回数(値が大きいほうが良い)\n        'num_iteration': 239,\n        #この値を大きくすれば精度が上がるが、処理が重くなる。意味は不明\n        'max_bin': x_1[4],\n        #冗長性の警告の設定(0はエラー出力で1はログ出力、２はデバッグ)\n        'verbose': 1\n    }\n#２個目も同様の手法で別の値に設定\nparams_2 = {\n        'task': 'train',\n        'boosting_type': 'gbdt',\n        'objective': 'binary',\n#         'objective': 'regression',\n        'learning_rate': x_2[0],\n        'num_leaves': x_2[1],\n        'min_data_in_leaf': x_2[2],\n#         'num_iteration': x_2[3],\n        'num_iteration': 172,\n        'max_bin': x_2[4],\n        'verbose': 1\n    }\n\ngbm_1 = lgb.train(params_1,\n        train_data,\n#大したこと書いてあるわけじゃないのですが、num_boost_roundは少なくとも200でやって、テストデータのerror rateの一番少ない回数選びなさいよとのことです。\n#このnum_boost_roundについては、納得出来てない部分もあるので、後ほど少し考察してみます。\n#とりあえずで、やってみました。133回が良さそうです。\n        #多分学習回数の設定（何週するかとか）\n        num_boost_round=133,\n        #答え合わせ用データの場所？\n        valid_sets=test_data,\n        #最低学習回数\n        early_stopping_rounds=5,\n#         fobj=exp_loss,\n        )\n#下記も同様に\ngbm_2 = lgb.train(params_2,\n        train_data,\n        num_boost_round=133,\n        valid_sets=test_data,\n        early_stopping_rounds=5,\n#         fobj=exp_loss,\n        )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6c66369ca2a32b9a86a6aaf98adbaf288bb91b49"},"cell_type":"code","source":"#1個目のデータと2個目のデータの平均出してる\nconfidence_test = (gbm_1.predict(X_test) + gbm_2.predict(X_test))/2\nconfidence_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"55589ca05e8c982439efe1ad9cf19fbf6ceefd4d"},"cell_type":"code","source":"#予測値の%　割合の算出\nconfidence_test = (confidence_test-confidence_test.min())/(confidence_test.max()-confidence_test.min())\nconfidence_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e5e6db1e2e7963aef26c4ffdc1832f4f0f6f9c0b"},"cell_type":"code","source":"#%に二乗して-1????\nconfidence_test = confidence_test*2-1\nprint(max(confidence_test),min(confidence_test))\nconfidence_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3dbef3e10363bc9df63292b146cba44311ba3b17"},"cell_type":"code","source":"# 最終スコアの計算に使用される実際のメトリックの計算\nr_test = r_test.clip(-1,1) # -1～１以外の値を取り除く　彼らはどこから来たのかという\n#学習の推測地と予測前の目的変数とユニバース値をかけてる？\nx_t_i = confidence_test * r_test * u_test\n#日付とスコア値だけのデータフレーム作り\ndata = {'day' : d_test, 'x_t_i' : x_t_i}\ndf = pd.DataFrame(data)\n#日付でグループバイ（集約）　で多次元配列を１次元に直してる\nx_t = df.groupby('day').sum().values.flatten()\n#スコアの平均値\nmean = np.mean(x_t)\n#スコアの標準偏差\nstd = np.std(x_t)\n#変動係数の逆数\nscore_test = mean / std\nprint(score_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"65da0a50ea0df84261ff437dbf934a31242b2ccd"},"cell_type":"code","source":"import gc\ndel X_train,X_test\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3351f7ef017a67f4ea8563be79230f5bd659d965"},"cell_type":"code","source":"#prediction\ndays = env.get_prediction_days()\nn_days = 0\nprep_time = 0\nprediction_time = 0\npackaging_time = 0\ntotal_market_obs_df = []\nfor (market_obs_df, news_obs_df, predictions_template_df) in days:\n    n_days +=1\n    if (n_days%50==0):\n        print(n_days,end=' ')\n    t = time.time()\n    market_obs_df['time'] = market_obs_df['time'].dt.date\n    \n    return_features = ['returnsClosePrevMktres10','returnsClosePrevRaw10','open','close']\n    total_market_obs_df.append(market_obs_df)\n    if len(total_market_obs_df)==1:\n        history_df = total_market_obs_df[0]\n    else:\n        history_df = pd.concat(total_market_obs_df[-(np.max(n_lag)+1):])\n    print(history_df)\n    \n    new_df = generate_lag_features(history_df,n_lag=[3,7,14])\n    market_obs_df = pd.merge(market_obs_df,new_df,how='left',on=['time','assetCode'])\n    \n#     return_features = ['open']\n#     new_df = generate_lag_features(market_obs_df,n_lag=[3,7,14])\n#     market_obs_df = pd.merge(market_obs_df,new_df,how='left',on=['time','assetCode'])\n    \n    market_obs_df = mis_impute(market_obs_df)\n    \n    market_obs_df = data_prep(market_obs_df)\n    \n#     market_obs_df = market_obs_df[market_obs_df.assetCode.isin(predictions_template_df.assetCode)]\n    \n    X_live = market_obs_df[fcol].values\n    X_live = 1 - ((maxs - X_live) / rng)\n    prep_time += time.time() - t\n    \n    t = time.time()\n    lp = (gbm_1.predict(X_live) + gbm_2.predict(X_live))/2\n    prediction_time += time.time() -t\n    \n    t = time.time()\n    \n    confidence = lp\n    confidence = (confidence-confidence.min())/(confidence.max()-confidence.min())\n    confidence = confidence * 2 - 1\n    \n    preds = pd.DataFrame({'assetCode':market_obs_df['assetCode'],'confidence':confidence})\n    predictions_template_df = predictions_template_df.merge(preds,how='left').drop('confidenceValue',axis=1).fillna(0).rename(columns={'confidence':'confidenceValue'})\n    env.predict(predictions_template_df)\n    packaging_time += time.time() - t\n    \nenv.write_submission_file()\nsub  = pd.read_csv(\"submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f189e556fd74e791c965476b07354fadc0ec1d7c"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}