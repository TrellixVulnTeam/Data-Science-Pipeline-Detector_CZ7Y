{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.\n\n# This competition settings\nfrom kaggle.competitions import twosigmanews\nenv = twosigmanews.make_env()\n# Read the data\n# Read market and news data\n(market_train_df, news_train_df) = env.get_training_data()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b1aac15bfad356852866d19a105ba898a683b997"},"cell_type":"code","source":"# some settings\ntoy = False\ndebug = False\ntesting = True","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"17fe7cb21bff094ae5291059a801c4eadc4bf1d2"},"cell_type":"code","source":"# remove too old data\nimport datetime\nstart = pd.Timestamp(year=2011, month=5, day=5, tz='UTC')\nmarket_train_df = market_train_df.loc[market_train_df['time'] >= start].reset_index(drop=True)\nnews_train_df = news_train_df.loc[news_train_df['time'] >= start].reset_index(drop=True)\n\n# We will reduce the number of samples for memory reasons\nif toy:\n    market_train_df = market_train_df.tail(100_000)\n    news_train_df = news_train_df.tail(300_000)\nelse:\n    market_train_df = market_train_df.tail(3_000_000)\n    news_train_df = news_train_df.tail(6_000_000)\n\nprint('market train size = {}, news train size = {}'.format(market_train_df.shape[0], news_train_df.shape[0]))\n\n# map stock code to name\nasset_name_df = market_train_df[['assetCode', 'assetName']].drop_duplicates(subset='assetCode').reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"15f92218d39b5eeaa742222e8c5679c58e81c6c3"},"cell_type":"code","source":"# some utilities\n\ndef to_float32(df):\n    df.loc[:, df.dtypes == np.float64] = df.loc[:, df.dtypes == np.float64].astype(np.float32)\n    \ndef debug_msg(message):\n    if debug:\n        print(message)\n        \nimport time\n\n# simple timing util, %time creates a new scope makes it hard to use\ndef print_time(start_time, task_description):\n    if debug:\n        elapsed = time.time() - start_time\n        print('{} time taken: {:.3f} s'.format(task_description, elapsed))\n        \n# using series append is also faster than using concat, but with the benefit that we keep\n# correct type information\ndef fast_concat(df_list):\n    df = pd.DataFrame()\n    # assign each column\n    for col in df_list[0].columns:\n        if pd.api.types.is_categorical_dtype(df_list[0][col]):\n            #print('categorical column: {}, using numpy concat'.format(col))\n            # categorical type concat is extremely slow\n            df[col] = np.concatenate([dfi[col].values for dfi in df_list])\n        else:\n            #print('column: {}, using series append'.format(col))\n            df[col] = df_list[0][col].append([dfi[col] for dfi in df_list[1:]], ignore_index=True)\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"595647c3654aafd061df395961a814a26a417dcb"},"cell_type":"code","source":"market_train_df.loc[market_train_df['assetCode'] == 'AAPL.O'].tail(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4c8299aa1e001e1c16a4e6764c7570cb2ffb0e32"},"cell_type":"code","source":"market_train_df.loc[:, market_train_df.dtypes == np.float64].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c7f6025d8357014ff424a3da04754aad0ce7b9f9"},"cell_type":"code","source":"df = market_train_df.head(100).copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7c3a09cf51f6520022bb1e5c4af99092b1717fd9"},"cell_type":"code","source":"news_train_df.tail(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f5bdedc66e64860f6daa187c0e8c04742ae2079f"},"cell_type":"code","source":"def addSplitAdjColumns(market_df):\n    # try to automatically detect splits\n    split_detect_df = market_df[['time', 'assetCode', 'close', 'returnsClosePrevRaw1']].copy()\n    asset_groupby = split_detect_df.groupby('assetCode')\n    split_detect_df['prevClose'] = asset_groupby['close'].transform(lambda x: x.shift(1))\n    split_detect_df['prevClosePlusReturn'] = split_detect_df['prevClose'] * (1.0 + split_detect_df['returnsClosePrevRaw1'])\n    split_detect_df['splitRatio'] = split_detect_df['prevClosePlusReturn'] / split_detect_df['close'] \n    split_detect_df.loc[split_detect_df['assetCode'] == 'AAPL.O'].tail(5)\n\n    # now find points where ratio is > 1.5 or < 0.6, smaller splits guess we cannot correct for\n    split_points = split_detect_df[(split_detect_df['splitRatio'] > 1.49) | (split_detect_df['splitRatio'] < 0.67)]\n    split_points = split_points[['time', 'assetCode', 'splitRatio']]\n\n    # create a date column\n    split_correct_df = market_df[['time', 'assetCode', 'open', 'close']].copy()\n\n    # merge in the split df\n    split_correct_df = split_correct_df.merge(split_points, left_on=['assetCode','time'], right_on=['assetCode','time'], how='left')\n    split_correct_df.loc[:,'splitRatio'].fillna(1.0, inplace=True)\n\n    # add a adjusted close column for split\n    asset_groupby = split_correct_df.groupby('assetCode')\n    split_correct_df['splitAdj'] = asset_groupby['splitRatio'].transform(lambda x : x.cumprod())\n    split_correct_df['splitAdj'] = asset_groupby['splitAdj'].transform(lambda x : x / x.iloc[-1])\n    split_correct_df['adjClose'] = split_correct_df['close'] * split_correct_df['splitAdj']\n    split_correct_df['adjOpen'] = split_correct_df['open'] * split_correct_df['splitAdj']\n\n    # put back into market train df\n    market_df['adjClose'] = split_correct_df['adjClose']\n    market_df['adjOpen'] = split_correct_df['adjOpen']\n\n    del split_correct_df\n    del asset_groupby","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f8995d6a04018fdb3753cff934dda0c91de475dd"},"cell_type":"code","source":"addSplitAdjColumns(market_train_df)\n\n# plot to make sure it is correct\nmarket_train_df[(market_train_df.assetCode == 'AAPL.O') & (market_train_df.time.dt.year == 2014) & (market_train_df.time.dt.month == 6)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7a06c3eee0104394afac51835bf37a6a9ad3acca"},"cell_type":"code","source":"def checkPropertyFreq(news_df, property_col):\n    property_df = news_df[[property_col]].copy()\n    # convert to string\n    property_df[property_col] = property_df[property_col].str.findall(f\"'([\\w\\./]+)'\")\n    property_df = pd.DataFrame({\n          col:np.repeat(property_df[col].values, property_df[property_col].str.len())\n          for col in property_df.columns.drop(property_col)}\n        ).assign(**{property_col:np.concatenate(property_df[property_col].values)})[property_df.columns]\n    return property_df[property_col].value_counts()\n\n#checkPropertyFreq(news_train_df, 'audiences')\n#checkPropertyFreq(news_train_df, 'subjects')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1aaa379564ec6e374b171579bfff0de4d86309e6"},"cell_type":"code","source":"# clustering the stocks into 10 different groups\nfrom sklearn import cluster, datasets\n\n# cluster by the returnsOpenNextMktres10\n# some stocks are newly listed so we need to use more recent data\n# we sample for 100 times, and use that to generate the clustering\ntimes_sample = np.random.choice(market_train_df.loc[market_train_df['time'].dt.year >= 2011].time.unique(), 500, replace=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d066161df19d356044143da20e9782dd08cb837e"},"cell_type":"code","source":"#cluster_by = 'returnsClosePrevRaw1'\ncluster_by = 'returnsOpenPrevRaw1'\nasset_return_df = market_train_df.loc[(market_train_df.time.isin(times_sample))][['time', 'assetCode', cluster_by]].copy()\n# set index\nasset_return_df.set_index(['time', 'assetCode'], inplace=True)\n\n# normalise return of each asset code\n# we should normalise return with the mean as well, such that stock that trend vs stock that don't can still be clustered together\n# logically we should clip the return such that we don't try to cluster stocks that just so happens to move 30% on the same day as\n# another by chance\nasset_return_df[cluster_by] = asset_return_df[cluster_by].clip(-0.2, 0.2)\nasset_return_df['returnNormalised'] = asset_return_df.groupby('assetCode')[cluster_by].transform(lambda x : (x - x.mean() / x.std()) if x.std() > 1e-10 else x)\nasset_return_df\n\nreturn_by_date_df = asset_return_df.drop([cluster_by], axis=1).unstack(level=0).fillna(0)\nreturn_by_date_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"a3fd1a8a5a47d9812761a41e92d8c400ca7b0ab7"},"cell_type":"code","source":"market_train_df.loc[market_train_df['assetCode'] == 'PNK.O']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7dfa652d57ab68a493727a0416098ed978180a33"},"cell_type":"code","source":"len(market_train_df['assetCode'].unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"abd3f9555d6e5a0aad0e81fab2c05d318a577105"},"cell_type":"code","source":"return_by_date_df.columns.get_level_values(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5bc359818d58fc42941a37443b159ae061168645"},"cell_type":"code","source":"# here time is one feature, so we need to convert the data into\n# assetCode, time1Return, time2Return ... timeNReturn\n\nk_means = cluster.KMeans(n_clusters=100, n_init=500, max_iter=10000, tol=1e-6, verbose=0)\nk_means.fit(return_by_date_df['returnNormalised'].values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"afb8d46ad5712fdd25bd29c71b1e05ae0bdcd48a"},"cell_type":"code","source":"labels = k_means.labels_\n\nreturn_by_date_df.index\n\nasset_cluster = pd.DataFrame()\nasset_cluster['assetCode'] = return_by_date_df.index.get_level_values(0)\nasset_cluster['cluster'] = labels\n# merge in the asset name\nasset_cluster = asset_cluster.merge(asset_name_df, left_on='assetCode', right_on='assetCode', how='left')\nprint('fb cluster = {}'.format(asset_cluster[asset_cluster['assetCode'] == 'FB.O'].iloc[0]['cluster']))\n\nasset_cluster[asset_cluster['cluster'] == 12]\n\nasset_cluster.drop('assetName', axis=1).groupby('cluster').count()\n\n#results = pd.DataFrame(data=labels, columns=['cluster'], index=collapsed.index)\n#results\n#return_by_date_df.shape\n#market_train_df[market_train_df['assetCode'] == 'FB.O']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f59416f7170e5331314a9d198bc359a52ded6493"},"cell_type":"code","source":"asset_cluster[asset_cluster['cluster'] == 19]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f13ab4f612b767f3b5b4a998c7e80dd2e00ed9f4"},"cell_type":"code","source":"market_train_df[market_train_df.assetCode == 'MMR.N'].plot(x='time', y=['close'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e28c1464b5e9e8042102e325e96aa407d2df89eb"},"cell_type":"code","source":"# 1. asset name makes more sense, don't expand data like in https://www.kaggle.com/bguberfain/a-simple-model-using-the-market-and-news-data\n# 2. want to make sure news is out by the time stock price is there, so for training data, we shift all news forward by 6 hours for training\n# 3.\nclass GeneratorEma:\n\n    column_factors = {}\n    asset_code_factors = {}\n    asset_audience_mode = None\n    \n    non_feature_columns = ['time', 'date', 'assetCode', 'assetName', 'returnsOpenNextMktres10']\n    \n    @staticmethod\n    def drop_non_feature_columns(df):\n        return df[df.columns[~df.columns.isin(__class__.non_feature_columns)]]\n    \n    # start time is used to trim down the data a bit after EMA is\n    # calculated, for speed reasons\n    @staticmethod\n    def generate(market_df, news_df, is_train, start_time=pd.Timestamp(year=2000, month=1, day=1, tz='UTC')):\n        \n        market_df = market_df.copy()\n        news_df = news_df.copy()\n        \n        to_float32(market_df)\n        to_float32(news_df)\n        \n        t1 = time.time()\n        \n        # merge in the cluster code\n        market_df = market_df.merge(asset_cluster.drop(['assetName'], axis=1), left_on='assetCode', right_on='assetCode', how='left')\n        \n        # factorize asset code\n        col = 'assetCodeFactorized'\n        if is_train:\n            market_df[col], uniques = pd.factorize(market_df.assetCode)\n            # reserve 0 for unknown\n            market_df[col] += 1\n            __class__.asset_code_factors = { uniques[i]:(i + 1) for i in range(len(uniques)) }\n        else:\n            market_df[col] = market_df.assetCode.map(lambda a: __class__.asset_code_factors.get(a, 0))\n        print_time(t1, 'factorize asset code')\n        \n        # preprocess\n        t1 = time.time()\n        market_df = __class__.process_mkt(market_df, start_time)\n        print_time(t1, 'process mkt')\n        \n        t1 = time.time()\n        news_df = __class__.process_news(news_df, is_train, start_time)\n        print_time(t1, 'process news')\n\n        # Join market and news\n        t1 = time.time()\n        train_df = market_df.join(news_df, on=['date', 'assetName'])\n        print_time(t1, 'join market and news')\n            \n        del news_df\n        del market_df\n\n        # merge the most freq audience in\n        # asset_df = asset_df.merge(_class__.asset_audience_mode, left_on='assetName', right_on='assetName', how='left')\n                                \n        # convert float64 columns to float32\n        train_df.loc[:, train_df.dtypes == np.float64] = train_df.loc[:, train_df.dtypes == np.float64].astype(np.float32)\n        \n        # fill na, not sure why we need to exclude any category type columns\n        train_df.loc[:,train_df.dtypes != 'category'] = train_df.loc[:,train_df.dtypes != 'category'].fillna(0)\n        \n        debug_msg('done')\n        \n        return train_df\n    \n    # we want to transform the news to index by date and asset\n    @staticmethod\n    def process_news(news_df, is_train, start_time):\n        \n        # following copied from\n        # https://www.kaggle.com/bguberfain/a-simple-model-using-the-market-and-news-data\n        news_cols_agg = {\n            'urgency': ['count'],\n            'takeSequence': ['max'],\n            'bodySize': ['mean', 'sum', 'std'],\n            'wordCount': ['mean'],\n            'sentenceCount': ['mean'],\n            'companyCount': ['mean'],\n            'marketCommentary': ['mean'],\n            'relevance': ['mean'],\n            'sentimentNegative': ['sum', 'mean', 'std'],\n            'sentimentNeutral': ['sum', 'mean', 'std'],\n            'sentimentPositive': ['sum', 'mean', 'std'],\n            'sentimentWordCount': ['sum', 'mean', 'std'],\n            'noveltyCount12H': ['mean'],\n            'noveltyCount24H': ['mean'],\n            'noveltyCount3D': ['mean'],\n            'noveltyCount5D': ['mean'],\n            'noveltyCount7D': ['mean'],\n            'volumeCounts12H': ['mean'],\n            'volumeCounts24H': ['mean'],\n            'volumeCounts3D': ['mean'],\n            'volumeCounts5D': ['mean'],\n            'volumeCounts7D': ['mean']\n        }\n\n        # Maybe drop those 3 days / 5 days count as they are highly correlated to the 7d\n\n        # drop some columns\n        news_df.drop(['assetCodes', 'sourceTimestamp', 'firstCreated', 'headline', 'audiences', 'subjects', 'sourceId'], axis=1, inplace=True)\n        news_df = news_df.loc[news_df.time >= start_time].reset_index(drop=True)\n        \n        # factorize some columns\n        # https://stackoverflow.com/questions/46761978/factorize-values-across-dataframe-columns-with-consistent-mappings\n        # also maybe one hot is better, see https://stackoverflow.com/questions/34265102/xgboost-categorical-variables-dummification-vs-encoding\n        # Factorize categorical columns\n        for col in ['headlineTag', 'provider']:\n            if is_train:\n                news_df[col], uniques = pd.factorize(news_df[col])\n                __class__.column_factors[col] = { uniques[i]:i for i in range(len(uniques)) }\n            else:\n                factors_dict = __class__.column_factors[col]\n                news_df[col] = news_df[col].map(lambda a: factors_dict.get(a, 0))\n\n        # convert these into lists of items\n        # news_df['subjects'] = news_df['subjects'].str.findall(f\"'([\\w\\./]+)'\")\n\n        # create a date column\n        news_df['date'] = news_df.time.dt.date\n\n        # drop the 'time' column\n        news_df.drop(['time'], axis=1, inplace=True)\n\n        # aggregate on date\n        cols = list(news_cols_agg.keys())\n        # Convert to float32 to save memory, also for bool type to work properly\n        news_df[cols] = news_df[cols].apply(np.float32)\n        news_df_agg = news_df.groupby(['date', 'assetName']).agg(news_cols_agg)\n        \n        # Flat columns\n        news_df_agg.columns = ['_'.join(col).strip() for col in news_df_agg.columns.values]\n\n        return news_df_agg\n\n    # mkt data is simpler\n    # but we want to convert volume to value\n    @staticmethod\n    def process_mkt(market_train, start_time):\n        \n        if 'returnsOpenNextMktres10' in market_train.columns:\n            # remove outliers\n            market_train = market_train.loc[(market_train['returnsOpenNextMktres10'] > -0.5) & (market_train['returnsOpenNextMktres10'] < 0.5)].reset_index(drop=True)\n        \n        if 'universe' in market_train.columns:\n            # remove all points with universe != 1\n            # they are not used for scoring, if we use time sequence data maybe will need to include them\n            market_train = market_train.loc[market_train['universe'] > 0.0]\n            market_train.drop(['universe'], axis=1, inplace=True)\n        \n        # create a value column, which is just approx value (cause we cannot be sure of price)\n        # opening price is usually not as good as closing price as a reflection of average trading price\n        market_train['value'] = market_train['volume'] * market_train['close']\n        \n        # create a open vs close column\n        market_train['dayChange'] = (market_train['close'] - market_train['open']) / market_train['open']\n        \n        # add some EMAs\n        asset_groupby = market_train.groupby('assetCode')\n        ema10Days = asset_groupby['adjClose'].transform(lambda x : x.ewm(span=10).mean())\n        ema50Days = asset_groupby['adjClose'].transform(lambda x : x.ewm(span=50).mean())\n        market_train['closeOverEma10days'] = (market_train['adjClose'] - ema10Days) / ema10Days\n        market_train['closeOverEma50days'] = (market_train['adjClose'] - ema50Days) / ema50Days\n        market_train['valueEma50days'] = asset_groupby['value'].transform(lambda x : x.ewm(span=50).mean())\n        market_train['valueOverEma'] = (market_train.value - market_train.valueEma50days) / market_train.valueEma50days\n        \n        # after EMA is calculated we can filter out by time\n        market_train = market_train.loc[market_train.time >= start_time].reset_index(drop=True)\n        \n        # drop open / close / volume / value\n        market_train.drop(['open', 'close', 'adjOpen', 'adjClose', 'volume', 'value'], axis=1, inplace=True)\n\n        # create a date column\n        market_train['date'] = market_train.time.dt.date\n\n        return market_train\n    \n    # extract some property for asset codes\n    # the idea is try to seperate them into sectors\n    @staticmethod\n    def extractAssetProperty(news_df, property_col):\n        property_df = news_df[['assetName', property_col]].copy()\n        # convert to string\n        property_df[property_col] = property_df[property_col].str.findall(f\"'([\\w\\./]+)'\")\n        property_df = pd.DataFrame({\n              col:np.repeat(property_df[col].values, property_df[property_col].str.len())\n              for col in property_df.columns.drop(property_col)}\n            ).assign(**{property_col:np.concatenate(property_df[property_col].values)})[property_df.columns]\n\n        property_df = property_df.groupby('assetName').agg(lambda x:x.mode().iloc[0]).dropna()\n        \n        # factorize the column\n        property_df[property_col], uniques = pd.factorize(property_df[property_col])\n        property_df[property_col] += 1\n        \n        return property_df\n    \n    # NOT used\n    # this code is interesting but we don't want to do it\n    @staticmethod\n    def expandAssetCodes(news_df):\n        # code copied from \n        # https://stackoverflow.com/questions/27263805/pandas-when-cell-contents-are-lists-create-a-row-for-each-element-in-the-list/48532692#48532692\n        # which expands all asset codes list into rows\n        code_col = 'assetCodes'\n        news_df = pd.DataFrame({\n              col:np.repeat(news_df[col].values, news_df[code_col].str.len())\n              for col in news_df.columns.drop(code_col)}\n            ).assign(**{code_col:np.concatenate(news_df[code_col].values)})[news_df.columns]\n\n        return news_df\n    \n    @staticmethod\n    def unit_test():\n        pass","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9151aff0ed91bff261650c8ef472d0b3f414a0f6","scrolled":true},"cell_type":"code","source":"# plot some data\n\nmarket_df = market_train_df.copy()\nmarket_df['ewm10'] = market_df.groupby('assetCode')['adjClose'].transform(lambda x : x.ewm(span=10).mean())\nmarket_df['ewm50'] = market_df.groupby('assetCode')['adjClose'].transform(lambda x : x.ewm(span=50).mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f03e044bc6e0c3ca9b1ffc40bc787fef91319a3e"},"cell_type":"code","source":"assetCode = 'AAPL.O'\nthisAssetMark_df = market_df[market_df['assetCode']==assetCode].sort_values(by='time',ascending=True) \n\nf, axs = plt.subplots(2,1, sharex=True, figsize=(12,8))\n\n# Price vs time\nthisAssetMark_df.plot(ax=axs[0], x='time', y=['adjClose','ewm50','ewm10'])\nthisAssetMark_df.plot(ax=axs[1], x='time', y=['returnsOpenNextMktres10'])\nf.suptitle('Close price and EMA vs time')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d1b98a786b751afae36535f4229506065f2fee29"},"cell_type":"code","source":"def train_test_split(x, bucket_days, test_size):\n    \"\"\"\n    we cannot use normal train test split as data is time related\n    we need to split by selecting chunks of time\n\n    :param: DataFrame x, y:      \n    :param: int bucket_days:  duration of bucket\n    :param: test_size: \n    :return: tuple:              the (x_train, x_test, y_train, y_test) tuple\n\n    \"\"\"\n\n    import random\n    import datetime\n\n    time_min = x.date.min()\n    time_max = x.date.max()\n\n    #print('time min: {} max: {}'.format(time_min, time_max))\n\n    train_or_test = []\n\n    # split the time into duration, and toss a dice to see which way it should go\n    t = time_min\n    while(t <= time_max):\n        is_test = random.random() < test_size\n        train_or_test.append((t, is_test))\n        #print('time : {} test?: {}'.format(t, is_test))\n        t = t + datetime.timedelta(days = bucket_days)\n\n    def is_test(input_t):\n        # pretty bad linear search, hope it is not too slow\n        for t in train_or_test:\n            if input_t <= t[0]:\n                return t[1]\n        return False\n\n    is_test = x.date.map(lambda t : is_test(t))\n\n    x_train = x[is_test == False].copy()\n    x_test = x[is_test].copy()\n\n    return (x_train, x_test, is_test)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"edfdafb4c3ae48f3d09cb80fe1162f2fd95531e2"},"cell_type":"code","source":"import xgboost as xgb\nimport sklearn\n\ndef train_model(train_df, test_df, colsample_bytree = 0.8, learning_rate = 0.1, max_depth = 15,\n                alpha = 10, n_estimators = 100):\n\n    # only use a small subset\n    #train_df = train_df.sample(frac=0.2).reset_index(drop=True)\n    #test_df = test_df.sample(frac=0.2).reset_index(drop=True)\n\n    y_train = train_df['returnsOpenNextMktres10']\n    y_test = test_df['returnsOpenNextMktres10']\n\n    # drop column\n    train_df = GeneratorEma.drop_non_feature_columns(train_df)\n    test_df = GeneratorEma.drop_non_feature_columns(test_df)\n\n    eval_set = [(test_df, y_test)]\n    xg_reg = xgb.XGBRegressor(objective = 'reg:linear', colsample_bytree = colsample_bytree, learning_rate = learning_rate, max_depth = max_depth, alpha = alpha, n_estimators = n_estimators)\n    xg_reg.fit(train_df, y_train, eval_metric='rmse', eval_set=eval_set, verbose=True)\n    preds = xg_reg.predict(test_df)\n    rmse = np.sqrt(sklearn.metrics.mean_squared_error(y_test, preds))\n    return xg_reg\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"946a31670cd6f04e83374c86779dff200a7660ff","scrolled":true},"cell_type":"code","source":"train_df = GeneratorEma.generate(market_train_df, news_train_df, is_train=True)\ntrain_df.tail(5)\n\n# split into train / test\ntrain_df, test_df, is_test = train_test_split(train_df, bucket_days=100, test_size=0.2)\n\nxg_reg = train_model(train_df, test_df, colsample_bytree = 0.8, learning_rate = 0.1,\n                     max_depth = 10, alpha = 1, n_estimators = 70)\nprint('finish training model!')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"c0e64eff7437c63b4d24b6c64059fe0cc9fa2ade"},"cell_type":"code","source":"fig, ax = plt.subplots(1, 1, figsize=(10, 10))\nxgb.plot_importance(xg_reg, ax=ax)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e9ddee29657d9a529614a3962e76e91e180502fb"},"cell_type":"code","source":"class Predictor:\n\n    def __init__(self, market_obs_df, news_obs_df):\n        # these are the dfs that we keep\n        self.market_obs_df = market_obs_df\n        self.news_obs_df = news_obs_df\n        self.drop_old_data\n        \n    # drop all data older than 50 days, that is what we need\n    def drop_old_data(self, current_time):\n        cutoff_time = current_time - pd.Timedelta(days=50)\n        self.market_obs_df = self.market_obs_df.loc[self.market_obs_df['time'] >= cutoff_time]\n        self.news_obs_df = self.news_obs_df.loc[self.news_obs_df['time'] >= cutoff_time]\n\n    # predictions template looks like:\n    #    assetCode   confidenceValue\n    #    A.N         0.0\n    #    AA.N        0.0\n    #    AAL.O       0.0\n    #    AAN.N       0.0\n    # we need to fill it in then call env.predict(predictions_template_df)\n    #\n    def make_predictions_day(self, xg_reg, predictions_template_df, market_day_df, news_day_df):\n\n        # sanity check to make sure the time are sane\n        time_min = market_day_df.time.min()\n        time_max = market_day_df.time.max()\n        \n        assert(time_min == time_max)\n                \n        # essentially run the batch mode\n        pred_df = self.make_predictions_impl(xg_reg, market_day_df, news_day_df)\n        \n        # join to get only the new rows\n        \n        #merge into the predictions df\n        pred_df = predictions_template_df.drop(['confidenceValue'], axis=1).merge(pred_df, left_on='assetCode', right_on='assetCode', how='left')\n        __class__.add_confidence_value(pred_df)\n        #print(predictions_df.shape[0])\n        #print(predictions_template_df.shape[0])\n        \n        # make sure row counts are correct\n        assert pred_df.shape[0] == predictions_template_df.shape[0]\n        return pred_df\n\n    # batch mode allows making prediction on many days together, which is more useful for fast iteration\n    # use case is save down the prediction days from the competition env and just run through it\n    # once and score\n    # return a df with\n    #  date assetCode confidenceValue\n    def make_predictions_batch(self, xg_reg, market_df, news_df):\n        return self.make_predictions_impl(xg_reg, market_df, news_df)\n    \n    def make_predictions_impl(self, xg_reg, market_df, news_df):\n        # sanity check to make sure the time are sane\n        time_min = market_df.time.min()\n        \n        # append to the running dfs\n        self.market_obs_df = fast_concat([self.market_obs_df[market_df.columns], market_df])\n        self.news_obs_df = fast_concat([self.news_obs_df[news_df.columns], news_df])\n\n        self.drop_old_data(time_min)\n        \n        # essentially run the batch mode\n        pred_df = GeneratorEma.generate(self.market_obs_df, self.news_obs_df, is_train=False, start_time=time_min)\n        pred_df['prediction'] = xg_reg.predict(GeneratorEma.drop_non_feature_columns(pred_df))\n        \n        __class__.add_confidence_value(pred_df)\n        return pred_df\n    \n    @staticmethod\n    def add_confidence_value(pred_df):\n        # normalize predictions into confidence\n        # anything outside 3x stddev is max confidence\n        pred_df['confidenceValue'] = pred_df.groupby('time')['prediction'].transform(lambda x : (x - x.mean()) / x.std())\n        pred_df['confidenceValue'] = pred_df['confidenceValue'].clip(lower=-1.0, upper=1.0)\n        \n\n# using the description given by the competition to score\n# see https://www.kaggle.com/c/two-sigma-financial-news#evaluation\n# predictions df must have following columns:\n# date assetCode confidenceValue\ndef score_predictions(predictions_df, market_df):\n    # just get the columns we needed\n    return_res_df = market_df[['time', 'assetCode', 'returnsOpenPrevMktres10']].copy()\n\n    # Shift -11 days gives us returnsOpenNextMktres10\n    return_res_df['returnsOpenNextMktres10'] = return_res_df.groupby(['assetCode'])['returnsOpenPrevMktres10'].shift(-11)\n\n    # merge the results column into the predictions\n    predictions_df = predictions_df.merge(return_res_df, left_on=['assetCode', 'time'], right_on=['assetCode', 'time'], how='left')\n    predictions_df.dropna(inplace=True)\n\n    #return predictions_df\n    \n    # for each day, take confidence \n    daily_score_x = predictions_df.groupby('time').apply(lambda x : (x['confidenceValue'] * x['returnsOpenNextMktres10']).sum())\n    #return daily_score_x\n\n    score = daily_score_x.mean() / daily_score_x.std()\n    return score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4cf3459b1879b6e00b01b27b2463cbc7c4ae80ee"},"cell_type":"code","source":"days = []\npredictor = Predictor(market_train_df, news_train_df)\npred_dfs = []\n\n# copy it so we can reuse it\nfor (market_day_df, news_day_df, predictions_template_df) in tqdm(env.get_prediction_days()):\n    days.append((market_day_df, news_day_df, predictions_template_df))\n    if testing:\n        env.predict(predictions_template_df)\n    else:\n        pred_df = predictor.make_predictions_day(xg_reg, predictions_template_df, market_day_df, news_day_df)\n        pred_dfs.append(pred_df)\n        env.predict(pred_df[['assetCode','confidenceValue']])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6536a58c8b36b67631330e2d9f805597e78f6ba6","scrolled":true},"cell_type":"code","source":"(market_day_df, news_day_df, predictions_template_df) = days[0]\nmarket_day_df[market_day_df.assetCode == 'AAPL.O'].tail(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"969cf0577303b7a78d66e066c49fe961604f8feb"},"cell_type":"code","source":"market_obs_df = fast_concat([day[0] for day in days])\nnews_obs_df = fast_concat([day[1] for day in days])\naddSplitAdjColumns(market_obs_df)\n\n# try scoring what we just did\nif not testing:\n    pred_df = fast_concat(pred_dfs)\n    score = score_predictions(fast_concat(pred_dfs), market_obs_df)\n    print('my calculated score = {:.3f}'.format(score))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b12b53e22d7b8feecea3f132d25ed274ce56ebce"},"cell_type":"code","source":"news_obs_df.tail(3)\nmarket_obs_df[(market_obs_df.assetCode == 'FB.O') & (market_obs_df.time.dt.year > 2018)].plot(x='time', y=['close'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"55f3defacf32d1ba20bdf7fca855b93c328739d9"},"cell_type":"code","source":"last_day_mkt_df = days[-1][0]\nlast_day_mkt_df[last_day_mkt_df.assetCode == 'AAPL.O']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"173f062a8afefd16bd8bedd022fcb3692632488d"},"cell_type":"code","source":"predictor = Predictor(market_train_df, news_train_df)\npredictions_df = predictor.make_predictions_batch(xg_reg, market_obs_df, news_obs_df)\n\n# those market ob for dates that have not occurred yet are just dummy data, remove them\nend = pd.Timestamp(year=2019, month=2, day=1, tz='UTC')\nscore = score_predictions(predictions_df.loc[predictions_df['time'] < end].reset_index(drop=True), market_obs_df)\n\nprint('Score = {:.3f}'.format(score))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"31689f1cf5d884fd6a5b137c2efcacae57f2dbb9"},"cell_type":"code","source":"pred_df = predictions_df\npred_df['confidenceValue'] = (pred_df['prediction'] - pred_df['prediction'].mean()) / pred_df['prediction'].std()\npred_df['confidenceValue'] = pred_df['confidenceValue'].clip(lower=-1.0, upper=1.0)\nscore_predictions(pred_df, market_obs_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d9b2a2aefbadd0c15085249f391d387e6288f2b7","scrolled":true},"cell_type":"code","source":"if testing:\n    # try doing day by day pred again\n    debug = False\n    predictor = Predictor(market_train_df, news_train_df)\n    #(market_day_df, news_day_df, predictions_template_df) = days[0]\n    #pred_df = predictor.make_predictions(xg_reg, predictions_template_df, market_day_df, news_day_df)\n\n    pred_dfs = []\n    for (market_day_df, news_day_df, predictions_template_df) in tqdm(days[0:2]):\n        pred_dfs.append(predictor.make_predictions_day(xg_reg, predictions_template_df, market_day_df, news_day_df))\n\n    # verify the pred\n    pred_df = fast_concat(pred_dfs)\n    score = score_predictions(fast_concat(pred_dfs), market_obs_df)\n    print('day by day pred score = {:.3f}'.format(score))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"92af92ed8987e9f323d63ff10bd6a06fe1195c45"},"cell_type":"code","source":"predictions_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dd64c63ba69da9d7212426184fd65022b8f04dfc"},"cell_type":"code","source":"pred_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"50a4fc5dc749a6b3aa9276b86480d35fff7b15c4"},"cell_type":"code","source":"# write submission file\nprint('Done!')\nenv.write_submission_file()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}