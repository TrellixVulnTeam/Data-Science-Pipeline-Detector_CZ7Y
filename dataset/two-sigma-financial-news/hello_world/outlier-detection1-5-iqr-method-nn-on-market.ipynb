{"cells":[{"metadata":{"_uuid":"512d64e04ad0533dd72ca6bcbca4882a7f289956"},"cell_type":"markdown","source":""},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport time\nfrom sklearn.metrics import accuracy_score\nfrom kaggle.competitions import twosigmanews\n\n\n#***********************************import keras\nfrom keras import initializers\nfrom keras import regularizers\nfrom keras import constraints\nfrom keras.utils import conv_utils\nfrom keras.utils.data_utils import get_file\nfrom keras.engine.topology import get_source_inputs\nfrom keras.engine import InputSpec\nfrom keras import backend as K\nfrom keras.layers import LeakyReLU\nfrom keras.layers import ZeroPadding2D\nfrom keras.losses import binary_crossentropy\nimport keras.callbacks as callbacks\nfrom keras.callbacks import Callback\nfrom keras.applications.xception import Xception\nfrom keras.layers import multiply\n\nimport keras\nfrom keras import optimizers\nfrom keras.legacy import interfaces\nfrom keras.utils.generic_utils import get_custom_objects\n\nfrom keras.engine.topology import Input\nfrom keras.engine.training import Model\nfrom keras.layers.convolutional import Conv2D, UpSampling2D, Conv2DTranspose\nfrom keras.layers.core import Activation, SpatialDropout2D\nfrom keras.layers.merge import concatenate\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.layers.pooling import MaxPooling2D\nfrom keras.layers import Input,Dropout,BatchNormalization,Activation,Add\nfrom keras.regularizers import l2\nfrom keras.layers.core import Dense, Lambda\nfrom keras.layers.merge import concatenate, add\nfrom keras.layers import GlobalAveragePooling2D, Reshape, Dense, multiply, Permute\nfrom keras.optimizers import SGD","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"scrolled":true},"cell_type":"code","source":"env = twosigmanews.make_env()\n(market_train, _) = env.get_training_data()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5dfa1843dfece6fccfca91896ef85a332b55e3e6"},"cell_type":"code","source":"cat_cols = ['assetCode']\nnum_cols = ['volume', 'close', 'open', 'returnsClosePrevRaw1', 'returnsOpenPrevRaw1', 'returnsClosePrevMktres1',\n                    'returnsOpenPrevMktres1', 'returnsClosePrevRaw10', 'returnsOpenPrevRaw10', 'returnsClosePrevMktres10',\n                    'returnsOpenPrevMktres10']\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e108339134e95473b4a983237d58adb64c3ef64a"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ntrain_indices, val_indices = train_test_split(market_train.index.values,test_size=0.25, random_state=23)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f51d00dc43857b446ae4a24b3718753f09040fd5"},"cell_type":"markdown","source":"# Handling categorical variables"},{"metadata":{"trusted":true,"_uuid":"301a65b834d8614a914883d49be1860550174f06"},"cell_type":"code","source":"def encode(encoder, x):\n    len_encoder = len(encoder)\n    try:\n        id = encoder[x]\n    except KeyError:\n        id = len_encoder\n    return id\n\nencoders = [{} for cat in cat_cols]\n\n\nfor i, cat in enumerate(cat_cols):\n    print('encoding %s ...' % cat, end=' ')\n    encoders[i] = {l: id for id, l in enumerate(market_train.loc[:, cat].astype(str).unique())}\n    market_train[cat] = market_train[cat].astype(str).apply(lambda x: encode(encoders[i], x))\n    print('Done')\n\nembed_sizes = [len(encoder) + 1 for encoder in encoders] #+1 for possible unknown assets\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"397360cbacbcb294daf5e0750f471e13ced31978"},"cell_type":"markdown","source":"# Handling numerical variables"},{"metadata":{"trusted":true,"_uuid":"8f2e3b0083bca38122f9b73deb98182ec6214c42"},"cell_type":"code","source":"#market_train['timess'] =market_train.time.dt.strftime(\"%Y%m%d\").astype(int)\n#market_train = market_train.loc[market_train['timess'] > 20110000]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6f6b8cb8279944227dc7ba78f404c52f5b4e6871"},"cell_type":"code","source":"market_train.head()\nmarket_train.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2f2ab4abee04b945eb01ca00be4146853abbdc26"},"cell_type":"code","source":"types = market_train['assetCode'].unique()\nd = {type: market_train[market_train['assetCode'] == type] for type in types} #creating dataframes for each unique assetCode","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8240cbad629854fd74450be2d5ab5ff2dd138f77"},"cell_type":"code","source":"market_train.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"60456ec0210633c9c0dec77172dd5385356fa263"},"cell_type":"code","source":"a=0\nfor type in types:\n    a=a+d[type].shape[0]\na    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c6aa9c639eb0d53852b575af61f60f2d680d1189"},"cell_type":"code","source":"market_train.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ec621c3d8a8b4250c74b4c7d3966d48c83134a38"},"cell_type":"code","source":"d[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9bbf77cc03078c118c5117adeeed306fa2e8e371"},"cell_type":"code","source":"#Setting quantiles and outlier borders\n\nlow = .25\nhigh = .75\n\nbounds = {}\nfor type in types:\n    filt_df = d[type].loc[:, d[type].columns != 'assetCode']# Remove 'Type' Column\n    filt_df = filt_df.loc[:, filt_df.columns != 'universe']\n    quant_df = filt_df.quantile([low, high])\n    IQR = quant_df.iloc[1,:]-  quant_df.iloc[0,:]\n    quant_df.iloc[0,:] = quant_df.iloc[0,:] - 1.5*IQR\n    quant_df.iloc[1,:] = quant_df.iloc[1,:] + 1.5*IQR\n    bounds[type] = quant_df\n    bounds[type] = bounds[type].reset_index()\n    bounds[type] = bounds[type].drop(\"index\", axis=1)\nbounds[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"25ca3c72a6b2dc29a8b1e9c8e4ab6db87b6228d3"},"cell_type":"code","source":"bounds[1].loc[1,\"volume\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a75895f1831df6cf321d39f9c40ac53770ce2d1c"},"cell_type":"code","source":"market_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"223d2501b8a063ae6d11b578b331cd4ff7e9c302"},"cell_type":"code","source":"market_train1=pd.DataFrame()\nfor type in types:\n    for column in num_cols:\n        d[type] = d[type].loc[d[type][column]>=bounds[type].loc[0,column]]\n        d[type] = d[type].loc[d[type][column]<=bounds[type].loc[1,column]]\n    market_train1=pd.concat([market_train1,d[type]], ignore_index=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"81e23676b2f72bd14685028165c6e106e9160eb6"},"cell_type":"code","source":"orig_len=market_train.shape[0]\nnew_len = market_train1.shape[0]\nrmv_len1 = np.abs(orig_len-new_len)\nprint('There were %i lines removed' %rmv_len1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a4825176c6f882a7bc4f57a76c28377cf613d03d"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c42866e084c5991f914a1686374ba811e2c5b2c2"},"cell_type":"code","source":"market_train1['close_open_ratio'] = np.abs(market_train1['close']/market_train1['open'])\nmarket_train1 = market_train1.loc[market_train1['close_open_ratio'] < 1.4]\nmarket_train1 = market_train1.loc[market_train1['close_open_ratio'] > 0.4]\n#market_train1 = market_train1.drop(columns=['close_open_ratio'])\n\nnewer_len=market_train1.shape[0]\nrmv_len2 = np.abs(new_len-newer_len)\nprint('There were %i lines removed additionally' %rmv_len2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c82a83242e7b3fecdbff7a3cb29f56491e697ace"},"cell_type":"code","source":"market_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d3bf89d9664f19ca6f7c93f5f810e17b036fb902"},"cell_type":"code","source":"market_train1=market_train1.dropna()\nrmv_len3 = np.abs(newer_len-market_train1.shape[0])\nprint('There were %i lines removed additionally' %rmv_len3)\ndeleted_rows=rmv_len1+rmv_len2+rmv_len3\nprint('There were %i lines in total removed' %deleted_rows)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cd6ca2706b8827c5464608d820ef1d5149fbe492"},"cell_type":"code","source":"\nmarket_train1['average'] = (market_train1['close'] + market_train1['open'])/2\nmarket_train1['pricevolume'] = market_train1['volume'] * market_train1['close']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f0f0b17c57d9d0ada26b6b7fe1b1edc30ad6a6fd"},"cell_type":"code","source":"market_train1.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d077eb3918f1e82ebf5ee0aec7fad65fb5126458"},"cell_type":"code","source":"num_cols = ['volume', 'close', 'open', 'returnsClosePrevRaw1', 'returnsOpenPrevRaw1', 'returnsClosePrevMktres1',\n                    'returnsOpenPrevMktres1', 'returnsClosePrevRaw10', 'returnsOpenPrevRaw10', 'returnsClosePrevMktres10',\n                    'returnsOpenPrevMktres10','close_open_ratio','average','pricevolume']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e214638bcf7248bb25bca8498bc54736763c5835"},"cell_type":"code","source":"market_train=market_train1.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c6200bd8828f99f17442c05c393ba4fa96deee5a"},"cell_type":"code","source":"train_indices, val_indices = train_test_split(market_train.index.values,test_size=0.25, random_state=23)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0e6c878e412b3e819e056cee40181b96fbac2f78"},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n \nmarket_train[num_cols] = market_train[num_cols].fillna(0)\nprint('scaling numerical columns')\n\nscaler = StandardScaler()\n\n#col_mean = market_train[col].mean()\n#market_train[col].fillna(col_mean, inplace=True)\nscaler = StandardScaler()\nmarket_train[num_cols] = scaler.fit_transform(market_train[num_cols])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6f4dceda44691863b8e8e52269b0c51119471c72"},"cell_type":"code","source":"market_train.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a2906d949125ddfc5c7a4525b5d93f099d03030d"},"cell_type":"code","source":"#pd.merge(market_train, newsgp, how='left', on=['time', 'assetCode']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"eae09951757845ad5bdf08e004e6477513ed739a"},"cell_type":"markdown","source":"# Define NN Architecture"},{"metadata":{"_uuid":"68473e82a5e1ca3a97d0d89fbb8ac0bd5ca6ea57"},"cell_type":"markdown","source":"Todo: add explanaition of architecture"},{"metadata":{"trusted":true,"_uuid":"f8661c00b3363e610d2ed00fdb86b507a71a41c3"},"cell_type":"code","source":"from keras.models import Model\nfrom keras.layers import Input, Dense, Embedding, Concatenate, Flatten, BatchNormalization\nfrom keras.losses import binary_crossentropy, mse\n\ncategorical_inputs = []\nfor cat in cat_cols:\n    categorical_inputs.append(Input(shape=[1], name=cat))\n\ncategorical_embeddings = []\nfor i, cat in enumerate(cat_cols):\n    categorical_embeddings.append(Embedding(embed_sizes[i], 10)(categorical_inputs[i]))\n\n#categorical_logits = Concatenate()([Flatten()(cat_emb) for cat_emb in categorical_embeddings])\ncategorical_logits = Flatten()(categorical_embeddings[0])\n#categorical_logits = Dense(32,activation='relu')(categorical_logits)\n#categorical_logits =Dropout(0.5)(categorical_logits)\n#categorical_logits =BatchNormalization()(categorical_logits)\ncategorical_logits = Dense(32,activation='relu')(categorical_logits)\n\na=len(num_cols)\nnumerical_inputs = Input(shape=(a,), name='num')\nnumerical_logits = numerical_inputs\nnumerical_logits = BatchNormalization()(numerical_logits)\n\nnumerical_logits = Dense(128,activation='relu')(numerical_logits)\n#numerical_logits=Dropout(0.3)(numerical_logits)\n#numerical_logits = BatchNormalization()(numerical_logits)\n#numerical_logits = Dense(128,activation='relu')(numerical_logits)\nnumerical_logits = Dense(64,activation='relu')(numerical_logits)\n\nlogits = Concatenate()([numerical_logits,categorical_logits])\nlogits = Dense(64,activation='relu')(logits)\nout = Dense(1, activation='sigmoid')(logits)\n\nmodel = Model(inputs = categorical_inputs + [numerical_inputs], outputs=out)\nmodel.compile(optimizer='adam',loss=binary_crossentropy)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"07b70a8c6647dd6adc4ed2e1f1ce06b6f77b89a2"},"cell_type":"code","source":"# Lets print our model\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cb217fc475f418f18720f87a0596aa23eddeb209"},"cell_type":"code","source":"def get_input(market_train, indices):\n    X_num = market_train.loc[indices, num_cols].values\n    X = {'num':X_num}\n    for cat in cat_cols:\n        X[cat] = market_train.loc[indices, cat_cols].values\n    y = (market_train.loc[indices,'returnsOpenNextMktres10']>=0).values\n    r = market_train.loc[indices,'returnsOpenNextMktres10'].values\n    u = market_train.loc[indices, 'universe']\n    d = market_train.loc[indices, 'time'].dt.date\n    return X,y,r,u,d\n\n# r, u and d are used to calculate the scoring metric\nX_train,y_train,r_train,u_train,d_train = get_input(market_train, train_indices)\nX_valid,y_valid,r_valid,u_valid,d_valid = get_input(market_train, val_indices)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"4ab97884c7dfc70dca9c2a4d107249f6dbc1c75e"},"cell_type":"code","source":"market_train.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fc3ecf42a27f26b56abddc0b4066030d20206b28"},"cell_type":"code","source":"class SnapshotCallbackBuilder:\n    def __init__(self, nb_epochs, nb_snapshots, init_lr=0.1):\n        self.T = nb_epochs\n        self.M = nb_snapshots\n        self.alpha_zero = init_lr\n\n    def get_callbacks(self, model_prefix='Model'):\n\n        callback_list = [\n            callbacks.ModelCheckpoint(\"model.hdf5\",monitor='val_my_iou_metric', \n                                   mode = 'max', save_best_only=True, verbose=1),\n            swa,\n            callbacks.LearningRateScheduler(schedule=self._cosine_anneal_schedule)\n        ]\n\n        return callback_list\n\n    def _cosine_anneal_schedule(self, t):\n        cos_inner = np.pi * (t % (self.T // self.M))  # t - 1 is used when t has 1-based indexing.\n        cos_inner /= self.T // self.M\n        cos_out = np.cos(cos_inner) + 1\n        return float(self.alpha_zero / 2 * cos_out)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ecd18fe6d1a0ecf20678f94d7ba4746a7103b407"},"cell_type":"markdown","source":"# Train NN model"},{"metadata":{"trusted":true,"_uuid":"542f77f1f5675067d810614da9add097266b85a5"},"cell_type":"code","source":"from keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n\n\"\"\"\nepochs = 10\nsnapshot = SnapshotCallbackBuilder(nb_epochs=epochs,nb_snapshots=1,init_lr=1e-3)\nbatch_size = 32\nswa = SWA('model_swa.hdf5',6)\nhistory = model.fit(X_train,y_train.astype(int),\n                    validation_data=(X_valid,y_valid.astype(int)),\n                    epochs=epochs,\n                    #batch_size=batch_size,\n                    callbacks=snapshot.get_callbacks(),shuffle=True,verbose=2)\n                    \nearly_stop = EarlyStopping( mode = 'max',patience=15, verbose=1)\ncheck_point = ModelCheckpoint('model.hdf5', mode = 'max', save_best_only=True, verbose=1)\nreduce_lr = ReduceLROnPlateau( mode = 'max',factor=0.5, patience=5, min_lr=0.0001, verbose=1)\n#check_point = ModelCheckpoint('model.hdf5',verbose=True, save_best_only=True)\n#early_stop = EarlyStopping(patience=5,verbose=True)\nmodel.fit(X_train,y_train.astype(int),\n                    validation_data=(X_valid,y_valid.astype(int)), \n                    epochs=15,\n                    callbacks=[check_point,reduce_lr,early_stop], \n                    verbose=2)\n\nmodel.fit(X_train,y_train.astype(int),\n          validation_data=(X_valid,y_valid.astype(int)),\n          epochs=10,\n          verbose=True,\n          callbacks=[early_stop,check_point]) \n\"\"\"\n\n\ncheck_point = ModelCheckpoint('model.hdf5',verbose=True, save_best_only=True)\nearly_stop = EarlyStopping(patience=5,verbose=True)\nmodel.fit(X_train,y_train.astype(int),\n          validation_data=(X_valid,y_valid.astype(int)),\n          epochs=5,\n          verbose=True,\n          callbacks=[early_stop,check_point]) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e3fdc5c7f2c530d264fcbd174e85eb35256c9f17"},"cell_type":"code","source":"\"\"\"\ntry:\n    print('using swa weight model')\n    model.load_weights('model_swa.hdf5')\nexcept:\n    model.load_weights('model.hdf5')\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"460d68972755dff7c94d8f91fba7da2177c48ae7"},"cell_type":"markdown","source":"# Evaluation of Validation Set"},{"metadata":{"trusted":true,"_uuid":"bebb9c3a62ed462a3d64b24559e2f1d447886bca"},"cell_type":"code","source":"# distribution of confidence that will be used as submission\nmodel.load_weights('model.hdf5')\nconfidence_valid = model.predict(X_valid)[:,0]*2 -1\nprint(accuracy_score(confidence_valid>0,y_valid))\nplt.hist(confidence_valid, bins='auto')\nplt.title(\"predicted confidence\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5d8097adcabf5794f04e4ec1f55f2993ef9176cb"},"cell_type":"code","source":"# calculation of actual metric that is used to calculate final score\nr_valid = r_valid.clip(-1,1) # get rid of outliers. Where do they come from??\nx_t_i = confidence_valid * r_valid * u_valid\ndata = {'day' : d_valid, 'x_t_i' : x_t_i}\ndf = pd.DataFrame(data)\nx_t = df.groupby('day').sum().values.flatten()\nmean = np.mean(x_t)\nstd = np.std(x_t)\nscore_valid = mean / std\nprint(score_valid)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a556452e64bef6bee50b7e281ef65f2923c554a7"},"cell_type":"markdown","source":"# Prediction"},{"metadata":{"trusted":true,"_uuid":"f57d1b1d8cf8bea0594e0d699f1b849566ebad86"},"cell_type":"code","source":"days = env.get_prediction_days()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0cea4e659153962bdf2062b0ca10943927549a26"},"cell_type":"code","source":"n_days = 0\nprep_time = 0\nprediction_time = 0\npackaging_time = 0\npredicted_confidences = np.array([])\nfor (market_obs_df, news_obs_df, predictions_template_df) in days:\n    n_days +=1\n    print(n_days,end=' ')\n    \n    t = time.time()\n\n    market_obs_df['assetCode_encoded'] = market_obs_df[cat].astype(str).apply(lambda x: encode(encoders[i], x))\n    market_obs_df['close_open_ratio'] = np.abs(market_obs_df['close']/market_obs_df['open'])\n    market_obs_df['average'] = (market_obs_df['close'] + market_obs_df['open'])/2\n    market_obs_df['pricevolume'] = market_obs_df['volume'] * market_obs_df['close']\n    market_obs_df[num_cols] = market_obs_df[num_cols].fillna(0)\n    market_obs_df[num_cols] = scaler.transform(market_obs_df[num_cols])\n    X_num_test = market_obs_df[num_cols].values\n    X_test = {'num':X_num_test}\n    X_test['assetCode'] = market_obs_df['assetCode_encoded'].values\n    \n    prep_time += time.time() - t\n    \n    t = time.time()\n    market_prediction = model.predict(X_test)[:,0]*2 -1\n    predicted_confidences = np.concatenate((predicted_confidences, market_prediction))\n    prediction_time += time.time() -t\n    \n    t = time.time()\n    preds = pd.DataFrame({'assetCode':market_obs_df['assetCode'],'confidence':market_prediction})\n    # insert predictions to template\n    predictions_template_df = predictions_template_df.merge(preds,how='left').drop('confidenceValue',axis=1).fillna(0).rename(columns={'confidence':'confidenceValue'})\n    env.predict(predictions_template_df)\n    packaging_time += time.time() - t\n\nenv.write_submission_file()\ntotal = prep_time + prediction_time + packaging_time\nprint(f'Preparing Data: {prep_time:.2f}s')\nprint(f'Making Predictions: {prediction_time:.2f}s')\nprint(f'Packing: {packaging_time:.2f}s')\nprint(f'Total: {total:.2f}s')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c0b10b1ee21cfa2ff48b7ad2b7351382c03daeaa"},"cell_type":"code","source":"# distribution of confidence as a sanity check: they should be distributed as above\nplt.hist(predicted_confidences, bins='auto')\nplt.title(\"predicted confidence\")\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}