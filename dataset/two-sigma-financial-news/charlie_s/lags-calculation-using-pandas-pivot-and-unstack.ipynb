{"cells":[{"metadata":{"_uuid":"ab78c671331f03bca78e8b09af34cab9adedf6fe"},"cell_type":"markdown","source":"This kernel shows how to prepare lags separately for train and test phases"},{"metadata":{"trusted":true,"_uuid":"fe2fd9c897a1b59e4fd7908bcebe07916f78dce1"},"cell_type":"code","source":"# Taken from Vadim's code https://www.kaggle.com/nareyko/fast-lags-calculation-concept-using-numpy-arrays\n# comparing with my own code\n\n# from twosigmanews import *\nfrom kaggle.competitions import twosigmanews","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport gc\nfrom resource import getrusage, RUSAGE_SELF\nfrom datetime import date, datetime\n\nimport multiprocessing\nfrom multiprocessing import Pool, cpu_count\n\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"global STARTED_TIME\nSTARTED_TIME = datetime.now()\n\n# It's better to use cpu_count from the system - who knows what happens during test phase\nglobal N_THREADS\nN_THREADS=multiprocessing.cpu_count()\n\nprint(f'N_THREADS: {N_THREADS}')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# FILTERDATE - start date for the train data\nFILTERDATE = date(2007, 1, 1)\n\n# SAMPLEDATE - I use it for sampling and fast sanity check of scripts\nSAMPLEDATE = None\n# SAMPLEDATE = date(2007, 1, 30)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0bea66838a06e3a7a9036e0ca13e5790d2102e01"},"cell_type":"code","source":"global N_LAG, RETURN_FEATURES\n\n# Let's try how it works for 1-year lags\nN_LAG = np.sort([5, 10, 20, 252])\n\n# Features for lags calculation\nRETURN_FEATURES = [\n    'returnsOpenPrevMktres10',\n    'returnsOpenPrevRaw10',\n    'open',\n    'close']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e48594ab7071aa960560b10b83cba87291b0e995"},"cell_type":"code","source":"# Tracking time and memory usage\nglobal MAXRSS\nMAXRSS = getrusage(RUSAGE_SELF).ru_maxrss\ndef using(point=\"\"):\n    global MAXRSS, STARTED_TIME\n    print(str(datetime.now()-STARTED_TIME).split('.')[0], point, end=' ')\n    max_rss = getrusage(RUSAGE_SELF).ru_maxrss\n    if max_rss > MAXRSS:\n        MAXRSS = max_rss\n    print(f'max RSS {MAXRSS/1024/1024:.1f}Gib')\n    gc.collect();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"070ea377f0e92fa5ef1bb468c795519bc5efbde3"},"cell_type":"code","source":"#Added this function\ndef charlies_calculation(market_train_df, only_last_day = False):\n    from collections import OrderedDict\n    cols = [\n    'returnsOpenPrevMktres10',\n    'returnsOpenPrevRaw10',\n    'open',\n    'close'] #source cols where you want to compute features on\n    calculation = ['median', 'max', 'min']  #add any calculation you want\n    my_feat_func = [] #add any function you want\n    rolling = [5, 10, 20, 252]\n    computed = OrderedDict()\n    if only_last_day:\n        for c in cols:\n            #convert into matrix ndays x nassetcode, this what saves the time when calculating rolling features \n            computed[ c ] = market_train_df.pivot(index = 'time', columns = 'codeint', values = c ).astype('float32')\n            for calc in calculation:\n                for r in rolling:\n                    computed[ c + str( r ) + str( calc )]  = computed[ c ].iloc[-r:].rolling(r).agg(calc).astype('float32')\n\n            #if using a defined function\n            for f in my_feat_func:\n                for r in rolling:\n                    computed[ c + str( r ) + str( f.__name__)]  = computed[ c ].iloc[-r:].rolling(r).agg(f).astype('float32')\n\n            #when done with a column delete the source data to avoid duplicate later when merging with market_train_df\n            del computed[ c ]\n    else:\n        for c in cols:\n            #convert into matrix ndays x nassetcode, this what saves the time when calculating rolling features \n            computed[ c ] = market_train_df.pivot(index = 'time', columns = 'codeint', values = c ).astype('float32')\n            for calc in calculation:\n                for r in rolling:\n                    computed[ c + str( r ) + str( calc )]  = computed[ c ].rolling(r).agg(calc).astype('float32')\n\n            #if using a defined function\n            for f in my_feat_func:\n                for r in rolling:\n                    computed[ c + str( r ) + str( f.__name__)]  = computed[ c ].rolling(r).agg(f).astype('float32')\n\n            #when done with a column delete the source data to avoid duplicate later when merging with market_train_df\n            del computed[ c ]\n\n    #unstack pivots\n    if only_last_day:\n        for keys in computed.keys():\n            computed[keys] = computed[keys].iloc[-1:]\n            computed[keys] = computed[keys].unstack()\n    else:\n        for keys in computed.keys():\n            computed[keys] = computed[keys].unstack()\n\n\n    reshape = pd.concat([computed[c] for c in computed.keys()], axis=1)\n    columns = computed.keys()\n    reshape.columns = computed.keys()\n\n    #now merging it back to market_train_df\n    calculated_cols = [str(ccc) for ccc in reshape.columns]\n    keepcols = ['time', 'codeint']\n    #if there are same columns in market_train_df, drop them first\n    todrop = [ccc for ccc in calculated_cols if ccc not in keepcols]\n    if only_last_day:\n        lastday = [ np.sort( market_train_df['time'].unique() )[-1]]\n        \n        market_train_df_last = market_train_df[market_train_df['time'].isin(lastday)]\n        market_train_df_last = market_train_df_last.drop(columns = todrop,errors='ignore')\n        market_train_df_last = market_train_df_last.merge(reshape, how='left', on=['time','codeint'])\n        market_train_df = market_train_df.drop(market_train_df['time'].isin(lastday).index)\n        market_train_df = market_train_df.append(market_train_df_last)\n        market_train_df = market_train_df.fillna(0)\n    else:\n        market_train_df = market_train_df.drop(columns = todrop,errors='ignore')\n        market_train_df = market_train_df.merge(reshape, how='left', on=['time','codeint'])\n        market_train_df = market_train_df.fillna(0)\n    return market_train_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4cd8ce7429b04098a560006b4cedbc5df9c506fb"},"cell_type":"code","source":"\n\nclass doctorstring():\n    def __init__(self):\n        self.mainlist = []\n        return\n    \n    def newitems(self, new):\n        #pass a DF series\n        temp = list(set(new))\n        new = [n for n in temp if n not in self.mainlist]\n        self.mainlist = self.mainlist + new\n        self.dictionary = dict(enumerate(self.mainlist))\n        self.inv_dict = dict(zip(self.dictionary.values(),self.dictionary.keys()))# get inverse mapping of above dictionary, replace key with values\n        return\n    \n    def encode(self, toencode):\n        try:\n            return self.inv_dict[toencode]\n        except:\n            return \n    \n    def encodeassetcodes(self, toencode):\n        string = str(toencode)\n        tmp = list(ast.literal_eval(string))\n        enc = [self.encode(n) for n in tmp]\n        return enc\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bd23eef7934857d8b025ab68cd0b2a87e97b4b75"},"cell_type":"code","source":"# Pre-processing of dataframe, this functions is the same for train and test periods\n# In production we had more calculations\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b5d51b46657369f8eebe53b235ab2e616fddda4b"},"cell_type":"markdown","source":"Let's start"},{"metadata":{"trusted":true,"_uuid":"6081b1fd43a5d854a2768154f23fd2f91c1425d6"},"cell_type":"code","source":"env = twosigmanews.make_env()\n(market_train_df, news_train_df) = env.get_training_data()\nusing('Done')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"eef75e58b7c48232101b83c1090ac7feaf56e398"},"cell_type":"code","source":"print('Dataframe pre-processing')\nmarket_train_df = market_train_df.drop(columns = ['assetName'])\nstocklist = doctorstring() #manages all stock codes conversion\nstocklist.newitems(market_train_df['assetCode'])\nmarket_train_df['codeint'] = market_train_df['assetCode'].apply(stocklist.encode)\n\nusing('Done')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f1763239aac5612acbe20cbc5aeaee00ca3bcde8"},"cell_type":"code","source":"# Dataframe filtering\nusing('Done')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1a88652b9b21b2ab8081c651163f4c76840c6fa2"},"cell_type":"code","source":"print('Lag features generation')\n#here i use my code\nmarket_train_df = charlies_calculation(market_train_df)\n# market_train_df.tail()\n# market_train_df.dtypes\n\n\nusing('Done')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8b5e34a28d9aecf22a955acebdc8f79690060be3"},"cell_type":"code","source":"print('keep only last n days to carry forward for test phase')\n\n# suppose that training is done and we dont need to keep the whole market train df, we only keep 260 days since our largest lag is 252  or one year\nkeepdays = 252\nuniquedates = np.sort( market_train_df['time'].unique() ) [-keepdays:]\nmarket_train_df = market_train_df[market_train_df['time'].isin(uniquedates)]\nusing('Done')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"11245043acf408ee4869995c7dcd49c3826c789e","scrolled":false},"cell_type":"code","source":"print('Prediction')\n#prediction\ndays = env.get_prediction_days()\nn_days = 0\nfor (market_obs_df, news_obs_df, predictions_template_df) in days:\n    n_days +=1\n    if n_days % 100 == 0:\n        using(f'{n_days}')\n    # Test data preprocessing    \n    stocklist.newitems(market_obs_df['assetCode'])\n    market_obs_df['codeint'] = market_obs_df['assetCode'].apply(stocklist.encode)\n\n    market_train_df = market_train_df.append(market_obs_df, ignore_index = True)\n    market_train_df = charlies_calculation(market_train_df, only_last_day = True)\n    # keep saving only last n days\n    uniquedates = np.sort( market_train_df['time'].unique() ) [-keepdays:]\n    market_train_df = market_train_df[market_train_df['time'].isin(uniquedates)]\n\n\n    confidence = 0\n    \n    preds = pd.DataFrame({'assetCode':market_obs_df['assetCode'],'confidence':confidence})\n\n    predictions_template_df = predictions_template_df.merge(preds,how='left')\\\n    .drop('confidenceValue',axis=1)\\\n    .fillna(0)\\\n    .rename(columns={'confidence':'confidenceValue'})\n    \n    env.predict(predictions_template_df)\n    gc.collect()\n    \nusing('Prediction done')\n\n# env.write_submission_file()\n# using('Done')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}