{"cells":[{"metadata":{"_uuid":"7201c5d4-fc22-47bc-a7cf-44599edaebb9","_cell_guid":"6ae05e78-bb51-432f-ab1e-b958faa68baf","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport time\nimport copy\nimport chainer\nimport chainer.functions as F\nimport chainer.links as L\nfrom plotly import tools\nfrom plotly.graph_objs import *\nfrom plotly.offline import init_notebook_mode, iplot, iplot_mpl\nfrom kaggle.competitions import twosigmanews\n\nfrom rl.agents.dqn import DQNAgent\nfrom rl.policy import EpsGreedyQPolicy\nfrom rl.memory import SequentialMemory\n\nimport gym\n\nenv = twosigmanews.make_env()\n(marketdf, newsdf) = env.get_training_data()\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('preparing data...')\ndef prepare_data(marketdf, newsdf):\n    # a bit of feature engineering\n    marketdf['time'] = marketdf.time.dt.strftime(\"%Y%m%d\").astype(int)\n    marketdf['bartrend'] = marketdf['close'] / marketdf['open']\n    marketdf['average'] = (marketdf['close'] + marketdf['open'])/2\n    marketdf['pricevolume'] = marketdf['volume'] * marketdf['close']\n    \n    newsdf['time'] = newsdf.time.dt.strftime(\"%Y%m%d\").astype(int)\n    newsdf['assetCode'] = newsdf['assetCodes'].map(lambda x: list(eval(x))[0])\n    newsdf['position'] = newsdf['firstMentionSentence'] / newsdf['sentenceCount']\n    newsdf['coverage'] = newsdf['sentimentWordCount'] / newsdf['wordCount']\n\n    # filter pre-2012 data, no particular reason\n    marketdf = marketdf.loc[marketdf['time'] > 20120000]\n    \n    # get rid of extra junk from news data\n    droplist = ['sourceTimestamp','firstCreated','sourceId','headline','takeSequence','provider','firstMentionSentence',\n                'sentenceCount','bodySize','headlineTag','marketCommentary','subjects','audiences','sentimentClass',\n                'assetName', 'assetCodes','urgency','wordCount','sentimentWordCount']\n    newsdf.drop(droplist, axis=1, inplace=True)\n    marketdf.drop(['assetName', 'volume'], axis=1, inplace=True)\n    \n    # combine multiple news reports for same assets on same day\n    newsgp = newsdf.groupby(['time','assetCode'], sort=False).aggregate(np.mean).reset_index()\n    \n    # join news reports to market data, note many assets will have many days without news data\n    return pd.merge(marketdf, newsgp, how='left', on=['time', 'assetCode'], copy=False) #, right_on=['time', 'assetCodes'])\n\ncdf = prepare_data(marketdf, newsdf)    \ndel marketdf, newsdf  # save the precious memory","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cdf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"assetsCode = cdf['assetCode'].unique()\ntargetcols = ['returnsOpenNextMktres10']\ntraincols = [col for col in cdf.columns if col not in ['assetCode',\n                                                        'time',\n                                                        'universe',\n                                                        'noveltyCount12H',\n                                                        'noveltyCount24H',\n                                                        'noveltyCount3D',\n                                                        'noveltyCount5D',\n                                                        'noveltyCount7D',\n                                                        'volumeCounts12H',\n                                                        'volumeCounts24H',\n                                                        'volumeCounts3D',\n                                                        'volumeCounts5D',\n                                                        'volumeCounts7D',\n                                                        'position',\n                                                        'coverage',\n                                                        'bartrend',\n                                                        'average',\n                                                        'pricevolume',\n                                                        'companyCount',\n                                                        'relevance',] + targetcols]\n\ndates = cdf['time'].unique()\ntrain = range(len(dates))[:int(0.85*len(dates))]\nval = range(len(dates))[int(0.85*len(dates)):]\n\n# we be classifyin\ncdf[targetcols[0]] = (cdf[targetcols[0]] > 0).astype(int)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def getdata_by_datase_assets(cdf,traincols, dates,targetcols, train, code):\n    # train data\n    filter_t = cdf['time'].isin(dates[train]) & cdf['assetCode'].isin([code])\n    Xt = cdf[traincols].fillna(0).loc[filter_t]\n    Yt = cdf[targetcols].fillna(0).loc[filter_t]\n\n    #print(Xt.head())\n    #print(Yt)\n\n    Xt = Xt.values\n    Yt = Yt.values\n\n    #print(Yt.shape)\n\n    return Xt, Yt\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c4609169-dd84-4a20-8720-17d0bee7a832","_cell_guid":"4b31831f-f6b7-4952-9889-eb7bdbe70ddd","trusted":true},"cell_type":"code","source":"def plot_train_test(train, test, date_split):\n    \n    data = [\n        Candlestick(x=train.index, open=train['Open'], high=train['High'], low=train['Low'], close=train['Close'], name='train'),\n        Candlestick(x=test.index, open=test['Open'], high=test['High'], low=test['Low'], close=test['Close'], name='test')\n    ]\n    layout = {\n         'shapes': [\n             {'x0': date_split, 'x1': date_split, 'y0': 0, 'y1': 1, 'xref': 'x', 'yref': 'paper', 'line': {'color': 'rgb(0,0,0)', 'width': 1}}\n         ],\n        'annotations': [\n            {'x': date_split, 'y': 1.0, 'xref': 'x', 'yref': 'paper', 'showarrow': False, 'xanchor': 'left', 'text': ' test data'},\n            {'x': date_split, 'y': 1.0, 'xref': 'x', 'yref': 'paper', 'showarrow': False, 'xanchor': 'right', 'text': 'train data '}\n        ]\n    }\n    figure = Figure(data=data, layout=layout)\n    iplot(figure)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"85a9c0c5-ccb5-4458-aeba-ccf0512cbbd3","_cell_guid":"7ab4ff95-52b1-40a9-9c49-07eebc7b8006","trusted":true},"cell_type":"code","source":"#plot_train_test(train, test, date_split)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"12434399-7a7a-4dc0-af7b-0a85c4db990b","_cell_guid":"03c81e19-89c9-4130-9365-831c8a656d9e","trusted":true},"cell_type":"code","source":"class Environment_stock:\n    \n    def __init__(self, Xdata, Ydata, history_t=1):\n        self.Xdata = Xdata\n        self.Ydata = Ydata\n        self.history_t = history_t\n        self.reset()\n        \n    def reset(self):\n        self.t = 0\n        self.done = False\n        self.len_output = (self.Xdata.shape)[1]\n        self.history = [0] *  self.len_output\n        return self.history # obs\n    \n    def step(self, act):\n        reward = 0\n        \n        if (act > 0 and self.Ydata[self.t] > 0) or (act == 0 and self.Ydata[self.t] == 0):\n            reward = 1\n        else:\n            reward = -1\n            \n        # set next time\n        self.t += 1\n        #self.position_value = 0\n        #self.history.pop(0)\n        #self.history.append(self.Xdata[self.t])\n        \n        self.history = self.Xdata[self.t]\n        \n        return self.history, reward, self.done # obs, reward, done\n    \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Environment1:\n    \n    def __init__(self, data, history_t=90):\n        self.data = data\n        self.history_t = history_t\n        self.reset()\n        \n    def reset(self):\n        self.t = 0\n        self.done = False\n        self.profits = 0\n        self.positions = []\n        self.position_value = 0\n        self.history = [0 for _ in range(self.history_t)]\n        return [self.position_value] + self.history # obs\n    \n    def step(self, act):\n        reward = 0\n        \n        # act = 0: stay, 1: buy, 2: sell\n        if act == 1:\n            self.positions.append(self.data.iloc[self.t, :]['Close'])\n        elif act == 2: # sell\n            if len(self.positions) == 0:\n                reward = -1\n            else:\n                profits = 0\n                for p in self.positions:\n                    profits += (self.data.iloc[self.t, :]['Close'] - p)\n                reward += profits\n                self.profits += profits\n                self.positions = []\n        \n        # set next time\n        self.t += 1\n        self.position_value = 0\n        for p in self.positions:\n            self.position_value += (self.data.iloc[self.t, :]['Close'] - p)\n        self.history.pop(0)\n        self.history.append(self.data.iloc[self.t, :]['Close'] - self.data.iloc[(self.t-1), :]['Close'])\n        \n        # clipping reward\n        if reward > 0:\n            reward = 1\n        elif reward < 0:\n            reward = -1\n        \n        return [self.position_value] + self.history, reward, self.done # obs, reward, done","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"02215cbb-70f3-43a5-9144-0a9e4cad9756","_cell_guid":"24cb0ba3-b10f-4d95-8e30-d01ac9190c9f","trusted":true},"cell_type":"code","source":"Xt, Yt = getdata_by_datase_assets(cdf,traincols, dates,targetcols, val, assetsCode[0])\nlen_input = Xt.shape[1]\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dddb4ccc-7aa3-4c82-ae39-a313095c81e2","_cell_guid":"a79092d2-6446-4267-a546-5d8ace105904","trusted":true},"cell_type":"code","source":"# DQN\n\ndef train_dqn(filterele):\n\n    class Q_Network(chainer.Chain):\n\n        def __init__(self, input_size, hidden_size, output_size):\n            super(Q_Network, self).__init__(\n                fc1 = L.Linear(input_size, hidden_size),\n                fc2 = L.Linear(hidden_size, hidden_size),\n                fc3 = L.Linear(hidden_size, output_size)\n            )\n\n        def __call__(self, x):\n            h = F.relu(self.fc1(x))\n            h = F.relu(self.fc2(h))\n            y = self.fc3(h)\n            return y\n\n        def reset(self):\n            self.zerograds()\n\n    Q = Q_Network(input_size=len_input, hidden_size=100, output_size=3)\n    Q_ast = copy.deepcopy(Q)\n    optimizer = chainer.optimizers.Adam()\n    optimizer.setup(Q)\n\n    epoch_num = 50\n    \n    memory_size = 200\n    batch_size = 20\n    epsilon = 1.0\n    epsilon_decrease = 1e-3\n    epsilon_min = 0.1\n    start_reduce_epsilon = 200\n    train_freq = 10\n    update_q_freq = 20\n    gamma = 0.97\n    show_log_freq = 5\n\n    memory = []\n    total_step = 0\n    total_rewards = []\n    total_losses = []\n    \n    reward_ok = 0\n    reward_ko = 0\n\n    start = time.time()\n    for epoch in range(epoch_num):\n        \n        print(\"Epoch: \" + str(epoch))\n        \n        len_assets = len(assetsCode)\n        count_asset = 0\n        \n        for asset in assetsCode:\n            Xt, Yt = getdata_by_datase_assets(cdf, traincols, dates, targetcols, filterele, asset)\n            \n            step_max = len(Xt)-1\n            count_asset = count_asset + 1\n            #print(\"Asset: \" + asset + \" (\" + str(count_asset) + \" of \" + str(len_assets) + \" )\")\n            \n            env = Environment_stock(Xt, Yt)\n\n            pobs = env.reset()\n            step = 0\n            done = False\n            total_reward = 0\n            total_loss = 0\n            \n            index = 0\n\n            while not done and step < step_max:\n\n                # select act\n                pact = np.random.randint(3)\n                if np.random.rand() > epsilon:\n                    pact = Q(np.array(pobs, dtype=np.float32).reshape(1, -1))\n                    pact = np.argmax(pact.data)\n\n                # act\n                obs, reward, done = env.step(pact)\n                \n                index = index + 1\n                #print(index)\n\n                # add memory\n                memory.append((pobs, pact, reward, obs, done))\n                if len(memory) > memory_size:\n                    memory.pop(0)\n                    \n                #print(1)\n\n                # train or update q\n                if len(memory) == memory_size:\n                    if total_step % train_freq == 0: \n                        shuffled_memory = np.random.permutation(memory)\n                        memory_idx = range(len(shuffled_memory))\n                        for i in memory_idx[::batch_size]:\n                            batch = np.array(shuffled_memory[i:i+batch_size])\n                            b_pobs = np.array(batch[:, 0].tolist(), dtype=np.float32).reshape(batch_size, -1)\n                            b_pact = np.array(batch[:, 1].tolist(), dtype=np.int32)\n                            b_reward = np.array(batch[:, 2].tolist(), dtype=np.int32)\n                            b_obs = np.array(batch[:, 3].tolist(), dtype=np.float32).reshape(batch_size, -1)\n                            b_done = np.array(batch[:, 4].tolist(), dtype=np.bool)\n\n                            q = Q(b_pobs)\n                            maxq = np.max(Q_ast(b_obs).data, axis=1)\n                            target = copy.deepcopy(q.data)\n                            for j in range(batch_size):\n                                target[j, b_pact[j]] = b_reward[j]+gamma*maxq[j]*(not b_done[j])\n                                \n                            #print(6)\n                            Q.reset()\n                            loss = F.mean_squared_error(q, target)\n                            total_loss += loss.data\n                            loss.backward()\n                            optimizer.update()\n\n                    if total_step % update_q_freq == 0:\n                        Q_ast = copy.deepcopy(Q)\n\n                # epsilon\n                if epsilon > epsilon_min and total_step > start_reduce_epsilon:\n                    epsilon -= epsilon_decrease\n\n                # next step\n                total_reward += reward\n                pobs = obs\n                step += 1\n                total_step += 1\n                \n                if reward > 0:\n                    reward_ok = reward_ok + 1\n                else:\n                    reward_ko = reward_ko + 1\n\n            total_rewards.append(total_reward)\n            total_losses.append(total_loss)\n            \n            #print(total_rewards)\n            #print(total_losses)\n\n            if (count_asset+1) % show_log_freq == 0:\n                #log_reward = sum(total_rewards[((count_asset+1)-show_log_freq):])/show_log_freq\n                #log_loss = sum(total_losses[((count_asset+1)-show_log_freq):])/show_log_freq\n                log_reward = sum(total_rewards)\n                log_loss = sum(total_losses)\n                elapsed_time = time.time()-start\n                print(\"Asset: \" + str(count_asset) + \" of \" + str(len_assets))\n                print('\\t'.join(map(str, [epoch+1, epsilon, total_step, log_reward, reward_ok, reward_ko, reward_ok / (reward_ok + reward_ko), log_loss, elapsed_time])))\n                reward_ok = reward_ko = 0\n                total_rewards = []\n                total_losses = []\n                start = time.time()\n            \n    return Q, total_losses, total_rewards\n\nprint(\"Start training\")\nQ, total_losses, total_rewards = train_dqn(train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3156106b-148f-455b-9261-f6b215e204a7","_cell_guid":"4728a161-2ae5-4a1e-8735-36ba38d589b0","trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"65baea17-45d5-4f34-861f-7df67028b898","_cell_guid":"b7d74979-8de5-4e56-8554-aea59e5982ff","trusted":true},"cell_type":"code","source":"def plot_loss_reward(total_losses, total_rewards):\n\n    figure = tools.make_subplots(rows=1, cols=2, subplot_titles=('loss', 'reward'), print_grid=False)\n    figure.append_trace(Scatter(y=total_losses, mode='lines', line=dict(color='skyblue')), 1, 1)\n    figure.append_trace(Scatter(y=total_rewards, mode='lines', line=dict(color='orange')), 1, 2)\n    figure['layout']['xaxis1'].update(title='epoch')\n    figure['layout']['xaxis2'].update(title='epoch')\n    figure['layout'].update(height=400, width=900, showlegend=False)\n    iplot(figure)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"afd33ba3-46c8-4199-957d-63a3240a6cdb","_cell_guid":"1c8763cc-6109-4f9a-a43d-478d2360c36b","trusted":true},"cell_type":"code","source":"plot_loss_reward(total_losses, total_rewards)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7e39d0f8-d029-46f3-b938-26a1ee8775c3","_cell_guid":"242b590e-bd34-4173-b56e-c5a8e5ec9121","trusted":true},"cell_type":"code","source":"def plot_train_test_by_q(train_env, test_env, Q, algorithm_name):\n    \n    # train\n    pobs = train_env.reset()\n    train_acts = []\n    train_rewards = []\n\n    for _ in range(len(train_env.data)-1):\n        \n        pact = Q(np.array(pobs, dtype=np.float32).reshape(1, -1))\n        pact = np.argmax(pact.data)\n        train_acts.append(pact)\n            \n        obs, reward, done = train_env.step(pact)\n        train_rewards.append(reward)\n\n        pobs = obs\n        \n    train_profits = train_env.profits\n    \n    # test\n    pobs = test_env.reset()\n    test_acts = []\n    test_rewards = []\n\n    for _ in range(len(test_env.data)-1):\n    \n        pact = Q(np.array(pobs, dtype=np.float32).reshape(1, -1))\n        pact = np.argmax(pact.data)\n        test_acts.append(pact)\n            \n        obs, reward, done = test_env.step(pact)\n        test_rewards.append(reward)\n\n        pobs = obs\n        \n    test_profits = test_env.profits\n    \n    # plot\n    train_copy = train_env.data.copy()\n    test_copy = test_env.data.copy()\n    train_copy['act'] = train_acts + [np.nan]\n    train_copy['reward'] = train_rewards + [np.nan]\n    test_copy['act'] = test_acts + [np.nan]\n    test_copy['reward'] = test_rewards + [np.nan]\n    train0 = train_copy[train_copy['act'] == 0]\n    train1 = train_copy[train_copy['act'] == 1]\n    train2 = train_copy[train_copy['act'] == 2]\n    test0 = test_copy[test_copy['act'] == 0]\n    test1 = test_copy[test_copy['act'] == 1]\n    test2 = test_copy[test_copy['act'] == 2]\n    act_color0, act_color1, act_color2 = 'gray', 'cyan', 'magenta'\n\n    data = [\n        Candlestick(x=train0.index, open=train0['Open'], high=train0['High'], low=train0['Low'], close=train0['Close'], increasing=dict(line=dict(color=act_color0)), decreasing=dict(line=dict(color=act_color0))),\n        Candlestick(x=train1.index, open=train1['Open'], high=train1['High'], low=train1['Low'], close=train1['Close'], increasing=dict(line=dict(color=act_color1)), decreasing=dict(line=dict(color=act_color1))),\n        Candlestick(x=train2.index, open=train2['Open'], high=train2['High'], low=train2['Low'], close=train2['Close'], increasing=dict(line=dict(color=act_color2)), decreasing=dict(line=dict(color=act_color2))),\n        Candlestick(x=test0.index, open=test0['Open'], high=test0['High'], low=test0['Low'], close=test0['Close'], increasing=dict(line=dict(color=act_color0)), decreasing=dict(line=dict(color=act_color0))),\n        Candlestick(x=test1.index, open=test1['Open'], high=test1['High'], low=test1['Low'], close=test1['Close'], increasing=dict(line=dict(color=act_color1)), decreasing=dict(line=dict(color=act_color1))),\n        Candlestick(x=test2.index, open=test2['Open'], high=test2['High'], low=test2['Low'], close=test2['Close'], increasing=dict(line=dict(color=act_color2)), decreasing=dict(line=dict(color=act_color2)))\n    ]\n    title = '{}: train s-reward {}, profits {}, test s-reward {}, profits {}'.format(\n        algorithm_name,\n        int(sum(train_rewards)),\n        int(train_profits),\n        int(sum(test_rewards)),\n        int(test_profits)\n    )\n    layout = {\n        'title': title,\n        'showlegend': False,\n         'shapes': [\n             {'x0': date_split, 'x1': date_split, 'y0': 0, 'y1': 1, 'xref': 'x', 'yref': 'paper', 'line': {'color': 'rgb(0,0,0)', 'width': 1}}\n         ],\n        'annotations': [\n            {'x': date_split, 'y': 1.0, 'xref': 'x', 'yref': 'paper', 'showarrow': False, 'xanchor': 'left', 'text': ' test data'},\n            {'x': date_split, 'y': 1.0, 'xref': 'x', 'yref': 'paper', 'showarrow': False, 'xanchor': 'right', 'text': 'train data '}\n        ]\n    }\n    figure = Figure(data=data, layout=layout)\n    iplot(figure)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5dfa244f-6057-4986-9b4e-98af28e03994","_cell_guid":"b2b4e46f-6813-437e-9542-f98d1aa47412","trusted":true},"cell_type":"code","source":"plot_train_test_by_q(Environment1(train), Environment1(test), Q, 'DQN')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f290be27-ab8c-4283-a4b0-ccdf58e0880e","_cell_guid":"7815d133-9faf-4871-b58c-c1c0584f2c90","trusted":true},"cell_type":"code","source":"# Double DQN\n\ndef train_ddqn(env):\n\n    class Q_Network(chainer.Chain):\n\n        def __init__(self, input_size, hidden_size, output_size):\n            super(Q_Network, self).__init__(\n                fc1 = L.Linear(input_size, hidden_size),\n                fc2 = L.Linear(hidden_size, hidden_size),\n                fc3 = L.Linear(hidden_size, output_size)\n            )\n\n        def __call__(self, x):\n            h = F.relu(self.fc1(x))\n            h = F.relu(self.fc2(h))\n            y = self.fc3(h)\n            return y\n\n        def reset(self):\n            self.zerograds()\n\n    Q = Q_Network(input_size=env.history_t+1, hidden_size=100, output_size=3)\n    Q_ast = copy.deepcopy(Q)\n    optimizer = chainer.optimizers.Adam()\n    optimizer.setup(Q)\n\n    epoch_num = 50\n    step_max = len(env.data)-1\n    memory_size = 200\n    batch_size = 50\n    epsilon = 1.0\n    epsilon_decrease = 1e-3\n    epsilon_min = 0.1\n    start_reduce_epsilon = 200\n    train_freq = 10\n    update_q_freq = 20\n    gamma = 0.97\n    show_log_freq = 5\n\n    memory = []\n    total_step = 0\n    total_rewards = []\n    total_losses = []\n\n    start = time.time()\n    for epoch in range(epoch_num):\n\n        pobs = env.reset()\n        step = 0\n        done = False\n        total_reward = 0\n        total_loss = 0\n\n        while not done and step < step_max:\n\n            # select act\n            pact = np.random.randint(3)\n            if np.random.rand() > epsilon:\n                pact = Q(np.array(pobs, dtype=np.float32).reshape(1, -1))\n                pact = np.argmax(pact.data)\n\n            # act\n            obs, reward, done = env.step(pact)\n\n            # add memory\n            memory.append((pobs, pact, reward, obs, done))\n            if len(memory) > memory_size:\n                memory.pop(0)\n\n            # train or update q\n            if len(memory) == memory_size:\n                if total_step % train_freq == 0:\n                    shuffled_memory = np.random.permutation(memory)\n                    memory_idx = range(len(shuffled_memory))\n                    for i in memory_idx[::batch_size]:\n                        batch = np.array(shuffled_memory[i:i+batch_size])\n                        b_pobs = np.array(batch[:, 0].tolist(), dtype=np.float32).reshape(batch_size, -1)\n                        b_pact = np.array(batch[:, 1].tolist(), dtype=np.int32)\n                        b_reward = np.array(batch[:, 2].tolist(), dtype=np.int32)\n                        b_obs = np.array(batch[:, 3].tolist(), dtype=np.float32).reshape(batch_size, -1)\n                        b_done = np.array(batch[:, 4].tolist(), dtype=np.bool)\n\n                        q = Q(b_pobs)\n                        \"\"\" <<< DQN -> Double DQN\n                        maxq = np.max(Q_ast(b_obs).data, axis=1)\n                        === \"\"\"\n                        indices = np.argmax(q.data, axis=1)\n                        maxqs = Q_ast(b_obs).data\n                        \"\"\" >>> \"\"\"\n                        target = copy.deepcopy(q.data)\n                        for j in range(batch_size):\n                            \"\"\" <<< DQN -> Double DQN\n                            target[j, b_pact[j]] = b_reward[j]+gamma*maxq[j]*(not b_done[j])\n                            === \"\"\"\n                            target[j, b_pact[j]] = b_reward[j]+gamma*maxqs[j, indices[j]]*(not b_done[j])\n                            \"\"\" >>> \"\"\"\n                        Q.reset()\n                        loss = F.mean_squared_error(q, target)\n                        total_loss += loss.data\n                        loss.backward()\n                        optimizer.update()\n\n                if total_step % update_q_freq == 0:\n                    Q_ast = copy.deepcopy(Q)\n\n            # epsilon\n            if epsilon > epsilon_min and total_step > start_reduce_epsilon:\n                epsilon -= epsilon_decrease\n\n            # next step\n            total_reward += reward\n            pobs = obs\n            step += 1\n            total_step += 1\n\n        total_rewards.append(total_reward)\n        total_losses.append(total_loss)\n\n        if (epoch+1) % show_log_freq == 0:\n            log_reward = sum(total_rewards[((epoch+1)-show_log_freq):])/show_log_freq\n            log_loss = sum(total_losses[((epoch+1)-show_log_freq):])/show_log_freq\n            elapsed_time = time.time()-start\n            print('\\t'.join(map(str, [epoch+1, epsilon, total_step, log_reward, log_loss, elapsed_time])))\n            start = time.time()\n            \n    return Q, total_losses, total_rewards","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ace26ec9-5356-478a-9fe7-ed57a456d195","_cell_guid":"824e1853-3a7b-4709-b775-204b6869074a","trusted":true},"cell_type":"code","source":"Q, total_losses, total_rewards = train_ddqn(Environment1(train))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"48b0c705-0357-40c1-b89e-aefbff9e6eca","_cell_guid":"bd07774d-61e1-465f-8b99-7522fc91b52f","trusted":true},"cell_type":"code","source":"plot_loss_reward(total_losses, total_rewards)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3202220c-45d7-4bbd-9b35-87d360916b06","_cell_guid":"9f650aeb-e554-482f-acfc-579cd5831544","trusted":true},"cell_type":"code","source":"plot_train_test_by_q(Environment1(train), Environment1(test), Q, 'Double DQN')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c83b0263-d98a-4874-849a-ce53dca23561","_cell_guid":"024edd6b-5792-4d61-8220-533309933755","trusted":true},"cell_type":"code","source":"# Dueling Double DQN\n\ndef train_dddqn(env):\n\n    \"\"\" <<< Double DQN -> Dueling Double DQN\n    class Q_Network(chainer.Chain):\n\n        def __init__(self, input_size, hidden_size, output_size):\n            super(Q_Network, self).__init__(\n                fc1 = L.Linear(input_size, hidden_size),\n                fc2 = L.Linear(hidden_size, hidden_size),\n                fc3 = L.Linear(hidden_size, output_size)\n            )\n\n        def __call__(self, x):\n            h = F.relu(self.fc1(x))\n            h = F.relu(self.fc2(h))\n            y = self.fc3(h)\n            return y\n\n        def reset(self):\n            self.zerograds()\n    === \"\"\"\n    class Q_Network(chainer.Chain):\n\n        def __init__(self, input_size, hidden_size, output_size):\n            super(Q_Network, self).__init__(\n                fc1 = L.Linear(input_size, hidden_size),\n                fc2 = L.Linear(hidden_size, hidden_size),\n                fc3 = L.Linear(hidden_size, hidden_size//2),\n                fc4 = L.Linear(hidden_size, hidden_size//2),\n                state_value = L.Linear(hidden_size//2, 1),\n                advantage_value = L.Linear(hidden_size//2, output_size)\n            )\n            self.input_size = input_size\n            self.hidden_size = hidden_size\n            self.output_size = output_size\n\n        def __call__(self, x):\n            h = F.relu(self.fc1(x))\n            h = F.relu(self.fc2(h))\n            hs = F.relu(self.fc3(h))\n            ha = F.relu(self.fc4(h))\n            state_value = self.state_value(hs)\n            advantage_value = self.advantage_value(ha)\n            advantage_mean = (F.sum(advantage_value, axis=1)/float(self.output_size)).reshape(-1, 1)\n            q_value = F.concat([state_value for _ in range(self.output_size)], axis=1) + (advantage_value - F.concat([advantage_mean for _ in range(self.output_size)], axis=1))\n            return q_value\n\n        def reset(self):\n            self.zerograds()\n    \"\"\" >>> \"\"\"\n\n    Q = Q_Network(input_size=env.history_t+1, hidden_size=100, output_size=3)\n    Q_ast = copy.deepcopy(Q)\n    optimizer = chainer.optimizers.Adam()\n    optimizer.setup(Q)\n\n    epoch_num = 50\n    step_max = len(env.data)-1\n    memory_size = 200\n    batch_size = 50\n    epsilon = 1.0\n    epsilon_decrease = 1e-3\n    epsilon_min = 0.1\n    start_reduce_epsilon = 200\n    train_freq = 10\n    update_q_freq = 20\n    gamma = 0.97\n    show_log_freq = 5\n\n    memory = []\n    total_step = 0\n    total_rewards = []\n    total_losses = []\n\n    start = time.time()\n    for epoch in range(epoch_num):\n\n        pobs = env.reset()\n        step = 0\n        done = False\n        total_reward = 0\n        total_loss = 0\n\n        while not done and step < step_max:\n\n            # select act\n            pact = np.random.randint(3)\n            if np.random.rand() > epsilon:\n                pact = Q(np.array(pobs, dtype=np.float32).reshape(1, -1))\n                pact = np.argmax(pact.data)\n\n            # act\n            obs, reward, done = env.step(pact)\n\n            # add memory\n            memory.append((pobs, pact, reward, obs, done))\n            if len(memory) > memory_size:\n                memory.pop(0)\n\n            # train or update q\n            if len(memory) == memory_size:\n                if total_step % train_freq == 0:\n                    shuffled_memory = np.random.permutation(memory)\n                    memory_idx = range(len(shuffled_memory))\n                    for i in memory_idx[::batch_size]:\n                        batch = np.array(shuffled_memory[i:i+batch_size])\n                        b_pobs = np.array(batch[:, 0].tolist(), dtype=np.float32).reshape(batch_size, -1)\n                        b_pact = np.array(batch[:, 1].tolist(), dtype=np.int32)\n                        b_reward = np.array(batch[:, 2].tolist(), dtype=np.int32)\n                        b_obs = np.array(batch[:, 3].tolist(), dtype=np.float32).reshape(batch_size, -1)\n                        b_done = np.array(batch[:, 4].tolist(), dtype=np.bool)\n\n                        q = Q(b_pobs)\n                        \"\"\" <<< DQN -> Double DQN\n                        maxq = np.max(Q_ast(b_obs).data, axis=1)\n                        === \"\"\"\n                        indices = np.argmax(q.data, axis=1)\n                        maxqs = Q_ast(b_obs).data\n                        \"\"\" >>> \"\"\"\n                        target = copy.deepcopy(q.data)\n                        for j in range(batch_size):\n                            \"\"\" <<< DQN -> Double DQN\n                            target[j, b_pact[j]] = b_reward[j]+gamma*maxq[j]*(not b_done[j])\n                            === \"\"\"\n                            target[j, b_pact[j]] = b_reward[j]+gamma*maxqs[j, indices[j]]*(not b_done[j])\n                            \"\"\" >>> \"\"\"\n                        Q.reset()\n                        loss = F.mean_squared_error(q, target)\n                        total_loss += loss.data\n                        loss.backward()\n                        optimizer.update()\n\n                if total_step % update_q_freq == 0:\n                    Q_ast = copy.deepcopy(Q)\n\n            # epsilon\n            if epsilon > epsilon_min and total_step > start_reduce_epsilon:\n                epsilon -= epsilon_decrease\n\n            # next step\n            total_reward += reward\n            pobs = obs\n            step += 1\n            total_step += 1\n\n        total_rewards.append(total_reward)\n        total_losses.append(total_loss)\n\n        if (epoch+1) % show_log_freq == 0:\n            log_reward = sum(total_rewards[((epoch+1)-show_log_freq):])/show_log_freq\n            log_loss = sum(total_losses[((epoch+1)-show_log_freq):])/show_log_freq\n            elapsed_time = time.time()-start\n            print('\\t'.join(map(str, [epoch+1, epsilon, total_step, log_reward, log_loss, elapsed_time])))\n            start = time.time()\n            \n    return Q, total_losses, total_rewards","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bb49d5dd-b171-4747-8c33-03edd3e791dd","_cell_guid":"7c89ee87-44aa-4cb2-b069-7395554a9295","trusted":true},"cell_type":"code","source":"Q, total_losses, total_rewards = train_dddqn(Environment1(train))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"133b7727-1d58-4f0d-856a-dc7e655f4c62","_cell_guid":"bb895481-975a-404b-901f-042f86c2739f","trusted":true},"cell_type":"code","source":"plot_loss_reward(total_losses, total_rewards)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ce34c25e-6dec-4780-ba3b-ec5b79a2bd19","_cell_guid":"6809bdff-36f3-4ca3-8c32-1a34c03f1f68","trusted":true},"cell_type":"code","source":"plot_train_test_by_q(Environment1(train), Environment1(test), Q, 'Dueling Double DQN')\n\n\nlgtrain, lgval = lgb.Dataset(Xt, Yt[:,0]), lgb.Dataset(Xv, Yv[:,0])\nlgbmodel = lgb.train(params, lgtrain, 2000, valid_sets=[lgtrain, lgval], early_stopping_rounds=100, verbose_eval=200)\n\n\n############################################################\nprint(\"generating predictions...\")\npreddays = env.get_prediction_days()\nfor marketdf, newsdf, predtemplatedf in preddays:\n    cdf = prepare_data(marketdf, newsdf)\n    Xp = cdf[traincols].fillna(0).values\n    preds = lgbmodel.predict(Xp, num_iteration=lgbmodel.best_iteration) * 2 - 1\n    predsdf = pd.DataFrame({'ast':cdf['assetCode'],'conf':preds})\n    predtemplatedf['confidenceValue'][predtemplatedf['assetCode'].isin(predsdf.ast)] = predsdf['conf'].values\n    env.predict(predtemplatedf)\n\nenv.write_submission_file()","execution_count":null,"outputs":[]}],"metadata":{"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}