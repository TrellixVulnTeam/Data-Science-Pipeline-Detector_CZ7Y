{"cells":[{"metadata":{"_uuid":"5ffb21374c7cf4b98e7239045ef9bf312effee25"},"cell_type":"markdown","source":"# Wall_Street_Winners Competition Kernel\n## Introduction\nThis Kernel is base on the Kaggle Official Getting Started Kernel and a kernel developed by Bruno G. do Amaral (\"A simple model - using the market and news data\"). The  competition attempts to predict how stocks will change based on the market state and news articles.  You will loop through a long series of trading days; for each day, you'll receive an updated state of the market, and a series of news articles which were published since the last trading day, along with impacted stocks and sentiment analysis.  You'll use this information to predict whether each stock will have increased or decreased ten trading days into the future.  Once you make these predictions, you can move on to the next trading day. \n\nThis competition is different from most Kaggle Competitions in that:\n* You can only submit from Kaggle Kernels, and you may not use other data sources, GPU, or internet access.\n* This is a **two-stage competition**.  In Stage One you can edit your Kernels and improve your model, where Public Leaderboard scores are based on their predictions relative to past market data.  At the beginning of Stage Two, your Kernels are locked, and we will re-run your Kernels over the next six months, scoring them based on their predictions relative to live data as those six months unfold.\n* You must use our custom **`kaggle.competitions.twosigmanews`** Python module.  The purpose of this module is to control the flow of information to ensure that you are not using future data to make predictions for the current trading day.\n\n## Based on the starter kernel, this kernel will use the **`twosigmanews`** module to get the training data, get test features and make predictions, and write the submission file.\n\nfrom kaggle.competitions import twosigmanews\nenv = twosigmanews.make_env()\n\n(market_train_df, news_train_df) = env.get_training_data()\ntrain_my_model(market_train_df, news_train_df)\n\nfor (market_obs_df, news_obs_df, predictions_template_df) in env.get_prediction_days():\n  predictions_df = make_my_predictions(market_obs_df, news_obs_df, predictions_template_df)\n  env.predict(predictions_df)\n  \nenv.write_submission_file()\n```\nNote that `train_my_model` and `make_my_predictions` are functions you need to write for the above example to work.\n"},{"metadata":{"_uuid":"3f20e33e0ab0b78ba4f17a966ea5eafb0d43e186"},"cell_type":"markdown","source":"## **Contents**   \n[1. Import Modules & Create Environment](#1)   \n[2. Examine Data Sets](#2)  \n&nbsp;&nbsp;&nbsp;&nbsp; [2.1 Basic dataframe info structure](#2.1)  \n&nbsp;&nbsp;&nbsp;&nbsp; [2.2 Preview 5 rows of each data frame](#2.2)  \n&nbsp;&nbsp;&nbsp;&nbsp; [2.3  Counts of unique values in market_train_df](#2.3)  \n&nbsp;&nbsp;&nbsp;&nbsp; [2.4  Counts of unique values in news_train_df](#2.4)  \n&nbsp;&nbsp;&nbsp;&nbsp; [2.5  Distribution graphs of market_train_df numerics ](#2.5)  \n[3. Create and test 1st-run data set](#1)  \n&nbsp;&nbsp;&nbsp;&nbsp; [3.1 Create aggregated list of numberic news data](#3.1)  \n&nbsp;&nbsp;&nbsp;&nbsp; [3.2 Prep market data and join with news data](#3.2)  \n&nbsp;&nbsp;&nbsp;&nbsp; [3.3 Create lgb datasets](#3.3)  \n&nbsp;&nbsp;&nbsp;&nbsp; [3.4 Set lgb parameters](#3.4)  \n&nbsp;&nbsp;&nbsp;&nbsp; [3.5 Fit lgb model and plot sigma score](#3.5)  \n&nbsp;&nbsp;&nbsp;&nbsp; [3.6 Plot feature importance](#3.6)  \n[4. Train full lgb model & write submission file](#4)  \n\n## <a id=\"1\">1. Import Module & Create Environment</a>  "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"####JHL? Should we set this to false and run a model using all the data to submit? Interesting to see how it might improve our rankings\n\ntoy=True      # if toy=true, rows in dataframe will be reduced for easy testing\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport scipy.stats\n\n# data visualization\nimport seaborn as sns\n%matplotlib inline\nfrom matplotlib import pyplot as plt\nfrom matplotlib import style\n\n# Algorithms\nfrom sklearn import linear_model\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.model_selection import train_test_split\nimport lightgbm as lgb\nfrom itertools import chain\n\n## JHL's function to Hot-Code a categorical variable_\n    # Takes as parameters 1) a dataframe 2) a string variable with the column name to recode\n    # Leaves in tack the initial variable that was recoded\ndef HotC(dframe,col):   # Function to Hot-Code a categorical variable\n    if not(isinstance(dframe,pd.DataFrame)):\n        print('!!ERROR!! The first variable in the HotC function must be a dataframe')\n        return\n    if not(isinstance(col,str)):\n        print('!!ERROR!! The second variable in the HotC function must be a string representing a column in the dataframe')\n        return\n    #df2=pd.DataFrame(dframe[col].str.get_dummies())\n    df2=pd.get_dummies(dframe[col],prefix=col)\n    df3=pd.concat([dframe,df2],axis=1)\n    return df3\n\n# Inport competion data\nfrom kaggle.competitions import twosigmanews\n# You can only call make_env() once, so don't lose it!\n# ours is <kaggle.competitions.twosigmanews.env.TwoSigmaNewsEnv object at 0x7ff474b06278>\n# however, it looks like you have to redefine it every time you start up a new session?\n# I've commented out the next command as the environment has already been created & saved\nif 'env' not in globals():\n    env = twosigmanews.make_env()\n# Set up dataframe--use a sample for memory reasons if toy=True\n(market_train_df, news_train_df) = env.get_training_data()\nif toy:\n    market_train_df = market_train_df.tail(500_000)\n    news_train_df = news_train_df.tail(800_000)\nelse:\n    market_train_df = market_train_df.tail(3_000_000)\n    news_train_df = news_train_df.tail(6_000_000)\nprint ('env value:', env)\nprint('Environment set & Data imported!')\nprint('Dataframe shapes of market_train_df.shape, news_train_df.shape')\nprint(market_train_df.shape, news_train_df.shape)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fd0ca3a82e7c1ec4bb33b1adfedb8c70361f5d3d"},"cell_type":"markdown","source":"## <a id=\"2\">2. Examine Data Sets</a>  \n### &nbsp;&nbsp;  <a id=\"2.1\">2.1  Basic data frame info structure </a>"},{"metadata":{"trusted":true,"_uuid":"2da2b0f9a655bb857700a8e76dec44a4dd7b223c"},"cell_type":"code","source":"#### Right now I don't believe any 'Validation' is being done. Should be take out 20% of the data to create a validation set?\n# List basic structure of each data frame\nprint ('<STRUCTURE: market_train_df.info>')\nprint(market_train_df.info())\nprint ('\\n','='*80,'\\n','<STRUCTURE:news_train_df.info>')\nprint(news_train_df.info())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"167e7e14ac0590b7ff984199387268f9678453d1"},"cell_type":"markdown","source":"### &nbsp;&nbsp;  <a id=\"2.2\">2.2  Preview 5 rows of each data frame </a>"},{"metadata":{"trusted":true,"_uuid":"cc00dff3f1ac14f6e00b879941f72caeeb85c45c"},"cell_type":"code","source":"## Basic Data Frame Description\n# First widen display\npd.options.display.max_columns=35   ## Force number of coluns to show\npd.options.display.max_rows=1000     ## Force number of rows to show\n# Now show 5 rows of all data sets\nprint ('<CONTENT OF market_train_df>')\nprint(market_train_df.head(5))\nprint ('______________________________________________________________')\nprint ('\\n<CONTENT OF market_train_df.tail>')\nprint (market_train_df.tail(5))\nprint ('______________________________________________________________')\nprint('\\n<CONTENT OF news_train_df>')\nprint(news_train_df.head(10))\nprint ('_____________________________________________________________')\nprint('\\nCONTENT OF <news_train_df.tail>')\nprint(news_train_df.tail(10))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"41673154ecaa70bb434ac1cd70fdb72ef4a70dfb"},"cell_type":"markdown","source":"### &nbsp;&nbsp;  <a id=\"2.3\">2.3  Counts of unique values in market_train_df </a>"},{"metadata":{"trusted":true,"_uuid":"09c47de665713a4029b424a17031c9904e8af330"},"cell_type":"code","source":"## Value Counts for market_train_df\nprint('Value Counts for market_train_df Features','\\n')\nfor elem in market_train_df.columns.values:\n    print(elem)\n    if market_train_df[elem].nunique()<8:\n        if market_train_df[elem].dtype != np.object:\n            if market_train_df[elem].nunique()>7:\n                print(market_train_df[elem].value_counts(bins=8),'\\nNaN Count:',market_train_df[elem].isna().sum())\n            else:\n                print(market_train_df[elem].value_counts(),'\\nNaN Count:',market_train_df[elem].isna().sum())\n        else:\n            print(market_train_df[elem].value_counts(),'\\nNaN Count:',market_train_df[elem].isna().sum())\n    else:\n        print('Unique values:',market_train_df[elem].nunique(),'\\nNaN Count:',market_train_df[elem].isna().sum())\n    print('\\n')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9abff2db13cba42950f9301892fca70afa823d23"},"cell_type":"markdown","source":"### &nbsp;&nbsp;  <a id=\"2.4\">2.4  Counts of unique values in news_train_df </a>"},{"metadata":{"trusted":true,"_uuid":"96bbd5dbde18ba410e9d515126531038acc933b3"},"cell_type":"code","source":"## Value Counts for news_train_df\nprint('Value Counts for news_train_df Features','\\n')\nfor elem in news_train_df.columns.values:\n    print(elem)\n    if news_train_df[elem].nunique()<8:\n        if news_train_df[elem].dtype != np.object:\n            if news_train_df[elem].nunique()>7:\n                print(news_train_df[elem].value_counts(bins=8),'\\nNaN Count:',news_train_df[elem].isna().sum())\n            else:\n                print(news_train_df[elem].value_counts(),'\\nNaN Count:',news_train_df[elem].isna().sum())\n        else:\n            print(news_train_df[elem].value_counts(),'\\nNaN Count:',news_train_df[elem].isna().sum())\n    else:\n        print('Unique values:',news_train_df[elem].nunique(),'\\nNaN Count:',news_train_df[elem].isna().sum())\n    print('\\n')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ae6998e3cc51a5021b12fe90e5e0eebf903a64f6"},"cell_type":"markdown","source":"### &nbsp;&nbsp;  <a id=\"2.5\">2.5  Distribution graphs of market_train_df numerics </a>"},{"metadata":{"trusted":true,"_uuid":"70a8ac91e84e0177cb4008f3c4d273fe62f51979"},"cell_type":"code","source":"#### JHL: Tried a few different bin sizes, but some graphs not looking right. How to fix?\nmarket_train_df.hist( bins = 1000, figsize =( 16,15))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d210670a431af21fc06230f2997e414b40b0640c"},"cell_type":"markdown","source":"## &nbsp;&nbsp;  <a id=\"3.\">3  Create and test 1st-run data set </a>  \n### &nbsp;&nbsp;  <a id=\"3.1\">3.1  Create aggregated list of numberic news data </a>  "},{"metadata":{"trusted":true,"_uuid":"e94f0a6a16496e8c7baa95ec121e17669abc5d7b"},"cell_type":"code","source":"####It's unclear to me what role these aggregates play and how the factor into the model?\n# Prep list of news columns that are numeric\nnews_cols_agg = {\n    'urgency': ['min', 'count'],\n    'takeSequence': ['max'],\n    'bodySize': ['min', 'max', 'mean', 'std'],\n    'wordCount': ['min', 'max', 'mean', 'std'],\n    'sentenceCount': ['min', 'max', 'mean', 'std'],\n    'companyCount': ['min', 'max', 'mean', 'std'],\n    'marketCommentary': ['min', 'max', 'mean', 'std'],\n    'relevance': ['min', 'max', 'mean', 'std'],\n    'sentimentNegative': ['min', 'max', 'mean', 'std'],\n    'sentimentNeutral': ['min', 'max', 'mean', 'std'],\n    'sentimentPositive': ['min', 'max', 'mean', 'std'],\n    'sentimentWordCount': ['min', 'max', 'mean', 'std'],\n    'noveltyCount12H': ['min', 'max', 'mean', 'std'],\n    'noveltyCount24H': ['min', 'max', 'mean', 'std'],\n    'noveltyCount3D': ['min', 'max', 'mean', 'std'],\n    'noveltyCount5D': ['min', 'max', 'mean', 'std'],\n    'noveltyCount7D': ['min', 'max', 'mean', 'std'],\n    'volumeCounts12H': ['min', 'max', 'mean', 'std'],\n    'volumeCounts24H': ['min', 'max', 'mean', 'std'],\n    'volumeCounts3D': ['min', 'max', 'mean', 'std'],\n    'volumeCounts5D': ['min', 'max', 'mean', 'std'],\n    'volumeCounts7D': ['min', 'max', 'mean', 'std']\n}\nprint(news_cols_agg)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c95281219c3ed354315b22bf33e37fe6c71305f3"},"cell_type":"markdown","source":"### &nbsp;&nbsp;  <a id=\"3.2\">3.2  Prep mareket data and join with news data </a>  "},{"metadata":{"_uuid":"9cd8317a5e52180b592ee2abc1d2177214642a3c","trusted":true},"cell_type":"code","source":"\n## Functions to use in combining data sets\n\ndef join_market_news(market_train_df, news_train_df):\n    # Fix asset codes (str -> list)\n    news_train_df['assetCodes'] = news_train_df['assetCodes'].str.findall(f\"'([\\w\\./]+)'\")    \n    \n    # Expand assetCodes\n    assetCodes_expanded = list(chain(*news_train_df['assetCodes']))\n    assetCodes_index = news_train_df.index.repeat( news_train_df['assetCodes'].apply(len) )\n\n    assert len(assetCodes_index) == len(assetCodes_expanded)\n    df_assetCodes = pd.DataFrame({'level_0': assetCodes_index, 'assetCode': assetCodes_expanded})\n\n    # Create expandaded news (will repeat every assetCodes' row)\n    news_cols = ['time', 'assetCodes'] + sorted(news_cols_agg.keys())\n    news_train_df_expanded = pd.merge(df_assetCodes, news_train_df[news_cols], left_on='level_0', right_index=True, suffixes=(['','_old']))\n\n    # Free memory\n    del news_train_df, df_assetCodes\n\n    # Aggregate numerical news features\n    news_train_df_aggregated = news_train_df_expanded.groupby(['time', 'assetCode']).agg(news_cols_agg)\n    \n    # Free memory\n    del news_train_df_expanded\n\n    # Convert to float32 to save memory\n    news_train_df_aggregated = news_train_df_aggregated.apply(np.float32)\n\n    # Flat columns\n    news_train_df_aggregated.columns = ['_'.join(col).strip() for col in news_train_df_aggregated.columns.values]\n\n    # Join with train\n    market_train_df = market_train_df.join(news_train_df_aggregated, on=['time', 'assetCode'])\n\n    # Free memory\n    del news_train_df_aggregated\n    \n    return market_train_df\n\nprint('Functions created to combine market and news data')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a3f2197ed790f1aff1356a6954575fde976a4935"},"cell_type":"code","source":"def get_xy(market_train_df, news_train_df, le=None):\n    x, le = get_x(market_train_df, news_train_df)\n    y = market_train_df['returnsOpenNextMktres10'].clip(-1, 1)\n    return x, y, le\n\n\ndef label_encode(series, min_count):\n    vc = series.value_counts()\n    le = {c:i for i, c in enumerate(vc.index[vc >= min_count])}\n    return le\n\n\n  #### What is le and what are these lines of code doing? Seems to be a 'label encoder'. What's that?\ndef get_x(market_train_df, news_train_df, le=None):\n    # Split date into before and after 22h (the time used in train data)\n    # E.g: 2007-03-07 23:26:39+00:00 -> 2007-03-08 00:00:00+00:00 (next day)\n    #      2009-02-25 21:00:50+00:00 -> 2009-02-25 00:00:00+00:00 (current day)\n    news_train_df['time'] = (news_train_df['time'] - np.timedelta64(22,'h')).dt.ceil('1D')\n\n    # Round time of market_train_df to 0h of curret day\n    market_train_df['time'] = market_train_df['time'].dt.floor('1D')\n\n    # Join market and news\n    x = join_market_news(market_train_df, news_train_df)\n    \n    # If not label-encoder... encode assetCode\n    if le is None:\n        le_assetCode = label_encode(x['assetCode'], min_count=10)\n        le_assetName = label_encode(x['assetName'], min_count=5)\n    else:\n        # 'unpack' label encoders\n        le_assetCode, le_assetName = le\n        \n    x['assetCode'] = x['assetCode'].map(le_assetCode).fillna(-1).astype(int)\n    x['assetName'] = x['assetName'].map(le_assetName).fillna(-1).astype(int)\n    \n    try:\n        x.drop(columns=['returnsOpenNextMktres10'], inplace=True)\n    except:\n        pass\n    try:\n        x.drop(columns=['universe'], inplace=True)\n    except:\n        pass\n    x['dayofweek'], x['month'] = x.time.dt.dayofweek, x.time.dt.month\n    x.drop(columns='time', inplace=True)\n#    x.fillna(-1000,inplace=True)\n\n    # Fix some mixed-type columns\n    for bogus_col in ['marketCommentary_min', 'marketCommentary_max']:\n        x[bogus_col] = x[bogus_col].astype(float)\n    \n    return x, (le_assetCode, le_assetName)\n\nprint('Additional functions created to help on join')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ca72b7003f24f4aa0c4afe25b600aae31abd64d5"},"cell_type":"code","source":"%%time\n####? What does a line of these variables look like?\n# This will take some time...\nX, y, le = get_xy(market_train_df, news_train_df)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6e1bbd222711f75ad73f45767bfbecb75b0ba458"},"cell_type":"code","source":"# Save universe data for latter use\nuniverse = market_train_df['universe']\ntime = market_train_df['time']\n\n# Free memory\ndel market_train_df, news_train_df\nX_ = X\nprint('universe saved')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6dd4990baa14566822b3ce965c33404a77e9b371"},"cell_type":"code","source":"# Keep only text columns\nX = X_#.iloc[:, X.columns.get_loc('urgency_min'):X.columns.get_loc('dayofweek')]\nX.tail()\n\nn_train = int(X.shape[0] * 0.8)\n\nX_train, y_train = X.iloc[:n_train], y.iloc[:n_train]\nX_valid, y_valid = X.iloc[n_train:], y.iloc[n_train:]\n\nprint('Sampple of X_train rows')\nprint(X_train.head(5))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"26b6b5e88172751b21f0e23e17cead4521de07f1"},"cell_type":"code","source":"# For valid data, keep only those with universe > 0. This will help calculate the metric\nu_valid = (universe.iloc[n_train:] > 0)\nt_valid = time.iloc[n_train:]\n\nX_valid = X_valid[u_valid]\ny_valid = y_valid[u_valid]\nt_valid = t_valid[u_valid]\ndel u_valid","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d9541dda09356e9c801743ff6ca81179db4d1be6"},"cell_type":"markdown","source":"### &nbsp;&nbsp;  <a id=\"3.3\">3.3  Create Lgb datasets </a>  "},{"metadata":{"trusted":true,"_uuid":"ed46f0cd94ff4430a805d9e41fbc0325c29f4ec3"},"cell_type":"code","source":"# Create lgb datasets\ntrain_cols = X.columns.tolist()\ncategorical_cols = [] # ['assetCode', 'assetName', 'dayofweek', 'month']\n# zzz=train_cols\n# train_cols = ['volume','close','open','returnsClosePrevMktres1','returnsOpenPrevMktres1','returnsClosePrevMktres10','returnsOpenPrevMktres10','urgency_min','ugency_count','sentimentNegative_mean','sentimentPositive_mean','noveltyCount12H_mean','volumeCounts12H_mean','dayofweek','month']\n# print ('train_cols'+train_cols)\n\n# Note: y data is expected to be a pandas Series, as we will use its group_by function in `sigma_score`\ndtrain = lgb.Dataset(X_train.values, y_train, feature_name=train_cols, categorical_feature=categorical_cols, free_raw_data=False)\ndvalid = lgb.Dataset(X_valid.values, y_valid, feature_name=train_cols, categorical_feature=categorical_cols, free_raw_data=False)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"97ba0e81b3194a14a31114703041006498d7eefa"},"cell_type":"markdown","source":"### &nbsp;&nbsp;  <a id=\"3.4\">3.4  Set lgb parameters </a>  "},{"metadata":{"trusted":true,"_uuid":"f31e6451d1deb478f1c236201adbc540b9208721"},"cell_type":"code","source":"# We will 'inject' an extra parameter in order to have access to df_valid['time'] inside sigma_score without globals\n\ndvalid.params = {\n    'extra_time': t_valid.factorize()[0]\n}\n\nlgb_params = dict(\n    objective = 'regression_l1',\n    learning_rate = 0.1,\n    num_leaves = 127,\n    max_depth = -1,\n#     min_data_in_leaf = 1000,\n#     min_sum_hessian_in_leaf = 10,\n    bagging_fraction = 0.75,\n    bagging_freq = 2,\n    feature_fraction = 0.5,\n    lambda_l1 = 0.0,\n    lambda_l2 = 1.0,\n    metric = 'None', # This will ignore the loss objetive and use sigma_score instead,\n    seed = 42 # Change for better luck! :)\n)\n\ndef sigma_score(preds, valid_data):\n    df_time = valid_data.params['extra_time']\n    labels = valid_data.get_label()\n    \n#    assert len(labels) == len(df_time)\n\n    x_t = preds * labels #  * df_valid['universe'] -> Here we take out the 'universe' term because we already keep only those equals to 1.\n    \n    # Here we take advantage of the fact that `labels` (used to calculate `x_t`)\n    # is a pd.Series and call `group_by`\n    x_t_sum = x_t.groupby(df_time).sum()\n    score = x_t_sum.mean() / x_t_sum.std()\n\n    return 'sigma_score', score, True\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"22ffd03b63a3a9fb688cc437682ddee3d03f17a7"},"cell_type":"markdown","source":"### &nbsp;&nbsp;  <a id=\"3.5\">3.5  Fit lgb model and plot sigma score </a>  "},{"metadata":{"trusted":true,"_uuid":"7e8f6e009bafbc8a816adf4ed00384bfa1b7ff9c"},"cell_type":"code","source":"# Fit model\nevals_result = {}\nm = lgb.train(lgb_params, dtrain, num_boost_round=1000, valid_sets=(dvalid,), valid_names=('valid',), verbose_eval=25,\n              early_stopping_rounds=100, feval=sigma_score, evals_result=evals_result)\ndf_result = pd.DataFrame(evals_result['valid'])\n # Plot sigma\nax = df_result.plot(figsize=(12, 8))\nax.scatter(df_result['sigma_score'].idxmax(), df_result['sigma_score'].max(), marker='+', color='red')\nnum_boost_round, valid_score = df_result['sigma_score'].idxmax()+1, df_result['sigma_score'].max()\nprint(lgb_params)\nprint(f'Best score was {valid_score:.5f} on round {num_boost_round}')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"332a19beed036a023bf83313d286165704f78e6b"},"cell_type":"markdown","source":"\n"},{"metadata":{"_uuid":"89caea476a9076b417658c2aea716d77cfb75072"},"cell_type":"markdown","source":"### &nbsp;&nbsp;  <a id=\"3.6\">3.6  Plot feature importance </a> "},{"metadata":{"_uuid":"ff62c167b459c5895383fb05fd9260c14be8c1b8","trusted":true},"cell_type":"code","source":"# plot importance figures\nfig, ax = plt.subplots(1, 2, figsize=(14, 14))\nlgb.plot_importance(m, ax=ax[0])\nlgb.plot_importance(m, ax=ax[1], importance_type='gain')\nfig.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"73835026c8910da11a4f9a6164fd367286ba75cd"},"cell_type":"markdown","source":"## &nbsp;&nbsp;  <a id=\"4\">4 Train full lgb model & write submission file </a> "},{"metadata":{"trusted":true,"_uuid":"3c97722eb097c366238e8c7fa5e0496945e60cfc"},"cell_type":"code","source":"# Train full model with num_boost_round found in validation\ndtrain_full = lgb.Dataset(X, y, feature_name=train_cols, categorical_feature=categorical_cols)\n\nmodel = lgb.train(lgb_params, dtrain, num_boost_round=num_boost_round)\n\n# generated predictions\ndef make_predictions(predictions_template_df, market_obs_df, news_obs_df, le):\n    x, _ = get_x(market_obs_df, news_obs_df, le)\n    predictions_template_df.confidenceValue = np.clip(model.predict(x), -1, 1)\n    \ndays = env.get_prediction_days()\n\nfor (market_obs_df, news_obs_df, predictions_template_df) in days:\n    make_predictions(predictions_template_df, market_obs_df, news_obs_df, le)\n    env.predict(predictions_template_df)\nprint('Model fit!')\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5619102fa7f88e66fd1c9d0c083c555df5f9094d"},"cell_type":"code","source":"# Write submission file\nenv.write_submission_file()\nprint('Submission file written')\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}