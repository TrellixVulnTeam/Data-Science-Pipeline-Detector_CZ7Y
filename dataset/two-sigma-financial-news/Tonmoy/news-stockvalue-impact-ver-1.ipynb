{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f722fc7ce1dd7f6ac36bfede982e423b9f8a00ab"},"cell_type":"markdown","source":"Beginning...\n\n\"From every dingy basement on every dingy street<br>\nEvery dragging hand clap over every dragging beat<br>\nThat's just the beat of time the beat that must go on<br>\nIf you've been trying for years we already heard your song\"<br>\n                                                                - lyrics of the song 'Death or Glory', The Clash"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"scrolled":false},"cell_type":"code","source":"from kaggle.competitions import twosigmanews\nenv = twosigmanews.make_env()\n\n(market_train_df, news_train_df) = env.get_training_data()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9b8794e36c0e73dcbc246d637045af3cba228e66"},"cell_type":"code","source":"#beginning data exploration\n\n#check columns\ncol_list_1 = market_train_df.columns\ncol_list_2 = news_train_df.columns\n\nprint(\"market_train has {} rows and {} columns. \\nnews_train has {} rows and {} columns.\"\\\n      .format(market_train_df.shape[0], market_train_df.shape[1], news_train_df.shape[0], news_train_df.shape[1]))\n\ncol_list_1 = [i for i in market_train_df.columns.values]\ncol_list_2 = [i for i in news_train_df.columns.values]\nprint(\"Columns of market_train_df:\\n\", \",\\n\".join(a for a in col_list_1), \\\n      \"\\n\\n\\nColumns of news_train_df:\\n\",\",\\n\".join(a for a in col_list_2) ,\"\\n\\n\")\n\n#check if the particular column has NaN/empty data\nmarket_cols_hasnan = {column:market_train_df[column].isnull().any() for column in col_list_1}\nnews_cols_hasnan = {column:news_train_df[column].isnull().any() for column in col_list_2}\ncols_null_market = [i for i in market_cols_hasnan.keys() if market_cols_hasnan.get(i)]\ncols_null_news = [i for i in news_cols_hasnan.keys() if news_cols_hasnan.get(i)]\nprint(\"Columns with NaN in market_train_df:\\n\",cols_null_market,\"\\n\\t There are \",len(cols_null_market),\" columns with null.\")\nprint(\"Columns with NaN in news_train_df:\\n\",cols_null_news,\"\\n\\t There are \",len(cols_null_news),\" columns with null.\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"eb4123a3d7e91d9ce796fc332b25d1ea5ea77c49"},"cell_type":"code","source":"#print a single row of each for visual familiarity of the data\nprint(\"First row of market_train_df\\n\",market_train_df.iloc[0])\nprint(\"First row of news_train_df\\n\",news_train_df.iloc[0])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5813f5a317e6f1de0d1046a5e8e5f06798694173"},"cell_type":"code","source":"#makes copies of the 2 dataframes \na_market_train_df = market_train_df.copy()\na_news_train_df = news_train_df.copy()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"6b96fd363b60074ed42fa3535eecc02b20bfd879"},"cell_type":"code","source":"\ndef list_cols_type(df, verbose=True): #list all column names with their datatypes\n    s = set(zip([e for e in df.columns.values],[df[i].dtype.name for i in df.columns.values ]))\n    for a in s:\n        print(a)\n    return s \n\nprint(\"market_dataframe_follows:\")\nlist_cols_type(a_market_train_df)\nprint(\"\\nnews_dataframe_follows:\")\nlist_cols_type(a_news_train_df)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6bb650c8bd00fecf782bf7a562c83367b6db1a8a"},"cell_type":"code","source":"def list_categorical_columns(df, verbose=True):\n    #names of different dtypes that may be \"categorical\" or \"categoricalish\"(sorry)\n    list_categorical_dtype_names = ['datetime64[ns, UTC]', 'category', 'object']\n    cat_col_names = [(i, df[i].dtype.name) for i in df.columns.values if df[i].dtype.name in list_categorical_dtype_names]\n    if(verbose):\n        print(cat_col_names)\n    return cat_col_names\n\nprint(\"market df follows:\")\nlist_categorical_columns(a_market_train_df)\nprint(\"\\nnews df follows:\")\nlist_categorical_columns(a_news_train_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"30222e34b308105e328b3ccf18e6e0ca2134361e"},"cell_type":"code","source":"#check the null values per column in the 2 dataframes\n#this is to help decide if the column should be dropped or nulls autopopulated by the proxy value\n\n#type(a_market_train_df.groupby('time')) #>>pandas.core.groupby.groupby.DataFrameGroupBy\n\ndef list_cols_with_null(df):\n    A = [a for a in df.columns.values if df[a].isnull().sum()>0]\n    #TODO: Try to do the following using dictionary comprehension\n    B = {} #Dictionary holding (column_name:percentage of null values)\n    for a in A:\n        num_null = df[a].isnull().sum()\n        total_count = len(df[a])\n        b = float(num_null/total_count)\n        B[a] = b\n    return B\n\nA = list_cols_with_null(a_market_train_df)\nB = list_cols_with_null(a_news_train_df)\n\nprint(\"market_df column null percentages:\")\nprint(A)\nprint(\"\\nnews_df column null percentages:\")\nprint(B)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e036572f94fb3ac388d3f826391c91ed48ca9e1a"},"cell_type":"code","source":"#Join the dataframes into a combined dataframe - 0\n\n#perform the necessary cleaning to the market & news dataframes\n#merge them into a single dataframe thereafter\ndef clean_data_fields(market_df, news_df, verbose=False):\n    #the timestamps are string, convert them to int\n    market_df['time'] = market_df.time.dt.strftime(\"%Y%m%d\").astype(int)#convert datetime to simple int\n    news_df['time'] = news_df.time.dt.strftime(\"%Y%m%d\").astype(int)\n    #news df has a list of 'assetCodes', create a column 'assetCode' with the first assetCode in assetCodes\n    #drop the column assetCodes thereafter\n    news_df['assetCode'] = news_df['assetCodes'].map(lambda x: list(eval(x))[0])#from: https://www.kaggle.com/rabaman/0-64-in-100-lines\n    news_df.drop(['assetCodes'], axis=1, inplace=True)\n    #market df has opening and closing prices, create a column with the average prices\n    market_df['average'] = (market_df['close'] + market_df['open'])/2\n    #columns to be dropped from the market & news dataframes\n    cols_dropped_market = ['assetName','universe']\n                        #['returnsClosePrevMktres1','returnsOpenPrevMktres1','returnsClosePrevMktres10',\\\n                        #'returnsOpenPrevMktres10','universe'] #,'returnsOpenNextMktres10'\n    cols_dropped_news   = ['noveltyCount12H','noveltyCount24H','noveltyCount3D','noveltyCount5D','noveltyCount7D',\\\n                       'volumeCounts12H','volumeCounts24H','volumeCounts3D','volumeCounts5D','volumeCounts7D', \\\n                       'sourceTimestamp', 'sourceId', 'takeSequence', 'headline', 'firstCreated', 'provider', \\\n                       'audiences', 'firstMentionSentence', 'wordCount', 'headlineTag', 'bodySize', 'companyCount',\\\n                       'marketCommentary','sentenceCount', 'subjects', 'assetName']#last 2 added\n    #drop columns\n    if(verbose):\n        print(\"a_market_train_df has {} columns, pre-drop\".format(market_df.shape[1]))\n        print(\"a_news_train_df has {} columns, pre-drop\".format(news_df.shape[1]))\n    market_df = market_df.drop(cols_dropped_market, axis=1)\n    news_df = news_df.drop(cols_dropped_news, axis=1)\n    if(verbose):\n        print(\"a_market_train_df has {} columns, post-drop\".format(market_df.shape[1]))\n        print(\"a_news_train_df has {} columns, post-drop\".format(news_df.shape[1]))\n        print(\"---\")\n        print(\"a_market_train_df has {:,} rows and {} columns.\".format(market_df.shape[0],market_df.shape[1]))\n        print(\"a_news_train_df has {:,} rows and {} columns.\".format(news_df.shape[0],news_df.shape[1]))\n    #aggregate news_df and \n    news_df = news_df.groupby(['time', 'assetCode'], sort=False).aggregate(np.mean).reset_index()\n    #merge merket & news dfs into a simgle df\n    #unified_df = pd.merge(market_df, news_df, how=\"inner\", on=['time', 'assetCode'], copy=False)\n    unified_df = pd.merge(market_df, news_df, how=\"left\", on=['time', 'assetCode'], copy=False)\n    return unified_df\n\nunified_df = clean_data_fields(a_market_train_df, a_news_train_df)\nprint(\"done.\")\n\n\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b64fc550a27c9d0141c432c29d7c7840ca7dba38"},"cell_type":"code","source":"import gc\n#clear mem\ndel a_market_train_df, market_train_df, news_train_df, a_news_train_df\n\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"58dd2d64702bc92741dc3e8702d0997e8f08b070"},"cell_type":"code","source":"#check if unified df has any null fields\nprint(list_cols_with_null(unified_df))\n#get list of columns with null\n#A = [a for a in unified_df.columns.values if unified_df[a].isnull().sum()>0]\nprint(unified_df.shape)\nprint(unified_df.iloc[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"839a9d59d92d233fa64c248a3384dca2a98ed7c7"},"cell_type":"code","source":"import gc\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#explore time column a bit, of the unified/merged DataFrame\ntime_unique = unified_df['time'].nunique()\nnum_rows = unified_df['time'].shape[0]\nprint(\"The Time column in unified_df has a total of {} rows which contain {} unique values\".format(num_rows, time_unique))\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e22d6eb69b61917078ea94cc804e85facc539c50"},"cell_type":"markdown","source":"from sklearn.model_selection import train_test_split\n\n#verbose=True\n#split the unified df into different sets for validation\nx_train ,x_test = train_test_split(unified_df,test_size=0.2,random_state=42) #Leave 20% of data for final testing\nx_train_1 ,x_train_2 = train_test_split(x_train,test_size=0.3,random_state=42) #Split training set to 70% & 30% split\nprint(\"x_train has {:,} rows and {} cols\".format(x_train.shape[0], x_train.shape[1]))\nprint(\"x_test has {:,} rows and {} cols\".format(x_test.shape[0], x_test.shape[1]))\nprint(\"x_train_1 has {:,} rows and {} cols\".format(x_train_1.shape[0], x_train_1.shape[1]))\nprint(\"x_train_2 has {:,} rows and {} cols\".format(x_train_2.shape[0], x_train_2.shape[1]))\n#strip off y which is 'returnsOpenNextMktres10'\ny_label = 'returnsOpenNextMktres10' #training target\ny_train_1 = x_train_1[y_label]\ny_train_2 = x_train_2[y_label]\nx_train_1.drop(columns=y_label, inplace=True)\nx_train_2.drop(columns=y_label, inplace=True)\nprint(\"---\\n x_train_1.shape == {}, y_train_1.shape == {}\".format(x_train_1.shape, y_train_1.shape))\n"},{"metadata":{"trusted":true,"_uuid":"d66aa886f3450a153f607609d9703c2678492451"},"cell_type":"code","source":"num_rows = unified_df.shape[0]\nnum_rows_train = int(0.7 * num_rows) #\nnum_rows_test = num_rows - num_rows_train\nprint(\"There should be {} training rows and {} val. rows \".format(num_rows_train, num_rows_test))\n\nu_train_df = unified_df.iloc[:num_rows_train,:]\nu_test_df = unified_df.iloc[num_rows_train:,:]\nprint(\"Training set has {} rows while val set has {} rows\".format(u_train_df.shape[0], u_test_df.shape[0] ))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"410f84f5c7c2572ca4e48b41339ecc2057d2620f"},"cell_type":"code","source":"import gc\n#check shapes of derived dataframes to ensure all seems well\nprint(u_train_df.shape)\nprint(u_test_df.shape)\nprint(u_train_df.shape[0]+u_test_df.shape[0])\nprint(unified_df.shape)\n\n#delete unified_df to save memory\ndel unified_df\ngc.collect()\nprint(\"done.\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"93c0eb85ea50385125479a9ec9d8c595c55365ac"},"cell_type":"code","source":"#Split the derived datasets into training and target col.sets.\n#strip off y which is 'returnsOpenNextMktres10'\ny_label = 'returnsOpenNextMktres10' #training target\ny_train_df = u_train_df[y_label]\ny_test_df = u_test_df[y_label]\nu_train_df.drop(columns=y_label, inplace=True)\nu_test_df.drop(columns=y_label, inplace=True)\nprint(\" u_train_df.shape == {}, y_train_df.shape == {}\".format(u_train_df.shape, y_train_df.shape))\nprint(\" u_test_df.shape == {}, y_test_df.shape == {}\".format(u_test_df.shape, y_test_df.shape))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4715e72b67a4dd458652fd26c49e9294e65f5b50"},"cell_type":"code","source":"#train a model\n#import lightgbm as lgb\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\n\nimport matplotlib.pyplot as plt\nfrom itertools import chain\n\n%matplotlib inline\n\ncategorical_cols = ['assetCode']\ncategorical_cols_index = [1]\ncategorical_cols_index_str = '1'\n\nu_train_df_ = u_train_df.copy()\n\n#x_train_1_ = x_train_1.copy()\n#x_train_2_ = x_train_2.copy()\n#\n#x_train_1_.drop(categorical_cols,axis=1,inplace=True)\n#x_train_2_.drop(categorical_cols,axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7dd3318996bfbf50d1ec1e5564a4fe83e197d303"},"cell_type":"code","source":"#get training columns list\nu_train_df_.columns.tolist()\n#x_train_1_.columns.tolist()\ntraining_columns = [\n 'close',\n 'open',\n 'returnsClosePrevRaw1',\n 'returnsOpenPrevRaw1',\n 'returnsClosePrevMktres1',\n 'returnsOpenPrevMktres1',\n 'returnsClosePrevRaw10',\n 'returnsOpenPrevRaw10',\n 'returnsClosePrevMktres10',\n 'returnsOpenPrevMktres10',\n 'average',\n 'urgency',\n 'relevance',\n 'sentimentClass',\n 'sentimentNegative',\n 'sentimentNeutral',\n 'sentimentPositive',\n 'sentimentWordCount']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"49d05a044f40815a9be4cc84594ccb551b400ab7"},"cell_type":"code","source":"import time\nimport lightgbm as lgb\n#%%time\nlgb_params = {\n    'boosting_type': 'gbdt',\n    'objective': 'binary', #'binary' 'regression'\n    'metric': {'binary_logloss'}, # 'l2', 'l1', 'logloss'\n    'num_leaves': 31,\n    'learning_rate': 0.05,\n    'feature_fraction': 0.9,\n    'bagging_fraction': 0.8,\n    'bagging_freq': 5,\n    'seed': 42,\n    'verbose': 50\n}\ntime_start = time.time()\n\n#below is without categorical data\n#check if any categorical data except 'assetCode'\n#print(\"1---\")\ntry:\n    #rerunning this cell becomes a problem as attempt is made to drop already dropped columns...\n    #hence this try catch block\n    u_train_df.drop(columns=['assetCode'], inplace=True)\nexcept KeyError:\n    print(\"There was a KeyError at u_train_df\")\ntry:\n    #rerunning this cell becomes a problem as attempt is made to drop already dropped columns...\n    #hence this try catch block\n    u_test_df.drop(columns=['assetCode'], inplace=True)\nexcept KeyError:\n    print(\"There was a KeyError at u_test_df\")\nlist_categorical_columns(u_train_df)\n#print(\"2---\")\nx_train = lgb.Dataset(u_train_df.values, y_train_df,feature_name=u_train_df.columns.tolist())\n#print(\"3---\")\nx_valid = lgb.Dataset(u_test_df.values, y_test_df,feature_name=u_test_df.columns.tolist(), reference=x_train)\nevals_result = {}#store evaluation results\n#print(\"4---\")\nmodel01 = lgb.train(lgb_params, x_train, num_boost_round=1000, \\\n                    valid_sets=[x_train, x_valid], valid_names=['eval1','eval2'], \\\n                    evals_result=evals_result, verbose_eval=50 )\ntime_end   = time.time()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d7a5a0b2b56b9672179b79cd88538d583d09c7c3","scrolled":false},"cell_type":"markdown","source":"import time\nimport lightgbm as lgb\n#%%time\nlgb_params = {\n    'boosting_type': 'gbdt',\n    'objective': 'binary', #'binary' 'regression'\n    'metric': {'binary_logloss'}, # 'l2', 'l1', 'logloss'\n    'num_leaves': 31,\n    'learning_rate': 0.05,\n    'feature_fraction': 0.9,\n    'bagging_fraction': 0.8,\n    'bagging_freq': 5,\n    'seed': 42,\n    'verbose': 50\n}\ntime_start = time.time()\n\n#x_train = lgb.Dataset(x_train_1_.values, y_train_1,feature_name=categorical_cols,categorical_feature=categorical_cols,free_raw_data=False )            \n#x_valid = lgb.Dataset(x_train_2_.values, y_train_2,feature_name=categorical_cols,categorical_feature=categorical_cols,free_raw_data=False ) \n\n#below is without categorical data\n#check if any categorical data except 'assetCode'\n#print(\"1---\")\ntry:\n    #rerunning this cell becomes a problem as attempt is made to drop already dropped columns...\n    #hence this try catch block\n    x_train_1_.drop(columns=['assetCode'], inplace=True)\n    x_train_2_.drop(columns=['assetCode'], inplace=True)\nexcept KeyError:\n    print(\"There was a KeyError\")\nlist_categorical_columns(x_train_1_)\nlist_categorical_columns(x_train_2_)\n#print(\"2---\")\nx_train = lgb.Dataset(x_train_1_.values, y_train_1,feature_name=x_train_1_.columns.tolist())#, free_raw_data=False )            \n#x_train = lgb.Dataset(x_train_1_.values, y_train_1,feature_name=training_columns)#, free_raw_data=False )            \n#print(\"3---\")\nx_valid = lgb.Dataset(x_train_2_.values, y_train_2,feature_name=x_train_2_.columns.tolist(), reference=x_train)#,free_raw_data=False ) \n#x_valid = lgb.Dataset(x_train_2_.values, y_train_2,feature_name=training_columns, reference=x_train)#,free_raw_data=False ) \nevals_result = {}#store evaluation results\n#print(\"4---\")\nmodel01 = lgb.train(lgb_params, x_train, num_boost_round=1000, \\\n                    valid_sets=[x_train, x_valid], valid_names=['eval1','eval2'], \\\n                    evals_result=evals_result, verbose_eval=50 )\n\n\n'''\n#print(\"5---\")\nmodel01 = lgb.train(lgb_params, x_train, num_boost_round=1000, \\\n                    valid_sets=[x_train], valid_names=['eval1'], \\\n                    evals_result=evals_result )\nmodel01 = lgb.train(lgb_params, x_train, num_boost_round=1000, \\\n                    valid_sets=[x_train, x_valid], valid_names=['eval1','eval2'], \\\n                    evals_result=evals_result )\nmodel01 = lgb.train(lgb_params, x_train, num_boost_round=1000, \\\n                    valid_sets=[x_train, x_valid], valid_names=['eval1','eval2'], \\\n                    evals_result=evals_result )\n'''\ntime_end   = time.time()"},{"metadata":{"trusted":true,"_uuid":"296e2a962ded40348c5e962716f8fdfe42f5e710"},"cell_type":"code","source":"print(\"The work took {:2.4f} minutes\".format((time_end-time_start)/60))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a9bf966b0bf7b43d579d52323879d858e020f142"},"cell_type":"code","source":"\n_ = lgb.plot_metric(evals_result)\n_ = lgb.plot_importance(model01)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"48a960540a740837c2205930e570915599e45642"},"cell_type":"markdown","source":"pred_y = model01.predict(x_train_2_)#(df.iloc[test_index])\n#pred_y[pred_y >= 0.5] = 1\n#pred_y[pred_y < 0.5] = 0\n#print(x_train_2_.shape,\"...\", pred_y.shape)\nprint(y_train_2.iloc[102])\nprint(pred_y[102])\n#print(y_train_2==pred_y)\n"},{"metadata":{"trusted":true,"_uuid":"042c26293179e638ed106ba4cf8c78767b86018a"},"cell_type":"code","source":"#Submission Process\n# You can only iterate through a result from `get_prediction_days()` once\n# so be careful not to lose it once you start iterating.\ndays = env.get_prediction_days()\nprint(\"done.\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5455abc7777b62283fa14dc1b1a50f68b82f9420"},"cell_type":"code","source":"def clean_data_fields_forPreds(market_df, news_df, verbose=False):\n    #the timestamps are string, convert them to int\n    #already int date only#market_df['time'] = market_df.time.dt.strftime(\"%Y%m%d\").astype(int)#convert datetime to simple int\n    #already int date only#news_df['time'] = news_df.time.dt.strftime(\"%Y%m%d\").astype(int)\n    market_df['time'] = market_df.time.astype(int)#convert datetime to simple int\n    news_df['time'] = news_df.time.astype(int)#convert datetime to simple int\n    \n    if 'assetCodes' in news_df.columns.values:\n        #news df has a list of 'assetCodes', create a column 'assetCode' with the first assetCode in assetCodes\n        #drop the column assetCodes thereafter\n        news_df['assetCode'] = news_df['assetCodes'].map(lambda x: list(eval(x))[0])#from: https://www.kaggle.com/rabaman/0-64-in-100-lines\n        news_df.drop(['assetCodes'], axis=1, inplace=True)\n    \n    \n    #market df has opening and closing prices, create a column with the average prices\n    market_df['average'] = (market_df['close'] + market_df['open'])/2\n    #columns to be dropped from the market & news dataframes\n    cols_dropped_market = ['assetName'] #,'universe'\n                        #['returnsClosePrevMktres1','returnsOpenPrevMktres1','returnsClosePrevMktres10',\\\n                        #'returnsOpenPrevMktres10','universe'] #,'returnsOpenNextMktres10'\n    cols_dropped_news   = ['noveltyCount12H','noveltyCount24H','noveltyCount3D','noveltyCount5D','noveltyCount7D',\\\n                       'volumeCounts12H','volumeCounts24H','volumeCounts3D','volumeCounts5D','volumeCounts7D', \\\n                       'sourceTimestamp', 'sourceId', 'takeSequence', 'headline', 'firstCreated', 'provider', \\\n                       'audiences', 'firstMentionSentence', 'wordCount', 'headlineTag', 'bodySize', 'companyCount',\\\n                       'marketCommentary','sentenceCount', 'subjects', 'assetName']#last 2 added\n    #drop columns\n    if(verbose):\n        print(\"a_market_train_df has {} columns, pre-drop\".format(market_df.shape[1]))\n        print(\"a_news_train_df has {} columns, pre-drop\".format(news_df.shape[1]))\n    market_df = market_df.drop(cols_dropped_market, axis=1)\n    news_df = news_df.drop(cols_dropped_news, axis=1)\n    if(verbose):\n        print(\"a_market_train_df has {} columns, post-drop\".format(market_df.shape[1]))\n        print(\"a_news_train_df has {} columns, post-drop\".format(news_df.shape[1]))\n        print(\"---\")\n        print(\"a_market_train_df has {:,} rows and {} columns.\".format(market_df.shape[0],market_df.shape[1]))\n        print(\"a_news_train_df has {:,} rows and {} columns.\".format(news_df.shape[0],news_df.shape[1]))\n    #aggregate news_df and \n    news_df = news_df.groupby(['time', 'assetCode'], sort=False)\\\n                     .aggregate(np.mean)\\\n                     .reset_index()\n    #merge merket & news dfs into a simgle df\n    #unified_df = pd.merge(market_df, news_df, how=\"inner\", on=['time', 'assetCode'], copy=False)\n    unified_df = pd.merge(market_df, news_df, how=\"left\", on=['time', 'assetCode'], copy=False)\n    return unified_df\nprint(\"done.\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"917a7b8207a35d619ead22ebfb1505acc75d5aa9"},"cell_type":"markdown","source":"(market_obs_df, news_obs_df, predictions_template_df) = next(days)\n"},{"metadata":{"_uuid":"801d0cc5b855bae40951ec4706f2db5a59400c13","trusted":true},"cell_type":"code","source":"#print(x_train_1_.columns.values,\"\\n\",market_obs_df.columns.values,\"\\n\",news_obs_df.columns.values,\"\\n\")\n#list_col_allowed = x_train_1_.columns.values\nlist_col_allowed = u_train_df.columns.values\ndef cols_needing_dropping(list_cols):\n    return [a for a in list_cols if a not in list_col_allowed]\ntarget_cols = ['returnsOpenNextMktres10']\n#current_df = clean_data_fields_forPreds(market_obs_df, news_obs_df, verbose=False)\nprint(\"done\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"96105e01f95b1bb1cd6bfe91a2802c45c199465e"},"cell_type":"markdown","source":"print(current_df.columns.values)\ncurrent_df_ = current_df[list_col_allowed]\nprint(current_df_.columns.values)"},{"metadata":{"_uuid":"c3255a6601c80fbb22b2eac1165e70df2d4a0285"},"cell_type":"markdown","source":"predictions = ((model01.predict(current_df_) * 2) - 1)\npredictions_df = pd.DataFrame({'assetCode':current_df['assetCode'], '_confidence':predictions})\npredictions_template_df = predictions_template_df.merge(predictions_df, how='left')\\\n                                                 .drop('confidenceValue', axis=1)\\\n                                                 .fillna(0)\\\n                                                 .rename(columns={'_confidence':'confidenceValue'})\nprint(predictions_df.head())\nprint(predictions_template_df.head())\nenv.predict(predictions_template_df)"},{"metadata":{"trusted":true,"_uuid":"212a50e4a2d9e42597cdc122a9c333cb6ea04404"},"cell_type":"code","source":"for market_obs_df, news_obs_df, predictions_template_df in days:\n    current_df = clean_data_fields_forPreds(market_obs_df, news_obs_df, verbose=False)\n    current_df_ = current_df[list_col_allowed]\n    print(current_df.shape,\"-----\",current_df_.shape)\n    predictions = ((model01.predict(current_df_) * 2) - 1)\n    predictions_df = pd.DataFrame({'assetCode':current_df['assetCode'], '_confidence':predictions})\n    predictions_template_df = predictions_template_df.merge(predictions_df, how='left')\\\n                                                     .drop('confidenceValue', axis=1)\\\n                                                     .fillna(0)\\\n                                                     .rename(columns={'_confidence':'confidenceValue'})\n    print(predictions_df.head())\n    print(predictions_template_df.head())\n    env.predict(predictions_template_df)    \nenv.write_submission_file()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f631a5db05d8defb29f4bb583a62c0779f2fee1d"},"cell_type":"markdown","source":"#print(current_df_.shape)\n#print(current_df_.head())\n#env.write_submission_file()\nprint(market_obs_df.columns.values)\n#list(set(market_obs_df['assetCode']) & set(news_obs_df['assetCode']))\nprint(len(list(set(market_obs_df['assetCode']))))\nprint(len(list(set(news_obs_df['assetCode']))))\n#print(len(list( set(market_obs_df['assetCode', 'time']) & set(news_obs_df['assetCode', 'time'])  )))"},{"metadata":{"trusted":true,"_uuid":"310d069c40d1eadd836ad0b1f739227a3d800822"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"00a6ad2b6a5d28bcf04a0983a74305aa8a0aba56"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8a553e4246faedda2c2efaaa7f4c5be4f57c1c9d"},"cell_type":"markdown","source":"Sources::\nhttps://www.kaggle.com/bguberfain/a-simple-model-using-the-market-and-news-data\nhttp://mlexplained.com/2018/01/05/lightgbm-and-xgboost-explained/\nhttps://martin-thoma.com/pandas-merge-join-concatenate/\nhttps://chartio.com/resources/tutorials/how-to-check-if-any-value-is-nan-in-a-pandas-dataframe/\nhttps://docs.python.org/3.1/library/re.html\nhttps://pyformat.info/\nhttps://stackoverflow.com/questions/15769246/pythonic-way-to-print-list-items\nhttps://docs.python.org/3/tutorial/datastructures.html\nhttps://docs.python.org/3/tutorial/datastructures.html#dictionaries\nhttps://stackoverflow.com/questions/29530232/how-to-check-if-any-value-is-nan-in-a-pandas-dataframe\nhttps://stackoverflow.com/questions/14507591/python-dictionary-comprehension\nhttps://www.digitalocean.com/community/tutorials/understanding-list-comprehensions-in-python-3\nhttps://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.drop.html\nhttps://pandas.pydata.org/pandas-docs/stable/merging.html\nhttps://ipython.org/ipython-doc/3/interactive/magics.html\nhttps://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.merge.html\nhttps://www.kaggle.com/jagangupta/memory-optimization-and-eda-on-entire-dataset\nhttps://www.kaggle.com/kunalkotian/easily-load-train-csv-w-o-crash-save-feather-file\nhttps://stackoverflow.com/questions/39662149/pandas-extract-date-and-time-from-timestamp\nhttps://lightgbm.readthedocs.io/en/latest/Parameters-Tuning.html\nhttps://github.com/Microsoft/LightGBM/blob/master/examples/python-guide/simple_example.py\nhttps://lightgbm.readthedocs.io/en/latest/Parameters.html\nhttps://www.kaggle.com/bguberfain/a-simple-model-using-the-market-and-news-data\nhttps://www.kaggle.com/artgor/eda-feature-engineering-and-everything\nhttps://www.kaggle.com/richardgg93/two-sigma-news-first-try\nhttps://www.kaggle.com/chocozzz/two-sigma-news-simple-eda-prophet-nlp\nhttps://www.kaggle.com/smasar/tutorial-timeseriesapproach\nhttps://www.kaggle.com/archermo/twosigma-day1\nhttps://www.kaggle.com/dansbecker/using-categorical-data-with-one-hot-encoding\nhttps://github.com/Microsoft/LightGBM/blob/master/docs/Advanced-Topics.rst\nhttps://github.com/Microsoft/LightGBM/blob/master/docs/Parameters.rst#categorical_feature\nhttps://www.kaggle.com/ogrellier/good-fun-with-ligthgbm/code\nhttps://stackoverflow.com/questions/47370240/multiclass-classification-with-lightgbm\nhttps://towardsdatascience.com/catboost-vs-light-gbm-vs-xgboost-5f93620723db\nhttps://lightgbm.readthedocs.io/en/latest/Python-API.html\nhttps://www.avanwyk.com/an-overview-of-lightgbm/\nhttp://www.chioka.in/differences-between-the-l1-norm-and-the-l2-norm-least-absolute-deviations-and-least-squares/\nhttps://www.kaggle.com/rabaman/0-64-in-100-lines\nhttp://dylan-chen.com/model/lightgbm-tutorial/\nhttps://www.kaggle.com/dmdm02/complete-eda-voting-lightgbm\nhttps://www.kaggle.com/kazuokiriyama/tuning-hyper-params-in-lgbm-achieve-0-66-in-lb\n\n\n"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}