{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\n#basic imports in this section\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport gc\nimport os\nprint(os.listdir(\"../input\"))\nimport time\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"677545d1632eb7632ac71efaea1ad4466d4d37e5"},"cell_type":"code","source":"#ignoring any future warnings in the code\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4a5f3f7f6e1ff5ff6744d53e03d5cc9bdff7052f"},"cell_type":"markdown","source":"**Loading the Two Sigma environment.**"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"from kaggle.competitions import twosigmanews\nenv = twosigmanews.make_env()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d6dbd83cbc7f10abf52145665c28d07c47689eef"},"cell_type":"markdown","source":"Two datasets, namely, market dataset and news dataset will be loaded in market and news dataframes respectively"},{"metadata":{"trusted":true,"_uuid":"35c988736497219b2f08eceb2f5d2be7529cf9e3"},"cell_type":"code","source":"#get_training_data is an inbiult kaggle method that returns datasets.\n(market, news) = env.get_training_data()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c0e14656694895a457a077a57a19af9e878821ba"},"cell_type":"markdown","source":"**Simple garbage collection**"},{"metadata":{"trusted":true,"_uuid":"a76222ecf66ac052e6587d0893debb9760a14154"},"cell_type":"code","source":"gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"539a0cb4518be4653321668b893cc916b299437a"},"cell_type":"markdown","source":"> **Data Description**"},{"metadata":{"trusted":true,"_uuid":"a094bf0593dc5982f970b6556c96b62475277965"},"cell_type":"code","source":"#number of rows in the market dataset\nmarket.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a25e0dc8fde5f3c9d54f7f732d587c5d8005b3c8"},"cell_type":"code","source":"#number of features in the market dataset\nmarket.shape[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2bccb592f6b3a350208939eb289aaa22532506ed"},"cell_type":"code","source":"#datatype of each feature in the market dataset \nmarket.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"057f0e0955446f40649eb902620012924d520eb5"},"cell_type":"code","source":"#number of rows in the news dataset\nnews.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"48e0447f607bcbb4a571774ca8113a0194ecbb78"},"cell_type":"code","source":"#number of features in the news dataset\nnews.shape[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0d8142edb4da0194fd2d3484b9e58dadb2b7a55f"},"cell_type":"code","source":"#datatype of each feature in the market dataset \nnews.dtypes","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"559a13785f607f4ea6d8252e4d73dba09fce0da4"},"cell_type":"markdown","source":"**Preprocessing starts from here!**"},{"metadata":{"trusted":true,"_uuid":"d3e10f9a5d8608b3198495e12ab047bb37f6e99a"},"cell_type":"code","source":"#this method contains code for basic preprocessing of both market and news dataset\n'''\nthis method\n1. converts time feature from market dataset whose datatype is datetime into date datatype\n2. converts time feature from news dataset whose datatype is datetime into float datatype considering only the hours of the entire datetime datatype\n3. converts sourceTimeStamp feature from news dataset whose datatype is datetime into float datatype considering only the hours of the entire datetime datatype\n4. converts firstCreated feature from news dataset whose datatype is datetime into date datatype\n5. converts universe and volume feartures from float datatype to integer datatype\n6. sorts the time columns in both the datasets\n7. places 0 in place of null values, which was proven damaging, hence, dropped the idea\n8. the eval function used will convert string column into a dictionary containing all the assetCodes\n    news['assetCodesLen'] = news['assetCodes'].map(lambda x: len(eval(x))) -> this line takes the length of the column of dictionary that was just created from the cloumn of strings using eval function and figures out how many assetCodes are there in news dataset.\n    news['assetCodes'] = news['assetCodes'].map(lambda x: list(eval(x))[0]) -> this line converts dictionary into a list. It takes the only first value from the list and attaches it to the assetCodes column.\n9. basic rounding of float datatype\n10. returns both, the market and news dataset.\n'''\ndef preprocess_data(market, news):\n    market.time = market.time.dt.date\n    news.time = news.time.dt.hour\n    news.sourceTimestamp = news.sourceTimestamp.dt.hour\n    news.firstCreated = news.firstCreated.dt.date\n    news['assetCodesLen'] = news['assetCodes'].map(lambda x: len(eval(x)))\n    news['assetCodes'] = news['assetCodes'].map(lambda x: list(eval(x))[0])\n    market['volume'] = market.volume.astype(int)\n    market['universe'] = market.universe.astype(int)\n    #this is just an experiment\n    market = market.sort_values('time')\n    news = news.sort_values('time')\n    #if accuracy decreases then change null values to raw values from that same row\n    #just uncomment the next four lines\n    '''columns = ['returnsClosePrevMktres1','returnsOpenPrevMktres1','returnsClosePrevMktres10', 'returnsOpenPrevMktres10']\n    columns_raw = ['returnsClosePrevRaw1', 'returnsOpenPrevRaw1','returnsClosePrevRaw10', 'returnsOpenPrevRaw10']\n    for i in range(len(column_raw)):\n        market[column_market[i]] = market[column_market[i]].fillna(market[column_raw[i]])'''\n    \n    \n    market.round({'close': 2, 'open' : 2, 'returnsClosePrevRaw1' : 4, 'returnsOpenPrevRaw1' : 4, 'returnsClosePrevRaw10' : 4, \n                  'returnsOpenPrevRaw10' : 4, 'returnsOpenNextMktres10' : 4})\n    news.round({'relevance' : 3, 'sentimentNegative' : 3, 'sentimentNeutral' : 3, 'sentimentPositive' : 3})\n    market_num_cols = ['volume', 'close', 'open', 'returnsClosePrevRaw1', 'returnsOpenPrevRaw1', 'returnsClosePrevMktres1',\n                    'returnsOpenPrevMktres1', 'returnsClosePrevRaw10', 'returnsOpenPrevRaw10', 'returnsClosePrevMktres10',\n                    'returnsOpenPrevMktres10']\n    news_num_cols = ['urgency', 'takeSequence', 'bodySize', 'companyCount', 'sentenceCount', 'wordCount', \n                     'firstMentionSentence', 'sentimentClass', 'sentimentNegative', 'sentimentNeutral', 'sentimentPositive', \n                     'sentimentWordCount', 'noveltyCount12H', 'noveltyCount24H', 'noveltyCount3D', 'noveltyCount5D', \n                     'noveltyCount7D', 'volumeCounts12H', 'volumeCounts24H', 'volumeCounts3D', \n                     'volumeCounts5D', 'volumeCounts7D']\n    #this helps reducing the accuracy so we will replace nan values with mean in next phase and comment the next two lines\n    #market[market_num_cols] = market[market_num_cols].fillna(0)\n    #news[news_num_cols] = news[news_num_cols].fillna(0)\n    return market, news;\n[market, news] = preprocess_data(market, news)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6ada7670fbd6e4095e3f03625425610ae7d8501f"},"cell_type":"markdown","source":"changing nan values by mean alues of the feature"},{"metadata":{"trusted":true,"_uuid":"e2945ba4268fd07a408b993d82aedc3c97954bba"},"cell_type":"code","source":"#this method changes all the nan values in every column by the mean of that respective column.\n#many classifiers cannot handle nan values and hence we have to compromise with the data(either drop row, which is loss of some important data or drop a column, which is also loss of important data, but less damaging than dropping a row)\n#there were only 6 columns remaining after dropping the columns containing nan values.\n#hence we decided to write this method.\n#the plus point of y\\this method is that we can now apply any classifier to the dataset and predict results.\n#the most logical way is to replace nan values by the mean of the column.\n#this way the mean of the column will not be affected and the further process won't get hurt.\ndef handling_nan_values(df):\n    for i in df.columns:\n        print(i, df[i].dtype)\n        \n        if df[i].dtype == \"object\":\n            df[i] = df[i].fillna(\"other\")\n        elif (df[i].dtype == \"int64\" or df[i].dtype == \"float64\" or df[i].dtype == \"int16\" or df[i].dtype == \"int8\" or df[i].dtype == \"int32\" or df[i].dtype == \"float32\"):\n            df[i] = df[i].fillna(df[i].mean())\n            print(i, df[i].mean())\n        else:\n            pass\n    return df","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"74ecd388eb7f8c4182f3590655d5c06f949a5bc4"},"cell_type":"markdown","source":"There's no need perform one hot encoding on string columns because tree-like classifiers can easily handle those."},{"metadata":{"_uuid":"63a3249971b95075a43b5f3dcda0cf6e77826b14"},"cell_type":"markdown","source":"\n**Next is the function to remove outliers.**"},{"metadata":{"trusted":true,"_uuid":"851d4cce7f1a0b2cedce4a232577ea22a80cd25d"},"cell_type":"code","source":"#adjusting outliers\n#rather than removing outliers, we decided to adjust them near to the farthest point after removing that outlier, or in more technical words, clipping the outliers.\n#outliers are the noise in the dataset.\n#they are corrupted data and affects prediction powers of the model severely in a bad way.\n#hence it only makes sense to control these outliers in the dataset by decreasing or increasing its values to a specific value.\n#here, in this method we are clipping all rows that contains outliers to a specified value(0.05 and  0.95).\n#an outlier is a data point which has high variance and its value is far away from the mean/ average value.\ndef remove_outliers(data_frame, column_list, low=0.05, high=0.95):\n    temp_frame = data_frame\n    for column in column_list:\n        this_column = data_frame[column]\n        quant_df = this_column.quantile([low,high])\n        low_limit = quant_df[low]\n        high_limit = quant_df[high]\n        temp_frame[column] = data_frame[column].clip(lower=low_limit, upper=high_limit)\n    return temp_frame","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3460b06f4d50ddfa3365d54898608668d92a0aef"},"cell_type":"markdown","source":"**outliers for news data**"},{"metadata":{"trusted":true,"_uuid":"82f02fb714eccacc0edb9ab386d40ed0beeead92"},"cell_type":"code","source":"#the outliers in the columns of news dataset, mentioned in columns_outliers are handled using the remove_outliers method\n#only the outliers in numerical columns are handled as we are planning to remove categorical columns\ncolumns_outlier = ['takeSequence', 'bodySize', 'sentenceCount', 'wordCount', 'sentimentWordCount', 'firstMentionSentence',\n                   'noveltyCount12H','noveltyCount24H', 'noveltyCount3D', 'noveltyCount5D', 'noveltyCount7D', \n                   'volumeCounts12H', 'volumeCounts24H','volumeCounts3D','volumeCounts5D','volumeCounts7D']\nnews = remove_outliers(news, columns_outlier)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"832c0d78522eb7ec70f8be8860f04bf78b0d73e9"},"cell_type":"markdown","source":"**outliers for market data**"},{"metadata":{"_uuid":"2371692ac29a6e902f214af5c02993f3852bf578"},"cell_type":"markdown","source":"the difference between open and close values can not be too much. The threshold in following code can be changed as per convenience. The lower threshold is 0.5 and upper threshold is 1.5. Every row not in the range[0.5, 1.5] will be dropped. Because those are noise data points."},{"metadata":{"trusted":true,"_uuid":"2e1d8fc83a2faae35e0d1318530361c3002c75b2"},"cell_type":"code","source":"#the outliers in market dataset are removed as per the logic mentioned above\nmarket['close_open_ratio'] = np.abs(market['close']/market['open'])\nthreshold = 0.5\nmarket = market.loc[market['close_open_ratio'] < 1.5]\nmarket = market.loc[market['close_open_ratio'] > 0.5]\nmarket = market.drop(columns=['close_open_ratio'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a7e004218fd4d86686973566f9d094fdecba9672"},"cell_type":"markdown","source":"If returns exceed by 50% or fall by 50% of their value then the data is noisy data"},{"metadata":{"trusted":true,"_uuid":"17dcc38dffb44ed7024ae028e749449ffe5aecc9"},"cell_type":"code","source":"columns =['returnsClosePrevMktres1','returnsOpenPrevMktres1','returnsClosePrevMktres10', 'returnsOpenPrevMktres10', 'returnsClosePrevRaw1', 'returnsOpenPrevRaw1','returnsClosePrevRaw10', 'returnsOpenPrevRaw10', 'returnsOpenNextMktres10']\nfor column in columns:\n    market = market.loc[market[column]>=-2]\n    market = market.loc[market[column]<=2]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"356ca2cc3e7ccc81209b8ade3db9e7eec989f173"},"cell_type":"markdown","source":"removing data having unknown asset name"},{"metadata":{"trusted":true,"_uuid":"37f20c8b48eaa8883dd35eaf6654f9b9d6765b93"},"cell_type":"code","source":"#market = market[~market['assetCode'].isin(['PGN.N','EBRYY.OB'])]\nmarket = market[~market['assetName'].isin(['Unknown'])]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6678ec380a72e95a591ca8596198598f13b0f8bb"},"cell_type":"code","source":"market","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9fadad919603a9ae7f95c3be4043e1f868bfdb2c"},"cell_type":"code","source":"news","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3bfb26fc84e11dd00824f62a19cdff2cb0789d25"},"cell_type":"markdown","source":"**Visualizing both datasets**"},{"metadata":{"_uuid":"a0aff7c05d1d08d9ae462400d25ae491c1c0a5c8","trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nplot = (market.nunique()).sort_values(ascending=False)\nplot.plot(kind=\"bar\", figsize = (20,10), fontsize = 15)\nplt.xlabel(\"Columns\", fontsize = 15)\nplt.ylabel(\"Unique Values\", fontsize = 15)\nprint('Done!')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ad0588faac6f8301d23213531001373ab482e2bd","trusted":true},"cell_type":"code","source":"market.nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f3627fce41a329b1dff75ff03c7082474447be75"},"cell_type":"code","source":"import matplotlib.pyplot as plt\nplot = (news.nunique()).sort_values(ascending=False)\nplot.plot(kind=\"bar\", figsize = (20,10), fontsize = 15)\nplt.xlabel(\"Columns\", fontsize = 15)\nplt.ylabel(\"Unique Values\", fontsize = 15)\nprint('Done!')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9140a40101bbf39528794565df4b128d45fd5097"},"cell_type":"code","source":"news.nunique()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4df013c3158ab8336b8738280f6bc627e14050d4"},"cell_type":"markdown","source":"**merging the dataset in next section**"},{"metadata":{"trusted":true,"_uuid":"42d0f933f2d1ea9aa51f48076633435f91a17c96"},"cell_type":"code","source":"#we will merge dataset based on assetCodes and not assetName because there are unknown values in the assetName column and also we handled the assetCodes column very nicely in data preprocessing\n#we will make a left join on time and assetCode of market data and a righ join on firstCreated and assetCodes. Because in preprocessing part we converted time and firstCreated into date format and assetCode and assetCodes is explained above.\ndef dataset_merge(market, news):\n    grouping_cols = ['firstCreated', 'assetCodes']\n    news = news.groupby(grouping_cols, as_index=False).mean() \n    market = pd.merge(market, news, how='left', left_on= ['time', 'assetCode'], right_on= ['firstCreated', 'assetCodes'])\n    return market\nmarket = dataset_merge(market, news)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ef74c4f5a63b5efae82a278f0692b6de724945db"},"cell_type":"markdown","source":"**Correlation among data** : Pearson Correlation"},{"metadata":{"trusted":true,"_uuid":"7b0531335f3c44788205a13be508878a357683c0"},"cell_type":"code","source":"pearson_dataframe = market.corr(method= 'pearson') ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"248befaf2799e145488bf1a6ee87cf089aaa5caf","scrolled":true},"cell_type":"code","source":"pearson_dataframe","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dddbb660a4f8503a6e1a9f46511ff05e42cfe1d5"},"cell_type":"markdown","source":"**Inferences based on the correlation matrix:**\nThe pairs having >0.7 correlation are asfollows:\n1. (open, close)\n2. (returnsClosePrevMktres1, returnsClosePrevRaw1)\n3. (returnsOpenPrevRaw1, returnsOpenPrevMktres1)\n4. (returnsOpenPrevRaw10, returnsClosePrevRaw10)\n5. (returnsOpenPrevMktres10, returnsOpenPrevRaw10)\n6. (returnsOpenPrevMktres10, returnsClosePrevMktres10)\n7. (time_y, source_time_stamp)\n8. (body_size, sentenceCount)\n9. (body_size, wordCount)\n10. (sentenceCount, wordCount)\n11. (sentimentClass, sentimentPositive) and \n12. every columns of valueCounts"},{"metadata":{"_uuid":"cdc60dbcc1f829fe23eb808ebcae65d217494b21"},"cell_type":"markdown","source":"**Covariance among data**"},{"metadata":{"trusted":true,"_uuid":"a346f9f2c13bfa038d2fca69a8e17ab785554973"},"cell_type":"code","source":"cov_dataframe = market.cov()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2efdf3d5270352bd854c4c9ff2f2e24ccd64f5ce"},"cell_type":"code","source":"cov_dataframe","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e549c0f6fdfcae7d8074bbca47d9f9aaac31ae26"},"cell_type":"code","source":"#plotting the correlation\nimport matplotlib.pyplot as plt\nplt.matshow(pearson_dataframe)\nplt.title(\"pearson correlation chart\\n\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e1e192e55c2b584fbe99328e1c3994ebe162f26d"},"cell_type":"code","source":"# defining only those columns that are important to the dataset\n# we deduced this result based on prior preprocessing\nreduced_cols = [c for c in market if c not in ['assetCode', 'assetCodes', 'assetCodesLen', 'assetName', 'audiences', 'firstCreated', 'headline', 'headlineTag', 'provider', 'bodySize', 'wordCount', 'returnsOpenNextMktres10', 'sourceId', 'subjects', 'time', 'time_x', 'time_y', 'universe','sourceTimestamp']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4c86825fbd80dbfa7b6740495daf0af78e1efe2c"},"cell_type":"code","source":"#we will make target variable in this section\ntarget = market.returnsOpenNextMktres10 >= 0\ntarget = target.values\nop = market.returnsOpenNextMktres10.values","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a92f617461e2c00e2ec3f9be653594ed02854261"},"cell_type":"markdown","source":"We already filled nan values by the mean.... so no need of the next block anymore"},{"metadata":{"trusted":true,"_uuid":"6b4838231a4ecb89942cbcd977ade05dff7c09e4"},"cell_type":"markdown","source":"market_num_cols = ['volume', 'close', 'open', 'returnsClosePrevRaw1', 'returnsOpenPrevRaw1', 'returnsClosePrevMktres1', 'returnsOpenPrevMktres1', 'returnsClosePrevRaw10', 'returnsOpenPrevRaw10', 'returnsClosePrevMktres10', 'returnsOpenPrevMktres10', 'urgency', 'takeSequence', 'companyCount', 'marketCommentary', 'sentenceCount', 'firstMentionSentence', 'relevance', 'sentimentClass', 'sentimentNegative', 'sentimentNeutral', 'sentimentPositive', 'sentimentWordCount', 'noveltyCount12H', 'noveltyCount24H', 'noveltyCount3D', 'noveltyCount5D', 'noveltyCount7D', 'volumeCounts12H', 'volumeCounts24H', 'volumeCounts3D', 'volumeCounts5D', 'volumeCounts7D']\nmarket[market_num_cols] = market[market_num_cols].fillna(0)"},{"metadata":{"trusted":true,"_uuid":"2ca7517dedfe0e850961f9aea85f51debdb132a2"},"cell_type":"code","source":"reduced_cols","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0d64004f45c8c1485ccca54eaa86cbfdcaccd6e4"},"cell_type":"code","source":"market = market[reduced_cols]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8070e1f72b426df3364d7333afe63da2ab7c92c0"},"cell_type":"code","source":"market","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a61e8244a2f873eaf8693f6f5dc170b1335162eb"},"cell_type":"code","source":"#after making a left join on the dataset, there will be some nan values in the combined dataset.\n#hence we will call handling_nan_values method again\nmarket = handling_nan_values(market)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fbb0d3bacdea952473c7f68dbd8f310860d4ee45"},"cell_type":"code","source":"# we will scale the merged dataset using standard scaler.\n# there are other approaches using min-max scaler, etc. \n# but we chose standard scaler because it handles categorical data which min-max fails to handle.\n# the range of standard scaler is typically [-2, 4] while the range of min-max scaler is [0, 1] which will not give us important data to classify a stock as negative\nfrom sklearn.preprocessing import StandardScaler\n\ndata_scaler = StandardScaler()\nmarket[reduced_cols] = data_scaler.fit_transform(market[reduced_cols])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c6737ac15ce323a829cddef38fc34c282b3c66ec"},"cell_type":"markdown","source":"The accuracy is decreasing when PCA is applied with less n_components, hence we will keep large values in n_components such as 20, 25, etc because the dataset is such that the model will require more data than a threshold to predict correctly"},{"metadata":{"trusted":true,"_uuid":"506d5dfe3bbda7ae8140e40858556a02ad4c2849"},"cell_type":"code","source":"from sklearn.decomposition import PCA\npca = PCA(n_components=27)\nmarket = pca.fit_transform(market)  \n#market = pca.transform(market)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ce3b31fdbace71c4ee7a9e2ee87e800977d8a32c"},"cell_type":"code","source":"market.shape[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3f6ff990573e0d683050c42e7a3066976af7d367"},"cell_type":"code","source":"op","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d08de390a11417a4240ca4a77a1597987bc90e1c"},"cell_type":"code","source":"#spliting the dataset using test_train_split\n#our next task is to split using time series as the dataset contains time related data\nfrom sklearn import *\nX_train, X_test, target_train, target_test, op_train, op_test = model_selection.train_test_split(market, target, op, test_size=0.2, random_state=5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c2611317657a00783e2feb7a1da436e736c4ad1a"},"cell_type":"code","source":"X_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fc7ba5e83202caae8cb199709b4756d3511c4d6d"},"cell_type":"markdown","source":"#fitting the dataset using XGBoost classifier\nfrom xgboost import XGBClassifier\nt = time.time()\nxgb = XGBClassifier(n_jobs=4, n_estimators=200, max_depth=8, eta=0.1)\nxgb.fit(X_train, target_train)\nprint(f'Done, time = {time.time() - t}')"},{"metadata":{"trusted":true,"_uuid":"19f642300c42ceabf8bab76e77187b87252a1d45","scrolled":true},"cell_type":"markdown","source":"#printing the confusion matrix and accuracy using built in library\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nt = time.time()\ntarget_test, prediction = target_test, xgb.predict(X_test)\nprint(classification_report(target_test, prediction))\nprint(\"Detailed confusion matrix:\")\nprint(confusion_matrix(target_test, prediction))\nprint(prediction)\naccuracy_score(target_test, prediction)\nprint(f'Done, time = {time.time() - t}')"},{"metadata":{"_uuid":"4fc57c705c64a797dcd5ef1d0e2a11f52e77d127"},"cell_type":"markdown","source":"**CatBoost next**"},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"93d4b5ae227b8029e03d7ae69ce8f9b68ae0e1d9"},"cell_type":"code","source":"#fitting the dataset using CatBoost classifier\nfrom catboost import CatBoostClassifier\nt = time.time()\ncat = CatBoostClassifier(thread_count=4, n_estimators=200, max_depth=8, eta=0.1, loss_function='Logloss' , verbose=10)\ncat.fit(X_train, target_train)\nprint(f'Done, time = {time.time() - t}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"edb6b1dc458e64dbcd7dc8edb8d754fa0306228f","scrolled":true},"cell_type":"code","source":"#printing the confusion matrix and accuracy using built in library\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nt = time.time()\ntarget_test, prediction = target_test, cat.predict(X_test)\nprint(classification_report(target_test, prediction))\nprint(\"Detailed confusion matrix:\")\nprint(confusion_matrix(target_test, prediction))\nprint(prediction)\nprint(accuracy_score(target_test, prediction))\nprint(f'Done, time = {time.time() - t}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"863f60958509d2a66e29e78d3b86d89904469b95"},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\nmean_squared_error(target_test, prediction)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"36971087dd36b3bc4089147410cee80b5bbe8510"},"cell_type":"markdown","source":"**ROC curve**"},{"metadata":{"trusted":true,"_uuid":"a60474570d32bbde20f30233faf17dc53bb55284"},"cell_type":"code","source":"ycat_pred_proba = cat.predict_proba(X_test)[::,1]\nfpr, tpr, _ = metrics.roc_curve(target_test,  ycat_pred_proba)\nauc = metrics.roc_auc_score(target_test, ycat_pred_proba)\nplt.figure()\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.plot([0, 1], [0, 1], color='navy', linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.plot(fpr, tpr, color='blue', lw=2, label=\"roc curve\" % auc)\nplt.legend(loc=\"lower right\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a38f9b9dccaef66b176ddc99860d2a0e55046101"},"cell_type":"code","source":"confidence_test = cat.predict_proba(X_test)[:,1]*2 -1\nprint(accuracy_score(confidence_test>0,target_test))\nplt.hist(confidence_test, bins='auto')\nplt.title(\"Cat predicted confidence\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9f5fb893e424ffccb401b2787c4ec36f7cbf024e"},"cell_type":"code","source":"# distribution of confidence that will be used as submission\nplt.hist(confidence_test, bins='auto', label='Prediciton')\nplt.hist(op_test, bins='auto',alpha=0.8, label='True data')\nplt.title(\"predicted confidence\")\nplt.legend(loc='best')\nplt.xlim(-1,1)\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}