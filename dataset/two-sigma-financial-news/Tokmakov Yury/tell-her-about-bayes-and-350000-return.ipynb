{"cells":[{"metadata":{"_uuid":"69b0c7798087e1072ab4a3c7379e87e8dfa6f16d"},"cell_type":"markdown","source":"# Introducting  \n\nBayes theorem is the foundation of machine learning. This kernel is about using this in the classification. Everything you see below is informative and entertaining.  \n\nYou will see how a simple non-parametric approach gives a competitive result.  \n  \nAre your friends lately talking about a priori and a posteriori probabilities? You want to keep the conversation going, but these words are not familiar to you.  \nRead this kernel and you will again become the life of the party!"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nfrom matplotlib import pyplot as plt\nplt.style.use('ggplot')\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"from kaggle.competitions import twosigmanews\nenv = twosigmanews.make_env()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"aafcde61b104d2900ddf685fa9cc412e2e5d0952"},"cell_type":"markdown","source":"Lets get training data from enviroment"},{"metadata":{"_uuid":"a5aaffea2fffd1e2ed76416ee68a9f5d65d67be9","trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"market_train_df, news_train_df = env.get_training_data()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a815eddf7505d1ec90ec71c809a435724203da6b"},"cell_type":"markdown","source":"# Summary about data\n\n"},{"metadata":{"_uuid":"a342fdd87dd955b85e33d2028fcb99adc89f28fd"},"cell_type":"markdown","source":"### Missing values"},{"metadata":{"_uuid":"3496e455121c0bb3d9d0de5518dc13d52d989903","trusted":true},"cell_type":"code","source":"import missingno\n\ndef plot_miss(df):\n    missingno.matrix(df)\n    plt.show()\n    \nplot_miss(market_train_df)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3584a66baaac957792ab378d0a3a804f78a51be3"},"cell_type":"markdown","source":"You can see that almost all the observations in the data are filled and we can ignore the missing values.  \n  \nMany out-of-the-box models suggest that the distribution of features is normal. If we look at the distribution of our signs, we will understand that this approach is applicable in our case as much as the circle is square."},{"metadata":{"_uuid":"3d35b428174f1af370e93fd7bf93b5679faad289"},"cell_type":"markdown","source":"### Numeric features in news data"},{"metadata":{"_uuid":"f64c0685a63568d06fe4a5243779b01c5e3aab66","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"def plot_distributions(df, labels, figsize, h, w):\n    i=0\n    fig = plt.figure(figsize=figsize)\n    for l in labels:\n        i+=1\n        fig.add_subplot(h,w,i)\n        plt.tight_layout()\n        plt.title(f'{l}, mean = {df[l].mean():.2f}')\n        lq = df[l].quantile(0.01)\n        uq = df[l].quantile(0.99)\n        feature = df[l].dropna()\n        plt.hist(feature[(feature > lq)&(feature < uq)], density=True, bins=100, alpha=0.7)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dfc70d8ce5393bc97861e7e8cf52b769cc5e0686","scrolled":false},"cell_type":"code","source":"news_labels = ['wordCount', 'sentenceCount', 'companyCount', 'bodySize', 'urgency',\n              'firstMentionSentence', 'relevance', 'sentimentClass',\n              'sentimentNegative', 'sentimentNeutral', 'sentimentPositive',\n              'sentimentWordCount', 'noveltyCount12H', 'noveltyCount24H',\n              'noveltyCount3D', 'noveltyCount5D', 'noveltyCount7D', 'volumeCounts12H',\n              'volumeCounts24H', 'volumeCounts3D', 'volumeCounts5D', 'volumeCounts7D']\n\nplot_distributions(news_train_df[news_labels], news_labels, figsize=(20,25), h=6, w=4)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8c6dc92d23b0bffffbdd16d8765fbdf83f100f4f"},"cell_type":"markdown","source":"### Numeric features in price data  \n\n\nHowever, market data is normally distributed. If all the data in the world were distributed as well, then data scientists would earn much more."},{"metadata":{"_uuid":"0f04d57493a6f8fcac50efe560e600e7c9557df3","trusted":true},"cell_type":"code","source":"market_labels = ['returnsClosePrevMktres10', 'returnsClosePrevMktres1']\n\nplot_distributions(market_train_df[market_labels], market_labels, figsize=(15,5), h=1, w=3)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6febf88e73ddd1fce77632bf5ac8fa14763e95bd"},"cell_type":"markdown","source":"# State truncating and demension reduction  \n\nTo demonstrate the essence of the Bayes equation, we need to translate our continuous features into a limited number of states.  \nIn market data, the most informative are relative features. This is what we will save, and so that there will not be too much of them, we will save only close features.  \nAbove, we have seen that some features in the news do not change their value. Let's drop these features and all the rest that contain something other than numbers from 0 to 9."},{"metadata":{"_uuid":"1359d3c7f82917f05262e425c56bb78cd21e3ec9"},"cell_type":"markdown","source":"![](https://i.imgflip.com/2lphog.jpg)"},{"metadata":{"_uuid":"2f6564a5515ed9a7998a9e7d791f24880b7f6065","trusted":true},"cell_type":"code","source":"market_drop_features = ['volume', 'close', 'open', 'returnsClosePrevRaw1', \n    'returnsOpenPrevRaw1','returnsOpenPrevMktres1', 'returnsClosePrevRaw10', \n    'returnsOpenPrevRaw10', 'returnsOpenPrevMktres10']\nnews_drop_features = ['urgency', 'takeSequence','provider', 'subjects', \n    'audiences','sentimentClass', 'headlineTag', 'sourceTimestamp', 'firstCreated', \n    'sourceId', 'headline', 'marketCommentary', 'assetCodes']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4b2a57754a2016c052e833d6cf4e0233a291f1df"},"cell_type":"markdown","source":"### Clustering sentiment features  \nIn the title of the competition we are urged to use the news, but all we have just a few semantic features. We will try to explore it in several states through clustering.\n"},{"metadata":{"_uuid":"e898bdcec6aed4faee4701b8eda3e4044c2ae8fd","trusted":true},"cell_type":"code","source":"sem_labels = ['sentimentPositive', 'sentimentNegative', 'sentimentNeutral']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"27246f838b32063919829980d1a4c19450df4d35","trusted":true},"cell_type":"code","source":"from sklearn.cluster import MiniBatchKMeans\n\nkmeans = MiniBatchKMeans(n_clusters = 9, init = 'k-means++')\nsemant_kmeans = kmeans.fit_predict(news_train_df[sem_labels].values)\nsemant_kmeans = np.reshape(semant_kmeans, (semant_kmeans.shape[0], 1))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"99fa11a2a770503f227170e111b33ab6c38bdcde"},"cell_type":"markdown","source":"And then we need encode cluster numbers to binary format. I will explain later"},{"metadata":{"trusted":true,"_uuid":"ccea90e44bc42fc05c1b24487a5f6acf6873aa8a"},"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\n\nonehotencoder = OneHotEncoder(categories='auto')\nkmean_dummies = onehotencoder.fit_transform(semant_kmeans).toarray()\n\ndef dummies_concat(df, dummies, pref):\n    dummies = pd.DataFrame(dummies, index=df.index)#dummies[:, 1:]\n    dummies.columns = [pref + str(x) for x in dummies.columns]\n    return pd.concat([df, dummies], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0814792386e1d72b21c2e3100ae665916a1f5a74"},"cell_type":"markdown","source":"### Numeric features in news data\nIf you look at the names of the variables in the news, then there is a suspicion that many of them will correlate with each other. Let's look at their correlation."},{"metadata":{"_kg_hide-input":true,"_uuid":"a35291c0d9feb5dc79e3765169bb31c635731332","trusted":true},"cell_type":"code","source":"def corr_plot(df):\n    corr = df.corr()\n\n    mask = np.zeros_like(corr, dtype=np.bool)\n    mask[np.triu_indices_from(mask)] = True\n\n    f, ax = plt.subplots(figsize=(11, 9))\n    cmap = sns.diverging_palette(220, 10, as_cmap=True)\n    sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n                square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"afb6d6a67f604ed7588eaa3b1fc9c2193c19546e"},"cell_type":"code","source":"num_labels = ['noveltyCount12H', 'noveltyCount24H','noveltyCount3D', 'noveltyCount5D', 'noveltyCount7D',\n             'volumeCounts12H', 'volumeCounts24H', 'volumeCounts3D', 'volumeCounts5D', 'volumeCounts7D',\n             'wordCount', 'sentenceCount', 'companyCount', 'bodySize', 'sentimentWordCount', 'relevance',\n             'firstMentionSentence']\n\ncorr_plot(news_train_df[num_labels])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c04228539824ad32db471c7a7ec5d2ee124035fe"},"cell_type":"markdown","source":"Indeed, many features tell us about the same thing, and the sign of relevance generally explains all other features. Apparently relevance is some kind of formula that is calculated on the basis of all the features presented. I like the formulas, so Iâ€™ll drop everything except this one!\nMoreover, I do not think that it is possible to cluster these features. Because of the exponential distributions, we get just one dominant cluster.  \nLet's take a closer look at relevance.\n"},{"metadata":{"trusted":true,"_uuid":"a82401e4af64b6d82570950125a712cd37662ffe","_kg_hide-input":true},"cell_type":"code","source":"def plot_relevance(s_relevance):\n    plt.title(f'Relevance < 1, {sum(news_train_df[\"relevance\"]<1)/news_train_df.shape[0]*100:.2f} % of data')\n    plt.hist(s_relevance[s_relevance<1], bins=100)\n    plt.show()\n    plt.title(f'Relevance = 1, {sum(news_train_df[\"relevance\"] == 1)/news_train_df.shape[0]*100:.2f} % of data')\n    plt.hist(s_relevance[s_relevance<1], bins=100)\n    plt.hist(s_relevance[s_relevance==1], bins=100)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ea939a7924cd946a32a03249c865a785962374ad"},"cell_type":"code","source":"plot_relevance(news_train_df['relevance'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f21f1c3d1c56b285adb8d572b37c18c30d09df9"},"cell_type":"markdown","source":"To translate continuous features into states, create several functions. Relevance can be equal to 1 or not. Returns may be positive or negative. Also, the Gaussian distribution allows us to tell us in which cases the return is abnormal."},{"metadata":{"trusted":true,"_uuid":"b2c9c528b660d19c86514e5f3630ad9ba28a7894"},"cell_type":"code","source":"def is_one(x):\n    return 1 if x == 1 else 0\n\ndef is_positive(x):\n    return 1 if x > 0 else 0\n\ndef is_abnormal(x, threshold):\n    return 1 if np.abs(x) > threshold else 0","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"15e94d8088be1302fb92d5169d5fba8384723054"},"cell_type":"markdown","source":"# News pooling  \nWe will not go deep, I will immediately say that for one day in the market we have a few news. In order to take them all, you need to do something like a pooling.\nHere, the cluster binarization performed above is useful. For each cluster we will see that it was on that day. And if there are two or more semantic clusters, max pooling will reflect this."},{"metadata":{"trusted":true,"_uuid":"5d133f527f7d298b470f58adc3c1409bf975d0f7"},"cell_type":"code","source":"def max_pooling(df, features, index, agg_method):\n    agg_methods = {k: agg_method for k in features}\n    return df.groupby(index).agg(agg_methods).reset_index()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"65de7f4d7e0b5186cb5a42c4c1fcbfcb77d2344a"},"cell_type":"markdown","source":"## Merging news and price data  \nThe following function will convert market and news data into a binarized state, such as 0100100010..."},{"metadata":{"_uuid":"0fb866040ae14cbddf85d7f50ea71d5a2b73e1fb","trusted":true},"cell_type":"code","source":"def state_reduce(market, news, kmean_dummies):\n    # check positive return\n    market['returnsClosePrevMktres1'] = market['returnsClosePrevMktres1'].apply(is_positive)\n    market['returnsClosePrevMktres10'] = market['returnsClosePrevMktres10'].apply(is_positive)\n    # check abnormal return\n    std = market['returnsClosePrevMktres1'].std()\n    market['abnormalClosePrevMktres1'] = market['returnsClosePrevMktres1'].apply(lambda x: is_abnormal(x, 2.5*std))\n    # check relevance is equal to one\n    news['relevance'] = news['relevance'].apply(is_one)\n    # drop features, which unusefull for us\n    market = market.drop(market_drop_features, axis=1)\n    news = news.drop(news_drop_features + sem_labels + num_labels, axis=1)\n    # add semant clusters\n    news = dummies_concat(news, kmean_dummies, 'sem_cl_')\n    # choose features that will be in state\n    bin_features = [l for l in news.columns if l not in ['time', 'assetName']]\n    # truncat date for merging data\n    market['date'] = market['time'].dt.round('d') \n    news['date'] = news['time'].dt.round('d')\n    # pooling news in one date\n    news = max_pooling(news,  bin_features, ['date', 'assetName'], 'max')\n    # merging data and fill NA\n    market_train_df = pd.merge(market, news, how='left', on=['date', 'assetName']).fillna(2)\n    # reduce state features to str state id\n    state_features = bin_features + ['returnsClosePrevMktres1', 'returnsClosePrevMktres10', 'abnormalClosePrevMktres1']\n    state = market_train_df[state_features[0]].astype(int).astype(str)\n    for col in state_features[1:]:\n        state += market_train_df[col].astype(int).astype(str)\n    market_train_df['state'] = state\n    \n    return market_train_df.drop(state_features + ['date', 'assetName'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d26fd52973b4fa285c9dc9189ed0d589b502b9bb"},"cell_type":"code","source":"%%time\n\nmarket_train_df = state_reduce(market_train_df, news_train_df, kmean_dummies)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5af0bdd1b1501a140a67b0a25918b055442714d4","trusted":true},"cell_type":"code","source":"market_train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3f66f6072d9f3e6be59444469384f2e69ddc1346","trusted":true},"cell_type":"code","source":"print('Count of unique states in train data', len(market_train_df['state'].unique()))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"50e5a05a4231318a0e4a599f49fda3865f61977f"},"cell_type":"markdown","source":"## Bayes  \nFor each state, it is necessary to obtain the posterior probability of a positive return. To do this, it is necessary to calculate all the components of the Bayes equation. We do not touch the negative return, because Positive and negative returns are mutually exclusive.  \n  \n$\\LARGE P(A \\mid B) = \\frac{P(B \\mid A) \\, P(A)}{P(B)}$  \n\nIf you multiply these letters into words, you get the following:  \n\"The probability of event A in the case of observing event B is equal to the product of the probability of event B in case of observing event A and the probability of occurrence of event A divided by the probability of event B\"  \nFor our problem, we can write the equation like this:  \n  \n$\\LARGE P(positiveReturn \\mid State_i) = \\frac{P(State_i \\mid positiveReturn) \\, P(positiveReturn)}{P(State_i)}$\n"},{"metadata":{"trusted":true,"_uuid":"0c4c6d3139a904e5478e1448ff7cce92df234982"},"cell_type":"code","source":"def fit_state_table(df):\n    # positive return observations\n    pos_reward = df[df['returnsOpenNextMktres10'] > 0]\n    # positive return probability\n    pos_reward_prob = pos_reward.shape[0] / df.shape[0]\n    # probability of each state\n    state_probs = train.groupby(['state']).size() / df.shape[0]\n    # probability of each state with positive return\n    pos_state_probs = pos_reward.groupby(['state']).size() / pos_reward.shape[0]\n    # concat all of this to one table\n    state_table = pd.concat([state_probs, pos_state_probs], axis = 1, \n                            keys=['state_prob', 'pos_state_probs'], sort=False).fillna(0)\n    \n    state_table.index.name = 'state'\n    \n    state_table['pos_return_prob'] = state_table['pos_state_probs'] * pos_reward_prob / state_table['state_prob']\n    \n    return state_table","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e2a690b4b67138318a66bcf2b74cd74978e8713e"},"cell_type":"markdown","source":"# Validation\nNow let's see what kind of profit this approach will bring."},{"metadata":{"_uuid":"586a6e13e38f278e336a4bb63751f9fdb0c1a07a","trusted":true},"cell_type":"code","source":"train, test = np.split(market_train_df[['time', 'universe', 'state', 'returnsOpenNextMktres10']], \n                       [int(.8*len(market_train_df))])\nprint(f'State coverage: { test[\"state\"].isin(train[\"state\"]).sum() / len(test) }')\n#test = test[test[\"state\"].isin(train[\"state\"])]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0235173ecb2fc9f43d88c4c83c93a05be63a8625","trusted":true},"cell_type":"code","source":"state_table = fit_state_table(train)\nstate_table.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"287455b8437a9980cab5a0102a92e712f2ce07db"},"cell_type":"markdown","source":"Let's compare two predictive approaches on test data, and for one we'll see what profit we would get for this period on the exchange. Our confidence will be the rate, the percentage of the starting amount."},{"metadata":{"trusted":true,"_uuid":"435148f232b79f470f04c3de5dedfb0dfd2dbf0d"},"cell_type":"code","source":"def max_prob_prediction(df, st):\n    df = pd.merge(df, st[['pos_return_prob']], how='left', on=['state']).fillna(0)\n    return df['pos_return_prob'].apply(lambda p: p if p >= 0.5 else p-1).tolist()\n\ndef diff_prob_prediction(df, st):\n    df = pd.merge(df, st[['pos_return_prob']], how='left', on=['state']).fillna(0)\n    return df['pos_return_prob'].apply(lambda p: 2*p - 1).tolist()\n\ndef all_in_prob_prediction(df, st):\n    df = pd.merge(df, st[['pos_return_prob']], how='left', on=['state']).fillna(0)\n    return df['pos_return_prob'].apply(lambda p: 1 if p >= 0.5 else -1).tolist()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"773ab71b97c59e87a843c14e298c1da512fa3284","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"max_pred = max_prob_prediction(test, state_table)\ndiff_pred = diff_prob_prediction(test, state_table)\nall_in_pred = all_in_prob_prediction(test, state_table)\nmax_rewards = test['returnsOpenNextMktres10'] * max_pred\ndiff_rewards = test['returnsOpenNextMktres10'] * diff_pred\nall_in_rewards = test['returnsOpenNextMktres10'] * all_in_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"77cd3b2874c4fa509f3e625a1931e7d3ebb1b5fc","_kg_hide-input":true},"cell_type":"code","source":"def score(r):\n    return np.mean(r)/np.std(r)\n    \nfig = plt.figure(figsize=(14,7))\nfig.add_subplot(1,2,1)\nplt.tight_layout()\nplt.title(f'Rewards')\nplt.hist(max_rewards.tolist(), bins=100, alpha=0.5,\n         label=f'MAX m={np.mean(max_rewards):.5f}, std={np.std(max_rewards):.5f}')\nplt.hist(diff_rewards.tolist(), bins=100, alpha=0.5,\n         label=f'DIFF m={np.mean(diff_rewards):.5f}, std={np.std(diff_rewards):.5f}')\nplt.hist(all_in_rewards.tolist(), bins=100, alpha=0.5,\n         label=f'ALL_IN m={np.mean(all_in_rewards):.5f}, std={np.std(all_in_rewards):.5f}')\nplt.legend()\nfig.add_subplot(1,2,2)\nplt.tight_layout()\nplt.title(f'Total reward')\nplt.plot(np.cumsum(max_rewards), \n         label=f'MAX sum={np.sum(max_rewards):.2f}, score={score(max_rewards):.4f}')\nplt.plot(np.cumsum(diff_rewards), \n         label=f'DIFF sum={np.sum(diff_rewards):.2f}, score={score(diff_rewards):.4f}')\nplt.plot(np.cumsum(all_in_rewards), \n         label=f'ALL_IN sum={np.sum(all_in_rewards):.2f}, score={score(all_in_rewards):.4f}')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a767b34cbf40354f2f7509daeb2aff57e2470dbd"},"cell_type":"markdown","source":"If you do not call the broker yet, lets make submission."},{"metadata":{"trusted":true,"_uuid":"472b0017d2f801a3c2addb8891cea29ae88d491c","scrolled":false},"cell_type":"code","source":"preddays = env.get_prediction_days()\nfor marketdf, newsdf, predtemplatedf in preddays:\n    \n    # cpredict semant clusters\n    pred_kmeans = kmeans.predict(newsdf[sem_labels].values)\n    pred_kmeans = np.reshape(pred_kmeans, (pred_kmeans.shape[0], 1))\n    \n    # encode semant clusters\n    pred_dummies = onehotencoder.transform(pred_kmeans).toarray()\n    \n    # merge and pool data\n    states = state_reduce(marketdf, newsdf, pred_dummies)\n    \n    # predict confidence\n    preds = max_prob_prediction(states, state_table)\n    \n    #prediction\n    predsdf = pd.DataFrame({'ast':states['assetCode'],'conf':preds})\n    predtemplatedf.loc[predtemplatedf['assetCode'].isin(predsdf.ast), 'confidenceValue'] = predsdf['conf'].values\n    \n    env.predict(predtemplatedf)\n\nenv.write_submission_file()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}