{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport random\n\nimport ast\nimport matplotlib.pyplot as plt\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\n\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\nfrom wordcloud import WordCloud\nimport plotly.graph_objs as go\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n%matplotlib inline\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"71d4caa390464f437c2348c20f873c7c2b6749bb"},"cell_type":"markdown","source":"**Prepare for the data**\n\nGet two datasets seperately."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# get the data from kaggle\nfrom kaggle.competitions import twosigmanews\nenv = twosigmanews.make_env()\nprint('Done!')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"88ede8da74a9ce77447602d7f471fae7f3f13ec7"},"cell_type":"code","source":"# two training data sets, market and news\n(market_train_df, news_train_df) = env.get_training_data()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"af4feb9dafb8ea30bb2f212cf805cd55b767d83d"},"cell_type":"markdown","source":"Print some first several observations of the two data sets."},{"metadata":{"trusted":true,"_uuid":"8e49cfb50e73ac610a115c783d82c2fa615f7d73"},"cell_type":"code","source":"market_train_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8a81f57b562e1879938dc5369e2b81577970a44b"},"cell_type":"code","source":"market_train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c41bd9fd3db3662764cde031535269107643fd02"},"cell_type":"markdown","source":"There are 4072956 rows and 16 columns in the market data."},{"metadata":{"trusted":true,"_uuid":"718ad35384f0e59a92cff824f7f0fef46b3b56b3"},"cell_type":"code","source":"news_train_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"32e789eeecb592ca7ce84d0e579e5e23ea016aa3"},"cell_type":"code","source":"news_train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"00d642b205340f674a68d8efe7a501831cd0a66a"},"cell_type":"markdown","source":"There are 9328750 rows and 35 columns in the news data."},{"metadata":{"_uuid":"792cfd5e79dc650a3620a99985ea4c005d6ffd96"},"cell_type":"markdown","source":"**Pick up needed observations from market data.**\n\nThere are too many observations for different companies and different years. We'd like to pick 100 different companies in different years.\n\n* *Deal with missing values in market data.*\n\nFirstly, there exist some missing values in market data, such as returnsClosePrevMktres1, returnsOpenPrevMktres1,  returnsOpenPrevMktres1. These variables have the effect on the response. "},{"metadata":{"trusted":true,"_uuid":"1737da46af8bdf48fa0004e6d859c4165b35a03c"},"cell_type":"code","source":"#  check out the missing values in market data\nmiss_market = pd.isnull(market_train_df)\nmiss_market.sum(axis=0)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"32709c99fe98a154b6e0340c8fb8ab99f506d30b"},"cell_type":"markdown","source":"From above, we could easily get that the four columns have the exact missing values, for example, the returnsClosePrevMktres1 has 15980 missing value,  returnsOpenPrevMktres1 has 15988 missing values, returnsClosePrevMktres10 has 93010 missing values and\nreturnsOpenPrevMktres10 has 93054 missing values. And because we'd like to reduce the rows in market data in this project. So, for better viewing the contribuiton of each variables, we want to clean data by dropping all the rows which have the missing values in these four variables."},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"1d15bb100099dc7b2056880134a7a628ec17998d"},"cell_type":"code","source":"train_market = market_train_df.dropna()\ntrain_market.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"991892e6bb4221fd58a5801cdc35084da8587996"},"cell_type":"markdown","source":"After dealing with missing values,  there are less rows in market data, which only contains 3979902 observations. Because our purpose is to analysis the market trend in next days. So, it has a big connection with time.  So the next thing is to randomgly pick up 30 comapnies in diferent years.\n\n* *Get the 30 comapnies in different years.*\n\nFirstly, we randomly choose 100 companies based on their different assetCode."},{"metadata":{"trusted":true,"_uuid":"ad13db7e2d6578c34f84e24330718541e3caf85a"},"cell_type":"code","source":"assetCode = train_market.loc[:,'assetCode']\nnp.random.seed(1234)\nsample_asset = np.random.choice(assetCode, 30, replace = False)\nsample_asset","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"51ce0715e7c9c787cc3348a928cc3bf5fd5d911a"},"cell_type":"markdown","source":"Above all, we choose 10 companies randomly without replacement. And then next thing we generate the data based on these 100 companies in different years."},{"metadata":{"trusted":true,"_uuid":"691c8fac0ecd4b91b410d19c99bb78898e7c8bd8"},"cell_type":"code","source":"train_market = train_market[train_market['assetCode'].isin(sample_asset)]\ntrain_market.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d993ba819b5087f868d11dd9ef7c581df53d0fdf"},"cell_type":"code","source":"# see the sample data\ntrain_market.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"383e2d42bac410a1636f94d2a71f251416b7d8f0"},"cell_type":"markdown","source":"Based on the result above, we get the 60383 observations with 30 companies in different year. \n\n* *Observe the target variable*\n\nFor better analysis, we get the quick look at the target variable. According the the proposal, the target variable would be the returnsOpenNextMktres10. "},{"metadata":{"trusted":true,"_uuid":"d2107eeed4b716013001eb57897b4924c6297c2d"},"cell_type":"code","source":"data = []\nfor i in [0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95]:\n    price_df = train_market.groupby('time')['returnsOpenNextMktres10'].quantile(i).reset_index()\n\n    data.append(go.Scatter(\n        x = price_df['time'].dt.strftime(date_format='%Y-%m-%d').values,\n        y = price_df['returnsOpenNextMktres10'].values,\n        name = f'{i} quantile'\n    ))\nlayout = go.Layout(dict(title = \"Trends of returnsOpenNextMktres10\",\n                  yaxis = dict(title = 'Price (USD)'),\n                  ),legend=dict(\n                orientation=\"h\"),)\npy.iplot(dict(data=data, layout=layout), filename='basic-line')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fbdf115876e129a3cd355c387dfd1b05ea43fa15"},"cell_type":"markdown","source":"* *Choose the data after 2010*\n\nBased on the result, we could easily get the Trends of returnsOpenNextMktres10. And based on the quantitles, we could roughly obtain the mean and the variance. Based on the result, it seems that the mean is roughly same whenever the time is, but the variance could be hugely different. For example,  almost in 2009, the variance seems larger than anyone else. \n\nBased on the phenomenon, we investigated what happened at that time so that the variance could be so huge like this.  We found that it was the Financial crisis that caused the variance.  And according to the investigation, we found that the reason of this special Financial crisis is related with the bursting of the US housing bubble, already-rising default rates on \"subprime\" and  increase of adjustable-rate mortgages (ARM). Multiple reasons jointly caused the Financial crisis. \n\nHowever, we'd like to use the news to predict the stock movement, thus, we may not take the Financial crisis into account. Therefore, we'd like to only consider this companies after 2010. Based on the picture above, the mean and the variance seems relatively flat, compared with before."},{"metadata":{"trusted":true,"_uuid":"10749f1e2cc2a0f0775ff7e94a66da52409bf404"},"cell_type":"code","source":"market_train_df = train_market.loc[market_train_df['time'] >= '2010-01-01 22:00:00+0000']\nmarket_train_df.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"acc8e0f6ea84fe12559a4218695bca59874cb121"},"cell_type":"markdown","source":"Finally, we prepare the market data with 43242 rows and 16 columns, which includes the randomly picking up 30 companies from 2010. \n\n* *Observe the final data set and see the trend of the close price*"},{"metadata":{"trusted":true,"_uuid":"860bc26baf6235f6125a7568290d28c701b6c9e3"},"cell_type":"code","source":"data = []\nfor asset in sample_asset:\n    asset_df = market_train_df[(market_train_df['assetCode'] == asset)]\n  \n    data.append(go.Scatter(\n        x = asset_df['time'].dt.strftime(date_format='%Y-%m-%d').values,\n        y = asset_df['close'].values,\n        name = asset\n    ))\nlayout = go.Layout(dict(title = \"Close price of 30 random assets\",\n                  yaxis = dict(title = 'Price (USD)'),\n                  ),legend=dict(\n                orientation=\"h\"))\npy.iplot(dict(data=data, layout=layout), filename='basic-line')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"69a795d223a4873d82bee5bd46fdf67129dca4f8"},"cell_type":"markdown","source":"It seems that this sample is enough random,  some of them are dramatically changing but some of them are flat. Even some of the company has disappearded in certain years.  Based on the result, we would roughly say the dataset could be ensough to illustrate our problem for this specific question.\n"},{"metadata":{"_uuid":"f5e6f4b0dbf4ed10e57245319670c1c8ceef42e4"},"cell_type":"markdown","source":"**Show the outcome**\n\nFor this specific question, we'd like to process the classification for the target variable \"returnsOpenNextMktres10\". Typically, we'd like to view stock's increase or decrease. So, if the outcome is positive, we'd like to consider this stock has been increasing over the 10 days, and vice versa."},{"metadata":{"trusted":true,"_uuid":"6ea88d0789c5cc1ea95b6efe947d767733f8d364"},"cell_type":"code","source":"market_train_df['indicator'] = market_train_df['returnsOpenNextMktres10'].map(lambda x: 1 if x > 0 else -1)\n(market_train_df['indicator'].value_counts() / 43242).plot('bar')\nplt.xticks(rotation=30)\nplt.title('Classification of returnsOpenNextMktres10(target variable)')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"64779e07e8fda7d5bc7826ff36a6ab4eec75aa6b"},"cell_type":"markdown","source":"Overall, the outcom has two class, either increase or decrease. And, obviously, \"1\" representing the increase has about 52% of the total observations, and \"-1\" representing the decrease has roughlt 48%. It is almost equal, for the targeted variables. Under this circumstance, doing classification seems reasonable."},{"metadata":{"_uuid":"3a4a376efefaef92f8788c8877ccb36251fbaa5f"},"cell_type":"markdown","source":"**Deal with the news data in order to merge with the market data**\n\nBecause we'd like to merge two data sets. And intutively, we think that the news before could have an impact on the stock movement. But in which way the news before influence the stock movement could be very important. So, maybe the recent days published news could differently influence our stock movement. Under this circumstance, firstly we should consider :\n\n* choosing how many days of news we should use to predict\n* After choosing days of news, we try to create a new observation which includes all the infomation about recent days' information.\n\nExtract the news data that contains the 30 assetCodes.\n"},{"metadata":{"_kg_hide-output":true,"trusted":true,"_uuid":"9319aef326e52bbf0adf4024c0af86bed5a9ffd3"},"cell_type":"code","source":"news_train_df['indicator'] = news_train_df['assetCodes'].map(lambda x: 1 if any(elem in ast.literal_eval(x) for elem in sample_asset) else 0)\nnews_train_df = news_train_df[news_train_df['indicator'] == 1]\nnews_train_df.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"116327d64661e7d76de2bddbc3a714e6e2034f41"},"cell_type":"markdown","source":"We choose the news data as long as the 'assetCodes' contains any one assetCodes in the sample we picker up before. So, following this criteria, we finally get the 203840 rows and 36 columns of the news data, which is much more smaller than the overall data size in original news data."},{"metadata":{"trusted":true,"_uuid":"ad02423dbb3cbec35208dec58ea6d51ccbd8eb82"},"cell_type":"markdown","source":"* *Explore the vairables*\n\nFirstly, we 'd like to explore the variables in news data and find some correlations between variables. \n\n1. *Headline*\n\nEach news have their unique headline, which could vary from days to days, news provider to news provider.  Hence, the headline is very difficult to view its influence on our target variable. We'd like to consider another straight and simple way to judge whether it has an impact on our stock movement. Based on the pictures above, we could easily get that stock movements of companies exist large difference in year 2013 - 2014,  for example, the campany named CELG.O has dramatically increasing, however, PMCS.O seems no change. We'd like to compare the headline of the two companies in this period, to see whether the wordcloud of the two companies has difference.  If they indeed are different, we could think that the headline influences the stock movements with some cofidence.\n\nFirstly, we choose all the assetCodes that includes the CLEG.0 and based on this selection, we make the wordcloud.  In this way,  we could get a quick glance at the important words in headline corresponding with the news mentioned CLEG.O.\n\nAnd, also, do the same thing for the PMCS.O. Then, we'd like to compare the difference between the top words, for better deciding whethter the headline could influence the target variable"},{"metadata":{"trusted":true,"_uuid":"5cc64bbfb8976bf5de241b4fcef0eeae0073f203"},"cell_type":"code","source":"# pick up the assetCodes that includes CLEG.O\nnews_train_df['CELG_indicator'] = news_train_df['assetCodes'].map(lambda x: 1 if x.find('CELG.O') > 0 else 0)\nnews_CELG = news_train_df[news_train_df['CELG_indicator'] == 1]\n\n\n\n# make the wordcloud\ntext = ' '.join(news_CELG['headline'].str.lower().values[-1000000:])\nwordcloud = WordCloud(max_font_size=None, stopwords=stop, background_color='white',\n                      width=1200, height=1000).generate(text)\nplt.figure(figsize=(12, 8))\nplt.imshow(wordcloud)\nplt.title('Top words in headline of Company CLEG.O')\nplt.axis(\"off\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ef36f85bf8f7e68a8b9c4bc0548b9155c814ee2b"},"cell_type":"code","source":"news_CELG.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3e760b1a360bc0c5ebada6ab0ed4a96374785f6f"},"cell_type":"code","source":"news_CELG['assetName'].unique()\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d9dc8ce1f1b84f474d3ca6c296c3985d06cda0cb"},"cell_type":"markdown","source":"According to the wordcloud, we found that the mostly mentioned is the assetName of the paricular assetCode."},{"metadata":{"_uuid":"069cdd3767e478e8ad1d348cc024b291c0c5251f","trusted":true},"cell_type":"code","source":"# pick up the assetCodes that includes PMCS.O\nnews_train_df['PMCS_indicator'] = news_train_df['assetCodes'].map(lambda x: 1 if x.find('PMCS.O') > 0 else 0)\nnews_PMCS = news_train_df[news_train_df['PMCS_indicator'] == 1]\n\n# make the word cloud\ntext = ' '.join(news_PMCS['headline'].str.lower().values[-1000000:])\nwordcloud = WordCloud(max_font_size=None, stopwords=stop, background_color='white',\n                      width=1200, height=1000).generate(text)\nplt.figure(figsize=(12, 8))\nplt.imshow(wordcloud)\nplt.title('Top words in headline of Company PMCS.O')\nplt.axis(\"off\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b27a5b8fe9cd79bcf7a79a7d17f16c8ce4a1bbcf"},"cell_type":"code","source":"news_PMCS['assetName'].unique()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"92424336d65db239874301c94c11d8df8ee7c3e4"},"cell_type":"markdown","source":"Also, the mostly mentioned in this wordcloud is the assetName for the specific assetCode.\n\nSo, we find that the headline connects closelg with the assetCode, and the two wordclouds significanly differ from each other. We would conclude that the headline indeed have the effect on the stock movements.\n\n2.  *urgency*\n\nUegency represents differentiates story types, \"1\" represents alert, \"3\" represents articles. We want to double check if there exist any other values and the amount of alert and articles.\n"},{"metadata":{"trusted":true,"_uuid":"20a59231a1251247ff8116d74317cc2b041ca309"},"cell_type":"code","source":"(news_train_df['urgency'].value_counts() / 1000000).plot('bar');\nplt.xticks(rotation=30);\nplt.title('Urgency counts (mln)');","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c3bc60a5b38186316024a1c548d8ae2f40390c36"},"cell_type":"markdown","source":"Based on the analysis above, there only exist \"1\" and \"3\" in data, and also the accounts for \"3\" seems double the accounts of \"1\".  And this variables can be seen as the weight of the news importance."},{"metadata":{"_uuid":"dc5c4c1d02b4ea59900c8741cfa29599bfe5958b"},"cell_type":"markdown","source":"3.  *takeSequence*\n\nThis special variablethe means take sequence number of the news item, starting at 1. For a given story, alerts and articles have separate sequences. As analysis above, we know the news item is divided into 2 types, articles and alert. For this variables, we want to specifically get the idea of what the sequence of different types varies.\n\nFirstly, we check when the news type is alert."},{"metadata":{"trusted":true,"_uuid":"352ed7c3b38cb1b6aaf7f945073633e6dc542825"},"cell_type":"code","source":"news_alert = news_train_df[news_train_df['urgency'] == 1]\n(news_alert['takeSequence'].value_counts()[:10] / len(news_alert['takeSequence'])).plot('bar');\nplt.xticks(rotation=30);\nplt.title('takeSequence counts if type is an alert');\nnews_alert['takeSequence'].unique()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ca93b307b2a9124c07aa51b73824a83c7a38b70b"},"cell_type":"markdown","source":"According to the result, if the news is an alert, there would be different sequence for the news item mentioned. Actually, the number is from 1 to 94. To have the stright view, we just plot the most frequenct sequences. And it seems that the news item were mentioned as early as possible. \n\nAnd then we check if the type of news is article, the sequence would be different or not."},{"metadata":{"trusted":true,"_uuid":"5c81ed677318592bfa4aa4cc5a3fbc399af888eb"},"cell_type":"code","source":"news_article = news_train_df[news_train_df['urgency'] == 3]\n(news_article['takeSequence'].value_counts()[:10]/ len(news_article['takeSequence'])).plot('bar');\nplt.xticks(rotation=30);\nplt.title('takeSequence counts if type is an article')\nnews_article['takeSequence'].unique()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"134985cf5cbf131ad925516d36540b6b36fdccb4"},"cell_type":"markdown","source":"According to the result, if the item is an article, the seqeunce for the item mentioned is different from 1 to 95 but less variety campred with the sequence in alert. And also, it seems clearly that if the item is an article, the takeSequencce \"1\" is much larger than others. So we may just have a simple assumption. If the news type is an article, we just think the taksSequence is \"1\"."},{"metadata":{"_uuid":"52a955d2d81a6eaa1ed2e0f25f374cf55dd218d3"},"cell_type":"markdown","source":"4. *headlineTag*\n\nWe have already seen the influence of the headline. And to be specifically, we want to find the headline tag's difference and wonder to know which headline appears much more frequent than anyone else."},{"metadata":{"trusted":true,"_uuid":"443f41192a32fcbb45dfe8aa89f0de5aa9ef0b33","scrolled":true},"cell_type":"code","source":"(news_train_df['headlineTag'].value_counts() / 1000)[:10].plot('barh');\nplt.title('headlineTag counts (thousands)');","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"60e30a6a7fb49e848f219b21f4debb51abe78dd0"},"cell_type":"markdown","source":"Based on the result, it seems that lots of the news don't have the specific headline Tag. And it seems that the \"BRIET\" has the most frequent mentioned in the headline Tag. The headline Tag is abstracted from the healine, so no doubt that it has a strong connection with healine. For our purpose, to avoid the collinearity, we may remove this variable in our model.\n\n"},{"metadata":{"_uuid":"3619ad02f47db754556533c6cfdd5920f121c6e4"},"cell_type":"markdown","source":"**5. provider**\n\nNow, we look at variable 'provide'.  First we count the number for each provider. Then I do a histogram and wordscloud to show my result."},{"metadata":{"trusted":true,"_uuid":"9f9820a7823fde354006c26955760ab805dda138"},"cell_type":"code","source":"news_train_df['provider'].value_counts()[:12].plot(kind='barh')\nplt.xticks(rotation=90)\nplt.title('Provider Count')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e16accab3c4457106d2c3b1e5f936514e4d7a70c"},"cell_type":"code","source":"text = ' '.join(news_train_df['provider'])\nwordcloud = WordCloud(collocations=False).generate(text)\nplt.figure(figsize=(12, 3))\nplt.imshow(wordcloud)\nplt.title('Top words in provider')\nplt.axis(\"off\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"62029447145fe7b2cdf6fca8ff228f8925e54f2e"},"cell_type":"markdown","source":"We can find that most of the privider is RTRS."},{"metadata":{"_uuid":"316e4f94c2989efbb9380d9dd2894a382a069d02"},"cell_type":"markdown","source":"**6. body size**\n\nLet's see body size. First, we look at the statistical summary. We find that the range is very large."},{"metadata":{"trusted":true,"_uuid":"1db671a0c2c35f4a6bdf4897cdcefe7700c6574a"},"cell_type":"code","source":"news_train_df['bodySize'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8950d866b6d82e9ba1a236f71e56efe3a6826985"},"cell_type":"code","source":"plt.boxplot(news_train_df['bodySize'][news_train_df['bodySize'] >0])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1abb3578d87a7058cbe32142130d565b7431af52"},"cell_type":"markdown","source":"From thr boxplot, we find that there are a lot of outliers. So we may consider the different group of body size in the future analysis."},{"metadata":{"_uuid":"1c7fe678182ce3a09ee12f4ac7a6f5f7b8ddfd8d"},"cell_type":"markdown","source":"**word count**\n\nWe want to use sentimentWordCount divide by wordCount to get a new valiable sentiPercent, which should have different values, then we can say that the big value can give a large weight of sentiment."},{"metadata":{"trusted":true,"_uuid":"001c5c524dbc7001c58051ff9349644f83efe21e"},"cell_type":"code","source":"news_train_df['sentiPercent'] = news_train_df['sentimentWordCount'] / news_train_df['wordCount']\nplt.boxplot(news_train_df['sentiPercent'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4202965dd79b2125d261c48a329b9146f3d503f5"},"cell_type":"markdown","source":"From the boxplot, we find that the mean value is almost 1, which means that the news has a lot of attribute."},{"metadata":{"_uuid":"6d752ddb11edf6d163b8c6c763933418d082e32c"},"cell_type":"markdown","source":"**sentenceCount**\n\nWhen we look at the sentenceCount, we use firstMensionSentence divide by sentenceCount to get a new variable firstMentionPercent, we can look at this varibale to find the important of news."},{"metadata":{"trusted":true,"_uuid":"5f6d5dd3011acc9194fb4f32627c47173b54df13"},"cell_type":"code","source":"news_train_df['firstMentionPercent'] = news_train_df['firstMentionSentence'] / news_train_df['sentenceCount']\nplt.boxplot(news_train_df['firstMentionPercent'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2f212c9fbf7c004dfaf8e0c12f41472dd6c0883b"},"cell_type":"markdown","source":"We find that the mean is around 0.2， which is a relatively small number \n"},{"metadata":{"_uuid":"38cdeb679893eb8e264e7ba2d58c79b493f71a49"},"cell_type":"markdown","source":"**sentimentClass**"},{"metadata":{"trusted":true,"_uuid":"43dcab082f8c14824c9daacb8b6cdbe4a4466e88"},"cell_type":"code","source":"\n(news_train_df['sentimentClass'].value_counts() / 1000000).plot('bar');\nplt.xticks(rotation=30);\nplt.title('sentimentCLass counts (mln)');\nfor i, j in zip([-1, 0, 1], ['negative', 'neutral', 'positive']):\n    df_sentiment = news_train_df.loc[news_train_df['sentimentClass'] == i, 'assetName']\n    print(f'Top mentioned companies for {j} sentiment are:')\n    print(df_sentiment.value_counts().head(5))\n    print('')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3b502becd70d0cb518a76cd07c98b2b69a37d4f7"},"cell_type":"markdown","source":"From the trend we could know, it seemlike that we could think the trend is towards more to negtive side. Among these company, Citigroup is most negative, Barclays is most neutral, and Barclays PLC is the most positive."},{"metadata":{"_uuid":"7477170d967feda46ee914984822ea051cd2b04b"},"cell_type":"markdown","source":"**marketCommentary**\n\nFirst, we plot a histogram of different group "},{"metadata":{"trusted":true,"_uuid":"a09edd8b0c00dbfc3ec986e2a5b91d18d8a73912"},"cell_type":"code","source":"(news_train_df['marketCommentary'].value_counts() / 1000000).plot('bar');\nplt.xticks(rotation=30);\nplt.title('marketCommentary counts (mln)');","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"733928588a0c72b9bf18fe4b16f4daad3875231d"},"cell_type":"markdown","source":"From the histogram, we find that most of maketCommentary is at the same group, which value is False. So we can consider that the variable marketCommentary has no effect on our news importance. We can delect this variable.\n"},{"metadata":{"_uuid":"2195dfe5a035c6c1752f8fe2217b316c429f7367"},"cell_type":"markdown","source":"Because we can ignore headlineTag, then we also can ignore headline. \nIn conclusion, we can ignore variables: headlinetag, headline, marketCommentory, provider, \nThen we try to create two new new variables based on 'subject' and 'audience'."},{"metadata":{"_uuid":"2340465919279b720fd87c30a2d7be3bf34ed376"},"cell_type":"markdown","source":""},{"metadata":{"_uuid":"0c99d5f8f0c6c4823013fd011deac73098f3a346"},"cell_type":"markdown","source":"**Merge data**\n\nFor this specific question, we want to use the kernal function to give the different weights for the previous news' information.\n\nPrevious news data may have different influence on the present stock movement. But based on the timeliness of the news, we wonder better evaluate the news' influence. To evaluate this possible effect of a news article in a specific assetCode, for the same assetCode at a given time, we define a time interval that we call the window of influence to represent the previous time. \n\nBut the way we could define the weights of previous time into the same assetCode given a specific time  is, we using the **sliding windows** on time series data. As for the weight, because we only care about the time's effect on news data, then the Exponential Distribution should be the prior consideration.\n\nAs we know before, one place the exponential distribution arises is in the modeling of time or distance between occurrence of events. And its definiation is as follows:\n\nThe random variable X that equals the distance between successive events from a Poisson process with mean number of events λ > 0 per unit interval is an exponential random variable with parameter λ. The probability density function of X is : f(x) = λe^(−λx) for 0 ≤ x < ∞.\n"},{"metadata":{"trusted":true,"_uuid":"5e4558beea2105c3e7d5b185c6d68104cfee2de3"},"cell_type":"code","source":"def generate():\n    proba = random.random()\n    max = 0\n    for i in range(1,M+1):\n         max += (1/2)**i\n         if(proba<= max):\n              return i-1\n    return generate()\nM = 10\nres = [0 for i in range(M+1)]\nfor i in range(1000):\n    res[generate()]+=1\nplt.plot([r/1000 for r in res])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f4afd64b82ea8a74d1b9a34978e1cfd3141f666b"},"cell_type":"markdown","source":"This exponential distribuition can clearly give us the straight kernal function. We give more weights to the closer days and little weights to the more far away days from present days.\n\nHence, we'd like to use the \"sliding windows\"  with exponential distribution as the weighting  for previous days before the given date, to create the new observations given the same assetCode and the date. And this new observation would definitely include all the previous information, which could give more accurate interpletation on the target variable based on the news information.\n\n* Firstly, we create a function that aggregates the news in the same day by calculating the average."},{"metadata":{"trusted":true,"_uuid":"f66c68bf8447f269a02a611aae465c742e586355"},"cell_type":"code","source":"asset_code_dict = {k: v for v, k in enumerate(market_train_df['assetCode'].unique())}#将code数值化，以字典形式存储\nprint(asset_code_dict)\ndrop_columns = [col for col in news_train_df.columns if col not in ['sourceTimestamp', 'urgency', 'takeSequence', 'bodySize', 'companyCount', \n               'sentenceCount', 'firstMentionSentence', 'relevance','firstCreated', 'assetCodes']]\ncolumns_news = ['firstCreated','relevance','sentimentClass','sentimentNegative','sentimentNeutral',\n               'sentimentPositive','noveltyCount12H','noveltyCount24H','noveltyCount3D', 'noveltyCount5D','noveltyCount7D',\n                'volumeCounts12H','volumeCounts24H','volumeCounts3D', 'volumeCounts5D','volumeCounts7D','assetCodes',\n                'sourceTimestamp','assetName','audiences', 'urgency', 'takeSequence', 'bodySize', 'companyCount', \n                 'sentenceCount','sentimentWordCount','wordCount','firstMentionSentence','time']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a072ff126425c1f7a84b54d81c0e004475a638b3"},"cell_type":"markdown","source":"The above is just made the assetCodes into the numeric value, to better process the later analysis."},{"metadata":{"trusted":true,"_uuid":"f74869e77e1e2ad0aaf392ff3bfe503c5aa5f95e"},"cell_type":"code","source":"def news_today(news_df):\n    news_df = news_df[columns_news]\n    news_df['sourceTimestamp']= news_df.sourceTimestamp.dt.hour\n    news_df['firstCreated'] = news_df.firstCreated.dt.date\n    news_df['assetCodesLen'] = news_df['assetCodes'].map(lambda x: len(eval(x)))\n    news_df['assetCodes'] = news_df['assetCodes'].map(lambda x: list(eval(x))[0])\n    news_df['emotion_importance'] = news_df['sentimentWordCount']/news_df['wordCount']*100\n    news_df['asset_count'] = news_df.groupby(['assetName', 'sentimentClass'])['time'].transform('count')\n    news_df['len_audiences'] = news_train_df['audiences'].map(lambda x: len(eval(x)))\n    kcol = ['firstCreated', 'assetCodes']\n    news_df = news_df.groupby(kcol, as_index=False).mean()\n    \n    return news_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a21fb1db1971faa13fbf4611e9b52bc1cd07ed6c"},"cell_type":"code","source":"news_train_day = news_today(news_train_df)\nnews_train_day.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1f6466d208ed486bfe08efdc4d813796bd693fad"},"cell_type":"code","source":"news_train_day.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5ece0df44874437f06268c5524e6fc951250a820"},"cell_type":"markdown","source":"For this special case, we create some new columns to represent some other features.\n\n*asset_count* represents the number of news for the assetCode mentioned at a given day. The higher the number is, means at a specific day, the assetCodes mentioned more frequently. \n\n*len_audiences* represents the number of audiences' type for a given assetCode in a special day. The largest numbers means the population of audience is bigger.\n\nAnd the overall news data becomes 24376 rows and 30 columns after we remove some trival categorical variables and create new variables to capture more features. At the same time, all the news data variables are numerial data."},{"metadata":{"_uuid":"f5fd106e70a57549ea901c86a24447c926ebf7a7"},"cell_type":"markdown","source":"* Secondly, we want use sliding windows with exponential distribution as weights to calculate the former 7-days news. In this way, we could definitely view the whole influence of the news above with considering the timeliness of the news."},{"metadata":{"trusted":true,"_uuid":"8479d39b4d1f57272b79340b36c2d336c971a8aa"},"cell_type":"code","source":"def exp_weighted_mean(assetCodes, day, df, lambd):\n    from datetime import datetime, timedelta\n    #day = datetime.strptime(day, '%Y-%m-%d').date()\n    temp = df[(df['assetCodes'] == assetCodes) & (df['firstCreated'] <= day)]\n    start_day = day - timedelta(days=6)\n    \n    if start_day in temp['firstCreated'].tolist():\n        temp = temp[temp['firstCreated'] >= start_day]\n    time = temp['firstCreated'].tolist()\n    weight = [(time[i] - time[-1]).days for i in range(len(temp))]\n    weight = np.exp(lambd * weight)\n    del temp['firstCreated']\n    del temp['assetCodes']\n    temp = temp.multiply(weight, axis=0)\n    temp = temp.sum() / np.sum(weight)\n    temp['assetCodes'] = assetCodes\n    temp['date'] = day\n    return temp","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bc44fdd2aa323d296d7366d584aab7e3bfe2b576"},"cell_type":"code","source":"news_train = news_train_day.apply(lambda row: exp_weighted_mean(row['assetCodes'], row['firstCreated'], news_train_day, 1), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"00bd514a5cd0bd71824c72c36700f0ad441cd9c2"},"cell_type":"code","source":"news_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"15282e562a38c4cb0695b7572e9a7753287a1ecb"},"cell_type":"markdown","source":"* Thirdly, merge the news data and market data based on the sams assetCode for a given time."},{"metadata":{"trusted":true,"_uuid":"95c2c3169076f6560f0549e9a1ca84f0ca7fa45d"},"cell_type":"code","source":"def data_prep(market_df,news_df):\n    #自定义一些有用的特征，变换数据的格式\n    market_df['date'] = market_df['time'].map(lambda x: x.date())\n    market_df['close_to_open'] = market_df['close'] / market_df['open']\n    market_df.drop(['time'], axis=1, inplace=True)\n    \n    market_df = pd.merge(market_df, news_df, how='left', left_on=['date', 'assetCode'], \n                            right_on=['date', 'assetCodes'])\n    del news_df\n    #market_df['assetCodeT'] = market_df['assetCode'].map(asset_code_dict)\n    market_df = market_df.drop(columns = ['assetCodes','assetName']).fillna(0) \n    return market_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bdfb577ed1d8025b660239ddeb2c91eb0df3b2d1"},"cell_type":"code","source":"print('Merging data ')\ntrain_market = data_prep(market_train_df, news_train)\ntrain_market.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0f3522a9a3e41e88ee5025f09245b191206d4cf7"},"cell_type":"code","source":"market_train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"18387e9c8996b2c1cebfd7dc5dbe9d1cba1be745"},"cell_type":"code","source":"from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis as QDA\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import scale\nfrom sklearn.decomposition import PCA","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"119d3ae3a73b94a7587670e22ce36b6f0291accc"},"cell_type":"code","source":"y=train_market['indicator']\nX=train_market.loc[:, train_market.columns != 'returnsOpenNextMktres10']\nX=X.loc[:,X.columns !='indicator']\nX=X.loc[:,X.columns !='date']\nX_origin=X.loc[:,X.columns !='assetCode']\nX.head()\nX=scale(X_origin)\nclf = QDA()\nclf.fit(X, y)\ny_hat=clf.predict(X)\nprint(accuracy_score(y,y_hat))\nprint(confusion_matrix(y,y_hat))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0f0d18590ae3f2d1b3f2bf3edf2e5c33efec5141"},"cell_type":"code","source":"clf_svc_cv = QDA()\nscores_clf_svc_cv = cross_val_score(clf_svc_cv,X,y,cv=20)\nprint(scores_clf_svc_cv)\nprint(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores_clf_svc_cv.mean(), scores_clf_svc_cv.std() * 2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"55c7b6f919134264bd7a827b477f6ff2b30ee857"},"cell_type":"code","source":"columns_market =['volume','close','open','returnsClosePrevRaw1','returnsOpenPrevRaw1',\n            'returnsClosePrevMktres1','returnsOpenPrevMktres1','returnsClosePrevRaw10',\n           'returnsOpenPrevRaw10','returnsClosePrevMktres10','returnsOpenPrevMktres10',\n            'universe','close_to_open','relevance','sentimentClass','sentimentNegative',\n         'sentimentNeutral','sentimentPositive','noveltyCount12H','noveltyCount24H']\nX_pca=X_origin[columns_market]\nX_pca=scale(X_pca)\nclf_pca = QDA()\nclf_pca.fit(X_pca, y)\ny_hat=clf_pca.predict(X_pca)\nprint(accuracy_score(y,y_hat))\nprint(confusion_matrix(y,y_hat))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"aeec56e6482417aef90404506ac11b1633b82d10"},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ea5faad0fb371942040bc106a05a0d3f231bc3b5"},"cell_type":"code","source":"rf0 = RandomForestClassifier()\nrf0.fit(X,y)\ny_hat1=rf0.predict(X)\nprint(accuracy_score(y,y_hat1))\nprint(confusion_matrix(y,y_hat1))\nimportances=rf0.feature_importances_\nindices=np.argsort(importances)[::-1]\nfor f in range(X.shape[1]):\n    print(\"%2d) %-*s %f\"%(f+1,30,X_origin.columns[f],importances[indices[f]]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"814d98c214e2b0f320fb14c80a96e113b2514b22"},"cell_type":"code","source":"plt.title('Feature Importance ')\nplt.barh(range(X_origin.shape[1]),importances[indices],height=0.8,color='lightblue',align='center')\nplt.yticks(range(X_origin.shape[1]),X_origin.columns)\nplt.ylim([-1,X_origin.shape[1]])\nplt.tight_layout()\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d0927e4ecc2f34c98d9d286808288bcc391a2239"},"cell_type":"code","source":"pca = PCA(n_components=20)\npca.fit(X)\nprint(pca.explained_variance_ratio_)\nindices=np.argsort(pca.explained_variance_ratio_)[::-1]\nfor f in range(20):\n    print(\"%2d) %-*s %f\"%(f+1,30,X_origin.columns[f],pca.explained_variance_ratio_[indices[f]]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5875634ffff9980e865b712a21b2138d660f0278"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}