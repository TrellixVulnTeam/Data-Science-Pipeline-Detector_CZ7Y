{"cells":[{"metadata":{"_uuid":"12d0bf091f843164f46325af5db99ff5dee84be2"},"cell_type":"markdown","source":"The notebook is bases on \"Two Sigma : Using News to Predict Stock Movements\"\nIn this approach I am creating and LSTM(Time Series) model for each Asset Code, provided that the asset code has data till end of 2016.\n\nThe concept of joining the data provided in the 2 CSV files is borrowed from https://www.kaggle.com/bguberfain/a-simple-model-using-the-market-and-news-data. Then I am filtering on asset codes and finding out assets who have time data till the end of 2016.For each of these asset codes I am creating an LSTM model with 50 time steps and storing the mdoels in a dictionary.I plan to use a generic model for asset codes which do not have the complete time data till Dec 2016.\n\nHowever this approach could not be used to submit data from this Kernel, since the prediction data is returned based on each day.For this mode to work we need data in prediciton by asset code.Also the time taken is not supported by the kernel. It would be interesting to see how this model works. For sample I am showing the results with test/train split on 3 asset codess"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"from kaggle.competitions import twosigmanews\n# get twosigma environment\nenv = twosigmanews.make_env()\nprint('Done!')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9f461b87f0d6d3d34f8796e28ac4a042fbafaff9"},"cell_type":"code","source":"#get data\n(market_train_df, news_train_df) = env.get_training_data()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"55fec25f07d6628c97bb0680d7754a59d78ff011"},"cell_type":"code","source":"market_train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ea6b88ab5aafa39b218d9b826227100799eddcaf"},"cell_type":"code","source":"news_train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a4dca78846c87e596506ad9ad41187498a98de98"},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom itertools import chain\nfrom sklearn.preprocessing import MinMaxScaler\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nfrom keras.layers import Dropout\n\nfrom keras.models import model_from_json\nimport os\nimport datetime\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9df2ae592b0aee3f20271faa736e4a30b2532b6f"},"cell_type":"markdown","source":"Flatten the news data based on Asset Code List to have one row per asset code and date entry. Each row has multiple asset codes. The rows will be split into multiple rows based on number of asset codes."},{"metadata":{"trusted":true,"_uuid":"c053f4b38d14ef9f5d15f0662398c9175c078676"},"cell_type":"code","source":"# flattening asset codes and making one row/per asset code\nnews_train_df['assetCodesList'] = news_train_df['assetCodes'].str.findall(f\"'([\\w\\./]+)'\")\nassetCodes_expanded = list(chain(*news_train_df['assetCodesList']))\nassetCodes_index = news_train_df.index.repeat( news_train_df['assetCodesList'].apply(len) )\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4b08876fa5852d003aac5820069ac84748dc3097"},"cell_type":"markdown","source":"If multiple news item appears on same day for same asset code aggregation will be required. "},{"metadata":{"trusted":true,"_uuid":"622b33f70e0f47029a3e755f85b7cfd622ff26a6"},"cell_type":"code","source":"# To aggregate for multiple news data on same asset code and same date\nnews_cols_agg = {\n    'urgency': ['min', 'count'],\n    'takeSequence': ['max'],\n    'bodySize': ['min', 'max', 'mean', 'std'],\n    'wordCount': ['min', 'max', 'mean', 'std'],\n    'sentenceCount': ['min', 'max', 'mean', 'std'],\n    'companyCount': ['min', 'max', 'mean', 'std'],\n    'marketCommentary': ['min', 'max', 'mean', 'std'],\n    'relevance': ['min', 'max', 'mean', 'std'],\n    'sentimentNegative': ['min', 'max', 'mean', 'std'],\n    'sentimentNeutral': ['min', 'max', 'mean', 'std'],\n    'sentimentPositive': ['min', 'max', 'mean', 'std'],\n    'sentimentWordCount': ['min', 'max', 'mean', 'std'],\n    'noveltyCount12H': ['min', 'max', 'mean', 'std'],\n    'noveltyCount24H': ['min', 'max', 'mean', 'std'],\n    'noveltyCount3D': ['min', 'max', 'mean', 'std'],\n    'noveltyCount5D': ['min', 'max', 'mean', 'std'],\n    'noveltyCount7D': ['min', 'max', 'mean', 'std'],\n    'volumeCounts12H': ['min', 'max', 'mean', 'std'],\n    'volumeCounts24H': ['min', 'max', 'mean', 'std'],\n    'volumeCounts3D': ['min', 'max', 'mean', 'std'],\n    'volumeCounts5D': ['min', 'max', 'mean', 'std'],\n    'volumeCounts7D': ['min', 'max', 'mean', 'std']\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"489d138d29ad0a4c8edcf2a039b49dcbe5b0f13a"},"cell_type":"code","source":"df_assetCodes = pd.DataFrame({'level_0': assetCodes_index, 'assetCode': assetCodes_expanded})\n# Create expandaded news (will repeat every assetCodes' row)\nnews_cols = ['time', 'assetCodes'] + sorted(news_cols_agg.keys())\nnews_train_df_expanded = pd.merge(df_assetCodes, news_train_df[news_cols], left_on='level_0', right_index=True, suffixes=(['','_old']))\n\n# Free memory\ndel news_train_df, df_assetCodes\nnews_train_df_expanded.drop('assetCodes', inplace=True, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"58822255b399b687bc59d8258d0907c0f5a4f83d"},"cell_type":"markdown","source":"Adjust the time for joining two dataframes"},{"metadata":{"trusted":true,"_uuid":"df817cfde82f023e7e22319129f75bf30821a70f"},"cell_type":"code","source":"# splitting time by 22H in news to synch with market\nnews_train_df_expanded['time'] = (news_train_df_expanded['time'] - np.timedelta64(22,'h')).dt.ceil('1D')\n\n# Round time of market_train_df to 0h of curret day\nmarket_train_df['time'] = market_train_df['time'].dt.floor('1D')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0ebf84b4ea9163386fd2768506ece81c63453785"},"cell_type":"markdown","source":"Create dummy variables for day of week and month"},{"metadata":{"trusted":true,"_uuid":"190c487dc8ab2c3c433ab11818659dac403ffed3"},"cell_type":"code","source":"# creating dummy variables for day of week and month\nmarket_train_df['dow'] = market_train_df['time'].dt.weekday_name\nmarket_train_df['month'] = market_train_df['time'].dt.month\nmarket_train_df=pd.get_dummies(market_train_df,columns=['dow','month'],drop_first =True,prefix=['d','m'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"62297c8439be96377e4be5585b8a35524daa956c"},"cell_type":"code","source":"#Find list of unique asset codes\nunique_asset_codes = market_train_df['assetCode'].unique()\n# unique_asset_codes = unique_asset_codes[1:]\nrefresh = True\nfile_list = os.listdir(\".\")\nlen(unique_asset_codes)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e9e16eed1a5de0622ff854552aae29636d51e57f"},"cell_type":"markdown","source":"Create a loop over all asset codes and join the market and training data for those asset codes. Missing value in Market data (returnsClose/OpenPrev1/10) are filled with mean. while missing data of News (ie dates where news item is absent) are filled with 0. Then we consider asset codes which have full data available till end of 2016 and build LSTM models with 50 timesteps for them and store in dictionary."},{"metadata":{"trusted":true,"_uuid":"1fd7294fe59641c249ce2b91c9cf5073bb82ca67"},"cell_type":"code","source":"count = 0\nmodel_count =0\nmodel_dict = dict()\ny_test_predict_list = list()\npredicted_asset_list = list()\nfor asset in unique_asset_codes:\n    # for each asset code\n    count = count+1\n    print('count :',count)\n    print('asset :',asset)\n    json_name = asset+\".json\"\n    h5_name = asset+\".h5\"\n    # I am also saving each model by asset names\n    if refresh or json_name not in file_list or h5_name not in file_list:\n        #refresh is just a switch if required to rebuild certain models already created or proceed with those\n        # which does not exit\n        market_asset_data = market_train_df[market_train_df['assetCode']==asset]\n        #I am filling the missing data , which is mostly the first few rows for returnsClose/OpenPrev1/10 with average\n        market_asset_data.fillna(market_asset_data.mean(),axis=0)\n        news_asset_data = news_train_df_expanded[news_train_df_expanded['assetCode']==asset]\n        news_asset_data_agg = news_asset_data.groupby(['time', 'assetCode']).agg(news_cols_agg)\n        del news_asset_data\n        news_asset_data_agg = news_asset_data_agg.apply(np.float32)\n\n        # Flat columns after aggregation\n        news_asset_data_agg.columns = ['_'.join(col).strip() for col in news_asset_data_agg.columns.values]\n        combined_asset_df = market_asset_data.join(news_asset_data_agg, on=['time', 'assetCode'])\n        \n        if combined_asset_df['time'].max().date()==datetime.date(2016,12,30):\n            # if complete time series is present till end of December 2016 then we take this approach\n            predicted_asset_list.append(asset)\n            # fill nana values where news data does not exit\n            fill_na_dict = dict()\n            for col in news_asset_data_agg.columns:\n                fill_na_dict[col]=0\n            del market_asset_data\n            del news_asset_data_agg\n            combined_asset_df.fillna(fill_na_dict,inplace=True)\n            combined_asset_df.drop(combined_asset_df.head(10).index, inplace=True)\n            combined_asset_df.set_index('time',inplace=True)\n            Y = combined_asset_df['returnsOpenNextMktres10'].values\n            X = combined_asset_df.drop(['returnsOpenNextMktres10','assetCode','assetName'],axis=1).values\n            sc = MinMaxScaler(feature_range=(0,1))\n            X = sc.fit_transform(X)\n\n            x_input = []\n            y_input = []\n            # take 50 time steps for lstm input (samples, time_steps,features)\n            for i in range(50,X.shape[0]):\n                x_input.append(X[i-50:i,:])\n                y_input.append(Y[i-1])\n            x_input,y_input = np.array(x_input),np.array(y_input)\n            x_train = x_input[0:round(0.9*x_input.shape[0]),:]\n            y_train = y_input[0:round(0.9*x_input.shape[0])]\n            x_test = x_input[round(0.9*x_input.shape[0]):,:]\n            y_test = y_input[round(0.9*x_input.shape[0]):]\n\n            #regression based LSTM model\n            regresser = Sequential()\n            regresser.add(LSTM(units = 50, return_sequences = True, input_shape=(x_train.shape[1],x_train.shape[2])))\n            regresser.add(Dropout(0.5))\n\n            regresser.add(LSTM(units = 50, return_sequences = True))\n            regresser.add(Dropout(0.5))\n\n            regresser.add(LSTM(units = 50, return_sequences = True))\n            regresser.add(Dropout(0.5))\n\n            regresser.add(LSTM(units = 50))\n            regresser.add(Dropout(0.5))\n\n            regresser.add(Dense(units = 1))\n            regresser.compile(optimizer = 'adam', loss = 'mean_squared_error')\n            regresser.fit(x_train,y_train,epochs=100,batch_size=32)\n\n            #model_dict[asset] = regresser\n            regresser.save_weights(h5_name)\n            model_json = regresser.to_json()\n            with open(json_name, \"w\") as json_file:\n                json_file.write(model_json)\n            \n            y_predict = regresser.predict(x_test)\n            # scoring and storing\n            score_test = regresser.evaluate(x_test,y_test)\n            score_train = regresser.evaluate(x_train,y_train)\n            detail = dict()\n            detail['model'] = regresser\n            detail['score_train'] = score_train\n            detail['score_test'] = score_test\n            model_dict[asset] = detail\n            y_test_predict_list.append({'y_test':y_test,'y_predict':y_predict})\n            print(score_test)\n            model_count = model_count+1\n            print('no of model created ',model_count)\n            if model_count == 3:\n                break\n            # print(regresser.evaluate(x_test))\n    \n    \n    \n    #break","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e66055f24762b02602ac151e956cae83a8c2d0f5"},"cell_type":"code","source":"print('train-',model_dict[predicted_asset_list[0]]['score_train'])\nprint('test-',model_dict[predicted_asset_list[0]]['score_test'])\nplt.plot(y_test_predict_list[0]['y_test'])\nplt.plot(y_test_predict_list[0]['y_predict'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"17047d2e7f60be03c79c5baa9338959c462688ad"},"cell_type":"code","source":"print('train-',model_dict[predicted_asset_list[1]]['score_train'])\nprint('test-',model_dict[predicted_asset_list[1]]['score_test'])\nplt.plot(y_test_predict_list[1]['y_test'])\nplt.plot(y_test_predict_list[1]['y_predict'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6efc0beb4c70716a591f2e01dba547fb225b319b"},"cell_type":"code","source":"print('train-',model_dict[predicted_asset_list[2]]['score_train'])\nprint('test-',model_dict[predicted_asset_list[2]]['score_test'])\nplt.plot(y_test_predict_list[2]['y_test'])\nplt.plot(y_test_predict_list[2]['y_predict'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7f6f1eabc39f487a754642b438336bceb7341c74"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}