{"cells":[{"metadata":{"_uuid":"3b334ba5e96ded19f69f84b57f77df1ed2e44239"},"cell_type":"markdown","source":"### B - News\n### C - Combinational\n### news data needs to be combined with market first to map with date and target.\n### data_processing() outputs a df with combined data first.\n### data_slice() then outputs df_news for B and a cleaned df for C.\n### All missing data are dropped once merged. Because once mapped with data and assetCode, news_df will have lots of missing data.\n### both classifier predict() bin_target [0,1] and the predict_proba() confidence value which can be converted to competition confidence value by predict_proba()[:, 1]*2-1."},{"metadata":{"trusted":true,"_uuid":"6039cc94371e0b283ac4a0061d2baff499e7711b"},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport gc\n\nfrom sklearn import *\nimport time\nfrom lightgbm import LGBMClassifier\nfrom sklearn.linear_model import LogisticRegression\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import confusion_matrix\n\nimport matplotlib.pyplot as plt\nfrom mlxtend.evaluate import confusion_matrix\nfrom mlxtend.plotting import plot_confusion_matrix\nimport seaborn as sns\n%matplotlib inline\n\nimport matplotlib as mpl\nmpl.rcParams['axes.titlesize'] = 20\nmpl.rcParams['axes.labelsize'] = 16\nmpl.rcParams['xtick.labelsize'] = 16\nmpl.rcParams['ytick.labelsize'] = 16","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"from kaggle.competitions import twosigmanews\n# You can only call make_env() once, so don't lose it!\nenv = twosigmanews.make_env()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c20fa6deeac9d374c98774abd90bdc76b023ee63"},"cell_type":"code","source":"(market_train_df, news_train_df) = env.get_training_data()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a79fe114f2b3a52d387e93ab69490694ee494602"},"cell_type":"markdown","source":"## Functions for data processing."},{"metadata":{"trusted":true,"_uuid":"2ea139ebe858d5d9baef015335dc93c9e96f89c2"},"cell_type":"code","source":"### Process market data.\ndef market_process(market_train_df):\n    \n    market_train_df['time'] = market_train_df.time.dt.date\n    market_train_df['bartrend'] = market_train_df['close'] / market_train_df['open']\n    market_train_df['average'] = (market_train_df['close'] + market_train_df['open'])/2\n    market_train_df['pricevolume'] = market_train_df['volume'] * market_train_df['close']\n    \n    # drop nans or not?\n    #market_train_df.dropna(axis=0, inplace=True)\n    market_train_df.drop('assetName', axis=1, inplace=True)\n\n    # Set datatype to float32 to save space\n    float_cols = {c: 'float32' for c in market_train_df.columns if c not in ['assetCode', 'time']}\n    \n    return market_train_df.astype(float_cols)\n\n### process news data.\ndef news_process(news_train_df):\n    \n    news_train_df['time'] = news_train_df.time.dt.date\n    news_train_df['position'] = news_train_df['firstMentionSentence'] / news_train_df['sentenceCount']\n    news_train_df['coverage'] = news_train_df['sentimentWordCount'] / news_train_df['wordCount']\n    droplist_for_now = ['sourceTimestamp','firstCreated','subjects','audiences','headline','assetName']\n    news_train_df.drop(droplist_for_now, axis=1, inplace=True)\n    \n    # factorize the following three\n    for col in ['headlineTag', 'provider', 'sourceId', 'marketCommentary']:\n        news_train_df[col], uniques = pd.factorize(news_train_df[col])\n        del uniques\n    \n    # Remove {} and '' from assetCodes column\n    news_train_df['assetCodes'] = news_train_df['assetCodes'].apply(lambda x: x[1:-1].replace(\"'\", \"\"))\n    return news_train_df\n\n## Unstack assetCodes.\ndef unstack_asset_codes(news_train_df):\n    codes = []\n    indexes = []\n    for i, values in news_train_df['assetCodes'].iteritems():\n        explode = values.split(\", \")\n        codes.extend(explode)\n        repeat_index = [int(i)]*len(explode)\n        indexes.extend(repeat_index)\n    index_df = pd.DataFrame({'news_index': indexes, 'assetCode': codes})\n    del codes, indexes\n    gc.collect()\n    return index_df\n\n## Merge news on index\ndef merge_news_on_index(news_train_df, index_df):\n    news_train_df['news_index'] = news_train_df.index.copy()\n\n    # Merge news on unstacked assets\n    news_unstack_df = index_df.merge(news_train_df, how='left', on='news_index')\n    news_unstack_df.drop(['news_index', 'assetCodes'], axis=1, inplace=True)\n    return news_unstack_df\n\n## Comine multiple news reports for same assets on same day.\ndef group_news(news_frame):\n    \n    aggregations = ['mean']\n    gp = news_frame.groupby(['assetCode', 'time']).agg(aggregations)\n    gp.columns = pd.Index([\"{}_{}\".format(e[0], e[1]) for e in gp.columns.tolist()])\n    gp.reset_index(inplace=True)\n    # Set datatype to float32\n    float_cols = {c: 'float32' for c in gp.columns if c not in ['assetCode', 'time']}\n    return gp.astype(float_cols)\n\n### Merge market and news data\ndef merge(market_train_df,news_agg_df):\n    \n    df = market_train_df.merge(news_agg_df, how='left', on=['time','assetCode'])\n    # drop nans or not?\n    #df.dropna(axis=0, inplace=True)\n    \n    del market_train_df, news_agg_df\n    return df\n\n######################################################\n\ndef data_processing(market_train_df, news_train_df):\n    ## Market\n    market_train_df = market_process(market_train_df)\n    print(\"Market data shape: \", market_train_df.shape)\n    \n    ## News\n    news_train_df = news_process(news_train_df)\n    index_df = unstack_asset_codes(news_train_df)\n    news_unstack_df = merge_news_on_index(news_train_df, index_df)\n    del news_train_df, index_df\n    news_agg_df = group_news(news_unstack_df)\n    del news_unstack_df\n    print('News data shape: ', news_agg_df.shape)\n          \n    ## Merge\n    df = merge(market_train_df,news_agg_df)\n    print('Merged shape: ', df.shape)\n    \n    df.dropna(axis=0, inplace=True)\n    print('wo missing shape: ', df.shape)\n    \n    gc.collect()\n    return df","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"477ba549b5c9e58e9f5b3fa933d4388c38d399b2"},"cell_type":"markdown","source":"## Data processing.\n### Get df_news and df."},{"metadata":{"trusted":true,"_uuid":"e072e158693c59874ea414487a64db09c7c42769"},"cell_type":"code","source":"df = data_processing(market_train_df, news_train_df)\n\n# extract useful data.\ndates = df.time\nnum_target = df.returnsOpenNextMktres10.astype('float32')\nbin_target = (df.returnsOpenNextMktres10 >= 0).astype('int8')\nuniverse = df.universe.astype('int8')\n\n#Slice out df_news for LR. Clean df for lgb.\ndef data_slice(df):\n    # Drop columns that are not features\n    df.drop(['returnsOpenNextMktres10', 'universe', 'assetCode', 'time'], axis=1, inplace=True)\n    \n    market_column = df.columns.tolist()[:14] #14\n    news_column = df.columns.tolist()[14:] #29\n    \n    # df_news for B.\n    df_news = df[news_column]\n    print('df_news shape: ', df_news.shape)\n    \n    # df for C.\n    drop_list = ['takeSequence_mean','provider_mean','firstMentionSentence_mean',\n                'headlineTag_mean','marketCommentary_mean',\n                'noveltyCount12H_mean','noveltyCount24H_mean','noveltyCount3D_mean','sourceId_mean',\n                'noveltyCount5D_mean','noveltyCount7D_mean','urgency_mean','sentimentClass_mean']\n    df.drop(drop_list, axis=1, inplace=True)\n    print('df shape: ', df.shape)\n    \n    return df_news, df\n\ndf_news, df = data_slice(df)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c71a787afc1306f129bbea8dafed5658294b37c5"},"cell_type":"markdown","source":"## Split data for training."},{"metadata":{"trusted":true,"_uuid":"aea498af5dd25013377f99f08fb2a5eac2d41ff0"},"cell_type":"code","source":"# random sample split\ntrain_index, test_index = model_selection.train_test_split(df.index.values, test_size=0.25, \n                                                           random_state = 11)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cb2403ef7113794ae3203a9830ea164e211e1290"},"cell_type":"markdown","source":"## B - Train logistic regression classifier for news only.\nIt will not converge even with iteraction=400, so don't bother adding more."},{"metadata":{"trusted":true,"_uuid":"f0605b7ad72513a294369678e31707454650e127"},"cell_type":"code","source":"def train_news_model(df_news):\n    t = time.time()\n    print('Fitting Up')\n    clf = LogisticRegression(solver='sag', max_iter=200, n_jobs=4) # Stochastic Average Gradient: fast\n    clf.fit(df_news.loc[train_index],bin_target.loc[train_index])\n    print('Done')\n    print(f'Done, time = {time.time() - t}')\n    return clf\n\ntrainedModel_B = train_news_model(df_news)\nprint(trainedModel_B)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e6be89e1febed90df405d5a72c961b6a9fe8c89d"},"cell_type":"markdown","source":"## B - LR Evaluation"},{"metadata":{"trusted":true,"_uuid":"32d7e09d42d439b7301473b40697ebc75cbe35f5"},"cell_type":"code","source":"print(\"LR clf accuracy : %f\" % \\\n      accuracy_score(trainedModel_B.predict(df_news.loc[test_index]),\n                     bin_target.loc[test_index]))\nprint(\"LR clf AUC : %f\" % \\\n      roc_auc_score(bin_target.loc[test_index].values,\n                    trainedModel_B.predict_proba(df_news.loc[test_index])[:, 1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e82d9c9cdaf77454053846bf9ddf6fec82c68a14"},"cell_type":"code","source":"plt.hist(trainedModel_B.predict_proba(df_news.loc[test_index])[:, 1]*2-1, \n         bins='auto', alpha=0.3, color='darkorange')\n#plt.legend(['Ground truth', 'Predicted'])\nplt.xlabel(\"Confidence\")\nplt.ylabel(\"Count\")\nplt.title(\"predicted confidence\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3e7d99f7d553b1f2e6e84336da141ef614fdb4ca"},"cell_type":"code","source":"cfm = confusion_matrix(y_target=np.array(bin_target.loc[test_index]), \n                       y_predicted=trainedModel_B.predict(df_news.loc[test_index]).tolist())\nfig, ax = plot_confusion_matrix(conf_mat=cfm)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a61ffc90bc2d1a3a9bd7f8353dec05faa34faa8e"},"cell_type":"markdown","source":"## C - Train lgb classifier for market+news combo."},{"metadata":{"trusted":true,"_uuid":"da818b8742cf6c2fc48e2f11aa565a1938c65453"},"cell_type":"code","source":"def train_combo_model(df):\n    ## best parameters for lgb.\n    lgb = LGBMClassifier(\n        objective='binary',\n        boosting='gbdt',\n        learning_rate = 0.05,\n        max_depth = 8,\n        num_leaves = 80,\n        n_estimators = 400,\n        bagging_fraction = 0.8,\n        feature_fraction = 0.9)\n\n    t = time.time()\n    print('Fitting Up')\n    lgb.fit(df.loc[train_index],bin_target.loc[train_index])\n    print('Done')\n    print(f'Done, time = {time.time() - t}')\n    return lgb\n\ntrainedModel_C = train_combo_model(df)\nprint(trainedModel_C)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"45926a64e07e6f31291e0e3810b82a64b917abde"},"cell_type":"code","source":"print(\"lgb accuracy : %f\" % \\\n      accuracy_score(trainedModel_C.predict(df.loc[test_index]),\n                     bin_target.loc[test_index]))\nprint(\"lgb AUC : %f\" % \\\n      roc_auc_score(bin_target.loc[test_index].values,\n                    trainedModel_C.predict_proba(df.loc[test_index])[:, 1]))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2e3a267ea3149403c49ff59515a1a669ca2d1f9f"},"cell_type":"markdown","source":"## Restart the Kernel to run your code again\nIn order to combat cheating, you are only allowed to call `make_env` or iterate through `get_prediction_days` once per Kernel run.  However, while you're iterating on your model it's reasonable to try something out, change the model a bit, and try it again.  Unfortunately, if you try to simply re-run the code, or even refresh the browser page, you'll still be running on the same Kernel execution session you had been running before, and the `twosigmanews` module will still throw errors.  To get around this, you need to explicitly restart your Kernel execution session, which you can do by pressing the Restart button in the Kernel Editor's bottom Console tab:\n![Restart button](https://i.imgur.com/hudu8jF.png)"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}