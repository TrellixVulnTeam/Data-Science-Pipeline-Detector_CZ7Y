{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\nfrom itertools import chain\nimport seaborn as sns\nfrom joblib import Parallel, delayed\nimport multiprocessing\nimport time\n\nimport lightgbm as lgb\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"from kaggle.competitions import twosigmanews\n# You can only call make_env() once, so don't lose it!\nenv = twosigmanews.make_env()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(m_train_df, n_train_df) = env.get_training_data()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# returnsClosePrevMktres1, returnsOpenPrevMktres1, returnsClosePrevMktres10, returnsOpenPrevMktres10\nglobal truncList\n\ntruncList = ['returnsClosePrevRaw1', 'returnsOpenPrevRaw1',\n             'returnsClosePrevMktres1', 'returnsOpenPrevMktres1',\n             'returnsClosePrevRaw10', 'returnsOpenPrevRaw10',\n             'returnsClosePrevMktres10', 'returnsOpenPrevMktres10']\n\n# Drop years with high fluctuation\ndef transer_MarketDate(df):\n    df['date'] = df['time'].dt.strftime('%Y%m%d').astype(int)\n    df = df[df['date'] > 20101231].reset_index()\n    df.drop(['index'], axis=1, inplace=True)\n    return df\n\n# Fix wrong (open | close) price\ndef refix_diffWrongPrice(df, maxLine, minLine):\n    assert isinstance(df, pd.DataFrame)\n    assert len(df) > 0\n    assert maxLine > minLine\n    df['c_o_divff'] = df['close'] / df['open']\n    # Fix wrong closePrice\n    max_divff = df['c_o_divff'].sort_values(ascending=False)\n    max_list = max_divff[max_divff >= maxLine].index.astype(list)\n    print('Fixing wrong closePrice, indexes: ', max_list)\n    for m in max_list:\n        stock_wrongInfo = df[df.index == m]\n        # get column value: date\n        get_nextDayInfo = df[(df['assetCode'] == stock_wrongInfo['assetCode'].values[0]) & \n                             (df['date'] == (stock_wrongInfo['date'].values[0])+1)]\n        # get column value: close\n        trueCloseNext1 = get_nextDayInfo['close'].values[0]\n        # get column value: returnsClosePrevRaw1\n        trueClosePrevRaw1 = get_nextDayInfo['returnsClosePrevRaw1'].values[0]\n        # calculate result\n        df.loc[m, 'close'] = np.array([trueCloseNext1 / (trueClosePrevRaw1 + 1)])\n    print('Fixing wrong closePrice complete !')\n    # Fix wrong openPrice\n    min_divff = df['c_o_divff'].sort_values(ascending=True)\n    min_list = min_divff[min_divff <= minLine].index.astype(list)\n    print('Fixing wrong openPrice, indexes: ', min_list)\n    for m in min_list:\n        stock_wrongInfo = df[df.index == m]\n        # get column value: date\n        get_nextDayInfo = df[(df['assetCode'] == stock_wrongInfo['assetCode'].values[0]) & \n                             (df['date'] == (stock_wrongInfo['date'].values[0])+1)]\n        # get column value: open\n        trueOpenNext1 = get_nextDayInfo['open'].values[0]\n        # get column value: returnsOpenPrevRaw1\n        trueOpenPrevRaw1 = get_nextDayInfo['returnsOpenPrevRaw1'].values[0]\n        # calculate result\n        df.loc[m, 'open'] = np.array([trueOpenNext1 / (trueOpenPrevRaw1 + 1)])\n    print('Fixing wrong openPrice complete !')\n    df.drop(['c_o_divff'], axis=1, inplace=True)\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class TruncOutliter:\n    def __init__(self, df, trans_column, max_value, min_value, resultType):\n        self.df = df\n        self.trans_column = trans_column\n        self.max_value = max_value\n        self.min_value = min_value\n        self.resultType = resultType\n        \n    def _trunc_outliter(self,s):\n        if s < self.min_value:\n            return self.min_value\n        elif s > self.max_value:\n            return self.max_value\n        else:\n            return s\n        \n    def trunc(self):\n        start = time.time()\n        pool = multiprocessing.Pool(processes=3)\n        result = pool.map(self._trunc_outliter, self.df.loc[:, self.trans_column])\n        print('truncating -- time cost : {0:.2f}'.format(time.time() - start))\n        df_trunc = pd.DataFrame(data=result, columns=[self.trans_column], dtype=self.resultType)\n#         final_df = self.df.drop([self.trans_column], axis=1)\n#         final_df = pd.concat([final_df, df_trunc], axis=1)\n        pool.close()\n        #del result, df_trunc\n        print('column -- {} -- completed !!'.format(self.trans_column))\n        return df_trunc\n\n    \ndef trunc_ouliter(columns, df, coef=1.5):\n    if coef < 1.5:\n        coef = 1.5\n    assert isinstance(columns, list)\n    assert len(df.columns) == len(set(columns).union(set(df.columns)))\n    for col in columns:\n        if df[col].isnull().sum() > 0:\n            fill_na_50 = df[df[col].notnull()][col].describe()['50%']\n            df[col].fillna(fill_na_50, inplace=True)\n        col_q1 = df[col].describe()['25%']\n        col_q3 = df[col].describe()['75%']\n        _min = col_q1 - (col_q3 - col_q1) * coef\n        _max = col_q3 + (col_q3 - col_q1) * coef\n        truncObj = TruncOutliter(df, col, _max, _min, 'float32')\n        _df = truncObj.trunc()\n        df.drop([col], axis=1, inplace=True)\n        df = pd.concat([df, _df], axis=1)\n    return df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Extract Market Features**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def extract_MarketFeatures(market_train_df):\n    start_time = time.time()\n    print('extract_MarketFeatures -- start...')\n    # Extract: divdiff_volume & divdiff_volume_labeled\n    market_train_df['avg_volume'] = market_train_df.groupby(by=['date'])['volume'].transform('mean')\n    market_train_df['divdiff_volume'] = market_train_df['volume'] / market_train_df['avg_volume']\n    market_train_df['divdiff_volume_labeled'] = (market_train_df['divdiff_volume'] >= 1).astype(int)\n    # Extract: price_divff & price_divff_labeled\n    market_train_df['price_divff'] = market_train_df['close'] / market_train_df['open']\n    market_train_df['price_divff_labeled'] = (market_train_df['price_divff'] >= 1).astype(int)\n    # Extract: vol_price_div & vol_price_div_labeled\n    market_train_df['vol_price_div'] = market_train_df['price_divff'] * market_train_df['divdiff_volume']\n    market_train_df['vol_price_div_labeled'] = (market_train_df['vol_price_div'] >= 1).astype(int)\n    # Extract: vol_price_div_sqrt & vol_price_div_sqrt_labeled\n    market_train_df['vol_price_div_sqrt'] = market_train_df['price_divff'].apply(np.square) * market_train_df['divdiff_volume'].apply(np.square)\n    market_train_df['vol_price_div_sqrt_labeled'] = (market_train_df['vol_price_div_sqrt'] >= 1).astype(int)\n    # Extract: all_1_return & all_1_return_labeled\n    market_train_df['all_1_return'] = market_train_df['returnsClosePrevRaw1'] * market_train_df['returnsOpenPrevRaw1'] * market_train_df['returnsClosePrevMktres1'] * market_train_df['returnsOpenPrevMktres1']\n    market_train_df['all_1_return_labeled'] = (market_train_df['all_1_return'] > 0).astype(int)\n\n    # Extract: all_1_return_p_d & all_1_return_p_d_labeled\n    # market_train_df['all_1_return_p_d'] = (market_train_df['returnsClosePrevRaw1'] + market_train_df['returnsOpenPrevRaw1'] + market_train_df['returnsClosePrevMktres1'] + market_train_df['returnsOpenPrevMktres1']) / ((market_train_df['returnsClosePrevRaw1'] * market_train_df['returnsOpenPrevRaw1'] * market_train_df['returnsClosePrevMktres1'] * market_train_df['returnsOpenPrevMktres1']) + 1)###\n    # market_train_df['all_1_return_p_d_labeled'] = (market_train_df['all_1_return_p_d'] > 0).astype(int)\n\n    # Extract: all_1_return_cont_p & all_1_return_cont_p_labeled\n    market_train_df['all_1_return_cont_p'] = market_train_df['returnsClosePrevRaw1'] + market_train_df['returnsOpenPrevRaw1'] + market_train_df['returnsClosePrevMktres1'] + market_train_df['returnsOpenPrevMktres1']\n    market_train_df['all_1_return_cont_p_labeled'] = (market_train_df['all_1_return_cont_p'] > 0).astype(int)\n    # Extract: ret_c_o_pr10 & ret_c_o_pr10_labeled\n    market_train_df['ret_c_o_pr10'] = (market_train_df['returnsClosePrevRaw10'] + 0.00001) / (market_train_df['returnsOpenPrevRaw10'] + 0.00001)\n    market_train_df['ret_c_o_pr10_labeled'] = (market_train_df['ret_c_o_pr10'] >= 1).astype(int)\n    # Extract: ret_c_o_mk10 & ret_c_o_mk10_labeled\n    market_train_df['ret_c_o_mk10'] = (market_train_df['returnsClosePrevMktres10'] + 0.00001) / (market_train_df['returnsOpenPrevMktres10'] + 0.00001)\n    market_train_df['ret_c_o_mk10_labeled'] = (market_train_df['ret_c_o_mk10'] >= 1).astype(int)\n\n    # Extract: label\n    if 'returnsOpenNextMktres10' in market_train_df.columns:\n        market_train_df['label'] = (market_train_df['returnsOpenNextMktres10'] > 0).astype(int)\n    print('extract_MarketFeatures -- completed! cost time :{} s'.format(round(time.time() - start_time, 2)))\n    \n    return market_train_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Check Market Null Values**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def check_nullValues(df):\n    df_columns = df.columns\n    df_total_len = df.shape[0]\n    null_nums = []\n    null_percents = []\n    for c in df_columns:\n        null_num = df[c].isnull().sum()\n        null_nums.append(null_num)\n        null_percents.append(null_num / df_total_len * 100)\n    return pd.DataFrame({'col_name':df.columns, 'null_nums':null_nums, 'null_percents(%)':null_percents})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> **News Data**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_news_dropList():\n    return ['sourceTimestamp','firstCreated','sourceId','headline','takeSequence','provider', \n            'bodySize','headlineTag','marketCommentary','subjects','audiences',\n            'assetName','urgency', 'sentimentClass', 'companyCount', 'relevance', \n            'noveltyCount12H', 'noveltyCount24H','sentenceCount','wordCount',\n            'firstMentionSentence','sentimentWordCount',\n            'noveltyCount3D', 'noveltyCount5D', 'noveltyCount7D', 'volumeCounts12H',\n            'volumeCounts24H', 'volumeCounts3D', 'volumeCounts5D', 'volumeCounts7D',]\ndef dorp_useless_newsInfo(news_train_df):\n    try:\n        news_train_df.drop(get_news_dropList(), axis=1, inplace=True)\n    except KeyError as e:\n        print('dorp_useless_newsInfo -- KeyError')\n    return news_train_df\n\ndef transform_news_info(news_train_df):\n    start_time = time.time()\n    print('transform_news_info -- start...')\n    try:\n        news_train_df['time'] = news_train_df['time'].dt.strftime('%Y%m%d').astype(int)\n    except AttributeError as e:\n        print('transform_news_info -- news_train_df[time] -- AttributeError')\n    # extracting assetCode from assetCodes\n    news_train_df['assetCodes'] = news_train_df['assetCodes'].str.findall(f\"'([\\w\\./]+)'\")\n    assetCodes_expanded = list(chain(*news_train_df['assetCodes']))\n    assetCodes_indexes = news_train_df.index.repeat(news_train_df['assetCodes'].apply(len))\n    \n    assert len(assetCodes_expanded) == len(assetCodes_indexes)\n    \n    df_assetCodes = pd.DataFrame({'lv_0':assetCodes_indexes, 'assetCode':assetCodes_expanded})\n    # merge asset codes data\n    news_train_df_expanded = pd.merge(df_assetCodes, news_train_df, left_on='lv_0', right_index=True, suffixes=('', '_old'))\n    # for saving precious memory\n    del assetCodes_expanded, assetCodes_indexes, df_assetCodes\n    print('transform_news_info -- completed! cost time :{} s'.format(round(time.time() - start_time, 2)))\n    return news_train_df_expanded\n\n# Extract News Features\ndef extract_NewsData(news_train_df):\n    #firstMentionSentence , sentenceCount\n    news_train_df['position'] = news_train_df['firstMentionSentence'] / news_train_df['sentenceCount']\n    news_train_df['coverage'] = news_train_df['sentimentWordCount'] / news_train_df['wordCount']\n    return news_train_df\n\nclass FillSentiment:\n    \n    def __init__(self, df, senti_name, sg_name):\n        self.df = df\n        self.senti_name = senti_name\n        self.sg_name = sg_name\n\n    def _fill_sentiment(self, s):\n        if np.isnan(s[0]):\n            return s[1]\n        else:\n            return s[0]\n\n    def fillin(self):\n        f_start_time = time.time()\n        pool = multiprocessing.Pool(processes=3)\n        try:\n            self.df[self.senti_name] = pool.map(self._fill_sentiment, \n                                           self.df.loc[:, [self.senti_name,self.sg_name]].values)\n        except Exception as e:\n            print('convert_sentiment -- fillin -- {}'.format(e))\n        finally:\n            pool.close()\n            print('Filling -- {} -- null values, cost time :{} s'.format(self.senti_name, \n                                                                        round((time.time() - f_start_time), 2)))\n        return self.df[self.senti_name]\n\n\ndef convert_sentiment(mergeData):\n    \n    def quantile_09(columns):\n        return columns.quantile(0.9)\n    \n    start_time = time.time()\n    print('convert_sentiment -- start...')\n    # get quantile-0.9-open-price data every day\n    group_q09 = mergeData[mergeData['sentimentNegative'].notnull()].groupby(by=['date'])['open'].apply(quantile_09).reset_index()\n    # merge quantile-0.9-open-price into dataframe\n    print('convert_sentiment -- merging -- group_q09')\n    mergeData = pd.merge(mergeData, group_q09, how='left', on=['date'], suffixes=('', '_q09'))\n    # for save memory\n    del group_q09\n    # group out steniments of barometer-stocks every day\n    group_senti = mergeData[(mergeData['sentimentNegative'].notnull()) & \n              (mergeData['open'] >= mergeData['open_q09'])].groupby(by=['date']).agg({'sentimentNegative':'mean',\n                                                                          'sentimentNeutral':'mean', \n                                                                          'sentimentPositive':'mean'}).reset_index()\n    group_senti.rename(columns={'sentimentNegative':'sg_neg',\n                                'sentimentNeutral':'sg_neu',\n                                'sentimentPositive':'sg_pos'}, inplace=True)\n    # merge steniments of barometer-stocks into dataframe\n    print('convert_sentiment -- merging -- group_senti')\n    mergeData = pd.merge(mergeData, group_senti, how='left', on=['date'])\n    # for save memory\n    del group_senti\n    # fill null-value for sentiments\n    mergeData['sentimentNegative'] = FillSentiment(mergeData, 'sentimentNegative', 'sg_neg').fillin()\n    mergeData['sentimentNeutral'] = FillSentiment(mergeData, 'sentimentNeutral', 'sg_neu').fillin()\n    mergeData['sentimentPositive'] = FillSentiment(mergeData, 'sentimentPositive', 'sg_pos').fillin()\n    \n    mergeData.drop(['sg_neg','sg_neu','sg_pos', 'open_q09'], axis=1, inplace=True)\n    mergeData['sentimentClass'] = mergeData[['sentimentNegative','sentimentNeutral','sentimentPositive']].apply(lambda x:np.argmax(x.values), axis=1)\n    print('convert_sentiment -- completed! cost time :{} s'.format(round(time.time() - start_time, 2)))\n    return mergeData","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"global all_data_dropList\nall_data_dropList = ['time', 'assetName', \n                     'volume', 'close', 'open', 'date', 'avg_volume', \n                     'universe', 'returnsOpenNextMktres10']\n\ndef prepare_data(marketdf, newsdf, deleteAfterTransfer=True, train=True):\n    ## Market data\n    marketdf = transer_MarketDate(marketdf)\n    if train:\n        # Fix market WrongPrice\n        marketdf = refix_diffWrongPrice(marketdf, 8, 0.001)\n    # Truncate market ouliters\n    marketdf = trunc_ouliter(truncList, marketdf, 5)\n    # Extract market features\n    marketdf = extract_MarketFeatures(marketdf)\n    \n    ## News data\n    newsdf = dorp_useless_newsInfo(newsdf)\n    newsdf = transform_news_info(newsdf)\n    newsdf.drop(['lv_0', 'assetCodes'], axis=1, inplace=True)\n    # Extract News Features\n    #newsdf = extract_NewsData(newsdf)\n    newsdf.rename(columns={'time':'date'}, inplace=True)\n    newsdf = newsdf.groupby(by=['date', 'assetCode']).agg('mean').reset_index()\n    \n    ## Merge Market & News\n    dataSetDf = pd.merge(marketdf, newsdf, on=['date','assetCode'], how='left', copy=False)\n    if deleteAfterTransfer:\n        del marketdf, newsdf\n    # fill sentiment null values and conver into 3 classes\n    dataSetDf = convert_sentiment(dataSetDf)\n    if train:\n        dataSetDf.drop(all_data_dropList, axis=1, inplace=True)\n    else:\n        tmp_drop_list = [col for col in all_data_dropList if col not in ['universe', 'returnsOpenNextMktres10']]\n        dataSetDf.drop(tmp_drop_list, axis=1, inplace=True)\n    #dataSetDf.iloc[:,-16:] = dataSetDf.iloc[:,-16:].fillna(0)\n    dataSetDf = pd.get_dummies(dataSetDf, columns=['sentimentClass'], drop_first=True)\n    dataSetDf.drop(['sentimentNegative', 'sentimentNeutral', 'sentimentPositive'], axis=1, inplace=True)\n    return dataSetDf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataSetDf = prepare_data(m_train_df, n_train_df, True)\ndataSetDf.drop(['assetCode'], axis=1, inplace=True)\ndata_x = dataSetDf[[col for col in dataSetDf.columns if col != 'label']]\ndata_y = dataSetDf['label']\ndel dataSetDf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_x, test_x, train_y, test_y = train_test_split(data_x, data_y, test_size=0.2, random_state=0)\ndel data_x, data_y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"continueList = []\nsparesList = []\nfor c, d in zip(train_x.columns, train_x.dtypes):\n    if str(d).find('int') != -1:\n        sparesList.append(c)\n    else:\n        continueList.append(c)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ssc_x = StandardScaler()\nssc_x.fit(train_x[continueList])\ntrain_x[continueList] = ssc_x.transform(train_x[continueList])\n\nssc_t_x = StandardScaler()\nssc_t_x.fit(test_x[continueList])\ntest_x[continueList] = ssc_t_x.transform(test_x[continueList])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {\n    'max_depth':-1,\n    'learning_rate':0.01,\n    'num_leaves':60,\n    'bagging_fraction':0.9,\n    'feature_fraction':0.9,\n    'bagging_freq':5,\n    'bagging_seed':2019,\n    'verbosity':-1,\n    'metric':'binary_logloss',\n    'objective':'binary',\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lgb_train = lgb.Dataset(train_x.values, train_y.values)\nlgb_test = lgb.Dataset(test_x.values, test_y.values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lgbmodel = lgb.train(params, lgb_train, 5000, valid_sets=[lgb_train, lgb_test], early_stopping_rounds=100, verbose_eval=200)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_x.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# params_2 = {\n#     'max_depth':-1,\n#     'learning_rate':0.01,\n#     'num_leaves':60,\n#     'bagging_fraction':0.9,\n#     'feature_fraction':0.9,\n#     'bagging_freq':5,\n#     'bagging_seed':2019,\n#     'verbosity':-1,\n#     'metric':'auc',\n#     'objective':'binary',\n# }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lgbmodel_2 = lgb.train(params_2, lgb_train, 2000, valid_sets=[lgb_train, lgb_test], early_stopping_rounds=100, verbose_eval=200)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"generating predictions...\")\npreddays = env.get_prediction_days()\nfor marketdf, newsdf, predtemplatedf in preddays:\n    dataSetDf = prepare_data(marketdf, newsdf, False, False)\n    for_pred_x = dataSetDf[[col for col in dataSetDf.columns if col != 'label']]\n    pred_assetCode = for_pred_x['assetCode'].copy(deep=True)\n    for_pred_x.drop(['assetCode'], axis=1, inplace=True)\n    \n    ssc_x_p = StandardScaler()\n    ssc_x_p.fit(for_pred_x[continueList])\n    for_pred_x[continueList] = ssc_x_p.transform(for_pred_x[continueList])\n    \n    preds = lgbmodel.predict(for_pred_x, num_iteration=lgbmodel.best_iteration) * 2 - 1\n    predsdf = pd.DataFrame({'ast':pred_assetCode,'conf':preds})\n    predtemplatedf['confidenceValue'][predtemplatedf['assetCode'].isin(predsdf.ast)] = predsdf['conf'].values\n    env.predict(predtemplatedf)\n\nenv.write_submission_file()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# env.write_submission_file()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}