{"cells":[{"metadata":{"_uuid":"40fee77bd62d3fa0d8096f59369768229c8cac73"},"cell_type":"markdown","source":"# EDA of the Market and News Data\nIn this kernel, I check the statistical summary of every feature in the two data sets, test the existence of correlation, and visualize the important features. I hope this kernel can help you find important information for your model building."},{"metadata":{"_uuid":"46d150ac2110ea20c98d3fbfbbc7c49ee0b392e6"},"cell_type":"markdown","source":"## Extract Data from the Source"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np, pandas as pd\nimport matplotlib.pyplot as plt\nfrom kaggle.competitions import twosigmanews\n# You can only call make_env() once, so don't lose it!\nenviroment = twosigmanews.make_env()\n(market_train_df, news_train_df) = enviroment.get_training_data()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"## Market Data Alone\n- Check statistics summary for each feature\n- Check correlation among features"},{"metadata":{"_uuid":"d1ddf88b186fd4cda0f742ed9ab9d3fa01fa2c6e"},"cell_type":"markdown","source":"### Check Statistics Summary"},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"dada14e1b64070500c67ba8c6c6b91e7e727908e"},"cell_type":"code","source":"market_train_df.head()\n# market_train_df.describe()\n# min(market_train_df['time']) 2007-02-01\n# max(market_train_df['time']) 2016-12-30\n# len(set(market_train_df['assetCode'])) 3780\n# len(set(market_train_df['assetName'])) 3511","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ffc5ab083db872579c504699594e7e81ca240906"},"cell_type":"code","source":"boxplot=market_train_df.iloc[:,4:-1].boxplot()\nplt.xticks(rotation='vertical')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8b7b0d3c207422a30eefd185788054f912937540"},"cell_type":"markdown","source":"#### Check mapping between assetCode and assetName"},{"metadata":{"trusted":true,"_uuid":"40a2718e7402ab8ede38307862b613c972d40faa"},"cell_type":"code","source":"# check the number of assetcodes that an assetName includes\ndf=market_train_df.groupby('assetName')['assetCode'].nunique()\nprint(np.unique(df,return_counts=True)) # there are 269 assetNames with 0 assetcodes,.... \ndf[df==110] # The assetname is unknown","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1e4bba5a3ed1121d9348a076d228905d9148283e"},"cell_type":"code","source":"# if each code corresponds to only one name\ndf=market_train_df.groupby('assetCode')['assetName'].nunique()\nprint(np.unique(df,return_counts=True)) \n# the result below shows that every code corresponds to only one name.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dc4b832868e2605b47d15eceb20f15a8d2542416"},"cell_type":"code","source":"# check universe for each day\ndf=market_train_df.groupby('time')['universe'].agg(['sum','count'])\ndf['proportion']=df['sum']/df['count']\nplt.hist(df['proportion'])\nplt.show()\nnp.mean(df['proportion'])\n# Generally speaking, the proportion of instruments that are avilable for trading is around 0.6","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8662fa76e5bd34b0c3712719cb9b553a3825805c"},"cell_type":"code","source":"# check missing data\n# check columns with missing data\nmissing_columns=market_train_df.columns[market_train_df.isnull().any()]\nprint(missing_columns) # only adjusted return columns have missing data\n# check proportion of missing data in each column\nfor i in missing_columns:\n    total_observation=market_train_df.shape[0]\n    print(i, ' :', sum(market_train_df['returnsClosePrevMktres1'].isnull())/total_observation)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c6a2fb08e76feab1b0a6fed02a1b9dde0da1e9e2"},"cell_type":"markdown","source":"The amount of missing data is small, and we are predicting the adjusted return for the next 10 days. Therefore, I will focus my analysis on adjusted returns and delete rows with missing data. "},{"metadata":{"_uuid":"43d013fab111a37f51e697a962238a91c7afa5cd"},"cell_type":"markdown","source":"### Check Correlation Among Numeric Features"},{"metadata":{"trusted":true,"_uuid":"d97613e8bc3755a864b85f7b33c577bdebc3be50"},"cell_type":"code","source":"import statsmodels.api as sm\ndf=market_train_df.iloc[:,3:-1]\nfor i in range(df.shape[1]-1):\n    for j in range(i+1,df.shape[1]):\n        trainset=df.iloc[:,[i,j]].dropna()\n        model=sm.OLS(trainset.iloc[:,0],trainset.iloc[:,1]).fit()\n        if abs(model.rsquared_adj)>0.3:\n            print(list(trainset),':',model.rsquared_adj)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d2eaa5eec06b5434de2ddc75b0ef1208c669e61b"},"cell_type":"code","source":"# The correlation between returnsClosePrevMktres10 and returnsClosePrevRaw10, but let me check:\nplt.scatter(market_train_df['returnsClosePrevRaw10'],market_train_df['returnsClosePrevMktres10'])\nplt.show()\n# The plot still shows high correlation.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2fe0721c7256816abe9687a054f648a4790a5e3c"},"cell_type":"markdown","source":"There are strong relationship between raw return  and adjusted return, but this information is not very helpful. There is also relationship between open and close. However, the correlation check does not provide us useful information. I will combine market data and article data together to see if I can find any useful correlation."},{"metadata":{"_uuid":"1864f7ff189f66710c8b7c888b7c8ef00b40eb43"},"cell_type":"markdown","source":"## Article Data Alone\n### Check Statistics Summary"},{"metadata":{"trusted":true,"_uuid":"e4408c9ec33b8451dfa8176f2ab37d51993a6496"},"cell_type":"code","source":"news_train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"395d0d51caaf06ab41b8571711f4e87f8a69139e"},"cell_type":"code","source":"news_train_df.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8434f6439f0778a3810cf92d3aca282cfcc112f7"},"cell_type":"markdown","source":"#### Understand sentiment"},{"metadata":{"trusted":true,"_uuid":"222f968587310b613831f133efd4a16022ee3cf6"},"cell_type":"code","source":"news_train_df.boxplot(column=['sentimentNegative','sentimentNeutral','sentimentPositive'])\nprint(np.unique(news_train_df['sentimentClass'],return_counts=True))\n# Therefore, the distribution of sentiments among assets is kind of even.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9d6a4f384ca0bf22163b9493abfa7ecd98892d9a"},"cell_type":"markdown","source":"#### Understand relevance, comany count  and sentimentwordcount/wordcount as well as their relationships"},{"metadata":{"trusted":true,"_uuid":"15bd2dd3c80a42d18f461e3284015cb8ad5b16c8"},"cell_type":"code","source":"df=news_train_df['relevance'].to_frame()\ndf['sentiment_wordcount_proportion']=news_train_df.sentimentWordCount/news_train_df.wordCount\n# plt.scatter(df.relevance,df.sentiment_wordcount_proportion)\n# plt.show()\n# There is no correlation between the two features.\ndf.boxplot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"aa0593b943a711493554d256ec4b3513d3d8f7ca"},"cell_type":"code","source":"news_train_df.boxplot(column=['companyCount'])\nplt.scatter(news_train_df.wordCount,news_train_df.companyCount)\nplt.show()\nimport statsmodels.api as sm\nmodel=sm.OLS(news_train_df.wordCount,news_train_df.companyCount).fit()\nmodel.summary()\n# Double checking the plot and the model, I find no relationship between wordcount and companycount.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0ae9d5fda1043a295ce5943ba7c8e8dd83b886e9"},"cell_type":"markdown","source":"#### Check novelty count and volumn count"},{"metadata":{"trusted":true,"_uuid":"6f7844c3bf9fb694eb5805301882b7ecdaab2dcf"},"cell_type":"code","source":"df=news_train_df.iloc[:,-10:]\n# x=list(range(df.shape[0]))\n# for i in range(df.shape[1]):\n#     plt.figure()\n#     plt.plot(x,df.iloc[:,i])\n# plt.show()\n# By ploting, I find that the novelty count and volum count increase with time, to check my hypothesis:\nfor i in range(df.shape[1]-1):\n    df['diff_'+str(i)]=df.iloc[:,i+1]-df.iloc[:,i]\n    print(i,'ï¼š' , sum(df['diff_'+str(i)]<0))\n# Therefore, the hypothesis is true.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c176bbad7d4d008041f6668b7f698ee624afab65"},"cell_type":"code","source":"# To check if the increase only affected by time\ndf=news_train_df.iloc[:,-10:-5]\nfor i in range(df.shape[1]-1):\n    df['diff_'+str(i)]=df.iloc[:,i+1]-df.iloc[:,i]\ndf.iloc[:,-4:].boxplot()\ndf.iloc[:,-4:].describe()\n# Therefore, only minority of assets increase their novelty count with time. I will check if the increase contribute to the prediction","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"300c1c066deaaec9b00527f3dd1e42f9d44c5074"},"cell_type":"code","source":"df=news_train_df.iloc[:,-5:]\nfor i in range(df.shape[1]-1):\n    df['diff_'+str(i)]=df.iloc[:,i+1]-df.iloc[:,i]\ndf.iloc[:,-4:].boxplot()\ndf.iloc[:,-4:].describe()\n# Therefore, only minority of assets increase their volumn count with time. I will check if the increase contribute to the prediction","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e77f7520916c3e5c7027efee862a3a885ec73aa7"},"cell_type":"markdown","source":"#### Understand urgency, takeSequence and the relationship between urgency and takeSequence"},{"metadata":{"trusted":true,"_uuid":"bb8c991d371af04d4ae30e40dac5900e707fb559"},"cell_type":"code","source":"# news_train_df.groupby(['urgency','takeSequence'])['takeSequence'].count()\n# print(min(news_train_df.takeSequence[news_train_df.urgency==1]))\n# print(max(news_train_df.takeSequence[news_train_df.urgency==1]))\n# print(min(news_train_df.takeSequence[news_train_df.urgency==3]))\n# print(max(news_train_df.takeSequence[news_train_df.urgency==3]))\n# There is no correlation between urgency and takeSequence","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f66d885ae2b845be861191d251d6bdbf939e5af0"},"cell_type":"markdown","source":"#### Check the assetCode and assetNames\n- If the assetcodes and assetnames in the two data sets are the same\n- If the mapping between assetName and assetCode is the same in news data and market data\n"},{"metadata":{"trusted":true,"_uuid":"17e5c152e1aa460eafb2894317d6854ef5bfa061"},"cell_type":"code","source":"# check if there are 3780 unique assetcode in news data\nassetcode_list=[]\nfor i in range(news_train_df.shape[0]):\n    assetcode_list.append(list(eval(news_train_df.assetCodes[i])))\n    if i%100==0:\n        if len(set([j for i in assetcode_list for j in i]))>3780:\n            print('Unique number of assetcode in news data becomes larger than 3780 before the ', i, ' row in the news data set. The total number of rows in the data is ',news_train_df.shape[0],'.')\n            break\n        else: \n            continue\n# there are much more asset codes in the news data set ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"709977544906d9b3db2591d61f467dc2b7ef13bc"},"cell_type":"code","source":"# check the number of unique assetcode in news data\nd={'codes':[i for i in news_train_df.assetCodes]}\ndf=pd.DataFrame(data=d)\ncodes=df.codes.apply(lambda x: list(eval(x))).tolist()\ncodes_news_unique=set([j for i in codes for j in i])\nlen(codes_news_unique) # There are 14293 unique asset codes in the news data set","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0adf3c32bddae17ae3b1e74700c1a50e884c13a0"},"cell_type":"code","source":"# check the overlap of assetcodes in both data sets\nac_list_market=set(market_train_df.assetCode)\nlen(list(codes_news_unique & ac_list_market)) # 3663","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"891b1f3d441ab09cd05b7100cbabd192076f7703"},"cell_type":"markdown","source":"There are 3780 unique assetcodes in the market data sets, but there are much more assetcodes (14293) in the news data set, with 3663 assetcodes listed in the both data sets. Therefore, I do not think assetcode is a good key used to join the two data sets. "},{"metadata":{"trusted":true,"_uuid":"9b5e61b82b2082a6845ed60e88c20a07a0d91e64"},"cell_type":"code","source":"# check assetNames\nprint('Number of unique assetName in market data:',len(set(market_train_df.assetName.tolist())))\nprint('Number of unique assetName in news data:',len(set(news_train_df.assetName.tolist())))\nprint('Number of unique assetName in both data sets:',len(set(market_train_df.assetName.tolist()) & set(news_train_df.assetName.tolist())))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ebfc3ba8bfcfdcc50779370cfbaab36c8ea53525"},"cell_type":"code","source":"# Check if the mapping between assetcode and assetname is the same for those in the both data sets:\ndf_news=news_train_df[['assetName','assetCodes']].sort_values('assetName').drop_duplicates()\ndf_news=df_news.groupby('assetName')['assetCodes'].apply(lambda x: ', '.join(x)).reset_index()\ndf_market=market_train_df[['assetName','assetCode']].sort_values('assetName').drop_duplicates()\ndf_market=df_market.groupby('assetName')['assetCode'].apply(lambda x: ', '.join(x)).reset_index()\ndf_market.columns=['assetName','assetCodeM']\ndf=df_market.join(df_news.set_index('assetName'),on='assetName')\ndf","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"defc5cf031ea05ef590cd600987587200b2ffd55"},"cell_type":"markdown","source":"As shown here, there are more assetcodes in the news dataset, so the mapping is a little bit different. There are more assetcodes in the news dataset.  I think we can use the assetname or assetcode with time as my key to join the two datasets."},{"metadata":{"_uuid":"c01460b87d47b024102ab946879e7b8a0366b55d"},"cell_type":"markdown","source":"#### Check sourceId\nI want to know if sourceId is a unique identifier."},{"metadata":{"trusted":true,"_uuid":"bd0b528c03d4536948fa5a54db8e6fe96d70d1dd"},"cell_type":"code","source":"len(set(news_train_df.sourceId))/len(news_train_df.sourceId)\n# SouceID is not a unique identifier","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0ac11d935a429d2a6f49a00258ddd9ed04a04ef7"},"cell_type":"markdown","source":"#### Understand grouping tags:\nThere are 4 grouping methods mentioned in the data set, including `headlineTag`,`subjects`,  `sourceId`, and `assetName`. I want to see if a group of assets enjoy the same type of sentiment within a day. Moreover, I can use NLP to form another grouping method."},{"metadata":{"trusted":true,"_uuid":"dd1204e18c479b886b2f022ec25b5e09ef3dfff1"},"cell_type":"code","source":"print(len(set(news_train_df.headlineTag))) # 163 different headlineTag\nprint(len(set(news_train_df.subjects))) # 1733963 different combination of subjects -- too many to use\nprint(len(set(news_train_df.sourceId))) # 6340206 different combination of subjects -- too many to use","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2ea9f78f12f1fabb3509b1873f1281a0d6c2bc4d"},"cell_type":"markdown","source":"Since there are too many unique subjects and souceId, it is not a good grouping method. Also, I found above that every assetName generally contains 1-3 assetcodes, so the group size will be small if I use assetName as a grouping method. Moreover, assetName is considered to be used as part of a reference key. Therefore, I will try to use headlineTag as my groupping method first."},{"metadata":{"_uuid":"28c95ff898367f889a71def4c5a7c101c59d4c2e"},"cell_type":"markdown","source":"## Combine market data and news data together\n- Check key for the join \n- Check relationship between sentiment and returns "},{"metadata":{"_uuid":"72596e764691bde80ba5f34acf1ce56091214597"},"cell_type":"markdown","source":"### Key for the join\nI will try the combination of \n- time and assetName in market data with firstCreated and assetName in news data\n- time and assetCode in market data with firstCreated and assetCodes in news data\n\nAs we discovered above, one `assetCode` corresponds to only one `assetName`, and one name only includes one to three codes. Since there are many missing data in `assetCodes` column in the news data set, and we want to use the grouping method to include the influence of articles about similar assets on one asset, `assetName` is a better identifier used to join the two data sets. \n\nAccording to the analysis above, I believe that the relevance, company count, increase of novlety counts as well as volume count alongside time, and the proportion of sentimentwordcount over total wordcount could affect sentiments' influence on the return. Moreover, I will keep headTag as a grouping method. Therefore, I include those features in the news dataset. I will also include `headline` in case I need to apply NLP in the future. The feature `bodySize` is included since it is shown useful in the [kernel](https://www.kaggle.com/artgor/eda-feature-engineering-and-everything). However, i do not quite understand the meaning of the feature.\n\nIn the market dataset, since we focus on adjusted return and want to predict for adjusted return in the next 10 days, I will drop missing rows and columns with raw returns in the market data set. "},{"metadata":{"trusted":true,"_uuid":"ab11853db116bb7947a03fc303413d6f2ab26d18"},"cell_type":"code","source":"def prepare_data(market_df, news_df,key='name'):\n    # market data\n    market_df['time'] = market_df.time.dt.date\n    market_df['price_diff']=market_df['close']-market_df['open']\n    market_df=market_df.drop(['returnsClosePrevRaw1','returnsOpenPrevRaw1','returnsClosePrevRaw10',\n                    'returnsOpenPrevRaw10','open','close'],axis=1)\n    # news data\n    counts=news_df.columns[25:35] # get the names of the novelty and volumn count columns\n    news_df['firstCreated']=news_df.firstCreated.dt.date\n    news_df['increase_novelty_count']=news_df['noveltyCount7D']-news_df['noveltyCount12H']\n    news_df['increase_volume_count']=news_df['noveltyCount7D']-news_df['noveltyCount12H']\n    news_df['sentiment_wordcount/wordcount']=news_df['sentimentWordCount']/news_df['wordCount']\n    news_df=news_df.drop(['time','sourceTimestamp','sourceId','urgency','takeSequence','provider','subjects',\n                 'audiences','marketCommentary','sentenceCount','sentenceCount','wordCount',\n                 'firstMentionSentence','sentimentWordCount'],axis=1)\n    news_df=news_df.drop(counts,axis=1)\n    # left join using time and assetname\n    if key=='name':\n        data=pd.merge(market_df,news_df,how='left',left_on=['time','assetName'],right_on=['firstCreated','assetName'])\n    if key=='code':\n        # since every code has only one name, we just need to drop where assetcode in market does not exist in the news data\n        data=pd.merge(market_df,news_df,how='left',left_on=['time','assetName'],right_on=['firstCreated','assetName']) \n        # remove where assetcodes is na in the news data set\n        data=data.dropna(subset=['assetCodes'])\n        def tell(x,y):\n            result=x in y\n            return result\n        index=data[['assetCode','assetCodes']].apply(lambda x:tell(x['assetCode'],x['assetCodes']),axis=1)\n        data=data[index]\n    return data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ce5ab513270db67b6e4cc95eeb688a39a676cf23"},"cell_type":"code","source":"market_df=market_train_df.copy()\nnews_df=news_train_df.copy()\ndf=prepare_data(market_df,news_df,key='name')\nprint(df.shape)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"98240deab0acd7ce6afef74ce4baa237f768a919"},"cell_type":"code","source":"# This method takes too long to run\n# market_df=market_train_df.copy()\n# news_df=news_train_df.copy()\n# df2=prepare_data(market_df,news_df,key='code')\n# print(df2.shape)\n# df2.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9dc0bfd8adb4dc3c9161de020117eb9fb20fa621"},"cell_type":"code","source":"# combine with assetcode step by step\ndata=df.dropna(subset=['assetCodes'])\ndata.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0f5031122f99803d03e1e49677b963bc87ac81ea"},"cell_type":"markdown","source":"We can find here that if we use `assetCode` as the key to join, we will lose a lot of information, so `assetName` is a better key.\n\nIn the next step, I will put the joint data into models to check prediction accuracy. Moreover, I will examine if  any grouping method can help improve the prediction."},{"metadata":{"_uuid":"8702074ab7b3d5144234690fce01206ee87d48bc"},"cell_type":"markdown","source":""}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}