{"cells":[{"metadata":{"trusted":true,"_uuid":"e95f5904a42a938cff25c88c5739413b1f18ab10"},"cell_type":"code","source":"!pip3 install --upgrade pandas --force-reinstall # restart the runtime \n!pip3 install seaborn==0.9.0","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"\nimport os\n\nfrom numba import jit, vectorize, autojit\nimport datetime\n\nimport time\n\nfrom scipy.cluster.vq import kmeans, vq \n\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.preprocessing import MinMaxScaler, QuantileTransformer \nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\n\nimport copy \n# import os\n# print(os.listdir(\"../input\"))\n# for carbage collection \nimport gc\ngc.enable()\n# to convert string of set to set \nfrom datetime import timedelta\n# import copy\nimport dask\nfrom dask import dataframe as dd\n\n\nimport tensorflow as tf \n\n%matplotlib inline\n\nimport keras\nfrom keras.models import Model, Sequential\nfrom keras.layers import Input, Activation\nfrom keras.layers import Dense, Conv1D, BatchNormalization , Flatten, Reshape, LSTM, RepeatVector, TimeDistributed\nfrom keras.activations import tanh\nfrom keras.utils import plot_model\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns \nsns.set()\nsns.set_context(None)\nplt.rcParams['figure.figsize'] = [25, 10]\n\nprint('Pandas', pd.__version__)\nprint('Seaborn', sns.__version__)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"scrolled":true,"_kg_hide-output":true,"_kg_hide-input":false},"cell_type":"code","source":"from kaggle.competitions import twosigmanews\n\nenv = twosigmanews.make_env()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f81deb2a2085bc4ed350a276a3b093543d2949af"},"cell_type":"markdown","source":"# note\naudiences : we can think of it .. that is the channel for news to reach the clients .. so if the channel is weak then the reached clients will be less than expected so each channel must have a weight \n\nheadlineTag :   is the importancy of news so the news will reach our audiance wich implaies in stock price \n\nmarketCommentary : any general news will be deleted as per it is general and not releated to the market \n\nvolumeCounts12H : maybe this is important "},{"metadata":{"trusted":true,"_uuid":"ee1cf17642fef378be80840c94932317cc9d1cff"},"cell_type":"code","source":"import re\n\n\n# get unique codes - not repeated - in one big set \n@jit(parallel=True )\ndef get_unique_code(cloumn_cat):\n  unique_set = cloumn_cat.cat.categories.values.flatten().tolist()\n  return  set( re.sub(r'[{\\ ]','', \"\".join( unique_set ) ).replace(\"'\",\"\").replace('}',\",\").split(',') ) \n\n@jit(parallel=True )\ndef get_hits_news(el):\n  for code in eval(el):\n    if code in subjects_hits:\n#       next convert this to list of strings then use count function \n      subjects_hits[code] +=1\n\n@jit\ndef get_count_news(column):\n  column.values.map(get_hits_news)\n\n\ndef get_most_repeated(hits) :\n  lis_dict_values =  list(hits.values())\n  codebook, _ = kmeans(lis_dict_values, 2)\n  return { k:v for k, v in  hits.items() if v >  min(codebook) }\n\n\n@jit(parallel=True )\ndef convert_to_stock(item):\n  for code in eval(item):\n    if  code in stock_universe_set: \n      return code  \n\n\n@jit(parallel=True )\ndef sum_sentment(row, subjects):  \n#     print(\"-\",end=\"\")\n    joined_codes = re.sub(r'[{\\ ]','', \"\".join(  row[1] + str( row[2])  ) ).replace(\"'\",\"\").replace('}',\",\").split(',') \n    for index in joined_codes  :\n      if index in subjects:\n        subjects[index]   +=  row[3]\n      \n# @jit\ndef input_total_sentment(grouped_news):\n  subjects = copy.deepcopy(stocks_sectors_dict)\n  # loop over array and set the sentmint \n  _ = [ sum_sentment(row, subjects) for row in grouped_news.values.tolist() ]\n  \n  return subjects","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3198404ce42bdb9da5caf9186c07ae594bff7731"},"cell_type":"code","source":"\n# Scale data\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler(feature_range=(-1, 1))\n\ntarget_index = []\nstocks_sectors_dict = {}\nstock_last_3M = {}\nstock_universe_set = {}\nsubjects_hits = {}\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"5217ecff9b06187a244461025a8c49c6132d3649"},"cell_type":"code","source":"def train_my_model(market_train_df, news_train_df):\n    global target_index, stocks_sectors_dict, stock_last_3M , subjects_hits , stock_universe_set , scaler\n\n    # clean some columns\n    news_train_df_cleaned = news_train_df.drop(columns=['sourceId','bodySize','firstCreated','sourceTimestamp','provider','companyCount','sentenceCount','marketCommentary','wordCount','firstMentionSentence'\n                                                       ,'sentimentWordCount'])\n    del news_train_df\n\n    market_train_df_cleaned = market_train_df #.drop(columns=['assetName'])\n    del market_train_df\n\n    # move any news after closing time 22:00 to next day \n    news_train_df_cleaned['time'] = news_train_df_cleaned['time'] + timedelta(hours=2)\n\n    # get all subjects in one big set \n    subject_set = get_unique_code(news_train_df_cleaned['subjects'])\n    print('subject_set : Done ')\n\n    # get all assetCodes in one big set \n    stock_universe_set = market_train_df_cleaned['assetCode'][market_train_df_cleaned['universe']> 0].unique()\n    print('stock_universe_set : Done ')\n\n\n\n    subjects_hits = dict.fromkeys(subject_set, 0.0)\n\n    # count total hits for each subject to get the most important subjects \n    get_count_news(news_train_df_cleaned['subjects'])\n\n\n    subjects_filtered = get_most_repeated(subjects_hits)\n\n\n    del subjects_hits\n\n\n    stocks_sectors = np.concatenate(( list(stock_universe_set), list(subjects_filtered.keys())))\n\n    stocks_sectors_dict = dict.fromkeys(set(stocks_sectors), 0.0) \n\n    print('Stock and subjects preparation ended ')\n\n\n    ### NEWS\n    news_train_df_cleaned['sentiment'] = ( news_train_df_cleaned['sentimentPositive'] - news_train_df_cleaned['sentimentNegative'] ) * news_train_df_cleaned['relevance'] \n\n    news_df_sentiment = news_train_df_cleaned[['time', 'subjects', 'assetCodes', 'sentiment']].copy()\n\n    ### Stock\n    market_train_df = market_train_df_cleaned[['time', 'assetCode', 'returnsClosePrevMktres1', 'returnsClosePrevMktres10', 'returnsOpenNextMktres10','universe' ]].copy() #, 'universe'\n\n    del market_train_df_cleaned , news_train_df_cleaned\n    \n    print('news ans market minimized ')\n\n    # clean RAM  \n    gc.collect()\n\n\n    # map assets codes in news to stock asset code \n    news_df_sentiment['assetCodes'] = news_df_sentiment['assetCodes'].map(convert_to_stock) \n\n\n\n    stock_last_3M = market_train_df.set_index('time').last('5M')['assetCode'][market_train_df.set_index('time').last('5M')['universe']>0].unique()\n    market_train_df =  market_train_df.drop(['universe'], axis=1)\n\n    #  group by year, month, day \n    sort_news_by_day = news_df_sentiment.groupby([news_df_sentiment.time.dt.year, news_df_sentiment.time.dt.month, news_df_sentiment.time.dt.day]) # days  # 1min 36s per loop \n    sort_stock_by_day = market_train_df.groupby([market_train_df.time.dt.year, market_train_df.time.dt.month, market_train_df.time.dt.day]) # days  # 1min 36s per loop \n\n    print('grouped by days ')\n    print('merge news and stock started ')\n\n    data_set = []\n    for news_name, news_group in sort_news_by_day:\n      total_sentmint_day = input_total_sentment(news_group)\n      total_sentmint_day_df = pd.DataFrame.from_dict(total_sentmint_day, orient='index').reset_index()\n      total_sentmint_day_df.rename(columns={'index': 'assetCode', 0: 'sentiment' }, inplace=True)\n      if news_name in sort_stock_by_day.groups :\n        stock_group = sort_stock_by_day.get_group(news_name)\n        stock_group.drop( columns=['time'], inplace=True)\n        df = pd.merge(total_sentmint_day_df, stock_group , how='left', on=['assetCode'])\n      else:\n        stock_group = pd.DataFrame(columns=('assetCode','returnsClosePrevMktres1','returnsClosePrevMktres10', 'returnsOpenNextMktres10') )\n        df = pd.merge(total_sentmint_day_df, stock_group , how='left', on=['assetCode'])\n      data_set.append( df.values )\n#       print(\"-\",end=\"\")\n\n    \n    print('data set collected')\n\n    np_data_set = np.stack( data_set, axis=0 )\n\n    del data_set, news_df_sentiment, market_train_df, sort_news_by_day, sort_stock_by_day, subject_set,  subjects_filtered \n\n    print('Scaling data')\n\n    # remove nan to zero \n    data_set = np_data_set[:,:,1:].astype(np.float) \n\n    where_are_NaNs = np.isnan(data_set)\n    data_set[where_are_NaNs] = 0\n\n    # prepare the data to scalling process \n    days, codes, data = data_set.shape\n    data_set = data_set.reshape((days, codes*data))\n\n    # scal the data set \n    data_set = scaler.fit_transform( data_set )\n    # reshape back the data \n    data_set = data_set.reshape((days, codes,data))\n\n    # slice data set to features and targets \n    features = data_set[:,:,:-1]\n    code_list = np_data_set[:,:,0]\n    target = data_set[:,:,-1]\n\n    target_index = [indx for indx ,code in enumerate(code_list[0]) if code  in stock_last_3M]\n\n    stock_target =  target[:,target_index]\n\n    print('target shape',target.shape)\n    print('stock_target shape',stock_target.shape)\n    print('features shape ', features.shape)\n\n\n    # roll back two days to get the future of the \n    # axis = 0 to get the roll on just days days \n    # stock_target[:,:,0]  = np.roll( stock_target[:,:,0], -1 , axis=0)\n    \n    output_length = stock_target.shape[1] \n    input_shape = features.shape[1:]\n\n    print('start model creation')\n\n    x_train, x_test, y_train, y_test = train_test_split(features , stock_target , test_size = 0.2, random_state = None)\n\n\n    epochs = 750\n    batch_size = 30\n\n\n    #  model creation \n    x_placeholder = Input(shape=input_shape, name='news_input') \n\n    # hidden\n    layer_conv1 = Conv1D(5, 1 , activation='tanh')(x_placeholder)\n    layer_conv2 = Conv1D(1, 1 , activation='tanh')(layer_conv1)\n\n    layer_flatten = Flatten()(layer_conv2)\n    layer_batch1 = BatchNormalization()(layer_flatten)\n    layer_dense = Dense(1000,  activation='tanh' )(layer_batch1)\n    layer_dense = Dense(200, activation='tanh')(layer_dense)\n\n    layer_dense = Dense(200, activation='tanh')(layer_dense)\n    output = Dense(output_length,  activation='linear')(layer_dense)\n\n    model = Model(inputs=x_placeholder,outputs=output)\n    plot_model(model,to_file='demo.png',show_shapes=True)\n\n\n    model.compile(loss='mean_absolute_error', optimizer='adadelta',\n                metrics=['mse'])\n\n\n    print('start fitting the model ')\n    history = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data = (x_test, y_test ),verbose=0)\n\n    result = y_test\n    predicted = model.predict(x_test)\n\n    print('fitting model ended, and model created ')\n\n    # print(hist.history.keys())\n    # \"Loss\"\n    plt.plot(result[:, 10] )\n    plt.plot(predicted[:, 10] )\n    plt.title('model loss') \n    plt.ylabel('change')\n    plt.xlabel('dayes')\n    plt.legend(['test', 'predicted'], loc='upper left')\n    plt.show()\n\n    return model\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d55d6e5b4e7bcccfc29f04ad6f6563f058bea821"},"cell_type":"code","source":"def make_my_predictions(market_obs_df, news_obs_df, predictions_template_df):\n    global target_index, stocks_sectors_dict, stock_last_3M , subjects_hits , scaler\n\n    ### minify NEWS\n    news_obs_df['sentiment'] = ( news_obs_df['sentimentPositive'] - news_obs_df['sentimentNegative'] ) * news_obs_df['relevance'] \n\n    news_obs_df = news_obs_df[['time', 'subjects', 'assetCodes', 'sentiment']].copy()\n\n    ### minify Stock\n    market_obs_df = market_obs_df[[ 'assetCode', 'returnsClosePrevMktres1', 'returnsClosePrevMktres10']].copy() #, 'universe'\n\n    # map assets codes in news to stock asset code \n#     news_obs_df['assetCodes'] = news_obs_df['assetCodes'].map(convert_to_stock) \n    \n\n    total_sentmint_day = input_total_sentment(news_obs_df)\n#     del news_obs_df \n    total_sentmint_day_df = pd.DataFrame.from_dict(total_sentmint_day, orient='index').reset_index()\n    total_sentmint_day_df.rename(columns={'index': 'assetCode', 0: 'sentiment' }, inplace=True)\n    \n    df = pd.merge(total_sentmint_day_df, market_obs_df , how='left', on=['assetCode'])\n    \n\n    np_data_set = df.values\n\n    # remove nan to zero \n    data_set = np_data_set[:,1:].astype(np.float) \n\n    where_are_NaNs = np.isnan(data_set)\n    data_set[where_are_NaNs] = 0.0\n    \n    data_set = np.array([data_set])\n   \n    # prepare the data to scalling process \n    days, codes, data = data_set.shape\n    data_set = data_set.reshape((days, codes*data))\n\n    # scal the data set \n    data_set = scaler.fit_transform( data_set )\n    # reshape back the data \n    data_set = data_set.reshape((days, codes,data))\n\n    # slice data set to features and targets \n    features = data_set\n    code_list = np_data_set[:,0]\n    # predict and get univers codes \n    predicted = model.predict(features)\n    predicted = predicted.flatten()\n\n    #     merge predicted values of univers stock with it stock code  \n    predicted_codes_dict = { code : predicted[i] for i,code in enumerate( code_list[target_index] ) }\n    #     assign each value to  predictions_template_df pandas dataframe \n    predictions_template_df = predictions_template_df.set_index('assetCode')\n    for code, value in predicted_codes_dict.items():\n        if code in predictions_template_df.index:\n            predictions_template_df.at[code,'confidenceValue' ] =  value\n        \n    predictions_template_df['confidenceValue'] = predictions_template_df['confidenceValue'].clip(-1,1)\n    return predictions_template_df.reset_index()\n    \n    \n    \n\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0252266978d8f5cfb589a8e9685bc130a59afc6d","scrolled":false},"cell_type":"code","source":"(market_train_df, news_train_df) = env.get_training_data()\nmodel = train_my_model(market_train_df, news_train_df)\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dd5a79baeb72b4b89509bfa93a3e45cc9d2986a7","scrolled":false},"cell_type":"code","source":"days = env.get_prediction_days()\nfor (market_obs_df, news_obs_df, predictions_template_df) in days:    \n  predictions_df = make_my_predictions(market_obs_df, news_obs_df, predictions_template_df)\n  env.predict(predictions_df)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"436c5be07cac070ce4525c0656238ab5e7f6e39a"},"cell_type":"code","source":"env.write_submission_file()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}