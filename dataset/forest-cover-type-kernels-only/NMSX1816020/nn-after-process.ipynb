{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import StratifiedKFold\n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.preprocessing import MinMaxScaler\n\nimport sklearn\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\n\nfrom keras.models import Sequential\nfrom keras.layers.core import Dense, Activation, Dropout\nfrom keras.optimizers import SGD\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\n\nimport lightgbm as lgb\n\nfrom tqdm import tqdm\n\nimport os\nimport gc\nfrom itertools import combinations, chain\nfrom datetime import datetime\n\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fbb8cf84d0e15a480cababfebefb85d9208f2860"},"cell_type":"code","source":"X=np.array([[1,2],[3,4],[5,6],[7,8],[9,10],[11,12]])\ny=np.array([1,1,1,2,2,2])\nskf=StratifiedKFold(n_splits=3)\n\nfor train_index,test_index in skf.split(X,y):\n    print(\"Train Index:\",train_index,\",Test Index:\",test_index)\n    X_train,X_test=X[train_index],X[test_index]\n    y_train,y_test=y[train_index],y[test_index]","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import tensorflow as tf\n\nfilepath = \"../input/\"\nINPUT_NODE = 47\nOUTPUT_NODE = 7\n\nREGULARIZATION_RATE = 0.0001\nBATCH_SIZE = 100\n\nLEARNING_RATE_BASE = 0.8\nLEARNING_RATE_DECAY = 0.99\n\nTRAINING_STEP = 150000","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"189c9dfaaf149a7753f1c61002dc7cbb8f68dfb0"},"cell_type":"code","source":"train_df = pd.read_csv(filepath + \"train.csv\")\ntest_df = pd.read_csv(filepath + \"test.csv\")\nsmpsb = pd.read_csv(filepath + \"sample_submission.csv\")\n# print(train_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1217db1636f78c7ee68766c29d7d5662283426fc"},"cell_type":"code","source":"def main(train_df, test_df):\n    # this is public leaderboard ratio\n    start = datetime.now()\n    type_ratio = np.array([0.37053, 0.49681, 0.05936, 0.00103, 0.01295, 0.02687, 0.03242])\n    \n    total_df = pd.concat([train_df.iloc[:, :-1], test_df])\n    \n    # Aspect\n    total_df[\"Aspect_Sin\"] = np.sin(np.pi*total_df[\"Aspect\"]/180)\n    total_df[\"Aspect_Cos\"] = np.cos(np.pi*total_df[\"Aspect\"]/180)\n    print(\"Aspect\", (datetime.now() - start).seconds)\n    \n    # Hillshade\n    hillshade_col = [\"Hillshade_9am\", \"Hillshade_Noon\", \"Hillshade_3pm\"]\n    for col1, col2 in combinations(hillshade_col, 2):\n        total_df[col1 + \"_add_\" + col2] = total_df[col2] + total_df[col1]\n        total_df[col1 + \"_dif_\" + col2] = total_df[col2] - total_df[col1]\n        total_df[col1 + \"_div_\" + col2] = (total_df[col2]+0.01) / (total_df[col1]+0.01)\n        total_df[col1 + \"_abs_\" + col2] = np.abs(total_df[col2] - total_df[col1])\n    \n    total_df[\"Hillshade_mean\"] = total_df[hillshade_col].mean(axis=1)\n    total_df[\"Hillshade_std\"] = total_df[hillshade_col].std(axis=1)\n    total_df[\"Hillshade_max\"] = total_df[hillshade_col].max(axis=1)\n    total_df[\"Hillshade_min\"] = total_df[hillshade_col].min(axis=1)\n    print(\"Hillshade\", (datetime.now() - start).seconds)\n    \n    # Hydrology ** I forgot to add arctan\n    total_df[\"Degree_to_Hydrology\"] = ((total_df[\"Vertical_Distance_To_Hydrology\"] + 0.001) /\n                                       (total_df[\"Horizontal_Distance_To_Hydrology\"] + 0.01))\n    \n    # Holizontal\n    horizontal_col = [\"Horizontal_Distance_To_Hydrology\",\n                      \"Horizontal_Distance_To_Roadways\",\n                      \"Horizontal_Distance_To_Fire_Points\"]\n    \n    \n    for col1, col2 in combinations(hillshade_col, 2):\n        total_df[col1 + \"_add_\" + col2] = total_df[col2] + total_df[col1]\n        total_df[col1 + \"_dif_\" + col2] = total_df[col2] - total_df[col1]\n        total_df[col1 + \"_div_\" + col2] = (total_df[col2]+0.01) / (total_df[col1]+0.01)\n        total_df[col1 + \"_abs_\" + col2] = np.abs(total_df[col2] - total_df[col1])\n    print(\"Holizontal\", (datetime.now() - start).seconds)\n    \n    \n    def categorical_post_mean(x):\n        p = (x.values)*type_ratio\n        p = p/p.sum()*x.sum() + 10*type_ratio\n        return p/p.sum()\n    \n    # Wilder\n    wilder = pd.DataFrame([(train_df.iloc[:, 11:15] * np.arange(1, 5)).sum(axis=1),\n                          train_df.Cover_Type]).T\n    wilder.columns = [\"Wilder_Type\", \"Cover_Type\"]\n    wilder[\"one\"] = 1\n    piv = wilder.pivot_table(values=\"one\",\n                             index=\"Wilder_Type\",\n                             columns=\"Cover_Type\",\n                             aggfunc=\"sum\").fillna(0)\n    \n    tmp = pd.DataFrame(piv.apply(categorical_post_mean, axis=1).tolist()).reset_index()\n    tmp[\"index\"] = piv.sum(axis=1).index\n    tmp.columns = [\"Wilder_Type\"] + [\"Wilder_prob_ctype_{}\".format(i) for i in range(1, 8)]\n    tmp[\"Wilder_Type_count\"] = piv.sum(axis=1).values\n    \n    total_df[\"Wilder_Type\"] = (total_df.filter(regex=\"Wilder\") * np.arange(1, 5)).sum(axis=1)\n    total_df = total_df.merge(tmp, on=\"Wilder_Type\", how=\"left\")\n    \n    for i in range(7):\n        total_df.loc[:, \"Wilder_prob_ctype_{}\".format(i+1)] = total_df.loc[:, \"Wilder_prob_ctype_{}\".format(i+1)].fillna(type_ratio[i])\n    total_df.loc[:, \"Wilder_Type_count\"] = total_df.loc[:, \"Wilder_Type_count\"].fillna(0)\n    print(\"Wilder_type\", (datetime.now() - start).seconds)\n    \n    \n    # Soil type\n    soil = pd.DataFrame([(train_df.iloc[:, -41:-1] * np.arange(1, 41)).sum(axis=1),\n                          train_df.Cover_Type]).T\n    soil.columns = [\"Soil_Type\", \"Cover_Type\"]\n    soil[\"one\"] = 1\n    piv = soil.pivot_table(values=\"one\",\n                           index=\"Soil_Type\",\n                           columns=\"Cover_Type\",\n                           aggfunc=\"sum\").fillna(0)\n    \n    tmp = pd.DataFrame(piv.apply(categorical_post_mean, axis=1).tolist()).reset_index()\n    tmp[\"index\"] = piv.sum(axis=1).index\n    tmp.columns = [\"Soil_Type\"] + [\"Soil_prob_ctype_{}\".format(i) for i in range(1, 8)]\n    tmp[\"Soil_Type_count\"] = piv.sum(axis=1).values\n    \n    total_df[\"Soil_Type\"] = (total_df.filter(regex=\"Soil\") * np.arange(1, 41)).sum(axis=1)\n    total_df = total_df.merge(tmp, on=\"Soil_Type\", how=\"left\")\n    \n    for i in range(7):\n        total_df.loc[:, \"Soil_prob_ctype_{}\".format(i+1)] = total_df.loc[:, \"Soil_prob_ctype_{}\".format(i+1)].fillna(type_ratio[i])\n    total_df.loc[:, \"Soil_Type_count\"] = total_df.loc[:, \"Soil_Type_count\"].fillna(0)\n    print(\"Soil_type\", (datetime.now() - start).seconds)\n    \n    icol = total_df.select_dtypes(np.int64).columns\n    fcol = total_df.select_dtypes(np.float64).columns\n    total_df.loc[:, icol] = total_df.loc[:, icol].astype(np.int32)\n    total_df.loc[:, fcol] = total_df.loc[:, fcol].astype(np.float32)\n    return total_df\n\ntotal_df = main(train_df, test_df)\none_col = total_df.filter(regex=\"(Type\\d+)|(Area\\d+)\").columns\ntotal_df = total_df.drop(one_col, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"db9d5ed8106c576aa0ca4ab56d13392a0e899b08"},"cell_type":"code","source":"y = train_df[\"Cover_Type\"].values\nX = total_df[total_df[\"Id\"] <= 15120].drop(\"Id\", axis=1)\nX_test = total_df[total_df[\"Id\"] > 15120].drop(\"Id\", axis=1)\n\n# 训练集的特征列，验证集的特征列，训练集的label列，\n# 验证集的label列 = train_test_split(数据集的特征列，数据集的label列，期望划分的验证集的大小，划分的随机种子值，是否打乱，分层抽样)\n# x_train,x_vali,y_train,y_vali = train_test_split(X,y,test_size=0.1,random_state=0,stratify=y)\n\ngc.collect() #内存回收","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d90876e4e7c8b5ee8832e20bc73bdd974668f197","_kg_hide-output":false},"cell_type":"code","source":"print(X.shape)\n# skf=StratifiedKFold(n_splits=3)\n# features = X.values\n# labels = processLabel(y)\n# testFeature = X_test.values\n# # print(type(testFeature))\n\n# for train_index,vali_index in skf.split(features,y):\n#     X_train, X_vali = features[train_index], features[test_index]\n#     y_train, y_vali = labels[train_index], labels[test_index]\n#     print(y_train.shape)\n#     train_feed = {x:X_train, y_:y_train}\n#     validate_feed = {x:X_vali, y_:y_vali}","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"954f5037065cb165d912532982cceec86428f46b"},"cell_type":"markdown","source":"自定义神经网络，有问题"},{"metadata":{"trusted":true,"_uuid":"c21b9f9163d1a876f402c1b37d777732edc50eea"},"cell_type":"code","source":"def processLabel(labels):\n    t = np.zeros(shape=[labels.shape[0],7])\n    for i in range(labels.shape[0]):\n        t[i, int(labels[i])-1] = 1.0\n    return t\n\ndef forward(input_tensor, weight, bias):\n    return tf.matmul(input_tensor, weight) + bias\n\ndef train(features, labels, testFeature, originLabel):\n    x = tf.placeholder(tf.float32, shape=[None, features.shape[1]], name=\"input\")\n    y_ = tf.placeholder(tf.float32, shape=[None, labels.shape[1]], name=\"y-input\")\n\n    Id = np.linspace(features.shape[0]+1, features.shape[0] + testFeature.shape[0], testFeature.shape[0])\n\n    weight = tf.Variable(tf.truncated_normal([INPUT_NODE, OUTPUT_NODE], stddev=0.1))\n    bias = tf.Variable(tf.constant(0.1,shape=[OUTPUT_NODE]))  \n    y = forward(x, weight, bias)\n\n    global_step = tf.Variable(0, trainable=False)\n\n    cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=y, labels=y_)\n    cross_entropy_mean = tf.reduce_mean(cross_entropy)\n\n    regularizer = tf.contrib.layers.l2_regularizer(REGULARIZATION_RATE)\n    regularization = regularizer(weight)\n    loss = cross_entropy_mean + regularization\n\n    learning_rate = tf.train.exponential_decay(LEARNING_RATE_BASE, global_step, features.shape[0] / BATCH_SIZE,\n                                               LEARNING_RATE_DECAY)\n    train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n\n    correct_predection = tf.equal(tf.arg_max(y, 1), tf.arg_max(y_, 1))\n    accuracy = tf.reduce_mean(tf.cast(correct_predection, tf.float32))\n\n    with tf.Session() as sess:\n        tf.global_variables_initializer().run()\n        validate_feed = {x:features[14000:,:], y_:labels[14000:]}\n        train_feature = features[0:14000,:]\n        train_lable = labels[0:14000]    \n        test_feed = {x:testFeature}\n    \n        for i in range(TRAINING_STEP):\n            xs = train_feature[i%140*100:i%140*100+100, :]\n            ys = train_lable[i%140*100:i%140*100+100]\n            sess.run(train_step, feed_dict={x:xs, y_:ys})\n            if i%1000 == 0 :\n                validate_acc = sess.run(accuracy, feed_dict=validate_feed)\n                print(\"after %g, the vali_acc is %g\" % (i,validate_acc ))\n        validate_acc = sess.run(accuracy, feed_dict=validate_feed)\n        print(\"After training the vali_acc is %g\" %validate_acc)\n        \n        outputTenor = sess.run(y, feed_dict=test_feed)\n        result = sess.run(tf.arg_max(outputTenor, 1)+1)\n        Id = sess.run(tf.to_int32(Id, name=\"ToInt32\"))\n        dataframe = pd.DataFrame({'Id': Id, 'Cover_Type': result}, columns=['Id', 'Cover_Type'])\n        dataframe.to_csv(\"sample_submission.csv\", index=False)\n        \ntrainFeature = X.values\ntrainLabel = processLabel(y)\ntestFeature = X_test.values\ntrain(trainFeature, trainLabel, testFeature, y)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0596dc56f2c0730aa9e80c6aa3afb7cfebc329fc"},"cell_type":"markdown","source":"KNN-Feature"},{"metadata":{"trusted":true,"_uuid":"5840e4cc3698a8d86e6f736724f08cea448681cd"},"cell_type":"code","source":"all_set =  [['Elevation', 500],\n            ['Horizontal_Distance_To_Roadways', 500],\n            ['Horizontal_Distance_To_Fire_Points', 500],\n            ['Horizontal_Distance_To_Hydrology', 500],\n            ['Hillshade_9am', 500],\n            ['Aspect', 500],\n            ['Hillshade_3pm', 500],\n            ['Slope', 500],\n            ['Hillshade_Noon', 500],\n            ['Vertical_Distance_To_Hydrology', 500],\n            ['Elevation_PLUS_Vertical_Distance_To_Hydrology', 200],\n            ['Elevation_PLUS_Hillshade_9am_add_Hillshade_Noon', 200],\n            ['Elevation_PLUS_Aspect', 200],\n            ['Elevation_PLUS_Hillshade_Noon_dif_Hillshade_3pm', 200],\n            ['Elevation_PLUS_Hillshade_Noon_abs_Hillshade_3pm', 200],\n            ['Elevation_PLUS_Hillshade_9am', 200],\n            ['Elevation_PLUS_Horizontal_Distance_To_Hydrology', 200],\n            ['Elevation_PLUS_Horizontal_Distance_To_Roadways', 100],\n            ['Elevation_PLUS_Vertical_Distance_To_Hydrology', 200],\n            ['Wilder_Type_PLUS_Elevation', 500],\n            ['Wilder_Type_PLUS_Hillshade_Noon_div_Hillshade_3pm', 500],\n            ['Wilder_Type_PLUS_Degree_to_Hydrology', 200],\n            ['Wilder_Type_PLUS_Hillshade_9am_div_Hillshade_3pm', 500],\n            ['Wilder_Type_PLUS_Aspect_Cos', 500],\n            ['Hillshade_9am_dif_Hillshade_Noon_PLUS_Hillshade_Noon_dif_Hillshade_3pm', 200],\n            ['Hillshade_Noon_PLUS_Hillshade_3pm', 200],\n            ['Hillshade_Noon_add_Hillshade_3pm_PLUS_Hillshade_Noon_dif_Hillshade_3pm', 200]]\n\n\ndef simple_feature_scores2(clf, cols, test=False, **params):\n    scores = []\n    bscores = []\n    lscores = []\n    \n    X_preds = np.zeros((len(y), 7))\n    scl = StandardScaler().fit(X.loc[:, cols])\n    \n    for train, val in StratifiedKFold(n_splits=10, shuffle=True, random_state=2018).split(X, y):\n        X_train = scl.transform(X.loc[train, cols])\n        X_val = scl.transform(X.loc[val, cols])\n        y_train = y[train]\n        y_val = y[val]\n        C = clf(**params) \n\n        C.fit(X_train, y_train)\n        X_preds[val] = C.predict_proba(X_val)\n        #scores.append(accuracy_score(y_val, C.predict(X_val)))\n        #bscores.append(balanced_accuracy_score(y_val, C.predict(X_val)))\n        #lscores.append(log_loss(y_val, C.predict_proba(X_val), labels=list(range(1, 8))))\n    \n    if test:\n        X_test_select = scl.transform(X_test.loc[:, cols])\n        C = clf(**params)\n        C.fit(scl.transform(X.loc[:, cols]), y)\n        X_test_preds = C.predict_proba(X_test_select)\n    else:\n        X_test_preds = None\n    return scores, bscores, lscores, X_preds, X_test_preds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7067f4d2828fa7aeeab95770ec01c5de77aa62d3"},"cell_type":"code","source":"import warnings\nimport gc\nfrom multiprocessing import Pool\n\nwarnings.filterwarnings(\"ignore\")\n\npreds = []\ntest_preds = []\nfor colname, neighbor in tqdm(all_set):\n    gc.collect()\n    #print(colname, depth)\n    ts, tbs, ls, pred, test_pred = simple_feature_scores2(KNeighborsClassifier,\n                                                          colname.split(\"_PLUS_\"),\n                                                          test=True,\n                                                          n_neighbors=neighbor)\n    preds.append(pred)\n    test_preds.append(test_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2e4614a343145bb026ed8c846235e9acea5c9b61"},"cell_type":"code","source":"cols = list(chain.from_iterable([[col[0] + \"_KNN_{}\".format(i) for i in range(1, 8)] for col in all_set]))\nknn_train_df = pd.DataFrame(np.hstack(preds)).astype(np.float32)\nknn_train_df.columns = cols\nknn_test_df = pd.DataFrame(np.hstack(test_preds)).astype(np.float32)\nknn_test_df.columns = cols","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"99e6451cc3c7624b0edb08422591c0c079c4dbca"},"cell_type":"markdown","source":"DT_features"},{"metadata":{"trusted":true,"_uuid":"77e63ddcbb912b06162a27c1f59f72c847d700a3"},"cell_type":"code","source":"all_set = [['Elevation', 4],\n           ['Horizontal_Distance_To_Roadways', 4],\n           ['Horizontal_Distance_To_Fire_Points', 3],\n           ['Horizontal_Distance_To_Hydrology', 4],\n           ['Hillshade_9am', 3],\n           ['Vertical_Distance_To_Hydrology', 3],\n           ['Slope', 4],\n           ['Aspect', 4],\n           ['Hillshade_3pm', 3],\n           ['Hillshade_Noon', 3],\n           ['Degree_to_Hydrology', 3],\n           ['Hillshade_Noon_dif_Hillshade_3pm', 3],\n           ['Hillshade_Noon_abs_Hillshade_3pm', 3],\n           ['Elevation_PLUS_Hillshade_9am_add_Hillshade_Noon', 5],\n           ['Elevation_PLUS_Hillshade_max', 5],\n           ['Elevation_PLUS_Horizontal_Distance_To_Hydrology', 5],\n           ['Aspect_Sin_PLUS_Aspect_Cos_PLUS_Elevation', 5],\n           ['Elevation_PLUS_Horizontal_Distance_To_Fire_Points', 5],\n           ['Wilder_Type_PLUS_Elevation', 5],\n           ['Elevation_PLUS_Hillshade_9am', 5],\n           ['Elevation_PLUS_Degree_to_Hydrology', 5],\n           ['Wilder_Type_PLUS_Horizontal_Distance_To_Roadways', 5],\n           ['Wilder_Type_PLUS_Hillshade_9am_add_Hillshade_Noon', 4],\n           ['Wilder_Type_PLUS_Horizontal_Distance_To_Hydrology', 5],\n           ['Wilder_Type_PLUS_Hillshade_Noon_abs_Hillshade_3pm', 4],\n           ['Hillshade_9am_add_Hillshade_Noon_PLUS_Hillshade_std', 4],\n           ['Hillshade_9am_PLUS_Hillshade_9am_add_Hillshade_Noon', 4],\n           ['Hillshade_9am_add_Hillshade_Noon_PLUS_Hillshade_Noon_add_Hillshade_3pm', 5]]\n\ndef simple_feature_scores(clf, cols, test=False, **params):\n    scores = []\n    bscores = []\n    lscores = []\n    \n    X_preds = np.zeros((len(y), 7))\n    \n    \n    for train, val in StratifiedKFold(n_splits=10, shuffle=True, random_state=2018).split(X, y):\n        X_train = X.loc[train, cols]\n        X_val = X.loc[val, cols]\n        y_train = y[train]\n        y_val = y[val]\n        C = clf(**params) \n\n        C.fit(X_train, y_train)\n        X_preds[val] = C.predict_proba(X_val)\n        #scores.append(accuracy_score(y_val, C.predict(X_val)))\n        #bscores.append(balanced_accuracy_score(y_val, C.predict(X_val)))\n        #lscores.append(log_loss(y_val, C.predict_proba(X_val), labels=list(range(1, 8))))\n    \n    if test:\n        X_test_select = X_test.loc[:, cols]\n        C = clf(**params)\n        C.fit(X.loc[:, cols], y)\n        X_test_preds = C.predict_proba(X_test_select)\n    else:\n        X_test_preds = None\n    return scores, bscores, lscores, X_preds, X_test_preds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"576b5ca438937af3e9225968abac8329320555bb"},"cell_type":"code","source":"preds = []\ntest_preds = []\nfor colname, depth in tqdm(all_set):\n    #print(colname, depth)\n    ts, tbs, ls, pred, test_pred = simple_feature_scores(DecisionTreeClassifier,\n                                                         colname.split(\"_PLUS_\"),\n                                                         test=True,\n                                                         max_depth=depth)\n    preds.append(pred)\n    test_preds.append(test_pred)\n\ncols = list(chain.from_iterable([[col[0] + \"_DT_{}\".format(i) for i in range(1, 8)] for col in all_set]))\ndt_train_df = pd.DataFrame(np.hstack(preds)).astype(np.float32)\ndt_train_df.columns = cols\n\ndt_test_df = pd.DataFrame(np.hstack(test_preds)).astype(np.float32)\ndt_test_df.columns = cols","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"93d06191d5fc9afc7256c926dea2c77838e6d350"},"cell_type":"code","source":"# target encoding features(1.2.3)\nte_train_df = total_df.filter(regex=\"ctype\").iloc[:len(train_df)]\nte_test_df = total_df.filter(regex=\"ctype\").iloc[len(train_df):]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b633997c5f606833b004dfbf3b9d851bfa628dcc"},"cell_type":"code","source":"train_level2 = train_df[[\"Id\"]]\ntest_level2 = test_df[[\"Id\"]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"05791938fcee89186c89393a7376b1b7d585498c"},"cell_type":"code","source":"y = train_df[\"Cover_Type\"].values\nX = total_df[total_df[\"Id\"] <= 15120].drop(\"Id\", axis=1)\nX_test = total_df[total_df[\"Id\"] > 15120].drop(\"Id\", axis=1)\ntype_ratio = np.array([0.37053, 0.49681, 0.05936, 0.00103, 0.01295, 0.02687, 0.03242])\nclass_weight = {k: v for k, v in enumerate(type_ratio, start=1)}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e8370d46ecdb6079799d07b0b345f0f31003711b"},"cell_type":"code","source":"RFC1_col = [\"RFC1_{}_proba\".format(i) for i in range(1, 8)]\nfor col in RFC1_col:\n    train_level2.loc[:, col] = 0\n    test_level2.loc[:, col] = 0\nprint(RFC1_col)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9f0da865b74ff79f5de511c88361b7fbd6d686ef"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}