{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's import the dataset train.csv that is located in \"../input/\" into our pandas [DataFrame](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html)"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/train.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The first thing we do after importing our dataset into our DataFrame is to perform some Exploratory Data Analysis [EDA](https://www.datacamp.com/community/tutorials/kaggle-machine-learning-eda) . We will see in the next sessions how to perform EDA in depth but for now we will explore the following:\n* Number of traininig examples, the number of columns(features)\n* Correletaion between features in the dataset\n* Number of NaNs in the dataset\n* Number of outliers in the dataset"},{"metadata":{},"cell_type":"markdown","source":"#### Number of traininig examples, the number of columns(features)\nLet's start by exploring the number of training examples and features in the dataset. The number of traininig examples is simply the number of rows in the DataFrame and the number of features is the number of columns. We will use DataFrame.shape which returns a tuple (n_rows, n_cols) as follows:"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train.shape)\nprint(\"Number of features in the dataset = %i\" % train.shape[0])\nprint(\"Number of features in the daataset = %i\" % train.shape[1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now it is the time to have a look on the first rows of the dataset using DataFrame.head(n_rows), where the default number of rows to be displayed is 5 if the n_rows parameter is not specified."},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head(7)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Correletaion between features in the dataset \nTo find the correlation between features in the dataset we will use DataFrame.corr(). the corr() function will provide us with the correlation betwwen every two features in the dataset. The correlation between any feature and itself is simply equals to 1 so the principal diagonal of this square matrix will be equal to 1."},{"metadata":{"trusted":true},"cell_type":"code","source":"train.corr()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The correlation matrix may not be easy to be read, so we use the heatmap to find the correlation between the features using some sort of colored visualization as follows"},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\n\n\nimport matplotlib.pyplot as plt\n\n\ncorr = train.corr()\nf, ax = plt.subplots(figsize=(25, 25))\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\nsns.heatmap(corr, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Number of NaNs in the dataset\nTo find the number of NaNs in the dataset we use the function DataFrame.isna() which provides us with true in the places where an element in the dataset is NaN or None. We then sum the matrix we have where True + True = 2 as True is treated as numerical 1. Using this way we will have the number of NaNs for each feature as follows:"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We didn't have NaNs to experiment with so let's inject some NaNs in the Elevation feature"},{"metadata":{"trusted":true},"cell_type":"code","source":"train['Elevation'] = None","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.drop(['Aspect'],inplace = True, axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.drop(['Cover_Type'],inplace = True, axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now let's try to remove the NaN values we have"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.dropna(inplace = True, axis = 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Number of outliers in the dataset\nWe will discover a way of removing the outliers based on the standard deviation of the data where statistically any point that lies away from the mean with 2.5 or 3 stds can be treated as an outlier. we will use a method called zscores from stats library. We need to remove the categorical data first before starting as we can't calculate the zcore for a binary feature"},{"metadata":{"trusted":true},"cell_type":"code","source":"numerical = train.drop([\"Soil_Type\"+str(x) for x in range(1, 41)], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"numerical = numerical.drop([\"Wilderness_Area\" + str(x) for x in range(1, 5)], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"If you missed some of those steps, don't worry we will explain this in details in the next sessions insha'Allah."},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy import stats\nimport numpy as np\n\nz = np.abs(stats.zscore(numerical))\n\nprint(len(np.unique(np.where(z > 3)[0])) )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"(Optional) removing outliers"},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy import stats\nimport numpy as np\n\nt_rain = train[(z <= 3).all(axis = 1)]\n\nprint( t_rain.shape )","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}