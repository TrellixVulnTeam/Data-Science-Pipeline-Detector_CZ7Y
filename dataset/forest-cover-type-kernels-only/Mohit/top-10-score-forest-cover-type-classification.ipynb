{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"#Thanks to \n#https://www.kaggle.com/justfor/ensembling-and-stacking-with-heamy","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\nimport pandas as pd\nfrom sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import normalize\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.model_selection import KFold\nfrom sklearn.naive_bayes import GaussianNB, MultinomialNB,  BernoulliNB\nfrom sklearn.metrics import accuracy_score, log_loss,jaccard_similarity_score\nfrom heamy.dataset import Dataset\nfrom heamy.estimator import Classifier\nfrom heamy.pipeline import ModelsPipeline\nfrom lightgbm import LGBMClassifier\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/forest-cover-type-kernels-only/train.csv.zip')\ntest = pd.read_csv('/kaggle/input/forest-cover-type-kernels-only/test.csv.zip')\nprint(train.shape)\nprint(test.shape)\nprint('missing train terms',train.isnull().sum().sum())\nprint('missing test terms',test.isnull().sum().sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.columns.difference(test.columns) ## finding target ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.Cover_Type.value_counts() # balanced dataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target = train['Cover_Type']\ntrain.drop('Cover_Type',axis=1, inplace=True)\nprint(\"Combine Train and Test\")\ndf = pd.concat([train,test],axis=0)\nprint('\\nAll Data shape: {} Rows, {} Columns'.format(*df.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import math\n#df['disp_hydro'] = np.sqrt(df['Vertical_Distance_To_Hydrology']**2 + df['Horizontal_Distance_To_Hydrology']**2)\n\ndef degrees(rad):\n    return rad * 180 / np.pi\n\ndf['Percentage'] = (df['Slope'].apply(lambda x : degrees(x)))\ndf['run'] = round((df['Elevation']*100)/df['Percentage'],2)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def add_feats(df):\n    df['HF1'] = df['Horizontal_Distance_To_Hydrology']+df['Horizontal_Distance_To_Fire_Points']\n    df['HF2'] = (df['Horizontal_Distance_To_Hydrology']-df['Horizontal_Distance_To_Fire_Points'])\n    df['HR1'] = (df['Horizontal_Distance_To_Hydrology']+df['Horizontal_Distance_To_Roadways'])\n    df['HR2'] = (df['Horizontal_Distance_To_Hydrology']-df['Horizontal_Distance_To_Roadways'])\n    df['FR1'] = (df['Horizontal_Distance_To_Fire_Points']+df['Horizontal_Distance_To_Roadways'])\n    df['FR2'] = (df['Horizontal_Distance_To_Fire_Points']-df['Horizontal_Distance_To_Roadways'])\n    df['EV1'] = df.Elevation+df.Vertical_Distance_To_Hydrology\n    df['EV2'] = df.Elevation-df.Vertical_Distance_To_Hydrology\n    df['Mean_HF1'] = df.HF1/2\n    df['Mean_HF2'] = df.HF2/2\n    df['Mean_HR1'] = df.HR1/2\n    df['Mean_HR2'] = df.HR2/2\n    df['Mean_FR1'] = df.FR1/2\n    df['Mean_FR2'] = df.FR2/2\n    df['Mean_EV1'] = df.EV1/2\n    df['Mean_EV2'] = df.EV2/2    \n    df['Elevation_Vertical'] = df['Elevation']+df['Vertical_Distance_To_Hydrology']    \n    df['Neg_Elevation_Vertical'] = df['Elevation']-df['Vertical_Distance_To_Hydrology']\n    \n    # Given the horizontal & vertical distance to hydrology, \n    # it will be more intuitive to obtain the euclidean distance: sqrt{(verticaldistance)^2 + (horizontaldistance)^2}    \n    df['slope_hyd_sqrt'] = (df['Horizontal_Distance_To_Hydrology']**2+df['Vertical_Distance_To_Hydrology']**2)**0.5\n    df.slope_hyd_sqrt=df.slope_hyd_sqrt.map(lambda x: 0 if np.isinf(x) else x) # remove infinite value if any\n    \n    df['slope_hyd2'] = np.sqrt(df['Horizontal_Distance_To_Hydrology']**2+df['Vertical_Distance_To_Hydrology']**2)\n    df.slope_hyd2=df.slope_hyd2.map(lambda x: 0 if np.isinf(x) else x) # remove infinite value if any\n    \n    #Mean distance to Amenities \n    df['Mean_Amenities']=(df.Horizontal_Distance_To_Fire_Points + df.Horizontal_Distance_To_Hydrology + df.Horizontal_Distance_To_Roadways) / 3 \n    #Mean Distance to Fire and Water \n    df['Mean_Fire_Hyd1']=(df.Horizontal_Distance_To_Fire_Points + df.Horizontal_Distance_To_Hydrology) / 2\n    df['Mean_Fire_Hyd2']=(df.Horizontal_Distance_To_Fire_Points + df.Horizontal_Distance_To_Roadways) / 2\n    \n    #Shadiness\n    df['Shadiness_morn_noon'] = df.Hillshade_9am/(df.Hillshade_Noon+1)\n    df['Shadiness_noon_3pm'] = df.Hillshade_Noon/(df.Hillshade_3pm+1)\n    df['Shadiness_morn_3'] = df.Hillshade_9am/(df.Hillshade_3pm+1)\n    df['Shadiness_morn_avg'] = (df.Hillshade_9am+df.Hillshade_Noon)/2\n    df['Shadiness_afternoon'] = (df.Hillshade_Noon+df.Hillshade_3pm)/2\n    df['Shadiness_mean_hillshade'] =  (df['Hillshade_9am']  + df['Hillshade_Noon'] + df['Hillshade_3pm'] ) / 3    \n    \n    # Shade Difference\n    df[\"Hillshade-9_Noon_diff\"] = df[\"Hillshade_9am\"] - df[\"Hillshade_Noon\"]\n    df[\"Hillshade-noon_3pm_diff\"] = df[\"Hillshade_Noon\"] - df[\"Hillshade_3pm\"]\n    df[\"Hillshade-9am_3pm_diff\"] = df[\"Hillshade_9am\"] - df[\"Hillshade_3pm\"]\n\n    # Mountain Trees\n    df[\"Slope*Elevation\"] = df[\"Slope\"] * df[\"Elevation\"]\n    # Only some trees can grow on steep montain\n    \n    ### More features\n    df['Neg_HorizontalHydrology_HorizontalFire'] = (df['Horizontal_Distance_To_Hydrology']-df['Horizontal_Distance_To_Fire_Points'])\n    df['Neg_HorizontalHydrology_HorizontalRoadways'] = (df['Horizontal_Distance_To_Hydrology']-df['Horizontal_Distance_To_Roadways'])\n    df['Neg_HorizontalFire_Points_HorizontalRoadways'] = (df['Horizontal_Distance_To_Fire_Points']-df['Horizontal_Distance_To_Roadways'])\n    \n    df['MeanNeg_Mean_HorizontalHydrology_HorizontalFire'] = (df['Horizontal_Distance_To_Hydrology']-df['Horizontal_Distance_To_Fire_Points'])/2\n    df['MeanNeg_HorizontalHydrology_HorizontalRoadways'] = (df['Horizontal_Distance_To_Hydrology']-df['Horizontal_Distance_To_Roadways'])/2\n    df['MeanNeg_HorizontalFire_Points_HorizontalRoadways'] = (df['Horizontal_Distance_To_Fire_Points']-df['Horizontal_Distance_To_Roadways'])/2   \n        \n    df[\"Vertical_Distance_To_Hydrology\"] = abs(df['Vertical_Distance_To_Hydrology'])\n    \n    df['Neg_Elev_Hyd'] = df.Elevation-df.Horizontal_Distance_To_Hydrology*0.2\n    \n    # Bin Features\n    bin_defs = [\n        # col name, bin size, new name\n        ('Elevation', 200, 'Binned_Elevation'), # Elevation is different in train vs. test!?\n        ('Aspect', 45, 'Binned_Aspect'),\n        ('Slope', 6, 'Binned_Slope'),\n        ('Horizontal_Distance_To_Hydrology', 140, 'Binned_Horizontal_Distance_To_Hydrology'),\n        ('Horizontal_Distance_To_Roadways', 712, 'Binned_Horizontal_Distance_To_Roadways'),\n        ('Hillshade_9am', 32, 'Binned_Hillshade_9am'),\n        ('Hillshade_Noon', 32, 'Binned_Hillshade_Noon'),\n        ('Hillshade_3pm', 32, 'Binned_Hillshade_3pm'),\n        ('Horizontal_Distance_To_Fire_Points', 717, 'Binned_Horizontal_Distance_To_Fire_Points')\n    ]\n    \n    for col_name, bin_size, new_name in bin_defs:\n        df[new_name] = np.floor(df[col_name]/bin_size)\n        \n    print('Total number of features : %d' % (df.shape)[1])\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_and_process_dataset():\n    train = pd.read_csv(\"{0}/train.csv.zip\".format(DATA_DIR))\n    test = pd.read_csv(\"{0}/test.csv.zip\".format(DATA_DIR))\n\n    y_train = train[TARGET].ravel() -1 # XGB needs labels starting with 0!\n    \n    classes = train.Cover_Type.unique()\n    num_classes = len(classes)\n    print(\"There are %i classes: %s \" % (num_classes, classes))        \n\n    train.drop([ID, TARGET], axis=1, inplace=True)\n    test.drop([ID], axis=1, inplace=True)\n    \n    train = add_feats(train)    \n    test = add_feats(test)    \n    \n    cols_to_normalize = [ 'Aspect', 'Slope', 'Horizontal_Distance_To_Hydrology',\n                       'Vertical_Distance_To_Hydrology', 'Horizontal_Distance_To_Roadways',\n                       'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm', \n                       'Horizontal_Distance_To_Fire_Points', \n                       'Shadiness_morn_noon', 'Shadiness_noon_3pm', 'Shadiness_morn_3',\n                       'Shadiness_morn_avg', \n                       'Shadiness_afternoon', \n                       'Shadiness_mean_hillshade',\n                       'HF1', 'HF2', \n                       'HR1', 'HR2', \n                       'FR1', 'FR2'\n                       ]\n\n    train[cols_to_normalize] = normalize(train[cols_to_normalize])\n    test[cols_to_normalize] = normalize(test[cols_to_normalize])\n\n    # elevation was found to have very different distributions on test and training sets\n    # lets just drop it for now to see if we can implememnt a more robust classifier!\n    train = train.drop('Elevation', axis=1)\n    test = test.drop('Elevation', axis=1)    \n    \n    x_train = train.values\n    x_test = test.values\n\n    return {'X_train': x_train, 'X_test': x_test, 'y_train': y_train}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = Dataset(preprocessor=load_and_process_dataset, use_cache=True)  \nDATA_DIR = \"/kaggle/input/forest-cover-type-kernels-only\"\nSUBMISSION_FILE = \"{0}/sample_submission.csv\".format(DATA_DIR)\nCACHE=False\nID = 'Id'\nTARGET = 'Cover_Type'\nNFOLDS = 5\nSEED = 1337\nMETRIC = log_loss\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Parameters for the classifiers\nrf_params = {\n    'n_estimators': 200,\n    'criterion': 'entropy',\n    'random_state': 0\n}\n\nrf1_params = {\n    'n_estimators': 200,\n    'criterion': 'gini',\n    'random_state': 0\n}\n\net1_params = {\n    'n_estimators': 200,\n    'criterion': 'gini',\n    'random_state': 0\n}\n\net_params = {\n    'n_estimators': 200,\n    'criterion': 'entropy',\n    'random_state': 0\n}\n\net1_params = {\n    'n_estimators': 200,\n    'criterion': 'gini',\n    'random_state': 0\n}\n\nlgb_params = {\n    'n_estimators': 200, \n    'learning_rate':0.1\n}\n\nlogr_params = {\n        'solver' : 'liblinear',\n        'multi_class' : 'ovr',\n        'C': 1,\n        'random_state': 0}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nrf = Classifier(dataset=dataset, estimator = RandomForestClassifier, use_cache=CACHE, parameters=rf_params,name='rf')\net = Classifier(dataset=dataset, estimator = ExtraTreesClassifier, use_cache=CACHE, parameters=et_params,name='et')   \nrf1 = Classifier(dataset=dataset, estimator=RandomForestClassifier, use_cache=CACHE, parameters=rf1_params,name='rf1')\net1 = Classifier(dataset=dataset, use_cache=CACHE, estimator=ExtraTreesClassifier, parameters=et1_params,name='et1')\nlgbc = Classifier(dataset=dataset, estimator=LGBMClassifier, use_cache=CACHE, parameters=lgb_params,name='lgbc')\ngnb = Classifier(dataset=dataset,estimator=GaussianNB, use_cache=CACHE, name='gnb')\nlogr = Classifier(dataset=dataset, estimator=LogisticRegression, use_cache=CACHE, parameters=logr_params,name='logr')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost as xgb\nfrom xgboost import XGBClassifier\ndef xgb_first(X_train, y_train, X_test, y_test=None):\n    xg_params = {\n        'seed': 0,\n        'colsample_bytree': 0.7,\n        'silent': 1,\n        'subsample': 0.7,\n        'learning_rate': 0.1,\n        'objective': 'multi:softprob',   \n        'num_class': 7,\n        'max_depth': 4,\n        'min_child_weight': 1,\n        'eval_metric': 'mlogloss',\n        'nrounds': 200\n    }    \n    X_train = xgb.DMatrix(X_train, label=y_train)\n    model = xgb.train(xg_params, X_train, xg_params['nrounds'])\n    return model.predict(xgb.DMatrix(X_test))\nxgb_first = Classifier(estimator=xgb_first, dataset=dataset, use_cache=CACHE, name='xgb_first')  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pipeline = ModelsPipeline(rf, et, et1, lgbc, logr, gnb, xgb_first) \n\nstack_ds = pipeline.stack(k=NFOLDS,seed=SEED)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train LogisticRegression on stacked data (second stage)\nlr = LogisticRegression\nlr_params = {'C': 5, 'random_state' : SEED, 'solver' : 'liblinear', 'multi_class' : 'ovr',}\nstacker = Classifier(dataset=stack_ds, estimator=lr, use_cache=False, parameters=lr_params)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Validate results using k-fold cross-validation\nresults = stacker.validate(k=NFOLDS,scorer=log_loss)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"models = [rf, et, et1, lgbc, logr, gnb, xgb_first]       \nprint(\"Log Loss\")\nfor index, element in enumerate(models):\n    print(index, element.name)\n    element.validate(k=NFOLDS,scorer=log_loss)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds_proba = stacker.predict() \n# Note: labels starting with 0 in xgboost, therefore adding +1!\npredictions = np.round(np.argmax(preds_proba, axis=1)).astype(int) + 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv('/kaggle/input/forest-cover-type-kernels-only/sample_submission.csv.zip')\nsubmission[TARGET] = predictions\nsubmission.to_csv('Stacking_with_heamy_logregr.sub.csv', index=None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use a xgb-model as 2nd-stage model\n\ndtrain = xgb.DMatrix(stack_ds.X_train, label=stack_ds.y_train)\ndtest = xgb.DMatrix(stack_ds.X_test)\n\nxgb_params = {\n    'seed': 0,\n    'colsample_bytree': 0.8,\n    'silent': 1,\n    'subsample': 0.6,\n    'learning_rate': 0.05,\n    'objective': 'multi:softprob',\n    'num_class': 7,        \n    'max_depth': 6,\n    'num_parallel_tree': 1,\n    'min_child_weight': 1,\n    'eval_metric': 'mlogloss',\n}\n\nres = xgb.cv(xgb_params, dtrain, num_boost_round=1000, \n             nfold=NFOLDS, seed=SEED, stratified=True,\n             early_stopping_rounds=20, verbose_eval=5, show_stdv=True)\n\nbest_nrounds = res.shape[0] - 1\ncv_mean = res.iloc[-1, 2]\ncv_std = res.iloc[-1, 3]\n\nprint('Ensemble-CV: {0}+{1}, best nrounds = {2}'.format(cv_mean, cv_std, best_nrounds))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train with best rounds\nmodel = xgb.train(xgb_params, dtrain, best_nrounds)\nxpreds_proba = model.predict(dtest)\n\n# Note: labels starting with 0 in xgboost, therefore adding +1!\npredictions = np.round(np.argmax(xpreds_proba, axis=1)).astype(int) + 1\nsubmission = pd.read_csv('/kaggle/input/forest-cover-type-kernels-only/sample_submission.csv.zip')\nsubmission[TARGET] = predictions\nsubmission.to_csv('Stacking2' + str(cv_mean) + '.sub.csv', index=None)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}