{"cells":[{"metadata":{"_uuid":"75bd9103-5ad0-4313-977b-c972b43a1648","_cell_guid":"362b9adb-3d2a-43f8-b054-938cf4027a18","trusted":true},"cell_type":"markdown","source":"## Visualizations and Decision Tree Classifications on the Forest Type Dataset"},{"metadata":{"_uuid":"c0396603-bf0a-4559-8baa-eee076d7de29","_cell_guid":"66121d91-0300-46a9-8306-a50056dc269a","trusted":true},"cell_type":"code","source":"# imports\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn import tree\nfrom sklearn.ensemble import RandomForestClassifier \nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\n\n\nfrom IPython.display import SVG\nfrom graphviz import Source\nfrom IPython.display import display\n\nimport os\nprint(os.listdir(\"../input/forest-cover-type-kernels-only\"))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"52ca1524-c841-4a82-8723-ca1a982189e9","_cell_guid":"e6302713-5113-4ae9-b620-f5661d9a8ec9","trusted":true},"cell_type":"code","source":"# new dataframe onthe training dataset\ntrain = pd.read_csv('../input/forest-cover-type-kernels-only/train.csv')\ntrain.columns","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7b81ddf7-7de2-41cd-8cb0-d862822aee5b","_cell_guid":"6ae65c83-8ec7-44fb-a795-c2fa835a51fe","trusted":true},"cell_type":"code","source":"train","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b3547e38-b9c8-472f-8d74-4a74d593c814","_cell_guid":"01adc603-5eca-4a7f-ae0a-0122da48c4cd","trusted":true},"cell_type":"code","source":"train.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1c888e11-28e5-4f81-bde8-d12ac56f80a3","_cell_guid":"1bbbfeb2-8120-4560-8a56-0bd10b4ef08b","trusted":true},"cell_type":"code","source":"train.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b507b708-d335-4f1c-88a8-2433c3676b62","_cell_guid":"9873379a-f449-4529-9a42-b07e09b7b008","trusted":true},"cell_type":"markdown","source":"Run some visualizations to look at the data.  First, look at all the continuous variables."},{"metadata":{"_uuid":"95443348-4485-4ba6-ab93-46b6df440d58","_cell_guid":"668f5db3-16db-4780-9eac-bf62eba45f73","trusted":true},"cell_type":"code","source":"train[train.columns[1:11]].hist(figsize = (20,15))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d8218658-9066-4771-b5e9-cd0bfe540ee0","_cell_guid":"7643af51-938f-4dfb-803a-7e96ad22db97","trusted":true},"cell_type":"markdown","source":"Data are usable, except that aspect is a cardinal direction and may need some different encoding (since 0 is the same as 360).  Work on that later.  \n\nAnother version of the same chart, this time in Seaborn rather than pandas plot"},{"metadata":{"_uuid":"d2e8af64-2363-47d1-878c-325dad9d13e4","_cell_guid":"0843a1da-b2ec-4f8c-a390-046353c4ed39","trusted":true},"cell_type":"code","source":"cols = train.columns[1:11]\nfig, axes = plt.subplots(nrows = 1, ncols = len(cols), figsize = (30,5))\nfor i, ax in enumerate(axes):\n    sns.distplot(train[cols[i]], ax=ax)\n    sns.despine()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b4cb2be5-b43a-4471-930a-7098df155d14","_cell_guid":"690dce33-ed81-429c-b776-71ccdf5afd2f","trusted":true},"cell_type":"markdown","source":"Now that we know what is in the dataset, let's look at the distributions for each forest type (the target variable).  A violin plot will work."},{"metadata":{"_uuid":"377f1319-3fe5-4b42-8549-8d3614876a6c","_cell_guid":"c066f7d2-1772-49c9-8417-ed0efdd7cb13","trusted":true},"cell_type":"code","source":"cols = train.columns[1:11]\nfig, axes = plt.subplots(nrows = 1, ncols = len(cols), figsize = (30,5))\nfor i, ax in enumerate(axes):\n    sns.violinplot(data=train, x = \"Cover_Type\", y = cols[i],ax=ax)\n    sns.despine()\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"26365a05-5f52-4b35-a171-fe8948577de6","_cell_guid":"33957435-748f-489a-b3e0-c0912499fffe","trusted":true},"cell_type":"markdown","source":"Elevation distinguishes the cover types faily well.  Aspect has some bimodal distributions (due to the data encoding problem stated before), other features show smaller variations.\n\nNow let's look at the categorical features.  Wilderness area and soil type are already one-hot encoded in the dataset.  For ploting convenience, create new columns that combine the categories back together."},{"metadata":{"_uuid":"0b507b6b-5e83-4edb-b74e-0c82c9420782","_cell_guid":"175c8401-d697-4188-8ae0-f4f1d1a1cfd0","trusted":true},"cell_type":"code","source":"areas_list  = [ 'Wilderness_Area1', 'Wilderness_Area2', 'Wilderness_Area3','Wilderness_Area4']\ntrain['Wilderness_Area'] = train.Wilderness_Area1 * 1 + train.Wilderness_Area2 * 2 + train.Wilderness_Area3 * 3 + train.Wilderness_Area4 *4","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b2994bae-222d-467d-baa1-3102c115ce98","_cell_guid":"87908cbc-ec36-4202-bea8-5961bda45440","trusted":true},"cell_type":"code","source":"train['Soil_Type'] = (train.Soil_Type1 * 1 + \n                    train.Soil_Type2 * 2 + \n                    train.Soil_Type3 * 3 + \n                    train.Soil_Type4 * 4 + \n                    train.Soil_Type5 * 5 + \n                    train.Soil_Type6 * 6 + \n                    train.Soil_Type7 * 7 + \n                    train.Soil_Type8 * 8 + \n                    train.Soil_Type9 * 9 + \n                    train.Soil_Type10 * 10 + \n                    train.Soil_Type11 * 11 + \n                    train.Soil_Type12 * 12 + \n                    train.Soil_Type13 * 13 + \n                    train.Soil_Type14 * 14 + \n                    train.Soil_Type15 * 15 + \n                    train.Soil_Type16 * 16 + \n                    train.Soil_Type17 * 17 + \n                    train.Soil_Type18 * 18 + \n                    train.Soil_Type19 * 19 + \n                    train.Soil_Type20 * 20 + \n                    train.Soil_Type21 * 21 + \n                    train.Soil_Type22 * 22 + \n                    train.Soil_Type23 * 23 + \n                    train.Soil_Type24 * 24 + \n                    train.Soil_Type25 * 25 + \n                    train.Soil_Type26 * 26 + \n                    train.Soil_Type27 * 27 + \n                    train.Soil_Type28 * 28 + \n                    train.Soil_Type29 * 29 + \n                    train.Soil_Type30 * 30 + \n                    train.Soil_Type31 * 31 + \n                    train.Soil_Type32 * 32 + \n                    train.Soil_Type33 * 33 + \n                    train.Soil_Type34 * 34 + \n                    train.Soil_Type35 * 35 + \n                    train.Soil_Type36 * 36 + \n                    train.Soil_Type37 * 37 + \n                    train.Soil_Type38 * 38 + \n                    train.Soil_Type39 * 39 + \n                    train.Soil_Type40 * 40)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2bf36259-9daf-4e0d-a4fe-a9775a9aa9b0","_cell_guid":"526c1e76-0639-4528-8f1e-19b62d6d6581","trusted":true},"cell_type":"markdown","source":"Now make a couple of bar charts for the categorical variables"},{"metadata":{"_uuid":"efedc726-2f67-4a61-a969-4cd72e2e0dbd","_cell_guid":"20d05f3b-3df4-4c68-8744-9e23bacc1cf8","trusted":true},"cell_type":"code","source":"# this is a useful plot for categorical variables\ncols = train.columns[-2:]\nfig, axes = plt.subplots(ncols = 1, nrows = len(cols), figsize = (20,10))\nfor i, ax in enumerate(axes):\n    sns.barplot(data=train.groupby(by = [cols[i],\"Cover_Type\"]).Id.count().reset_index(),\n                  x=cols[i], y=\"Id\", hue=\"Cover_Type\", ax=ax)\n    sns.despine()\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"304df83a-49e3-4b5e-9033-7250430782fc","_cell_guid":"44f3641a-6f8c-4a81-89e6-fe929c8be666","trusted":true},"cell_type":"markdown","source":"Just knowing the winderness type is a major way to identify the species (e.g., cover type 4 is only found in wilderness area 4).\n\nMake a decision tree, use the one-hot encoded fields for categorical variables."},{"metadata":{"_uuid":"b4de7ccb-a36f-433d-a367-1f04f2cdc115","_cell_guid":"85f162c7-5886-41a4-b927-7ac5bea85951","trusted":true},"cell_type":"code","source":"# Make sure I'm getting the right columns\ntrain.columns[1:-3]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fb0ad86f-e30a-40e3-a482-f2aaaa28e8ac","_cell_guid":"7de28a4a-357b-4b07-80b9-8f00fdb1282f","trusted":true},"cell_type":"code","source":"labels = train.columns[1:-3]\ny = train.Cover_Type\nX = train[labels]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7657fb63-5345-4f7c-80fe-d0b788533ff3","_cell_guid":"1289ee49-09f2-42e4-962f-b65537c392c0","trusted":true},"cell_type":"code","source":"X","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"76aa560e-52a1-4bc7-8bea-3820f09d3ed9","_cell_guid":"0873e455-d0eb-4c17-9cfd-c204a3fab154","trusted":true},"cell_type":"markdown","source":"Run and visualize a decision tree for fun"},{"metadata":{"_uuid":"358b365f-80a1-4f7c-8d28-59f437ce9fe7","_cell_guid":"79f62f40-ccba-4a9a-b797-8856e167a254","trusted":true},"cell_type":"code","source":"estimator = tree.DecisionTreeClassifier(max_depth = 10)\nestimator.fit(X, y)\n\ngraph = Source(tree.export_graphviz(estimator, out_file=None\n   , feature_names= labels, class_names = ['0', '1', '2', '3', '4' ,'5','6']\n   , filled = True))\n\ndisplay(SVG(graph.pipe(format='svg')))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8ea7ae31-ee31-489c-8dbf-6a1bc37f8614","_cell_guid":"74b98965-203d-47ea-8b0b-b86df87b85cc","trusted":true},"cell_type":"markdown","source":"Cool, but not too informative.  We see that elevation is a key feature.  \n\nNow split the data and try some classifier algorithms."},{"metadata":{"_uuid":"ff4d3b1f-7a2a-4336-9589-6bbb79ebf5fa","_cell_guid":"ad897025-aaf0-4911-a94e-6382335f5c0f","trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=43)\n\nestimator = tree.DecisionTreeClassifier(max_depth = 5)\nestimator.fit(X_train, y_train)\n\n#calculate the percent correct\nsum(estimator.predict(X_test)==y_test)/len(y_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b5f85f83-293b-4496-9878-5d4f33a81bcc","_cell_guid":"be97b82b-6bef-40a2-9da9-9346f6507fd7","trusted":true},"cell_type":"markdown","source":"Let's see how the performance varies with the depth of the tree."},{"metadata":{"_uuid":"85589dad-f291-4cfb-b0ab-982817e5854c","_cell_guid":"382e9d1f-1d23-4b77-ae87-d32abc58b6e0","trusted":true},"cell_type":"code","source":"depths = []\nperformance = []\nfor depth in range(2,50):\n    estimator = tree.DecisionTreeClassifier(max_depth = depth)\n    estimator.fit(X_train, y_train)\n    correct = sum(estimator.predict(X_test)==y_test)/len(y_test)\n    #print('Depth = ',depth,' correct = ',correct)\n    depths.append(depth)\n    performance.append(correct)\nresults =  pd.DataFrame()\nresults['tree_depths'] = depths\nresults['performance'] = performance\nresults.plot(x = 'tree_depths', y = 'performance')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9b68bb51-35e2-4908-b86c-f38766123a98","_cell_guid":"e2aa19ba-f38f-424e-b51c-64c4b36c1a26","trusted":true},"cell_type":"markdown","source":"Interesting, I thought that the tree would over-fit with greater depth, but performance didn't taper significantly.  Still, a depth of 15  looks about right.  Look at the min_wieght_fraction_leaf hyperparameter."},{"metadata":{"_uuid":"841286fd-0c3b-4898-9591-02dd7c18979e","_cell_guid":"96e62979-d481-4d47-910c-333ee8f6f8cd","trusted":true},"cell_type":"code","source":"depths = []\nperformance = []\nfor depth in range(0,50):\n    fraction = depth/100\n    estimator = tree.DecisionTreeClassifier(min_weight_fraction_leaf = fraction)\n    estimator.fit(X_train, y_train)\n    correct = sum(estimator.predict(X_test)==y_test)/len(y_test)\n    #print('Depth = ',depth,' correct = ',correct)\n    depths.append(fraction)\n    performance.append(correct)\nresults =  pd.DataFrame()\nresults['tree_depths'] = depths\nresults['performance'] = performance\nresults.plot(x = 'tree_depths', y = 'performance')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9a1c15b0-5a42-4f39-b28e-fcb479500665","_cell_guid":"163c64e1-7f56-44ca-b662-ed888b109964","trusted":true},"cell_type":"markdown","source":"No overfitting there!"},{"metadata":{"_uuid":"d7651b87-7aa9-418d-a39d-14d1b63bcc5d","_cell_guid":"ff1286f7-b115-46c5-92c7-05f2caf613bb","trusted":true},"cell_type":"markdown","source":"For a tree of depth 15, let's look at the performance closer.   How did we do for each cover type? \n\nLook at a confusion matrix."},{"metadata":{"_uuid":"acc63e68-3987-4dfb-8df6-13bcbcd80368","_cell_guid":"da12b2cf-23b4-427d-85a7-c3ec563f65cf","trusted":true},"cell_type":"code","source":"estimator = tree.DecisionTreeClassifier(max_depth = 15)\nestimator.fit(X_train, y_train)\n#calculate the percent correct\nsum(estimator.predict(X_test)==y_test)/len(y_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b0a44697-c883-4892-b37c-76dfb3323ec6","_cell_guid":"9d6877e0-67c3-4284-8d96-2aa6fa7a1f42","trusted":true},"cell_type":"markdown","source":"~76%"},{"metadata":{"_uuid":"2ae71ad6-2201-4002-a556-b5f348cc3321","_cell_guid":"c3449035-52fc-400d-8fe3-01c5a8ab29ed","trusted":true},"cell_type":"code","source":"conf_mx = confusion_matrix(y_test, estimator.predict(X_test))\n\nax = sns.heatmap(conf_mx, annot = True, fmt = 'd')\nax.set(xlabel='Predicted', ylabel='Actual')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"75bf0059-d7ac-41f9-8608-8ae326994053","_cell_guid":"607d01a1-3fe8-4e97-8765-32844bad2901","trusted":true},"cell_type":"markdown","source":"Zero out the diagonals to make the colors pop, and present as a fraction of each actual type."},{"metadata":{"_uuid":"29ef7d42-a49d-48c9-86e0-19e156133663","_cell_guid":"4f24c40d-b915-494a-b0ff-0756436ff2ba","trusted":true},"cell_type":"code","source":"row_sums = conf_mx.sum(axis=1, keepdims = True)\nnorm_conf_mx = conf_mx/row_sums\nnp.fill_diagonal(norm_conf_mx, 0)\nax = sns.heatmap(norm_conf_mx, annot = True)#, fmt = 'd')\nax.set(xlabel='Predicted', ylabel='Actual')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5b6b46e7-1942-4b13-b696-969af3d544aa","_cell_guid":"9d581e6b-2839-4ce6-957f-1145dbd6bba8","trusted":true},"cell_type":"markdown","source":"We may want to target feature engineering to splitting out the types that are presenting a problem.   \nThe next steps: \n1. work on the aspect feature\n2. work on scaling\n3. work on other features\n4. compare to other algorithms\n5. play with decision rationales and percent probabilities."},{"metadata":{"_uuid":"bf38fdef-e657-4cf8-b496-d643bb34671e","_cell_guid":"962c85f5-147f-492f-bb07-dd6a768eca28","trusted":true},"cell_type":"markdown","source":"Let's at least take care of the aspect data - let's convert degrees into unit circle Xs and Ys - two features, same information."},{"metadata":{"_uuid":"5986361d-e85f-4779-a237-611032d4bff1","_cell_guid":"761c9850-65ee-4f00-bfa4-aa47c69cd665","trusted":true},"cell_type":"code","source":"# test my formula - use np instead of math\nfor deg in range(0,370,30):\n    print (deg, np.sin(deg*np.pi/180),np.cos(deg*np.pi/180))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9a3a5cb5-0439-436c-9e0c-5785defe78d1","_cell_guid":"56c39cdd-6fc8-464b-a689-a6f1743e5b00","trusted":true},"cell_type":"code","source":"train['Aspect_N_S'] = np.cos(train.Aspect*np.pi/180)\ntrain['Aspect_E_W'] = np.sin(train.Aspect*np.pi/180)\ntrain[['Aspect', 'Aspect_N_S', 'Aspect_E_W']]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e75ffcd8-1d77-4806-bf95-05fbb5993d13","_cell_guid":"ed13cd0a-528f-461e-8b46-13dc723d4ce5","trusted":true},"cell_type":"code","source":"# new column names of interest.  \nX_col_names = [train.columns[1]]+train.columns[3:-5].tolist()+train.columns[-2:].tolist()\nX_col_names","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d4145fb8-bbef-4827-b418-0008d86aeb90","_cell_guid":"31ec6f6a-e541-4865-8ec8-46b8b8c41151","trusted":true},"cell_type":"code","source":"y = train.Cover_Type\nX = train[X_col_names]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=43)\n\nestimator = tree.DecisionTreeClassifier(max_depth = 15)\nestimator.fit(X_train, y_train)\n\n#calculate the percent correct\nsum(estimator.predict(X_test)==y_test)/len(y_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1c388f49-0cd2-43d9-bf06-7f6d96689940","_cell_guid":"af32b76c-b599-40f8-ae60-f722ac4dc109","trusted":true},"cell_type":"markdown","source":"~77%  A little better.  Now try with a standard scaler."},{"metadata":{"_uuid":"ab3e2040-0a45-4f8b-905b-c0e0964a4c6a","_cell_guid":"d0b9f855-7d0f-40db-a1ce-5a04a67fe44c","trusted":true},"cell_type":"code","source":"scaler = StandardScaler()\n\n\nestimator = tree.DecisionTreeClassifier(max_depth = 15)\nestimator.fit(scaler.fit_transform(X_train), y_train)\n\n#calculate the percent correct\nsum(estimator.predict(scaler.transform(X_test))==y_test)/len(y_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"873cf09f-3adc-4376-a0a9-85a44dc2495d","_cell_guid":"63839183-ef80-4ffb-9b8a-84c9ffb1bd62","trusted":true},"cell_type":"markdown","source":"Not different, but I don't think we would expect a decision tree to give different results.  \n\nNow try logistic regression before and after scaling."},{"metadata":{"_uuid":"d7b582f6-764a-4c01-bd9c-4633935f5124","_cell_guid":"fb0d62d8-29eb-419b-a1c1-f7f5accc0203","trusted":true},"cell_type":"code","source":"# logistic regression code for comparison\nestimator = LogisticRegression()\nestimator.fit(X_train, y_train)\n\n#calculate the percent correct\nprint('unscaled = ',sum(estimator.predict(X_test)==y_test)/len(y_test))\n\n# logistic regression code for comparison\nestimator = LogisticRegression()\nestimator.fit(scaler.fit_transform(X_train), y_train)\n\n#calculate the percent correct\nprint('scaled = ',sum(estimator.predict(scaler.transform(X_test))==y_test)/len(y_test))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"742da44d-ae92-4007-846b-b1c0be7d7772","_cell_guid":"8834e580-d2a1-4550-aae0-e9368bac56db","trusted":true},"cell_type":"markdown","source":"Not much different and not too good.  \n\nTry random forest\n"},{"metadata":{"_uuid":"0898895a-87a0-43c0-b643-9deb7746b4ea","_cell_guid":"d8a2c7b6-54db-45a5-8b7e-3176247bb6bb","trusted":true},"cell_type":"code","source":"estimator = RandomForestClassifier()\nestimator.fit(X_train, y_train)\n\n#calculate the percent correct\nsum(estimator.predict(X_test)==y_test)/len(y_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"85e263e3-308e-4d60-88ad-baeb3aa18986","_cell_guid":"fd2af7bd-f9b3-41d0-aedc-24e9d52795eb","trusted":true},"cell_type":"markdown","source":"Magically up to 81%. . . let's look at feature importance"},{"metadata":{"_uuid":"74746344-dc2d-41aa-afae-b8ba2ab26d26","_cell_guid":"667a7450-b8f9-4e6f-88c5-0b09acce352e","trusted":true},"cell_type":"code","source":"pd.DataFrame(index = X_col_names, columns  = ['feature_importance'], data = estimator.feature_importances_).sort_values(ascending = False,by='feature_importance')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ab002b1f-b5ff-4110-9f17-f4e10c5728dc","_cell_guid":"d96ce3f2-c5f4-4cc9-a2f1-efece444234b","trusted":true},"cell_type":"markdown","source":"Experiment with logisitic regression for the continuous variables only"},{"metadata":{"_uuid":"ad8f0c11-7ce1-47d0-893f-a3f3e3add974","_cell_guid":"bddf2d84-8af2-4d19-bc4d-7f331fdec4c4","trusted":true},"cell_type":"code","source":"# new column names of interest.  \nX2_col_names = [train.columns[1]]+train.columns[3:11].tolist()+train.columns[-2:].tolist()\nX2_col_names","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e210a09f-e661-4047-bcc7-5db6573249a7","_cell_guid":"15f55592-a2f1-4780-b4ad-57cfff4e13f1","trusted":true},"cell_type":"code","source":"y2 = train.Cover_Type\nX2 = train[X2_col_names]\n\nX2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, test_size=0.3, random_state=43)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"23fc77b4-64f2-4ca4-9a99-4877849da869","_cell_guid":"1c67b7f9-6d9a-4a78-bb66-5e6b91c040c2","trusted":true},"cell_type":"code","source":"# logistic regression code for comparison\nestimator = LogisticRegression()\nestimator.fit(X2_train, y2_train)\n\n#calculate the percent correct\nprint('unscaled = ',sum(estimator.predict(X2_test)==y2_test)/len(y2_test))\n\n# logistic regression code for comparison\nestimator = LogisticRegression()\nestimator.fit(scaler.fit_transform(X2_train), y2_train)\n\n#calculate the percent correct\nprint('scaled = ',sum(estimator.predict(scaler.transform(X2_test))==y2_test)/len(y2_test))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e748dbd4-6045-4d19-9991-3bae7d68f7dd","_cell_guid":"fd3916ed-49af-4d2e-983c-fc33374de618","trusted":true},"cell_type":"markdown","source":"Wow, super bad.  The soil type and wilderness areas really make a difference.  \n\nOne hot encoding of categorical variables might be bad for decision trees.  Let's try binary encoding to reduce the number of columns that are needed (code a new column for each place value in the binary value for the category.)"},{"metadata":{"_uuid":"2151d731-8b3e-4813-8b42-2848891c54b5","_cell_guid":"3b97aa92-997f-4cf6-8d7d-e216cb205a02","trusted":true},"cell_type":"code","source":"wilderness_area_lookup = {}\nfor n in range(5):\n    binstr =format(n, '03b')\n    vals = [int(binstr[i]) for i in range(len(binstr))]\n    wilderness_area_lookup[n] = vals\nwilderness_area_lookup","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"03df5a2c-c0c9-42f9-805c-d28a464b44ad","_cell_guid":"d8d2da5b-8354-4442-ac36-95ab3cf5a132","trusted":true},"cell_type":"code","source":"# looping is a slow way to do this but it is adequate \nfor row in train.index:\n    bin_list = wilderness_area_lookup[train.loc[row,'Wilderness_Area']]\n    train.loc[row,'Wilderness_Area_bin0'] = bin_list[0]\n    train.loc[row,'Wilderness_Area_bin1'] = bin_list[1]\n    train.loc[row,'Wilderness_Area_bin2'] = bin_list[2]\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6a2c93a9-88a2-4943-a7d6-7f8333ac9970","_cell_guid":"2214c28d-df8e-41c1-b156-eece84e44c3f","trusted":true},"cell_type":"code","source":"soil_type_lookup = {}\nfor n in range(41):\n    binstr =format(n, '06b')\n    vals = [int(binstr[i]) for i in range(len(binstr))]\n    soil_type_lookup[n] = vals\nsoil_type_lookup","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b3af9d02-1454-4cc6-b620-b023cd4bfbd2","_cell_guid":"abe6fdad-4813-4785-a7ad-38e367c41364","trusted":true},"cell_type":"code","source":"# looping is a slow way to do this but it is adequate \nfor row in train.index:\n    bin_list = soil_type_lookup[train.loc[row,'Soil_Type']]\n    train.loc[row,'Soil_Type_bin0'] = bin_list[0]\n    train.loc[row,'Soil_Type_bin1'] = bin_list[1]\n    train.loc[row,'Soil_Type_bin2'] = bin_list[2]\n    train.loc[row,'Soil_Type_bin3'] = bin_list[3]\n    train.loc[row,'Soil_Type_bin4'] = bin_list[4]\n    train.loc[row,'Soil_Type_bin5'] = bin_list[5]\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2932295d-b1ac-4b8b-9a68-87edfbe189b0","_cell_guid":"83a8ae78-4802-4d6b-b82d-8776c3aa5ea8","trusted":true},"cell_type":"code","source":"train.columns","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a9e5a2ea-d7b9-4902-8cd3-e7d3d0037d77","_cell_guid":"34f57ae3-9ff6-4f32-aa89-09d8abf9e0ea","trusted":true},"cell_type":"code","source":"# new column names of interest.  \nX_col_names = [train.columns[1]]+train.columns[3:11].tolist()+train.columns[-11:].tolist()\nX_col_names","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0dffdb8f-6420-4263-a3c8-6125fd3982db","_cell_guid":"1f00082e-8f76-48db-99b4-73f9defc7394","trusted":true},"cell_type":"code","source":"y = train.Cover_Type\nX = train[X_col_names]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=43)\n\nestimator = tree.DecisionTreeClassifier(max_depth = 15)\nestimator.fit(X_train, y_train)\n\n#calculate the percent correct\nsum(estimator.predict(X_test)==y_test)/len(y_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"de8415e4-2455-4e0b-8270-73bc69ab4f94","_cell_guid":"1f252d25-e7b3-44f5-931d-cdab3efdb96c","trusted":true},"cell_type":"code","source":"estimator = RandomForestClassifier()\nestimator.fit(X_train, y_train)\n\n#calculate the percent correct\nsum(estimator.predict(X_test)==y_test)/len(y_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8bf89158-72ca-4a58-881b-804976309c85","_cell_guid":"fb769be4-bc6d-4ae1-814c-43478fa8b58e","trusted":true},"cell_type":"markdown","source":".... Not much different!  Consider trying the H2O random forest for another comparison."},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame(index = X_col_names, columns  = ['feature_importance'], data = estimator.feature_importances_).sort_values(ascending = False,by='feature_importance')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Intuitively it seems like slope would be a multiplier on aspect.  Aspect runs from about 0 to 40, let's multiply slope by aspect.    "},{"metadata":{"trusted":true},"cell_type":"code","source":"train['Aspect_N_S_Slope'] = train['Aspect_N_S'] * train['Slope'] \ntrain['Aspect_E_W_Slope'] = train['Aspect_E_W'] * train['Slope'] ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# new column names of interest.  \nX_col_names = [train.columns[1]]+train.columns[3:11].tolist()+train.columns[-13:].tolist()\nX_col_names","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = train.Cover_Type\nX = train[X_col_names]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=43)\n\nestimator = tree.DecisionTreeClassifier(max_depth = 15)\nestimator.fit(X_train, y_train)\n\n#calculate the percent correct\nsum(estimator.predict(X_test)==y_test)/len(y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"estimator = RandomForestClassifier()\nestimator.fit(X_train, y_train)\n\n#calculate the percent correct\nsum(estimator.predict(X_test)==y_test)/len(y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame(index = X_col_names, columns  = ['feature_importance'], \n             data = estimator.feature_importances_\n            ).sort_values(ascending = False,by='feature_importance')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Not much different.  Try enhancing the Elevation field with the Aspect_N_S_Slope field.  Seems like an addition/ subtraction problem.  Let's iteratively try it and see if it makes a difference:"},{"metadata":{"trusted":true},"cell_type":"code","source":"factors = [0,1,2,4,6,8,10,20,100, 1000, 10]\nfor factor in factors:\n    train['Elev_Asp_Slope'] = train['Aspect_N_S_Slope'] * factor +  train['Elevation'] \n    X_col_names = [train.columns[1]]+train.columns[3:11].tolist()+train.columns[-14:].tolist()\n    y = train.Cover_Type\n    X = train[X_col_names]\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=43)\n    estimator = RandomForestClassifier()\n    estimator.fit(X_train, y_train)\n    pct = sum(estimator.predict(X_test)==y_test)/len(y_test)\n    print('factor = ',factor, '; percent correct = ', pct) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame(index = X_col_names, columns  = ['feature_importance'], data = estimator.feature_importances_).sort_values(ascending = False,by='feature_importance')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Elev_asp_slope was used by the tree, but it didn't improve the prediction (all in the noise).  See if it helped logistic regression.  \n "},{"metadata":{"trusted":true},"cell_type":"code","source":"# logistic regression code for comparison\nestimator = LogisticRegression()\nestimator.fit(scaler.fit_transform(X_train), y_train)\npct = sum(estimator.predict(scaler.transform(X_test))==y_test)/len(y_test)\npct","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Nope - even worse than before!  Let's do some cross-validation before doing hyperparameter tuning on the random forest."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Cross validation\nestimator = RandomForestClassifier()\nscores = cross_val_score(estimator, X_train, y_train, cv=10)\n\npd.Series(scores).describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"estimator.get_params()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Number of trees in random forest\nn_estimators = [3, 5, 10, 50, 100]\n# Number of features to consider at every split\nmax_features = ['auto', None]\n# Maximum number of levels in tree\nmax_depth = [3, 5, 10, 50, 100, None]\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4, 10]\n# Method of selecting samples for training each tree\nbootstrap = [True, False]# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use the random grid to search for best hyperparameters\n# Random search of parameters, using 3 fold cross validation, \n# search across 100 different combinations, and use all available cores\nrandom_search = RandomizedSearchCV(estimator, param_distributions = random_grid, \n                               n_iter = 20, cv = 3, verbose=2, random_state=42)# Fit the random search model\nrandom_search.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"random_search.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"random_search.best_score_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"random_search.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"random_search.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# see how it performs on the test set\npct = sum(random_search.predict(X_test)==y_test)/len(y_test)\npct","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The accuracy is creeping up with hyperparameter tuning.  On the to-do list:  \n* package the data wrangling and feature engineering as a pipeline\n* develop and tune other models (SVM, LR, etc.), ensemble with random forest\n* more feature analysis/ engineering (e.g., understand confusions)\n* start on neural nets "}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}