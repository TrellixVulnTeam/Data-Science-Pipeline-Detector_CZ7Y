{"cells":[{"metadata":{"_uuid":"75bd9103-5ad0-4313-977b-c972b43a1648","_cell_guid":"362b9adb-3d2a-43f8-b054-938cf4027a18","trusted":true},"cell_type":"markdown","source":"## Pipeline Constructions on the Forest Type Dataset"},{"metadata":{"_uuid":"c0396603-bf0a-4559-8baa-eee076d7de29","_cell_guid":"66121d91-0300-46a9-8306-a50056dc269a","trusted":true},"cell_type":"code","source":"# imports\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn import tree\nfrom sklearn.ensemble import RandomForestClassifier \nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.pipeline import Pipeline\n\nfrom IPython.display import SVG\nfrom graphviz import Source\nfrom IPython.display import display\n\nimport os\nprint(os.listdir(\"../input/forest-cover-type-kernels-only\"))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"52ca1524-c841-4a82-8723-ca1a982189e9","_cell_guid":"e6302713-5113-4ae9-b620-f5661d9a8ec9","trusted":true},"cell_type":"code","source":"# new dataframe onthe training dataset\ntrain = pd.read_csv('../input/forest-cover-type-kernels-only/train.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# wrap the feature engineering in a function for convenience and tuning\n# this function results in a setting with copy warning in pandas - fix this later\npd.set_option('mode.chained_assignment', None)  # remove copy warnings for now\ndef add_features(df, split_aspect=True, aspect_slope=True, drop_aspects=True, \n                 elev_asp_slope=True, asp_slope_factor=10):\n    if split_aspect:\n        df['Aspect_N_S'] = np.cos(df.Aspect*np.pi/180)\n        df['Aspect_E_W'] = np.sin(df.Aspect*np.pi/180)\n        df.drop(columns = 'Aspect', inplace=True)\n        \n        if aspect_slope:\n            df['Aspect_N_S_Slope'] = df['Aspect_N_S'] * df['Slope'] \n            df['Aspect_E_W_Slope'] = df['Aspect_E_W'] * df['Slope']\n            if drop_aspects:\n                df.drop(columns = ['Aspect_N_S', 'Aspect_E_W'], inplace=True)\n\n            if elev_asp_slope:\n                df['Elev_Asp_Slope'] = df['Aspect_N_S_Slope'] * asp_slope_factor +  df['Elevation']\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pipeline = Pipeline([\n        ('add_features', FunctionTransformer(add_features, validate=False, \n                                            kw_args={'split_aspect':True, 'aspect_slope':True, \n                                                     'drop_aspects':True, 'elev_asp_slope':True,\n                                                     'asp_slope_factor':10}))\n    ])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels = train.columns.tolist()\nlabels.remove('Id')\nlabels.remove('Cover_Type')\nlabels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = train.Cover_Type\nX = train[labels]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=43)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0898895a-87a0-43c0-b643-9deb7746b4ea","_cell_guid":"d8a2c7b6-54db-45a5-8b7e-3176247bb6bb","trusted":true},"cell_type":"code","source":"estimator = RandomForestClassifier()\npipeline.fit_transform(X_train) #modifies X_train inplace\nestimator.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0898895a-87a0-43c0-b643-9deb7746b4ea","_cell_guid":"d8a2c7b6-54db-45a5-8b7e-3176247bb6bb","trusted":true},"cell_type":"code","source":"#calculate the percent correct\npipeline.transform(X_test)\nsum(estimator.predict(X_test)==y_test)/len(y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Cross validation\nestimator = RandomForestClassifier()\nscores = cross_val_score(estimator, X_train, y_train, cv=10)\n\npd.Series(scores).describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"estimator.get_params()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Inputs for random search CV - todo: try using continuous distributions as inputs\n# Number of trees in random forest\nn_estimators = [3, 5, 10, 50, 100]\n# Number of features to consider at every split\nmax_features = ['auto', None]\n# Maximum number of levels in tree\nmax_depth = [3, 5, 10, 50, 100, None]\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4, 10]\n# Method of selecting samples for training each tree\nbootstrap = [True, False]# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use the random grid to search for best hyperparameters\n# Random search of parameters, using 3 fold cross validation, \n# search across 100 different combinations, # and use all available cores (not implemented)\n\n# comment out to save compute time\n'''\nrandom_search = RandomizedSearchCV(estimator, param_distributions = random_grid, \n                               n_iter = 20, cv = 3, verbose=2, random_state=42)# Fit the random search model\nrandom_search.fit(X_train, y_train)\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print(random_search.best_params_)\n\n# print(random_search.best_score_)\n\n# print(random_search.best_estimator_)\n\n# # see how it performs on the test set\n# print(sum(random_search.predict(X_test)==y_test)/len(y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Based on the parameters found in the random search, try new pipeline with preparation and prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"pipeline = Pipeline([\n        ('add_features', FunctionTransformer(add_features, validate=False, \n                                            kw_args={'split_aspect':True, 'aspect_slope':True, \n                                                     'drop_aspects':True, 'elev_asp_slope':True,\n                                                     'asp_slope_factor':10})),\n        ('random_forest', RandomForestClassifier(bootstrap=False, class_weight=None, criterion='gini',\n                       max_depth=100, max_features='auto', max_leaf_nodes=None,\n                       min_impurity_decrease=0.0, min_impurity_split=None,\n                       min_samples_leaf=2, min_samples_split=5,\n                       min_weight_fraction_leaf=0.0, n_estimators=50,\n                       n_jobs=None, oob_score=False, random_state=None,\n                       verbose=0, warm_start=False))\n    ])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = train.Cover_Type\nX = train[labels]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=43)\n\npipeline.fit(X_train, y_train)\nsum(pipeline.predict(X_test)==y_test)/len(y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pipeline.get_params()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#after some trial and error, this code is working, but there are a lot of dictionaries - not sure how to fix\ny = train.Cover_Type\nX = train[labels]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=43)\n\nparam_grid = [{\n    'add_features__kw_args':[{'split_aspect':True},{'split_aspect':False}]\n}]\n\ngrid_search_prep = GridSearchCV(pipeline, param_grid, cv=5, verbose=2, n_jobs=4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = train.Cover_Type\nX = train[labels]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=43)\n\nparam_grid = [{\n    'add_features__kw_args':[{'split_aspect':False},\n                             {'split_aspect':True, 'aspect_slope': False},\n                             {'split_aspect':True, 'aspect_slope': True},\n                             {'split_aspect':True, 'aspect_slope':True, 'drop_aspects':False}, \n                             {'asp_slope_factor':.5},\n                              {'asp_slope_factor':1},\n                              {'asp_slope_factor':10}],\n    'random_forest__max_depth': [50, 100,200]\n}]\n\ngrid_search_prep = GridSearchCV(pipeline, param_grid, cv=5, verbose=2, n_jobs=4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_search_prep.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_search_prep.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_search_prep.cv_results_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i, param in enumerate(grid_search_prep.cv_results_['params']):\n    print(param,':\\n     ', grid_search_prep.cv_results_['mean_test_score'][i])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sum(grid_search_prep.predict(X_test)==y_test)/len(y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# a function that unonehots the data\ndef unonehot(df):\n    df['Wilderness_Area'] = df.Wilderness_Area1 * 1 + df.Wilderness_Area2 * 2 + df.Wilderness_Area3 * 3 + df.Wilderness_Area4 *4\n    df['Soil_Type'] = (df.Soil_Type1 * 1 + \n                    df.Soil_Type2 * 2 + \n                    df.Soil_Type3 * 3 + \n                    df.Soil_Type4 * 4 + \n                    df.Soil_Type5 * 5 + \n                    df.Soil_Type6 * 6 + \n                    df.Soil_Type7 * 7 + \n                    df.Soil_Type8 * 8 + \n                    df.Soil_Type9 * 9 + \n                    df.Soil_Type10 * 10 + \n                    df.Soil_Type11 * 11 + \n                    df.Soil_Type12 * 12 + \n                    df.Soil_Type13 * 13 + \n                    df.Soil_Type14 * 14 + \n                    df.Soil_Type15 * 15 + \n                    df.Soil_Type16 * 16 + \n                    df.Soil_Type17 * 17 + \n                    df.Soil_Type18 * 18 + \n                    df.Soil_Type19 * 19 + \n                    df.Soil_Type20 * 20 + \n                    df.Soil_Type21 * 21 + \n                    df.Soil_Type22 * 22 + \n                    df.Soil_Type23 * 23 + \n                    df.Soil_Type24 * 24 + \n                    df.Soil_Type25 * 25 + \n                    df.Soil_Type26 * 26 + \n                    df.Soil_Type27 * 27 + \n                    df.Soil_Type28 * 28 + \n                    df.Soil_Type29 * 29 + \n                    df.Soil_Type30 * 30 + \n                    df.Soil_Type31 * 31 + \n                    df.Soil_Type32 * 32 + \n                    df.Soil_Type33 * 33 + \n                    df.Soil_Type34 * 34 + \n                    df.Soil_Type35 * 35 + \n                    df.Soil_Type36 * 36 + \n                    df.Soil_Type37 * 37 + \n                    df.Soil_Type38 * 38 + \n                    df.Soil_Type39 * 39 + \n                    df.Soil_Type40 * 40)\n    df.drop(columns = [ 'Wilderness_Area1', 'Wilderness_Area2', 'Wilderness_Area3','Wilderness_Area4', \n                     'Soil_Type1',\n                     'Soil_Type2',\n                     'Soil_Type3',\n                     'Soil_Type4',\n                     'Soil_Type5',\n                     'Soil_Type6',\n                     'Soil_Type7',\n                     'Soil_Type8',\n                     'Soil_Type9',\n                     'Soil_Type10',\n                     'Soil_Type11',\n                     'Soil_Type12',\n                     'Soil_Type13',\n                     'Soil_Type14',\n                     'Soil_Type15',\n                     'Soil_Type16',\n                     'Soil_Type17',\n                     'Soil_Type18',\n                     'Soil_Type19',\n                     'Soil_Type20',\n                     'Soil_Type21',\n                     'Soil_Type22',\n                     'Soil_Type23',\n                     'Soil_Type24',\n                     'Soil_Type25',\n                     'Soil_Type26',\n                     'Soil_Type27',\n                     'Soil_Type28',\n                     'Soil_Type29',\n                     'Soil_Type30',\n                     'Soil_Type31',\n                     'Soil_Type32',\n                     'Soil_Type33',\n                     'Soil_Type34',\n                     'Soil_Type35',\n                     'Soil_Type36',\n                     'Soil_Type37',\n                     'Soil_Type38',\n                     'Soil_Type39',\n                     'Soil_Type40'], inplace=True)\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# a function that makes columns for binary encoding to reduce dimensionality\ndef wilderness_bin_onehot(df):\n    # make the lookup dicts\n    bin0 = {}\n    bin1 = {}\n    bin2 = {}\n    for i in range(5):\n        binstr =format(i, '03b')\n        bin0[i] = binstr[0]\n        bin1[i] = binstr[1]\n        bin2[i] = binstr[2]\n    \n    df['Wilderness_Area_bin0'] = df['Wilderness_Area'].map(bin0)\n    df['Wilderness_Area_bin1'] = df['Wilderness_Area'].map(bin1)\n    df['Wilderness_Area_bin2'] = df['Wilderness_Area'].map(bin2)\n    \n    df.drop(columns = 'Wilderness_Area', inplace = True)\n\n    return df\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# a function that makes columns for binary encoding to reduce dimensionality\ndef soil_type_bin_onehot(df):\n    # make the lookup dicts\n    bin0 = {}\n    bin1 = {}\n    bin2 = {}\n    bin3 = {}\n    bin4 = {}\n    bin5 = {}\n    for i in range(41):\n        binstr =format(i, '06b')\n        bin0[i] = binstr[0]\n        bin1[i] = binstr[1]\n        bin2[i] = binstr[2]\n        bin3[i] = binstr[3]\n        bin4[i] = binstr[4]\n        bin5[i] = binstr[5]\n    \n    \n    df['Soil_Type_bin0'] = df['Soil_Type'].map(bin0)\n    df['Soil_Type_bin1'] = df['Soil_Type'].map(bin1)\n    df['Soil_Type_bin2'] = df['Soil_Type'].map(bin2)\n    df['Soil_Type_bin3'] = df['Soil_Type'].map(bin3)\n    df['Soil_Type_bin4'] = df['Soil_Type'].map(bin4)\n    df['Soil_Type_bin5'] = df['Soil_Type'].map(bin5)\n    \n    df.drop(columns = 'Soil_Type', inplace = True)\n\n    return df\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# wrap the previous functions up in a single function so that they can be added to the pipelinea function that makes columns for binary encoding to reduce dimensionality\ndef bin_onehot(df, perf_bin_onehot = True):\n    if perf_bin_onehot: \n        unonehot(df)\n        wilderness_bin_onehot(df)\n        soil_type_bin_onehot(df)\n    return df\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pipeline = Pipeline([\n        ('add_features', FunctionTransformer(add_features, validate=False, \n                                            kw_args={'split_aspect':True, 'aspect_slope':True, \n                                                     'drop_aspects':True, 'elev_asp_slope':True,\n                                                     'asp_slope_factor':10})),\n        ('bin_onehot', FunctionTransformer(bin_onehot, validate=False, \n                                            kw_args={'perf_bin_onehot':True})),\n        ('random_forest', RandomForestClassifier(bootstrap=False, class_weight=None, criterion='gini',\n                       max_depth=100, max_features='auto', max_leaf_nodes=None,\n                       min_impurity_decrease=0.0, min_impurity_split=None,\n                       min_samples_leaf=2, min_samples_split=5,\n                       min_weight_fraction_leaf=0.0, n_estimators=50,\n                       n_jobs=None, oob_score=False, random_state=None,\n                       verbose=0, warm_start=False))\n    ])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = train.Cover_Type\nX = train[labels]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=43)\n\npipeline.fit(X_train, y_train)\nsum(pipeline.predict(X_test)==y_test)/len(y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = train.Cover_Type\nX = train[labels]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=43)\n\nparam_grid = [{\n    'add_features__kw_args':[{'split_aspect':False}, \n                             {'asp_slope_factor':.5}],\n    'bin_onehot__kw_args': [{'perf_bin_onehot':True},\n                           {'perf_bin_onehot':False}],\n    'random_forest__max_depth': [100,200]\n}]\n\ngrid_search_prep = GridSearchCV(pipeline, param_grid, cv=5, verbose=2, n_jobs=4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_search_prep.fit(X_train, y_train)\nprint(grid_search_prep.best_params_)\nfor i, param in enumerate(grid_search_prep.cv_results_['params']):\n    print(param,':\\n     ', grid_search_prep.cv_results_['mean_test_score'][i])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sum(grid_search_prep.predict(X_test)==y_test)/len(y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"interesting.  the deliberate feature engineering did not help, but the binary digit dimensionality reduction encoder, which is sort of random, improved performance.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"# new dataframe onthe training dataset\ntest = pd.read_csv('../input/forest-cover-type-kernels-only/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels = test.columns[1:]\nlabels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred = grid_search_prep.predict(test[labels])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submit = pd.DataFrame(test.Id)\nsubmit['Cover_Type'] = pred\nsubmit.to_csv('submission.csv', index=False)\nsubmit","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The accuracy is creeping up with hyperparameter tuning.  On the to-do list:  \n* package the data wrangling and feature engineering as a pipeline\n* develop and tune other models (SVM, LR, etc.), ensemble with random forest\n* more feature analysis/ engineering (e.g., understand confusions)\n* start on neural nets "}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}