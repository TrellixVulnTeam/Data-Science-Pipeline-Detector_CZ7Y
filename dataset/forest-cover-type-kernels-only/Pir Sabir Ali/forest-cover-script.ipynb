{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.preprocessing import MinMaxScaler\nimport sklearn\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\nfrom keras.models import Sequential\nfrom keras.layers.core import Dense, Activation, Dropout\nfrom keras.optimizers import SGD\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\nimport lightgbm as lgb\nfrom tqdm import tqdm\nimport os\nimport gc\nfrom itertools import combinations, chain\nfrom datetime import datetime","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train_df = pd.read_csv(\"../input/train.csv\")\ntest_df = pd.read_csv(\"../input/test.csv\")\nsmpsb = pd.read_csv(\"../input/sample_submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"type_ratio = np.array([0.37053, 0.49681, 0.05936, 0.00103, 0.01295, 0.02687, 0.03242])\nclass_weight = {k: v for k, v in enumerate(type_ratio, start=1)}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def balanced_accuracy_score(y_true, y_pred):\n    return accuracy_score(y_true, y_pred, sample_weight=np.apply_along_axis(lambda x: type_ratio[x], 0, y_true-1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv(\"../input/train.csv\")\ntest_df = pd.read_csv(\"../input/test.csv\")\nsmpsb = pd.read_csv(\"../input/sample_submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def main(train_df, test_df):\n    # this is public leaderboard ratio\n    start = datetime.now()\n    type_ratio = np.array([0.37053, 0.49681, 0.05936, 0.00103, 0.01295, 0.02687, 0.03242])\n    \n    total_df = pd.concat([train_df.iloc[:, :-1], test_df])\n    \n    # Aspect\n    total_df[\"Aspect_Sin\"] = np.sin(np.pi*total_df[\"Aspect\"]/180)\n    total_df[\"Aspect_Cos\"] = np.cos(np.pi*total_df[\"Aspect\"]/180)\n    print(\"Aspect\", (datetime.now() - start).seconds)\n    \n    # Hillshade\n    hillshade_col = [\"Hillshade_9am\", \"Hillshade_Noon\", \"Hillshade_3pm\"]\n    for col1, col2 in combinations(hillshade_col, 2):\n        total_df[col1 + \"_add_\" + col2] = total_df[col2] + total_df[col1]\n        total_df[col1 + \"_dif_\" + col2] = total_df[col2] - total_df[col1]\n        total_df[col1 + \"_div_\" + col2] = (total_df[col2]+0.01) / (total_df[col1]+0.01)\n        total_df[col1 + \"_abs_\" + col2] = np.abs(total_df[col2] - total_df[col1])\n    \n    total_df[\"Hillshade_mean\"] = total_df[hillshade_col].mean(axis=1)\n    total_df[\"Hillshade_std\"] = total_df[hillshade_col].std(axis=1)\n    total_df[\"Hillshade_max\"] = total_df[hillshade_col].max(axis=1)\n    total_df[\"Hillshade_min\"] = total_df[hillshade_col].min(axis=1)\n    print(\"Hillshade\", (datetime.now() - start).seconds)\n    \n    # Hydrology ** I forgot to add arctan\n    total_df[\"Degree_to_Hydrology\"] = ((total_df[\"Vertical_Distance_To_Hydrology\"] + 0.001) /\n                                       (total_df[\"Horizontal_Distance_To_Hydrology\"] + 0.01))\n    \n    # Holizontal\n    horizontal_col = [\"Horizontal_Distance_To_Hydrology\",\n                      \"Horizontal_Distance_To_Roadways\",\n                      \"Horizontal_Distance_To_Fire_Points\"]\n    \n    \n    for col1, col2 in combinations(hillshade_col, 2):\n        total_df[col1 + \"_add_\" + col2] = total_df[col2] + total_df[col1]\n        total_df[col1 + \"_dif_\" + col2] = total_df[col2] - total_df[col1]\n        total_df[col1 + \"_div_\" + col2] = (total_df[col2]+0.01) / (total_df[col1]+0.01)\n        total_df[col1 + \"_abs_\" + col2] = np.abs(total_df[col2] - total_df[col1])\n    print(\"Holizontal\", (datetime.now() - start).seconds)\n    \n    \n    def categorical_post_mean(x):\n        p = (x.values)*type_ratio\n        p = p/p.sum()*x.sum() + 10*type_ratio\n        return p/p.sum()\n    \n    # Wilder\n    wilder = pd.DataFrame([(train_df.iloc[:, 11:15] * np.arange(1, 5)).sum(axis=1),\n                          train_df.Cover_Type]).T\n    wilder.columns = [\"Wilder_Type\", \"Cover_Type\"]\n    wilder[\"one\"] = 1\n    piv = wilder.pivot_table(values=\"one\",\n                             index=\"Wilder_Type\",\n                             columns=\"Cover_Type\",\n                             aggfunc=\"sum\").fillna(0)\n    \n    tmp = pd.DataFrame(piv.apply(categorical_post_mean, axis=1).tolist()).reset_index()\n    tmp[\"index\"] = piv.sum(axis=1).index\n    tmp.columns = [\"Wilder_Type\"] + [\"Wilder_prob_ctype_{}\".format(i) for i in range(1, 8)]\n    tmp[\"Wilder_Type_count\"] = piv.sum(axis=1).values\n    \n    total_df[\"Wilder_Type\"] = (total_df.filter(regex=\"Wilder\") * np.arange(1, 5)).sum(axis=1)\n    total_df = total_df.merge(tmp, on=\"Wilder_Type\", how=\"left\")\n    \n    for i in range(7):\n        total_df.loc[:, \"Wilder_prob_ctype_{}\".format(i+1)] = total_df.loc[:, \"Wilder_prob_ctype_{}\".format(i+1)].fillna(type_ratio[i])\n    total_df.loc[:, \"Wilder_Type_count\"] = total_df.loc[:, \"Wilder_Type_count\"].fillna(0)\n    print(\"Wilder_type\", (datetime.now() - start).seconds)\n    \n    \n    # Soil type\n    soil = pd.DataFrame([(train_df.iloc[:, -41:-1] * np.arange(1, 41)).sum(axis=1),\n                          train_df.Cover_Type]).T\n    soil.columns = [\"Soil_Type\", \"Cover_Type\"]\n    soil[\"one\"] = 1\n    piv = soil.pivot_table(values=\"one\",\n                           index=\"Soil_Type\",\n                           columns=\"Cover_Type\",\n                           aggfunc=\"sum\").fillna(0)\n    \n    tmp = pd.DataFrame(piv.apply(categorical_post_mean, axis=1).tolist()).reset_index()\n    tmp[\"index\"] = piv.sum(axis=1).index\n    tmp.columns = [\"Soil_Type\"] + [\"Soil_prob_ctype_{}\".format(i) for i in range(1, 8)]\n    tmp[\"Soil_Type_count\"] = piv.sum(axis=1).values\n    \n    total_df[\"Soil_Type\"] = (total_df.filter(regex=\"Soil\") * np.arange(1, 41)).sum(axis=1)\n    total_df = total_df.merge(tmp, on=\"Soil_Type\", how=\"left\")\n    \n    for i in range(7):\n        total_df.loc[:, \"Soil_prob_ctype_{}\".format(i+1)] = total_df.loc[:, \"Soil_prob_ctype_{}\".format(i+1)].fillna(type_ratio[i])\n    total_df.loc[:, \"Soil_Type_count\"] = total_df.loc[:, \"Soil_Type_count\"].fillna(0)\n    print(\"Soil_type\", (datetime.now() - start).seconds)\n    \n    icol = total_df.select_dtypes(np.int64).columns\n    fcol = total_df.select_dtypes(np.float64).columns\n    total_df.loc[:, icol] = total_df.loc[:, icol].astype(np.int32)\n    total_df.loc[:, fcol] = total_df.loc[:, fcol].astype(np.float32)\n    return total_df\n\ntotal_df = main(train_df, test_df)\none_col = total_df.filter(regex=\"(Type\\d+)|(Area\\d+)\").columns\ntotal_df = total_df.drop(one_col, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = train_df[\"Cover_Type\"].values\nX = total_df[total_df[\"Id\"] <= 15120].drop(\"Id\", axis=1)\nX_test = total_df[total_df[\"Id\"] > 15120].drop(\"Id\", axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_set =  [['Elevation', 500],\n            ['Horizontal_Distance_To_Roadways', 500],\n            ['Horizontal_Distance_To_Fire_Points', 500],\n            ['Horizontal_Distance_To_Hydrology', 500],\n            ['Hillshade_9am', 500],\n            ['Aspect', 500],\n            ['Hillshade_3pm', 500],\n            ['Slope', 500],\n            ['Hillshade_Noon', 500],\n            ['Vertical_Distance_To_Hydrology', 500],\n            ['Elevation_PLUS_Vertical_Distance_To_Hydrology', 200],\n            ['Elevation_PLUS_Hillshade_9am_add_Hillshade_Noon', 200],\n            ['Elevation_PLUS_Aspect', 200],\n            ['Elevation_PLUS_Hillshade_Noon_dif_Hillshade_3pm', 200],\n            ['Elevation_PLUS_Hillshade_Noon_abs_Hillshade_3pm', 200],\n            ['Elevation_PLUS_Hillshade_9am', 200],\n            ['Elevation_PLUS_Horizontal_Distance_To_Hydrology', 200],\n            ['Elevation_PLUS_Horizontal_Distance_To_Roadways', 100],\n            ['Elevation_PLUS_Vertical_Distance_To_Hydrology', 200],\n            ['Wilder_Type_PLUS_Elevation', 500],\n            ['Wilder_Type_PLUS_Hillshade_Noon_div_Hillshade_3pm', 500],\n            ['Wilder_Type_PLUS_Degree_to_Hydrology', 200],\n            ['Wilder_Type_PLUS_Hillshade_9am_div_Hillshade_3pm', 500],\n            ['Wilder_Type_PLUS_Aspect_Cos', 500],\n            ['Hillshade_9am_dif_Hillshade_Noon_PLUS_Hillshade_Noon_dif_Hillshade_3pm', 200],\n            ['Hillshade_Noon_PLUS_Hillshade_3pm', 200],\n            ['Hillshade_Noon_add_Hillshade_3pm_PLUS_Hillshade_Noon_dif_Hillshade_3pm', 200]]\n\n\ndef simple_feature_scores2(clf, cols, test=False, **params):\n    scores = []\n    bscores = []\n    lscores = []\n    \n    X_preds = np.zeros((len(y), 7))\n    scl = StandardScaler().fit(X.loc[:, cols])\n    \n    for train, val in StratifiedKFold(n_splits=10, shuffle=True, random_state=2018).split(X, y):\n        X_train = scl.transform(X.loc[train, cols])\n        X_val = scl.transform(X.loc[val, cols])\n        y_train = y[train]\n        y_val = y[val]\n        C = clf(**params) \n\n        C.fit(X_train, y_train)\n        X_preds[val] = C.predict_proba(X_val)\n        #scores.append(accuracy_score(y_val, C.predict(X_val)))\n        #bscores.append(balanced_accuracy_score(y_val, C.predict(X_val)))\n        #lscores.append(log_loss(y_val, C.predict_proba(X_val), labels=list(range(1, 8))))\n    \n    if test:\n        X_test_select = scl.transform(X_test.loc[:, cols])\n        C = clf(**params)\n        C.fit(scl.transform(X.loc[:, cols]), y)\n        X_test_preds = C.predict_proba(X_test_select)\n    else:\n        X_test_preds = None\n    return scores, bscores, lscores, X_preds, X_test_preds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nimport gc\nfrom multiprocessing import Pool\n\nwarnings.filterwarnings(\"ignore\")\n\npreds = []\ntest_preds = []\nfor colname, neighbor in tqdm(all_set):\n    gc.collect()\n    #print(colname, depth)\n    ts, tbs, ls, pred, test_pred = simple_feature_scores2(KNeighborsClassifier,\n                                                          colname.split(\"_PLUS_\"),\n                                                          test=True,\n                                                          n_neighbors=neighbor)\n    preds.append(pred)\n    test_preds.append(test_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols = list(chain.from_iterable([[col[0] + \"_KNN_{}\".format(i) for i in range(1, 8)] for col in all_set]))\nknn_train_df = pd.DataFrame(np.hstack(preds)).astype(np.float32)\nknn_train_df.columns = cols\nknn_test_df = pd.DataFrame(np.hstack(test_preds)).astype(np.float32)\nknn_test_df.columns = cols","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_set = [['Elevation', 4],\n           ['Horizontal_Distance_To_Roadways', 4],\n           ['Horizontal_Distance_To_Fire_Points', 3],\n           ['Horizontal_Distance_To_Hydrology', 4],\n           ['Hillshade_9am', 3],\n           ['Vertical_Distance_To_Hydrology', 3],\n           ['Slope', 4],\n           ['Aspect', 4],\n           ['Hillshade_3pm', 3],\n           ['Hillshade_Noon', 3],\n           ['Degree_to_Hydrology', 3],\n           ['Hillshade_Noon_dif_Hillshade_3pm', 3],\n           ['Hillshade_Noon_abs_Hillshade_3pm', 3],\n           ['Elevation_PLUS_Hillshade_9am_add_Hillshade_Noon', 5],\n           ['Elevation_PLUS_Hillshade_max', 5],\n           ['Elevation_PLUS_Horizontal_Distance_To_Hydrology', 5],\n           ['Aspect_Sin_PLUS_Aspect_Cos_PLUS_Elevation', 5],\n           ['Elevation_PLUS_Horizontal_Distance_To_Fire_Points', 5],\n           ['Wilder_Type_PLUS_Elevation', 5],\n           ['Elevation_PLUS_Hillshade_9am', 5],\n           ['Elevation_PLUS_Degree_to_Hydrology', 5],\n           ['Wilder_Type_PLUS_Horizontal_Distance_To_Roadways', 5],\n           ['Wilder_Type_PLUS_Hillshade_9am_add_Hillshade_Noon', 4],\n           ['Wilder_Type_PLUS_Horizontal_Distance_To_Hydrology', 5],\n           ['Wilder_Type_PLUS_Hillshade_Noon_abs_Hillshade_3pm', 4],\n           ['Hillshade_9am_add_Hillshade_Noon_PLUS_Hillshade_std', 4],\n           ['Hillshade_9am_PLUS_Hillshade_9am_add_Hillshade_Noon', 4],\n           ['Hillshade_9am_add_Hillshade_Noon_PLUS_Hillshade_Noon_add_Hillshade_3pm', 5]]\n\ndef simple_feature_scores(clf, cols, test=False, **params):\n    scores = []\n    bscores = []\n    lscores = []\n    \n    X_preds = np.zeros((len(y), 7))\n    \n    \n    for train, val in StratifiedKFold(n_splits=10, shuffle=True, random_state=2018).split(X, y):\n        X_train = X.loc[train, cols]\n        X_val = X.loc[val, cols]\n        y_train = y[train]\n        y_val = y[val]\n        C = clf(**params) \n\n        C.fit(X_train, y_train)\n        X_preds[val] = C.predict_proba(X_val)\n        #scores.append(accuracy_score(y_val, C.predict(X_val)))\n        #bscores.append(balanced_accuracy_score(y_val, C.predict(X_val)))\n        #lscores.append(log_loss(y_val, C.predict_proba(X_val), labels=list(range(1, 8))))\n    \n    if test:\n        X_test_select = X_test.loc[:, cols]\n        C = clf(**params)\n        C.fit(X.loc[:, cols], y)\n        X_test_preds = C.predict_proba(X_test_select)\n    else:\n        X_test_preds = None\n    return scores, bscores, lscores, X_preds, X_test_preds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = []\ntest_preds = []\nfor colname, depth in tqdm(all_set):\n    #print(colname, depth)\n    ts, tbs, ls, pred, test_pred = simple_feature_scores(DecisionTreeClassifier,\n                                                         colname.split(\"_PLUS_\"),\n                                                         test=True,\n                                                         max_depth=depth)\n    preds.append(pred)\n    test_preds.append(test_pred)\n\ncols = list(chain.from_iterable([[col[0] + \"_DT_{}\".format(i) for i in range(1, 8)] for col in all_set]))\ndt_train_df = pd.DataFrame(np.hstack(preds)).astype(np.float32)\ndt_train_df.columns = cols\n\ndt_test_df = pd.DataFrame(np.hstack(test_preds)).astype(np.float32)\ndt_test_df.columns = cols","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"te_train_df = total_df.filter(regex=\"ctype\").iloc[:len(train_df)]\nte_test_df = total_df.filter(regex=\"ctype\").iloc[len(train_df):]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_level2 = train_df[[\"Id\"]]\ntest_level2 = test_df[[\"Id\"]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = train_df[\"Cover_Type\"].values\nX = total_df[total_df[\"Id\"] <= 15120].drop(\"Id\", axis=1)\nX_test = total_df[total_df[\"Id\"] > 15120].drop(\"Id\", axis=1)\ntype_ratio = np.array([0.37053, 0.49681, 0.05936, 0.00103, 0.01295, 0.02687, 0.03242])\nclass_weight = {k: v for k, v in enumerate(type_ratio, start=1)}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"RFC1_col = [\"RFC1_{}_proba\".format(i) for i in range(1, 8)]\nfor col in RFC1_col:\n    train_level2.loc[:, col] = 0\n    test_level2.loc[:, col] = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rfc = RandomForestClassifier(n_estimators=150,\n                             max_depth=12,\n                             class_weight=class_weight,\n                             n_jobs=-1)\n\nconfusion = np.zeros((7, 7))\nscores = []\nfor train, val in tqdm(StratifiedKFold(n_splits=10, random_state=2434, shuffle=True).split(X, y)):\n    X_train = X.iloc[train, :]\n    X_val = X.iloc[val, :]\n\n    y_train = y[train]\n    y_val = y[val]\n    rfc.fit(X_train, y_train)\n    y_val_pred = rfc.predict(X_val)\n    y_val_proba = rfc.predict_proba(X_val)\n    \n    confusion += confusion_matrix(y_val, y_val_pred)    \n    train_level2.loc[val, RFC1_col] = y_val_proba\n    scores.append(balanced_accuracy_score(y_val, y_val_pred))\n\nrfc.fit(X, y)\ntest_level2.loc[:, RFC1_col] = rfc.predict_proba(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"KNN1_col = [\"KNN1_{}_proba\".format(i) for i in range(1, 8)]\nfor col in KNN1_col:\n    train_level2.loc[:, col] = 0\n    test_level2.loc[:, col] = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_col = X.filter(regex=\"Soil_Type|Wilderness\").columns.tolist()[:-1] + [\"Wilder_Type\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"knn = KNeighborsClassifier(n_neighbors=2, n_jobs=-1)\nscl = StandardScaler().fit(X_test.drop(cat_col, axis=1))\nX_scl = scl.transform(X.drop(cat_col, axis=1))\nX_test_scl = scl.transform(X_test.drop(cat_col, axis=1))\npca = PCA(n_components=23).fit(X_test_scl)\nX_pca = pca.transform(X_scl)\nX_test_pca = pca.transform(X_test_scl)\n\nconfusion = np.zeros((7, 7))\nscores = []\nfor train, val in tqdm(StratifiedKFold(n_splits=10, random_state=2434, shuffle=True).split(X, y)):\n    X_train = X_pca[train]\n    X_val = X_pca[val]\n\n    y_train = y[train]\n    y_val = y[val]\n    knn.fit(X_train, y_train)\n    y_val_pred = knn.predict(X_val)\n    y_val_proba = knn.predict_proba(X_val)\n    \n    confusion += confusion_matrix(y_val, y_val_pred)    \n    train_level2.loc[val, KNN1_col] = y_val_proba\n    scores.append(balanced_accuracy_score(y_val, y_val_pred))\n\nknn.fit(X_pca, y)\ntest_level2.loc[:, KNN1_col] = knn.predict_proba(X_test_pca)\n#smpsb.loc[:, \"Cover_Type\"] = knn.predict(X_test_pca)\n#smpsb.to_csv(\"KNN1.csv\", index=None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"LGBM1_col = [\"LGBM1_{}_proba\".format(i) for i in range(1, 8)]\nfor col in LGBM1_col:\n    train_level2.loc[:, col] = 0\n    test_level2.loc[:, col] = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_col = X.filter(regex=\"Soil_Type|Wilderness\").columns.tolist()[:-1] + [\"Wilder_Type\"]\ncategorical_feature = [29, 38]\nlgbm_col = X.drop(cat_col[:-2], axis=1).columns.tolist()\nclass_weight_lgbm = {i: v for i, v in enumerate(type_ratio)}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gbm = lgb.LGBMClassifier(n_estimators=15,\n                         num_class=7,\n                         learning_rate=0.1,\n                         bagging_fraction=0.6,\n                         num_boost_round=370,\n                         max_depth=8,\n                         max_cat_to_onehot=40,\n                         class_weight=class_weight_lgbm,\n                         device=\"cpu\",\n                         n_jobs=4,\n                         silent=-1,\n                         verbose=-1)\n\nconfusion = np.zeros((7, 7))\nscores = []\nfor train, val in tqdm(StratifiedKFold(n_splits=10, random_state=2434, shuffle=True).split(X, y)):\n    X_train = X.loc[train, lgbm_col]\n    X_val = X.loc[val, lgbm_col]\n\n    y_train = y[train]\n    y_val = y[val]\n    gbm.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_val, y_val)],\n            verbose=50, categorical_feature=categorical_feature)\n\n    y_val_pred = gbm.predict(X_val)\n    y_val_proba = gbm.predict_proba(X_val)\n    \n    scores.append(balanced_accuracy_score(y_val, y_val_pred))\n    confusion += confusion_matrix(y_val, y_val_pred)\n    train_level2.loc[val, LGBM1_col] = y_val_proba\n\n\nX_all = X.loc[:, lgbm_col]\nX_test_lgbm = X_test.loc[:, lgbm_col]\ngbm.fit(X_all, y, verbose=50, categorical_feature=categorical_feature)\ntest_level2.loc[:, LGBM1_col] = gbm.predict_proba(X_test_lgbm)\n#smpsb[\"Cover_Type\"] = gbm.predict(X_test_lgbm)\n#smpsb.to_csv(\"LGBM1.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_p = pd.concat([knn_train_df, dt_train_df, te_train_df], axis=1).astype(np.float32)\nX_test_p = pd.concat([knn_test_df, dt_test_df, te_test_df.reset_index(drop=True)], axis=1).astype(np.float32)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"KNNDT_RF_col = [\"KNNDT_RF_{}_proba\".format(i) for i in range(1, 8)]\nfor col in KNNDT_RF_col:\n    train_level2.loc[:, col] = 0\n    test_level2.loc[:, col] = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rfc = RandomForestClassifier(n_jobs=-1,\n                             n_estimators=200,\n                             max_depth=None,\n                             max_features=.7,\n                             max_leaf_nodes=220,\n                             class_weight=class_weight)\n\nconfusion = np.zeros((7, 7))\nscores = []\nfor train, val in tqdm(StratifiedKFold(n_splits=10, shuffle=True, random_state=2434).split(X_p, y)):\n    X_train = X_p.iloc[train, :]\n    y_train = y[train]\n    X_val = X_p.iloc[val, :]\n    y_val = y[val]\n    rfc.fit(X_train, y_train)\n\n    y_pred = rfc.predict(X_val)\n    scores.append(balanced_accuracy_score(y_val, y_pred))\n    confusion += confusion_matrix(y_val, y_pred)\n    train_level2.loc[val, KNNDT_RF_col] = rfc.predict_proba(X_val)\n\nrfc.fit(X_p, y)\ntest_level2.loc[:, KNNDT_RF_col] = rfc.predict_proba(X_test_p)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"KNNDT_LR_col = [\"KNNDT_LR_{}_proba\".format(i) for i in range(1, 8)]\nfor col in KNNDT_LR_col:\n    train_level2.loc[:, col] = 0\n    test_level2.loc[:, col] = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"confusion = np.zeros((7, 7))\nscores = []\nfor train, val in tqdm(StratifiedKFold(n_splits=10, shuffle=True, random_state=2434).split(X, y)):\n    X_train = X_p.iloc[train, :]\n    y_train = y[train]\n    X_val = X_p.iloc[val, :]\n    y_val = y[val]\n    lr = LogisticRegression(n_jobs=-1, multi_class=\"multinomial\", C=10**9, solver=\"saga\", class_weight=class_weight)\n    lr.fit(X_train, y_train)\n    y_val_pred = lr.predict(X_val)\n    train_level2.loc[val, KNNDT_LR_col] = lr.predict_proba(X_val)\n    scores.append(balanced_accuracy_score(y_val, y_val_pred))\n    confusion += confusion_matrix(y_val, y_val_pred)\n\nlr.fit(X_p, y)\ntest_level2.loc[:, KNNDT_LR_col] = lr.predict_proba(X_test_p)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"KNNDT_LGB_col = [\"KNNDT_LGB_{}_proba\".format(i) for i in range(1, 8)]\nfor col in KNNDT_LGB_col:\n    train_level2.loc[:, col] = 0\n    test_level2.loc[:, col] = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = total_df[total_df[\"Id\"] <= 15120].drop(\"Id\", axis=1)\nX_test = total_df[total_df[\"Id\"] > 15120].drop(\"Id\", axis=1).reset_index(drop=True)\n\nX_d = pd.concat([X.drop(total_df.filter(regex=\"Type\\d+\").columns, axis=1),\n                 knn_train_df,\n                 dt_train_df], axis=1)\n\nX_test_d = pd.concat([X_test.drop(total_df.filter(regex=\"Type\\d+\").columns, axis=1),\n                 knn_test_df,\n                 dt_test_df], axis=1)\n\nfcol = X_d.select_dtypes(np.float64).columns\nX_d.loc[:, fcol] = X_d.loc[:, fcol].astype(np.float32)\nX_d = X_d.values.astype(np.float32)\nX_test_d.loc[:, fcol] = X_test_d.loc[:, fcol].astype(np.float32)\nX_test_d = X_test_d.values.astype(np.float32)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class_weight_lgbm = {i: v for i, v in enumerate(type_ratio)}\n\ngbm = lgb.LGBMClassifier(n_estimators=300,\n                         num_class=8,\n                         num_leaves=32,\n                         feature_fraction=0.3,\n                         min_child_samples=20,\n                         learning_rate=0.05,\n                         num_boost_round=430,\n                         max_depth=-1,                         \n                         class_weight=class_weight_lgbm,\n                         device=\"cpu\",\n                         n_jobs=4,\n                         silent=-1,\n                         verbose=-1)\n\nconfusion = np.zeros((7, 7))\nscores = []\nfor train, val in tqdm(StratifiedKFold(n_splits=10, shuffle=True, random_state=2434).split(X_p, y)):\n    X_train = X_d[train]\n    X_val = X_d[val]\n\n    y_train = y[train]\n    y_val = y[val]\n    gbm.fit(X_train, y_train, categorical_feature=[33, 42])\n\n    y_pred = gbm.predict(X_val)\n    scores.append(balanced_accuracy_score(y_val, y_pred))\n    confusion += confusion_matrix(y_val, y_pred)\n    train_level2.loc[val, KNNDT_LGB_col] = gbm.predict_proba(X_val)\n    \ngbm.fit(X_d, y, categorical_feature=[33, 42])\ntest_level2.loc[:, KNNDT_LGB_col] = gbm.predict_proba(X_test_d)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n#import warnings\n#warnings.filterwarnings('ignore')\n\n# Any results you write to the current directory are saved as output.\n\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nfrom sklearn.model_selection import StratifiedKFold, GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.svm import SVC\nimport lightgbm as lgb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train=pd.read_csv('../input/train.csv')\ntest=pd.read_csv('../input/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.drop(['Soil_Type7', 'Soil_Type15'], axis=1, inplace=True)\ntest.drop(['Soil_Type7', 'Soil_Type15'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['HF1'] = train['Horizontal_Distance_To_Hydrology']+train['Horizontal_Distance_To_Fire_Points']\ntrain['HF2'] = abs(train['Horizontal_Distance_To_Hydrology']-train['Horizontal_Distance_To_Fire_Points'])\ntrain['HR1'] = abs(train['Horizontal_Distance_To_Hydrology']+train['Horizontal_Distance_To_Roadways'])\ntrain['HR2'] = abs(train['Horizontal_Distance_To_Hydrology']-train['Horizontal_Distance_To_Roadways'])\ntrain['FR1'] = abs(train['Horizontal_Distance_To_Fire_Points']+train['Horizontal_Distance_To_Roadways'])\ntrain['FR2'] = abs(train['Horizontal_Distance_To_Fire_Points']-train['Horizontal_Distance_To_Roadways'])\ntrain['ele_vert'] = train.Elevation-train.Vertical_Distance_To_Hydrology\ntrain['Mean_Amenities']=(train.Horizontal_Distance_To_Fire_Points + train.Horizontal_Distance_To_Hydrology + train.Horizontal_Distance_To_Roadways) / 3  \ntrain['Mean_Fire_Hyd']=(train.Horizontal_Distance_To_Fire_Points + train.Horizontal_Distance_To_Hydrology) / 2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['HF1'] = test['Horizontal_Distance_To_Hydrology']+test['Horizontal_Distance_To_Fire_Points']\ntest['HF2'] = abs(test['Horizontal_Distance_To_Hydrology']-test['Horizontal_Distance_To_Fire_Points'])\ntest['HR1'] = abs(test['Horizontal_Distance_To_Hydrology']+test['Horizontal_Distance_To_Roadways'])\ntest['HR2'] = abs(test['Horizontal_Distance_To_Hydrology']-test['Horizontal_Distance_To_Roadways'])\ntest['FR1'] = abs(test['Horizontal_Distance_To_Fire_Points']+test['Horizontal_Distance_To_Roadways'])\ntest['FR2'] = abs(test['Horizontal_Distance_To_Fire_Points']-test['Horizontal_Distance_To_Roadways'])\ntest['ele_vert'] = test.Elevation-test.Vertical_Distance_To_Hydrology \ntest['Mean_Amenities']=(test.Horizontal_Distance_To_Fire_Points + test.Horizontal_Distance_To_Hydrology + test.Horizontal_Distance_To_Roadways) / 3  \ntest['Mean_Fire_Hyd']=(test.Horizontal_Distance_To_Fire_Points + test.Horizontal_Distance_To_Hydrology) / 2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Id_train=train['Id']\nId_test=test['Id']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.drop('Id', axis=1, inplace=True)\ntest.drop('Id', axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train=train.drop('Cover_Type', axis=1)\ny_train=train['Cover_Type']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train_L2=pd.DataFrame(Id_train)\nx_test_L2=pd.DataFrame(Id_test)\nrf_cul=['rf'+str(i+1) for i in range(7)]\n\n#prepare cols to store pred proba\nfor i in rf_cul:\n    x_train_L2.loc[:, i]=0\n    x_test_L2.loc[:, i]=0\n\nrf=RandomForestClassifier(max_depth=None, max_features=20,n_estimators=500, random_state=1)\n\n#StratifiedKfold to avoid leakage\nfor train_index, val_index in tqdm(StratifiedKFold(n_splits=10, shuffle=True, random_state=1).split(x_train, y_train)):\n    x_train_L1=x_train.iloc[train_index, :]\n    y_train_L1=y_train.iloc[train_index]\n    x_val_L1=x_train.iloc[val_index, :]\n    y_val_L1=y_train.iloc[val_index]\n\n    rf.fit(x_train_L1, y_train_L1)\n    y_val_proba=rf.predict_proba(x_val_L1)\n    x_train_L2.loc[val_index, rf_cul]=y_val_proba\n\nrf.fit(x_train, y_train)\nx_test_L2.loc[:, rf_cul]=rf.predict_proba(test)\n\n#prepare df for submission\n#submit_df=pd.DataFrame(rf.predict(test))\n#submit_df.columns=['Cover_Type']\n#submit_df['Id']=Id_test\n#submit_df=submit_df.loc[:, ['Id', 'Cover_Type']]\n#submit_df.to_csv('rf.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lgbm_cul=['lgbm'+str(i+1) for i in range(7)]\n\n#prepare cols to store pred proba\nfor i in lgbm_cul:\n    x_train_L2.loc[:, i]=0\n    x_test_L2.loc[:, i]=0\n\nlgbm=lgb.LGBMClassifier(learning_rate=0.3, max_depth=-1, min_child_samples=20, n_estimators=300, num_leaves=200, random_state=1, n_jobs=4)\n\n#StratifiedKfold to avoid leakage\nfor train_index, val_index in tqdm(StratifiedKFold(n_splits=10, shuffle=True, random_state=1).split(x_train, y_train)):\n    x_train_L1=x_train.iloc[train_index, :]\n    y_train_L1=y_train.iloc[train_index]\n    x_val_L1=x_train.iloc[val_index, :]\n    y_val_L1=y_train.iloc[val_index]\n\n    lgbm.fit(x_train_L1, y_train_L1)\n    y_val_proba=lgbm.predict_proba(x_val_L1)\n    x_train_L2.loc[val_index, lgbm_cul]=y_val_proba\n\nlgbm.fit(x_train, y_train)\nx_test_L2.loc[:, lgbm_cul]=lgbm.predict_proba(test)\n\n#prepare df for submission\n#submit_df=pd.DataFrame(lgbm.predict(test))\n#submit_df.columns=['Cover_Type']\n#submit_df['Id']=Id_test\n#submit_df=submit_df.loc[:, ['Id', 'Cover_Type']]\n#submit_df.to_csv('lgbm.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr_cul=['lr'+str(i+1) for i in range(7)]\n\n#prepare cols to store pred proba\nfor i in lr_cul:\n    x_train_L2.loc[:, i]=0\n    x_test_L2.loc[:, i]=0\n    \npca=PCA(n_components=40)\nx_train_pca=pd.DataFrame(pca.fit_transform(x_train))\ntest_pca=pd.DataFrame(pca.transform(test))\n\npipeline=Pipeline([('scaler', StandardScaler()), ('lr', LogisticRegression(C=10, solver='newton-cg', multi_class='multinomial',max_iter=500))])\n\n#StratifiedKfold to avoid leakage\nfor train_index, val_index in tqdm(StratifiedKFold(n_splits=10, shuffle=True, random_state=1).split(x_train_pca, y_train)):\n    x_train_L1=x_train_pca.iloc[train_index, :]\n    y_train_L1=y_train.iloc[train_index]\n    x_val_L1=x_train_pca.iloc[val_index, :]\n    y_val_L1=y_train.iloc[val_index]\n\n    pipeline.fit(x_train_L1, y_train_L1)\n    y_val_proba=pipeline.predict_proba(x_val_L1)\n    x_train_L2.loc[val_index, lr_cul]=y_val_proba\n\npipeline.fit(x_train_pca, y_train)\nx_test_L2.loc[:, lr_cul]=pipeline.predict_proba(test_pca)\n\n#prepare df for submission\n#submit_df=pd.DataFrame(pipeline.predict(test_pca))\n#submit_df.columns=['Cover_Type']\n#submit_df['Id']=Id_test\n#submit_df=submit_df.loc[:, ['Id', 'Cover_Type']]\n#submit_df.to_csv('lr.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svm_cul=['svm'+str(i+1) for i in range(7)]\n\n#prepare cols to store pred proba\nfor i in svm_cul:\n    x_train_L2.loc[:, i]=0\n    x_test_L2.loc[:, i]=0\n    \n#pca=PCA(n_components=40)\n#x_train_pca=pca.fit_transform(x_train)\n#test_pca=pca.transform(test)\n\npipeline=Pipeline([('scaler', StandardScaler()), ('svm', SVC(C=10, gamma=0.1, probability=True))])\n\n\n#StratifiedKfold to avoid leakage\nfor train_index, val_index in tqdm(StratifiedKFold(n_splits=10, shuffle=True, random_state=1).split(x_train_pca, y_train)):\n    x_train_L1=x_train_pca.iloc[train_index, :]\n    y_train_L1=y_train.iloc[train_index]\n    x_val_L1=x_train_pca.iloc[val_index, :]\n    y_val_L1=y_train.iloc[val_index]\n\n    pipeline.fit(x_train_L1, y_train_L1)\n    y_val_proba=pipeline.predict_proba(x_val_L1)\n    x_train_L2.loc[val_index, svm_cul]=y_val_proba\n\npipeline.fit(x_train_pca, y_train)\nx_test_L2.loc[:, svm_cul]=pipeline.predict_proba(test_pca)\n\n#prepare df for submission\n#submit_df=pd.DataFrame(pipeline.predict(test_pca))\n#submit_df.columns=['Cover_Type']\n#submit_df['Id']=Id_test\n#submit_df=submit_df.loc[:, ['Id', 'Cover_Type']]\n#submit_df.to_csv('svm.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_L2 = pd.concat([x_train_L2.iloc[:, 1:].reset_index(drop=True), train_level2.iloc[:, 1:].reset_index(drop=True)], axis=1)\ntest_L2 = pd.concat([x_test_L2.iloc[:, 1:].reset_index(drop=True), test_level2.iloc[:, 1:].reset_index(drop=True)], axis=1)\ntrain_L2.to_csv(\"Wtrain_L2.csv\", index=False)\ntest_L2.to_csv(\"Wtest_L2.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = pd.read_csv(\"../input/train.csv\")[\"Cover_Type\"].values\nmodel_scores = {}\ntext = []\n\nfor i in range(10):\n    y_pred = np.argmax(train_L2.iloc[:, 7*i:7*(i+1)].values, axis=1) + 1\n    score = balanced_accuracy_score(y, y_pred)\n    model_scores[cols[i*7]] = score\n    text.append(\"{}\\t{:<.5}\".format(train_L2.columns[i*7], score))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score = []\nfor train, val in tqdm(StratifiedKFold(n_splits=10, random_state=2434, shuffle=True).split(X, y)):\n    X_train = train_level2.iloc[train, 1:]\n    X_val = train_level2.iloc[val, 1:]\n    y_train = y[train]\n    y_val = y[val]\n    lr = LogisticRegression(n_jobs=1, class_weight=class_weight)\n    lr.fit(X_train, y_train)\n    y_pred = lr.predict(X_val)\n    score.append(balanced_accuracy_score(y_val, y_pred))\n    #print(score[-1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score = []\nfor train, val in tqdm(StratifiedKFold(n_splits=10, random_state=2434, shuffle=True).split(X, y)):\n    X_train = x_train_L2.iloc[train, 1:]\n    X_val = x_train_L2.iloc[val, 1:]\n    y_train = y[train]\n    y_val = y[val]\n    lr = LogisticRegression(n_jobs=1, class_weight=class_weight)\n    lr.fit(X_train, y_train)\n    y_pred = lr.predict(X_val)\n    score.append(balanced_accuracy_score(y_val, y_pred))\nprint(np.mean(score))\n\nlr = LogisticRegression(n_jobs=1, class_weight=class_weight)\nlr.fit(x_train_L2, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score = []\nfor train, val in tqdm(StratifiedKFold(n_splits=10, random_state=2434, shuffle=True).split(X, y)):\n    X_train = train_L2.iloc[train, 1:]\n    X_val = train_L2.iloc[val, 1:]\n    y_train = y[train]\n    y_val = y[val]\n    lr = LogisticRegression(n_jobs=1, class_weight=class_weight)\n    lr.fit(X_train, y_train)\n    y_pred = lr.predict(X_val)\n    score.append(balanced_accuracy_score(y_val, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wtrain = train_L2.values.astype(np.float32)\nwtest = test_L2.values.astype(np.float32)\ny = pd.read_csv(\"../input/train.csv\")[\"Cover_Type\"].values\nsmpsb = pd.read_csv(\"../input/sample_submission.csv\")\ncols = train_L2.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gbm = lgb.LGBMClassifier(n_estimators=300,\n                         num_class=8,\n                         num_leaves=25,\n                         learning_rate=5,\n                         min_child_samples=20,\n                         bagging_fraction=.3,\n                         bagging_freq=1,\n                         reg_lambda = 10**4.5,\n                         reg_alpha = 1,\n                         feature_fraction=.2,\n                         num_boost_round=4000,\n                         max_depth=-1,\n                         class_weight=class_weight_lgbm,\n                         device=\"cpu\",\n                         n_jobs=4,\n                         silent=-1,\n                         verbose=-1)\n\ngbm.fit(wtrain, y, verbose=-1)\nsmpsb[\"Cover_Type\"] = gbm.predict(wtest)\nsmpsb.to_csv(\"final_submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scores = []\ngbm = lgb.LGBMClassifier(n_estimators=300,\n                         num_class=8,\n                         num_leaves=25,\n                         learning_rate=5,\n                         min_child_samples=20,\n                         bagging_fraction=.3,\n                         bagging_freq=1,\n                         reg_lambda = 10**4.5,\n                         reg_alpha = 1,\n                         feature_fraction=.2,\n                         num_boost_round=8000,\n                         max_depth=-1,\n                         class_weight=class_weight_lgbm,\n                         device=\"cpu\",\n                         n_jobs=-1,\n                         silent=-1,\n                         verbose=-1)\n\nproba = np.zeros((wtest.shape[0], 7))\nfor train, val in tqdm(StratifiedKFold(n_splits=5, shuffle=True, random_state=2434).split(wtrain, y)):\n    X_train = wtrain[train]\n    X_val = wtrain[val]\n    y_train = y[train]\n    y_val = y[val]\n    gbm.fit(X_train, y_train, verbose=-1, \n            eval_set=[(X_train, y_train), (X_val, y_val)], early_stopping_rounds=20)\n    proba += gbm.predict_proba(wtest) / 10\n    y_pred = gbm.predict(X_val)\n    scores.append(balanced_accuracy_score(y_val, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"smpsb[\"Cover_Type\"] = np.argmax(proba, axis=1) + 1\nsmpsb.to_csv(\"final_submission_bagging.csv\", index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}