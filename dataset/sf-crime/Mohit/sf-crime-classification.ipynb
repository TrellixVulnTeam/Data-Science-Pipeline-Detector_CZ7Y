{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\npd.set_option(\"display.max_columns\", None)\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn.preprocessing import LabelEncoder, OrdinalEncoder, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.base import clone\nfrom sklearn.model_selection import StratifiedKFold, train_test_split\nfrom sklearn.metrics import log_loss\nfrom sklearn.decomposition import PCA\nfrom sklearn.mixture import GaussianMixture\nimport catboost\nimport gensim\nfrom shapely.geometry import  Point\nimport geopandas as gpd\nimport matplotlib.pyplot as plt\nfrom sklearn.impute import SimpleImputer\nimport seaborn as sns\nfrom matplotlib import cm\nimport urllib.request\nimport shutil\nimport zipfile\nimport os\nimport re\nimport contextily as ctx\nimport geoplot as gplt\nimport lightgbm as lgb\nimport eli5\nfrom eli5.sklearn import PermutationImportance\nfrom lightgbm import LGBMClassifier\nfrom pdpbox import pdp, get_dataset, info_plots\nimport shap","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"dateparse = lambda x: pd.datetime.strptime(x, '%Y-%m-%d %H:%M:%S')\n\ntrain = pd.read_csv('/kaggle/input/sf-crime/train.csv.zip',parse_dates=['Dates'], date_parser=dateparse)\ntest = pd.read_csv('/kaggle/input/sf-crime/test.csv.zip',parse_dates=['Dates'], date_parser=dateparse)\n############################\nprint('done')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train.duplicated().sum())\ntrain.drop_duplicates(inplace=True)\nassert train.duplicated().sum() ==0\n############################\nprint('done')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train.columns.difference(test.columns))\n# we need to predict the category of the crime. \n# find a way to use columns 'Descript' and 'Resolution' ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#After cleaning the dataset from outliers and duplicates, we examine the variables.\n# visualizing longitude and latitude point on the world map\n# special thanks to  :- https://www.kaggle.com/yannisp/sf-crime-analysis-prediction/output\ndef create_gdf(df):\n    gdf = df.copy()\n    gdf['Coordinates'] = list(zip(gdf.X, gdf.Y))\n    gdf.Coordinates = gdf.Coordinates.apply(Point)\n    gdf = gpd.GeoDataFrame(\n        gdf, geometry='Coordinates', crs={'init': 'epsg:4326'})\n    return gdf\n\ntrain_gdf = create_gdf(train)\n\nworld = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\nax = world.plot(color='white', edgecolor='black')\ntrain_gdf.plot(ax=ax, color='red')\nplt.show()\n\n# mapping X and Y shows that few points have erroronous longitude and latitude value and need to be corrected\n# by suitable technique","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_gdf.loc[train_gdf.Y > 50].count()[0])\n#train_gdf.loc[train_gdf.Y > 50].sample(5)\n# all mislabelled X and Y values have X = -120.5 and Y =90.0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We will replace the outlying coordinates with the average coordinates of the district they belong.\ntrain.replace({'X': -120.5, 'Y': 90.0}, np.NaN, inplace=True)\ntest.replace({'X': -120.5, 'Y': 90.0}, np.NaN, inplace=True)\n\nimp = SimpleImputer(strategy='most_frequent')\n\nfor district in train['PdDistrict'].unique():\n    train.loc[train['PdDistrict'] == district, ['X', 'Y']] = imp.fit_transform(\n        train.loc[train['PdDistrict'] == district, ['X', 'Y']])\n    test.loc[test['PdDistrict'] == district, ['X', 'Y']] = imp.transform(\n        test.loc[test['PdDistrict'] == district, ['X', 'Y']])\n\ntrain_gdf = create_gdf(train)\n\n############################\nprint('done')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot of incidence count in the day \n#Dates & Day of the week\n#These variables are distributed uniformly between 1/1/2003 to 5/13/2015 (and Monday to Sunday) and split between the training and the testing dataset as mentioned before. We did not notice any anomalies on these variables.\n#The median frequency of incidents is 389 per day with a standard deviation of 48.51.\n\ncol = sns.color_palette()\n\ntrain['Date'] = train.Dates.dt.date\ntrain['Hour'] = train.Dates.dt.hour\n\nplt.figure(figsize=(10, 6))\ndata = train.groupby('Date').count().iloc[:, 0]\nsns.kdeplot(data=data, shade=True)\nplt.axvline(x=data.median(), ymax=0.95, linestyle='--', color=col[1])\nplt.annotate(\n    'Median: ' + str(data.median()),\n    xy=(data.median(), 0.004),\n    xytext=(200, 0.005),\n    arrowprops=dict(arrowstyle='->', color=col[1], shrinkB=10))\nplt.title(\n    'Distribution of number of incidents per day', fontdict={'fontsize': 16})\nplt.xlabel('Incidents')\nplt.ylabel('Density')\nplt.legend().remove()\nplt.show()\n############################\nprint('done')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# number of incidents by weekdays\n# special thanks to :- https://www.kaggle.com/yannisp/sf-crime-analysis-prediction/output\n\ndata = train.groupby('DayOfWeek').count().iloc[:, 0]\n\ndata = data.reindex([\n    'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday',\n    'Sunday'\n])\n\nplt.figure(figsize=(10, 5))\nwith sns.axes_style(\"whitegrid\"):\n    ax = sns.barplot(\n        data.index, (data.values / data.values.sum()) * 100,\n        orient='v',\n        palette=cm.ScalarMappable(cmap='Reds').to_rgba(data.values))\n\nplt.title('Incidents per Weekday', fontdict={'fontsize': 16})\nplt.xlabel('Weekday')\nplt.ylabel('Incidents (%)')\n\n############################\nprint('done')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 'Incidents per Crime Category'\n# special thanks to :- https://www.kaggle.com/yannisp/sf-crime-analysis-prediction/output\n\ndata = train.groupby('Category').count().iloc[:, 0].sort_values(\n    ascending=False)\ndata = data.reindex(np.append(np.delete(data.index, 1), 'OTHER OFFENSES'))\n\nplt.figure(figsize=(10, 10))\nwith sns.axes_style(\"whitegrid\"):\n    ax = sns.barplot(\n        (data.values / data.values.sum()) * 100,\n        data.index,\n        orient='h',\n        palette=\"Reds_r\")\n\nplt.title('Incidents per Crime Category', fontdict={'fontsize': 16})\nplt.xlabel('Incidents (%)')\n\nplt.show()\n\n############################\nprint('done')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 'Average number of incidents per hour'\n# special thanks to :- https://www.kaggle.com/yannisp/sf-crime-analysis-prediction/output\n\ndata = train.groupby(['Hour', 'Date', 'Category'],\n                     as_index=False).count().iloc[:, :4]\ndata.rename(columns={'Dates': 'Incidents'}, inplace=True)\ndata = data.groupby(['Hour', 'Category'], as_index=False).mean()\ndata = data.loc[data['Category'].isin(\n    ['ROBBERY', 'GAMBLING', 'BURGLARY', 'ARSON', 'PROSTITUTION'])]\n\nsns.set_style(\"whitegrid\")\nfig, ax = plt.subplots(figsize=(14, 4))\nax = sns.lineplot(x='Hour', y='Incidents', data=data, hue='Category')\nax.legend(loc='upper center', bbox_to_anchor=(0.5, 1.15), ncol=6)\nplt.suptitle('Average number of incidents per hour')\nfig.tight_layout(rect=[0, 0.03, 1, 0.95])\nplt.show()\n\n############################\nprint('done')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dateparse = lambda x: pd.datetime.strptime(x, '%Y-%m-%d %H:%M:%S')\n\ntrain = pd.read_csv('/kaggle/input/sf-crime/train.csv.zip',parse_dates=['Dates'], date_parser=dateparse)\ntest = pd.read_csv('/kaggle/input/sf-crime/test.csv.zip',parse_dates=['Dates'], date_parser=dateparse)\n############################\nprint('done')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We will replace the outlying coordinates with the average coordinates of the district they belong.\ntrain.replace({'X': -120.5, 'Y': 90.0}, np.NaN, inplace=True)\ntest.replace({'X': -120.5, 'Y': 90.0}, np.NaN, inplace=True)\n\nimp = SimpleImputer(strategy='most_frequent')\n\nfor district in train['PdDistrict'].unique():\n    train.loc[train['PdDistrict'] == district, ['X', 'Y']] = imp.fit_transform(\n        train.loc[train['PdDistrict'] == district, ['X', 'Y']])\n    test.loc[test['PdDistrict'] == district, ['X', 'Y']] = imp.transform(\n        test.loc[test['PdDistrict'] == district, ['X', 'Y']])\n############################\nprint('done')\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train = train['Category']\ntrain_des = train['Descript']\ntrain_res = train['Resolution']\ntrain.drop([\"Category\", \"Descript\", \"Resolution\"], axis=1, inplace=True)\n\n############################\nprint('done')\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_ID = test[\"Id\"]\ntest.drop(\"Id\", axis=1, inplace=True)\n\n############################\nprint('done')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"le = LabelEncoder()\ny_train = le.fit_transform(y_train)\nprint(le.classes_)\n\n############################\nprint('done')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train.drop(['Address','clean_text'],axis=1,inplace = True)\n#test.drop(['Address','clean_text'],axis=1,inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_train = train.shape[0]\nall_data = pd.concat((train, test), ignore_index=True)\n\n############################\nprint('done')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# related to data and address\n\ndef feature_engineering(data):\n    date = pd.to_datetime(all_data['Dates'])\n    data['year'] = date.dt.year\n    data['month'] = date.dt.month\n    data['day'] = date.dt.day\n    data['hour'] = date.dt.hour\n    data['minute'] = date.dt.minute\n    data['special_time'] = data['minute'].isin([0, 30]).astype(int)\n  # all_data['second'] = date.dt.second  # all zero\n    data[\"n_days\"] = (date - date.min()).apply(lambda x: x.days)\n    data.drop(\"Dates\", axis=1, inplace=True)\n    data['block'] = data[\"Address\"].str.contains(\"block\", case=False)\n    data['ST'] = data['Address'].str.contains('ST', case=False)\n    #data.drop(['Address','clean_text'],axis=1,inplace=True)\n    return data\n\nall_data1 = feature_engineering(all_data)\n\n############################\nprint('done')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# related to \"X\" and \"Y\"\n\ndef feature_engineering2(data):\n    data[\"X+Y\"] = data[\"X\"] + data[\"Y\"]\n    data[\"X-Y\"] = data[\"X\"] - data[\"Y\"]\n    data[\"XY30_1\"] = data[\"X\"] * np.cos(np.pi / 6) + data[\"Y\"] * np.sin(np.pi / 6)\n    data[\"XY30_2\"] = data[\"Y\"] * np.cos(np.pi / 6) - data[\"X\"] * np.sin(np.pi / 6)\n    data[\"XY60_1\"] = data[\"X\"] * np.cos(np.pi / 3) + data[\"Y\"] * np.sin(np.pi / 3)\n    data[\"XY60_2\"] = data[\"Y\"] * np.cos(np.pi / 3) - data[\"X\"] * np.sin(np.pi / 3)\n    data[\"XY1\"] = (data[\"X\"] - data[\"X\"].min()) ** 2 + (data[\"Y\"] - data[\"Y\"].min()) ** 2\n    data[\"XY2\"] = (data[\"X\"].max() - data[\"X\"]) ** 2 + (data[\"Y\"] - data[\"Y\"].min()) ** 2\n    data[\"XY3\"] = (data[\"X\"] - data[\"X\"].min()) ** 2 + (data[\"Y\"].max() - data[\"Y\"]) ** 2\n    data[\"XY4\"] = (data[\"X\"].max() - data[\"X\"]) ** 2 + (data[\"Y\"].max() - data[\"Y\"]) ** 2\n    #data[\"XY5\"] = (data[\"X\"] - X_median) ** 2 + (data[\"Y\"] - Y_median) ** 2\n    pca = PCA(n_components=2).fit(data[[\"X\", \"Y\"]])\n    XYt = pca.transform(data[[\"X\", \"Y\"]])\n    data[\"XYpca1\"] = XYt[:, 0]\n    data[\"XYpca2\"] = XYt[:, 1]\n    #return data\n    # n_components selected by aic/bic\n    clf = GaussianMixture(n_components=150, covariance_type=\"diag\",random_state=0).fit(data[[\"X\", \"Y\"]])\n    data[\"XYcluster\"] = clf.predict(data[[\"X\", \"Y\"]])\n    return data\n\nall_data2 = feature_engineering2(all_data1)\n\n############################\nprint('done')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# cleaning \"text\" column of train and test dataset and saving it to column 'clean_txt'\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nvectorizer = TfidfVectorizer(lowercase=True, preprocessor=None,tokenizer=lambda x : x.split(), analyzer='word', stop_words=None, token_pattern='(?u)\\b\\w\\w+\\b', ngram_range=(1, 3), max_df=0.75, min_df=5, max_features=12500)\n\ntrain_vect = vectorizer.fit_transform(train['Address'])\ntest_vect = vectorizer.transform(test['Address'])\n\n##################\nprint('done')\n\n#print(vectorizer.get_feature_names())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sentences = []\nfor s in all_data2[\"Address\"]:\n    sentences.append(s.split(\" \"))\naddress_model = gensim.models.Word2Vec(sentences, min_count=1)\nencoded_address = np.zeros((all_data2.shape[0], 100))\nfor i in range(len(sentences)):\n    for j in range(len(sentences[i])):\n        encoded_address[i] += address_model.wv[sentences[i][j]]\n    encoded_address[i] /= len(sentences[i])\n\nprint('done')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data2.drop(['Address'],axis=1,inplace=True)\n\ncategorical_features = [\"DayOfWeek\", \"PdDistrict\", \"block\", \"special_time\", \"XYcluster\",'ST']\nct = ColumnTransformer(transformers=[(\"categorical_features\", OrdinalEncoder(), categorical_features)],\n                       remainder=\"passthrough\")\nall_data3 = ct.fit_transform(all_data2)\n\n############################\nprint('done')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train_data =sp.sparse.hstack((X_train, train_vec))\n#test_data = np.hstack((X_test, test_vec))\n#train_data = np.hstack(vectorizer.fit_transform(train.clean_text),train2)\n#test_data = sp.sparse.hstack((vectorizer.transform(test.clean_text),test2),format='csr')\nall_data4 = np.hstack((all_data3,encoded_address))\n############################\nprint('done')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train2 = all_data3[:num_train]\ntest2 = all_data3[num_train:]\n\n############################\nprint('done')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy import sparse\n#sA = sparse.csr_matrix(train2)\ntrain_2 = sparse.hstack((train2.astype(float),train_vect))\ntest_2 = sparse.hstack((test2.astype(float),test_vect))\n\n#test_2 = pd.concat((test2,test_vect))\n\n############################\nprint('done')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from lightgbm import LGBMClassifier\nmodel = LGBMClassifier(objective=\"multiclass\", num_class=39, max_bin = 465, max_delta_step = 0.9,\n                      learning_rate=0.4, num_leaves = 42, n_estimators=100, verbose=50)\nmodel.fit(train2, y_train)\npreds = model.predict_proba(test2)\nsubmission = pd.DataFrame(preds, columns=le.inverse_transform(np.linspace(0, 38, 39, dtype='int16')), index=test.index)\nsubmission.to_csv('LGBM_final1.csv', index_label='Id')\n############################\nprint('done')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from lightgbm import LGBMClassifier\nmodel2 = LGBMClassifier(objective=\"multiclass\", num_class=39, max_bin = 465, max_delta_step = 0.9,\n                      learning_rate=0.4, num_leaves = 42, n_estimators=100, verbose=50)\nmodel2.fit(train_2, y_train)\npreds2 = model2.predict_proba(test_2)\nsubmission2 = pd.DataFrame(preds2, columns=le.inverse_transform(np.linspace(0, 38, 39, dtype='int16')), index=test.index)\nsubmission2.to_csv('LGBM_final2.csv', index_label='Id')\n############################\nprint('done')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_tot = (preds+preds2+prob)/3\nsubmission4 = pd.DataFrame(sub_tot, columns=le.inverse_transform(np.linspace(0, 38, 39, dtype='int16')), index=test.index)\nsubmission4.to_csv('final.csv', index_label='Id')\n############################\nprint('done')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}