{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\n\ntrain_data = pd.read_csv(\"../input/sf-crime/train.csv.zip\")\ntest_data = pd.read_csv(\"../input/sf-crime/test.csv.zip\")\n\ntrain_data.shape\ntest_data.shape\ntest_ids = test_data['Id']\ntrain_data.head(10)\ntest_data.head(10)\n\ntrain_data.info()\ntrain_data.isnull().sum()\n\n\n\n#Removing Unwanted Locations\ntrain_data.drop(train_data[train_data['Y']>40].index,inplace = True)\n\n\n#Dates Cleaning\n#Dates is string type\n\ndef DataCleaning(df):\n    #Duplicates\n    df.duplicated().sum()\n    df.drop_duplicates(inplace = True)\n    df['Dates'] = pd.to_datetime(df['Dates'])\n    df[\"Year\"] = df.Dates.dt.year\n    df['Month'] = df.Dates.dt.month\n    df['Hour'] = df.Dates.dt.hour\n    \n    df['DayOfWeekint'] =df['DayOfWeek'].map({\n        'Monday': 1,\n        'Tuesday': 2,\n        'Wednesday': 3,\n        'Thursday': 4,\n        'Friday': 5,\n        'Saturday': 6,\n        'Sunday': 7      })    \n    \n       \n    return df\n    \nclean_train = DataCleaning(train_data)\nclean_test = DataCleaning(test_data)    \n\n\ngb = train_data.groupby(\"Year\")['Descript'].count()\ngb.plot(xticks = gb.index.values)\nplt.ylabel(\"Count of crimes\")\n\nfor y in train_data['Year'].unique():\n    train_data[train_data['Year']==y].groupby('DayOfWeek')['Descript'].count().sort_values().plot(kind = 'barh',title = 'Count Of Crimes Per Day')\n    plt.show()\nfor y in train_data['Year'].unique():\n    train_data[train_data['Year']==y].groupby('Month')['Descript'].count().sort_values().plot(kind = 'barh',title = 'Count Of Crimes in {}'.format(y))\n    plt.show()\n\ntrain_data['IsDay'] = train_data.Hour.apply(lambda x : 1 if x>7 & x<20 else 0)\n\ntest_data['IsDay'] = test_data.Hour.apply(lambda x : 1 if x>7 & x<20 else 0)\n\nfor y in train_data['Year'].unique():\n    train_data[train_data['Year']==y].groupby('IsDay')['Descript'].count().sort_values().plot(kind = 'bar',title = 'Count Of Crimes D/N {}'.format(y))\n    plt.show()\n\nplt.figure(figsize = (20,20))\ntrain_data['Category'].value_counts().sort_values().plot(kind = 'barh')\nplt.xlabel(\"Count\")\nplt.ylabel(\"Categories\")\nplt.show()\n\nplt.figure(figsize = (20,20))\ntrain_data['PdDistrict'].value_counts().sort_values().plot(kind = 'barh')\nplt.xlabel(\"Count\")\nplt.ylabel(\"PdDistrict\")\nplt.show()\n\nct = pd.crosstab(train_data.Category, train_data.IsDay)\nct.plot(kind='barh', figsize=(14,14), title='Crime Categories by Day/Night (2003 ~ mid 2015)')","metadata":{"execution":{"iopub.status.busy":"2021-08-08T15:03:20.710787Z","iopub.execute_input":"2021-08-08T15:03:20.711206Z","iopub.status.idle":"2021-08-08T15:03:44.998332Z","shell.execute_reply.started":"2021-08-08T15:03:20.711169Z","shell.execute_reply":"2021-08-08T15:03:44.996476Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clean_train.drop(['Resolution','Descript'],axis = 'columns',inplace = True) \nclean_train.drop(['DayOfWeek'],axis = 'columns',inplace = True) \nclean_test.drop(['DayOfWeek','Id'],axis = 'columns',inplace = True) \n\n\nclean_train['Block'] = clean_train['Address'].str.contains('block', case=False)\nclean_train['IsBlock'] = clean_train.Block.apply(lambda x : 1 if x ==True else 0)\nclean_train.drop(['Address','Block'],axis = 'columns',inplace = True) \n\nclean_test['Block'] = clean_test['Address'].str.contains('block', case=False)\nclean_test['IsBlock'] = clean_test.Block.apply(lambda x : 1 if x ==True else 0)\nclean_test.drop(['Address','Block'],axis = 'columns',inplace = True) \n\n#PreProcessing\ncategorical_features = [feature for feature in clean_train.columns if clean_train[feature].dtypes == 'O' and feature not in ['Category']]\nlen(categorical_features)\nfor features in categorical_features:\n    dummies = pd.get_dummies(clean_train[features])\n    merged = pd.concat([clean_train,dummies],axis = 'columns')\n    clean_train_pro = merged.copy()\n\n\n\ncategorical_features_test = [feature for feature in clean_test.columns if clean_test[feature].dtypes == 'O']\n     \nfor features in categorical_features_test:\n    dummies = pd.get_dummies(clean_test[features])\n    merged = pd.concat([clean_test,dummies],axis = 'columns')\n    clean_test_pro = merged.copy()\n\nfrom sklearn.preprocessing import LabelEncoder\ncat_le = LabelEncoder()\nclean_train_pro['CategoryInt'] = (cat_le.fit_transform(clean_train_pro.Category))\n\nclean_train_pro.drop(['PdDistrict','Category','Dates'],axis = 'columns',inplace = True) \nclean_test_pro.drop(['PdDistrict','Dates'],axis = 'columns',inplace = True) \n\nX_train = clean_train_pro.drop('CategoryInt',axis = 'columns')\ny_train = clean_train_pro['CategoryInt']\n\nX_test = clean_test_pro\n\n","metadata":{"execution":{"iopub.status.busy":"2021-08-08T15:03:55.795435Z","iopub.execute_input":"2021-08-08T15:03:55.795906Z","iopub.status.idle":"2021-08-08T15:04:00.116985Z","shell.execute_reply.started":"2021-08-08T15:03:55.795868Z","shell.execute_reply":"2021-08-08T15:04:00.115573Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import xgboost as xgb\ntrain_xgb = xgb.DMatrix(X_train, label=y_train)\ntest_xgb  = xgb.DMatrix(X_test)","metadata":{"execution":{"iopub.status.busy":"2021-08-08T15:04:07.435886Z","iopub.execute_input":"2021-08-08T15:04:07.436305Z","iopub.status.idle":"2021-08-08T15:04:08.097048Z","shell.execute_reply.started":"2021-08-08T15:04:07.436271Z","shell.execute_reply":"2021-08-08T15:04:08.095896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nparams = {\n    'max_depth': 4,  # the maximum depth of each tree\n    'eta': 0.3,  # the training step for each iteration\n    'silent': 1,  # logging mode - quiet\n    'objective': 'multi:softprob',  # error evaluation for multiclass training\n    'num_class': 39,\n}","metadata":{"execution":{"iopub.status.busy":"2021-08-08T15:04:10.37934Z","iopub.execute_input":"2021-08-08T15:04:10.379774Z","iopub.status.idle":"2021-08-08T15:04:10.385974Z","shell.execute_reply.started":"2021-08-08T15:04:10.379735Z","shell.execute_reply":"2021-08-08T15:04:10.384283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CROSS_VAL = False\nif CROSS_VAL:\n    print('Doing Cross-validation ...')\n    cv = xgb.cv(params, train_xgb, nfold=3, early_stopping_rounds=10, metrics='mlogloss', verbose_eval=True)\n    cv\n","metadata":{"execution":{"iopub.status.busy":"2021-08-08T15:04:12.305673Z","iopub.execute_input":"2021-08-08T15:04:12.306267Z","iopub.status.idle":"2021-08-08T15:04:12.312317Z","shell.execute_reply.started":"2021-08-08T15:04:12.30623Z","shell.execute_reply":"2021-08-08T15:04:12.3109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data.info()","metadata":{"execution":{"iopub.status.busy":"2021-08-08T15:02:52.692005Z","iopub.execute_input":"2021-08-08T15:02:52.69259Z","iopub.status.idle":"2021-08-08T15:02:52.827678Z","shell.execute_reply.started":"2021-08-08T15:02:52.692538Z","shell.execute_reply":"2021-08-08T15:02:52.82579Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nSUBMIT = not CROSS_VAL\nif SUBMIT:\n    print('Fitting Model ...')\n    m = xgb.train(params, train_xgb, 10)\n    res = m.predict(test_xgb)\n    cols = ['Id'] + cat_le.classes_\n    submission = pd.DataFrame(res, columns=cat_le.classes_)\n    submission.insert(0, 'Id', test_ids)\n    submission.to_csv('submission.csv', index=False)\n    print('Done Outputing !')\n    print(submission.sample(3))\nelse:\n    print('NOT SUBMITING')","metadata":{"execution":{"iopub.status.busy":"2021-08-08T15:04:22.227698Z","iopub.execute_input":"2021-08-08T15:04:22.228131Z","iopub.status.idle":"2021-08-08T15:07:29.901362Z","shell.execute_reply.started":"2021-08-08T15:04:22.22809Z","shell.execute_reply":"2021-08-08T15:07:29.899798Z"},"trusted":true},"execution_count":null,"outputs":[]}]}