{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/sf-crime/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/sf-crime/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Intro"},{"metadata":{},"cell_type":"markdown","source":"I'm just creating a quick notebook for this competition to try to keep my skills sharp. I've been using mostly SQL and Tableau at work lately, and I didn't want my python and data science skills to get too rusty. I'm not spending much time improving my score or going into much detail.  I'm just doing some very basic EDA, Feature Engineering, and getting a basic, functioning model up and running."},{"metadata":{},"cell_type":"markdown","source":"## Exploratory Data Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Columns in test not in train"},{"metadata":{"trusted":true},"cell_type":"code","source":"np.setdiff1d(test.columns,train.columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Columns in train not in test"},{"metadata":{"trusted":true},"cell_type":"code","source":"np.setdiff1d(train.columns,test.columns).tolist()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"No Null Values"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Unique Value Counts"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.nunique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### A closer look at a few select counts"},{"metadata":{"trusted":true},"cell_type":"code","source":"train['Resolution'].value_counts().plot.barh();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Resolution excluding NONE"},{"metadata":{"trusted":true},"cell_type":"code","source":"train[train['Resolution']!='NONE']['Resolution'].value_counts().plot.barh();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['PdDistrict'].value_counts().plot.barh();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['DayOfWeek'].value_counts().plot.barh();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['Category'].value_counts().plot.barh(figsize = (5,18));","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Engineering"},{"metadata":{},"cell_type":"raw","source":"dropping \"Descript\" and \"Resolution\" from train data since those features aren't in the test data."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_feats = train.drop(labels = ['Descript','Resolution'],axis = 1)\ntrain_feats.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Will create dummy variables for PdDistrict and DayOfWeek"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dummies = pd.get_dummies(train_feats[['PdDistrict','DayOfWeek']])\ntrain_feats_dummies = pd.merge(train_feats.drop(['PdDistrict','DayOfWeek'],1),train_dummies,left_index = True, right_index = True)\ntrain_feats_dummies.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_dummies = pd.get_dummies(test[['PdDistrict','DayOfWeek']])\ntest_feats_dummies = pd.merge(test.drop(['PdDistrict','DayOfWeek'],1),test_dummies,left_index = True, right_index = True)\ntest_feats_dummies = test_feats_dummies.drop(['Dates','Address'],1)\ntest_feats_dummies.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = train_feats_dummies.drop('Category',1)\ny = train_feats_dummies[['Category']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Dropping \"Dates\" and \"Address\" for now...\nI might come up with something to do for those but not sure..."},{"metadata":{"trusted":true},"cell_type":"code","source":"X = X.drop(['Dates','Address'],1)\nX.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y.head(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Set up to predict probability for all categories, so get dummies"},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.get_dummies(y,prefix = None,prefix_sep = '')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train/Test Split"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Modeling"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.naive_bayes import GaussianNB","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"y_train.Category.tolist()\npd.get_dummies(y_train)\nenc = OneHotEncoder(handle_unknown='ignore')\nenc.fit(y_train.Category)\nX_train.pop('X')\nle = LabelEncoder()\nle.fit_transform(y_train)\nenc.transform(y_train).toarray()"},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"#clf = GaussianNB().fit(X_train, y_train['Category']) #worked (maybe)\n#clf = LogisticRegression(random_state=0, solver='lbfgs',multi_class='multinomial').fit(X_train, y_train['Category']) #worked\nclf = LogisticRegression(random_state=0, solver='lbfgs',multi_class='ovr').fit(X_train, y_train['Category']) #worked","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looking for prediction probabilities for each category.\nCan see below that there are unique coefficients for each of the 39 categories."},{"metadata":{"trusted":true},"cell_type":"code","source":"clf.coef_.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf.classes_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model Evaluation"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report\nfrom sklearn.metrics import accuracy_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = clf.predict(X_test)\n\nprint(np.unique(y_pred,return_counts = True))\n#print(classification_report(y_test,y_pred))\nprint(accuracy_score(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_feats_dummies.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_predictions = clf.predict(test_feats_dummies.drop('Id',1))\ntest_predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_probability_predictions = clf.predict_proba(test_feats_dummies.drop('Id',1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_probability_predictions.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf.classes_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.merge(test['Id'],pd.DataFrame(data = test_probability_predictions,columns = clf.classes_), \\\n         left_index = True, right_index = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv('submission.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Closing Thoughts"},{"metadata":{},"cell_type":"markdown","source":"I could add a lot more to make this better. I really didn't try many models. I didn't even train on the full train data, which I should have done after deciding which model to use. I also would have found the 50 or so most common addresses and created dummy variables from them and added them as features. I expect this would have been a significant added value."},{"metadata":{},"cell_type":"markdown","source":"I'm just leaving this as is, since my objective was just to keep my skills sharp. I've been using mostly SQL and Tableau these days, so I didn't want my python and data science skills to get too rusty."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}