{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"!pip install pyspark\n!unzip ../input/sf-crime/train.csv.zip \n#ls /kaggle/input/sf-crime\n#!ls","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from pyspark.sql import SparkSession\nspark = SparkSession.builder.getOrCreate()\nspark","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = spark.read.csv(\"train.csv\",header=True, inferSchema=True)\ndf.printSchema()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from pyspark.sql.functions import *\nfrom pyspark.sql.types import DoubleType\ndf = df.dropDuplicates()\n\ndf = df.withColumn('Year',year(\"Dates\"))\ndf = df.withColumn('Month',month(\"Dates\"))\ndf = df.withColumn('Day',dayofmonth(\"Dates\"))\ndf = df.withColumn('Hour',hour(\"Dates\"))\n\ndf = df.withColumn('X',df[\"X\"].cast(DoubleType()))\ndf = df.withColumn('Y',df[\"Y\"].cast(DoubleType()))\ndf = df.withColumn('X',df[\"X\"].cast(DoubleType()))\ndf = df.withColumn('Y',df[\"Y\"].cast(DoubleType()))\n\nAvgX1 = df.groupBy(\"PdDistrict\").agg({\"X\": \"avg\"})\\\n.withColumnRenamed(\"avg(X)\", \"X_avg\")\n\nAvgY1 = df.groupBy(\"PdDistrict\").agg({\"Y\": \"avg\"})\\\n.withColumnRenamed(\"avg(Y)\", \"Y_avg\")\n\n\ndf_avg_x = df.join(AvgX1,on =\"PdDistrict\")\ndf = df_avg_x.join(AvgY1,on =\"PdDistrict\")\n\ndf = df.withColumn(\"Y11\", when(col(\"Y\") > 50, col(\"Y_avg\")).otherwise(col(\"Y\")))\ndf = df.withColumn(\"X11\", when(col(\"Y\") > 50, col(\"X_avg\")).otherwise(col(\"X\")))\n\ndf = df.withColumn('SPOT', when(df.Address.like(\"%Block%\") , lit(0)).otherwise(lit(1)))\n\ndf = df.drop(\"Dates\",\"Address\",\"Descript\",\"Resolution\")\ndf.printSchema()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from pyspark.sql.functions import *\nfrom pyspark.ml.classification import  RandomForestClassifier\nfrom pyspark.ml.feature import StringIndexer, OneHotEncoderEstimator, VectorAssembler, VectorSlicer\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\nfrom pyspark.ml.linalg import Vectors\nfrom pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\nfrom pyspark.ml.feature import StringIndexer,OneHotEncoder, OneHotEncoderEstimator, VectorAssembler, VectorSlicer\n#gathering the string fields in one object execluding the target column\nencoding_var = [i[0] for i in df.dtypes if (i[1]=='string') & (i[0]!='Category')]\nencoding_var\n\n#gathering the string integer in one object execluding the target column\nnum_var = [i[0] for i in df.dtypes if ((i[1]=='int') | (i[1]=='double')) & (i[0]!='Category')]\nnum_var\n\nstring_indexes = [StringIndexer(inputCol = c, outputCol = 'IDX_' + c, handleInvalid = 'skip') for c in encoding_var]\nstring_indexes\n\nonehot_indexes = [OneHotEncoderEstimator(inputCols = ['IDX_' + c], outputCols = ['OHE_' + c]) for c in encoding_var]\n\nlabel_indexes = StringIndexer(inputCol = 'Category', outputCol = 'label', handleInvalid = 'skip')\n\nlabel_hotcodes =  OneHotEncoder().setInputCol(\"label\").setOutputCol(\"categoryOHE\")\n\nassembler = VectorAssembler(inputCols = num_var + ['OHE_' + c for c in encoding_var], outputCol = \"features\")\n\n## Defining two pipelines so as to be able to transform the test dataset using pipe1 only\npipe1 = Pipeline(stages = string_indexes + onehot_indexes + [assembler])\ndf_transformer1 = pipe1.fit(df)\ndf = df_transformer1.transform(df)\n\n## pip2 will be used for indexing labels only in training phase\npipe2 = Pipeline(stages = [label_indexes] + [label_hotcodes])\ndf_transformer2 = pipe2.fit(df)   ### This transformer will be used to access the indexed labels when submitting to kaggle\ndf_train = df_transformer2.transform(df)\ndf_train.printSchema()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from pyspark.ml.classification import  RandomForestClassifier\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\n\ndf_train = df_train.select('features','label')         #creating final data with only 2 columns\n#train,test=final_data.randomSplit([0.5,0.5])  \n\nrf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", seed = 8464,\\\n                            numTrees=8, cacheNodeIds = False, subsamplingRate = 0.7,maxDepth=10, maxBins=30 )\nrfModel = rf.fit(df_train)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!unzip ../input/sf-crime/test.csv.zip \n# Import test data and do same steps of adding feature columns\ntest_df = spark.read.csv(\"test.csv\",header=True, inferSchema=True)\n\n#df_train_processed = pipe1.fit(df).transform(df)\n\ntest_df = test_df.withColumn('Year',year(\"Dates\"))\ntest_df = test_df.withColumn('Month',month(\"Dates\"))\ntest_df = test_df.withColumn('Day',dayofmonth(\"Dates\"))\ntest_df = test_df.withColumn('Hour',hour(\"Dates\"))\n\ntest_df = test_df.withColumn('X',test_df[\"X\"].cast(DoubleType()))\ntest_df = test_df.withColumn('Y',test_df[\"Y\"].cast(DoubleType()))\n\ndf_avg_x = test_df.join(AvgX1,on =\"PdDistrict\")\ntest_df = df_avg_x.join(AvgY1,on =\"PdDistrict\")\n\n\ntest_df = test_df.withColumn(\"Y11\", when(col(\"Y\") > 50, col(\"Y_avg\")).otherwise(col(\"Y\")))\ntest_df = test_df.withColumn(\"X11\", when(col(\"Y\") > 50, col(\"X_avg\")).otherwise(col(\"X\")))\n\n\ntest_df = test_df.withColumn('SPOT', when(test_df.Address.like(\"%Block%\") , lit(0)).otherwise(lit(1)))\n#test_df = test_df.drop('X','Y','X_avg','Y_avg')\ntest_df = test_df.drop(\"Dates\",\"Address\")\n\ntest_df.printSchema()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test_processed = df_transformer1.transform(test_df)\ndf_test_processed.printSchema()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_predictions = rfModel.transform(df_test_processed).select(\"id\", \"probability\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"label_list = df_transformer2.stages[0].labels\n\nfrom pyspark.sql import types as T\n\n#Build a function to convert predictions from DenseVector to Array\ndef dense_to_array(dv):\n    dvArray = list([float(i) for i in dv])\n    return dvArray\n#Create corresponding UDF\ndense_to_array_udf = udf(dense_to_array, T.ArrayType(T.FloatType()))\n\n#Use the UDF\nresults = final_predictions.withColumn('probability', dense_to_array_udf('probability'))\n\n#Build the columns with target category names and corresponding probabilities\nfor i in range(39):\n    results = results.withColumn(label_list[i], results.probability[i])\n\nresults.printSchema()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results.drop(\"probability\").toPandas().to_csv('submission.csv',index=False,header=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submit_df = spark.read.csv(\"submission.csv\",header=True, inferSchema=True)\nsubmit_df.count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submit_df.printSchema()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#kaggle competitions submit -c sf-crime -f ../output/submission.csv -m \"Spark_Submission_Kaggle_Kernel\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}