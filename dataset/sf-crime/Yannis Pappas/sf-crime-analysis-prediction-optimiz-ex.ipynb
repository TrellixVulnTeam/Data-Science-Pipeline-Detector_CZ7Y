{"cells":[{"metadata":{"_uuid":"97b42e5e6139c6b29e16a30e2a541fe22c5daf2d"},"cell_type":"markdown","source":"Note: This Notebook is an example of the Bayesian Optimization that took place during the hyperparameters of the model, and part of the [SF-Crime Analysis & Prediction](https://www.kaggle.com/yannisp/sf-crime-analysis-prediction). Please start there for a full analysis."},{"metadata":{"_uuid":"cc2ac21e0918ce51c1c4c1ca70e9d2b185cea752"},"cell_type":"markdown","source":"# Importing libraries"},{"metadata":{"trusted":true,"_uuid":"7bce80874a0f99b8d210a4c6a2fdee7037258632"},"cell_type":"code","source":"from tqdm import tqdm\nimport pandas as pd\nimport numpy as np\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import LabelEncoder\nfrom hyperopt import hp, Trials, fmin, tpe, STATUS_OK\nimport lightgbm as lgb\nimport ast\nimport csv\nimport pickle\n# Optional if you want to run it locally and inspect it in real time using Tensorboard\n#from tensorboardX import SummaryWriter \n\nout_file = 'LGB.csv'\nMAX_EVALS = 5 #This has been set to a small number for demonstration. Increase it!\nN_FOLDS = 5\npbar = tqdm(total=MAX_EVALS, desc=\"Hyperopt\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2975d1a9ad8e591d50481f34b3da54feddb20ff2"},"cell_type":"markdown","source":"# Importing and preprocessessing data"},{"metadata":{"ExecuteTime":{"end_time":"2018-12-28T16:45:27.471547Z","start_time":"2018-12-28T16:45:27.464707Z"},"trusted":true,"_uuid":"ad481b6c072e20df86a6257281d0fe912e74fcd4"},"cell_type":"code","source":"# Loading the data\ntrain = pd.read_csv('../input/train.csv', parse_dates=['Dates'])\n\n# Wrangling the dataset\ntrain.drop_duplicates(inplace=True)\ntrain.replace({'X': -120.5, 'Y': 90.0}, np.NaN, inplace=True)\n\nimp = SimpleImputer(strategy='mean')\n\nfor district in train['PdDistrict'].unique():\n    train.loc[train['PdDistrict'] == district, ['X', 'Y']] = imp.fit_transform(\n        train.loc[train['PdDistrict'] == district, ['X', 'Y']])\n\n# Feature Engineering\ndef feature_engineering(data):\n    data['Date'] = pd.to_datetime(data['Dates'].dt.date)\n    data['n_days'] = (\n        data['Date'] - data['Date'].min()).apply(lambda x: x.days)\n    data['Day'] = data['Dates'].dt.day\n    data['DayOfWeek'] = data['Dates'].dt.weekday\n    data['Month'] = data['Dates'].dt.month\n    data['Year'] = data['Dates'].dt.year\n    data['Hour'] = data['Dates'].dt.hour\n    data['Minute'] = data['Dates'].dt.minute\n    data['Block'] = data['Address'].str.contains('block', case=False)\n    \n    data.drop(columns=['Dates','Date','Address'], inplace=True)\n        \n    return data\n\ntrain = feature_engineering(train)\ntrain.drop(columns=['Descript','Resolution'], inplace=True)\n\n# Encoding Categorical Variables\nle1 = LabelEncoder()\ntrain['PdDistrict'] = le1.fit_transform(train['PdDistrict'])\n\nle2 = LabelEncoder()\ny = le2.fit_transform(train.pop('Category'))\n\n# Forming the dataset\ntrain_set = lgb.Dataset(\n    train, label=y, categorical_feature=['PdDistrict'], free_raw_data=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a00ea76f728698956080c62771f2ba0fd14def69"},"cell_type":"markdown","source":"# Helper functions\n### (Optional) Function to write Tensorboard logs so that you can use Tensorboard to monitor the process\n```python\n# This is a Markdown cell it will not run\n# Convert it to a code cell if you download the Notebook and want to use Tensorboard\ndef logspy(env):\n    \"\"\" Function that writes logs that can be read by Tensorboard.\n    It has no use if you run this kernel on Kaggle\"\"\"\n    global t_writer\n    \n    if env.iteration == 0:\n        t_writer = SummaryWriter('../logs/LGB'+str(ITERATION))\n        \n    t_writer.add_scalar('train', env.evaluation_result_list[0][1], env.iteration)\n    t_writer.add_scalar('val', env.evaluation_result_list[0][2], env.iteration)\n    \n    return\n```"},{"metadata":{"_uuid":"e7ce610ba921ae766b15f184d13009aa6dcb6189"},"cell_type":"markdown","source":"### Function that flattens nested dictionaries"},{"metadata":{"ExecuteTime":{"end_time":"2018-12-27T20:46:50.728553Z","start_time":"2018-12-27T20:46:50.720921Z"},"trusted":true,"_uuid":"02611ef114e79f2252dd48b8bbc49bf19c98d3dc"},"cell_type":"code","source":"def param_flatten(d, params={}):\n    \"\"\"Function that accepts a dictionary with nested dictionaries and returns a flattened dictionary\"\"\"\n    for key, value in d.items():\n        if not isinstance(value, dict):\n            params[key] = value\n        else:\n            param_flatten(value, params)\n            \n    return params","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ad7172d763c7fc03bfa6ee5d4ec70a7abae44a4b"},"cell_type":"markdown","source":"# Bayesian Optimization \n### Objective function to minimize"},{"metadata":{"ExecuteTime":{"end_time":"2018-12-27T20:46:50.737938Z","start_time":"2018-12-27T20:46:50.730068Z"},"trusted":true,"_uuid":"7d3a496dde30072684baf66216e5de2aeeadb60c"},"cell_type":"code","source":"def objective(params, n_folds=N_FOLDS):\n    \"\"\"Objective function for LightGBM Hyperparameter Optimization\"\"\"\n\n    # Keep track of evals\n    global ITERATION\n    ITERATION += 1\n    \n    # We need all the parameters in a flattened dictionary\n    params = param_flatten(params)\n\n    # Make sure parameters that need to be integers are integers\n    for key, value in params.items():\n        if key in ['num_leaves', 'min_data_in_leaf']:\n            params[key] = int(value)\n            \n    print(params)\n\n    # Perform n_folds cross validation.\n    # If you download this notebook you can add callbacks=[logspy] to use Tensorboard\n    try:\n        cv_results = lgb.cv(\n            params,\n            train_set,\n            num_boost_round=100,\n            nfold=n_folds,\n            early_stopping_rounds=10,\n            metrics='multi_logloss')\n\n        # Extract the best score\n        loss = min(cv_results['multi_logloss-mean'])\n        print('loss: ',loss)\n\n        # Boosting rounds that returned the highest cv score\n        epochs = np.argmin(cv_results['multi_logloss-mean']) + 1\n        \n        # Write to the csv file ('a' means append)\n        of_connection = open(out_file, 'a')\n        writer = csv.writer(of_connection)\n        writer.writerow([loss, params, ITERATION, epochs])\n\n        pbar.update()\n\n        # Dictionary with information for evaluation\n        return {\n            'loss': loss,\n            'params': params,\n            'iteration': ITERATION,\n            'epochs': epochs,\n            'status': STATUS_OK\n        }\n    except Exception as e:\n        print('EXCEPTION\\n')\n        print(e)\n        return{'status': 'fail'}","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6e5b4177d025eebe02b8224295ff1427f670f33f"},"cell_type":"markdown","source":"### Space over which to search"},{"metadata":{"ExecuteTime":{"end_time":"2018-12-28T16:52:27.258675Z","start_time":"2018-12-28T16:52:27.253451Z"},"trusted":true,"_uuid":"d6d0af35080816fc42d7e8fc844bf4c970e1e79d"},"cell_type":"code","source":"space = {\n    'boosting':\n    hp.choice('boosting', [\n        {\n            'boosting': 'gbdt',\n            'max_delta_step': hp.quniform('gbdt_max_delta_step', 0, 2, 0.1),\n            'min_data_in_leaf': hp.quniform('gbdt_min_data_in_leaf', 10, 30,\n                                            1),\n            'num_leaves': hp.quniform('gbdt_num_leaves', 20, 40, 1)\n        },\n        {\n            'boosting': 'dart',\n            'max_delta_step': hp.quniform('dart_max_delta_step', 0, 2, 0.1),\n            'min_data_in_leaf': hp.quniform('dart_min_data_in_leaf', 10, 30,\n                                            1),\n            'num_leaves': hp.quniform('dart_num_leaves', 20, 40, 1),\n        },\n    ]),\n    'objective':\n    'multiclass',\n    'num_class':\n    39\n}","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-12-27T20:46:50.76061Z","start_time":"2018-12-27T20:46:50.753429Z"},"trusted":true,"_uuid":"3c0e9d647b7cc84e3279d0655fceed6fabcb1b3f"},"cell_type":"code","source":"def run_trials():\n    \"\"\"Function to run the trials and save the results after every iteration.\n    This is usefull in case you need to interupt the execution and continue from where you left.\"\"\"\n\n    trials_step = 1  # how many additional trials to do after loading saved trials. 1 = save after iteration\n    max_trials = 1  # initial max_trials. put something small to not have to wait\n\n    try:  # try to load an already saved trials object, and increase the max\n        trials = pickle.load(open(\"LGB.hyperopt\", \"rb\"))\n        print(\"Found saved Trials! Loading...\")\n        max_trials = len(trials.trials) + trials_step\n        print(\"Rerunning from {} trials to {} (+{}) trials\".format(\n            len(trials.trials), max_trials, trials_step))\n    except:  # create a new trials object and start searching\n        trials = Trials()\n\n    best = fmin(\n        fn=objective,\n        space=space,\n        algo=tpe.suggest,\n        max_evals=max_trials,\n        trials=trials)\n\n    print(\"Best:\", best)\n\n    # save the trials object\n    with open(\"LGB.hyperopt\", \"wb\") as f:\n        pickle.dump(trials, f)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"47a1fa53f81e43d607287b72cd8dbf4db47806a3"},"cell_type":"code","source":"#File to save first results\n\nof_connection = open(out_file, 'w')\nwriter = csv.writer(of_connection)\n\n# Write the headers to the file\nwriter.writerow(\n    ['loss', 'params', 'iteration', 'epochs'])\nof_connection.close()","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-12-27T20:46:50.768884Z","start_time":"2018-12-27T20:46:50.761979Z"},"trusted":true,"_uuid":"f4312675df2fbd3a722a9fc7bb67f2f6767a1e0f"},"cell_type":"code","source":"ITERATION = 0","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-12-27T23:41:48.704782Z","start_time":"2018-12-27T20:47:11.169972Z"},"scrolled":true,"trusted":true,"_uuid":"ead5fc08880c39316cc36c4431e88ab3441878b9"},"cell_type":"code","source":"while ITERATION <= MAX_EVALS:\n    run_trials()\npbar.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1f25b4f34e6ad4edfc684dbf046085929f7d82a8"},"cell_type":"code","source":"trials = pickle.load(open(\"LGB.hyperopt\", \"rb\"))\nresults = pd.DataFrame(trials.results)\n\nbayes_params = pd.DataFrame(columns = list(results.loc[0, 'params'].keys()),\n                            index = list(range(len(results))))\n\n# Add the results with each parameter a different column\nfor i, params in enumerate(results['params']):\n    bayes_params.loc[i, :] = list(params.values())\n    \nbayes_params['loss'] = results['loss']\nbayes_params['iteration'] = results['iteration']\nbayes_params.sort_values('loss', inplace=True)\n\nbayes_params.head()","execution_count":null,"outputs":[]}],"metadata":{"hide_input":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false}},"nbformat":4,"nbformat_minor":1}