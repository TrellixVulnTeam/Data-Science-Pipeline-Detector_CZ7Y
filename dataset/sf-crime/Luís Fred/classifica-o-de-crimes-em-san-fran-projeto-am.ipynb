{"cells":[{"metadata":{"_uuid":"cbca0b5443548453af86c5ac0b809c5be194fa6b"},"cell_type":"markdown","source":"# Descrição dos dados\n\nNossos dados contém registros sobre a ocorrência de crimes em San Francisco e são divididos em dois arquivos: treino e teste, sendo que o primeiro conjunto possui **877982** exemplos. Trata-se de um dataset ruidoso e que não é fácil, o que aumenta o desafio e exige mais destreza de quem está modelando o problema. Isto contribui em grande medida para o enriquecimento das habilidades de qualquer cientista de dados iniciante.\n\n\n* **Dates** - timestamp of the crime incident\n* **Category** - category of the crime incident (only in train.csv). This is the target variable you are going to predict.\n* **Descript** - detailed description of the crime incident (only in train.csv)\n* **DayOfWeek** - the day of the week\n* **PdDistrict** - name of the Police Department District\n* **Resolution** - how the crime incident was resolved (only in train.csv)\n* **Address** - the approximate street address of the crime incident \n* **X** - Longitude\n* **Y** - Latitude\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"!pip install -q imbalanced-learn\n\nimport numpy as np\nimport operator\nimport string\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.over_sampling import RandomOverSampler\nfrom imblearn.over_sampling import ADASYN\n\nfrom sklearn import manifold\nfrom sklearn.preprocessing import LabelBinarizer, StandardScaler, Normalizer\nfrom sklearn.model_selection import train_test_split\n\n# Estimadores que vamos testar\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nimport xgboost as xgb\n\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score\nfrom sklearn.model_selection import validation_curve\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.metrics import log_loss\nfrom sklearn.preprocessing import LabelEncoder\n\n# utilitários para plots\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\nimport folium\nfrom folium import plugins\nfrom folium.plugins import HeatMap\nfrom folium.plugins import FastMarkerCluster\nfrom folium.plugins import MarkerCluster\n\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\n\nplt.style.use('ggplot')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"aea3860304427715c162d0e731087040cbe892ad"},"cell_type":"markdown","source":"# Função para plot de alguns gráficos"},{"metadata":{"trusted":true,"_uuid":"28189714f41dc8818804aac827d6727374bbcaf4"},"cell_type":"code","source":"sns.set_style(\"whitegrid\")\nsns.despine()\n\ndef plot_bar(df, title, filename):    \n    p = (\n        'Set2', \n        'Paired', \n        'colorblind', \n        'husl',\n        'Set1', \n        'coolwarm', \n        'RdYlGn', \n        #'spectral'\n    )\n    color = sns.color_palette(np.random.choice(p), len(df))\n    bar   = df.plot(kind='barh',\n                    title=title,\n                    fontsize=8,\n                    figsize=(12,8),\n                    stacked=False,\n                    width=1,\n                    color=color,\n    )\n\n    bar.figure.savefig(filename)\n\n    plt.show()\n\ndef plot_top_crimes(df, column, title, fname, items=0):\n    try:        \n        by_col         = df.groupby(column)\n        col_freq       = by_col.size()\n        col_freq.index = col_freq.index.map(string.capwords)\n        col_freq.sort_values(ascending=True, inplace=True)\n        plot_bar(col_freq[slice(-1, - items, -1)], title, fname)\n    except Exception:\n        plot_bar(df, title, fname)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4fa739f1b566cbcbe7b63e522fee43efadeb67e2"},"cell_type":"markdown","source":"# Importando o dataset"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train_data = pd.read_csv(\"../input/train.csv\", parse_dates =['Dates'])\ntest_data = pd.read_csv(\"../input/test.csv\", parse_dates =['Dates'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"739fd5db6cdf46a8f0efee4a820bac3a52e8fc84"},"cell_type":"code","source":"print('Shape dos dados de treino:',train_data.shape)\nprint('Shape dos dados de teste :',test_data.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9708229322fa13f704212a2a4754bdd16cef98ab"},"cell_type":"code","source":"train_data.head(6)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7c222bb174ceb52c206108159caefb34bae0e721"},"cell_type":"markdown","source":"# Extraindo features a partir das datas\n\nDatas e horários podem ser bastante informativas. Para o nosso caso, iremos \"quebrar\" as datas em seu componentes mais elementares, de modo a vermos o que determinados dias ou horários no dizem a respeito da ocorrência de crimes. Um outro efeito disto é que esta prática vai aumentar a quantidade de features disponíveis para o algoritmo. Se tomarmos como exemplo a data **2018-10-28 23:53:00**, nós poderíamos quebrar esta data em:\n\n* **ano:** 2018\n* **mês:** 10\n* **dia:** 28\n* **hora:** 23h\n* **minutos:** 53\n\nNa prática, isto nos daria mais 5 colunas em nosso dataframe, aumentando a quantidade de informações."},{"metadata":{"trusted":true,"_uuid":"2a6deb161f37e28f43cc86769cb61a0f6e5799c4"},"cell_type":"code","source":"# separa as datas em ano, mês, dia, hora, minuto e segundo.\n# cada parte da data em uma coluna separada. Isto aumenta a quantidade de features\n\nfor x in [train_data, test_data]: \n    x['years'] = x['Dates'].dt.year\n    x['months'] = x['Dates'].dt.month\n    x['days'] = x['Dates'].dt.day\n    x['hours'] = x['Dates'].dt.hour\n    x['minutes'] = x['Dates'].dt.minute","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9a2ad6b69cefa351c0566309558bf1f56b9b8126"},"cell_type":"markdown","source":"# Extraindo features a partir das coordenadas\nVamos compor uma nova coluna no Dataframe apartir do produto entre as features X e Y, que correspondem aos valores de latitude e longitude, respectivamente."},{"metadata":{"trusted":true,"_uuid":"ad3c6a81956009ca477901278b9b103079842859"},"cell_type":"code","source":"train_data['XY'] = train_data.X * train_data.Y\ntest_data['XY'] = test_data.X * test_data.Y","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1380a102c01526a84a821d9bb6694e798884d23f"},"cell_type":"markdown","source":"# Visualizações\nPelas visualizações é possível notar que os crimes no distrito de  **SOUTHERN** têm uma frequência mais elevada do que nos outros distritos. Além disso, **entre 01:00 am e 07:00 am os crimes são menos frequentes**. Horários de **maior pico** estão **entre 17:00 e 18:00**.\n\n**Classes desbalanceadas** \n\nUm problema patente pode ser notado com relação à distribuição das classes ao longo do conjunto de aprendizagem, onde se pode notar que estão **bastante desbalanceadas**. Há classes com menos de 10 exemplos, enquanto há outras com 126182, 174900, etc . Classes desbalanceadas introduzem um viés no modelo, que terá a tendência de favorecer as classes majoritárias.\n\nEliminar as classes menos representativas podería resultar em um modelo mais estável. O problema disso é que a Kaggle não computaria o nosso score, pois espera que todas as classes estejam presentes no output do nosso modelo."},{"metadata":{"trusted":true,"_uuid":"cc394feb115707ed8a3542b119eb6de3417387ed"},"cell_type":"code","source":"plot_top_crimes(train_data, 'Category', 'Por categoria', 'category.png')\n# quantidade de crimes associado à cada uma das categorias\nprint(train_data.Category.value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a7ecb906d48860398694b61e9d37a7b5202ba083"},"cell_type":"code","source":"plot_top_crimes(train_data, 'Address', 'Principais localizações de ocorrências',  'location.png', items=50)\nprint(train_data.Address.value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"eb14b7ff583b63041cc0d24153bd0c9efd03e905"},"cell_type":"code","source":"plot_top_crimes(train_data, 'PdDistrict', 'Departamentos com mais atividades',  'police.png')\n# quantidade de incidentes associada à cada distrito policial\nprint(train_data.PdDistrict.value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3375aabb01d2a7058592f8e9cec6f18fa4996af9","scrolled":true},"cell_type":"code","source":"fig, ((axis1,axis2)) = plt.subplots(nrows=1, ncols=2)\nfig.set_size_inches(15,4)\n\nsns.countplot(data=train_data, x='days', ax=axis1)\nsns.countplot(data=train_data, x='hours', ax=axis2)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"de7fdad92b06a67775ceac2daeed328f1b225cc8"},"cell_type":"markdown","source":"Percentual de incidentes por endereço"},{"metadata":{"trusted":true,"_uuid":"5c39593945815b25726eabd5f7678d7e82527e11"},"cell_type":"code","source":"addr = train_data['Address'].apply(lambda x: ' '.join(x.split(' ')[-2:]))\n\nyear_count=addr.value_counts().reset_index().sort_values(by='index').head(10)\nyear_count.columns=['addr','Count']\n# Create a trace\ntag = (np.array(year_count.addr))\nsizes = (np.array((year_count['Count'] / year_count['Count'].sum())*100))\nplt.figure(figsize=(15,8))\n\ntrace = go.Pie(labels=tag, values=sizes)\nlayout = go.Layout(title='Endereços com mais incidentes')\ndata = [trace]\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig, filename=\"Inncidentes\")\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e451ec6acc8127229224e5c745262b3fca4908ba"},"cell_type":"markdown","source":"Número de crimes que ocorreram durante cada mês e ao longo dos anos registrados no conjunto de dados. "},{"metadata":{"trusted":true,"_uuid":"0b2ed6636ed2c3606ef062cf442203d3f01f0b38"},"cell_type":"code","source":"data=[]\nfor i in range(2003,2015):\n    year=train_data[train_data['years']==i]\n    year_count=year['months'].value_counts().reset_index().sort_values(by='index')\n    year_count.columns=['months','Count']\n    trace = go.Scatter(\n    x = year_count.months,\n    y = year_count.Count,\n    name = i)\n    data.append(trace)\n    \n\npy.iplot(data, filename='basic-line')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"91cfb7e8dacba8486e6e93e6342ccc75b43d95b6"},"cell_type":"markdown","source":"Número de crimes por lacalidade (limitamos em 1000 registros)"},{"metadata":{"trusted":true,"_uuid":"852e2cc1435ffb9ce329944803faf57583a5f22c"},"cell_type":"code","source":"m = folium.Map(\n    location=[train_data.Y.mean(), train_data.X.mean()],\n    tiles='Cartodb Positron',\n    zoom_start=13\n)\n\nmarker_cluster = MarkerCluster(\n    name='Locais de crimes em San Francisco',\n    overlay=True,\n    control=False,\n    icon_create_function=None\n)\nfor k in range(1000):\n    location = train_data.Y.values[k], train_data.X.values[k]\n    marker = folium.Marker(location=location,icon=folium.Icon(color='green'))\n    popup = train_data.Address.values[k]\n    #popup = train_data.Address.apply(lambda x: ' '.join(x.split(' ')[-2:])).values[k]\n    folium.Popup(popup).add_to(marker)\n    marker_cluster.add_child(marker)\n\nmarker_cluster.add_to(m)\n\nfolium.LayerControl().add_to(m)\n\nm.save(\"cluster.html\")\n\nm","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"56da4c1396b9dc6d2ad9f4723d065ec9ade26c8a"},"cell_type":"markdown","source":"**LARCENY/THEFT** é a categoria de crimes com maior número de ocorrências. Onde estas ocorrências mais se concentram?"},{"metadata":{"trusted":true,"_uuid":"fa177933998c93854b6a735211c0a3e28c535bdd"},"cell_type":"code","source":"new=train_data[train_data['Category']=='LARCENY/THEFT']\nM= folium.Map(location=[train_data.Y.mean(), train_data.X.mean() ],tiles= \"Stamen Terrain\",\n                    zoom_start = 13) \n\nheat_data = [[[row['Y'],row['X']] \n                for index, row in new.head(1000).iterrows()] \n                 for i in range(0,11)]\n\nhm = plugins.HeatMapWithTime(heat_data,auto_play=True,max_opacity=0.8)\nhm.add_to(M)\n\nhm.save('heatmap.html')\n\nM","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b7e34c5aa4b9d42a5457431676ea25f79d6876ce"},"cell_type":"markdown","source":"Features altamnte correlacionadas deveriam ser evitadas, já que fornecem pouca informação extra. Neste caso, podemos checar a correlação entre features numéricas.\n\n**Atualização:** há um correlação muito fraca entre as features, como pode ser visto abaixo. "},{"metadata":{"trusted":true,"_uuid":"ba1aabee0282bde0da0e0da74430b894ea4dbe93"},"cell_type":"code","source":"# correlações entre as variáveis\ntrain_data.corr().style.format(\"{:.2}\").background_gradient(cmap=plt.get_cmap('tab20c'), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6c971ab0041a4e8f642d74ffca600c0f2684c77b"},"cell_type":"markdown","source":"# Extraindo features a partir dos endereços\n\nÉ possível que o fato de o crime ter ocorrido numa rua ou numa avenida possa ter um caráter discriminatório forte. O fato de que avenidas costumam ser mais movimentadas e monitoradas do que muitas ruas pode inviabilizar a prática de determinados crimes. \n\nNós podemos extrair esta informação a partir da coluna **'Address'** presente em nosso Dataframe. A string que corresponde ao endereço contém algumas abreviações. Entre estas abreviações nós temos:\n\n* **ST**: Abreviação para Street\n* **AV**: Abreviação para Avenue\n\nNós iremos extrair estas informações, transformá-las em features categóricas e, em seguida, adicionar duas colunas no Dataframe para acomodar estas infos categóricas. Basicamente, estas colunas indicarão se o crime ocorreu numa rua ou numa avenida."},{"metadata":{"trusted":true,"_uuid":"08208d7eaa1c48211ed56533073b8aa4ac64d8a0"},"cell_type":"code","source":"def street_addr(x):\n    street=x.split(' ')\n    return (''.join(street[-1]))\n\ntrain_data['Address_Type'] = train_data['Address'].apply(lambda x:street_addr(x))\ntest_data['Address_Type'] = test_data['Address'].apply(lambda x:street_addr(x))\n\nfor x in [train_data,test_data]:\n    x['is_street'] = (x['Address_Type'] == 'ST')\n    x['is_avenue'] = (x['Address_Type'] == 'AV')\n\ntrain_data['is_street'] = train_data['is_street'].apply(lambda x:int(x))\ntrain_data['is_avenue'] = train_data['is_avenue'].apply(lambda x:int(x))\n\ntest_data['is_avenue'] = test_data['is_avenue'].apply(lambda x:int(x))\ntest_data['is_street'] = test_data['is_street'].apply(lambda x:int(x))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2995f1bed6c859533cf06c6f897cf31b3196dfd7"},"cell_type":"markdown","source":"Há uma grande quantidade de incidentes cujos endereços associados contém este termo **\"Block\"** no endereço. Nós podemos criar mais uma feature categórica a partir disto."},{"metadata":{"trusted":true,"_uuid":"3054d11febb573bb9e79df0f9339522ab01a2792"},"cell_type":"code","source":"def is_block(x):\n    if 'Block' in x:\n        return 1\n    else:\n        return 0\n\ntrain_data['is_block'] = train_data['Address'].apply(lambda x:is_block(x)) \ntest_data['is_block'] = test_data['Address'].apply(lambda x:is_block(x)) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"be893fa876b58b3d3763d5314d4952000a8cb849"},"cell_type":"code","source":"train_data.head(20)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ec4628a844a4191298f0f35ad035dad5855a0c02"},"cell_type":"markdown","source":"# Codificando os rótulos\n\nÀ princípio, as categorias estão representadas numa forma textual, ou seja, na forma de dados discretos. Contudo, estas informações precisam de uma representação numérica para que nosso modelo possa trabalhar com elas. Nós faremos exatamente isto com os dados da coluna **Category**."},{"metadata":{"trusted":true,"_uuid":"d791e07eac97c636b066fd1b9f785a77780be976"},"cell_type":"code","source":"category = LabelEncoder()\ntrain_data['Category'] = category.fit_transform(train_data.Category)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"091523982afe64015e4e598b66b49bb3e3d5bcd0"},"cell_type":"markdown","source":"# Codificando outras features categoricas\nNosso conjunto de dados possui outras features que também são categóricas, como é o caso dos dias da semana nos quais ocorrem os incidentes, bem como o departamento de polícia associado.  Como estas informações estão na forma de texto, nós iremos codificá-las numericamente."},{"metadata":{"trusted":true,"_uuid":"1fefd838fcfb00f14f2cfccc9865daa201bfadc5"},"cell_type":"code","source":"# codifica outras features categoricas, incluindo-as como novas colunas no dataframe\nfeature_cols =['DayOfWeek', 'PdDistrict']\ntrain_data = pd.get_dummies(train_data, columns=feature_cols)\ntest_data = pd.get_dummies(test_data, columns=feature_cols)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"aa9c75bef7dc0c5edeb5d45443a3771e759155dd"},"cell_type":"markdown","source":"# Descartando colunas que não utilizaremos\nNós não precisaremos mais das colunas indicadas abaixo."},{"metadata":{"trusted":true,"_uuid":"a7254d46c319f1910002bb4611dd30e283fd60ed"},"cell_type":"code","source":"# Nós não precisaremos das colunas abaixo, motivo pelo qual irems descartá-las.\ntrain_data = train_data.drop(['Dates', 'Address', 'Address_Type', 'Resolution'], axis = 1)\ntrain_data = train_data.drop(['Descript'], axis = 1)\ntest_data = test_data.drop(['Address','Address_Type', 'Dates'], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"45738c159e68687ab0a12a88143fa2be3e5dd1d5"},"cell_type":"code","source":"train_data.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"08667a2b097024e2ea130acdda45a280cf73fbf1"},"cell_type":"code","source":"test_data.head(5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ecfb00dbd909b844411c46dd8e6ffc1d082b3634"},"cell_type":"markdown","source":"# Modelagem"},{"metadata":{"trusted":true,"_uuid":"a380b5e605dd91b87b14e2ddab4e37ac72282849"},"cell_type":"code","source":"feature_cols = [x for x in train_data if x!='Category']\nX = train_data[feature_cols]\ny = train_data['Category']\nX_train, x_test,y_train, y_test = train_test_split(X, y)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d705d534064b9b4890b2cc04d18e530d765817c5"},"cell_type":"markdown","source":"# Normalização\n\nNormalizar e padronizar os dados nos previne contra problemas envolvendo as escalas dos números. Além disso, vai permitir que nossos dados tenham as propriedades de uma distribuição normal padrão, com média **0** e desvio padrão **1**. Trata-se de um procedimento, a bem dizer, mandatório em muitas tarefas envolvendo análise de dados."},{"metadata":{"trusted":true,"_uuid":"a35b4f7c99031d84d6ee99356892b317cd975441"},"cell_type":"code","source":"del X\ndel y\n\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nx_test = scaler.transform(x_test)\n\nnormalizer = Normalizer()\nX_train = normalizer.fit_transform(X_train)\nx_test = normalizer.transform(x_test)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"656c00f4e99ff67697765afa60417b8a3e15268b"},"cell_type":"markdown","source":"# Tratando classes desbalanceadas\n\nclasses desbalanceadas introduzem um viés nos modelos, com forte tendência ao favorecimento da classe majoritária. Abaixo nós tentamos resolver este problema por meio de uma técnica de *oversamplig*, que gera dados sintéticos com bases nos vizinhos mais próximos de determinados segmentos da classe minoritária.\n\n**atualização**\n\nNenhum dos métodos de oversampling testados abaixo trouxe melhora no desempenho do classificador. Undersampling não seria uma boa alternativa, considerando que perderíamos informação de mais das outras classes para balancear estes rótulos de acordo com as classes minoritárias."},{"metadata":{"trusted":true,"_uuid":"c32e22da54f1c41a5fe64c0204c91f6d60c4959e"},"cell_type":"code","source":"#ros = RandomOverSampler(random_state=42, sampling_strategy='not majority')\n#X_resampled, y_resampled = ros.fit_resample(X_train, y_train)\n\n#sm = SMOTE(random_state=42, k_neighbors=3)\n#X_resampled, y_resampled = sm.fit_resample(X_train, y_train)\n\n#ada = ADASYN(random_state=42, n_neighbors=4)\n#X_resampled, y_resampled = ada.fit_resample(X_train, y_train)\n\n#from collections import Counter\n\n#print('shape do dataset original %s' % Counter(y_train))\n#print('shape do dataset com oversampling %s' % Counter(y_resampled))\n\n#print(sorted(Counter(y_resampled).items()))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3fe0c2bd1078d923be082962eefa8ac93b47a729"},"cell_type":"markdown","source":"## Random Forest\n\nRandom Forest é um algorítmo baseado em ensemble bastante popular, que cria vários classificadores, baseados em árvore de decisão, em cima dos dados de treino e combina todas as saídas destes classificadores para obter uma acurácia mais estável.\n\nOutras abordagens testadas aqui, e que não se saíram melhor do que a Random Forest, foram:\n1. SVM\n2. Multi layer Perceptron\n3. Gradient Boosting\n4. Regressão Logística\n\nApesar de nós estarmos usando a métrica acurácia (abaixo), é bom lembrar que ela não é uma medida de desempenho adequada para modelos que lidam com classes desbalanceadas, já que trata todas as classes com igual importância.\n\n**Melhor score alcançado:**\n\n* **accuracy_score**:  0.3242495888626186\n* **f1_score_weighted** 0.36493050029624635*\n\n**Melhor score público:** 2.71003\n\n**Score público do líder na competição:** 1.95936"},{"metadata":{"trusted":true,"_uuid":"daabaf0919c4bd4042f1117de3c1b6596b795770"},"cell_type":"code","source":"random_forest = RandomForestClassifier(n_estimators=100, max_depth=23)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a2a9a165d937f53c051c634a67f0a4f0857c42bb"},"cell_type":"code","source":"random_forest.fit(X_train, y_train.ravel())\n#random_forest.fit(X_resampled, y_resampled)\npred = random_forest.predict(x_test)\nprint(\"accuracy_score: \", accuracy_score(pred,y_test))\nprint(\"f1_score_weighted\", f1_score(pred,y_test, average='weighted'))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"87b8d453015d1d4155bc27e0cbdc9e7ebd07ffc4"},"cell_type":"markdown","source":"# Submissão"},{"metadata":{"trusted":true,"_uuid":"feac02e5c1380e8e0c7d8ea3a3a986e97611fec7"},"cell_type":"code","source":"#X_test =test_data.drop(['Id'], axis = 1)\npredicted_sub = random_forest.predict_proba(test_data.drop(['Id'], axis = 1))\nsubmission_results = pd.DataFrame(predicted_sub, columns=category.classes_)\nsubmission_results.to_csv('submission.csv', index_label = 'Id')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}