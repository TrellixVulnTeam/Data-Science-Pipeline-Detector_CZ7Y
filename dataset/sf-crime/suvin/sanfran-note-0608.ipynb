{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# 컬럼 뜻 확인하기\n# 데이터양이 많은 경우, 모델을 빨리 돌릴 수 있는 것도 필요\n# 정답값 확인하기 CATEGORY(정답값)\n# 전처리 할때 데이터를 가져와야한다. \n# 1. 날짜 데이터 파싱해서 불러오기(index_col='Id':ID값을 인덱스로 쓰겠다.)\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\ntrain = pd.read_csv('../input/train.csv', parse_dates=['Dates'])\ntest = pd.read_csv('../input/test.csv', parse_dates=['Dates'], index_col='Id')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#데이터가 어떻게 생겼는지 보자\ntrain.head()\n#2. 문자형 데이터가 많다. 어떻게 처리할까(라벨 인코더)\n# -> 문자를 숫자로 바꿔야겠다. 어떻게 바꿀까?\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 2.1 PdDistrict는 유니크한 카테고리가 10개 있다. \ntrain[\"PdDistrict\"].nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 2.2 자동으로 텍스트를 숫자로 바꿔주기(레이블 인코더, 라벨 인코더)\n# 라벨 인코더 불러오기\nfrom sklearn.preprocessing import LabelEncoder","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 2.3 선언하기 \nle1 = LabelEncoder()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 2.4 모델링에서는 훈련/ 예측 이지만 여기서는 등록과 변환(le1 ->fit)\n# 2.5 transform 사전(look up table)\n# -> train(10개 데이터)에 사전을 등록\ntrain['PdDistrict'] = le1.fit_transform(train['PdDistrict'])\ntest['PdDistrict'] = le1.transform(test['PdDistrict'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 2.6 우리 모델이 학습할텐데, 숫자로 바꿔주면 데이터 사이에 거리가 생기진 않을까? 잘못된 정보\n# -> 다행이도 괜찮다. -> 트리 모델은 각각 값이 질문을 해가면서 분류 작업을해나간다.\n# -> 트리모델은 각각의 데이터가 다르기만 하면 잘 분류 된다(거리개념 적용x)\n# -> 만약 회기 등이라면 가중치를 주며 학습할 수 있다. \n# 트리모델이 대표적:엔트로피가 낮다:무질서도 이기 때문에 데이터가 다 분류 된 상태라면 엔트로피가 낮아졌다.(만개를 분류할 때)\n# 데이터에 가중치가 들어가는 모델일 경우(트리모델 외)= 원핫 인코딩, dummy화 하여 모델이 착각하지 않게 넣어줄 수 있다.\n## 인코딩 기억해야할 것 2가지(라벨인코딩,원핫인코딩,pca차원축소를 통해 최대한 실제 정보를 보존하면서 칼럼수를 줄여버림)\n# 또 인코딩 해줘야하는 것이 무엇일까?\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#2.6.1 address도 인코딩\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#2.6.2 train, test가 개수가 다르다.\n# train에는 없는데, test에만 있을 수 있다\n# 이럴 경우 어떻게 하는게 좋을까?\ntrain['Address'].nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['Address'].nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#2.6.3 위의 값 진행하지 않고 아래 코드만 실행!\n#train 과 test 유니크 값이 다르더라도 괜찮음.\n#현업에서는 같이 등록하는 것 자체가 제한이 있다(test 확인x), \n#아래는 test 셋을 볼 수 있어서 대회 등에서 사용할 수 있다.\nle3 = LabelEncoder()\nle3.fit(list(train['Address']) + list(test['Address']))\ntrain['Address'] = le3.transform(train['Address'])\ntest['Address'] = le3.transform(test['Address'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#2.6.4. 라벨 인코딩 진행\n# 기본 베이스라인 점수가 나옴\nle2 = LabelEncoder()\ny= le2.fit_transform(train['Category'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#값 확인하기\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#트리 모델은 칼럼이 많을수록 좋다.\n#트리모델은 학습할 때 알아서 피처를 선택한다. \n#중요한 칼럼이 있다면 가중치를 주면서 독점 학습\n#반대로 안 중요한 것들은 학습 안함.\n#모델에게 학습을 맡긴 후 이후에 빼도 된다.\ntrain['Day'] = train['Dates'].dt.day\ntrain['DayOfWeek'] = train['Dates'].dt.weekday\ntrain['Month'] = train['Dates'].dt.month\ntrain['Year'] = train['Dates'].dt.year\ntrain['Hour'] = train['Dates'].dt.hour\ntrain['Minute'] = train['Dates'].dt.minute\n\ntest['Day'] = test['Dates'].dt.day\ntest['DayOfWeek'] = test['Dates'].dt.weekday\ntest['Month'] = test['Dates'].dt.month\ntest['Year'] = test['Dates'].dt.year\ntest['Hour'] = test['Dates'].dt.hour\ntest['Minute'] = test['Dates'].dt.minute","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#범죄 카테고리를 39개로 나눠야한다.\n#범죄 트렌드를 추가해주면 모델 성능이 오를것이다.\n#가령 사이버 범죄는 이전 보다 요즘 많다. \n#n_days 칼럼을 만들려면 train['date']코드를 만들어줘야한다.\n#트레인 데이트에서 트레인 dt.date하면 연, 월,일 정보가 뽑힌다.\n#(train['Date'] - train['Date'].min()) \n# : 어떤 범죄가 발생한 날짜 - 가장 최초 생긴 날짜\n# = 3650 days 가 나온다. days를 삭제해줌(=3650)\n# .apply(lambda x: x.days) : 사용자 지정 함수(한줄로 한방에)\n# 요즘 많이 발생하는 범죄는 n.days가 크다.()\n\ntrain['Date'] = (train['Dates'].dt.date)\ntrain['n_days'] = (train['Date'] - train['Date'].min()).apply(lambda x: x.days)\n\ntest['Date'] = (test['Dates'].dt.date)\ntest['n_days'] = (test['Date'] - test['Date'].min()).apply(lambda x: x.days)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#00608문자를 숫자로만들어줌(원핫인코딩) -> 칼럼, 그릇이 너무 커진다.\n#레이블인코딩 +랜덤포레스트 사용\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\ntrain = pd.read_csv('../input/train.csv', parse_dates=['Dates'])\ntest = pd.read_csv('../input/test.csv', parse_dates=['Dates'], index_col='Id')\n\ntrain['Date'] = pd.to_datetime(train['Dates'].dt.date)\ntrain['n_days'] = (train['Date'] - train['Date'].min()).apply(lambda x: x.days)\ntrain['Day'] = train['Dates'].dt.day\ntrain['DayOfWeek'] = train['Dates'].dt.weekday\ntrain['Month'] = train['Dates'].dt.month\ntrain['Year'] = train['Dates'].dt.year\ntrain['Hour'] = train['Dates'].dt.hour\ntrain['Minute'] = train['Dates'].dt.minute\ntrain['Block'] = train['Address'].str.contains('block', case=False).apply(lambda x: 1 if x == True else 0)\ntrain['ST'] = train['Address'].str.contains('ST', case=False).apply(lambda x: 1 if x == True else 0)\ntrain[\"X_Y\"] = train[\"X\"] - train[\"Y\"]\ntrain[\"XY\"] = train[\"X\"] + train[\"Y\"]\n\ntest['Date'] = pd.to_datetime(test['Dates'].dt.date)\ntest['n_days'] = (test['Date'] - test['Date'].min()).apply(lambda x: x.days)\ntest['Day'] = test['Dates'].dt.day\ntest['DayOfWeek'] = test['Dates'].dt.weekday\ntest['Month'] = test['Dates'].dt.month\ntest['Year'] = test['Dates'].dt.year\ntest['Hour'] = test['Dates'].dt.hour\ntest['Minute'] = test['Dates'].dt.minute\ntest['Block'] = test['Address'].str.contains('block', case=False).apply(lambda x: 1 if x == True else 0)\ntest['ST'] = test['Address'].str.contains('ST', case=False).apply(lambda x: 1 if x == True else 0)\ntest[\"X_Y\"] = test[\"X\"] - test[\"Y\"]\ntest[\"XY\"] = test[\"X\"] + test[\"Y\"]\n\nfrom sklearn.preprocessing import LabelEncoder\nle1 = LabelEncoder()\ntrain['PdDistrict'] = le1.fit_transform(train['PdDistrict'])\ntest['PdDistrict'] = le1.transform(test['PdDistrict'])\n\nle2 = LabelEncoder()\ny= le2.fit_transform(train['Category'])\n\nle3 = LabelEncoder()\nle3.fit(list(train['Address']) + list(test['Address']))\ntrain['Address'] = le3.transform(train['Address'])\ntest['Address'] = le3.transform(test['Address'])\n\ntrain.drop(['Dates','Date','Descript','Resolution', 'Category'], 1, inplace=True)\ntest.drop(['Dates','Date',], 1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#모델링 과정 4가지 \n#1.모델 가져오기 #모델 불러올 때 주의할 점:모든 모델은 분류/회기 모두 사용\n#라이쥐비엔 \nfrom lightgbm import LGBMClassifier\n\n#2.모델 선언\nmodel = LGBMClassifier()\n#랜덤포레스트와, 부스팅 모델은 여러 나무를 만들고 각각의 나무가 전체 데이터 셋을 보면서 학습한다.\n#랜덤포레스트 나무가100그루이면, 학습을 100번한다.학습을할 때 서로를 도와주지 않는다.\n#각각의 나무들이(디시젼트리,의사결정)이 모여서 숲을 이룬다. 100그루이면 100번학습한다. \n#랜덤포레스트는 서로 돕지 않는다. 부스팅 모델은 서로서로 도와준다.\n#부스팅 모델의 학습방법(1번나무에서 에러율이 10이 나왔다고 가정, \n#2번나무에서는 1번 나무의 틀ㄹ린것을 어떻게든 학습하려고 노력함,에러9)\n#단, 이럴경우 오버피팅될 수 있다.(과접합문제) ->파라미터를 섬세하게 세팅\n#모델의 성능을 극한으로 올리고 싶으면, 부스팅모델 사용\n\n#<정말 중요한 하이퍼파라미터 설명>\n#2.1 필수! colsample_bytree': 0.625 -> 과접합 문제 간접적으로 해결함(보통 0.7이고, 0.5~0.6까지 내려도 큰상관 없음)\n#이 값을 설정안해주면, 모든 트리마다 같은 칼럼이 들어간다 \n#나무마다 칼럼을 6개씩 다른 조합이 들어간다.62.5%이면 6개 \n#왜이렇게 할까? 너무 같은 값, 100%로 들어가면 과접합\n#트리모델은 탐욕적이라서 어떤 나무에 칼럼이 들어오면 중요한 칼럼이 있으면 가중치를 주면서 학습을 한다. \n#별로 안중요하면 칼럼을 버림=> 트리모델은 자동으로 피처를 선택(효율적임)\n#4개의 칼럼이 있었음, 2번 칼럼이 가장 중요한 칼럼으로 과접합될 수 있어서, 퍼센테이지를 준다.\n\nmodel = LGBMClassifier(colsample_bytree = 0.625)\n\n#1번 나무에 1,2,3번 칼럼이 들어감 -> 2번 칼럼이 많이 들어감\n#2번 나무에 1,3,4번 칼럼이 들어감 ->1번과 3번을 많이 포함, \n#별로 안중요한것도 학습되어 대처할 수 있으며, 과접합문제를 간접적으로 해결할 수 있다.\n#각나무가 하나의 모델처럼, 앙상블과 비슷하다. (모델 성능이 어느정도 뒷받침,다양성확보)\n#모델 속도도 40%가 높아진다.\n\n#2.2 필수! subsample :0.8이 기본! 로우값 샘플링(데이터가 많으면 0.7까지는 낮출수있다.) \n#모델 학습속도 개선, 다양성확보,아웃라이어 \n#데이터를 나무에 넣어줄 때 칼럼만 샘플링할수있을까?\n#로우도 가능하다. \n\nmodel = LGBMClassifier(colsample_bytree = 0.625, subsample= 0.8)\n\n#2.3 num_leaves:나뭇잎 개수:자식 노드 수:피쳐가 질문=피쳐(칼럼..), 무질서도가 낮아진다=앤트로피가 낮아진다.\n#나뭇잎개수가 커지면, 오버피팅?언더피팅?->오버피팅 /정답 클래스가 39개이면(보이스피싱, 범죄) num_leaves도 최소 39개\n#나뭇잎개수가 233개라면, 233만큼의 깊이, =?lgbm은 트리의 깊이 나뭇잎으로 설정한다.\n#트리의 깊이가 7번,8번 정도 인거다. 2의7승=233개 \n# num_leaves 기본은 31인데, 이거는 왜이렇게 많이 해줬을까? \n# 클래스가 39로 많아서, ex)주소데이터도 2만개로 복잡하다.\n#우선 기본값으로 돌려보고 성능평가해본다음에 진행\nmodel = LGBMClassifier(colsample_bytree = 0.625, subsample= 0.8, num_leaves = 233)\n\n#2.4 n_estimators,learning_rate 짝꿍! 반비례,하나를 바꿔주면 같이 바꿔줘야함.\n#n_estimators 나무 개수(보통 100개), \n#learning_rate(학습률 -> 학습속도, 하나의 나무에서 얼마나 학습을 할것인지!)\n#러닝레이트가 크면 학습을 빨리한다. 최적의 코스트를 놓칠수있다. 오버슈팅 문제\n#러닝레이트가 크다면 모델이 최적의 에러값으로 수렴하는데까지 걸리는 시간이 작다\n#나무를 조금만 만들어줘도된다!!러닝레이트가 0.1 나무의갯수 150 \n#/ 0.05 러닝레이트를 0.05로 낮춰주는 동시에 300개 0.025 나무의갯수 600개\n\nmodel = LGBMClassifier(colsample_bytree = 0.625, subsample= 0.8, num_leaves = 233, learning_rate = 0.025, n_estimators = 600)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#학습\nmodel.fit(train, y, categorical_feature=[\"PdDistrict\", \"DayOfWeek\"])\n#lgbm모델은 대용량 속도 가장 빠름! 문자열 처리 잘함 \n#(Catboost은 문자열 가장 잘함, 최신 모델,대용량에는 약함)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#proba : 확률로 제출 ex) 정답값이 소매치기, 소매치기 확률 49%, 절도 51%이면 패널티를 적게 먹을 수있음\npreds = model.predict_proba(test)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame(preds, columns=le2.inverse_transform(np.linspace(0, 38, 39, dtype='int16')), index=test.index)\nsubmission.to_csv('LGBM_final.csv', index_label='Id')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}