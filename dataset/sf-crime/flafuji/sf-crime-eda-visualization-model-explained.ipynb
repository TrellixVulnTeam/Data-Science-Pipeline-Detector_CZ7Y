{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<img src=\"https://upload.wikimedia.org/wikipedia/commons/9/97/San_Francisco%E2%80%93Oakland_Bay_Bridge%2C_Partial_View_from_Embarcadero_20110804_1.jpg\"/>\n<center>[DXR, CC BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0) , via Wikimedia Commons</center>","metadata":{"papermill":{"duration":0.020806,"end_time":"2021-12-06T05:15:53.334052","exception":false,"start_time":"2021-12-06T05:15:53.313246","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"# About Competition\nFrom 1934 to 1963, San Francisco was infamous for housing some of the world's most notorious criminals on the inescapable island of Alcatraz.\n Today, the city is known more for its tech scene than its criminal past. With rising wealth inequality and housing shortages, there is no scarcity of crime in the city by the bay.Competition file is available [here](https://www.kaggle.com/c/sf-crime).","metadata":{"papermill":{"duration":0.017479,"end_time":"2021-12-06T05:15:53.371475","exception":false,"start_time":"2021-12-06T05:15:53.353996","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"# Load Competition Dataset","metadata":{"papermill":{"duration":0.017526,"end_time":"2021-12-06T05:15:53.407876","exception":false,"start_time":"2021-12-06T05:15:53.39035","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"Competition dataset located in \"/kaggle/input\"; This path defined by Kaggle to access the competition file. We will list two files from this path as input files.","metadata":{"papermill":{"duration":0.017621,"end_time":"2021-12-06T05:15:53.443381","exception":false,"start_time":"2021-12-06T05:15:53.42576","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        path=os.path.join(dirname, filename)\n        if 'train' in path:\n            __training_path=path\n        elif 'test' in path:\n            __test_path=path","metadata":{"papermill":{"duration":0.04515,"end_time":"2021-12-06T05:15:53.506662","exception":false,"start_time":"2021-12-06T05:15:53.461512","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-12-08T17:05:10.158403Z","iopub.execute_input":"2021-12-08T17:05:10.159277Z","iopub.status.idle":"2021-12-08T17:05:10.195008Z","shell.execute_reply.started":"2021-12-08T17:05:10.159148Z","shell.execute_reply":"2021-12-08T17:05:10.193898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Input Dataset","metadata":{"papermill":{"duration":0.019972,"end_time":"2021-12-06T05:15:53.545493","exception":false,"start_time":"2021-12-06T05:15:53.525521","status":"completed"},"tags":[]}},{"cell_type":"code","source":"#loaded files\nprint(f'Training path:{__training_path}\\nTest path:{__test_path}')","metadata":{"papermill":{"duration":0.02889,"end_time":"2021-12-06T05:15:53.592443","exception":false,"start_time":"2021-12-06T05:15:53.563553","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-12-08T17:05:10.196809Z","iopub.execute_input":"2021-12-08T17:05:10.197086Z","iopub.status.idle":"2021-12-08T17:05:10.201836Z","shell.execute_reply.started":"2021-12-08T17:05:10.197052Z","shell.execute_reply":"2021-12-08T17:05:10.201032Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Kaggle Environment Prepration\n#update kaggle env\nimport sys\n#you may update the environment that allow you to run the whole code\n!{sys.executable} -m pip install --upgrade scikit-learn==\"0.24.2\"","metadata":{"_kg_hide-output":"True","papermill":{"duration":16.717311,"end_time":"2021-12-06T05:16:10.330254","exception":false,"start_time":"2021-12-06T05:15:53.612943","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-12-08T17:05:10.203053Z","iopub.execute_input":"2021-12-08T17:05:10.203743Z","iopub.status.idle":"2021-12-08T17:05:27.73277Z","shell.execute_reply.started":"2021-12-08T17:05:10.203696Z","shell.execute_reply":"2021-12-08T17:05:27.731797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#record this information if you need to run the Kernel internally\nimport sklearn; sklearn.show_versions() ","metadata":{"papermill":{"duration":1.227732,"end_time":"2021-12-06T05:16:11.586619","exception":false,"start_time":"2021-12-06T05:16:10.358887","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-12-08T17:05:27.735994Z","iopub.execute_input":"2021-12-08T17:05:27.737231Z","iopub.status.idle":"2021-12-08T17:05:28.906737Z","shell.execute_reply.started":"2021-12-08T17:05:27.737174Z","shell.execute_reply":"2021-12-08T17:05:28.905704Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.028646,"end_time":"2021-12-06T05:16:11.643405","exception":false,"start_time":"2021-12-06T05:16:11.614759","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Exploratory Data Analysis (EDA)\n## General Structure\nSan Francisco Crime dataset includes <b>9</b> columns and <b>878049</b> rows.\nThere are <b>2</b> different data types as follows: *object, float64*.","metadata":{"papermill":{"duration":0.028039,"end_time":"2021-12-06T05:16:11.699706","exception":false,"start_time":"2021-12-06T05:16:11.671667","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"Let's review the dataset description:\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>count</th>\n      <th>mean</th>\n      <th>std</th>\n      <th>min</th>\n      <th>25%</th>\n      <th>50%</th>\n      <th>75%</th>\n      <th>max</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>X</th>\n      <td>878049.0</td>\n      <td>-122.422616</td>\n      <td>0.030354</td>\n      <td>-122.513642</td>\n      <td>-122.432952</td>\n      <td>-122.416420</td>\n      <td>-122.406959</td>\n      <td>-120.5</td>\n    </tr>\n    <tr>\n      <th>Y</th>\n      <td>878049.0</td>\n      <td>37.771020</td>\n      <td>0.456893</td>\n      <td>37.707879</td>\n      <td>37.752427</td>\n      <td>37.775421</td>\n      <td>37.784369</td>\n      <td>90.0</td>\n    </tr>\n  </tbody>\n</table>","metadata":{"papermill":{"duration":0.029087,"end_time":"2021-12-06T05:16:11.757451","exception":false,"start_time":"2021-12-06T05:16:11.728364","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"# Finding Intresting Datapoints\nLet's process each field by their histogram frequency and check if there is any intresting data point.\n\nThere are <b>52</b> number of intresting values in the following columns.\nThe below table shows each <b>Value</b> of each <b>Field</b>(column) with their total frequencies, <b>Lower</b> shows the lower frequency of normal distribution, <b>Upper</b> shows the upper bound frequency of normal distribution, and <b>Criteria</b> shows if the frequnecy passed <b>Upper bound</b> or <b>Lower bound</b>.\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Field</th>\n      <th>Value</th>\n      <th>Frequency</th>\n      <th>Lower</th>\n      <th>Upper</th>\n      <th>Criteria</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Dates</td>\n      <td>2013-12-01 00:01:00</td>\n      <td>44</td>\n      <td>1.0000</td>\n      <td>40.0000</td>\n      <td>Upper</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Dates</td>\n      <td>2013-11-01 00:01:00</td>\n      <td>52</td>\n      <td>1.0000</td>\n      <td>40.0000</td>\n      <td>Upper</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Dates</td>\n      <td>2013-06-01 12:00:00</td>\n      <td>46</td>\n      <td>1.0000</td>\n      <td>40.0000</td>\n      <td>Upper</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Dates</td>\n      <td>2013-05-01 00:01:00</td>\n      <td>51</td>\n      <td>1.0000</td>\n      <td>40.0000</td>\n      <td>Upper</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Dates</td>\n      <td>2012-06-01 00:01:00</td>\n      <td>46</td>\n      <td>1.0000</td>\n      <td>40.0000</td>\n      <td>Upper</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Dates</td>\n      <td>2012-01-01 00:01:00</td>\n      <td>94</td>\n      <td>1.0000</td>\n      <td>40.0000</td>\n      <td>Upper</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>Dates</td>\n      <td>2011-06-01 00:01:00</td>\n      <td>50</td>\n      <td>1.0000</td>\n      <td>40.0000</td>\n      <td>Upper</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>Dates</td>\n      <td>2011-01-01 00:01:00</td>\n      <td>185</td>\n      <td>1.0000</td>\n      <td>40.0000</td>\n      <td>Upper</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>Dates</td>\n      <td>2010-12-01 00:01:00</td>\n      <td>41</td>\n      <td>1.0000</td>\n      <td>40.0000</td>\n      <td>Upper</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>Dates</td>\n      <td>2010-11-01 00:01:00</td>\n      <td>51</td>\n      <td>1.0000</td>\n      <td>40.0000</td>\n      <td>Upper</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>Dates</td>\n      <td>2010-08-01 00:01:00</td>\n      <td>55</td>\n      <td>1.0000</td>\n      <td>40.0000</td>\n      <td>Upper</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>Dates</td>\n      <td>2010-06-01 00:01:00</td>\n      <td>56</td>\n      <td>1.0000</td>\n      <td>40.0000</td>\n      <td>Upper</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>Dates</td>\n      <td>2009-11-01 00:01:00</td>\n      <td>42</td>\n      <td>1.0000</td>\n      <td>40.0000</td>\n      <td>Upper</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>Dates</td>\n      <td>2009-10-01 00:01:00</td>\n      <td>45</td>\n      <td>1.0000</td>\n      <td>40.0000</td>\n      <td>Upper</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>Dates</td>\n      <td>2009-09-01 00:01:00</td>\n      <td>46</td>\n      <td>1.0000</td>\n      <td>40.0000</td>\n      <td>Upper</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>Dates</td>\n      <td>2009-05-01 00:01:00</td>\n      <td>41</td>\n      <td>1.0000</td>\n      <td>40.0000</td>\n      <td>Upper</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>Dates</td>\n      <td>2009-04-01 00:01:00</td>\n      <td>45</td>\n      <td>1.0000</td>\n      <td>40.0000</td>\n      <td>Upper</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>Dates</td>\n      <td>2008-11-01 00:01:00</td>\n      <td>51</td>\n      <td>1.0000</td>\n      <td>40.0000</td>\n      <td>Upper</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>Dates</td>\n      <td>2008-10-01 00:01:00</td>\n      <td>45</td>\n      <td>1.0000</td>\n      <td>40.0000</td>\n      <td>Upper</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>Dates</td>\n      <td>2008-06-01 00:01:00</td>\n      <td>48</td>\n      <td>1.0000</td>\n      <td>40.0000</td>\n      <td>Upper</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>Dates</td>\n      <td>2008-05-01 00:01:00</td>\n      <td>41</td>\n      <td>1.0000</td>\n      <td>40.0000</td>\n      <td>Upper</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>Dates</td>\n      <td>2008-04-01 00:01:00</td>\n      <td>53</td>\n      <td>1.0000</td>\n      <td>40.0000</td>\n      <td>Upper</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>Dates</td>\n      <td>2007-07-01 00:01:00</td>\n      <td>44</td>\n      <td>1.0000</td>\n      <td>40.0000</td>\n      <td>Upper</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>Dates</td>\n      <td>2007-06-01 00:01:00</td>\n      <td>61</td>\n      <td>1.0000</td>\n      <td>40.0000</td>\n      <td>Upper</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>Dates</td>\n      <td>2006-12-01 00:01:00</td>\n      <td>43</td>\n      <td>1.0000</td>\n      <td>40.0000</td>\n      <td>Upper</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>Dates</td>\n      <td>2006-11-01 00:01:00</td>\n      <td>46</td>\n      <td>1.0000</td>\n      <td>40.0000</td>\n      <td>Upper</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>Dates</td>\n      <td>2006-07-01 00:01:00</td>\n      <td>51</td>\n      <td>1.0000</td>\n      <td>40.0000</td>\n      <td>Upper</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>Dates</td>\n      <td>2006-06-01 00:01:00</td>\n      <td>58</td>\n      <td>1.0000</td>\n      <td>40.0000</td>\n      <td>Upper</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>Dates</td>\n      <td>2006-05-01 00:01:00</td>\n      <td>43</td>\n      <td>1.0000</td>\n      <td>40.0000</td>\n      <td>Upper</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>Dates</td>\n      <td>2006-01-01 12:00:00</td>\n      <td>63</td>\n      <td>1.0000</td>\n      <td>40.0000</td>\n      <td>Upper</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>Dates</td>\n      <td>2006-01-01 00:01:00</td>\n      <td>136</td>\n      <td>1.0000</td>\n      <td>40.0000</td>\n      <td>Upper</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>Dates</td>\n      <td>2005-07-01 00:01:00</td>\n      <td>49</td>\n      <td>1.0000</td>\n      <td>40.0000</td>\n      <td>Upper</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>Dates</td>\n      <td>2005-06-01 00:01:00</td>\n      <td>50</td>\n      <td>1.0000</td>\n      <td>40.0000</td>\n      <td>Upper</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>Dates</td>\n      <td>2004-10-01 00:01:00</td>\n      <td>44</td>\n      <td>1.0000</td>\n      <td>40.0000</td>\n      <td>Upper</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>Dates</td>\n      <td>2003-11-01 00:01:00</td>\n      <td>43</td>\n      <td>1.0000</td>\n      <td>40.0000</td>\n      <td>Upper</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>Dates</td>\n      <td>2003-10-01 00:01:00</td>\n      <td>44</td>\n      <td>1.0000</td>\n      <td>40.0000</td>\n      <td>Upper</td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td>Dates</td>\n      <td>2003-06-01 00:01:00</td>\n      <td>43</td>\n      <td>1.0000</td>\n      <td>40.0000</td>\n      <td>Upper</td>\n    </tr>\n    <tr>\n      <th>37</th>\n      <td>Category</td>\n      <td>LARCENY/THEFT</td>\n      <td>174900</td>\n      <td>6.0608</td>\n      <td>174714.8716</td>\n      <td>Upper</td>\n    </tr>\n    <tr>\n      <th>38</th>\n      <td>Descript</td>\n      <td>GRAND THEFT FROM LOCKED AUTO</td>\n      <td>60022</td>\n      <td>1.0000</td>\n      <td>57537.8746</td>\n      <td>Upper</td>\n    </tr>\n    <tr>\n      <th>39</th>\n      <td>PdDistrict</td>\n      <td>SOUTHERN</td>\n      <td>157182</td>\n      <td>45212.6936</td>\n      <td>157148.4534</td>\n      <td>Upper</td>\n    </tr>\n    <tr>\n      <th>40</th>\n      <td>Resolution</td>\n      <td>NONE</td>\n      <td>526790</td>\n      <td>51.2656</td>\n      <td>526277.3808</td>\n      <td>Upper</td>\n    </tr>\n    <tr>\n      <th>41</th>\n      <td>Address</td>\n      <td>800 Block of BRYANT ST</td>\n      <td>26533</td>\n      <td>1.0000</td>\n      <td>4763.3282</td>\n      <td>Upper</td>\n    </tr>\n    <tr>\n      <th>42</th>\n      <td>Address</td>\n      <td>2000 Block of MISSION ST</td>\n      <td>5097</td>\n      <td>1.0000</td>\n      <td>4763.3282</td>\n      <td>Upper</td>\n    </tr>\n    <tr>\n      <th>43</th>\n      <td>Address</td>\n      <td>800 Block of MARKET ST</td>\n      <td>6581</td>\n      <td>1.0000</td>\n      <td>4763.3282</td>\n      <td>Upper</td>\n    </tr>\n    <tr>\n      <th>44</th>\n      <td>X</td>\n      <td>-122.403405</td>\n      <td>26354</td>\n      <td>1.0000</td>\n      <td>3100.4312</td>\n      <td>Upper</td>\n    </tr>\n    <tr>\n      <th>45</th>\n      <td>X</td>\n      <td>-122.407634</td>\n      <td>3170</td>\n      <td>1.0000</td>\n      <td>3100.4312</td>\n      <td>Upper</td>\n    </tr>\n    <tr>\n      <th>46</th>\n      <td>X</td>\n      <td>-122.419658</td>\n      <td>4449</td>\n      <td>1.0000</td>\n      <td>3100.4312</td>\n      <td>Upper</td>\n    </tr>\n    <tr>\n      <th>47</th>\n      <td>X</td>\n      <td>-122.406539</td>\n      <td>3891</td>\n      <td>1.0000</td>\n      <td>3100.4312</td>\n      <td>Upper</td>\n    </tr>\n    <tr>\n      <th>48</th>\n      <td>Y</td>\n      <td>37.775421</td>\n      <td>26354</td>\n      <td>1.0000</td>\n      <td>3100.4312</td>\n      <td>Upper</td>\n    </tr>\n    <tr>\n      <th>49</th>\n      <td>Y</td>\n      <td>37.784189</td>\n      <td>3170</td>\n      <td>1.0000</td>\n      <td>3100.4312</td>\n      <td>Upper</td>\n    </tr>\n    <tr>\n      <th>50</th>\n      <td>Y</td>\n      <td>37.764221</td>\n      <td>4449</td>\n      <td>1.0000</td>\n      <td>3100.4312</td>\n      <td>Upper</td>\n    </tr>\n    <tr>\n      <th>51</th>\n      <td>Y</td>\n      <td>37.756486</td>\n      <td>3891</td>\n      <td>1.0000</td>\n      <td>3100.4312</td>\n      <td>Upper</td>\n    </tr>\n  </tbody>\n</table>\n\n\n\nFor example, in the <b>Dates</b> column the value of <b>2013-12-01 00:01:00</b> has <b>44</b> repeatation but this number is not between Lower bound(1.0) and Upper bound(40.0).\n\n\nLet     $C_0=2013-12-01 00:01:00$   and   $Freq(C_0)=44$     ,   $Upper(C_0)=40.0$     ,   $Lower(C_0)=1.0$\n\n$Freq(C_0) > Upper(C_0)$.","metadata":{"papermill":{"duration":0.029029,"end_time":"2021-12-06T05:16:11.815002","exception":false,"start_time":"2021-12-06T05:16:11.785973","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"# Input Dataset","metadata":{"papermill":{"duration":0.02804,"end_time":"2021-12-06T05:16:11.871742","exception":false,"start_time":"2021-12-06T05:16:11.843702","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def __load__data(__training_path, __test_path, concat=False):\n\t\"\"\"load data as input dataset\n\tparams: __training_path: the training path of input dataset\n\tparams: __test_path: the path of test dataset\n\tparams: if it is True, then it will concatinate the training and test dataset as output\n\treturns: generate final loaded dataset as dataset, input and test\n\t\"\"\"\n\t# LOAD DATA\n\timport pandas as pd\n\t__train_dataset = pd.read_csv(__training_path, delimiter=',')\n\t__test_dataset = pd.read_csv(__test_path, delimiter=',')\n\treturn __train_dataset, __test_dataset\n__train_dataset, __test_dataset = __load__data(__training_path, __test_path, concat=True)\n__train_dataset.head()","metadata":{"_kg_hide-input":false,"_kg_hide-output":false,"papermill":{"duration":6.177827,"end_time":"2021-12-06T05:16:18.077824","exception":false,"start_time":"2021-12-06T05:16:11.899997","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-12-08T17:05:28.908853Z","iopub.execute_input":"2021-12-08T17:05:28.909214Z","iopub.status.idle":"2021-12-08T17:05:35.005999Z","shell.execute_reply.started":"2021-12-08T17:05:28.909162Z","shell.execute_reply":"2021-12-08T17:05:35.005088Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print (\"The figures below present the distributions of some numerical, categorical and target columns.\")\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\n%matplotlib inline\n\n# numerical columns\nnumerical_columns_for_visualization = ['X', 'Y']\nfig, axes = plt.subplots(1, 2, figsize=(15, 5))\nfig.suptitle('Distribution Plots for Numerical Columns')\naxes = axes.ravel()\nfor index,col in enumerate(numerical_columns_for_visualization): \n    sns.histplot(__train_dataset[col],ax=axes[index])\n    axes[index].tick_params(axis='x', rotation=90)\n    axes[index].set_title('Distribution of %s' %col )\nfig.tight_layout();\n\n# categorical columns\ncategorical_columns_for_visualization = ['DayOfWeek', 'PdDistrict', 'Resolution']\nfig, axes = plt.subplots(1, 3, figsize=(15, 8))\nfig.suptitle('Distribution Plots for Categorical Columns')\naxes = axes.ravel()\nfor index,col in enumerate(categorical_columns_for_visualization): \n    sns.histplot(__train_dataset[col],ax=axes[index])\n    axes[index].tick_params(axis='x', rotation=90)\n    axes[index].set_title('Distribution of %s' %col )\nfig.tight_layout();\n    \n# target columns\ntarget_column_for_visualization = 'Category'\nplt.figure(figsize=(15,15))\nsns.histplot(__train_dataset[target_column_for_visualization])\nplt.xticks(rotation=90)\nplt.title('Distribution Plots for Target Column')\nfig.tight_layout();","metadata":{"execution":{"iopub.status.busy":"2021-12-08T17:05:35.007947Z","iopub.execute_input":"2021-12-08T17:05:35.008239Z","iopub.status.idle":"2021-12-08T17:09:00.333661Z","shell.execute_reply.started":"2021-12-08T17:05:35.008201Z","shell.execute_reply":"2021-12-08T17:09:00.33256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# STORE SUBMISSION RELEVANT COLUMNS\n__test_dataset_submission_columns = __test_dataset['Id']","metadata":{"_kg_hide-input":false,"_kg_hide-output":false,"papermill":{"duration":0.040985,"end_time":"2021-12-06T05:16:18.147637","exception":false,"start_time":"2021-12-06T05:16:18.106652","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-12-08T17:09:00.335284Z","iopub.execute_input":"2021-12-08T17:09:00.335555Z","iopub.status.idle":"2021-12-08T17:09:00.340679Z","shell.execute_reply.started":"2021-12-08T17:09:00.335519Z","shell.execute_reply":"2021-12-08T17:09:00.340032Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Discard Irrelevant Columns\nIn the given input dataset there are <b>2</b> columns that can be removed as follows:* Descript,Resolution *.","metadata":{"papermill":{"duration":0.031124,"end_time":"2021-12-06T05:16:18.208822","exception":false,"start_time":"2021-12-06T05:16:18.177698","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# DISCARD IRRELEVANT COLUMNS\n__train_dataset.drop(['Descript', 'Resolution'], axis=1, inplace=True)","metadata":{"_kg_hide-input":false,"_kg_hide-output":false,"papermill":{"duration":0.090512,"end_time":"2021-12-06T05:16:18.329415","exception":false,"start_time":"2021-12-06T05:16:18.238903","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-12-08T17:09:00.34241Z","iopub.execute_input":"2021-12-08T17:09:00.342681Z","iopub.status.idle":"2021-12-08T17:09:00.412789Z","shell.execute_reply.started":"2021-12-08T17:09:00.342649Z","shell.execute_reply":"2021-12-08T17:09:00.411667Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# PREPROCESSING-1\n_DATE_COLUMNS = ['Dates']\nfor _col in _DATE_COLUMNS:\n    train_date_col = pd.to_datetime(__train_dataset[_col], errors='coerce')\n    __train_dataset[\"year\"] = train_date_col.dt.year\n    __train_dataset[\"month\"] = train_date_col.dt.month\n    __train_dataset[\"day\"] = train_date_col.dt.day\n    __train_dataset.drop(_col, axis=1, inplace=True)\n    test_date_col = pd.to_datetime(__test_dataset[_col], errors='coerce')\n    __test_dataset[\"year\"] = test_date_col.dt.year\n    __test_dataset[\"month\"] = test_date_col.dt.month\n    __test_dataset[\"day\"] = test_date_col.dt.day\n    __test_dataset.drop(_col, axis=1, inplace=True)","metadata":{"_kg_hide-input":false,"_kg_hide-output":false,"papermill":{"duration":1.769007,"end_time":"2021-12-06T05:16:20.128003","exception":false,"start_time":"2021-12-06T05:16:18.358996","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-12-08T17:09:00.414171Z","iopub.execute_input":"2021-12-08T17:09:00.414429Z","iopub.status.idle":"2021-12-08T17:09:02.441379Z","shell.execute_reply.started":"2021-12-08T17:09:00.414397Z","shell.execute_reply":"2021-12-08T17:09:02.440422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print (\"The figure below shows the Pearson Correlation of feature heatmap.\")\nfrom pandas.api.types import is_numeric_dtype\nnumerical_columns = [] \nfor var in __train_dataset.columns:\n    if is_numeric_dtype(__train_dataset[var]): \n        numerical_columns.append(var)\nif target_column_for_visualization not in numerical_columns:\n    numerical_columns.append(target_column_for_visualization)\n__train_dataset_heatmap = __train_dataset[numerical_columns].copy()\n__train_dataset_heatmap[target_column_for_visualization] = __train_dataset_heatmap[target_column_for_visualization].factorize()[0]\n\ncolormap = plt.cm.RdBu\nplt.figure(figsize=(10,10))\nplt.title('Pearson Correlation HeatMap of Features', y=1.05, size=15)\nsns.heatmap(__train_dataset_heatmap.corr(),linewidths=0.1,vmax=1.0, square=True, cmap=colormap, linecolor='white', annot=True)","metadata":{"execution":{"iopub.status.busy":"2021-12-08T17:09:02.44504Z","iopub.execute_input":"2021-12-08T17:09:02.44581Z","iopub.status.idle":"2021-12-08T17:09:03.240802Z","shell.execute_reply.started":"2021-12-08T17:09:02.445748Z","shell.execute_reply":"2021-12-08T17:09:03.239785Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Target Column\nThe target column is the value which we need to predict.\nTherefore, we need to detach the target columns in prediction.\nNote that if we don't drop this fields, it will generate a model with high accuracy on training and worst accuracy on test (because the value in test dataset is Null).\nHere is the list of *target column*: <b>Category</b>","metadata":{"papermill":{"duration":0.029313,"end_time":"2021-12-06T05:16:20.186673","exception":false,"start_time":"2021-12-06T05:16:20.15736","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# DETACH TARGET\n__feature_train = __train_dataset.drop(['Category'], axis=1)\n__target_train =__train_dataset['Category']\n__feature_test = __test_dataset.drop(['Id'], axis=1)\nfrom sklearn.preprocessing import OneHotEncoder\n_CATEGORICAL_COLS = ['Address', 'PdDistrict', 'DayOfWeek']\n__col_indices = [__feature_train.columns.get_loc(col) for col in _CATEGORICAL_COLS]\n_ohe = OneHotEncoder(handle_unknown='ignore')\nfrom sklearn.compose import ColumnTransformer\n__ct = ColumnTransformer([(\"ohe\", _ohe, __col_indices)], remainder = 'passthrough')\n__feature_train = __ct.fit_transform(__feature_train)\n__feature_test = __ct.transform(__feature_test)","metadata":{"_kg_hide-input":false,"_kg_hide-output":false,"papermill":{"duration":4.128639,"end_time":"2021-12-06T05:16:24.344752","exception":false,"start_time":"2021-12-06T05:16:20.216113","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-12-08T17:09:03.255273Z","iopub.execute_input":"2021-12-08T17:09:03.255575Z","iopub.status.idle":"2021-12-08T17:09:07.817483Z","shell.execute_reply.started":"2021-12-08T17:09:03.255531Z","shell.execute_reply":"2021-12-08T17:09:07.816265Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training Model and Prediction\nFirst, we will train a model based on preprocessed values of training data set.\nSecond, let's predict test values based on the trained model.","metadata":{"papermill":{"duration":0.028942,"end_time":"2021-12-06T05:16:24.4033","exception":false,"start_time":"2021-12-06T05:16:24.374358","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"## Cat Boost Classifier\nWe will use *CatBoostClassifier* which is Training and applying models for the classification problems. It provides compatibility with the scikit-learn tools.\nMore detail about *CatBoostClassifier* can be found [here](https://catboost.ai/en/docs/concepts/python-reference_catboostclassifier).","metadata":{"papermill":{"duration":0.029028,"end_time":"2021-12-06T05:16:24.461392","exception":false,"start_time":"2021-12-06T05:16:24.432364","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"We need to predict the probabilities for each group, therefore we will use *_model.predict_proba()* features  that generates probability for each group.","metadata":{"papermill":{"duration":0.034603,"end_time":"2021-12-06T05:16:24.527723","exception":false,"start_time":"2021-12-06T05:16:24.49312","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# MODEL\nimport numpy as np\nfrom catboost import CatBoostClassifier\n__model = CatBoostClassifier()\n__model.fit(__feature_train, __target_train)\n__y_pred = __model.predict_proba(__feature_test)","metadata":{"_kg_hide-input":false,"_kg_hide-output":false,"papermill":{"duration":7100.592822,"end_time":"2021-12-06T07:14:45.163152","exception":false,"start_time":"2021-12-06T05:16:24.57033","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-12-08T17:09:07.818833Z","iopub.execute_input":"2021-12-08T17:09:07.819081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission File\nWe have to maintain the target columns in \"submission.csv\" which will be submitted as our prediction results.","metadata":{"papermill":{"duration":0.325567,"end_time":"2021-12-06T07:14:45.816575","exception":false,"start_time":"2021-12-06T07:14:45.491008","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# SUBMISSION\nsubmission = pd.DataFrame(columns=['Id'], data=__test_dataset_submission_columns)\nsubmission = pd.concat([submission, pd.DataFrame(__y_pred, columns=[\"ARSON\", \"ASSAULT\", \"BAD CHECKS\", \"BRIBERY\", \"BURGLARY\", \"DISORDERLY CONDUCT\",\n\"DRIVING UNDER THE INFLUENCE\", \"DRUG/NARCOTIC\", \"DRUNKENNESS\", \"EMBEZZLEMENT\", \"EXTORTION\",\n\"FAMILY OFFENSES\", \"FORGERY/COUNTERFEITING\", \"FRAUD\", \"GAMBLING\", \"KIDNAPPING\", \"LARCENY/THEFT\",\n\"LIQUOR LAWS\", \"LOITERING\", \"MISSING PERSON\", \"NON-CRIMINAL\", \"OTHER OFFENSES\", \"PORNOGRAPHY/OBSCENE MAT\",\n\"PROSTITUTION\", \"RECOVERED VEHICLE\", \"ROBBERY\", \"RUNAWAY\", \"SECONDARY CODES\", \"SEX OFFENSES FORCIBLE\",\n\"SEX OFFENSES NON FORCIBLE\", \"STOLEN PROPERTY\", \"SUICIDE\", \"SUSPICIOUS OCC\", \"TREA\", \"TRESPASS\",\n\"VANDALISM\", \"VEHICLE THEFT\", \"WARRANTS\", \"WEAPON LAWS\"])], axis=1)\nsubmission.head()","metadata":{"_kg_hide-input":false,"_kg_hide-output":false,"papermill":{"duration":0.559857,"end_time":"2021-12-06T07:14:46.708188","exception":false,"start_time":"2021-12-06T07:14:46.148331","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# save submission file\nsubmission.to_csv(\"kaggle_submission.csv\", index=False)","metadata":{"_kg_hide-input":false,"_kg_hide-output":false,"papermill":{"duration":78.816595,"end_time":"2021-12-06T07:16:05.859241","exception":false,"start_time":"2021-12-06T07:14:47.042646","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]}]}