{"cells":[{"metadata":{},"cell_type":"markdown","source":"# San Francisco Crime Classification from a top ranker"},{"metadata":{},"cell_type":"markdown","source":"본 notebook은 Yannis Pappas 커널을 참고하여 작성했습니다. (https://www.kaggle.com/yannisp/sf-crime-analysis-prediction)"},{"metadata":{},"cell_type":"markdown","source":"## Data Science Life Cycle\nData Science Life Cycle은 아래의 단계로 구성되어 있으며, 본 경진 대회에서도 아래의 전체 Life Cycle대로 진행할 예정입니다.\n1. 데이터 품질을 향상시키기 위한 Data Wrangling\n2. 탐색적 데이터 분석 (EDA)\n3. 현재 Feature들을 기반으로 추가적인 Feature들을 만드는 Feature Engineering\n4. (필요 시) 데이터 정규화 및 변환\n5. 모델 성능 측정을 위한 훈련 데이터, 테스트 데이터 생성 및 파라미터 조정\n6. 모델 선택 및 평가, 결과 예측을 위한 모델 생성"},{"metadata":{"scrolled":false,"trusted":false},"cell_type":"code","source":"import pandas as pd\nfrom shapely.geometry import  Point\nimport geopandas as gpd\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nimport seaborn as sns\nfrom matplotlib import cm\nimport urllib.request\nimport shutil\nimport zipfile\nimport os\nimport re\nimport contextily as ctx\nimport geoplot as gplt\nimport lightgbm as lgb\nimport eli5\nfrom eli5.sklearn import PermutationImportance\nfrom lightgbm import LGBMClassifier\nfrom matplotlib import pyplot as plt\nfrom pdpbox import pdp, get_dataset, info_plots\nimport shap","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train = pd.read_csv('data/train.csv', parse_dates=['Dates'])\ntest = pd.read_csv('data/test.csv', parse_dates=['Dates'], index_col='Id')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train.Dates.describe()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"train.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"훈련 데이터는 2003.1.6.부터 2015.5.13.까지의 범죄를 담고 있으며, 총 9개의 features가 있습니다."},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Dates - 범죄가 일어난 일시\n- Category - 범죄 유형 (이 값이 Target variable임)\n- Descript - 범죄에 대한 자세한 설명\n- DayOfWeek - 요일\n- PdDistrict - 경찰 관할 지역 명칭\n- Resolution - 범죄 해결 여부\n- Address - 범죄 발생 주소\n- X - 경도(Longitude)\n- Y - 위도(Latitude)"},{"metadata":{"trusted":false},"cell_type":"code","source":"train.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"object type, 즉 string type은 카테고리형 데이터이기 때문에 추후 인코딩이 필요합니다."},{"metadata":{"trusted":false},"cell_type":"code","source":"train.duplicated().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"2323개의 중복행이 존재해 제거해줘야 합니다."},{"metadata":{"trusted":false},"cell_type":"code","source":"def create_gdf(df):\n    gdf = df.copy()\n    gdf['Coordinates'] = list(zip(gdf.X, gdf.Y))\n    gdf['Coordinates'] = gdf['Coordinates'].apply(Point)\n    gdf = gpd.GeoDataFrame(gdf, geometry='Coordinates',\n                          crs={'init': 'epsg:4326'})\n    return gdf\n\ntrain_gdf = create_gdf(train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n\nf, ax = plt.subplots(1, figsize=(9,9))\nax = world.plot(color='white', edgecolor='black', axes=ax)\ntrain_gdf.plot(ax=ax, color='red');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"캘리포니아 지역의 범죄에 대한 데이터인데 엉뚱한 곳에 찍혀 있는 데이터가 있습니다. 어떤 데이터인지 확인해보겠습니다."},{"metadata":{"trusted":false},"cell_type":"code","source":"train_gdf[train_gdf.Y > 70]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train_gdf[train_gdf.Y > 70].count()[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"총 67개의 데이터의 좌표가 잘못되어 있습니다.\n우선, 중복행은 제거를 시켜줍니다. 그리고 67개의 outlier는 평균값으로 대체합니다."},{"metadata":{"trusted":false},"cell_type":"code","source":"train.drop_duplicates(inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train.replace({'X': -120.5, 'Y': 90.0}, np.NaN, inplace=True)\ntest.replace({'X': -120.5, 'Y': 90.0}, np.NaN, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"imputer = SimpleImputer(strategy='mean')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"for district in train['PdDistrict'].unique():\n    train.loc[train['PdDistrict']==district, ['X', 'Y']] = imputer.fit_transform(\n        train.loc[train['PdDistrict']==district, ['X', 'Y']])\n    # fit은 train 데이터로 해주었기때문에 transform만 적용\n    test.loc[test['PdDistrict']==district, ['X', 'Y']] = imputer.transform(\n        test.loc[test['PdDistrict']==district, ['X', 'Y']])    \n    \ntrain_gdf = create_gdf(train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 날짜와 요일"},{"metadata":{"trusted":false},"cell_type":"code","source":"train['Date'] = train['Dates'].dt.date\ntrain['Year'] = train['Dates'].dt.year\ntrain['Month'] = train['Dates'].dt.month\ntrain['Day'] = train['Dates'].dt.day\ntrain['Hour'] = train['Dates'].dt.hour","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"year_series = train.groupby('Year').count().iloc[:, 0]\ng = sns.barplot(x=year_series.index, y=year_series)\ng.set_xticklabels(g.get_xticklabels(), rotation=45);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"연도에 따른 범죄수입니다. 2003년부터 2014년까지는 범죄수가 거의 유사했지만, 2015년에 급감한 수치를 보입니다."},{"metadata":{"trusted":false},"cell_type":"code","source":"month_series = train.groupby('Month').count().iloc[:, 0]\nsns.barplot(x=month_series.index, y=month_series);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"월에 따른 범죄수입니다. 8월, 12월에 범죄수가 가장 적었고, 5월, 10월에 가장 많았습니다. 날씨가 안 좋을 때 (더울 때 혹은 추울 때)는 범죄도 적고, 날씨가 좋을 때는 범죄도 많다는 것을 알 수 있습니다."},{"metadata":{"trusted":false},"cell_type":"code","source":"hour_series = train.groupby('Hour').count().iloc[:, 0]\nsns.barplot(x=hour_series.index, y=hour_series);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"시간에 따른 범죄수입니다. 예상대로 모두가 잠든 새벽 시간에 범죄가 가장 적고, 18시가 가장 많습니다. 아침부터 18시까지 점차 증가하는 추세를 보이는데 12시에 유독 많은 것을 볼 수 있습니다. 점심 시간, 저녁 시간 등과 관련이 있지 않나 추측해봅니다."},{"metadata":{"trusted":false},"cell_type":"code","source":"palette = sns.color_palette()\n\nplt.figure(figsize=(10, 6))\ndate_count = train.groupby('Date').count().iloc[:, 0]\nsns.kdeplot(data=date_count, shade=True)\nplt.axvline(x=date_count.median(), ymax=0.95, linestyle='--', color=palette[1])\nplt.annotate('Median ' + str(date_count.median()),\n             xy=(date_count.median(), 0.004),\n             xytext=(200, 0.005),\n             arrowprops=dict(arrowstyle='->', color=palette[1], shrinkB=10))\nplt.title('Distribution of number of incidents per day',fontdict={'fontsize':16})\nplt.xlabel('Crime Incidents')\nplt.ylabel('Density')\nplt.legend().remove()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"하루동안 발생하는 범죄 건수는 정규 분포를 그리고 있고, 그 중앙값은 389회입니다. "},{"metadata":{"scrolled":false,"trusted":false},"cell_type":"code","source":"weekday_series = train.groupby('DayOfWeek').count().iloc[:,0]\nweekday_series = weekday_series.reindex([\n    'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday',\n    'Sunday'])\n\nwith sns.axes_style(\"whitegrid\"):\n    f, ax = plt.subplots(1, figsize=(10, 6))\n    sns.barplot(\n        weekday_series.index, (weekday_series.values / weekday_series.values.sum()) * 100,\n        palette=cm.ScalarMappable(cmap='Blues').to_rgba(weekday_series.values))\n\nplt.title('Incidents per Weekday', fontdict={'fontsize':16})\nplt.xlabel('Weekday')\nplt.ylabel('Percent of Incients (%)');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"금요일에 범죄 건수가 가장 많고, 수요일, 토요일, 목요일 등이 그 뒤를 이었습니다"},{"metadata":{},"cell_type":"markdown","source":"### Category"},{"metadata":{"trusted":false},"cell_type":"code","source":"category_counts = train.groupby('Category').count().iloc[:, 0].sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"category_counts","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# OTHER OFFENSES를 제일 아래 두기 위해\ncategory_counts = category_counts.reindex(\n    np.append(np.delete(category_counts.index, 1), 'OTHER OFFENSES'))","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":false},"cell_type":"code","source":"with sns.axes_style(\"whitegrid\"):\n    f, ax = plt.subplots(1, figsize=(10, 10))\n    sns.barplot(\n        category_counts.values / category_counts.values.sum() * 100,\n        category_counts.index,\n        orient='h',\n        palette='Blues_d')\nplt.title('Incidents per Crime Category', fontdict={'fontsize': 16})\nplt.xlabel('Incidents (%)');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"절도의 비율이 가장 큽니다."},{"metadata":{},"cell_type":"markdown","source":"### Police District"},{"metadata":{},"cell_type":"markdown","source":"샌프란시스코의 Police Distirct를 불러와 train 데이터와 merging시킵니다."},{"metadata":{"trusted":false},"cell_type":"code","source":"# Downloading the shapefile of the area \nurl = 'https://data.sfgov.org/api/geospatial/wkhw-cjsf?method=export&format=Shapefile'\nwith urllib.request.urlopen(url) as response, open('pd_data.zip', 'wb') as out_file:\n    shutil.copyfileobj(response, out_file)\n    \n# Unzipping it\nwith zipfile.ZipFile('pd_data.zip', 'r') as zip_ref:\n    zip_ref.extractall('pd_data')\n    \n# Loading to a geopandas dataframe\nfor filename in os.listdir('./pd_data/'):\n    if re.match(\".+\\.shp\", filename):\n        pd_districts = gpd.read_file('./pd_data/'+filename)\n        break\n        \n# Merging our train dataset with the geo-dataframe\npd_districts = pd_districts.merge(\n    train.groupby('PdDistrict').count().iloc[:, [0]].rename(\n        columns={'Dates': 'Incidents'}),\n    left_on='district',\n    right_index=True)\n\n# Transforming the coordinate system to Spherical Mercator for\n# compatibility with the tiling background\npd_districts = pd_districts.to_crs({'init': 'epsg:3857'})\n\n# Calculating the incidents per day for every district\ntrain_days = train.groupby('Date').count().shape[0]\npd_districts['inc_per_day'] = pd_districts.Incidents/train_days","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"각 district별 하루 평균 범죄 발생 건수를 시각화합니다."},{"metadata":{"trusted":false},"cell_type":"code","source":"# Ploting the data\nfig, ax = plt.subplots(figsize=(10, 10))\npd_districts.plot(\n    column='inc_per_day',\n    cmap='Reds',\n    alpha=0.6,\n    edgecolor='r',\n    linestyle='-',\n    linewidth=1,\n    legend=True,\n    ax=ax);","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":false},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(10, 10))\npd_districts.plot(\n    column='inc_per_day',\n    cmap='Reds',\n    alpha=0.6,\n    edgecolor='r',\n    linestyle='-',\n    linewidth=1,\n    legend=True,\n    ax=ax);\n\ndef add_basemap(ax, zoom, url='http://tile.stamen.com/terrain/tileZ/tileX/tileY.png'):\n    \"\"\"Function that add the tile background to the map\"\"\"\n    xmin, xmax, ymin, ymax = ax.axis()\n    basemap, extent = ctx.bounds2img(xmin, ymin, xmax, ymax, zoom=zoom, url=url)\n    ax.imshow(basemap, extent=extent, interpolation='bilinear')\n    # restore original x/y limits\n    ax.axis((xmin, xmax, ymin, ymax))\n\n# Adding the background\nadd_basemap(ax, zoom=11, url=ctx.sources.ST_TONER_LITE)\n\n# Adding the name of the districts\nfor index in pd_districts.index:\n    plt.annotate(\n        pd_districts.loc[index].district,\n        (pd_districts.loc[index].geometry.centroid.x,\n         pd_districts.loc[index].geometry.centroid.y),\n        color='#353535',\n        fontsize='large',\n        fontweight='heavy',\n        horizontalalignment='center'\n    )\n\nax.set_axis_off()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Address"},{"metadata":{},"cell_type":"markdown","source":"범죄별로 발생 지역을 시각화해줍니다."},{"metadata":{"trusted":false},"cell_type":"code","source":"crimes = train['Category'].unique().tolist()\ncrimes.remove('TREA')\n\npd_districts = pd_districts.to_crs({'init': 'epsg:4326'})\n\n# geometry containing the union of all geometries \nsf_land = pd_districts.unary_union","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sf_land","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sf_land = gpd.GeoDataFrame(gpd.GeoSeries(sf_land), crs={'init':'epsg:4326'})\nsf_land = sf_land.rename(columns={0:'geometry'}).set_geometry('geometry')","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"fig, ax = plt.subplots(3, 3, sharex=True, sharey=True, figsize=(12,12))\nfor i, crime in enumerate(np.random.choice(crimes, size=9, replace=False)):\n    data = train_gdf.loc[train_gdf['Category'] == crime]\n    ax = fig.add_subplot(3, 3, i+1)\n    gplt.kdeplot(data,\n                shade=True,\n                shade_lowest=False, # False일 때, 0에 가까운 빈도를 가진 구역은 진하기를 표현하지 않음\n                clip=sf_land.geometry,# 주어진 구역만 시각화\n                cmap='Reds',\n                ax=ax)\n    gplt.polyplot(sf_land, ax=ax)\n    ax.set_title(crime)\nplt.suptitle('Geographic Density of Diffenrent Crimes')\nfig.tight_layout(rect=[0, 0.03, 1, 0.95]);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"시간대별 주요 범죄 발생 건수를 시각화해봅니다."},{"metadata":{"trusted":false},"cell_type":"code","source":"# as_index=False: Hour, Date, Category를 index로 지정하지 않음\ndata = train.groupby(['Hour', 'Date', 'Category'],\n                    as_index=False).count().iloc[:, :4]\ndata","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"data.rename(columns={'Dates': 'Incidents'}, inplace=True)\ndata = data.groupby(['Hour', 'Category'], as_index=False).mean()\ndata = data.loc[data['Category'].isin(\n    ['ROBBERY', 'GAMBLING', 'BUGLARY', 'ARSON', 'PROSTITUTION'])]\n\ndata","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sns.set_style('whitegrid')\nfig, ax = plt.subplots(figsize=(14,4))\nax = sns.lineplot(data=data, x='Hour', y='Incidents', hue='Category')\nax.legend(loc='upper center', bbox_to_anchor=(0.5, 1.15), ncol=6)\nplt.suptitle('Average number of incidents per hour')\nfig.tight_layout(rect=[0, 0, 1, 0.95])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"도박은 새벽부터 다음날 아침까지 많이 발생합니다. 매춘은 저녁시간부터 밤새 많이 발생하며, 이른 아침부터 저녁까지 꾸준히 증가하는 것을 볼 수 있습니다."},{"metadata":{},"cell_type":"markdown","source":"### Naive Prediction"},{"metadata":{"trusted":false},"cell_type":"code","source":"naive_vals = train.groupby('Category').count().iloc[:, 0] / train.shape[0]\nn_rows = test.shape[0]\n\nnaive_vals","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"submission = pd.DataFrame(\n    np.repeat(np.array(naive_vals), n_rows).reshape(39, n_rows).transpose(),\n    columns=naive_vals.index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"submission","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"train 데이터에서 각 Category의 비율을 test 데이터에 그대로 넣은 것입니다. 모든 row의 값은 동일합니다. submission했을 때 score는 2.68015입니다."},{"metadata":{},"cell_type":"markdown","source":"### Methodology"},{"metadata":{},"cell_type":"markdown","source":"#### Data Wranling"},{"metadata":{},"cell_type":"markdown","source":"2323개의 중복행과 67개의 잘못된 위도, 경도 값이 있었습니다. 이는 이미 위에서 처리했습니다."},{"metadata":{},"cell_type":"markdown","source":"#### Feature Engineering"},{"metadata":{"trusted":false},"cell_type":"code","source":"def feature_engineering(data):\n    # object type -> datetime type으로\n    data['Date'] = pd.to_datetime(data['Dates'].dt.date) \n    # timedelta type -> int type으로\n    data['n_days'] = (\n        data['Date'] - data['Date'].min()).apply(lambda x: x.days)\n    data['Day'] = data['Dates'].dt.day\n    data['DayOfWeek'] = data['Dates'].dt.weekday\n    data['Month'] = data['Dates'].dt.month\n    data['Year'] = data['Dates'].dt.year\n    data['Hour'] = data['Dates'].dt.hour\n    data['Minute'] = data['Dates'].dt.minute\n    data['Block'] = data['Address'].str.contains('block', case=False)\n    \n    data.drop(columns=['Dates', 'Date', 'Address'], inplace=True)\n    \n    return data","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train = feature_engineering(train)\ntrain.drop(columns=['Descript', 'Resolution'], inplace=True)\ntest = feature_engineering(test)\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Descript와 Resolution은 train 데이터에만 있으므로 예측하는데 필요없는 feature입니다. 분석을 위한 feature가 되기 위해서는 train 데이터에도 test 데이터에도 모두 존재해야 합니다."},{"metadata":{},"cell_type":"markdown","source":"#### Feature Scaling"},{"metadata":{},"cell_type":"markdown","source":"Tree-based model에서는 feature scaling이 따로 필요없습니다."},{"metadata":{},"cell_type":"markdown","source":"#### Feature Selection"},{"metadata":{},"cell_type":"markdown","source":"Feature engineering 후 총 11개의 feature가 남았습니다. "},{"metadata":{"trusted":false},"cell_type":"code","source":"PdDistrict_le = LabelEncoder()\ntrain['PdDistrict'] = PdDistrict_le.fit_transform(train['PdDistrict'])\ntest['PdDistrict'] = PdDistrict_le.transform(test['PdDistrict'])\n\nCategory_le = LabelEncoder()\ny = Category_le.fit_transform(train.pop('Category'))\n\ntrain_X, val_X, train_y, val_y = train_test_split(train, y)\nmodel = LGBMClassifier(objective='multiclass', num_class=39).fit(train_X, train_y)\n\n# 하나의 column을 섞어 성능을 구했을 때 성능 감소량이 그 feature의 중요도임\nperm = PermutationImportance(model).fit(val_X, val_y)\neli5.show_weights(perm, feature_names=val_X.columns.tolist())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Permutation Importance: 이미 훈련된 모델에서 어떤 feature가 중요한지 판단하는 지표입니다. 하나의 feature를 섞고, 나머지 feature는 그대로 둔 채 성능을 평가합니다. 성능의 감소치만큼 해당 feature가 중요하다는 것을 의미합니다. 같은 방식으로 모든 feature를 섞어가며 해당 feature의 중요도를 측정합니다. 위 도표는 feature의 중요도를 순서대로 나타낸 표입니다.\n결론적으로 Permutation Importance는 특정 feature가 예측 정확도에 얼마나 많은 영향을 미치는가 판단할 수 있는 지표입니다. 하지만 예측을 더 정확하게 하는 방향으로 영향을 미치는지 더 부정확하게 하는 방향으로 영향을 미치는지 (즉, direction)은 알 수가 없습니다."},{"metadata":{},"cell_type":"markdown","source":"## Building Model"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Loading the data\ntrain = pd.read_csv('./data/train.csv', parse_dates=['Dates'])\ntest = pd.read_csv('./data/test.csv', parse_dates=['Dates'], index_col='Id')\n\n# Data cleaning\ntrain.drop_duplicates(inplace=True)\ntrain.replace({'X': -120.5, 'Y': 90.0}, np.NaN, inplace=True)\ntest.replace({'X': -120.5, 'Y': 90.0}, np.NaN, inplace=True)\n\nimp = SimpleImputer(strategy='mean')\n\nfor district in train['PdDistrict'].unique():\n    train.loc[train['PdDistrict'] == district, ['X', 'Y']] = imp.fit_transform(\n        train.loc[train['PdDistrict'] == district, ['X', 'Y']])\n    test.loc[test['PdDistrict'] == district, ['X', 'Y']] = imp.transform(\n        test.loc[test['PdDistrict'] == district, ['X', 'Y']])\ntrain_data = lgb.Dataset(\n    train, label=y, categorical_feature=['PdDistrict'], free_raw_data=False)\n\n# Feature Engineering\ntrain = feature_engineering(train)\ntrain.drop(columns=['Descript','Resolution'], inplace=True)\ntest = feature_engineering(test)\n\n# Encoding the Categorical Variables\nle1 = LabelEncoder()\ntrain['PdDistrict'] = le1.fit_transform(train['PdDistrict'])\ntest['PdDistrict'] = le1.transform(test['PdDistrict'])\n\nle2 = LabelEncoder()\nX = train.drop(columns=['Category'])\ny= le2.fit_transform(train['Category'])\n\n# Creating the model\ntrain_data = lgb.Dataset(\n    X, label=y, categorical_feature=['PdDistrict'])\n\nparams = {'boosting':'gbdt',\n          'objective':'multiclass',\n          'num_class':39,\n          'max_delta_step':0.9,\n          'min_data_in_leaf': 21,\n          'learning_rate': 0.4,\n          'max_bin': 465,\n          'num_leaves': 41\n         }\n\nbst = lgb.train(params, train_data, 100)\n\npredictions = bst.predict(test)\n\n# Submitting the results\nsubmission = pd.DataFrame(\n    predictions,\n    columns=le2.inverse_transform(np.linspace(0, 38, 39, dtype='int16')),\n    index=test.index)\nsubmission.to_csv(\n    'LGBM_final.csv', index_label='Id')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":1}