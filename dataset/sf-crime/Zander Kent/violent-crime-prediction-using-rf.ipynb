{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"import pandas as pd\nimport numpy as np\nimport pylab as plt\nfrom datetime import datetime\n#Parse the dates as datetime instead of an object\ndateparse = lambda x: pd.datetime.strptime(x, '%Y-%m-%d %H:%M:%S')\ntraining_data = pd.read_csv(\"../input/train.csv\",   parse_dates=['Dates'], date_parser=dateparse)\ntraining_data.head()"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"#Selects only the violent crimes from the list of crimes\ntraining_data = (training_data.loc[training_data['Category'].isin(['ASSAULT','ROBBERY','SEX OFFENSES FORCIBLE'])])\ntraining_data.head()"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"#Make new columns for each year, month, day, hour and minute since a datetime is too unique and\n#is not useful for a randomforest classifier\ntraining_data['Year'] = training_data['Dates'].dt.year\ntraining_data['Month'] = training_data['Dates'].dt.month\ntraining_data['Day'] = training_data['Dates'].dt.day\ntraining_data['Hour'] = training_data['Dates'].dt.hour\ntraining_data['Minute'] = training_data['Dates'].dt.minute\ntraining_data.head()"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"#Check to see what are object types in the data so we can change them into an integer for\n#the RF classifier\ntraining_data.dtypes[training_data.dtypes.map(lambda x: x == 'object')]"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"#We are going to drop the columns that we will not be given in the testing data as well as the address\n#The address is too specific so we will get rid of it.\ntraining_data = training_data.drop('Descript',1)\ntraining_data = training_data.drop('Resolution',1)\ntraining_data = training_data.drop('Address',1)\ntraining_data.head()"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"#Drop the 'Dates' column since we are using the split up version of it\ntraining_data = training_data.drop('Dates',1)\ntraining_data.head()"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"#Since the PdDistric is a object we will be switching it for an integer\n#In order to do this we need to get all the uniqe PdDistrict's in that column\nPdDistrict = sorted(training_data['PdDistrict'].unique())\n#We now need to map then to a dict so we can switch them out in the next step\nPdDistrict_mapping = dict(zip(PdDistrict, range(0, len(PdDistrict) + 1)))\nPdDistrict_mapping"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"#We create a new column for which we use the mapped version of the PdDistrict and use the type int\ntraining_data['PdDistrict_Val'] = training_data['PdDistrict'].map(PdDistrict_mapping).astype(int)\ntraining_data.head()"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"#If we were to do the same thing for the DayofWeek we would get a strange looking list because\n#if we were to sort if in alphbetical order it wouldn't look the same\n#We need to create a variable that has the list sorted the way we want it to look, then sort the\n#training data the way we want it to look.\ns = [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\",\"Sunday\"]\nDayOfWeek = sorted((training_data['DayOfWeek'].unique()), key=s.index)\nprint(DayOfWeek)\n#Then we do the same thing that we did for the PdDistrict for the DayofWeek\nDayOfWeek_mapping = dict(zip(DayOfWeek, range(1, len(DayOfWeek) + 1)))\nprint(DayOfWeek_mapping)\n#We create a new column for which we use the mapped version of the DayOfWeek and use the type int\ntraining_data['DayOfWeek_Val'] = training_data['DayOfWeek'].map(DayOfWeek_mapping).astype(int)\ntraining_data.head()"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"#Check to see what objects are left, so we know what we need to change, and we see that Category is left\ntraining_data.dtypes[training_data.dtypes.map(lambda x: x == 'object')]"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"#We are going to map Category the same way we mapped PdDistrict\nCategory = sorted(training_data['Category'].unique())\nCategory_mapping = dict(zip(Category, range(0, len(Category) + 1)))\n##We create a new column for which we use the mapped version of the Category and use the type int\ntraining_data['Category_val'] = training_data['Category'].map(Category_mapping).astype(int)\ntraining_data.head()"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"#Now we will drop the columns that we changed since we no longer need them\ntraining_data = training_data.drop('DayOfWeek',1)\ntraining_data = training_data.drop('PdDistrict',1)\ntraining_data = training_data.drop('Category',1)\ntraining_data.head()"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"#It is easier to have the target variable at the beginning of the data, so we will do something like\n#this to move it to the front and re-print the dataframe to see if it worked\ncols = training_data.columns.tolist()\ncols = cols[-1:] + cols[:-1]\ntraining_data = training_data[cols]\ntraining_data.head()"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"#We will now import the Random Forest classifier and set the number of estimators to 100\n#and the number of processors that will be used to -1, -1 by default uses every processor that your\n#computer has which makes the classifier much faster than just using one\nfrom sklearn.ensemble import RandomForestClassifier\n\nclf = RandomForestClassifier(n_estimators=100, n_jobs = -1)"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"#If you don't have the hardware to process the entire dataset with this part you can split it and use\n#whatever percent you want to use by switching out the 1 for any decimal\nmsk = np.random.rand(len(training_data)) < 1 #I used 1 for 100% of the data\ntrain = training_data[msk]\n#Checks the size of the training set\ntrain.shape"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# Training data features, skip the first column 'Category'\ntrain_features = train.ix[:, 1:]\n\n# 'Category' column values\ntrain_target = train.ix[:, 0]"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"#Run the model, score it, then print out the accuracy of the Random Forests,\n#this may take some time depending on how good your hardware is\nclf = clf.fit(train_features, train_target)\nscore = clf.score(train_features, train_target)\n\"Mean accuracy of Random Forest: {0}\".format(score)"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"testing_data = pd.read_csv(\"../input/test.csv\",   parse_dates=['Dates'], date_parser=dateparse)\ntesting_data.head()"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":""},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"#The next couple of steps will be the exact same as the training data so I will pick up my notes\n#when we do something new\nPdDistrict = sorted(testing_data['PdDistrict'].unique())\nPdDistrict_mapping = dict(zip(PdDistrict, range(0, len(PdDistrict) + 1)))\nPdDistrict_mapping"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"testing_data['PdDistrict_Val'] = testing_data['PdDistrict'].map(PdDistrict_mapping).astype(int)\ntesting_data.head()"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"s = [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\",\"Sunday\"]\nDayOfWeek = sorted((testing_data['DayOfWeek'].unique()), key=s.index)\nprint(DayOfWeek)\nDayOfWeek_mapping = dict(zip(DayOfWeek, range(1, len(DayOfWeek) + 1)))\ntesting_data['DayOfWeek_Val'] = testing_data['DayOfWeek'].map(DayOfWeek_mapping).astype(int)\ntesting_data.head()"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"testing_data = testing_data.drop('Dates',1)\ntesting_data = testing_data.drop('DayOfWeek',1)\ntesting_data = testing_data.drop('PdDistrict',1)\ntesting_data = testing_data.drop('Address',1)\ntesting_data.head()"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"#When I first started this my computer would freeze when I ran the testing dataset, I was given this script\n#to split it out into chunks for the model\n#Test_x is the information that we will be using, we are skipping the ID since that will not be useful\ntest_x = testing_data.ix[:, 1:]\ndef chunks(l, n):\n    \"\"\" Yield successive n-sized chunks from l.\"\"\"\n    for i in range(0, len(l), n):\n        yield l[i:i + n]"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"#Create a new Pandas DataFrame for the results of our prediction to go and use the function Chucks\ntest_result = pd.DataFrame()\nfor chunk in chunks(test_x.index, 10000):\n    test_data = test_x.ix[chunk]\n    test_result = pd.concat([test_result, pd.DataFrame(clf.predict(test_data))])\n    #This last line prints a period everyime the for loop starts over giving you a nice little progress\n    #bar so you know your computer is actually doing something\n    print(end= \". \")"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"#Send the dataframe to a set of values as it is easier to add to a dataframe than it is to\n#add a dataframe to another dataframe\ntest_result_list = test_result.values\ntesting_data['Predicted Category Numeric'] = test_result_list\ntesting_data.head()"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"#Now we need to reverse the numeric transforations that we did so we ca get the categorical data back\n#so we can make some sense of the prediction\ninv_map_Category = {v: k for k, v in Category_mapping.items()}\ninv_map_DayOfWeek = {v: k for k, v in DayOfWeek_mapping.items()}\ninv_map_PdDistrict = {v: k for k, v in PdDistrict_mapping.items()}\ntesting_data['PdDistrict'] = testing_data['PdDistrict_Val'].map(inv_map_PdDistrict)\ntesting_data['DayOfWeek'] = testing_data['DayOfWeek_Val'].map(inv_map_DayOfWeek)\ntesting_data['Predicted Category'] = testing_data['Predicted Category Numeric'].map(inv_map_Category)\ntesting_data.head()"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"#Drop the numeric versions of the categorical data that we transformed\ntesting_data = testing_data.drop('Predicted Category Numeric',1)\ntesting_data = testing_data.drop('PdDistrict_Val',1)\ntesting_data = testing_data.drop('DayOfWeek_Val',1)\ntesting_data.head()"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"testing_data.to_csv('predicted.csv', index=False)"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":0}