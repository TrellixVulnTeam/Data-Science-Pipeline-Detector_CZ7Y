{"cells":[{"metadata":{},"cell_type":"markdown","source":"# San Francisco Crime Classification : lgbm + bo\n\nThis notebook uses LightGBM for models and uses bayesian optimization for searching optimized hyperparameters."},{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib inline\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\n\nsns.set() # seaborn attributet set default","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"../input/sf-crime/train.csv.zip\")\ntest = pd.read_csv(\"../input/sf-crime/test.csv.zip\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Briefly check features"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Check number of empty values"},{"metadata":{},"cell_type":"markdown","source":"### EDA / Feature engineering"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(train[train['Category'].isnull()]))\ntrain = train[train['Category'].isnull() == False]\nprint(len(train[train['Category'].isnull()]))\nprint(len(test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# categories :\ntrain['Category'].unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Remove train's 'Descript','Resolution' features not in test dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"if 'Descript' in train:\n    train = train.drop(['Descript'], axis=1)\nif 'Resolution' in train:\n    train = train.drop(['Resolution'], axis=1)\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Convert 'Dates' string to pandas.datetime data object and extract new features from it."},{"metadata":{"trusted":true},"cell_type":"code","source":"def rebuild_datetime(df):\n    df['Dates'] = pd.to_datetime(df['Dates'])\n    df['Date'] = df['Dates'].dt.date\n    df['Hour'] = df['Dates'].dt.hour\n    df['Minute'] = df['Dates'].dt.minute\n    df['DayOfWeek'] = df['Dates'].dt.weekday\n    df['Month'] = df['Dates'].dt.month\n    df['Year'] = df['Dates'].dt.year\n    df['Block'] = df['Address'].str.contains('block', case=False)\n    \n    return df\n\ntrain = rebuild_datetime(train)\ntest = rebuild_datetime(test)\n\n# check wrong datetime exists.\nprint(\"wrong Dates(train):\", len(train[train['Dates'].isnull()]))\nprint(\"wrong Dates(test):\", len(test[test['Dates'].isnull()]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Check GPS X/Y values with geopandas <br>\nYou can see abnormal GPS values(has value of arctic latitude)."},{"metadata":{"trusted":true},"cell_type":"code","source":"from shapely.geometry import Point\nimport geopandas as gpd\n\ndef create_gdf(df):\n    gdf = df.copy()\n    gdf['Coordinates'] = list(zip(gdf.X, gdf.Y))\n    gdf.Coordinates = gdf.Coordinates.apply(Point)\n    gdf = gpd.GeoDataFrame(\n        gdf, geometry='Coordinates', crs={'init': 'epsg:4326'})\n    return gdf\n\ntrain_gdf = create_gdf(train)\n\nworld = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\nax = world.plot(color='white', edgecolor='black')\ntrain_gdf.plot(ax=ax, color='red')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Fix wrong GPS values by copy mean value of same PdDistricts"},{"metadata":{"trusted":true},"cell_type":"code","source":"wrongxycnt = lambda df : len(df[(df['X'] == -120.5) & (df['Y'] == 90.0)])\nprint(wrongxycnt(train))\nprint(wrongxycnt(test))\n\ndef fix_gps(df):\n    cnt = 0\n    d = df[(df['X'] == -120.5) & (df['Y'] == 90.0)]\n    for idx, row in d.iterrows():\n        district = row['PdDistrict']\n        xys = df[df['PdDistrict'] == district][['X', 'Y']]\n        #print(\"PdDistrict:\", district)\n        df.loc[idx, ['X']] = xys['X'].mean()\n        df.loc[idx, ['Y']] = xys['Y'].mean()\n        #print(df.loc[idx, ['X']].values[0], df.loc[idx, ['Y']].values[0])\n        cnt = cnt + 1\n    print('cnt', cnt)\n    \ndef fix_gps_values():\n    fix_gps(train)\n    fix_gps(test)\n    \ndef drop_wrong_gps(df):\n    df = df.drop(df[df['X'] == -120.5].index)\n    return df\n    \nfix_gps(train)\nfix_gps(test)\n\nprint(wrongxycnt(train))\nprint(wrongxycnt(test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train['Category'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plot counts by 'Category'."},{"metadata":{"trusted":true},"cell_type":"code","source":"data = train.groupby('Category').count()\ndata = data['Dates'].sort_values(ascending=False)\n\nplt.figure(figsize=(20, 12))\nax = sns.barplot((data.values / data.values.sum()) * 100,data.index)\n\nplt.title('Count by Category', fontdict={'fontsize': 24})\nplt.xlabel('Percentage')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Draw graph of the number of events over time."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(24,16))\nfont = {'family' : 'normal',\n        'weight' : 'bold',\n        'size'   : 32}\n\nimport matplotlib\nmatplotlib.rc('font', **font)\n\nfig.suptitle('Num. Incidents / Hour (Category)')\nfor category in train['Category'].unique():\n    #print(category)\n    data = train[train['Category'] == category].groupby('Hour')\n    a = data['Hour'].count()\n    ax.plot(a.index, data['Hour'].count(), label=category)\n    \nplt.legend()\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Draw a graph of number of events by week.<br>\nAs you can see from the graph, it's hard to see the correlation."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(24,16))\n\nimport matplotlib\nmatplotlib.rc('font', **font)\n\ndef plotbycol(df, col, title):\n    fig.suptitle(title)\n    for category in df['Category'].unique():\n        #print(category)\n        data = df[df['Category'] == category].groupby(col)\n        a = data[col].count()\n        ax.plot(a.index, data[col].count(), label=category)\n    \n    plt.legend()\n    plt.show()\n    \nplotbycol(train, 'DayOfWeek', \"Incidents by Week\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['PdDistrict'].unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Delete Address feature.<br>"},{"metadata":{"trusted":true},"cell_type":"code","source":"if 'Address' in train:\n    train = train.drop(['Address'], axis=1)\n    \nif 'Address' in test:\n    test = test.drop(['Address'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train.head())\nprint(test.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def show_incidents_count_by_year_graph(df):\n    fig, ax = plt.subplots(1, 1, figsize=(24,16))\n\n    data = df\n    data['datetime'] = data['Date'].astype('datetime64')\n    data['Year'] = data['datetime'].dt.year    \n\n    for cat in df.Category.unique():\n        curdata = data[data['Category'] == cat]\n        counts = curdata.groupby('Year')\n        a = counts.size()    \n        x = list(a.index)\n        y = list(a)    \n        ax.plot(x, y, label=cat)\n\n    plt.legend()\n    plt.show()\n    \nshow_incidents_count_by_year_graph(train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if 'Dates' in train:\n    train = train.drop(['Dates'], axis=1)\nif 'Date' in train:\n    train = train.drop(['Date'], axis=1)\n    \nif 'Dates' in test:\n    test = test.drop(['Dates'], axis=1)\nif 'Date' in test:\n    test = test.drop(['Date'], axis=1)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if 'DoWN' in train:\n    train = train.drop(['DoWN'], axis=1)\n\nif 'DoWN' in test:\n    test = test.drop(['DoWN'], axis=1)\n\nif 'datetime' in train:\n    train = train.drop(['datetime'], axis=1)\n    \nif 'Year' in train:\n    train = train.drop(['Year'], axis=1)\n    \nif 'datetime' in test:\n    test = test.drop(['datetime'], axis=1)\n    \nif 'Year' in test:\n    test = test.drop(['Year'], axis=1)\n    \ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_ids = test['Id'].astype('int')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if \"Id\" in test:\n    test = test.drop(['Id'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_category = train['Category']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\nif 'Category' in train:\n    train = train.drop(['Category'], axis=1)\n    \ntrain_X = train          \n\n# names of categorical features.(need to pass LGBM model)\ncategoricals = [\"PdDistrict\"]\n\nle_pdDistrict = LabelEncoder()\ntrain_X['PdDistrict'] = le_pdDistrict.fit_transform(train_X['PdDistrict'])\ntest['PdDistrict'] = le_pdDistrict.transform(test['PdDistrict'])\n\nle_category = LabelEncoder()\ntrain_Y = le_category.fit_transform(train_category)\nnum_category = len(list(le_category.classes_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(list(le_category.classes_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_X","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_Y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"train_category : \", train_category)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Check feature importances with trained model and PermutationImportance."},{"metadata":{"trusted":true},"cell_type":"code","source":"import eli5\nfrom eli5.sklearn import PermutationImportance\nfrom lightgbm import LGBMClassifier\nimport lightgbm as lgb\nfrom sklearn.model_selection import train_test_split\n\ndef show_feature_importance(df_X, df_Y):\n    params = {\n        'n_estimators' : 3,\n        'learning_rate' : 0.4,\n        'max_delta_step' : 0.9,\n        'min_data_in_leaf' : 21,\n        'max_bin' : 465,\n        'num_leaves' : 41,\n    }\n\n    _train_X, _val_X, _train_y, _val_y = train_test_split(df_X, df_Y)\n\n    model = LGBMClassifier(objective='multiclass', num_class=num_category, n_estimators=200)\n    model.set_params(**params)\n    model.fit(_train_X, _train_y)\n\n    perm = PermutationImportance(model).fit(_val_X, _val_y)\n    display(eli5.show_weights(perm, feature_names=_val_X.columns.tolist(), include_styles=False))\n\n    \nshow_feature_importance(train_X, train_Y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model train/predict.\n\nUse bayesian optimization for searching optimal value "},{"metadata":{"trusted":true},"cell_type":"code","source":"import lightgbm as lgb\nfrom sklearn import metrics\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import NuSVR, SVR\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold, GroupKFold, GridSearchCV, train_test_split, TimeSeriesSplit, RepeatedStratifiedKFold\nfrom sklearn import metrics\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.metrics import cohen_kappa_score, mean_squared_error\nfrom sklearn import linear_model\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import StratifiedKFold\n\nimport eli5\nfrom eli5.sklearn import PermutationImportance\n\nimport warnings\n\nfrom bayes_opt import BayesianOptimization\n\nn_splits = 8\n\ndef get_param(learning_rate, max_delta_step, min_data_in_leaf, max_bin, num_leaves):\n    params = {'n_estimators' : 400,\n                'boosting_type' : 'gbdt',\n                'objective' : 'multiclass',\n                'max_delta_step': max_delta_step,\n                'min_data_in_leaf': int(min_data_in_leaf),               \n                'max_bin': int(max_bin),\n                'num_leaves': int(num_leaves),\n                'learning_rate' : learning_rate,\n                'num_class' : num_category,\n                'early_stopping_rounds': 5,\n              }\n    return params\n\ndef opt_test_func(learning_rate, max_delta_step, min_data_in_leaf, max_bin, num_leaves):\n    \n    params = get_param(learning_rate, max_delta_step, min_data_in_leaf, max_bin, num_leaves)\n    print(\"params : \", params)\n    acc, _ = train(params)\n    return acc\n\ndef multiclass_logloss(predictions, labels, epsilon=1e-12):\n    predictions = np.clip(predictions, epsilon, 1. - epsilon)\n    N = predictions.shape[0]\n    loss = -np.sum(labels*np.log(predictions+1e-9))/N\n    return loss\n\ndef train_cv(params):\n    models = []\n    kfold = KFold(n_splits=n_splits, shuffle=True, random_state=7)\n    all_predictions = np.zeros((len(train_X), len(list(le_category.classes_))))\n\n    for train_idx, test_idx in kfold.split(train_X, train_Y):\n        X_train, y_train = train_X.loc[train_idx], train_Y[train_idx]\n        X_valid, y_valid = train_X.loc[test_idx], train_Y[test_idx]\n        \n        #print(X_train.shape, y_train.shape)\n\n        train_data = lgb.Dataset(X_train, label=y_train, categorical_feature=categoricals)\n        valid_data = lgb.Dataset(X_valid, label=y_valid, categorical_feature=categoricals)\n        model = lgb.train(params, train_set=train_data, num_boost_round=120, valid_sets=[valid_data], verbose_eval=5)        \n\n        pred = model.predict(X_valid)\n#         print(\"pred.shape :\", pred.shape)\n#         print(\"all_predictions.shape : \", all_predictions.shape)\n        \n        all_predictions[test_idx] = pred        \n        models.append(model)\n        \n    labels_one_hot = np.eye(len(list(le_category.classes_)))[train_Y]\n    loss = multiclass_logloss(all_predictions, labels_one_hot)\n    print(\"validation multiclass logloss :\", loss)\n    \n    return models, loss\n\n\ndef bo_eval_func(learning_rate, max_delta_step, min_data_in_leaf, max_bin, num_leaves):\n    params = get_param(learning_rate, max_delta_step, min_data_in_leaf, max_bin, num_leaves)\n    _, loss = train_cv(params)\n    return -loss\n    \n    \ndef get_optimized_hyperparameters():\n    \"\"\"use this function for refining hyperparameters\n    \n    Returns:\n        dictionary of hyperparameters\n    \"\"\"\n    bo_params = {        \n        'learning_rate' : (0.01, 0.4),\n        'max_delta_step': (0.5, 2.5),\n        'min_data_in_leaf': (15, 45),\n        'max_bin': (200, 500),\n        'num_leaves': (20, 50),\n    }\n    \n    optimizer = BayesianOptimization(bo_eval_func, bo_params, random_state=1030)\n    \n    with warnings.catch_warnings():\n        warnings.filterwarnings('ignore')\n        init_points = 16\n        n_iter = 16\n        optimizer.maximize(init_points = init_points, n_iter = n_iter, acq='ucb', xi=0.0, alpha=1e-6)\n        return optimizer.max['params']\n\n\n#params = get_optimized_hyperparameters()\n\nparams = {\n    'learning_rate' : 0.4,\n    'max_delta_step' : 0.9,\n    'min_data_in_leaf' : 21,\n    'max_bin' : 465,\n    'num_leaves' : 41,\n}\n\nparams = get_param(**params)\nmodels, loss = train_cv(params)\nprint(\"train loss : \", loss)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Make prediction on test dataset and make submission."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_X","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict(models, test):\n    preds = []\n    for model in models:\n        pred = model.predict(test)        \n        preds.append(pred)\n\n    predsCnt = len(preds)\n    preds = np.array(preds)\n    preds = np.sum(preds, axis=0) / predsCnt\n    return preds\n\npred = predict(models, test)\n\nsubmission = pd.DataFrame(pred, columns=le_category.inverse_transform(np.linspace(0, 38, 39, dtype='int')), index=test.index)\nsubmission.to_csv('submission.csv', index_label='Id')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"}},"nbformat":4,"nbformat_minor":1}