{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\n## copy from crimeSF_NN_logodds.html\n\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.grid_search import GridSearchCV\nimport matplotlib.pylab as plt\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import preprocessing\nfrom sklearn.metrics import log_loss\nfrom sklearn.metrics import make_scorer\nfrom sklearn.cross_validation import StratifiedShuffleSplit\nfrom matplotlib.colors import LogNorm\nfrom sklearn.decomposition import PCA\nfrom copy import deepcopy\n# %matplotlib inline  "},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"trainDF=pd.read_csv(\"../input/train.csv\")\n"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"xy_scaler=preprocessing.StandardScaler()\nxy_scaler.fit(trainDF[[\"X\",\"Y\"]])\ntrainDF[[\"X\",\"Y\"]]=xy_scaler.transform(trainDF[[\"X\",\"Y\"]])\ntrainDF=trainDF[abs(trainDF[\"Y\"])<100]\ntrainDF.index=range(len(trainDF))"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"def parse_time(x):\n    DD=datetime.strptime(x,\"%Y-%m-%d %H:%M:%S\")\n    time=DD.hour#*60+DD.minute\n    day=DD.day\n    month=DD.month\n    year=DD.year\n    return time,day,month,year\n\ndef get_season(x):\n    summer=0\n    fall=0\n    winter=0\n    spring=0\n    if (x in [5, 6, 7]):\n        summer=1\n    if (x in [8, 9, 10]):\n        fall=1\n    if (x in [11, 0, 1]):\n        winter=1\n    if (x in [2, 3, 4]):\n        spring=1\n    return summer, fall, winter, spring"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"def parse_data(df,logodds,logoddsPA):\n    feature_list=df.columns.tolist()\n    if \"Descript\" in feature_list:\n        feature_list.remove(\"Descript\")\n    if \"Resolution\" in feature_list:\n        feature_list.remove(\"Resolution\")\n    if \"Category\" in feature_list:\n        feature_list.remove(\"Category\")\n    if \"Id\" in feature_list:\n        feature_list.remove(\"Id\")\n    cleanData=df[feature_list]\n    cleanData.index=range(len(df))\n    print \"Creating address features\"\n    address_features=cleanData[\"Address\"].apply(lambda x: logodds[x])\n    address_features.columns=[\"logodds\"+str(x) for x in range(len(address_features.columns))]\n    print \"Parsing dates\"\n    cleanData[\"Time\"], cleanData[\"Day\"], cleanData[\"Month\"], cleanData[\"Year\"]=zip(*cleanData[\"Dates\"].apply(parse_time))\n#     dummy_ranks_DAY = pd.get_dummies(cleanData['DayOfWeek'], prefix='DAY')\n    days = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n#     cleanData[\"DayOfWeek\"]=cleanData[\"DayOfWeek\"].apply(lambda x: days.index(x)/float(len(days)))\n    print \"Creating one-hot variables\"\n    dummy_ranks_PD = pd.get_dummies(cleanData['PdDistrict'], prefix='PD')\n    dummy_ranks_DAY = pd.get_dummies(cleanData[\"DayOfWeek\"], prefix='DAY')\n    cleanData[\"IsInterection\"]=cleanData[\"Address\"].apply(lambda x: 1 if \"/\" in x else 0)\n    cleanData[\"logoddsPA\"]=cleanData[\"Address\"].apply(lambda x: logoddsPA[x])\n    print \"droping processed columns\"\n    cleanData=cleanData.drop(\"PdDistrict\",axis=1)\n    cleanData=cleanData.drop(\"DayOfWeek\",axis=1)\n    cleanData=cleanData.drop(\"Address\",axis=1)\n    cleanData=cleanData.drop(\"Dates\",axis=1)\n    feature_list=cleanData.columns.tolist()\n    print \"joining one-hot features\"\n    features = cleanData[feature_list].join(dummy_ranks_PD.ix[:,:]).join(dummy_ranks_DAY.ix[:,:]).join(address_features.ix[:,:])\n    print \"creating new features\"\n    features[\"IsDup\"]=pd.Series(features.duplicated()|features.duplicated(take_last=True)).apply(int)\n    features[\"Awake\"]=features[\"Time\"].apply(lambda x: 1 if (x==0 or (x>=8 and x<=23)) else 0)\n    features[\"Summer\"], features[\"Fall\"], features[\"Winter\"], features[\"Spring\"]=zip(*features[\"Month\"].apply(get_season))\n    if \"Category\" in df.columns:\n        labels = df[\"Category\"].astype('category')\n#         label_names=labels.unique()\n#         labels=labels.cat.rename_categories(range(len(label_names)))\n    else:\n        labels=None\n    return features,labels"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"addresses=sorted(trainDF[\"Address\"].unique())\ncategories=sorted(trainDF[\"Category\"].unique())\nC_counts=trainDF.groupby([\"Category\"]).size()\nA_C_counts=trainDF.groupby([\"Address\",\"Category\"]).size()\nA_counts=trainDF.groupby([\"Address\"]).size()\nlogodds={}\nlogoddsPA={}\nMIN_CAT_COUNTS=2\ndefault_logodds=np.log(C_counts/len(trainDF))-np.log(1.0-C_counts/float(len(trainDF)))\nfor addr in addresses:\n    PA=A_counts[addr]/float(len(trainDF))\n    logoddsPA[addr]=np.log(PA)-np.log(1.-PA)\n    logodds[addr]=deepcopy(default_logodds)\n    for cat in A_C_counts[addr].keys():\n        if (A_C_counts[addr][cat]>MIN_CAT_COUNTS) and A_C_counts[addr][cat]<A_counts[addr]:\n            PA=A_C_counts[addr][cat]/float(A_counts[addr])\n            logodds[addr][categories.index(cat)]=np.log(PA)-np.log(1.0-PA)\n    logodds[addr]=pd.Series(logodds[addr])\n    logodds[addr].index=range(len(categories))"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"features, labels=parse_data(trainDF,logodds,logoddsPA)"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"print features.columns.tolist()\nprint len(features.columns)"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# num_feature_list=[\"Time\",\"Day\",\"Month\",\"Year\",\"DayOfWeek\"]\ncollist=features.columns.tolist()\nscaler = preprocessing.StandardScaler()\nscaler.fit(features)\nfeatures[collist]=scaler.transform(features)"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"new_PCA=PCA(n_components=60)\nnew_PCA.fit(features)\nplt.plot(new_PCA.explained_variance_ratio_)\nplt.yscale('log')\nplt.title(\"PCA explained ratio of features\")\nprint new_PCA.explained_variance_ratio_"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"plt.plot(new_PCA.explained_variance_ratio_.cumsum())\nplt.title(\"cumsum of PCA explained ratio\")"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"sss = StratifiedShuffleSplit(labels, train_size=0.5)\nfor train_index, test_index in sss:\n    features_train,features_test=features.iloc[train_index],features.iloc[test_index]\n    labels_train,labels_test=labels[train_index],labels[test_index]\nfeatures_test.index=range(len(features_test))\nfeatures_train.index=range(len(features_train))\nlabels_train.index=range(len(labels_train))\nlabels_test.index=range(len(labels_test))\nfeatures.index=range(len(features))\nlabels.index=range(len(labels))"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":0}