{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import csv\nimport datetime\nimport numpy as np\nfrom keras.layers.advanced_activations import LeakyReLU\nfrom keras.layers.core import Dense, Dropout, Activation\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.models import Sequential\nfrom keras.utils import np_utils\nfrom keras import applications\nfrom keras import regularizers\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import confusion_matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as ticker\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.neighbors import RadiusNeighborsClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom datetime import tzinfo, timedelta, datetime\nfrom sklearn import metrics\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import StandardScaler\n\nfrom keras.layers.advanced_activations import PReLU\nfrom keras.layers.core import Dense, Dropout, Activation\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.models import Sequential\nfrom keras.utils import np_utils\nimport zipfile\nimport csv","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"def get_data(fn):\n    data = []\n    with open(fn) as f:\n        reader = csv.DictReader(f)\n        data = [row for row in reader]\n    return data\n\ndef get_fields(data, fields):\n    extracted = []\n    for row in data:\n        extract = []\n        for field, f in sorted(fields.items()):\n            info = f(row[field])\n        if type(info) == list:\n            extract.extend(info)\n        else:\n            extract.append(info)\n        extracted.append(np.array(extract, dtype=np.float32))\n    return extracted\n\ndef preprocess_data(X, scaler=None):\n    if not scaler:\n        scaler = StandardScaler()\n        scaler.fit(X)\n    X = scaler.transform(X)\n    return X\n\n\ndef dating(x):\n    date, time = x.split(' ')\n    year, month, day = map(int, date.split('-'))\n    hour, minute, second = time.split(':')\n    return [day, month, year, hour, minute, datetime(year, month, day).isocalendar()[1], getHourPart(int(hour))]\n\ndef getHourPart(hour):\n    if(hour >= 2 and hour < 8): return 1;\n    if(hour >= 8 and hour < 12): return 2;\n    if(hour >= 12 and hour < 14): return 3;\n    if(hour >= 14 and hour < 18): return 4;\n    if(hour >= 18 and hour < 22): return 5;\n    if(hour < 2 or hour >= 22): return 6;\n\ndays = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n\ndata_fields = {\n    'X': lambda x: float(x),\n    'Y': lambda x: float(x),\n    'Dates' : lambda x: dating(x),\n    'DayOfWeek' : lambda x : [days.index(x), 1 if days.index(x) > 4 else 0],\n    'Address': lambda x: [1 if ('/' in x.lower() and 'of' not in x.lower()) else 0],\n    'PdDistrict': lambda x: districts.index(x),\n}\n\nprint('Loading training data...')\n        \n\nz1 = zipfile.ZipFile('../input/sf-crime/train.csv.zip')\nz2 = zipfile.ZipFile('../input/sf-crime/test.csv.zip')\nz1.extractall()\nz2.extractall()\ntrain_data = pd.read_csv(z1.open('train.csv'))\ntest_data = pd.read_csv(z2.open('test.csv'))\n\nraw_train = get_data('../working/train.csv')    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"districts = np.unique([row['PdDistrict'] for row in raw_train]).tolist()\nlabels = np.unique([row['Category'] for row in raw_train]).tolist()\nlabel_fields = {'Category': lambda x: labels.index(x.replace(',', ''))}\nprint('days')\nprint(days)\nprint('districts')\nprint(districts)\nprint('labels')\nprint(labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"label_fields = {'Category': lambda x: labels.index(x.replace(',', ''))}\nprint('Loading training data...')\nraw_train = get_data('../working/train.csv')\nprint('Creating training data...')\nX = np.array(get_fields(raw_train, data_fields), dtype=np.float32)\nprint('Creating training labels...')\ny = np.array(get_fields(raw_train, label_fields))\ndel raw_train","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"X = preprocess_data(X)\nY = np_utils.to_categorical(y)\n\ninput_dim = X.shape[1]\noutput_dim = len(labels)\n\ndef build_model(input_dim, output_dim, hn=32, dp=0.5, layers=1):\n    model = Sequential()\n    #model.add(Dense(hn, input_shape=(input_dim,), init='glorot_uniform',  activity_regularizer=regularizers.l1(0.01)))\n    model.add(Dense(hn, input_shape=(input_dim,), init='glorot_uniform'))\n    model.add(LeakyReLU())\n    model.add(Dropout(dp))\n\n    for i in range(layers):\n        #model.add(Dense(hn, input_shape=(hn,), init='glorot_uniform',  activity_regularizer=regularizers.l1(0.01)))\n        model.add(Dense(hn, input_shape=(hn,), init='glorot_uniform'))\n        model.add(LeakyReLU())\n        model.add(BatchNormalization())\n        model.add(Dropout(dp))\n\n    #model.add(Dense(output_dim, input_shape=(hn,), init='glorot_uniform',  activity_regularizer=regularizers.l1(0.01)))\n    model.add(Dense(output_dim, input_shape=(hn,), init='glorot_uniform'))\n    model.add(Activation('softmax'))\n\n    model.compile(loss='categorical_crossentropy', optimizer='adam')\n    return model\n\nEPOCHS = 40\nBATCHES = 128\nHN = 256\nLAYERS = 0\nDROPOUT = 0.01\nITERATIONS = 5\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.25, random_state=42,stratify=Y)\nmodel = build_model(input_dim, output_dim, HN, DROPOUT, LAYERS)\nmodel.fit(X_train, y_train, epochs=EPOCHS, batch_size=BATCHES, validation_data=(X_test, y_test), verbose=2)\nprediction = model.predict(X_test, verbose=2)\npredictedVals = []\nfor row in prediction:\n    predictedVals.append(np.argmax(row))\nvalidVals = []\nfor row in y_test:\n    validVals.append(np.argmax(row))\nprint(\"F1 score (micro): \", f1_score(predictedVals, validVals, average='micro'))\nprint(\"F1 score (macro): \", f1_score(predictedVals, validVals, average='macro'))\nprint(\"F1 score (weighted): \", f1_score(predictedVals, validVals, average='weighted'))\nc = confusion_matrix(predictedVals, validVals)\nreverse_c = list(zip(*np.array(c)))\nfor i in range(len(c[1])):\n    print(labels[i])\n    fn = sum(c[i]) - c[i][i]\n    fp = sum(reverse_c[i]) - c[i][i]\n    print(\"Правильных результатов: \" + str(c[i][i]))\n    print(\"Ошибки первого рода: \"+ str(fn))\n    print(\"Ошибки второго рода: \" + str(fp))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictedVals","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}