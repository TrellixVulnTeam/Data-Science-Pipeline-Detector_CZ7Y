{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport datetime\n\nfrom tensorflow.keras.layers import Dense, Input, Flatten, concatenate, Dropout, Lambda, BatchNormalization\nfrom tensorflow.keras.models import Model\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, Callback\n\n# from tensorflow.keras.losses import binary_crossentropy\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.callbacks import History\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.metrics import f1_score\n# import codecs\n# import re\n\npd.set_option('display.max_columns', 100)\npd.set_option('display.max_rows', 25)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"t_start = datetime.datetime.now()\n\ntrain_df = pd.read_csv('/kaggle/input/sf-crime/train.csv.zip')\ntest_df = pd.read_csv('/kaggle/input/sf-crime/test.csv.zip')\n\nsample_submission = pd.read_csv('/kaggle/input/sf-crime/sampleSubmission.csv.zip')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Preprocessing Function"},{"metadata":{},"cell_type":"markdown","source":"This function only does some very basic preprocessing on the date information provided in the dataset. We can expand this function to include additional features. For example, some of the public kernals for the original competition included preprocessing of the address feature to extract individual street names and/or block numbers.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess(df):\n    \n    df['Dates'] = df['Dates'].apply(lambda x: datetime.datetime.strptime(x, '%Y-%m-%d %H:%M:%S'))\n    df['Year'] = df['Dates'].apply(lambda x: x.year)\n    df['Month'] = df['Dates'].apply(lambda x: x.month)\n    df['Day'] = df['Dates'].apply(lambda x: x.day)\n    df['Hour'] = df['Dates'].apply(lambda x: x.hour)\n        \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = preprocess(train_df)\ntest_df = preprocess(test_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Drop Duplicate Training Entries & Unuseful Columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.drop_duplicates(inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"drop_cols = ['Dates', 'Descript', 'Resolution', 'Id']\n\nfor col in drop_cols:\n    if col in train_df.columns:\n        train_df.drop(col, axis=1, inplace=True)\n    if col in test_df.columns:\n        test_df.drop(col, axis=1, inplace=True)\n        \nX = train_df.drop('Category', axis=1)\nX_test = test_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Convert Training Labels into Submission File Format"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_cats = train_df['Category']\nunique_cats = np.sort(y_cats.unique())\n\ny = np.zeros((y_cats.shape[0], 39))\nfor idx, target in enumerate(list(y_cats)):\n    y[idx, np.where(unique_cats == target)] = 1\n\ny = pd.DataFrame(y, columns = unique_cats)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Label Encoding"},{"metadata":{"trusted":true},"cell_type":"code","source":"X['train'] = 1\nX_test['train'] = 0\n\ncombined = pd.concat([X, X_test])\n\nfor col in combined.columns:\n    if combined.dtypes[col] == 'object':\n        le = LabelEncoder()\n        combined[col] = le.fit_transform(combined[col])\n        \nX = combined[combined['train'] == 1]\nX.drop(['train'], axis=1, inplace=True)\nX_test = combined[combined['train'] == 0]\nX_test.drop(['train'], axis=1, inplace=True)  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature Standardization"},{"metadata":{},"cell_type":"markdown","source":"NOTE:  I was confused about the difference between standardization and normalization since they seemed to be describing the same process. Here's a brief article I found describing the difference between the two terms:\n\nhttps://www.statisticshowto.datasciencecentral.com/normalized/"},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = StandardScaler()\nX = scaler.fit_transform(X)\nX_test = scaler.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Training / Validation Set Split"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, random_state=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Create Neural Network "},{"metadata":{"trusted":true},"cell_type":"code","source":"The model developed below is designed to serve as a starting point for further development and tuning. The hyperparameters chosen are mostly arbitrary and I haven't attempted to do any tuning. Here are some hyperparameters we can consider modifying to further refine the model:\n\n* Number of Hidden Layers (currently 3, chosen arbitrarily) \n* Number of Neurons per Layer (currently arbitrary powers of 2)\n* Dropout Rates for each Hidden Layer (currently 0.5 for all 3)\n* Optimizer (adam is usually best but sometimes it's not, it's worth trying some others)\n* Batch Size (currently 256, an arbitrary power of 2)\n* Learning Rate \n* Weight Decay\n* Momentum \n* Regularization (L1 and/or L2, incorporate into loss function)\n\nI also want to add cross validation. ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model Generation Function"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_model(x_tr, y_tr, x_val, y_val):\n    K.clear_session()\n    inp = Input(shape = (x_tr.shape[1],))\n    \n    dl_1 = 1024  \n    drop_1 = 0.5\n    dl_2 = 512 \n    drop_2 = 0.5 \n    dl_3 = 256\n    drop_3 = 0.5 \n    \n    x = Dense(dl_1, input_dim=X.shape[1], activation='relu')(inp) \n    x = Dropout(drop_1)(x)\n    x = BatchNormalization()(x)\n    x = Dense(dl_2, activation='relu')(x)\n    x = Dropout(drop_2)(x)\n    x = BatchNormalization()(x)\n    x = Dense(dl_3, activation='relu')(x)\n    x = Dropout(drop_3)(x)\n    x = BatchNormalization()(x)\n    \n    out = Dense(39, activation='softmax')(x)\n    \n    model = Model(inp,out)\n    \n    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=[])\n        \n    \n    bsz = 256\n    steps = x_tr.shape[0]/bsz\n    \n    es = EarlyStopping(monitor='loss', patience=10) \n\n    y_tr = np.asarray(y_tr)\n    y_val = np.asarray(y_val)\n    history = model.fit(x_tr, y_tr, callbacks=[es], epochs=50, batch_size=bsz, verbose=1)\n\n    return model, history.history['loss'][-1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mod, loss = get_model(X_train, y_train, X_val, y_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = mod.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Generate Prediction Submission File"},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df = pd.DataFrame(preds, columns=unique_cats)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df.index = sub_df.index.set_names(['Id'])\nsub_df.reset_index(drop=False, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df.to_csv('sub_file_area.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The file generated by this model scores 2.52497 when submitted to the original Kaggle competition (the competition is closed but you can still submit to evaluate your model's performance). That puts it pretty close to the middle of the leaderboard for the original competition, which isn't bad considering it's an out of the box model with minimal processing of the raw data. The top score for the competition was 1.95936, so there is plenty of room for improvement.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"t_final = datetime.datetime.now()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Total Execution Time:  {}'.format(t_final - t_start))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}