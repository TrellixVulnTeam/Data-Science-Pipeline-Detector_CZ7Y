{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"\r\n\r\nimport pandas as pd\r\nimport numpy as np\r\nfrom datetime import datetime\r\nfrom sklearn.cross_validation import train_test_split\r\nfrom sklearn.grid_search import GridSearchCV\r\nimport matplotlib.pylab as plt\r\nfrom sklearn.tree import DecisionTreeClassifier\r\nfrom sklearn import preprocessing\r\nfrom sklearn.metrics import log_loss\r\nfrom sklearn.metrics import make_scorer\r\nfrom sklearn.cross_validation import StratifiedShuffleSplit\r\nfrom matplotlib.colors import LogNorm\r\nfrom sklearn.decomposition import PCA\r\nfrom keras.layers.advanced_activations import PReLU\r\nfrom keras.layers.core import Dense, Dropout, Activation\r\nfrom keras.layers.normalization import BatchNormalization\r\nfrom keras.models import Sequential\r\nfrom keras.utils import np_utils\r\nfrom copy import deepcopy\r\n\r\ntrainDF=pd.read_csv(\"../input/train.csv\")\r\nxy_scaler=preprocessing.StandardScaler()\r\nxy_scaler.fit(trainDF[[\"X\",\"Y\"]])\r\ntrainDF[[\"X\",\"Y\"]]=xy_scaler.transform(trainDF[[\"X\",\"Y\"]])\r\ntrainDF=trainDF[abs(trainDF[\"Y\"])<100]\r\ntrainDF.index=range(len(trainDF))\r\nplt.plot(trainDF[\"X\"],trainDF[\"Y\"],'.')\r\nplt.show()\r\n\r\n\r\nNX=100\r\nNY=100\r\n\r\n\r\ndef parse_time(x):\r\n    DD=datetime.strptime(x,\"%Y-%m-%d %H:%M:%S\")\r\n    time=DD.hour#*60+DD.minute\r\n    day=DD.day\r\n    month=DD.month\r\n    year=DD.year\r\n    return time,day,month,year\r\n\r\ndef get_season(x):\r\n    summer=0\r\n    fall=0\r\n    winter=0\r\n    spring=0\r\n    if (x in [5, 6, 7]):\r\n        summer=1\r\n    if (x in [8, 9, 10]):\r\n        fall=1\r\n    if (x in [11, 0, 1]):\r\n        winter=1\r\n    if (x in [2, 3, 4]):\r\n        spring=1\r\n    return summer, fall, winter, spring\r\n    \r\ndef parse_data(df,logodds,logoddsPA):\r\n    feature_list=df.columns.tolist()\r\n    if \"Descript\" in feature_list:\r\n        feature_list.remove(\"Descript\")\r\n    if \"Resolution\" in feature_list:\r\n        feature_list.remove(\"Resolution\")\r\n    if \"Category\" in feature_list:\r\n        feature_list.remove(\"Category\")\r\n    if \"Id\" in feature_list:\r\n        feature_list.remove(\"Id\")\r\n    cleanData=df[feature_list]\r\n    cleanData.index=range(len(df))\r\n    print (\"Creating address features\")\r\n    address_features=cleanData[\"Address\"].apply(lambda x: logodds[x])\r\n    address_features.columns=[\"logodds\"+str(x) for x in range(len(address_features.columns))]\r\n    print (\"Parsing dates\")\r\n    cleanData[\"Time\"], cleanData[\"Day\"], cleanData[\"Month\"], cleanData[\"Year\"]=zip(*cleanData[\"Dates\"].apply(parse_time))\r\n#     dummy_ranks_DAY = pd.get_dummies(cleanData['DayOfWeek'], prefix='DAY')\r\n    days = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\r\n#     cleanData[\"DayOfWeek\"]=cleanData[\"DayOfWeek\"].apply(lambda x: days.index(x)/float(len(days)))\r\n    print (\"Creating one-hot variables\")\r\n    dummy_ranks_PD = pd.get_dummies(cleanData['PdDistrict'], prefix='PD')\r\n    dummy_ranks_DAY = pd.get_dummies(cleanData[\"DayOfWeek\"], prefix='DAY')\r\n    cleanData[\"IsInterection\"]=cleanData[\"Address\"].apply(lambda x: 1 if \"/\" in x else 0)\r\n    cleanData[\"logoddsPA\"]=cleanData[\"Address\"].apply(lambda x: logoddsPA[x])\r\n    print (\"droping processed columns\")\r\n    cleanData=cleanData.drop(\"PdDistrict\",axis=1)\r\n    cleanData=cleanData.drop(\"DayOfWeek\",axis=1)\r\n    cleanData=cleanData.drop(\"Address\",axis=1)\r\n    cleanData=cleanData.drop(\"Dates\",axis=1)\r\n    feature_list=cleanData.columns.tolist()\r\n    print (\"joining one-hot features\")\r\n    features = cleanData[feature_list].join(dummy_ranks_PD.ix[:,:]).join(dummy_ranks_DAY.ix[:,:]).join(address_features.ix[:,:])\r\n    print (\"creating new features\")\r\n    features[\"IsDup\"]=pd.Series(features.duplicated()|features.duplicated(take_last=True)).apply(int)\r\n    features[\"Awake\"]=features[\"Time\"].apply(lambda x: 1 if (x==0 or (x>=8 and x<=23)) else 0)\r\n    features[\"Summer\"], features[\"Fall\"], features[\"Winter\"], features[\"Spring\"]=zip(*features[\"Month\"].apply(get_season))\r\n    if \"Category\" in df.columns:\r\n        labels = df[\"Category\"].astype('category')\r\n#         label_names=labels.unique()\r\n#         labels=labels.cat.rename_categories(range(len(label_names)))\r\n    else:\r\n        labels=None\r\n    return features,labels\r\n    \r\n\r\naddresses=sorted(trainDF[\"Address\"].unique())\r\ncategories=sorted(trainDF[\"Category\"].unique())\r\nC_counts=trainDF.groupby([\"Category\"]).size()\r\nA_C_counts=trainDF.groupby([\"Address\",\"Category\"]).size()\r\nA_counts=trainDF.groupby([\"Address\"]).size()\r\nlogodds={}\r\nlogoddsPA={}\r\nMIN_CAT_COUNTS=2\r\ndefault_logodds=np.log(C_counts/len(trainDF))-np.log(1.0-C_counts/float(len(trainDF)))\r\nfor addr in addresses:\r\n    PA=A_counts[addr]/float(len(trainDF))\r\n    logoddsPA[addr]=np.log(PA)-np.log(1.-PA)\r\n    logodds[addr]=deepcopy(default_logodds)\r\n    for cat in A_C_counts[addr].keys():\r\n        if (A_C_counts[addr][cat]>MIN_CAT_COUNTS) and A_C_counts[addr][cat]<A_counts[addr]:\r\n            PA=A_C_counts[addr][cat]/float(A_counts[addr])\r\n            logodds[addr][categories.index(cat)]=np.log(PA)-np.log(1.0-PA)\r\n    logodds[addr]=pd.Series(logodds[addr])\r\n    logodds[addr].index=range(len(categories))\r\n    \r\nfeatures, labels=parse_data(trainDF,logodds,logoddsPA)\r\n\r\nprint (features.columns.tolist())\r\nprint (len(features.columns))\r\n\r\n# num_feature_list=[\"Time\",\"Day\",\"Month\",\"Year\",\"DayOfWeek\"]\r\ncollist=features.columns.tolist()\r\nscaler = preprocessing.StandardScaler()\r\nscaler.fit(features)\r\nfeatures[collist]=scaler.transform(features)\r\n\r\nnew_PCA=PCA(n_components=60)\r\nnew_PCA.fit(features)\r\nplt.plot(new_PCA.explained_variance_ratio_)\r\nplt.yscale('log')\r\nplt.title(\"PCA explained ratio of features\")\r\nprint (new_PCA.explained_variance_ratio_)\r\n\r\nplt.plot(new_PCA.explained_variance_ratio_.cumsum())\r\nplt.title(\"cumsum of PCA explained ratio\")\r\n\r\n\r\nsss = StratifiedShuffleSplit(labels, train_size=0.5)\r\nfor train_index, test_index in sss:\r\n    features_train,features_test=features.iloc[train_index],features.iloc[test_index]\r\n    labels_train,labels_test=labels[train_index],labels[test_index]\r\nfeatures_test.index=range(len(features_test))\r\nfeatures_train.index=range(len(features_train))\r\nlabels_train.index=range(len(labels_train))\r\nlabels_test.index=range(len(labels_test))\r\nfeatures.index=range(len(features))\r\nlabels.index=range(len(labels))\r\n\r\ndef build_and_fit_model(X_train,y_train,X_test=None,y_test=None,hn=32,dp=0.5,layers=1,epochs=1,batches=64,verbose=0):\r\n    input_dim=X_train.shape[1]\r\n    output_dim=len(labels_train.unique())\r\n    Y_train=np_utils.to_categorical(y_train.cat.rename_categories(range(len(y_train.unique()))))\r\n   \r\n#     print output_dim\r\n    model = Sequential()\r\n    model.add(Dense(input_dim, hn, init='glorot_uniform'))\r\n    model.add(PReLU((hn,)))\r\n    model.add(Dropout(dp))\r\n\r\n    for i in range(layers):\r\n      model.add(Dense(hn, hn, init='glorot_uniform'))\r\n      model.add(PReLU((hn,)))\r\n      model.add(BatchNormalization((hn,)))\r\n      model.add(Dropout(dp))\r\n\r\n    model.add(Dense(hn, output_dim, init='glorot_uniform'))\r\n    model.add(Activation('softmax'))\r\n    model.compile(loss='categorical_crossentropy', optimizer='adam')\r\n    \r\n    if X_test is not None:\r\n        Y_test=np_utils.to_categorical(y_test.cat.rename_categories(range(len(y_test.unique()))))\r\n        fitting=model.fit(X_train, Y_train, nb_epoch=epochs, batch_size=batches,verbose=verbose,validation_data=(X_test,Y_test))\r\n        test_score = log_loss(y_test, model.predict_proba(X_test,verbose=0))\r\n    else:\r\n        model.fit(X_train, Y_train, nb_epoch=epochs, batch_size=batches,verbose=verbose)\r\n        fitting=0\r\n        test_score = 0\r\n    return test_score, fitting, model\r\n\r\nlen(features.columns)\r\n\r\n\r\nN_EPOCHS=20\r\nN_HN=128\r\nN_LAYERS=1\r\nDP=0.5\r\nscore, fitting, model = build_and_fit_model(features_train.as_matrix(),labels_train,X_test=features_test.as_matrix(),y_test=labels_test,hn=N_HN,layers=N_LAYERS,epochs=N_EPOCHS,verbose=2,dp=DP)\r\n\r\nprint (\"all\", log_loss(labels, model.predict_proba(features.as_matrix(),verbose=0)))\r\nprint (\"train\", log_loss(labels_train, model.predict_proba(features_train.as_matrix(),verbose=0)))\r\nprint (\"test\", log_loss(labels_test, model.predict_proba(features_test.as_matrix(),verbose=0)))\r\n\r\n\r\nplt.plot(fitting.history['val_loss'],label=\"validation\")\r\nplt.plot(fitting.history['loss'],label=\"train\")\r\n# plt.xscale('log')\r\nplt.legend()\r\n\r\nscore, fitting, model = build_and_fit_model(features.as_matrix(),labels,hn=N_HN,layers=N_LAYERS,epochs=N_EPOCHS,verbose=2,dp=DP)\r\n\r\n\r\nprint (\"all\", log_loss(labels, model.predict_proba(features.as_matrix(),verbose=0)))\r\nprint (\"train\", log_loss(labels_train, model.predict_proba(features_train.as_matrix(),verbose=0)))\r\nprint (\"test\", log_loss(labels_test, model.predict_proba(features_test.as_matrix(),verbose=0)))\r\n\r\ntestDF=pd.read_csv(\"../input/test.csv\")\r\ntestDF[[\"X\",\"Y\"]]=xy_scaler.transform(testDF[[\"X\",\"Y\"]])\r\n#set outliers to 0\r\ntestDF[\"X\"]=testDF[\"X\"].apply(lambda x: 0 if abs(x)>5 else x)\r\ntestDF[\"Y\"]=testDF[\"Y\"].apply(lambda y: 0 if abs(y)>5 else y)\r\n\r\n\r\nnew_addresses=sorted(testDF[\"Address\"].unique())\r\nnew_A_counts=testDF.groupby(\"Address\").size()\r\nonly_new=set(new_addresses+addresses)-set(addresses)\r\nonly_old=set(new_addresses+addresses)-set(new_addresses)\r\nin_both=set(new_addresses).intersection(addresses)\r\nfor addr in only_new:\r\n    PA=new_A_counts[addr]/float(len(testDF)+len(trainDF))\r\n    logoddsPA[addr]=np.log(PA)-np.log(1.-PA)\r\n    logodds[addr]=deepcopy(default_logodds)\r\n    logodds[addr].index=range(len(categories))\r\nfor addr in in_both:\r\n    PA=(A_counts[addr]+new_A_counts[addr])/float(len(testDF)+len(trainDF))\r\n    logoddsPA[addr]=np.log(PA)-np.log(1.-PA)\r\n    \r\nfeatures_sub, _=parse_data(testDF,logodds,logoddsPA)\r\n# scaler.fit(features_test)\r\n\r\n\r\ncollist=features_sub.columns.tolist()\r\nprint (collist)\r\n\r\nfeatures_sub[collist]=scaler.transform(features_sub[collist])\r\npredDF=pd.DataFrame(model.predict_proba(features_sub.as_matrix(),verbose=0),columns=sorted(labels.unique()))\r\n\r\npredDF.head()\r\npredDF.to_csv(\"crimeSF_NN_logodds.csv\",index_label=\"Id\",na_rep=\"0\")"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":0}