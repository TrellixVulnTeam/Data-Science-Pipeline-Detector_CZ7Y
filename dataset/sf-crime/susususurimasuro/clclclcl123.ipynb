{"cells":[{"metadata":{},"cell_type":"markdown","source":"> # 필요한 패키지 불러오기"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import Imputer\nfrom sklearn.preprocessing import PolynomialFeatures \nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.feature_selection import VarianceThreshold  \nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans\nimport xgboost as xgb \nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport os\nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 디버깅할때 데이터 줄여서 읽을 수 있도록 "},{"metadata":{"trusted":true},"cell_type":"code","source":"# debug = True\n\n# if debug:\n#     nrows = 50000\n# else:\n#     nrows = None","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# %%time\n# kaggle = '../input/'\n# train = pd.read_csv(kaggle +'train.csv', nrows = nrows,parse_dates=['Dates'])\n# test = pd.read_csv(kaggle + 'test.csv', nrows = nrows, parse_dates=['Dates'], index_col='Id')\n# test = test.sample(frac = 0.1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 필요한 패키지를 불러오고 데이터를 불러온다."},{"metadata":{"trusted":true},"cell_type":"code","source":"# %%time\n# kaggle = '../input/'\n# train = pd.read_csv(kaggle +'train.csv', nrows = nrows,parse_dates=['Dates'])\n# test = pd.read_csv(kaggle + 'test.csv', nrows = nrows, parse_dates=['Dates'], index_col='Id')\n# test = test.sample(frac = 0.1)import pandas as pd\n\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder\nimport lightgbm as lgb\npd.options.display.max_columns=100\ntrain = pd.read_csv('../input/train.csv', parse_dates=['Dates'])\ntest = pd.read_csv('../input/test.csv', parse_dates=['Dates'], index_col='Id')\n\ndef feature_engineering(data):\n    data['Date'] = pd.to_datetime(data['Dates'].dt.date)\n    data['n_days'] = (data['Date'] - data['Date'].min()).apply(lambda x: x.days)\n    data['Day'] = data['Dates'].dt.day\n    data['DayOfWeek'] = data['Dates'].dt.weekday\n    data['Month'] = data['Dates'].dt.month\n    data['Year'] = data['Dates'].dt.year\n    data['Hour'] = data['Dates'].dt.hour\n    data['Minute'] = data['Dates'].dt.minute\n    data['Block'] = data['Address'].str.contains('block', case=False).apply(lambda x: 1 if x == True else 0)\n    data[\"X_Y\"] = data[\"X\"] - data[\"Y\"]\n    data[\"XY\"] = data[\"X\"] + data[\"Y\"]\n    data.drop(columns=['Dates','Date','Address'], inplace=True)\n    return data\ntrain = feature_engineering(train)\ntest = feature_engineering(test)\ntrain.drop(columns=['Descript','Resolution'], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 'Category'변수에서 지울 것 지우기\n- eda시에 category 변수에서 드물게 나오는 것들을 확인 할 수 있었음 \n- 그러므로 우리의 classifier가 좀 더 잘 작동할 수 있도록 지워주겠음"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# rare_cats = set(['FAMILY OFFENSES', 'BAD CHECKS', 'BRIBERY', 'EXTORTION',\n#        'SEX OFFENSES NON FORCIBLE', 'GAMBLING', 'PORNOGRAPHY/OBSCENE MAT',\n#        'TREA'])\n# all_cats = set(train['Category'].unique())\n# common_cats = all_cats-rare_cats","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# train= train[train['Category'].isin(common_cats)]\n# train = train.reset_index(drop = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ac6f3987baa497133b3edb6eeb2f294c3d88487f"},"cell_type":"markdown","source":"# PCA로 위도 경도 처리\n- pca는 차원 축소에 주로 사용되는 알고리즘\n- X,Y(위경도)는 2차원 \n- 그러므로 축소하지 않고 차원을 유지하면서 pca.transform 을 실시 \n- 그러면 원본 데이터를 변형해서 새로운 피쳐를 만들 수 있음 "},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in train,test:\n    data = i\n  \n    coord = data[['X','Y']]\n    pca = PCA(n_components=2)\n    pca.fit(coord)\n\n    coord_pca = pca.transform(coord)\n\n    data['coord_pca1'] = coord_pca[:, 0]\n    data['coord_pca2'] = coord_pca[:, 1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"44cc1e206aad452897b17a0c3dbc3bced5d33506"},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# K-means 군집 -위도,경도\n- k-means 군집은 군집에 사용되는 unsupervised learning 알고리즘\n- k-means 군집을 하면 가까운 지역끼리 군집이되기 때문에 zipcode 같이 유사한 개념의 새로운 피쳐를 만들 수 있다\n\n### k를 어떻게 결정할까?\n- k-means 군집에서는 k를 어떻게 정하는지가 중요하다.\n- 일반적으로는 k값을 늘리면서, 군집간의 거리의 합인 inertia가 급격하게 떨어지는 구간을 k값으로 한다.\n- 이런 방법을 elbow method라고 한다 \n\n- 하지만 여기서는 그렇게 결과 값이 좋게 나타나지 않았으므로 사용하지 않는다"},{"metadata":{"trusted":true},"cell_type":"code","source":"# inertia_arr = []\n\n# k_range = range(2, 16)\n\n# for k in k_range:\n#     kmeans = KMeans(n_clusters=k, random_state=42).fit(coord)\n \n#     # Sum of distances of samples to their closest cluster center\n#     interia = kmeans.inertia_\n#     print (\"k:\",k, \" cost:\", interia)\n#     inertia_arr.append(interia)\n    \n# inertia_arr = np.array(inertia_arr)\n\n# plt.plot(k_range, inertia_arr)\n# plt.vlines(5, ymin=inertia_arr.min()*0.9999, ymax=inertia_arr.max()*1.0003, linestyles='--', colors='b')\n# plt.title('Elbow Method')\n# plt.xlabel('Number of clusters')\n# plt.ylabel('Inertia');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# for i in train,test:\n#     data = i\n#     coord = data[['X','Y']]\n#     # kmeans for lat, long\n#     kmeans = KMeans(n_clusters=5, random_state=42).fit(coord)\n#     coord_cluster = kmeans.predict(coord)\n#     data['coord_cluster'] = coord_cluster\n#     data['coord_cluster'] = data['coord_cluster'].map(lambda x: 'c_' + str(x).rjust(2, '0'))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# one-hot encoding \n- one-hot encoding으로 categorical 전처리\n- 모두 [0,1,0,0] , [0,0,0,1] 이러한 형태로 바꿔준다 \n- 하지만 이번 분석에서는 ___원핫인코딩을 했을때 보다 단순히 카테고리 변수로 모델을 훈련시켰을때 성능이 더 좋았기에 원 핫으로 처리하지 않음___"},{"metadata":{"trusted":true},"cell_type":"code","source":"# v = cat_val\n# for f in v:\n#     dist_values = train[f].value_counts().shape[0]\n#     print('Variable {} has {} distinct values'.format(f, dist_values))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print('더미 전 트레인 변수의 수'.format(train.shape[1]))\n# train = pd.get_dummies(train,columns=v, drop_first=True)\n# print('터미 후 트레인 변수의 수 '.format(train.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{},"cell_type":"markdown","source":"# LabelEncoder()을 이용해 categorical 변수 처리"},{"metadata":{"trusted":true,"_uuid":"7c0790058887488b5b08fc84a3628dab63c643fa"},"cell_type":"code","source":"le1 = LabelEncoder()\ntrain['PdDistrict'] = le1.fit_transform(train['PdDistrict'])\ntest['PdDistrict'] = le1.transform(test['PdDistrict'])\n\nle2 = LabelEncoder()\nX = train.drop(columns=['Category'])\ny= le2.fit_transform(train['Category'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# sklearn의 PolynomialFeatures 을 이용해 변수끼리 interaction\n- sklearn의 PolynomialFeatures 함수를 이용해 피쳐끼리 곱해줘 새로운 피쳐를 만들어 준다.\n- 서로 상관이 없어 보이던 피쳐들도 서로 곱해지면 유의미한 변수가 될 수 있음\n- ___하지만 여기에서는 이 작업을 해도 스코어가 비슷했음___\n- ___그러므로 시간을 절약하고, 컴퓨팅 파워를 절약하기위해 생략___"},{"metadata":{},"cell_type":"markdown","source":"### 위경도 데이터끼리 interaction"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# #모둔 조합에 대해서 degree 2로 제곱(중첩되는 것은 날려줌, 그러넫 자신을 제곱하는 항이 있냐 없냐는 F하면 제곱\n# #하는데 의미 없다고 생각함),\n# #인털엑션하고 나머지 바이어스는 뺀다. #self interaction을 \n\n# v1 = ['X','Y','coord_pca1','coord_pca2']\n\n# poly = PolynomialFeatures(degree=2, interaction_only=False, include_bias=False)\n\n# interactions = pd.DataFrame(data=poly.fit_transform(train[v1]), columns=poly.get_feature_names(v1))\n# #poly(곱하는 거에)train[v]를 fit시키고 data로 지정해주고,\n# #컬럼을 poly식에서 피쳐 네임을 뽑아준다.\n\n# #\n# interactions.drop(v1, axis=1, inplace=True)  # Remove the original columns\n\n\n# # Concat the interaction variables to the train data\n# print('Before creating interactions we have {} variables in train'.format(train.shape[1]))\n\n# #여기서 중요한거는 엑스트라 변수를 어디다가 더해주냐인데 axis가 1(열)로 해줘야지 옆으로 붙음 axis=0 이면 아래로 붙어서\n# train = pd.concat([train, interactions], axis=1) \n# print('After creating interactions we have {} variables in train'.format(train.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" ### 시간 관련 데이터끼리 interaction****"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# #모둔 조합에 대해서 degree 2로 제곱(중첩되는 것은 날려줌, 그러넫 자신을 제곱하는 항이 있냐 없냐는 F하면 제곱\n# #하는데 의미 없다고 생각함),\n# #인털엑션하고 나머지 바이어스는 뺀다. #self interaction을 \n\n# # v = ['X','Y','coord_pca1','coord_pca2']\n# v = ['n_days','Day','Month','Year','Hour','Minute']\n# poly = PolynomialFeatures(degree=2, interaction_only=False, include_bias=False)\n\n# interactions = pd.DataFrame(data=poly.fit_transform(train[v]), columns=poly.get_feature_names(v))\n# #poly(곱하는 거에)train[v]를 fit시키고 data로 지정해주고,\n# #컬럼을 poly식에서 피쳐 네임을 뽑아준다.\n\n# #\n# interactions.drop(v, axis=1, inplace=True)  # Remove the original columns\n\n\n# # Concat the interaction variables to the train data\n# print('Before creating interactions we have {} variables in train'.format(train.shape[1]))\n\n# #여기서 중요한거는 엑스트라 변수를 어디다가 더해주냐인데 axis가 1(열)로 해줘야지 옆으로 붙음 axis=0 이면 아래로 붙어서\n# #바보같아질 수 있음 \n# train = pd.concat([train, interactions], axis=1) \n# print('After creating interactions we have {} variables in train'.format(train.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# lightGBM"},{"metadata":{"trusted":true,"_uuid":"ce0040eca79c729365b71b4b9a5ae2e5685db7ce"},"cell_type":"code","source":"train_data = lgb.Dataset(X, label=y, categorical_feature=['PdDistrict', ])\nparams = {'boosting':'gbdt',\n          'objective':'multiclass',\n          'num_class':39,\n          'max_delta_step':0.9,\n          'min_data_in_leaf': 20,\n          'learning_rate': 0.29,\n          'max_bin': 501,\n          'num_leaves': 41,\n          'verbose' : 1}\n\nbst = lgb.train(params, train_data, 120)\npredictions_lgb = bst.predict(test)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = lgb.Dataset(X, label=y, categorical_feature=['PdDistrict', ])\nparams = {'boosting':'gbdt',\n          'objective':'multiclass',\n          'num_class':39,\n          'max_delta_step':0.9,\n          'min_data_in_leaf': 4,\n          'learning_rate': 0.29,\n          'max_bin': 501,\n          'num_leaves': 41,\n          'verbose' : 1}\n\nbst = lgb.train(params, train_data, 120)\npredictions_lgb1 = bst.predict(test)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions_lgb1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions_lgb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"con2 = (predictions_lgb + predictions_lgb1)/2\ncon2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# XGboost"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# # train_xgb = xgb.DMatrix(X, label=y)\n# # test_xgb  = xgb.DMatrix(test)\n# dtrain = xgb.DMatrix(X, label=y)\n# dtest  = xgb.DMatrix(test) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# params = {\n#     'max_depth': 5, \n#     'eta': 0.3,  \n#     'num_boost_rounds' : 150 ,\n#     'silent': 1, \n#     'objective': 'multi:softprob',  \n#     'eval_metric' : 'mlogloss',\n#     'learning_data' : 0.07,\n#     'num_class': 39,\n#     'min_child_weight':1,\n# }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# model_xgb = xgb.train(params, dtrain, 10)\n# predictions_xgb = model_xgb.predict(dtest)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#하이퍼 파라미터 테스트의 수행 속도를 향상시키기 위해 n_estimators 를 100으로 감소\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# GridSearch\n- 하이퍼 파라미터를 튜닝하는데 최적의 파라미터를 찾아준다\n- 단점으로 시간이 매우 오래 걸린다."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# predictions_xgb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# con1 = (predictions_lgb + predictions_xgb)/2\n# con1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame(con2, columns=le2.inverse_transform(np.linspace(0, 38, 39, dtype='int16')),\n                          index=test.index)\n \nsubmission.to_csv('submission.csv', index='Id')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}