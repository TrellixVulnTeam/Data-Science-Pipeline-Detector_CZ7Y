{"cells":[{"metadata":{"_cell_guid":"60ba07a5-5adf-48f2-82fc-f8148ff37e35","_uuid":"2f04e8d0100200bebf7da8f342bee3e308499f47"},"cell_type":"markdown","source":"# San Francisco Crime Classification\n\nFrom 1934 to 1963, San Francisco was infamous for housing some of the\nworld's most notorious criminals on the inescapable island of Alcatraz.\nToday, the city is known more for its tech scene than its criminal past. But,\nwith rising wealth inequality, housing shortages, and a proliferation of\nexpensive digital toys riding BART to work, there is no scarcity of crime in\nthe city by the bay.\n\n--------------------------------"},{"metadata":{"_cell_guid":"7fec2129-1d5a-4120-bb2b-45268edcf816","_uuid":"59f3245cd14cf94457a5af62b564cfff3b60f7a2"},"cell_type":"markdown","source":"# Data Exploration\nIn this section, We will load data and see it's content.\n\n## 1. Loading the data files"},{"metadata":{"_cell_guid":"117474b2-ba55-4c83-8a63-100f50ab1d87","collapsed":true,"_uuid":"13b469b03dbf105e379c926ee67e02be608d227d","trusted":false},"cell_type":"code","source":"# import pandas\nimport pandas as pd\n\n# load train and test as dataframes\ntrain_df = pd.read_csv(\"../input/train.csv\")\ntest_df = pd.read_csv(\"../input/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"e2b1f0f2-0a5c-4ce1-bd75-5b84bb7c1ecd","_uuid":"db6b3c2831b30c87bce4ea9565441ea33bdc69eb"},"cell_type":"markdown","source":"## 2. Showing data"},{"metadata":{"_cell_guid":"4922203f-3689-4944-877e-615f01a94e27","_uuid":"44c8a4e9aed01fc255a0993c06828463ce919f4e","_kg_hide-output":false,"trusted":false,"collapsed":true},"cell_type":"code","source":"# import display function from ipython\nfrom IPython.display import display, HTML\n\n# display the first rows of each dataset\ndisplay(train_df.head())\nprint(\"train shape: {}\".format(train_df.shape))\ndisplay(test_df.head())\nprint(\"test shape: {}\".format(test_df.shape))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"036af9c1-e026-4109-8bde-9a706af49978","_uuid":"8a197135b65b6ef99418beceacfca1755602eea2"},"cell_type":"markdown","source":"* As shown above, The test dataset doesn't contain 3 columns {Category, Descript, Resolution} as the category column is the target column to found in the test set.\n* The test set will be used for testing only at the end of project\n* Training data will be divided into training and testing -Validation- sets to train the models later."},{"metadata":{"_cell_guid":"c81a5de0-bab7-45ca-a284-abdcec5dd8f0","_uuid":"c3a600635bddfeeed4540e7cc373abacfdc0c857"},"cell_type":"markdown","source":"----\n# Data visualizing and preprocessing\nIn this section, We will visualize data and remove unnecessary data."},{"metadata":{"_cell_guid":"03f33634-75f1-4885-8da4-c1eb6a9a99c2","_uuid":"4c4ec775553da8f1c524402fafb00c95e78ff586"},"cell_type":"markdown","source":"## 1. Removing redundant features\nAs shown above, there exist 2 columns -features- that are considered as redundancy. `Descript` and `Resolution` are these 2 columns as they don't exist in the testing values and also not a label required from the models, so they should be removed."},{"metadata":{"_cell_guid":"55cfa5a6-649e-4962-ae2a-1f109a52dcf5","_uuid":"cc7063622c5c1f67699a6fa685f425e55346d593","trusted":false,"collapsed":true},"cell_type":"code","source":"train_df = train_df.drop( columns = ['Descript', 'Resolution'] )\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"1feead3f-8340-4d04-802d-b31a416ebd6d","_uuid":"6b7927d6894a7d9f0b8057ba2ed42f5d545eb977"},"cell_type":"markdown","source":"## 2. Removing redundant rows\nNow, lets visualize location information"},{"metadata":{"_cell_guid":"31205136-96bf-42ff-981c-68c0eacd0a1f","_uuid":"4ab0e41ec1d987ff13e9b2210b9f6f29b76d0ab2","trusted":false,"collapsed":true},"cell_type":"code","source":"# lets see the statistics summary of locations\nlons = train_df['X'] # longitudes \nlats = train_df['Y'] # latitudes\n\nprint (\"Longitudes summary:\")\nprint (lons.describe())\nprint (\"\\nLatitudes summary:\")\nprint (lats.describe())","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"3903a2e5-b4ee-4d54-ae47-f688a6cdb0d5","_uuid":"9d307038e14c27b806ec556fa05e605afc034d44"},"cell_type":"markdown","source":"-----------------------\n**Observation:**\n* longitudes are between [-122.52, -120.5], each value different slightly from others\n* latitudes are between [37.708, 90]\n* here as shown there exist some bad values -i.e. close to 90-, the reasons that this is bad that \n    * first, san fransisco latitudes are between [37.707, 37.83269] , reference: [google maps](https://www.google.com.eg/maps/place/San+Francisco,+CA,+USA/@37.7407396,-122.4303937,12z/data=!4m5!3m4!1s0x80859a6d00690021:0x4a501367f076adff!8m2!3d37.7749295!4d-122.4194155)\n    * second as shown in the statistics that the most values are close to 37.7\n    * Also in longitudes, san fransisco longitudes are between [-122.517652, -122.3275], from google maps\n\nNow, to demonstrate the locations, let's plot them using scatter plot"},{"metadata":{"_cell_guid":"1444ef96-1d32-4252-9ad9-a325c097910e","_uuid":"e623b1860dbfcb52047395fbd39819e69d223a21","trusted":false,"collapsed":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n\nplt.scatter(lons, lats)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"cab4a57c-6cfd-4eb7-9d9d-2f2a54e38325","_uuid":"8ffc65d894ad433a2ea2c9adedf8505621cc7e29"},"cell_type":"markdown","source":"This also shows the bad points that are close to 90 in latitudes and close to -120 longitudes so we will execlude these values"},{"metadata":{"_cell_guid":"43f95eb5-e4b3-4311-b6ed-40426cf1fc3f","_uuid":"11f34b50dcc3c816c1a2c680fa71f176495e74f4","trusted":false,"collapsed":true},"cell_type":"code","source":"# eliminate rows with latitudes out of San Francisco range\ntrain_df = train_df.drop(train_df[(train_df['Y'] > 37.84) | (train_df['Y'] < 37.7)].index)\n# eliminate rows with longitudes out of San Francisco range\ntrain_df = train_df.drop(train_df[((train_df['X'] > -122.32) | (train_df['X'] < -122.52))].index)\ntrain_df.describe()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"cb1da62b-b4bb-4beb-96eb-f8fbb96d701c","_uuid":"5b9dfc1114889a135ccb6f39090b65ab073031d9"},"cell_type":"markdown","source":"## 3. Visualize according to locations"},{"metadata":{"_cell_guid":"c15ab9a0-81cd-4c25-b629-cab9b9f0bab7","_uuid":"d8039cfeea16079f6173f766b80e8917d8043816","trusted":false,"collapsed":true},"cell_type":"code","source":"new_lons = train_df['X'] # longitudes \nnew_lats = train_df['Y'] # latitudes\n\n# scatter plot for lons vs lats\nplt.scatter(new_lons, new_lats)\nplt.xlabel('lons')\nplt.ylabel('lats')\nplt.show()\n\n# histogram plot for lons and lats\nplt.hist(new_lons)\nplt.xlabel('lons')\nplt.ylabel('ocurrance number')\nplt.show()\nplt.hist(new_lats)\nplt.xlabel('lats')\nplt.ylabel('ocurrance number')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"5a7ec005-6a12-4d7a-8af7-0a0ad9f63be7","_uuid":"ea16e60c3c4980b5148779fec8b53464bec9a29a"},"cell_type":"markdown","source":"**Observation:**  the most crimes are in the location of longitude = [-122.44, -122.40] and latitude = [37.76, 37.80]"},{"metadata":{"_cell_guid":"1cceb05e-8f04-4aa7-9163-34dfe1a6bedb","_uuid":"84ffd0d424c972938a02ab90f4b493536d33aff7"},"cell_type":"markdown","source":"---\nFinding number of occurances of for each category in data"},{"metadata":{"_cell_guid":"6163558f-5e95-40ce-833d-57b897ffb66a","_uuid":"9b9f00097a2b79d278063bef07068bc67b891343","trusted":false,"collapsed":true},"cell_type":"code","source":"from collections import Counter\n\ndef printCategoriesOccurrence():\n    \n    categories = train_df['Category']\n    # count the number of occurances for each category\n    occurances = Counter(categories)\n    sorted_occ = sorted(occurances.items(), key=lambda pair: pair[1], reverse=True)\n    for key, value in sorted_occ:\n        print (key, value)\n    return sorted_occ\n        \nsorted_occ = printCategoriesOccurrence()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"0cd0242c-0001-4688-8632-d40605061288","_uuid":"21e77adbb839e7f98f8d7ba842cc265f12475d5a"},"cell_type":"markdown","source":"**Observation:**\nThe most committed crime in San Francisco is the LARCENY/THEFT. TREA is the least."},{"metadata":{"_cell_guid":"42ed54fa-dac4-4e74-a223-bc621859019c","_uuid":"870c0da75a5b1d07b4b1f5dfc29cf2c4b138a6cb"},"cell_type":"markdown","source":"----\nNow, We will plot crimes categories data on map using basemap library from mpl_toolkits."},{"metadata":{"_cell_guid":"ae64d8d6-b7c6-4e29-9108-f6a785529aad","_uuid":"3c1339fddb60753aec576fac43ee216e91b8a308","trusted":false,"collapsed":true},"cell_type":"code","source":"from mpl_toolkits.basemap import Basemap\nimport numpy as np\n# minimum and maximum longitude and latitude for map zooming\nlon_min = min(new_lons) \nlon_max = max(new_lons) \nlat_min = min(new_lats) \nlat_max = max(new_lats) \n\nfig = plt.figure(figsize=(24,12)) # to make plot bigger\nfig.add_subplot(111, frame_on=False)\n\n# Here, we add some padding with 0.01 to the map width and height\nmap = Basemap(\n    llcrnrlon=lon_min-0.01,\n    llcrnrlat=lat_min-0.01,\n    urcrnrlon=lon_max+0.01,\n    urcrnrlat=lat_max+0.01\n)\n\nparallels = np.arange(37,38,0.02)\nmeridians = np.arange(-122.6,-122.3,.02)\nmap.drawcounties()\nmap.drawparallels(parallels,labels=[False,True,True,False])\nmap.drawmeridians(meridians,labels=[True,False,False,True])\nmap.scatter(new_lons, new_lats)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"631ba1c2-e5f6-42fa-9ff5-98e568b436d7","_uuid":"c4388ec9961ff74b2a0e05cf6ee1af75bcb8eeb5"},"cell_type":"markdown","source":"Using contour plot"},{"metadata":{"_cell_guid":"8b2c1e27-7bc5-4f10-8526-e4ea6fb2c62f","_uuid":"3e6d2d17ea40d6fb1ff182429278af557ab52e07","trusted":false,"collapsed":true},"cell_type":"code","source":"data = train_df.groupby(['X', 'Y']).size().reset_index(name='occurances')\ndata = data[data.occurances < 500]\n\nfig = plt.figure(figsize=(24,12)) # to make plot bigger\nfig.add_subplot(111, frame_on=False)\n\n# Here, we add some padding with 0.01 to the map width and height\nmap = Basemap(\n    llcrnrlon=lon_min-0.01,\n    llcrnrlat=lat_min-0.01,\n    urcrnrlon=lon_max+0.01,\n    urcrnrlat=lat_max+0.01\n)\n\nparallels = np.arange(37,38,0.02)\nmeridians = np.arange(-122.6,-122.3,.02)\nmap.drawcounties()\nmap.drawparallels(parallels,labels=[False,True,True,False])\nmap.drawmeridians(meridians,labels=[True,False,False,True])\n# map.scatter(new_lons, new_lats)\nx = data.X.as_matrix()\ny = data.Y.as_matrix()\nz = data.occurances.as_matrix()\nmymap= map.contour(x, y, z, tri=True)\nmap.colorbar(mymap,location='bottom',pad=\"5%\")\nplt.title(\"size=\"+str(15))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"00f85366-ba48-45de-bcb0-025be39672ac","_uuid":"9434bbf2cf2dd6645357728c68e60e9ea6db89d2"},"cell_type":"markdown","source":"The plots above don't till good information so we will plot each crime category alone in descent order i.e. most committed category first."},{"metadata":{"scrolled":false,"_cell_guid":"1fe5a264-be28-468d-b7bd-ac1332b71bff","_uuid":"3150cba36483a42b59b5e5fc115a66f530203b5d","trusted":false,"collapsed":true},"cell_type":"code","source":"from matplotlib import gridspec\n\nplt.subplots(figsize=(24, 78))\ni = 0 # subplot number\n\ngrid = gridspec.GridSpec(13,3)\nfor pair in sorted_occ[:]:\n    ax = plt.subplot(grid[i])\n    map = Basemap(\n        llcrnrlon=lon_min-0.01,\n        llcrnrlat=lat_min-0.01,\n        urcrnrlon=lon_max+0.01,\n        urcrnrlat=lat_max+0.01\n    )\n    parallels = np.arange(37,38,0.02)\n    meridians = np.arange(-122.6,-122.3,.02)\n    map.drawcounties()\n    map.drawparallels(parallels,labels=[False,True,True,False])\n    map.drawmeridians(meridians,labels=[True,False,False,True])\n    category_data = train_df[train_df['Category'] == pair[0]]\n    \n    lons = category_data['X']\n    lats = category_data['Y']\n    map.scatter(lons, lats)\n    plt.title(pair[0]) # pair[0] = category name\n    i+=1\n\nplt.show()\n    ","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"63f479bf-997e-4662-bfa2-73d81b7cf513","_uuid":"304e15883727c4ba6fbd3e61450454268256f669"},"cell_type":"markdown","source":"**Observation:** The maps above till us that there exist some crimes that is focused on certain locations, for example `prostitution` is focused mostly in (37.79N, -122.41W) and (37.76N, -122.41W). Other crimes are spread into the map like `Driving under the influence`."},{"metadata":{"_cell_guid":"37490396-ecb5-4eb1-a83b-f9c83dcc5d6e","_uuid":"6c52c7a62bc58f7d55f6e5ac6da331053555beeb"},"cell_type":"markdown","source":"## 4. Visualize according to time\nVisualizing each crime category according to months and week days. We will do this for the most common crime only -i.e. `LARCENY/THEFT`- as example to try to find some pattern according to time.\n\nFirst, according to each month"},{"metadata":{"scrolled":false,"_cell_guid":"0c49ed1c-aff1-4931-ad4f-b60b6951c928","_uuid":"4c60daaec923433c63d29f4a42d7fa1fd43422ae","trusted":false,"collapsed":true},"cell_type":"code","source":"import re\n\n    \nplt.subplots(figsize=(12, 36))\ni = 0 # subplot number\n\nmonths = ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12']\ncategory = sorted_occ[0][0]\ncategory_data = train_df[train_df['Category'] == category]\n\ndef getMonthData(month):\n    regex = \"((19[0-9][0-9])|(20[0-9][0-9])-\" + month + \"-[0-9]* (\\w|:)*)\"\n    columns = ['Dates', 'Category', 'DayOfWeek', 'PdDistrict', 'Address', 'X', 'Y']\n    result = pd.DataFrame(columns=columns)\n    for index, row in category_data.iterrows():\n        if re.match(regex,row['Dates']):\n            row = row.transpose()\n            result = result.append(row)\n            category_data.drop(index)\n    return result\n\ngrid = gridspec.GridSpec(6,2)\nfor month in months:\n    ax = plt.subplot(grid[i])\n    map = Basemap(\n        llcrnrlon=lon_min-0.01,\n        llcrnrlat=lat_min-0.01,\n        urcrnrlon=lon_max+0.01,\n        urcrnrlat=lat_max+0.01\n    )\n    parallels = np.arange(37,38,0.02)\n    meridians = np.arange(-122.6,-122.3,.02)\n    map.drawcounties()\n    map.drawparallels(parallels,labels=[False,True,True,False])\n    map.drawmeridians(meridians,labels=[True,False,False,True])\n    month_data = getMonthData(month)\n    lons = month_data['X']\n    lats = month_data['Y']\n    map.scatter(lons, lats)\n    plt.title(month + \"  size=\" + str(len(month_data)))\n    i+=1\n\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"23973071-61ca-4d13-817a-fc25480ac0db","_uuid":"a1844fd2be386b4832f112a3f036bc5e80aefbf2"},"cell_type":"markdown","source":"according to days of week"},{"metadata":{"scrolled":false,"_cell_guid":"7383744b-c985-4720-82ce-40864e98ef7a","_uuid":"cb63980f4d03d7e6ffdadaafd209065a74f7142a","trusted":false,"collapsed":true},"cell_type":"code","source":"plt.subplots(figsize=(12, 36))\ni = 0 # subplot number\n\ndays = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\ncategory = sorted_occ[0][0]\ncategory_data = train_df[train_df['Category'] == category]\n\ngrid = gridspec.GridSpec(7,1)\nfor day in days:\n    ax = plt.subplot(grid[i])\n    map = Basemap(\n        llcrnrlon=lon_min-0.01,\n        llcrnrlat=lat_min-0.01,\n        urcrnrlon=lon_max+0.01,\n        urcrnrlat=lat_max+0.01\n    )\n    parallels = np.arange(37,38,0.02)\n    meridians = np.arange(-122.6,-122.3,.02)\n    map.drawcounties()\n    map.drawparallels(parallels,labels=[False,True,True,False])\n    map.drawmeridians(meridians,labels=[True,False,False,True])\n    day_data = category_data[category_data['DayOfWeek'] == day]\n    lons = day_data['X']\n    lats = day_data['Y']\n    map.scatter(lons, lats)\n    plt.title(day + \"  size=\" + str(len(day_data)))\n    i+=1\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"8a225f24-c803-4350-8b52-13fe9ae8d290","_uuid":"524b8c0960d26259dc849decfc63673b249a745c"},"cell_type":"markdown","source":"**Observation:** As shown in plots above the crime pattern is likely to be equally ditributed on months and days of week. Some months like `April` and `May` have more crime rate than others like `December` and `Junauary`, and logically this may be due to the weather conditions that is good at spring and bad in winter.\nAlso days that before the week end is more than other days a little. "},{"metadata":{"_cell_guid":"ce0b7652-cb6c-4f0d-8788-101d99959d41","_uuid":"6301a06b0f24cc7ca71825aaeea822927f0c9f28"},"cell_type":"markdown","source":"## 5. Data preprocessing"},{"metadata":{"_cell_guid":"1821461c-01a0-40f9-ae5f-fddf754e1af4","_uuid":"68f445c9ee5bf0d8426ddf9c4801cbfebdae4915"},"cell_type":"markdown","source":"**Enhanceing data imbalancing:**\nNow as shown above, The data is imbalanced and some features has very low number of contributions in data existance. And to enhance imbalanced data we can replicate the data with low contributions and give them more weights in training.\n\nHere we will replicate the data that has less than 1000 occurances to be 1000. Later we will give them more weights while training."},{"metadata":{"_cell_guid":"dea5830d-32c2-4f99-a49a-6e9b145eceb9","_uuid":"8ea8e137815e6bf6a462a5d800910b6bf02b577c","trusted":false,"collapsed":true},"cell_type":"code","source":"import math\n# if category size < 1000, duplicate it to be = 1000\nfor key,value in sorted_occ:\n    if value<1000:\n        \n        temp = train_df[train_df['Category'] == key]\n        train_df = train_df.append([temp]*int(math.ceil((1000-value)/float(value))), ignore_index=True)\n\nsorted_occ = printCategoriesOccurrence()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"ef191dae-918e-48ac-b384-2d6dcc229f5a","_uuid":"2834f83f4aac6bbdd8e90e629cbfa6428ac3f1aa"},"cell_type":"markdown","source":"### Data Encoding"},{"metadata":{"_cell_guid":"9d7eebbe-b1f0-4305-85a8-f3fea640ed29","collapsed":true,"_uuid":"c5dbfdd9625e99ffc6c109e05fb512702b4f92e8","trusted":false},"cell_type":"code","source":"# spliting train data into target and other features\ntarget = train_df['Category']\ndata = train_df.drop(columns=['Category'])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"6ae7fa62-4ea5-412b-b5a1-99917c1d8258","_uuid":"afa75544bb7bdcd6318f72dd1fd7ea7e355db423"},"cell_type":"markdown","source":"4 features from the data given are not numbers, so we need to convert them into numbers in order to be able to train the models on them. These features are:\n* Dates\n* DayOfWeek\n* PdDistrict\n* Address\n\nWe can use label Encoding and 1-hot encoding. 1-hot encoding may produce very high numbers of dimensions due to the many data labeles in each feature, but it is better due to the problem with label encoding is that it assumes higher the categorical value, better the category which produce more errors.\n\nSo let's use one-hot encoding with the features with little unique values and use the label encoding with the features with very high unique values\n"},{"metadata":{"_cell_guid":"34440854-b3ba-40aa-ac39-15a9e1514451","_uuid":"1c36df4b394874f70e20b4768e08d903a3522830","trusted":false,"collapsed":true},"cell_type":"code","source":"features = ['Dates', 'DayOfWeek', 'PdDistrict', 'Address']\nfor feature in features:\n    print (\"feature: {}    unique_size: {}\".format(feature ,len(data[feature].unique())))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"92be4f06-8e02-435a-9350-17be87fb3fbf","_uuid":"4a9d6d6539cf36b8a0e3433e36d2c7497b2bac2f"},"cell_type":"markdown","source":"As shown above, `DayOfWeek` and `PdDistrict` can one-hot encoded. But `Address`, `Dates` should be encoded using label encoding.\n\nFor `Dates`, lets neglect the miniutes and seconds of each date as this will not affect greatly on our predictions but on the contrary it may produce good environment for overfitting, so let's convert `Dates` by our hand to integers then convert `Address` using `cat.codes` tool.\n\n### 5.1 Label Encoding"},{"metadata":{"_cell_guid":"3b229e09-b7d5-4025-b0c8-32aa7c3c88c9","_uuid":"bd44b03abdd973666e0574b208b3ab116eac37f4","trusted":false,"collapsed":true},"cell_type":"code","source":"# convert given list of dates it will trim seconds, minutes and return result\ndef trimMinAndSecFromDates(dates):\n    result = []\n    for date in dates:\n        result.append(date[:-6])\n    return result\n\n# trim minutes and seconds from dates\ndata['Dates'] = trimMinAndSecFromDates(data['Dates'])\n\n# encode Dates using label encoding\ndata['Dates'] = data['Dates'].astype('category')\ndata['Dates_int'] = data['Dates'].cat.codes\n\n# encode Address using label encoding\ndata['Address'] = data['Address'].astype('category')\ndata['Address_int'] = data['Address'].cat.codes\n\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"0a3e7a34-d0ad-4d25-b29c-9792e4f65390","_uuid":"53df070cd3d35da449a37852b007348cf1ff20d0"},"cell_type":"markdown","source":"Now let's drop the `Dates` and `Address` cols as they are now useless."},{"metadata":{"_cell_guid":"53050be1-f0d0-40f5-9ac9-21fa8e484c92","_uuid":"e009d88bf2f4472dd5c0e5d6c8bbdb5265d0e3d1","trusted":false,"collapsed":true},"cell_type":"code","source":"data.drop(columns=['Dates', 'Address'], inplace=True)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"21c93181-c41f-48af-9fe6-24a530c60f01","_uuid":"96a9cede277d603f7fb8d00f98cd707b3c101360"},"cell_type":"markdown","source":"### 5.2 One-Hot Encoding\nNow let's one hot encode the `DayOfWeek` and `PdDistrict` using pandas.get_dummies()"},{"metadata":{"_cell_guid":"b235bf00-83c5-4d3e-bfbc-b66ef080f1da","_uuid":"802db4f14e26c10678e30f385ec5af24f875ca5e","trusted":false,"collapsed":true},"cell_type":"code","source":"# get dummies for each feature\nDayOfWeek_dummies = pd.get_dummies(data['DayOfWeek'])\nPdDistrict_dummies = pd.get_dummies(data['PdDistrict'])\n\n# join dummies to the original dataframe\ndata = data.join(DayOfWeek_dummies)\ndata = data.join(PdDistrict_dummies)\n\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"6196f1ca-f7b0-471c-a9ee-a158d47471a1","_uuid":"5f633966b1ab06a9fb00f28157ee15c9d525d35b"},"cell_type":"markdown","source":"Now let's drop the `DayOfWeek` and `PdDistrict` cols as they are now useless."},{"metadata":{"_cell_guid":"bd7c63d3-c414-40ed-aa46-8b7522f6d344","_uuid":"96c67ba0521a9f61e7d6133814fa332e271e901e","trusted":false,"collapsed":true},"cell_type":"code","source":"data.drop(columns=['DayOfWeek', 'PdDistrict'], inplace=True)\nprint (\"data size =\",len(data))\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"e56cb04c-0001-4ae6-a396-b05d01a94ff2","_uuid":"1236dc8c6b1fbef4ceb81c51f05d29313743b07f"},"cell_type":"markdown","source":"**Observation:** Now the training data are ready to be used for our models with 21 dimensions and 877982 samples to be traind on"},{"metadata":{"_cell_guid":"a51d9639-4733-4063-9fa6-259c3c8288b9","_uuid":"0be9f0d483a2fcdaff87cba3868331e6ad9bd081"},"cell_type":"markdown","source":"## 5. Data splitting\nsplit training data into train set with size 80% and test set with size 20% - i.e. validation set."},{"metadata":{"_cell_guid":"ded7c567-4226-469e-aad0-b05dd654e9e8","_uuid":"b859c67be975362b0e329c2dd937dfc02eb92c8b","trusted":false,"collapsed":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test =  train_test_split(data, target, test_size=0.2, random_state=0, stratify=target)\nprint (\"train size: {}, test size: {}\".format(X_train.shape[0], X_test.shape[0]))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"e83dd380-c734-4f37-9e21-e8588c3880f3","_uuid":"94e5d2fa3febaaceb8e02bb453f8e78f791bde91"},"cell_type":"markdown","source":"\n--------------\n# Models Implementation"},{"metadata":{"_cell_guid":"990646da-a28c-4b39-816b-ccb00c0fd1cb","_uuid":"1719937483eeb3e2050801647697ab54eb3c5184"},"cell_type":"markdown","source":"We are going to implement various models and train them on our data and comapre their performance later using using mult-class logarithmec loss."},{"metadata":{"_cell_guid":"ec9c1894-dacd-49cb-87c3-f50df4219fd4","_uuid":"ca946beeda345b82a4f940842dfc82470b6dd4fc"},"cell_type":"markdown","source":"## 1. Implementing Models\nIn this section, we will train various model on data."},{"metadata":{"_cell_guid":"94b90367-d88d-4c29-b5c8-5a8645c892ae","_uuid":"35cc391dbea5f527035aa9d72014560d0a88b4f3"},"cell_type":"markdown","source":"### Training pipline\nNow, we will train various learners, so we will make a pipline -i.e function- to call it in training the models instead of repeating it with each model. \n\nThe function returns a result dict which contains the train time, fbeta score and log loss of all samples in train data.\n\n**Hint:** We shouldn't use accuracy here as the data is imbalanced."},{"metadata":{"_cell_guid":"f4da157d-e205-48a7-a569-c7479df3773a","collapsed":true,"_uuid":"5d1fb245d49a7f01f7713524dec864992c18a3ba","trusted":false},"cell_type":"code","source":"from sklearn.metrics import log_loss, fbeta_score\nfrom time import time\n\n# train function takes learner, the train data and target\ndef train_test_pipeline(learner, X_train, y_train, X_test, y_test):\n    \n    results = {}\n    \n    # training learner\n    start = time()\n    learner.fit(X_train, y_train)\n    end = time()\n    results['train_time'] = end - start\n    \n     # remove missed classes after fitting for logloss\n    for category in list(set(target) - set(learner.classes_)):\n        X_train = X_train.drop(y_train[y_train == category].index)\n        y_train = y_train[y_train != category]\n        X_test = X_test.drop(y_test[y_test == category].index)\n        y_test = y_test[y_test != category]\n    # predict samples in training set\n    predictions = learner.predict(X_train)\n    predictions_proba = learner.predict_proba(X_train)\n    \n    # calculate fbeta and log loss\n    results['fscore'] = fbeta_score(y_train, predictions, beta=.5, average='micro')\n    results['logloss'] = log_loss(y_train, predictions_proba)\n    \n    # predict testing samples and time of prediction\n    start = time()\n    predictions = learner.predict(X_test)\n    predictions_proba = learner.predict_proba(X_test)\n    end = time()\n    results['test_time'] = end - start\n    \n    # calculate fbeta and log loss for testing set\n    results['fscore_test'] = fbeta_score(y_test, predictions, beta=.5, average='micro')\n    results['logloss_test'] = log_loss(y_test, predictions_proba)\n    \n    \n    print (\"{} trained\".format(learner.__class__.__name__))\n    \n    return results\n\n# do train_test_pipeline then visualize resutls for given models on first n samples and test on first m\n# returns predictions proba for last model in given list - this will be used for 1 model only -\ndef train_test_models(models, names=None, n=len(y_train), m=len(y_test)):\n    results = {}\n    i = 0\n    for model in models:\n        if not names:\n            model_name = model.__class__.__name__\n        else:\n            model_name = names[i]\n            i += 1\n        results[model_name] = train_test_pipeline(model, X_train[:n], y_train[:n], X_test[:m], y_test[:m])\n\n    # print results\n    for model in results:\n        model_res = results[model]\n        print (\"model: {}\".format(model))\n        print (\"fscore:\\t\\t{}\\nlogloss:\\t{}\\ntrain time:\\t{}\".format(model_res['fscore'], model_res['logloss'], model_res['train_time']))\n        print (\"fscore_test:\\t\\t{}\\nlogloss_test:\\t{}\\ntest time:\\t{}\".format(model_res['fscore_test'], model_res['logloss_test'], model_res['test_time']))\n\n    # visualize the results    \n    visualize(results, random_results)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b81d9fb7-3eff-4f9c-87d0-a397d839f1b7","_uuid":"ac8827f37caf706dd1b56edf553ca4201a8d53ac"},"cell_type":"markdown","source":"### Visualization\nvisualize training and testing results"},{"metadata":{"_cell_guid":"ece7cc50-825d-4262-8ee9-715e7f198321","collapsed":true,"_uuid":"d9f3c1575e591bb77c207d5f7685f274339d5fb4","trusted":false},"cell_type":"code","source":"import matplotlib.pyplot as plt\ndef visualize(results, random_results):\n    bar_width = 0.3\n    fig, ax = plt.subplots(6,1,figsize = (12,32))\n    for j, metric in enumerate(['train_time', 'fscore', 'logloss', 'test_time', 'fscore_test', 'logloss_test']):\n        ax[j].set_xlabel(\"Learners\")\n        ax[j].set_ylabel(metric)\n        ax[j].set_title(metric)\n        for k, learner in enumerate(results.keys()):\n            ax[j].bar(learner, results[learner][metric], width=bar_width)\n    \n    # add horizontal line for random model results\n    ax[0].axhline(y=random_results['train_time'], linestyle='dashed')\n    ax[1].axhline(y=random_results['fscore'], linestyle='dashed')\n    ax[2].axhline(y=random_results['logloss'], linestyle='dashed')  \n    ax[3].axhline(y=random_results['test_time'], linestyle='dashed')\n    ax[4].axhline(y=random_results['fscore_test'], linestyle='dashed')\n    ax[5].axhline(y=random_results['logloss_test'], linestyle='dashed')        ","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"3d56d739-ea30-4fdd-ab58-0a0bf009eed1","_uuid":"49ee897f61936ba7a315d42c3c16ea0ad929b360"},"cell_type":"markdown","source":"### 1.1 Naive Random Predictor\nRandom predictor which always predict the category randomly."},{"metadata":{"_cell_guid":"03efec5a-1c1d-4816-abcd-bac934ea774c","collapsed":true,"_uuid":"88e445db302b7dd2970d3169c768f450c27ba493","trusted":false},"cell_type":"code","source":"import random\nclass random_model:\n \n    def __init__(self, categories):\n        self.categories = categories\n        self.classes_ = categories\n\n    # always return a random value from categories\n    def __getRandomValue(self): return random.choice(self.categories) \n    \n    # no need for fit here\n    def fit(self, X_train, y_train): pass\n    \n    def predict(self, X):\n        result = [[] for i in range(len(X))]\n        for j in range(len(X)):\n            result[j] = self.__getRandomValue()\n        return result\n        \n    def predict_proba(self, X):\n        result = [[] for i in range(len(X))]\n        for j in range(len(X)):\n            row = [0.0]*len(self.categories)\n            prediction = self.__getRandomValue()\n            for i in range(len(self.categories)): \n                if(self.categories[i] == prediction):\n                    row[i] = 1.0\n                    break\n            result[j] = row\n        return result","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"31c77bbd-93cc-45e3-9fbf-17c1bca8aa00","_uuid":"c0f2760e083710dfb0a2d63b7862887fb8a0098f"},"cell_type":"markdown","source":"### 1.2 other models\nIn this section, We will train other models using training pipline that we implemented previously. The models to use are:\n* KNNeighbors\n* DecisionTree\n* ExtraTrees\n* Neural network MLP\n* Support vector machine\n* xgboost"},{"metadata":{"_cell_guid":"9a5c2e81-0a92-46b4-9a79-033fc628f8bb","_uuid":"bdddb6c0981499f4f4f6a946556925ec335fb0c3"},"cell_type":"markdown","source":"#### 1.2.1 Initializing models\nHere, we enhance imbalanced data more by putting the `class_weight` parameter = `balanced` which mean that the weights will be automatically adjusted inversely proportional to class frequencies in the input data as `n_samples / (n_classes * np.bincount(y))`"},{"metadata":{"_cell_guid":"c876390e-7d98-48cb-b1e9-e62482ef1515","collapsed":true,"_uuid":"e7caa54066d9fd432b30cd6950393c60e15fc4cd","trusted":false},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.svm import SVC \nfrom xgboost import XGBClassifier\n\n# initializing models\nmodel_KNN = KNeighborsClassifier(n_jobs=-1, weights='distance')\nmodel_tree = DecisionTreeClassifier(class_weight='balanced')\nmodel_extraTrees = ExtraTreesClassifier(n_jobs=-1, class_weight='balanced')\nmodel_NN = MLPClassifier(learning_rate='invscaling', shuffle=True)\nmodel_SVC = SVC(probability=True, class_weight='balanced') # One-to-One\nmodel_XGB = XGBClassifier(one_drop=1)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"ee7e3d87-05aa-4a38-9c98-10bda838eaf3","_uuid":"bfb20058edb2afe119cbd4eafa99ad8c662499ad"},"cell_type":"markdown","source":"#### 1.2.2 get random model results"},{"metadata":{"_cell_guid":"77af7cac-6a03-4ac7-ada0-7741f29d9840","collapsed":true,"_uuid":"4d5f1e9b8e5edf30015ef7ac7ec4d7633ede2c7d","trusted":false},"cell_type":"code","source":"# get random model results\nmodel_random = random_model(categories=target.unique())\nrandom_results = train_test_pipeline(model_random, X_train, y_train, X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"834d4536-d653-455f-b7e6-2314287984b6","_uuid":"0de04559c9c24aaab600d70b72d19444aeb7eaa4"},"cell_type":"markdown","source":"#### 1.2.3 train and visualize other models with small number of samples"},{"metadata":{"_cell_guid":"25b40f84-2432-445b-bbd1-2a06db1182c8","_uuid":"4208dbe32d3cd9607f7411e59ca23d787e3fea20","trusted":false,"collapsed":true},"cell_type":"code","source":"models = [model_KNN, model_tree, model_extraTrees, model_NN, model_SVC, model_XGB]\ntrain_test_models(models, n=10000, m=2000)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"91a07d2a-8035-4748-8a80-28be87fb6f2b","_uuid":"e3c9a274fda03fb2222fe0df57b524c836ce3c75"},"cell_type":"markdown","source":"**Observation:**\n\nFor training:\n* The slowest model is SVC then XGBClassifier\n* All models do better than random predictor\n\nFor test data: \n* The best model in f1 score and logloss is XGBClassifier then SVC\n* The slowest model in testing time is SVC the XGBClassifier\n* All models do better thean random predictor\n    \nAs shown, It seems that the best model to use is XGBClassifier due to it's scores and it has a suitable time in training and testing. SVC also did well but it needs huge amount of time in processing data in training and testing."},{"metadata":{"_cell_guid":"57fbc5e5-e796-4f9f-b018-a121c0a4c4fb","_uuid":"f3095219a03df53e2dc29e0924cc9d47fdad67a4"},"cell_type":"markdown","source":"let's train XGBClassifier on all samples and see the results"},{"metadata":{"_cell_guid":"4952f880-b9a9-4a69-bbde-c3d87f4325dd","_uuid":"c385fbbd79d4dce8922d62746e38e8a2470553a0","trusted":false,"collapsed":true},"cell_type":"code","source":"# training XGBClassifier on all samples\nmodels = [model_XGB]\ntrain_test_models(models)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"6788e2f2-db78-4719-9f8b-73a10997dd40","_uuid":"956f75b2e8f8de30c97ef80b3e99cdfce0efe33c"},"cell_type":"markdown","source":"The XGBClassifier gives bad results than expected, This is due to that at first time we trained on very small numbers of training data, Now, let's try other classifiers on all data except SVC and ExtraTree due to that they need huge amount of available resources."},{"metadata":{"_cell_guid":"7264a86d-acb6-45f4-a955-2ba9fc7dd5a1","_uuid":"e560d61131aba5b81e6239c4d2824cb989d9aed8","trusted":false,"collapsed":true},"cell_type":"code","source":"# training other models on all data\nmodels = [model_KNN, model_tree, model_NN]\ntrain_test_models(models)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"3f662609-af5a-4d90-994a-4ef65b77fa2f","_uuid":"40b5198bd3010fed7e78aaaee3049a76ce18b1aa"},"cell_type":"markdown","source":"MLPClassifier gives the best results for all data from all other predictors. It almost equal to XGBClassifier but faster 6 times in training than XGBClassifier.\nNow, let's tune MLPClassifier parameters."},{"metadata":{"_cell_guid":"5f45edaa-1b54-4708-a245-3ef9e861d9d8","_uuid":"380667ab318480dc309c0abee1094980b41deefa"},"cell_type":"markdown","source":"---\n## 2 . Model Refinement\nLet's test for changing hiddenlayers, learning rate initialization value and solver algorithm used."},{"metadata":{"_cell_guid":"183e802b-f86d-49cf-bddb-cee7c011ea2c","_uuid":"323f9b5a7e366f0c133c21e80a643a1041738dd2","trusted":false,"collapsed":true},"cell_type":"code","source":"model_NN_tuned1 = MLPClassifier(learning_rate='invscaling', shuffle=True, learning_rate_init=0.001,\n                               hidden_layer_sizes=100, solver='adam')\nmodel_NN_tuned2 = MLPClassifier(learning_rate='adaptive', shuffle=True, learning_rate_init=0.001,\n                               hidden_layer_sizes=100, solver='adam')\nmodel_NN_tuned3 = MLPClassifier(learning_rate='adaptive', shuffle=True, learning_rate_init=0.001,\n                               hidden_layer_sizes=50, solver='adam')\nmodel_NN_tuned4 = MLPClassifier(learning_rate='adaptive', shuffle=True, learning_rate_init=0.0001,\n                               hidden_layer_sizes=200, solver='adam')\nmodel_NN_tuned5 = MLPClassifier(learning_rate='adaptive', shuffle=True, learning_rate_init=0.001,\n                               hidden_layer_sizes=100, solver='lbfgs')\nmodel_NN_tuned6 = MLPClassifier(learning_rate='adaptive', shuffle=True, learning_rate_init=0.001,\n                               hidden_layer_sizes=100, solver='sgd')\n\nmodels = [model_NN_tuned1, model_NN_tuned2,model_NN_tuned3,model_NN_tuned4,model_NN_tuned5,model_NN_tuned6]\nnames = [\"model 1\", \"model 2\", \"model 3\", \"model 4\", \"model 5\", \"model 6\"]\ntrain_test_models(models, names=names)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"d95f8e38-a5f2-4381-ae5b-8cbe732a719e","_uuid":"2f3552fca9dfa2ccf483bb73319b7fcec3976ef2"},"cell_type":"markdown","source":"**Observations:**\nModel 2 gives the best logloss on test data\n\nNow, let's try more"},{"metadata":{"_cell_guid":"821d74c3-043d-4772-b2d9-251d800aa0f4","_uuid":"996bb0a879f4a1299739490a0d10f5d0140d95c5","trusted":false,"collapsed":true},"cell_type":"code","source":"model_NN_tuned = MLPClassifier(learning_rate='adaptive', shuffle=True, epsilon=1e-8, activation='relu',\n                               hidden_layer_sizes=100, solver='adam', verbose=True)\n\nmodels = [model_NN_tuned]\ntrain_test_models(models)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"772da49e-9a12-47d2-b564-e20f59743176","_uuid":"6872202bc5b744b58b438e290ccb03373c30245a"},"cell_type":"markdown","source":"after trying the following parameters:\n* `identity` activation instead of `relu`\n* `epsilon` = 10e-4 instead of default value which = 1e-8\n* `stop_early` which make the training faster\n\nThe logloss and fscore became worse than the perviouse reached value which make `loss = 2.669` So we will use this last model with these parameters."},{"metadata":{"_cell_guid":"c198f10e-c7c4-41de-ab28-16ced277c4d3","_uuid":"a8fcb0318e1a872c553d21f3ddfba832ed7f655a"},"cell_type":"markdown","source":"## 3. Predict final results on testing data given\n\n### 3.1 Testing data preprocessing "},{"metadata":{"_cell_guid":"59afdd0e-2325-4b88-9687-37aa2caaca61","collapsed":true,"_uuid":"e47f918cf0ff74ce3a788474eb7d8ec00d84f3ba","trusted":false},"cell_type":"code","source":"test_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"f9ce7bb5-243d-49ed-99fa-15e7b9a0bb71","collapsed":true,"_uuid":"608b235477f3bf6885fdfa6daa839e587bf1172c","trusted":false},"cell_type":"code","source":"# encode Dates and Address\ntest_df['Dates'] = trimMinAndSecFromDates(test_df['Dates'])\ntest_df['Dates'] = test_df['Dates'].astype('category')\ntest_df['Dates_int'] = test_df['Dates'].cat.codes\ntest_df['Address'] = test_df['Address'].astype('category')\ntest_df['Address_int'] = test_df['Address'].cat.codes\ntest_df.drop(columns=['Dates', 'Address'], inplace=True)\n\n# encode DayOfWeek and PdDistrict\nDayOfWeek_dummies = pd.get_dummies(test_df['DayOfWeek'])\nPdDistrict_dummies = pd.get_dummies(test_df['PdDistrict'])\ntest_df = test_df.join(DayOfWeek_dummies)\ntest_df = test_df.join(PdDistrict_dummies)\ntest_df.drop(columns=['DayOfWeek', 'PdDistrict'], inplace=True)\n\n# drop id column\ntest_df.drop(columns=['Id'], inplace=True)\n\ntest_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b9e393d2-4c1d-4c85-be27-467ecec21e35","_uuid":"8b976b47dc646b4953c57602a039b2a2992b220c"},"cell_type":"markdown","source":"### 3.2 Predict results"},{"metadata":{"_cell_guid":"a9ffb649-0d8d-4f81-bab8-f2dcf1a2a074","collapsed":true,"_uuid":"0c2293e7304b4e359a04e05f0fdb776a19c16e46","trusted":false},"cell_type":"code","source":"results = model_NN_tuned.predict_proba(test_df)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"62441b19-01a1-4b22-92fd-f191ddea33b9","_uuid":"f17b6fa86d9ba7db721f57998ceb010a7f230fdf","trusted":false,"collapsed":true},"cell_type":"code","source":"results.shape","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"6dd1ae91-6fed-43c0-b28e-1c3f24ded879","_uuid":"f182b1d81988305fc8606f38cee5970a007e13b7"},"cell_type":"markdown","source":"### 3.3 Put results into pandas dataframe"},{"metadata":{"_cell_guid":"895f5e08-3b50-46e6-acfb-013ebecc4f82","collapsed":true,"_uuid":"7a6b3dc3ff11c09215a4de0a40bd448c5545e206","trusted":false},"cell_type":"code","source":"output_df = pd.DataFrame(data=results, columns=model_NN_tuned.classes_)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"04f366fa-02b7-4be5-991f-6460465e1d28","_uuid":"b01f57a4e7ee06205c30e7473637602c5f74a503","trusted":false,"collapsed":true},"cell_type":"code","source":"output_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"3c67e3fb-af30-4116-824d-bbb4744629f1","_uuid":"6208c684dc920da08e77ab08573e78682bad85ae"},"cell_type":"markdown","source":"**Produce the results into csv file: **"},{"metadata":{"_cell_guid":"ef35a179-fd6f-42cc-9d4f-d9fb958675cd","collapsed":true,"_uuid":"7a03bb106a7a6a5586bafa68eb725ce21d1bd411","trusted":false},"cell_type":"code","source":"output_df.index.name = 'Id'\noutput_df.to_csv(\"output.csv\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}