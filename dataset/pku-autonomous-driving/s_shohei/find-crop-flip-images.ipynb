{"cells":[{"metadata":{},"cell_type":"markdown","source":"This kernel shows:\n* How to check image flip/crop with simple way\n* Flip/crop images are not inclueded leaderboard score calculation"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport matplotlib.pylab as plt\nimport pandas as pd\n\n#from PIL import ImageDraw, Image\nimport cv2\n\nfrom tqdm import tqdm","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"### average train images"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv(\"../input/pku-autonomous-driving/train.csv\")\nprint(train_df.shape)\n\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df = pd.read_csv(\"../input/pku-autonomous-driving/sample_submission.csv\")\nprint(test_df.shape)\n\ntest_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_average_img = np.zeros([2710, 3384, 3])\ntrain_dir = \"../input/pku-autonomous-driving/train_images/\"\n\nfor imgid in tqdm(train_df[\"ImageId\"]):\n    color_img = cv2.imread(train_dir + imgid + \".jpg\")\n    train_average_img += color_img","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_average_img_ = (train_average_img / train_df.shape[0]).astype(np.uint8)\ntrain_average_img_ = cv2.cvtColor(train_average_img_, cv2.COLOR_BGR2RGB)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(13, 13))\nplt.imshow(train_average_img_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.imsave(\"train_all_average.png\",train_average_img_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### template matching\nEV bonnet mark as a template"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_template = train_average_img_[2600:,2200:2800,:]\nplt.imshow(train_template)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"imgid = train_df.iloc[0][\"ImageId\"]\nimgid","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tmp_img = cv2.imread(\"../input/pku-autonomous-driving/train_images/\" + imgid + \".jpg\")\ntmp_img = cv2.cvtColor(tmp_img, cv2.COLOR_BGR2RGB)\n\n# for \ntmp_img = tmp_img[1500:,:,:]\n\nplt.figure(figsize=(13, 13))\nplt.imshow(tmp_img)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"_, w,h = train_template.shape[::-1]\n\nresult = cv2.matchTemplate(tmp_img, train_template, cv2.TM_CCOEFF_NORMED)\nmin_val, max_val, min_loc, max_loc = cv2.minMaxLoc(result)\nprint(f\"max value: {max_val}, position: {max_loc}\")\n\ntop_left = max_loc\nbottom_right = (top_left[0] + w, top_left[1] + h)\n\ntmp_img_rect = cv2.rectangle(tmp_img,top_left, bottom_right, [255,0,0], 3)\n\nplt.figure(figsize=(13, 13))\nplt.imshow(tmp_img_rect)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"If a image is not cropped and/or flipped, this \"EV bonnet mark\" should be found here."},{"metadata":{"trusted":true},"cell_type":"code","source":"imgid_list = []\nx_list = []\ny_list = []\n\ntest_dir = \"../input/pku-autonomous-driving/test_images/\"\n\nfor imgid in tqdm(test_df[\"ImageId\"]):\n    filename = test_dir+imgid+\".jpg\"\n    \n    color_img = cv2.imread(filename)\n    \n    color_img = color_img[2000:,1500:,:] # for matching speedup\n    \n    color_img = cv2.cvtColor(color_img, cv2.COLOR_BGR2RGB)\n    color_img = cv2.copyMakeBorder(color_img,0,55,0,0,cv2.BORDER_REPLICATE) # for robust matching\n    color_img = color_img.astype(np.uint8)\n    \n    result = cv2.matchTemplate(color_img, train_template, cv2.TM_CCOEFF_NORMED)\n    min_val, max_val, min_loc, max_loc = cv2.minMaxLoc(result)\n\n    imgid_list.append(imgid)\n    x_list.append(max_loc[0])\n    y_list.append(max_loc[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"matched_df = pd.DataFrame({\n    \"ImageId\":imgid_list,\n    \"x\":x_list,\n    \"y\":y_list,\n})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(matched_df.shape)\nmatched_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(matched_df[\"x\"],matched_df[\"y\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(matched_df[\"x\"],matched_df[\"y\"])\nplt.xlim(645,760)\nplt.ylim(580,620)\nplt.grid()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"matched_df[\"no_crop_no_flip\"] = (\n    (matched_df[\"x\"] >= 680) &\n    (matched_df[\"x\"] <= 720) & \n    (matched_df[\"y\"] >= 592) & \n    (matched_df[\"y\"] <= 608)).astype(\"int\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"matched_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"flip and do same template matching"},{"metadata":{"trusted":true},"cell_type":"code","source":"x_list = []\ny_list = []\n\ntest_dir = \"../input/pku-autonomous-driving/test_images/\"\n\nfor imgid in tqdm(test_df[\"ImageId\"]):\n    \n    if matched_df[matched_df[\"ImageId\"] == imgid].iloc[0][\"no_crop_no_flip\"] ==1: # no need to match again\n        x_list.append(-1)\n        y_list.append(-1)\n        continue\n    \n    filename = test_dir+imgid+\".jpg\"\n    \n    color_img = cv2.imread(filename)\n    color_img = np.fliplr(color_img).copy()\n    \n    color_img = color_img[2000:,1500:,:] # for matching speedup\n    \n    color_img = cv2.cvtColor(color_img, cv2.COLOR_BGR2RGB)\n    color_img = cv2.copyMakeBorder(color_img,0,55,0,0,cv2.BORDER_REPLICATE) # for robust matching\n    color_img = color_img.astype(np.uint8)\n    \n    result = cv2.matchTemplate(color_img, train_template, cv2.TM_CCOEFF_NORMED)\n    min_val, max_val, min_loc, max_loc = cv2.minMaxLoc(result)\n\n    x_list.append(max_loc[0])\n    y_list.append(max_loc[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"matched_df[\"flip_x\"] = x_list\nmatched_df[\"flip_y\"] = y_list","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(matched_df[\"flip_x\"],matched_df[\"flip_y\"])\nplt.xlim(645,760)\nplt.ylim(580,620)\nplt.grid()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"matched_df[\"no_crop_flip\"] = (\n    (matched_df[\"flip_x\"] >= 680) &\n    (matched_df[\"flip_x\"] <= 720) & \n    (matched_df[\"flip_y\"] >= 592) & \n    (matched_df[\"flip_y\"] <= 608)).astype(\"int\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"matched_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print((matched_df[\"no_crop_no_flip\"] == 1).sum())\nprint((matched_df[\"no_crop_flip\"] == 1).sum())\nprint(((matched_df[\"no_crop_no_flip\"] != 1) & (matched_df[\"no_crop_flip\"] != 1)).sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In 2021 test images, there are 1230 no-crop and no-flip images, and 246 no-crop and h-flip images.  \nRest(545) images are cropped."},{"metadata":{},"cell_type":"markdown","source":"### visualize"},{"metadata":{},"cell_type":"markdown","source":"No crop, no flip images"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(17, 17))\nnum=1\n\nfor idx,row in matched_df[matched_df[\"no_crop_no_flip\"] == 1].sample(16).iterrows():\n    filename = test_dir+row[\"ImageId\"] +\".jpg\"\n    img = cv2.imread(filename)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    ax = fig.add_subplot(4, 4, num)\n    ax.set_aspect('equal')\n    \n    ax.imshow(img)\n    num+=1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"no crop, but flipped images"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(17, 17))\nnum=1\n\nfor idx,row in matched_df[matched_df[\"no_crop_flip\"] == 1].sample(16).iterrows():\n    filename = test_dir+row[\"ImageId\"] +\".jpg\"\n    img = cv2.imread(filename)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    ax = fig.add_subplot(4, 4, num)\n    ax.set_aspect('equal')\n    \n    ax.imshow(img)\n    num+=1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"cropped images"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(17, 17))\nnum=1\n\nfor idx,row in matched_df[(matched_df[\"no_crop_no_flip\"] != 1) & (matched_df[\"no_crop_flip\"] != 1)].sample(16).iterrows():\n    filename = test_dir+row[\"ImageId\"] +\".jpg\"\n    img = cv2.imread(filename)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    ax = fig.add_subplot(4, 4, num)\n    ax.set_aspect('equal')\n    \n    ax.imshow(img)\n    num+=1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"matched_df.to_csv(\"matched_df.csv\",index=None)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### check submission score\nhttps://www.kaggle.com/hocop1/centernet-baseline?scriptVersionId=23634825  \nOriginal Private Score:0.029  \nOriginal Public Score:0.027"},{"metadata":{"trusted":true},"cell_type":"code","source":"orig_sub_df = pd.read_csv(\"../input/sample-submission/predictions.csv\")\nprint(orig_sub_df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"orig_sub_df[\"no_crop_flip\"] = matched_df[\"no_crop_flip\"]\norig_sub_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"PredictionString_list=[]\nfor idx,row in orig_sub_df.iterrows():\n    if row[\"no_crop_flip\"] == 0:\n        PredictionString_list.append(row[\"PredictionString\"])\n    else:\n        PredictionString_list.append(\"\") # blank prediction","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df = pd.read_csv(\"../input/pku-autonomous-driving/sample_submission.csv\")\nprint(test_df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df[\"PredictionString\"] = PredictionString_list","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(test_df[\"PredictionString\"] == \"\").sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df[18:25]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"246/2021 images are blank predictions,  \nthen Public or Private score must be worse.\n\nBut Public and Private LB score remains same."},{"metadata":{},"cell_type":"markdown","source":"You can try with your submission file.  \nNote: This template matching method is not perfect, so you may encount small score change."},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}