{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"print('From LL5')\nprint('Bounding Box-RGB Image')\n\n\nimport os\nimport gc\nimport numpy as np\nimport pandas as pd\n\nimport json\nimport math\nimport sys\nimport time\nfrom datetime import datetime\nfrom typing import Tuple, List\n\nimport cv2\nimport matplotlib.pyplot as plt\nfrom collections import OrderedDict\nfrom shapely.geometry import MultiPoint, box\nfrom pyquaternion import Quaternion\nfrom tqdm import tqdm\n\n\nfrom lyft_dataset_sdk.utils.data_classes import Box, LidarPointCloud, RadarPointCloud  # NOQA\nfrom lyft_dataset_sdk.utils.map_mask import MapMask\nfrom lyft_dataset_sdk.lyftdataset import LyftDataset\nfrom lyft_dataset_sdk.utils.geometry_utils import view_points, box_in_image, BoxVisibility\nfrom lyft_dataset_sdk.utils.geometry_utils import transform_matrix\n\nfrom pathlib import Path\n\nimport struct\nfrom abc import ABC, abstractmethod\nfrom functools import reduce\nfrom typing import Tuple, List, Dict\nimport copy\nimport scipy.io as sio\n\n\n###############################################################################\n# box_vis_level: BoxVisibility = BoxVisibility.ANY  # Requires at least one corner visible in the image.\nbox_vis_level: BoxVisibility = BoxVisibility.ALL  # Requires all corners are inside the image.\n\ndef post_process_coords(corner_coords, w, h):\n    imsize=(w,h)\n    \"\"\"\n    Get the intersection of the convex hull of the reprojected bbox corners and the image canvas, return None if no\n    intersection.\n    :param corner_coords: Corner coordinates of reprojected bounding box.\n    :param imsize: Size of the image canvas.\n    :return: Intersection of the convex hull of the 2D box corners and the image canvas.\n    \"\"\"\n    polygon_from_2d_box = MultiPoint(corner_coords).convex_hull\n    img_canvas = box(0, 0, imsize[0], imsize[1])\n\n    if polygon_from_2d_box.intersects(img_canvas):\n        img_intersection = polygon_from_2d_box.intersection(img_canvas)\n        intersection_coords = np.array([coord for coord in img_intersection.exterior.coords])\n\n        min_x = min(intersection_coords[:, 0])\n        min_y = min(intersection_coords[:, 1])\n        max_x = max(intersection_coords[:, 0])\n        max_y = max(intersection_coords[:, 1])\n\n        return min_x, min_y, max_x, max_y\n    else:\n        return []\n\ndef LL5_render_annotation(my_annotation_token, w, h, view = np.eye(4),box_vis_level=box_vis_level):\n    ann_token=my_annotation_token\n    w=w\n    h=h\n    \"\"\"Render selected annotation.\n\n    Args:\n        ann_token: Sample_annotation token.\n        margin: How many meters in each direction to include in LIDAR view.\n        view: LIDAR view point.\n        box_vis_level: If sample_data is an image, this sets required visibility for boxes.\n        out_path: Optional path to save the rendered figure to disk.\n\n    \"\"\"\n\n    ann_record = lyftd.get(\"sample_annotation\", ann_token)\n    categoria=ann_record['category_name']\n    sample_record = lyftd.get(\"sample\", ann_record[\"sample_token\"])\n    lidar_top = [key for key in sample_record[\"data\"].keys() if \"LIDAR_TOP\" in key]\n    if lidar_top==['LIDAR_TOP']:\n        boxes_cam, cam, boxes_lidar = [], [], []\n        cams = [key for key in sample_record[\"data\"].keys() if \"CAM\" in key]\n        camera=[]\n        for cam in cams:\n            data_path_cam, boxes_cam, camera_intrinsic = lyftd.get_sample_data(\n                sample_record[\"data\"][cam], box_vis_level=box_vis_level, selected_anntokens=[ann_token])\n            if len(boxes_cam) > 0:\n                break  # We found an image that matches. Let's abort.\n        if len(boxes_cam)==1:\n            camera=cam\n            cam = sample_record[\"data\"][cam]\n                    # CAMERA view\n            path_cam, boxes_cam, camera_intrinsic = lyftd.get_sample_data(cam, selected_anntokens=[ann_token])\n            data_path_cam=path_cam.parts[5]\n            corners_camera3D=[]\n            corners_camera2D=[]\n            for f in range(len(boxes_cam)):\n                 corners_camera3D=boxes_cam[f].corners().T\n                 corners_3d = boxes_cam[f].corners()\n\n                # Project 3d box to 2d.\n                 corner_coords = view_points(corners_3d, camera_intrinsic, True).T[:, :2].tolist()\n    \n                # Keep only corners that fall within the image.\n                 final_coords = post_process_coords(corner_coords, w, h)\n                 #min_x, min_y, max_x, max_y = final_coords\n                 corners_camera2D=final_coords\n                 \n            # LIDAR view\n            lidar = sample_record[\"data\"][\"LIDAR_TOP\"]\n            path_lidar, boxes_lidar, lidar_intrinsic = lyftd.get_sample_data(lidar, selected_anntokens=[ann_token])\n            data_path_lidar=path_lidar.parts[5]\n            pc=LidarPointCloud.from_file(path_lidar)\n            pointclouds = pc.points.T\n            corners_lidar=[]\n            #for box in boxes_lidar:\n            for v in range(len(boxes_lidar)):\n                boxes3D=boxes_lidar[v].corners()\n                corners_lidar=boxes3D.T\n    \n            return data_path_cam, camera, corners_camera3D, corners_camera2D, data_path_lidar, corners_lidar, pointclouds, categoria\n        else:\n            b='boxes_cam vazio'\n            print(b)\n            data_path_cam=[]\n            camera=[] \n            corners_camera3D=[]\n            corners_camera2D=[]\n            data_path_lidar=[]\n            corners_lidar=[]\n            pointclouds=[]\n            categoria=[]\n            return data_path_cam, camera, corners_camera3D, corners_camera2D, data_path_lidar, corners_lidar, pointclouds, categoria\n    else:\n         a='lidar_top Ã© diferente de LIDAR_TOP'\n         print(a)\n\nDATA_PATH = 'D:/Dataset_LyftLevel5/Perception/train/'\n\nlyftd = LyftDataset(data_path=DATA_PATH, json_path=DATA_PATH+'train_data')\nlyftd.category\nlyftd.attribute\nattribute=[]\nattribute1=lyftd.attribute\nfor u in range(len(attribute1)):\n    attribute.append(attribute1[u]['token'])\n\ntotal_scene = lyftd.scene\n\ndata_path_cam_final=[]\ncamera_final=[]\nboxes_cam_final3D=[]\ncamera_intrinsic_final=[]\ndata_path_lidar_final=[]\nboxes_lidar_final=[]\ncategoria_final=[]\npointclouds_final=[]\nboxes_cam_final2D=[]\n\n\nj=1 # Odd Frames; j = 0 Even Frames \nfor jj in range(len(total_scene)):\n    print('Impar:', j)\n    my_scene = lyftd.scene[j]\n    my_sample_token_first = my_scene[\"first_sample_token\"]\n    my_sample_first = lyftd.get('sample', my_sample_token_first)\n    lidar_top_first = my_sample_first['data']['LIDAR_TOP']\n    first_token=my_sample_token_first\n    my_sample_token_last = my_scene[\"last_sample_token\"]\n    my_sample_last = lyftd.get('sample', my_sample_token_last)\n    lidar_top_last = my_sample_last['data']['LIDAR_TOP']\n    last_token=my_sample_token_last\n    next_token=my_sample_token_first\n    nexttoken=[]\n    while next_token!='':\n      nexttoken.append(next_token)\n      next_token=lyftd.get('sample', next_token)['next']\n    next_flip=nexttoken\n    first_next_last_token= nexttoken\n    for v in range(len(first_next_last_token)):\n        sensor_channel = 'CAM_FRONT'\n        my_sampleLL5 = lyftd.get('sample', first_next_last_token[v])\n        my_sampleLL5_data = lyftd.get('sample_data', my_sampleLL5['data'][sensor_channel])\n        w=my_sampleLL5_data['width']\n        h=my_sampleLL5_data['height']\n        for k in range(len(my_sampleLL5['anns'])):\n            print(str(j)+'_'+str(v)+'_'+str(k))\n            my_annotation_token = my_sampleLL5['anns'][k]\n            box_vis_level: BoxVisibility = BoxVisibility.ALL\n            data_path_cam, camera, corners_camera3D, corners_camera2D, data_path_lidar, corners_lidar, pointclouds, categoria = LL5_render_annotation(my_annotation_token, w, h, view = np.eye(4), box_vis_level=box_vis_level)\n            if len(data_path_cam)!=0 and len(camera)!=0 and len(corners_camera3D)!=0 and len(corners_camera2D)!=0 and len(data_path_lidar)!=0 and len(corners_lidar)!=0 and len(pointclouds)!=0 and len(categoria)!=0:\n                if w==1920 and h==1080:\n                    sio.savemat('D:/Dataset_LyftLevel5/1920_impar/data_path_cam/'+str(j)+'_'+str(v)+'_'+str(k)+'.mat',{'data_path_cam':data_path_cam})\n                    sio.savemat('D:/Dataset_LyftLevel5/1920_impar/camera/'+str(j)+'_'+str(v)+'_'+str(k)+'.mat',{'camera':camera})\n                    sio.savemat('D:/Dataset_LyftLevel5/1920_impar/corners_camera3D/'+str(j)+'_'+str(v)+'_'+str(k)+'.mat',{'corners_camera3D':corners_camera3D})\n                    sio.savemat('D:/Dataset_LyftLevel5/1920_impar/corners_camera2D/'+str(j)+'_'+str(v)+'_'+str(k)+'.mat',{'corners_camera2D':corners_camera2D})\n                    sio.savemat('D:/Dataset_LyftLevel5/1920_impar/data_path_lidar/'+str(j)+'_'+str(v)+'_'+str(k)+'.mat',{'data_path_lidar':data_path_lidar})\n                    sio.savemat('D:/Dataset_LyftLevel5/1920_impar/corners_lidar/'+str(j)+'_'+str(v)+'_'+str(k)+'.mat',{'corners_lidar':corners_lidar})\n                    sio.savemat('D:/Dataset_LyftLevel5/1920_impar/pointclouds/'+str(j)+'_'+str(v)+'_'+str(k)+'.mat',{'pointclouds':pointclouds})\n                    sio.savemat('D:/Dataset_LyftLevel5/1920_impar/categoria/'+str(j)+'_'+str(v)+'_'+str(k)+'.mat',{'categoria':categoria})\n                elif w==1224 and h==1024:\n                    sio.savemat('D:/Dataset_LyftLevel5/1224_impar/data_path_cam/'+str(j)+'_'+str(v)+'_'+str(k)+'.mat',{'data_path_cam':data_path_cam})\n                    sio.savemat('D:/Dataset_LyftLevel5/1224_impar/camera/'+str(j)+'_'+str(v)+'_'+str(k)+'.mat',{'camera':camera})\n                    sio.savemat('D:/Dataset_LyftLevel5/1224_impar/corners_camera3D/'+str(j)+'_'+str(v)+'_'+str(k)+'.mat',{'corners_camera3D':corners_camera3D})\n                    sio.savemat('D:/Dataset_LyftLevel5/1224_impar/corners_camera2D/'+str(j)+'_'+str(v)+'_'+str(k)+'.mat',{'corners_camera2D':corners_camera2D})\n                    sio.savemat('D:/Dataset_LyftLevel5/1224_impar/data_path_lidar/'+str(j)+'_'+str(v)+'_'+str(k)+'.mat',{'data_path_lidar':data_path_lidar})\n                    sio.savemat('D:/Dataset_LyftLevel5/1224_impar/corners_lidar/'+str(j)+'_'+str(v)+'_'+str(k)+'.mat',{'corners_lidar':corners_lidar})\n                    sio.savemat('D:/Dataset_LyftLevel5/1224_impar/pointclouds/'+str(j)+'_'+str(v)+'_'+str(k)+'.mat',{'pointclouds':pointclouds})\n                    sio.savemat('D:/Dataset_LyftLevel5/1224_impar/categoria/'+str(j)+'_'+str(v)+'_'+str(k)+'.mat',{'categoria':categoria})\n    j=j+2","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"code","source":"print(\"From LL5\")\nprint('Bounding Box-Point Cloud')\n\nimport json\nimport math\nimport os\nimport sys\nimport time\nfrom datetime import datetime\nfrom pathlib import Path\nfrom typing import List, Tuple\n\nimport cv2\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport sklearn.metrics\nfrom matplotlib.axes import Axes\nfrom PIL import Image\nfrom pyquaternion import Quaternion\nfrom tqdm import tqdm\n\nimport scipy.io as sio\n\nfrom lyft_dataset_sdk.lyftdataset import LyftDataset\nfrom lyft_dataset_sdk.utils.data_classes import Box, RadarPointCloud, LidarPointCloud  # NOQA\nfrom lyft_dataset_sdk.utils.geometry_utils import BoxVisibility, box_in_image, view_points  # NOQA\nfrom lyft_dataset_sdk.utils.map_mask import MapMask\n\n# Load the dataset\n# Adjust the dataroot parameter below to point to your local dataset path.\n# The correct dataset path contains at least the following four folders (or similar): images, lidar, maps, v1.0.1-train\nlevel5data = LyftDataset(data_path='D:/Dataset_LyftLevel5/Perception/train', json_path='D:/Dataset_LyftLevel5/Perception/train/train_data', verbose=True)\n\n\n#ALL = 0  # Requires all corners are inside the image.\n#ANY = 1  # Requires at least one corner visible in the image.\n#NONE = 2  # Requires no corners to be inside, i.e. box can be fully outside the image.\n\n#######################\nimport pandas as pd\ntrain = pd.read_csv('D:/Dataset_LyftLevel5/Perception/train/' + 'train.csv')\n# Taken from https://www.kaggle.com/gaborfodor/eda-3d-object-detection-challenge\n\nobject_columns = ['sample_id', 'object_id', 'center_x', 'center_y', 'center_z',\n                  'width', 'length', 'height', 'yaw', 'class_name']\nobjects = []\nfor sample_id, ps in tqdm(train.values[:]):\n    object_params = ps.split()\n    n_objects = len(object_params)\n    for i in range(n_objects // 8):\n        x, y, z, w, l, h, yaw, c = tuple(object_params[i * 8: (i + 1) * 8])\n        objects.append([sample_id, i, x, y, z, w, l, h, yaw, c])\ntrain_objects = pd.DataFrame(\n    objects,\n    columns = object_columns\n)\n\nnumerical_cols = ['object_id', 'center_x', 'center_y', 'center_z', 'width', 'length', 'height', 'yaw']\ntrain_objects[numerical_cols] = np.float32(train_objects[numerical_cols].values)\ntrain_objects.head()\n########################\n\n#class LidarPointCloud(LidarPointCloud):\n#    @staticmethod\n#    def nbr_dims() -> int:\n#        \"\"\"Returns the number of dimensions.\n#\n#        Returns: Number of dimensions.\n#\n#        \"\"\"\n#        return 4\n#\n#    @classmethod\n#    def from_file(cls, file_name: Path) -> \"LidarPointCloud\":\n#        \"\"\"Loads LIDAR data from binary numpy format. Data is stored as (x, y, z, intensity, ring index).\n#\n#        Args:\n#            file_name: Path of the pointcloud file on disk.\n#\n#        Returns: LidarPointCloud instance (x, y, z, intensity).\n#\n#        \"\"\"\n#\n#        assert file_name.suffix == \".bin\", \"Unsupported filetype {}\".format(file_name)\n#\n#        scan = np.fromfile(str(file_name), dtype=np.float32)\n#        if len(scan) % 5 == 0:\n#            points = scan.reshape((-1, 5))[:, : cls.nbr_dims()]\n#        else:\n#            scan0=[]\n#            scan1=[]\n#            scan2=[]\n#            scan3=[]\n#            scan4=[]\n#            scan_pontos = []\n#            scan_pontos = scan.reshape((-1, 1))\n#            dv = 0\n#            for divisao in range(math.floor(len(scan)/5)):\n#                if (dv+4)<=len(scan):\n#                    scan0.append(scan_pontos[dv])\n#                    scan1.append(scan_pontos[dv+1])\n#                    scan2.append(scan_pontos[dv+2])\n#                    scan3.append(scan_pontos[dv+3])\n#                    scan4.append(scan_pontos[dv+4])\n#                dv = dv+5\n#            points = np.concatenate((scan0,scan1,scan2,scan3),axis=1)\n#        return cls(points.T)\n\n\n#def view_points(points: np.ndarray, view: np.ndarray, normalize: bool) -> np.ndarray:\n#    \"\"\"This is a helper class that maps 3d points to a 2d plane. It can be used to implement both perspective and\n#    orthographic projections. It first applies the dot product between the points and the view. By convention,\n#    the view should be such that the data is projected onto the first 2 axis. It then optionally applies a\n#    normalization along the third dimension.\n#\n#    For a perspective projection the view should be a 3x3 camera matrix, and normalize=True\n#    For an orthographic projection with translation the view is a 3x4 matrix and normalize=False\n#    For an orthographic projection without translation the view is a 3x3 matrix (optionally 3x4 with last columns\n#     all zeros) and normalize=False\n#\n#    Args:\n#        points: <np.float32: 3, n> Matrix of points, where each point (x, y, z) is along each column.\n#        view: <np.float32: n, n>. Defines an arbitrary projection (n <= 4).\n#        The projection should be such that the corners are projected onto the first 2 axis.\n#        normalize: Whether to normalize the remaining coordinate (along the third axis).\n#\n#    Returns: <np.float32: 3, n>. Mapped point. If normalize=False, the third coordinate is the height.\n#\n#    \"\"\"\n#\n#    assert view.shape[0] <= 4\n#    assert view.shape[1] <= 4\n#    assert points.shape[0] == 3\n#\n#    viewpad = np.eye(4)\n#    viewpad[: view.shape[0], : view.shape[1]] = view\n#\n#    nbr_points = points.shape[1]\n#\n#    # Do operation in homogenous coordinates\n#    points = np.concatenate((points, np.ones((1, nbr_points))))\n#    points = np.dot(viewpad, points)\n#    points = points[:3, :]\n#\n#    if normalize:\n#        points = points / points[2:3, :].repeat(3, 0).reshape(3, nbr_points)\n#\n#    return points\n\ndef LL5_map_pointcloud_to_image(pointsensor_token, camera_token):\n    \"\"\"Given a point sensor (lidar/radar) token and camera sample_data token, load point-cloud and map it to\n    the image plane.\n\n    Args:\n        pointsensor_token: Lidar/radar sample_data token.\n        camera_token: Camera sample_data token.\n\n    Returns: (pointcloud <np.float: 2, n)>, coloring <np.float: n>, image <Image>).\n\n    \"\"\"\n\n    cam = level5data.get(\"sample_data\", camera_token)\n    nome_cam=Path(cam[\"filename\"]).parts\n    nome_cam_part=nome_cam[1]\n    nome_cam_size=len(nome_cam_part)\n    nome_cam_final=nome_cam_part[0:nome_cam_size-5]\n    pointsensor = level5data.get(\"sample_data\", pointsensor_token)\n    pcl_path = level5data.data_path / pointsensor[\"filename\"]\n\n    if pointsensor[\"sensor_modality\"] == \"lidar\":\n        pc = LidarPointCloud.from_file((pcl_path))\n    else:\n        pc = RadarPointCloud.from_file((pcl_path))\n    image = Image.open(str(level5data.data_path / cam[\"filename\"]))\n\n    # Points live in the point sensor frame. So they need to be transformed via global to the image plane.\n    # First step: transform the point-cloud to the ego vehicle frame for the timestamp of the sweep.\n    cs_record = level5data.get(\"calibrated_sensor\", pointsensor[\"calibrated_sensor_token\"])\n    pc.rotate(Quaternion(cs_record[\"rotation\"]).rotation_matrix)\n    pc.translate(np.array(cs_record[\"translation\"]))\n\n    # Second step: transform to the global frame.\n    poserecord = level5data.get(\"ego_pose\", pointsensor[\"ego_pose_token\"])\n    pc.rotate(Quaternion(poserecord[\"rotation\"]).rotation_matrix)\n    pc.translate(np.array(poserecord[\"translation\"]))\n\n    # Third step: transform into the ego vehicle frame for the timestamp of the image.\n    poserecord = level5data.get(\"ego_pose\", cam[\"ego_pose_token\"])\n    pc.translate(-np.array(poserecord[\"translation\"]))\n    pc.rotate(Quaternion(poserecord[\"rotation\"]).rotation_matrix.T)\n\n    # Fourth step: transform into the camera.\n    cs_record = level5data.get(\"calibrated_sensor\", cam[\"calibrated_sensor_token\"])\n    pc.translate(-np.array(cs_record[\"translation\"]))\n    pc.rotate(Quaternion(cs_record[\"rotation\"]).rotation_matrix.T)\n\n    # Fifth step: actually take a \"picture\" of the point cloud.\n    # Grab the depths (camera frame z axis points away from the camera).\n    depths = pc.points[2, :]\n\n    # Retrieve the color from the depth.\n    coloring = depths\n\n    # Take the actual picture (matrix multiplication with camera-matrix + renormalization).\n    points = view_points(pc.points[:3, :], np.array(cs_record[\"camera_intrinsic\"]), normalize=True)\n\n    # Remove points that are either outside or behind the camera. Leave a margin of 1 pixel for aesthetic reasons.\n    mask = np.ones(depths.shape[0], dtype=bool)\n    mask = np.logical_and(mask, depths > 0)\n    mask = np.logical_and(mask, points[0, :] > 1)\n    mask = np.logical_and(mask, points[0, :] < image.size[0] - 1)\n    mask = np.logical_and(mask, points[1, :] > 1)\n    mask = np.logical_and(mask, points[1, :] < image.size[1] - 1)    \n    points = points[:, mask]\n    coloring = coloring[mask]\n    \n    auxiliar = pc.points\n    auxiliar = auxiliar[:, mask]\n    auxiliar = np.delete(auxiliar, 0, 0)\n    auxiliar = np.delete(auxiliar, 0, 0)\n    points = np.delete(points, 2, 0)\n    points = np.concatenate((points,auxiliar),axis=0)\n    \n    return points, coloring, image, nome_cam_final\n\n#level5data.list_scenes()\n\nfor k in range(0,180):\n    my_scene = level5data.scene[k]\n    print(k)\n    name = my_scene[\"name\"]\n    nbr_samples = my_scene['nbr_samples']\n    for j in range(0,nbr_samples):\n        if j==0:\n            my_sample_token = my_scene[\"first_sample_token\"]\n            #level5data.render_sample(my_sample_token)\n            my_sample = level5data.get('sample', my_sample_token)\n            #my_sample\n            #level5data.list_sample(my_sample['token'])\n            codigo_token = my_sample['token']\n        else:\n            my_sample = level5data.get('sample', codigo_token)\n            codigo_token = my_sample[\"next\"]\n            \n        sample_token = codigo_token\n        sample_record = level5data.get(\"sample\", sample_token)\n        pointsensor_channel = \"LIDAR_TOP\"\n        #camera = [\"CAM_FRONT_ZOOMED\"]\n        camera = [\"CAM_FRONT\",\"CAM_FRONT_LEFT\",\"CAM_FRONT_RIGHT\",\"CAM_FRONT_ZOOMED\",\"CAM_BACK\",\"CAM_BACK_LEFT\",\"CAM_BACK_RIGHT\"]\n        # Here we just grab the front camera and the point sensor.\n        for cam in range(0,len(camera)):\n            camera_channel = camera[cam]\n            pointsensor_token = sample_record[\"data\"][pointsensor_channel]\n            camera_token = sample_record[\"data\"][camera_channel]\n            points, coloring, image, nome_cam_final = LL5_map_pointcloud_to_image(pointsensor_token, camera_token)\n            # Here we just grab the front camera and the point sensor.\n    #        plt.figure(figsize=(9, 16))\n    #        plt.imshow(im)\n    #        plt.scatter(points[0, :], points[1, :], c=coloring, s=1)\n    #        plt.axis(\"off\")\n            im = np.array(image)\n#            if (len(im[:,0])==1080 and len(im[0,:])==1920):\n#                name_final = str(k)+'_'+str(j)+'_'+name\n#                sio.savemat('E:/LyfLevel5/DataSet/PointCloud_Image/'+name_final+'.mat',{'im':im})\n#                sio.savemat('E:/LyfLevel5/DataSet/PointCloud_Projected/'+name_final+'.mat',{'points':points})\n#                sio.savemat('E:/LyfLevel5/DataSet/PointCloud_Coloring/'+name_final+'.mat',{'coloring':coloring})\n\n            #name_final = str(k)+'_'+str(j)+'_'+camera_channel\n            sio.savemat('D:/Dataset_LyftLevel5/RangeView/PointCloud_Image/'+nome_cam_final+'.mat',{'im':im})\n            sio.savemat('D:/Dataset_LyftLevel5/RangeView/PointCloud_Projected/'+nome_cam_final+'.mat',{'points':points})\n            sio.savemat('D:/Dataset_LyftLevel5/RangeView/PointCloud_Coloring/'+nome_cam_final+'.mat',{'coloring':coloring})","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}