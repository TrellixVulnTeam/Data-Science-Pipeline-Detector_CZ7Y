{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## In this kernel we convert LEVEL5 Lyft data (NuScenes format) to KITTI format, which is usually used in public repositories. After this you can search for repos, that solve KITTI 3d-detection task.\n\n## Thanks to https://www.kaggle.com/stalkermustang/converting-lyft-dataset-to-kitty-format\n### This above version by user - stalkermustang was not working for me so made little changes below","metadata":{}},{"cell_type":"code","source":"!pip install nuscenes-devkit","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pathlib import Path\nfrom PIL import Image","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# dir with all input data from Kaggle\nINP_DIR = Path('/kaggle/input/3d-object-detection-for-autonomous-vehicles/')\nprint(INP_DIR)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# dir with index json tables (scenes, categories, logs, etc...)\nTABLES_DIR = INP_DIR.joinpath('train_data')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Adjust the dataroot parameter below to point to your local dataset path.\n# The correct dataset path contains at least the following four folders (or similar): images, lidar, maps\n!ln -s {INP_DIR}/train_images images\n!ln -s {INP_DIR}/train_maps maps\n!ln -s {INP_DIR}/train_lidar lidar","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DATA_DIR = Path().absolute() \n# Empty init equals '.'.\n# We use this because we link train dirs to current dir (cell above)","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# dir to write KITTY-style dataset\nSTORE_DIR = DATA_DIR.joinpath('kitti_format')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install -U git+https://github.com/lyft/nuscenes-devkit","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import lyft_dataset_sdk\nimport lyft_dataset_sdk.utils.export_kitti","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python -m lyft_dataset_sdk.utils.export_kitti nuscenes_gt_to_kitti -h","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# convertation to KITTY-format\n!python -m lyft_dataset_sdk.utils.export_kitti nuscenes_gt_to_kitti \\\n        --lyft_dataroot {DATA_DIR} \\\n        --table_folder {TABLES_DIR} \\\n        --samples_count 20 \\\n        --parallel_n_jobs 2 \\\n        --get_all_detections True \\\n        --store_dir {STORE_DIR}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check created (converted) files. velodyne = LiDAR poinclouds data (in binary)\n!ls {STORE_DIR}/velodyne | head -2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# render converted data for check. Currently don't support multithreading :(\n!python -m lyft_dataset_sdk.utils.export_kitti render_kitti \\\n        --store_dir {STORE_DIR}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Script above write images to 'render' folder\n# in store_dir (where we have converted dataset)\nRENDER_DIR = STORE_DIR.joinpath('render')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get all rendered files\nall_renders = list(RENDER_DIR.glob('*'))\nall_renders.sort()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# render radar data (bird view) and camera data with bboxes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Image.open(all_renders[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Image.open(all_renders[1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls ./kitti_format","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!cat ./kitti_format/label_2/095d5bb88eb9cdd223b90d2a1475c0cf2f4b4c2a8aca82ba0ae51f6fba540440.txt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls kitti_format/image_2/","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path = 'kitti_format/image_2/095d5bb88eb9cdd223b90d2a1475c0cf2f4b4c2a8aca82ba0ae51f6fba540440.png'\nfrom IPython.display import Image\nImage(path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls ./kitti_format/label_2/ | wc -l","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls ./kitti_format/image_2/ | wc -l","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls ./kitti_format/render | wc -l","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 40 denotes, 20 samples were taken - which was then splitted into 20 lidar + 20 camera images","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}