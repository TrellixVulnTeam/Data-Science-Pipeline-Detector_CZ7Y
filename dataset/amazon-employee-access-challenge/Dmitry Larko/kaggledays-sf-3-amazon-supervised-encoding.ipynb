{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Supervised categorical encodings\n\nBy “supervised” here I mean we are going to use the information about target we are trying to predict in order to build our categorical embeddings.\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# Loading data directly from CatBoost\nfrom catboost.datasets import amazon\ntrain, test = amazon()\ntarget = \"ACTION\"\ncol4train = [x for x in train.columns if x not in [target, \"ROLE_TITLE\"]]\ny = train[target].values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Helper functions"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.ensemble import ExtraTreesClassifier\n\n#our small helper function, returns ExtraTrees instance\ndef get_model():\n    params = {\n        \"n_estimators\":300, \n        \"n_jobs\": 3,\n        \"random_state\":5436,\n    }\n    return ExtraTreesClassifier(**params)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Simple Target Encoding\n\nThe simplest way is just encode each unique value by target mean. For unseen values we going to use dataset target mean.\n\n**Advantages**\n* Straight-forward, easy to implement\n* Easy to understand\n* Powerful, task-specific encoding\n\n**Disadvantages**\n* Introduces leakage (too much info about target is in data now, so no generalization)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.base import BaseEstimator, TransformerMixin\nclass TargetEncoding(BaseEstimator, TransformerMixin):\n    def __init__(self, columns_names ):\n        self.columns_names = columns_names\n        self.learned_values = {}\n        self.dataset_mean = np.nan\n    \n    def fit(self, X, y, **fit_params):\n        X_ = X.copy()\n        self.learned_values = {}\n        X_[\"__target__\"] = y\n        for c in [x for x in X_.columns if x in self.columns_names]:\n            self.learned_values[c] = (X_[[c,\"__target__\"]]\n                                      .groupby(c)[\"__target__\"].mean()\n                                      .reset_index())\n        self.dataset_mean = np.mean(y)\n        return self\n    \n    def transform(self, X, **fit_params):\n        transformed_X = X[self.columns_names].copy()\n        for c in transformed_X.columns:\n            transformed_X[c] = (transformed_X[[c]]\n                                .merge(self.learned_values[c], on = c, how = 'left')\n                               )[\"__target__\"]\n        transformed_X = transformed_X.fillna(self.dataset_mean)\n        return transformed_X\n    \n    def fit_transform(self, X, y, **fit_params):\n        self.fit(X,y)\n        return self.transform(X)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's transform our data and run CV to get AUC score."},{"metadata":{"trusted":true},"cell_type":"code","source":"skf = StratifiedKFold(n_splits=5, random_state = 5451, shuffle = True)\nte = TargetEncoding(columns_names=col4train)\nX_tr = te.fit_transform(train, y).values\n\nscores = []\ntr_scores = []\nfor train_index, test_index in skf.split(train, y):\n    train_df, valid_df = X_tr[train_index], X_tr[test_index]\n    train_y, valid_y = y[train_index], y[test_index]\n\n    model = get_model()\n    model.fit(train_df,train_y)\n\n    predictions = model.predict_proba(valid_df)[:,1]\n    scores.append(roc_auc_score(valid_y, predictions))\n\n    train_preds = model.predict_proba(train_df)[:,1]\n    tr_scores.append(roc_auc_score(train_y, train_preds))\n\nprint(\"Train AUC score: {:.4f} Valid AUC score: {:.4f}, STD: {:.4f}\".format(\n    np.mean(tr_scores), np.mean(scores), np.std(scores)\n))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Wow, AUC of 0.97?! If you think this is too good to be true - you're right. That is an example of target leakage, because of our transformation too much information about target leaked into data.\n\nSo rule number 1. If you building features using target - always do that *inside* CV loop.\n\nThat's the proper way of doing that."},{"metadata":{"trusted":true},"cell_type":"code","source":"scores = []\ntr_scores = []\nfor train_index, test_index in skf.split(train, y):\n    train_df = train.loc[train_index,col4train].reset_index(drop = True)\n    valid_df = train.loc[test_index,col4train].reset_index(drop = True)\n    train_y, valid_y = y[train_index], y[test_index]\n    \n    te = TargetEncoding(columns_names=col4train)\n    X_tr = te.fit_transform(train_df, train_y).values\n    X_val = te.transform(valid_df).values\n\n    model = get_model()\n    model.fit(X_tr,train_y)\n\n    predictions = model.predict_proba(X_val)[:,1]\n    scores.append(roc_auc_score(valid_y, predictions))\n\n    train_preds = model.predict_proba(X_tr)[:,1]\n    tr_scores.append(roc_auc_score(train_y, train_preds))\n\nprint(\"Train AUC score: {:.4f} Valid AUC score: {:.4f}, STD: {:.4f}\".format(\n    np.mean(tr_scores), np.mean(scores), np.std(scores)\n))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And you can see that AUC score is quite bad. How to improve that?"},{"metadata":{},"cell_type":"markdown","source":"## Target Encoding Smoothing\nWe could try to make target encoding more robust to leakage by addressing main problem - low-frequency values. If in your feature there are unique values which occurs just couple of times - they are one of the main source of leak.\n\nWhat if instead of encoding by mean we will take weighted sum of 2 means: **dataset mean** and **level mean**, where level mean is the mean of particular unique value in your feature. \n\nWeighted sum:\n\n\\\\(f(n)*mean(level) + (1-f(n))*mean(dataset)\\\\)\n\nWeighting function:\n\n\\\\(f(x) = \\frac{1}{1+exp(\\frac{-(x-k)}{f})}\\\\)\n\nwhere,\n\n\\\\(k\\\\) - inflection point, that's the point where \\\\(f(x)\\\\) is equal 0.5 \n\n\\\\(f\\\\) - steepness, a value which controls how step is our function.\n\nIn that case with carefully tuned \\\\(k\\\\) and \\\\(f\\\\) we could force all encodings of unfrequent values to be very close to dataset mean, while mean of frequent values will be closer to their actual value.\n\n**Advantages**\n* Fairly easy to implement\n* Easy to understand\n* Powerful, task-specific encoding\n\n**Disadvantages**\n* Introduces 2 additional parameters PER feature, which is could be hard to tune.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"class TargetEncodingSmoothing(BaseEstimator, TransformerMixin):\n    def __init__(self, columns_names,k, f ):\n        self.columns_names = columns_names\n        self.learned_values = {}\n        self.dataset_mean = np.nan\n        self.k = k #\n        self.f = f #\n    def smoothing_func(self, N): #\n        return 1 / (1 + np.exp(-(N-self.k)/self.f))\n    def fit(self, X, y, **fit_params):\n        X_ = X.copy()\n        self.learned_values = {}\n        self.dataset_mean = np.mean(y)\n        X_[\"__target__\"] = y\n        for c in [x for x in X_.columns if x in self.columns_names]:\n            stats = (X_[[c,\"__target__\"]]\n                     .groupby(c)[\"__target__\"].\n                     agg(['mean', 'size'])) \n            stats[\"alpha\"] = self.smoothing_func(stats[\"size\"])\n            stats[\"__target__\"] = (stats[\"alpha\"]*stats[\"mean\"] \n                                   + (1-stats[\"alpha\"])*self.dataset_mean)\n            stats = (stats\n                     .drop([x for x in stats.columns if x not in [\"__target__\",c]], axis = 1)\n                     .reset_index())\n            self.learned_values[c] = stats\n        self.dataset_mean = np.mean(y)\n        return self\n    def transform(self, X, **fit_params):\n        transformed_X = X[self.columns_names].copy()\n        for c in transformed_X.columns:\n            transformed_X[c] = (transformed_X[[c]]\n                                .merge(self.learned_values[c], on = c, how = 'left')\n                               )[\"__target__\"]\n        transformed_X = transformed_X.fillna(self.dataset_mean)\n        return transformed_X\n    def fit_transform(self, X, y, **fit_params):\n        self.fit(X,y)\n        return self.transform(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib inline\nx = np.linspace(0,100,100)\nplot = pd.DataFrame()\nte = TargetEncodingSmoothing([], 1,1)\nplot[\"k=1|f=1\"] = te.smoothing_func(x)\nte = TargetEncodingSmoothing([], 33,5)\nplot[\"k=33|f=5\"] = te.smoothing_func(x)\nte = TargetEncodingSmoothing([], 66,15)\nplot[\"k=66|f=15\"] = te.smoothing_func(x)\nplot.plot(figsize = (15,8))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scores = []\ntr_scores = []\nfor train_index, test_index in skf.split(train, y):\n    train_df = train.loc[train_index,col4train].reset_index(drop = True)\n    valid_df = train.loc[test_index,col4train].reset_index(drop = True)\n    train_y, valid_y = y[train_index], y[test_index]\n    te = TargetEncodingSmoothing(\n        columns_names= col4train,\n        k = 3, f = 1.5\n    )\n    X_tr = te.fit_transform(train_df, train_y).values\n    X_val = te.transform(valid_df).values\n\n    model = get_model()\n    model.fit(X_tr,train_y)\n\n    predictions = model.predict_proba(X_val)[:,1]\n    scores.append(roc_auc_score(valid_y, predictions))\n\n    train_preds = model.predict_proba(X_tr)[:,1]\n    tr_scores.append(roc_auc_score(train_y, train_preds))\n\nprint(\"Train AUC score: {:.4f} Valid AUC score: {:.4f}, STD: {:.4f}\".format(\n    np.mean(tr_scores), np.mean(scores), np.std(scores)\n))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Results are getting better, but not enough. Smoothing is a very helpful technique for medium/big data. Here we have small one. And we need to add something else.\n\n## Adding noise. CV inside CV.\n\nI call this adding noise because we try to make our embedding noisy, so powerful model like LightGBM won't memorize it instead of generalization.\n\nOne of the way to add noise is this. Let's think of our target encoding as a \"0-level model\", which predicts target. In that case we would like to have a \"predictions\" on unseen data, right? And how to do that? By cross-validation of course.\n\nSo we split our train dataset into n folds, and we use n-1 folds to create target mean embedding and use it for the last n-th fold.\n\n\nHere is the function which does that:"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_CV_target_encoding(data, y, encoder, cv = 5):\n    skfTE = StratifiedKFold(n_splits=cv, random_state = 545167, shuffle = True)\n    result = []\n    for train_indexTE, test_indexTE in skfTE.split(data, y):\n        encoder.fit(data.iloc[train_indexTE,:].reset_index(drop = True), y[train_indexTE])\n        tmp =  encoder.transform(data.iloc[test_indexTE,:].reset_index(drop = True))\n        tmp[\"index\"] = test_indexTE\n        result.append(tmp)\n    result = pd.concat(result, ignore_index = True)\n    result = result.sort_values('index').reset_index(drop = True).drop('index', axis = 1)\n    return result","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's try it in action on our `TargetEncodingSmoothing`."},{"metadata":{"trusted":true},"cell_type":"code","source":"scores = []\ntr_scores = []\nfor train_index, test_index in skf.split(train, y):\n    train_df = train.loc[train_index,col4train].reset_index(drop = True)\n    valid_df = train.loc[test_index,col4train].reset_index(drop = True)\n    train_y, valid_y = y[train_index], y[test_index]\n    te = TargetEncodingSmoothing(\n        columns_names= col4train,\n        k = 3, f = 1.5\n    )\n    \n    X_tr = get_CV_target_encoding(train_df, train_y, te, cv = 5)\n\n    te.fit(train_df, train_y)\n    X_val = te.transform(valid_df).values\n\n    model = get_model()\n    model.fit(X_tr,train_y)\n\n    predictions = model.predict_proba(X_val)[:,1]\n    scores.append(roc_auc_score(valid_y, predictions))\n\n    train_preds = model.predict_proba(X_tr)[:,1]\n    tr_scores.append(roc_auc_score(train_y, train_preds))\n\nprint(\"Train AUC score: {:.4f} Valid AUC score: {:.4f}, STD: {:.4f}\".format(\n    np.mean(tr_scores), np.mean(scores), np.std(scores)\n))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From .78 to .85. It really works :)"},{"metadata":{},"cell_type":"markdown","source":"## Adding noise. Expanding mean.\n\nNext idea how to add noise is called expanding mean and you will now understand why.\n\nImagine algorithm rolling trough data and for each new row it uses all previously seen rows to calculate this new row mean. For the very first row there is no previously seen rows available so it's mean will be dataset mean. For the second row you can use first (and only first) row, because you already saw it.\n\nThis approach especially suited for streaming (that is if you have infinite stream of data coming to you).\n\n**Advantages**\n* Powerful, task-specific encoding\n\n**Disadvantages**\n* Can introduce too much noise :)\n\nHere is the class which implements it:"},{"metadata":{"trusted":true},"cell_type":"code","source":"class TargetEncodingExpandingMean(BaseEstimator, TransformerMixin):\n    def __init__(self, columns_names):\n        self.columns_names = columns_names\n        self.learned_values = {}\n        self.dataset_mean = np.nan\n    def fit(self, X, y, **fit_params):\n        X_ = X.copy()\n        self.learned_values = {}\n        self.dataset_mean = np.mean(y)\n        X_[\"__target__\"] = y\n        for c in [x for x in X_.columns if x in self.columns_names]:\n            stats = (X_[[c,\"__target__\"]]\n                     .groupby(c)[\"__target__\"]\n                     .agg(['mean', 'size'])) #\n            stats[\"__target__\"] = stats[\"mean\"]\n            stats = (stats\n                     .drop([x for x in stats.columns if x not in [\"__target__\",c]], axis = 1)\n                     .reset_index())\n            self.learned_values[c] = stats\n        return self\n    def transform(self, X, **fit_params):\n        transformed_X = X[self.columns_names].copy()\n        for c in transformed_X.columns:\n            transformed_X[c] = (transformed_X[[c]]\n                                .merge(self.learned_values[c], on = c, how = 'left')\n                               )[\"__target__\"]\n        transformed_X = transformed_X.fillna(self.dataset_mean)\n        return transformed_X\n    \n    def fit_transform(self, X, y, **fit_params):\n        self.fit(X,y)\n    \n        #Expanding mean transform\n        X_ = X[self.columns_names].copy().reset_index(drop = True)\n        X_[\"__target__\"] = y\n        X_[\"index\"] = X_.index\n        X_transformed = pd.DataFrame()\n        for c in self.columns_names:\n            X_shuffled = X_[[c,\"__target__\", \"index\"]].copy()\n            X_shuffled = X_shuffled.sample(n = len(X_shuffled),replace=False)\n            X_shuffled[\"cnt\"] = 1\n            X_shuffled[\"cumsum\"] = (X_shuffled\n                                    .groupby(c,sort=False)['__target__']\n                                    .apply(lambda x : x.shift().cumsum()))\n            X_shuffled[\"cumcnt\"] = (X_shuffled\n                                    .groupby(c,sort=False)['cnt']\n                                    .apply(lambda x : x.shift().cumsum()))\n            X_shuffled[\"encoded\"] = X_shuffled[\"cumsum\"] / X_shuffled[\"cumcnt\"]\n            X_shuffled[\"encoded\"] = X_shuffled[\"encoded\"].fillna(self.dataset_mean)\n            X_transformed[c] = X_shuffled.sort_values(\"index\")[\"encoded\"].values\n        return X_transformed","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scores = []\ntr_scores = []\nfor train_index, test_index in skf.split(train, y):\n    train_df = train.loc[train_index,col4train].reset_index(drop = True)\n    valid_df = train.loc[test_index,col4train].reset_index(drop = True)\n    train_y, valid_y = y[train_index], y[test_index]\n    te = TargetEncodingExpandingMean(columns_names=col4train)\n\n    X_tr = te.fit_transform(train_df, train_y)\n    X_val = te.transform(valid_df).values\n\n    model = get_model()\n    model.fit(X_tr,train_y)\n\n    predictions = model.predict_proba(X_val)[:,1]\n    scores.append(roc_auc_score(valid_y, predictions))\n\n    train_preds = model.predict_proba(X_tr)[:,1]\n    tr_scores.append(roc_auc_score(train_y, train_preds))\n\nprint(\"Train AUC score: {:.4f} Valid AUC score: {:.4f}, STD: {:.4f}\".format(\n    np.mean(tr_scores), np.mean(scores), np.std(scores)\n))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Good score, but still worse compare to unsupervised features."},{"metadata":{},"cell_type":"markdown","source":"But why don't we add some new features? How? Let's use feature pairs to create a new set of categorical features. Just take pair of existing features and concat them together:"},{"metadata":{"trusted":true},"cell_type":"code","source":"train[col4train] = train[col4train].values.astype(str)\ntest[col4train] = test[col4train].values.astype(str)\n\nfrom itertools import combinations\nnew_col4train = col4train\nfor c1,c2 in combinations(col4train, 2):\n    name = \"{}_{}\".format(c1,c2)\n    new_col4train.append(name)\n    train[name] = train[c1] + \"_\" + train[c2]\n    test[name] = test[c1] + \"_\" + test[c2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train[new_col4train].shape, test[new_col4train].shape)\ntrain[new_col4train].head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now instead of 8 features we have 36."},{"metadata":{"trusted":true},"cell_type":"code","source":"train[new_col4train].apply(lambda x: len(x.unique()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And a lot of them are high-cardinality categorical features. Luckily for us we now know how to handle them.\nLet's use both `TargetEncodingExpandingMean` and `TargetEncodingSmoothing` with CV to create embeddings."},{"metadata":{"trusted":true},"cell_type":"code","source":"scores = []\ntr_scores = []\nfor train_index, test_index in skf.split(train, y):\n    train_df = train.loc[train_index,new_col4train].reset_index(drop = True)\n    valid_df = train.loc[test_index,new_col4train].reset_index(drop = True)\n    train_y, valid_y = y[train_index], y[test_index]\n    te = TargetEncodingExpandingMean(columns_names=new_col4train)\n\n    X_tr = te.fit_transform(train_df, train_y)\n    X_val = te.transform(valid_df)\n    \n    te2 = TargetEncodingSmoothing(\n        columns_names= new_col4train,\n        k = 3, f = 1.5,\n    )\n    \n    X_tr2 = get_CV_target_encoding(train_df, train_y, te2, cv = 5)\n    te2.fit(train_df, train_y)\n    X_val2 = te2.transform(valid_df)\n    \n    X_tr = pd.concat([X_tr, X_tr2], axis = 1)\n    X_val = pd.concat([X_val, X_val2], axis = 1)\n\n    model = get_model()\n    model.fit(X_tr,train_y)\n\n    predictions = model.predict_proba(X_val)[:,1]\n    scores.append(roc_auc_score(valid_y, predictions))\n\n    train_preds = model.predict_proba(X_tr)[:,1]\n    tr_scores.append(roc_auc_score(train_y, train_preds))\n\nprint(\"Train AUC score: {:.4f} Valid AUC score: {:.4f}, STD: {:.4f}\".format(\n    np.mean(tr_scores), np.mean(scores), np.std(scores)\n))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"AUC score is 0.8795. Let's check it on leaderboard."},{"metadata":{"trusted":true},"cell_type":"code","source":"te = TargetEncodingExpandingMean(columns_names=new_col4train)\n\nX_tr = te.fit_transform(train[new_col4train], y)\nX_val = te.transform(test[new_col4train])\n\nte2 = TargetEncodingSmoothing(\n    columns_names= new_col4train,\n    k = 3, f = 1.5,\n)\n\nX_tr2 = get_CV_target_encoding(train[new_col4train], y, te2, cv = 5)\nte2.fit(train[new_col4train], y)\nX_val2 = te2.transform(test[new_col4train])\n\nX = pd.concat([X_tr, X_tr2], axis = 1)\nX_te = pd.concat([X_val, X_val2], axis = 1)\n\nmodel = get_model()\nmodel.fit(X,y)\npredictions = model.predict_proba(X_te)[:,1]\n\nsubmit = pd.DataFrame()\nsubmit[\"Id\"] = test[\"id\"]\nsubmit[\"ACTION\"] = predictions\n\nsubmit.to_csv(\"submission.csv\", index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}