{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport gc\nimport os\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport lightgbm as lgbm\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.model_selection import StratifiedKFold\nimport itertools\n\nfrom itertools import combinations\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_dataset():\n    sampleSubmission = pd.read_csv(\"../input/amazon-employee-access-challenge/sampleSubmission.csv\")\n    test = pd.read_csv(\"../input/amazon-employee-access-challenge/test.csv\")\n    train = pd.read_csv(\"../input/amazon-employee-access-challenge/train.csv\")\n    print(\"dataset loaded\")\n    return sampleSubmission,train,test\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class TargetEncoding():\n    def __init__(self, columns_names ):\n        self.columns_names = columns_names\n        self.learned_values = {}\n        self.dataset_mean = np.nan\n        \n    def fit(self, X, y, **fit_params):\n        X_ = X.copy()\n        self.learned_values = {}\n        X_[\"__target__\"] = y\n        for c in [x for x in X_.columns if x in self.columns_names]:\n            self.learned_values[c] = (X_[[c,\"__target__\"]]\n                                      .groupby(c)[\"__target__\"].mean()\n                                      .reset_index())\n\n        self.dataset_mean = np.mean(y)\n        return self\n    \n    def transform(self, X, **fit_params):\n        transformed_X = X[self.columns_names].copy()\n        for c in transformed_X.columns:\n            transformed_X[c] = (transformed_X[[c]]\n                                .merge(self.learned_values[c], on = c, how = 'left')\n                               )[\"__target__\"]\n        transformed_X = transformed_X.fillna(self.dataset_mean)\n        return transformed_X\n    \n    def fit_transform(self, X, y, **fit_params):\n        self.fit(X,y)\n        return self.transform(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MJTCP = 32292 #Michael Jordan total career points\n\ndef assign_rnd_integer(dataset, number_of_times = 5, seed = MJTCP):\n    new_dataset = pd.DataFrame()\n    np.random.seed(seed)\n    for c in dataset.columns:\n        for i in range(number_of_times):\n            col_name = c+\"_\"+str(i)\n            unique_vals = dataset[c].unique()\n            labels = np.array(list(range(len(unique_vals))))\n            np.random.shuffle(labels)\n            mapping = pd.DataFrame({c: unique_vals, col_name: labels})\n            new_dataset[col_name] = (dataset[[c]]\n                                     .merge(mapping, on = c, how = 'left')[col_name]\n                                    ).values\n    return new_dataset\n\ndef get_col_interactions_svd(dataset, tfidf = True):\n    new_dataset = pd.DataFrame()\n    for col1,col2 in itertools.permutations(dataset.columns, 2):\n        data = extract_col_interaction(dataset, col1,col2, tfidf)\n        col_name = [x for x in data.columns if \"svd\" in x][0]\n        new_dataset[col_name] = (dataset[[col1]]\n                                 .merge(data, on = col1, how = 'left')\n                                )[col_name]\n    return new_dataset\n\n\ndef get_freq_encoding(dataset):\n    new_dataset = pd.DataFrame()\n    for c in dataset.columns:\n        data = dataset.groupby([c]).size().reset_index()\n        new_dataset[c+\"_freq\"] = dataset[[c]].merge(data, on = c, how = \"left\")[0]\n    return new_dataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def extract_col_interaction(dataset, col1, col2, tfidf = True):\n    data = dataset.groupby([col1])[col2].agg(lambda x: \" \".join(list([str(y) for y in x])))\n    if tfidf:\n        vectorizer = TfidfVectorizer(tokenizer=lambda x: x.split(\" \"))\n    else:\n        vectorizer = CountVectorizer(tokenizer=lambda x: x.split(\" \"))\n    \n    data_X = vectorizer.fit_transform(data)\n    dim_red = TruncatedSVD(n_components=1, random_state = 5511)\n    data_X = dim_red.fit_transform(data_X)\n    \n    result = pd.DataFrame()\n    result[col1] = data.index.values\n    result[col1+\"_{}_svd\".format(col2)] = data_X.ravel()\n    return result","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def transform_dataset(train, test, func, func_params = {}):\n    dataset = pd.concat([train, test], ignore_index = True)\n    dataset = func(dataset, **func_params)\n    if isinstance(dataset, pd.DataFrame):\n        new_train = dataset.iloc[:train.shape[0],:].reset_index(drop = True)\n        new_test =  dataset.iloc[train.shape[0]:,:].reset_index(drop = True)\n    else:\n        new_train = dataset[:train.shape[0]]\n        new_test =  dataset[train.shape[0]:]\n    return new_train, new_test\n\n\ndef get_model(params):\n    return lgbm.LGBMClassifier(\n        n_estimators=250,\n        metric='auc',\n        objective='binary', \n        n_jobs=3,\n        random_state = 42,\n        **params\n    )\n\n    \ndef get_CV_target_encoding(data, y, encoder, cv = 5):\n    skfTE = StratifiedKFold(n_splits=cv, random_state = 545167, shuffle = True)\n    result = []\n    for train_indexTE, test_indexTE in skfTE.split(data, y):\n        encoder.fit(data.iloc[train_indexTE,:].reset_index(drop = True), y[train_indexTE])\n        tmp =  encoder.transform(data.iloc[test_indexTE,:].reset_index(drop = True))\n        tmp[\"index\"] = test_indexTE\n        result.append(tmp)\n    result = pd.concat(result, ignore_index = True)\n    result = result.sort_values('index').reset_index(drop = True).drop('index', axis = 1)\n    return result\n\n\ndef get_submission(predictions_1,predictions_2):\n    submission = pd.DataFrame()\n    submission[\"Id\"] = test[\"id\"]\n    submission[\"ACTION\"] = (predictions_1 + predictions_2) / 2\n    print(\"get submission\")\n    submission.to_csv(\"submission.csv\", index = False)\n    \n    \ndef model_fitting(parms):\n    model = get_model(params)\n    model.fit(data_train, y)\n    predictions = model.predict_proba(data_test)[:,1]\n    return predictions    \n    \nprint(\"function done\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sampleSubmission,train,test =load_dataset()\n# sampleSubmission = pd.read_csv(\"../input/amazon-employee-access-challenge/sampleSubmission.csv\")\n# test = pd.read_csv(\"../input/amazon-employee-access-challenge/test.csv\")\n# train = pd.read_csv(\"../input/amazon-employee-access-challenge/train.csv\")\ntarget = \"ACTION\"\ncol4train = [x for x in train.columns if x not in [target, \"ROLE_TITLE\"]]\ny = train[target].values\n\ntrain[col4train] = train[col4train].values.astype(str)\ntest[col4train] = test[col4train].values.astype(str)\n\nnew_col4train = col4train\nfor c1,c2 in combinations(col4train, 2):\n    name = \"{}_{}\".format(c1,c2)\n    new_col4train.append(name)\n    train[name] = train[c1] + \"_\" + train[c2]\n    test[name] = test[c1] + \"_\" + test[c2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#dataset #1\ncols_svd = ['MGR_ID_ROLE_CODE','MGR_ID_ROLE_DEPTNAME','MGR_ID_ROLE_FAMILY', \n            'RESOURCE_MGR_ID','RESOURCE_ROLE_CODE', 'RESOURCE_ROLE_FAMILY',\n            'RESOURCE_ROLE_ROLLUP_1','RESOURCE_ROLE_ROLLUP_2','RESOURCE',\n            'ROLE_DEPTNAME_ROLE_CODE','ROLE_DEPTNAME_ROLE_FAMILY',\n            'ROLE_FAMILY_DESC_ROLE_FAMILY','ROLE_FAMILY_ROLE_CODE',\n            'ROLE_FAMILY','ROLE_ROLLUP_1_ROLE_DEPTNAME',\n            'ROLE_ROLLUP_1_ROLE_FAMILY_DESC', 'ROLE_ROLLUP_1_ROLE_FAMILY',\n            'ROLE_ROLLUP_1','ROLE_ROLLUP_2']\n\ncols_rnd = ['MGR_ID_ROLE_DEPTNAME','MGR_ID_ROLE_FAMILY','MGR_ID_ROLE_ROLLUP_1',\n 'MGR_ID_ROLE_ROLLUP_2','MGR_ID','RESOURCE_MGR_ID','RESOURCE_ROLE_CODE',\n 'RESOURCE_ROLE_FAMILY_DESC','RESOURCE_ROLE_FAMILY','RESOURCE_ROLE_ROLLUP_1',\n 'RESOURCE_ROLE_ROLLUP_2','ROLE_DEPTNAME_ROLE_FAMILY_DESC','ROLE_FAMILY_DESC_ROLE_CODE',\n 'ROLE_FAMILY_DESC_ROLE_FAMILY','ROLE_FAMILY','ROLE_ROLLUP_1_ROLE_CODE',\n 'ROLE_ROLLUP_1_ROLE_DEPTNAME','ROLE_ROLLUP_1_ROLE_FAMILY_DESC','ROLE_ROLLUP_2_ROLE_FAMILY']\n\ncols_freq = ['MGR_ID_ROLE_DEPTNAME','RESOURCE_MGR_ID','RESOURCE_ROLE_CODE',\n 'RESOURCE_ROLE_DEPTNAME','RESOURCE_ROLE_FAMILY_DESC','RESOURCE_ROLE_FAMILY',\n 'RESOURCE_ROLE_ROLLUP_1','ROLE_DEPTNAME_ROLE_FAMILY_DESC','ROLE_DEPTNAME_ROLE_FAMILY',\n 'ROLE_DEPTNAME','ROLE_FAMILY_DESC_ROLE_CODE','ROLE_FAMILY_DESC_ROLE_FAMILY',\n 'ROLE_ROLLUP_1_ROLE_CODE','ROLE_ROLLUP_2_ROLE_DEPTNAME']\n\n#dataset #2\ncols_svd_2 = ['MGR_ID','RESOURCE_MGR_ID','RESOURCE_ROLE_CODE',\n 'RESOURCE_ROLE_DEPTNAME','RESOURCE_ROLE_FAMILY_DESC','RESOURCE_ROLE_FAMILY',\n 'RESOURCE_ROLE_ROLLUP_1','RESOURCE','ROLE_CODE',\n 'ROLE_DEPTNAME_ROLE_CODE','ROLE_DEPTNAME_ROLE_FAMILY','ROLE_FAMILY_DESC_ROLE_CODE',\n 'ROLE_FAMILY_DESC_ROLE_FAMILY','ROLE_FAMILY_DESC','ROLE_ROLLUP_1_ROLE_DEPTNAME',\n 'ROLE_ROLLUP_1_ROLE_FAMILY','ROLE_ROLLUP_1_ROLE_ROLLUP_2','ROLE_ROLLUP_2_ROLE_FAMILY_DESC',\n 'ROLE_ROLLUP_2_ROLE_FAMILY','ROLE_ROLLUP_2']\n\ncols_rnd_2 = ['MGR_ID_ROLE_CODE','MGR_ID_ROLE_DEPTNAME','MGR_ID_ROLE_FAMILY_DESC',\n 'MGR_ID_ROLE_ROLLUP_1','MGR_ID','RESOURCE_ROLE_DEPTNAME',\n 'RESOURCE_ROLE_FAMILY','RESOURCE_ROLE_ROLLUP_1','ROLE_CODE',\n 'ROLE_DEPTNAME_ROLE_FAMILY_DESC','ROLE_FAMILY_DESC_ROLE_CODE',\n 'ROLE_FAMILY_DESC_ROLE_FAMILY','ROLE_FAMILY_ROLE_CODE',\n 'ROLE_ROLLUP_1_ROLE_CODE','ROLE_ROLLUP_1_ROLE_FAMILY_DESC',\n 'ROLE_ROLLUP_1_ROLE_ROLLUP_2']\n\ncols_freq_2 = ['MGR_ID_ROLE_CODE','MGR_ID_ROLE_DEPTNAME','MGR_ID_ROLE_ROLLUP_1',\n 'MGR_ID_ROLE_ROLLUP_2','MGR_ID','RESOURCE_MGR_ID',\n 'RESOURCE_ROLE_DEPTNAME','RESOURCE_ROLE_FAMILY_DESC','RESOURCE_ROLE_ROLLUP_2',\n 'RESOURCE','ROLE_DEPTNAME_ROLE_FAMILY_DESC','ROLE_DEPTNAME',\n 'ROLE_FAMILY_DESC','ROLE_FAMILY','ROLE_ROLLUP_1_ROLE_FAMILY_DESC',\n 'ROLE_ROLLUP_1_ROLE_FAMILY','ROLE_ROLLUP_1_ROLE_ROLLUP_2',\n 'ROLE_ROLLUP_1','ROLE_ROLLUP_2_ROLE_CODE','ROLE_ROLLUP_2']\n\ncols_te = ['MGR_ID','RESOURCE_MGR_ID','RESOURCE_ROLE_CODE',\n 'RESOURCE_ROLE_DEPTNAME','RESOURCE_ROLE_ROLLUP_2','RESOURCE',\n 'ROLE_CODE','ROLE_DEPTNAME_ROLE_FAMILY_DESC','ROLE_DEPTNAME_ROLE_FAMILY',\n 'ROLE_FAMILY_DESC_ROLE_CODE','ROLE_FAMILY_DESC','ROLE_FAMILY_ROLE_CODE',\n 'ROLE_ROLLUP_1_ROLE_FAMILY','ROLE_ROLLUP_2_ROLE_FAMILY','ROLE_ROLLUP_2']\n\nprint(\"input necessary frame\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_svd = transform_dataset(train[cols_svd], test[cols_svd], get_col_interactions_svd)\ndata_rnd = transform_dataset(train[cols_rnd], test[cols_rnd], \n                             assign_rnd_integer, {\"number_of_times\":5})\ndata_freq = transform_dataset(train[cols_freq], test[cols_freq], get_freq_encoding)\n\ndata_train = pd.concat([x[0] for x in [data_svd, data_rnd, data_freq]], axis = 1)\ndata_test = pd.concat([x[1] for x in [data_svd, data_rnd, data_freq]], axis = 1)\ndata_train = data_train.values\ndata_test = data_test.values\nprint(\"Dataset shape, Train: {}, Test: {}\".format(data_train.shape, data_test.shape))\nprint(\"data_set_1 setted\")\nparams = {'colsample_bytree': 0.312002398119274,\n 'lambda_l1': 1.919962415701389,\n 'learning_rate': 0.03363113877976891,\n 'max_bin': 484,\n 'max_depth': 10,\n 'min_child_weight': 0.035307873174480586,\n 'num_leaves': 220}\n\nprediction_1 = model_fitting(params)\n\nprint(\"get prediction_1\")\n\ndata_svd_2 = transform_dataset(train[cols_svd_2], test[cols_svd_2], get_col_interactions_svd)\ndata_rnd_2 = transform_dataset(train[cols_rnd_2], test[cols_rnd_2], \n                             assign_rnd_integer, {\"number_of_times\":5})\ndata_freq_2 = transform_dataset(train[cols_freq_2], test[cols_freq_2], get_freq_encoding)\nte = TargetEncoding(columns_names = cols_te)\ndata_te_tr = get_CV_target_encoding(train[cols_te], y, te, 5)\nte.fit(train[cols_te], y)\ndata_te_te = te.transform(test[cols_te])\n\ndata_train_2 = pd.concat([x[0] for x in [data_svd_2, data_rnd_2, data_freq_2]], axis = 1)\ndata_test_2 = pd.concat([x[1] for x in [data_svd_2, data_rnd_2, data_freq_2]], axis = 1)\ndata_train_2 = pd.concat([data_train_2, data_te_tr], axis = 1)\ndata_test_2 = pd.concat([data_test_2, data_te_te], axis = 1)\nprint(\"Dataset shape, Train: {}, Test: {}\".format(data_train_2.shape, data_test_2.shape))\ndata_train_2 = data_train_2.values\ndata_test_2 = data_test_2.values\n\nparams_2 = {'colsample_bytree': 0.5280533549534434,\n 'lambda_l1': 0.1267270702844549,\n 'learning_rate': 0.012220447574715732,\n 'max_bin': 131,\n 'max_depth': 18,\n 'min_child_weight': 1.1518716916679328,\n 'num_leaves': 184}\n\nprediction_2 = model_fitting(params_2)\nprint(\"get prediction_2\")\n\nget_submission(prediction_1,prediction_2)\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}