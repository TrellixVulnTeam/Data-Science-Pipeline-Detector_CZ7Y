{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport seaborn as sn\nimport matplotlib.pylab as plt \n%matplotlib inline\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#load the data\ntrain=pd.read_csv('../input/detecting-insults-in-social-commentary/train.csv')\ntest=pd.read_csv('../input/detecting-insults-in-social-commentary/test.csv')\nverification_labels_df=pd.read_csv('../input/detecting-insults-in-social-commentary/impermium_verification_labels.csv')\ntest_with_solutions_df=pd.read_csv('../input/detecting-insults-in-social-commentary/test_with_solutions.csv')\nimperium_verification_set_df=pd.read_csv('../input/detecting-insults-in-social-commentary/impermium_verification_set.csv')\nsample_submission_null_df=pd.read_csv('../input/detecting-insults-in-social-commentary/sample_submission_null.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_len, test_len= len(train.index),len(test.index)\nprint(train_len,test_len)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"miss_val_train=train.isnull().sum(axis=0)/train_len\nmiss_val_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.Insult = train.Insult.astype(\"category\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train[train.Insult==0].count())\nprint(train[train.Insult==1].count())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_label = train[\"Insult\"]\ntrain_comment = train[\"Comment\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_comment = test[\"Comment\"]\ntrain_label.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"type(test_comment)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Data_to_clean = pd.concat([train_comment,test_comment],axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Data_to_clean.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"type(Data_to_clean)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\nfrom nltk import word_tokenize\nfrom nltk import sent_tokenize\nimport re ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"CONTRACTION_MAP = {\n\"ain't\": \"is not\",\n\"aren't\": \"are not\",\n\"can't\": \"cannot\",\n\"can't've\": \"cannot have\",\n\"'cause\": \"because\",\n\"could've\": \"could have\",\n\"couldn't\": \"could not\",\n\"couldn't've\": \"could not have\",\n\"didn't\": \"did not\",\n\"doesn't\": \"does not\",\n\"don't\": \"do not\",\n\"hadn't\": \"had not\",\n\"hadn't've\": \"had not have\",\n\"hasn't\": \"has not\",\n\"haven't\": \"have not\",\n\"he'd\": \"he would\",\n\"he'd've\": \"he would have\",\n\"he'll\": \"he will\",\n\"he'll've\": \"he he will have\",\n\"he's\": \"he is\",\n\"how'd\": \"how did\",\n\"how'd'y\": \"how do you\",\n\"how'll\": \"how will\",\n\"how's\": \"how is\",\n\"I'd\": \"I would\",\n\"I'd've\": \"I would have\",\n\"I'll\": \"I will\",\n\"I'll've\": \"I will have\",\n\"I'm\": \"I am\",\n\"I've\": \"I have\",\n\"i'd\": \"i would\",\n\"i'd've\": \"i would have\",\n\"i'll\": \"i will\",\n\"i'll've\": \"i will have\",\n\"i'm\": \"i am\",\n\"i've\": \"i have\",\n\"isn't\": \"is not\",\n\"it'd\": \"it would\",\n\"it'd've\": \"it would have\",\n\"it'll\": \"it will\",\n\"it'll've\": \"it will have\",\n\"it's\": \"it is\",\n\"let's\": \"let us\",\n\"ma'am\": \"madam\",\n\"mayn't\": \"may not\",\n\"might've\": \"might have\",\n\"mightn't\": \"might not\",\n\"mightn't've\": \"might not have\",\n\"must've\": \"must have\",\n\"mustn't\": \"must not\",\n\"mustn't've\": \"must not have\",\n\"needn't\": \"need not\",\n\"needn't've\": \"need not have\",\n\"o'clock\": \"of the clock\",\n\"oughtn't\": \"ought not\",\n\"oughtn't've\": \"ought not have\",\n\"shan't\": \"shall not\",\n\"sha'n't\": \"shall not\",\n\"shan't've\": \"shall not have\",\n\"she'd\": \"she would\",\n\"she'd've\": \"she would have\",\n\"she'll\": \"she will\",\n\"she'll've\": \"she will have\",\n\"she's\": \"she is\",\n\"should've\": \"should have\",\n\"shouldn't\": \"should not\",\n\"shouldn't've\": \"should not have\",\n\"so've\": \"so have\",\n\"so's\": \"so as\",\n\"that'd\": \"that would\",\n\"that'd've\": \"that would have\",\n\"that's\": \"that is\",\n\"there'd\": \"there would\",\n\"there'd've\": \"there would have\",\n\"there's\": \"there is\",\n\"they'd\": \"they would\",\n\"they'd've\": \"they would have\",\n\"they'll\": \"they will\",\n\"they'll've\": \"they will have\",\n\"they're\": \"they are\",\n\"they've\": \"they have\",\n\"to've\": \"to have\",\n\"wasn't\": \"was not\",\n\"we'd\": \"we would\",\n\"we'd've\": \"we would have\",\n\"we'll\": \"we will\",\n\"we'll've\": \"we will have\",\n\"we're\": \"we are\",\n\"we've\": \"we have\",\n\"weren't\": \"were not\",\n\"what'll\": \"what will\",\n\"what'll've\": \"what will have\",\n\"what're\": \"what are\",\n\"what's\": \"what is\",\n\"what've\": \"what have\",\n\"when's\": \"when is\",\n\"when've\": \"when have\",\n\"where'd\": \"where did\",\n\"where's\": \"where is\",\n\"where've\": \"where have\",\n\"who'll\": \"who will\",\n\"who'll've\": \"who will have\",\n\"who's\": \"who is\",\n\"who've\": \"who have\",\n\"why's\": \"why is\",\n\"why've\": \"why have\",\n\"will've\": \"will have\",\n\"won't\": \"will not\",\n\"won't've\": \"will not have\",\n\"would've\": \"would have\",\n\"wouldn't\": \"would not\",\n\"wouldn't've\": \"would not have\",\n\"y'all\": \"you all\",\n\"y'all'd\": \"you all would\",\n\"y'all'd've\": \"you all would have\",\n\"y'all're\": \"you all are\",\n\"y'all've\": \"you all have\",\n\"you'd\": \"you would\",\n\"you'd've\": \"you would have\",\n\"you'll\": \"you will\",\n\"you'll've\": \"you will have\",\n\"you're\": \"you are\",\n\"you've\": \"you have\"\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#import contractions\n#from contractions import contractions_dict\n#from contractions import contractions_re_keys\n#import re","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_characters_before_tokenization(sentence,keep_apostrophes=False):\n    sentence = sentence.strip()\n    if keep_apostrophes:\n        PATTERN = r'[?|$|&|*|%|@|(|)|~]' # add other characters here tocremove them\n        filtered_sentence = re.sub(PATTERN, r'', sentence)\n    else:\n        PATTERN = r'[^a-zA-Z0-9 ]' # only extract alpha-numeric characters\n        filtered_sentence = re.sub(PATTERN, r'', sentence)\n    return filtered_sentence\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Data_to_clean1 = [remove_characters_before_tokenization(i) for i in Data_to_clean]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def tokenize_text(text):\n    tokens = nltk.word_tokenize(text)\n    tokens = [token.strip() for token in tokens]\n    return tokens\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def expand_contractions(sentence, contraction_mapping):\n    import re\n    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), \n                                      flags=re.IGNORECASE|re.DOTALL)\n    \n    def expand_match(contraction):\n        \n        match = contraction.group(0)\n        first_char = match[0]\n        expanded_contraction = contraction_mapping.get(match)\\\n                               if contraction_mapping.get(match)\\\n                               else contraction_mapping.get(match.lower())\n        expanded_contraction = first_char+expanded_contraction[1:]\n        return expanded_contraction\n    expanded_sentence = contractions_pattern.sub(expand_match, sentence)\n    return expanded_sentence","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stopword_list = nltk.corpus.stopwords.words('english')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef remove_stopwords(text):\n    tokens = tokenize_text(text)\n    filtered_tokens = [token for token in tokens if token not in stopword_list]\n    filtered_text = ' '.join(filtered_tokens)\n    return filtered_text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def normalize_corpus(corpus, tokenize=False):\n    normalized_corpus = []\n    for text in corpus:\n        text = expand_contractions(text, CONTRACTION_MAP)\n        text = remove_stopwords(text)\n        normalized_corpus.append(text)\n        if tokenize:\n            text = tokenize_text(text)\n            normalized_corpus.append(text)\n    return normalized_corpus","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"normalized_data = normalize_corpus(corpus=Data_to_clean1,tokenize=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"normalized_data\n\nfrom nltk.corpus import wordnet","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_repeated_characters(tokens):\n    repeat_pattern = re.compile(r'(\\w*)(\\w)\\2(\\w*)')\n    match_substitution = r'\\1\\2\\3'\n    def replace(old_word):\n        if wordnet.synsets(old_word):\n            return old_word\n        new_word = repeat_pattern.sub(match_substitution, old_word)\n        return replace(new_word) if new_word != old_word else new_word\n    correct_tokens = [replace(word) for word in tokens]\n    return correct_tokens","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Data_to_clean2 = remove_repeated_characters(normalized_data)\nData_to_clean2\n\nData=[]\n\nfor text in Data_to_clean2:\n    Data.append(text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Data\n\n#splitting data back \n\ntrain_corpus = Data[:3947]\ntest_corpus = Data[3947:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import sklearn\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ndef feat_extract(data,ngram_range):\n    vectorizer = CountVectorizer(min_df=1,ngram_range=ngram_range)\n    feature = vectorizer.fit_transform(data)\n    return(vectorizer,feature)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ntrain_vec,train_feat = feat_extract(data=train_corpus,ngram_range=(1,3))\n\ntrain_vec.get_feature_names()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_vec)\nprint(train_feat)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features = train_feat.todense()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_vec,test_feat = feat_extract(data=test_corpus,ngram_range=(1,3))\n\ntest_vec.get_feature_names()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_features = test_feat.todense()\ntest_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfTransformer\n\ndef tfidf_transformer(matrix):\n    transform = TfidfTransformer(norm='l2',smooth_idf=True,use_idf=True)\n    tfidf_matrix = transform.fit_transform(matrix)\n    \n    return(transform, tfidf_matrix)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_transform , train_matrix = tfidf_transformer(train_features)\n\ntrain_final_feature = train_matrix.todense()\n\ntest_transform,test_matrix = tfidf_transformer(test_features)\n\ntest_final_feature = test_matrix.todense()\n\ntest_final_feature\n\ntest_final_feature.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import scipy\nfrom scipy import sparse\n\n\ntest_final_feature.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# converting to sparse matrix\nX_training,X_testing=sparse.csr_matrix(train_final_feature),sparse.csr_matrix(test_final_feature)\n\n# inspecting the transformed data\ntype(X_training),type(X_testing), X_training.shape, X_testing.shape\n\ntype(train_label)\n\nX_train = X_training[0:3157,0:14180]\nX_test = X_training[790:,0:14180]\ny_train = train_label[:3157]\ny_test = train_label[790:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.shape\n\n#train_label\n\n#Modelling....\n#Naive Bayes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#from sklearn import cross_validation as cv\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score,recall_score,precision_score\nfrom sklearn.model_selection import cross_val_score\n\nfrom sklearn.naive_bayes import MultinomialNB","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"NB = MultinomialNB()\n\nimport numpy as np\ndata = np.array(train_final_feature)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#(X_train,X_test,Y_train,_Y_test) = train_test_split(X=X_training,y=np.array(train_label),test_size=0.33,random_state=42)\n\nNB.fit(X=X_train,y=y_train)\n\ncross_val_score(estimator=NB,X=X_test,y=y_test,cv=5)\n\nNB_pred = NB.predict(X_test)\n\nprint(accuracy_score(y_true=y_test,y_pred=NB_pred))\n      \n\nprint(accuracy_score(y_true=y_test,y_pred=NB_pred),\"Accuracy\")\n\nprint(f1_score(y_true=y_test,y_pred=NB_pred,average='weighted'),\"F1_score\")\n\nprint(recall_score(y_true=y_test,y_pred=NB_pred,average='weighted'),\"recall_score/sensitivity\")\n\nprint(precision_score(y_true=y_test,y_pred=NB_pred,average='weighted'),\"precision_score\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#SVM\n\n\nfrom sklearn.linear_model import SGDClassifier\nSDG = SGDClassifier()\n\nSDG.fit(X=X_train,y=y_train)\n\ncross_val_score(estimator=SDG,X=X_test,y=y_test,cv=5)\n\nSDG_pred = SDG.predict(X_test)\n\nprint(accuracy_score(y_true=y_test,y_pred=SDG_pred),\"Accuracy\")\n\nprint(f1_score(y_true=y_test,y_pred=SDG_pred,average='weighted'),\"F1_score\")\n\nprint(recall_score(y_true=y_test,y_pred=SDG_pred,average='weighted'),\"recall_score/sensitivity\")\n\nprint(precision_score(y_true=y_test,y_pred=SDG_pred,average='weighted'),\"precision_score\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}