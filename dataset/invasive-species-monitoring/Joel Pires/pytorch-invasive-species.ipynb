{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"%reload_ext autoreload\n%autoreload 2\n%matplotlib inline\n\nimport pandas as pd\nimport numpy as np\nimport torch\nimport torch.optim as optim\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport random\nimport os\nimport cv2\nimport matplotlib.pyplot as plt \nimport zipfile\n\nfrom pathlib import Path\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import models\nfrom sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def read_image(path):\n    im = cv2.imread(str(path))\n    return cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n\ndef show_image(path):\n    im = read_image(path)\n    plt.imshow(im)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.system('7z x ../input/invasive-species-monitoring/train.7z -o./')\nos.system('7z x ../input/invasive-species-monitoring/test.7z -o./')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with zipfile.ZipFile(\"../input/invasive-species-monitoring/train_labels.csv.zip\",\"r\") as z:\n    z.extractall(\"./\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_pic = 'train/100.jpg'\nshow_image(sample_pic)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_filenames(path, folder):\n    \"\"\" function to retrive all image names in train/test folder\n    \"\"\"\n    file_path = os.path.join(path, folder) \n    file_names = [f for f in os.listdir(file_path) if not f.startswith(\".\")]\n    \n    return file_names\n\ndef get_labels(file_names, label_file):\n    \"\"\" function to find image labels given list of image names\n    \"\"\"\n    label_df = pd.read_csv(label_file)\n    label_names = [int(f.replace(\".jpg\",'')) for f in file_names]\n    labels = [label_df.iloc[n-1]['invasive'] for n in label_df.name.values] \n    \n    return labels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"files = get_filenames('./', 'train')\nlabels = get_labels(files, 'train_labels.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_files, valid_files, train_labels, valid_labels = train_test_split(files, labels, test_size=0.15, random_state=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"%d in train, %d in validaiton\" %(len(train_files), len(valid_files)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import math\n\ndef crop(im, r, c, target_r, target_c): \n    return im[r:r+target_r, c:c+target_c]\n\ndef random_crop(x, target_r, target_c):\n    \"\"\" Returns a random crop\"\"\"\n    r,c,*_ = x.shape\n    rand_r = random.uniform(0, 1)\n    rand_c = random.uniform(0, 1)\n    start_r = np.floor(rand_r*(r - target_r)).astype(int)\n    start_c = np.floor(rand_c*(c - target_c)).astype(int)\n    return crop(x, start_r, start_c, target_r, target_c)\n\ndef rotate_cv(im, deg, mode=cv2.BORDER_REFLECT, interpolation=cv2.INTER_AREA):\n    \"\"\" Rotates an image by deg degrees\"\"\"\n    r,c,*_ = im.shape\n    M = cv2.getRotationMatrix2D((c/2,r/2),deg,1)\n    return cv2.warpAffine(im,M,(c,r), borderMode=mode, flags=cv2.WARP_FILL_OUTLIERS+interpolation)\n\ndef center_crop(im, min_sz=None):\n    \"\"\" Returns a center crop of an image\"\"\"\n    r,c,*_ = im.shape\n    if min_sz is None: min_sz = min(r,c)\n    start_r = math.ceil((r-min_sz)/2)\n    start_c = math.ceil((c-min_sz)/2)\n    return crop(im, start_r, start_c, min_sz, min_sz)\n\ndef normalize(im):\n    \"\"\"Normalizes images with Imagenet stats.\"\"\"\n    imagenet_stats = np.array([[0.485, 0.456, 0.406], [0.229, 0.224, 0.225]])\n    return (im - imagenet_stats[0])/imagenet_stats[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Species_Dataset(Dataset):\n    def __init__(self, files, labels=None, folder='train', transform=False, size=300):\n        self.files = files\n        self.labels = labels\n        self.folder = folder\n        self.size = size\n        self.transform = transform\n        \n    def __len__(self):\n        return len(self.files)\n    \n    def __getitem__(self, idx):\n        path = os.path.join('./', self.folder, self.files[idx])\n        x = cv2.imread(path).astype(np.float32)\n        x = cv2.cvtColor(x, cv2.COLOR_BGR2RGB)/255\n        \n        if self.transform:\n            size2 = int(self.size * 1.05)\n            x = cv2.resize(x, (size2, size2))\n            x = random_crop(x, self.size, self.size)\n            rdeg = (np.random.random()-.50)*20\n            x = rotate_cv(x, rdeg)\n            if np.random.random() > 0.5: x = np.fliplr(x).copy() \n        else:\n            x = cv2.resize(x, (self.size, self.size))\n        \n        # substract numbers from resnet34\n        x = normalize(x)\n        \n        if self.labels:\n            return np.rollaxis(x, 2), self.labels[idx]\n        else: \n            return np.rollaxis(x, 2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_ds = Species_Dataset(train_files, train_labels, transform=True)\nvalid_ds = Species_Dataset(valid_files, valid_labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 16\ntrain_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\nvalid_dl = DataLoader(valid_ds, batch_size=batch_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        resnet = models.resnet34(pretrained=True)\n        # freezing parameters\n        for param in resnet.parameters():\n            param.requires_grad = False\n        # convolutional layers of resnet34\n        layers = list(resnet.children())[:8]\n        self.top_model = nn.Sequential(*layers).cuda()\n        self.bn1 = nn.BatchNorm1d(512)\n        self.bn2 = nn.BatchNorm1d(512)\n        self.fc1 = nn.Linear(512, 512)\n        self.fc2 = nn.Linear(512, 1)\n    \n    def forward(self, x):\n        x = F.relu(self.top_model(x))\n        x = nn.AdaptiveAvgPool2d((1,1))(x)\n        x = x.view(x.shape[0], -1) # flattening \n        #x = nn.Dropout(0.2)(x)\n        x = self.bn1(x)\n        x = F.relu(self.fc1(x))\n        #x = nn.Dropout(0.2)(x)\n        x = self.bn2(x)\n        x = self.fc2(x)\n        return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_optimizer(model, lr = 0.01, wd = 0.0):\n    parameters = filter(lambda p: p.requires_grad, model.parameters())\n    optim = torch.optim.Adam(parameters, lr=lr, weight_decay=wd)\n    return optim","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def save_model(m, p): torch.save(m.state_dict(), p)\n    \ndef load_model(m, p): m.load_state_dict(torch.load(p))\n\ndef LR_range_finder(model, train_dl, lr_low=1e-5, lr_high=1, epochs=2):\n    losses = []\n    p = \"mode_tmp.pth\"\n    save_model(model, str(p))\n    iterations = epochs * len(train_dl)\n    delta = (lr_high - lr_low)/iterations\n    lrs = [lr_low + i*delta for i in range(iterations)]\n    model.train()\n    ind = 0\n    for i in range(epochs):\n        for x,y in train_dl:\n            optim = get_optimizer(model, lr=lrs[ind])\n            x = x.cuda().float()\n            y = y.cuda().float().unsqueeze(1)\n            out = model(x)\n            loss = F.binary_cross_entropy_with_logits(out, y)\n            optim.zero_grad()\n            loss.backward()\n            optim.step()\n            losses.append(loss.item())\n            ind +=1\n            \n    load_model(model, str(p))\n    return lrs, losses ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Net().cuda()\nlrs, losses = LR_range_finder(model, train_dl, lr_low=1e-6, lr_high=1e-4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(lrs[:30], losses[:30])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_triangular_lr2(lr_low, lr_high, stepesize):\n    iterations = 2*stepesize\n    iter1 = int(0.35*iterations)\n    iter2 = int(0.85*iter1)\n    iter3 = iterations - iter1 - iter2\n    delta1 = (lr_high - lr_low)/iter1\n    delta2 = (lr_high - lr_low)/(iter1 -1)\n    lrs1 = [lr_low + i*delta1 for i in range(iter1)]\n    lrs2 = [lr_high - i*(delta1) for i in range(0, iter2)]\n    delta2 = (lrs2[-1] - lr_low)/(iter3)\n    lrs3 = [lrs2[-1] - i*(delta2) for i in range(1, iter3+1)]\n    return lrs1+lrs2+lrs3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_triangular_policy(model, train_dl, valid_dl, epochs=4, lr_low=1e-5, lr_high=0.01):\n    idx = 0\n    stepesize = 2*len(train_dl)\n    lrs = get_triangular_lr2(lr_low, lr_high, stepesize)\n    for i in range(epochs):\n        model.train()\n        total = 0\n        sum_loss = 0\n        for i, (x, y) in enumerate(train_dl):\n            optim = get_optimizer(model, lr = lrs[idx], wd =0)\n            batch = y.shape[0]\n            x = x.cuda().float()\n            y = y.cuda().float().unsqueeze(1)\n            out = model(x)\n            loss = F.binary_cross_entropy_with_logits(out, y)\n            optim.zero_grad()\n            loss.backward()\n            optim.step()\n            idx += 1\n            total += batch\n            sum_loss += batch*(loss.item())\n        print(\"train loss\", sum_loss/total)\n        val_metrics(model, valid_dl)\n    return sum_loss/total","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def val_metrics(model, valid_dl):\n    model.eval()\n    total = 0\n    sum_loss = 0\n    correct = 0 \n    for i, (x, y) in enumerate(valid_dl):\n        batch = y.shape[0]\n        x = x.cuda().float()\n        y = y.cuda().unsqueeze(1)\n        out = model(x)\n        pred = (out > 0.0).long()\n        correct += pred.eq(y.data).sum().item()\n        y = y.float()\n        loss = F.binary_cross_entropy_with_logits(out, y)\n        sum_loss += batch*(loss.item())\n        total += batch\n    print(\"val loss and accuracy\", sum_loss/total, correct/total)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from datetime import datetime\n\ndef training_loop(model, train_dl, valid_dl, epochs=4, steps=3, lr_low=1e-6, lr_high=1e-4):\n    for i in range(steps):\n        start = datetime.now() \n        loss = train_triangular_policy(model, train_dl, valid_dl, epochs, lr_low, lr_high)\n        end = datetime.now()\n        t = 'Time elapsed {}'.format(end - start)\n        print(\"----End of step\", t)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Net().cuda()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"val_metrics(model, valid_dl)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training_loop(model, train_dl, valid_dl, steps=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_files = files = get_filenames('./', 'test')\ntest_ds = Species_Dataset(test_files, folder='test',transform=False)\ntest_dl = DataLoader(test_ds, batch_size=batch_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict(model, test_dl):\n    model.eval()\n    preds = []\n    ys = []\n    for x in test_dl:\n        x = x.cuda().float()\n        out = model(x)\n        preds.append(out.cpu().detach().numpy())\n\n    return np.vstack(preds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = predict(model, test_dl)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def sigmoid(x): return 1/(1+np.exp(-x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prob_preds = sigmoid(preds.reshape(-1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prob_preds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def pred_to_df(files, preds):\n    name = [int(f.replace(\".jpg\",'')) for f in files]\n    pred = pd.DataFrame({\"name\": name, \"invasive\":preds})\n    pred = pred.sort_values(by='name')\n    pred = pred[['name', 'invasive']]\n    return pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_df = pred_to_df(test_files, prob_preds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_df.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# DISCLAIMER: THIS CODE IS A RESULT OF FOLLOWING THIS TUTORIAL: https://www.youtube.com/watch?v=TvwYV0viIQE","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.3"}},"nbformat":4,"nbformat_minor":1}