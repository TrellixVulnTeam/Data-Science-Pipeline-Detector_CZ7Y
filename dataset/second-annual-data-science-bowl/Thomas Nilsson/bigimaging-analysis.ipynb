{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport os\n\ndef load_patient_data(pid):\n    pid = str(pid)\n    filename = \"patient_%s.npz\" % pid\n    path = os.path.join(\"..\", \"input\",\"bigimaging-preprocess\", filename)\n    np_data = np.load(path)\n    slice_dist = np.float(np_data['slice_dist'])\n    images = np_data['images']\n    \n    return images, slice_dist","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom skimage.util import montage as montage2d\n\nmontage3d = lambda x, **k: montage2d(np.stack([montage2d(y, **k) for y in x], 0))\n\ndef plot_patient_slices_3d(patient_slices, title=False, figsize=(20, 20)):\n    '''Plots a 2D image per slice in series (3D in total)'''\n    fig, ax = plt.subplots(1, 1, figsize=figsize)\n    image = montage2d(patient_slices)\n    if title: ax.set_title(title)\n    ax.imshow(image, cmap='bone')\n\n\ndef plot_patient_data_4d(patient_data, all_slices=False, num_slices=[0], figsize=(20, 20)):\n    '''Plots a 3D image per time step in patient data (4D in total)'''\n    if all_slices:\n        # Number of slices is equal to the first dimension of the patient image array\n        num_slices = range(patient_data.shape[0])\n    for i in num_slices:\n        plot_patient_slices_3d(patient_data[i],\n                               title=('Showing slice %i' % i))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Load a patient from input "},{"metadata":{"trusted":true},"cell_type":"code","source":"images, slice_dist = np_data = load_patient_data(13)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Plot patient data"},{"metadata":{"trusted":true},"cell_type":"code","source":"slice_idx = 4#int(len(images) / 2)\nplot_patient_data_4d(images, num_slices=[slice_idx]) #all_slices=True","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Segmentation and watershed"},{"metadata":{"trusted":true},"cell_type":"code","source":"from skimage.filters import try_all_threshold\n\nfig, ax = try_all_threshold(images[6,0], figsize=(10, 8), verbose=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import cv2\nfrom sklearn.cluster import KMeans\nfrom skimage.filters import threshold_otsu\n\ndef Segmentation(patient_img):\n    \"\"\"Returns matrix\n    Segmententation of patient_img with k-means\n    \"\"\"\n    \"\"\"Z = np.float32(np.ravel(patient_img))\n    criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 10, 1.0)\n    flags = cv2.KMEANS_RANDOM_CENTERS\n    compactness, labels, centers = cv2.kmeans(Z, 2, None, criteria, 10, flags)\n    return labels.reshape(patient_img.shape)\n    \"\"\"\n    \"\"\"\n    xx, yy = np.meshgrid(np.arange(patient_img.shape[1]),np.arange(patient_img.shape[0]))\n    pat_df = pd.DataFrame(dict(x=xx.ravel(),y=yy.ravel(),intensity=patient_img.ravel()))\n    km = KMeans(n_clusters=2, random_state=2018)\n    \n    scale_pat_df = pat_df.copy()\n    scale_pat_df.x = scale_pat_df.x/xy_div\n    scale_pat_df.y = scale_pat_df.y/xy_div\n    scale_pat_df['group'] = km.fit_predict(scale_pat_df[['intensity']].values)\n\n    return scale_pat_df['group'].values.reshape(patient_img.shape)\n    \"\"\"\n    \n    thresh = threshold_otsu(patient_img)\n    binary = patient_img > thresh\n    return binary\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# NOT USED\n\nfrom skimage.feature import canny\nfrom skimage.transform import hough_circle, hough_circle_peaks\n\ndef hough_circle_transform_xy(image, min_radii = 30, max_radii = 35):\n    \"\"\"\n    Returns two lists\n    Circle Hough Transform on image and returns x and y coordinates for 10 prominent circles\n    \"\"\"    \n    # Radii for hough transform circles\n    hough_radii = np.arange(min_radii, max_radii, 2)\n    \n    # Detect edges for hough transform.\n    edges = canny(image, sigma=2.0, low_threshold=0.55, high_threshold=0.8)\n    \n    # Detect circles with circle hough transform. https://en.wikipedia.org/wiki/Circle_Hough_Transform\n    hough_res = hough_circle(edges, hough_radii)\n            \n    # Select the most prominent 10 circles\n    accums, cx, cy, radii = hough_circle_peaks(hough_res, hough_radii,total_num_peaks=10)\n    \n    return cx, cy    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from skimage.morphology import opening, disk\nfrom scipy.ndimage import distance_transform_edt\nfrom skimage.morphology import watershed\nfrom skimage.feature import peak_local_max\n\n# Code from: https://nbviewer.jupyter.org/github/kmader/Quantitative-Big-Imaging-2019/blob/master/Lectures/07-ComplexObjects.ipynb\ndef watershed_img(image):\n    # Distance map\n    image_dmap = distance_transform_edt(image)\n    # Distance peaks\n    image_peaks = label(peak_local_max(image_dmap, indices=False, footprint=np.ones((40, 40)),labels=image, exclude_border=True))\n    # Watershed first once\n    ws_labels = watershed(-image_dmap, image_peaks, mask=image)\n    \n    # Reomve small segments\n    label_area_dict = {i: np.sum(ws_labels == i)for i in np.unique(ws_labels[ws_labels > 0])}\n    clean_label_maxi = image_peaks.copy()\n    lab_areas = list(label_area_dict.values())\n    # Remove 20 percentile\n    area_cutoff = np.percentile(lab_areas, 20)\n    for i, k in label_area_dict.items():\n        if k <= area_cutoff:\n            clean_label_maxi[clean_label_maxi == i] = 0\n    # Watershed again\n    ws_labels = watershed(-image_dmap, clean_label_maxi, mask=image)\n\n    return ws_labels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from skimage.measure import label\n\ndef labeled_segmented_images(images):\n    \"\"\"\n    Returns numpy array (4d)\n    Segments image and used watershed for labeling.\n    \"\"\"\n    \n    num_slices, time, height, width = images.shape\n    segmented_slices = np.zeros((num_slices, time, height, width))\n    \n    # Iterate over all slices and whole timeseries for images\n    for i in range(num_slices):\n        for j in range(time):\n            # Segmentation\n            seg_slice = Segmentation(images[i,j])\n            \n            # Makes all segmented images same, Only used for Kmeans. (Background = 0)\n            #if seg_slice.sum() > seg_slice.size*0.5:\n            #    seg_slice = 1 - seg_slice\n            \n            # Watershed\n            labels = watershed_img(seg_slice)\n            \n            # Writes labeled segmented object to return images                     \n            segmented_slices[i,j] = labels\n\n    return segmented_slices","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from skimage.measure import regionprops\n\ndef find_left_ventricle(images):\n    \"\"\"\n    Returns numpy array (4d)\n    Finds left ventricle from labeled segmented images\n    \"\"\"\n    \n    num_slices, time, height, width = images.shape\n    segmented_slices = np.zeros((num_slices, time, height, width))\n    \n    all_labels = labeled_segmented_images(images)\n    \n    # Iterate over all slices and whole timeseries for images\n    for i in range(num_slices):\n        for j in range(time):\n            \n            labels = all_labels[i,j]\n            min_dist = 50\n            min_dist_label = 0\n            segment_found =  False\n            \n            # Iterate over every label in watershed labels to predict which is the left ventricle.\n            for label in np.unique(labels):\n        \n                # yx coordinates for labaled segmentation\n                yx_coord_labels = np.where(labels == label)\n                \n                # Do not count small or big segmatations (removes dots and background)\n                if len(yx_coord_labels[0]) > 8000 or len(yx_coord_labels[0]) < 100:\n                    continue\n                \n                # Upper right middle coordinates\n                cx = 3*(height/4)\n                cy = width/4\n                \n                \n                # Calculates euclidiean distance between mean coordinates for segmentated labels and middle of image\n                euclidiean_dist = np.sqrt((int(cy)-np.mean(yx_coord_labels[0]))**2+(int(cx)-np.mean(yx_coord_labels[1]))**2)\n                \n                # Gets min distance\n                if euclidiean_dist < min_dist:\n                    \n                    # Check if segment shape is round.\n                    regions = regionprops((labels == label).astype(int))\n                    props = regions[0]\n                    y0, x0 = props.centroid\n                    orientation = props.orientation\n                    x1 = x0 + np.cos(orientation) * 0.5 * props.major_axis_length\n                    y1 = y0 - np.sin(orientation) * 0.5 * props.major_axis_length\n                    x2 = x0 - np.sin(orientation) * 0.5 * props.minor_axis_length\n                    y2 = y0 - np.cos(orientation) * 0.5 * props.minor_axis_length\n                \n                    d1_dist = np.sqrt(abs(x0-x1)**2+abs(y0-y1)**2)\n                    d2_dist = np.sqrt(abs(x0-x2)**2+abs(y0-y2)**2)\n                    \n                    # Checks if segment is round.\n                    if abs(d1_dist-d2_dist) > 15:\n                        continue\n                    \n                    min_dist_label = label\n                    min_dist = euclidiean_dist\n                    segment_found = True\n            \n            # Checks if we found a image or not\n            if segment_found:\n                # Writes segmented object to return images                     \n                segmented_slices[i,j] = (labels == min_dist_label).astype(int)\n            else:\n                segmented_slices[i,j] = np.zeros(labels.shape)\n                \n    return segmented_slices, all_labels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"segmented_left_ventricle, all_segments = find_left_ventricle(images)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax  = plt.subplots(1,3, figsize=(30,10))\n\ni = 4\nj = 15\n\nfig.suptitle('Patient #150', fontsize=16)\n\nax[0].imshow(images[i,j])\nax[1].imshow(segmented_left_ventricle[i,j])\nax[2].imshow(all_segments[i,j], cmap='gray')\n\nax[0].set_title('Original Slice')\nax[1].set_title('Selected Segment')\nax[2].set_title('All Watershedded Segments')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plot_patient_data_4d(segmented_left_ventricle, num_slices=[i])\nplot_patient_data_4d(segmented_left_ventricle, all_slices= True) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_patient_data_4d(all_segments, all_slices= True) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Analysis for Volume"},{"metadata":{"trusted":true},"cell_type":"code","source":"def smooth(y, box_pts):\n    box = np.ones(box_pts)/box_pts\n    y_smooth = np.convolve(y, box, mode='same')\n    return y_smooth","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def volume_for_patient(patient_images, slice_dist):\n    \"\"\"\n    Return numpy array\n    Array of total volume at each time for segmented images\n    \"\"\"\n    \n    num_slices, time, height, width = patient_images.shape\n    volume = np.zeros((time))\n    \n    if slice_dist == 0:\n        print(\"WARNING! Slice ditance is: 0 \\n Setting slice distance to 10.\")\n        slice_dist = 10\n    \n    for i in range(time):\n        time_volume = 0\n        for j in range(num_slices):\n            xy_size = np.sum(patient_images[j,i])\n            time_volume = time_volume + xy_size * slice_dist\n            \n        # Volume in ml instead of mm^3\n        volume[i] = time_volume/1000\n        \n    # Smoothing volume with convolution and removes last elements\n    smooth_volume = smooth(volume,4)[2:-2]\n    return smooth_volume    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"v1 = volume_for_patient(segmented_left_ventricle, slice_dist)\nplt.plot(v1)\nplt.xlabel('Time')\nplt.ylabel('Volume')\nplt.title('Total Volume')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### Analysis for multiple patients"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Analysis for nbr patients and saves their max/min volume\n\nnbr = 49 #Nbr of patients to be loaded\n\nmin_max_volumes = np.zeros((nbr,2))\n\nfor i in range(nbr):\n    print(i+1)\n    \n    # Special dumb patients...\n    if i == 40:\n        print(\"Patient 41 does not exist. Replaces with patient 40 data. Not a solution...\")\n        min_max_volumes[i,0] = min(volumes)\n        min_max_volumes[i,1] = max(volumes)\n        continue\n    #if i == 2:\n        #print(\"Patient 3 is bad outlier. Replaces with patient 40 data. Not a solution...\")\n        #min_max_volumes[i,0] = min(volumes)\n        #min_max_volumes[i,1] = max(volumes)\n        #continue    \n    \n    # Load patient\n    images, slice_dist = np_data = load_patient_data(i+1)\n    # Segment and load images of patient\n    segmented_left_ventricle, _ = find_left_ventricle(images)\n    # Calculate volume for patient\n    volumes = volume_for_patient(segmented_left_ventricle, slice_dist)\n    \n    min_max_volumes[i,0] = min(volumes)\n    min_max_volumes[i,1] = max(volumes)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import training data. (There is probably a easyer and faster way way to do this...)\n\nimport pandas as pd\nfilename = \"train (1).csv\"\npath = os.path.join(\"..\", \"input\",\"true-volume\", filename)\ntrue_min_max = pd.read_csv(path)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"true_min_max = true_min_max[(true_min_max.Id < nbr+1) & (true_min_max.Id > 0)]\ntrue_min_max_volumes = np.zeros((nbr,2))\n\nfor Id in range(nbr):\n    true_min_max_volumes[Id,0] = true_min_max.Systole.iloc[Id]\n    true_min_max_volumes[Id,1] = true_min_max.Diastole.iloc[Id]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(true_min_max_volumes[:,0], '*', label=\"true\")\nplt.plot(min_max_volumes[:,0], '+', color='r', label=\"predicted\")\nplt.suptitle('Max Volume for each patient', fontsize=16)\nplt.ylabel('Volume')\nplt.xlabel('Patient')\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(min_max_volumes[:,0],true_min_max_volumes[:,0], '+')\nplt.suptitle('True vs Pred - min Volume', fontsize=16)\nplt.ylabel('True min Volume')\nplt.xlabel('Predicted min Volume')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(true_min_max_volumes[:,1], '*', label=\"true\")\nplt.plot(min_max_volumes[:,1], '+', color='r', label=\"predicted\")\nplt.suptitle('Max Volume for each patient', fontsize=16)\nplt.ylabel('Volume')\nplt.xlabel('Patient')\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(min_max_volumes[:,1],true_min_max_volumes[:,1], '+')\nplt.suptitle('True vs Pred - max Volume', fontsize=16)\nplt.ylabel('True max Volume')\nplt.xlabel('Predicted max Volume')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Correlation check\ncorr_max = np.corrcoef(min_max_volumes[:,0], true_min_max_volumes[:,0])\ncorr_min = np.corrcoef(min_max_volumes[:,1], true_min_max_volumes[:,1])\n\nprint(corr_max[0,1], \"Correlation predicted and true max values\")\nprint(corr_min[0,1], \"Correlation predicted and true min values\")\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Linear regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"min_max_volumes_re = np.zeros(min_max_volumes.shape)\n\nmin_slope,min_offset = np.polyfit(min_max_volumes[:,0], true_min_max_volumes[:,0], 1)\nmax_slope,max_offset = np.polyfit(min_max_volumes[:,1], true_min_max_volumes[:,1], 1)\n\nmin_max_volumes_re[:,0] = min_offset + min_slope*min_max_volumes[:,0]\nmin_max_volumes_re[:,1] = max_offset + max_slope*min_max_volumes[:,1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def ejection_func(volume_diastole, volume_systole):\n    \"\"\"\n    Returns int\n    Calculates ejection\n    \"\"\"\n    ejection = 100 * ((volume_systole-volume_diastole) / volume_diastole)\n    return ejection","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculating ture and predicted ejection for each patient\ntrue_ejection = np.zeros(len(true_min_max_volumes))\npred_ejection = np.zeros(len(true_min_max_volumes))\n\nfor i in range(len(true_min_max_volumes)):\n    true_ejection[i] = ejection_func(true_min_max_volumes[i,0],true_min_max_volumes[i,1])\n    pred_ejection[i] = ejection_func(min_max_volumes_re[i,0],min_max_volumes_re[i,1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(true_ejection, '*', label=\"true\")\nplt.plot(pred_ejection, '+', color='r', label=\"predicted\")\nplt.suptitle('Ejection for each patient', fontsize=16)\nplt.ylabel('Ejection')\nplt.xlabel('Patient')\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculating MSE\nmse_error = sum((true_ejection-pred_ejection)**2)/nbr","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(mse_error, \"MSE\")\nprint((np.sqrt(mse_error)/np.mean(true_ejection)), \"sqrt(MSE) divided on mean true ejection\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":1}