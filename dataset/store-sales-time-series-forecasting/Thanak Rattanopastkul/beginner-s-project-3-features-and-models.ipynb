{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-03-25T20:04:20.189473Z","iopub.status.idle":"2022-03-25T20:04:20.189941Z","shell.execute_reply.started":"2022-03-25T20:04:20.189681Z","shell.execute_reply":"2022-03-25T20:04:20.189706Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### This is a work in progress\n\nThis is my 3rd. notebook of learning Time Series ML with this Store Sales dataset. The 3rd. attempt here will be focusing on 3 things:\n1. Feature Selection\n2. More Compilated ML models\n3. Pipeline\n\n\nFor anyone interested in the earlier notebooks in the series, here's the link to them.\n* [1# attempt : Focusing on EDA](https://www.kaggle.com/code/thanakr/beginner-s-first-project-store-sales)\n* [2# attempt : Focusing on data formatting and expanding windows](https://www.kaggle.com/code/thanakr/beginner-s-project-2nd-attempt)\n\nAlso I'm trying to improve the score in each attempts and eventually beat the Kaggle's course score of 0.5109. Here's my record.\n\n### Score Log\n\n#### ---- First Notebook----\n#### 1st. edit -> RSMLE 3.44178\n* LinearRegression\n* without checking collinearity\n* no zero sales formatting\n\n#### ---- Second Notebook----\n#### 2nd. edit -> RSMLE 2.37238\n* LinearRegression\n* Flawed data formatting\n* Need to drop NA\n* Wrong formatted data lag\n* Collinearity Checked\n* Manually mark the zero sales product\n\n#### 3rd. edit -> RSMLE 1.99939 The score Improve Significantly\n* LinearRegression\n* Drop NA / Lag fixed\n* Reduced sales / transactions features to only MA15\n* Collinearity Checked\n* Manually mark the zero sales product\n\n#### 4th. edit -> RSMLE 1.97205 Best Score Yet\n* LinearRegression + Expanding windows\n* Drop NA / Lag fixed\n* Reduced sales / transactions features to only MA15\n* Collinearity Checked\n* Manually mark the zero sales product\n\n#### ---- Third Notebook----\nwe are here\n\n\n### Let's begin\n\nReference: \nhttps://machinelearningmastery.com/modeling-pipeline-optimization-with-scikit-learn/\nhttps://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html\n","metadata":{}},{"cell_type":"markdown","source":"# The Plan\n1. Impliment the Feature selection/dimensions reduction method and comparing them.\n    * using sklearn.feature_selection\n    * using sklearn.decomposition\n    \n2. Try different models\n    * Linear Regression\n    * Poisson Regression\n    * Polymomial Regression\n    * Random Forest\n    * XGBoost\n    * LGB\n    \n3. Try build the Pipeline","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\n\nfrom warnings import simplefilter\nsimplefilter(\"ignore\")  # ignore warnings to clean up output cells\n\nimport gc # for garbage cleaning\n\n%config Completer.use_jedi = False # for autocomplete ","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-03-25T20:04:20.191134Z","iopub.status.idle":"2022-03-25T20:04:20.191609Z","shell.execute_reply.started":"2022-03-25T20:04:20.191356Z","shell.execute_reply":"2022-03-25T20:04:20.191382Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"holiday_data = pd.read_csv(\"/kaggle/input/store-sales-time-series-forecasting/holidays_events.csv\",)\ntrain_data = pd.read_csv(\"/kaggle/input/store-sales-time-series-forecasting/train.csv\",)\ntest_data = pd.read_csv(\"/kaggle/input/store-sales-time-series-forecasting/test.csv\")\ntransaction_data = pd.read_csv(\"/kaggle/input/store-sales-time-series-forecasting/transactions.csv\")","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-03-25T20:04:20.193887Z","iopub.status.idle":"2022-03-25T20:04:20.194381Z","shell.execute_reply.started":"2022-03-25T20:04:20.194102Z","shell.execute_reply":"2022-03-25T20:04:20.194127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.tail()","metadata":{"execution":{"iopub.status.busy":"2022-03-25T20:04:20.19622Z","iopub.status.idle":"2022-03-25T20:04:20.197116Z","shell.execute_reply.started":"2022-03-25T20:04:20.196808Z","shell.execute_reply":"2022-03-25T20:04:20.196841Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transaction_data.tail()","metadata":{"execution":{"iopub.status.busy":"2022-03-25T20:04:20.198341Z","iopub.status.idle":"2022-03-25T20:04:20.198805Z","shell.execute_reply.started":"2022-03-25T20:04:20.198554Z","shell.execute_reply":"2022-03-25T20:04:20.198579Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"holiday_data.tail()","metadata":{"execution":{"iopub.status.busy":"2022-03-25T20:04:20.199823Z","iopub.status.idle":"2022-03-25T20:04:20.200299Z","shell.execute_reply.started":"2022-03-25T20:04:20.200054Z","shell.execute_reply":"2022-03-25T20:04:20.200079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data.tail()","metadata":{"execution":{"iopub.status.busy":"2022-03-25T20:04:20.202543Z","iopub.status.idle":"2022-03-25T20:04:20.203018Z","shell.execute_reply.started":"2022-03-25T20:04:20.202753Z","shell.execute_reply":"2022-03-25T20:04:20.202777Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Constructing the main inputs DataFrame (Code are from the 2nd attempt [notebook.](https://www.kaggle.com/code/thanakr/beginner-s-project-2nd-attempt/notebook))","metadata":{}},{"cell_type":"code","source":"stack_sales = pd.concat([train_data,test_data]) #stack train and test together for them to be formatted together.\n\nstack_sales['date'] = pd.to_datetime(stack_sales['date']) #format 'date' column as datetime\n\ntransaction_data['date'] = pd.to_datetime(transaction_data['date']) #format 'date' column as datetime\n\nstack_sales = stack_sales.merge(transaction_data,how='left',on=['date','store_nbr']) # merge then together\n\nstack_sales = stack_sales.sort_values(['family','store_nbr','date']).reset_index() # sort by product family and store for picking the right lag\n\nstack_sales.fillna(0, inplace= True) # Fill sales and transcation NaN with zero\n\n#----this section is for manually making the dummy variabie ----\nnbr_dummy = pd.get_dummies(stack_sales['store_nbr'],prefix='store',drop_first=True) # create dummy\nfamily_dummy = pd.get_dummies(stack_sales['family'],prefix='fam',drop_first=True) # create dummy\n\nstack_sales = stack_sales.merge(nbr_dummy, how ='left', left_index = True, right_index = True) # merge dummy\nstack_sales = stack_sales.merge(family_dummy, how ='left', left_index = True, right_index = True) # merge dummy\n\nstack_sales.drop(['store_nbr','family'],axis = 1, inplace = True)\n#----this section is for manually making the dummy variabie ----\n\nstack_sales.drop(['index'],axis = 1, inplace = True)\n\nstack_sales.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-25T20:04:20.204571Z","iopub.status.idle":"2022-03-25T20:04:20.205056Z","shell.execute_reply.started":"2022-03-25T20:04:20.204785Z","shell.execute_reply":"2022-03-25T20:04:20.20481Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Constructing the holiday section of main inputs DataFrame (Code adapted from the 2nd attempt [notebook.](https://www.kaggle.com/code/thanakr/beginner-s-project-2nd-attempt/notebook))\n","metadata":{}},{"cell_type":"code","source":"holiday = holiday_data.set_index('date')\nholiday.index = pd.to_datetime(holiday.index) #format index as datetime\nnat_holiday_data = holiday.loc[holiday['locale'].str.contains('National')] #filter only National Holidays\n\ncalendar = pd.DataFrame(index = pd.date_range('2013-01-01','2017-08-31')) #create datetime index from first to last day to use as anchors\ncalendar = calendar.join(nat_holiday_data).fillna(0) #merge holiday with datetime anchor\n\ncalendar['is_ny'] = 0 # add new year dummy\ncalendar.loc['2013-01-01', 'is_ny'] = 1 # add new year \ncalendar.loc['2014-01-01', 'is_ny'] = 1 # add new year \ncalendar.loc['2015-01-01', 'is_ny'] = 1 # add new year \ncalendar.loc['2016-01-01', 'is_ny'] = 1 # add new year \ncalendar.loc['2017-01-01', 'is_ny'] = 1 # add new year \n\ncalendar['dow'] = calendar.index.weekday+1 #set Monday to 1 and Sunday to 7\ncalendar['moy'] = calendar.index.month \n\ncalendar['is_non_workday'] = 0\n\ncalendar.loc[calendar['locale'] == 'National', 'is_non_workday'] = 1 #make national events a non_workday\ncalendar.loc[calendar['dow'] > 5 , 'is_non_workday'] = 1 #make Sat and Sun non_workday\ncalendar.loc[calendar['type'] == 'Work Day', 'is_non_workday'] = 0 #make Work Day holiday a working day\ncalendar.loc[(calendar['transferred'] == True) & \n             (calendar['dow'] < 6), \n             'is_non_workday'] = 0 #mark a transfered weekday a working day\n\ncalendar['is_football'] = 0 #add football dummy column\ncalendar['is_eq'] = 0 #add earthquake dummy column\ncalendar['is_shopping'] = 0 # add shopping events dummy column\n\n#mark football matches\ncalendar.loc[(calendar['locale'] == 'National') & \n             (calendar['description'].str.contains('futbol')), 'is_football'] = 1\n\n#mark the earthquake\ncalendar.loc[(calendar['locale'] == 'National') & \n             (calendar['description'].str.contains('Terremoto')), 'is_eq'] = 1\n\n#mark Cyber Mondays\ncalendar.loc[(calendar['locale'] == 'National') & \n             (calendar['description'].str.contains('Cyber Monday')), 'is_shopping'] = 1\n\n#mark Black Fridays\ncalendar.loc[(calendar['locale'] == 'National') & \n             (calendar['description'].str.contains('Black Friday')), 'is_shopping'] = 1\n\n#----this section is for manually making the dummy variabie ----\n\ndow_dummy = pd.get_dummies(calendar['dow'],prefix = 'dow',drop_first = True) #make dummy for day of week\nmoy_dummy = pd.get_dummies(calendar['moy'],prefix = 'moy',drop_first = True) # make dummy for month of yeawr\n\ncalendar = calendar.merge(dow_dummy, how='left', left_index = True, right_index = True) #merge dow dummy\ncalendar = calendar.merge(moy_dummy, how='left', left_index = True, right_index = True) #merge moy dummy\n\n#slice only the needed columns\n\ncalendar_rdy = calendar[['is_ny',\n       'is_non_workday', 'is_football', 'is_eq', 'is_shopping',\n       'moy_2', 'moy_3',\n       'moy_4', 'moy_5', 'moy_6', 'moy_7', 'moy_8', 'moy_9', 'moy_10',\n       'moy_11', 'moy_12']] \n\n#----this section is for manually making the dummy variabie ----\n\n\n#calendar_rdy = calendar[['is_ny','is_non_workday', 'is_football', 'is_eq', 'is_shopping','dow','moy']] ","metadata":{"execution":{"iopub.status.busy":"2022-03-25T20:04:20.206457Z","iopub.status.idle":"2022-03-25T20:04:20.206952Z","shell.execute_reply.started":"2022-03-25T20:04:20.206677Z","shell.execute_reply":"2022-03-25T20:04:20.206703Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"calendar_rdy.tail()","metadata":{"execution":{"iopub.status.busy":"2022-03-25T20:04:20.209348Z","iopub.status.idle":"2022-03-25T20:04:20.20983Z","shell.execute_reply.started":"2022-03-25T20:04:20.209566Z","shell.execute_reply":"2022-03-25T20:04:20.209592Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stack_sales = stack_sales.merge(calendar_rdy, how ='left', left_on = 'date', right_on = calendar_rdy.index) #merge sales with holiday","metadata":{"execution":{"iopub.status.busy":"2022-03-25T20:04:20.211531Z","iopub.status.idle":"2022-03-25T20:04:20.212031Z","shell.execute_reply.started":"2022-03-25T20:04:20.211751Z","shell.execute_reply":"2022-03-25T20:04:20.211779Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del calendar\ndel calendar_rdy\ndel holiday_data\ndel train_data\ndel moy_dummy\ndel dow_dummy\n\ngc.collect()","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-03-25T20:04:20.213994Z","iopub.status.idle":"2022-03-25T20:04:20.214497Z","shell.execute_reply.started":"2022-03-25T20:04:20.21421Z","shell.execute_reply":"2022-03-25T20:04:20.214236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's try various feature engineering. They will be filtered with feature selection.","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\ndef sd_scaling(df, column_names):\n    sdScale = StandardScaler()\n    for col in column_names:\n        sdScale.fit(df[[col]])\n        df['{}_scaled'.format(col)] = sdScale.transform(df[[col]]) \n        \nsd_scaling(stack_sales, ['sales','onpromotion', 'transactions'])","metadata":{"execution":{"iopub.status.busy":"2022-03-25T20:04:20.215757Z","iopub.status.idle":"2022-03-25T20:04:20.216227Z","shell.execute_reply.started":"2022-03-25T20:04:20.215976Z","shell.execute_reply":"2022-03-25T20:04:20.216002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def make_column_lag(df, column_name, lag_list):\n#     for lag in lag_list:\n#         df['{}_lag_{}'.format(column_name, lag)] = df[column_name].shift(lag)\n        \n# make_column_lag(stack_sales,'sales_scaled',[1 ,2 ,3 ,4 ,5 ,6 ,7 ,8 ,13 ,14])\n# make_column_lag(stack_sales,'transactions_scaled',[21, 22, 28])","metadata":{"execution":{"iopub.status.busy":"2022-03-25T20:04:20.218011Z","iopub.status.idle":"2022-03-25T20:04:20.2185Z","shell.execute_reply.started":"2022-03-25T20:04:20.21822Z","shell.execute_reply":"2022-03-25T20:04:20.218245Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_column_ma(df, column_name, ma_list):\n    for ma in ma_list:\n        df['{}_ma_{}'.format(column_name, ma)] = df[column_name].rolling(ma, \n                                                                         min_periods = ma, \n                                                                         closed='left').mean()\n#ma_list = [8, 15, 31]\nma_list = [8]\n# ma_list+1 is to shift the lag further back 1 step to offset the exclusion of the current day.\nmake_column_ma(stack_sales,'sales_scaled',ma_list)\nmake_column_ma(stack_sales,'transactions_scaled',ma_list)","metadata":{"execution":{"iopub.status.busy":"2022-03-25T20:04:20.220038Z","iopub.status.idle":"2022-03-25T20:04:20.220732Z","shell.execute_reply.started":"2022-03-25T20:04:20.220252Z","shell.execute_reply":"2022-03-25T20:04:20.220278Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stack_sales = stack_sales.drop(['onpromotion','transactions','sales_scaled','transactions_scaled'],axis = 1)\nstack_sales.dropna(inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-03-25T20:04:20.222757Z","iopub.status.idle":"2022-03-25T20:04:20.223094Z","shell.execute_reply.started":"2022-03-25T20:04:20.222919Z","shell.execute_reply":"2022-03-25T20:04:20.222936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"check the current columns.","metadata":{}},{"cell_type":"code","source":"X_train = stack_sales.set_index('date').loc[:'2017-08-15'].drop(['id','sales'],axis=1)\ny_train = stack_sales.set_index('date').loc[:'2017-08-15', 'sales']\nX_test = stack_sales.set_index('date').loc['2017-08-16':].drop(['id','sales'],axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-03-25T20:04:20.224223Z","iopub.status.idle":"2022-03-25T20:04:20.224593Z","shell.execute_reply.started":"2022-03-25T20:04:20.2244Z","shell.execute_reply":"2022-03-25T20:04:20.224425Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del stack_sales\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-03-25T20:04:20.225502Z","iopub.status.idle":"2022-03-25T20:04:20.22583Z","shell.execute_reply.started":"2022-03-25T20:04:20.225649Z","shell.execute_reply":"2022-03-25T20:04:20.225672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Comparing Models\n\nHere I will write a loop to test all the model listed above with each featured selection criteria tested earlier. They will be compared with the accuracy to see how well thay can produce the output with the reduced feature.\n\n### Let't begin by train and predict model using the full inputs as a benchmark.\n\n* LinearRegression\n* ~~PolynomialFeature~~ Computational expensive for PolynomialFeature()\n    * will be test again in feature selection\n* Poisson Distribution Regression\n* ~~Randomforest~~ \n    * using RandomForest with timeseries regression is not a very good choice. Random Forest can't extrapolate for the newly introduced magnitude of features\n* XGBoost\n* LGB\n","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression, PoissonRegressor\n\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor","metadata":{"execution":{"iopub.status.busy":"2022-03-25T20:04:20.229338Z","iopub.status.idle":"2022-03-25T20:04:20.229683Z","shell.execute_reply.started":"2022-03-25T20:04:20.229508Z","shell.execute_reply":"2022-03-25T20:04:20.229531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.info(verbose=True)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-03-25T20:04:20.230928Z","iopub.status.idle":"2022-03-25T20:04:20.231217Z","shell.execute_reply.started":"2022-03-25T20:04:20.231064Z","shell.execute_reply":"2022-03-25T20:04:20.231079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from sklearn.metrics import mean_squared_log_error\nfrom sklearn.pipeline import make_pipeline \nfrom sklearn.preprocessing import PolynomialFeatures\n\nimport re\nX_train = X_train.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))  #fix the \"LightGBMError: Do not support special JSON characters in feature name.\"\nX_train.tail()\n\nlr_pipe = make_pipeline(LinearRegression())\n# poly_pipe = make_pipeline(PolynomialFeatures(degree=3, include_bias=False),LinearRegression())\npoisson_pipe = make_pipeline(PoissonRegressor())\nxgb_pipe = make_pipeline(XGBRegressor(tree_method='hist'))\nlgb_pipe = make_pipeline(LGBMRegressor())\n\npipes = [lr_pipe,poisson_pipe,xgb_pipe,lgb_pipe]\npipe_dic = {0: 'Linear Regression',1:'Poission Regression',2:'XGBregressor',3:'LGBRegressor'}","metadata":{"execution":{"iopub.status.busy":"2022-03-25T20:04:20.232057Z","iopub.status.idle":"2022-03-25T20:04:20.232424Z","shell.execute_reply.started":"2022-03-25T20:04:20.232202Z","shell.execute_reply":"2022-03-25T20:04:20.232224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From all available features (all the dummy and only lag/ma 8) Xgb seem to do the best job at accuracy on the training job. Let's see them working with the test data.","metadata":{}},{"cell_type":"code","source":"y_pred_dic = {}\n\nfor i,pipe in enumerate(pipe_dic):\n    print('training {}'.format(pipe_dic[i]))\n    pipes[i].fit(X_train,y_train)\n    print('{} score : {} \\n'.format(pipe_dic[i],pipes[i].score(X_train,y_train)))\n    print('Predict using {} \\n'.format(pipe_dic[i]))\n    y_pred_dic[pipe] = pipes[i].predict(X_test)\n    \n    ","metadata":{"execution":{"iopub.status.busy":"2022-03-25T20:04:20.233268Z","iopub.status.idle":"2022-03-25T20:04:20.23361Z","shell.execute_reply.started":"2022-03-25T20:04:20.233449Z","shell.execute_reply":"2022-03-25T20:04:20.233466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred_all_feature_df = pd.DataFrame(y_pred_dic)\ny_pred_all_feature_df = y_pred_all_feature_df.rename(pipe_dic,axis=1)\ny_pred_all_feature_df","metadata":{"execution":{"iopub.status.busy":"2022-03-25T20:04:20.238636Z","iopub.status.idle":"2022-03-25T20:04:20.238988Z","shell.execute_reply.started":"2022-03-25T20:04:20.238798Z","shell.execute_reply":"2022-03-25T20:04:20.238828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred_all_feature_df = test_data.merge(y_pred_all_feature_df,how='left',left_index=True, right_index=True)","metadata":{"execution":{"iopub.status.busy":"2022-03-25T20:04:20.24017Z","iopub.status.idle":"2022-03-25T20:04:20.240611Z","shell.execute_reply.started":"2022-03-25T20:04:20.240426Z","shell.execute_reply":"2022-03-25T20:04:20.240452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"let's compare them together using line plot","metadata":{}},{"cell_type":"code","source":"\nfig,axs = plt.subplots(4,figsize=(30,30))\n\n\naxs[0].plot(y_pred_all_feature_df['Linear Regression'],c='r',label='Linear Regression')\naxs[1].plot(y_pred_all_feature_df['Poission Regression'],c='g',label='Poission Regression')\naxs[2].plot(y_pred_all_feature_df['XGBregressor'],c='b',label='XGBregressor')\naxs[3].plot(y_pred_all_feature_df['LGBRegressor'],c='purple',label='LGBRegressor')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-25T20:04:20.241665Z","iopub.status.idle":"2022-03-25T20:04:20.241991Z","shell.execute_reply.started":"2022-03-25T20:04:20.241813Z","shell.execute_reply":"2022-03-25T20:04:20.241835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"let's try submit the result from all feature just to see the score. I will take a note of the test score below\n","metadata":{}},{"cell_type":"code","source":"y_pred_all_feature_df","metadata":{"execution":{"iopub.status.busy":"2022-03-25T20:04:20.243793Z","iopub.status.idle":"2022-03-25T20:04:20.244394Z","shell.execute_reply.started":"2022-03-25T20:04:20.244069Z","shell.execute_reply":"2022-03-25T20:04:20.244098Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submit = y_pred_all_feature_df[['id','LGBRegressor']]\nsubmit = submit.rename({'LGBRegressor':'sales'}, axis=1)\nsubmit.to_csv('submission.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2022-03-25T20:04:20.245986Z","iopub.status.idle":"2022-03-25T20:04:20.246486Z","shell.execute_reply.started":"2022-03-25T20:04:20.246201Z","shell.execute_reply":"2022-03-25T20:04:20.246226Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Linear Regression score : 0.8878299241319337 \nPredict using Linear Regression -> Score 1.97205\n\nPoission Regression score : 0.8856629555251515 \nPredict using Poission Regression -> Score 2.86212\n\nXGBregressor score : 0.9468573006502172 \nPredict using XGBregressor -> Score 3.57705\n\nLGBRegressor score : 0.9340568917907807 \nPredict using LGBRegressor -> Score ","metadata":{}},{"cell_type":"code","source":"#current working cell","metadata":{"execution":{"iopub.status.busy":"2022-03-25T20:04:20.248112Z","iopub.status.idle":"2022-03-25T20:04:20.248623Z","shell.execute_reply.started":"2022-03-25T20:04:20.248351Z","shell.execute_reply":"2022-03-25T20:04:20.248378Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Selection\n\nNow, let's apply some feature selection class.\n* VarianceThreshold -> Drop thge low varience feature.\n* SelectKbest -> picking only the big weight features\n* ~~Recursive feature elimination -> Recursively pick the good features in or filter bad features out.~~ Computationally Expensive\n* Seelct From model -> pick only the bigger weight from the .coef_ attribute after the model is fitted.","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_selection import VarianceThreshold, SelectFromModel, SelectKBest, f_regression","metadata":{"execution":{"iopub.status.busy":"2022-03-25T20:04:20.2502Z","iopub.status.idle":"2022-03-25T20:04:20.250693Z","shell.execute_reply.started":"2022-03-25T20:04:20.250436Z","shell.execute_reply":"2022-03-25T20:04:20.250462Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#VarianceThreshold\nvar_thresh = VarianceThreshold()\nvar_thresh_X_train = var_thresh.fit(X_train)","metadata":{"execution":{"iopub.status.busy":"2022-03-25T20:04:20.2521Z","iopub.status.idle":"2022-03-25T20:04:20.25259Z","shell.execute_reply.started":"2022-03-25T20:04:20.252324Z","shell.execute_reply":"2022-03-25T20:04:20.25235Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"var_thresh_X_train = var_thresh_X_train.transform(X_train)","metadata":{"execution":{"iopub.status.busy":"2022-03-25T20:04:20.25409Z","iopub.status.idle":"2022-03-25T20:04:20.254578Z","shell.execute_reply.started":"2022-03-25T20:04:20.254317Z","shell.execute_reply":"2022-03-25T20:04:20.254345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(var_thresh_X_train.shape)\nprint(X_train.shape)","metadata":{"execution":{"iopub.status.busy":"2022-03-25T20:04:20.256566Z","iopub.status.idle":"2022-03-25T20:04:20.257001Z","shell.execute_reply.started":"2022-03-25T20:04:20.256826Z","shell.execute_reply":"2022-03-25T20:04:20.256846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del var_thresh\ndel var_thresh_X_train\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-03-25T20:04:20.257951Z","iopub.status.idle":"2022-03-25T20:04:20.258268Z","shell.execute_reply.started":"2022-03-25T20:04:20.258099Z","shell.execute_reply":"2022-03-25T20:04:20.258122Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Seem like the varianceThreshold can't reduce any dimension here. let's continue on SelectKBest","metadata":{}},{"cell_type":"code","source":"k_best = SelectKBest(f_regression, k=100) #let's keep only 100 features\nk_best.fit(X_train,y_train)","metadata":{"execution":{"iopub.status.busy":"2022-03-25T20:04:20.260031Z","iopub.status.idle":"2022-03-25T20:04:20.260415Z","shell.execute_reply.started":"2022-03-25T20:04:20.260194Z","shell.execute_reply":"2022-03-25T20:04:20.260218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"col = k_best.get_support(indices=True)\nX_train.iloc[:,col].columns","metadata":{"execution":{"iopub.status.busy":"2022-03-25T20:04:20.261845Z","iopub.status.idle":"2022-03-25T20:04:20.262176Z","shell.execute_reply.started":"2022-03-25T20:04:20.262007Z","shell.execute_reply":"2022-03-25T20:04:20.262023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"k_best = k_best.transform(X_train)","metadata":{"execution":{"iopub.status.busy":"2022-03-25T20:04:20.263843Z","iopub.status.idle":"2022-03-25T20:04:20.264175Z","shell.execute_reply.started":"2022-03-25T20:04:20.26399Z","shell.execute_reply":"2022-03-25T20:04:20.264016Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(k_best.shape)\nprint(X_train.shape)","metadata":{"execution":{"iopub.status.busy":"2022-03-25T20:04:20.265415Z","iopub.status.idle":"2022-03-25T20:04:20.265745Z","shell.execute_reply.started":"2022-03-25T20:04:20.265565Z","shell.execute_reply":"2022-03-25T20:04:20.265581Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let's do the SelecrFromModel Class.","metadata":{}},{"cell_type":"code","source":"#linear regression\nfrom_model_lr = SelectFromModel(LinearRegression(fit_intercept=True))\nfrom_model_lr = from_model_lr.fit(X_train,y_train)","metadata":{"execution":{"iopub.status.busy":"2022-03-25T20:04:20.267122Z","iopub.status.idle":"2022-03-25T20:04:20.267535Z","shell.execute_reply.started":"2022-03-25T20:04:20.267291Z","shell.execute_reply":"2022-03-25T20:04:20.267344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from_model_lr.get_feature_names_out()","metadata":{"execution":{"iopub.status.busy":"2022-03-25T20:04:20.268639Z","iopub.status.idle":"2022-03-25T20:04:20.268971Z","shell.execute_reply.started":"2022-03-25T20:04:20.268787Z","shell.execute_reply":"2022-03-25T20:04:20.26881Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This reduce a lot of dimentionality for linear regression. Now, let's try polynomial","metadata":{}},{"cell_type":"code","source":"X_train_to_poly = X_train[['onpromotion_scaled','sales_scaled_ma_8','transactions_scaled_ma_8']]","metadata":{"execution":{"iopub.status.busy":"2022-03-25T20:04:20.270859Z","iopub.status.idle":"2022-03-25T20:04:20.271238Z","shell.execute_reply.started":"2022-03-25T20:04:20.271012Z","shell.execute_reply":"2022-03-25T20:04:20.271034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"poly_transformer = PolynomialFeatures(degree=3, include_bias=False)\nX_train_to_poly = poly_transformer.fit_transform(X_train_to_poly)","metadata":{"execution":{"iopub.status.busy":"2022-03-25T20:04:20.27261Z","iopub.status.idle":"2022-03-25T20:04:20.272945Z","shell.execute_reply.started":"2022-03-25T20:04:20.272755Z","shell.execute_reply":"2022-03-25T20:04:20.272778Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"temp = pd.DataFrame(X_train_to_poly,columns = ['poly_{}'.format(i) for i in range(19)])","metadata":{"execution":{"iopub.status.busy":"2022-03-25T20:04:20.274181Z","iopub.status.idle":"2022-03-25T20:04:20.27475Z","shell.execute_reply.started":"2022-03-25T20:04:20.274563Z","shell.execute_reply":"2022-03-25T20:04:20.274588Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train","metadata":{"execution":{"iopub.status.busy":"2022-03-25T20:04:20.276146Z","iopub.status.idle":"2022-03-25T20:04:20.276518Z","shell.execute_reply.started":"2022-03-25T20:04:20.276317Z","shell.execute_reply":"2022-03-25T20:04:20.276355Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"temp = temp.merge(X_train.reset_index(),how='left',left_index=True,right_index=True)","metadata":{"execution":{"iopub.status.busy":"2022-03-25T20:04:20.27827Z","iopub.status.idle":"2022-03-25T20:04:20.278857Z","shell.execute_reply.started":"2022-03-25T20:04:20.278555Z","shell.execute_reply":"2022-03-25T20:04:20.278588Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"temp = temp.drop(['onpromotion_scaled','sales_scaled_ma_8','transactions_scaled_ma_8'], axis = 1)","metadata":{"execution":{"iopub.status.busy":"2022-03-25T20:04:20.280618Z","iopub.status.idle":"2022-03-25T20:04:20.281104Z","shell.execute_reply.started":"2022-03-25T20:04:20.280835Z","shell.execute_reply":"2022-03-25T20:04:20.280861Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"temp.set_index('date', inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-03-25T20:04:20.282628Z","iopub.status.idle":"2022-03-25T20:04:20.283103Z","shell.execute_reply.started":"2022-03-25T20:04:20.282837Z","shell.execute_reply":"2022-03-25T20:04:20.282863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"temp","metadata":{"execution":{"iopub.status.busy":"2022-03-25T20:04:20.287037Z","iopub.status.idle":"2022-03-25T20:04:20.287586Z","shell.execute_reply.started":"2022-03-25T20:04:20.287264Z","shell.execute_reply":"2022-03-25T20:04:20.287289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from_model_poly = SelectFromModel(LinearRegression())\nfrom_model_poly = from_model_poly.fit(temp,y_train)","metadata":{"execution":{"iopub.status.busy":"2022-03-25T20:04:20.28921Z","iopub.status.idle":"2022-03-25T20:04:20.289709Z","shell.execute_reply.started":"2022-03-25T20:04:20.28945Z","shell.execute_reply":"2022-03-25T20:04:20.289476Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from_model_poly.get_feature_names_out()","metadata":{"execution":{"iopub.status.busy":"2022-03-25T20:04:20.291565Z","iopub.status.idle":"2022-03-25T20:04:20.292053Z","shell.execute_reply.started":"2022-03-25T20:04:20.291785Z","shell.execute_reply":"2022-03-25T20:04:20.29181Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's try Poisson Distribution Regressor","metadata":{}},{"cell_type":"code","source":"from_model_p = SelectFromModel(PoissonRegressor())\nfrom_model_p = from_model_p.fit(X_train,y_train)","metadata":{"execution":{"iopub.status.busy":"2022-03-25T20:04:20.293804Z","iopub.status.idle":"2022-03-25T20:04:20.294271Z","shell.execute_reply.started":"2022-03-25T20:04:20.294022Z","shell.execute_reply":"2022-03-25T20:04:20.294047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from_model_p.get_feature_names_out()","metadata":{"execution":{"iopub.status.busy":"2022-03-25T20:04:20.295808Z","iopub.status.idle":"2022-03-25T20:04:20.296281Z","shell.execute_reply.started":"2022-03-25T20:04:20.296026Z","shell.execute_reply":"2022-03-25T20:04:20.296052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"feature for XGBoost","metadata":{}},{"cell_type":"code","source":"xgb = XGBRegressor(tree_method='hist')\nxgb = xgb.fit(X_train,y_train)","metadata":{"execution":{"iopub.status.busy":"2022-03-25T20:04:20.297412Z","iopub.status.idle":"2022-03-25T20:04:20.297861Z","shell.execute_reply.started":"2022-03-25T20:04:20.297615Z","shell.execute_reply":"2022-03-25T20:04:20.29764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's see the top 20 most important features for XGB.","metadata":{}},{"cell_type":"code","source":"from xgboost import plot_importance\nplot_importance(xgb,max_num_features=20,)","metadata":{"execution":{"iopub.status.busy":"2022-03-25T20:04:20.299475Z","iopub.status.idle":"2022-03-25T20:04:20.299948Z","shell.execute_reply.started":"2022-03-25T20:04:20.299686Z","shell.execute_reply":"2022-03-25T20:04:20.299711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Feature for LGB","metadata":{}},{"cell_type":"code","source":"import re\nX_train = X_train.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))  #fix the \"LightGBMError: Do not support special JSON characters in feature name.\"\n\n\nlgb = LGBMRegressor()\nlgb.fit(X_train,y_train)","metadata":{"execution":{"iopub.status.busy":"2022-03-25T20:04:20.3018Z","iopub.status.idle":"2022-03-25T20:04:20.302327Z","shell.execute_reply.started":"2022-03-25T20:04:20.30202Z","shell.execute_reply":"2022-03-25T20:04:20.302047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"importance = pd.DataFrame()\nimportance['Features'] = lgb.feature_name_\nimportance['Importance']  = lgb.feature_importances_\n\nimportance.sort_values('Importance',ascending=False)[:21]","metadata":{"execution":{"iopub.status.busy":"2022-03-25T20:04:20.303535Z","iopub.status.idle":"2022-03-25T20:04:20.304039Z","shell.execute_reply.started":"2022-03-25T20:04:20.303815Z","shell.execute_reply":"2022-03-25T20:04:20.303843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"lgb weight a bit different than xgb. For instance, is_eq has more say than fam_GROCERYI.","metadata":{}},{"cell_type":"markdown","source":"\n\n","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Create formatting pipeline","metadata":{}}]}