{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Hybrid Model - Introduction #\n- simple (linear) model recursively improved by the second (gradient boosting) model\n\n- based on Kaggle Time Series Course: https://www.kaggle.com/learn/time-series\n\n- motivation to share this notebook is presented in Discussion about course\n\nRun the cell below to:\n(1) import libraries;\n(2) load data from files;\n(3) set deafaults for charts.","metadata":{}},{"cell_type":"code","source":"# Imports\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport time\nfrom statsmodels.tsa.deterministic import DeterministicProcess, CalendarFourier\nfrom sklearn.metrics import mean_squared_log_error, mean_squared_error, mean_absolute_error\nfrom sklearn.linear_model import Ridge, LinearRegression\nfrom sklearn.feature_selection import mutual_info_regression\nfrom lightgbm import LGBMRegressor\nfrom learntools.time_series.utils import (plot_lags, make_lags, make_leads,\n                                          plot_multistep, make_multistep_target,\n                                          plot_periodogram, seasonal_plot)\nfrom pathlib import Path\n\n# Load data from files\ncomp_dir = Path('../input/store-sales-time-series-forecasting')\n\ntrain = pd.read_csv(comp_dir / 'train.csv',\n                    parse_dates=['date'], infer_datetime_format=True,\n                    dtype={'store_nbr': 'category',\n                           'family': 'category',\n                           'sales': 'float32',\n                           'onpromotion': 'uint64'})\ntrain['date'] = train.date.dt.to_period(freq=\"D\")\n\ntest = pd.read_csv(comp_dir / 'test.csv',\n                   parse_dates=['date'], infer_datetime_format=True,\n                   dtype={'store_nbr': 'category',\n                          'family': 'category',\n                          'onpromotion': 'uint32'})\ntest['date'] = test.date.dt.to_period(freq='D')\n\ntransactions = pd.read_csv(comp_dir / 'transactions.csv',\n                           parse_dates=['date'], infer_datetime_format=True,\n                           dtype={'store_nbr': 'category',\n                                  'transactions': 'uint32'})\n\nstores = pd.read_csv(comp_dir / 'stores.csv')\n\noil = pd.read_csv(comp_dir / 'oil.csv',\n                  parse_dates=['date'], infer_datetime_format=True)\noil = oil.set_index('date').to_period('D')\n\nholidays_events = pd.read_csv(comp_dir / 'holidays_events.csv',\n                              parse_dates=['date'], infer_datetime_format=True,\n                              dtype={'type': 'category',\n                                     'locale': 'category',\n                                     'locale_name': 'category',\n                                     'description': 'category',\n                                     'transferred': 'bool'})\nholidays_events = holidays_events.set_index('date').to_period('D')\n\n\n# Set Matplotlib defaults\nsns.set_theme(color_codes=True)\nplt.rc('figure',\n       autolayout=True,\n       figsize=(11, 4),\n       titlesize=18,\n       titleweight='bold')\nplt.rc('axes',\n       labelweight='bold',\n       labelsize='large',\n       titleweight='bold',\n       titlepad=10,\n       titlesize=16)\nplot_params = dict(color='0.75',\n                   style='.-',\n                   markeredgecolor='0.25',\n                   markerfacecolor='0.25',\n                   legend=False)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-16T06:47:35.221833Z","iopub.execute_input":"2022-05-16T06:47:35.222158Z","iopub.status.idle":"2022-05-16T06:47:42.322406Z","shell.execute_reply.started":"2022-05-16T06:47:35.222122Z","shell.execute_reply":"2022-05-16T06:47:42.321396Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In the next cell we define some useful functions. Feel free to skip it out and return here only when you need it. ","metadata":{}},{"cell_type":"code","source":"# Share plot: EDA\ndef share_plot_eda(ts, START, END, shop, start_col, num_families, show_trend, show_model_1, show_improvement,\n                   trend_frame=None, model_1_fit=None, model_1_predict=None, hybrid_predict=None):\n    \"\"\"Plot charts of many (\"num_families\") families,\n    starting from given family (\"start_col\"), in given store (\"shop\") at once;\n    for time series table \"ts\", from \"START\" to \"END\" date\"\"\"\n    families = ts[shop].columns[start_col:start_col + num_families]\n    axs = ts[shop].loc(axis=1)[families].loc[START:END].plot(\n        subplots=True, sharex=True, figsize=(11, 9), **plot_params, alpha=1)\n    if show_trend:\n        _ = trend_frame['sales'][shop].loc(axis=1)[families].loc[START:END].plot(\n            subplots=True, color='blue', alpha=0.5, linewidth=1.5, ax=axs, legend=False)\n    if show_model_1:\n        _ = model_1_fit['sales'][shop].loc(axis=1)[families].loc[START:END].plot(subplots=True, color='blue', ax=axs)\n        _ = model_1_predict['sales'][shop].loc(axis=1)[families].plot(subplots=True, color='green', ax=axs)\n    if show_improvement:\n        _ = hybrid_predict['sales'][shop].loc(axis=1)[families].plot(subplots=True, color='red', ax=axs)\n    for ax, family in zip(axs, families):\n        if show_model_1:\n            ax.legend(['sales', 'fit', 'predict'], loc='upper left')\n        ax.set_title(family + \" || \" + shop)\n\n\n# Share plot: residuals\ndef share_plot_residuals(ts, START, END, shop, start_col, num_families):\n    \"\"\"Plot charts of many (\"num_families\") families,\n    starting from given family (\"start_col\"), in given store (\"shop\") at once;\n    for residuals table \"ts\", from \"START\" to \"END\" date\"\"\"\n    families = ts[shop].columns[start_col:start_col + num_families]\n    axs = ts[shop].loc(axis=1)[families].loc[START:END].plot(\n        subplots=True, sharex=True, figsize=(11, 9), alpha=0.5, style='.'\n    )\n    for ax, family in zip(axs, families):\n        ax.set_title(family + \" || \" + shop)\n\n\n\n# Create features based on datetime index\ndef create_index_features(basic_set, steps, fourier_freq='M', fourier_order=4, seasonal=True, order=1,\n                          holidays=True, oil_prices=True):\n    \"\"\"Use DeterministicProcess and CalendarFourier to create:\n    - \"trend-feature\" of given order;\n    - indicators for weekly seasons (\"seasonal\");\n    - Fourier features for long-time seasons;\n    - indicators for holidays;\n    - oil prices.\n    Validation or test set starts after the last day of basic set\"\"\"\n    fourier = CalendarFourier(freq=fourier_freq, order=fourier_order)\n    dp = DeterministicProcess(index=basic_set.index,\n                              constant=True,\n                              order=order,\n                              seasonal=seasonal,\n                              additional_terms=[fourier],\n                              drop=True)\n    # Create features for the first model (features derived from index) - train set\n    X_train = dp.in_sample()\n    X_test = dp.out_of_sample(steps=steps)\n    X_test.index.rename('date', inplace=True)\n    if holidays:\n        X_train = X_train.merge(X_holidays, how='left', left_index=True, right_index=True).fillna(0)\n        X_test = X_test.merge(X_holidays, how='left', left_index=True, right_index=True).fillna(0)\n    if oil_prices:\n        X_train = X_train.merge(oil, how='left', left_index=True, right_index=True) \\\n            .fillna(method='ffill').fillna(method='bfill')\n        X_test = X_test.merge(oil, how='left', left_index=True, right_index=True) \\\n            .fillna(method='ffill').fillna(method='bfill')\n    return X_train, X_test\n\n\n# Error function\ndef error_check(y_train_, y_fit_, y_valid_, y_pred_):\n    \"\"\"Returns RMSLE, RMSE and MAE for train and validation data.\"\"\"\n    # Root mean square log error for train and validation data\n    rmsle_train = mean_squared_log_error(y_train_, y_fit_) ** 0.5\n    rmsle_valid = mean_squared_log_error(y_valid_, y_pred_) ** 0.5\n    print(f'Training RMSLE: {rmsle_train:.5f}')\n    print(f'Validation RMSLE: {rmsle_valid:.5f}')\n    # Root mean square error for train and validation data\n    rmse_train = mean_squared_error(y_train_, y_fit_) ** 0.5\n    rmse_valid = mean_squared_error(y_valid_, y_pred_) ** 0.5\n    print(f'Training RMSE: {rmse_train:.5f}')\n    print(f'Validation RMSE: {rmse_valid:.5f}')\n    # Mean absolute error for train and validation data\n    mae_train = mean_absolute_error(y_train_, y_fit_)\n    mae_valid = mean_absolute_error(y_valid_, y_pred_)\n    print(f'Training MAE: {mae_train:.5f}')\n    print(f'Validation MAE: {mae_valid:.5f}')\n\n\n# Prepare submission file\ndef prepare_submission(forecast, path):\n    \"\"\"Prepare submission file.\"\"\"\n    forecast_ = forecast.unstack()\n    forecast_ = forecast_.reset_index()\n    # forecast_ = forecast_.drop(columns={'level_0'})\n    forecast_ = forecast_.rename(columns={forecast_.columns[-1]: 'sales'})\n    sub = test.merge(forecast_, on=['date', 'store_nbr', 'family'])\n    submit = sub[['id', 'sales']]\n    submit.to_csv(path, index=False)\n\n\n# Compute lags\ndef lets_lag(df, group, column, name, lag):\n    \"\"\"Compute n-lag of time series in long format.\"\"\"\n    lag = df.groupby(group)[column].shift(lag)\n    lag.index = df.index\n    lag = lag.to_frame()\n    lag = lag.rename(columns={lag.columns[0]: name})\n    return lag\n\n\n# Compute rolling means\ndef lets_roll(df, group, column, name, window, min_periods=1, center=False):\n    \"\"\"Compute rolling mean for (1-lagged) time series.\"\"\"\n    rolling_mean = df.groupby(group)[column]. \\\n        rolling(window=window, min_periods=min_periods, center=center).mean()\n    rolling_mean.index = df.index\n    # rolling_mean = rolling_mean.to_frame()\n    rolling_mean = rolling_mean.rename(columns={rolling_mean.columns[0]: name})\n    return rolling_mean\n\n\n# Compute ewm\ndef lets_ewm(df, group, column, name, alpha):\n    \"\"\"Compute ewm for (1-lagged) time series.\"\"\"\n    ewm = df.groupby(group)[column]. \\\n        ewm(alpha=alpha).mean()\n    ewm.index = df.index\n    # ewm = ewm.to_frame()\n    ewm = ewm.rename(columns={ewm.columns[0]: name})\n    return ewm\n\n\n# Create basic tables\ndef create_next_day_table(basic_set):\n    \"\"\"Given basetable in wide format, creates features (lags, rollings, etc.),\n    splits data into train set ('before'), with corresponding 'target', and test/valid set ('after').\"\"\"\n    # Add next day to existing table and transform table to the long format\n    next_day = pd.DataFrame(columns=basic_set.columns,\n                            index=pd.period_range(start=basic_set.index.max() + 1, periods=1, name='date'))\n    concat = pd.concat([basic_set, next_day]).fillna(0)\n    concat = concat.stack(['store_nbr', 'family'])\n    concat = concat.reset_index()\n    concat = concat.set_index(['store_nbr', 'family', 'date']).sort_index()\n    # Create 1 - 7 'lag' features\n    y_lag_1 = lets_lag(df=concat, group=['store_nbr', 'family'], column=\"sales\", name=\"y_lag_1\", lag=1)\n    y_lag_2 = lets_lag(df=concat, group=['store_nbr', 'family'], column=\"sales\", name=\"y_lag_2\", lag=2)\n    y_lag_3 = lets_lag(df=concat, group=['store_nbr', 'family'], column=\"sales\", name=\"y_lag_3\", lag=3)\n    y_lag_4 = lets_lag(df=concat, group=['store_nbr', 'family'], column=\"sales\", name=\"y_lag_4\", lag=4)\n    y_lag_5 = lets_lag(df=concat, group=['store_nbr', 'family'], column=\"sales\", name=\"y_lag_5\", lag=5)\n    y_lag_6 = lets_lag(df=concat, group=['store_nbr', 'family'], column=\"sales\", name=\"y_lag_6\", lag=6)\n    y_lag_7 = lets_lag(df=concat, group=['store_nbr', 'family'], column=\"sales\", name=\"y_lag_7\", lag=7)\n    # Merge tables\n    concat = concat.merge(y_lag_1, left_index=True, right_index=True) \\\n        .merge(y_lag_2, left_index=True, right_index=True) \\\n        .merge(y_lag_3, left_index=True, right_index=True) \\\n        .merge(y_lag_4, left_index=True, right_index=True) \\\n        .merge(y_lag_5, left_index=True, right_index=True) \\\n        .merge(y_lag_6, left_index=True, right_index=True) \\\n        .merge(y_lag_7, left_index=True, right_index=True)\n    # Create rolling features\n    rolling_mean_3 = lets_roll(df=concat, group=['store_nbr', 'family'], column=['y_lag_1'], name=\"rolling_mean_3\",\n                               window=3)\n    rolling_mean_7 = lets_roll(df=concat, group=['store_nbr', 'family'], column=['y_lag_1'], name=\"rolling_mean_6\",\n                               window=7)\n    rolling_mean_14 = lets_roll(df=concat, group=['store_nbr', 'family'], column=['y_lag_1'], name=\"rolling_mean_12\",\n                                window=14, min_periods=13)\n    ewm_01 = lets_ewm(df=concat, group=['store_nbr', 'family'], column=['y_lag_1'], alpha=0.1, name='ewm_01')\n    ewm_03 = lets_ewm(df=concat, group=['store_nbr', 'family'], column=['y_lag_1'], alpha=0.3, name='ewm_03')\n    ewm_09 = lets_ewm(df=concat, group=['store_nbr', 'family'], column=['y_lag_1'], alpha=0.9, name='ewm_09')\n    # Merge tables\n    concat = concat.merge(rolling_mean_3, left_index=True, right_index=True) \\\n        .merge(rolling_mean_7, left_index=True, right_index=True) \\\n        .merge(rolling_mean_14, left_index=True, right_index=True) \\\n        .merge(ewm_01, left_index=True, right_index=True) \\\n        .merge(ewm_03, left_index=True, right_index=True) \\\n        .merge(ewm_09, left_index=True, right_index=True)\n    # Create features from index\n    concat['day_of_week'] = concat.index.get_level_values(2).dayofweek\n    concat['day_of_month'] = concat.index.get_level_values(2).day\n    concat['month'] = concat.index.get_level_values(2).month\n    concat['year'] = concat.index.get_level_values(2).year\n    concat['fam'] = concat.index.get_level_values(1)\n    concat['fam'], _ = concat['fam'].factorize()\n    concat['store'] = concat.index.get_level_values(0)\n    # Create features by merging with another data\n    concat = concat.reset_index()\n    # With 'stores'\n    concat['store'] = concat['store'].astype('int64')\n    concat = concat.merge(stores[['store_nbr', 'type', 'cluster']],\n                          how='left', left_on='store', right_on='store_nbr')\n    concat = concat.drop(columns={'store_nbr_y'})\n    concat = concat.rename(columns={'store_nbr_x': 'store_nbr'})\n    concat['store_52'] = np.where(concat.store == 52, 1, 0)\n    # With 'onpromotion'\n    concat = concat.merge(onpromotion, on=['store_nbr', 'family', 'date'])\n    # Uncomment when you need to merge with X_holidays\n    # concat = concat.merge(X_holidays, how='left', on='date')\n    # concat.iloc[:, -13:] = concat.iloc[:, -13:].fillna(0)\n    # Uncomment when you need to merge with 'holidays'\n    # concat = concat.fillna(0)\n    # concat = concat.merge(holidays, how='left', on='date')\n    # fillna = concat.loc[:, 'is_holiday'].fillna(0).values\n    # concat.loc[:, 'is_holiday'] = fillna\n    # concat['is_holiday'] = concat.is_holiday.astype('int32')\n    # concat = concat.drop(columns={'description'})\n    # Uncomment when you need to merge with oil prices\n    # concat = concat.merge(oil, how='left', on='date')\n    # concat.loc[:, 'dcoilwtico'] = concat.loc[:, 'dcoilwtico'].fillna(method='ffill').fillna(method='bfill')\n    # Fit-predict split\n    before = concat[concat.date <= basic_set.index.max()]\n    after = concat[concat.date == (basic_set.index.max() + 1)]\n    # Create target\n    target = before[['store_nbr', 'family', 'date', 'sales']]\n    target = target[target.date >= '2017-01-14']\n    target = target.set_index([\"store_nbr\", \"family\", \"date\"])\n    before = before.drop(columns={'sales'})\n    after = after.drop(columns={'sales'})\n    # Create fit-table\n    before = before.set_index(['store_nbr', 'family', 'date'])\n    before = before.dropna()\n    # Create predict-table\n    after = after.set_index(['store_nbr', 'family', 'date'])\n    after = after.dropna()\n    return before, after, target\n\n\n# Make prediction for the next day\ndef make_one_day_forecast(booster, features, before, after, target, clip):\n    \"\"\"Make prediction for the next day. Use in recursive loop.\"\"\"\n    model = booster\n    model.fit(before[features], target)\n    # model_fit = pd.DataFrame(model.predict(before[features]), index=before.index, columns=['sales']).clip(0.0)\n    model_pred = pd.DataFrame(model.predict(after[features]), index=after.index, columns=['sales'])\n    if clip:\n        model_pred = pd.DataFrame(model.predict(after[features]), index=after.index, columns=['sales']).clip(0.0)\n    one_day_pred = model_pred.unstack(['store_nbr', 'family'])\n    # cut = one_day_pred.index.max() - 20\n    return one_day_pred\n\n\n# Mutual information scores\ndef make_mi_scores(X, y, discrete_features='auto'):\n    \"\"\"Returns mutual cross-entropy score for given train-set features and the target.\"\"\"\n    mi_scores = mutual_info_regression(X, y, discrete_features=discrete_features)\n    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n    mi_scores = mi_scores.sort_values(ascending=False)\n    return mi_scores\n\n\n# Plot mi scores\ndef plot_mi_scores(scores):\n    \"\"\"Plot mi scores\"\"\"\n    scores = scores.sort_values(ascending=True)\n    width = np.arange(len(scores))\n    ticks = list(scores.index)\n    plt.barh(width, scores)\n    plt.yticks(width, ticks)\n    plt.title(\"Mutual Information Scores\")\n\n\n# Corrplot\ndef corrplot(df, method=\"pearson\", annot=True, **kwargs):\n    \"\"\"Plot correlation matrix-heatmap\"\"\"\n    sns.clustermap(\n        df.corr(method),\n        vmin=-1.0,\n        vmax=1.0,\n        cmap=\"icefire\",\n        method=\"complete\",\n        annot=annot,\n        **kwargs,\n    )","metadata":{"execution":{"iopub.status.busy":"2022-05-16T06:47:42.324211Z","iopub.execute_input":"2022-05-16T06:47:42.324474Z","iopub.status.idle":"2022-05-16T06:47:42.399208Z","shell.execute_reply.started":"2022-05-16T06:47:42.324443Z","shell.execute_reply":"2022-05-16T06:47:42.397776Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We also need to prepare our data (i.e. set indexes, transform to the wide format etc.).","metadata":{}},{"cell_type":"code","source":"# Prepare data\nstore_sales = train.set_index(['store_nbr', 'family', 'date']).sort_index()\nstore_sales_wide = store_sales[['sales', 'onpromotion']].unstack(['store_nbr', 'family'])  # wide format\n\nholidays = holidays_events.query(\"locale in ['National', 'Regional']\").loc['2017-01-01':'2017-08-31', ['description']] \\\n    .assign(description=lambda x: x.description.cat.remove_unused_categories())\nX_holidays = pd.get_dummies(holidays)\nholidays['is_holiday'] = 1\n\nstore_sales_test = test.set_index(['store_nbr', 'family', 'date']).sort_index()\nstore_sales_test_wide = store_sales_test[['onpromotion']].unstack(['store_nbr', 'family'])  # wide format\n\nonpromotion = pd.concat([store_sales_wide['onpromotion'], store_sales_test_wide['onpromotion']])\nonpromotion = onpromotion.stack(['store_nbr', 'family'])  # wide format\nonpromotion = onpromotion.to_frame(name='onpromotion')","metadata":{"execution":{"iopub.status.busy":"2022-05-16T06:47:48.279593Z","iopub.execute_input":"2022-05-16T06:47:48.27994Z","iopub.status.idle":"2022-05-16T06:47:52.215365Z","shell.execute_reply.started":"2022-05-16T06:47:48.279903Z","shell.execute_reply":"2022-05-16T06:47:52.214328Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we are ready, so it is a good idea to do some short EDA to check (or just recollect) how our data look like in general.\n\nOne can easily manipulate these plots by changing parameters of the function below. Shops are numbered from '1' to '54', and there are 33 families ordered alphabetically.","metadata":{}},{"cell_type":"code","source":"# EDA: Share-plots of data\nshare_plot_eda(ts=store_sales_wide['sales'], START='2017-01-01', END='2017-08-15',\n               shop='17', start_col=5, num_families=4, show_trend=False, show_model_1=False, show_improvement=False)","metadata":{"execution":{"iopub.status.busy":"2022-05-16T06:47:55.432388Z","iopub.execute_input":"2022-05-16T06:47:55.432691Z","iopub.status.idle":"2022-05-16T06:47:57.020537Z","shell.execute_reply.started":"2022-05-16T06:47:55.432659Z","shell.execute_reply":"2022-05-16T06:47:57.019225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's look at the trends. It seems that simple linear function will be quite a good choice here.","metadata":{}},{"cell_type":"code","source":"# Estimate trend\nstore_sales_trend = store_sales_wide[['sales']].rolling(window=90, center=True, min_periods=45).mean()\n\n# Plot trend\nshare_plot_eda(ts=store_sales_wide['sales'], START='2017-01-01', END='2017-08-15',\n               shop='17', start_col=5, num_families=4, show_trend=True, trend_frame=store_sales_trend,\n               show_model_1=False, show_improvement=False)","metadata":{"execution":{"iopub.status.busy":"2022-05-16T06:48:22.803643Z","iopub.execute_input":"2022-05-16T06:48:22.804032Z","iopub.status.idle":"2022-05-16T06:48:24.578717Z","shell.execute_reply.started":"2022-05-16T06:48:22.803995Z","shell.execute_reply":"2022-05-16T06:48:24.577211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's compare total sales with 'holidays' data. This comparison will show that we can use 'holidays' data as an informative feature in our model.","metadata":{}},{"cell_type":"code","source":"# EDA: Compare holidays with total sales\ntotal = transactions.groupby('date').sum().loc['2017'].reset_index()\ntotal['date'] = total.date.dt.to_period(freq=\"D\")\ntotal = total.set_index('date')\nholidays_restricted = holidays.loc['2017-01-01':'2017-08-15']\n\nax = total.plot(**plot_params)\nplt.plot_date(holidays_restricted.index, total.loc[holidays_restricted.index], color='C3')\nax.set_title('National and Regional Holidays')\nplt.show()\nplt.clf()","metadata":{"execution":{"iopub.status.busy":"2022-05-16T06:48:29.271694Z","iopub.execute_input":"2022-05-16T06:48:29.272824Z","iopub.status.idle":"2022-05-16T06:48:29.944208Z","shell.execute_reply.started":"2022-05-16T06:48:29.272762Z","shell.execute_reply":"2022-05-16T06:48:29.943296Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1) Set parameters, create features based on datetime index and fit simple linear model \nEverything we did above was just a preparation. Let's begin then by setting special dates and splitting our basic dataset into the 'train' and 'validation' parts.","metadata":{}},{"cell_type":"code","source":"# Special dates\ntrain_start = train[train.date == '2017-01-01'].date.min()\ntrain_end = train[train.date == '2017-07-30'].date.min()\nvalid_start = train_end + 1\nvalid_end = valid_start + 15\ntest_start = valid_end + 1\ntest_end = test_start + 15\nprint(\"Train start:\")\nprint(train_start)\nprint(\"Train end:\")\nprint(train_end)\nprint(\"Validation start:\")\nprint(valid_start)\nprint(\"Validation end:\")\nprint(valid_end)\nprint(\"Test start:\")\nprint(test_start)\nprint(\"Test end:\")\nprint(test_end)","metadata":{"execution":{"iopub.status.busy":"2022-05-16T08:13:56.459994Z","iopub.execute_input":"2022-05-16T08:13:56.460348Z","iopub.status.idle":"2022-05-16T08:13:56.503173Z","shell.execute_reply.started":"2022-05-16T08:13:56.460314Z","shell.execute_reply":"2022-05-16T08:13:56.501853Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Basic sets: train & valid\nstore_sales_train = store_sales_wide.loc[train_start: train_end][['sales']]\nstore_sales_valid = store_sales_wide.loc[valid_start: valid_end][['sales']]\nstore_sales_train_and_valid = store_sales_wide.loc[train_start: valid_end][['sales']]","metadata":{"execution":{"iopub.status.busy":"2022-05-16T08:14:03.221726Z","iopub.execute_input":"2022-05-16T08:14:03.222123Z","iopub.status.idle":"2022-05-16T08:14:03.235756Z","shell.execute_reply.started":"2022-05-16T08:14:03.222088Z","shell.execute_reply":"2022-05-16T08:14:03.234551Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we will use the function `create_index_features` to create features based on datetime index. It may be beneficial to check how this function works (it was defined in the second cell of this notebook), one can also explore its output in the console. Then we will define the model (using Ridge estimator - it seems to be better than Linear Regression), predict both on train and validation data and show the results.","metadata":{}},{"cell_type":"code","source":"# Create train & valid features\nX_train, X_valid = create_index_features(store_sales_train, steps=16, order=1, holidays=True, oil_prices=True,\n                                         seasonal=True)\n\n# Define the model and fit it\nmodel_1 = Ridge(alpha=0.1, fit_intercept=False, random_state=1842)\n# model_1 = LinearRegression(fit_intercept=False)\nmodel_1.fit(X_train, store_sales_train)\n\n# Predict on train data\nmodel_1_fit = pd.DataFrame(model_1.predict(X_train), index=X_train.index, columns=store_sales_train.columns).clip(0.0)\n\n# Predict on valid data\nmodel_1_predict = pd.DataFrame(model_1.predict(X_valid), index=X_valid.index, columns=store_sales_train.columns) \\\n    .clip(0.0)","metadata":{"execution":{"iopub.status.busy":"2022-05-16T06:48:52.307211Z","iopub.execute_input":"2022-05-16T06:48:52.307525Z","iopub.status.idle":"2022-05-16T06:48:52.479275Z","shell.execute_reply.started":"2022-05-16T06:48:52.307492Z","shell.execute_reply":"2022-05-16T06:48:52.477493Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot the result of the model\nshare_plot_eda(ts=store_sales_wide['sales'], START='2017-05-01', END='2017-08-15',\n               shop='17', start_col=5, num_families=4, show_trend=False,\n               show_model_1=True, model_1_fit=model_1_fit, model_1_predict=model_1_predict, show_improvement=False)","metadata":{"execution":{"iopub.status.busy":"2022-05-16T06:48:58.578666Z","iopub.execute_input":"2022-05-16T06:48:58.578987Z","iopub.status.idle":"2022-05-16T06:48:59.925277Z","shell.execute_reply.started":"2022-05-16T06:48:58.578956Z","shell.execute_reply":"2022-05-16T06:48:59.923914Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Predictions look nice, but we also need to check them in general. Besides RMSLE (competition's metric), we use also two other simple metrics RMSE and MAE.","metadata":{}},{"cell_type":"code","source":"# Check error\nerror_check(y_train_=store_sales_train, y_fit_=model_1_fit, y_valid_=store_sales_valid, y_pred_=model_1_predict)","metadata":{"execution":{"iopub.status.busy":"2022-05-16T06:49:22.390869Z","iopub.execute_input":"2022-05-16T06:49:22.391194Z","iopub.status.idle":"2022-05-16T06:49:22.640158Z","shell.execute_reply.started":"2022-05-16T06:49:22.391159Z","shell.execute_reply":"2022-05-16T06:49:22.638921Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"If we retrain the model that was defined above on all the data from 2017 year, i.e. on `store_sales_train_and_valid`, then predict on the test set and submit the result, we will get a test score around 0.51. It seems quite good, but it is (nearly) the same as the score one can achieve in the third excercise (\"Seasonality\") in Time Series Tutorial. So our aim is to improve this outcome.\n\nThis is the place when we start to get into some troubles. I tried almost everything I learn in the Tutorial about hybrid models and the ways they could improve the result. I started from DirRec and Direct Strategies: scores were generally bad or even horrible; the best I was able to achieve was not to disturb the score of the first, Ridge model. It seemed to me, after some time, that the only reasonable strategy was DirRec,  but in a slightly different fashion than it is explained in the last lesson of the Time Series Tutorial. What difference I mean? 'RegressorChain' estimator uses forecasts based on previous step as new lag features, but it also uses every feature we had already used (in previous steps). Let's say we have 3 lag features: 'lag1', 'lag2' and 'lag3' at the begining. After step 1 we have, in fact, four lag features: old 'lag1', 'lag2', 'lag3' and the forecast, i.e. new 'lag1' (old 'lag1' becomes actually 'lag2' etc., in particular, we have now also 'lag4' feature!). When we don't use such 'remaining' features, our model seems to perform better in general. I tried to write some code; there were many trials. Finally I found the way to improve the result and I will present it below.\n\nIn this version of my efforts I resigned from using of HybridBooster class defined in the fifth lesson of the Tutorial. Idea of such a class was one of the most important things I learn from this amazing course and I used such class when I made my experiments with DirRec and Direct Strategies and also in other tries. However, the way I eventually found seems to be clearer when we follow it step by step, so without HybridBooster; at least it seems so for me.","metadata":{}},{"cell_type":"markdown","source":"# 2) Residuals and feature selection\nAt the beginning we need to compute the residuals of the first model. Then we will check:\n\n(1) general statistics of the residuals (or rather of their absolute values);\n\n(2) in which time series they are the greatest;\n\n(3) how do they look like.","metadata":{}},{"cell_type":"code","source":"# Compute residuals of the first model\nresiduals = store_sales_train - model_1_fit\n\n# Long format:\nresiduals_long = residuals.stack(['store_nbr', 'family'])\nresiduals_long = residuals_long.rename(columns={'sales': 'residuals'})\n\n# Greatest residuals\nprint(\"General description of residuals:\")\nprint(np.abs(residuals_long).describe())\n\nresiduals_grouped = np.abs(residuals_long).groupby(['store_nbr', 'family']).mean().\\\n    sort_values(ascending=False, by='residuals')\nprint()\nprint(\"Greatest residuals on average:\")\nprint(residuals_grouped.head())","metadata":{"execution":{"iopub.status.busy":"2022-05-16T06:49:56.790898Z","iopub.execute_input":"2022-05-16T06:49:56.791854Z","iopub.status.idle":"2022-05-16T06:49:56.980802Z","shell.execute_reply.started":"2022-05-16T06:49:56.791763Z","shell.execute_reply":"2022-05-16T06:49:56.979291Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot residuals\nshare_plot_residuals(ts=residuals['sales'], START='2017-01-01', END='2017-08-31', shop='17',start_col=5, num_families=4)","metadata":{"execution":{"iopub.status.busy":"2022-05-16T06:50:02.165614Z","iopub.execute_input":"2022-05-16T06:50:02.165998Z","iopub.status.idle":"2022-05-16T06:50:03.572445Z","shell.execute_reply.started":"2022-05-16T06:50:02.165963Z","shell.execute_reply":"2022-05-16T06:50:03.571296Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For EDA purposes we will use the function `create_next_day_table` which produces:\n\n(1) train set `before`;\n\n(2) column with the values of residuals: `target` (and also (3) validation set `after` - we will use it later).\n\nWe will check how does the table with the train features look like. You can see that there are many features in it that we can use:\n\n(1) lagged values;\n\n(2) rolling means and exponentially weighted means of (1-step-lagged) time series;\n\n(3) features derived from datetime index;\n\n(4) another features like (encoded) family, store, onpromotion etc.","metadata":{}},{"cell_type":"code","source":"# Create tables - just for EDA purposes\nbefore, after, target = create_next_day_table(basic_set=residuals[['sales']])","metadata":{"execution":{"iopub.status.busy":"2022-05-16T06:50:22.613883Z","iopub.execute_input":"2022-05-16T06:50:22.615309Z","iopub.status.idle":"2022-05-16T06:51:02.629985Z","shell.execute_reply.started":"2022-05-16T06:50:22.615258Z","shell.execute_reply":"2022-05-16T06:51:02.629015Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# How does the table look like?\nbefore.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-16T06:51:37.508323Z","iopub.execute_input":"2022-05-16T06:51:37.508658Z","iopub.status.idle":"2022-05-16T06:51:37.541736Z","shell.execute_reply.started":"2022-05-16T06:51:37.508626Z","shell.execute_reply":"2022-05-16T06:51:37.541125Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As usual, one of the most difficult thing is to choose the best features for our model. We will check:\n\n(1) mutual information score (you can check it on the greater fractions `frac`, but the results will be similar);\n\n(2) correlations between features and target;\n\n(3) you can also check the relation between the selected feature (e.g. 'y_lag_1') and the target, i.e. residuals.\n\nThese things may be helpful to get some ideas about the features, at least for the good start.","metadata":{}},{"cell_type":"code","source":"# Mutual info\nsample_train = before.sample(frac=0.05, random_state=1842)\nsample_target = target.sample(frac=0.05, random_state=1842)\nsample_concat = pd.concat([sample_train, sample_target], axis=1)\n\nmi_scores = make_mi_scores(sample_train.select_dtypes(['uint64', 'int32', 'int64', 'float32', 'float64']),\n                           np.ravel(sample_target))","metadata":{"execution":{"iopub.status.busy":"2022-05-16T06:52:40.126911Z","iopub.execute_input":"2022-05-16T06:52:40.127232Z","iopub.status.idle":"2022-05-16T06:52:42.748266Z","shell.execute_reply.started":"2022-05-16T06:52:40.127201Z","shell.execute_reply":"2022-05-16T06:52:42.747162Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot mi scores\nplot_mi_scores(mi_scores)\n\n# Correlations between features and target\ncorrplot(sample_concat.select_dtypes(['float32', 'float64', 'uint64']), annot=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-16T06:52:51.19651Z","iopub.execute_input":"2022-05-16T06:52:51.197207Z","iopub.status.idle":"2022-05-16T06:52:53.900154Z","shell.execute_reply.started":"2022-05-16T06:52:51.197165Z","shell.execute_reply":"2022-05-16T06:52:53.898956Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Relations of features and target\nsns.relplot(x=\"y_lag_1\", y=\"sales\", data=sample_concat)","metadata":{"execution":{"iopub.status.busy":"2022-05-16T06:52:58.266717Z","iopub.execute_input":"2022-05-16T06:52:58.268067Z","iopub.status.idle":"2022-05-16T06:52:58.923036Z","shell.execute_reply.started":"2022-05-16T06:52:58.267962Z","shell.execute_reply":"2022-05-16T06:52:58.921913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After endless trials I achieve the best score with the features you can see below, even though local validation was sometimes slightly better for another set of them. It is not surprising however - local validation is a bit arbitrary. Better validation strategies may be a furhter way to improve this model. I tried e.g. TimeSeriesSplit & GridSearch, but without satisfactory results.\n\nI also tried another feature ideas, for example I used the lags of the original time series (i.e. sales values) instead of the lags of the residuals. It worked worse. Rolling features were very promissing, but turned out to be rather dissapointing here. I also try to move some features used by the first model, e.g. days of week or holidays, to the second model, but it didn't help either.","metadata":{}},{"cell_type":"code","source":"# Select features\n# 'y_lag_1', 'y_lag_2', 'y_lag_3', 'y_lag_4', 'y_lag_5', 'y_lag_6', 'y_lag_7'\n# 'rolling_mean_3', 'rolling_mean_6', 'rolling_mean_12', 'ewm_01', 'ewm_03', 'ewm_09'\n# 'day_of_week', 'day_of_month', 'month', 'year'\n# 'fam', 'store', 'store_52', 'type', 'cluster', onpromotion\n\nfeatures = ['y_lag_1', 'y_lag_2', 'y_lag_3', 'y_lag_4', 'y_lag_5', 'y_lag_6', 'y_lag_7', 'store', 'store_52']","metadata":{"execution":{"iopub.status.busy":"2022-05-16T06:56:54.499909Z","iopub.execute_input":"2022-05-16T06:56:54.500241Z","iopub.status.idle":"2022-05-16T06:56:54.50708Z","shell.execute_reply.started":"2022-05-16T06:56:54.500207Z","shell.execute_reply":"2022-05-16T06:56:54.505822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3) Improve the result by the hybrid model: DirRec Strategy\nWe need to define basic set and the number of steps we want to predtict. Later, i.e. when we predict on the test data, we will need to choose 16 steps, but here, for validation purposes, we can take less steps. The main reason to do it is that every step takes some time (around 40 seconds, so you need 2 minutes to achieve step 3).\n\nDirRec loop goes as follows:\n\n(1) we start by establishing `basic_set` of residuals (train set);\n\n(2) we make train features `before`, `target` and validation features `after` using `create_next_day_table` function that we have already seen in action;\n\n(3) we make a forecast for the next day using `make_one_day_forecast` function;\n\n(4) at last, one-day-forecast `one_day_pred`, concatenated with `basic_set`, becomes a new basic set.","metadata":{}},{"cell_type":"code","source":"# Define basic set\nbasic_set = residuals[['sales']]\n\n# Choose the number of steps\nn = 3","metadata":{"execution":{"iopub.status.busy":"2022-05-16T06:56:59.78681Z","iopub.execute_input":"2022-05-16T06:56:59.787487Z","iopub.status.idle":"2022-05-16T06:56:59.797116Z","shell.execute_reply.started":"2022-05-16T06:56:59.787445Z","shell.execute_reply":"2022-05-16T06:56:59.796362Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# DirRec strategy\nfor _ in range(n):\n    t = time.time()\n    before, after, target = create_next_day_table(basic_set=basic_set)\n    one_day_pred = make_one_day_forecast(booster=LGBMRegressor(random_state=1842, n_estimators=100), features=features,\n                                         before=before, after=after, target=target, clip=False)\n    new_basic_set = pd.concat([basic_set, one_day_pred])\n    basic_set = new_basic_set\n    print(\"Step:\")\n    print(_)\n    print(\"Time:\")\n    print(time.time() - t)","metadata":{"execution":{"iopub.status.busy":"2022-05-16T06:57:05.813275Z","iopub.execute_input":"2022-05-16T06:57:05.813688Z","iopub.status.idle":"2022-05-16T06:59:09.383557Z","shell.execute_reply.started":"2022-05-16T06:57:05.813656Z","shell.execute_reply":"2022-05-16T06:59:09.382573Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"When the loop is over, we just get predtictions from the newest `basic_set` and add them to the corresponding part of the first model's prediction.","metadata":{}},{"cell_type":"code","source":"# Prediction of model_2, i.e. predicted residuals:\nresiduals_predict = basic_set.loc[valid_start:]\ncut = residuals_predict.index.max()\n\n# Add prediction of models 1 & 2\nhybrid_predict = model_1_predict.loc[:cut] + residuals_predict\nhybrid_predict = hybrid_predict.clip(0.0)","metadata":{"execution":{"iopub.status.busy":"2022-05-16T07:00:28.310282Z","iopub.execute_input":"2022-05-16T07:00:28.310571Z","iopub.status.idle":"2022-05-16T07:00:28.331937Z","shell.execute_reply.started":"2022-05-16T07:00:28.310541Z","shell.execute_reply":"2022-05-16T07:00:28.330507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we can check the error of the hybrid model. Only 'Validation' metrics are different than earlier (i.e. when we check Ridge model), because we didn't predcit on train set with recursive strategy. Anyway we can see some improvement. We will check it also on charts. The improvemnt is obvious especially in a problematic shop '52'.","metadata":{}},{"cell_type":"code","source":"# Check error\nerror_check(y_train_=store_sales_train, y_fit_=model_1_fit, y_valid_=store_sales_valid.loc[:cut], y_pred_=hybrid_predict)","metadata":{"execution":{"iopub.status.busy":"2022-05-16T07:00:33.815347Z","iopub.execute_input":"2022-05-16T07:00:33.815678Z","iopub.status.idle":"2022-05-16T07:00:34.072101Z","shell.execute_reply.started":"2022-05-16T07:00:33.815644Z","shell.execute_reply":"2022-05-16T07:00:34.071257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot the results of the model\nshare_plot_eda(ts=store_sales_wide['sales'], START='2017-04-01', END='2017-08-31',\n               shop='52', start_col=5, num_families=4, show_trend=False, show_model_1=True, show_improvement=True,\n               model_1_fit=model_1_fit, model_1_predict=model_1_predict, hybrid_predict=hybrid_predict)","metadata":{"execution":{"iopub.status.busy":"2022-05-16T07:00:38.796487Z","iopub.execute_input":"2022-05-16T07:00:38.796938Z","iopub.status.idle":"2022-05-16T07:00:40.377996Z","shell.execute_reply.started":"2022-05-16T07:00:38.796885Z","shell.execute_reply":"2022-05-16T07:00:40.377072Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4) Retrain the model and submit the forecast\nNow we need to retrain the model on the train set concateneted with the validation set. It seems to be a generally good idea to use the most 'up-to-date' data available (our validation set is of this kind); here, it means when we use recursive strategy, it is necessary and crucial thing. Retraining is very simple: we just use `store_sales_train_and_valid` instead of `store_sales_train`.","metadata":{}},{"cell_type":"code","source":"# Retrain model on all data available: create train & test features\nX_train, X_test = create_index_features(store_sales_train_and_valid, steps=16, order=1, holidays=True,\n                                        oil_prices=True, seasonal=True)\n\n# Define the model and fit it\nmodel_1 = Ridge(alpha=0.1, fit_intercept=False, random_state=1842)\nmodel_1.fit(X_train, store_sales_train_and_valid)\n\n# Predict on train data\nmodel_1_fit = pd.DataFrame(model_1.predict(X_train), index=X_train.index,\n                           columns=store_sales_train_and_valid.columns).clip(0.0)\n\n# Predict on valid data\nmodel_1_forecast = pd.DataFrame(model_1.predict(X_test), index=X_test.index,\n                                columns=store_sales_train_and_valid.columns).clip(0.0)","metadata":{"execution":{"iopub.status.busy":"2022-05-16T07:10:44.177976Z","iopub.execute_input":"2022-05-16T07:10:44.178349Z","iopub.status.idle":"2022-05-16T07:10:44.284558Z","shell.execute_reply.started":"2022-05-16T07:10:44.17831Z","shell.execute_reply":"2022-05-16T07:10:44.283469Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot the result of the model\nshare_plot_eda(ts=store_sales_wide['sales'], START='2017-04-01', END='2017-08-31',\n               shop='52', start_col=5, num_families=4, show_trend=False,\n               show_model_1=True, model_1_fit=model_1_fit, model_1_predict=model_1_forecast, show_improvement=False)","metadata":{"execution":{"iopub.status.busy":"2022-05-16T07:33:30.41967Z","iopub.execute_input":"2022-05-16T07:33:30.420023Z","iopub.status.idle":"2022-05-16T07:33:31.876505Z","shell.execute_reply.started":"2022-05-16T07:33:30.419989Z","shell.execute_reply":"2022-05-16T07:33:31.875005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"If you would like to submit the result of this simple model in the competition, you need to use `prepare_submission` function with argument forecast=model_1_forecast. We will use this function later, after hybrid improvement.\n\nNow we are going to compute the residuals and make use of DirRec Loop again; this time we need to choose 16 steps, so it will take a while (around 10 minutes).","metadata":{}},{"cell_type":"code","source":"# Compute residuals of the first model\nresiduals = store_sales_train_and_valid - model_1_fit\n\n# Define basic set\nbasic_set = residuals[['sales']]\n\n# Choose the number of steps\nn = 16","metadata":{"execution":{"iopub.status.busy":"2022-05-16T07:19:03.638462Z","iopub.execute_input":"2022-05-16T07:19:03.63893Z","iopub.status.idle":"2022-05-16T07:19:03.656359Z","shell.execute_reply.started":"2022-05-16T07:19:03.63889Z","shell.execute_reply":"2022-05-16T07:19:03.655323Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# DirRec prediction\nfor _ in range(n):\n    t = time.time()\n    before, after, target = create_next_day_table(basic_set=basic_set)\n    one_day_pred = make_one_day_forecast(booster=LGBMRegressor(random_state=1842, n_estimators=100), features=features,\n                                         before=before, after=after, target=target, clip=False)\n    new_basic_set = pd.concat([basic_set, one_day_pred])\n    basic_set = new_basic_set\n    print(\"Step:\")\n    print(_)\n    print(\"Time:\")\n    print(time.time() - t)","metadata":{"execution":{"iopub.status.busy":"2022-05-16T07:19:44.627874Z","iopub.execute_input":"2022-05-16T07:19:44.629138Z","iopub.status.idle":"2022-05-16T07:31:40.696197Z","shell.execute_reply.started":"2022-05-16T07:19:44.629085Z","shell.execute_reply":"2022-05-16T07:31:40.695476Z"},"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Prediction of model_2, i.e. predicted residuals:\nresiduals_forecast = basic_set.loc[test_start:]\n\n# Add prediction of models 1 & 2\nhybrid_forecast = model_1_forecast + residuals_forecast\nhybrid_forecast = hybrid_forecast.clip(0.0)","metadata":{"execution":{"iopub.status.busy":"2022-05-16T07:32:07.610791Z","iopub.execute_input":"2022-05-16T07:32:07.611441Z","iopub.status.idle":"2022-05-16T07:32:07.628848Z","shell.execute_reply.started":"2022-05-16T07:32:07.611398Z","shell.execute_reply":"2022-05-16T07:32:07.627338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot the results of the model\nshare_plot_eda(ts=store_sales_wide['sales'], START='2017-04-01', END='2017-08-31',\n               shop='52', start_col=27, num_families=4, show_trend=False, show_model_1=True, show_improvement=True,\n               model_1_fit=model_1_fit, model_1_predict=model_1_forecast, hybrid_predict=hybrid_forecast)","metadata":{"execution":{"iopub.status.busy":"2022-05-16T07:35:56.78897Z","iopub.execute_input":"2022-05-16T07:35:56.789943Z","iopub.status.idle":"2022-05-16T07:35:58.285399Z","shell.execute_reply.started":"2022-05-16T07:35:56.789894Z","shell.execute_reply":"2022-05-16T07:35:58.284262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see, some improvements seem to make sense.\n\nEventually, we can prepare our submission file and test our forecast. If you don't know how to do it, you can read a step-by-step instruction in the end of the excersise 3 of the \"Time series tutorial\". ","metadata":{}},{"cell_type":"code","source":"# Prepare submission file\nprepare_submission(forecast=hybrid_forecast,path='hybrid_forecast.csv')","metadata":{"execution":{"iopub.status.busy":"2022-05-16T07:53:11.488013Z","iopub.execute_input":"2022-05-16T07:53:11.488326Z","iopub.status.idle":"2022-05-16T07:53:11.60645Z","shell.execute_reply.started":"2022-05-16T07:53:11.488294Z","shell.execute_reply":"2022-05-16T07:53:11.605459Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5) Next steps\nI hope I will find some way to improve the hybrid model performance; however I decided to share my work because at this point I don't see a clear way how to do it. Maybe someone will just tell me, it would be much appreciated.\n\nNonetheless, it seems to me that it will be necessary to build not only one (hybrid), but many models to get significantly better results. I am going to try a few ideas: 'one model per store', 'one per family' or maybe even 'one per time series'. It will make things longer and more complicated, so I hope it is not the only way.","metadata":{}}]}