{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# An Extensive Analysis of Store Sales\n\nWhat was done in this lengthy notebook are:\n\n* Basics Statistical Information: Numbers in Data, Sales analytics by City and State\n\n* Seasonality Analytics and Trends. Sales by Month, Quater, Sales by Day in Week and Month.\n\n* Basics Transaction analytics and correlation with Sales.\n\n* Autocorrelation of Sales.\n\n* Deseasonalized Trend which shows cyclic/irregular variations.\n\n* ACF and PACF Plots to find significant lags as well as dissimilar families of products.\n\n* Earthquake impact and analytics.\n\n* Holidays and events influence.\n\n---\nReferences: I would like to thank the authors of these notebooks: I tried to expand with more useful information for modeling the Time series.\n\nhttps://www.kaggle.com/code/kashishrastogi/store-sales-analysis-time-serie\n\nhttps://www.kaggle.com/code/ekrembayar/store-sales-ts-forecasting-a-comprehensive-guide/notebook","metadata":{}},{"cell_type":"markdown","source":"# Importing Packages","metadata":{}},{"cell_type":"code","source":"import warnings\nwarnings.simplefilter(action='ignore')\n\nimport numpy as np\nimport pandas as pd\nfrom datetime import datetime, date\n\n# Plotting import\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Patch\nfrom matplotlib.ticker import MaxNLocator\n\nimport seaborn as sns\nfrom IPython.display import Markdown, display\ndef printmd(string):\n    display(Markdown(string))","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-04-05T22:11:04.078422Z","iopub.execute_input":"2022-04-05T22:11:04.078792Z","iopub.status.idle":"2022-04-05T22:11:04.085374Z","shell.execute_reply.started":"2022-04-05T22:11:04.078735Z","shell.execute_reply":"2022-04-05T22:11:04.084739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Functions defined and modified\n# From Lesson 3\ndef seasonal_plot(X, y, period, freq, ax=None, atext=\"\"):\n    if ax is None:\n        _, ax = plt.subplots()\n    palette = sns.color_palette(\n        \"husl\",\n        n_colors=X[period].nunique(),\n    )\n    ax = sns.lineplot(\n        x=freq,\n        y=y,\n        hue=period,\n        data=X,\n        ci=False,\n        ax=ax,\n        palette=palette,\n        legend=False,\n    )\n    if atext: atext = atext + \" \";\n    ax.set_title(atext + f\"Seasonal Plot ({period}/{freq})\")\n    for line, name in zip(ax.lines, X[period].unique()):\n        y_ = line.get_ydata()[-1]\n        ax.annotate(\n            name,\n            xy=(1, y_),\n            xytext=(6, 0),\n            color=line.get_color(),\n            xycoords=ax.get_yaxis_transform(),\n            textcoords=\"offset points\",\n            size=14,\n            va=\"center\",\n        )\n    return ax\n\n\ndef plot_periodogram(ts, detrend='linear', ax=None, atext=\"\"):\n    from scipy.signal import periodogram\n    fs = 365.25\n    freqencies, spectrum = periodogram(\n        ts,\n        fs=fs,\n        detrend=detrend,\n        window=\"boxcar\",\n        scaling='spectrum',\n    )\n    if ax is None:\n        _, ax = plt.subplots()\n    ax.step(freqencies, spectrum, color=\"purple\")\n    ax.set_xscale(\"log\")\n    ax.set_xticks([1, 2, 4, 6, 12, 26, 52, 104])\n    ax.set_xticklabels(\n        [\n            \"Annual (1)\",\n            \"Semiannual (2)\",\n            \"Quarterly (4)\",\n            \"Bimonthly (6)\",\n            \"Monthly (12)\",\n            \"Biweekly (26)\",\n            \"Weekly (52)\",\n            \"Semiweekly (104)\",\n        ],\n        rotation=30,\n    )\n    if atext: atext = atext + \" \";\n    ax.ticklabel_format(axis=\"y\", style=\"sci\", scilimits=(0, 0))\n    ax.set_ylabel(\"Variance\")\n    ax.set_title(atext + \"Periodogram\")\n    return ax\n# From Lesson 4\ndef lagplot(x, y=None, shift=1, standardize=False, ax=None, atext=\"\", **kwargs):\n    from matplotlib.offsetbox import AnchoredText\n    x_ = x.shift(shift)\n    if standardize:\n        x_ = (x_ - x_.mean()) / x_.std()\n    if y is not None:\n        y_ = (y - y.mean()) / y.std() if standardize else y\n    else:\n        y_ = x\n    corr = y_.corr(x_)\n    if ax is None:\n        fig, ax = plt.subplots()\n    scatter_kws = dict(\n        alpha=0.75,\n        s=3,\n    )\n    line_kws = dict(color='C3', )\n    ax = sns.regplot(x=x_,\n                     y=y_,\n                     scatter_kws=scatter_kws,\n                     line_kws=line_kws,\n                     lowess=True,\n                     ax=ax,\n                     **kwargs)\n    at = AnchoredText(\n        f\"{corr:.2f}\",\n        prop=dict(size=\"large\"),\n        frameon=True,\n        loc=\"upper left\",\n    )\n    at.patch.set_boxstyle(\"square, pad=0.0\")\n    ax.add_artist(at)\n    title = f\"Lag {shift}\" if shift > 0 else f\"Lead {-shift}\"\n    if atext: title = atext + \": \" + title\n    ax.set(title=title, xlabel=x_.name, ylabel=y_.name)\n    return ax\n\n\ndef plot_lags(x,\n              y=None,\n              lags=6,\n              leads=None,\n              nrows=1,\n              lagplot_kwargs={},\n              **kwargs):\n    import math\n    kwargs.setdefault('nrows', nrows)\n    orig = leads is not None\n    leads = leads or 0\n    kwargs.setdefault('ncols', math.ceil((lags + orig + leads) / nrows))\n    kwargs.setdefault('figsize', (kwargs['ncols'] * 2, nrows * 2 + 0.5))\n    fig, axs = plt.subplots(sharex=True, sharey=True, squeeze=False, **kwargs)\n    for ax, k in zip(fig.get_axes(), range(kwargs['nrows'] * kwargs['ncols'])):\n        k -= leads + orig\n        if k + 1 <= lags:\n            ax = lagplot(x, y, shift=k + 1, ax=ax, **lagplot_kwargs)\n            title = f\"Lag {k + 1}\" if k + 1 >= 0 else f\"Lead {-k - 1}\"\n            ax.set_title(title, fontdict=dict(fontsize=14))\n            ax.set(xlabel=\"\", ylabel=\"\")\n        else:\n            ax.axis('off')\n    plt.setp(axs[-1, :], xlabel=x.name)\n    plt.setp(axs[:, 0], ylabel=y.name if y is not None else x.name)\n    fig.tight_layout(w_pad=0.1, h_pad=0.1)\n    return fig","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-04-05T22:11:04.099109Z","iopub.execute_input":"2022-04-05T22:11:04.099422Z","iopub.status.idle":"2022-04-05T22:11:04.132556Z","shell.execute_reply.started":"2022-04-05T22:11:04.099387Z","shell.execute_reply":"2022-04-05T22:11:04.131766Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Importing Data","metadata":{}},{"cell_type":"code","source":"from pathlib import Path\ncomp_dir = Path('../input/store-sales-time-series-forecasting')\n\ntrain_data = pd.read_csv(comp_dir / \"train.csv\", index_col=\"id\", header=0, parse_dates=['date'])\nstores_data = pd.read_csv(comp_dir / \"stores.csv\", index_col=\"store_nbr\", header=0)\ntransactions_data = pd.read_csv(comp_dir / \"transactions.csv\", index_col=None, header=0, parse_dates=['date'])","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-04-05T22:11:04.13427Z","iopub.execute_input":"2022-04-05T22:11:04.134718Z","iopub.status.idle":"2022-04-05T22:11:07.00116Z","shell.execute_reply.started":"2022-04-05T22:11:04.134683Z","shell.execute_reply":"2022-04-05T22:11:07.000211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Basics Statistical Information\n\nWe will look at simple aspects of sales to see which store, city, and state have the most sale on average.","metadata":{}},{"cell_type":"code","source":"total_records = train_data.shape[0]\nfirst_date    = train_data.date.iloc[0]\nlast_date     = train_data.date.iloc[-1]\ntotal_days    = (train_data.date.iloc[-1] - train_data.date.iloc[0]).days\nstore_nbr_id  = stores_data.index.values # stores_data.store_nbr.unique()\nfamily_unique = train_data.family.unique()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-04-05T22:11:07.003473Z","iopub.execute_input":"2022-04-05T22:11:07.003725Z","iopub.status.idle":"2022-04-05T22:11:07.232597Z","shell.execute_reply.started":"2022-04-05T22:11:07.003697Z","shell.execute_reply":"2022-04-05T22:11:07.231745Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"printmd(\"### Number of Records\")\nprint( \"{} from {} to {}\".format(total_records,first_date.to_period(\"D\"), last_date.to_period(\"D\") ) )\nprint( \"(Total of {} days or {} months)\".format(total_days,total_days//30 ) ) \nprintmd(\"### Number of Stores\")\nprint( \"{} stores\".format(len(store_nbr_id) ) )\nprintmd(\"### Number of Product Family\")\nprint( \"{} types\".format(len(family_unique) ) )\nprintmd(\"### Number of Cities and States\")\nprint( \"{} cities in {} states\". format(len(stores_data.city.unique()), len(stores_data.state.unique()) )  )\n# print( [stores_data.loc[store]['city'] for store in stores_data.index] )","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-04-05T22:11:07.233922Z","iopub.execute_input":"2022-04-05T22:11:07.234134Z","iopub.status.idle":"2022-04-05T22:11:07.256604Z","shell.execute_reply.started":"2022-04-05T22:11:07.234108Z","shell.execute_reply":"2022-04-05T22:11:07.255616Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Distribution of Store by City and State","metadata":{}},{"cell_type":"code","source":"top_city  = stores_data.groupby('city') .size().sort_values(ascending=False)\ntop_state = stores_data.groupby('state').size().sort_values(ascending=False)\nstates = top_state.index.values\n\nstores_data_grouped = stores_data.groupby(['state','city']).agg({'city':'count'})\ncolor = [\"tab:red\",\"tab:orange\",\"tab:brown\",\"tab:cyan\",\"tab:green\",\"tab:purple\",\"tab:blue\",\"tab:gray\",\"tab:pink\", \"navy\",\"darkred\"]\nfig, ax = plt.subplots( figsize=(16,5))\nax.yaxis.set_major_locator(MaxNLocator(integer=True))\nax.set_title(\"Distribution of Store by City and State\")\nax.set_ylabel(\"Number of stores\")\nax.set_xlabel(\"City\")\ncustom_label=[]\ncustom_legend = []\nfor i in range(0,10):\n    u = stores_data_grouped.loc[ states[i] ]\n    ax.bar(u.index.values, u.values.flatten(),color=color[i])\n    custom_label.append ( Patch(facecolor=color[i])  )\n    custom_legend.append( states[i] + \" (Total {})\".format(top_state[i]))\nax.legend(custom_label, custom_legend, fontsize=\"large\", labelcolor=\"black\", \n          fancybox=True, title = \"States\", title_fontsize = \"x-large\")\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-04-05T22:11:07.259265Z","iopub.execute_input":"2022-04-05T22:11:07.259502Z","iopub.status.idle":"2022-04-05T22:11:07.658309Z","shell.execute_reply.started":"2022-04-05T22:11:07.259474Z","shell.execute_reply":"2022-04-05T22:11:07.657458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Conclusion: Distribution of Store by City and State\n\nOnly 5 states have more than 2 stores. From 10th states, there is only one store per state.\n\n<hr style=\"height:2px;border-width:0;color:black;background-color:black\">\n\n## Daily Average Sales by All Store\n\nNow we will see how the effectiveness of these stores and their locations. Below is the calculation of sales taking average daily for the whole period (of 1600 days)","metadata":{}},{"cell_type":"code","source":"sales_grouped = train_data.groupby(['store_nbr','date']).agg({'sales':'sum'})\nsales_by_store_id = {}\nfor store in store_nbr_id:\n    sales_by_store_id[store] = {'sale': sales_grouped.loc[store].sales.mean() ,\n                                'city': stores_data.city[store] ,\n                                'city-id': stores_data.city[store]+\"-{}\".format(store) ,\n                                'state': stores_data.state[store] ,\n                                'type': stores_data.type[store] ,\n                                'cluster': stores_data.cluster[store] }\nsales_by_store_id = pd.DataFrame.from_dict(sales_by_store_id, orient='index') # convert to Pandas DataFrame \n\nsales_by_city      = {}\nfor city in top_city.index.values:\n    sales_by_city[city]      = {'sum': sales_by_store_id [ stores_data.city==city ].sale.sum(),\n                                'mean': sales_by_store_id [ stores_data.city==city ].sale.mean() }\nsales_by_city      = pd.DataFrame.from_dict(sales_by_city, orient='index')    ","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-04-05T22:11:07.659992Z","iopub.execute_input":"2022-04-05T22:11:07.660313Z","iopub.status.idle":"2022-04-05T22:11:07.915525Z","shell.execute_reply.started":"2022-04-05T22:11:07.660266Z","shell.execute_reply":"2022-04-05T22:11:07.914784Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Sorting \nsales_by_store_id.sort_values( by='sale', inplace=True, ascending=False )\nsales_by_city_sum  = sales_by_city.sort_values( by='sum', ascending=False )","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-04-05T22:11:07.916744Z","iopub.execute_input":"2022-04-05T22:11:07.916972Z","iopub.status.idle":"2022-04-05T22:11:07.924935Z","shell.execute_reply.started":"2022-04-05T22:11:07.916937Z","shell.execute_reply":"2022-04-05T22:11:07.924012Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Ploting\nfigsize = (12,5)\nfig, ax0 = plt.subplots(figsize=figsize)\nsales_by_store_id.plot(kind=\"bar\",x='city-id',y='sale',figsize=figsize, ax=ax0,color=color, align='center', legend=False)\nplt.suptitle(\"The Average Sales Daily\",  fontsize=16, ha='center')\nax0.set_ylabel('Average Sales by All Store',  fontsize=16);","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-04-05T22:11:07.926138Z","iopub.execute_input":"2022-04-05T22:11:07.92639Z","iopub.status.idle":"2022-04-05T22:11:08.974385Z","shell.execute_reply.started":"2022-04-05T22:11:07.926358Z","shell.execute_reply":"2022-04-05T22:11:08.973743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Conclusion: Daily Average Sales by All Store\n\nThere are 54 stores at different locations (including different cities and states). However, the averages of sales per day are not similar. Most effective stores are located in Quito city. There are also about 10 stores running badly. An available suggestion is to shut down these stores or look for effective solutions to improve the business.\n\n<hr style=\"height:2px;border-width:0;color:black;background-color:black\">\n\n## Average Sales by City","metadata":{}},{"cell_type":"code","source":"color1 = \"tab:red\"\ncolor2 = \"tab:blue\"\nfig, ax1 = plt.subplots(figsize=figsize)\nsales_by_city_sum.plot(kind=\"bar\",y='sum', figsize=figsize,ax=ax1,color=color1, align='edge', width=-0.3, legend=False)\nax2 = ax1.twinx()\nsales_by_city_sum.plot(kind=\"bar\",y='mean',figsize=figsize,ax=ax2,color=color2, align='edge', width=0.3,  legend=False)\n############# Style Set up ###############\nax1.set_ylabel('Average Total Sales by City', color=color1, fontsize=16)\nax1.tick_params(axis='y', labelcolor=color1)\nax2.set_ylabel('Average Sales by Store by City', color=color2, fontsize=16)  \nax2.tick_params(axis='y', labelcolor=color2)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-04-05T22:11:08.975581Z","iopub.execute_input":"2022-04-05T22:11:08.976058Z","iopub.status.idle":"2022-04-05T22:11:09.512268Z","shell.execute_reply.started":"2022-04-05T22:11:08.976011Z","shell.execute_reply":"2022-04-05T22:11:09.51136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Conclusion: Average Sales by City\n\n1. When looking at the graph by City, it's clear that Quito has the biggest daily-average sales both per city and per store. \n\n2. The second runer is Cayambe city where there is only 1 store. An available suggestion is to open more stores in this city.\n\n<hr style=\"height:2px;border-width:0;color:black;background-color:black\">\n\n## Correlations among stores","metadata":{}},{"cell_type":"code","source":"##### Find Correlations among stores ########\na = train_data[[\"store_nbr\", \"sales\"]]\n# Create another column to index sales for each store by day by family\na[\"ind\"] = 1\na[\"ind\"] = a.groupby(\"store_nbr\").ind.cumsum().values\na = pd.pivot(a, index = \"ind\", columns = \"store_nbr\", values = \"sales\").corr()\nmask = np.triu(a) # Create upper triangle to hide in heatmap","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-04-05T22:11:09.513907Z","iopub.execute_input":"2022-04-05T22:11:09.514401Z","iopub.status.idle":"2022-04-05T22:11:10.999617Z","shell.execute_reply.started":"2022-04-05T22:11:09.514353Z","shell.execute_reply":"2022-04-05T22:11:10.998733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot a heat map\nplt.figure(figsize=(15, 15))\nsns.heatmap(a, annot=True, fmt='.1f', cmap='bwr', square=True, mask=mask, linewidths=1, cbar=False)\nplt.title(\"Correlations among stores\",fontsize = 16)\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-04-05T22:11:11.001024Z","iopub.execute_input":"2022-04-05T22:11:11.001344Z","iopub.status.idle":"2022-04-05T22:11:16.994692Z","shell.execute_reply.started":"2022-04-05T22:11:11.001302Z","shell.execute_reply":"2022-04-05T22:11:16.993642Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"irreg_stores = [20, 21, 22, 25, 26, 29, 32, 35, 42, 52]\nss_sorted = sales_by_store_id.reset_index()\nprintmd(\"**Low correlation stores:**\")\nfor store in irreg_stores:\n    print(\"Store #{} in ranking {}/54\".format(store, ss_sorted.index[ ss_sorted[\"index\"]==store].tolist()[0] + 1 ) )","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-04-05T22:11:16.99581Z","iopub.execute_input":"2022-04-05T22:11:16.996031Z","iopub.status.idle":"2022-04-05T22:11:17.012569Z","shell.execute_reply.started":"2022-04-05T22:11:16.996003Z","shell.execute_reply":"2022-04-05T22:11:17.011719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Conclusion: Correlations among stores\n\nMost stores are highly correlations, meaning they are very similar in business and sales. However, there are few stores which are different from the others. By looking at the store by ranking, these stores are at the bottom in sales ranking. The business perhaps does not run well there.\n\n<hr style=\"height:2px;border-width:0;color:black;background-color:black\">\n\n## Average Sales by Month & Quarter","metadata":{}},{"cell_type":"code","source":"sales_grouped  = train_data.groupby('date').agg({'sales':'sum'}).to_period(\"D\")\nsales_grouped['year']      = sales_grouped.index.year\nsales_grouped['quarter']   = sales_grouped.index.quarter\nsales_grouped['month']     = sales_grouped.index.month\nsales_grouped['week']      = sales_grouped.index.week\nsales_grouped['dayofweek'] = sales_grouped.index.dayofweek  # Monday=0, Sunday=6\nsales_grouped['dayofmonth']= sales_grouped.index.day  # day in month from 01 to 31\nsales_grouped['dayofyear'] = sales_grouped.index.dayofyear\n\nsales_smooth7  = sales_grouped.copy()\nsales_smooth30 = sales_grouped.copy()\nsales_smooth365= sales_grouped.copy()\n\nsales_smooth7[\"sales\"]   = sales_smooth7.  sales.rolling(window=7,  center=True, min_periods=3 ).mean()\nsales_smooth30[\"sales\"]  = sales_smooth30. sales.rolling(window=30, center=True, min_periods=15).mean()\nsales_smooth365[\"sales\"] = sales_smooth365.sales.rolling(window=365,center=True, min_periods=183).mean()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-04-05T22:11:17.014118Z","iopub.execute_input":"2022-04-05T22:11:17.015189Z","iopub.status.idle":"2022-04-05T22:11:17.092921Z","shell.execute_reply.started":"2022-04-05T22:11:17.015152Z","shell.execute_reply":"2022-04-05T22:11:17.091753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"figsize = (14,5)\nfig, (ax1,ax2) = plt.subplots(1,2,figsize=figsize)\nsales_grouped.groupby(['month']).agg({'sales':'mean'}).plot(kind=\"barh\",ax=ax1)\nax1.set(title=\"Average Sales by Month\")\nax1.set(ylabel=\"Month\", xlabel=\"Average Sales\")\nax1.get_legend().remove()\nlabels1 = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']\nax1.set_yticks(range(0,12), labels1)\n\nsales_grouped.groupby(['quarter']).agg({'sales':'mean'}).plot.pie(y=\"sales\",ax=ax2, legend=False, autopct='%1.f%%',\n                             startangle=90, labels=[\"Quarter 1\",\"Quarter 2\",\"Quarter 3\",\"Quarter 4\"], fontsize=\"x-large\")\nax2.set(title=\"Average Sales by Quarter\")\n\n\n\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-04-05T22:11:17.098341Z","iopub.execute_input":"2022-04-05T22:11:17.0992Z","iopub.status.idle":"2022-04-05T22:11:17.440536Z","shell.execute_reply.started":"2022-04-05T22:11:17.099145Z","shell.execute_reply":"2022-04-05T22:11:17.439597Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Conclusion\n\n<table style=\"\"><tr><td width=\"50%\"  style=\"text-align:left;font-size: medium;vertical-align:top;background-color:#ffffff\">\n\n<h4>Average Sales by Month</h4>\n\nMost of the sales occurred in December. People tend to stock for the end of the year to prepare for a new year or to decorate the house, or schedule for a long holiday starting from Christmas.\n</td><td   style=\"text-align:left;font-size:medium;background-color:#ffffff;vertical-align:top;\">\n    \n<h4>Average Sales by Quarter</h4>\n\nAlthough the sales in each quarter are very similar, the last (4th) quarter is the biggist, which agrees to the sales by month where there is a jump in December.\n</td></tr></table>\n\n<hr style=\"height:2px;border-width:0;color:black;background-color:black;\">\n\n## Average Sales by Day in Week and Month","metadata":{}},{"cell_type":"code","source":"figsize = (15,5)\nfig, (ax1,ax2) = plt.subplots(1,2,figsize=figsize)\n\nsales_grouped.groupby(['dayofweek']).agg({'sales':'mean'}).plot.barh(ax=ax1)\nax1.set(title=\"Average Sales in Week\")\nax1.set(ylabel=\"Day of Week\", xlabel=\"Average Sales\")\nax1.get_legend().remove()\nlabels1 = [\"Monday\",\"Tuesday\",\"Wednesday\",\"Thursday\", \"Friday\",\"Saturday\",\"Sunday\"]\nax1.set_yticks(range(0,7), labels1)\n\nsales_grouped.groupby(['dayofmonth']).agg({'sales':'mean'}).plot.bar(ax=ax2,  fontsize=\"large\")\nax2.set(title=\"Average Sales in Month\")\nax2.set(ylabel=\"Average Sales\", xlabel=\"Day of Month\")\nax2.get_legend().remove()\n\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-04-05T22:11:17.441624Z","iopub.execute_input":"2022-04-05T22:11:17.441848Z","iopub.status.idle":"2022-04-05T22:11:18.020805Z","shell.execute_reply.started":"2022-04-05T22:11:17.441822Z","shell.execute_reply":"2022-04-05T22:11:18.019887Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Conclusion\n\n<table style=\"\"><tr><td width=\"50%\"  style=\"text-align:left;font-size: medium;vertical-align:top;background-color:#ffffff\">\n\n<h4>Average Sales by Day in Week</h4>\n\nObviously, people spent more money on weekends than weekdays.\n\n</td><td   style=\"text-align:left;font-size:medium;background-color:#ffffff;vertical-align:top;\">\n    \n<h4>Average Sales by Day in Month</h4>\n\nDespite people spending more money on weekends, the pattern in a month sticks out with a peak in the middle and high sales at the beginning and the end of a month. It is perhaps due to the dates people get paid in the public sector.\n</td></tr></table>\n\n<hr style=\"height:2px;border-width:0;color:black;background-color:black;\">\n\n## Top Product Families in 2017","metadata":{}},{"cell_type":"code","source":"top_family = train_data.loc[train_data.date.dt.year==2017].groupby(['family']).agg({'sales':'sum'}).sort_values(\n                by=\"sales\", ascending=True)\nfigsize = (13,13)\nfig, ax1 = plt.subplots(figsize=figsize)\ntop_family.plot(kind=\"barh\",ax=ax1)\nax1.set(title=\"Top Product Families in 2017 (in Log scale)\")\nax1.set(ylabel=\"Product Family\", xlabel=\"Average Sales (in Log scale)\", xscale=\"log\")\nax1.get_legend().remove()\n\nfor item in ([ax1.xaxis.label, ax1.yaxis.label]+ax1.get_xticklabels() + ax1.get_yticklabels()):\n    item.set_fontsize(\"large\")\nax1.title.set_fontsize(\"xx-large\")    \nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-04-05T22:11:18.022015Z","iopub.execute_input":"2022-04-05T22:11:18.022266Z","iopub.status.idle":"2022-04-05T22:11:19.076238Z","shell.execute_reply.started":"2022-04-05T22:11:18.022237Z","shell.execute_reply":"2022-04-05T22:11:19.075358Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Conclusion: Top Product Families in 2017\n\nNote that the plot in the Log scale shows all the sales by all product families because some of them had very small sales in comparison to other types.\n\nHighest sales are made by the product families like grocery and beverages.\n\n<hr style=\"height:2px;border-width:0;color:black;background-color:black\">\n\n## Top Sales by Type and Cluster in 2017","metadata":{}},{"cell_type":"code","source":"top_stores_2017 = train_data.loc[train_data.date.dt.year==2017].groupby(\"store_nbr\").agg({\"sales\":\"sum\"})\ntop_stores_2017 = pd.merge(top_stores_2017, stores_data, on=\"store_nbr\").drop([\"city\",\"state\"],axis=1)\n\nfigsize = (15,5)\nfig, (ax1,ax2) = plt.subplots(1,2,figsize=figsize)\ntop_stores_2017.groupby(['type']).agg({'sales':'mean'}).plot.pie(y=\"sales\",ax=ax1, legend=False, autopct='%1.f%%',\n                             startangle=90, labels=[\"Type A\",\"Type B\",\"Type C\",\"Type D\",\"Type E\"], fontsize=\"x-large\")\nax1.set(title=\"Average Sales by Type\")\n\ntop_stores_2017.groupby(['cluster']).agg({'sales':'mean'}).plot.bar(ax=ax2,  fontsize=\"large\")\nax2.set(title=\"Average Sales by Cluster\")\nax2.set(ylabel=\"Average Sales\", xlabel=\"Cluster\")\nax2.get_legend().remove()\n\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-04-05T22:11:19.077835Z","iopub.execute_input":"2022-04-05T22:11:19.078826Z","iopub.status.idle":"2022-04-05T22:11:19.742299Z","shell.execute_reply.started":"2022-04-05T22:11:19.078764Z","shell.execute_reply":"2022-04-05T22:11:19.741212Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Conclusion: Top Sales by Type and Cluster in 2017\n\n* Store Type A has the highest sales which is 37%, followed by Types B and D.\n\n* Store Cluster 5 has the highest sales, followed by Clusters 11, 14, and 8.\n\n<hr style=\"height:2px;border-width:0;color:black;background-color:black\">","metadata":{}},{"cell_type":"markdown","source":"# Visualization of Seasonality and Trend\n\nWe would like to know how the sales behave at different times to understand their seasonality and the overall trend. Seasonalities include each season, monthly, quarterly.","metadata":{}},{"cell_type":"code","source":"figsize = (14,4)\nfig, ax = plt.subplots(figsize=figsize)\nsales_grouped. plot(ax=ax, alpha=0.3)\nsales_smooth7. plot(ax=ax)\nsales_smooth365.plot(ax=ax, color=\"r\")\nax.legend([\"Daily Sales\",\"7-day Moving Average\",\"365-day Moving Average\"],bbox_to_anchor=(1.0, 1.0))\nax.set(ylim=2e5, title=\"Sales per Day for all Stores\")\nplt.show()","metadata":{"scrolled":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-04-05T22:11:19.743873Z","iopub.execute_input":"2022-04-05T22:11:19.744216Z","iopub.status.idle":"2022-04-05T22:11:20.72849Z","shell.execute_reply.started":"2022-04-05T22:11:19.744169Z","shell.execute_reply":"2022-04-05T22:11:20.727571Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Conclusion: Sales per Day for all Stores\n\nBased on the above graph, we could see the sales were peaking at every new-year time.\n\n<hr style=\"height:2px;border-width:0;color:black;background-color:black\">","metadata":{}},{"cell_type":"code","source":"figsize = (16,5)\nfig, (ax1,ax2) = plt.subplots(1,2,figsize=figsize)\nsales_grouped. plot(ax=ax1)\nsales_smooth7. plot(ax=ax1, color=\"r\")\nax1.legend([\"Daily sales\",\"7-Day Moving Average Sales\"])\nax1.set(ylim=[2e5, 8e5], title=\"2013 Sales per Day for all Stores\")\nax1.set(xlim=[sales_grouped.index[0],sales_grouped.index[365]] )\n##############\nsales_grouped. plot(ax=ax2)\nsales_smooth7. plot(ax=ax2, color=\"r\")\nax2.legend([\"Daily Sales\",\"7-Day Moving Average Sales\"])\nax2.set(ylim=4e5, title=\"2016-2017 Sales per Day for all Stores\")\nax2.set(xlim=[sales_grouped.index[-365],sales_grouped.index[-1]] )\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-04-05T22:11:20.729651Z","iopub.execute_input":"2022-04-05T22:11:20.729903Z","iopub.status.idle":"2022-04-05T22:11:21.964427Z","shell.execute_reply.started":"2022-04-05T22:11:20.729873Z","shell.execute_reply":"2022-04-05T22:11:21.96348Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Conclusion: Sales in period of 365 days\n\nBased on zoomed-in graphs, we could see 12 peaks of sales for 365 days (equivalently 12 months or a year). Hence, there is a monthly frequency. Moreover, the sales increased dramatically during the transition times from the preview year to the new year which presented an annual pattern. (The sales were smoothed with 7-day MA to eliminate the weekly pattern)\n\n<hr style=\"height:2px;border-width:0;color:black;background-color:black\">","metadata":{}},{"cell_type":"code","source":"figsize = (16,4)\nfig, (ax1,ax2,ax3) = plt.subplots(1,3,figsize=figsize)\nseasonal_plot(sales_grouped.loc[sales_grouped['year']==2013], y='sales', period='week', freq='dayofweek',ax=ax1, atext=\"2013 \")\nseasonal_plot(sales_grouped.loc[sales_grouped['year']==2016], y='sales', period='week', freq='dayofweek',ax=ax2, atext=\"2016 \")\nseasonal_plot(sales_grouped.loc[sales_grouped['year']==2017], y='sales', period='week', freq='dayofweek',ax=ax3, atext=\"2017 \");","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-04-05T22:11:21.965948Z","iopub.execute_input":"2022-04-05T22:11:21.966204Z","iopub.status.idle":"2022-04-05T22:11:24.571233Z","shell.execute_reply.started":"2022-04-05T22:11:21.966172Z","shell.execute_reply":"2022-04-05T22:11:24.570345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Conclusion: Seasonality: Weekly\n\nThere was a pattern for weekly sales in which weekends had more sales than weekdays. It makes sense because people are off on weekends and have available time to spend money.\n\n<hr style=\"height:2px;border-width:0;color:black;background-color:black\">\n\nNext, we will see how about a monthly pattern. Before that, we have to smooth the data to minimize the weekly patterns.","metadata":{}},{"cell_type":"code","source":"sales_smooth              = sales_smooth7.copy()\nfigsize = (16,6)\nfig, (ax1,ax2,ax3) = plt.subplots(1,3,figsize=figsize)\nseasonal_plot(sales_smooth.loc[ (sales_smooth['year']==2013) & (sales_smooth['month']!=12 )\n                              ], y='sales', period='month', freq='dayofmonth',ax=ax1, atext=\"2013\")\nseasonal_plot(sales_smooth.loc[ (sales_smooth['year']==2015) & (sales_smooth['month']!=12 )\n                              ], y='sales', period='month', freq='dayofmonth',ax=ax2, atext=\"2015\")\nseasonal_plot(sales_smooth.loc[sales_smooth['year']==2016], y='sales', period='month', freq='dayofmonth',ax=ax3, atext=\"2016\");","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-04-05T22:11:24.572623Z","iopub.execute_input":"2022-04-05T22:11:24.573532Z","iopub.status.idle":"2022-04-05T22:11:25.896292Z","shell.execute_reply.started":"2022-04-05T22:11:24.573478Z","shell.execute_reply":"2022-04-05T22:11:25.89521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Conclusion: Seasonality: Monthly\n\nThe behavior of the last month of the year is shown in the last figure (on the right). By eliminating this month, we can see a strong monthly pattern for sales. \n\n1. There were 2 troughs. The sales went down to trough around days 10 and 25 \n2. There were 2 peaks. The sales went up to the biggest peak at the end of the month. There was a minor peak at around days 15 to 20.\n\n<hr style=\"height:2px;border-width:0;color:black;background-color:black\">","metadata":{}},{"cell_type":"code","source":"sales_smooth              = sales_smooth30.copy()\nfigsize = (13,6)\nfig, ax = plt.subplots(figsize=figsize)\nseasonal_plot(sales_smooth, y=\"sales\", period=\"year\", freq=\"dayofyear\", ax=ax, atext=\"Annual Sales\");\nax.set(ylim=3e5);","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-04-05T22:11:25.89804Z","iopub.execute_input":"2022-04-05T22:11:25.898358Z","iopub.status.idle":"2022-04-05T22:11:26.324097Z","shell.execute_reply.started":"2022-04-05T22:11:25.898309Z","shell.execute_reply":"2022-04-05T22:11:26.32341Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Conclusion: Seasonality: Annually\n\nAfter smoothing the data in a period of 30 days to eliminate the impact of monthly and weekly seasonalities, we can conclude that the sales were rising by the end of each year and slowly went down after the new year period. During 2014, the price of crude oil was in trouble, hence, could influence the business and caused abnormal peaks of sales.\n\n<hr style=\"height:2px;border-width:0;color:black;background-color:black\">","metadata":{}},{"cell_type":"code","source":"# train_data.groupby('date').sum().squeeze().loc['2013'].sales\nsales_2013 = sales_grouped.loc[ sales_grouped.year == 2013]\nsales_2016 = sales_grouped.loc[ sales_grouped.year == 2016]\nsales_2017 = sales_grouped.loc[ sales_grouped.year == 2017]\nfigsize = (16,4)\nfig, (ax1,ax2,ax3) = plt.subplots(1,3,figsize=figsize)\nplot_periodogram(sales_2013.sales, ax=ax1, atext=\"2013\")\nplot_periodogram(sales_2016.sales, ax=ax2, atext=\"2016\")\nplot_periodogram(sales_2017.sales, ax=ax3, atext=\"2017\");","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-04-05T22:11:26.325429Z","iopub.execute_input":"2022-04-05T22:11:26.326191Z","iopub.status.idle":"2022-04-05T22:11:27.442631Z","shell.execute_reply.started":"2022-04-05T22:11:26.326151Z","shell.execute_reply":"2022-04-05T22:11:27.441599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Conclusion: Periodograms\n\nBased on these periodograms:\n1. There are strong patterns of weekly frequency. \n2. The semi-weekly patterns were also found in the previous analysis.\n3. There were weak patterns of monthly and bimonthly frequencies.\n4. There was a slight impact on the annual frequency\n\n<hr style=\"height:2px;border-width:0;color:black;background-color:black\">","metadata":{}},{"cell_type":"markdown","source":"# Transaction Analysis\n\nThis feature is highly correlated with sales. More transactions often mean more sales.","metadata":{}},{"cell_type":"code","source":"top_stores_by_sale = sales_by_store_id.index[0:10].values\nprint(\"Top stores are: {}\".format(top_stores_by_sale))","metadata":{"scrolled":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-04-05T22:11:27.444567Z","iopub.execute_input":"2022-04-05T22:11:27.445371Z","iopub.status.idle":"2022-04-05T22:11:27.451805Z","shell.execute_reply.started":"2022-04-05T22:11:27.445325Z","shell.execute_reply":"2022-04-05T22:11:27.450602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transaction_top_stores = transactions_data.drop( transactions_data[ ~transactions_data.store_nbr.isin( \n            top_stores_by_sale) ].index )\ntop_stores_by_sale_r = pd.Series( range(len(top_stores_by_sale)), index=top_stores_by_sale ).to_dict()\ntransaction_top_stores['top'] = transactions_data.store_nbr.map(top_stores_by_sale_r )\ntransaction_top_stores.sort_values([\"top\", \"date\"], inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-04-05T22:11:27.453126Z","iopub.execute_input":"2022-04-05T22:11:27.453427Z","iopub.status.idle":"2022-04-05T22:11:27.502213Z","shell.execute_reply.started":"2022-04-05T22:11:27.453377Z","shell.execute_reply":"2022-04-05T22:11:27.501256Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"figsize = (13,6)\nfig, ax = plt.subplots(figsize=figsize)\nsns.lineplot( data=transaction_top_stores , x='date', y='transactions', hue='store_nbr', ax=ax, palette=sns.color_palette(\n                                    \"husl\", n_colors=len(top_stores_by_sale))  )\nax.set(title=\"Transactions of top stores from 2013 to 2017\");","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-04-05T22:11:27.503462Z","iopub.execute_input":"2022-04-05T22:11:27.504389Z","iopub.status.idle":"2022-04-05T22:11:28.904567Z","shell.execute_reply.started":"2022-04-05T22:11:27.504328Z","shell.execute_reply":"2022-04-05T22:11:28.903816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transaction_top_stores['year']      = transaction_top_stores.date.dt.year\ntransaction_top_stores['month']     = transaction_top_stores.date.dt.month\ntransaction_top_stores['dayofmonth']= transaction_top_stores.date.dt.day\ntransaction_top_stores['dayofweek'] = transaction_top_stores.date.dt.dayofweek\n\nfigsize = (13,6)\nfig, ax = plt.subplots(figsize=figsize)\nsns.lineplot( data=transaction_top_stores [ (transaction_top_stores['year']  == 2016)] # &(transaction_top_stores['month']==5) \n             , x='date', y='transactions', hue='store_nbr', ax=ax, palette=sns.color_palette(\n                                    \"husl\", n_colors=len(top_stores_by_sale))  )\nax.set(title=\"Transactions of top stores in year 2016\");","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-04-05T22:11:28.906126Z","iopub.execute_input":"2022-04-05T22:11:28.906625Z","iopub.status.idle":"2022-04-05T22:11:29.677987Z","shell.execute_reply.started":"2022-04-05T22:11:28.906582Z","shell.execute_reply":"2022-04-05T22:11:29.676877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trans_2016 = transaction_top_stores.loc[ transaction_top_stores.year == 2016].groupby(['dayofweek','store_nbr']).agg(\n                                        {'transactions':'sum'}).reset_index()\nfigsize = (16,4)\nfig, (ax1,ax2) = plt.subplots(1,2,figsize=figsize)\nseasonal_plot(sales_grouped.loc[sales_grouped['year']==2016],y='sales',period='week',freq='dayofweek',ax=ax1,atext=\"2016 Sales\")\nsns.lineplot(data= trans_2016,ax=ax2, x=\"dayofweek\", y=\"transactions\", hue=\"store_nbr\", palette=sns.color_palette(\n                                    \"husl\", n_colors=len(top_stores_by_sale)) )\nax1.set(ylim=5e5)\nax2.set(title=\"2016 Transactions Seasonal Plot\");","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-04-05T22:11:29.679363Z","iopub.execute_input":"2022-04-05T22:11:29.679636Z","iopub.status.idle":"2022-04-05T22:11:31.129566Z","shell.execute_reply.started":"2022-04-05T22:11:29.679602Z","shell.execute_reply":"2022-04-05T22:11:31.128491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Notice\n\nWe can see the similarities between Sales and Transactions. Let's calculate their correlation coefficient.","metadata":{}},{"cell_type":"markdown","source":"## Spearman Correlation","metadata":{}},{"cell_type":"code","source":"temp = pd.merge(train_data.groupby([\"date\", \"store_nbr\"]).sales.sum().reset_index(), transactions_data, how = \"left\")\nprint(\"Spearman Correlation between Total Sales and Transactions: {:,.2f}\".format(\n                                            temp.corr(\"spearman\").sales.loc[\"transactions\"]))","metadata":{"scrolled":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-04-05T22:11:31.131083Z","iopub.execute_input":"2022-04-05T22:11:31.131358Z","iopub.status.idle":"2022-04-05T22:11:31.411547Z","shell.execute_reply.started":"2022-04-05T22:11:31.131324Z","shell.execute_reply":"2022-04-05T22:11:31.410468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.regplot(data=temp.loc[temp.date.dt.year ==2016], x = \"transactions\", y = \"sales\",marker=\".\",line_kws={'color':'r'} );","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-04-05T22:11:31.412934Z","iopub.execute_input":"2022-04-05T22:11:31.413183Z","iopub.status.idle":"2022-04-05T22:11:32.834978Z","shell.execute_reply.started":"2022-04-05T22:11:31.413153Z","shell.execute_reply":"2022-04-05T22:11:32.834308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A high value of Spearman Correlation shows that there is a strong correlation between Sales and Transactions.\n\n<hr style=\"height:2px;border-width:0;color:black;background-color:black\">\n\n# Autocorrelation","metadata":{}},{"cell_type":"code","source":"# We want to remove outlier to make graph better understanding\nfrom scipy import stats\ndef outlierFilter(df, columns=\"\"):\n    # calculate z-scores of df and remove row where zscore is high\n    if columns:\n        return df[(np.abs(stats.zscore(df[columns])) < 3)]\n    else:\n        return df[(np.abs(stats.zscore(df)) < 3).all(axis=1)]","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-04-05T22:11:32.836286Z","iopub.execute_input":"2022-04-05T22:11:32.837018Z","iopub.status.idle":"2022-04-05T22:11:32.843607Z","shell.execute_reply.started":"2022-04-05T22:11:32.836978Z","shell.execute_reply":"2022-04-05T22:11:32.842589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Lags in different years","metadata":{}},{"cell_type":"code","source":"acs = {\"2013\":sales_2013.copy(), \"2016\":sales_2016.copy(), \"2017\":sales_2017.copy()}\nfigsize = (13,8)\nfig, ax = plt.subplots(3,3,figsize=figsize)\ni = 0;\nfor year in acs:\n    ac = outlierFilter(acs[year],\"sales\") # Remove outlier to make graph better understanding\n    print(\"{}: Number of outlier points: {} / total points {}\".format(year,  len(acs[year].index) -len(ac.index )\n                                                                      , len(acs[year].index) ) )\n    ac = ac.loc [ac.sales > 1e5 ] # Remove outliers\n    lagplot(ac[\"sales\"],shift=1,ax =ax[i,0], atext=year )\n    lagplot(ac[\"sales\"],shift=7,ax =ax[i,1], atext=year )\n    lagplot(ac[\"sales\"],shift=30,ax=ax[i,2], atext=year )\n    i += 1\n\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-04-05T22:11:32.845148Z","iopub.execute_input":"2022-04-05T22:11:32.845454Z","iopub.status.idle":"2022-04-05T22:11:34.311186Z","shell.execute_reply.started":"2022-04-05T22:11:32.845413Z","shell.execute_reply":"2022-04-05T22:11:34.310265Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ac = sales_2017.copy()\nac = outlierFilter(ac,\"sales\")\nac[\"lag_week\"]  = ac.sales.shift(7)\nac[\"lag_1day\"]   = ac.sales.shift(1)\nac[\"lag_30day\"]   = ac.sales.shift(30)\nprintmd(\"**For sales in 2017:**\")\nprint( \"Spearman Correlation: Sales vs lag_week: {:,.2f}\".format(ac.corr(\"spearman\").sales[\"lag_week\"] ) )\nprint( \"Spearman Correlation: Sales vs lag_1day: {:,.2f}\".format(ac.corr(\"spearman\").sales[\"lag_1day\"] ) )\nprint( \"Spearman Correlation: Sales vs lag_30day: {:,.2f}\".format(ac.corr(\"spearman\").sales[\"lag_30day\"] ) )","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-04-05T22:11:34.31292Z","iopub.execute_input":"2022-04-05T22:11:34.313265Z","iopub.status.idle":"2022-04-05T22:11:34.355438Z","shell.execute_reply.started":"2022-04-05T22:11:34.313219Z","shell.execute_reply":"2022-04-05T22:11:34.354286Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Conclusion\n\nThere were a strong Lag 1 and the strongest Lag 7-day.\n\n# Deseasonalized Trend\n\nBy removing the impact of seasonality, we will see how each family of products shows cyclic patterns.","metadata":{}},{"cell_type":"code","source":"from statsmodels.tsa.deterministic import CalendarFourier, DeterministicProcess\nfrom sklearn.linear_model import LinearRegression\ndef create_date_features(df):\n    df = df.to_timestamp()\n    df['is_month_start'] = df.index.is_month_start.astype(\"int8\")\n    df['is_month_end'] = df.index.is_month_end.astype(\"int8\")\n    df['is_year_start'] = df.index.is_year_start.astype(\"int8\")\n    df['is_year_end'] = df.index.is_year_end.astype(\"int8\")\n    df = df.to_period(\"D\")\n    return df","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-04-05T22:11:34.35656Z","iopub.execute_input":"2022-04-05T22:11:34.356798Z","iopub.status.idle":"2022-04-05T22:11:34.364599Z","shell.execute_reply.started":"2022-04-05T22:11:34.35677Z","shell.execute_reply":"2022-04-05T22:11:34.363579Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axes = plt.subplots(7, 5, figsize = (20,20))\nyears = [\"2015\",\"2016\",\"2017\"] #\"2013\",\"2014\",\nfamily_sales_all = train_data.groupby(['family','date']).sum().unstack('family').to_period(\"D\")\nfamily_sales_all['dayofyear'] = family_sales_all.index.dayofyear\nx = range(365)\nfor year in years:\n    family_sales = family_sales_all.loc[year, ['sales','onpromotion','dayofyear']]\n    for i, family in enumerate(family_unique):\n        supply_sales = family_sales.loc(axis=1)[:, family]\n        y = supply_sales.loc[:, 'sales'].squeeze()\n        # onpromotion = supply_sales.loc[:, 'onpromotion'].squeeze().rename('onpromotion')\n        fourier = CalendarFourier(freq='M', order=4)\n        dp = DeterministicProcess(\n            constant=True,\n            index=y.index,\n            order=1,\n            seasonal=True,\n            drop=True,\n            additional_terms=[fourier],\n        )\n        X_time = dp.in_sample()\n        #X_time['NewYearsDay']  = (X_time.index.dayofyear == 1)\n        X_time = create_date_features(X_time)\n\n        model = LinearRegression(fit_intercept=False)\n        model.fit(X_time, y)\n        y_deseason = y - model.predict(X_time)\n        y_deseason = y_deseason.rolling(7, center=True).mean()\n        y_deseason.name = 'sales_deseasoned'\n        #y_deseason.plot(x='dayofyear', ax=axes[(i//5), i%5])\n        axes[(i//5), i%5].plot( family_sales['dayofyear'], y_deseason, label=year )\n        axes[(i//5), i%5].set_title(\"{}\".format(family), fontsize = 12);\n        axes[(i//5), i%5].legend()\n        \nfig.delaxes(axes[6][3]);fig.delaxes(axes[6][4])\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-04-05T22:11:34.366364Z","iopub.execute_input":"2022-04-05T22:11:34.366728Z","iopub.status.idle":"2022-04-05T22:11:41.902221Z","shell.execute_reply.started":"2022-04-05T22:11:34.366649Z","shell.execute_reply":"2022-04-05T22:11:41.901105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Conclusion\n\nBy deseasonalizing (weekly and monthly) and comparing to different years, we see other patterns for some product families and some cases where the behaviors are `irregular variations`. For examples:\n\n* Lingerie: rose at the beginning and the end of summers perhaps due to the on-sale clearances.\n* Prepared Foods: between March to May. I do not come up with a reason.\n* School and office supplies: Obviously the needs suddenly jumped at the beginning of every academic year.\n* Seafood: We see a lot of fluctuations there, which need to be carefully modeled.\n\n### Irregular variations\n\nWhen comparing different years, we found there are a couple of abnormal peaks and troughs. By analyzing these irregularities, we could learn and improve the business.","metadata":{}},{"cell_type":"markdown","source":"# ACF and PACF Plots\n\nTo understand more about `Autocorrelation`, we investigate sales with ACF (a complete auto-correlation function) and PACF (a partial auto-correlation function) Plots. Based on that, we can determine the orders of auto-regressive (AR) and moving average (MA),i.e., calculate values of p and q in (p,d,q)m to feed into the AR-I-MA model (for seasonal time series). `p` for AR part, `q` for MA part, `d` is order of I (itegrative) part - a number of times to achieve stationary.\n\nIn this case of seasonal time series for sales, ACF shows Geometric Decat at each `m` Lag for AR and significant at `m` Lag for MA; invesely, PACF reverses this order.\n\nFurthermore, contrary to ACF where all correlations are found for original data, PACF finds the correlation of present and then correlation of residuals (which remains after explained by previous correlations). Hence, by analyzing PACF, we can select which families and their values of lag. Read more about AR, MA, ACF, PCF [here](https://towardsdatascience.com/significance-of-acf-and-pacf-plots-in-time-series-analysis-2fa11a5d10a8).\n\nFor example, if we found a `significant lag` _\"x\"_ which is significant in ACF, we should see geometric decays at Lag _2x, 3x, 4x_ and so on in PACF.\n","metadata":{}},{"cell_type":"code","source":"# PACF - ACF\nimport statsmodels.api as sm\na = train_data[(train_data.sales.notnull())].groupby([\"date\", \"family\"]).sales.mean().reset_index().set_index(\"date\")\nfamily_unique = a.family.unique()","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-04-05T22:11:41.90405Z","iopub.execute_input":"2022-04-05T22:11:41.904362Z","iopub.status.idle":"2022-04-05T22:11:42.402484Z","shell.execute_reply.started":"2022-04-05T22:11:41.904323Z","shell.execute_reply":"2022-04-05T22:11:42.401655Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for num, i in enumerate(family_unique):\n    fig, ax = plt.subplots(1,3,figsize=(25,3))\n    temp = a[(a.family == i)]#& (a.sales.notnull())\n    sm.graphics.tsa.plot_acf(temp.sales, lags=365, ax=ax[0], title = \"ACF: \" + i)\n    sm.graphics.tsa.plot_acf(temp.sales, lags=365, ax=ax[1], title = \"Zoomed ACF: \" + i)\n    sm.graphics.tsa.plot_pacf(temp.sales, lags=365, ax=ax[2], title = \"PACF: \" + i)\n    ax[0].set(ylim=[-0.25,1])\n    ax[1].set(xlim=[-1,60], ylim=[-0.25,1])\n    ax[2].set(ylim=[-0.25,1])","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-04-05T22:11:42.403584Z","iopub.execute_input":"2022-04-05T22:11:42.403818Z","iopub.status.idle":"2022-04-05T22:12:58.164082Z","shell.execute_reply.started":"2022-04-05T22:11:42.403791Z","shell.execute_reply":"2022-04-05T22:12:58.16302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Conclusion\n\nMost product families have a strong correlation with Lag 7-day or weekly frequency. They are clearly seasonal time series where the geometric decay repeats every 7-day Lag in ACF.\n\nHowever, some of them show weak correlations, for example, Baby Care, Celebration, Home and Kitchen I & II, Home care, Lawn and Garden, Magazines, Pet supplies, Players and Electronics, Produce. The needs of these products are a must, i.e., when someone needs them, one will probably go and buy them right away, instead of waiting until the weekend. Hence, they are non-seasonal time series.\n\nIn addition, we see some strong dependence on the frequency of 365-day such as Books, Frozen Foods, Liquor Wine Beer, Prepared Foods, School and Office Supplies. People often stock up on these products at special events or on holidays such as the end of a year or the beginning of an academic year. However, we can consider them as cyclic fluctuations with annual frequency.\n\nBy understanding well these behaviors, we can model better our data to predict more accurately.","metadata":{}},{"cell_type":"markdown","source":"# Earthquakes\n\nThere were 2 major earthquakes between period of data we are working on:\n1. 2014-08-12 at Pichincha with 5.1 Magnitude caused 4 deaths. Although it was not included in data, we could consider it later.\n2. 2016-04-16 at Pedernales, Manabí with 7.8 Magnitude caused severe damage, 676 deaths. In Holidays and Events dataset, we can search for `description` with begining `Terremoto Manabi` until `Terremoto Manabi+30`\n\n2016-04-16\tEvent\tNational\tEcuador\tTerremoto Manabi\tFALSE\n2016-05-16\tEvent\tNational\tEcuador\tTerremoto Manabi+30\tFALSE","metadata":{}},{"cell_type":"code","source":"sales_smooth              = sales_grouped.copy()\nsales_smooth['sales']     = sales_smooth.sales.rolling(window=7, center=False, min_periods=2).mean()\n\nplotdata = sales_smooth.loc[:,[\"sales\",\"year\", \"dayofyear\"] ]\nplotdata = plotdata.loc[ plotdata.index.month.isin([3,4,5])]\nx_start = date(2016, 4, 16).timetuple().tm_yday\nx_end   = date(2016, 5, 16).timetuple().tm_yday\n\nfigsize = (13,6)\nfig, ax = plt.subplots(figsize=figsize)\nseasonal_plot(plotdata, y=\"sales\", period=\"year\", freq=\"dayofyear\", ax=ax, atext=\"Earthquake Periods in\");\nax.set(ylim=3e5);\n\nax.axvline(x=x_start, color='r', linestyle='--')\nax.axvline(x=x_end,   color='r', linestyle='--')\n\nax.annotate(\"Event Earthquake began\",(x_start+1,6.1e5), fontsize=16)\nax.annotate(\"Event Earthquake ended\",(x_end-1,6.9e5),   fontsize=16, horizontalalignment='right');","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-04-05T22:12:58.165769Z","iopub.execute_input":"2022-04-05T22:12:58.166118Z","iopub.status.idle":"2022-04-05T22:12:58.576787Z","shell.execute_reply.started":"2022-04-05T22:12:58.166063Z","shell.execute_reply":"2022-04-05T22:12:58.575864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Summary\n\nWe can see a sudden jump once the earthquake started. The peak was about $30\\%$ increase than average.\nPeople might want to stock food as inventory, or they bought and donated to people in damaged area. Let's see what families of products got boosted.\n\nTherefore, we only look at the range from before earthquake 2 weeks, and after that 2 weeks.","metadata":{}},{"cell_type":"code","source":"family_group = train_data.drop( [\"store_nbr\",\"onpromotion\"] ,axis=1).loc[( (train_data.date.dt.month == 4) )].loc[( \n                    (train_data.date.dt.day > 16-2*7) & (train_data.date.dt.day < 16+2*7) )  ]\nfamily_group = family_group.groupby([\"family\",\"date\"]).agg({\"sales\":\"sum\"}).reset_index().set_index(\"family\")\nfamily_group[\"year\"] = family_group.date.dt.year\nfamily_group[\"day\"] = family_group.date.dt.day\nfamilies     = family_group.index.unique().values\nfig, axes = plt.subplots(7, 5, figsize = (20,20))\npalette=sns.color_palette( \"husl\", n_colors=len(family_group.year.unique()))\nfor i, fam in enumerate(families):\n    #print(\"{} {}\".format(i, fam))\n    plotdata = family_group.loc[fam].copy()\n    plotdata[\"sales\"] = plotdata.sales.rolling(window=7, center=True, min_periods=2).mean()\n    #plotdata.plot( ax=axes[(i//6), i%6-1], x=\"date\", y=\"sales\",legend=None, style=\"year\"  )\n    sns.lineplot(data=plotdata.reset_index(), x=\"day\", hue=\"year\" ,y=\"sales\", ax=axes[(i//5), i%5], palette=palette)\n    axes[(i//5), i%5].set_title(fam, fontsize = 12)\nfig.delaxes(axes[6][3]);fig.delaxes(axes[6][4])\nplt.show()\n#sns.lineplot(data=family_group, x=\"date\", hue=\"family\" ,y=\"sales\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-04-05T22:12:58.578096Z","iopub.execute_input":"2022-04-05T22:12:58.578346Z","iopub.status.idle":"2022-04-05T22:13:08.681948Z","shell.execute_reply.started":"2022-04-05T22:12:58.578314Z","shell.execute_reply":"2022-04-05T22:13:08.680832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Conclusion\n\nBased on separate family graph, we can identify which families of product were boosted:\n\n1. Food: Beverages, Bread/Bakery, Grocery I\n2. Home and tools: Cleaning, Hardware, Home Appliances and Home Care, Players and Electronics\n3. Essential personal stuff: Personal Care","metadata":{}},{"cell_type":"markdown","source":"# Holidays and Events\n\n**They are important factors, because:**\n1. People are off and have time on holidays, it's more likely that they will spend money on shoping.\n2. Events usually promote sales with good deal, motivate people to spend more money.\n\n**Type of Holidays:**\n* A holiday that is `transferred` officially falls on that calendar day, but was moved to another date by the government. A transferred day is more like a normal day than a holiday. To find the day that it was actually celebrated, look for the corresponding row where type is `Transfer`. For example, the holiday Independencia de Guayaquil was transferred from 2012-10-09 to 2012-10-12, which means it was celebrated on 2012-10-12. \n* Days that are type `Bridge` are extra days that are added to a holiday (e.g., to extend the break across a long weekend). These are frequently made up by the type `Work Day` which is a day not normally scheduled for work (e.g., Saturday) that is meant to payback the Bridge.\n* `Additional` holidays are days added a regular calendar holiday, for example, as typically happens around Christmas (making Christmas Eve a holiday).\n* `Event` is special days, for example, an earthquake event.","metadata":{}},{"cell_type":"code","source":"range_begin = \"2017\"\nrange_end   = \"2017-08-15\"\nholidays_events_data = pd.read_csv(comp_dir / \"holidays_events.csv\", index_col=None, header=0, parse_dates=['date'])\nholidays_events_data = holidays_events_data.astype({'type': 'category', 'locale': 'category', 'locale_name': 'category', \n                                                    'description': 'category', 'transferred': 'bool',})\nholidays_events_data = holidays_events_data.set_index('date').to_period('D')\nholidays_events_data = holidays_events_data.loc[range_begin:range_end]\n\nprint(\"Types of Holidays: {}\"         .format(holidays_events_data.type.unique().tolist()))\nprint(\"Location types of Holidays: {}\".format(holidays_events_data.locale.unique().tolist()))","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-04-05T22:13:08.683271Z","iopub.execute_input":"2022-04-05T22:13:08.683518Z","iopub.status.idle":"2022-04-05T22:13:08.706568Z","shell.execute_reply.started":"2022-04-05T22:13:08.683487Z","shell.execute_reply":"2022-04-05T22:13:08.705649Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"########### Remove transferred Holidays\n# Query only transferred-related days\ntransferred_days    = holidays_events_data.loc[(holidays_events_data.transferred == True), [\"type\",\"description\"]]\nnew_transferal_days = holidays_events_data.loc[(holidays_events_data.type == \"Transfer\")]\n# replace \"Tranfer\" type with \"Holiday\", replace descriptions back to original text\nclean_transferal_days = pd.concat([new_transferal_days.reset_index(),transferred_days.reset_index()],\n                                  axis=1).iloc[:,[0,2,3,7,8]].set_index(\"date\")\n# Remove transferred Holidays\nholidays = holidays_events_data.loc[(holidays_events_data.transferred == False) & (\n                holidays_events_data.type != \"Transfer\")].drop(\"transferred\", axis = 1)\nholidays = holidays.append(clean_transferal_days).sort_index()\n# Clean special letters and numbers in `description`\nholidays[\"description\"] = holidays[\"description\"].str.replace(\"-\", \"\").str.replace(\"+\", \"\").str.replace('\\d+', '')\n# `Additional` is also holiday\nholidays[\"type\"] = np.where(holidays[\"type\"] == \"Additional\", \"Holiday\", holidays[\"type\"])\n# Bridge Holidays is also holiday\nholidays[\"description\"] = holidays[\"description\"].str.replace(\"Puente \", \"\")\nholidays[\"type\"] = np.where(holidays[\"type\"] == \"Bridge\", \"Holiday\", holidays[\"type\"])","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-04-05T22:13:08.711258Z","iopub.execute_input":"2022-04-05T22:13:08.711519Z","iopub.status.idle":"2022-04-05T22:13:08.740038Z","shell.execute_reply.started":"2022-04-05T22:13:08.711488Z","shell.execute_reply":"2022-04-05T22:13:08.739076Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Separate types of Holiday\n# Remove Work Day type\nwork_day = holidays.loc[holidays.type == \"Work Day\"]  \nholidays = holidays.loc[holidays.type != \"Work Day\"] \n\nevents   = holidays.loc[holidays.type == \"Event\"]\nholidays = holidays.loc[holidays.type != \"Event\"].drop(\"type\",axis=1)\n\nholidays = holidays.reset_index()#.drop_duplicates(subset=\"date\",keep='first').set_index('date')\nholidays_National = holidays.loc[holidays.locale==\"National\"].loc[:,['date','description']].set_index('date')\nholidays_Regional = holidays.loc[holidays.locale==\"Regional\"].loc[:,['date','locale_name','description']].set_index('date')\nholidays_Local    = holidays.loc[holidays.locale==\"Local\"]   .loc[:,['date','locale_name','description']].set_index('date')\n############## Rename columns ##############\nholidays_National = holidays_National.rename({\"description\":\"holiday_national\"}, axis = 1)\nholidays_Regional = holidays_Regional.rename({\"description\":\"holiday_state\", \"locale_name\":\"state\", }, axis = 1)\nholidays_Local    = holidays_Local   .rename({\"description\":\"holiday_city\" , \"locale_name\":\"city\",  }, axis = 1)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-04-05T22:13:08.741591Z","iopub.execute_input":"2022-04-05T22:13:08.741885Z","iopub.status.idle":"2022-04-05T22:13:08.76451Z","shell.execute_reply.started":"2022-04-05T22:13:08.741853Z","shell.execute_reply":"2022-04-05T22:13:08.763825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_data = pd.merge(train_data, stores_data, on=\"store_nbr\")\nall_data[\"store_nbr\"] = all_data[\"store_nbr\"].astype(\"int8\") \nall_data[\"onpromotion\"] = all_data[\"onpromotion\"].astype(\"int16\")\n\nfigsize = (13,5)\nfig, ax = plt.subplots(figsize=figsize)\nsales_holidays = sales_grouped.loc[range_begin:range_end]\nsales_holidays.plot( y=\"sales\",  ax=ax );\nax.plot_date(holidays_National.index, sales_holidays.loc[holidays_National.index].sales, color='r')\nax.plot_date(holidays_Regional.index, sales_holidays.loc[holidays_Regional.index].sales, color='b',marker=\"s\")\nax.plot_date(holidays_Local.index,    sales_holidays.loc[holidays_Local.index]   .sales, color='k',marker=\"^\")\nax.legend([\"Sales\",\"National\",\"Regional\",\"Local\"])\nax.set(ylim=4e5);","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-04-05T22:13:08.765775Z","iopub.execute_input":"2022-04-05T22:13:08.766447Z","iopub.status.idle":"2022-04-05T22:13:09.677728Z","shell.execute_reply.started":"2022-04-05T22:13:08.766406Z","shell.execute_reply":"2022-04-05T22:13:09.676992Z"},"trusted":true},"execution_count":null,"outputs":[]}]}