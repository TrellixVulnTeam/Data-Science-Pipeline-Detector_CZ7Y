{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Store Sales - Time Series Forecasting\n\n## Links\n\n### [View interactive visualization at Kaggle:](https://www.kaggle.com/linhhlp/store-sales-time-series-forecasting-full-analysis/)\n> https://www.kaggle.com/linhhlp/store-sales-time-series-forecasting-full-analysis/\n\n**Database link [on Kaggle](https://www.kaggle.com/c/store-sales-time-series-forecasting/data)**\n\n**Full code hosted in [GitHub](https://github.com/linhhlp/Store-Sales-Time-Series-Forecasting-Kaggle/)**\n\n## Introduction\n\n* Using time-series forecasting to forecast store sales on data from CorporaciÃ³n Favorita, a large Ecuadorian-based grocery retailer.\n\n* Building a model that more accurately predicts the unit sales for thousands of items sold at different Favorita stores.\n\n* Considering external impacts such as \n\n> 1. Holidays and Events. \n>\n> 2. Dates to pay wages in the public sector.\n>\n> 3. Major crisis (A magnitude 7.8 earthquake struck Ecuador on April 16, 2016).\n>\n> 4. Daily oil price (Ecuador is an oil-dependent country).\n\n## Benefits to having a predictive model\n\n1. Forecasts are especially relevant to brick-and-mortar grocery stores, which must dance delicately with how much inventory to buy. Predict a little over, and grocers are stuck with overstocked, perishable goods. Guess a little under, and popular items quickly sell out, leading to lost revenue and upset customers. More accurate forecasting, thanks to machine learning, could help ensure retailers please customers by having just enough of the right products at the right time.\n\n2. Current subjective forecasting methods for retail have little data to back them up and are unlikely to be automated. The problem becomes even more complex as retailers add new locations with unique needs, new products, ever-transitioning seasonal tastes, and unpredictable product marketing.\n\n3. More accurate forecasting can decrease food waste related to overstocking and improve customer satisfaction. The results of this ongoing competition, over time, might even ensure your local store has exactly what you need the next time you shop.","metadata":{}},{"cell_type":"markdown","source":"# Importing Packages","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom datetime import datetime, date\n\n\nfrom sklearn.linear_model import LinearRegression\nfrom statsmodels.tsa.deterministic import Fourier,CalendarFourier, DeterministicProcess","metadata":{"execution":{"iopub.status.busy":"2022-03-19T07:08:15.24307Z","iopub.execute_input":"2022-03-19T07:08:15.244069Z","iopub.status.idle":"2022-03-19T07:08:16.420351Z","shell.execute_reply.started":"2022-03-19T07:08:15.243929Z","shell.execute_reply":"2022-03-19T07:08:16.419535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plotting import\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Patch\nfrom matplotlib.ticker import MaxNLocator\nfrom learntools.time_series.utils import plot_periodogram, seasonal_plot # Kaggle Learning tools\nfrom learntools.time_series.style import *  # plot style settings\n\nimport seaborn as sns\n","metadata":{"execution":{"iopub.status.busy":"2022-03-19T07:08:16.421815Z","iopub.execute_input":"2022-03-19T07:08:16.422031Z","iopub.status.idle":"2022-03-19T07:08:16.58628Z","shell.execute_reply.started":"2022-03-19T07:08:16.422005Z","shell.execute_reply":"2022-03-19T07:08:16.585272Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pathlib import Path\ncomp_dir = Path('../input/store-sales-time-series-forecasting')","metadata":{"execution":{"iopub.status.busy":"2022-03-19T07:08:16.587801Z","iopub.execute_input":"2022-03-19T07:08:16.588059Z","iopub.status.idle":"2022-03-19T07:08:16.59251Z","shell.execute_reply.started":"2022-03-19T07:08:16.588025Z","shell.execute_reply":"2022-03-19T07:08:16.591634Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nX_scaler = MinMaxScaler(feature_range=(0, 1))","metadata":{"execution":{"iopub.status.busy":"2022-03-19T07:08:16.594138Z","iopub.execute_input":"2022-03-19T07:08:16.594399Z","iopub.status.idle":"2022-03-19T07:08:16.603768Z","shell.execute_reply.started":"2022-03-19T07:08:16.594372Z","shell.execute_reply":"2022-03-19T07:08:16.603061Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Time Related Features\ndef create_date_features(df):\n    df = df.to_timestamp()\n    df['is_month_start'] = df.index.is_month_start.astype(\"int8\")\n    df['is_month_end'] = df.index.is_month_end.astype(\"int8\")\n    df[\"wageday\"]= np.where( df.index.is_month_end | (df.index.day == 15), 1, 0).astype(\"int8\")\n    df = df.to_period(\"D\")\n    return df","metadata":{"execution":{"iopub.status.busy":"2022-03-19T07:08:16.605062Z","iopub.execute_input":"2022-03-19T07:08:16.605378Z","iopub.status.idle":"2022-03-19T07:08:16.616028Z","shell.execute_reply.started":"2022-03-19T07:08:16.605346Z","shell.execute_reply":"2022-03-19T07:08:16.615147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Importing Data","metadata":{}},{"cell_type":"code","source":"train_data   = pd.read_csv(comp_dir / \"train.csv\", index_col=\"id\", header=0, parse_dates=['date'])\nstores_data  = pd.read_csv(comp_dir / \"stores.csv\", index_col=\"store_nbr\", header=0)\nstore_nbr_id = stores_data.index.values","metadata":{"execution":{"iopub.status.busy":"2022-03-19T07:08:16.617451Z","iopub.execute_input":"2022-03-19T07:08:16.61769Z","iopub.status.idle":"2022-03-19T07:08:20.19557Z","shell.execute_reply.started":"2022-03-19T07:08:16.617658Z","shell.execute_reply":"2022-03-19T07:08:20.194687Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#  Preparing Data","metadata":{}},{"cell_type":"markdown","source":"## Holidays and Events","metadata":{}},{"cell_type":"code","source":"######### Range of Date to Modeling #############\nrange_begin = \"2017-05-01\"\nrange_end   = \"2017-08-15\"","metadata":{"execution":{"iopub.status.busy":"2022-03-19T07:08:20.196875Z","iopub.execute_input":"2022-03-19T07:08:20.197097Z","iopub.status.idle":"2022-03-19T07:08:20.200873Z","shell.execute_reply.started":"2022-03-19T07:08:20.197071Z","shell.execute_reply":"2022-03-19T07:08:20.200069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"holidays_events_data = pd.read_csv(comp_dir / \"holidays_events.csv\", index_col=None, header=0, parse_dates=['date'])\nholidays_events_data = holidays_events_data.astype({'type': 'category', 'locale': 'category', 'locale_name': 'category', \n                                                    'description': 'category', 'transferred': 'bool',})\nholidays_events_data = holidays_events_data.set_index('date').to_period('D')\nholidays_events_data = holidays_events_data.loc[range_begin:]","metadata":{"execution":{"iopub.status.busy":"2022-03-19T07:08:20.202069Z","iopub.execute_input":"2022-03-19T07:08:20.202674Z","iopub.status.idle":"2022-03-19T07:08:20.235638Z","shell.execute_reply.started":"2022-03-19T07:08:20.202628Z","shell.execute_reply":"2022-03-19T07:08:20.235038Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"########### Remove transferred Holidays ##############\n# Query only transferred-related days\ntransferred_days    = holidays_events_data.loc[(holidays_events_data.transferred == True), [\"type\",\"description\"]]\nnew_transferal_days = holidays_events_data.loc[(holidays_events_data.type == \"Transfer\")]\n# replace \"Tranfer\" type with \"Holiday\", replace descriptions back to original text\nclean_transferal_days = pd.concat([new_transferal_days.reset_index(),transferred_days.reset_index()],\n                                  axis=1).iloc[:,[0,2,3,7,8]].set_index(\"date\")\n# Remove transferred Holidays\nholidays = holidays_events_data.loc[(holidays_events_data.transferred == False) & (\n                holidays_events_data.type != \"Transfer\")].drop(\"transferred\", axis = 1)\nholidays = holidays.append(clean_transferal_days).sort_index()","metadata":{"execution":{"iopub.status.busy":"2022-03-19T07:08:20.236526Z","iopub.execute_input":"2022-03-19T07:08:20.237215Z","iopub.status.idle":"2022-03-19T07:08:20.257686Z","shell.execute_reply.started":"2022-03-19T07:08:20.237183Z","shell.execute_reply":"2022-03-19T07:08:20.256799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Clean special letters and numbers in `description`\nholidays[\"description\"] = holidays[\"description\"].str.replace(\"-\", \"\").str.replace(\"+\", \"\").str.replace('\\d+', '')\n# `Additional` is also holiday\nholidays[\"type\"] = np.where(holidays[\"type\"] == \"Additional\", \"Holiday\", holidays[\"type\"])\n# Bridge Holidays is also holiday\nholidays[\"description\"] = holidays[\"description\"].str.replace(\"Puente \", \"\")\nholidays[\"type\"] = np.where(holidays[\"type\"] == \"Bridge\", \"Holiday\", holidays[\"type\"])","metadata":{"execution":{"iopub.status.busy":"2022-03-19T07:08:20.260146Z","iopub.execute_input":"2022-03-19T07:08:20.260436Z","iopub.status.idle":"2022-03-19T07:08:20.274546Z","shell.execute_reply.started":"2022-03-19T07:08:20.260405Z","shell.execute_reply":"2022-03-19T07:08:20.273532Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Separate types of Holiday\n# Remove Work Day type\nwork_day = holidays.loc[holidays.type == \"Work Day\"]  \nholidays = holidays.loc[holidays.type != \"Work Day\"] \n\nevents   = holidays.loc[holidays.type == \"Event\"].drop([\"type\",\"locale\",\"locale_name\"],axis=1).drop_duplicates()\nevents   = events.rename({\"description\":\"event_national\"}, axis = 1)\nholidays = holidays.loc[holidays.type != \"Event\"].drop(\"type\",axis=1)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-03-19T07:08:20.275669Z","iopub.execute_input":"2022-03-19T07:08:20.27657Z","iopub.status.idle":"2022-03-19T07:08:20.293169Z","shell.execute_reply.started":"2022-03-19T07:08:20.276527Z","shell.execute_reply":"2022-03-19T07:08:20.292185Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"holidays = holidays.reset_index()#.drop_duplicates(subset=\"date\",keep='first').set_index('date')\nholidays_National = holidays.loc[holidays.locale==\"National\"].loc[:,['date','description']].set_index('date')\nholidays_Regional = holidays.loc[holidays.locale==\"Regional\"].loc[:,['date','locale_name','description']].set_index('date')\nholidays_Local    = holidays.loc[holidays.locale==\"Local\"]   .loc[:,['date','locale_name','description']].set_index('date')\n############## Rename columns ##############\nholidays_National = holidays_National.rename({\"description\":\"holiday_national\"}, axis = 1).drop_duplicates()\nholidays_Regional = holidays_Regional.rename({\"description\":\"holiday_state\", \"locale_name\":\"state\"}, axis = 1).drop_duplicates()\nholidays_Local    = holidays_Local   .rename({\"description\":\"holiday_city\" , \"locale_name\":\"city\" }, axis = 1).drop_duplicates()","metadata":{"execution":{"iopub.status.busy":"2022-03-19T07:08:20.294652Z","iopub.execute_input":"2022-03-19T07:08:20.295576Z","iopub.status.idle":"2022-03-19T07:08:20.318764Z","shell.execute_reply.started":"2022-03-19T07:08:20.295523Z","shell.execute_reply":"2022-03-19T07:08:20.317668Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train and Test Data","metadata":{}},{"cell_type":"code","source":"##################### Import Test Data ################################\ndf_test = pd.read_csv( comp_dir / 'test.csv',  parse_dates=['date'], infer_datetime_format=True, )\ndf_test = df_test.astype({'store_nbr': 'category', 'family': 'category', 'onpromotion': 'float32'})\ndf_test['date'] = df_test.date.dt.to_period('D')\n################# Append All data  ###########################\ntrain_data_clean = train_data.copy()\ntrain_data_clean['date'] = train_data.date.dt.to_period(\"D\")\ntrain_data_clean = train_data_clean.append(df_test)\ntrain_data_clean = train_data_clean.astype({'family': 'category', 'sales': 'float32', \"onpromotion\":\"float32\"})\ntrain_data_clean['onpromotion'] = X_scaler.fit_transform(train_data_clean[['onpromotion']]).flatten()","metadata":{"execution":{"iopub.status.busy":"2022-03-19T07:08:20.320015Z","iopub.execute_input":"2022-03-19T07:08:20.320265Z","iopub.status.idle":"2022-03-19T07:08:21.264983Z","shell.execute_reply.started":"2022-03-19T07:08:20.320235Z","shell.execute_reply":"2022-03-19T07:08:21.264085Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Merging Data","metadata":{}},{"cell_type":"code","source":"# Include Store information\nall_data = pd.merge(train_data_clean, stores_data, on=\"store_nbr\")\nall_data = all_data.astype({'store_nbr': 'category'})\n# Add hoilidays\nall_data = pd.merge(all_data, holidays_National, how = \"left\", on=\"date\")\nall_data = pd.merge(all_data, events,            how = \"left\", on=\"date\")\nall_data = pd.merge(all_data, holidays_Regional, how = \"left\", on = [\"date\", \"state\"])\nall_data = pd.merge(all_data, holidays_Local,    how = \"left\", on = [\"date\", \"city\"])","metadata":{"execution":{"iopub.status.busy":"2022-03-19T07:08:21.266283Z","iopub.execute_input":"2022-03-19T07:08:21.266523Z","iopub.status.idle":"2022-03-19T07:08:26.84212Z","shell.execute_reply.started":"2022-03-19T07:08:21.266494Z","shell.execute_reply":"2022-03-19T07:08:26.841223Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Manipulating Data","metadata":{}},{"cell_type":"code","source":"##################### Splitting Data #########################\ny_store    = np.empty(len(store_nbr_id )+1, pd.DataFrame)\npromos     = np.empty(len(store_nbr_id )+1, pd.DataFrame)\nholidays   = np.empty(len(store_nbr_id )+1, pd.DataFrame)\nX_dummies  = np.empty(len(store_nbr_id )+1, pd.DataFrame)\nY_training = np.empty(len(store_nbr_id )+1, pd.DataFrame)\ntmp2 = dict(iter(all_data.groupby('store_nbr')))\nfor store in store_nbr_id: \n    #tmp = all_data.loc[all_data.store_nbr==store]\n    tmp = tmp2[store]\n    y_store[store] = tmp[ [\"date\",\"family\",\"sales\"]]\n    \n    promos[store]  =  tmp[ [\"date\",\"family\",\"onpromotion\"]].replace({'0':np.nan, 0:np.nan})\n    promos[store].set_index(\"date\")\n        \n    holidays[store] = tmp[[\"date\",\"holiday_national\",\"holiday_state\",\"holiday_city\"] ].drop_duplicates()\n    holidays[store] = holidays[store].set_index(\"date\")\n\n#del all_data","metadata":{"execution":{"iopub.status.busy":"2022-03-19T07:08:26.843283Z","iopub.execute_input":"2022-03-19T07:08:26.843494Z","iopub.status.idle":"2022-03-19T07:10:56.040118Z","shell.execute_reply.started":"2022-03-19T07:08:26.843469Z","shell.execute_reply":"2022-03-19T07:10:56.039228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##################### Manipulating Data #########################\nfor store in store_nbr_id: \n    y_store[store] = y_store[store].set_index(['family', 'date']).sort_index()\n    y_store[store] = y_store[store].unstack('family')\n    y_store[store] = y_store[store].fillna(0)\n    \n    promos[store]  =  promos[store].groupby([\"date\",\"family\"]).agg(\n                                    {\"onpromotion\":\"mean\"}).unstack(\"family\").dropna(how=\"all\",axis=0).fillna(0)\n    promos[store].columns = promos[store].columns.droplevel(level=0)\n    \n    Y_training[store] = y_store[store].loc[range_begin:range_end]\n    X_dummies[store]  = pd.get_dummies(holidays[store])\n    if ~promos[store].empty: X_dummies[store]  = pd.concat([X_dummies[store],promos[store]], axis=1).fillna(0)","metadata":{"execution":{"iopub.status.busy":"2022-03-19T07:10:56.041479Z","iopub.execute_input":"2022-03-19T07:10:56.041795Z","iopub.status.idle":"2022-03-19T07:10:59.593248Z","shell.execute_reply.started":"2022-03-19T07:10:56.041753Z","shell.execute_reply":"2022-03-19T07:10:59.592348Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Engeneering features","metadata":{}},{"cell_type":"code","source":"##################### Creating Training Data #########################\nfourier = CalendarFourier(freq='M', order=4)  # Fourier(period=7, order=3)# \ndp = np.empty(len(store_nbr_id )+1, DeterministicProcess)\nX_training  = np.empty(len(store_nbr_id )+1 , pd.DataFrame)\nfor store in store_nbr_id: \n    dp[store] = DeterministicProcess(\n        index=Y_training[store].index,\n        constant=False,\n        order=1,\n        seasonal=True,\n        additional_terms=[fourier],\n        drop=True,\n        period=7,\n    )\n    X = dp[store].in_sample()\n    X = create_date_features(X)\n    X_training[store] = pd.concat([X,X_dummies[store].loc[range_begin:range_end]], axis=1).fillna(0)","metadata":{"execution":{"iopub.status.busy":"2022-03-19T07:10:59.594285Z","iopub.execute_input":"2022-03-19T07:10:59.594491Z","iopub.status.idle":"2022-03-19T07:11:00.365887Z","shell.execute_reply.started":"2022-03-19T07:10:59.594465Z","shell.execute_reply":"2022-03-19T07:11:00.364874Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Building model","metadata":{}},{"cell_type":"code","source":"##################### Creating Model & Fitting #########################\nmodel  = np.empty(len(store_nbr_id )+1 , LinearRegression)\ny_pred = np.empty(len(store_nbr_id )+1, pd.DataFrame)\ntmp = np.empty(len(store_nbr_id )+1, pd.DataFrame)\nfor store in store_nbr_id: \n    model[store] = LinearRegression(fit_intercept=False)\n    model[store].fit(X_training[store], Y_training[store])\n    y_pred[store] = pd.DataFrame(model[store].predict(X_training[store]), \n                                 index=X_training[store].index, columns=Y_training[store].columns)\n    tmp[store] = y_pred[store].stack([ 'family'])\n    tmp[store]['store_nbr'] = store\n    tmp[store]['store_nbr'] = tmp[store]['store_nbr'].astype({'store_nbr': 'category'})\n    tmp[store] = tmp[store].set_index('store_nbr',append=True).reorder_levels(['store_nbr', 'family','date']) \n\ny_model = pd.concat(tmp[1:])\n\n##################### Visualizing ################################\nfigsize = (13,6)\nfig, ax = plt.subplots(figsize=figsize)\ny_model.reset_index().groupby(\"date\").agg({\"sales\":\"sum\"}).reset_index().set_index(\"date\").plot(ax=ax)\ntrain_data.groupby(\"date\").agg({\"sales\":\"sum\"}).reset_index().set_index(\"date\").plot(ax=ax)\nax.legend([\"Fitting Data\", \"Train Data\"])\nax.set_xlim(date(2017, 5, 1) )\nax.set_title(\"Average sales by day\");","metadata":{"execution":{"iopub.status.busy":"2022-03-19T07:11:00.367223Z","iopub.execute_input":"2022-03-19T07:11:00.367522Z","iopub.status.idle":"2022-03-19T07:11:02.175265Z","shell.execute_reply.started":"2022-03-19T07:11:00.367483Z","shell.execute_reply":"2022-03-19T07:11:02.174255Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"STORE_NBR = 1  # 1 - 54\nFAMILY = 'PRODUCE'  # display(store_sales.index.get_level_values('family').unique())\nfigsize = (13,6)\nfig, ax = plt.subplots(figsize=figsize)\nY_training[STORE_NBR].loc(axis=1)['sales', FAMILY].plot(**plot_params, ax=ax)\ny_pred[STORE_NBR].loc(axis=1)['sales', FAMILY].plot(ax=ax)\nax.set_xlim(date(2017, 5, 1) )\nax.set_title(f'{FAMILY} Sales at Store {STORE_NBR}');","metadata":{"execution":{"iopub.status.busy":"2022-03-19T07:11:02.176499Z","iopub.execute_input":"2022-03-19T07:11:02.17673Z","iopub.status.idle":"2022-03-19T07:11:02.482887Z","shell.execute_reply.started":"2022-03-19T07:11:02.176702Z","shell.execute_reply":"2022-03-19T07:11:02.481922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Predict future sales","metadata":{}},{"cell_type":"code","source":"test_data = df_test.set_index(['store_nbr', 'family', 'date']).sort_index()\n##################### Creating Testing Features #########################\nX_test    = np.empty(len(store_nbr_id )+1 , pd.DataFrame)\ny_pred    = np.empty(len(store_nbr_id )+1, pd.DataFrame)\ntmp       = np.empty(len(store_nbr_id )+1, pd.DataFrame)\ny_submit  = pd.DataFrame(index=test_data.index)\nfor store in store_nbr_id: \n    X_test[store] = dp[store].out_of_sample(steps=16)\n    X_test[store].index.name = 'date'\n    X_test[store] = create_date_features(X_test[store])\n    X_test[store] = X_test[store].join(X_dummies[store]).fillna(0.0)\n    ##################### Modeling Predicting #########################\n    y_pred[store] = pd.DataFrame(model[store].predict(X_test[store]), \n                                 index=X_test[store].index, columns=Y_training[store].columns)\n    tmp[store] = y_pred[store].stack([ 'family'])\n    tmp[store]['store_nbr'] = store\n    tmp[store]['store_nbr'] = tmp[store]['store_nbr'].astype({'store_nbr': 'category'})\n    tmp[store] = tmp[store].set_index('store_nbr',append=True).reorder_levels(['store_nbr', 'family','date']) \n\ny_submit = pd.concat(tmp[1:])\ny_submit = y_submit.join(test_data.id).reindex(columns=['id', 'sales'])\ny_submit['sales'] = np.where (y_submit['sales'] <0 ,0, y_submit['sales'])\n##################### Visualizing ################################\nfigsize = (13,6)\nfig, ax = plt.subplots(figsize=figsize)\ny_submit.reset_index().groupby(\"date\").agg({\"sales\":\"sum\"}).reset_index().set_index(\"date\").plot(ax=ax)\ntrain_data.groupby(\"date\").agg({\"sales\":\"sum\"}).reset_index().set_index(\"date\").plot(ax=ax)\nax.legend([\"Predicting Data\", \"Train Data\"])\nax.set_xlim(date(2017, 4, 1) )\nax.set_title(\"Average sales by day\");","metadata":{"execution":{"iopub.status.busy":"2022-03-19T07:11:02.484308Z","iopub.execute_input":"2022-03-19T07:11:02.484766Z","iopub.status.idle":"2022-03-19T07:11:04.242634Z","shell.execute_reply.started":"2022-03-19T07:11:02.484715Z","shell.execute_reply":"2022-03-19T07:11:04.241972Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"STORE_NBR = 1  # 1 - 54\nFAMILY    = 'PRODUCE'  # display(store_sales.index.get_level_values('family').unique())\nfigsize   = (13,6)\nfig, ax   = plt.subplots(figsize=figsize)\nY_training[STORE_NBR].loc(axis=1)['sales', FAMILY].plot(**plot_params, ax=ax)\ny_submit.loc[STORE_NBR, FAMILY].sales.plot(ax=ax)\nax.set_xlim(date(2017, 8, 1) )\nax.set_title(f'{FAMILY} Sales at Store {STORE_NBR}');","metadata":{"execution":{"iopub.status.busy":"2022-03-19T07:11:04.243689Z","iopub.execute_input":"2022-03-19T07:11:04.244377Z","iopub.status.idle":"2022-03-19T07:11:04.580673Z","shell.execute_reply.started":"2022-03-19T07:11:04.24434Z","shell.execute_reply":"2022-03-19T07:11:04.57988Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# To Do\n\nFuture works\n\n1. Lag features\n\n2. Combination of models","metadata":{}}]}