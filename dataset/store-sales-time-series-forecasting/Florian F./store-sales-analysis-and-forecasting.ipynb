{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Store Sales Analysis and Forecasting\nüìã **Table of Content:**\n1. [EDA](#eda)\n    1. [Sales](#sales)\n        - [Seasonality](#seasonality)\n    1. [Oil Price](#oil)\n    1. [Holidays / Events](#events)\n1. [Machine Learning Forecasting](#forecasting)\n    1. [Preprocessing](#preprocessing)\n        - [Optimizing trainset length](#train-length)\n    1. [Models comparison](#models)\n        - [CustomRegressor](#customreg)\n    1. [Predictions on test set](#testpreds)\n\n---\nüìâ **Evaluation Metric:**  \nThe evaluation metric for this competition is Root Mean Squared Logarithmic Error.  \n\\begin{align}\nRMSLE = \\sqrt{ \\frac{1}{n} \\sum_{i=1}^n \\left(\\log (1 + \\hat{y}_i) - \\log (1 + y_i)\\right)^2}\n\\end{align}  \n- $n$ is the total number of instances\n- $\\hat{y}_i$ is the predicted value of the target for instance $i$\n- $y_i$ is the actual value of the target for instance $i$\n  \n---\nüìö **Sources:**\n- EDA:\n    - [Notebook: üìùStore Sales Analysis‚è≥ Time Serie](https://www.kaggle.com/kashishrastogi/store-sales-analysis-time-serie/notebook)\n    - [Nicer seasonal decompose chart](https://gist.github.com/tomron/8798256fcee5438edd58c17654adf443)\n- Seasonality\n    - [Kaggle Time Series Tutorial - Seasonality](https://www.kaggle.com/ryanholbrook/seasonality)\n    - [Tensorflow TimeSeries Tutorial](https://www.tensorflow.org/tutorials/structured_data/time_series)\n- Lag Features\n    - [Kaggle Time Series Tutorial - Time Series as Features](https://www.kaggle.com/ryanholbrook/time-series-as-features)\n    - [Plot_pacf, plot_acf, autocorrelation_plot and lag_plot](https://community.plotly.com/t/plot-pacf-plot-acf-autocorrelation-plot-and-lag-plot/24108)\n- Models\n    - [Notebook: Store Sales simple XG Boost GPU [LB=0.44579]](https://www.kaggle.com/koheishima/store-sales-simple-xg-boost-gpu-lb-0-44579)\n    - [Notebook: TS + Ridge + RF by AS](https://www.kaggle.com/code/dkomyagin/simple-ts-ridge-rf/notebook)\n    - [Scikit-learn doc](https://scikit-learn.org/)\n    - [XGBoost: A Complete Guide to Fine-Tune and Optimize your Model](https://towardsdatascience.com/xgboost-fine-tune-and-optimize-your-model-23d996fab663)\n    - Custom Regressor\n        - [Notebook: Store Sales: Ridge+Voting(Bagging(ET)+Bagging(RF))](https://www.kaggle.com/code/hiro5299834/store-sales-ridge-voting-bagging-et-bagging-rf/notebook) The used class in this notebook is an optimization of the class and the models hyperparameters made in the notebook [üìùStore Sales Analysis‚è≥ Time Serie](https://www.kaggle.com/kashishrastogi/store-sales-analysis-time-serie/notebook)\n        - [Joblib doc](https://joblib.readthedocs.io/en/latest/parallel.html#common-usage)","metadata":{}},{"cell_type":"code","source":"import os\nimport time\nimport itertools\nimport calendar as cal\nimport numpy as np \nimport pandas as pd\nimport seaborn as sns\nfrom datetime import datetime, timedelta\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nfrom scipy.signal import periodogram\nfrom statsmodels.tsa.seasonal import seasonal_decompose, DecomposeResult\nfrom statsmodels.tsa.deterministic import CalendarFourier, DeterministicProcess\nfrom statsmodels.tsa.stattools import pacf, acf\n\n\n\n# plotly settings\nplotly_base_params = {\n    'template': \"plotly_white\",\n    'title_font': dict(size=29, color='#8a8d93', family=\"Lato, sans-serif\"),\n    'font': dict(color='#8a8d93'), \n    'hoverlabel': dict(bgcolor=\"#f2f2f2\", font_size=13, font_family=\"Lato, sans-serif\")\n}","metadata":{"execution":{"iopub.status.busy":"2022-05-27T20:58:00.360529Z","iopub.execute_input":"2022-05-27T20:58:00.361204Z","iopub.status.idle":"2022-05-27T20:58:02.988775Z","shell.execute_reply.started":"2022-05-27T20:58:00.361113Z","shell.execute_reply":"2022-05-27T20:58:02.987281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DATA_DIR = \"/kaggle/input/store-sales-time-series-forecasting\"","metadata":{"execution":{"iopub.status.busy":"2022-05-27T20:58:02.993034Z","iopub.execute_input":"2022-05-27T20:58:02.993494Z","iopub.status.idle":"2022-05-27T20:58:02.997341Z","shell.execute_reply.started":"2022-05-27T20:58:02.993457Z","shell.execute_reply":"2022-05-27T20:58:02.996653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv(os.path.join(DATA_DIR, \"train.csv\"), parse_dates=['date'])\ntest = pd.read_csv(os.path.join(DATA_DIR, \"test.csv\"), parse_dates=['date'])\noil = pd.read_csv(os.path.join(DATA_DIR, \"oil.csv\"), parse_dates=['date'])\nholidays_events = pd.read_csv(os.path.join(DATA_DIR, \"holidays_events.csv\"), parse_dates=['date'])\ntransactions = pd.read_csv(os.path.join(DATA_DIR, \"transactions.csv\"), parse_dates=['date'])\nstores = pd.read_csv(os.path.join(DATA_DIR, \"stores.csv\"))\n\nprint(f\"Training Data: from {train.date.min()} to {train.date.max()} - {train.date.max() - train.date.min()}\")\nprint(f\"Testing Data: from {test.date.min()} to {test.date.max()} - {test.date.max() - test.date.min()}\")","metadata":{"execution":{"iopub.status.busy":"2022-05-27T20:58:02.998311Z","iopub.execute_input":"2022-05-27T20:58:02.999202Z","iopub.status.idle":"2022-05-27T20:58:06.814314Z","shell.execute_reply.started":"2022-05-27T20:58:02.999167Z","shell.execute_reply":"2022-05-27T20:58:06.813126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train.info())\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-27T20:58:06.816618Z","iopub.execute_input":"2022-05-27T20:58:06.816952Z","iopub.status.idle":"2022-05-27T20:58:06.847498Z","shell.execute_reply.started":"2022-05-27T20:58:06.816914Z","shell.execute_reply":"2022-05-27T20:58:06.84645Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(test.info())\ntest.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-27T20:58:06.849225Z","iopub.execute_input":"2022-05-27T20:58:06.849711Z","iopub.status.idle":"2022-05-27T20:58:06.877215Z","shell.execute_reply.started":"2022-05-27T20:58:06.849664Z","shell.execute_reply":"2022-05-27T20:58:06.876263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# calendar dataset covering train + test dates\ncalendar = pd.DataFrame(index=pd.date_range(train.date.min(), test.date.max()))\n# days of week\ncalendar['weekday'] = calendar.index.dayofweek ","metadata":{"execution":{"iopub.status.busy":"2022-05-27T20:58:06.878655Z","iopub.execute_input":"2022-05-27T20:58:06.878989Z","iopub.status.idle":"2022-05-27T20:58:06.902419Z","shell.execute_reply.started":"2022-05-27T20:58:06.878959Z","shell.execute_reply":"2022-05-27T20:58:06.901436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"EDA\"></a>\n# EDA","metadata":{}},{"cell_type":"code","source":"# Extend training set for the EDA\ntrain_ext = train.merge(stores, on='store_nbr', how='left')\ntrain_ext = train_ext.merge(transactions, on=['date', 'store_nbr'], how='left')\ntrain_ext = train_ext.rename(columns={\"type\": \"store_type\"})","metadata":{"execution":{"iopub.status.busy":"2022-05-27T20:58:06.903776Z","iopub.execute_input":"2022-05-27T20:58:06.904143Z","iopub.status.idle":"2022-05-27T20:58:08.696215Z","shell.execute_reply.started":"2022-05-27T20:58:06.904111Z","shell.execute_reply":"2022-05-27T20:58:08.695305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Parsing dates\ntrain_ext['date'] = train_ext['date'].astype('datetime64[ns]')\ntrain_ext['year'] = train_ext['date'].dt.year\ntrain_ext['month'] = train_ext['date'].dt.month\ntrain_ext['week'] = train_ext['date'].dt.isocalendar().week\ntrain_ext['quarter'] = train_ext['date'].dt.quarter\ntrain_ext['weekday'] = train_ext['date'].dt.dayofweek\ntrain_ext['day_name'] = train_ext['date'].dt.day_name()","metadata":{"execution":{"iopub.status.busy":"2022-05-27T20:58:08.697542Z","iopub.execute_input":"2022-05-27T20:58:08.697915Z","iopub.status.idle":"2022-05-27T20:58:11.909602Z","shell.execute_reply.started":"2022-05-27T20:58:08.697883Z","shell.execute_reply":"2022-05-27T20:58:11.908683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Extended training data:\", train_ext.shape)\ntrain_ext.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-27T20:58:11.910737Z","iopub.execute_input":"2022-05-27T20:58:11.911053Z","iopub.status.idle":"2022-05-27T20:58:11.932992Z","shell.execute_reply.started":"2022-05-27T20:58:11.911027Z","shell.execute_reply":"2022-05-27T20:58:11.931981Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"sales\"></a>\n## Sales","metadata":{}},{"cell_type":"code","source":"def print_summary(data:dict, title):\n    fig=go.Figure()\n    fig.add_trace(go.Scatter(\n        x=np.arange(start=0, stop=len(data)),\n        y=np.full(len(data), 1.6),\n        mode=\"text\", \n        text=[f\"<span style='font-size:33px'><b>{data[x]}</b></span>\" for x in data],\n        textposition=\"bottom center\",\n        hoverinfo='skip'\n    ))\n    fig.add_trace(go.Scatter(\n        x=np.arange(start=0, stop=len(data)),\n        y=np.full(len(data), 1.1),\n        mode=\"text\", \n        text=[x for x in data],\n        textposition=\"bottom center\",\n        hoverinfo='skip'\n    ))\n    fig.add_hline(y=2.2, line_width=5, line_color='gray')\n    fig.add_hline(y=0.3, line_width=3, line_color='gray')\n    fig.update_yaxes(visible=False)\n    fig.update_xaxes(visible=False)\n    fig.update_layout(\n        showlegend=False, height=300, width=1200,\n        title=title, title_x=0.5, title_y=0.9,\n        yaxis_range=[-0.2,2.2],\n        plot_bgcolor='#fafafa', paper_bgcolor='#fafafa',\n        font=dict(size=23, color='#323232'),\n        title_font=dict(size=35, color='#222'),\n        margin=dict(t=90,l=70,b=0,r=70)\n    )\n    fig.show(config={'staticPlot': False})","metadata":{"execution":{"iopub.status.busy":"2022-05-27T20:58:11.936538Z","iopub.execute_input":"2022-05-27T20:58:11.937032Z","iopub.status.idle":"2022-05-27T20:58:11.952678Z","shell.execute_reply.started":"2022-05-27T20:58:11.936986Z","shell.execute_reply":"2022-05-27T20:58:11.951555Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# compute number of months\nstart_date = train_ext.date.min()\nend_date = train_ext.date.max()\nnb_months = round((end_date.year - start_date.year) * 12 + (end_date.month - start_date.month) + (end_date.day / 30.5), 1)\n\nsummary = {\n    \"Stores\": stores.shape[0],\n    \"Store types\": train_ext.store_type.nunique(),\n    \"Store clusters\": train_ext.cluster.nunique(),\n    \"Product families\": train_ext.family.nunique(),\n    \"States\": train_ext.state.nunique(),\n    \"Months\": nb_months\n}\nprint_summary(summary, \"Stores Summary\")","metadata":{"execution":{"iopub.status.busy":"2022-05-27T20:58:11.954036Z","iopub.execute_input":"2022-05-27T20:58:11.954698Z","iopub.status.idle":"2022-05-27T20:58:12.827661Z","shell.execute_reply.started":"2022-05-27T20:58:11.954651Z","shell.execute_reply":"2022-05-27T20:58:12.826779Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# data\ndf_sales = train.groupby('date').agg({\"sales\" : \"sum\"}).reset_index()\ndf_sales['sales_ma'] = df_sales['sales'].rolling(7).mean()\ndf_trans = train_ext.groupby('date').agg({\"transactions\" : \"sum\"}).reset_index()\ndf_trans['transactions_ma'] = df_trans['transactions'].rolling(7).mean()\n\n# chart\nfig = make_subplots(rows=3, cols=1,\n                    subplot_titles=[\"Sales\", \"Transactions\", \"Sales / Transactions\"],\n                    vertical_spacing=.1)\nfig.add_scatter(x=df_sales['date'], y=df_sales['sales'],\n                mode='lines', marker=dict(color='#428bca'),\n                name='Sales', row=1, col=1)\nfig.add_scatter(x=df_sales['date'], y=df_sales['sales_ma'],\n                mode='lines', marker=dict(color='#d9534f'),\n                name='7d moving avearge', row=1, col=1)\nfig.add_scatter(x=df_trans['date'], y=df_trans['transactions'],\n                mode='lines', marker=dict(color='#428bca'),\n                name='Transactions', row=2, col=1)\nfig.add_scatter(x=df_trans['date'], y=df_trans['transactions_ma'],\n                mode='lines', marker=dict(color='#d9534f'),\n                name='7d moving avearge', row=2, col=1)\nfig.add_scatter(x=df_sales['sales'], y=df_trans['transactions'],\n                mode='markers', marker=dict(color='#428bca', size=2),\n                name='Sales/Transactions', row=3, col=1)\n# style\nfig.update_xaxes(title='Sales', row=3, col=1)\nfig.update_yaxes(title='Transactions', row=3, col=1)\nfig.update_layout(height=750, width=1200, showlegend=False, **plotly_base_params)\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-27T20:58:12.828826Z","iopub.execute_input":"2022-05-27T20:58:12.829257Z","iopub.status.idle":"2022-05-27T20:58:14.247261Z","shell.execute_reply.started":"2022-05-27T20:58:12.829226Z","shell.execute_reply":"2022-05-27T20:58:14.246443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# data c1\ndf_st_sa = train_ext.groupby('store_type').agg({\"sales\" : \"mean\"}).sort_values(by='sales', ascending=False).reset_index()\n# data c2\ndf_fa_sa = train_ext.groupby('family').agg({\"sales\" : \"mean\"}).sort_values(by='sales', ascending=False)[:10].reset_index()\ndf_fa_sa['percent'] = round((df_fa_sa['sales'] / df_fa_sa['sales'].sum()) * 100, 1)\ndf_fa_sa['percent'] = df_fa_sa['percent'].astype(str) + \"%\"\ndf_fa_sa['color'] = '#c6ccd8'\ndf_fa_sa['color'].at[df_fa_sa.sales.idxmax()] = '#496595' # highest value color\n# data c3\ndf_cl_sa = train_ext.groupby('cluster').agg({\"sales\" : \"mean\"}).reset_index()\ndf_cl_sa['percent'] = round((df_cl_sa['sales'] / df_cl_sa['sales'].sum()) * 100, 1)\ndf_cl_sa['percent'] = df_cl_sa['percent'].astype(str) + \"%\"\ndf_cl_sa['color'] = '#c6ccd8'\ndf_cl_sa['color'].at[df_cl_sa.sales.idxmax()] = '#496595'\n\n# charts\nfig = make_subplots(rows=2, cols=2,\n                    column_widths=[0.5, 0.5], vertical_spacing=0, horizontal_spacing=0.02,\n                    specs=[[{\"type\": \"bar\"}, {\"type\": \"pie\"}], [{\"colspan\": 2}, None]],\n                    subplot_titles=(\"Top 10 Highest Product Sales\", \"Sales per Store Types\", \"Sales per Clusters\"))\nfig.add_trace(go.Bar(x=df_fa_sa['sales'], y=df_fa_sa['family'], text=df_fa_sa['percent'],\n                     marker=dict(color=df_fa_sa['color']), name='Family', orientation='h'),\n              row=1, col=1)\nfig.add_trace(go.Pie(values=df_st_sa['sales'], labels=df_st_sa['store_type'],\n                     name='Store type', hole=0.7,\n                     marker=dict(colors=['#334668','#496595','#6D83AA','#91A2BF','#C8D0DF']),\n                     hoverinfo='label+percent+value', textinfo='label'),\n              row=1, col=2)\nfig.add_trace(go.Bar(x=df_cl_sa['cluster'], y=df_cl_sa['sales'], text=df_cl_sa['percent'],\n                     marker=dict(color=df_cl_sa['color']), name='Cluster'), \n              row=2, col=1)\n\n# styling\nfig.update_yaxes(showgrid=False, ticksuffix=' ', categoryorder='total ascending', row=1, col=1)\nfig.update_xaxes(visible=False, row=1, col=1)\nfig.update_xaxes(tickmode = 'array', tickvals=df_cl_sa.cluster, ticktext=[i for i in range(1,17)], row=2, col=1)\nfig.update_yaxes(visible=False, row=2, col=1)\nfig.update_layout(height=500, width=1000, bargap=0.2,\n                  margin=dict(b=0,r=20,l=20), xaxis=dict(tickmode='linear'),\n                  title_text=\"Average Sales Analysis\",\n                  showlegend=False, **plotly_base_params)\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-27T20:58:14.248703Z","iopub.execute_input":"2022-05-27T20:58:14.249015Z","iopub.status.idle":"2022-05-27T20:58:14.940559Z","shell.execute_reply.started":"2022-05-27T20:58:14.248987Z","shell.execute_reply":"2022-05-27T20:58:14.939447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Groceries and beverages account for more than half of these stores' sales.","metadata":{}},{"cell_type":"code","source":"# data\ndf_m_sa = train_ext.groupby('month').agg({\"sales\" : \"mean\"}).reset_index()\ndf_m_sa['sales'] = round(df_m_sa['sales'], 2)\ndf_m_sa['month_text'] = df_m_sa['month'].apply(lambda x: cal.month_abbr[x])\ndf_m_sa['text'] = df_m_sa['month_text'] + ' - ' + df_m_sa['sales'].astype(str)\ndf_m_sa['color'] = '#c6ccd8'\ndf_m_sa['color'].at[df_m_sa.sales.idxmax()] = '#496595'\n\ndf_dw_sa = train_ext.groupby('weekday').agg({\"sales\" : \"mean\"}).reset_index()\ndf_dw_sa.sales = round(df_dw_sa.sales, 2)\ndf_dw_sa['day_name'] = df_dw_sa['weekday'].apply(lambda x: cal.day_name[x])\ndf_dw_sa['text'] = df_dw_sa['day_name'] + ' - ' + df_m_sa['sales'].astype(str)\ndf_dw_sa['color'] = '#c6ccd8'\ndf_dw_sa['color'].at[df_dw_sa.sales.idxmax()] = '#496595'\n\ndf_w_sa = train_ext.groupby('week').agg({\"sales\" : \"mean\"}).reset_index()\ndf_q_sa = train_ext.groupby('quarter').agg({\"sales\" : \"mean\"}).reset_index()\ndf_w_sa['color'] = '#c6ccd8'\n\n# chart\nfig = make_subplots(rows=2, cols=3,\n                    vertical_spacing=0.08,\n                    row_heights=[0.7, 0.3],\n                    specs=[[{\"type\": \"bar\"}, {\"type\": \"pie\"}, {\"type\": \"bar\"}], [{\"colspan\": 3}, None, None]],\n                    subplot_titles=(\"Month wise Avg Sales Analysis\",\n                                    \"Quarter wise Avg Sales Analysis\",\n                                    \"Day wise Avg Sales Analysis\",\n                                    \"Week wise Avg Sales Analysis\"))\n# Row 1\nfig.add_trace(go.Bar(x=df_m_sa['sales'], y=df_m_sa['month'][::-1],\n                     marker=dict(color=df_m_sa['color']),\n                     text=df_m_sa['text'], textposition='auto',\n                     name='Month', orientation='h'), row=1, col=1)\nfig.add_trace(go.Pie(values=df_q_sa['sales'], labels=df_q_sa['quarter'], name='Quarter',\n                     marker=dict(colors=['#334668','#496595','#6D83AA','#91A2BF','#C8D0DF']),\n                     hole=0.7, hoverinfo='label+percent+value', textinfo='label+percent'), row=1, col=2)\nfig.add_trace(go.Bar(x=df_dw_sa['sales'], y=df_dw_sa['weekday'][::-1],\n                     marker=dict(color=df_dw_sa['color']),\n                     text=df_dw_sa['text'], textposition='auto',\n                     name='Day', orientation='h'), row=1, col=3)\n# Row 2\nfig.add_trace(go.Scatter(x=df_w_sa['week'], y=df_w_sa['sales'],\n                         mode='lines+markers', fill='tozeroy', fillcolor='#c6ccd8',\n                         marker=dict(color='#496595'), name='Week'), row=2, col=1)\n\n# styling\nfig.update_yaxes(visible=False, row=1, col=1)\nfig.update_yaxes(visible=False, row=1, col=3)\nfig.update_xaxes(tickmode = 'array', tickvals=df_w_sa.week, ticktext=[i for i in range(1,53)], \n                 row=2, col=1)\nfig.update_layout(height=500, width=1000, bargap=0.15,\n                  margin=dict(b=0,r=20,l=20), \n                  title_text=\"Average Sales Analysis Over Time\",\n                  showlegend=False, **plotly_base_params)\nfig.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-27T20:58:14.941845Z","iopub.execute_input":"2022-05-27T20:58:14.942171Z","iopub.status.idle":"2022-05-27T20:58:15.341232Z","shell.execute_reply.started":"2022-05-27T20:58:14.942141Z","shell.execute_reply":"2022-05-27T20:58:15.340195Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Sales are more important on Sundays and for the Christmas holidays.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"seasonality\"></a>\n### Seasonality","metadata":{"_kg_hide-input":true}},{"cell_type":"code","source":"# source: https://gist.github.com/tomron/8798256fcee5438edd58c17654adf443\ndef plot_seasonal_decompose(result: DecomposeResult, title=\"Seasonal Decomposition\"):\n    return (\n        make_subplots(rows=4, cols=1, subplot_titles=[\"Observed\", \"Trend\", \"Seasonal\", \"Residuals\"])\n        .add_trace(go.Scatter(x=result.seasonal.index, y=result.observed, mode=\"lines\", name=\"Observed\"),\n                   row=1, col=1)\n        .add_trace(go.Scatter(x=result.trend.index, y=result.trend, mode=\"lines\", name=\"Trend\"),\n                   row=2, col=1)\n        .add_trace(go.Scatter(x=result.seasonal.index, y=result.seasonal, mode=\"lines\", name=\"Seasonal\"),\n                   row=3, col=1)\n        .add_trace(go.Scatter(x=result.resid.index, y=result.resid, mode=\"lines\", name=\"Residuals\"),\n                   row=4, col=1)\n        .update_layout(template=\"plotly_white\", height=1250, width=1000, title=title, margin=dict(t=100), title_x=0.5, showlegend=False)\n    )","metadata":{"execution":{"iopub.status.busy":"2022-05-27T20:58:15.342958Z","iopub.execute_input":"2022-05-27T20:58:15.343904Z","iopub.status.idle":"2022-05-27T20:58:15.358027Z","shell.execute_reply.started":"2022-05-27T20:58:15.343851Z","shell.execute_reply":"2022-05-27T20:58:15.357006Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seasonnal = seasonal_decompose(df_sales.set_index('date')['sales'], model='multiplicative', period=365)\nplot_seasonal_decompose(seasonnal, title=\"Annual Seasonal Decomposition of Sales\")","metadata":{"execution":{"iopub.status.busy":"2022-05-27T20:58:15.359469Z","iopub.execute_input":"2022-05-27T20:58:15.360416Z","iopub.status.idle":"2022-05-27T20:58:15.733908Z","shell.execute_reply.started":"2022-05-27T20:58:15.360353Z","shell.execute_reply":"2022-05-27T20:58:15.732702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def seasonal_plot(X, y, period, freq, ax=None):\n    if ax is None:\n        _, ax = plt.subplots()\n    palette = sns.color_palette(\"husl\", n_colors=X[period].nunique(),)\n    ax = sns.lineplot(\n        x=freq, y=y, data=X, hue=period, ci=False,\n        ax=ax, palette=palette, legend=False,\n    )\n    ax.set_title(f\"Seasonal Plot ({period}/{freq})\")\n    for line, name in zip(ax.lines, X[period].unique()):\n        y_ = line.get_ydata()[-1]\n        ax.annotate(\n            name, xy=(1, y_), xytext=(6, 0),\n            color=line.get_color(),\n            xycoords=ax.get_yaxis_transform(),\n            textcoords=\"offset points\", size=14, va=\"center\",\n        )\n    return ax\n\ndef plot_periodogram(ts, detrend='linear', ax=None):\n    # fs = (exact value of deprecated '1Y') / '1D'\n    fs = pd.Timedelta('365 days 05:49:12') / pd.Timedelta(\"1D\")\n    freqencies, spectrum = periodogram(\n        ts, fs=fs,\n        detrend=detrend,\n        window=\"boxcar\",\n        scaling='spectrum',\n    )\n    if ax is None:\n        _, ax = plt.subplots()\n    ax.step(freqencies, spectrum, color=\"purple\")\n    ax.set_xscale(\"log\")\n    ax.set_xticks([1, 2, 4, 6, 12, 26, 52, 104])\n    ax.set_xticklabels([\n        \"Annual (1)\",\n        \"Semiannual (2)\",\n        \"Quarterly (4)\",\n        \"Bimonthly (6)\",\n        \"Monthly (12)\",\n        \"Biweekly (26)\",\n        \"Weekly (52)\",\n        \"Semiweekly (104)\",\n        ], rotation=30)\n    ax.ticklabel_format(axis=\"y\", style=\"sci\", scilimits=(0, 0))\n    ax.set_ylabel(\"Variance\")\n    ax.set_title(\"Periodogram\")\n    return ax","metadata":{"execution":{"iopub.status.busy":"2022-05-27T20:58:15.73552Z","iopub.execute_input":"2022-05-27T20:58:15.735959Z","iopub.status.idle":"2022-05-27T20:58:15.756943Z","shell.execute_reply.started":"2022-05-27T20:58:15.73592Z","shell.execute_reply":"2022-05-27T20:58:15.755909Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_sales = df_sales.set_index('date').to_period(\"D\")\n# days within a week\ndf_sales['day'] = df_sales.index.dayofweek # the x-axis (freq)\ndf_sales['week'] = df_sales.index.week # the seasonal period (period)\n\n# days within a year\ndf_sales['dayofyear'] = df_sales.index.dayofyear\ndf_sales['year'] = df_sales.index.year\ndf_sales['month'] = df_sales.index.month","metadata":{"execution":{"iopub.status.busy":"2022-05-27T20:58:15.758236Z","iopub.execute_input":"2022-05-27T20:58:15.758688Z","iopub.status.idle":"2022-05-27T20:58:15.775111Z","shell.execute_reply.started":"2022-05-27T20:58:15.758644Z","shell.execute_reply":"2022-05-27T20:58:15.774414Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, (ax0, ax1) = plt.subplots(2, 1, figsize=(15, 10))\nseasonal_plot(df_sales, y=\"sales\", period=\"week\", freq=\"day\", ax=ax0)\nseasonal_plot(df_sales, y=\"sales\", period=\"year\", freq=\"dayofyear\", ax=ax1)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-27T20:58:15.776652Z","iopub.execute_input":"2022-05-27T20:58:15.77752Z","iopub.status.idle":"2022-05-27T20:58:27.709221Z","shell.execute_reply.started":"2022-05-27T20:58:15.777475Z","shell.execute_reply":"2022-05-27T20:58:27.708207Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(10, 5))\nax = plot_periodogram(df_sales[\"sales\"], ax=ax)\nax.set_title(\"Product Sales Frequency Components\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-27T20:58:27.711007Z","iopub.execute_input":"2022-05-27T20:58:27.711632Z","iopub.status.idle":"2022-05-27T20:58:28.108346Z","shell.execute_reply.started":"2022-05-27T20:58:27.711588Z","shell.execute_reply":"2022-05-27T20:58:28.107564Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = train.groupby(['date', 'family']).agg({\"sales\" : \"sum\"})\ndf = df.unstack(level=0).T.droplevel(level=0, axis=0).rolling(7).mean()\nfig =  go.Figure()\nfor col in df.columns:\n    fig.add_trace(go.Scatter(x=df.index, y=df[col], name=col, mode='lines'))\nfig.update_layout(height=850,\n                  title_text=\"Weekly moving average per product Family\",\n                  **plotly_base_params)\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-27T20:58:28.109401Z","iopub.execute_input":"2022-05-27T20:58:28.110105Z","iopub.status.idle":"2022-05-27T20:58:30.192477Z","shell.execute_reply.started":"2022-05-27T20:58:28.110062Z","shell.execute_reply":"2022-05-27T20:58:30.191136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It is possible to choose the curves to be displayed by clicking on the product families. (double click on a family to display only this family)\n\nWe can clearly see different trends according to the product families.  \nThere are also sales that appear to be stable in periodicity but undergo large increases/decreases (while maintaining the same periodicity). This may be the result of the conglomerate's desire to increase/decrease its sales of this type of product. For example: DAIRY\n\nThe sales of the following products are much lower than the other product families: 'AUTOMOTIVE', 'BEAUTY', 'CELEBRATION', 'GROCERY II', 'HARDWARE', 'HOME AND KITCHEN I', 'HOME AND KITCHEN II', 'HOME APPLIANCES', 'LADIESWEAR', 'LAWN AND GARDEN', 'LINGERIE', 'LIQUOR,WINE,BEER', 'MAGAZINES', 'PET SUPPLIES', 'PLAYERS AND ELECTRONICS', 'SCHOOL AND OFFICE SUPPLIES', 'SEAFOOD'","metadata":{}},{"cell_type":"markdown","source":"<a id=\"oil\"></a>\n## Oil Price","metadata":{}},{"cell_type":"code","source":"# compute 7-days moving average\noil['ma_oil'] = oil['dcoilwtico'].rolling(7).mean()\noil = oil.set_index('date')\n\n# adding oil price to calendar\ncalendar['ma_oil'] = oil['ma_oil'].loc[train.date.min():test.date.max()]\ncalendar['ma_oil'].fillna(method='ffill', inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-27T20:58:30.193989Z","iopub.execute_input":"2022-05-27T20:58:30.194454Z","iopub.status.idle":"2022-05-27T20:58:30.224296Z","shell.execute_reply.started":"2022-05-27T20:58:30.194413Z","shell.execute_reply":"2022-05-27T20:58:30.223185Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# chart\nfig = go.Figure()\nfig.add_scatter(x=oil.index, y=oil.dcoilwtico,\n                mode='lines', name='Oil Price',\n                line=dict(color='#428bca', width=2))\nfig.add_scatter(x=oil.index, y=oil.ma_oil.fillna(method='ffill'),\n                mode='lines', name='7d moving avergae',\n                line=dict(color='purple', width=1))\nfig.update_layout(title='Avg Sales with Holydays and Events', **plotly_base_params)\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-27T20:58:30.225899Z","iopub.execute_input":"2022-05-27T20:58:30.226841Z","iopub.status.idle":"2022-05-27T20:58:30.381734Z","shell.execute_reply.started":"2022-05-27T20:58:30.22679Z","shell.execute_reply":"2022-05-27T20:58:30.380589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_ext = train_ext.merge(oil, on='date', how='left')\n\nfig = px.imshow(train_ext[['ma_oil', 'sales', 'transactions']].corr(), color_continuous_scale='reds')\nfig.update_layout(title='Correlation between Oil / Sales / Transactions', height=400, width=700, **plotly_base_params)","metadata":{"execution":{"iopub.status.busy":"2022-05-27T20:58:30.3832Z","iopub.execute_input":"2022-05-27T20:58:30.383649Z","iopub.status.idle":"2022-05-27T20:58:32.627065Z","shell.execute_reply.started":"2022-05-27T20:58:30.383609Z","shell.execute_reply":"2022-05-27T20:58:32.626082Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Ecuador is an oil-dependent country and it's economical health is highly vulnerable to shocks in oil prices.  \nIt can be seen that the oil price is almost not correlated with the sales of shops that sell mainly food products.","metadata":{}},{"cell_type":"code","source":"print(\"Correlation between oil price and :\")\nfor f in train_ext.family.unique():\n    df = train_ext[train_ext.family == f][['sales', 'ma_oil']].copy()\n    corr = df.corr().unstack().drop_duplicates().unstack()['ma_oil'].values[0]\n    print(f\" - {f}: {corr:.3f}\")","metadata":{"execution":{"iopub.status.busy":"2022-05-27T20:58:32.628165Z","iopub.execute_input":"2022-05-27T20:58:32.628504Z","iopub.status.idle":"2022-05-27T20:58:48.822144Z","shell.execute_reply.started":"2022-05-27T20:58:32.628475Z","shell.execute_reply":"2022-05-27T20:58:48.821156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Oil Lag Features**  \n\nACF is a measure of the correlation between the timeseries with a lagged version of itself. For instance at lag 5, ACF would compare series at time instant ‚Äòt1‚Äô‚Ä¶‚Äôtn‚Äô with series at instant ‚Äòt1-5‚Äô‚Ä¶‚Äôtn-5‚Äô (t1-5 and tn being end points.  \nPACF, also measures the correlation between the timeseries with a lagged version of itself but after eliminating the variations explained by the intervening comparisons. Eg. at lag 5, it will check the correlation but remove the effects already explained by lags 1 to 4.","metadata":{}},{"cell_type":"code","source":"def plot_acf_pacf(series, n_lags=20, plot_pacf=False):\n    corr_array = pacf(series.dropna(), alpha=0.05, nlags=n_lags) if plot_pacf else acf(series.dropna(), alpha=0.05, fft=False, nlags=n_lags)\n    lower_y = corr_array[1][:,0] - corr_array[0]\n    upper_y = corr_array[1][:,1] - corr_array[0]\n    fig = go.Figure()\n    [fig.add_scatter(x=(x,x), y=(0,corr_array[0][x]), mode='lines',line_color='#3f3f3f') \n     for x in range(len(corr_array[0]))]\n    fig.add_scatter(x=np.arange(len(corr_array[0])), y=corr_array[0],\n                    mode='markers', marker_color='#1f77b4', marker_size=12)\n    fig.add_scatter(x=np.arange(len(corr_array[0])), y=upper_y,\n                    mode='lines', line_color='rgba(255,255,255,0)')\n    fig.add_scatter(x=np.arange(len(corr_array[0])), y=lower_y,\n                    mode='lines',fillcolor='rgba(32, 146, 230,0.3)',\n                    fill='tonexty', line_color='rgba(255,255,255,0)',\n                    name='No-correlation interval')\n    fig.update_traces(showlegend=False)\n    fig.update_xaxes(range=[-1,n_lags+1])\n    fig.update_yaxes(zerolinecolor='#000000')\n    title='Partial Autocorrelation (PACF)' if plot_pacf else 'Autocorrelation (ACF)'\n    fig.update_layout(title=title, height=500, width=1000)\n    fig.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-27T20:58:48.823501Z","iopub.execute_input":"2022-05-27T20:58:48.823811Z","iopub.status.idle":"2022-05-27T20:58:48.836596Z","shell.execute_reply.started":"2022-05-27T20:58:48.823784Z","shell.execute_reply":"2022-05-27T20:58:48.835639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_lags(ts, lags):\n    return pd.concat(\n        {f'{ts.name}_lag_{i}': ts.shift(i) for i in range(1, lags + 1)},\n        axis=1\n    )","metadata":{"execution":{"iopub.status.busy":"2022-05-27T20:58:48.838192Z","iopub.execute_input":"2022-05-27T20:58:48.838551Z","iopub.status.idle":"2022-05-27T20:58:48.84994Z","shell.execute_reply.started":"2022-05-27T20:58:48.838518Z","shell.execute_reply":"2022-05-27T20:58:48.849203Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_acf_pacf(calendar['ma_oil'].dropna(), plot_pacf=False)\nplot_acf_pacf(calendar['ma_oil'].dropna(), plot_pacf=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-27T20:58:48.853409Z","iopub.execute_input":"2022-05-27T20:58:48.854182Z","iopub.status.idle":"2022-05-27T20:58:48.977838Z","shell.execute_reply.started":"2022-05-27T20:58:48.854142Z","shell.execute_reply":"2022-05-27T20:58:48.976611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# add lagged oil features to calendar\noil_lags = make_lags(calendar['ma_oil'], 4).fillna(method='ffill')\ncalendar = calendar.join(oil_lags)","metadata":{"execution":{"iopub.status.busy":"2022-05-27T20:58:48.97998Z","iopub.execute_input":"2022-05-27T20:58:48.980925Z","iopub.status.idle":"2022-05-27T20:58:48.993016Z","shell.execute_reply.started":"2022-05-27T20:58:48.980863Z","shell.execute_reply":"2022-05-27T20:58:48.991595Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"events\"></a>\n## Holidays / Events","metadata":{}},{"cell_type":"code","source":"mask = holidays_events.description=='Viernes Santo'\nholidays_events[mask]","metadata":{"execution":{"iopub.status.busy":"2022-05-27T20:58:48.995317Z","iopub.execute_input":"2022-05-27T20:58:48.996258Z","iopub.status.idle":"2022-05-27T20:58:49.020768Z","shell.execute_reply.started":"2022-05-27T20:58:48.996196Z","shell.execute_reply":"2022-05-27T20:58:49.019687Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There is an error in the events/holidays dataset. According to the [2013 list of holidays in Ecuador](https://www.turismo.gob.ec/wp-content/uploads/downloads/2014/01/Feriados-20131.pdf), Good Friday was March 29th, not April 29th.","metadata":{}},{"cell_type":"code","source":"# 'Good Friday' mistake correction\nholidays_events['date'][mask].replace({'2013-04-29': pd.to_datetime('2013-03-29')}, inplace=True)\nholidays_events = holidays_events.set_index('date').sort_index()\n# keep National level only for simplicity\nholidays_events = holidays_events[holidays_events.locale=='National']\n# keep only one event per day\nholidays_events = holidays_events.groupby(holidays_events.index).first()","metadata":{"execution":{"iopub.status.busy":"2022-05-27T20:58:49.022725Z","iopub.execute_input":"2022-05-27T20:58:49.023703Z","iopub.status.idle":"2022-05-27T20:58:49.042459Z","shell.execute_reply.started":"2022-05-27T20:58:49.023651Z","shell.execute_reply":"2022-05-27T20:58:49.041304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# data\navg_sales = train.groupby('date').agg({\"sales\" : \"mean\"}).reset_index()\ndf_s_he = avg_sales.merge(holidays_events, on='date', how='left')\ndf_s_he = df_s_he.rename(columns={\"type\": \"event_type\"})\n\n\n# chart\nfig = go.Figure()\nfig.add_scatter(x=df_s_he.date, y=df_s_he.sales,\n                mode='lines', name='Avg Sales', line=dict(width=.5))\nfig.add_scatter(x=df_s_he['date'][df_s_he.event_type=='Holiday'],\n                y=df_s_he['sales'][df_s_he.event_type=='Holiday'],\n                mode='markers', name='Holidays',\n                marker=dict(color='orange', size=4))\nfig.add_scatter(x=df_s_he['date'][df_s_he.event_type=='Event'],\n                y=df_s_he['sales'][df_s_he.event_type=='Event'],\n                mode='markers', name='Events',\n                marker=dict(color='purple', size=5))\nfig.update_layout(title='Avg Sales with Holydays and Events',\n                  height=500, width=1000, **plotly_base_params)\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-27T12:33:35.729833Z","iopub.execute_input":"2022-05-27T12:33:35.730208Z","iopub.status.idle":"2022-05-27T12:33:35.891325Z","shell.execute_reply.started":"2022-05-27T12:33:35.730177Z","shell.execute_reply":"2022-05-27T12:33:35.89031Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- New Year's Day is the only day when all stores are closed\n- The earthquake in 2016 led to an increase in sales","metadata":{}},{"cell_type":"code","source":"# correction of workdays using holidays/events dates\n\ndef compute_workdays(df, dofw_col):\n    df['workday'] = True\n    # exclude week-ends\n    df.loc[df[dofw_col] > 4, 'workday'] = False\n    # friday bridges are not working days\n    df.loc[df.event_type=='Bridge', 'workday'] = False\n    # some bridges are recovered by working at weekends\n    df.loc[df.event_type=='Work Day', 'workday'] = True\n    # handling Transfered events\n    df.loc[df.event_type=='Transfer', 'workday'] = False\n    df.loc[(df.event_type=='Holiday')&(df.transferred==False), 'workday'] = False\n    df.loc[(df.event_type=='Holiday')&(df.transferred==True ), 'workday'] = True\n    return df                 ","metadata":{"execution":{"iopub.status.busy":"2022-05-27T12:33:35.892526Z","iopub.execute_input":"2022-05-27T12:33:35.892837Z","iopub.status.idle":"2022-05-27T12:33:35.900164Z","shell.execute_reply.started":"2022-05-27T12:33:35.892809Z","shell.execute_reply":"2022-05-27T12:33:35.899247Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# holidays / events\ncalendar = calendar.merge(holidays_events, how='left', left_index=True, right_index=True)\ncalendar = calendar.rename(columns={\"type\": \"event_type\"})\n# days of work\ncalendar = compute_workdays(calendar, 'weekday')\ncalendar['workday'] = calendar['workday'] * 1\ncalendar.drop(columns=['locale', 'locale_name', 'description', 'transferred'], inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-27T12:33:35.901362Z","iopub.execute_input":"2022-05-27T12:33:35.901675Z","iopub.status.idle":"2022-05-27T12:33:35.926265Z","shell.execute_reply.started":"2022-05-27T12:33:35.901648Z","shell.execute_reply":"2022-05-27T12:33:35.925361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"forecasting\"></a>\n# Machine Learning Forecasting","metadata":{}},{"cell_type":"code","source":"import warnings\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.metrics import make_scorer, r2_score, mean_squared_error\nfrom sklearn.linear_model import Ridge, Lasso, LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import GridSearchCV\nfrom xgboost import XGBRegressor\nfrom joblib import Parallel, delayed","metadata":{"execution":{"iopub.status.busy":"2022-05-27T12:33:35.927489Z","iopub.execute_input":"2022-05-27T12:33:35.927807Z","iopub.status.idle":"2022-05-27T12:33:36.39214Z","shell.execute_reply.started":"2022-05-27T12:33:35.927779Z","shell.execute_reply":"2022-05-27T12:33:36.391203Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"preprocessing\"></a>\n## Preprocessing","metadata":{}},{"cell_type":"markdown","source":"Clean re-loading and ordering datasets:\n- exclude columns `id` and `onpromotion`\n- set `store_nbr` and `family` as categorical columns\n- set dates as periods with daily frequency\n- set `store_nbr`, `family` and `date` as index","metadata":{}},{"cell_type":"code","source":"# calendar dates to period\ncalendar.index = calendar.index.to_period('D')","metadata":{"execution":{"iopub.status.busy":"2022-05-27T12:33:36.393304Z","iopub.execute_input":"2022-05-27T12:33:36.393632Z","iopub.status.idle":"2022-05-27T12:33:36.398721Z","shell.execute_reply.started":"2022-05-27T12:33:36.393602Z","shell.execute_reply":"2022-05-27T12:33:36.397717Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# clean reload train set\ntrain = pd.read_csv(os.path.join(DATA_DIR, \"train.csv\"),\n                    usecols=['store_nbr', 'family', 'date', 'sales'], \n                    dtype={'store_nbr': 'category', 'family': 'category', 'sales': 'float32'},\n                    parse_dates=['date'], infer_datetime_format=True)\ntrain['date'] = train.date.dt.to_period('D')\ntrain = train.set_index(['store_nbr', 'family', 'date']).sort_index()\n\n# clean reload test set\ntest = pd.read_csv(os.path.join(DATA_DIR, \"test.csv\"),\n                   usecols=['store_nbr', 'family', 'date'],\n                   dtype={'store_nbr': 'category', 'family': 'category'},\n                   parse_dates=['date'], infer_datetime_format=True)\ntest['date'] = test.date.dt.to_period('D')\ntest = test.set_index(['store_nbr', 'family', 'date']).sort_index()","metadata":{"execution":{"iopub.status.busy":"2022-05-27T12:33:36.399756Z","iopub.execute_input":"2022-05-27T12:33:36.400697Z","iopub.status.idle":"2022-05-27T12:33:40.192535Z","shell.execute_reply.started":"2022-05-27T12:33:36.400619Z","shell.execute_reply":"2022-05-27T12:33:40.191674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The training set is reduced to speed up the search for the model to use. An optimization of the size of the training set will be done afterwards.","metadata":{}},{"cell_type":"code","source":"# trainset dates\ntrain_start_dt = train.index.get_level_values('date').min()\ntrain_end_dt = train.index.get_level_values('date').max()\n\n# testset dates\ntest_start_dt = test.index.get_level_values('date').min()\ntest_end_dt = test.index.get_level_values('date').max()\n\nprint(f\"Initial train set: from {train_start_dt} to {train_end_dt}\")\nprint(f\"Initial test set: from {test_start_dt} to {test_end_dt}\")","metadata":{"execution":{"iopub.status.busy":"2022-05-27T12:33:40.193877Z","iopub.execute_input":"2022-05-27T12:33:40.194744Z","iopub.status.idle":"2022-05-27T12:33:40.245134Z","shell.execute_reply.started":"2022-05-27T12:33:40.194711Z","shell.execute_reply":"2022-05-27T12:33:40.244388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def compute_trainset(dates):\n    # compute seasonnal features\n    fourier = CalendarFourier(freq='W', order=4)\n    dp = DeterministicProcess(index=dates,\n                              constant=False,\n                              order=1,\n                              seasonal=False,\n                              additional_terms=[fourier],\n                              drop=True)\n    X = dp.in_sample()\n    # add calendar features\n    X = X.merge(calendar, how='left', left_index=True, right_index=True)\n    # encode categorical features\n    X = pd.get_dummies(X, columns=['weekday'], drop_first=True)\n    X = pd.get_dummies(X, columns=['event_type'], drop_first=False)\n    # fill missing lagged oil values\n    X = X.fillna(method='bfill')\n    return X, dp\n\n# extract y\ny = train.unstack(['store_nbr', 'family']).loc[train_start_dt:train_end_dt]\ny = np.log1p(y)\nX, dp = compute_trainset(y.index)\n\nprint('trainset shape:', X.shape)","metadata":{"execution":{"iopub.status.busy":"2022-05-27T12:33:40.246167Z","iopub.execute_input":"2022-05-27T12:33:40.246581Z","iopub.status.idle":"2022-05-27T12:33:41.489668Z","shell.execute_reply.started":"2022-05-27T12:33:40.246552Z","shell.execute_reply":"2022-05-27T12:33:41.488715Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nfrom sklearn.model_selection import cross_validate     \ndef compare_models_cv(X_train, y, models:dict, cv:int, metrics:list):\n    \"Returns the mean score of each metrics applied by cross-validation to passed models\"   \n    all_scores = []\n    for m in models:\n        # calculate the model scores for each y\n        partial_scores = cross_validate(models[m], \n                                    X_train, \n                                    y, \n                                    cv=cv,\n                                    scoring=metrics, \n                                    return_train_score=True)\n        # convert to a dataframe\n        partial_scores = pd.DataFrame.from_dict(partial_scores)\n        # get mean score for each metrics and pivot the dataframe\n        partial_scores = partial_scores.mean().to_frame().T\n        # add the model name into the df\n        partial_scores['model'] = m\n        # add scores to the scoreslist\n        all_scores.append(partial_scores)\n    # concat all scores into a single df\n    all_scores = pd.concat(all_scores, ignore_index=True).set_index('model')\n    return all_scores\n\"\"\"\n\ndef gs_tuning(X_train, y_train, pipe, params, scoring, cv=None, abs_scores=True, **kwargs):\n    # init GridSearchCV\n    gs = GridSearchCV(pipe, params, scoring=scoring, cv=cv,\n                      refit=kwargs.get('refit', True),\n                      verbose=kwargs.get('verbose', 0),\n                      return_train_score=True, n_jobs=-1)\n    # fit gridsearch without warnings\n    with warnings.catch_warnings():\n        warnings.simplefilter('ignore')\n        gs.fit(X_train, y_train)\n    # get model name\n    model_name = gs.best_estimator_['model'].__class__.__name__\n    # init results\n    res = {\n        'model_name': model_name,\n        'fit_time': gs.cv_results_['mean_fit_time'][gs.best_index_],\n    }\n    # add train + test scores for each scoring metric\n    if not isinstance(scoring, list) and not isinstance(scoring, dict):\n        metrics = [scoring]\n    elif isinstance(scoring, dict):\n        metrics = scoring.keys()\n    for s in metrics:\n        res['train_' + s] = gs.cv_results_['mean_train_' + s][gs.best_index_]\n        res['test_' + s] = gs.cv_results_['mean_test_' + s][gs.best_index_]\n    # print best score\n    print(f\"{model_name} best score: {gs.best_score_}\")\n    params_display = [f\" - {k}: {v}\" for k,v in gs.best_params_.items() if k != 'model']\n    print('\\n'.join(params_display))\n    return gs.best_estimator_, res\n\ndef print_results(preds, y):\n    # Results of the training stage\n    preds = preds.stack(['store_nbr', 'family']).reset_index()\n    y_target = y.stack(['store_nbr', 'family']).reset_index().copy()\n    y_target['sales_pred'] = preds['sales'].clip(0.) # Sales should be >= 0\n    scores = y_target.groupby('family').apply(lambda r: mean_squared_error(r['sales'], r['sales_pred'], squared=False))\n    print('Scores by Family on training set:')\n    print(scores)\n    print(\"> Average :\", scores.mean())","metadata":{"execution":{"iopub.status.busy":"2022-05-27T12:33:41.522146Z","iopub.execute_input":"2022-05-27T12:33:41.522615Z","iopub.status.idle":"2022-05-27T12:33:41.538542Z","shell.execute_reply.started":"2022-05-27T12:33:41.522586Z","shell.execute_reply":"2022-05-27T12:33:41.537677Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# used scoring mehtods\nscoring = {\n    # using RMSE because y is already passed to log\n    'rmsle': make_scorer(mean_squared_error, greater_is_better=False, squared=False),\n    # 'r2': make_scorer(r2_score, greater_is_better=True)\n}\npipeline = Pipeline([\n    ('scaler', StandardScaler()),\n    ('model', None)\n])\ncv = 3\nSEED=12","metadata":{"execution":{"iopub.status.busy":"2022-05-27T12:33:41.539777Z","iopub.execute_input":"2022-05-27T12:33:41.540678Z","iopub.status.idle":"2022-05-27T12:33:41.552525Z","shell.execute_reply.started":"2022-05-27T12:33:41.540628Z","shell.execute_reply":"2022-05-27T12:33:41.551784Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The RMSLE is not a metric directly available on sklearn, it's possible to obtain it with the `mean_squared_log_error` method, but if the model makes negative predictions during the GridSearch it causes errors. The used solution is to log the variable to predict beforehand and then use the RMSE to evaluate the models.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"train-length\"></a>\n## Optimizing trainset length\nTraining a linear regression with different training set lengths to determine the optimal size","metadata":{}},{"cell_type":"code","source":"from dateutil.relativedelta import relativedelta\n\nstart_dt = train_start_dt\nlength_scores = []\n\nwhile start_dt <= pd.Period('01-08-2018', 'D'):\n    try:\n        # compute X and y\n        y = train.unstack(['store_nbr', 'family']).loc[start_dt:train_end_dt]\n        y = np.log1p(y)\n        X, dp = compute_trainset(y.index)\n        \n        print(f\"{start_dt}:\", end=' ')\n        model, res = gs_tuning(X, y, pipeline,\n                               dict(model=[LinearRegression(fit_intercept=True)]),\n                               scoring, cv, refit='rmsle')\n        \n        # add the start_dt\n        res['start_dt'] = str(start_dt)\n        \n        # add scores to the scoreslist\n        length_scores.append(res)\n    except ValueError as e:\n        print(f\"{start_dt}: {e}\")\n    # add 1 month to start_dt\n    start_dt = (start_dt.to_timestamp() + relativedelta(months=1)).to_period('D')\n# concat all scores into a single df\nlength_scores = pd.DataFrame.from_dict(length_scores).set_index('start_dt')\nlength_scores['test_rmsle'] = length_scores['test_rmsle']*-1","metadata":{"_kg_hide-input":true,"scrolled":true,"execution":{"iopub.status.busy":"2022-05-27T12:33:41.553748Z","iopub.execute_input":"2022-05-27T12:33:41.554528Z","iopub.status.idle":"2022-05-27T12:35:23.275959Z","shell.execute_reply.started":"2022-05-27T12:33:41.554495Z","shell.execute_reply":"2022-05-27T12:35:23.274716Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"length_scores['test_rmsle'].plot(figsize=(10, 5))\nplt.xticks(rotation=70)\nplt.ylim((0, 2))\nplt.show()\nprint(\"Best scores obtained when trainset start from : \",\n      f\"{length_scores['test_rmsle'].idxmin()}\")\nprint(f\"- RMSLE: {length_scores['test_rmsle'].min():.5f}\")","metadata":{"execution":{"iopub.status.busy":"2022-05-27T13:06:50.036605Z","iopub.execute_input":"2022-05-27T13:06:50.037059Z","iopub.status.idle":"2022-05-27T13:06:50.256303Z","shell.execute_reply.started":"2022-05-27T13:06:50.037026Z","shell.execute_reply":"2022-05-27T13:06:50.255438Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There is an error that prevents the dataset from being reduced beyond September 2017 (caused by DeterministicProcess).  \nIt can be seen that the best performance is obtained when the dataset starts in June 2017.","metadata":{}},{"cell_type":"code","source":"# make new trainset\ntrain_start_dt = length_scores['test_rmsle'].idxmin()\ny = train.unstack(['store_nbr', 'family']).loc[train_start_dt:train_end_dt]\ny = np.log1p(y)\nX, dp = compute_trainset(y.index)\n\nprint('Trainset shape:', X.shape)","metadata":{"execution":{"iopub.status.busy":"2022-05-27T13:06:53.600811Z","iopub.execute_input":"2022-05-27T13:06:53.601254Z","iopub.status.idle":"2022-05-27T13:06:54.956086Z","shell.execute_reply.started":"2022-05-27T13:06:53.601218Z","shell.execute_reply":"2022-05-27T13:06:54.955001Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = px.imshow(X.corr(), color_continuous_scale='reds')\nfig.update_layout(title='Correlation in trainset', height=400, width=700, **plotly_base_params)","metadata":{"execution":{"iopub.status.busy":"2022-05-27T13:06:54.957993Z","iopub.execute_input":"2022-05-27T13:06:54.958531Z","iopub.status.idle":"2022-05-27T13:06:55.05379Z","shell.execute_reply.started":"2022-05-27T13:06:54.958485Z","shell.execute_reply":"2022-05-27T13:06:55.053023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"models\"></a>\n## Models Comparison","metadata":{}},{"cell_type":"code","source":"%%time\n\n# define models and their params to try\n# may be necessary to comment some models \n# to faster tune another with more parameters or to sumbmit predictions\nmodels_grid = [\n    {\n        # Linear Regression\n        'model': [LinearRegression()],\n        'model__fit_intercept': [True],\n    },\n    {\n        # Ridge\n        'model': [Ridge(random_state=SEED)],\n        'model__fit_intercept': [True],\n        'model__alpha':  [83.695], # list(10**np.linspace(10,-2,100)*0.5)    83.695\n        'model__solver': ['saga'],\n    },\n    {\n        # RandomForest\n        'model': [RandomForestRegressor(random_state=SEED)],\n        'model__n_estimators': [221],\n        'model__criterion': ['absolute_error'],\n        'model__max_features': [None], \n        'model__max_depth': [None],  # [2, 5, 10, None]\n        'model__min_samples_leaf': [1],  # [1, 3, 5]\n        'model__min_samples_split': [5],  # [2, 5, 10]\n        'model__oob_score': [True],\n        'model__bootstrap': [True],\n    },\n    {\n        # XGB\n        'model': [XGBRegressor(random_state=SEED)],\n        'model__max_depth': [2],\n        'model__learning_rate': [.1],\n        'model__n_estimators': [100],\n        'model__colsample_bytree': [0.3]\n    },\n]\n\n# inside a loop to manually compare each model\nscores, models = [], []\nfor m in models_grid:\n    model, res = gs_tuning(X, y, pipeline, m, scoring, cv, refit='rmsle')\n    scores.append(res)\n    models.append(model)\n    # predictions on train set\n    preds = pd.DataFrame(model.predict(X), index=X.index, columns=y.columns)\n    print_results(preds, y)\n    print('\\n')\n    ","metadata":{"execution":{"iopub.status.busy":"2022-05-27T12:35:23.293346Z","iopub.status.idle":"2022-05-27T12:35:23.294031Z","shell.execute_reply.started":"2022-05-27T12:35:23.293746Z","shell.execute_reply":"2022-05-27T12:35:23.293775Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As described in [sklearn GridSearchCV with Pipeline](https://stackoverflow.com/questions/21050110/sklearn-gridsearchcv-with-pipeline):  \nThe unified scoring API always maximizes the score, so scores which need to be minimized are negated in order for the unified scoring API to work correctly. The score that is returned is therefore negated when it is a score that should be minimized and left positive if it is a score that should be maximized.  \nSo the real 'gs.best_score_' of each model is the absolute value of the negative one displayed.\n\nThe model which obtains the best score is not necessarily the best for each product family.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"customreg\"></a>\n### Custom Regressor\nhttps://www.kaggle.com/code/andrej0marinchenko/hyperparamaters#Model-Creation","metadata":{}},{"cell_type":"code","source":"df = train_ext[['date', 'sales', 'family']].set_index('date')\nmask = df.index.to_series().between('2017-06-25', '2017-08-15')\ndf = df[mask].groupby(['date', 'family']).agg({\"sales\" : \"mean\"})\ndf = df.unstack(level=0).T.droplevel(level=0, axis=0)\n\nfig =  go.Figure()\nfor col in df.columns:\n    fig.add_trace(go.Scatter(x=df.index, y=df[col], name=col, mode='lines'))\nfig.update_layout(height=850,\n                  title_text=\"Weekly moving average per product Family\",\n                  **plotly_base_params)\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-27T12:35:23.295282Z","iopub.status.idle":"2022-05-27T12:35:23.29597Z","shell.execute_reply.started":"2022-05-27T12:35:23.295734Z","shell.execute_reply":"2022-05-27T12:35:23.295756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"By displaying only the 'SCHOOL AND OFFICE SUPPLIES' family (double click on it in the legend of the graph) we can see that the sales of the family have a very different trend from the other families\n\nBABY CARE\nBOOKS\nHOME APPLIANCE\n\n\n\n\nAUTOMOTIVE\nBEAUTY\nCELEBRATION\nGROCERY II\nHOME AND KITCHEN I\nHOME AND KITCHEN II\nLADIESWEAR\nLAWN AND GARDEN\nLINGERIE\nMAGAZINE\nPET SUPPLIES\nPLAYERS AND ELECTRONIS\nSEAFOOD","metadata":{}},{"cell_type":"code","source":"%%time\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.ensemble import BaggingRegressor\nfrom sklearn.ensemble import VotingRegressor\n\n\nclass CustomRegressor():\n    def __init__(self, n_jobs=-1, verbose=0):\n        self.n_jobs = n_jobs\n        self.verbose = verbose\n        self.estimators_ = None\n        \n    def _estimator_(self, X, y):\n        warnings.simplefilter(action='ignore', category=FutureWarning)\n        if y.name[2] in ['SCHOOL AND OFFICE SUPPLIES']: \n            et = ExtraTreesRegressor(n_estimators=230,  n_jobs=-1, random_state=SEED)            \n            rf = RandomForestRegressor(n_estimators=221, criterion='absolute_error',\n                                       min_samples_leaf=1, min_samples_split=5, \n                                       oob_score=True, n_jobs=-1, random_state=SEED)\n            b1 = BaggingRegressor(base_estimator=et, n_estimators=15, n_jobs=-1, random_state=SEED)\n            b2 = BaggingRegressor(base_estimator=rf, n_estimators=15, n_jobs=-1, random_state=SEED)\n            # Averaging the result\n            model = VotingRegressor([('et', b1), ('rf', b2)])\n        else:\n            ridge = Ridge(fit_intercept=True, solver='saga', alpha=83.695, random_state=SEED)\n            svr = SVR(C=0.2, kernel='rbf')\n            # Averaging result\n            model = VotingRegressor([('ridge', ridge), ('svr', svr)])\n        model.fit(X, y)\n        return model\n\n    def fit(self, X, y):\n        from tqdm.auto import tqdm\n        if self.verbose == 0 :\n            idx_list = range(y.shape[1])\n        else :\n            # using a pretty progress bar\n            idx_list = tqdm(range(y.shape[1]))\n            print('Fit Progress')\n        # fit model with parallel computing\n        self.estimators_ = Parallel(n_jobs=self.n_jobs, verbose=0)(delayed(self._estimator_)(X, y.iloc[:, i]) for i in idx_list)\n        return\n    \n    def predict(self, X):\n        from tqdm.auto import tqdm # pretty progress bar package\n        if self.verbose == 0 :\n            estimators_list = self.estimators_\n        else :\n            estimators_list = tqdm(self.estimators_)\n            print('Predict Progress')\n        # predictions with parallel computing\n        y_pred = Parallel(n_jobs=self.n_jobs, verbose=0)(delayed(e.predict)(X) for e in estimators_list)\n        return np.stack(y_pred, axis=1)\n    \n\nmodel, res = gs_tuning(X, y, pipeline,\n                       dict(model=[CustomRegressor(verbose=0)]),\n                       scoring, cv, refit='rmsle')\nscores.append(res)\nmodels.append(model)\ny_pred = pd.DataFrame(model.predict(X), index=X.index, columns=y.columns)\nprint_results(y_pred, y)","metadata":{"execution":{"iopub.status.busy":"2022-05-27T12:35:23.297502Z","iopub.status.idle":"2022-05-27T12:35:23.298311Z","shell.execute_reply.started":"2022-05-27T12:35:23.298103Z","shell.execute_reply":"2022-05-27T12:35:23.298125Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def model_comparison_chart(scores_df, title, metrics=[], **kwargs):\n    train_color = '#FF8019'\n    test_color = '#2DB42D'\n    time_color = '#5CA3F9'\n    n_metrics = len(metrics)+1 # fit_time always displayed\n    \n    fig = make_subplots(rows=1, cols=n_metrics,\n                        horizontal_spacing=0.1,\n                        specs=kwargs.get('specs', None),\n                        subplot_titles=kwargs.get('subplot_titles', None))\n    \n    for i, m in enumerate(metrics):\n        fig.add_trace(go.Bar(x=scores_df['model_name'], y=scores_df['train_' + m], marker_color=train_color, name='Train'), row=1, col=i+1)\n        fig.add_trace(go.Bar(x=scores_df['model_name'], y=scores_df['test_' + m], marker_color=test_color, name='Test'), row=1, col=i+1)\n    # fit time\n    fig.add_trace(go.Bar(x=scores_df['model_name'], y=scores_df['fit_time'], marker_color=time_color), row=1, col=n_metrics)\n    \n    # style\n    fig.update_traces(texttemplate='%{y:.3f}', textposition='inside')\n    fig.for_each_annotation(lambda a: a.update(text=f'<b>{a.text}</b>'))\n    fig.update_layout(height=kwargs.get('height', 500), width=kwargs.get('width', n_metrics*500),\n                      barmode='group', title=title, showlegend=False, **plotly_base_params)\n    fig.show()\n\n\nscores_df = pd.DataFrame(scores)\n# get positive RMSLE\nscores_df['train_rmsle'] = scores_df['train_rmsle']*-1\nscores_df['test_rmsle'] = scores_df['test_rmsle']*-1\n# display chart\nmodel_comparison_chart(scores_df, 'Models Comparison', scoring.keys(),\n                       specs=[[{\"type\": \"bar\"}, {\"type\": \"bar\"}]],\n                       subplot_titles=(\"RMSLE\", \"Fit Time\"))","metadata":{"execution":{"iopub.status.busy":"2022-05-27T12:35:23.299278Z","iopub.status.idle":"2022-05-27T12:35:23.299631Z","shell.execute_reply.started":"2022-05-27T12:35:23.299463Z","shell.execute_reply":"2022-05-27T12:35:23.299481Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The `CustomRegressor` model obtains one of the lowest RMSLE and one of the smallest differences between the training and test sets.\nIt is therefore this model that is selected for the final predictions","metadata":{}},{"cell_type":"markdown","source":"<a id=\"testpreds\"></a>\n## Predictions on testset\n","metadata":{}},{"cell_type":"code","source":"# DeterministicProcess for testset dates\nX_test = dp.out_of_sample(steps=16)\n# adding other columns\ncols = list(oil_lags.columns)\ncols.extend(['ma_oil', 'weekday', 'workday'])\nfor col in cols:\n    X_test[col] = calendar.loc[test_start_dt:test_end_dt][col].values\n# encoding weekday column\nX_test = pd.get_dummies(X_test, columns=['weekday'], drop_first=True)\n# adding events columns\nX_test[[col for col in X.columns if 'event' in col]] = 0 # no events known for these dates\n# reorder testset columns \nX_test = X_test[X.columns]\n\nprint(\"Model used :\", model)\nprint(\"Testset shape:\", X.shape)","metadata":{"execution":{"iopub.status.busy":"2022-05-27T12:35:23.300982Z","iopub.status.idle":"2022-05-27T12:35:23.30167Z","shell.execute_reply.started":"2022-05-27T12:35:23.301479Z","shell.execute_reply":"2022-05-27T12:35:23.301507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sales_pred = pd.DataFrame(model.predict(X_test), index=X_test.index, columns=y.columns)\nsales_pred = sales_pred.stack(['store_nbr', 'family'])\n# reverse log1p\nsales_pred = np.expm1(sales_pred.clip(0.)) # Sales should be >= 0","metadata":{"execution":{"iopub.status.busy":"2022-05-27T12:35:23.302753Z","iopub.status.idle":"2022-05-27T12:35:23.303369Z","shell.execute_reply.started":"2022-05-27T12:35:23.303169Z","shell.execute_reply":"2022-05-27T12:35:23.30319Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sales_pred","metadata":{"execution":{"iopub.status.busy":"2022-05-27T12:35:23.30456Z","iopub.status.idle":"2022-05-27T12:35:23.304909Z","shell.execute_reply.started":"2022-05-27T12:35:23.304743Z","shell.execute_reply":"2022-05-27T12:35:23.30476Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# submission\nsub = pd.read_csv(os.path.join(DATA_DIR, 'sample_submission.csv'), index_col='id')\nsub['sales'] = sales_pred.values\nsub.to_csv('submission.csv', index=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-27T12:35:23.306464Z","iopub.status.idle":"2022-05-27T12:35:23.307033Z","shell.execute_reply.started":"2022-05-27T12:35:23.306846Z","shell.execute_reply":"2022-05-27T12:35:23.306864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub","metadata":{"execution":{"iopub.status.busy":"2022-05-27T12:35:23.30803Z","iopub.status.idle":"2022-05-27T12:35:23.308556Z","shell.execute_reply.started":"2022-05-27T12:35:23.308369Z","shell.execute_reply":"2022-05-27T12:35:23.308387Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### ","metadata":{}}]}