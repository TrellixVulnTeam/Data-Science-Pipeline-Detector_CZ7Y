{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"\n<a id=\"top\"></a>\n\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='color:white; background:#1777C4; border:0' role=\"tab\" aria-controls=\"home\"><center>CRISP-DM Methodology</center></h3>\n\n* [Buissness Understanding](#1)\n* [Data Understanding](#2)\n* [Data Preparation](#3)\n* [Data Modeling](#4)   \n* [Data Evaluation](#5)\n\nIn this section we overview our selected method for engineering our solution. CRISP-DM stands for Cross-Industry Standard Process for Data Mining. It is an open standard guide that describes common approaches that are used by data mining experts. CRISP-DM includes descriptions of the typical phases of a project, including tasks details and provides an overview of the data mining lifecycle. The lifecycle model consists of six phases with arrows indicating the most important and frequent dependencies between phases. The sequence of the phases is not strict. In fact, most projects move back and forth between phases as necessary. It starts with business understanding, and then moves to data understanding, data preparation, modelling, evaluation, and deployment. The CRISP-DM model is flexible and can be customized easily.\n## Buissness Understanding\n\n    Tasks:\n\n    1.Determine business objectives\n\n    2.Assess situation\n\n    3.Determine data mining goals\n\n    4.Produce project plan\n\n## Data Understanding\n     Tasks:\n\n    1.Collect data\n\n    2.Describe data\n\n    3.Explore data    \n\n## Data Preparation\n    Tasks\n    1.Data selection\n\n    2.Data preprocessing\n\n    3.Feature engineering\n\n    4.Dimensionality reduction\n\n            Steps:\n\n            Data cleaning\n\n            Data integration\n\n            Data sampling\n\n            Data dimensionality reduction\n\n            Data formatting\n\n            Data transformation\n\n            Scaling\n\n            Aggregation\n\n            Decomposition\n\n## Data Modeling :\n\nModeling is the part of the Cross-Industry Standard Process for Data Mining (CRISP-DM) process model that i like best. Our data is already in good shape, and now we can search for useful patterns in our data.\n\n    Tasks\n    1. Select modeling technique Select technique\n\n    2. Generate test design\n\n    3. Build model\n\n    4. Assess model\n\n## Data Evaluation :\n    Tasks\n\n    1.Evaluate Result\n\n    2.Review Process\n\n    3.Determine next steps\n\n<a id=\"top\"></a>\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='color:white; background:#1777C4; border:0' role=\"tab\" aria-controls=\"home\">\n<center>Buissness Understanding</center></h3>\n\n**Goal of the Competition**\nIn this “getting started” competition, you’ll use time-series forecasting to forecast store sales on data from Corporación Favorita, a large Ecuadorian-based grocery retailer.\n\nSpecifically, you'll build a model that more accurately predicts the unit sales for thousands of items sold at different Favorita stores. You'll practice your machine learning skills with an approachable training dataset of dates, store, and item information, promotions, and unit sales.\n\n<a id=\"top\"></a>\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='color:white; background:#1777C4; border:0' role=\"tab\" aria-controls=\"home\">\n<center>Data Understanding</center></h3>\n\n    \n## Step 1: Import helpful libraries","metadata":{}},{"cell_type":"code","source":"#Load the librarys\nimport pandas as pd #To work with dataset\nimport numpy as np #Math library\nimport matplotlib.gridspec as gridspec\nimport seaborn as sns #Graph library that use matplot in background\nimport matplotlib.pyplot as plt #to plot some parameters in seaborn\nimport warnings\nfrom sklearn.preprocessing import LabelEncoder, OrdinalEncoder\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import PowerTransformer, StandardScaler,Normalizer,RobustScaler,MaxAbsScaler,MinMaxScaler,QuantileTransformer\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.neighbors import KNeighborsClassifier\n# Import StandardScaler from scikit-learn\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.impute import KNNImputer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.pipeline import make_pipeline, Pipeline\n\nfrom sklearn.manifold import TSNE\n# Import train_test_split()\n# Metrics\nfrom sklearn.metrics import roc_auc_score, average_precision_score\nfrom sklearn.metrics import make_scorer\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import roc_curve\nfrom datetime import datetime, date\nfrom sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\nfrom sklearn.linear_model import LinearRegression, RidgeCV\n\nimport lightgbm as lgbm\nfrom catboost import CatBoostRegressor\nimport tensorflow as tf \nfrom tensorflow.keras import layers\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.callbacks import LearningRateScheduler\n#import smogn\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.ensemble import GradientBoostingRegressor,RandomForestRegressor\n# For training random forest model\nimport lightgbm as lgb\nfrom scipy import sparse\nfrom sklearn.neighbors import KNeighborsRegressor \nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans \n# Model selection\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_regression,f_classif\nfrom sklearn.feature_selection import mutual_info_regression\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_score\n\nfrom itertools import combinations\n#import smong \n\nimport category_encoders as ce\nimport warnings\nimport optuna \nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2021-10-17T16:17:42.651084Z","iopub.execute_input":"2021-10-17T16:17:42.651448Z","iopub.status.idle":"2021-10-17T16:17:53.849552Z","shell.execute_reply.started":"2021-10-17T16:17:42.651345Z","shell.execute_reply":"2021-10-17T16:17:53.848765Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n## Step 2: Load the data\n\nNext, we'll load the training and test data.\n\nWe set index_col=0 in the code cell below to use the id column to index the DataFrame. (If you're not sure how this works, try temporarily removing index_col=0 and see how it changes the result.)\n","metadata":{}},{"cell_type":"code","source":"%%time\npath = '/kaggle/input/store-sales-time-series-forecasting/'\noil_data = pd.read_csv(path+'oil.csv')\ntrain = pd.read_csv(path+'train.csv', parse_dates = True, low_memory = False)\ntest = pd.read_csv(path+'test.csv')\nsubmission_sample = pd.read_csv(path+'sample_submission.csv')\nholidays_data = pd.read_csv(path+'holidays_events.csv',parse_dates = True, low_memory = False)\nstore_data =  pd.read_csv(path+'stores.csv')\ntransaction_data = pd.read_csv(path+'transactions.csv', parse_dates = True, low_memory = False)\n\n\n# time series as indexes\ntrain.index","metadata":{"execution":{"iopub.status.busy":"2021-10-17T16:18:05.79911Z","iopub.execute_input":"2021-10-17T16:18:05.79946Z","iopub.status.idle":"2021-10-17T16:18:09.736854Z","shell.execute_reply.started":"2021-10-17T16:18:05.799418Z","shell.execute_reply":"2021-10-17T16:18:09.736055Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Number of train samples: ', train.shape)\nprint('Number of test samples: ', test.shape)\nprint('Number of store data: ', store_data.shape)\nprint('Number of Holiday data: ', holidays_data.shape)\nprint('Number of Oil Price data: ', oil_data.shape)\nprint('Number of features: ', len(train.columns))","metadata":{"execution":{"iopub.status.busy":"2021-10-17T16:18:09.738723Z","iopub.execute_input":"2021-10-17T16:18:09.739898Z","iopub.status.idle":"2021-10-17T16:18:09.750591Z","shell.execute_reply.started":"2021-10-17T16:18:09.739826Z","shell.execute_reply":"2021-10-17T16:18:09.749422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data collection +Data Curation ","metadata":{}},{"cell_type":"code","source":"\ndef reduce_mem_usage(df):\n    start_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n\n    for col in df.columns:\n        col_type = df[col].dtype\n    if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.uint8).min and c_max < np.iinfo(np.uint8).max:\n                    df[col] = df[col].astype(np.uint8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.uint16).min and c_max < np.iinfo(np.uint16).max:\n                    df[col] = df[col].astype(np.uint16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.uint32).min and c_max < np.iinfo(np.uint32).max:\n                    df[col] = df[col].astype(np.uint32)                    \n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n                elif c_min > np.iinfo(np.uint64).min and c_max < np.iinfo(np.uint64).max:\n                    df[col] = df[col].astype(np.uint64)\n            elif str(col_type)[:5] == 'float':\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n    return df","metadata":{"execution":{"iopub.status.busy":"2021-10-17T16:18:09.759756Z","iopub.execute_input":"2021-10-17T16:18:09.760403Z","iopub.status.idle":"2021-10-17T16:18:09.78161Z","shell.execute_reply.started":"2021-10-17T16:18:09.760357Z","shell.execute_reply":"2021-10-17T16:18:09.780861Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['year'] = pd.DatetimeIndex(train['date']).year\ntrain['month'] = pd.DatetimeIndex(train['date']).month\ntrain['day'] = pd.DatetimeIndex(train['date']).day\ntrain['day_of_week'] = pd.DatetimeIndex(train['date']).weekday\ntrain['week_of_year'] = pd.DatetimeIndex(train['date']).weekofyear\ntrain['quarter'] = pd.DatetimeIndex(train['date']).quarter\ntrain['season'] = train.month%12 // 3 + 1\ntrain1=reduce_mem_usage(train)\ndel train \nholidays_data['month'] = pd.DatetimeIndex(holidays_data['date']).month\nholidays_data['week_of_year'] = pd.DatetimeIndex(holidays_data['date']).weekofyear\nholidays_data['quarter'] = pd.DatetimeIndex(holidays_data['date']).quarter\nholidays_data['season'] = holidays_data.month%12 // 3 + 1\nholidays_data=holidays_data.drop(['date'], axis=1).head()\nholidays_data=reduce_mem_usage(holidays_data)\ntrain1 = pd.merge(train1, holidays_data,  how='left', left_on=['month','week_of_year','quarter','season'], right_on = ['month','week_of_year','quarter','season'])\ndel holidays_data\noil_data=reduce_mem_usage(oil_data)\ntrain1 = pd.merge(train1, oil_data,  how='left', left_on=['date'], right_on = ['date'])\nstore_data=reduce_mem_usage(store_data)\ntrain1 = pd.merge(train1, store_data,  how='left', left_on=['store_nbr'], right_on = ['store_nbr'])\ntransaction_data=reduce_mem_usage(transaction_data)\ntrain1 = pd.merge(train1, transaction_data,  how='left', left_on=['store_nbr', 'date'], right_on = ['store_nbr','date'])\n\ndel transaction_data\ndel oil_data\ndel store_data ","metadata":{"execution":{"iopub.status.busy":"2021-10-17T16:18:09.782979Z","iopub.execute_input":"2021-10-17T16:18:09.783248Z","iopub.status.idle":"2021-10-17T16:18:22.698354Z","shell.execute_reply.started":"2021-10-17T16:18:09.783218Z","shell.execute_reply":"2021-10-17T16:18:22.697379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train1.columns","metadata":{"execution":{"iopub.status.busy":"2021-10-17T16:18:22.699793Z","iopub.execute_input":"2021-10-17T16:18:22.700048Z","iopub.status.idle":"2021-10-17T16:18:22.706933Z","shell.execute_reply.started":"2021-10-17T16:18:22.700013Z","shell.execute_reply":"2021-10-17T16:18:22.706253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train1 = train1.rename(columns = {\"type_x\" : \"holiday_type\", \"type_y\" : \"store_type\"})","metadata":{"execution":{"iopub.status.busy":"2021-10-17T16:18:22.709889Z","iopub.execute_input":"2021-10-17T16:18:22.710142Z","iopub.status.idle":"2021-10-17T16:18:24.251977Z","shell.execute_reply.started":"2021-10-17T16:18:22.710105Z","shell.execute_reply":"2021-10-17T16:18:24.250867Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" train1=reduce_mem_usage(train1)","metadata":{"execution":{"iopub.status.busy":"2021-10-17T16:18:24.253887Z","iopub.execute_input":"2021-10-17T16:18:24.254151Z","iopub.status.idle":"2021-10-17T16:18:24.314993Z","shell.execute_reply.started":"2021-10-17T16:18:24.254116Z","shell.execute_reply":"2021-10-17T16:18:24.313683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train1[['holiday_type', 'locale', 'locale_name', 'description', 'transferred']]=train1[['holiday_type', 'locale', 'locale_name', 'description', 'transferred']].fillna('NoEvent')\ntrain1[['dcoilwtico', 'transactions']]=train1[['dcoilwtico', 'transactions']].fillna(np.nan)\ntrain1[train1.select_dtypes(['float64','float16']).columns] = train1[train1.select_dtypes(['float64','float16']).columns].apply(pd.to_numeric)\ntrain1[train1.select_dtypes(['object','int64','int8']).columns] = train1.select_dtypes(['object','int64','int8']).apply(lambda x: x.astype('category'))","metadata":{"execution":{"iopub.status.busy":"2021-10-17T16:18:24.316794Z","iopub.execute_input":"2021-10-17T16:18:24.317051Z","iopub.status.idle":"2021-10-17T16:18:32.343449Z","shell.execute_reply.started":"2021-10-17T16:18:24.317021Z","shell.execute_reply":"2021-10-17T16:18:32.342682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train1.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-17T16:18:32.345171Z","iopub.execute_input":"2021-10-17T16:18:32.346239Z","iopub.status.idle":"2021-10-17T16:18:32.388629Z","shell.execute_reply.started":"2021-10-17T16:18:32.346178Z","shell.execute_reply":"2021-10-17T16:18:32.38754Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Grouping data according to store","metadata":{}},{"cell_type":"code","source":"list_store=train1.groupby(['store_nbr'])\nlist_store=[list_store.get_group(x) for x in list_store.groups]","metadata":{"execution":{"iopub.status.busy":"2021-10-17T16:18:32.390476Z","iopub.execute_input":"2021-10-17T16:18:32.390831Z","iopub.status.idle":"2021-10-17T16:18:33.202802Z","shell.execute_reply.started":"2021-10-17T16:18:32.390783Z","shell.execute_reply":"2021-10-17T16:18:33.201538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Grouping data based on item for each store\n\n* creating separate file for each item of each store","metadata":{}},{"cell_type":"code","source":"for store in list_store:\n    store_groups=store.groupby(['family'])\n    list_item=[store_groups.get_group(x) for x in store_groups.groups]\n    for list_store_item in list_item:\n        list_store_item=pd.DataFrame(list_store_item).reset_index(drop=True)\n        store_id=list_store_item[\"store_nbr\"][0]\n        item_id=list_store_item[\"family\"][0]\n        list_store_item.to_csv(f\"file_{store_id}_{item_id}.csv\",index=None)\n        break\n    ","metadata":{"execution":{"iopub.status.busy":"2021-10-17T16:18:51.85008Z","iopub.execute_input":"2021-10-17T16:18:51.851807Z","iopub.status.idle":"2021-10-17T16:19:07.007392Z","shell.execute_reply.started":"2021-10-17T16:18:51.851717Z","shell.execute_reply":"2021-10-17T16:19:07.00643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"store1_Family1= pd.read_csv('./file_1_AUTOMOTIVE.csv')\nstore1_Family1.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-17T16:19:38.230354Z","iopub.execute_input":"2021-10-17T16:19:38.230646Z","iopub.status.idle":"2021-10-17T16:19:38.274808Z","shell.execute_reply.started":"2021-10-17T16:19:38.230615Z","shell.execute_reply":"2021-10-17T16:19:38.273529Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"reference : \n\nhttps://www.researchgate.net/publication/330484523_Machine-Learning_Models_for_Sales_Time_Series_Forecasting\n\nhttps://medium.com/analytics-vidhya/predicting-sales-time-series-analysis-forecasting-with-python-b81d3e8ff03f\n\nhttps://stackoverflow.com/questions/55545501/how-to-perform-time-series-analysis-that-contains-multiple-groups-in-python-usin","metadata":{}}]}