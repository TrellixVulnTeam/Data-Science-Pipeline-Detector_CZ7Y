{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-04-14T20:43:05.471604Z","iopub.execute_input":"2022-04-14T20:43:05.472255Z","iopub.status.idle":"2022-04-14T20:43:05.505115Z","shell.execute_reply.started":"2022-04-14T20:43:05.472172Z","shell.execute_reply":"2022-04-14T20:43:05.504342Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Let's get some info about the data we will be working with.\n* train.csv  is the training data (store_nbr, family, sales, promotion and date )\n store_nbr : store number or store ID\n family identifies the group a product belongs to ex. automotivr, sport, food, beverage, etc..\n onPromotion :  number of articles on promotion of a given family\n sales : total amount of sales for product/family/store at a given date\n \n* test.cvs is testing data with the same structure of train.csv\n* store.csav : store_nbr\tcity\tstate\ttype\tcluster\n* We are asking you to predict total sales for every product/ store in the next month.","metadata":{}},{"cell_type":"markdown","source":"### Defining a path variable to simplify coding","metadata":{}},{"cell_type":"code","source":"from path import Path\npath = Path('/kaggle/input/store-sales-time-series-forecasting/')","metadata":{"execution":{"iopub.status.busy":"2022-04-14T20:43:05.535163Z","iopub.execute_input":"2022-04-14T20:43:05.535445Z","iopub.status.idle":"2022-04-14T20:43:05.547291Z","shell.execute_reply.started":"2022-04-14T20:43:05.535416Z","shell.execute_reply":"2022-04-14T20:43:05.546487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Reading data from files","metadata":{}},{"cell_type":"code","source":"train_sales = pd.read_csv(path/'train.csv')\ntest_sales = pd.read_csv(path/'test.csv')\nshops = pd.read_csv(path/'stores.csv')\nholidays = pd.read_csv(path/'holidays_events.csv')\ntransaction = pd.read_csv(path/'transactions.csv')","metadata":{"execution":{"iopub.status.busy":"2022-04-14T20:43:05.591974Z","iopub.execute_input":"2022-04-14T20:43:05.592178Z","iopub.status.idle":"2022-04-14T20:43:08.387619Z","shell.execute_reply.started":"2022-04-14T20:43:05.592154Z","shell.execute_reply":"2022-04-14T20:43:08.38683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Let's do some Data Analisys","metadata":{}},{"cell_type":"code","source":"train_sales.head(5)","metadata":{"execution":{"iopub.status.busy":"2022-04-14T20:43:08.389092Z","iopub.execute_input":"2022-04-14T20:43:08.389376Z","iopub.status.idle":"2022-04-14T20:43:08.411076Z","shell.execute_reply.started":"2022-04-14T20:43:08.38934Z","shell.execute_reply":"2022-04-14T20:43:08.410281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_sales.info()","metadata":{"execution":{"iopub.status.busy":"2022-04-14T20:43:08.412658Z","iopub.execute_input":"2022-04-14T20:43:08.413096Z","iopub.status.idle":"2022-04-14T20:43:08.430309Z","shell.execute_reply.started":"2022-04-14T20:43:08.41306Z","shell.execute_reply":"2022-04-14T20:43:08.429467Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_sales","metadata":{"execution":{"iopub.status.busy":"2022-04-14T20:43:08.433112Z","iopub.execute_input":"2022-04-14T20:43:08.433447Z","iopub.status.idle":"2022-04-14T20:43:08.454245Z","shell.execute_reply.started":"2022-04-14T20:43:08.433408Z","shell.execute_reply":"2022-04-14T20:43:08.453467Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Number of Time Series (train):\")\nunique_keys_train = set(zip(train_sales.store_nbr, train_sales.family))\nprint(len(unique_keys_train))\n\n","metadata":{"execution":{"iopub.status.busy":"2022-04-14T20:43:08.455723Z","iopub.execute_input":"2022-04-14T20:43:08.456178Z","iopub.status.idle":"2022-04-14T20:43:09.493488Z","shell.execute_reply.started":"2022-04-14T20:43:08.456141Z","shell.execute_reply":"2022-04-14T20:43:09.492743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_sales.family.unique()","metadata":{"execution":{"iopub.status.busy":"2022-04-14T20:43:09.494609Z","iopub.execute_input":"2022-04-14T20:43:09.495957Z","iopub.status.idle":"2022-04-14T20:43:09.752429Z","shell.execute_reply.started":"2022-04-14T20:43:09.495916Z","shell.execute_reply":"2022-04-14T20:43:09.751759Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_sales.store_nbr.unique()","metadata":{"execution":{"iopub.status.busy":"2022-04-14T20:43:09.753828Z","iopub.execute_input":"2022-04-14T20:43:09.754074Z","iopub.status.idle":"2022-04-14T20:43:09.776879Z","shell.execute_reply.started":"2022-04-14T20:43:09.754041Z","shell.execute_reply":"2022-04-14T20:43:09.776092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_sales.corr()","metadata":{"execution":{"iopub.status.busy":"2022-04-14T20:43:09.778211Z","iopub.execute_input":"2022-04-14T20:43:09.778457Z","iopub.status.idle":"2022-04-14T20:43:10.027067Z","shell.execute_reply.started":"2022-04-14T20:43:09.778424Z","shell.execute_reply":"2022-04-14T20:43:10.025832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport matplotlib as mpl\nfrom cycler import cycler\nimport seaborn as sns\n\ncorrelation = train_sales.corr()\nmax_correlation = correlation.index[abs(correlation[\"sales\"])>0.01]\nplt.figure(figsize=(10,10))\ngraph=sns.heatmap(train_sales[max_correlation].corr(), annot=True, cmap=\"YlGnBu\")","metadata":{"execution":{"iopub.status.busy":"2022-04-14T20:43:10.030286Z","iopub.execute_input":"2022-04-14T20:43:10.030536Z","iopub.status.idle":"2022-04-14T20:43:11.667521Z","shell.execute_reply.started":"2022-04-14T20:43:10.0305Z","shell.execute_reply":"2022-04-14T20:43:11.666864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"shops.head(5)","metadata":{"execution":{"iopub.status.busy":"2022-04-14T20:43:11.673287Z","iopub.execute_input":"2022-04-14T20:43:11.675123Z","iopub.status.idle":"2022-04-14T20:43:11.690473Z","shell.execute_reply.started":"2022-04-14T20:43:11.675085Z","shell.execute_reply":"2022-04-14T20:43:11.689718Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(shops)","metadata":{"execution":{"iopub.status.busy":"2022-04-14T20:43:11.69541Z","iopub.execute_input":"2022-04-14T20:43:11.697316Z","iopub.status.idle":"2022-04-14T20:43:11.706118Z","shell.execute_reply.started":"2022-04-14T20:43:11.697281Z","shell.execute_reply":"2022-04-14T20:43:11.705164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"holidays","metadata":{"execution":{"iopub.status.busy":"2022-04-14T20:43:11.710255Z","iopub.execute_input":"2022-04-14T20:43:11.71051Z","iopub.status.idle":"2022-04-14T20:43:11.733976Z","shell.execute_reply.started":"2022-04-14T20:43:11.710477Z","shell.execute_reply":"2022-04-14T20:43:11.732991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transaction","metadata":{"execution":{"iopub.status.busy":"2022-04-14T20:43:11.73796Z","iopub.execute_input":"2022-04-14T20:43:11.73821Z","iopub.status.idle":"2022-04-14T20:43:11.755874Z","shell.execute_reply.started":"2022-04-14T20:43:11.738177Z","shell.execute_reply":"2022-04-14T20:43:11.754524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Let's Plot Total Sales of all Stores","metadata":{}},{"cell_type":"code","source":"# Total sales of all stores\nfrom learntools.time_series.style import *   #  plot style setings\n\ntotal_sales_all_stores = train_sales.groupby('date').sum()['sales']\nax = total_sales_all_stores.plot(**plot_params)\nax.set(title=\"Total sales of all stores\")","metadata":{"execution":{"iopub.status.busy":"2022-04-14T20:43:11.759286Z","iopub.execute_input":"2022-04-14T20:43:11.761227Z","iopub.status.idle":"2022-04-14T20:43:12.499675Z","shell.execute_reply.started":"2022-04-14T20:43:11.761187Z","shell.execute_reply":"2022-04-14T20:43:12.499036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#x  Total sales  x  store number\n\ntotal_sales_storenumber = train_sales.groupby('store_nbr').sum()['sales']\nsx = total_sales_storenumber.plot(**plot_params)\nsx.set(title=\"Total Sales x Store Number \")\n","metadata":{"execution":{"iopub.status.busy":"2022-04-14T20:43:12.500641Z","iopub.execute_input":"2022-04-14T20:43:12.500932Z","iopub.status.idle":"2022-04-14T20:43:12.933298Z","shell.execute_reply.started":"2022-04-14T20:43:12.500881Z","shell.execute_reply":"2022-04-14T20:43:12.93264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#x  Total sales  x  product family\n\ntotal_sales_family = train_sales.groupby('family').sum()['sales']\nfx = total_sales_family.plot(**plot_params)\nfx.set(title=\"Total Sales x Product's Family \")","metadata":{"execution":{"iopub.status.busy":"2022-04-14T20:43:12.934464Z","iopub.execute_input":"2022-04-14T20:43:12.935211Z","iopub.status.idle":"2022-04-14T20:43:13.578544Z","shell.execute_reply.started":"2022-04-14T20:43:12.935173Z","shell.execute_reply":"2022-04-14T20:43:13.577867Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_sales.family.unique()","metadata":{"execution":{"iopub.status.busy":"2022-04-14T20:43:13.57959Z","iopub.execute_input":"2022-04-14T20:43:13.579965Z","iopub.status.idle":"2022-04-14T20:43:13.807355Z","shell.execute_reply.started":"2022-04-14T20:43:13.579928Z","shell.execute_reply":"2022-04-14T20:43:13.806649Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_sales.groupby('family').sum()['sales']","metadata":{"execution":{"iopub.status.busy":"2022-04-14T20:43:13.808606Z","iopub.execute_input":"2022-04-14T20:43:13.808939Z","iopub.status.idle":"2022-04-14T20:43:14.230558Z","shell.execute_reply.started":"2022-04-14T20:43:13.808894Z","shell.execute_reply":"2022-04-14T20:43:14.229695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Let's try to do a kind of sql join\n* Using the value sales.store_nbr which is the column for store_id in the table ( file ) sales\n* we are going to access the table shops which also has store_id as one of its columns and then\n* from that table we will get the column city which is the city where the store is located","metadata":{}},{"cell_type":"code","source":"train_sales['city']= train_sales.store_nbr.map(shops.city)\ntrain_sales['city']\n","metadata":{"execution":{"iopub.status.busy":"2022-04-14T20:43:14.232161Z","iopub.execute_input":"2022-04-14T20:43:14.232424Z","iopub.status.idle":"2022-04-14T20:43:14.351769Z","shell.execute_reply.started":"2022-04-14T20:43:14.232389Z","shell.execute_reply":"2022-04-14T20:43:14.350905Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###  Let's take a look at   Total Sales by Date and by  City","metadata":{}},{"cell_type":"code","source":"sales_by_city= train_sales.groupby(['date',train_sales['city']])['sales'].sum().reset_index()\nsales_by_city = sales_by_city.set_index('date')\nsales_by_city","metadata":{"execution":{"iopub.status.busy":"2022-04-14T20:43:14.353228Z","iopub.execute_input":"2022-04-14T20:43:14.353491Z","iopub.status.idle":"2022-04-14T20:43:14.962246Z","shell.execute_reply.started":"2022-04-14T20:43:14.353457Z","shell.execute_reply":"2022-04-14T20:43:14.961468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###  Total Sales by City","metadata":{}},{"cell_type":"code","source":" train_sales.groupby('city').sum()['sales']","metadata":{"execution":{"iopub.status.busy":"2022-04-14T20:43:14.963472Z","iopub.execute_input":"2022-04-14T20:43:14.963888Z","iopub.status.idle":"2022-04-14T20:43:15.594495Z","shell.execute_reply.started":"2022-04-14T20:43:14.96385Z","shell.execute_reply":"2022-04-14T20:43:15.59372Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#x  Total sales  x  city\n\ntotal_sales_city = train_sales.groupby('city').sum()['sales']\nfx = total_sales_city.plot(**plot_params)\nfx.set(title=\"Total Sales x City \")","metadata":{"execution":{"iopub.status.busy":"2022-04-14T20:43:15.595931Z","iopub.execute_input":"2022-04-14T20:43:15.596186Z","iopub.status.idle":"2022-04-14T20:43:16.240257Z","shell.execute_reply.started":"2022-04-14T20:43:15.596152Z","shell.execute_reply":"2022-04-14T20:43:16.239626Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### OK now let break down the column date in to Year, Month, Week and Day\n* This will be helpful to analyze the data and build some nice Graphs","metadata":{}},{"cell_type":"code","source":"import calendar\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\nimport plotly.figure_factory as ff\nimport plotly.offline as offline\nimport plotly.graph_objs as go\n\ntrain_sales['date'] = pd.to_datetime(train_sales['date'])\ntrain_sales['year'] = train_sales['date'].dt.year\ntrain_sales['month']= train_sales['date'].dt.month\ntrain_sales['week']= train_sales['date'].dt.isocalendar().week\ntrain_sales['day_of_week']= train_sales['date'].dt.day_name()","metadata":{"execution":{"iopub.status.busy":"2022-04-14T20:43:16.24165Z","iopub.execute_input":"2022-04-14T20:43:16.24216Z","iopub.status.idle":"2022-04-14T20:43:23.01322Z","shell.execute_reply.started":"2022-04-14T20:43:16.242123Z","shell.execute_reply":"2022-04-14T20:43:23.01236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"total_sales_month = train_sales.groupby('month').sum()['sales']\nsx = total_sales_month.plot(**plot_params)\nsx.set(title=\"Total Sales x Month \")","metadata":{"execution":{"iopub.status.busy":"2022-04-14T20:43:23.014452Z","iopub.execute_input":"2022-04-14T20:43:23.014699Z","iopub.status.idle":"2022-04-14T20:43:23.890468Z","shell.execute_reply.started":"2022-04-14T20:43:23.014667Z","shell.execute_reply":"2022-04-14T20:43:23.889671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"total_sales_dayweek = train_sales.groupby('day_of_week').sum()['sales']\nfdw = total_sales_dayweek.plot(**plot_params)\nfdw.set(title='Total Sales by Day of the Week')\n","metadata":{"execution":{"iopub.status.busy":"2022-04-14T20:43:23.89172Z","iopub.execute_input":"2022-04-14T20:43:23.892167Z","iopub.status.idle":"2022-04-14T20:43:24.623376Z","shell.execute_reply.started":"2022-04-14T20:43:23.892117Z","shell.execute_reply":"2022-04-14T20:43:24.622682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"total_sales_dayweek = train_sales.groupby('day_of_week').agg({\"sales\": \"mean\"}).reset_index()\nfdw = total_sales_dayweek.plot(**plot_params)\nfdw.set(title='Average Sales by Day of the Week')","metadata":{"execution":{"iopub.status.busy":"2022-04-14T20:43:24.624733Z","iopub.execute_input":"2022-04-14T20:43:24.625188Z","iopub.status.idle":"2022-04-14T20:43:25.136269Z","shell.execute_reply.started":"2022-04-14T20:43:24.625151Z","shell.execute_reply":"2022-04-14T20:43:25.135643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# data\ndf_dw_sa = train_sales.groupby('day_of_week').agg({\"sales\" : \"mean\"}).reset_index()\ndf_dw_sa.sales = round(df_dw_sa.sales, 2)\n\n# chart\nfig = px.bar(df_dw_sa, y='day_of_week', x='sales', title='Avg Sales vs Day of Week',\n             color_discrete_sequence=['#c6ccd8'], text='sales',\n             category_orders=dict(day_of_week=[\"Monday\",\"Tuesday\",\"Wednesday\",\"Thursday\", \"Friday\",\"Saturday\",\"Sunday\"]))\nfig.update_yaxes(showgrid=False, ticksuffix=' ', showline=False)\nfig.update_xaxes(visible=False)\nfig.update_layout(margin=dict(t=60, b=0, l=0, r=0), height=350,\n                  hovermode=\"y unified\", \n                  yaxis_title=\" \", template='plotly_white',\n                  title_font=dict(size=25, color='#8a8d93', family=\"Lato, sans-serif\"),\n                  font=dict(color='#8a8d93'),\n                  hoverlabel=dict(bgcolor=\"#c6ccd8\", font_size=13, font_family=\"Lato, sans-serif\"))","metadata":{"execution":{"iopub.status.busy":"2022-04-14T20:43:25.137615Z","iopub.execute_input":"2022-04-14T20:43:25.138082Z","iopub.status.idle":"2022-04-14T20:43:26.167988Z","shell.execute_reply.started":"2022-04-14T20:43:25.138045Z","shell.execute_reply":"2022-04-14T20:43:26.167273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n#offline.init_notebook_mode(connected = True)\n\nfig=go.Figure()\nfig.add_trace(go.Scatter(\n    x=[0, 1, 2, 3],\n    y=[1.6, 1.6, 1.6, 1.6],\n    mode=\"text\", \n    text=[\"<span style='font-size:33px'><b>54</b></span>\", \n          \"<span style='font-size:33px'><b>33</b></span>\",\n          \"<span style='font-size:33px'><b>16</b></span>\",\n          \"<span style='font-size:33px'><b>56</b></span>\"],\n    textposition=\"bottom center\"\n))\nfig.add_trace(go.Scatter(\n    x=[0, 1, 2, 3],\n    y=[1.1, 1.1, 1.1, 1.1],\n    mode=\"text\", \n    text=[\"Stores\", \"Products\", \"States\", \"Months\"],\n    textposition=\"bottom center\"\n))\nfig.add_hline(y=2.2, line_width=5, line_color='gray')\nfig.add_hline(y=0.3, line_width=3, line_color='gray')\nfig.update_yaxes(visible=False)\nfig.update_xaxes(visible=True)\nfig.update_layout(showlegend=False, height=300, width=700, \n                  title='Store Sales Summary', title_x=0.5, title_y=0.9,\n                  xaxis_range=[-0.5,3.6], yaxis_range=[-0.2,2.2],\n                  plot_bgcolor='#fafafa', paper_bgcolor='#fafafa',\n                  font=dict(size=23, color='#323232'),\n                  title_font=dict(size=35, color='#222'),\n                  margin=dict(t=90,l=70,b=0,r=70), \n    )","metadata":{"execution":{"iopub.status.busy":"2022-04-14T20:43:26.173354Z","iopub.execute_input":"2022-04-14T20:43:26.173543Z","iopub.status.idle":"2022-04-14T20:43:26.225864Z","shell.execute_reply.started":"2022-04-14T20:43:26.173519Z","shell.execute_reply":"2022-04-14T20:43:26.225216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_sales[:2]","metadata":{"execution":{"iopub.status.busy":"2022-04-14T20:43:26.227067Z","iopub.execute_input":"2022-04-14T20:43:26.227455Z","iopub.status.idle":"2022-04-14T20:43:26.244687Z","shell.execute_reply.started":"2022-04-14T20:43:26.22742Z","shell.execute_reply":"2022-04-14T20:43:26.244079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nsf =train_sales.groupby(['store_nbr', train_sales['family']])\nprint(sf)\n","metadata":{"execution":{"iopub.status.busy":"2022-04-14T20:43:26.245777Z","iopub.execute_input":"2022-04-14T20:43:26.246095Z","iopub.status.idle":"2022-04-14T20:43:26.251757Z","shell.execute_reply.started":"2022-04-14T20:43:26.24606Z","shell.execute_reply":"2022-04-14T20:43:26.250648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Slice [start:stop:step], starting from index 5 take every 6th record.\ntrain_df = train_sales[5::6]\n#date_time = pd.to_datetime(tain_df.pop('Date Time'), format=''%d.%m.%y')\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-14T20:43:26.253539Z","iopub.execute_input":"2022-04-14T20:43:26.254551Z","iopub.status.idle":"2022-04-14T20:43:26.275371Z","shell.execute_reply.started":"2022-04-14T20:43:26.254513Z","shell.execute_reply":"2022-04-14T20:43:26.274701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.describe().transpose()","metadata":{"execution":{"iopub.status.busy":"2022-04-14T20:43:26.276402Z","iopub.execute_input":"2022-04-14T20:43:26.276729Z","iopub.status.idle":"2022-04-14T20:43:26.847389Z","shell.execute_reply.started":"2022-04-14T20:43:26.276692Z","shell.execute_reply":"2022-04-14T20:43:26.846648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Let's do One Hot Encoding ..","metadata":{}},{"cell_type":"code","source":"train_sales_features = ['store_nbr', 'family','sales']\ntrain_sales_one_hot = train_sales[train_sales_features]\ntrain_sales_one_hot.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-14T20:43:26.848868Z","iopub.execute_input":"2022-04-14T20:43:26.849108Z","iopub.status.idle":"2022-04-14T20:43:26.886366Z","shell.execute_reply.started":"2022-04-14T20:43:26.849076Z","shell.execute_reply":"2022-04-14T20:43:26.88568Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_sales_one_hot = pd.get_dummies(train_sales_one_hot, drop_first=True)","metadata":{"execution":{"iopub.status.busy":"2022-04-14T20:43:26.887604Z","iopub.execute_input":"2022-04-14T20:43:26.887879Z","iopub.status.idle":"2022-04-14T20:43:27.382308Z","shell.execute_reply.started":"2022-04-14T20:43:26.887844Z","shell.execute_reply":"2022-04-14T20:43:27.381538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_sales.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-14T20:43:27.383623Z","iopub.execute_input":"2022-04-14T20:43:27.38391Z","iopub.status.idle":"2022-04-14T20:43:27.399229Z","shell.execute_reply.started":"2022-04-14T20:43:27.383874Z","shell.execute_reply":"2022-04-14T20:43:27.39825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_sales_one_hot.tail()","metadata":{"execution":{"iopub.status.busy":"2022-04-14T20:43:27.400777Z","iopub.execute_input":"2022-04-14T20:43:27.401131Z","iopub.status.idle":"2022-04-14T20:43:27.425515Z","shell.execute_reply.started":"2022-04-14T20:43:27.401092Z","shell.execute_reply":"2022-04-14T20:43:27.424707Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_sales_one_hot['date'] = str(train_sales_one_hot['date'])","metadata":{"execution":{"iopub.status.busy":"2022-04-14T20:43:27.426775Z","iopub.execute_input":"2022-04-14T20:43:27.427061Z","iopub.status.idle":"2022-04-14T20:43:27.430953Z","shell.execute_reply.started":"2022-04-14T20:43:27.427024Z","shell.execute_reply":"2022-04-14T20:43:27.429997Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## OK, let's split the data","metadata":{}},{"cell_type":"code","source":"column_indices = {name: i for i, name in enumerate(train_sales_one_hot.columns)}\nnl = len(train_sales_one_hot)\ntrain_df = train_sales_one_hot[0:int(nl*0.85)]    # int(0.85 * len(nl))\nval_df =  train_sales_one_hot[0:int(nl*0.15)]      #  int(0.15 * len(nl))\nnum_features = train_sales_one_hot.shape[1]\n\nprint(\"train_df :\", train_df)\n","metadata":{"execution":{"iopub.status.busy":"2022-04-14T20:43:27.432605Z","iopub.execute_input":"2022-04-14T20:43:27.433283Z","iopub.status.idle":"2022-04-14T20:43:27.589648Z","shell.execute_reply.started":"2022-04-14T20:43:27.433242Z","shell.execute_reply":"2022-04-14T20:43:27.588056Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###  Now, Let's Normalize the data\n* It is important to scale features before training a neural network. Normalization is a common way of doing this scaling: subtract the mean and divide by the standard deviation of each feature.\n\n* The mean and standard deviation should only be computed using the training data so that the models have no access to the values in the validation and test sets.","metadata":{}},{"cell_type":"code","source":"train_df['sales'] = (train_df['sales'] - train_df['sales'].mean())/train_df['sales'].std()","metadata":{"execution":{"iopub.status.busy":"2022-04-14T20:43:27.591176Z","iopub.execute_input":"2022-04-14T20:43:27.591418Z","iopub.status.idle":"2022-04-14T20:43:27.625251Z","shell.execute_reply.started":"2022-04-14T20:43:27.591389Z","shell.execute_reply":"2022-04-14T20:43:27.624496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"raw_data : \", train_df['sales'])","metadata":{"execution":{"iopub.status.busy":"2022-04-14T20:43:27.627375Z","iopub.execute_input":"2022-04-14T20:43:27.628112Z","iopub.status.idle":"2022-04-14T20:43:27.635292Z","shell.execute_reply.started":"2022-04-14T20:43:27.628073Z","shell.execute_reply":"2022-04-14T20:43:27.634321Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.tail()","metadata":{"execution":{"iopub.status.busy":"2022-04-14T20:43:27.637227Z","iopub.execute_input":"2022-04-14T20:43:27.637721Z","iopub.status.idle":"2022-04-14T20:43:27.660227Z","shell.execute_reply.started":"2022-04-14T20:43:27.637683Z","shell.execute_reply":"2022-04-14T20:43:27.659437Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Let's creates some functions ( tools ) \n* I am going to use Tensorflow timeseries weather forcasting as a guide :  https://www.tensorflow.org/tutorials/structured_data/time_series#split_the_data\n\n## 1. Creates window Generator:\n* Start by creating the WindowGenerator class. The __init__ method includes all the necessary logic for the input and label indices.\n* It also takes the training, evaluation, and test DataFrames as input. These will be converted to tf.data.Datasets of windows later. \n\n","metadata":{}},{"cell_type":"code","source":"import os\nimport datetime\n\nimport IPython\nimport IPython.display\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport tensorflow as tf\n\nmpl.rcParams['figure.figsize'] = (8, 6)\nmpl.rcParams['axes.grid'] = False","metadata":{"execution":{"iopub.status.busy":"2022-04-14T20:43:27.661636Z","iopub.execute_input":"2022-04-14T20:43:27.661995Z","iopub.status.idle":"2022-04-14T20:43:31.349452Z","shell.execute_reply.started":"2022-04-14T20:43:27.661957Z","shell.execute_reply":"2022-04-14T20:43:31.348723Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#  1.  Windows Generator\n\nclass WindowGenerator():\n  def __init__(self, input_width, label_width, shift,\n               train_df=train_df, val_df=val_df, test_df=test_sales,\n               label_columns=None):\n    # Store the raw data.\n    self.train_df = train_df\n    self.val_df = val_df\n    self.test_df = test_df\n\n    # Work out the label column indices.\n    self.label_columns = label_columns\n    if label_columns is not None:\n      self.label_columns_indices = {name: i for i, name in\n                                    enumerate(label_columns)}\n    self.column_indices = {name: i for i, name in\n                           enumerate(train_df.columns)}\n\n    # Work out the window parameters.\n    self.input_width = input_width\n    self.label_width = label_width\n    self.shift = shift\n\n    self.total_window_size = input_width + shift\n\n    self.input_slice = slice(0, input_width)\n    self.input_indices = np.arange(self.total_window_size)[self.input_slice]\n\n    self.label_start = self.total_window_size - self.label_width\n    self.labels_slice = slice(self.label_start, None)\n    self.label_indices = np.arange(self.total_window_size)[self.labels_slice]\n\n  def __repr__(self):\n    return '\\n'.join([\n        f'Total window size: {self.total_window_size}',\n        f'Input indices: {self.input_indices}',\n        f'Label indices: {self.label_indices}',\n        f'Label column name(s): {self.label_columns}'])\n\n","metadata":{"execution":{"iopub.status.busy":"2022-04-14T20:43:31.350767Z","iopub.execute_input":"2022-04-14T20:43:31.351206Z","iopub.status.idle":"2022-04-14T20:43:31.360896Z","shell.execute_reply.started":"2022-04-14T20:43:31.351169Z","shell.execute_reply":"2022-04-14T20:43:31.360108Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Let's creates some functions ( tools ) \n\n## 2. Spliting the Data\n* Given a list of consecutive inputs, the split_window method will convert them to a window of inputs and a window of labels.\n","metadata":{}},{"cell_type":"code","source":"# 2.  Spliting windows\n\ndef split_window(self, features):\n  inputs = features[:, self.input_slice, :]\n  labels = features[:, self.labels_slice, :]\n  if self.label_columns is not None:\n    labels = tf.stack(\n        [labels[:, :, self.column_indices[name]] for name in self.label_columns],\n        axis=-1)\n\n  # Slicing doesn't preserve static shape information, so set the shapes\n  # manually. This way the `tf.data.Datasets` are easier to inspect.\n  inputs.set_shape([None, self.input_width, None])\n  labels.set_shape([None, self.label_width, None])\n\n  return inputs, labels\n\nWindowGenerator.split_window = split_window","metadata":{"execution":{"iopub.status.busy":"2022-04-14T20:43:31.362256Z","iopub.execute_input":"2022-04-14T20:43:31.362691Z","iopub.status.idle":"2022-04-14T20:43:31.373929Z","shell.execute_reply.started":"2022-04-14T20:43:31.362653Z","shell.execute_reply":"2022-04-14T20:43:31.37313Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"​\n## 3.  Create Dataset\n* Finally, this make_dataset method will take a time series DataFrame and convert it to a tf.data.Dataset of (input_window, label_window) pairs using the tf.keras.utils.timeseries_dataset_from_array function:\n* The WindowGenerator object holds training, validation, and test data.\n* Add properties for accessing them as tf.data.Datasets using the make_dataset method you defined earlier. Also, add a standard example batch for easy access and plotting:\n\n","metadata":{}},{"cell_type":"code","source":"\n# 3.  create Dataset.\n\ndef make_dataset(self, data):\n  data = np.array(data, dtype=np.float32)\n  ds = tf.keras.utils.timeseries_dataset_from_array(\n      data=data,\n      targets=None,\n      sequence_length=self.total_window_size,\n      sequence_stride=1,\n      shuffle=True,\n      batch_size=32,)\n\n  ds = ds.map(self.split_window)\n\n  return ds\n\nWindowGenerator.make_dataset = make_dataset\n\n@property\ndef train(self):\n  return self.make_dataset(self.train_df)\n\n@property\ndef val(self):\n  return self.make_dataset(self.val_df)\n\n@property\ndef test(self):\n  return self.make_dataset(self.test_df)\n\n@property\ndef example(self):\n  \"\"\"Get and cache an example batch of `inputs, labels` for plotting.\"\"\"\n  result = getattr(self, '_example', None)\n  if result is None:\n    # No example batch was found, so get one from the `.train` dataset\n    result = next(iter(self.train))\n    # And cache it for next time\n    self._example = result\n  return result\n\nWindowGenerator.train = train\nWindowGenerator.val = val\nWindowGenerator.test = test\nWindowGenerator.example = example\n\n","metadata":{"execution":{"iopub.status.busy":"2022-04-14T20:43:31.375448Z","iopub.execute_input":"2022-04-14T20:43:31.375974Z","iopub.status.idle":"2022-04-14T20:43:31.38699Z","shell.execute_reply.started":"2022-04-14T20:43:31.375894Z","shell.execute_reply":"2022-04-14T20:43:31.386282Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4. Compile and Fit the model.\n* Parameters model, window, max-epochs, learning_rate, patience\n* Define loss function, optimizer as well as the metrics\n* Fit.model parameters input dataset, epochs, validation dataset and callbacks\n* Return history","metadata":{}},{"cell_type":"code","source":"# 4.  Compile and Fit the model.\n\ndef compile_and_fit(model, window, MAX_EPOCHS=20, learning_rate=0.2, patience=10):\n    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=patience, mode='min')\n    \n    model.compile(loss=tf.losses.MeanSquaredError(),\n                optimizer=tf.optimizers.Adam(learning_rate=learning_rate),\n                metrics=[tf.metrics.MeanAbsoluteError()])\n    \n    history = model.fit(window.train, epochs=MAX_EPOCHS,\n                      validation_data=window.val,\n                      callbacks=[early_stopping])\n    \n    return history","metadata":{"execution":{"iopub.status.busy":"2022-04-14T20:43:31.389269Z","iopub.execute_input":"2022-04-14T20:43:31.390428Z","iopub.status.idle":"2022-04-14T20:43:31.39794Z","shell.execute_reply.started":"2022-04-14T20:43:31.390388Z","shell.execute_reply":"2022-04-14T20:43:31.397219Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data windowing\n* We will make a set of predictions based on a window of consecutive samples from the data.\n\n* The main features of the input windows are:\n\n1. The **width** (number of time steps) of the input and label windows.\n2. The **time offset** between them. ( shift )\n3. Which **features ( label_width )** are used as inputs, labels, or both.\nThis tutorial builds a variety of models (including Linear, DNN, CNN and RNN models), and uses them for both:\n\nSingle-output, and multi-output predictions.\nSingle-time-step and multi-time-step predictions.\n\nFor example, to make a single prediction 24 hours into the future, given 24 hours of history, you might define a window like this:\n\nOne prediction 24 hours into the future.\n\n[0][1][2] ..... [23][24][25] .........[46][47]\ninput_width = 24\noffset = 24\nlabel_width = 1\ntotal_width = 48e","metadata":{}},{"cell_type":"code","source":"window1 = WindowGenerator(input_width=24,     # 24 hours history\n                          label_width = 1,   # one single prediction\n                          shift=24,          # 24 hours in to the future\n                         label_columns=['sales'])\nwindow1","metadata":{"execution":{"iopub.status.busy":"2022-04-14T20:43:31.399257Z","iopub.execute_input":"2022-04-14T20:43:31.399725Z","iopub.status.idle":"2022-04-14T20:43:31.411467Z","shell.execute_reply.started":"2022-04-14T20:43:31.399689Z","shell.execute_reply":"2022-04-14T20:43:31.410592Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Stack three slices, the length of the total window.\ntrain_df= tf.convert_to_tensor(train_df, np.float32)\nexample_window = tf.stack([np.array(train_df[:window1.total_window_size]),\n                           np.array(train_df[100:100+window1.total_window_size]),\n                           np.array(train_df[200:200+window1.total_window_size])])\n\nexample_inputs, example_labels = window1.split_window(example_window)\n\nprint('All shapes are: (batch, time, features)')\nprint(f'Window shape: {example_window.shape}')\nprint(f'Inputs shape: {example_inputs.shape}')\nprint(f'Labels shape: {example_labels.shape}')","metadata":{"execution":{"iopub.status.busy":"2022-04-14T20:43:31.412855Z","iopub.execute_input":"2022-04-14T20:43:31.413436Z","iopub.status.idle":"2022-04-14T20:43:34.696208Z","shell.execute_reply.started":"2022-04-14T20:43:31.413395Z","shell.execute_reply":"2022-04-14T20:43:34.695486Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"example_window.example = example_inputs, example_labels","metadata":{"execution":{"iopub.status.busy":"2022-04-14T20:43:34.697245Z","iopub.execute_input":"2022-04-14T20:43:34.697478Z","iopub.status.idle":"2022-04-14T20:43:34.703506Z","shell.execute_reply.started":"2022-04-14T20:43:34.697445Z","shell.execute_reply":"2022-04-14T20:43:34.70257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Let's  create a wider WindowGenerator that generates windows 30 days of consecutive inputs and labels at a time.\n* The new wide_window variable doesn't change the way the model operates.\n* The model still makes predictions one day into the future based on 30 day ( one week ) inputs time step. \n* Here, the time axis acts like the batch axis: each prediction is made independently with no interaction between time steps:","metadata":{}},{"cell_type":"code","source":"window30 = WindowGenerator(\n    input_width=30,\n    label_width=1,\n    shift=1,\n    label_columns=['sales'])\n\nwindow30","metadata":{"execution":{"iopub.status.busy":"2022-04-14T20:43:34.705127Z","iopub.execute_input":"2022-04-14T20:43:34.705388Z","iopub.status.idle":"2022-04-14T20:43:34.71359Z","shell.execute_reply.started":"2022-04-14T20:43:34.705354Z","shell.execute_reply":"2022-04-14T20:43:34.712692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Stack three slices, the length of the total window.\ntrain_df= tf.convert_to_tensor(train_df, np.float32)\nexample_window30 = tf.stack([np.array(train_df[:window30.total_window_size]),\n                           np.array(train_df[100:100+window30.total_window_size]),\n                           np.array(train_df[200:200+window30.total_window_size])])\n\nexample_inputs, example_labels = window30.split_window(example_window30)\n\nprint('All shapes are: (batch, time, features)')\nprint(f'Window shape: {example_window30.shape}')\nprint(f'Inputs shape: {example_inputs.shape}')\nprint(f'Labels shape: {example_labels.shape}')","metadata":{"execution":{"iopub.status.busy":"2022-04-14T20:43:34.714816Z","iopub.execute_input":"2022-04-14T20:43:34.715355Z","iopub.status.idle":"2022-04-14T20:43:34.729554Z","shell.execute_reply.started":"2022-04-14T20:43:34.715318Z","shell.execute_reply":"2022-04-14T20:43:34.728811Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"example_window30.example = example_inputs, example_labels","metadata":{"execution":{"iopub.status.busy":"2022-04-14T20:43:34.730778Z","iopub.execute_input":"2022-04-14T20:43:34.731112Z","iopub.status.idle":"2022-04-14T20:43:34.736645Z","shell.execute_reply.started":"2022-04-14T20:43:34.731076Z","shell.execute_reply":"2022-04-14T20:43:34.735918Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Typically, data in TensorFlow is packed into arrays\n* where the outermost index is across examples (the \"batch\" dimension).\n* The middle indices are the \"time\" or \"space\" (width, height) dimension(s). The innermost indices are the features.\n\n* The code above took a batch of three 48-time step windows with 34 features at each time step. It splits them into a batch of 26-time step 19-feature inputs, and a 1-time step 1-feature label. The label only has one feature because the WindowGenerator was initialized with label_columns=['T (degC)']. Initially, this tutorial will build models that predict single output labels.","metadata":{}},{"cell_type":"code","source":"def plot(self, model=None, plot_col='sales', max_subplots=3):\n  inputs, labels = self.example\n  plt.figure(figsize=(12, 8))\n  plot_col_index = self.column_indices[plot_col]\n  max_n = min(max_subplots, len(inputs))\n  for n in range(max_n):\n    plt.subplot(max_n, 1, n+1)\n    plt.ylabel(f'{plot_col} [normed]')\n    plt.plot(self.input_indices, inputs[n, :, plot_col_index],\n             label='Inputs', marker='.', zorder=-10)\n\n    if self.label_columns:\n      label_col_index = self.label_columns_indices.get(plot_col, None)\n    else:\n      label_col_index = plot_col_index\n\n    if label_col_index is None:\n      continue\n\n    plt.scatter(self.label_indices, labels[n, :, label_col_index],\n                edgecolors='k', label='Labels', c='#2ca02c', s=64)\n    if model is not None:\n      predictions = model(inputs)\n      plt.scatter(self.label_indices, predictions[n, :, label_col_index],\n                  marker='X', edgecolors='k', label='Predictions',\n                  c='#ff7f0e', s=64)\n\n    if n == 0:\n      plt.legend()\n\n  plt.xlabel('Time [h]')\n\nWindowGenerator.plot = plot","metadata":{"execution":{"iopub.status.busy":"2022-04-14T20:43:34.737839Z","iopub.execute_input":"2022-04-14T20:43:34.738306Z","iopub.status.idle":"2022-04-14T20:43:34.749568Z","shell.execute_reply.started":"2022-04-14T20:43:34.738263Z","shell.execute_reply":"2022-04-14T20:43:34.748795Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"window1.plot()","metadata":{"execution":{"iopub.status.busy":"2022-04-14T20:43:34.750713Z","iopub.execute_input":"2022-04-14T20:43:34.751362Z","iopub.status.idle":"2022-04-14T20:43:38.253459Z","shell.execute_reply.started":"2022-04-14T20:43:34.751325Z","shell.execute_reply":"2022-04-14T20:43:38.252721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"window1.train.element_spec","metadata":{"execution":{"iopub.status.busy":"2022-04-14T20:43:38.254763Z","iopub.execute_input":"2022-04-14T20:43:38.255161Z","iopub.status.idle":"2022-04-14T20:43:39.591577Z","shell.execute_reply.started":"2022-04-14T20:43:38.255114Z","shell.execute_reply":"2022-04-14T20:43:39.590883Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"window1.val.element_spec","metadata":{"execution":{"iopub.status.busy":"2022-04-14T20:43:39.592831Z","iopub.execute_input":"2022-04-14T20:43:39.593272Z","iopub.status.idle":"2022-04-14T20:43:39.879991Z","shell.execute_reply.started":"2022-04-14T20:43:39.593231Z","shell.execute_reply":"2022-04-14T20:43:39.879279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"window30.plot()","metadata":{"execution":{"iopub.status.busy":"2022-04-14T20:43:39.881231Z","iopub.execute_input":"2022-04-14T20:43:39.881629Z","iopub.status.idle":"2022-04-14T20:43:42.210664Z","shell.execute_reply.started":"2022-04-14T20:43:39.88159Z","shell.execute_reply":"2022-04-14T20:43:42.210005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for example_inputs, example_labels in window1.train.take(1):\n    print(f'Inputs shape (batch, time, features): {example_inputs.shape}')\n    print(f'Labels shape (batch, time, features): {example_labels.shape}')","metadata":{"execution":{"iopub.status.busy":"2022-04-14T20:43:42.211712Z","iopub.execute_input":"2022-04-14T20:43:42.212079Z","iopub.status.idle":"2022-04-14T20:43:44.042552Z","shell.execute_reply.started":"2022-04-14T20:43:42.212045Z","shell.execute_reply":"2022-04-14T20:43:44.04178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Create Baseline model","metadata":{}},{"cell_type":"code","source":"class Baseline(tf.keras.Model):\n  def __init__(self, label_index=None):\n    super().__init__()\n    self.label_index = label_index\n\n  def call(self, inputs):\n    if self.label_index is None:\n      return inputs\n    result = inputs[:, :, self.label_index]\n    return result[:, :, tf.newaxis]","metadata":{"execution":{"iopub.status.busy":"2022-04-14T20:43:44.043996Z","iopub.execute_input":"2022-04-14T20:43:44.04446Z","iopub.status.idle":"2022-04-14T20:43:44.050592Z","shell.execute_reply.started":"2022-04-14T20:43:44.044419Z","shell.execute_reply":"2022-04-14T20:43:44.04985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"window30.val","metadata":{"execution":{"iopub.status.busy":"2022-04-14T20:43:44.051798Z","iopub.execute_input":"2022-04-14T20:43:44.052065Z","iopub.status.idle":"2022-04-14T20:43:44.387699Z","shell.execute_reply.started":"2022-04-14T20:43:44.05203Z","shell.execute_reply":"2022-04-14T20:43:44.386934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"baseline = Baseline(label_index = column_indices['sales'])\nbaseline.compile(loss=tf.losses.MeanSquaredError(),\n                metrics = [tf.metrics.MeanAbsoluteError()])\n\nval_performance = {}\nperformance = {}\nval_performance['Baseline'] = baseline.evaluate(window30.val)\n#performance['Baseline'] = baseline.evaluate(window30.test, verbose=0)","metadata":{"execution":{"iopub.status.busy":"2022-04-14T20:43:44.391783Z","iopub.execute_input":"2022-04-14T20:43:44.392095Z","iopub.status.idle":"2022-04-14T20:44:38.671708Z","shell.execute_reply.started":"2022-04-14T20:43:44.39206Z","shell.execute_reply":"2022-04-14T20:44:38.671025Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Input shape:', window30.example[0].shape)\nprint('Output shape:', baseline(window30.example[0]).shape)","metadata":{"execution":{"iopub.status.busy":"2022-04-14T20:44:38.673263Z","iopub.execute_input":"2022-04-14T20:44:38.67351Z","iopub.status.idle":"2022-04-14T20:44:38.681628Z","shell.execute_reply.started":"2022-04-14T20:44:38.67348Z","shell.execute_reply":"2022-04-14T20:44:38.680686Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Ok now I am going to try using favorite book Deep Learning with Python Time Series example.\n\n### We will use the time series dataset from array() to instatiate three datasets ( Data Objects )\n* One for training, one for validation and one for testing.\n* Note that each **dataset is a tuple ( samples, targets )** samples(batch_size,sequence_length )\n* **sample_rate = 6** Observation will be sampled at one datapoint per hour. We will only keep one datapoint out of 6\n* **sequence_length = 120** Observations will go back 5 days ( 120 hours )\n* **delay** = sampling_rate * ( sequence_length + 24 - 1 ) the target for a sequence will be the temperature 24 hours after the end of the sequence.\n* **train_dataset start_index = 0**, end_index=num_train_samples we will take all num_train_samples\n* **val_dataset** start_index= num_train_samples end_index= num_train_samples + num_val_samples we will take all num_val_samples after num_train_samples\n* **test_dataset** : start_index= num_train_samples + num_val_samples we will take all the rest of datapoint after train_dataset and val_dataset\n* **batch_size = 256** target temperatures\n* The samples are randomly shuffled, so two consecutive sequences in a batch (like samples[0] and samples[1] ) aren't necessarily temporally close.","metadata":{}},{"cell_type":"markdown","source":"### OK Let's Split the data\n* We calculate the number of records for training, validation and testing\n* In this particular case training = 80%, validation = 15% and the rest for training = 5%","metadata":{}},{"cell_type":"code","source":"num_train_samples = int(0.80 * len(train_sales_one_hot))   # number of record for training 80%\nnum_val_samples = int(0.15 * len(train_sales_one_hot))     # number of records for validation 15%\nnum_test_samples = len(train_sales_one_hot) - num_train_samples - num_val_samples\n\nprint(\"Num_train_samples :\", num_train_samples)\nprint(\"num_val_samples : \", num_val_samples)\nprint(\"num_test_samples : \", num_test_samples)","metadata":{"execution":{"iopub.status.busy":"2022-04-14T20:44:38.68343Z","iopub.execute_input":"2022-04-14T20:44:38.684046Z","iopub.status.idle":"2022-04-14T20:44:38.692704Z","shell.execute_reply.started":"2022-04-14T20:44:38.684007Z","shell.execute_reply":"2022-04-14T20:44:38.691365Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### I am going to use the following features:\n*  Date,  store_nbr,   family,  sales\n* To keep the original date unchanged I will use another variable to do the one hot encoding  ( train_sales_one_hot  )\n*  I also convert date timestamp to string ","metadata":{}},{"cell_type":"code","source":"train_sales_features = ['date','store_nbr', 'family','sales']\ntrain_sales_one_hot = train_sales[train_sales_features]\ntrain_sales_one_hot['date'] = str(train_sales_one_hot['date'])\ntrain_sales_one_hot.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-14T20:44:38.694203Z","iopub.execute_input":"2022-04-14T20:44:38.694538Z","iopub.status.idle":"2022-04-14T20:44:38.775204Z","shell.execute_reply.started":"2022-04-14T20:44:38.694501Z","shell.execute_reply":"2022-04-14T20:44:38.774374Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Let's do One Hot Encoding ..","metadata":{}},{"cell_type":"code","source":"# Slice [start:stop:step], starting from index 30 take every 31th record.\n# train_df = train_sales_one_hot[30::31]\ntrain_df = train_sales_one_hot\n#date_time = pd.to_datetime(tain_df.pop('Date Time'), format=''%d.%m.%y')\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-14T20:44:38.776666Z","iopub.execute_input":"2022-04-14T20:44:38.776933Z","iopub.status.idle":"2022-04-14T20:44:38.787586Z","shell.execute_reply.started":"2022-04-14T20:44:38.7769Z","shell.execute_reply":"2022-04-14T20:44:38.786629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Let's normalize sales ","metadata":{}},{"cell_type":"code","source":"train_df['sales'] = (train_df['sales'] - train_df['sales'].mean())/train_df['sales'].std()","metadata":{"execution":{"iopub.status.busy":"2022-04-14T20:44:38.789599Z","iopub.execute_input":"2022-04-14T20:44:38.790309Z","iopub.status.idle":"2022-04-14T20:44:38.834269Z","shell.execute_reply.started":"2022-04-14T20:44:38.790266Z","shell.execute_reply":"2022-04-14T20:44:38.833515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###  Let's take a look at the standarized data","metadata":{}},{"cell_type":"code","source":"train_df.describe().transpose()","metadata":{"execution":{"iopub.status.busy":"2022-04-14T20:44:38.836411Z","iopub.execute_input":"2022-04-14T20:44:38.837191Z","iopub.status.idle":"2022-04-14T20:44:39.142453Z","shell.execute_reply.started":"2022-04-14T20:44:38.837153Z","shell.execute_reply":"2022-04-14T20:44:39.141686Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(train_df)","metadata":{"execution":{"iopub.status.busy":"2022-04-14T20:44:39.143839Z","iopub.execute_input":"2022-04-14T20:44:39.144099Z","iopub.status.idle":"2022-04-14T20:44:39.150424Z","shell.execute_reply.started":"2022-04-14T20:44:39.144063Z","shell.execute_reply":"2022-04-14T20:44:39.149584Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### OK Let's now do one hot encoding ","metadata":{}},{"cell_type":"code","source":"train_df_one_hot = pd.get_dummies(train_df, drop_first=True)","metadata":{"execution":{"iopub.status.busy":"2022-04-14T20:44:39.152225Z","iopub.execute_input":"2022-04-14T20:44:39.152829Z","iopub.status.idle":"2022-04-14T20:44:41.022672Z","shell.execute_reply.started":"2022-04-14T20:44:39.152771Z","shell.execute_reply":"2022-04-14T20:44:41.021795Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df_one_hot.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-14T20:44:41.024388Z","iopub.execute_input":"2022-04-14T20:44:41.024825Z","iopub.status.idle":"2022-04-14T20:44:41.047531Z","shell.execute_reply.started":"2022-04-14T20:44:41.024737Z","shell.execute_reply":"2022-04-14T20:44:41.046584Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### We will use the time series dataset from array() to instatiate three datasets ( Data Objects )\n* One for training, one for validation and one for testing.\n* Note that each **dataset is a tuple ( samples, targets )** samples(batch_size,sequence_length )\n* **sample_rate = 1** Observation will be sampled at one datapoint per day. We will only keep one datapoint out of 1\n* **sequence_length = 300** Observations will go back 30 days \n* **delay** = sampling_rate * ( sequence_length + 24 - 1 ) the target for a sequence will be the sales 30 days after the end of the sequence.\n* **train_dataset start_index = 0**, end_index=num_train_samples we will take all num_train_samples\n* **val_dataset** start_index= num_train_samples end_index= num_train_samples + num_val_samples we will take all num_val_samples after num_train_samples\n* **test_dataset** : start_index= num_train_samples + num_val_samples we will take all the rest of datapoint after train_dataset and val_dataset\n* **batch_size = 256** target temperatures\n* The samples are randomly shuffled, so two consecutive sequences in a batch (like samples[0] and samples[1] ) aren't necessarily temporally close.","metadata":{}},{"cell_type":"code","source":"from posixpath import devnull\nfrom tensorflow import keras \nimport numpy as np\n\nsampling_rate = 1\nsequence_length = 30\ndelay = sampling_rate * (sequence_length + 1 - 1)\nbatch_size =256\ntrain_target = train_df_one_hot['sales']\n# Trainig Dataset.. first 50% of raw_data\ntrain_dataset = keras.utils.timeseries_dataset_from_array(  # train_dataset ( samples,targets) samples(256,120,14) targets(256,)\n    train_df_one_hot[:-delay],\n    targets = train_target[delay:],\n    sampling_rate = sampling_rate,\n    sequence_length = sequence_length,\n    shuffle = True,\n    batch_size = batch_size,\n    start_index = 0,\n    end_index = num_train_samples\n)\n#  Validation dataset .. next 25% of raw_data\nval_dataset = keras.utils.timeseries_dataset_from_array(\n    train_df_one_hot[:-delay],\n    targets=train_target[delay:],\n    sampling_rate = sampling_rate,\n    sequence_length=  sequence_length,\n    shuffle  = True,\n    batch_size = batch_size,\n    start_index = num_train_samples,\n    end_index = num_train_samples + num_val_samples\n)\n# Testing dataset .. last 25% of raw data\ntest_dataset = keras.utils.timeseries_dataset_from_array(\n    train_df_one_hot[:-delay],\n    targets = train_target[delay:],\n    sampling_rate = sampling_rate,\n    sequence_length = sequence_length,\n    shuffle = True,\n    batch_size = batch_size,\n    start_index = num_train_samples + num_val_samples\n)\n\n","metadata":{"execution":{"iopub.status.busy":"2022-04-14T20:44:41.049376Z","iopub.execute_input":"2022-04-14T20:44:41.049873Z","iopub.status.idle":"2022-04-14T20:44:43.408371Z","shell.execute_reply.started":"2022-04-14T20:44:41.049826Z","shell.execute_reply":"2022-04-14T20:44:43.407554Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for samples, targets in train_dataset:\n  print(\"samples shape : \", samples.shape)\n  print(\"targets shape : \", targets.shape)\n  print(\" Len samples : \",len(samples) )\n  break","metadata":{"execution":{"iopub.status.busy":"2022-04-14T20:44:43.411467Z","iopub.execute_input":"2022-04-14T20:44:43.412967Z","iopub.status.idle":"2022-04-14T20:44:44.230368Z","shell.execute_reply.started":"2022-04-14T20:44:43.412923Z","shell.execute_reply":"2022-04-14T20:44:44.229471Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(train_dataset)","metadata":{"execution":{"iopub.status.busy":"2022-04-14T20:44:44.231766Z","iopub.execute_input":"2022-04-14T20:44:44.232299Z","iopub.status.idle":"2022-04-14T20:44:44.239545Z","shell.execute_reply.started":"2022-04-14T20:44:44.232259Z","shell.execute_reply":"2022-04-14T20:44:44.238692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras import layers\n\n# 1.  Build the model\n\ninputs = tf.keras.Input(shape=(sequence_length,  train_df_one_hot.shape[-1] ))\nx = tf.keras.layers.Flatten() ( inputs )\nx = tf.keras.layers.Dense(16, activation ='relu') (x)\noutputs = tf.keras.layers.Dense(1) (x)\nmodel = tf.keras.Model(inputs, outputs)\n\ncallbacks =[\n    tf.keras.callbacks.ModelCheckpoint(\"jena_dense_keras\",\n                                      save_best_only=True)\n    \n]\n\n# 2.  Compile the model\n\nmodel.compile(\n    tf.keras.optimizers.RMSprop(),\n    tf.keras.losses.mse,\n    metrics = ['mae'])\n\n# 3. Fit / Train the model\n\nhistory = model.fit(\n    train_dataset,\n    epochs = 4,\n    validation_data = val_dataset,\n    callbacks = callbacks)\n\nmodel = tf.keras.models.load_model(\"jena_dense_keras\")\nprint(f\"test MAE : {model.evaluate(test_dataset)[1]:2f}\")","metadata":{"execution":{"iopub.status.busy":"2022-04-14T20:48:19.932547Z","iopub.execute_input":"2022-04-14T20:48:19.932844Z","iopub.status.idle":"2022-04-14T21:16:41.781206Z","shell.execute_reply.started":"2022-04-14T20:48:19.932793Z","shell.execute_reply":"2022-04-14T21:16:41.780474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nloss = history.history[\"mae\"]\nval_loss = history.history[\"val_mae\"]\nepochs = range(1, len(loss) +1)\nplt.figure()\nplt.plot(epochs, loss,\"bo\", label=\"Training MAE\")\nplt.plot(epochs, val_loss,\"b\", label=\"Validation MAE\")\nplt.title(\"Training and validation MAE\")\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-14T22:13:32.095866Z","iopub.execute_input":"2022-04-14T22:13:32.09621Z","iopub.status.idle":"2022-04-14T22:13:32.181534Z","shell.execute_reply.started":"2022-04-14T22:13:32.096114Z","shell.execute_reply":"2022-04-14T22:13:32.180306Z"},"trusted":true},"execution_count":null,"outputs":[]}]}