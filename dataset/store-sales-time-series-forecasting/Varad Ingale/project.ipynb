{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# <b>1 <span style='color:#F1C40F'>|</span> Introduction to Date and Time</b>\n\n<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#323232;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:white;\"><b>1.1 | How to import data ?</b></p>\n</div>\n\nFirst, we import all the datasets needed for this kernel. The required time series column is imported as a datetime column using **<span style='color:#F1C40F'>parse_dates</span>** parameter and is also selected as index of the dataframe using **<span style='color:#F1C40F'>index_col</span>** parameter.\n\n<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#323232;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:white;\"><b>1.2 | Timestamps and Periods</b></p>\n</div>\n\nTimestamps are used to represent a point in time. Periods represent an interval in time. Periods can used to check if a specific event in the given period. They can also be converted to each other's form.\n\n\n<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#323232;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:white;\"><b>1.3 | Using date_range</b></p>\n</div>\n\ndate_range is a method that returns a fixed **<span style='color:#F1C40F'>frequency datetimeindex</span>**. It is quite useful when creating your own time series attribute for pre-existing data or arranging the whole data around the time series attribute created by you.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport calendar\nimport datetime\nfrom learntools.time_series.style import *\n\nfrom pathlib import Path\nfrom statsmodels.tsa.deterministic import CalendarFourier, DeterministicProcess\nfrom statsmodels.tsa.stattools import adfuller\nfrom sklearn.preprocessing import OrdinalEncoder,OneHotEncoder\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import LinearRegression, Ridge\nfrom sklearn.model_selection import cross_val_score, train_test_split\nfrom sklearn.metrics import mean_absolute_error\nfrom xgboost import XGBRegressor\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.neighbors import KNeighborsRegressor\n\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\nimport plotly.figure_factory as ff\nimport plotly.offline as offline\nimport plotly.graph_objs as go\n\ncomp_dir = Path('../input/store-sales-time-series-forecasting')\ntrain = pd.read_csv(comp_dir / 'train.csv')\ntest = pd.read_csv(comp_dir / 'test.csv')\nstores = pd.read_csv(comp_dir / 'stores.csv')\noil = pd.read_csv(comp_dir / 'oil.csv')\ntransactions =  pd.read_csv(comp_dir / 'transactions.csv')\nholidays_events = pd.read_csv(comp_dir / 'holidays_events.csv')\n\ndef seasonal_plot(X, y, period, freq, ax=None):\n    if ax is None:\n        _, ax = plt.subplots()\n    palette = sns.color_palette(\"husl\", n_colors=X[period].nunique(),)\n    ax = sns.lineplot(\n        x=freq,\n        y=y,\n        hue=period,\n        data=X,\n        ci=False,\n        ax=ax,\n        palette=palette,\n        legend=False,\n    )\n    for line, name in zip(ax.lines, X[period].unique()):\n        y_ = line.get_ydata()[-1]\n        ax.annotate(\n            name,\n            xy=(1, y_),\n            xytext=(6, 0),\n            color=line.get_color(),\n            xycoords=ax.get_yaxis_transform(),\n            textcoords=\"offset points\",\n            size=14,\n            va=\"center\",\n        )\n    return ax\n\ndef plot_periodogram(ts, detrend='linear', ax=None):\n    from scipy.signal import periodogram\n    fs = pd.Timedelta(\"1Y\") / pd.Timedelta(\"1D\")\n    freqencies, spectrum = periodogram(\n        ts,\n        fs=fs,\n        detrend=detrend,\n        window=\"boxcar\",\n        scaling='spectrum',\n    )\n    if ax is None:\n        _, ax = plt.subplots()\n    ax.step(freqencies, spectrum, color=\"purple\")\n    ax.set_xscale(\"log\")\n    ax.set_xticks([1, 2, 4, 6, 12, 26, 52, 104])\n    ax.set_xticklabels(\n        [\n            \"Annual (1)\",\n            \"Semiannual (2)\",\n            \"Quarterly (4)\",\n            \"Bimonthly (6)\",\n            \"Monthly (12)\",\n            \"Biweekly (26)\",\n            \"Weekly (52)\",\n            \"Semiweekly (104)\",\n        ],\n        rotation=30,\n    )\n    ax.ticklabel_format(axis=\"y\", style=\"sci\", scilimits=(0, 0))\n    ax.set_ylabel(\"Variance\")\n    ax.set_title(\"Periodogram\")\n    return ax\n\ndef lagplot(x, y=None, lag=1, standardize=False, ax=None, **kwargs):\n    from matplotlib.offsetbox import AnchoredText\n    x_ = x.shift(lag)\n    if standardize:\n        x_ = (x_ - x_.mean()) / x_.std()\n    if y is not None:\n        y_ = (y - y.mean()) / y.std() if standardize else y\n    else:\n        y_ = x\n    corr = y_.corr(x_)\n    if ax is None:\n        fig, ax = plt.subplots()\n    scatter_kws = dict(\n        alpha=0.75,\n        s=3,\n    )\n    line_kws = dict(color='C3', )\n    ax = sns.regplot(x=x_,\n                     y=y_,\n                     scatter_kws=scatter_kws,\n                     line_kws=line_kws,\n                     lowess=True,\n                     ax=ax,\n                     **kwargs)\n    at = AnchoredText(\n        f\"{corr:.2f}\",\n        prop=dict(size=\"large\"),\n        frameon=True,\n        loc=\"upper left\",\n    )\n    at.patch.set_boxstyle(\"square, pad=0.0\")\n    ax.add_artist(at)\n    ax.set(title=f\"Lag {lag}\", xlabel=x_.name, ylabel=y_.name)\n    return ax\n\ndef plot_lags(x, y=None, lags=6, nrows=1, lagplot_kwargs={}, **kwargs):\n    import math\n    kwargs.setdefault('nrows', nrows)\n    kwargs.setdefault('ncols', math.ceil(lags / nrows))\n    kwargs.setdefault('figsize', (kwargs['ncols'] * 2, nrows * 2 + 0.5))\n    fig, axs = plt.subplots(sharex=True, sharey=True, squeeze=False, **kwargs)\n    for ax, k in zip(fig.get_axes(), range(kwargs['nrows'] * kwargs['ncols'])):\n        if k + 1 <= lags:\n            ax = lagplot(x, y, lag=k + 1, ax=ax, **lagplot_kwargs)\n            ax.set_title(f\"Lag {k + 1}\", fontdict=dict(fontsize=14))\n            ax.set(xlabel=\"\", ylabel=\"\")\n        else:\n            ax.axis('off')\n    plt.setp(axs[-1, :], xlabel=x.name)\n    plt.setp(axs[:, 0], ylabel=y.name if y is not None else x.name)\n    fig.tight_layout(w_pad=0.1, h_pad=0.1)\n    return fig","metadata":{"_uuid":"9c45147d-c8b2-44b2-b72e-e78eacf4bb9e","_cell_guid":"4a8f53c4-de09-43b7-a941-15bff3ba2120","collapsed":false,"_kg_hide-input":true,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-06-18T08:45:05.60184Z","iopub.execute_input":"2022-06-18T08:45:05.602289Z","iopub.status.idle":"2022-06-18T08:45:13.59751Z","shell.execute_reply.started":"2022-06-18T08:45:05.602224Z","shell.execute_reply":"2022-06-18T08:45:13.596558Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will break down the date into different columns: \n* One for the year\n* One for the month\n* One for the week\n* One for the quarter of the year\n* One for the day of the week","metadata":{"_uuid":"e563d2c4-d05f-4f23-b8c3-28cafd7bc647","_cell_guid":"920a7185-1335-4760-bd92-76103698e003","trusted":true}},{"cell_type":"code","source":"df_data = pd.concat([train, test], sort=True)\ndf_data = df_data.merge(stores, how=\"left\", on='store_nbr')   \ndf_data = df_data.merge(oil, how=\"left\", on='date')      \ndf_data = df_data.merge(transactions, how=\"left\", on=['date','store_nbr'])  \ndf_data = df_data.merge(holidays_events,on='date',how='left')\ndf_data = df_data.rename(columns={'type_x' : 'store_type','type_y':'holiday_type'})\n\ndf_data.date = pd.to_datetime(df_data.date)\ndf_data['year'] = df_data['date'].dt.year\ndf_data['month'] = df_data['date'].dt.month\ndf_data['week'] = df_data['date'].dt.isocalendar().week\ndf_data['quarter'] = df_data['date'].dt.quarter\ndf_data['day_of_week'] = df_data['date'].dt.day_name()\ndf_data.head()","metadata":{"_uuid":"1ec59133-7615-440e-b149-a0061908fa87","_cell_guid":"a2b27fc9-0ec8-4715-b7bf-d016e3c6a683","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-06-18T08:45:13.599367Z","iopub.execute_input":"2022-06-18T08:45:13.599698Z","iopub.status.idle":"2022-06-18T08:45:20.988608Z","shell.execute_reply.started":"2022-06-18T08:45:13.599665Z","shell.execute_reply":"2022-06-18T08:45:20.987735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <b>2 <span style='color:#F1C40F'>|</span> Missing Values</b>\n\n<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#323232;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:white;\"><b>2.1 | Oil Price</b></p>\n</div>\n\nLet's start with oil missing values. Firstly, we are going to plot oil price during the years together with trending graph.","metadata":{"_uuid":"a6d7d020-a74e-4606-8dc2-8f331e2635d8","_cell_guid":"ee7c8d0b-8101-4010-9ae4-74eacab8154e","trusted":true}},{"cell_type":"code","source":"moving_average_oil = oil.rolling(\n    window=365,       # 365-day window\n    center=True,      # puts the average at the center of the window\n    min_periods=183,  # choose about half the window size\n).median()              # compute the mean (could also do median, std, min, max, ...)\nmoving_average_oil['date'] = oil['date']\nmoving_average_oil.loc[[0,1],'dcoilwtico'] = moving_average_oil.loc[2,'dcoilwtico']\nmoving_average_oil.date = pd.to_datetime(moving_average_oil.date)\n\ndf_yr_oil = oil[['date','dcoilwtico']]\nfig = make_subplots(rows=1, cols=1, vertical_spacing=0.08,                    \n                    subplot_titles=(\"Oil price during time\"))\nfig.add_trace(go.Scatter(x=df_yr_oil['date'], y=df_yr_oil['dcoilwtico'], mode='lines', fill='tozeroy', fillcolor='#c6ccd8',\n                     marker=dict(color= '#496595'), name='Oil price'), \n                     row=1, col=1)\nfig.add_trace(go.Scatter(x=moving_average_oil.date,y=moving_average_oil.dcoilwtico,mode='lines',name='Trend'))\nfig.update_layout(height=350, bargap=0.15,\n                  margin=dict(b=0,r=20,l=20), \n                  title_text=\"Oil price trend during time\",\n                  template=\"plotly_white\",\n                  title_font=dict(size=25, color='#8a8d93', family=\"Lato, sans-serif\"),\n                  font=dict(color='#8a8d93'),\n                  hoverlabel=dict(bgcolor=\"#f2f2f2\", font_size=13, font_family=\"Lato, sans-serif\"),\n                  showlegend=False)\nfig.show()","metadata":{"_uuid":"9fd03edc-7da2-4f19-856b-f12bd918cb7f","_cell_guid":"207ba451-a6d4-4ce9-963f-c1d943a82dd5","execution":{"iopub.status.busy":"2022-06-18T08:45:20.990034Z","iopub.execute_input":"2022-06-18T08:45:20.990995Z","iopub.status.idle":"2022-06-18T08:45:21.920458Z","shell.execute_reply.started":"2022-06-18T08:45:20.99095Z","shell.execute_reply":"2022-06-18T08:45:21.919702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"📌 **Interpret:** As can be seen in the graph above, we can divide the oil price trend into **<span style='color:#F1C40F'>three phases</span>**. The first and last of these, Jan2013-Jul2014 and Jan2015-Jul2107 respectively, show stabilised trends with ups and downs. However, in the second phase, Jul2014-Jan2015, oil prices decrease considerably.\n\nNow, taking into account the issue of missing values for oil price, we are going to fill them by **<span style='color:#F1C40F'>backward fill technique</span>**. That means filling missing values with next data point (Forward filling means fill missing values with previous data).","metadata":{"_uuid":"8f68aed2-55f7-43cf-a57b-a8ab8f897764","_cell_guid":"d6306409-a967-4842-8f8a-a512975d6ab9","trusted":true}},{"cell_type":"code","source":"df_data['dcoilwtico'] = df_data['dcoilwtico'].fillna(method='bfill')\ndf_data.dcoilwtico.isnull().sum()","metadata":{"_uuid":"43481a11-dd35-4016-9512-e3f3f015025d","_cell_guid":"88798114-e87a-43df-8664-a182c922a7a9","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-06-18T08:45:21.922221Z","iopub.execute_input":"2022-06-18T08:45:21.923611Z","iopub.status.idle":"2022-06-18T08:45:21.973373Z","shell.execute_reply.started":"2022-06-18T08:45:21.923569Z","shell.execute_reply":"2022-06-18T08:45:21.972429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#323232;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:white;\"><b>2.2 | Transactions</b></p>\n</div>\n\nWith respect to transactions, we understand that since there is no data recorded, this is 0.","metadata":{"_uuid":"06a49cda-fdf2-42a7-a8a5-e54716320d48","_cell_guid":"b245be77-96eb-488b-a4e6-11882d4bef8f","trusted":true}},{"cell_type":"code","source":"df_data.transactions = df_data.transactions.replace(np.nan,0)","metadata":{"_uuid":"0cbfc138-16d2-4b9b-acdd-444ba9c46bd4","_cell_guid":"7ffdd0e3-6891-448c-ac36-1955c4ae4c81","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-06-18T08:45:21.978035Z","iopub.execute_input":"2022-06-18T08:45:21.978699Z","iopub.status.idle":"2022-06-18T08:45:22.015812Z","shell.execute_reply.started":"2022-06-18T08:45:21.978637Z","shell.execute_reply":"2022-06-18T08:45:22.014912Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#323232;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:white;\"><b>2.3 | Holidays</b></p>\n</div>\n\nAs we can see above the `holidays_events` DataFrame contains a row for each of the national, regional or local holidays. The transferred column refers to whether the holiday has been moved or not. We assume then that the missing data corresponding to this DataFrame in the training set correspond to those days for which no public holiday has been recorded. Therefore, we will replace the `type` by **<span style='color:#F1C40F'>Working day</span>**. The rest of the categorical variables in this DataFrame will be changed to the empty string, and in `transferred` we will set all values to `false`.","metadata":{"_uuid":"734f0133-fcb2-4e0d-99f7-a3c587addc99","_cell_guid":"f2b65473-1446-4dcb-80b3-44e14d360191","trusted":true}},{"cell_type":"code","source":"df_data[['locale','locale_name', 'description']] = df_data[['locale','locale_name', 'description']].replace(np.nan,'')\ndf_data['holiday_type'] = df_data['holiday_type'].replace(np.nan,'Working Day')\ndf_data['transferred'] = df_data['transferred'].replace(np.nan,False)","metadata":{"_uuid":"56d04b8c-a5de-42d8-b4eb-d7bcf987ddac","_cell_guid":"7d733fc0-67fb-4491-882b-7adba4d9e8b1","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-06-18T08:45:22.020292Z","iopub.execute_input":"2022-06-18T08:45:22.020949Z","iopub.status.idle":"2022-06-18T08:45:24.538616Z","shell.execute_reply.started":"2022-06-18T08:45:22.020911Z","shell.execute_reply":"2022-06-18T08:45:24.5378Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <b>3 <span style='color:#F1C40F'>|</span> Data Visualization</b>\n\n<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#323232;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:white;\"><b>3.1 | Average Sales Analysis</b></p>\n</div>\n\nIn this section we are going to carry out various studies of the data obtained above, using various graphs. We will focus on seeing: \n* The shops with the **<span style='color:#F1C40F'>highest percentage of sales</span>**\n* The **<span style='color:#F1C40F'>types of products most sold</span>**. \n* The sales of each **<span style='color:#F1C40F'>cluster</span>**.\n* The **<span style='color:#F1C40F'>sales history</span>** for each of the months of the year. \n* The percentages of sales per **<span style='color:#F1C40F'>quarter</span>** of the year.\n* **<span style='color:#F1C40F'>Average sales per week</span>**.","metadata":{"_uuid":"c90b60ba-0c7a-430e-9528-ba2e54c363da","_cell_guid":"cdc3b62e-8d5b-43ba-8972-5ea50a51a044","trusted":true}},{"cell_type":"code","source":"# data\n# Agrupamos por tipo de tienda, y al DataFrame le añadimos un único campo 'sales' con la media de los precios de venta ordenados ascendentemente\ndf_st_sa = df_data[:train.shape[0]].groupby('store_type').agg({\"sales\" : \"mean\"}).reset_index().sort_values(by='sales', ascending=False)\ndf_fa_sa = df_data[:train.shape[0]].groupby('family').agg({\"sales\" : \"mean\"}).reset_index().sort_values(by='sales', ascending=False)[:10]\ndf_cl_sa = df_data[:train.shape[0]].groupby('cluster').agg({\"sales\" : \"mean\"}).reset_index() \n# chart color\ndf_fa_sa['color'] = '#496595'\ndf_fa_sa['color'][2:] = '#c6ccd8'\ndf_cl_sa['color'] = '#c6ccd8'\n\n# chart\nfig = make_subplots(rows=2, cols=2, \n                    specs=[[{\"type\": \"bar\"}, {\"type\": \"pie\"}],\n                           [{\"colspan\": 2}, None]],\n                    column_widths=[0.7, 0.3], vertical_spacing=0, horizontal_spacing=0.02,\n                    subplot_titles=(\"Top 10 Highest Product Sales\", \"Highest Sales in Stores\", \"Clusters Vs Sales\"))\n\nfig.add_trace(go.Bar(x=df_fa_sa['sales'], y=df_fa_sa['family'], marker=dict(color= df_fa_sa['color']),\n                     name='Family', orientation='h'), \n                     row=1, col=1)\nfig.add_trace(go.Pie(values=df_st_sa['sales'], labels=df_st_sa['store_type'], name='Store type',\n                     marker=dict(colors=['#334668','#496595','#6D83AA','#91A2BF','#C8D0DF']), hole=0.7,\n                     hoverinfo='label+percent+value', textinfo='label'), \n                    row=1, col=2)\nfig.add_trace(go.Bar(x=df_cl_sa['cluster'], y=df_cl_sa['sales'], \n                     marker=dict(color= df_cl_sa['color']), name='Cluster'), \n                     row=2, col=1)\n\n# styling\nfig.update_yaxes(showgrid=False, ticksuffix=' ', categoryorder='total ascending', row=1, col=1)\nfig.update_xaxes(visible=False, row=1, col=1)\nfig.update_xaxes(tickmode = 'array', tickvals=df_cl_sa.cluster, ticktext=[i for i in range(1,17)], row=2, col=1)\nfig.update_yaxes(visible=False, row=2, col=1)\nfig.update_layout(height=500, bargap=0.2,\n                  margin=dict(b=0,r=20,l=20), xaxis=dict(tickmode='linear'),\n                  title_text=\"Average Sales Analysis\",\n                  template=\"plotly_white\",\n                  title_font=dict(size=29, color='#8a8d93', family=\"Lato, sans-serif\"),\n                  font=dict(color='#8a8d93'), \n                  hoverlabel=dict(bgcolor=\"#f2f2f2\", font_size=13, font_family=\"Lato, sans-serif\"),\n                  showlegend=False)\nfig.show()","metadata":{"_uuid":"6373f379-773e-433f-b101-2f749ac8ab02","_cell_guid":"96a02bc3-9b3c-4cc6-94ae-ea1c0e19d3ea","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-06-18T08:45:24.540067Z","iopub.execute_input":"2022-06-18T08:45:24.540427Z","iopub.status.idle":"2022-06-18T08:45:25.196906Z","shell.execute_reply.started":"2022-06-18T08:45:24.540392Z","shell.execute_reply":"2022-06-18T08:45:25.196159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"📌 **Interpret:** Highest sales are made by the products like **<span style='color:#F1C40F'>grocery and beverages</span>**.\nStore A has the highest sales which is 38%.","metadata":{"_uuid":"e442eb17-0606-4fa6-85a5-7ee22ce9ecf4","_cell_guid":"38411049-374f-4539-a1b6-13b1216b2f2d","trusted":true}},{"cell_type":"code","source":"# data \ndf_2013 = df_data[df_data['year']==2013][:train.shape[0]][['month','sales']]\ndf_2013 = df_2013.groupby('month').agg({\"sales\" : \"mean\"}).reset_index().rename(columns={'sales':'s13'})\ndf_2014 = df_data[df_data['year']==2014][:train.shape[0]][['month','sales']]\ndf_2014 = df_2014.groupby('month').agg({\"sales\" : \"mean\"}).reset_index().rename(columns={'sales':'s14'})\ndf_2015 = df_data[df_data['year']==2015][:train.shape[0]][['month','sales']]\ndf_2015 = df_2015.groupby('month').agg({\"sales\" : \"mean\"}).reset_index().rename(columns={'sales':'s15'})\ndf_2016 = df_data[df_data['year']==2016][:train.shape[0]][['month','sales']]\ndf_2016 = df_2016.groupby('month').agg({\"sales\" : \"mean\"}).reset_index().rename(columns={'sales':'s16'})\ndf_2017 = df_data[df_data['year']==2017][:train.shape[0]][['month','sales']]\ndf_2017 = df_2017.groupby('month').agg({\"sales\" : \"mean\"}).reset_index()\ndf_2017_no = pd.DataFrame({'month': [9,10,11,12], 'sales':[0,0,0,0]})\ndf_2017 = df_2017.append(df_2017_no).rename(columns={'sales':'s17'})\ndf_year = df_2013.merge(df_2014,on='month').merge(df_2015,on='month').merge(df_2016,on='month').merge(df_2017,on='month')\n\n# top levels\ntop_labels = ['2013', '2014', '2015', '2016', '2017']\n\ncolors = ['rgba(38, 24, 74, 0.8)', 'rgba(71, 58, 131, 0.8)',\n          'rgba(122, 120, 168, 0.8)', 'rgba(164, 163, 204, 0.85)',\n          'rgba(190, 192, 213, 1)']\n\n# X axis value \ndf_year = df_year[['s13','s14','s15','s16','s17']].replace(np.nan,0)\nx_data = df_year.values\n\n# y axis value (Month)\ndf_2013['month'] =['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']\ny_data = df_2013['month'].tolist()\n\nfig = go.Figure()\nfor i in range(0, len(x_data[0])):\n    for xd, yd in zip(x_data, y_data):\n        fig.add_trace(go.Bar(\n            x=[xd[i]], y=[yd],\n            orientation='h',\n            marker=dict(\n                color=colors[i],\n                line=dict(color='rgb(248, 248, 249)', width=1)\n            )\n        ))\n\nfig.update_layout(title='Avg Sales for each Year',\n    xaxis=dict(showgrid=False, \n               zeroline=False, domain=[0.15, 1]),\n    yaxis=dict(showgrid=False, showline=False,\n               showticklabels=False, zeroline=False),\n    barmode='stack', \n    template=\"plotly_white\",\n    margin=dict(l=0, r=50, t=100, b=10),\n    showlegend=False, \n)\n\nannotations = []\nfor yd, xd in zip(y_data, x_data):\n    # labeling the y-axis\n    annotations.append(dict(xref='paper', yref='y',\n                            x=0.14, y=yd,\n                            xanchor='right',\n                            text=str(yd),\n                            font=dict(family='Arial', size=14,\n                                      color='rgb(67, 67, 67)'),\n                            showarrow=False, align='right'))\n    # labeling the first Likert scale (on the top)\n    if yd == y_data[-1]:\n        annotations.append(dict(xref='x', yref='paper',\n                                x=xd[0] / 2, y=1.1,\n                                text=top_labels[0],\n                                font=dict(family='Arial', size=14,\n                                          color='rgb(67, 67, 67)'),\n                          showarrow=False))\n    space = xd[0]\n    for i in range(1, len(xd)):\n            # labeling the Likert scale\n            if yd == y_data[-1]:\n                annotations.append(dict(xref='x', yref='paper',\n                                        x=space + (xd[i]/2), y=1.1,\n                                        text=top_labels[i],\n                                        font=dict(family='Arial', size=14,\n                                                  color='rgb(67, 67, 67)'),\n                                        showarrow=False))\n            space += xd[i]\nfig.update_layout(\n    annotations=annotations)\nfig.show()","metadata":{"_uuid":"0613172a-d786-4789-8d1c-5b8b4ee5c5a4","_cell_guid":"4c21e1c6-ac46-4c35-965b-20eb7ab5b7c2","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-06-18T08:45:25.198436Z","iopub.execute_input":"2022-06-18T08:45:25.198808Z","iopub.status.idle":"2022-06-18T08:45:25.883476Z","shell.execute_reply.started":"2022-06-18T08:45:25.198774Z","shell.execute_reply":"2022-06-18T08:45:25.882724Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"📌 **Interpret:** Highest sales are made in **<span style='color:#F1C40F'>December</span>** month and then decreases in January. Sales are **<span style='color:#F1C40F'>increasing gradually</span>** from 2013 to 2017. Note: We don't have data for 2017: 9th to 12th month.","metadata":{"_uuid":"03f01bf9-1130-474b-ba86-385b0735499b","_cell_guid":"dfea3a76-75bf-4568-9eaf-90b927a42d6a","trusted":true}},{"cell_type":"code","source":"# data\ndf_m_sa = df_data[:train.shape[0]].groupby('month').agg({\"sales\" : \"mean\"}).reset_index()\ndf_m_sa['sales'] = round(df_m_sa['sales'],2)\ndf_m_sa['month_text'] = df_m_sa['month'].apply(lambda x: calendar.month_abbr[x])\ndf_m_sa['text'] = df_m_sa['month_text'] + ' - ' + df_m_sa['sales'].astype(str) \n\ndf_w_sa = df_data[:train.shape[0]].groupby('week').agg({\"sales\" : \"mean\"}).reset_index() \ndf_q_sa = df_data[:train.shape[0]].groupby('quarter').agg({\"sales\" : \"mean\"}).reset_index() \n# chart color\ndf_m_sa['color'] = '#496595'\ndf_m_sa['color'][:-1] = '#c6ccd8'\ndf_w_sa['color'] = '#c6ccd8'\n\n# chart\nfig = make_subplots(rows=2, cols=2, vertical_spacing=0.08,\n                    row_heights=[0.7, 0.3], \n                    specs=[[{\"type\": \"bar\"}, {\"type\": \"pie\"}],\n                           [{\"colspan\": 2}, None]],\n                    column_widths=[0.7, 0.3],\n                    subplot_titles=(\"Month wise Avg Sales Analysis\", \"Quarter wise Avg Sales Analysis\", \n                                    \"Week wise Avg Sales Analysis\"))\n\nfig.add_trace(go.Bar(x=df_m_sa['sales'], y=df_m_sa['month'], marker=dict(color= df_m_sa['color']),\n                     text=df_m_sa['text'],textposition='auto',\n                     name='Month', orientation='h'), \n                     row=1, col=1)\nfig.add_trace(go.Pie(values=df_q_sa['sales'], labels=df_q_sa['quarter'], name='Quarter',\n                     marker=dict(colors=['#334668','#496595','#6D83AA','#91A2BF','#C8D0DF']), hole=0.7,\n                     hoverinfo='label+percent+value', textinfo='label+percent'), \n                     row=1, col=2)\nfig.add_trace(go.Scatter(x=df_w_sa['week'], y=df_w_sa['sales'], mode='lines+markers', fill='tozeroy', fillcolor='#c6ccd8',\n                     marker=dict(color= '#496595'), name='Week'), \n                     row=2, col=1)\n\n# styling\nfig.update_yaxes(visible=False, row=1, col=1)\nfig.update_xaxes(visible=False, row=1, col=1)\nfig.update_xaxes(tickmode = 'array', tickvals=df_w_sa.week, ticktext=[i for i in range(1,53)], \n                 row=2, col=1)\nfig.update_yaxes(visible=False, row=2, col=1)\nfig.update_layout(height=750, bargap=0.15,\n                  margin=dict(b=0,r=20,l=20), \n                  title_text=\"Average Sales Analysis\",\n                  template=\"plotly_white\",\n                  title_font=dict(size=25, color='#8a8d93', family=\"Lato, sans-serif\"),\n                  font=dict(color='#8a8d93'),\n                  hoverlabel=dict(bgcolor=\"#f2f2f2\", font_size=13, font_family=\"Lato, sans-serif\"),\n                  showlegend=False)\nfig.show()","metadata":{"_uuid":"e712c090-895f-4ffd-b167-a70f1afcbcf6","_cell_guid":"1048609b-6675-4e8d-9427-620bb1bda06b","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-06-18T08:45:25.884987Z","iopub.execute_input":"2022-06-18T08:45:25.885352Z","iopub.status.idle":"2022-06-18T08:45:26.118078Z","shell.execute_reply.started":"2022-06-18T08:45:25.885318Z","shell.execute_reply":"2022-06-18T08:45:26.117215Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"📌 **Interpret:** Highest sales are made in the **<span style='color:#F1C40F'>last quarter</span>** of the year, followed by the third. The one with less saling is the first one.","metadata":{"_uuid":"a84577dc-d23f-4d54-80c9-528607fa1eb2","_cell_guid":"3cc98d38-8114-425a-b35d-e6afa7e66cdb","trusted":true}},{"cell_type":"code","source":"# data\ndf_dw_sa = df_data[:train.shape[0]].groupby('day_of_week').agg({\"sales\" : \"mean\"}).reset_index()\ndf_dw_sa.sales = round(df_dw_sa.sales, 2)\n\n# chart\nfig = px.bar(df_dw_sa, y='day_of_week', x='sales', title='Avg Sales vs Day of Week',\n             color_discrete_sequence=['#c6ccd8'], text='sales',\n             category_orders=dict(day_of_week=[\"Monday\",\"Tuesday\",\"Wednesday\",\"Thursday\", \"Friday\",\"Saturday\",\"Sunday\"]))\nfig.update_yaxes(showgrid=False, ticksuffix=' ', showline=False)\nfig.update_xaxes(visible=False)\nfig.update_layout(margin=dict(t=60, b=0, l=0, r=0), height=350,\n                  hovermode=\"y unified\", \n                  yaxis_title=\" \", template='plotly_white',\n                  title_font=dict(size=25, color='#8a8d93', family=\"Lato, sans-serif\"),\n                  font=dict(color='#8a8d93'),\n                  hoverlabel=dict(bgcolor=\"#c6ccd8\", font_size=13, font_family=\"Lato, sans-serif\"))","metadata":{"_uuid":"00d80fff-4891-4207-8554-8e739189a125","_cell_guid":"cca7616b-e70f-46d9-8b61-0555da4e2b95","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-06-18T08:45:26.121098Z","iopub.execute_input":"2022-06-18T08:45:26.121527Z","iopub.status.idle":"2022-06-18T08:45:26.468916Z","shell.execute_reply.started":"2022-06-18T08:45:26.121488Z","shell.execute_reply":"2022-06-18T08:45:26.468154Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"📌 **Interpret:** Highest sales are made in the **<span style='color:#F1C40F'>weekend</span>**. Surprisingly, Mondays are the third day with most sales.","metadata":{"_uuid":"c5aa5bd2-bffa-421b-9461-5d66fed3367d","_cell_guid":"a72adb66-5353-4715-970c-5652c5fe0588","trusted":true}},{"cell_type":"code","source":"df_train = df_data[:train.shape[0]][['state','sales','store_type','year']]\nfig = plt.figure(figsize=(22,8))\nsns.set_style('whitegrid')\nmy_palette = ['#C8D0DF','#91A2BF','#6D83AA','#496595','#334668']\nsns.barplot(x='state',y='sales',hue = 'year', palette = my_palette, data=df_train[df_train['store_type'] == 'A'])\nplt.title(\"State vs Sales of Store A (per year)\")","metadata":{"_uuid":"7079d848-7647-42b9-85b5-f285d929d192","_cell_guid":"c7c4ba1e-52da-47cd-8a5a-44e260fbf6e5","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-06-18T08:45:26.470367Z","iopub.execute_input":"2022-06-18T08:45:26.47093Z","iopub.status.idle":"2022-06-18T08:45:32.758416Z","shell.execute_reply.started":"2022-06-18T08:45:26.470894Z","shell.execute_reply":"2022-06-18T08:45:32.757711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"📌 **Interpret:** Highest sales are made in **<span style='color:#F1C40F'>Pichincha</span>** state. Sales have been increasing during the recorded period in every state, except Manabi where Store A is new in 2017.\n# <b>4 <span style='color:#F1C40F'>|</span> Time Series Components</b>\n\nIf we assume an **<span style='color:#F1C40F'>additive decomposition</span>**, then we can write $𝑦_𝑡=𝑆_𝑡+𝑇_𝑡+𝑅_𝑡$, where $𝑦_𝑡$ is the data, $S_t$ is the seasonal component, $𝑇_𝑡$ is the trend-cycle component and $𝑅_𝑡$ is the residual component, all at period 𝑡. Also,for a **<span style='color:#F1C40F'>multiplicative decomposition</span>**, we have\n$𝑦_𝑡=𝑆_𝑡∗𝑇_𝑡∗𝑅_𝑡$.\n\nThe additive decomposition is the most appropriate if the magnitude of the seasonal fluctuations, or the variation around the trend-cycle, does not vary with the level of the time series. When the variation in the seasonal pattern, or the variation around the trend-cycle, appears to be proportional to the level of the time series, then a multiplicative decomposition is more appropriate. Multiplicative decompositions are common with economic time series.\n\n<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#323232;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:white;\"><b>4.1 | Trend</b></p>\n</div>\n\n### **What is Trend ?**\n\nThe trend component of a time series represents a **<span style='color:#F1C40F'>persistent, long-term change in the mean of the series</span>**. The trend is the slowest-moving part of a series, the part representing the largest time scale of importance. In a time series of product sales, an increasing trend might be the effect of a market expansion as more people become aware of the product year by year.\n\n### **Moving Average Plot**\nTo see what kind of trend a time series might have, we can use a moving average plot. To compute a moving average of a time series, we compute the average of the values within a **<span style='color:#F1C40F'>sliding window</span>** of some defined width. Each point on the graph represents the average of all the values in the series that fall within the window on either side. The idea is to **<span style='color:#F1C40F'>smooth out</span>** any short-term **<span style='color:#F1C40F'>fluctuations</span>** in the series so that only long-term changes remain.","metadata":{"_uuid":"013cb807-c9cb-4314-afca-1de086554cf9","_cell_guid":"dc4c289a-e97f-46c0-b554-c3e16d75a8d9","trusted":true}},{"cell_type":"code","source":"sales = df_data[:train.shape[0]].groupby('date').agg({\"sales\" : \"mean\"}).reset_index()\nsales.set_index('date',inplace=True)\nmoving_average = sales.rolling(\n    window=365,       # 365-day window\n    center=True,      # puts the average at the center of the window\n    min_periods=183,  # choose about half the window size\n).mean()              # compute the mean (could also do median, std, min, max, ...)\nmoving_average['date'] = sales.index\n\nfig = make_subplots(rows=1, cols=1, vertical_spacing=0.08,                    \n                    subplot_titles=(\"Sales 365 - Day Moving Average\"))\nfig.add_trace(go.Scatter(x=sales.index, y=sales['sales'], mode='lines', fill='tozeroy', fillcolor='#c6ccd8',\n                     marker=dict(color= '#334668'), name='365-Day Moving Average'))\nfig.add_trace(go.Scatter(x=moving_average.date,y=moving_average.sales,mode='lines',name='Trend'))\nfig.update_layout(height=350, bargap=0.15,\n                  margin=dict(b=0,r=20,l=20), \n                  title_text=\"Sales trend during years\",\n                  template=\"plotly_white\",\n                  title_font=dict(size=25, color='#8a8d93', family=\"Lato, sans-serif\"),\n                  font=dict(color='#8a8d93'),\n                  hoverlabel=dict(bgcolor=\"#f2f2f2\", font_size=13, font_family=\"Lato, sans-serif\"),\n                  showlegend=False)\nfig.show()","metadata":{"_uuid":"391a7f61-11dc-401f-a242-add91c41f57f","_cell_guid":"eab7a401-a45d-4893-a000-eb8734360b64","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-06-18T08:45:32.75964Z","iopub.execute_input":"2022-06-18T08:45:32.760155Z","iopub.status.idle":"2022-06-18T08:45:33.057956Z","shell.execute_reply.started":"2022-06-18T08:45:32.760118Z","shell.execute_reply":"2022-06-18T08:45:33.057181Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"📌 **Interpret:** As we can appreeciate, sales has an constantly increasing trend during recorded years. \n<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#323232;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:white;\"><b>4.2 | Seasonality </b></p>\n</div>\n\nWe say that a time series exhibits seasonality whenever there is a **<span style='color:#F1C40F'>regular, periodic change</span>** in the mean of the series. Seasonal changes generally follow the clock and calendar - repetitions over a day, a week, or a year are common. Seasonality is often driven by the cycles of the natural world over days and years or by conventions of social behavior surrounding dates and times.","metadata":{}},{"cell_type":"code","source":"store_sales = pd.read_csv(\n    comp_dir / 'train.csv',\n    usecols=['store_nbr', 'family', 'date', 'sales'],\n    dtype={\n        'store_nbr': 'category',\n        'family': 'category',\n        'sales': 'float32',\n    },\n    parse_dates=['date'],\n    infer_datetime_format=True,\n)\nstore_sales['date'] = store_sales.date.dt.to_period('D')\nstore_sales = store_sales.set_index(['store_nbr', 'family', 'date']).sort_index()\naverage_sales = (\n    store_sales\n    .groupby('date').mean()\n    .squeeze()\n    #.loc['2017']\n)\n\nX = average_sales.to_frame()\nX[\"week\"] = X.index.week\nX[\"day\"] = X.index.dayofweek\nX['year'] = X.index.year\nX['dayofyear'] = X.index.dayofyear\nfig,(ax0,ax1) = plt.subplots(1, 2, figsize=(22, 10))\nseasonal_plot(X.loc['2017'], y='sales', period=\"week\", freq=\"day\", ax=ax0)\nax0.set_title('Seasonal Plot (week/day) 2017')\nseasonal_plot(X, y=\"sales\", period=\"year\", freq=\"dayofyear\", ax=ax1);\nax1.set_title('Seasonal Plot (year/dayofyear)')","metadata":{"_uuid":"9c59af0e-80d1-45d3-9391-058eaa3123e8","_cell_guid":"3254e63d-3324-4656-b5f3-d0f2dca34178","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-06-18T08:45:33.059366Z","iopub.execute_input":"2022-06-18T08:45:33.059943Z","iopub.status.idle":"2022-06-18T08:45:37.739401Z","shell.execute_reply.started":"2022-06-18T08:45:33.059907Z","shell.execute_reply":"2022-06-18T08:45:37.738515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_periodogram(average_sales.loc['2017']);","metadata":{"_uuid":"f2ef0f50-4888-41f8-8f89-45a6292c80bf","_cell_guid":"099abb34-470c-47e7-9770-15f2bb82ab76","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-06-18T08:45:37.74099Z","iopub.execute_input":"2022-06-18T08:45:37.741383Z","iopub.status.idle":"2022-06-18T08:45:38.398473Z","shell.execute_reply.started":"2022-06-18T08:45:37.741347Z","shell.execute_reply":"2022-06-18T08:45:38.397749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"📌 **Interpret:** both the seasonal plot and the periodogram suggest a **<span style='color:#F1C40F'>strong weekly seasonality</span>**, and a weak annual seasonality. From the periodogram, it appears there may be some **<span style='color:#F1C40F'>monthly</span>** and **<span style='color:#F1C40F'>biweekly</span>** components as well. In fact, the notes to the Store Sales dataset say wages in the public sector are paid out biweekly, on the 15th and last day of the month -- a possible origin for these seasons.\n\n<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#323232;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:white;\"><b>4.3 | Decomposition </b></p>\n</div>\n\nLet's now combine all above time series features in aan **<span style='color:#F1C40F'>unique</span>** graph. ","metadata":{}},{"cell_type":"code","source":"from statsmodels.tsa.seasonal import seasonal_decompose\ndecomp = seasonal_decompose(sales['sales'], period=365, model='additive', extrapolate_trend='freq')\nfig, ax = plt.subplots(ncols=2, nrows=2, sharex=True, figsize=(22,10))\nax[0,0].set_title('Observed values for Sales', fontsize=16)\ndecomp.observed.plot(ax = ax[0,0], legend=False, color='dodgerblue')\n\nax[0,1].set_title('Sales Trend', fontsize=16)\ndecomp.trend.plot(ax = ax[0,1],legend=False, color='dodgerblue')\n\nax[1,0].set_title('Sales Seasonality', fontsize=16)\ndecomp.seasonal.plot(ax = ax[1,0],legend=False, color='dodgerblue')\n\nax[1,1].set_title('Noise', fontsize=16)\ndecomp.resid.plot(ax = ax[1,1],legend=False, color='dodgerblue')","metadata":{"_uuid":"13d1803b-a7f4-47cc-9ce8-ee3befb91a19","_cell_guid":"2c648f00-62c5-4e40-9389-56645ee1b64a","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-06-18T08:45:38.399632Z","iopub.execute_input":"2022-06-18T08:45:38.400621Z","iopub.status.idle":"2022-06-18T08:45:40.232488Z","shell.execute_reply.started":"2022-06-18T08:45:38.400576Z","shell.execute_reply":"2022-06-18T08:45:40.231717Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"📌 **Interpret:** The three components are shown separately in the bottom three panels. These components can be **<span style='color:#F1C40F'>added/multiplied</span>** together to reconstruct the data shown in the top panel. We can see that the seasonal component changes slowly over time. But this doesn't mean years far apart won't have different seasonal patterns.\n\nThe residual component shown in the bottom panel is what is left over when the seasonal and trend-cycle components have been subtracted from the data.\n<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#323232;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:white;\"><b>4.4 | Stationarity </b></p>\n</div>\n\n### **What is Stationarity ?**\n\nA stationary Time Series is one whose properties **<span style='color:#F1C40F'>do not depend</span>** on the time at which the series is observed. Thus, time series with trends, or with seasonality, are not stationary. A time series with **<span style='color:#F1C40F'>cyclic behaviour</span>** (but with no trend or seasonality) is stationary.\n\n* Strong stationarity: is a **<span style='color:#F1C40F'>stochastic process</span>** whose unconditional joint probability distribution does not change when shifted in time. Consequently, parameters such as mean and variance also do not change over time.\n* Weak stationarity: is a process where mean, variance, autocorrelation are constant throughout the time\n\nStationarity is important as non-stationary series that depend on time have too many parameters to account for when modelling the time series. **<span style='color:#F1C40F'>diff method()</span>** can easily convert a non-stationary series to a stationary series.\n\n### **What is stationarity used for ?**\n\nMost statistical forecasting methods are designed to work on a stationary time series. The **<span style='color:#F1C40F'>first step</span>** in the forecasting process is typically to do some transformation to **<span style='color:#F1C40F'>convert a non-stationary series to stationary</span>**. Forecasting a stationary series is relatively easier and the forecasts are more reliable. We know that linear regression works best if the predictors (X variables) are not correlated against each other. So, stationarizing the series solves this problem since it removes any persistent autocorrelation, thereby making the predictors (lags of the series) in the forecasting models nearly independent.\n\n### **How to make a Time Series stationary ?**\n\nThere are several ways to do that: \n\n* Difference the series once or more times (subtracting the next value by the current value)\n* Take the log of the series (helps to stabilize the variance of a time series.)\n* Take the 𝑛𝑡ℎ root of the series Combinations of the above\n\nBut first, to test if a time series is stationary we can:\n\n* Look at the time plot.\n* Split the series into 2 parts and compute descriptive statistics. If they differ, then it is not stationary.\n* Perform statistical tests called Unit Root Tests like Augmented Dickey Fuller test (ADF Test), Kwiatkowski-Phillips-Schmidt-Shin — KPSS test (trend stationary), and Philips Perron test (PP Test).\n\nThe most commonly used is the ADF test, where the **<span style='color:#F1C40F'>null hypothesis</span>** is that the time series possesses a unit root (or random walk with drift) and is non-stationary. So, if the P-Value in ADF test is less than the significance level (0.05), you reject the null hypothesis and the series is stationary.","metadata":{"_kg_hide-input":true}},{"cell_type":"code","source":"# check for stationarity\ndef adf_test(series, title=''):\n    \"\"\"\n    Pass in a time series and an optional title, returns an ADF report\n    \"\"\"\n    print('Augmented Dickey-Fuller Test: {}'.format(title))\n    # .dropna() handles differenced data\n    result = adfuller(series.dropna(),autolag='AIC') \n    \n    labels = ['ADF test statistic','p-value','# lags used','# observations']\n    out = pd.Series(result[0:4],index=labels)\n\n    for key,val in result[4].items():\n        out['critical value ({})'.format(key)]=val\n        \n    # .to_string() removes the line \"dtype: float64\"\n    print(out.to_string())          \n    \n    if result[1] <= 0.05:\n        print(\"Strong evidence against the null hypothesis\")\n        print(\"Reject the null hypothesis\")\n        print(\"Data has no unit root and is stationary\")\n    else:\n        print(\"Weak evidence against the null hypothesis\")\n        print(\"Fail to reject the null hypothesis\")\n        print(\"Data has a unit root and is non-stationary\")","metadata":{"execution":{"iopub.status.busy":"2022-06-18T08:45:40.233881Z","iopub.execute_input":"2022-06-18T08:45:40.234413Z","iopub.status.idle":"2022-06-18T08:45:40.244728Z","shell.execute_reply.started":"2022-06-18T08:45:40.234378Z","shell.execute_reply":"2022-06-18T08:45:40.243541Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Aggregating the Time Series to a monthly scaled index\ny = df_data[['date','sales']].copy()\ny.set_index('date', inplace=True)\ny.index = pd.to_datetime(y.index)\ny = y.resample('1M').mean()\n        \nadf_test(y['sales'],title='') ","metadata":{"execution":{"iopub.status.busy":"2022-06-18T08:45:40.2465Z","iopub.execute_input":"2022-06-18T08:45:40.247217Z","iopub.status.idle":"2022-06-18T08:45:40.524483Z","shell.execute_reply.started":"2022-06-18T08:45:40.247174Z","shell.execute_reply":"2022-06-18T08:45:40.52361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"If the data is not stationary but we want to use a model such as ARIMA (that requires this characteristic), the data has to be transformed. The two most common methods to transform series into stationarity ones are:\n\n* Transformation: e.g. log or square root to stabilize non-constant variance\n* Differencing: subtracts the current value from the previous\n\nHereafter, we are going to transform sales trend from non-stationarity to stationarity using diff method: ","metadata":{}},{"cell_type":"code","source":"fig = plt.figure(figsize=(22,8))\ndecomp.trend.diff().plot()","metadata":{"execution":{"iopub.status.busy":"2022-06-18T08:45:40.525933Z","iopub.execute_input":"2022-06-18T08:45:40.52649Z","iopub.status.idle":"2022-06-18T08:45:41.142258Z","shell.execute_reply.started":"2022-06-18T08:45:40.52645Z","shell.execute_reply":"2022-06-18T08:45:41.141285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#323232;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:white;\"><b>4.5 | Autocorrelation Analysis</b></p>\n</div>\n\nAfter a time series has been stationarized by differencing, the next step in fitting an ARIMA model is to **<span style='color:#F1C40F'>determine whether AR or MA terms</span>** are needed to correct any autocorrelation that remains in the differenced series. Of course, with software like Statgraphics, you could just try some different combinations of terms and see what works best. But there is a more systematic way to do this. By looking at the **<span style='color:#F1C40F'>autocorrelation function </span>**and **<span style='color:#F1C40F'>partial autocorrelation function</span>** plots of the differenced series, you can tentatively identify the numbers of AR and/or MA terms that are needed.\n\n* Autocorrelation Function (ACF): P = Periods to lag for eg: (if P= 3 then we will use the three previous periods of our time series in the autoregressive portion of the calculation) P helps adjust the line that is being fitted to forecast the series. P corresponds with MA parameter\n* Partial Autocorrelation Function (PACF): D = In an ARIMA model we transform a time series into stationary one(series without trend or seasonality) using differencing. D refers to the number of differencing transformations required by the time series to get stationary. D corresponds with AR parameter.\n\n\n\n","metadata":{}},{"cell_type":"code","source":"#from statsmodels.tsa.stattools import acf\n#fig, ax = plt.subplots(nrows=1, ncols=2,figsize=(15, 6))\n#plot_lags(df_data[df_data.date >= datetime.datetime(2017,1,1)]['sales'], lags=12, nrows=2)\n#plot_acf(df_data[df_data.date >= datetime.datetime(2017,1,1)]['sales'].tolist(), lags=12, ax=ax[0], fft=False);\n#plot_pacf(df_data[df_data.date >= datetime.datetime(2017,1,1)]['sales'].tolist(), lags=12, ax=ax[1]);","metadata":{"execution":{"iopub.status.busy":"2022-06-18T08:45:41.143719Z","iopub.execute_input":"2022-06-18T08:45:41.14408Z","iopub.status.idle":"2022-06-18T08:45:41.148118Z","shell.execute_reply.started":"2022-06-18T08:45:41.144047Z","shell.execute_reply":"2022-06-18T08:45:41.147268Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"📌 **Interpret:** For autocorrelation, the y-axis is the value for the correlation between a value and its lag. The lag is on the x-axis. The zero-lag has a correlation of 1 because it correlates with itself perfectly.","metadata":{}},{"cell_type":"markdown","source":"# <b>5 <span style='color:#F1C40F'>|</span> Feature Transformation</b>\n\n<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#323232;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:white;\"><b>5.1 | Labeling Non-Numerical Features</b></p>\n</div>\n\nUsing **<span style='color:#F1C40F'>LabelEncoder</span>**, we are going to convert non-numerical features to numerical type. LabelEncoder basically labels the classes from **<span style='color:#F1C40F'>0 to n</span>**. This process is necessary for models to learn from those features.","metadata":{}},{"cell_type":"code","source":"non_numerical_cols =  [col for col in df_data.columns if df_data[col].dtype == 'object']\nfor feature in non_numerical_cols:        \n    df_data[feature] = LabelEncoder().fit_transform(df_data[feature])\ndf_data.head().style.set_properties(subset=non_numerical_cols, **{'background-color': '#F1C40F'})","metadata":{"execution":{"iopub.status.busy":"2022-06-18T08:45:41.149957Z","iopub.execute_input":"2022-06-18T08:45:41.15035Z","iopub.status.idle":"2022-06-18T08:45:49.812352Z","shell.execute_reply.started":"2022-06-18T08:45:41.150291Z","shell.execute_reply":"2022-06-18T08:45:49.81147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_data.dtypes","metadata":{"execution":{"iopub.status.busy":"2022-06-18T08:45:49.813894Z","iopub.execute_input":"2022-06-18T08:45:49.81425Z","iopub.status.idle":"2022-06-18T08:45:49.821556Z","shell.execute_reply.started":"2022-06-18T08:45:49.814214Z","shell.execute_reply":"2022-06-18T08:45:49.820715Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#323232;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:white;\"><b>5.2 | One Hot Encoding</b></p>\n</div>\n\nTo finish with, we are going to one hot encoded non-ordinal features. **<span style='color:#F1C40F'>All</span>** labeled features above are **<span style='color:#F1C40F'>non-ordinal</span>** features. Therefore, we are going to one hot encoded those which have a low cardinality. ","metadata":{}},{"cell_type":"code","source":"df_data.dtypes","metadata":{"execution":{"iopub.status.busy":"2022-06-18T08:45:49.823201Z","iopub.execute_input":"2022-06-18T08:45:49.823892Z","iopub.status.idle":"2022-06-18T08:45:49.833853Z","shell.execute_reply.started":"2022-06-18T08:45:49.823839Z","shell.execute_reply":"2022-06-18T08:45:49.832862Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"low_card_cols = [col for col in non_numerical_cols if len(df_data[col].unique()) < 15]","metadata":{"execution":{"iopub.status.busy":"2022-06-18T08:45:49.835617Z","iopub.execute_input":"2022-06-18T08:45:49.836265Z","iopub.status.idle":"2022-06-18T08:45:49.978621Z","shell.execute_reply.started":"2022-06-18T08:45:49.836228Z","shell.execute_reply":"2022-06-18T08:45:49.977876Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encoded_features = []\n\nfor feature in low_card_cols:\n    encoded_feat = OneHotEncoder().fit_transform(df_data[feature].values.reshape(-1, 1)).toarray()\n    n = df_data[feature].nunique()\n    cols = ['{}_{}'.format(feature, n) for n in range(1, n + 1)]\n    encoded_df = pd.DataFrame(encoded_feat, columns=cols)\n    encoded_df.index = df_data.index\n    encoded_features.append(encoded_df)\n\ndf_data = pd.concat([df_data, *encoded_features[:9]], axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-06-18T08:45:49.979968Z","iopub.execute_input":"2022-06-18T08:45:49.980321Z","iopub.status.idle":"2022-06-18T08:45:52.020147Z","shell.execute_reply.started":"2022-06-18T08:45:49.980287Z","shell.execute_reply":"2022-06-18T08:45:52.019311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_data.head().style.set_properties(subset=low_card_cols, **{'background-color': '#F1C40F'})","metadata":{"execution":{"iopub.status.busy":"2022-06-18T08:45:52.021594Z","iopub.execute_input":"2022-06-18T08:45:52.022115Z","iopub.status.idle":"2022-06-18T08:45:52.045236Z","shell.execute_reply.started":"2022-06-18T08:45:52.022077Z","shell.execute_reply":"2022-06-18T08:45:52.044316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_data = df_data.drop(low_card_cols,axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-06-18T08:45:52.046747Z","iopub.execute_input":"2022-06-18T08:45:52.047143Z","iopub.status.idle":"2022-06-18T08:45:53.164475Z","shell.execute_reply.started":"2022-06-18T08:45:52.047107Z","shell.execute_reply":"2022-06-18T08:45:53.163631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <b>6 <span style='color:#F1C40F'>|</span> Modeling</b>\n\n<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#323232;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:white;\"><b>6.1 | Cross Validation</b></p>\n</div>\n\nTime series can be either **<span style='color:#F1C40F'>univariate</span>** or **<span style='color:#F1C40F'>multivariate</span>**:\n\n* Univariate time series only has a single time-dependent variable.\n* Multivariate time series have a multiple time-dependent variable.\n\nBut, first of all we are going to see how does **<span style='color:#F1C40F'>cross validation</span>** technic works in TimeSeries Analysis.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import TimeSeriesSplit\nN_SPLITS = 3\n\nX = df_data['date']\ny = df_data['sales']\n\nfolds = TimeSeriesSplit(n_splits=N_SPLITS)","metadata":{"execution":{"iopub.status.busy":"2022-06-18T08:46:58.650468Z","iopub.execute_input":"2022-06-18T08:46:58.65082Z","iopub.status.idle":"2022-06-18T08:46:58.65573Z","shell.execute_reply.started":"2022-06-18T08:46:58.650789Z","shell.execute_reply":"2022-06-18T08:46:58.654936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#323232;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:white;\"><b>6.2 | Univariate Time Series Models</b></p>\n</div>\n\n**<span style='color:#F1C40F'>Univariate time series:</span>** Only one variable is varying over time. For example, data collected from a sensor measuring the temperature of a room every second. Therefore, each second, you will only have a one-dimensional value, which is the temperature.\n\n### Prophet\n\nThe first model (which also can handle multivariate problems) we are going to try is Facebook Prophet. Prophet, or “Facebook Prophet,” is an open-source library for univariate (one variable) time series forecasting developed by Facebook. Prophet implements what they refer to as an **<span style='color:#F1C40F'>additive</span>** time series forecasting model, and the implementation supports **<span style='color:#F1C40F'>trends, seasonality, and holidays</span>**. In our case, we are going to use it to show **<span style='color:#F1C40F'>average sales per day</span>** (it is an univariate time series)","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import mean_absolute_error, mean_squared_error\nimport math\n! pip install fbprophet\nfrom fbprophet import Prophet\n\ntrain = df_data[df_data['date']<= datetime.datetime(2017,8,15)][['date','sales']].groupby('date').mean().reset_index('date')\ntrain.columns = ['ds', 'y']\nx_valid = pd.DataFrame(df_data[df_data['date']>= datetime.datetime(2017,8,16)]['date'])\nx_valid.columns = ['ds']\n\n# Train the model\nmodel = Prophet()\nmodel.fit(train)\ny_pred = model.predict(x_valid)","metadata":{"execution":{"iopub.status.busy":"2022-06-18T08:47:20.420644Z","iopub.execute_input":"2022-06-18T08:47:20.421019Z","iopub.status.idle":"2022-06-18T08:49:31.033892Z","shell.execute_reply.started":"2022-06-18T08:47:20.420987Z","shell.execute_reply":"2022-06-18T08:49:31.03303Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f, ax = plt.subplots(1)\nf.set_figheight(10)\nf.set_figwidth(22)\n\nmodel.plot(y_pred, ax=ax)\nsns.lineplot(x=train['ds'], y=train['y'], ax=ax, color='darkorange') #navajowhite\n\n#ax.set_title(f'Prediction \\n MAE: {score_mae:.2f}, RMSE: {score_rmse:.2f}', fontsize=14)\nax.set_xlabel(xlabel='Date', fontsize=14)\nax.set_ylabel(ylabel='Sales', fontsize=14)\nax.set_title('Average Sales per Day')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-18T08:49:31.035716Z","iopub.execute_input":"2022-06-18T08:49:31.036224Z","iopub.status.idle":"2022-06-18T08:49:31.906255Z","shell.execute_reply.started":"2022-06-18T08:49:31.036185Z","shell.execute_reply":"2022-06-18T08:49:31.904656Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### ARIMA\n\nThe Auto-Regressive Integrated Moving Average (ARIMA) model describes the autocorrelations in the data. The model assumes that the time-series is stationary. It consists of three main parts:\n\n* Auto-Regressive (AR) filter (long term):\n$yt=c+\\alpha_1 y_t−1+ \\cdots \\alpha_n y_{t−n}+\\epsilon_t=c+\\sum_{i=1}^p\\alpha_i y_{t−i}+\\epsilon_t \\rightarrow p$\n\n* Integration filter (stochastic trend)\n$\\rightarrow d$\n\n* Moving Average (MA) filter (short term):\n\n$y_t=c+\\epsilon_t+\\beta_1 \\epsilon_t−1+ \\cdots +\\beta_q \\epsilon_t−q=c+\\epsilon_t+\\sum_{i=1}^q \\beta_i \\epsilon_t−i \\rightarrow q$\n\nARIMA: $y_t=c+\\alpha_1 y_{t−1}+\\cdots+\\alpha_p y_{t−p}+\\epsilon_t+ \\beta_1 \\epsilon_{t−1}+\\cdots+\\beta_q\\epsilon_{t−q}$\n\nARIMA( p, d, q)\n\n* p: Lag order (reference PACF in Autocorrelation Analysis)\n* d: Degree of differencing. (reference Differencing in Stationarity)\n* q: Order of moving average (check out ACF in Autocorrelation Analysis)\n\nSteps to analyze ARIMA\n\n* **<span style='color:#F1C40F'>Step 1 — Check stationarity:</span>** If a time series has a trend or seasonality component, it must be made stationary before we can use ARIMA to forecast. .\n* **<span style='color:#F1C40F'>Step 2 — Difference:</span>** If the time series is not stationary, it needs to be stationarized through differencing. Take the first difference, then check for stationarity. Take as many differences as it takes. Make sure you check seasonal differencing as well.\n* **<span style='color:#F1C40F'>Step 3 — Filter out a validation sample:</span>** This will be used to validate how accurate our model is. Use train test validation split to achieve this\n* **<span style='color:#F1C40F'>Step 4 — Select AR and MA terms:</span>**Step 4 — Select AR and MA terms: Use the ACF and PACF to decide whether to include an AR term(s), MA term(s), or both.\n* **<span style='color:#F1C40F'>Step 5 — Build the model:</span>** Build the model and set the number of periods to forecast to N (depends on your needs).\n* **<span style='color:#F1C40F'>Step 6 — Validate model:</span>** Compare the predicted values to the actuals in the validation sample.","metadata":{}},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#323232;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:white;\"><b>6.3 | Multivariate Time Series Models</b></p>\n</div>\n\nFinnally, we are going to analize multivariate TimeSeries forecasting.\n\n**<span style='color:#F1C40F'>Multivariate time series:</span>** Multiple variables are varying over time. For example, a tri-axial accelerometer. There are three accelerations, one for each axis (x,y,z) and they vary simultaneously over time.","metadata":{}},{"cell_type":"code","source":"train_multivariate = df_data[df_data['date']<= datetime.datetime(2017,8,15)][['date','sales','dcoilwtico']].groupby('date').mean().reset_index('date')\ntrain_multivariate.columns = ['ds', 'y','dcoilwtico']\nx_valid = pd.DataFrame(df_data[df_data['date']>= datetime.datetime(2017,8,16)][['date','dcoilwtico']])\nx_valid.columns = ['ds','dcoilwtico']\n\n# Train the model\nmodel_multivariate = Prophet()\nmodel_multivariate.add_regressor('dcoilwtico')\n\nmodel_multivariate.fit(train_multivariate)\ny_pred_multivariate = model_multivariate.predict(x_valid)","metadata":{"execution":{"iopub.status.busy":"2022-06-18T08:49:36.166481Z","iopub.execute_input":"2022-06-18T08:49:36.166826Z","iopub.status.idle":"2022-06-18T08:49:56.271725Z","shell.execute_reply.started":"2022-06-18T08:49:36.166795Z","shell.execute_reply":"2022-06-18T08:49:56.270921Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f, ax = plt.subplots(1)\nf.set_figheight(10)\nf.set_figwidth(22)\n\nmodel.plot(y_pred_multivariate, ax=ax)\nsns.lineplot(x=train_multivariate['ds'], y=train_multivariate['y'], ax=ax, color='darkorange') #navajowhite\n\n#ax.set_title(f'Prediction \\n MAE: {score_mae:.2f}, RMSE: {score_rmse:.2f}', fontsize=14)\nax.set_xlabel(xlabel='Date', fontsize=14)\nax.set_ylabel(ylabel='Sales', fontsize=14)\nax.set_title('Average Sales per Day')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-18T08:49:56.273264Z","iopub.execute_input":"2022-06-18T08:49:56.27361Z","iopub.status.idle":"2022-06-18T08:49:57.441905Z","shell.execute_reply.started":"2022-06-18T08:49:56.273574Z","shell.execute_reply":"2022-06-18T08:49:57.441035Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}