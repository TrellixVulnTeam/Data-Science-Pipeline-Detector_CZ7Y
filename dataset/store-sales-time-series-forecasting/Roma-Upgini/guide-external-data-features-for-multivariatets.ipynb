{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# üîé How to find relevant external features & data for kaggle competitions in 10 minutes [3/3]\n### Part #3 -  Improve accuracy of Multivariate Time Series kernel from external features & data\n##### [Part #1 Link](https://www.kaggle.com/code/romaupgini/how-to-find-external-data-for-1-private-lb-4-50)\n##### [Part #2 Link](https://www.kaggle.com/code/romaupgini/zero-feature-engineering-with-upgini-pycaret)\n______________________________\n*updated 2022-06-06 [@roma-upgini](https://www.kaggle.com/romaupgini)*\n\n**‚ùìBefore reading the notebook, what will you learn from it?**\n\n1. How external data & features might help on Kaggle: two scenarios\n2. How to find relevant external features in less than 10 minutes and save time on feature engineering \n3. How to calculate metrics and uplifts from new external features\n4. What external data sources might help you on Kaggle competitions\n\nExtra **tips & tricks**:  \n\n5. How and when to do **zero forecasting**\n6. How to do **target log(y)+1 transform** for better model fit, as part of a scikit-learn model \n7. How to do **rounding to the nearest integer** for better prediction accuracy, as part of a scikit-learn model\n8. How quickly analyse dependences between components of Multivariate Time Series\n9. How to check TS on stationarity \n10. How generate hypotesis on model type for specific time series component in Multivariate TS\n\nüó£ Share this notebook: [Shareable Link](https://www.kaggle.com/code/romaupgini/external-data-features-for-multivariate-ts)\n______________________________\n### Table of contents\n* [Intro](#Intro)\n\n* [How external data & features might help on Kaggle?](#How-external-data-&-features-might-help-on-Kaggle?)\n\n* [1Ô∏è‚É£ Make/fork a baseline notebook for feature enrichment](#1Ô∏è‚É£-Make/fork-a-baseline-notebook-for-feature-enrichment)\n\n* [2Ô∏è‚É£ Quick analysis of Multivariate Time Series data](#2Ô∏è‚É£-Quick-analysis-of-Multivariate-Time-Series-data)\n\n* [3Ô∏è‚É£ Find relevant external features for several TS components in Multivariate Time Series](#3Ô∏è‚É£-Find-relevant-external-features-for-several-TS-components-in-Multivariate-Time-Series)\n\n* [4Ô∏è‚É£ Train final model](#4Ô∏è‚É£-Train-final-model)\n\n* [5Ô∏è‚É£ Submit prediction with enriched features and calculate final leaderbord progress](#5Ô∏è‚É£-Submit-prediction-with-enriched-features-and-calculate-final-leaderbord-progress)\n    \n* [Relevant external features & data sources](#üåé-Relevant-external-features-&-data-sources)   \n\n## Intro\n**Competition**: [Store Sales - Time Series Forecasting](https://www.kaggle.com/competitions/store-sales-time-series-forecasting), [RMSLE as a target metric](https://www.kaggle.com/code/carlmcbrideellis/store-sales-using-the-average-of-the-last-16-days#Note-regarding-calculating-the-average)  \n**Special thanks**:\n* EDA with a lot of small but important details: [Store Sales TS Forecasting - A Comprehensive Guide](https://www.kaggle.com/ekrembayar/store-sales-ts-forecasting-a-comprehensive-guide) by [Ekrem Bayar](https://www.kaggle.com/ekrembayar)  \n* Inital regressor architecture: [Simple TS + Ridge + RF](https://www.kaggle.com/dkomyagin/simple-ts-ridge-rf) by [KDJ2020](https://www.kaggle.com/dkomyagin)  \n* Further tuning of regressor with voting ensemble: [Notebook](https://www.kaggle.com/hiro5299834/store-sales-ridge-voting-bagging-et-bagging-rf) by [BIZEN's](https://www.kaggle.com/hiro5299834)  \n* Greate note on [asymmetrical nature of RMSLE](https://www.kaggle.com/code/carlmcbrideellis/store-sales-using-the-average-of-the-last-16-days#Note-regarding-calculating-the-average) by [Carl McBride Ellis](https://www.kaggle.com/carlmcbrideellis)\n\nüìö In this notebook we'll use:\n* [Upgini](https://github.com/upgini/upgini#readme) - Low-code Feature search and enrichment library for supervised machine learning applications.   \n<a href=\"https://github.com/upgini/upgini\">\n    <img src=\"https://img.shields.io/badge/GitHub-100000?style=for-the-badge&logo=github&logoColor=white\"  align='center'>\n</a>  ","metadata":{}},{"cell_type":"markdown","source":"## How external data & features might help on Kaggle?\nKaggle is always about learning and leader board progress (hopefully from learning, not cheating ;-))  \nAnd every Kaggler wants to progress as fast as possible, so time saving tips & tricks is a big deal as well.  \nThat's why low-code tools is adopted among kagglers.\n\nSo, there are **two major scenarios** of external features & data introduction in competitions on Kaggle:\n\n1. **Final improvement of a polished kernel**  \nIn this scenario you want **to improve already polished kernel** (optimized features, model architecture and hyperparams) with new external features.  \nBefore that, most of the juice already has been \"squeezed\" from competition data by significant efforts in feature engineering.  \nAnd you want to answer the simple question - *Is there any external data sources and features which might boost accuracy a bit more?*  \nHowever, there is a caveat to this approach: current model architecture & hyperparameters might be suboptimal for the new feature set, after introduction even single new var.  \nSo, extra step back for model tuning might be needed.\n\n2. **Low-code initial feature engineering - add relevant external features @start**  \nHere you want to **save time on feature search and engineering**. If there are some ready-to-use external features and data, let's use it to speed up the overall progress.  \nIn this scenario always make sense to check that new external features have optimal representation for specific task and target model architecture. Example - category features for linear regression models should be one-hot-encoded.\nThis type of feature preparation should be done manually in any case.  \nSame as scenario #1, there is a caveat to this approach: a lot of features not always a good thing - they might lead to dimensionality increase and model overfitting.  \nYou have to check model accuracy improvement metrics after enrichment with the new features and ALWAYS with appropriate cross-validation strategy.\n \nIn this Guide we'll go with **Scenario #1**. Also you can check out Part #2 Guide for the [**Scenario #2**](https://www.kaggle.com/code/romaupgini/zero-feature-engineering-with-upgini-pycaret), and another example for Scenario #1 - [Part 1](https://www.kaggle.com/code/romaupgini/how-to-find-external-data-for-1-private-lb-4-50)","metadata":{}},{"cell_type":"markdown","source":"## 1Ô∏è‚É£ Make/fork a baseline notebook for feature enrichment\n\nIn this tutorial we forked [Hyperparamaters](https://www.kaggle.com/code/rizkykiky/hyperparamaters) public notebook by [Rizky Anugrah Ananto](https://www.kaggle.com/rizkykiky) (thank you!), which has a public LB score **0.40419**  \nAnd made some changes:\n\n* Added time dependent features based on salary schedule in the Ecuador public sector;\n* Added Zero forecasting for zero-sales product families in specific stores;\n* Updated blending approach to \"Same store sales predicitions as new features\" and made a quick explanation why this works;\n* Added Power transformation (Yeo-Johnson) for X and Log(y)+1 transformation for y in regressors;\n* Added different regressor types (Trees, KNN, Bayesian) for different product groups timeseries.\n\n[‚¨áÔ∏è Step 2](#2Ô∏è‚É£-Quick-analysis-of-Multivariate-Time-Series-data)","metadata":{}},{"cell_type":"markdown","source":"___________________________________________________________\n### Packages and functions","metadata":{}},{"cell_type":"code","source":"%pip install -Uq upgini\nimport numpy as np\nimport pandas as pd\n\nimport statsmodels.api as sm\nfrom statsmodels.tsa.deterministic import DeterministicProcess, CalendarFourier\nfrom statsmodels.graphics.tsaplots import plot_pacf\nfrom statsmodels.tsa.stattools import adfuller\n# visualization tools\nfrom matplotlib import pyplot as plt, style\nstyle.use('seaborn-darkgrid')\nimport seaborn as sns\nsns.set_style('darkgrid')\nimport plotly.express as px\nfrom tqdm import tqdm\n\nimport gc\ngc.enable()\nfrom warnings import filterwarnings, simplefilter\nfilterwarnings('ignore')\nsimplefilter('ignore')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-06T15:39:16.979769Z","iopub.execute_input":"2022-06-06T15:39:16.980563Z","iopub.status.idle":"2022-06-06T15:39:35.204627Z","shell.execute_reply.started":"2022-06-06T15:39:16.980438Z","shell.execute_reply":"2022-06-06T15:39:35.203695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Reading train and test datasets","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('../input/store-sales-time-series-forecasting/train.csv',\n                    parse_dates = ['date'], infer_datetime_format = True,\n                    dtype = {'store_nbr' : 'category',\n                             'family' : 'category'},\n                    usecols = ['date', 'store_nbr', 'family', 'sales'])\ntrain['date'] = train.date.dt.to_period('D')\ntrain = train.set_index(['date', 'store_nbr', 'family']).sort_index()\nprint(train.shape)\ntrain.head()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-06T15:39:35.206782Z","iopub.execute_input":"2022-06-06T15:39:35.20786Z","iopub.status.idle":"2022-06-06T15:39:39.273Z","shell.execute_reply.started":"2022-06-06T15:39:35.207817Z","shell.execute_reply":"2022-06-06T15:39:39.272028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = pd.read_csv('../input/store-sales-time-series-forecasting/test.csv',\n                   parse_dates = ['date'], infer_datetime_format = True)\ntest['date'] = test.date.dt.to_period('D')\ntest = test.set_index(['date', 'store_nbr', 'family']).sort_values('id')\nprint(test.shape)\ntest.head()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-06T15:39:39.274137Z","iopub.execute_input":"2022-06-06T15:39:39.27467Z","iopub.status.idle":"2022-06-06T15:39:39.332589Z","shell.execute_reply.started":"2022-06-06T15:39:39.274632Z","shell.execute_reply":"2022-06-06T15:39:39.331637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Oil price lag features","metadata":{}},{"cell_type":"code","source":"calendar = pd.DataFrame(index = pd.date_range('2013-01-01', '2017-08-31')).to_period('D')\noil = pd.read_csv('../input/store-sales-time-series-forecasting/oil.csv',\n                  parse_dates = ['date'], infer_datetime_format = True,\n                  index_col = 'date').to_period('D')\noil['avg_oil'] = oil['dcoilwtico'].rolling(7).mean()\ncalendar = calendar.join(oil.avg_oil)\ncalendar['avg_oil'].fillna(method = 'ffill', inplace = True)\ncalendar.dropna(inplace = True)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-06T15:39:39.334375Z","iopub.execute_input":"2022-06-06T15:39:39.334748Z","iopub.status.idle":"2022-06-06T15:39:39.36473Z","shell.execute_reply.started":"2022-06-06T15:39:39.334716Z","shell.execute_reply":"2022-06-06T15:39:39.363512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We concatenated calendar with oil price and replaced NaN with **last valid** observation price.   \nNow, let's calculate *partial autocorrelation* to select proper lags for oil price features.  \nThe lag feature - shifting a time series forward one step or more.","metadata":{}},{"cell_type":"code","source":"_ = plot_pacf(calendar.avg_oil, lags = 10)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-06T15:39:39.366302Z","iopub.execute_input":"2022-06-06T15:39:39.367118Z","iopub.status.idle":"2022-06-06T15:39:39.777268Z","shell.execute_reply.started":"2022-06-06T15:39:39.367068Z","shell.execute_reply":"2022-06-06T15:39:39.776165Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Oil price lags for the first 3 days should be enough:","metadata":{}},{"cell_type":"code","source":"n_lags = 3\nfor l in range(1, n_lags + 1):\n    calendar[f'oil_lags{l}'] = calendar.avg_oil.shift(l)\ncalendar.dropna(inplace = True)","metadata":{"execution":{"iopub.status.busy":"2022-06-06T15:39:39.778912Z","iopub.execute_input":"2022-06-06T15:39:39.779607Z","iopub.status.idle":"2022-06-06T15:39:39.792279Z","shell.execute_reply.started":"2022-06-06T15:39:39.779559Z","shell.execute_reply":"2022-06-06T15:39:39.791495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Holiday features","metadata":{}},{"cell_type":"code","source":"hol = pd.read_csv('../input/store-sales-time-series-forecasting/holidays_events.csv',\n                  parse_dates = ['date'], infer_datetime_format = True,\n                  index_col = 'date').to_period('D')\nhol = hol[hol.locale == 'National']  # Only National holidays so there'll be no false positive.\nhol = hol.groupby(hol.index).first() # Remove duplicated holidays\nprint(hol.shape)\nhol.head()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-06T15:39:39.793687Z","iopub.execute_input":"2022-06-06T15:39:39.794302Z","iopub.status.idle":"2022-06-06T15:39:39.834158Z","shell.execute_reply.started":"2022-06-06T15:39:39.794259Z","shell.execute_reply":"2022-06-06T15:39:39.83314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"calendar = calendar.join(hol)               # Joining calendar with holiday dataset\ncalendar['dofw'] = calendar.index.dayofweek # Day of week\ncalendar['wd'] = 1\ncalendar.loc[calendar.dofw > 4, 'wd'] = 0   # If it's saturday or sunday then it's not Weekday\ncalendar.loc[calendar.type == 'Work Day', 'wd'] = 1 # If it's Work Day event then it's a workday\ncalendar.loc[calendar.type == 'Transfer', 'wd'] = 0 # If it's Transfer event then it's not a work day\ncalendar.loc[calendar.type == 'Bridge', 'wd'] = 0 # If it's Bridge event then it's not a work day\ncalendar.loc[(calendar.type == 'Holiday') & (calendar.transferred == False), 'wd'] = 0 # If it's holiday and the holiday is not transferred then it's non working day\ncalendar.loc[(calendar.type == 'Holiday') & (calendar.transferred == True), 'wd'] = 1 # If it's holiday and transferred then it's working day\ncalendar = pd.get_dummies(calendar, columns = ['dofw'], drop_first = True) # One-hot encoding (Make sure to drop one of the columns by 'drop_first = True')\ncalendar = pd.get_dummies(calendar, columns = ['type']) # One-hot encoding for type holiday (No need to drop one of the columns because there's a \"No holiday\" already)\ncalendar.drop(['locale', 'locale_name', 'description', 'transferred'], axis = 1, inplace = True) # Unused columns\nprint(calendar.shape)\ncalendar.head()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-06T15:39:39.835566Z","iopub.execute_input":"2022-06-06T15:39:39.836317Z","iopub.status.idle":"2022-06-06T15:39:39.87486Z","shell.execute_reply.started":"2022-06-06T15:39:39.836269Z","shell.execute_reply":"2022-06-06T15:39:39.873717Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Time dependent features\nWages in the public sector are paid every two weeks on the 15th and on the last day of the month","metadata":{}},{"cell_type":"code","source":"calendar['wageday']=0\ncalendar.loc[(calendar.index.to_timestamp().is_month_end) | (calendar.index.day == 15), 'wageday'] = 1","metadata":{"execution":{"iopub.status.busy":"2022-06-06T15:39:39.876159Z","iopub.execute_input":"2022-06-06T15:39:39.876544Z","iopub.status.idle":"2022-06-06T15:39:39.887158Z","shell.execute_reply.started":"2022-06-06T15:39:39.876511Z","shell.execute_reply":"2022-06-06T15:39:39.886153Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ax = plt.subplots(1,1,figsize = (20,4))\ntrain.loc[\"2016-07-01\":].filter(like = 'SCHOOL AND OFFICE SUPPLIES', axis=0).groupby([\"date\"]).sales.sum().plot(title = \"SCHOOL AND OFFICE SUPPLIES\")\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-06T15:39:39.89138Z","iopub.execute_input":"2022-06-06T15:39:39.891878Z","iopub.status.idle":"2022-06-06T15:39:45.211455Z","shell.execute_reply.started":"2022-06-06T15:39:39.891841Z","shell.execute_reply":"2022-06-06T15:39:45.210384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"School seasons - April and May, than August and September. Let's put this information into the flag:","metadata":{}},{"cell_type":"code","source":"school_season = []\nfor i, r in calendar.iterrows():\n    if i.month in [4, 5, 8, 9] :\n        school_season.append(1)\n    else :\n        school_season.append(0)\ncalendar['school_season'] = school_season","metadata":{"execution":{"iopub.status.busy":"2022-06-06T15:39:45.212783Z","iopub.execute_input":"2022-06-06T15:39:45.213138Z","iopub.status.idle":"2022-06-06T15:39:45.288584Z","shell.execute_reply.started":"2022-06-06T15:39:45.213106Z","shell.execute_reply":"2022-06-06T15:39:45.287611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Zero forecasting\nSome stores do not sell specific products at all OR probably stop selling products after some time (assortiment enhancement trials?). Which looks like the case for HOME APPLIANCES in certain stores.   \nSo let's make it as a hard coded prediction with following logic: **15 days of zero sales** from 2017-08-01 till 2017-08-15 will lead to zero forecast. Except SCHOOL AND OFFICE SUPPLIES products, which has very specific seasonality pattern.\nCode credits to [@Ekrem Bayar](https://www.kaggle.com/ekrembayar/store-sales-ts-forecasting-a-comprehensive-guide#5.-Sales)\n> **Note**: Zero forecasting is one of the approaches to reduce compute & memory consumption. That definitley will be usefull for this notebook, keeping in mind amount of computations in CustomRegressor.","metadata":{}},{"cell_type":"code","source":"c = train.groupby([\"store_nbr\",\"family\"]).tail(15).groupby([\"store_nbr\",\"family\"]).sales.sum().reset_index()\nc = c[c.sales == 0].drop(\"sales\",axis = 1)\nc = c[c.family != \"SCHOOL AND OFFICE SUPPLIES\"]\nc.shape","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-06T15:39:45.290097Z","iopub.execute_input":"2022-06-06T15:39:45.290762Z","iopub.status.idle":"2022-06-06T15:39:45.627255Z","shell.execute_reply.started":"2022-06-06T15:39:45.290724Z","shell.execute_reply":"2022-06-06T15:39:45.626325Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Shape of train before zero forecasting:\", train.shape)\nouter_join = train.reset_index().merge(c, how = 'outer', indicator = True)\ntrain = outer_join[~(outer_join._merge == 'both')].drop('_merge', axis = 1)\ntrain = train.set_index(['date', 'store_nbr', 'family']).sort_index()\ndel outer_join\ngc.collect()\nprint(\"Shape of train after zero forecasting:\", train.shape)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-06T15:39:45.629172Z","iopub.execute_input":"2022-06-06T15:39:45.62999Z","iopub.status.idle":"2022-06-06T15:39:48.182404Z","shell.execute_reply.started":"2022-06-06T15:39:45.62994Z","shell.execute_reply":"2022-06-06T15:39:48.181358Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As you can see - now we removed zero sales from train data, so lets' save it into separate dataframe. And then append to test set on submission phase","metadata":{}},{"cell_type":"code","source":"zero_prediction = []\nfor i in range(0, len(c)):\n    zero_prediction.append(\n        pd.DataFrame({\n            \"date\":pd.date_range(\"2017-08-16\", \"2017-08-31\").tolist(),\n            \"store_nbr\":c.store_nbr.iloc[i],\n            \"family\":c.family.iloc[i],\n            \"sales\":0\n        })\n    )\nzero_prediction = pd.concat(zero_prediction)\nzero_prediction['date'] = zero_prediction.date.dt.to_period('D')\ndel c\ngc.collect()\nzero_prediction = zero_prediction.set_index(['date', 'store_nbr', 'family'])\nzero_prediction.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-06T15:39:48.183666Z","iopub.execute_input":"2022-06-06T15:39:48.184014Z","iopub.status.idle":"2022-06-06T15:39:48.493769Z","shell.execute_reply.started":"2022-06-06T15:39:48.183983Z","shell.execute_reply":"2022-06-06T15:39:48.492868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Week seasonality features","metadata":{}},{"cell_type":"markdown","source":"It's important to select training period correctly. Let's check stores opening, as we want a period where ALL data for all stores are available, so there is **no zero sales values as store hasn't been opened yet**","metadata":{}},{"cell_type":"code","source":"a = train.groupby([\"date\",\"store_nbr\"]).sum().reset_index()\na = a[a[\"sales\"] > 0].groupby(\"store_nbr\")[[\"date\"]].min().sort_values(by=\"date\",ascending = False).head(5)\na.rename(columns = {'date':'open_date'}, inplace = True)\na","metadata":{"_kg_hide-input":true,"_kg_hide-output":false,"execution":{"iopub.status.busy":"2022-06-06T15:39:48.494868Z","iopub.execute_input":"2022-06-06T15:39:48.4952Z","iopub.status.idle":"2022-06-06T15:39:48.719043Z","shell.execute_reply.started":"2022-06-06T15:39:48.49517Z","shell.execute_reply":"2022-06-06T15:39:48.718016Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Store #52 was the latest opened store (**open date is 2017-04-20**),\nso let's take next date as start - **2017-04-21**: ","metadata":{}},{"cell_type":"code","source":"y = train.unstack(['store_nbr', 'family']).loc[\"2017-04-21\":]\nfourier = CalendarFourier(freq = 'W', order = 3)\ndp = DeterministicProcess(index = y.index,\n                          order = 1,\n                          seasonal = False,\n                          constant = False,\n                          additional_terms = [fourier],\n                          drop = True)\nx = dp.in_sample()\nx = x.join(calendar)\nx.index.name = \"date\"\n\n# Test will have a prediction for the next 16 days from 15.08 till 31.08\nxtest = dp.out_of_sample(steps = 16) \nxtest = xtest.join(calendar)\nxtest.index.name = \"date\"\n\ndel hol\ndel calendar\ndel dp\ndel oil\n_ = gc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-06-06T15:39:48.720253Z","iopub.execute_input":"2022-06-06T15:39:48.720668Z","iopub.status.idle":"2022-06-06T15:39:49.520284Z","shell.execute_reply.started":"2022-06-06T15:39:48.720625Z","shell.execute_reply":"2022-06-06T15:39:49.519243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"_____________________________________\n## 2Ô∏è‚É£ Quick analysis of Multivariate Time Series data\n\nFirst, what is a multivariate time series?  \n\"A Multivariate Time Series consist of more than one time-dependent variable and each variable depends not only on its past values but also has **some dependency on other variables**.\"  \nWhat we have in this playground competition is a Multivariate TS - each combination of FAMILY x STORE is a *SEPARATE* Time Series. And we have 33 x 54 = 1782 TS values (**y** to predict). If we take into account our zero forecasting trick, we get 1658 different TS, which is a little better.\n\nSo let's build a correlation matrix between time-dependent variables (product families here), as it's a very well known fact, that for the FMCG goods there is a strong correlation between product categories - when buying product X, consumer buys product Y as well:\n\n[‚¨áÔ∏è Step 3](#3Ô∏è‚É£-Find-relevant-external-features-for-several-TS-components-in-Multivariate-Time-Series)\n","metadata":{}},{"cell_type":"code","source":"#from 2017-04-30 for faster calculation\na = train[\"2017-04-30\":].reset_index()\na[\"ind\"] = 1\na[\"ind\"] = a.groupby(\"family\").ind.cumsum().values\na = pd.pivot(a, index = \"ind\", columns = \"family\", values = \"sales\").corr(method=\"spearman\")\nmask = np.triu(a.corr(method=\"spearman\"))\nplt.figure(figsize=(20, 20))\nsns.heatmap(a,\n        annot=True,\n        fmt='.1f',\n        cmap='coolwarm',\n        square=True,\n        mask=mask,\n        linewidths=1,\n        cbar=False)\nplt.title(\"Sales correlations for product families\",fontsize = 20)\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-06T15:39:49.521327Z","iopub.execute_input":"2022-06-06T15:39:49.521648Z","iopub.status.idle":"2022-06-06T15:39:52.160309Z","shell.execute_reply.started":"2022-06-06T15:39:49.52162Z","shell.execute_reply":"2022-06-06T15:39:52.159303Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**BABY CARE, BOOKS, HOME APPLIANCES, LADIESWEAR, LAWN AND GARDEN, PET SUPPLIES** are not correlated with other products sales at all.  \n**SCHOOL AND OFFICE SUPPLIES, LINGERIE** has a very low correlation.\nBut the rest of the product families' sales are all correlated, so **we clearly have a multivariate time series task.**  \n\n> **Note:** We can use sales forecast of other products families in the SAME store **as A FEATURE** to improve prediction accuracy of specific product, as there's an influence on sales numbers between products. This is what we'll do later.  \n\nNow let's check trends, seasonality and anomalies for these low correlated product families:","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(1,2,figsize = (20,4))\ntrain.loc[\"2016-08-01\":].filter(like = 'BABY CARE', axis=0).groupby([\"date\"]).sales.sum().plot(ax = ax[0], title = \"BABY CARE\")\ntrain.loc[\"2016-08-01\":].filter(like = 'BOOKS', axis=0).groupby([\"date\"]).sales.sum().plot(ax = ax[1], title = \"BOOKS\")\nplt.show()\nfig, ax = plt.subplots(1,2,figsize = (20,4))\ntrain.loc[\"2016-01-01\":].filter(like = 'HOME APPLIANCES', axis=0).groupby([\"date\"]).sales.sum().plot(ax = ax[0], title = \"HOME APPLIANCES\")\ntrain.loc[\"2016-08-01\":].filter(like = 'LADIESWEAR', axis=0).groupby([\"date\"]).sales.sum().plot(ax = ax[1], title = \"LADIESWEAR\")\nplt.show()\nfig, ax = plt.subplots(1,2,figsize = (20,4))\ntrain.loc[\"2016-08-01\":].filter(like = 'LAWN AND GARDEN', axis=0).groupby([\"date\"]).sales.sum().plot(ax = ax[0], title = \"LAWN AND GARDEN\")\ntrain.loc[\"2016-08-01\":].filter(like = 'PET SUPPLIES', axis=0).groupby([\"date\"]).sales.sum().plot(ax = ax[1], title = \"PET SUPPLIES\")\nplt.show()\nfig, ax = plt.subplots(1,2,figsize = (20,4))\ntrain.loc[\"2016-08-01\":].filter(like = 'SCHOOL AND OFFICE SUPPLIES', axis=0).groupby([\"date\"]).sales.sum().plot(ax = ax[0], title = \"SCHOOL AND OFFICE SUPPLIES\")\ntrain.loc[\"2016-08-01\":].filter(like = 'LINGERIE', axis=0).groupby([\"date\"]).sales.sum().plot(ax = ax[1], title = \"LINGERIE\")\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-06T15:39:52.161477Z","iopub.execute_input":"2022-06-06T15:39:52.161808Z","iopub.status.idle":"2022-06-06T15:40:29.901792Z","shell.execute_reply.started":"2022-06-06T15:39:52.161777Z","shell.execute_reply":"2022-06-06T15:40:29.900789Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**What we see here:**\n* 1st of January has zero sales - ie stores are closed on 1st of Jan. Make sense to start train period after 2017.01.01, just to avoid extra feature engineering on closing dates\n* SCHOOL AND OFFICE SUPPLIES - looks like a [non-stationary TS](https://towardsdatascience.com/stationarity-in-time-series-analysis-90c94f27322), let's check that using Augmented Dickey-Fuller test further. Looks like this **y** will be difficult to predict by linear models, tree based models might be a better fit;\n* BOOKS -  this product category might be some kind of an assortiment trial and on clearance since the begining of 2017. Sharp sales decline might be result of assortiment decline, which for books lead to near zero daily sales. Same - tree based models might have a better prediction results;\n* LAWN AND GARDEN - there is an uplift in sales numbers from Dec'2016, most probably from introduction of this product category in more stores around the country. Feel free to check that ;-). But in overall - no specific ideas in terms of model type selection - ie linear models should be ok, as soon as you'll start train period **at least from 2017-01-02**;\n* HOME APPLIANCES - looks like a non-stationary TS, let's check that using Augmented Dickey-Fuller test further. Tree based models might be a better fit for this product family;\n* PET SUPPLIES - some monotonic trend component in TS, non-stationary TS;\n* LINGERIE - same, some monotonic trend component in TS, non-stationary TS;\n\nLet's check our time series for Stationarity using [Augmented Dickey-Fuller test](https://en.wikipedia.org/wiki/Augmented_Dickey‚ÄìFuller_test) with a significance level of less than 5%.  \nThe intuition behind this test is that it determines how strongly a time series is defined by a trend.  \nAs an example, let's take PET SUPPLIES, feel free to do the same for the rest of the \"suspicious\" product families:","metadata":{}},{"cell_type":"code","source":"result = adfuller(\n    np.log1p(\n        y.loc[\"2016-08-01\":, y.columns.get_level_values(\"family\").isin([\"PET SUPPLIES\"])].mean(axis=\"columns\")\n    )\n)\nprint('ADF Statistic: %f' % result[0])\nprint('p-value: %f' % result[1])\nprint('Critical Values:')\nfor key, value in result[4].items():\n    print('\\t%s: %.3f' % (key, value))\n\nif result[0] < result[4][\"1%\"]:\n    print (\"Reject Ho - Time Series is Stationary\")\nelse:\n    print (\"Failed to Reject Ho - Time Series is Non-Stationary\")","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2022-06-06T15:40:29.90301Z","iopub.execute_input":"2022-06-06T15:40:29.903365Z","iopub.status.idle":"2022-06-06T15:40:29.927691Z","shell.execute_reply.started":"2022-06-06T15:40:29.903333Z","shell.execute_reply":"2022-06-06T15:40:29.92663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Ok, now we want to test **tree based regressors (not linear)** for non-satitionary TS with a low multivariate correlation.  \nI did it already and found out, that only \"SCHOOL AND OFFICE SUPPLIES\", \"BOOKS\" families will have an improvement from Tree based regressors vs. baseline Ridge.  \nFor these product families I'll use **GradientBoostingRegressor + ExtraTreesRegressor ensemble model**:","metadata":{}},{"cell_type":"code","source":"non_st_ts = [\"SCHOOL AND OFFICE SUPPLIES\",\"BOOKS\"]","metadata":{"execution":{"iopub.status.busy":"2022-06-06T15:40:29.929096Z","iopub.execute_input":"2022-06-06T15:40:29.929665Z","iopub.status.idle":"2022-06-06T15:40:29.935474Z","shell.execute_reply.started":"2022-06-06T15:40:29.929629Z","shell.execute_reply":"2022-06-06T15:40:29.934209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"a = train.groupby(\"family\").sales.mean().sort_values(ascending = False).reset_index()\npx.bar(a, y = \"family\", x=\"sales\", color = \"family\", title = \"Top selling product families\")","metadata":{"_kg_hide-input":true,"_kg_hide-output":false,"execution":{"iopub.status.busy":"2022-06-06T15:40:29.938166Z","iopub.execute_input":"2022-06-06T15:40:29.938737Z","iopub.status.idle":"2022-06-06T15:40:31.319974Z","shell.execute_reply.started":"2022-06-06T15:40:29.938695Z","shell.execute_reply":"2022-06-06T15:40:31.319158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In FMCG high sales product categories usually have a low price elastisity and strong intra-week correlation.  \nSo, let's take TOP-40% most sold product families (personal guess, feel free to improve that üòâ) and add **K-nearest neighbors regressor** (for strong intra-week/week seasonality) and **Bayesian regressor** into ensemble model for them.  \nTo simplify the things in CustomRegressor - I'll use inverse logic, with bottom 60% of low sales products in the separate list.","metadata":{}},{"cell_type":"code","source":"low_sales_ts = [\"MAGAZINES\",\"LAWN AND GARDEN\",\"BABY CARE\",\n                \"CELEBRATION\",\"GROCERY II\",\"HARDWARE\",\"AUTOMOTIVE\",\n                \"HOME AND KITCHEN I\",\"HOME AND KITCHEN II\",\n                \"HOME APPLIANCES\",\"LINGERIE\",\n                \"LADIESWEAR\",\"SEAFOOD\",\"PLAYERS AND ELECTRONICS\",\n                \"PET SUPPLIES\",\"BEAUTY\",\"PREPARED FOODS\",\n                \"HOME CARE\",\"CLEANING\"]","metadata":{"execution":{"iopub.status.busy":"2022-06-06T15:40:31.320931Z","iopub.execute_input":"2022-06-06T15:40:31.321599Z","iopub.status.idle":"2022-06-06T15:40:31.32676Z","shell.execute_reply.started":"2022-06-06T15:40:31.321559Z","shell.execute_reply":"2022-06-06T15:40:31.325677Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's take start date with some margin to stabilize sales numbers after opening - **2017-04-30**:","metadata":{}},{"cell_type":"code","source":"sdate = '2017-04-30'\nx=x.loc[sdate:]\ny=y.loc[sdate:]","metadata":{"execution":{"iopub.status.busy":"2022-06-06T15:40:31.327781Z","iopub.execute_input":"2022-06-06T15:40:31.32862Z","iopub.status.idle":"2022-06-06T15:40:31.346773Z","shell.execute_reply.started":"2022-06-06T15:40:31.328583Z","shell.execute_reply":"2022-06-06T15:40:31.345955Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3Ô∏è‚É£ Find relevant external features for several TS components in Multivariate Time Series\n### Step 3.1 Select columns for external features search  \n\nTo find new features we'll use [Upgini Feature search and enrichment library for supervised machine learning applications](https://github.com/upgini/upgini#readme).\n\nTo initiate search with Upgini library, you need to define so called [*search keys*](https://github.com/upgini/upgini#-search-key-types-we-support-more-is-coming) - a set of columns to join external data sources and features. In this competition we can use the following keys:\n\n1. Column **date** should be used as **SearchKey.DATE**.;  \n2. **Country** as \"EC\" (ISO-3166 country code for Ecuador), as all the data came from Favorita retailer in Ecuador.\n    \nWith this set of search keys, our dataset will be matched with [different time-specific features (such as weather data, calendar data, financial data, etc)](https://github.com/upgini/upgini#-connected-data-sources-and-coverage), taking into account the country where sales happened. Than relevant selection and ranking will be done.  \nAs a result, we'll add new, only relevant features with additional information about specific dates in Ecuador.  \n\nWe already have date as index in *train* and *test* datasets, so let's use it. Index name is `date`, so column will have the same name.   \n\nTo start the search, we need to initiate *scikit-learn* compartible `FeaturesEnricher` transformer with appropriate **search** parameters and cross-validation type (here we use [TimeSeries](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.TimeSeriesSplit.html) CV, because our target variable strongly depends on time, ie we have TS prediction task).  \nAfter that, we can call the **fit** or **fit_transform**  method of `features_enricher` to start the search.\n\n[‚¨áÔ∏è Step 4](#4Ô∏è‚É£-Train-final-model)\n","metadata":{}},{"cell_type":"code","source":"from upgini import FeaturesEnricher, SearchKey, ModelTaskType\nfrom upgini.metadata import CVType\n\nenricher = FeaturesEnricher(\n    # search keys for feature enrichment - date and country\n    search_keys={\n        \"date\": SearchKey.DATE\n    },\n    country_code = \"EC\",\n    # time series prediction tasks for relevant external feature selection\n    cv = CVType.time_series,\n)","metadata":{"execution":{"iopub.status.busy":"2022-06-06T15:40:31.348199Z","iopub.execute_input":"2022-06-06T15:40:31.348869Z","iopub.status.idle":"2022-06-06T15:40:36.695306Z","shell.execute_reply.started":"2022-06-06T15:40:31.348823Z","shell.execute_reply":"2022-06-06T15:40:36.694228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Step 3.2 Re-use existing training dataset and generate Univariate TS from Multivariate TS\nFor `FeaturesEnricher.fit_transform()` method, just like in all scikit-learn transformers, we should pass **X_train** as the first argument and **y_train** as the second argument.   \n**y_train** is needed to select **only relevant features & datasets, which will improve accuracy**. And rank new external features according to their prediction contribution.  \nMultioutput is not supported yet in `FeaturesEnricher` transformer  (supported dimension of y = 1), but that's what we have in this competition - each combination of FAMILY x STORE is a *SEPARATE* time series/y. And we have 1658 different **y** (considering Zero forecast). \n\nWorkaround is very simpe - let's search for the new features to predict **mean number of daily sales in all stores for ONE product family**. And then enrich all **X_train** and **y** for THIS product family in Multivariate TS. As the same product category should be influenced by the same new features, no matter what store we predict.\n\nAs example, let's try to find features relevant for LIQUOR,WINE,BEER and CLEANING families, one by one - this types of FMCG goods usually have a strong correlation with holidays and work week calendar. And Upgini has a lot of datascources and features with types of holiday, distance to holidays, holidays influence scores on specific commercial activities, etc. for 232 countries with 22 years of history.  \n","metadata":{}},{"cell_type":"markdown","source":"That's just a quick guess, feel free to test other product families by fork & change of this kernel üòâ","metadata":{}},{"cell_type":"code","source":"enriched_ts_map = {}\ny_fe_1 = y.loc[:, y.columns.get_level_values(\"family\").isin([\"LIQUOR,WINE,BEER\"])].mean(axis=\"columns\")\ny_fe_2 = y.loc[:, y.columns.get_level_values(\"family\").isin([\"CLEANING\"])].mean(axis=\"columns\")\n\n# log transform to reshape y distribution closer to normal, as we'll use same approach in the final model\ny_fe_1 = np.log1p(y_fe_1)\ny_fe_2 = np.log1p(y_fe_2)","metadata":{"execution":{"iopub.status.busy":"2022-06-06T15:40:36.6967Z","iopub.execute_input":"2022-06-06T15:40:36.697044Z","iopub.status.idle":"2022-06-06T15:40:36.710478Z","shell.execute_reply.started":"2022-06-06T15:40:36.697015Z","shell.execute_reply":"2022-06-06T15:40:36.709341Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Step 3.3 Initiate feature search and enrichment  \n`FeaturesEnricher.fit_transform()` has a flag `calculate_metrics` for the quick estimation of quality improvement from the new external features on cross-validation and eval sets. This step is quite similar to [sklearn.model_selection.cross_val_score](https://scikit-learn.org/stable/modules/cross_validation.html#computing-cross-validated-metrics), so you can pass exact metric with `scoring` parameter:\n\n1. Built-in scoring [functions](https://github.com/upgini/upgini/blob/main/README.md#-accuracy-and-uplift-metrics-calculations);\n2. Custom scorer.    \n \nNotice that you should pass **X_train** as the first argument and **y_train** as the second argument for `FeaturesEnricher.fit_transform()`, just like in scikit-learn.  \n\n*Step will take around 2 minutes*","metadata":{}},{"cell_type":"code","source":"%%time\nfrom upgini import FeaturesEnricher, SearchKey, ModelTaskType\nfrom upgini.metadata import RuntimeParameters, CVType\n\n# LIQUOR,WINE,BEER\nX_enriched = enricher.fit_transform(\n    x.copy().reset_index(), \n    y_fe_1.reset_index(drop=True).values,\n    calculate_metrics=True,\n    keep_input=True, #optional, save all initial columns from input data frame,\n    max_features=2,  #optional, enrich X ONLY with TOP2 new features based on SHAP value\n).set_index(\"date\")","metadata":{"execution":{"iopub.status.busy":"2022-06-06T07:21:53.856442Z","iopub.execute_input":"2022-06-06T07:21:53.856937Z","iopub.status.idle":"2022-06-06T07:23:30.376717Z","shell.execute_reply.started":"2022-06-06T07:21:53.856894Z","shell.execute_reply":"2022-06-06T07:23:30.375676Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We've got **3 new relevant external features**, which might improve accuracy of the model ranked by [SHAP values](https://en.wikipedia.org/wiki/Shapley_value).   \n\nInitial features from search dataset will be checked for relevancy as well, so you don't need an extra feature selection step.\n\nMSE uplift after enrichment using all of the new external features is *positive* and *significant* - more than +30%.  \nWe enriched initial feature space with only **TOP-2** most important features, according to the SHAP values (parameter `max_features=2`).\n\n>Generally it's a bad idea to put a lot of features with unknown structure (and possibly high pairwise correlation) into a linear model, even with regularization like Ridge or Lasso, without careful selection and pre-processing.\n\nLet's add these **TOP-2** features to external features map (enriched_ts_map) with key LIQUOR,WINE,BEER:","metadata":{}},{"cell_type":"code","source":"enriched_ts_map[\"LIQUOR,WINE,BEER\"] = list(set(X_enriched.columns) - set(x.columns))","metadata":{"execution":{"iopub.status.busy":"2022-06-06T07:23:30.378007Z","iopub.execute_input":"2022-06-06T07:23:30.379064Z","iopub.status.idle":"2022-06-06T07:23:30.385004Z","shell.execute_reply.started":"2022-06-06T07:23:30.379019Z","shell.execute_reply":"2022-06-06T07:23:30.383985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next step is to enrich test dataset with the **TOP-2** new features as well. And that's all for enrichment step.  \n\n*Step will take around 2 minutes*","metadata":{}},{"cell_type":"code","source":"X_test_enriched = enricher.transform(\n    xtest.copy().reset_index(),\n    keep_input=True,\n    max_features=2, \n).set_index(\"date\")","metadata":{"execution":{"iopub.status.busy":"2022-06-06T07:23:30.386417Z","iopub.execute_input":"2022-06-06T07:23:30.387052Z","iopub.status.idle":"2022-06-06T07:24:53.37777Z","shell.execute_reply.started":"2022-06-06T07:23:30.387006Z","shell.execute_reply":"2022-06-06T07:24:53.376543Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, the same `fit_transform` step for CLEANING product family. Using enrichment pipelining for already enriched `X_enriched` and `X_test_enriched`.","metadata":{}},{"cell_type":"code","source":"# CLEANING\nX_enriched2 = enricher.fit_transform(\n    X_enriched.reset_index(), \n    y_fe_2.reset_index(drop=True).values,\n    calculate_metrics=True,\n    keep_input=True, \n    max_features=2,\n).set_index(\"date\")\n\nX_test_enriched2 = enricher.transform(\n    X_test_enriched.reset_index(),\n    keep_input=True,\n    max_features=2, \n).set_index(\"date\")","metadata":{"execution":{"iopub.status.busy":"2022-06-06T07:24:53.380447Z","iopub.execute_input":"2022-06-06T07:24:53.381952Z","iopub.status.idle":"2022-06-06T07:27:42.975618Z","shell.execute_reply.started":"2022-06-06T07:24:53.381898Z","shell.execute_reply":"2022-06-06T07:27:42.974481Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Again, let's add most important features to external features map with key CLEANING:","metadata":{}},{"cell_type":"code","source":"enriched_ts_map[\"CLEANING\"] = list(set(X_enriched2.columns) - set(X_enriched.columns))","metadata":{"execution":{"iopub.status.busy":"2022-06-06T07:27:42.977137Z","iopub.execute_input":"2022-06-06T07:27:42.977477Z","iopub.status.idle":"2022-06-06T07:27:42.982818Z","shell.execute_reply.started":"2022-06-06T07:27:42.977451Z","shell.execute_reply":"2022-06-06T07:27:42.981743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Summary on external feature search & enrichment","metadata":{}},{"cell_type":"code","source":"print(\"Number of features, initial -> after enrichment:\",x.shape[1],\"->\",X_enriched2.shape[1])\nint_features = set(x.columns.to_list())\next_features = [col for ext_features_ in enriched_ts_map.values() for col in ext_features_]\n\nx = X_enriched2\nxtest = X_test_enriched2\ndel X_enriched, X_enriched2\ndel X_test_enriched, X_test_enriched2\ndel y_fe_1\ndel y_fe_2\n_ = gc.collect()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-06T07:27:42.983715Z","iopub.execute_input":"2022-06-06T07:27:42.984645Z","iopub.status.idle":"2022-06-06T07:27:43.200049Z","shell.execute_reply.started":"2022-06-06T07:27:42.984599Z","shell.execute_reply":"2022-06-06T07:27:43.198772Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4Ô∏è‚É£ Train final model\n### 4.1 Same store sales predictions as new features (\"not-so-blending\")\n\nOnce again - it's a very well known fact, that for the FMCG goods there is a strong correlation between product categories - when buying product X, consumer buys product Y as well.  \nAnd we already check what product families are correlated and what is not. Most of the product famlilies are correlated (25 of 33).\nSo let's try to use sales PREDICTION by product family as a new features, to catch information on joint sales\n> **Note on original notebook approach**: This approach was called \"blending\" in the original notebook. But there was no holdout set to avoid overfitting  and blending linear model are being trained on the same data as the ensembling (final) model. I strongly recommend you to read about blending with great visualisation [here](https://towardsdatascience.com/ensemble-learning-stacking-blending-voting-b37737c4f483). In addition to that, in a final Custom Regressor there was no filter \"same store sales predictions\" and sales from ALL stores was used as features for every single store, so I fixed that.\n\n[‚¨áÔ∏è Step 5](#5Ô∏è‚É£-Submit-prediction-with-enriched-features-and-calculate-final-leaderbord-progress)","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nfrom sklearn.compose import TransformedTargetRegressor, ColumnTransformer\nfrom sklearn.preprocessing import PowerTransformer\nfrom sklearn.pipeline import make_pipeline\n\nlnr_reg = TransformedTargetRegressor(\n    regressor = LinearRegression(fit_intercept = True, n_jobs = -1),\n    func=np.log1p,\n    inverse_func=np.expm1\n)\nlnr = make_pipeline(\n    ColumnTransformer([(\"drop_f\", \"drop\", ext_features)], remainder=\"passthrough\"),\n    PowerTransformer(),\n    lnr_reg\n)\n\nlnr.fit(x, y)\nyfit_lnr = pd.DataFrame(lnr.predict(x), index = x.index, columns = y.columns).clip(0.)\nypred_lnr = pd.DataFrame(lnr.predict(xtest), index = xtest.index, columns = y.columns).clip(0.)\n\ny_ = y.stack(['store_nbr', 'family'])\ny_['lnr'] = yfit_lnr.stack(['store_nbr', 'family'])['sales']","metadata":{"execution":{"iopub.status.busy":"2022-06-06T07:27:43.201455Z","iopub.execute_input":"2022-06-06T07:27:43.201983Z","iopub.status.idle":"2022-06-06T07:27:43.626753Z","shell.execute_reply.started":"2022-06-06T07:27:43.201949Z","shell.execute_reply":"2022-06-06T07:27:43.625954Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now add this linear regression's prediction to existing train and test data:","metadata":{}},{"cell_type":"code","source":"#from 30.04.17 till 31.08.17\nylnr = yfit_lnr.append(ypred_lnr)\nx = x.join(ylnr)\nxtest = xtest.join(ylnr)\ndel yfit_lnr\ndel ypred_lnr\ndel ylnr\n_ = gc.collect()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-06T07:27:43.627937Z","iopub.execute_input":"2022-06-06T07:27:43.629003Z","iopub.status.idle":"2022-06-06T07:27:43.83894Z","shell.execute_reply.started":"2022-06-06T07:27:43.628957Z","shell.execute_reply":"2022-06-06T07:27:43.83733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4.2 Final model - Custom Regressor","metadata":{}},{"cell_type":"markdown","source":"What `CustomRegressor` is doing here - it's fiting individual model on EVERY component of Multivariate TS - single TS as combination FAMILY x STORE. And there will be 1658 independent models (excluding Zero forecast).  \nIn our analysis on cross product correlations, we decided to introduce 3 groups of models in `CustomRegressor`, customized for specific product family set:\n* Selected non stationary time-series with low correlations: **GradientBoostingRegressor + ExtraTreesRegressor voting ensemble model**\n* High sales product categories (TOP 40% by sales): **K-nearest neighbors regressor + Bayesian regressor + Ridge + SVR voting ensemble model**\n* Product families enriched with new external features: simpe **Ridge regressor**\n* Low sales product categories (BOTTOM 60% by sales): **Ridge regressor + SVR voting ensemble**","metadata":{}},{"cell_type":"code","source":"import warnings\nfrom sklearn.linear_model import Ridge, ARDRegression\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import ExtraTreesRegressor, GradientBoostingRegressor\nfrom sklearn.ensemble import BaggingRegressor, VotingRegressor\nfrom sklearn.compose import ColumnTransformer, TransformedTargetRegressor\nfrom sklearn.pipeline import make_pipeline\n\n# SEED for reproducible result\nSEED = 5\n\nclass CustomRegressor():\n    \n    def __init__(self, ext_features = None, knn_features = None, non_st_ts = None, low_sales_ts = None, enriched_ts_map = None, n_jobs=-1):\n        \n        self.n_jobs = n_jobs\n        self.ext_features = ext_features\n        self.knn_features = knn_features\n        self.non_st_ts = non_st_ts\n        self.low_sales_ts = low_sales_ts\n        self.enriched_ts_map = enriched_ts_map\n        self.estimators_ = None\n        self.product_names_ = None\n        self.store_names_ = None\n        \n    def _estimator_(self, X, y):\n        warnings.simplefilter(action='ignore', category=FutureWarning)\n        \n        # We remove external features for the products, which univariate TS were note used during feature search & enrichment\n        # As these features won't be relevant for the rest of product families and might decrease accuracy\n        remove_ext_features = ColumnTransformer([(\"drop_f\", \"drop\", self.ext_features)],remainder=\"passthrough\")\n        \n        if y.name[2] in self.non_st_ts:\n            b1 = GradientBoostingRegressor(n_estimators = 175, max_depth=3, loss='huber', random_state=SEED)\n            r1 = ExtraTreesRegressor(n_estimators = 250, n_jobs=self.n_jobs, random_state=SEED)\n            b2 = BaggingRegressor(base_estimator=r1,\n                                  n_estimators=10,\n                                  n_jobs=self.n_jobs,\n                                  random_state=SEED)\n            model = make_pipeline(\n                remove_ext_features,\n                VotingRegressor ([('gbr', b1), ('et', b2)])\n            )      \n        elif y.name[2] in self.enriched_ts_map.keys():\n            ## use only external features selected for this particular TS\n            ext_features_ = [col for col in self.ext_features if col not in self.enriched_ts_map[y.name[2]]]\n            remove_ext_features_ = ColumnTransformer([(\"drop_f\", \"drop\", ext_features_)],remainder=\"passthrough\")\n            \n            # Yeo-Johnson transform for X to reshape X distribution closer to normal (Gaussian) for linear model\n            power_tr = PowerTransformer()\n            \n            # Log(y) + 1 transform to reshape y distribution closer to normal for linear model,\n            # then round transform - as  LIQUOR,WINE,BEER and CLEANING sales must be integer\n            ridge = TransformedTargetRegressor(\n                regressor = Ridge(fit_intercept=True, solver='auto', alpha=0.7, normalize=True, random_state=SEED),\n                func=np.log1p,\n                inverse_func=np.expm1\n            )\n            ridge_round_to_int = TransformedTargetRegressor(\n                regressor=ridge,\n                inverse_func=np.rint\n            )\n            model = make_pipeline(\n                remove_ext_features_,\n                power_tr,\n                ridge_round_to_int\n            )\n            \n        elif y.name[2] in self.low_sales_ts:\n            ridge = TransformedTargetRegressor(\n                regressor = Ridge(fit_intercept=True, solver='auto', alpha=0.75, normalize=True, random_state=SEED),\n                func=np.log1p,\n                inverse_func=np.expm1\n            )\n            svr = TransformedTargetRegressor(\n                regressor = SVR(C = 0.2, kernel = 'rbf'),\n                func=np.log1p,\n                inverse_func=np.expm1\n            )\n            model = VotingRegressor([('ridge', ridge), ('svr', svr)])\n        else:\n            ridge = make_pipeline(\n                remove_ext_features,\n                TransformedTargetRegressor(\n                    regressor = Ridge(fit_intercept=True, solver='auto', alpha=0.6, normalize=True, random_state=SEED),\n                    func=np.log1p,\n                    inverse_func=np.expm1)\n            )\n            svr = make_pipeline(\n                remove_ext_features,\n                TransformedTargetRegressor(\n                    regressor = SVR(C = 0.2, kernel = 'rbf'),\n                    func=np.log1p,\n                    inverse_func=np.expm1)\n            )\n            # We'll use specific feature set for KNN to cluster observations in a way to catch week and intra-week seasonality\n            knn = make_pipeline(\n                ColumnTransformer([(\"selector\", \"passthrough\", self.knn_features)], remainder=\"drop\"),\n                PowerTransformer(),\n                KNeighborsRegressor(n_neighbors=3, n_jobs=self.n_jobs)\n            )\n            ard = make_pipeline(\n                remove_ext_features,\n                TransformedTargetRegressor(\n                    regressor = ARDRegression(fit_intercept=True, normalize=True, n_iter=300),\n                    func=np.log1p,\n                    inverse_func=np.expm1)\n            )\n            estimators = [\n                ('ridge', ridge),\n                ('svr', svr),\n                (\"ard\", ard),\n                (\"knn\",knn)\n            ]\n            model = VotingRegressor(estimators)\n            \n        model.fit(X, y)\n        return model\n    \n    def fit(self, X, y):\n        print(\"Fit stage...\")\n        self.product_names_ = [str(y.iloc[:, i].name[2]) for i in range(y.shape[1])]\n        self.store_names_ = [str(y.iloc[:, i].name[1]) for i in range(y.shape[1])]\n        self.estimators_ = []\n        for i, n in tqdm(enumerate(self.product_names_)):\n            estimator_ = self._estimator_(\n                # select as features only predictions of product sales in the same store or same product in other stores\n                X.filter(\n                    regex= n + \"'\\)$|\\(\\d|^[a-zA-Z_0-9., ]+$|\\('sales', '\" + str(y.iloc[:, i].name[1]) + \"',\",\n                    axis=1,\n                ),\n                y.iloc[:, i],\n            )\n            self.estimators_.append(estimator_)\n        \n    def predict(self, X):\n        print(\"Prediction stage...\")\n        y_pred = []\n        for e, n, m in tqdm(zip(self.estimators_, self.product_names_, self.store_names_)):\n            y_pred_ = e.predict(\n                # select as features only predictions of product sales in the same store or same product in other stores\n                X.filter(\n                    regex= n + \"'\\)$|\\(\\d|^[a-zA-Z_0-9., ]+$|\\('sales', '\" + m + \"',\",\n                    axis=1,\n                )\n            )\n            y_pred.append(y_pred_)\n            \n        return np.stack(y_pred, axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-06-06T08:41:38.907878Z","iopub.execute_input":"2022-06-06T08:41:38.908384Z","iopub.status.idle":"2022-06-06T08:41:38.936914Z","shell.execute_reply.started":"2022-06-06T08:41:38.908351Z","shell.execute_reply":"2022-06-06T08:41:38.935903Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Be patient - training may take up to 15 minutes:","metadata":{}},{"cell_type":"code","source":"%%time \n\n# manual selection for KNN regression\nknn_features = list(int_features - set(['oil_lags2', 'oil_lags1',\"trend\"]))\nmodel = CustomRegressor(ext_features, knn_features, non_st_ts, low_sales_ts, enriched_ts_map, n_jobs=-1)\nmodel.fit(x, y)\n\ny_pred = pd.DataFrame(model.predict(x), index=x.index, columns=y.columns)","metadata":{"execution":{"iopub.status.busy":"2022-06-06T08:41:43.414818Z","iopub.execute_input":"2022-06-06T08:41:43.415345Z","iopub.status.idle":"2022-06-06T08:55:10.907409Z","shell.execute_reply.started":"2022-06-06T08:41:43.415283Z","shell.execute_reply":"2022-06-06T08:55:10.906139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5Ô∏è‚É£ Submit prediction with enriched features and calculate final leaderbord progress\nLet's quickly estimate model accuracy (no cross validation!) and submit:","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import mean_squared_log_error as msle\ny_pred = y_pred.stack(['store_nbr', 'family']).clip(0.)\ny_ = y.stack(['store_nbr', 'family']).clip(0.)\ny_['pred'] = y_pred.values\nprint(y_.groupby('family').apply(lambda r : np.sqrt(np.sqrt(msle(r['sales'], r['pred'])))))\nprint('RMSLE : ', np.sqrt(np.sqrt(msle(y_['sales'], y_['pred']))))","metadata":{"execution":{"iopub.status.busy":"2022-06-06T08:55:10.909935Z","iopub.execute_input":"2022-06-06T08:55:10.910767Z","iopub.status.idle":"2022-06-06T08:55:11.216454Z","shell.execute_reply.started":"2022-06-06T08:55:10.910704Z","shell.execute_reply":"2022-06-06T08:55:11.21561Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time \n\nypred = pd.DataFrame(model.predict(xtest), index = xtest.index, columns = y.columns).clip(0.)\nypred = ypred.stack(['store_nbr', 'family'])\nypred = ypred.append(zero_prediction).sort_index()\nsub = pd.read_csv('../input/store-sales-time-series-forecasting/sample_submission.csv')\nsub['sales'] = ypred.values\nsub.to_csv('submission.csv', index = False)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-06T08:55:11.217608Z","iopub.execute_input":"2022-06-06T08:55:11.218159Z","iopub.status.idle":"2022-06-06T08:57:40.694177Z","shell.execute_reply.started":"2022-06-06T08:55:11.218124Z","shell.execute_reply":"2022-06-06T08:57:40.693382Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This submission has score **0.39261** on LB and **2nd place** by the time of publication (2022/06/06).  \nJust a reminder - baseline solution had **0.40419**\n\n#### ‚ö°Ô∏è We've got a significant improvement AND still we not used ALL competition datasets:\n* promotion information on product family from train and test datasets  \n* store dataset - location, category, cluster for the stores \n\n#### Go ahead and improve this solution further! üìàüéâ","metadata":{}},{"cell_type":"markdown","source":"## üåé Relevant external features & data sources\nHere is the description of relevant features from Upgini enrichment:\n\n* **f_days_to_holiday_5ce1a653** - number of days to the nearest holiday in Ecuador (any type - national or local), which is a non working day. Transfers will be taken into account.\n\n* **f_days_from_holiday_fbad7b66** -  number of days from the nearest holiday in Ecuador (any type - national or local), which is a non working day. Transfers will be taken into account.\n\n* **f_holiday_code_3d_ahead_d4998487** - weighted sum of holidays in Ecuador in a 3-day window from today to the day after tomorrow, where national holidays have a weight of 2 and local holidays have a weight of 1.\n\n* **f_month_cos1_c2b0dad4** - 1st order component to take into account the intra-month seasonality, calculated according to the formula $month\\_cos1 = \\cos(\\frac{2 \\pi \\cdot order \\cdot day\\_of\\_month}{days\\_in\\_month})$.\n\n#### üöÄ Happy kaggling and external features search! \n<sup>üòî Found error in the library or a bug in notebook code? Our bad! <a href=\"https://github.com/upgini/upgini/issues/new?assignees=&title=readme%2Fbug\">\nPlease report it here.</a></sup>","metadata":{}}]}