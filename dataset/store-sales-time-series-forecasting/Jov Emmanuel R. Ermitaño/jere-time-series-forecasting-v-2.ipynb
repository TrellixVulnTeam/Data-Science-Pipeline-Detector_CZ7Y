{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nimport seaborn as sns\nsns.set_style('ticks')\nimport plotly.express as px\nfrom statsmodels.tsa.deterministic import DeterministicProcess, CalendarFourier\n# Deterministic Process is used to construct basic configurations such as a constant, a time trend of any order, and either a seasonal or a Fourier transformation.\n# The Fourier transformation separates noise from the data. It can be used to separate the noise in a Calendar data.\nfrom statsmodels.graphics.tsaplots import plot_pacf\n# To plot the Partial Autocorrelation Function (PACF). I want to plot PACF to be specific with the regression model of Time Series Data.\nfrom statsmodels.tsa.stattools import adfuller\n# The Augmented Dickey-Fuller test (adfuller) is the statistical test used to determine whether a Time Series Data is Stationary, by looking at the trend of the data.\nfrom matplotlib import pyplot as plt, style\nstyle.use('seaborn-ticks')\nfrom tqdm import tqdm\n# tqdm is a library in Python which is used for creating Progress Meters/Bars.\n# Progress Meters/Bars are used as a visual cue that is a reliable estimate of the execution time of my code.\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.compose import TransformedTargetRegressor, ColumnTransformer\n# The TransformedTargetRegressor is useful for applying non-linear transformation to the Target in Regression.\n# I decided to import ColumnTransformer because I'll be doing different transformations on different columns, and I want to avoid transforming each column separately then stitching them together; Column Transformer will do this work for me.\nfrom sklearn.preprocessing import PowerTransformer\n# PowerTransformer is used to make Features more Gaussian-like, which is useful for Heteroscedasticity (non-constant variance) issues.\n# In other words, PowerTransformer changes the Distribution of the Features into more Gaussian-like to be able to perform Regression.\nfrom sklearn.pipeline import make_pipeline\n# To create a Pipeline that would do the needed transforms on the Numerical and Categorical Columns and combine several other steps that can be cross-validated together, while keeping the original dataset.\nfrom sklearn.linear_model import Ridge, ARDRegression\n# Ridge adds a tolerable Bias in exchange for the significant drop in Variance of the Model to prevent an overfitted Regression Line.\n# Automatic Relevance Determination (ARD) Regression has a Gaussian Distribution that is narrower with higher peak because more of its Coefficients are 0. This means that the Features are more stable as there is less varaince\nfrom sklearn.neighbors import KNeighborsRegressor\n# I imported KNeighborsRegressor to predict continuous values by taking the Mean of the nearest kneighbor.\nfrom sklearn.svm import SVR\n# Support Vector Regression acknowledges the presence of non-linearity in the data that is used to predict discrete values by finding the best fit line through Support Vectors.\nfrom sklearn.ensemble import ExtraTreesRegressor, GradientBoostingRegressor\n# ExtraTreesRegressor implements a meta-estimator that fits randomized decision trees (extra trees) on various subsamples of the dataset then uses averages to improve accuracy and control over-fitting.\n# GradientBoostingRegressor is used for tabular datasets to create a new model that can predict the Errors of prior models. It then adds the weaker models together and makes a final prediction with the least loss when adding new models.\nfrom sklearn.ensemble import BaggingRegressor, VotingRegressor\n# BaggingRegressor implements a meta-estimator that fits base regressors each on random subsets of the original dataset and then aggregate their individual predictions (either by voting or by averaging) to form a final prediction and reduce the variance by introducing randomization into its construction procedure, and then making an ensemble out of it.\n# In contrast, VotingRegressor A voting regressor implements a meta-estimator that fits several base regressors, each on the whole dataset, then it averages the individual predictions to form a final prediction.\n%pip install -Uq upgini\nfrom upgini import FeaturesEnricher, SearchKey, ModelTaskType\nfrom upgini.metadata import RuntimeParameters, CVType\n# Upgini is a simple feature search & enrichment library in Python. With Upgini, I spend less time for external data search and feature engineering, which it will do automatically.\n# Feature Enrichment means combining existing Features in the dataset with Features received from external sources, a popular way to augment existing datasets and improve the quality of the conclusions.\n# SearchKey is used to define search keys that are used to initiate search with the Upgini library. Search Keys are a set of columns that join external data sources and features.\n# ModelTaskType is declared to use the appropriate task type for the Model.\n# RuntimeParameters is imported to use optional parameters.\n# CVType is the Cross-Validation Type parameter for Features Enricher.\nimport gc\ngc.enable()\n# To enable automatic garbage collection.\n# Garbage collection helps in memory management by using the “stop-the-world” process where all execution stops while the garbage collector looks for and deletes objects to be collected.\n\nimport warnings\nfrom warnings import filterwarnings, simplefilter\nfilterwarnings('ignore')\n# filterwarnings is a function that, when used with (’ignore’), does not display the warning message.\nsimplefilter('ignore')\n# simplefilter is a function that, when used with (’ignore’), allows me to use known-deprecated code without having to see the warning.\n# But simplefilter does not suppress the warning for other code that might not be aware of its use of deprecated code.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-06-29T08:16:30.30949Z","iopub.execute_input":"2022-06-29T08:16:30.310166Z","iopub.status.idle":"2022-06-29T08:16:51.668548Z","shell.execute_reply.started":"2022-06-29T08:16:30.310034Z","shell.execute_reply":"2022-06-29T08:16:51.667579Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('../input/store-sales-time-series-forecasting/train.csv',\n                    parse_dates = ['date'], infer_datetime_format = True,\n                    dtype = {'store_nbr' : 'category',\n                             'family' : 'category'},\n                    usecols = ['date', 'store_nbr', 'family', 'sales'])\ntrain['date'] = train.date.dt.to_period('D')\ntrain = train.set_index(['date', 'store_nbr', 'family']).sort_index()\nprint(train.shape)\ntrain.head(15)","metadata":{"execution":{"iopub.status.busy":"2022-06-29T08:16:51.670394Z","iopub.execute_input":"2022-06-29T08:16:51.67098Z","iopub.status.idle":"2022-06-29T08:16:56.438057Z","shell.execute_reply.started":"2022-06-29T08:16:51.67094Z","shell.execute_reply":"2022-06-29T08:16:56.437088Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = pd.read_csv('../input/store-sales-time-series-forecasting/test.csv',\n                   parse_dates = ['date'], infer_datetime_format = True)\ntest['date'] = test.date.dt.to_period('D')\ntest = test.set_index(['date', 'store_nbr', 'family']).sort_values('id')\nprint(test.shape)\ntest.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-29T08:16:56.441154Z","iopub.execute_input":"2022-06-29T08:16:56.441509Z","iopub.status.idle":"2022-06-29T08:16:56.504144Z","shell.execute_reply.started":"2022-06-29T08:16:56.441479Z","shell.execute_reply":"2022-06-29T08:16:56.503117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature Engineering","metadata":{}},{"cell_type":"code","source":"calendar = pd.DataFrame(index = pd.date_range('2013-01-01', '2017-08-31')).to_period('D')\noil = pd.read_csv('../input/store-sales-time-series-forecasting/oil.csv',\n                  parse_dates = ['date'], infer_datetime_format = True,\n                  index_col = 'date').to_period('D')\noil['avg_oil'] = oil['dcoilwtico'].rolling(7).mean()\ncalendar = calendar.join(oil.avg_oil)\ncalendar['avg_oil'].fillna(method = 'ffill', inplace = True)\ncalendar.dropna(inplace = True)\n# I concatenated calendar with oil price and replaced NaN with the last valid observed price.","metadata":{"execution":{"iopub.status.busy":"2022-06-29T08:16:56.50638Z","iopub.execute_input":"2022-06-29T08:16:56.506733Z","iopub.status.idle":"2022-06-29T08:16:56.538603Z","shell.execute_reply.started":"2022-06-29T08:16:56.506702Z","shell.execute_reply":"2022-06-29T08:16:56.53759Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"_ = plot_pacf(calendar.avg_oil, lags = 10)\n# I calculated partial autocorrelation to select proper lags for oil price features.\n# Lags are the number of intervals between two autocorrelated observations. They are used to compute the Autocorrelation among Features to observe the seasonality of the data.\n# The Partial Autocorrelation Function (PACF) is the correlation between the Target and a determined Feature, taking into account how both that Target and the determined Feature are related to other Features.","metadata":{"execution":{"iopub.status.busy":"2022-06-29T08:16:56.539927Z","iopub.execute_input":"2022-06-29T08:16:56.540281Z","iopub.status.idle":"2022-06-29T08:16:56.784247Z","shell.execute_reply.started":"2022-06-29T08:16:56.540251Z","shell.execute_reply":"2022-06-29T08:16:56.783123Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_lags = 3\n# I chose 3 as the oil price lags because it is nearest to zero after lags 1 and 2 - considered as \"white noise.\"\nfor l in range(1, n_lags + 1):\n    calendar[f'oil_lags{l}'] = calendar.avg_oil.shift(l)\ncalendar.dropna(inplace = True)","metadata":{"execution":{"iopub.status.busy":"2022-06-29T08:16:56.785494Z","iopub.execute_input":"2022-06-29T08:16:56.785893Z","iopub.status.idle":"2022-06-29T08:16:56.798423Z","shell.execute_reply.started":"2022-06-29T08:16:56.785826Z","shell.execute_reply":"2022-06-29T08:16:56.797306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hol = pd.read_csv('../input/store-sales-time-series-forecasting/holidays_events.csv',\n                  parse_dates = ['date'], infer_datetime_format = True,\n                  index_col = 'date').to_period('D')\nhol = hol[hol.locale == 'National']  \nhol = hol.groupby(hol.index).first()\nprint(hol.shape)\nhol.head(30)\n# This is the data set containing the Holidays in Ecuador. I included this to know the spending behavior on National holidays.\n# I only included National Holidays to prevent false positives (There might be non-National Holidays that weren't transferred but were mistakenly labeled as transferred).\n# I then parsed the dates, and removed duplicate holidays.","metadata":{"execution":{"iopub.status.busy":"2022-06-29T08:16:56.799906Z","iopub.execute_input":"2022-06-29T08:16:56.80028Z","iopub.status.idle":"2022-06-29T08:16:56.846073Z","shell.execute_reply.started":"2022-06-29T08:16:56.80025Z","shell.execute_reply":"2022-06-29T08:16:56.845059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"calendar = calendar.join(hol)\n# Here, I joined the calendar and holiday datasets.\ncalendar['dofw'] = calendar.index.dayofweek\n# This outputs the ordinal values of the day of the week, starting at 0 for Monday and ending at 6 for Sunday.\ncalendar['wd'] = 1\n# I added a column for weekdays, represented by value 1.\ncalendar.loc[calendar.dofw > 4, 'wd'] = 0\n# I added 0 as the representation for weekends.\ncalendar.loc[calendar.type == 'Work Day', 'wd'] = 1.\n# A workday event is also represented by value 1 since it's on a weekday.\ncalendar.loc[calendar.type == 'Transfer', 'wd'] = 0\n# 'Transfer' contains the dates on which the holidays that were transferred, meaning it's not a work day.\n# Since there is no work and a 'Transfer' day is essentially a holiday, I decided to have it represented as 0.\ncalendar.loc[calendar.type == 'Bridge', 'wd'] = 0\n# 'Bridge' days are extra days added to a Holiday, so I used 0 to represent it as well.\ncalendar.loc[(calendar.type == 'Holiday') & (calendar.transferred == False), 'wd'] = 0\n# If the Holiday is not transferred, then that means it is not a work day. Hence, it is represented by 0.\ncalendar.loc[(calendar.type == 'Holiday') & (calendar.transferred == True), 'wd'] = 1\n# In this case however, since Holiday has been tagged as transferred, these Holidays are treated as normal days on which there is work - hence the representation of value 1.\n\ncalendar = pd.get_dummies(calendar, columns = ['dofw'], drop_first = True)\ncalendar = pd.get_dummies(calendar, columns = ['type'])\n# I used One-hot Encoding to create Dummies for Nominal Data and transform them into Binary (0s and 1s) so that these values can be understood by the algorithm.\ncalendar.drop(['locale', 'locale_name', 'description', 'transferred'], axis = 1, inplace = True)\n# I dropped these unused columns as they have not gained any relevance in my analysis.\n\nprint(calendar.shape)\ncalendar.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-29T08:16:56.847895Z","iopub.execute_input":"2022-06-29T08:16:56.848624Z","iopub.status.idle":"2022-06-29T08:16:56.892662Z","shell.execute_reply.started":"2022-06-29T08:16:56.848581Z","shell.execute_reply":"2022-06-29T08:16:56.891817Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"calendar['wageday']=0\ncalendar.loc[(calendar.index.to_timestamp().is_month_end) | (calendar.index.day == 15), 'wageday'] = 1\n# Since Wages are paid every two weeks on the 15th and on the last day of the month, I engineered a feature in which 1 represents the 15th and the last days of the months on which the customers received their wages, and 0 which represents other days.\n# This is a time dependent feature.","metadata":{"execution":{"iopub.status.busy":"2022-06-29T08:16:56.894087Z","iopub.execute_input":"2022-06-29T08:16:56.894417Z","iopub.status.idle":"2022-06-29T08:16:56.904994Z","shell.execute_reply.started":"2022-06-29T08:16:56.894388Z","shell.execute_reply":"2022-06-29T08:16:56.904065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ax = plt.subplots(1,1,figsize = (20,4))\ntrain.loc[\"2016-07-01\":].filter(like = 'SCHOOL AND OFFICE SUPPLIES', axis=0).groupby([\"date\"]).sales.sum().plot(title = \"SCHOOL AND OFFICE SUPPLIES\")\nplt.show()\n# Here, I plotted School and Office Supplies family because I figured this is a time series data with regular trend at the start of the School Season.\n# This plot shows the peak sales of school and office supplies, which coincides with the school seasons: April and May, than August and September.","metadata":{"execution":{"iopub.status.busy":"2022-06-29T08:16:56.908139Z","iopub.execute_input":"2022-06-29T08:16:56.909021Z","iopub.status.idle":"2022-06-29T08:17:04.322926Z","shell.execute_reply.started":"2022-06-29T08:16:56.908985Z","shell.execute_reply":"2022-06-29T08:17:04.322176Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"school_season = []\nfor i, r in calendar.iterrows():\n    if i.month in [4, 5, 8, 9] :\n        school_season.append(1)\n    else :\n        school_season.append(0)\ncalendar['school_season'] = school_season\n# During the school season months, I placed 1 in the column school season to indicate the spike, otherwise 0. \n# Based on the plot above, I created variable 'school_season' in the dataset in order to represent the months of the School Season as 1 and the rest as 0.","metadata":{"execution":{"iopub.status.busy":"2022-06-29T08:17:04.324169Z","iopub.execute_input":"2022-06-29T08:17:04.3247Z","iopub.status.idle":"2022-06-29T08:17:04.418997Z","shell.execute_reply.started":"2022-06-29T08:17:04.324667Z","shell.execute_reply":"2022-06-29T08:17:04.418003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Zero Forecasting","metadata":{}},{"cell_type":"markdown","source":"> ","metadata":{}},{"cell_type":"code","source":"c = train.groupby([\"store_nbr\",\"family\"]).tail(15).groupby([\"store_nbr\",\"family\"]).sales.sum().reset_index()\n# Can I change this to 14? What will be the implication?\nc = c[c.sales == 0].drop(\"sales\",axis = 1)\nc = c[c.family != \"SCHOOL AND OFFICE SUPPLIES\"]\nc.shape\n# It is a common practice of grocery stores that some of their store branches don't sell specific products or stop selling some products after some time. \n# So I decided to do a zero forecast on products that did not sell for 14 consecutive days, except for School and Office Supplies products that sells on a seasonal basis.","metadata":{"execution":{"iopub.status.busy":"2022-06-29T08:17:04.420489Z","iopub.execute_input":"2022-06-29T08:17:04.420956Z","iopub.status.idle":"2022-06-29T08:17:04.755172Z","shell.execute_reply.started":"2022-06-29T08:17:04.420913Z","shell.execute_reply":"2022-06-29T08:17:04.754161Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Shape of train before zero forecasting:\", train.shape)\nouter_join = train.reset_index().merge(c, how = 'outer', indicator = True)\ntrain = outer_join[~(outer_join._merge == 'both')].drop('_merge', axis = 1)\ntrain = train.set_index(['date', 'store_nbr', 'family']).sort_index()\ndel outer_join\ngc.collect()\nprint(\"Shape of train after zero forecasting:\", train.shape)\n# Here I compared the shape of the Training Dataset before and after removing zero sales through Zero Forecasting. The reduction in the number of values is easily noticeable.\n# I will save the products with zero sales into a separate dataframe, then append it to the Test Set during the submission phase of this Notebook.","metadata":{"execution":{"iopub.status.busy":"2022-06-29T08:17:04.756826Z","iopub.execute_input":"2022-06-29T08:17:04.757765Z","iopub.status.idle":"2022-06-29T08:17:07.626775Z","shell.execute_reply.started":"2022-06-29T08:17:04.757713Z","shell.execute_reply":"2022-06-29T08:17:07.625601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"zero_prediction = []\nfor i in range(0, len(c)):\n    zero_prediction.append(\n        pd.DataFrame({\n            \"date\":pd.date_range(\"2017-08-16\", \"2017-08-31\").tolist(),\n            \"store_nbr\":c.store_nbr.iloc[i],\n            \"family\":c.family.iloc[i],\n            \"sales\":0\n        })\n    )\nzero_prediction = pd.concat(zero_prediction)\nzero_prediction['date'] = zero_prediction.date.dt.to_period('D')\ndel c\ngc.collect()\nzero_prediction = zero_prediction.set_index(['date', 'store_nbr', 'family'])\nzero_prediction.head()\n# Here I saved the products with zero sales into a separate dataframe called 'zero_prediction'. I will use this dataframe later in this submission phase.","metadata":{"execution":{"iopub.status.busy":"2022-06-29T08:17:07.62823Z","iopub.execute_input":"2022-06-29T08:17:07.628799Z","iopub.status.idle":"2022-06-29T08:17:07.978232Z","shell.execute_reply.started":"2022-06-29T08:17:07.62875Z","shell.execute_reply":"2022-06-29T08:17:07.976929Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"a = train.groupby([\"date\",\"store_nbr\"]).sum().reset_index()\na = a[a[\"sales\"] > 0].groupby(\"store_nbr\")[[\"date\"]].min().sort_values(by=\"date\",ascending = False).head(5)\na.rename(columns = {'date':'open_date'}, inplace = True)\na\n# I want to have the data of all stores, so here I check the opening dates of all stores to be sure that I start my analysis on the date all stores are in operation.\n# Since the latest store opened was 20 April 2017, I will start my forecasting on 21 April 2017.","metadata":{"execution":{"iopub.status.busy":"2022-06-29T08:17:07.979702Z","iopub.execute_input":"2022-06-29T08:17:07.980102Z","iopub.status.idle":"2022-06-29T08:17:08.205904Z","shell.execute_reply.started":"2022-06-29T08:17:07.980069Z","shell.execute_reply":"2022-06-29T08:17:08.204696Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training","metadata":{}},{"cell_type":"code","source":"y = train.unstack(['store_nbr', 'family']).loc[\"2017-04-21\":]\n# Here I unstacked the columns 'store_nbr' and 'family' and located 21 April 2017 to prepare my data for training.\nfourier = CalendarFourier(freq = 'W', order = 3)\n# The Fourier transform separates noise from the data. CalendarFourier can be used to separate the noise in a Calendar data.\n# I set the parameter frequency to Week and Order to 3 because...\n# What are the parameters of this?\n\ndp = DeterministicProcess(index = y.index,\n                          order = 1,\n                          seasonal = False,\n                          constant = False,\n                          additional_terms = [fourier],\n                          drop = True)\n# Deterministic Process is used to construct basic configurations such as a constant, a time trend of any order, and either a seasonal or a Fourier component.\nx = dp.in_sample()\n# The .in_sample() returns the full set of values that match the index required by the Deterministic Process.\nx = x.join(calendar)\nx.index.name = \"date\"\n\n\nxtest = dp.out_of_sample(steps = 16)\n# I used out_of_sample because the Test Data will have a prediction for the next 16 days from 15.08 till 31.08.\nxtest = xtest.join(calendar)\nxtest.index.name = \"date\"\n\ndel hol\ndel calendar\ndel dp\ndel oil\n_ = gc.collect()\n# Invoking the garbage collector manually during the execution of a program is a good idea to handle memory being consumed by reference cycles.","metadata":{"execution":{"iopub.status.busy":"2022-06-29T08:17:08.207457Z","iopub.execute_input":"2022-06-29T08:17:08.207821Z","iopub.status.idle":"2022-06-29T08:17:08.980362Z","shell.execute_reply.started":"2022-06-29T08:17:08.20779Z","shell.execute_reply":"2022-06-29T08:17:08.979244Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"a = train[\"2017-04-21\":].reset_index()\na[\"ind\"] = 1\na[\"ind\"] = a.groupby(\"family\").ind.cumsum().values\na = pd.pivot(a, index = \"ind\", columns = \"family\", values = \"sales\").corr(method=\"spearman\")\nmask = np.triu(a.corr(method=\"spearman\"))\n# I used Spearman Rank Correlation calculate for correlations of the continuous ordinal data that follow a monotonic relationship in this dataset.\nplt.figure(figsize=(20, 20))\nsns.heatmap(a,\n        annot=True,\n        fmt='.1f',\n        cmap='YlGnBu',\n        square=True,\n        mask=mask,\n            # Here I used a mask to reduce redundancy in the Heatmap and add symmtery to the plot.\n        linewidths=1,\n            # 'linewidth' is an optional float parameter that draws the width of the lines that will divide each cell and add lines between each cell.\n        cbar=False)\n\nplt.title(\"Sales Correlations for Product Families\",fontsize = 24)\nplt.show()\n# I created this Heatmap to analyze the Sales Correlations for all 33 Products in the Dataset.\n# It is apparent in this Heatmap that the Products Baby Care, Books, Home Appliances, Ladieswear, Lawn and Garden, and Pet Supplies aren't correlated with other products at all.\n# There are other Products that have very low correlation with each other. Nonetheless, since the rest of the products are correlated, this Heatmap shows that this is a Multivariate Time Series problem.","metadata":{"execution":{"iopub.status.busy":"2022-06-29T08:17:08.981963Z","iopub.execute_input":"2022-06-29T08:17:08.982495Z","iopub.status.idle":"2022-06-29T08:17:11.636184Z","shell.execute_reply.started":"2022-06-29T08:17:08.982448Z","shell.execute_reply":"2022-06-29T08:17:11.635401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(1,2,figsize = (20,4))\ntrain.loc[\"2016-08-01\":].filter(like = 'BABY CARE', axis=0).groupby([\"date\"]).sales.sum().plot(ax = ax[0], title = \"BABY CARE\")\ntrain.loc[\"2016-08-01\":].filter(like = 'BOOKS', axis=0).groupby([\"date\"]).sales.sum().plot(ax = ax[1], title = \"BOOKS\")\nplt.show()\nfig, ax = plt.subplots(1,2,figsize = (20,4))\ntrain.loc[\"2016-01-01\":].filter(like = 'HOME APPLIANCES', axis=0).groupby([\"date\"]).sales.sum().plot(ax = ax[0], title = \"HOME APPLIANCES\")\ntrain.loc[\"2016-08-01\":].filter(like = 'LADIESWEAR', axis=0).groupby([\"date\"]).sales.sum().plot(ax = ax[1], title = \"LADIESWEAR\")\nplt.show()\nfig, ax = plt.subplots(1,2,figsize = (20,4))\ntrain.loc[\"2016-08-01\":].filter(like = 'LAWN AND GARDEN', axis=0).groupby([\"date\"]).sales.sum().plot(ax = ax[0], title = \"LAWN AND GARDEN\")\ntrain.loc[\"2016-08-01\":].filter(like = 'PET SUPPLIES', axis=0).groupby([\"date\"]).sales.sum().plot(ax = ax[1], title = \"PET SUPPLIES\")\nplt.show()\nfig, ax = plt.subplots(1,2,figsize = (20,4))\ntrain.loc[\"2016-08-01\":].filter(like = 'SCHOOL AND OFFICE SUPPLIES', axis=0).groupby([\"date\"]).sales.sum().plot(ax = ax[0], title = \"SCHOOL AND OFFICE SUPPLIES\")\ntrain.loc[\"2016-08-01\":].filter(like = 'LINGERIE', axis=0).groupby([\"date\"]).sales.sum().plot(ax = ax[1], title = \"LINGERIE\")\nplt.show()\nfig, ax = plt.subplots(1,2,figsize = (20,4))\ntrain.loc[\"2016-08-01\":].filter(like = 'GROCERY II', axis=0).groupby([\"date\"]).sales.sum().plot(ax = ax[0], title = \"GROCERY II\")\ntrain.loc[\"2016-08-01\":].filter(like = 'HARDWARE', axis=0).groupby([\"date\"]).sales.sum().plot(ax = ax[1], title = \"HARDWARE\")\nplt.show()\nfig, ax = plt.subplots(1,2,figsize = (20,4))\ntrain.loc[\"2016-08-01\":].filter(like = 'LIQUOR,WINE,BEER', axis=0).groupby([\"date\"]).sales.sum().plot(ax = ax[0], title = \"LIQUOR,WINE,BEER\")\ntrain.loc[\"2016-08-01\":].filter(like = 'MAGAZINES', axis=0).groupby([\"date\"]).sales.sum().plot(ax = ax[1], title = \"MAGAZINES\")\nplt.show()\n# Here, I plotted the Sales of products with that aren't correlated with other products at all: Baby Care, Books, Home Appliances, Books, Ladieswear, Lawn and Garden, Pet Supplies.\n# I also plotted the Sales of School and Office Supplies products and the products with very low correlation with it: Lingerie, Grocery II, Hardware, Liquor, Wine, & Beer, and Magazines.\n# I did this to check for Trends, Seasonality, and Anomalies for these low-correlated Products.\n# The plos of Pet Supplies, Hardware, Ladiesware, Grocery II, Lingerie products show a monotonic relationship.\n# The plot of School and Office Supplies products stands out as it looks like a Non-stationary Time Series (TS) Data. I need to verify this with the Augmented Dickey-Fuller Test.\n# The plot of Home Appliances products also appears to be a Non-stationary TS, which must be verified using the Augmented Dickey-Fuller Test.\n# The plot of Books products shows that Sales started sometime in October 2016, and has sharply declined since. The near-zero daily sales for this product might be a result of assortiment decline.\n# The plot of Lawn and Garden products shows an uptick in Sales starting in December 2016, which may be due to the introduction of these products in more store branches.\n# The plot of Liquor, Wine, Beer products shows a significant uptick in the last weeks of December 2016, and peaking therein. On the first week of January 2017, its sales significant dropped, which may be because of the demand of these products during the Holiday season.\n# After running the Augmented Dickey-Fuller Test below, I discovered that the sales of products Books, Baby Care, and Lawn and Garden are Stationary, while the sales of the rest are not.","metadata":{"execution":{"iopub.status.busy":"2022-06-29T08:17:11.637376Z","iopub.execute_input":"2022-06-29T08:17:11.637818Z","iopub.status.idle":"2022-06-29T08:18:28.74604Z","shell.execute_reply.started":"2022-06-29T08:17:11.637781Z","shell.execute_reply":"2022-06-29T08:18:28.744993Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result = adfuller(\n    np.log1p(\n        y.loc[\"2016-08-01\":, y.columns.get_level_values(\"family\").isin([\"BABY CARE\"])].mean(axis=\"columns\")\n    )\n)\n# Here I used the Augmented Dickey-Fuller (ADF) Test to check for Stationarity of the variables.\nprint('ADF Statistic: %f' % result[0])\n# The ADF Statistic is used to know whether to reject the Null (Time Series Data is Non-Stationary). The Null is rejected if it falls within the ADF Critical Values.\n# If p-value is > 0: Non-stationary. If p-value =0: Stationary.\nprint('p-value: %f' % result[1])\nprint('Critical Values:')\nfor key, value in result[4].items():\n    print('\\t%s: %.3f' % (key, value))\n\nif result[0] < result[4][\"1%\"]:\n    print (\"Reject Ho - Time Series is Stationary\")\nelse:\n    print (\"Failed to Reject Ho - Time Series is Non-Stationary\")","metadata":{"execution":{"iopub.status.busy":"2022-06-29T08:18:28.747402Z","iopub.execute_input":"2022-06-29T08:18:28.74818Z","iopub.status.idle":"2022-06-29T08:18:28.771303Z","shell.execute_reply.started":"2022-06-29T08:18:28.74814Z","shell.execute_reply":"2022-06-29T08:18:28.770164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result = adfuller(\n    np.log1p(\n        y.loc[\"2016-08-01\":, y.columns.get_level_values(\"family\").isin([\"BOOKS\"])].mean(axis=\"columns\")\n    )\n)\n# Here I used the Augmented Dickey-Fuller (ADF) Test to check for Stationarity of the variables.\nprint('ADF Statistic: %f' % result[0])\n# The ADF Statistic is used to know whether to reject the Null (Time Series Data is Non-Stationary). The Null is rejected if it falls within the ADF Critical Values.\n# If p-value is > 0: Non-stationary. If p-value =0: Stationary.\nprint('p-value: %f' % result[1])\nprint('Critical Values:')\nfor key, value in result[4].items():\n    print('\\t%s: %.3f' % (key, value))\n\nif result[0] < result[4][\"1%\"]:\n    print (\"Reject Ho - Time Series is Stationary\")\nelse:\n    print (\"Failed to Reject Ho - Time Series is Non-Stationary\")","metadata":{"execution":{"iopub.status.busy":"2022-06-29T08:18:28.772574Z","iopub.execute_input":"2022-06-29T08:18:28.773393Z","iopub.status.idle":"2022-06-29T08:18:28.794176Z","shell.execute_reply.started":"2022-06-29T08:18:28.773358Z","shell.execute_reply":"2022-06-29T08:18:28.793028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result = adfuller(\n    np.log1p(\n        y.loc[\"2016-08-01\":, y.columns.get_level_values(\"family\").isin([\"HOME APPLIANCES\"])].mean(axis=\"columns\")\n    )\n)\n# Here I used the Augmented Dickey-Fuller (ADF) Test to check for Stationarity of the variables.\nprint('ADF Statistic: %f' % result[0])\n# The ADF Statistic is used to know whether to reject the Null (Time Series Data is Non-Stationary). The Null is rejected if it falls within the ADF Critical Values.\n# If p-value is > 0: Non-stationary. If p-value =0: Stationary.\nprint('p-value: %f' % result[1])\nprint('Critical Values:')\nfor key, value in result[4].items():\n    print('\\t%s: %.3f' % (key, value))\n\nif result[0] < result[4][\"1%\"]:\n    print (\"Reject Ho - Time Series is Stationary\")\nelse:\n    print (\"Failed to Reject Ho - Time Series is Non-Stationary\")","metadata":{"execution":{"iopub.status.busy":"2022-06-29T08:18:28.795536Z","iopub.execute_input":"2022-06-29T08:18:28.796094Z","iopub.status.idle":"2022-06-29T08:18:28.815142Z","shell.execute_reply.started":"2022-06-29T08:18:28.796059Z","shell.execute_reply":"2022-06-29T08:18:28.814315Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result = adfuller(\n    np.log1p(\n        y.loc[\"2016-08-01\":, y.columns.get_level_values(\"family\").isin([\"LADIESWEAR\"])].mean(axis=\"columns\")\n    )\n)\n# Here I used the Augmented Dickey-Fuller (ADF) Test to check for Stationarity of the variables.\nprint('ADF Statistic: %f' % result[0])\n# The ADF Statistic is used to know whether to reject the Null (Time Series Data is Non-Stationary). The Null is rejected if it falls within the ADF Critical Values.\n# If p-value is > 0: Non-stationary. If p-value =0: Stationary.\nprint('p-value: %f' % result[1])\nprint('Critical Values:')\nfor key, value in result[4].items():\n    print('\\t%s: %.3f' % (key, value))\n\nif result[0] < result[4][\"1%\"]:\n    print (\"Reject Ho - Time Series is Stationary\")\nelse:\n    print (\"Failed to Reject Ho - Time Series is Non-Stationary\")","metadata":{"execution":{"iopub.status.busy":"2022-06-29T08:18:28.816547Z","iopub.execute_input":"2022-06-29T08:18:28.817122Z","iopub.status.idle":"2022-06-29T08:18:28.837447Z","shell.execute_reply.started":"2022-06-29T08:18:28.817089Z","shell.execute_reply":"2022-06-29T08:18:28.836752Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result = adfuller(\n    np.log1p(\n        y.loc[\"2016-08-01\":, y.columns.get_level_values(\"family\").isin([\"LAWN AND GARDEN\"])].mean(axis=\"columns\")\n    )\n)\n# Here I used the Augmented Dickey-Fuller (ADF) Test to check for Stationarity of the variables.\nprint('ADF Statistic: %f' % result[0])\n# The ADF Statistic is used to know whether to reject the Null (Time Series Data is Non-Stationary). The Null is rejected if it falls within the ADF Critical Values.\n# If p-value is > 0: Non-stationary. If p-value =0: Stationary.\nprint('p-value: %f' % result[1])\nprint('Critical Values:')\nfor key, value in result[4].items():\n    print('\\t%s: %.3f' % (key, value))\n\nif result[0] < result[4][\"1%\"]:\n    print (\"Reject Ho - Time Series is Stationary\")\nelse:\n    print (\"Failed to Reject Ho - Time Series is Non-Stationary\")","metadata":{"execution":{"iopub.status.busy":"2022-06-29T08:18:28.838744Z","iopub.execute_input":"2022-06-29T08:18:28.839311Z","iopub.status.idle":"2022-06-29T08:18:28.860235Z","shell.execute_reply.started":"2022-06-29T08:18:28.839277Z","shell.execute_reply":"2022-06-29T08:18:28.859218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result = adfuller(\n    np.log1p(\n        y.loc[\"2016-08-01\":, y.columns.get_level_values(\"family\").isin([\"SCHOOL AND OFFICE SUPPLIES\"])].mean(axis=\"columns\")\n    )\n)\n# Here I used the Augmented Dickey-Fuller (ADF) Test to check for Stationarity of the variables.\nprint('ADF Statistic: %f' % result[0])\n# The ADF Statistic is used to know whether to reject the Null (Time Series Data is Non-Stationary). The Null is rejected if it falls within the ADF Critical Values.\n# If p-value is > 0: Non-stationary. If p-value =0: Stationary.\nprint('p-value: %f' % result[1])\nprint('Critical Values:')\nfor key, value in result[4].items():\n    print('\\t%s: %.3f' % (key, value))\n\nif result[0] < result[4][\"1%\"]:\n    print (\"Reject Ho - Time Series is Stationary\")\nelse:\n    print (\"Failed to Reject Ho - Time Series is Non-Stationary\")","metadata":{"execution":{"iopub.status.busy":"2022-06-29T08:18:28.861941Z","iopub.execute_input":"2022-06-29T08:18:28.862325Z","iopub.status.idle":"2022-06-29T08:18:28.882771Z","shell.execute_reply.started":"2022-06-29T08:18:28.862295Z","shell.execute_reply":"2022-06-29T08:18:28.881948Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result = adfuller(\n    np.log1p(\n        y.loc[\"2016-08-01\":, y.columns.get_level_values(\"family\").isin([\"LINGERIE\"])].mean(axis=\"columns\")\n    )\n)\n# Here I used the Augmented Dickey-Fuller (ADF) Test to check for Stationarity of the variables.\nprint('ADF Statistic: %f' % result[0])\n# The ADF Statistic is used to know whether to reject the Null (Time Series Data is Non-Stationary). The Null is rejected if it falls within the ADF Critical Values.\n# If p-value is > 0: Non-stationary. If p-value =0: Stationary.\nprint('p-value: %f' % result[1])\nprint('Critical Values:')\nfor key, value in result[4].items():\n    print('\\t%s: %.3f' % (key, value))\n\nif result[0] < result[4][\"1%\"]:\n    print (\"Reject Ho - Time Series is Stationary\")\nelse:\n    print (\"Failed to Reject Ho - Time Series is Non-Stationary\")","metadata":{"execution":{"iopub.status.busy":"2022-06-29T08:18:28.883763Z","iopub.execute_input":"2022-06-29T08:18:28.884465Z","iopub.status.idle":"2022-06-29T08:18:28.904433Z","shell.execute_reply.started":"2022-06-29T08:18:28.88443Z","shell.execute_reply":"2022-06-29T08:18:28.90362Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result = adfuller(\n    np.log1p(\n        y.loc[\"2016-08-01\":, y.columns.get_level_values(\"family\").isin([\"GROCERY II\"])].mean(axis=\"columns\")\n    )\n)\n# Here I used the Augmented Dickey-Fuller (ADF) Test to check for Stationarity of the variables.\nprint('ADF Statistic: %f' % result[0])\n# The ADF Statistic is used to know whether to reject the Null (Time Series Data is Non-Stationary). The Null is rejected if it falls within the ADF Critical Values.\n# If p-value is > 0: Non-stationary. If p-value =0: Stationary.\nprint('p-value: %f' % result[1])\nprint('Critical Values:')\nfor key, value in result[4].items():\n    print('\\t%s: %.3f' % (key, value))\n\nif result[0] < result[4][\"1%\"]:\n    print (\"Reject Ho - Time Series is Stationary\")\nelse:\n    print (\"Failed to Reject Ho - Time Series is Non-Stationary\")","metadata":{"execution":{"iopub.status.busy":"2022-06-29T08:18:28.905622Z","iopub.execute_input":"2022-06-29T08:18:28.906128Z","iopub.status.idle":"2022-06-29T08:18:28.925675Z","shell.execute_reply.started":"2022-06-29T08:18:28.906095Z","shell.execute_reply":"2022-06-29T08:18:28.924889Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result = adfuller(\n    np.log1p(\n        y.loc[\"2016-08-01\":, y.columns.get_level_values(\"family\").isin([\"LIQUOR,WINE,BEER\"])].mean(axis=\"columns\")\n    )\n)\n# Here I used the Augmented Dickey-Fuller (ADF) Test to check for Stationarity of the variables.\nprint('ADF Statistic: %f' % result[0])\n# The ADF Statistic is used to know whether to reject the Null (Time Series Data is Non-Stationary). The Null is rejected if it falls within the ADF Critical Values.\n# If p-value is > 0: Non-stationary. If p-value =0: Stationary.\nprint('p-value: %f' % result[1])\nprint('Critical Values:')\nfor key, value in result[4].items():\n    print('\\t%s: %.3f' % (key, value))\n\nif result[0] < result[4][\"1%\"]:\n    print (\"Reject Ho - Time Series is Stationary\")\nelse:\n    print (\"Failed to Reject Ho - Time Series is Non-Stationary\")","metadata":{"execution":{"iopub.status.busy":"2022-06-29T08:18:28.927124Z","iopub.execute_input":"2022-06-29T08:18:28.927492Z","iopub.status.idle":"2022-06-29T08:18:28.950003Z","shell.execute_reply.started":"2022-06-29T08:18:28.927462Z","shell.execute_reply":"2022-06-29T08:18:28.94891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result = adfuller(\n    np.log1p(\n        y.loc[\"2016-08-01\":, y.columns.get_level_values(\"family\").isin([\"MAGAZINES\"])].mean(axis=\"columns\")\n    )\n)\n# Here I used the Augmented Dickey-Fuller (ADF) Test to check for Stationarity of the variables.\nprint('ADF Statistic: %f' % result[0])\n# The ADF Statistic is used to know whether to reject the Null (Time Series Data is Non-Stationary). The Null is rejected if it falls within the ADF Critical Values.\n# If p-value is > 0: Non-stationary. If p-value =0: Stationary.\nprint('p-value: %f' % result[1])\nprint('Critical Values:')\nfor key, value in result[4].items():\n    print('\\t%s: %.3f' % (key, value))\n\nif result[0] < result[4][\"1%\"]:\n    print (\"Reject Ho - Time Series is Stationary\")\nelse:\n    print (\"Failed to Reject Ho - Time Series is Non-Stationary\")","metadata":{"execution":{"iopub.status.busy":"2022-06-29T08:18:28.954118Z","iopub.execute_input":"2022-06-29T08:18:28.954489Z","iopub.status.idle":"2022-06-29T08:18:28.975739Z","shell.execute_reply.started":"2022-06-29T08:18:28.954456Z","shell.execute_reply":"2022-06-29T08:18:28.974461Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result = adfuller(\n    np.log1p(\n        y.loc[\"2016-08-01\":, y.columns.get_level_values(\"family\").isin([\"HARDWARE\"])].mean(axis=\"columns\")\n    )\n)\n# Here I used the Augmented Dickey-Fuller (ADF) Test to check for Stationarity of the variables.\nprint('ADF Statistic: %f' % result[0])\n# The ADF Statistic is used to know whether to reject the Null (Time Series Data is Non-Stationary). The Null is rejected if it falls within the ADF Critical Values.\n# If p-value is > 0: Non-stationary. If p-value =0: Stationary.\nprint('p-value: %f' % result[1])\nprint('Critical Values:')\nfor key, value in result[4].items():\n    print('\\t%s: %.3f' % (key, value))\n\nif result[0] < result[4][\"1%\"]:\n    print (\"Reject Ho - Time Series is Stationary\")\nelse:\n    print (\"Failed to Reject Ho - Time Series is Non-Stationary\")","metadata":{"execution":{"iopub.status.busy":"2022-06-29T08:18:28.976688Z","iopub.execute_input":"2022-06-29T08:18:28.977061Z","iopub.status.idle":"2022-06-29T08:18:28.998904Z","shell.execute_reply.started":"2022-06-29T08:18:28.97703Z","shell.execute_reply":"2022-06-29T08:18:28.998098Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"non_st_ts = [\"SCHOOL AND OFFICE SUPPLIES\",\"BOOKS\"]\n# For the non-stationary sales of products with a low multivariate correlation, Tree-based Regressors are used.\n# Here I use School and Office Supplies and Books products because only these non-stationary TS variables will improve the Tree-based Regressors versus baseline Ridge.\n# I used GradientBoostingRegressor + the ExtraTreesRegressor ensemble models here.","metadata":{"execution":{"iopub.status.busy":"2022-06-29T08:18:29.000095Z","iopub.execute_input":"2022-06-29T08:18:29.00072Z","iopub.status.idle":"2022-06-29T08:18:29.004675Z","shell.execute_reply.started":"2022-06-29T08:18:29.000688Z","shell.execute_reply":"2022-06-29T08:18:29.003964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"a = train.groupby(\"family\").sales.mean().sort_values(ascending = False).reset_index()\npx.bar(a, y = \"family\", x=\"sales\", color = \"family\", title = \"Top Selling Product Families\")\n# Here I wanted to know the top selling Products, which are inelastic and have a strong intra-week seasonality.\n# The inelasticity of the top selling Products is due to the fact that almost all of them are necessities.\n# I decided to use K-nearest Neighbors and Bayesian Regressors for the top 15 Products, and separate the bottom selling Products.\n# K-nearest Neighbors Regression predicts the actual numerical value by taking the mean of the nearest kneighbors, depending on the value of k. Deciding on the value of k is a trial and error process.","metadata":{"execution":{"iopub.status.busy":"2022-06-29T08:18:29.005964Z","iopub.execute_input":"2022-06-29T08:18:29.006634Z","iopub.status.idle":"2022-06-29T08:18:30.420515Z","shell.execute_reply.started":"2022-06-29T08:18:29.00659Z","shell.execute_reply":"2022-06-29T08:18:30.419576Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"low_sales_ts = [\"MAGAZINES\",\"LAWN AND GARDEN\",\"BABY CARE\",\n                \"CELEBRATION\",\"GROCERY II\",\"HARDWARE\",\"AUTOMOTIVE\",\n                \"HOME AND KITCHEN I\",\"HOME AND KITCHEN II\",\n                \"HOME APPLIANCES\",\"LINGERIE\",\n                \"LADIESWEAR\",\"PLAYERS AND ELECTRONICS\",\n                \"PET SUPPLIES\",\"BEAUTY\",\"PREPARED FOODS\",\n                \"HOME CARE\",\"CLEANING\"]","metadata":{"execution":{"iopub.status.busy":"2022-06-29T08:18:30.421813Z","iopub.execute_input":"2022-06-29T08:18:30.422273Z","iopub.status.idle":"2022-06-29T08:18:30.427725Z","shell.execute_reply.started":"2022-06-29T08:18:30.422242Z","shell.execute_reply":"2022-06-29T08:18:30.426741Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sdate = '2017-04-21'\nx=x.loc[sdate:]\ny=y.loc[sdate:]","metadata":{"execution":{"iopub.status.busy":"2022-06-29T08:18:30.429241Z","iopub.execute_input":"2022-06-29T08:18:30.429586Z","iopub.status.idle":"2022-06-29T08:18:30.446447Z","shell.execute_reply.started":"2022-06-29T08:18:30.429557Z","shell.execute_reply":"2022-06-29T08:18:30.445496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Upgini is a Python library used for a simple feature search & enrichment.\nenricher = FeaturesEnricher(\n    # Here I defined the search keys date and country to initiate the search of enriching features in the Upgini library.\n    # I want to use this low-code library to boost my accuracy while saving time on feature engineering. The external Features from Upgini enrich my Dataset's features.\n    search_keys={\n        \"date\": SearchKey.DATE\n    },\n    country_code = \"EC\",\n    # 'EC' stands for Ecuador\n    cv = CVType.time_series,\n)\n# Here I initiated a Cross Validation time series parameter to FeaturesEnricher.\n# Cross Validation lets me evaluate and increase the accuracy of my model (lower Bias) without increasing its variance, thereby finding the sweet spot in the Bias-Variance Tradeoff.","metadata":{"execution":{"iopub.status.busy":"2022-06-29T08:18:30.447782Z","iopub.execute_input":"2022-06-29T08:18:30.448136Z","iopub.status.idle":"2022-06-29T08:18:35.094601Z","shell.execute_reply.started":"2022-06-29T08:18:30.448106Z","shell.execute_reply":"2022-06-29T08:18:35.093318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"enriched_ts_map = {}\ny_fe_1 = y.loc[:, y.columns.get_level_values(\"family\").isin([\"LIQUOR,WINE,BEER\"])].mean(axis=\"columns\")\ny_fe_2 = y.loc[:, y.columns.get_level_values(\"family\").isin([\"CLEANING\"])].mean(axis=\"columns\")\n\n\ny_fe_1 = np.log1p(y_fe_1)\ny_fe_2 = np.log1p(y_fe_2)\n# I used the log transformation to reshape y distribution closer to Gaussian.\n# The np.log1p() is a mathematical numpy library function that helps calculate the natural logarithmic value of x+1, where x belongs to all the input array elements. The log1p() is reverse of exp(x) – 1.\n# Natural logarithms are preferred because they are directly interpretable as approximate proportional differences. ","metadata":{"execution":{"iopub.status.busy":"2022-06-29T08:18:35.096072Z","iopub.execute_input":"2022-06-29T08:18:35.09645Z","iopub.status.idle":"2022-06-29T08:18:35.108247Z","shell.execute_reply.started":"2022-06-29T08:18:35.096415Z","shell.execute_reply":"2022-06-29T08:18:35.107436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\nX_enriched = enricher.fit_transform(\n    x.copy().reset_index(), \n    y_fe_1.reset_index(drop=True).values,\n    # 'For LIQUOR, WINE, BEER' products\n    calculate_metrics=True,\n    keep_input=True,\n    max_features=3,\n).set_index(\"date\")\n# Here, I tried to do feature enrichment on the LIQUOR,WINE,BEER products.\n# The x new features might improve accuracy as ranked by the SHapley Additive exPlanataion (SHAP) value. \n# The SHAP Value is used to take into account the local contribution of a Feature, and how its contribution changes as the value of the Feature changes.\n# The higher the SHAP value, the higher the contribution of that feature to the prediction, and vice-versa.\n# However, SHAP value cannot evaluate the quality/accuracy of the prediction through the feature.","metadata":{"execution":{"iopub.status.busy":"2022-06-29T08:18:35.109982Z","iopub.execute_input":"2022-06-29T08:18:35.110793Z","iopub.status.idle":"2022-06-29T08:26:57.597092Z","shell.execute_reply.started":"2022-06-29T08:18:35.110746Z","shell.execute_reply":"2022-06-29T08:26:57.595991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"enriched_ts_map[\"LIQUOR,WINE,BEER\"] = list(set(X_enriched.columns) - set(x.columns))\n# Here I omitted the unenriched columns and retained the feature-enriched columns.","metadata":{"execution":{"iopub.status.busy":"2022-06-29T08:26:57.5998Z","iopub.execute_input":"2022-06-29T08:26:57.600357Z","iopub.status.idle":"2022-06-29T08:26:57.606179Z","shell.execute_reply.started":"2022-06-29T08:26:57.60032Z","shell.execute_reply":"2022-06-29T08:26:57.604787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test_enriched = enricher.transform(\n    xtest.copy().reset_index(),\n    keep_input=True,\n    max_features=3, \n).set_index(\"date\")\n# Here I enriched all the features in the training dataset.","metadata":{"execution":{"iopub.status.busy":"2022-06-29T08:26:57.607999Z","iopub.execute_input":"2022-06-29T08:26:57.608861Z","iopub.status.idle":"2022-06-29T08:30:52.996937Z","shell.execute_reply.started":"2022-06-29T08:26:57.608696Z","shell.execute_reply":"2022-06-29T08:30:52.995808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_enriched2 = enricher.fit_transform(\n    X_enriched.reset_index(), \n    y_fe_2.reset_index(drop=True).values,\n    # 'For CLEANING' products\n    calculate_metrics=True,\n    keep_input=True, \n    max_features=3,\n).set_index(\"date\")\n\nX_test_enriched2 = enricher.transform(\n    X_test_enriched.reset_index(),\n    keep_input=True,\n    max_features=3, \n).set_index(\"date\")\n# # Here, I tried to do feature enrichment on the CLEANING products using the same process I did on LIQUOR,WINE,BEER products.","metadata":{"execution":{"iopub.status.busy":"2022-06-29T08:30:52.99833Z","iopub.execute_input":"2022-06-29T08:30:52.99873Z","iopub.status.idle":"2022-06-29T08:40:06.02042Z","shell.execute_reply.started":"2022-06-29T08:30:52.998698Z","shell.execute_reply":"2022-06-29T08:40:06.019448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"enriched_ts_map[\"CLEANING\"] = list(set(X_enriched2.columns) - set(X_enriched.columns))\n# Here I omitted the unenriched columns and retained the feature-enriched columns.","metadata":{"execution":{"iopub.status.busy":"2022-06-29T08:40:06.021726Z","iopub.execute_input":"2022-06-29T08:40:06.022099Z","iopub.status.idle":"2022-06-29T08:40:06.026716Z","shell.execute_reply.started":"2022-06-29T08:40:06.022068Z","shell.execute_reply":"2022-06-29T08:40:06.025932Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Number of Features, Initial -> After Enrichment:\",x.shape[1],\"->\",X_enriched2.shape[1])\nint_features = set(x.columns.to_list())\next_features = [col for ext_features_ in enriched_ts_map.values() for col in ext_features_]\n\nx = X_enriched2\nxtest = X_test_enriched2\ndel X_enriched, X_enriched2\ndel X_test_enriched, X_test_enriched2\ndel y_fe_1\ndel y_fe_2\n_ = gc.collect()\n# To capture joint sales, I decided to use the Products for my sales prediction. This is because FMCG have strong correlation between product categories: when a person buys product X, it may follow that he buys product Y too.\n# And true enough, after checking the correlation of the products, I have found that most products are correlated (25 out of 33).","metadata":{"execution":{"iopub.status.busy":"2022-06-29T08:40:06.028038Z","iopub.execute_input":"2022-06-29T08:40:06.028377Z","iopub.status.idle":"2022-06-29T08:40:06.241423Z","shell.execute_reply.started":"2022-06-29T08:40:06.028339Z","shell.execute_reply":"2022-06-29T08:40:06.240153Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lnr_reg = TransformedTargetRegressor(\n    regressor = LinearRegression(fit_intercept = True, n_jobs = -1),\n    func=np.log1p,\n    inverse_func=np.expm1\n)\n# The TransformedTargetRegressor is useful for applying a non-linear transformation to the target y in regression problems. \n# In other words, it is used to scale and transform the Target to prepare it for Regression using sklearn.\nlnr = make_pipeline(\n    ColumnTransformer([(\"drop_f\", \"drop\", ext_features)], remainder=\"passthrough\"),\n    PowerTransformer(),\n    lnr_reg\n)\n# 'make_pipeline' is a utility function to construct pipelines. A pipeline takes in estimates and allows for automatically applying the transformers and the model to the ‘pipeline’ and fit the model, instead of doing all this manually.\n# The purpose of the pipeline is to combine several steps that can be cross-validated together while setting different parameters.\n# The pipeline also does needed transformations and keeps the original data.\n# The ColumnTransformer allows different columns or column subsets of the input to be transformed separately, and the features generated by each Transformer will be concatenated to form a single feature space. \n# ColumnTransformer is useful for heterogeneous or columnar data in order to combine several feature extraction mechanisms or transformations into a single transformer.\n# By specifying remainder='passthrough', all remaining columns that were not specified in transformers will be automatically passed through, then will be concatenated with the output of the transformers. \n\nlnr.fit(x, y)\nyfit_lnr = pd.DataFrame(lnr.predict(x), index = x.index, columns = y.columns).clip(0.)\nypred_lnr = pd.DataFrame(lnr.predict(xtest), index = xtest.index, columns = y.columns).clip(0.)\n\ny_ = y.stack(['store_nbr', 'family'])\ny_['lnr'] = yfit_lnr.stack(['store_nbr', 'family'])['sales']\n# I will then add the prediction of this Regression to the existing train and test data.","metadata":{"execution":{"iopub.status.busy":"2022-06-29T08:40:06.243451Z","iopub.execute_input":"2022-06-29T08:40:06.243977Z","iopub.status.idle":"2022-06-29T08:40:06.679728Z","shell.execute_reply.started":"2022-06-29T08:40:06.243931Z","shell.execute_reply":"2022-06-29T08:40:06.678907Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ylnr = yfit_lnr.append(ypred_lnr)\nx = x.join(ylnr)\nxtest = xtest.join(ylnr)\ndel yfit_lnr\ndel ypred_lnr\ndel ylnr\n_ = gc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-06-29T08:40:06.680796Z","iopub.execute_input":"2022-06-29T08:40:06.681686Z","iopub.status.idle":"2022-06-29T08:40:06.862592Z","shell.execute_reply.started":"2022-06-29T08:40:06.681649Z","shell.execute_reply":"2022-06-29T08:40:06.861719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SEED = 6192022\n# For reproducibility of results\n\nclass CustomRegressor():\n    \n    def __init__(self, ext_features = None, knn_features = None, non_st_ts = None, low_sales_ts = None, enriched_ts_map = None, n_jobs=-1):\n        # To initiate the creation of this Class.\n        # Why did I call self here?\n        self.n_jobs = n_jobs\n        self.ext_features = ext_features\n        self.knn_features = knn_features\n        self.non_st_ts = non_st_ts\n        self.low_sales_ts = low_sales_ts\n        self.enriched_ts_map = enriched_ts_map\n        self.estimators_ = None\n        self.product_names_ = None\n        self.store_names_ = None\n# This CustomRegressor fits the models on every component of the Multivariate TS - Single TS as the combination Family x Store.\n# There will be 1658 independent models.\n# There are 4 groups of models in this CustomRegressor. \n# 1st Group: GradientBoostingRegressor + ExtraTreesRegressor Voting Ensemble Model. This is used on selected non-stationary Time Series with low correlations.\n# 2nd Group: K-nearest Neighbors Regressor + Bayesian Regressor + Ridge + SVR Voting Ensemble Model. This is used on High Sales Products I defined earlier.\n# 3rd Group: Ridge + SVR Voting Ensemble. This is used on Products with Low Sales Products I also defined earlier.\n# 4th Group: Ridge. This is used on Products enriched with External Features.\n\n    def _estimator_(self, X, y):\n        warnings.simplefilter(action='ignore', category=FutureWarning)\n        # For base class warnings about deprecated features.\n        \n        # We remove external features for the products, which univariate TS were not used during feature search & enrichment\n        # These external features won't be relevant; including them would reduce the accuracy of my Model.\n        remove_ext_features = ColumnTransformer([(\"drop_f\", \"drop\", self.ext_features)],remainder=\"passthrough\")\n        \n        if y.name[2] in self.non_st_ts:\n        #The 1st Group\n            b1 = GradientBoostingRegressor(n_estimators = 2, max_depth=3, loss='huber', random_state=SEED)\n            # Gradient Boosting Regressor is used to predict a continuous value. \n            # It is an ensemble learning technique where new models are created that predict the Errors/Residuals of prior models, and then added together to make the final prediction. \n            # While adding the Models, the Errors/Residuals are minimized to get a better prediction. This is the result of using a Gradient Descent Algorithm, hence its name.\n            # Gradient Boosting Regressor adds the Models sequentially until no further significant improvements on the Accuracy can be made.\n            # In addition, it is powerful enough to find nonlinear relationships between the Target and Features.\n            # The parameter 'n_estimators' sets the number of boosting stages to perform (default=100). A large number of boosting stages usually results in better performance.\n            # The parameter 'max_depth' is optional. It sets maximum depth of the individual regression estimators, which limits the number of nodes in the tree (default=3).\n            # Tha parameter 'loss='huber'' sets the loss function to be optimized through huber - a  combination of least squares regression and least absolute regression that allows quantile regression (use alpha to specify the quantile).\n            # The loss function is used during model training to evaluate the performance of the Model in predicting the data. The lower the Loss Function, the better.\n            r1 = ExtraTreesRegressor(n_estimators = 250, n_jobs=self.n_jobs, random_state=SEED)\n            # The ExtraTreesRegressor is another meta-estimator that fits a number of randomized decision trees (a.k.a. extra-trees) on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.\n            # Extremely Randomized (ExtRa) Trees uses the entire input sample as a Tree, but may increase Variance and decrease Bias. Extra Trees also chooses the Node Splits randomly, then chooses the best split among all subsets of Features.\n            # The ExtRa Trees algorithm is faster than Random Forest and their results are practically the same.\n        \n        # The 2nd Group\n            b2 = BaggingRegressor(base_estimator=r1,\n                                  n_estimators=10,\n                                  n_jobs=self.n_jobs,\n                                  random_state=SEED)\n            # BaggingRegressor is a meta-estimator that fits base regressors each on random subsets of the original dataset and then aggregate their individual predictions (either by voting or by averaging) to form a final prediction.\n            # This meta-estimator reduces the variance by introducing randomization in its construction procedure.\n            model = make_pipeline(\n                remove_ext_features,\n                VotingRegressor ([('gbr', b1), ('et', b2)]))\n                # The VotingRegressor is a meta-estimator that fits several base regressors, each on the whole dataset. Then it averages the individual predictions to form a final prediction.\n                # This meta-estimator can be used with any collection of existing models and it treats all models the same, meaning all models generally equally to the prediction (unweighted).\n                \n        elif y.name[2] in self.enriched_ts_map.keys():\n            ext_features_ = [col for col in self.ext_features if col not in self.enriched_ts_map[y.name[2]]]\n            remove_ext_features_ = ColumnTransformer([(\"drop_f\", \"drop\", ext_features_)],remainder=\"passthrough\")\n            \n            power_tr = PowerTransformer()\n            # The PowerTransform applies a power transform on the Feature to reshape their distributions  closer to Gaussian (Normal).\n            # There are many kinds of Power Transforms. In this case, PowerTransformer is useful for modeling issues related to Heteroscedasticity.\n            # This PowerTransformer uses Yeo-Johnson (Log(y)+1 Transform) to reshape the Distribution of the positive and negative Numeric (continuous) variables in the Dataset closer into a Normal Distribution..\n            \n            ridge = TransformedTargetRegressor(\n                regressor = Ridge(fit_intercept=True, solver='auto', alpha=0.7, normalize=True, random_state=SEED),\n                # The TransformedTargetRegressor is a meta-estimator to regress on a transformed Target. It is useful for applying a non-linear transformation to the Target in Regression problems.\n                # In other words, it is used to scale and transform the Target to prepare it for Regression using sklearn.\n                # The main idea behind Ridge is to introduce a small amount of Bias in the training data, in exchange for a much lower Variance.\n                # In other words, Ridge improves the robustness of the Model.\n                # 'fit_intercept' set to True to fit the Intercept since I don't expect X and y to be centered.\n                # 'solver' set to Auto to automatically choose the computational routines based on the Data Type.\n                # 'alpha' set to 0.7 It is a non-negative constant (float) that controls the strength of the Ridge by multiplying the Ridge Regression (L2) Term.\n                # 'normalize' set to True to normalize regressors X by subtracting the Mean and dividing by the L2-norm.\n                func=np.log1p,\n                inverse_func=np.expm1\n            )\n            ridge_round_to_int = TransformedTargetRegressor(\n                regressor=ridge,\n                inverse_func=np.rint\n            # 'np.rint' is a mathematical function that rounds elements of the array to the nearest integer.\n            )\n            model = make_pipeline(\n                remove_ext_features_,\n                power_tr,\n                ridge_round_to_int\n            )\n            \n            \n        elif y.name[2] in self.low_sales_ts:\n            # Here I did the same for the low sales time series data.\n            ridge = TransformedTargetRegressor(\n                regressor = Ridge(fit_intercept=True, solver='auto', alpha=0.75, normalize=True, random_state=SEED),\n                func=np.log1p,\n                inverse_func=np.expm1\n            )\n            svr = TransformedTargetRegressor(\n                regressor = SVR(C = 0.2, kernel = 'rbf'),\n                func=np.log1p,\n                inverse_func=np.expm1\n            )\n            model = VotingRegressor([('ridge', ridge), ('svr', svr)])\n        else:\n            ridge = make_pipeline(\n                remove_ext_features,\n                TransformedTargetRegressor(\n                    regressor = Ridge(fit_intercept=True, solver='auto', alpha=0.6, normalize=True, random_state=SEED),\n                    func=np.log1p,\n                    inverse_func=np.expm1)\n            )\n            svr = make_pipeline(\n                remove_ext_features,\n                TransformedTargetRegressor(\n                    regressor = SVR(C = 0.2, kernel = 'rbf'),\n                    func=np.log1p,\n                    inverse_func=np.expm1)\n            ) \n          \n            knn = make_pipeline(\n                ColumnTransformer([(\"selector\", \"passthrough\", self.knn_features)], remainder=\"drop\"),\n                PowerTransformer(),\n                KNeighborsRegressor(n_neighbors=3, n_jobs=self.n_jobs)\n            # KNeighborsRegressor predicts continuous values, not labels, by taking the Mean of the nearest K neighbor.\n            # Here the target is predicted by local interpolation of the targets with the nearest neighbors in the training set.\n            # I used a specific feature set for KNN to cluster observations in a way that captures week and intra-week seasonality.\n            )\n            ard = make_pipeline(\n                remove_ext_features,\n                TransformedTargetRegressor(\n                    regressor = ARDRegression(fit_intercept=True, normalize=True, n_iter=300),\n                    func=np.log1p,\n                    inverse_func=np.expm1)\n            )\n            # ARDRegression fits the weight of a Regression Model using an Automatic Relevance Determination (ARD) prior, which is used to update the weights to get the final output.\n            # The weights of the Regression Model are assumed to be normally distributed, though there are more 0s (sharp peak at 0), hence the Distribution is narrower.\n            # The output of ARDRegression has a lower Mean Squared Error (MSE) than that of Ridge, meaning it is a more performant model.\n            estimators = [\n                ('ridge', ridge),\n                ('svr', svr),\n                (\"ard\", ard),\n                (\"knn\",knn)\n            ]\n            model = VotingRegressor(estimators)\n            \n        model.fit(X, y)\n        return model\n    \n    def fit(self, X, y):\n        print(\"Fitting Stage...\")\n        self.product_names_ = [str(y.iloc[:, i].name[2]) for i in range(y.shape[1])]\n        self.store_names_ = [str(y.iloc[:, i].name[1]) for i in range(y.shape[1])]\n        self.estimators_ = []\n        for i, n in tqdm(enumerate(self.product_names_)):\n            estimator_ = self._estimator_(\n                # Here, I selected as features only predictions of product sales in the same store or same product in other stores\n                X.filter(\n                    regex= n + \"'\\)$|\\(\\d|^[a-zA-Z_0-9., ]+$|\\('sales', '\" + str(y.iloc[:, i].name[1]) + \"',\",\n                    axis=1,\n                ),\n                y.iloc[:, i],\n            )\n            self.estimators_.append(estimator_)\n            # What is regex here?\n            \n    def predict(self, X):\n        print(\"Prediction stage...\")\n        y_pred = []\n        for e, n, m in tqdm(zip(self.estimators_, self.product_names_, self.store_names_)):\n            y_pred_ = e.predict(\n                # Select as features only predictions of product sales in the same store or same product in other stores\n                X.filter(\n                    regex= n + \"'\\)$|\\(\\d|^[a-zA-Z_0-9., ]+$|\\('sales', '\" + m + \"',\",\n                    axis=1,\n                )\n            )\n            y_pred.append(y_pred_)\n            \n        return np.stack(y_pred, axis=1)\n# In this code block, I essentially declared the class CustomRegressor to define my own Regressors to work together with sklearn and hyperparameter optimizations.","metadata":{"execution":{"iopub.status.busy":"2022-06-29T08:40:06.864222Z","iopub.execute_input":"2022-06-29T08:40:06.864571Z","iopub.status.idle":"2022-06-29T08:40:06.901026Z","shell.execute_reply.started":"2022-06-29T08:40:06.86454Z","shell.execute_reply":"2022-06-29T08:40:06.900051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time \n\nknn_features = list(int_features - set(['oil_lags2', 'oil_lags1',\"trend\"]))\n# I manually selection the features for the KNN Regression.\nmodel = CustomRegressor(ext_features, knn_features, non_st_ts, low_sales_ts, enriched_ts_map, n_jobs=-1)\nmodel.fit(x, y)\n\ny_pred = pd.DataFrame(model.predict(x), index=x.index, columns=y.columns)","metadata":{"execution":{"iopub.status.busy":"2022-06-29T08:40:06.902598Z","iopub.execute_input":"2022-06-29T08:40:06.90302Z","iopub.status.idle":"2022-06-29T08:54:02.868349Z","shell.execute_reply.started":"2022-06-29T08:40:06.902957Z","shell.execute_reply":"2022-06-29T08:54:02.867527Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import mean_squared_log_error as msle\n# I decided to use MSLE because I don't want large Errors to be significantly more penalized than the small Errors.\n# The introduction of the logarithm in this loss function means that MSLE treats small differences between small true and predicted values in the same way that it treats the big differences between large true and predicted values.\ny_pred = y_pred.stack(['store_nbr', 'family']).clip(0.)\ny_ = y.stack(['store_nbr', 'family']).clip(0.)\n# .clip() is used to limit the values in an array.\n# Given an interval, values outside the interval are clipped to the interval edges. For example, if an interval of [0, 1] is specified, values smaller than 0 become 0, and values larger than 1 become 1.\ny_['pred'] = y_pred.values\nprint(y_.groupby('family').apply(lambda r : np.sqrt(np.sqrt(msle(r['sales'], r['pred'])))))\nprint('RMSLE : ', np.sqrt(np.sqrt(msle(y_['sales'], y_['pred']))))\n# I used the RMSLE here because I want to penalize underestimations of my predictions more than the overestimations.\n# This is because in this industry, to some extent having extra inventory or supply might be more preferable to not being able to providing product as much as the demand.\n# The RMSLE is a loss function metric that evaluates the performance of my predictions by measuring the Relative Error between the predicted and the actual values, where the scale of the Error is insignificant.","metadata":{"execution":{"iopub.status.busy":"2022-06-29T08:54:02.870538Z","iopub.execute_input":"2022-06-29T08:54:02.871023Z","iopub.status.idle":"2022-06-29T08:54:03.154634Z","shell.execute_reply.started":"2022-06-29T08:54:02.870977Z","shell.execute_reply":"2022-06-29T08:54:03.153162Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time \n\nypred = pd.DataFrame(model.predict(xtest), index = xtest.index, columns = y.columns).clip(0.)\nypred = ypred.stack(['store_nbr', 'family'])\nypred = ypred.append(zero_prediction).sort_index()\nsub = pd.read_csv('../input/store-sales-time-series-forecasting/sample_submission.csv')\nsub['sales'] = ypred.values\nsub.to_csv('submission.csv', index = False)\n# My Submission","metadata":{"execution":{"iopub.status.busy":"2022-06-29T08:54:03.155937Z","iopub.execute_input":"2022-06-29T08:54:03.156381Z","iopub.status.idle":"2022-06-29T08:56:54.113021Z","shell.execute_reply.started":"2022-06-29T08:54:03.156351Z","shell.execute_reply":"2022-06-29T08:56:54.112178Z"},"trusted":true},"execution_count":null,"outputs":[]}]}