{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"\n# -1. Still a draft\n-  require periodic code cleanup/rewrite/streamlining to prevent crumbling to \"technical debt\"\n\n- __note: this is currently the most updated version__\n\n- need to join \"train & test\" to have a number of features that match,\n    - indicate via column: \"is_test\" to still be able to seggregate back.","metadata":{}},{"cell_type":"markdown","source":"# 0. General Workflow <a class=\"anchor\" id=\"Ch0\"></a>:\n1. [__Problem def__](#Ch1): \n    - project objective: prediction contraints, selection of relevant models\n    - additional objective: explain how/why those results/methods as much as possible \n        - (to convince other DS-experts, as well as experts in the field of the project's topic)\n<br>\n    - [1.1](#Ch1.1) goal\n    - [1.2](#Ch1.2) global strategy\n    - [1.3](#Ch1.3) milestones\n    \n2. [__Data acquisition__](#Ch2):\n    - [2.1](#Ch2.a) location\n    - [2.2](#Ch2.b) retrival\n    - [2.3](#Ch2.c) variables definitions\n    - [2.4](#Ch2.d) early __feature proposals__ from (human) prior understanding\n<br>   \n3. [__Data pre-processing__](#Ch3):\n\n    - [3.1 train & test](#Ch3.1)  :: [(a) priority](#Ch3.1a) [(b) processing](#Ch3.1b)\n    - [3.2 stores & events](#Ch3.2) :: [(a) stores](#Ch3.2) [(b) events](#Ch3.2b) [(c) sp_events](#Ch3.2c)\n    - [3.3 paydays](#Ch3.3)\n    - [3.4 oil](#Ch3.4)\n    - __[3.5 processed storage](#Ch3.5)__\n\n4. [__Model ensembles__](#Ch4):\n     \n    - [(a)](#Ch4.a) model ensembles construction\n    - [4.1 construction](#Ch4.1)\n    - [4.2 eval](#Ch4.2)\n    - __[4.3 one model pipe](#Ch4.3)__\n    - [4.3 global](#Ch4.4)\n    \n    - [(b)](#Ch4.b) for all models: gain function maximization + estimated error/deviation of the \"gain\"\n    - [(c)](#Ch4.c) for all models: feedback/interpretations for each model (if possible)\n    - [(d)](#Ch4.d) unique (top 3?) model selection\n<br>\n6. [__Prediction and scenarios__](#Ch6):\n    - [(a)](#Ch6.a) multiple scenarios: predictions from selected model(s)\n    - [(b)](#Ch6.b) key points/insights, applicable to each scenario\n<br>\n7. [__Bonus: Iterations for in the future__](#Ch7):\n    - compare how model(s) are holding up with real data, posterior to project prediction\n    - sources of unexplained variance ? suggest new sources of features for future tasks/models\n    - repeat (1.) with more of (both) the same kind data and new kind of data\n    \n    \n[to end](#End)","metadata":{}},{"cell_type":"markdown","source":"# 1. Problem def <a class=\"anchor\" id=\"Ch1\"></a>: \n\n[goto (0.) General Workflow](#Ch0)\n\n## 1.1 Goal of the project [(to kaggle)](https://www.kaggle.com/c/store-sales-time-series-forecasting/overview): <a class=\"anchor\" id=\"Ch1.1\"></a>\n\n1. Primary/Major objective: Predict future sales $Y[s,p](t_{+})$ for each $(s,p) \\in \\{\\text{store}, \\text{product-class}\\}$, based on previous sales $Y[s,p](t_{-})$ of those same $(s,p)$\n    - hence all \"time series\" machine learing [methods](https://en.wikipedia.org/wiki/Time_series) are (potentially) applicable\n    - from the context (= sales, economics) mostly methods from [here](https://link.springer.com/book/10.1007/978-1-4419-0320-4) or equivalent --i.e. stationary processes-- will be tested\n<br><br>\n2. Secondary objective: What ML-algo provides the 'best' prediction ? \n    - for the [given metric](https://www.kaggle.com/c/store-sales-time-series-forecasting/overview/evaluation): \"Root Mean Squared Logarithmic Error\"\n        - $M(\\hat{y})=\\sqrt{ \\frac{1}{n} \\sum_{i=1}^n \\left(\\log (1 + \\hat{y}_i) - \\log (1 + y_i)\\right)^2}$\n    <br><br>\n    - and its (estimated) associated deviations on this metric (time-series cross validation)\n        \n    - (bonus) theoretical (?) deviations on this metric ([error propagation](https://en.wikipedia.org/wiki/Propagation_of_uncertainty)):\n        - $\\sigma^2(M(\\hat{y})) \\approx \\frac{4}{n^2} \\left[ \\sum_{i=1}^n \\frac{(\\Delta \\log)^2_i}{(1+y_i)^2} \\sigma^2(\\hat{y}_i) + 2 \\sum_{i=1}^n \\sum_{j=i+1}^n \\frac{(\\Delta \\log)_i (\\Delta \\log)_j}{(1+y_i)(1+y_j)} \\sigma(\\hat{y}_j)\\sigma(\\hat{y}_i)\\rho_{ij}\\right]/M(\\hat{y})^2$\n<br><br> \n3. Secondary objective: What are the different insights/interpretation to be gathered from those ML-algo ?\n<br><br>\n4. Minor/major objective: Additional questions to investigate:   \n      (Major --> probably what would interest the big boss and the marketing departement)  \n      (Minor --> here to train myself with techniques of ML and work of any DS-expert, covering most aspect of any forecast)\n    - Can from this data alone, infer the consequences of opening a new store ? <br><br>\n    - What are the most stable products sales-wise?  I.e. products which sales does not vary through seasons/years and are therefore references/markers of (secure) future sales. \n    - Can those product be related to the \"Maslow's pyramid of needs\"-concepts ? <br><br>\n    - Conversely,what are the most volatile poducts ?\n    - What are the characteristics of those volatiles product (store-related,time-related, location-related, no relation whatsoever,...) ? \n","metadata":{}},{"cell_type":"markdown","source":"## 1.2 Global Strategy: induction: <a class=\"anchor\" id=\"Ch1.2\"></a>\n1. Divide and Conquer: segregate the full data $D$ into $D[s,p]$<br>\n2. Simple at first: start with (almost) arbitrary pair $(s^* , p^*)$ and proceed with/ try (full) ML-methods on that pair<br>\n3. From one to many: then constuct the desired pipeline applicable for all pairs (= 2 for loops)<br>\n4. cleanup code\n5. (Bonus?) Interactions: correlations/mutual info between pairs $(s_i,p_j)$ and $(s_k,p_l)$\n \n\n## 1.3  Milestones: <a class=\"anchor\" id=\"Ch1.3\"></a>\n___Done___<br>\nA. Start small/compact: aim for minimum intervention to get first (global, but opaque) results fast:\n\n    1. the only goal is the primary objective: merely to provide a submission file\n    2. only main train/test as source\n    3. less exploratory pre-processing:\n                - day/week/month as feature\n                - CalendarFourier\n    4. models considered: \n                - Linear Regression/ridge\n                - Random Forests\n                - xgboost\n                - HistGradientBoost\n    5. business as usual scenario\n               \n<br>___Currently___<br>\n\nB. Dissect afterwards: explore more complex aspects (more interventions, more time consuming):\n\n    2. include complementary files \n    3. complementary preprocessing:\n                - trend segregation\n                - event-response features\n                \n\n\nnote: general workflow will be applied multiple times\n- for the initial/fast/opaque\n- for the specific (fixed) pair $(s^* , p^*)$\n- for each pair $(s,p)$ ,a systematized (pipeline) version of the specific pair $(s^* , p^*)$\n- (bonus) for higher order interactions (network)\n    - for same store $(s^*)$ and different products $\\{(p)\\}$ : hence pairs $\\{(s^*,p)\\}$\n    - for same product $(p^*)$ and different stores $\\{(s)\\}$ : hence pairs $\\{(s,p^*)\\}$\n    - more ?","metadata":{}},{"cell_type":"markdown","source":"# 2. Data acquisition <a class=\"anchor\" id=\"Ch2\"></a>: \n\n[goto (0.) General Workflow](#Ch0)\n\n## (2.1) & (2.2) Location <a class=\"anchor\" id=\"Ch2.a\"></a> and retrieval <a class=\"anchor\" id=\"Ch2.b\"></a>","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# For example, here's several helpful packages to load\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\n#import matplotlib as plt\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# additional dependencies\n#from pandas.tseries.offsets import MonthEnd\nimport re\n\nfrom pathlib import Path\n\n#work_path = Path('../input/store-sales-time-series-forecasting')\n\n# specific path to locate different data files\ncomp_dir = Path('../input/store-sales-time-series-forecasting')\n\n\n### configs:\nDO_REGENERATE_DATA=True\n\nwork_path = Path('../working')\n\nsubmission = pd.DataFrame(columns=[\"id\",\"sales\"])\nsubmission.to_csv('submission.csv',mode='a',  index=False, header=False)","metadata":{"execution":{"iopub.status.busy":"2022-03-27T20:27:02.011663Z","iopub.execute_input":"2022-03-27T20:27:02.012213Z","iopub.status.idle":"2022-03-27T20:27:02.036506Z","shell.execute_reply.started":"2022-03-27T20:27:02.012175Z","shell.execute_reply":"2022-03-27T20:27:02.035616Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if DO_REGENERATE_DATA:\n\n\n    # major features\n    train = pd.read_csv(\n        comp_dir / 'train.csv',\n        usecols=['id','store_nbr', 'family', 'date', 'sales', 'onpromotion'],\n        dtype={\n            'id':'uint32',\n            'store_nbr': 'category',\n            'family': 'category',\n            'sales': 'float32',\n            'onpromotion': 'uint32',\n        },\n        parse_dates=['date'],\n        infer_datetime_format=True\n    )\n    #train['date'] = train.date.dt.to_period('D')\n    train = train.set_index(['store_nbr', 'family', 'date']).sort_index()\n\n    \nif DO_REGENERATE_DATA:\n\n\n    #test data\n    test = pd.read_csv(\n        comp_dir / 'test.csv',\n        dtype={\n            'store_nbr': 'category',\n            'family': 'category',\n            'onpromotion': 'uint32',\n        },\n        parse_dates=['date'],\n        infer_datetime_format=True,\n    )\n    #test['date'] = test.date.dt.to_period('D')\n    test = test.set_index(['store_nbr', 'family', 'date']).sort_index()\n    test.head()\n\n    # oil prices \n    oil = pd.read_csv(\n        comp_dir / 'oil.csv',\n        parse_dates=['date'],\n        infer_datetime_format=True,)\n    #oil['date'] = oil.date.dt.to_period('D')\n    # stores categories/types & locations\n    stores = pd.read_csv(comp_dir / 'stores.csv')\n\n    # holidays & events\n    events = pd.read_csv(comp_dir / 'holidays_events.csv',\n        parse_dates=['date'],\n        infer_datetime_format=True,)\n    #events['date'] = events.date.dt.to_period('D')\n    ## to do :\n    ### construct a panda data frame which incorporates the suggestion of the (additional) notes : \n    ### -> see \"(2.c) Variable definitions\" markdown/link\n","metadata":{"execution":{"iopub.status.busy":"2022-03-27T20:27:06.015993Z","iopub.execute_input":"2022-03-27T20:27:06.016583Z","iopub.status.idle":"2022-03-27T20:27:09.657054Z","shell.execute_reply.started":"2022-03-27T20:27:06.016543Z","shell.execute_reply":"2022-03-27T20:27:09.656387Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.DataFrame(columns=[\"id\",\"sales\"])\nsubmission.to_csv('submission.csv',mode='w',  index=False, header=False)","metadata":{"execution":{"iopub.status.busy":"2022-03-27T20:27:09.982575Z","iopub.execute_input":"2022-03-27T20:27:09.982854Z","iopub.status.idle":"2022-03-27T20:27:09.990243Z","shell.execute_reply.started":"2022-03-27T20:27:09.982825Z","shell.execute_reply":"2022-03-27T20:27:09.989417Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.3 Variables definitions <a class=\"anchor\" id=\"Ch2.c\"></a>\n\n[meanings of data](https://www.kaggle.com/c/store-sales-time-series-forecasting/data?select=train.csv)\n\n## 2.4 Features proposals <a class=\"anchor\" id=\"Ch2.d\"></a>\n\n[goto (0.) General Workflow](#Ch0)\n\n(see current objective in [milestones](#Ch1.3) )\n\n___If we focus ourselves on a specific pair $(s^*,p^*)$:___<br>\n\n0. ___his means that we hypothesise that___\n    - the direct influence between stores $\\{(s)\\}$is of second or third order (less impactfull than rest of features/interactions) \n    - the correlation in sales/promotions between different product-types $\\{(p)\\}$ are also neglected\n    - which one of the above interactions (network) is the most impactfull (correlated) one will be investigated later (bonus)\n<br><br>\n1. ___'stores'-data:___\n    we will (probably) only need the pair ('city','state') location in 'stores'-data, because:\n   - 'city' : there are city dependent holidays/events possible\n   - 'state' : idem\n   - 'type' : (expected/hypothesised to be only) relevant when comparing different pairs $\\{(s,p)\\}$, \n        hence this feature will clutter the ML-algo's processing and memory until we analyse/investigate interactions of all pairs $\\{(s,p)\\}$ (=another pipeline)\n   - 'cluster': idem <br><br>\n\n2. ___'events'-data:___\n   - is an __interactions feature__ between 'date' and ('city' and/or 'state') and therefore 'store_nbr', certainly worth keeping/including this aspect <br><br>\n   - rows with values ( val= 'Transferred' in col = 'is_transferred') will be dropped (=a dupplicate, however with caveats that are expected/hypothesised to be of negligible importance in magnitude and frequency) <br><br>\n   - investigate carefully if the effects of holiday/events have a similar effect year on year (= feature importance with meaningful interpretation) <br><br>\n   - some events have (probably/hypothetically) more and different impacts on sales (store-wise,and product-wise) than others: \n       - we will avoid the (total) anonymization of the holidays/events\n       - we will construct a dictionary of (yearly) repeated events/holidays, non repeating events/holidays will be anonymized (= more volatile/ less reliable) <br><br>\n   - given that we will investigate \"stationary process\" models, there could be \"anticipation\" and/or \"after(burn)\" prior and/or posterior respectively to specific events/holidays: those can be modelled in a time lag manner:\n       - anticipation: $\\sim$ min amount of days/weeks before the next event/holiday happens\n       - afterburn : $\\sim$ min amount of days/weeks after the previous event/holiday happened <br><br>\n   - because in holiday week we are more flexible to go shopping than in a normal week or on events (?):\n       - segregate the previously mentioned \"time lag\"-like features for events-only and holiday-only<br><br>\n3. ___'oil'-data:___ \n    - seem to be generally applicable, however stores with gasoline/petrol pumps/distribution may be affected to a greather degree :\n        - however only way to evaluate such effect is to compare the sales of different stores being part of the same 'cluster'&'type' and those which differ in 'cluster'&'type'\n    - sales may also be affected by the high difference in pricing ($\\sim$ time lag), but over which time span ?\n        - multiple (finite-)difference price of oil  over different time span as feature\n        \n4. ___'train'-data:___\n    - lagged sales (=sales from before) as features: motivated if stationary process\n    - onpromotion: no changes \n        - lagging: promotion does not linger\n        - random-persieved: those promotions are (likely) not announced & anticipated prior to the day of promotion, hence --from buyer-perspective-- at random ?\n    - expanded dates into day/week/month + their respective trigonometric version.\n            ","metadata":{}},{"cell_type":"markdown","source":"# 3. Data pre-processing <a class=\"anchor\" id=\"Ch3\"></a> <a class=\"anchor\" id=\"Ch3.a\"></a> <a class=\"anchor\" id=\"Ch3.b\"></a>: \n\n[goto (0.) General Workflow](#Ch0)\n\n## 3.0) Setup of general functions","metadata":{}},{"cell_type":"code","source":"from statsmodels.tsa.deterministic import CalendarFourier, DeterministicProcess\n\n\nplot_params = dict(\n    color=\"0.75\",\n    style=\".-\",\n    markeredgecolor=\"0.25\",\n    markerfacecolor=\"0.25\",\n    legend=False,\n)\n","metadata":{"execution":{"iopub.status.busy":"2022-03-27T20:27:12.287723Z","iopub.execute_input":"2022-03-27T20:27:12.288155Z","iopub.status.idle":"2022-03-27T20:27:12.292723Z","shell.execute_reply.started":"2022-03-27T20:27:12.288117Z","shell.execute_reply":"2022-03-27T20:27:12.292008Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_plot3(df1,x_axis,y_axis,\n               df2=pd.DataFrame(),\n               df3 =pd.DataFrame(),  title=False, line=1):\n   \n    plt.style.use(\"seaborn-whitegrid\")\n    plt.rc(\n    \"figure\",\n    autolayout=True,\n    figsize=(11, 4),\n    titlesize=18,\n    titleweight='bold',\n    )\n    plt.rc(\n    \"axes\",\n    labelweight=\"bold\",\n    labelsize=\"large\",\n    titleweight=\"bold\",\n    titlesize=16,\n    titlepad=10,\n    )\n    %config InlineBackend.figure_format = 'retina'\n\n    fig, ax = plt.subplots()\n    df1.plot(**plot_params)\n    if (not df2.empty):\n        df2.plot(ax=ax, linewidth=line)\n    if (not df3.empty):\n        df3.plot(ax=ax, linewidth=line, color='red')\n    \n    ax.set_ylabel(y_axis)\n    ax.set_xlabel(x_axis)\n    if title:\n        plt.title(title)\n    plt.show()\n    pass\n    \ndef make_plot3_rol(df1,x_axis,y_axis,\n               df2=pd.DataFrame(),\n               df3 =pd.DataFrame(),  title=False, line=1):\n    return make_plot3(\n        df1.rolling(\n    window=21,\n    center=True,\n    min_periods=11,\n).mean(),\n        x_axis,y_axis,\n        df2.rolling(\n    window=21,\n    center=True,\n    min_periods=11,\n).mean(),\n        df3.rolling(\n    window=21,\n    center=True,\n    min_periods=11,\n).mean() ,title=False, line=1)","metadata":{"execution":{"iopub.status.busy":"2022-03-27T20:27:14.548562Z","iopub.execute_input":"2022-03-27T20:27:14.549456Z","iopub.status.idle":"2022-03-27T20:27:14.565534Z","shell.execute_reply.started":"2022-03-27T20:27:14.549397Z","shell.execute_reply":"2022-03-27T20:27:14.564912Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def check_integrity(data):\n### ---------------------------------\n    ##-# general data check\n    missing = data.isna().sum()\n    \n    print(f'total (initial) missing values {missing.sum()}')\n    print('*'*40)\n    \n    ##-# check dtypes\n    types_df = pd.DataFrame(data.dtypes, columns=['type'])\n    u_types=types_df['type'].unique()\n    print(f'unique_types in data = {u_types}')\n    print('*'*40)\n    \n    ##-# setup to check columns\n    cols_u = []\n    for ku in np.arange(len(u_types)):\n        tupl = types_df.loc[types_df['type']==u_types[ku]].index.to_numpy()\n        cols_u.append(tupl)\n    print(f'cols_u = {cols_u}')\n    print('*'*40)\n### ---------------------------------    \n    print('*'*40)\n    ##-# check index: check index if missing wrt range\n    if isinstance(data.index, pd.MultiIndex):\n        print(\"MultiIndex data is currently not supported\")\n    elif isinstance(data.index, pd.DatetimeIndex):  \n        itype= \"DatetimeIndex\"\n        print( f\"index is: {itype}\")\n        \n        idx_start=data.index[0]\n        idx_end = data.index[-1]\n\n        idx_full_range=pd.date_range(start=idx_start,end=idx_end)\n    elif isinstance(data.index, pd.RangeIndex):\n        itype= \"RangeIndex\"\n        print( f\"index is:{itype}\")\n        idx_start=data.index[0]\n        idx_end = data.index[-1]\n        \n        idx_full_range=np.arange(start=idx_start,stop=idx_end+1,dtype=type(idx_start))\n    else:\n        itype= type(data.index[0])\n        print( f\"index is:{itype}\")\n        print(\"passing index range integrity check\")\n        idx_full_range = data.index\n    ### ---    \n    _n_idx = -len(data.index)+len(idx_full_range)\n    if (_n_idx!=0):\n        print(f\"index not at full range! increasing index range by (n={_n_idx})\")\n        \n        DF = pd.DataFrame(index=idx_full_range)\n        DF = DF.join(data)\n    else:\n        print(f\"index seems to be at full range\")\n        DF = data.copy()\n    print('*'*40)    \n### ---------------------------------    \n\n    ##-# check NaN, per cols/col_type\n    for ku in np.arange(len(u_types)):\n        print('*'*40)\n        print(f'checking type = {u_types[ku]}')\n        print('-'*30)\n        print('-'*30)\n        for kcu in np.arange(len(cols_u[ku])):\n            missing_cu = DF[cols_u[ku][kcu]].copy().isna().sum()\n            if (missing_cu != 0):\n                print(f'missing (n={missing_cu}) values in (col={cols_u[ku][kcu]})')\n            else:\n                print(f'all ok in (col={cols_u[ku][kcu]})')\n    \n    ##--# check repeated/uniques values in obj_cols\n            if (u_types[ku]==\"object\"):\n                nu = DF[cols_u[ku][kcu]].nunique()\n                #amount_uniq = len(pd.unique(DF[cols_u[ku][kcu]]))\n                print( f'non-unique elements (n={nu})')\n                #print( f'unique elements (n={amount_uniq})')\n                print('-'*30)\n    \n    #check inconsistent obj_cols entries: need to be tailored to data: not here \n    return types_df, u_types,cols_u, DF","metadata":{"execution":{"iopub.status.busy":"2022-03-27T20:27:16.898151Z","iopub.execute_input":"2022-03-27T20:27:16.898781Z","iopub.status.idle":"2022-03-27T20:27:16.913775Z","shell.execute_reply.started":"2022-03-27T20:27:16.898727Z","shell.execute_reply":"2022-03-27T20:27:16.912909Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_lag(DF, column,lag_lst=[1,],interpol=True):\n    #DF = df.copy()\n    for lag in lag_lst:\n        if interpol:\n            DF[column+\"_\"+str(lag)] = DF[column].shift(lag).interpolate(limit_direction='both')\n        else:\n            DF[column+\"_\"+str(lag)] = DF[column].shift(lag)\n    pass","metadata":{"execution":{"iopub.status.busy":"2022-03-27T20:27:19.198621Z","iopub.execute_input":"2022-03-27T20:27:19.199398Z","iopub.status.idle":"2022-03-27T20:27:19.204187Z","shell.execute_reply.started":"2022-03-27T20:27:19.199347Z","shell.execute_reply":"2022-03-27T20:27:19.20358Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.1) Train and Test <a class=\"anchor\" id=\"Ch3.1\"></a>\n  [goto (0.) General Workflow](#Ch0)  ","metadata":{}},{"cell_type":"code","source":"print('Missing values in train:', train.isna().sum().sum())\nprint('Missing values in test:', test.isna().sum().sum())\n\ntrain.head()\n","metadata":{"execution":{"iopub.status.busy":"2022-03-27T20:27:21.501641Z","iopub.execute_input":"2022-03-27T20:27:21.502483Z","iopub.status.idle":"2022-03-27T20:27:21.531308Z","shell.execute_reply.started":"2022-03-27T20:27:21.502426Z","shell.execute_reply":"2022-03-27T20:27:21.530764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lst_s = train.index.get_level_values(0).unique().to_numpy()\nlst_p = train.index.get_level_values(1).unique().to_numpy()\nlst_t = train.index.get_level_values(2).unique().to_numpy()\n\nprint(lst_s, len(lst_s))\nprint(\"*\"*40)\nprint(lst_p, len(lst_p))\nprint(\"*\"*40)\nprint(lst_t, len(lst_t))","metadata":{"execution":{"iopub.status.busy":"2022-03-27T20:27:23.883174Z","iopub.execute_input":"2022-03-27T20:27:23.883692Z","iopub.status.idle":"2022-03-27T20:27:24.048141Z","shell.execute_reply.started":"2022-03-27T20:27:23.883643Z","shell.execute_reply":"2022-03-27T20:27:24.047489Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.1a) prioritize (s,p) pairs to be generated <a class=\"anchor\" id=\"Ch3.1\"></a>\n[goto (0.) General Workflow](#Ch0)  \n\n- relevant for [3.5 generate](#Ch3.5)","metadata":{}},{"cell_type":"code","source":"train_s_pt = train.groupby(by='store_nbr').sum()\nref_s = train_s_pt.index\nimp_s = train_s_pt.sort_values(by=\"sales\",ascending=False).index","metadata":{"execution":{"iopub.status.busy":"2022-03-27T20:27:25.515218Z","iopub.execute_input":"2022-03-27T20:27:25.515585Z","iopub.status.idle":"2022-03-27T20:27:25.678266Z","shell.execute_reply.started":"2022-03-27T20:27:25.515541Z","shell.execute_reply":"2022-03-27T20:27:25.67729Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(imp_s[0],ref_s[1])\n","metadata":{"execution":{"iopub.status.busy":"2022-03-27T20:27:26.338651Z","iopub.execute_input":"2022-03-27T20:27:26.339742Z","iopub.status.idle":"2022-03-27T20:27:26.345721Z","shell.execute_reply.started":"2022-03-27T20:27:26.339698Z","shell.execute_reply":"2022-03-27T20:27:26.344907Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"order_s= []\nfor s_sort in imp_s:\n    for ks in np.arange(len(ref_s)):\n        if ref_s[ks] == s_sort:\n            order_s.append(ks)\n            break","metadata":{"execution":{"iopub.status.busy":"2022-03-27T20:27:27.295179Z","iopub.execute_input":"2022-03-27T20:27:27.296043Z","iopub.status.idle":"2022-03-27T20:27:27.313572Z","shell.execute_reply.started":"2022-03-27T20:27:27.296001Z","shell.execute_reply":"2022-03-27T20:27:27.312633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ref_s[order_s[0]]","metadata":{"execution":{"iopub.status.busy":"2022-03-27T20:27:28.266859Z","iopub.execute_input":"2022-03-27T20:27:28.267246Z","iopub.status.idle":"2022-03-27T20:27:28.273705Z","shell.execute_reply.started":"2022-03-27T20:27:28.267196Z","shell.execute_reply":"2022-03-27T20:27:28.272628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_p_st = train.groupby(by='family').sum()\nref_p = train_p_st.index\nimp_p = train_p_st.sort_values(by=\"sales\",ascending=False).index\norder_p= []\n\nfor p_sort in imp_p:\n    for kp in np.arange(len(ref_p)):\n        if ref_p[kp] == p_sort:\n            order_p.append(kp)\n            break\nref_p[order_p[0]]","metadata":{"execution":{"iopub.status.busy":"2022-03-27T20:27:28.919678Z","iopub.execute_input":"2022-03-27T20:27:28.920289Z","iopub.status.idle":"2022-03-27T20:27:29.086038Z","shell.execute_reply.started":"2022-03-27T20:27:28.920243Z","shell.execute_reply":"2022-03-27T20:27:29.085109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.1b) (s,p) grabbing and processing <a class=\"anchor\" id=\"Ch3.1b\"></a>\n[goto (0.) General Workflow](#Ch0)  ","metadata":{}},{"cell_type":"code","source":"def to_log_sales(y):\n    z = y.copy()\n    z[\"sales\"] = z[\"sales\"].apply(np.log1p)\n    return z\n\ndef from_log_sales(y):\n    z = y.copy()\n    z[\"sales\"] = z[\"sales\"].apply(np.expm1)\n    return z\n    ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def select_sp(DF,s,p):\n    return DF.xs(s,level=\"store_nbr\").xs(p,level='family')\n\ndef join_sp_train_test(train_sp,test_sp):\n    tr = train_sp.copy()\n    tr[\"is_test\"]=np.full(len(tr),False)\n    te = test_sp.copy()\n    te[\"is_test\"]=np.full(len(te),True)\n    te[\"sales\"] = np.full(len(te),np.nan) \n    sp = pd.concat([tr,te])\n    return sp \n\ndef DataProcess_to_Xy(sp,is_test=False):\n    \n    #X_pre = sp.copy().dropna()   \n    X_pre = sp.copy()\n    if not is_test:\n        y_plt = X_pre.pop(\"sales\")\n        y = pd.DataFrame(y_plt,columns=[\"sales\"]).join(X_pre[[\"id\"]])\n        y = to_log_sales(y)\n    ### ---\n    else:\n        y = X_pre[[\"id\"]]\n        \n    fourier = CalendarFourier(freq='W', order=4)\n    fourier_2 = CalendarFourier(freq='M', order=4)\n    dp = DeterministicProcess(index=y.index,\n                              constant=False,\n                              order=1,\n                              seasonal=False,\n                              additional_terms=[fourier,fourier_2],\n                              drop=True)\n\n    X = dp.in_sample()\n    X = X.join(X_pre,how=\"outer\")\n    \n    # explicit \"day\",\"week\" and \"month\"\n    X[\"day\"] = X.index.dayofweek\n    X[\"week\"] = X.index.isocalendar().week\n    X[\"month\"]= X.index.month\n    \n    #y, X = y.align(X, join='inner')\n    \n    \n    \n    return X,y","metadata":{"execution":{"iopub.status.busy":"2022-03-27T20:27:30.352121Z","iopub.execute_input":"2022-03-27T20:27:30.352398Z","iopub.status.idle":"2022-03-27T20:27:30.359965Z","shell.execute_reply.started":"2022-03-27T20:27:30.352366Z","shell.execute_reply":"2022-03-27T20:27:30.359376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tdf, u_t , u_cols, DF= check_integrity(select_sp(DF=train,s= lst_s[0],p =lst_p[7]))","metadata":{"execution":{"iopub.status.busy":"2022-03-27T20:27:32.582858Z","iopub.execute_input":"2022-03-27T20:27:32.58337Z","iopub.status.idle":"2022-03-27T20:27:32.604833Z","shell.execute_reply.started":"2022-03-27T20:27:32.583318Z","shell.execute_reply":"2022-03-27T20:27:32.604033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def sp_pipe(DF, is_test=False):\n    sp = DF.copy()\n    sp[\"id\"] = sp[\"id\"].fillna(-1)\n    sp[\"onpromotion\"] = sp[\"onpromotion\"].fillna(0)\n    \n    ### keeping track of interpolated instances (=less reliable value)\n    if not is_test:\n        #sp[\"sales_na\"] = sp[\"sales\"].isna() \n        sp[\"sales\"]= sp[\"sales\"].interpolate(limit_direction='both')\n    \n    return sp","metadata":{"execution":{"iopub.status.busy":"2022-03-27T20:27:34.931811Z","iopub.execute_input":"2022-03-27T20:27:34.932094Z","iopub.status.idle":"2022-03-27T20:27:34.937823Z","shell.execute_reply.started":"2022-03-27T20:27:34.932046Z","shell.execute_reply":"2022-03-27T20:27:34.936953Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ks0 = 2\nkp0 = 13\nsp0 = select_sp(DF=train,s=lst_s[ks0],p=lst_p[kp0])\nsp0 = sp_pipe(DF=sp0)\nsp0.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-27T20:27:36.964608Z","iopub.execute_input":"2022-03-27T20:27:36.965154Z","iopub.status.idle":"2022-03-27T20:27:36.98565Z","shell.execute_reply.started":"2022-03-27T20:27:36.965115Z","shell.execute_reply":"2022-03-27T20:27:36.98508Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_pre = sp0.copy()\ny_plt = X_pre.pop(\"sales\")\ny = pd.DataFrame(y_plt,columns=[\"sales\"]).join(X_pre[[\"id\"]])\nfourier = CalendarFourier(freq='W', order=4)\nfourier_2 = CalendarFourier(freq='M', order=4)\ndp = DeterministicProcess(index=y.index,\n                              constant=False,\n                              order=1,\n                              seasonal=False,\n                              additional_terms=[fourier,fourier_2],\n                              drop=True)\nX = dp.in_sample()\nX = X.join(X_pre,how=\"outer\")\nX[\"day\"] = X.index.dayofweek\nX[\"week\"] = X.index.isocalendar().week\nX[\"month\"]= X.index.month\n    \nX.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-27T20:27:38.960937Z","iopub.execute_input":"2022-03-27T20:27:38.961243Z","iopub.status.idle":"2022-03-27T20:27:39.035898Z","shell.execute_reply.started":"2022-03-27T20:27:38.961213Z","shell.execute_reply":"2022-03-27T20:27:39.034835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(y),len(X))","metadata":{"execution":{"iopub.status.busy":"2022-03-27T20:27:40.649635Z","iopub.execute_input":"2022-03-27T20:27:40.649919Z","iopub.status.idle":"2022-03-27T20:27:40.654952Z","shell.execute_reply.started":"2022-03-27T20:27:40.649888Z","shell.execute_reply":"2022-03-27T20:27:40.654334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.2 \"stores\" and \"events\" data:<a class=\"anchor\" id=\"Ch3.2\"></a>\n[goto (0.) General Workflow](#Ch0)\n### 3.2a \"stores\"-data:\n    this first, because need a dictionary build from it for \"events\"-data","metadata":{}},{"cell_type":"code","source":"stores.tail(3)","metadata":{"execution":{"iopub.status.busy":"2022-03-27T20:27:42.643685Z","iopub.execute_input":"2022-03-27T20:27:42.64397Z","iopub.status.idle":"2022-03-27T20:27:42.655031Z","shell.execute_reply.started":"2022-03-27T20:27:42.64393Z","shell.execute_reply":"2022-03-27T20:27:42.65417Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tdf, u_t , u_cols, DF= check_integrity(stores)","metadata":{"execution":{"iopub.status.busy":"2022-03-27T20:27:44.914405Z","iopub.execute_input":"2022-03-27T20:27:44.914799Z","iopub.status.idle":"2022-03-27T20:27:44.928405Z","shell.execute_reply.started":"2022-03-27T20:27:44.914766Z","shell.execute_reply":"2022-03-27T20:27:44.927775Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if DO_REGENERATE_DATA:\n    N_stores= stores.shape\n    print(N_stores)\n    store_ids = stores[\"store_nbr\"].to_numpy()\n    stores.loc[stores[\"store_nbr\"] == 1].head()\n\nif DO_REGENERATE_DATA:\n    locale = []\n    for k in store_ids:\n        locale.append(stores[[\"state\",\"city\"]].loc[stores[\"store_nbr\"]==k].to_numpy()[0])\n\n    loc_dict = {str(store_ids[ks]):locale[ks] for ks in np.arange(len(store_ids))}","metadata":{"execution":{"iopub.status.busy":"2022-03-27T20:27:47.155685Z","iopub.execute_input":"2022-03-27T20:27:47.156093Z","iopub.status.idle":"2022-03-27T20:27:47.206208Z","shell.execute_reply.started":"2022-03-27T20:27:47.156049Z","shell.execute_reply":"2022-03-27T20:27:47.20513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.2b) \"events\"-data <a class=\"anchor\" id=\"Ch3.2b\"></a>","metadata":{}},{"cell_type":"code","source":"events.head(3)","metadata":{"execution":{"iopub.status.busy":"2022-03-27T20:27:49.460674Z","iopub.execute_input":"2022-03-27T20:27:49.460961Z","iopub.status.idle":"2022-03-27T20:27:49.473583Z","shell.execute_reply.started":"2022-03-27T20:27:49.460927Z","shell.execute_reply":"2022-03-27T20:27:49.47258Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tdf, u_t , u_cols, DF= check_integrity(events)","metadata":{"execution":{"iopub.status.busy":"2022-03-27T20:27:51.698715Z","iopub.execute_input":"2022-03-27T20:27:51.699014Z","iopub.status.idle":"2022-03-27T20:27:51.716324Z","shell.execute_reply.started":"2022-03-27T20:27:51.698966Z","shell.execute_reply":"2022-03-27T20:27:51.715619Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_parsed_descrip_1(x):\n    x = tuple(re.split(\"[0-9' '\\-\\+]+\",x))\n    return x\n\ndef make_simple_descrip(x):\n    if \"Recupero\" in x:\n        return \"Recupero\"\n    \n    elif \"Traslado\" in x:\n        z = \"\"\n        for s in x:\n            if s!=\"Traslado\":\n                z+=s+\" \"\n        return z.rstrip()\n    \n    elif \"Puente\" in x:\n        z = \"\"\n        for s in x:\n            if s!=\"Puente\":\n                z+=s+\" \"\n        return z.rstrip()\n    \n    elif (\"Terremoto\" in x)and(\"Manabi\"in x):\n        return \"Terremoto\"+\" \"+\"Manabi\"\n    \n    elif (\"Mundial\" in x)and(\"futbol\"in x):\n        return \"Mundial de futbol\"\n    \n    else:\n        y=\"\"\n        for s in x:\n            y+= s+\" \"\n        return y.rstrip()\n    pass\n\ndef is_core_event(x):\n    \n    y = tuple(re.split(\"[a-z' '\\-\\+]+\",x))\n    for s in y:\n        if s.isdigit():\n            return False\n        else:\n            continue\n    return True","metadata":{"execution":{"iopub.status.busy":"2022-03-27T20:27:54.082259Z","iopub.execute_input":"2022-03-27T20:27:54.082909Z","iopub.status.idle":"2022-03-27T20:27:54.090731Z","shell.execute_reply.started":"2022-03-27T20:27:54.082875Z","shell.execute_reply":"2022-03-27T20:27:54.089768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\nOH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n\ndef events_sp_pipe(ks): \n    # events relevant for (s,p)\n    E_s = events.loc[events[\"locale\"]==\"National\"].copy()\n    E_s = pd.concat([E_s,events.loc[events[\"locale\"]==\"Local\"].loc[events[\"locale_name\"]==loc_dict[lst_s[ks]][0]]])\n    E_s = pd.concat([E_s,events.loc[events[\"locale\"]==\"Local\"].loc[events[\"locale_name\"]==loc_dict[lst_s[ks]][1]]])\n    \n    col = \"description\"\n    \n    E_s[\"parsed_\"+col] = E_s[col].apply(make_parsed_descrip_1)\n    E_s[\"simple_\"+col] = E_s[\"parsed_\"+col].apply(make_simple_descrip)\n    E_s = E_s.drop(\"parsed_\"+col, axis=1)\n    \n    E_s[\"core_event\"] = E_s[\"description\"].apply(is_core_event)\n    E_s[\"core_event\"].loc[E_s[\"type\"] == \"Bridge\"] = False\n    E_s[\"core_event\"].loc[E_s[\"type\"] == \"Transfer\"] = True\n    E_s[\"core_event\"].loc[E_s[\"type\"] == \"Work Day\"] = False\n    E_s[\"core_event\"].loc[E_s[\"transferred\"] == True] = False\n    \n\n    E_s = E_s.drop(\"type\",axis=1)\n    E_s = E_s.loc[E_s[\"transferred\"]==False].copy()\n    E_s = E_s.drop(\"transferred\",axis=1)\n    E_s = E_s.drop(\"locale\",axis=1)\n    E_s = E_s.drop(\"locale_name\",axis=1)\n    E_s = E_s.drop(col,axis=1)\n    \n    #OH_cols_train = pd.DataFrame(OH_encoder.fit_transform(E_s[\"simple_\"+col]))\n    #OH_cols_valid = pd.DataFrame(OH_encoder.transform(X_valid[low_cardinality_cols]))\n\n    \n    return E_s\n    ","metadata":{"execution":{"iopub.status.busy":"2022-03-27T20:27:56.412145Z","iopub.execute_input":"2022-03-27T20:27:56.41246Z","iopub.status.idle":"2022-03-27T20:27:56.424493Z","shell.execute_reply.started":"2022-03-27T20:27:56.412413Z","shell.execute_reply":"2022-03-27T20:27:56.423407Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"E_s0 = events_sp_pipe(ks=ks0)\nE_s0.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-27T20:27:58.748568Z","iopub.execute_input":"2022-03-27T20:27:58.749204Z","iopub.status.idle":"2022-03-27T20:27:58.783591Z","shell.execute_reply.started":"2022-03-27T20:27:58.749165Z","shell.execute_reply":"2022-03-27T20:27:58.782671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.2c matching events onto \"sp\"-data <a class=\"anchor\" id=\"Ch3.2c\"></a>\n[goto (0.) General Workflow](#Ch0)\n\n[goto 4.3 global](#Ch4.3)","metadata":{}},{"cell_type":"code","source":"def sp_pipe_2(DF,E_s, is_test=False):\n    EE = E_s.copy().set_index(\"date\")\n    sp = DF.copy()\n    #print(f'init (n_sp={len(sp)})')\n    \n    #sp = pd.concat([sp,EE],axis=1)\n    #sp = sp.join(EE)\n    cols = [\"core_event\",\"simple_description\"]\n    match_on_dates_2(DF_target=sp,DF_source=EE,cols=cols)\n    \n    sp[\"core_event\"] = sp[\"core_event\"].fillna(False)\n    \n    lst_ev = EE[\"simple_description\"].unique()\n    \n    ### OH encoding\n    if True:\n        \n        sp[\"sd\"] = sp[\"simple_description\"]\n        sp = sp.drop(\"simple_description\",axis=1)\n        sp = pd.get_dummies(sp, columns = [\"sd\"])\n        \n        if not is_test:\n            sp = sp.loc[sp[\"sales\"]>=0]\n    \n    return sp, lst_ev","metadata":{"execution":{"iopub.status.busy":"2022-03-27T20:28:01.051458Z","iopub.execute_input":"2022-03-27T20:28:01.052019Z","iopub.status.idle":"2022-03-27T20:28:01.058255Z","shell.execute_reply.started":"2022-03-27T20:28:01.051981Z","shell.execute_reply":"2022-03-27T20:28:01.057665Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"E_s0[\"simple_description\"].unique()","metadata":{"execution":{"iopub.status.busy":"2022-03-27T20:28:03.20845Z","iopub.execute_input":"2022-03-27T20:28:03.208859Z","iopub.status.idle":"2022-03-27T20:28:03.214206Z","shell.execute_reply.started":"2022-03-27T20:28:03.208828Z","shell.execute_reply":"2022-03-27T20:28:03.213658Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"EE0= E_s0.set_index(\"date\")\nEE0","metadata":{"execution":{"iopub.status.busy":"2022-03-27T20:28:05.054459Z","iopub.execute_input":"2022-03-27T20:28:05.054724Z","iopub.status.idle":"2022-03-27T20:28:05.067686Z","shell.execute_reply.started":"2022-03-27T20:28:05.054695Z","shell.execute_reply":"2022-03-27T20:28:05.067042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def match_on_dates_2(DF_target,DF_source,cols):\n    \n    for col in cols:\n        DF_target[col] = np.full(len(DF_target),np.nan)\n        \n    for idx in DF_target.index:\n        try:\n            for col in cols:\n                DF_target.loc[idx,col] = DF_source.loc[idx,col]\n        except:\n            continue\n    \n    pass","metadata":{"execution":{"iopub.status.busy":"2022-03-27T20:28:06.622199Z","iopub.execute_input":"2022-03-27T20:28:06.6227Z","iopub.status.idle":"2022-03-27T20:28:06.627174Z","shell.execute_reply.started":"2022-03-27T20:28:06.622664Z","shell.execute_reply":"2022-03-27T20:28:06.626576Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sp0_1 , evs0 = sp_pipe_2(DF=sp0,E_s=E_s0, is_test=False)","metadata":{"execution":{"iopub.status.busy":"2022-03-27T20:28:08.061014Z","iopub.execute_input":"2022-03-27T20:28:08.061585Z","iopub.status.idle":"2022-03-27T20:28:08.331313Z","shell.execute_reply.started":"2022-03-27T20:28:08.061538Z","shell.execute_reply":"2022-03-27T20:28:08.33056Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evs0","metadata":{"execution":{"iopub.status.busy":"2022-03-27T20:28:09.343764Z","iopub.execute_input":"2022-03-27T20:28:09.344741Z","iopub.status.idle":"2022-03-27T20:28:09.350088Z","shell.execute_reply.started":"2022-03-27T20:28:09.344701Z","shell.execute_reply":"2022-03-27T20:28:09.3495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def next_date_next_val_2(DF,col_name,col_val,current_idx):\n    dates_idxs = DF.loc[current_idx:].loc[DF[col_name]==col_val].index.to_numpy()\n    try:\n        next_date_idx = dates_idxs[0]\n        next_date = DF.loc[next_date_idx]\n    except:\n        next_date_idx = np.nan\n        next_date = np.nan\n    return next_date_idx,next_date\n\ndef prev_date_prev_val_2(DF,col_name,col_val,current_idx):\n    dates_idxs = DF.loc[:current_idx].loc[DF[col_name]==col_val].index.to_numpy()\n    try:\n        prev_date_idx = dates_idxs[-1]\n        prev_date = DF.loc[prev_date_idx]\n    except:\n        prev_date_idx = np.nan\n        prev_date = np.nan\n    return prev_date_idx,prev_date\n","metadata":{"execution":{"iopub.status.busy":"2022-03-27T20:28:10.792586Z","iopub.execute_input":"2022-03-27T20:28:10.793213Z","iopub.status.idle":"2022-03-27T20:28:10.799743Z","shell.execute_reply.started":"2022-03-27T20:28:10.793171Z","shell.execute_reply":"2022-03-27T20:28:10.798827Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def N_bef(DF, col_name, col_val, target_col, drop=False):\n    DF[target_col] = np.full(len(DF),np.nan)\n    for i in DF.index.to_numpy():\n        y,z = next_date_next_val_2(DF,\n                         col_name,\n                         col_val,\n                         current_idx=i)\n        if not pd.isna(y):\n            DF[target_col].loc[i:i] = pd.to_timedelta(y-i).days\n    if drop:\n        DF = DF.drop(col_name,axis=1)\n        \n    pass\n\ndef N_aft(DF, col_name, col_val, target_col,drop=False):\n    DF[target_col] = np.full(len(DF),np.nan)\n    for i in DF.index.to_numpy():\n        y,z = prev_date_prev_val_2(DF,\n                         col_name,\n                         col_val,\n                         current_idx=i)\n        if not pd.isna(y):\n            DF[target_col].loc[i:i] = pd.to_timedelta(i-y).days\n    if drop:\n        DF = DF.drop(col_name,axis=1)\n    pass\n","metadata":{"execution":{"iopub.status.busy":"2022-03-27T20:28:12.356595Z","iopub.execute_input":"2022-03-27T20:28:12.356852Z","iopub.status.idle":"2022-03-27T20:28:12.364778Z","shell.execute_reply.started":"2022-03-27T20:28:12.356824Z","shell.execute_reply":"2022-03-27T20:28:12.363631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"t0 = sp0_1.index[1]\na,b = next_date_next_val_2(DF = sp0_1,\n                             col_name=\"core_event\",\n                             col_val=True,\n                             current_idx=t0)\nprint(a)\nprint('*'*30)\nb\n\nprint((t0-a).days)\nprint('*'*30)\nprint(a-t0)\nprint(type(a-t0))\nprint('*'*30)\n(a-t0).days","metadata":{"execution":{"iopub.status.busy":"2022-03-27T20:28:14.272941Z","iopub.execute_input":"2022-03-27T20:28:14.273382Z","iopub.status.idle":"2022-03-27T20:28:14.286647Z","shell.execute_reply.started":"2022-03-27T20:28:14.273345Z","shell.execute_reply":"2022-03-27T20:28:14.28569Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"N_bef(DF=sp0_1,\n       col_name=\"core_event\",\n       col_val=True,\n       target_col=\"N_bef_core_event\")","metadata":{"execution":{"iopub.status.busy":"2022-03-27T20:28:16.362987Z","iopub.execute_input":"2022-03-27T20:28:16.363479Z","iopub.status.idle":"2022-03-27T20:28:19.410042Z","shell.execute_reply.started":"2022-03-27T20:28:16.363433Z","shell.execute_reply":"2022-03-27T20:28:19.409263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sp0_1","metadata":{"execution":{"iopub.status.busy":"2022-03-27T20:28:19.411441Z","iopub.execute_input":"2022-03-27T20:28:19.411651Z","iopub.status.idle":"2022-03-27T20:28:19.439714Z","shell.execute_reply.started":"2022-03-27T20:28:19.411625Z","shell.execute_reply":"2022-03-27T20:28:19.438842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"__note__: \"sp_pipe_4\" requires lots of computation/time $O(N \\cdot n)$ \n- with $N = N(\\text{index_dates}), n = N(\\text{lst_evs})$ ","metadata":{}},{"cell_type":"code","source":"def sp_pipe_3(DF, lst_evs):\n    sp = DF.copy()\n    \n    ### core_events\n    if False:\n        N_bef(DF=sp,\n                  col_name=\"core_event\",\n                  col_val=True,\n                  target_col=\"N_bef_core_event\")\n\n        N_bef(DF=sp,\n                  col_name=\"core_event\",\n                  col_val=True,\n                  target_col=\"N_aft_core_event\")\n    ### ---\n    ### lst of events\n    cols=[]\n    for ev in lst_evs:\n        cols.append(\"sd_\"+str(ev))\n    \n    for kc in np.arange(len(cols)):\n        N_bef(DF=sp,\n              col_name=cols[kc],\n              col_val=True,\n              target_col=\"N_bef_\"+str(lst_evs[kc]),\n              drop=True)\n        \n        N_aft(DF=sp,\n              col_name=cols[kc],\n              col_val=True,\n              target_col=\"N_aft_\"+str(lst_evs[kc]),\n              drop=True)\n\n    return sp","metadata":{"execution":{"iopub.status.busy":"2022-03-27T20:28:19.746939Z","iopub.execute_input":"2022-03-27T20:28:19.74782Z","iopub.status.idle":"2022-03-27T20:28:19.754428Z","shell.execute_reply.started":"2022-03-27T20:28:19.747777Z","shell.execute_reply":"2022-03-27T20:28:19.753586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def h_f1(x):\n    if pd.isna(x):\n        return 0\n    else:\n        return 1/(1+abs(x)/7) \n\n    \ndef h_f2(x):\n    if pd.isna(x):\n        return 0\n    else:\n        return 1/(1+(abs(x)/7)**2) \n\ndef h_f3(x):\n    if pd.isna(x):\n        return 0\n    else:\n        return 1/(1+(abs(x)/7)**3) \n    \ndef make_x_bef_aft(DF,cols,funct=h_f2):\n    for col in cols:\n        DF[\"x\"+col] = DF[col].apply(funct)\n    \n    pass","metadata":{"execution":{"iopub.status.busy":"2022-03-27T20:28:20.74266Z","iopub.execute_input":"2022-03-27T20:28:20.742953Z","iopub.status.idle":"2022-03-27T20:28:20.750865Z","shell.execute_reply.started":"2022-03-27T20:28:20.742924Z","shell.execute_reply":"2022-03-27T20:28:20.750232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def sp_pipe_4(DF, lst_evs, funct=h_f1):\n    sp = DF.copy()\n    \n    cols_sd = []\n    for kc in np.arange(len(lst_evs)):\n        cols_sd.append(\"sd_\"+str(lst_evs[kc]))\n    \n    sp = sp.copy().drop(cols_sd,axis=1)\n    \n    bef_cols= []\n    for col in sp.columns:\n        s = re.search('^N_bef', col)\n        if not pd.isna(s):\n            bef_cols= np.append(bef_cols,col)\n    aft_cols =[]\n    for col in sp.columns:\n        s = re.search('^N_aft', col)\n        if not pd.isna(s):\n            aft_cols= np.append(aft_cols,col)\n\n    #print(aft_cols)\n    bef_aft_cols = np.append(bef_cols,aft_cols)\n    \n    make_x_bef_aft(sp,bef_aft_cols,funct=funct)\n    \n    sp = sp.copy().drop(bef_aft_cols,axis=1)\n    \n    return sp","metadata":{"execution":{"iopub.status.busy":"2022-03-27T20:28:21.555373Z","iopub.execute_input":"2022-03-27T20:28:21.556207Z","iopub.status.idle":"2022-03-27T20:28:21.563055Z","shell.execute_reply.started":"2022-03-27T20:28:21.556171Z","shell.execute_reply":"2022-03-27T20:28:21.562476Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n## 3.3 ignoring \"oil\" and \"paydays\"-data for now <a class=\"anchor\" id=\"Ch3.3\"></a>\n<a class=\"anchor\" id=\"Ch3.4\"></a>\n[goto (0.) General Workflow](#Ch0)","metadata":{}},{"cell_type":"code","source":"tdf, u_t , u_cols, DF= check_integrity(oil.set_index(\"date\",drop=False))","metadata":{"execution":{"iopub.status.busy":"2022-03-27T20:28:23.453875Z","iopub.execute_input":"2022-03-27T20:28:23.454402Z","iopub.status.idle":"2022-03-27T20:28:23.472526Z","shell.execute_reply.started":"2022-03-27T20:28:23.454354Z","shell.execute_reply":"2022-03-27T20:28:23.471348Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.5 global generation and storage of processed <a class=\"anchor\" id=\"Ch3.5\"></a>\n\n[goto (.0) General Workflow](#Ch0)\n- because it takes a long time to process data for each pair (s,p),\n- generate and store the processed data:\n- compute models on the already processed data","metadata":{}},{"cell_type":"code","source":"def generate_sp_train(ks,kp, funct=h_f1,pipe3=False,pipe2=False, to_csv=True):\n    sp_fname = f'sp_ks{ks}_kp{kp}.csv'\n    \n    s = lst_s[ks]\n    p = lst_p[kp]\n\n    E_s = events_sp_pipe(ks)\n\n    sp_tr= select_sp(DF= train,s=s,p=p)\n    sp_tr= sp_pipe(DF=sp_tr,is_test=False)\n    if pipe2:\n        sp_tr, lst_evs = sp_pipe_2(DF=sp_tr,E_s=E_s)\n        if pipe3:\n            sp_tr = sp_pipe_3(DF=sp_tr, lst_evs=lst_evs)\n            sp_tr = sp_pipe_4(DF=sp_tr, lst_evs=lst_evs, funct=funct)\n\n    if to_csv:\n        sp_tr.to_csv(sp_fname)\n        print (\"processing of 'train'-data done, stored away\")\n        return sp_tr\n    else:\n        return sp_tr\n\ndef generate_sp_test(ks,kp, funct=h_f1,pipe3=False,pipe2=False,to_csv=True):\n    sp_fname = f'spt_ks{ks}_kp{kp}.csv'\n    \n    s = lst_s[ks]\n    p = lst_p[kp]\n\n    E_s = events_sp_pipe(ks)\n\n    sp_tr= select_sp(DF=test,s=s,p=p)\n    sp_tr= sp_pipe(DF=sp_tr,is_test=True)\n    if pipe2:\n        sp_tr, lst_evs = sp_pipe_2(DF=sp_tr,E_s=E_s,is_test=True)\n        if pipe3:\n            sp_tr = sp_pipe_3(DF=sp_tr, lst_evs=lst_evs)\n            sp_tr = sp_pipe_4(DF=sp_tr, lst_evs=lst_evs, funct=funct)\n    \n    if to_csv:\n        sp_tr.to_csv(sp_fname)\n        print (\"processing of 'test'-data done, stored away\")\n        return sp_tr\n    else:\n        return sp_tr\n    ","metadata":{"execution":{"iopub.status.busy":"2022-03-27T20:28:25.52392Z","iopub.execute_input":"2022-03-27T20:28:25.524228Z","iopub.status.idle":"2022-03-27T20:28:25.533092Z","shell.execute_reply.started":"2022-03-27T20:28:25.524194Z","shell.execute_reply":"2022-03-27T20:28:25.532374Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"__ks = order_s\\[0\\]__","metadata":{}},{"cell_type":"code","source":"if True:\n    for ks in order_s[0:1]:\n        for kp in order_p[0:1]:\n            generate_sp_train(ks=ks,kp=kp,pipe3=False)\n            generate_sp_test(ks=ks,kp=kp,pipe3=False)","metadata":{"execution":{"iopub.status.busy":"2022-03-27T20:28:27.266035Z","iopub.execute_input":"2022-03-27T20:28:27.266958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4 Models <a class=\"anchor\" id=\"Ch4\"></a>\n[goto (0.) General Workflow](#Ch0)\n## 4.0 Setup\nreminder of  models to work with: \n\n            - Linear Regression/ridge\n            - Random Forests\n            - xgboost\n            - HistGradientBoost\n            \nnote: evaluate function [from sklearn examples:](https://scikit-learn.org/stable/auto_examples/applications/plot_cyclical_feature_engineering.html?highlight=time)","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import RobustScaler\n\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import RidgeCV\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.ensemble import HistGradientBoostingRegressor\n\n\nfrom sklearn.pipeline import make_pipeline\n\nfrom sklearn.model_selection import TimeSeriesSplit\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.metrics import mean_squared_log_error\n\n\nts_cv = TimeSeriesSplit(\n    n_splits=7,\n    gap=0,\n    max_train_size=1000,\n    test_size=100,\n)\n\n#all_splits = list(ts_cv.split(X, y))\n\ndef evaluate(model,X,y,cv):\n    cv_results = cross_validate(\n        model,\n        X,\n        y,\n        cv=cv,\n        scoring=[\"neg_mean_absolute_error\", \"neg_root_mean_squared_error\",\"neg_mean_squared_log_error\"],\n    )\n    mae = -cv_results[\"test_neg_mean_absolute_error\"]\n    rmse = -cv_results[\"test_neg_root_mean_squared_error\"]\n    msle= -cv_results[\"test_neg_mean_squared_log_error\"]\n    print(\n        f\"Mean Absolute Error:     {mae.mean():.3f} +/- {mae.std():.3f}\\n\"\n        f\"Root Mean Squared Error: {rmse.mean():.3f} +/- {rmse.std():.3f}\\n\"\n        f\"Mean Squared Log Error: {msle.mean():.3f} +/- {msle.std():.3f}\"\n    )\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def rmsle(y_true,y_pred):\n    return np.sqrt((y_true - y_pred).apply(np.log1p).apply(np.square).mean())\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4.1) models contruction: <a class=\"anchor\" id=\"Ch4.1\"></a>\n[goto (0.) General Workflow](#Ch0)\n\n### 4.1a simple models","metadata":{}},{"cell_type":"code","source":"ridge = make_pipeline(RobustScaler(),\n                      RidgeCV())\n\nRF = make_pipeline(RobustScaler(),\n                   RandomForestRegressor(max_depth=5))\n\nXGB = make_pipeline(RobustScaler(),\n                    XGBRegressor(n_estimators=100, learning_rate=0.05, n_jobs=4))\nHGB = make_pipeline(HistGradientBoostingRegressor())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4.1b Compound Models <a class=\"anchor\" id=\"Ch4.1b\"></a>\n\n[goto (0.) General Workflow](#Ch0)\n\n- primarily to decompose into \"trend\" and \"detrended\"","metadata":{}},{"cell_type":"code","source":"def default_selector(X):\n    cols=X.columns\n    cols1 = cols[0:1]\n    cols2 = cols[1:]\n    return [cols1,cols2,[]]\n\nclass CompoundModel:\n    def __init__(self, model_1, model_2, model_3=None,sel_funct=default_selector):\n        self.model_1 = model_1\n        self.model_2 = model_2\n        self.y_columns = None\n        self.model_3 = model_3\n        self.sel_funct = sel_funct\n\ndef CompoundModelFit(self, X, y):\n    \n    mtx_cols = self.sel_funct(X)\n    X_1 = X[mtx_cols[0]]\n    X_2 = X[mtx_cols[1]]\n    X_3 = X[mtx_cols[2]]\n    #print(X_1.shape,X_2.shape,X_3.shape)\n    self.model_1.fit(X_1,y)\n\n    y_fit = pd.DataFrame(\n        self.model_1.predict(X_1), \n        index=X_1.index, columns=y.columns,\n    )\n\n    y_resid = y - y_fit\n    y_resid = y_resid.stack().squeeze() # wide to long\n    if not (X_2.empty):\n        self.model_2.fit(X_2, y_resid)\n        \n        if not (X_3.empty):\n            y_resid2 = y_fit-y_resid\n            y_resid2 = y_resid2.stack().squeeze() # wide to long\n            self.model_3.fit(X_3, y_resid2)\n\n        else:\n            y_resid2 = None\n            #print(\"Note: in 'CompoundModelFit':: X_3 = NaN\")\n    else:\n        print(\"Warning: in 'CompoundModelFit' :: X_2 = NaN, \\n check sel_funct \\n or check cols in X\")\n    \n    # Save column names for predict method\n    self.y_columns = y.columns\n    \ndef CompoundModelPredict(self, X):\n    mtx_cols = self.sel_funct(X)\n    X_1 = X[mtx_cols[0]]\n    X_2 = X[mtx_cols[1]]\n    X_3 = X[mtx_cols[2]]\n    #print(X_1.shape,X_2.shape,X_3.shape)\n    y_pred = pd.DataFrame(\n        self.model_1.predict(X_1),\n        index=X_1.index, columns=self.y_columns,)\n    y_pred = y_pred.stack().squeeze()  # wide to long\n    \n    if not (X_2.empty):\n        y_pred += self.model_2.predict(X_2)\n        #y_pred = y_pred.stack().squeeze()\n    elif (X_2.empty):\n        print('Warning: in \"CompoundModelPredict\" :: X_2 = NaN, \\n check sel_funct \\n or check cols in X')\n    \n    elif not (X_3.empty):\n        y_pred  += self.model_3.predict(X_3)\n    #print(\"Note: in 'CompoundModelPedict':: X_3 = NaN\")\n    return y_pred.unstack()  # long to wide\n\n### ---\n\nCompoundModel.fit = CompoundModelFit\nCompoundModel.predict = CompoundModelPredict\n\ndef X_selector2(X):\n    cols=list(X.columns.to_numpy())\n    K = \"trend\"\n    cols1 = list([\"trend\",])\n    while(K in cols) :\n        cols.remove(K)\n    return list([cols1,cols,[]])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4.2 Evaluation of predictions <a class=\"anchor\" id=\"Ch4.2\"></a>\n[goto (0.) General Workflow](#Ch0)","metadata":{}},{"cell_type":"code","source":"s = lst_s[4]\np = lst_p[7]\n\nsp_tr= select_sp(DF= train,s=s,p=p)\nX_1,y_1 = DataProcess_to_Xy(sp_tr, is_test=False)\n\nsp_te = select_sp(DF=test,s=s,p=p)\nX_1_te ,y_1_te = DataProcess_to_Xy(sp_te, is_test=True)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Ridge\")\nevaluate(model=ridge,X=X_1,y=y_1[\"sales\"],cv=ts_cv)\nprint('*'*40)\nprint(\"RF\")\nevaluate(model=RF,X=X_1,y=y_1[\"sales\"],cv=ts_cv)\nprint('*'*40)\nprint(\"XGB\")\nevaluate(model=XGB,X=X_1,y=y_1[\"sales\"],cv=ts_cv)\nprint('*'*40)\nprint(\"HGB\")\nevaluate(model=HGB,X=X_1,y=y_1[\"sales\"],cv=ts_cv)\nprint('*'*40)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def Make_preds_Y(model,X_tr,y_tr,X_te, plotting=False, verbose=False):\n    model.fit(X_tr,y_tr[[\"sales\"]])\n\n    yp_tr = pd.DataFrame(model.predict(X_tr), index=X_tr.index,columns=[\"sales\"])\n    yp_te = pd.DataFrame(model.predict(X_te), index=X_te.index,columns=[\"sales\"])\n\n    if plotting:\n        make_plot3(df1=y_tr[\"sales\"],df2=yp_tr[\"sales\"],df3=yp_te[\"sales\"],x_axis=\"date\",y_axis=\"sales\",title=False)\n        make_plot3_rol(df1=y_tr[\"sales\"],df2=yp_tr[\"sales\"],df3=yp_te[\"sales\"],x_axis=\"date\",y_axis=\"sales\",title=False)\n\n    if verbose:\n        print(f\"training fit rmsle= {rmsle(y_tr['sales'],yp_tr['sales'])} \\n\")\n        #print( f\"testing fit rmsle= {rmsle(y_te['sales'],yp_0_te)}\")\n    \n    Yp_te = X_te[[\"id\"]].join(pd.DataFrame(yp_te,index=yp_te.index,columns=[\"sales\"]))\n    Yp_tr = X_tr[[\"id\"]].join(pd.DataFrame(yp_tr,index=yp_tr.index,columns=[\"sales\"]))\n    return Yp_te,Yp_tr","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def Compare_True(Yp_te,Yp_tr,Y_true, plotting=False):\n    Y_fit = pd.concat([Yp_tr,Yp_te])\n    r = rmsle(Y_fit['sales'],Y_true['sales'])\n    print(f\"fit rmsle= {r}\")\n    if plotting:\n        make_plot3(df1=Y_true['sales'],df2=Yp_tr['sales'],df3=Yp_te[\"sales\"],x_axis=\"date\",y_axis=\"sales\",title=False)\n        make_plot3_rol(df1=Y_true['sales'],df2=Yp_tr['sales'],df3=Yp_te[\"sales\"],x_axis=\"date\",y_axis=\"sales\",title=False)\n    pass","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4.3 one model pipeline : <a class=\"anchor\" id=\"Ch4.3\"></a>\n\n[goto (0.) General Workflow](#Ch0)","metadata":{}},{"cell_type":"code","source":"def modelize_from_csv(ks,kp, model=XGB, plotting=False,\n             on_train_subset=False, from_work=True):\n    \n    sp_fname=f'sp_ks{ks}_kp{kp}.csv'\n    spt_fname=f'spt_ks{ks}_kp{kp}.csv'\n    ###-------------------------------------\n    \n    if from_work:\n        sp_tr1 = pd.read_csv(work_path/sp_fname,\n                         parse_dates=['date'],\n                         infer_datetime_format=True)\n    else:\n        sp_tr1 = pd.read_csv(comp_dir/sp_fname,\n                         parse_dates=['date'],\n                         infer_datetime_format=True)\n    \n    sp_tr1 = sp_tr1.set_index(\"date\")\n    \n    X_1,y_1 = DataProcess_to_Xy(sp_tr1, is_test=False)\n    \n    if not (on_train_subset is False):\n        \n        X_tr, X_te, y_tr, y_te = train_test_split(X_1, y_1, test_size=on_train_subset,\n                                                      random_state=1, shuffle=False)\n        Yp_sp_te,Yp_sp_tr = Make_preds_Y(model,\n                                X_tr=X_tr,\n                                y_tr=y_tr,\n                                X_te=X_te, \n                                plotting=False)\n        Compare_True(Yp_sp_te , Yp_sp_tr,y_1, plotting=plotting)\n        return Yp_sp_te,Yp_sp_tr\n    \n    else:\n\n        sp_te1 = pd.read_csv(work_path/spt_fname,\n                     parse_dates=['date'],\n                     infer_datetime_format=True)\n    \n        sp_te1 = sp_te1.set_index(\"date\")\n        \n        X_1_te ,y_1_te = DataProcess_to_Xy(sp_te1, is_test=True)\n        Yp_sp_te,Yp_sp_tr = Make_preds_Y(model,\n                                X_tr=X_1,\n                                y_tr=y_1,\n                                X_te=X_1_te, \n                                plotting=plotting)\n        \n        \n        return Yp_sp_te,Yp_sp_tr\n    \n    pass ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def modelize_sp(sp_tr,sp_te=pd.DataFrame(), model=XGB, plotting=False,\n             on_train_subset=False):\n    \n    sp_tr1= sp_tr.copy()\n    \n    #sp_tr1 = sp_tr1.set_index(\"date\")\n    \n    X_1,y_1 = DataProcess_to_Xy(sp_tr1, is_test=False)\n    \n    if not (on_train_subset is False):\n        \n        X_tr, X_te, y_tr, y_te = train_test_split(X_1, y_1, test_size=on_train_subset,\n                                                      random_state=1, shuffle=False)\n        Yp_sp_te,Yp_sp_tr = Make_preds_Y(model,\n                                X_tr=X_tr,\n                                y_tr=y_tr,\n                                X_te=X_te, \n                                plotting=False)\n        Compare_True(Yp_sp_te , Yp_sp_tr,y_1, plotting=plotting)\n        return Yp_sp_te,Yp_sp_tr\n    \n    elif not(sp_te.empty):\n        \n        sp_te1 = sp_te.copy()\n    \n        #sp_te1 = sp_te1.set_index(\"date\")\n        \n        X_1_te ,y_1_te = DataProcess_to_Xy(sp_te1, is_test=True)\n        Yp_sp_te,Yp_sp_tr = Make_preds_Y(model,\n                                X_tr=X_1,\n                                y_tr=y_1,\n                                X_te=X_1_te, \n                                plotting=plotting)\n        \n        \n        return Yp_sp_te,Yp_sp_tr\n    \n    pass ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"__ = modelize_from_csv(ks=order_s[0],kp=order_p[0], model=XGB, plotting=True, on_train_subset=0.15,)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4.4 Compound model analysis <a class=\"anchor\" id=\"Ch4.4\"></a>","metadata":{}},{"cell_type":"code","source":"### Debugging; please ignore\n### debug both \"modelize\" and \"generate\"\nif False:\n    ks0 = order_s[0]\n    kp0 = order_p[0]\n    \n    sp_fname = f'sp_ks{ks0}_kp{kp0}.csv'\n    DO_GEN = False\n    \n    ### ----------------------------------------\n    if DO_GEN:\n        s = lst_s[ks0]\n        p = lst_p[kp0]\n\n        E_s = events_sp_pipe(ks0)\n\n        sp_tr= select_sp(DF= train,s=s,p=p)\n        sp_tr= sp_pipe(DF=sp_tr,is_test=False)\n\n        sp_tr, lst_evs = sp_pipe_2(DF=sp_tr,E_s=E_s)\n\n        sp_tr = sp_pipe_3(DF=sp_tr, lst_evs=lst_evs)\n\n        sp_tr = sp_pipe_4(DF=sp_tr, lst_evs=lst_evs, funct=h_f1)\n\n\n        sp_tr.to_csv(sp_fname)\n    ### ----------------------------------------\n    \n    sp_tr1 = pd.read_csv(work_path/sp_fname,\n                         parse_dates=['date'],\n                         infer_datetime_format=True)\n\n    sp_tr1 = sp_tr1.set_index(\"date\")\n    X_1,y_1 = DataProcess_to_Xy(sp_tr1, is_test=False)\n\n    all_splits = list(ts_cv.split(X_1, y_1))\n    k = 4\n    X_tr = X_1.iloc[all_splits[k][0]]\n    X_te = X_1.iloc[all_splits[k][1]]\n\n    y_tr = y_1.iloc[all_splits[k][0]]\n    y_te = y_1.iloc[all_splits[k][1]]\n\n    model_0 = HGB\n    Yp_te,Yp_tr=Make_preds_Y(model= model_0,\n                 X_tr=X_tr,\n                 y_tr=y_tr,\n                 X_te=X_te, \n                 plotting=False,verbose=True)\n    Compare_True(Yp_te , Yp_tr,y_1,plotting=True)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CustomModel = CompoundModel(ridge,HGB,sel_funct=X_selector2)\n\n__ = modelize_from_csv(ks=order_s[0],kp=order_p[0], model=CustomModel, plotting=True, on_train_subset=0.15)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def CompoundPred_1(self, X):\n    mtx_cols = self.sel_funct(X)\n    X_1 = X[mtx_cols[0]]\n    X_2 = X[mtx_cols[1]]\n    X_3 = X[mtx_cols[2]]\n    #print(X_1.shape,X_2.shape,X_3.shape)\n    y_pred = pd.DataFrame(\n        self.model_1.predict(X_1),\n        index=X_1.index, columns=self.y_columns,\n    )\n    y_pred = y_pred#.stack().squeeze()  # wide to long\n    \n    \n    return y_pred\n\nCompoundModel.pred_1 = CompoundPred_1\n    \ndef CompoundPred_2(self, X):\n    mtx_cols = self.sel_funct(X)\n    X_1 = X[mtx_cols[0]]\n    X_2 = X[mtx_cols[1]]\n    X_3 = X[mtx_cols[2]]\n    #print(X_1.shape,X_2.shape,X_3.shape)\n    y_pred = pd.DataFrame(\n        self.model_2.predict(X_2),\n        index=X_2.index, columns=self.y_columns,\n    )\n    y_pred = y_pred#.stack().squeeze()  # wide to long\n\n    return y_pred\n\nCompoundModel.pred_2 = CompoundPred_2\n\ndef show_decomposition(self,X,Y_true, on_train_subset=0.15):\n    plotting=True\n    \n    X_tr, X_te, y_tr, y_te = train_test_split(X, Y_true, test_size=on_train_subset,\n                                                      random_state=1, shuffle=False)\n### global compound performance\n    yp_trG = self.predict(X_tr)\n    yp_teG = self.predict(X_te)\n    Y_fitG = pd.concat([yp_trG,yp_teG])\n    print(f\"fit rmsle global predict = {rmsle(Y_fitG['sales'],Y_true['sales'])}\")\n    \n### trend graph:\n    yp_tr = self.pred_1(X_tr)\n    yp_te = self.pred_1(X_te)\n    #print(yp_tr.head())\n    \n    Y_fit = pd.concat([yp_tr,yp_te])\n    print(f\"fit rmsle pred_1 = {rmsle(Y_fit['sales'],Y_true['sales'])}\")\n    #print(f\"fit rmsle= {rmsle(Y_fit[['sales']],Y_true[['sales']])}\")\n    if plotting:\n        make_plot3(df1=Y_true['sales'],df2=yp_tr['sales'],df3=yp_te[\"sales\"],\n                   x_axis=\"date\",y_axis=\"sales\",title=False, line=3)\n        \n        make_plot3_rol(df1=Y_true['sales'],df2=yp_tr['sales'],df3=yp_te[\"sales\"],\n                       x_axis=\"date\",y_axis=\"sales\",title=False, line=5)\n### off-trend graph\n\n    Y_true_2 = Y_true-self.pred_1(X)\n\n    yp_tr2 = self.pred_2(X_tr)\n    yp_te2 = self.pred_2(X_te)\n    \n    Y_fit2 = pd.concat([yp_tr2,yp_te2])\n    print(f\"fit rmsle pred_2 = {rmsle(Y_fit['sales'],Y_true['sales'])}\")\n    if plotting:\n        make_plot3(df1=Y_true_2['sales'],df2=yp_tr2['sales'],df3=yp_te2[\"sales\"],\n                   x_axis=\"date\",y_axis=\"detrended sales\",title=False, line=1)\n        \n        make_plot3_rol(df1=Y_true_2['sales'],df2=yp_tr2['sales'],df3=yp_te2[\"sales\"],\n                       x_axis=\"date\",y_axis=\"detrended sales\",title=False, line=1)\n    \n    pass\n\nCompoundModel.show_decomp = show_decomposition","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from_work =True\nif True:\n    sp_fname=f'sp_ks{ks}_kp{kp}.csv'\n    if from_work:\n        sp_tr1 = pd.read_csv(work_path/sp_fname,\n                         parse_dates=['date'],\n                         infer_datetime_format=True)\n    else:\n        sp_tr1 = pd.read_csv(comp_dir/sp_fname,\n                         parse_dates=['date'],\n                         infer_datetime_format=True)\n        \n    sp_tr1 = sp_tr1.set_index(\"date\")\n    \n    X_1,y_1 = DataProcess_to_Xy(sp_tr1, is_test=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def Show_decomp(model,ks,kp,on_train_subset=0.15,from_work=True):\n    sp_fname=f'sp_ks{ks}_kp{kp}.csv'\n    if from_work:\n        sp_tr1 = pd.read_csv(work_path/sp_fname,\n                         parse_dates=['date'],\n                         infer_datetime_format=True)\n    else:\n        sp_tr1 = pd.read_csv(comp_dir/sp_fname,\n                         parse_dates=['date'],\n                         infer_datetime_format=True)\n        \n    sp_tr1 = sp_tr1.set_index(\"date\")\n    \n    X_1,y_1 = DataProcess_to_Xy(sp_tr1, is_test=False)\n    \n    return CustomModel.show_decomp(X=X_1,Y_true=y_1,on_train_subset=on_train_subset)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Show_decomp(model=CustomModel,ks=order_s[0],kp=order_p[0],on_train_subset=0.15,from_work=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5 Global submission <a class=\"anchor\" id=\"Ch5\"></a>\n[goto (0.) General Workflow](#Ch0)","metadata":{}},{"cell_type":"code","source":"def submit_from_csv(ks=ks,kp=kp,model=CustomModel):\n    \n    Yp_sp_te,_ = modelize_from_csv(ks=ks,kp=kp, model=model, plotting=False, on_train_subset=False, from_work=True)\n    Yp_sp_te = from_log_sales(Yp_sp_te)\n    Yp_sp_te.to_csv('submission.csv', mode='a', index=False, header=False)\n    pass","metadata":{"execution":{"iopub.status.busy":"2022-03-28T10:21:09.861302Z","iopub.execute_input":"2022-03-28T10:21:09.861582Z","iopub.status.idle":"2022-03-28T10:21:09.935308Z","shell.execute_reply.started":"2022-03-28T10:21:09.861554Z","shell.execute_reply":"2022-03-28T10:21:09.934182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def submit_sp(sp_tr,sp_te,model=XGB,):\n    if (sp_tr.empty) or (sp_te.empty):\n        print(f'WARNING: sp_tr/te is empty')\n    else:\n        Yp_sp_te,_ = modelize_sp(model=model,sp_tr=sp_tr,sp_te=sp_te, plotting=False, on_train_subset=False)\n        Yp_sp_te.to_csv('submission.csv', mode='a', index=False, header=False)\n    pass","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"to_csv=False\n\nif True:\n    for ks in order_s:\n        for kp in order_p:\n            \n            sp_tr = generate_sp_train(ks=ks,kp=kp,pipe3=False,pipe2=False,to_csv=to_csv)\n            sp_te = generate_sp_test(ks=ks,kp=kp,pipe3=False,pipe2=False,to_csv=to_csv)\n            \n            if to_csv:\n                submit_from_csv(ks=ks,kp=kp,model=CustomModel)\n            else:\n                submit_sp(model=CustomModel,sp_tr=sp_tr,sp_te=sp_te)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### NOTE: DEPRECATED\n\ndef complete_pipe(ks,kp,model=ridge,plotting=False, on_train_subset=False):\n    if False:\n        s = lst_s[ks]\n        p = lst_p[kp]\n\n        E_s = events_sp_pipe(ks=ks)\n\n        sp_tr= select_sp(DF= train,s=s,p=p)\n        sp_tr = sp_pipe(sp_tr)\n        sp_tr = sp_pipe_2(sp_tr,E_s)\n        X_1,y_1 = DataProcess_to_Xy(sp_tr, is_test=False)\n\n        sp_te = select_sp(DF=test,s=s,p=p)\n        sp_te = sp_pipe(sp_te,is_test=True)\n        #sp_te = sp_pipe_2(sp_te,E_s)\n        X_1_te ,y_1_te = DataProcess_to_Xy(sp_te, is_test=True)\n\n        if on_train_subset is False:\n            Yp_sp_te,Yp_sp_tr = Make_preds_Y(model,\n                                    X_tr=X_1,\n                                    y_tr=y_1,\n                                    X_te=X_1_te, \n                                    plotting=plotting)\n        else:\n            X_tr, X_te, y_tr, y_te = train_test_split(X_1, y_1, test_size=on_train_subset,\n                                                          random_state=1, shuffle=False)\n            Yp_sp_te,Yp_sp_tr = Make_preds_Y(model,\n                                    X_tr=X_tr,\n                                    y_tr=y_tr,\n                                    X_te=X_te, \n                                    plotting=False)\n            Compare_True(Yp_sp_te , Yp_sp_tr,y_1, plotting=plotting)\n        return Yp_sp_te,Yp_sp_tr\n    return print(\"function deprecated\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"__=complete_pipe(ks=order_s[0],kp=order_p[0],model=XGB,plotting=True,on_train_subset=0.1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#submission.to_csv('submission.csv',mode='w',  index=False, header=False)\n\nmodel_loop = XGB\nif False:\n    for ks in np.arange(len(lst_s[0:2])):\n        for kp in np.arange(len(lst_p[0:2])):\n            Yp_sp_te,_ = complete_pipe(ks=ks,kp=kp,model=model_loop,plotting=False)\n            Yp_sp_te.to_csv('submission.csv', mode='a', index=False, header=False)\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Fin <a class=\"anchor\" id=\"End\"></a>\n\n[to (0.) General Workflow](#Ch0)","metadata":{}}]}