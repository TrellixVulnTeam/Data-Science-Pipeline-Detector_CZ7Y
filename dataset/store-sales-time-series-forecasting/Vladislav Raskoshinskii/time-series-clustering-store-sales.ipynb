{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Intro\nIn this notebook, we are going to explore Time Series Clustering Task and techniques/methods that can be applied. I'd like to share my experience and demonstrate an approach to the task. Let's dive in!\n\n### Content \n- <a href='#1'>1. Data Description</a>\n- <a href='#2'>2. Dealing with Missing Data</a>\n- <a href='#3'>3. Time Series Feature Extraction</a>\n- <a href='#4'>4. Clustering Methods</a>\n    - <a href='#4.1'>4.1 Time Series Smoothing</a>\n    - <a href='#4.2'>4.2 Time Series Scaling</a>\n    - <a href='#4.3'>4.3 Time Series K-Means</a>\n    - <a href='#4.4'>4.4 Downsizing Feature Space</a>\n        - <a href='#4.4.1'>4.4.1 t-SNE</a>\n        - <a href='#4.4.2'>4.4.2 MultiDimensional Scaling (MDS)</a>\n    - <a href='#4.5'>4.5 Hierarchical Agglomerative Clustering (HAC)</a>\n    - <a href='#4.6'>4.6 Time Series KMeans Results</a>\n- <a href='#5'>5. Cluster Series Extraction</a>\n    - <a href='#5.1'>5.1 Cluster Series DBA</a>\n- <a href='#6'>6. Time Series Embeddings</a>\n- <a href='#7'>7. References</a>","metadata":{}},{"cell_type":"code","source":"# Some libraries installation\n! git clone https://github.com/tejaslodaya/timeseries-clustering-vae.git\n! pip install tslearn\n! pip uninstall scikit-learn --yes \n! pip install scikit-learn==0.24.1","metadata":{"execution":{"iopub.status.busy":"2022-02-21T10:22:09.380981Z","iopub.execute_input":"2022-02-21T10:22:09.381482Z","iopub.status.idle":"2022-02-21T10:22:32.693501Z","shell.execute_reply.started":"2022-02-21T10:22:09.381339Z","shell.execute_reply":"2022-02-21T10:22:32.69259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nimport os\n\nfrom tslearn.clustering import TimeSeriesKMeans\nfrom tslearn.barycenters import dtw_barycenter_averaging\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.manifold import TSNE, MDS\nfrom sklearn.cluster import AgglomerativeClustering\n\nfrom scipy.cluster.hierarchy import dendrogram\nfrom tqdm.autonotebook import tqdm\n\nwarnings.filterwarnings(\"ignore\")\nsns.set_style(\"darkgrid\")\n\nSEED=23","metadata":{"execution":{"iopub.status.busy":"2022-02-21T10:23:05.793241Z","iopub.execute_input":"2022-02-21T10:23:05.793952Z","iopub.status.idle":"2022-02-21T10:23:06.950043Z","shell.execute_reply.started":"2022-02-21T10:23:05.793905Z","shell.execute_reply":"2022-02-21T10:23:06.948647Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <a id='1'>1. Data Description</a>\n\nSo in total, we have 54 different stores where 33 different product categories are sold. A typical task that can be formulated in retail: \"Sales prediction for each product category or even certain product in a certain store\"\n\nAccording to the data we have, we can solve several Time Series Clustering Tasks:\n- Cluster product categories\n- Cluster stores \n\nAll these can help to:\n- Find similar patterns in/between categories/stores \n- Reduce the number of models to be trained (cluster models)\n- ...\n\n**We are going to find similar stores according to sales feature**","metadata":{}},{"cell_type":"code","source":"# Data Reading \ndata = pd.read_csv('../input/store-sales-time-series-forecasting/train.csv', parse_dates=['date'])\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-21T10:23:07.457924Z","iopub.execute_input":"2022-02-21T10:23:07.45822Z","iopub.status.idle":"2022-02-21T10:23:10.23188Z","shell.execute_reply.started":"2022-02-21T10:23:07.458189Z","shell.execute_reply":"2022-02-21T10:23:10.230858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"First of all let's have a look at the number of unique stores and unique products that are sold","metadata":{}},{"cell_type":"code","source":"print('Number of Unique Stores: ', data['store_nbr'].unique().shape[0])\nprint('Unique Product Categories: ', data['family'].unique().shape[0])","metadata":{"execution":{"iopub.status.busy":"2022-02-21T10:23:10.233327Z","iopub.execute_input":"2022-02-21T10:23:10.233581Z","iopub.status.idle":"2022-02-21T10:23:10.49207Z","shell.execute_reply.started":"2022-02-21T10:23:10.23355Z","shell.execute_reply":"2022-02-21T10:23:10.491143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Make sure that each store has all these unique product categories\nassert data.groupby(['store_nbr', 'family']).size().shape[0] == 54*33","metadata":{"execution":{"iopub.status.busy":"2022-02-21T10:23:10.49311Z","iopub.execute_input":"2022-02-21T10:23:10.493327Z","iopub.status.idle":"2022-02-21T10:23:10.93347Z","shell.execute_reply.started":"2022-02-21T10:23:10.493301Z","shell.execute_reply":"2022-02-21T10:23:10.932695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Time Series data for finding similar stores\nstore_sales = data.groupby(['date', 'store_nbr'], as_index=False).aggregate({'sales': 'sum'})\n\n# Time Series data for finding similar product categories\nproduct_sales = data.groupby(['date', 'store_nbr', 'family'], as_index=False).aggregate({'sales': 'sum'})","metadata":{"execution":{"iopub.status.busy":"2022-02-21T10:23:10.935216Z","iopub.execute_input":"2022-02-21T10:23:10.935476Z","iopub.status.idle":"2022-02-21T10:23:13.206831Z","shell.execute_reply.started":"2022-02-21T10:23:10.935431Z","shell.execute_reply":"2022-02-21T10:23:13.205775Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's find similar stores by sales feature. First, we have to make sure that the data is correct and we have no missing values\n\n### <a id='2'>2. Dealing wtih Missing Data</a>","metadata":{}},{"cell_type":"code","source":"# Check if we have stores with zero sales \nstores_to_drop = []\nfor store in store_sales['store_nbr'].unique():\n    if (store_sales.query(f'store_nbr == {store}')['sales'] == 0).all():\n        stores_to_drop.append(store)\n        \nprint('Stores with no sales: ', stores_to_drop)\n\n# Exlude these stores\nstore_sales = store_sales[~store_sales['store_nbr'].isin(stores_to_drop)]","metadata":{"execution":{"iopub.status.busy":"2022-02-21T10:23:13.20811Z","iopub.execute_input":"2022-02-21T10:23:13.208358Z","iopub.status.idle":"2022-02-21T10:23:13.375079Z","shell.execute_reply.started":"2022-02-21T10:23:13.208328Z","shell.execute_reply":"2022-02-21T10:23:13.374495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's have a look at 5 different stores \nrand_stores = np.random.choice(store_sales['store_nbr'].unique(), 3)\nselected_stores = store_sales[store_sales['store_nbr'].isin(rand_stores)][['date', 'store_nbr', 'sales']]\n\nplt.figure(figsize=(20,10))\nplt.grid(True)\nsns.lineplot(data=selected_stores, x='date', y='sales', hue='store_nbr', ci=False, legend=True, palette='Set1');","metadata":{"execution":{"iopub.status.busy":"2022-02-21T10:23:13.376403Z","iopub.execute_input":"2022-02-21T10:23:13.376897Z","iopub.status.idle":"2022-02-21T10:23:14.208557Z","shell.execute_reply.started":"2022-02-21T10:23:13.376856Z","shell.execute_reply":"2022-02-21T10:23:14.197409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are missing values. It will affect the clustering and the results might be biased due to the noisy data\n\nLet's apply a simple heuristic: threshold: 0.3% of missing data (everything above - drop)","metadata":{}},{"cell_type":"code","source":"# Find noisy series \nstore_to_drop = []\nfor store in store_sales['store_nbr'].unique():\n    current_store = store_sales.query(f'store_nbr == {store}')\n    if current_store.query('sales == 0').shape[0]/current_store.shape[0] > 0.3:\n        store_to_drop.append(store)\n        \nprint('Noisy Stores: ', store_to_drop)\n\n# Drop noisy stores\nstore_sales = store_sales[~store_sales['store_nbr'].isin(store_to_drop)]","metadata":{"execution":{"iopub.status.busy":"2022-02-21T10:23:14.209877Z","iopub.execute_input":"2022-02-21T10:23:14.210689Z","iopub.status.idle":"2022-02-21T10:23:14.457774Z","shell.execute_reply.started":"2022-02-21T10:23:14.210647Z","shell.execute_reply":"2022-02-21T10:23:14.456702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Besides we have to fill missing values","metadata":{}},{"cell_type":"code","source":"# First convert zero values to NaN\nstore_sales['sales'] = store_sales['sales'].apply(lambda x: np.nan if x == 0 else x)\n\n# Fill missing values \nres_df = pd.DataFrame()\nfor store in store_sales['store_nbr'].unique():\n    current_store = store_sales.query(f'store_nbr == {store}')\n    # Use interpolation to fill values between dates\n    current_store['sales'].interpolate(method ='linear', limit_direction ='forward', inplace=True)\n    # Some stores probably were closed on 01-01-YYYYY, fill it by the value on the next day\n    current_store['sales'].fillna(method ='bfill', inplace=True)\n    res_df = res_df.append(current_store)\n    \nstore_sales = res_df","metadata":{"execution":{"iopub.status.busy":"2022-02-21T10:23:14.45985Z","iopub.execute_input":"2022-02-21T10:23:14.460064Z","iopub.status.idle":"2022-02-21T10:23:14.706256Z","shell.execute_reply.started":"2022-02-21T10:23:14.460037Z","shell.execute_reply":"2022-02-21T10:23:14.704989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's have a look at 5 different stores \nrand_stores = np.random.choice(store_sales['store_nbr'].unique(), 3)\nselected_stores = store_sales[store_sales['store_nbr'].isin(rand_stores)][['date', 'store_nbr', 'sales']]\n\nplt.figure(figsize=(20,10))\nplt.grid(True)\nsns.lineplot(data=selected_stores, x='date', y='sales', hue='store_nbr', ci=False, legend=True, palette='Set1');","metadata":{"execution":{"iopub.status.busy":"2022-02-21T10:23:14.707365Z","iopub.execute_input":"2022-02-21T10:23:14.707642Z","iopub.status.idle":"2022-02-21T10:23:15.555594Z","shell.execute_reply.started":"2022-02-21T10:23:14.707606Z","shell.execute_reply":"2022-02-21T10:23:15.554651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <a id='3'>3. Time Series Feature Extraction</a>\n\nIn general, time series clustering can be divided into 2 types:\n- **Feature-Based approach**: we try to extract everything possible from the signal/time series (feature extraction)\n- **Raw data-Based approach**: directly applied to time series vectors without any spatial transformations\n\nIn this notebook, we are going to use **Raw-data Based approach**. It means that we will have a matrix of features where:\n- Rows: Different Time Series (stores in our case)\n- Features: Time Observations (sales)\n\nIn this case, we will be clustering in a very high dimensional space and will most likely run into a problem known as the **Curse of Dimensionality**. As a result, obtained clusters may have sparse shapes, overlap with other clusters and so on.\n\nTo prevent this, we will need to use **dimensionality reduction methods** (t-SNE, PCA, MDS...)\n\n### <a id='4'>4. Clustering Methods</a>\n\nWe will focus on the following clustering methods:\n- `K-Means/TimeSeriesKMeans: (ts_learn library)`\n- `Hierarchical Agglomerative Clustering`\n\nBut any known clustering algorithm can be applied\n\n**If the time series is a signal** (data from various devices), then the best way to extract features would be methods from the `signal processing` area\n\nFor example, Fourier transformation for finding different frequencies, spectrograms and wavelet transformations\n\n**If the series is noisy, then it would be nice to smooth it first** (various smoothing methods) so as not to find false patterns\n\n### <a id='4.1'>4.1 Time Series Smoothing</a>\nNice, we don't have missing values **but the series is still looking noisy**. Let's apply moving average (window size = 7: weekly trend )","metadata":{}},{"cell_type":"code","source":"# Time Series Smoothing \nres_df = pd.DataFrame()\nfor store in store_sales['store_nbr'].unique():\n    current_store = store_sales.query(f'store_nbr == {store}')\n    current_store['smoth_7'] = current_store['sales'].rolling(7).mean()\n    res_df = res_df.append(current_store[['date', 'store_nbr', 'smoth_7']])\n    \nstore_sales = res_df\nstore_sales = store_sales.dropna()","metadata":{"execution":{"iopub.status.busy":"2022-02-21T10:23:15.556844Z","iopub.execute_input":"2022-02-21T10:23:15.557103Z","iopub.status.idle":"2022-02-21T10:23:15.810836Z","shell.execute_reply.started":"2022-02-21T10:23:15.557074Z","shell.execute_reply":"2022-02-21T10:23:15.810106Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's have a look at 3 different stores \nrand_stores = np.random.choice(store_sales['store_nbr'].unique(), 3)\nselected_stores = store_sales[store_sales['store_nbr'].isin(rand_stores)][['date', 'store_nbr', 'smoth_7']]\n\nplt.figure(figsize=(20,10))\nplt.grid(True)\nsns.lineplot(data=selected_stores, x='date', y='smoth_7', hue='store_nbr', ci=False, legend=True, palette='Set1');","metadata":{"execution":{"iopub.status.busy":"2022-02-21T10:23:15.81249Z","iopub.execute_input":"2022-02-21T10:23:15.812808Z","iopub.status.idle":"2022-02-21T10:23:16.543889Z","shell.execute_reply.started":"2022-02-21T10:23:15.812765Z","shell.execute_reply":"2022-02-21T10:23:16.542836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After smoothing we can get more insights about the series as well as define similarities between them\n\nInitial preprocessing has been done and we can create the main feature matrix  ","metadata":{}},{"cell_type":"code","source":"# Feature matrix with shape (n_series x time_observations)\nseries_df = store_sales.pivot(index='store_nbr', columns='date', values='smoth_7')\nseries_df = series_df.dropna(axis='columns')\nseries_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-21T10:23:16.546375Z","iopub.execute_input":"2022-02-21T10:23:16.546805Z","iopub.status.idle":"2022-02-21T10:23:16.60133Z","shell.execute_reply.started":"2022-02-21T10:23:16.546768Z","shell.execute_reply":"2022-02-21T10:23:16.600503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <a id='4.2'>4.2 Time Series Scaling</a>\nScaling must be applied to each series independently","metadata":{}},{"cell_type":"code","source":"# Scaling\nscaler = StandardScaler()\n\n# First transposition - to have series in columns (allows scaling each series independently)\n# Second Transposition - come back to initial feature matrix shape (n_series x time_observations)\nscaler = StandardScaler()\nscaled_ts = scaler.fit_transform(series_df.T).T ","metadata":{"execution":{"iopub.status.busy":"2022-02-21T10:23:16.602779Z","iopub.execute_input":"2022-02-21T10:23:16.603285Z","iopub.status.idle":"2022-02-21T10:23:16.613889Z","shell.execute_reply.started":"2022-02-21T10:23:16.603237Z","shell.execute_reply":"2022-02-21T10:23:16.612994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <a id='4.3'>4.3 Time Series K-Means</a>\n\nWhen using `K-Means` clustering, it is better to use the **Feature-Based Approach**. We extract a bunch of features from the series and hope that they will describe the time series well then perform clustering. I'd like to demonstrate **out of the box solution** (Raw-Data Approach). For Feature-Based Approach, you have to get features for each series and group them using any clustering algorithm. These libraries can help: \n- <a href='https://github.com/fraunhoferportugal/tsfel'>ts_fel</a>\n- <a href='https://github.com/blue-yonder/tsfresh'>ts_fresh</a>\n\nIt is important how we define the similarity between observations in a feature space. When using KMeans we can use:\n- `Euclidean distance` \n- `Dynamic Time Warping Matching (DTW)`\n\n\nWhen using <a href='https://tslearn.readthedocs.io/en/stable/user_guide/dtw.html'> Dynamic Time Warping Matching </a> the **Feature-Based approach is not suitable**, since we are trying to determine a measure of the similarity of the series (how they overlap, peaks size/similarity/location...)\n\nFor `DTW` better downsample the series using `resampling` (i.e. change the frequency of the series). For example, instead of minute observations/ticks, take 5/10/15 minute ones but we have to keep in mind that the main patterns (peaks, fluctuations) fall into this interval. It allows keeping the series structure, making it shorter and therefore much faster to identify similar series with `DTW`\n\n\nFirst, apply KMeans algorithm from <a href='https://tslearn.readthedocs.io/en/stable/index.html'>ts_learn library</a>","metadata":{}},{"cell_type":"code","source":"# Run KMeans and plot the results \ndef get_kmeans_results(data, max_clusters=10, metric='euclidean', seed=23):\n    \"\"\"\n    Runs KMeans n times (according to max_cluster range)\n\n    data: pd.DataFrame or np.array\n        Time Series Data\n    max_clusters: int\n        Number of different clusters for KMeans algorithm\n    metric: str\n        Distance metric between the observations\n    seed: int\n        random seed\n    Returns: \n    -------\n    None      \n    \"\"\"\n    # Main metrics\n    distortions = []\n    silhouette = []\n    clusters_range = range(1, max_clusters+1)\n    \n    for K in tqdm(clusters_range):\n        kmeans_model = TimeSeriesKMeans(n_clusters=K, metric=metric, n_jobs=-1, max_iter=10, random_state=seed)\n        kmeans_model.fit(data)\n        distortions.append(kmeans_model.inertia_)\n        if K > 1:\n            silhouette.append(silhouette_score(data, kmeans_model.labels_))\n        \n    # Visualization\n    plt.figure(figsize=(10,4))\n    plt.plot(clusters_range, distortions, 'bx-')\n    plt.xlabel('k')\n    plt.ylabel('Distortion')\n    plt.title('Elbow Method')\n    \n    plt.figure(figsize=(10,4))\n    plt.plot(clusters_range[1:], silhouette, 'bx-')\n    plt.xlabel('k')\n    plt.ylabel('Silhouette score')\n    plt.title('Silhouette');","metadata":{"execution":{"iopub.status.busy":"2022-02-21T10:23:18.126625Z","iopub.execute_input":"2022-02-21T10:23:18.126925Z","iopub.status.idle":"2022-02-21T10:23:18.137054Z","shell.execute_reply.started":"2022-02-21T10:23:18.12689Z","shell.execute_reply":"2022-02-21T10:23:18.135982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's try finding similar series using DTW metric","metadata":{}},{"cell_type":"code","source":"%%time\n\n# Run the algorithm using DTW algorithm \nget_kmeans_results(data=scaled_ts, max_clusters=5, metric='dtw', seed=SEED)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Well, we can hardly say anything according to Elbow Method but Silhouette says that 2 clusters are good\n\nLet's have a look at obtained clusters ","metadata":{}},{"cell_type":"code","source":"# Visualization for obtained clusters   \ndef plot_clusters(data, cluster_model, dim_red_algo):\n    \"\"\"\n    Plots clusters obtained by clustering model \n\n    data: pd.DataFrame or np.array\n        Time Series Data\n    cluster_model: Class\n        Clustering algorithm \n    dim_red_algo: Class\n        Dimensionality reduction algorithm (e.g. TSNE/PCA/MDS...) \n    Returns:\n    -------\n    None\n    \"\"\"\n    cluster_labels = cluster_model.fit_predict(data)\n    centroids = cluster_model.cluster_centers_\n    u_labels = np.unique(cluster_labels)\n    \n    # Centroids Visualization\n    plt.figure(figsize=(16, 10))\n    plt.scatter(centroids[:, 0] , centroids[:, 1] , s=150, color='r', marker=\"x\")\n    \n    # Downsize the data into 2D\n    if data.shape[1] > 2:\n        data_2d = dim_red_algo.fit_transform(data)\n        for u_label in u_labels:\n            cluster_points = data[(cluster_labels == u_label)]\n            plt.scatter(cluster_points[:, 0], cluster_points[:, 1], label=u_label)\n    else:\n        for u_label in u_labels:\n            cluster_points = data[(cluster_labels == u_label)]\n            plt.scatter(cluster_points[:, 0], cluster_points[:, 1], label=u_label)\n\n    plt.title('Clustered Data')\n    plt.xlabel(\"Feature space for the 1st feature\")\n    plt.ylabel(\"Feature space for the 2nd feature\")\n    plt.grid(True)\n    plt.legend(title='Cluster Labels');","metadata":{"execution":{"iopub.status.busy":"2022-02-21T10:23:20.053894Z","iopub.execute_input":"2022-02-21T10:23:20.054524Z","iopub.status.idle":"2022-02-21T10:23:20.064762Z","shell.execute_reply.started":"2022-02-21T10:23:20.054475Z","shell.execute_reply":"2022-02-21T10:23:20.063754Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\n# let's look at the cluster shape\nmodel = TimeSeriesKMeans(n_clusters=2, metric='dtw', n_jobs=-1, max_iter=10, random_state=SEED)\n\nplot_clusters(data=scaled_ts,\n              cluster_model=model,\n              dim_red_algo=TSNE(n_components=2, init='pca', random_state=SEED))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Clusters overlap and cluster number 2 looks like a noise","metadata":{}},{"cell_type":"code","source":"# let's compare with the euclidean metric\nget_kmeans_results(data=scaled_ts, max_clusters=5, metric='euclidean', seed=SEED)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The results are much worse in comparison with `DTW` algorithm. Let's try downsizing the features\n\n### <a id='4.4'>4.4 Downsizing Feature Space</a> \n\nLet's apply dimensionality reduction methods (t-SNE, MDS, VRAE...)\n\n### <a id='4.4.1'>4.4.1 t-SNE</a> ","metadata":{}},{"cell_type":"code","source":"# Downsize the features into 2D\ntsne = TSNE(n_components=2, init='pca', random_state=SEED)\ndata_tsne = tsne.fit_transform(scaled_ts)\n\nget_kmeans_results(data=data_tsne, max_clusters=10, metric='euclidean', seed=SEED)","metadata":{"execution":{"iopub.status.busy":"2022-02-21T10:23:24.186104Z","iopub.execute_input":"2022-02-21T10:23:24.186907Z","iopub.status.idle":"2022-02-21T10:23:25.387728Z","shell.execute_reply.started":"2022-02-21T10:23:24.186855Z","shell.execute_reply":"2022-02-21T10:23:25.386896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# let's look at the cluster shape\nmodel = TimeSeriesKMeans(n_clusters=2, metric='euclidean', n_jobs=-1, max_iter=10, random_state=SEED)\n\nplot_clusters(data=data_tsne,\n              cluster_model=model,\n              dim_red_algo=TSNE(n_components=2, init='pca', random_state=SEED))","metadata":{"execution":{"iopub.status.busy":"2022-02-21T10:23:25.389523Z","iopub.execute_input":"2022-02-21T10:23:25.389762Z","iopub.status.idle":"2022-02-21T10:23:25.828323Z","shell.execute_reply.started":"2022-02-21T10:23:25.389731Z","shell.execute_reply":"2022-02-21T10:23:25.82735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Cluster shape is relatively good, observations don't overlap but are a bit sparse\n\n### <a id='4.4.2'>4.4.2 MultiDimensional Scaling (MDS)</a> ","metadata":{}},{"cell_type":"code","source":"mds = MDS(n_components=2, n_init=3, max_iter=100, random_state=SEED)\ndata_mds = mds.fit_transform(scaled_ts) \n\nget_kmeans_results(data=data_mds, max_clusters=10, metric='euclidean', seed=SEED)","metadata":{"execution":{"iopub.status.busy":"2022-02-21T10:23:26.629373Z","iopub.execute_input":"2022-02-21T10:23:26.630227Z","iopub.status.idle":"2022-02-21T10:23:27.861479Z","shell.execute_reply.started":"2022-02-21T10:23:26.630182Z","shell.execute_reply":"2022-02-21T10:23:27.860491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# let's look at the cluster shape\nmodel = TimeSeriesKMeans(n_clusters=2, metric='euclidean', n_jobs=-1, max_iter=10, random_state=SEED)\n\nplot_clusters(data=data_mds,\n              cluster_model=model,\n              dim_red_algo=TSNE(n_components=2, init='pca', random_state=SEED))","metadata":{"execution":{"iopub.status.busy":"2022-02-21T10:23:27.863378Z","iopub.execute_input":"2022-02-21T10:23:27.864275Z","iopub.status.idle":"2022-02-21T10:23:28.274875Z","shell.execute_reply.started":"2022-02-21T10:23:27.864234Z","shell.execute_reply":"2022-02-21T10:23:28.273853Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can choose between 2 and 5 clusters\n\n### <a id='4.5'>4.5 Hierarchical Agglomerative Clustering (HAC)</a> ","metadata":{}},{"cell_type":"code","source":"# HAC clustering (similar to get_kmeans_results function)\ndef get_hac_results(data, max_clusters=10, linkage='euclidean', seed=23):\n    silhouette = []\n    clusters_range = range(2, max_clusters+1)\n    for K in tqdm(clusters_range):\n        model = AgglomerativeClustering(n_clusters=K, linkage=linkage)\n        model.fit(data)\n        silhouette.append(silhouette_score(data, model.labels_))\n        \n    # Plot\n    plt.figure(figsize=(10,4))\n    plt.plot(clusters_range, silhouette, 'bx-')\n    plt.xlabel('k')\n    plt.ylabel('Silhouette score')\n    plt.title('Silhouette')\n    plt.grid(True);","metadata":{"execution":{"iopub.status.busy":"2022-02-21T10:23:28.609685Z","iopub.execute_input":"2022-02-21T10:23:28.610379Z","iopub.status.idle":"2022-02-21T10:23:28.618608Z","shell.execute_reply.started":"2022-02-21T10:23:28.610325Z","shell.execute_reply":"2022-02-21T10:23:28.617706Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Look at all results at a time \nfeatures_df = [scaled_ts, data_tsne, data_mds]\nfor df in features_df:\n    get_hac_results(data=df, max_clusters=10, linkage='ward', seed=SEED)","metadata":{"execution":{"iopub.status.busy":"2022-02-21T10:23:29.626832Z","iopub.execute_input":"2022-02-21T10:23:29.627745Z","iopub.status.idle":"2022-02-21T10:23:30.765982Z","shell.execute_reply.started":"2022-02-21T10:23:29.627694Z","shell.execute_reply":"2022-02-21T10:23:30.765065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's choose 3 clusters with MDS features","metadata":{}},{"cell_type":"code","source":"def plot_dendrogram(data, model, figsize=(16,10), **kwargs):\n    \"\"\"\n    Plots a dendogram using HAC \n\n    data: pd.DataFrame or np.array\n        Time Series Data\n    model: Class\n        Clustering Model \n    figsize: tuple\n        Figure size\n    Returns:\n    -------\n    None \n    \"\"\"\n    model.fit(data)\n    counts = np.zeros(model.children_.shape[0])\n    n_samples = len(model.labels_)\n    for i, merge in enumerate(model.children_):\n        current_count = 0\n        for child_idx in merge:\n            if child_idx < n_samples:\n                current_count += 1  \n            else:\n                current_count += counts[child_idx - n_samples]\n        counts[i] = current_count\n\n    linkage_matrix = np.column_stack([model.children_, model.distances_, counts]).astype(float)\n    \n    plt.figure(figsize=figsize, dpi=200)\n    dendrogram(linkage_matrix, **kwargs)\n    plt.title('Dendogram')\n    plt.xlabel('Objects')\n    plt.ylabel('Distance')\n    plt.grid(False)\n    plt.tight_layout();","metadata":{"execution":{"iopub.status.busy":"2022-02-21T10:23:30.767426Z","iopub.execute_input":"2022-02-21T10:23:30.767691Z","iopub.status.idle":"2022-02-21T10:23:30.777293Z","shell.execute_reply.started":"2022-02-21T10:23:30.76766Z","shell.execute_reply":"2022-02-21T10:23:30.77651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Dendrogram\nmodel = AgglomerativeClustering(n_clusters=3, linkage='ward', affinity='euclidean', compute_distances=True)\n\nplot_dendrogram(data=features_df[-1],\n                model=model,\n                color_threshold=60)","metadata":{"execution":{"iopub.status.busy":"2022-02-21T10:23:31.57902Z","iopub.execute_input":"2022-02-21T10:23:31.579715Z","iopub.status.idle":"2022-02-21T10:23:32.739755Z","shell.execute_reply.started":"2022-02-21T10:23:31.579664Z","shell.execute_reply":"2022-02-21T10:23:32.738923Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###  <a id='4.6'>4.6 Time Series KMeans Results</a> \nFinally, we will choose TimeSeriesKMeans with downsized features using MDS and 5 clusters. I think that two clusters are simply not enough. It's likely that the data is various and with 5 clusters we will get clusters with similar series.","metadata":{}},{"cell_type":"code","source":"# Train TimeSeriesKMeans with MDS\nkmeans_model = TimeSeriesKMeans(n_clusters=5, metric='euclidean', n_jobs=-1, max_iter=10, random_state=SEED)\ncluster_labels = kmeans_model.fit_predict(data_mds)\n\nts_clustered = [ scaled_ts[(cluster_labels == lable), :] for lable in np.unique(cluster_labels)]","metadata":{"execution":{"iopub.status.busy":"2022-02-21T10:23:36.27495Z","iopub.execute_input":"2022-02-21T10:23:36.27585Z","iopub.status.idle":"2022-02-21T10:23:36.323321Z","shell.execute_reply.started":"2022-02-21T10:23:36.275812Z","shell.execute_reply":"2022-02-21T10:23:36.322715Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Objects distribution in the obtained clusters \nlabels = [f'Cluster_{i}' for i in range(len(ts_clustered))]\nsamples_in_cluster = [val.shape[0] for val in ts_clustered]\n\nplt.figure(figsize=(16,5))\nplt.bar(labels, samples_in_cluster);","metadata":{"execution":{"iopub.status.busy":"2022-02-21T10:23:37.229043Z","iopub.execute_input":"2022-02-21T10:23:37.229336Z","iopub.status.idle":"2022-02-21T10:23:37.631026Z","shell.execute_reply.started":"2022-02-21T10:23:37.229302Z","shell.execute_reply":"2022-02-21T10:23:37.630357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" def plot_cluster_ts(current_cluster):\n    \"\"\"\n    Plots time series in a cluster \n\n    current_cluster: np.array\n        Cluster with time series \n    Returns:\n    -------\n    None \n    \"\"\"\n    fig, ax = plt.subplots(\n        int(np.ceil(current_cluster.shape[0]/4)),4,\n        figsize=(45, 3*int(np.ceil(current_cluster.shape[0]/4)))\n    )\n    fig.autofmt_xdate(rotation=45)\n    ax = ax.reshape(-1)\n    for indx, series in enumerate(current_cluster):\n        ax[indx].plot(series)\n        plt.xticks(rotation=45)\n\n    plt.tight_layout()\n    plt.show();","metadata":{"execution":{"iopub.status.busy":"2022-02-21T10:23:38.627244Z","iopub.execute_input":"2022-02-21T10:23:38.627908Z","iopub.status.idle":"2022-02-21T10:23:38.634396Z","shell.execute_reply.started":"2022-02-21T10:23:38.627871Z","shell.execute_reply":"2022-02-21T10:23:38.633698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's have a look at the obtained clusters","metadata":{}},{"cell_type":"code","source":"for cluster in range(len(ts_clustered)):\n    print(f\"==========Cluster number: {cluster}==========\")\n    plot_cluster_ts(ts_clustered[cluster])","metadata":{"execution":{"iopub.status.busy":"2022-02-21T10:23:40.111626Z","iopub.execute_input":"2022-02-21T10:23:40.112078Z","iopub.status.idle":"2022-02-21T10:23:49.040929Z","shell.execute_reply.started":"2022-02-21T10:23:40.112037Z","shell.execute_reply":"2022-02-21T10:23:49.040027Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Most of the series within its cluster are alike and it is cool. We have found out that all the stores can be clustered into 5 different groups. There are stores that have the same patterns \n\n### <a id='5'>5. Cluster Series Extraction</a>\nAlright, we clustered the series data, what's next? Well, it depends on the task you are dealing with. Probably, after clustering the series you will want to get a cluster series (a series that describes all the series in the cluster)\n\nThere are several options:\n- Use cluster centroid \n- Take the mean of all the series in a cluster\n- Takes a series that has a minimum distance to the cluster centroid \n- <a href='https://tslearn.readthedocs.io/en/stable/variablelength.html#barycenter-computation'>DBA method</a>\n\nWe will cover:\n- DBA\n- Cluster Mean\n- Closest Series to Cluster Centroid ","metadata":{}},{"cell_type":"code","source":"# Closest Series to Cluster Centroid\nclosest_clusters_indxs = [np.argmin([np.linalg.norm(cluster_center - point, ord=2) for point in data_mds]) \\\n                                                                        for cluster_center in kmeans_model.cluster_centers_]\n\nclosest_ts = scaled_ts[closest_clusters_indxs, :]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# DBA\ndba_ts = [dtw_barycenter_averaging(cluster_serieses, max_iter=10, verbose=True) for cluster_serieses in ts_clustered]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's compare how a certain method affects a final cluster shape\n\n**Choose a cluster with a few series. This will help to see the differences between the algorithms!**","metadata":{}},{"cell_type":"code","source":"CLUSTER_N = 2\n\nplt.figure(figsize=(25, 5))\nplt.plot(ts_clustered[CLUSTER_N].T,  alpha = 0.4) # all series in the cluster_1\nplt.plot(closest_ts[CLUSTER_N], c = 'r', label='Cluster Time Series')\nplt.title('Cluster Series - Closest to Cluster Centroid. Cluster 1')\nplt.legend();\n\nplt.figure(figsize=(25, 5))\nplt.plot(ts_clustered[CLUSTER_N].T,  alpha = 0.4) \nplt.plot(np.mean(ts_clustered[CLUSTER_N], axis=0), c = 'r', label='Cluster Time Series')\nplt.title('Cluster Series - Cluster Mean. Cluster 1')\nplt.legend();\n\nplt.figure(figsize=(25, 5))\nplt.plot(ts_clustered[CLUSTER_N].T,  alpha = 0.4) \nplt.plot(dba_ts[CLUSTER_N], c = 'r', label='Cluster Time Series')\nplt.title('Cluster Series - DBA. Cluster 1')\nplt.legend();","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Why not choose the first option? Well, it has a big spike and doesn't describe all series data. As a solution, smoothing can be applied (I think it's always a good idea to apply smoothing in this case because noisy series might be chosen)\n\nDBA or Mean method look good. Both can be chosen!\n\n### <a id='5.1'>5.1 Cluster Series DBA</a>\nAll clusters series extracted by DBA","metadata":{}},{"cell_type":"code","source":"for indx, series in enumerate(dba_ts):\n    plt.figure(figsize=(25, 5))\n    plt.plot(ts_clustered[indx].T,  alpha = 0.15)\n    plt.plot(series, c = 'r', label='Cluster Time Series')\n    plt.title(f'Scaled Sales. Cluster {indx}')\n    plt.legend();","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <a id='6'>6. Time Series Embeddings</a>\n\nIn this approach, we will train NN (Recurrent Auto-encoders with LSTM / GRU blocks) and get compressed vector representations of series (embeddings)\n\nTrying to train the encoder and decoder in such a way that in all the variety of data at the input they would receive series close to each other, and those that differ were separated, according to the distance that we choose.\n\nThe algorithm is trained in unsupervised mode. Obtained embeddings will be clustered in the end ","metadata":{}},{"cell_type":"code","source":"os.chdir('./timeseries-clustering-vae')\n\nfrom vrae.vrae import VRAE\nfrom vrae.utils import *\n\nimport torch\nimport plotly\nfrom torch.utils.data import DataLoader, TensorDataset\nplotly.offline.init_notebook_mode()","metadata":{"execution":{"iopub.status.busy":"2022-02-21T10:26:35.577854Z","iopub.execute_input":"2022-02-21T10:26:35.578204Z","iopub.status.idle":"2022-02-21T10:26:35.634963Z","shell.execute_reply.started":"2022-02-21T10:26:35.578167Z","shell.execute_reply":"2022-02-21T10:26:35.634168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vrae_df = scaled_ts.copy()\ndload = '/content/timeseries_clustering_vae/' ","metadata":{"execution":{"iopub.status.busy":"2022-02-21T10:26:41.31025Z","iopub.execute_input":"2022-02-21T10:26:41.310564Z","iopub.status.idle":"2022-02-21T10:26:41.314748Z","shell.execute_reply.started":"2022-02-21T10:26:41.310531Z","shell.execute_reply":"2022-02-21T10:26:41.313903Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Model Params\nhidden_size = 50\nhidden_layer_depth = 1\nlatent_length = 20\nbatch_size = 5\nlearning_rate = 0.005\nn_epochs = 40\ndropout_rate = 0.1\noptimizer = 'Adam' # Adam/SGD\ncuda = True # Train on GPU\nprint_every=30\nclip = True \nmax_grad_norm=5\nloss = 'MSELoss' # SmoothL1Loss/MSELoss\nblock = 'LSTM' # LSTM/GRU","metadata":{"execution":{"iopub.status.busy":"2022-02-21T10:26:42.507348Z","iopub.execute_input":"2022-02-21T10:26:42.508266Z","iopub.status.idle":"2022-02-21T10:26:42.513523Z","shell.execute_reply.started":"2022-02-21T10:26:42.508226Z","shell.execute_reply":"2022-02-21T10:26:42.512938Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We don't use test_df, create train_df using all the data we have\nX_train = np.expand_dims(scaled_ts, -1)\ntrain_dataset = TensorDataset(torch.from_numpy(X_train))\n\nsequence_length = X_train.shape[1] \nnumber_of_features = X_train.shape[2] ","metadata":{"execution":{"iopub.status.busy":"2022-02-21T10:26:44.300539Z","iopub.execute_input":"2022-02-21T10:26:44.301665Z","iopub.status.idle":"2022-02-21T10:26:44.312534Z","shell.execute_reply.started":"2022-02-21T10:26:44.301605Z","shell.execute_reply":"2022-02-21T10:26:44.31159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Model Creation \nvrae = VRAE(sequence_length=sequence_length,\n            number_of_features = number_of_features,\n            hidden_size = hidden_size, \n            hidden_layer_depth = hidden_layer_depth,\n            latent_length = latent_length,\n            batch_size = batch_size,\n            learning_rate = learning_rate,\n            n_epochs = n_epochs,\n            dropout_rate = dropout_rate,\n            optimizer = optimizer, \n            cuda = cuda,\n            print_every=print_every, \n            clip=clip, \n            max_grad_norm=max_grad_norm,\n            loss = loss,\n            block = block,\n            dload = dload)","metadata":{"execution":{"iopub.status.busy":"2022-02-21T10:26:45.588597Z","iopub.execute_input":"2022-02-21T10:26:45.589321Z","iopub.status.idle":"2022-02-21T10:26:45.626873Z","shell.execute_reply.started":"2022-02-21T10:26:45.58928Z","shell.execute_reply":"2022-02-21T10:26:45.625976Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time \n\nvrae.fit(train_dataset)","metadata":{"execution":{"iopub.status.busy":"2022-02-21T10:26:47.304822Z","iopub.execute_input":"2022-02-21T10:26:47.305828Z","iopub.status.idle":"2022-02-21T10:34:10.673676Z","shell.execute_reply.started":"2022-02-21T10:26:47.305778Z","shell.execute_reply":"2022-02-21T10:34:10.67276Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get embeddings\nembeddings = vrae.transform(train_dataset)\n\n# Cluster the embeddings\nget_kmeans_results(data=embeddings, max_clusters=10, metric='euclidean', seed=SEED)","metadata":{"execution":{"iopub.status.busy":"2022-02-21T10:34:10.676861Z","iopub.execute_input":"2022-02-21T10:34:10.67712Z","iopub.status.idle":"2022-02-21T10:34:12.178362Z","shell.execute_reply.started":"2022-02-21T10:34:10.677088Z","shell.execute_reply":"2022-02-21T10:34:12.177504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = TimeSeriesKMeans(n_clusters=3, metric='euclidean', n_jobs=-1, max_iter=10, random_state=SEED)\n \nplot_clusters(data=embeddings,\n              cluster_model=model,\n              dim_red_algo=TSNE(n_components=2, init='pca', random_state=SEED))","metadata":{"execution":{"iopub.status.busy":"2022-02-21T10:34:12.179896Z","iopub.execute_input":"2022-02-21T10:34:12.180157Z","iopub.status.idle":"2022-02-21T10:34:12.957376Z","shell.execute_reply.started":"2022-02-21T10:34:12.180128Z","shell.execute_reply":"2022-02-21T10:34:12.956514Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Obtained a good cluster shape can apply this technique as well!\n\n### <a id='7'>7. References</a>\n- https://www.kaggle.com/izzettunc/introduction-to-time-series-clustering/notebook","metadata":{}}]}