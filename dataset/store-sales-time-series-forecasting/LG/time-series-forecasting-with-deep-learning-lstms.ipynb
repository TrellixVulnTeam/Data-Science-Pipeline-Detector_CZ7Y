{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 1. Introduction\n\nAlthough I am not convinced by the efficiency of using Deep Learning for this particular competition, I still wanted to give it a go to see what we could get from it. I checked other existing notebooks for this given competition but couldn't find any finalised notebooks. So here is an attempt to use LSTMs for Time Series Forecasting. Before you read further, let me mention a few points:\n- the LSTM model was not able to beat my baseline model (very simplistic approach using the values in the last week of the training set). I have more tricks up my sleeve but wasn't able to try them yet. If you think of any other ideas please let me know.\n- this approach can only work if all the pairs store-family in the test set are also in the training set. This check was made in another notebook so I won't duplicate the code here: https://www.kaggle.com/loicge/sales-top-down-approach-with-prophet-0-52\n- the code implementation was inspired from this tensorflow tutorial: https://www.tensorflow.org/tutorials/structured_data/time_series#recurrent_neural_network","metadata":{}},{"cell_type":"markdown","source":"# 2. Loading data and libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport random\nfrom datetime import datetime, timedelta\nfrom tqdm import tqdm\nfrom sklearn.preprocessing import MinMaxScaler\n\nimport os\nimport tensorflow as tf\n\ndata_di = {}\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        path = os.path.join(dirname, filename)\n        print(path)\n        name = path.split('/')[-1].split('.')[0]\n        try:\n            data_di[name] = pd.read_csv(path, parse_dates=['date'])\n        except:\n            data_di[name] = pd.read_csv(path)\n        \n        # Rename columns to prophet standards\n        if 'sales' in data_di[name].columns:\n            data_di[name] = data_di[name].rename(columns={'sales': 'y'})\n        if 'date' in data_di[name].columns:\n            data_di[name] = data_di[name].rename(columns={'date': 'ds'})","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-02-21T13:59:47.256333Z","iopub.execute_input":"2022-02-21T13:59:47.256747Z","iopub.status.idle":"2022-02-21T13:59:56.852622Z","shell.execute_reply.started":"2022-02-21T13:59:47.256654Z","shell.execute_reply":"2022-02-21T13:59:56.851884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Utils","metadata":{}},{"cell_type":"code","source":"def rmsle(y_hat, y):\n    \"\"\"Compute Root Mean Squared Logarithmic Error\"\"\"\n    metric = np.sqrt(sum((np.array(list(map(lambda x : np.log(x + 1), y_hat)))\n                         - np.array(list(map(lambda x : np.log(x + 1), y))))**2)/len(y))\n                \n    return round(metric, 4)","metadata":{"execution":{"iopub.status.busy":"2022-02-21T13:59:56.853982Z","iopub.execute_input":"2022-02-21T13:59:56.854731Z","iopub.status.idle":"2022-02-21T13:59:56.861096Z","shell.execute_reply.started":"2022-02-21T13:59:56.854697Z","shell.execute_reply":"2022-02-21T13:59:56.86033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class WindowGenerator():\n    def __init__(self, input_width, label_width, shift, train_df, val_df, test_df, label_columns=None):\n        # Store the raw data.\n        self.train_df = train_df\n        self.val_df = val_df\n        self.test_df = test_df\n\n        # Work out the label column indices.\n        self.label_columns = label_columns\n        if label_columns is not None:\n            self.label_columns_indices = {name: i for i, name in enumerate(label_columns)}\n        self.column_indices = {name: i for i, name in enumerate(train_df.columns)}\n\n        # Work out the window parameters.\n        self.input_width = input_width\n        self.label_width = label_width\n        self.shift = shift\n\n        self.total_window_size = input_width + shift\n\n        self.input_slice = slice(0, input_width)\n        self.input_indices = np.arange(self.total_window_size)[self.input_slice]\n\n        self.label_start = self.total_window_size - self.label_width\n        self.labels_slice = slice(self.label_start, None)\n        self.label_indices = np.arange(self.total_window_size)[self.labels_slice]\n\n    def __repr__(self):\n        return '\\n'.join([\n            f'Total window size: {self.total_window_size}',\n            f'Input indices: {self.input_indices}',\n            f'Label indices: {self.label_indices}',\n            f'Label column name(s): {self.label_columns}'])","metadata":{"execution":{"iopub.status.busy":"2022-02-21T13:59:56.862265Z","iopub.execute_input":"2022-02-21T13:59:56.862632Z","iopub.status.idle":"2022-02-21T13:59:56.875Z","shell.execute_reply.started":"2022-02-21T13:59:56.862596Z","shell.execute_reply":"2022-02-21T13:59:56.873916Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def split_window(self, features):\n    inputs = features[:, self.input_slice, :]\n    labels = features[:, self.labels_slice, :]\n    if self.label_columns is not None:\n        labels = tf.stack(\n            [labels[:, :, self.column_indices[name]] for name in self.label_columns], axis=-1)\n\n    # Slicing doesn't preserve static shape information, so set the shapes\n    # manually. This way the `tf.data.Datasets` are easier to inspect.\n    inputs.set_shape([None, self.input_width, None])\n    labels.set_shape([None, self.label_width, None])\n\n    return inputs, labels\n\n\ndef plot(self, model=None, plot_col='T (degC)', max_subplots=3):\n    inputs, labels = self.example\n    plt.figure(figsize=(12, 8))\n    plot_col_index = self.column_indices[plot_col]\n    max_n = min(max_subplots, len(inputs))\n    \n    for n in range(max_n):\n        plt.subplot(max_n, 1, n+1)\n        plt.ylabel(f'{plot_col} [normed]')\n        plt.plot(self.input_indices, inputs[n, :, plot_col_index],\n                 label='Inputs', marker='.', zorder=-10)\n\n        if self.label_columns:\n            label_col_index = self.label_columns_indices.get(plot_col, None)\n        else:\n            label_col_index = plot_col_index\n\n        if label_col_index is None:\n            continue\n\n        plt.scatter(self.label_indices, labels[n, :, label_col_index], edgecolors='k', label='Labels', c='#2ca02c', s=64)\n\n        if model is not None:\n            predictions = model(inputs)\n            plt.scatter(self.label_indices, predictions[n, :, label_col_index],\n                      marker='X', edgecolors='k', label='Predictions',\n                      c='#ff7f0e', s=64)\n\n        if n == 0:\n            plt.legend()\n        \n    plt.xlabel('Days')\n\n    \ndef make_dataset(self, data):\n    data = np.array(data, dtype=np.float32)\n    ds = tf.keras.utils.timeseries_dataset_from_array(data=data, targets=None, sequence_length=self.total_window_size,\n                                                      sequence_stride=1, shuffle=True, batch_size=32,)\n    \n    ds = ds.map(self.split_window)\n    \n    return ds\n\n@property\ndef train(self):\n    return self.make_dataset(self.train_df)\n\n@property\ndef val(self):\n    return self.make_dataset(self.val_df)\n\n@property\ndef test(self):\n    return self.make_dataset(self.test_df)\n\n@property\ndef example(self):\n    \"\"\"Get and cache an example batch of `inputs, labels` for plotting.\"\"\"\n    result = getattr(self, '_example', None)\n    if result is None:\n        # No example batch was found, so get one from the `.train` dataset\n        result = next(iter(self.train))\n        # And cache it for next time\n        self._example = result\n    return result\n\n\ndef compile_and_fit(model, window, MAX_EPOCHS=20, learning_rate=0.2, patience=10):\n    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=patience, mode='min')\n    \n    model.compile(loss=tf.losses.MeanSquaredError(),\n                optimizer=tf.optimizers.Adam(learning_rate=learning_rate),\n                metrics=[tf.metrics.MeanAbsoluteError()])\n    \n    history = model.fit(window.train, epochs=MAX_EPOCHS,\n                      validation_data=window.val,\n                      callbacks=[early_stopping])\n    \n    return history","metadata":{"execution":{"iopub.status.busy":"2022-02-21T13:59:56.876467Z","iopub.execute_input":"2022-02-21T13:59:56.876834Z","iopub.status.idle":"2022-02-21T13:59:56.899486Z","shell.execute_reply.started":"2022-02-21T13:59:56.876798Z","shell.execute_reply":"2022-02-21T13:59:56.898451Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Baseline model - last week value\n\nIt is always good to start with the a baseline model before writing complex models. In one of my previous notebook, I used the last year value as a baseline model (see here for more details: https://www.kaggle.com/loicge/sales-top-down-approach-with-prophet-0-52). To make things a bit different this time, I decided to use the last available week value. Let's see what we get.","metadata":{}},{"cell_type":"code","source":"def get_last_available_week(df):\n    # Get date of the last available week\n    df = df.assign(diff_from_max_train_ds=df.ds - (df.ds.min() - timedelta(days=1))) # - datetime.strptime(df.ds.min(), \"%Y-%m-%d\"))\n    df = df.assign(nb_weeks=np.ceil(df.diff_from_max_train_ds.dt.days/7).astype('int'))\n    df = df.assign(last_week_ds=df.ds - (df.nb_weeks*7).map(lambda x: timedelta(x)))\n    \n    return df\n\ndef get_yhat(df):\n    df = pd.merge(df, \n    df[['ds', 'store_nbr', 'family', 'prop_family_per_store']].rename(columns={'prop_family_per_store': 'last_week_prop_family_per_store'}),\n         left_on = ['last_week_ds', 'store_nbr', 'family'],\n         right_on=['ds', 'store_nbr', 'family'],\n         how='left'\n        ).drop(['diff_from_max_train_ds', 'nb_weeks', 'last_week_ds', 'ds_y'], axis=1).rename(columns={'ds_x': 'ds'})\n\n    df = df.assign(yhat=df.yhat_store_nbr * df.last_week_prop_family_per_store)\n    \n    return df","metadata":{"execution":{"iopub.status.busy":"2022-02-21T13:59:56.902201Z","iopub.execute_input":"2022-02-21T13:59:56.902734Z","iopub.status.idle":"2022-02-21T13:59:56.917109Z","shell.execute_reply.started":"2022-02-21T13:59:56.902691Z","shell.execute_reply":"2022-02-21T13:59:56.916084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's first perform a cross-validation on the last two weeks of the training set.","metadata":{}},{"cell_type":"code","source":"train_df, test_df = data_di['train'], data_di['test']\n\n# Cross validation\nval_df = train_df[(train_df.ds >= '2017-08-01') & (train_df.ds <= '2017-08-15')]\n\nval_df = pd.merge(get_last_available_week(val_df)[['id', 'ds', 'last_week_ds', 'store_nbr', 'family', 'y']],\n                    train_df[['ds', 'store_nbr', 'family', 'y']],\n                    left_on=['last_week_ds', 'store_nbr', 'family'], right_on=['ds', 'store_nbr', 'family'], how='left').rename(columns={'ds_x': 'ds', 'y_x': 'y', 'y_y': 'yhat'})\n\n\nprint('RMSLE: %s' %rmsle(val_df.yhat, val_df.y))","metadata":{"execution":{"iopub.status.busy":"2022-02-21T13:59:56.918502Z","iopub.execute_input":"2022-02-21T13:59:56.918835Z","iopub.status.idle":"2022-02-21T13:59:57.884937Z","shell.execute_reply.started":"2022-02-21T13:59:56.918802Z","shell.execute_reply":"2022-02-21T13:59:57.883827Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We obtain a score of <b>0.6124</b> which is actually much better than the score obtained with the last year value model on the same validation period (<b>0.9495</b>). Let's now submit on the leaderboard and see what we get.","metadata":{}},{"cell_type":"code","source":"submission_df = pd.merge(get_last_available_week(test_df)[['id', 'ds', 'last_week_ds', 'store_nbr', 'family']],\n                    train_df[['ds', 'store_nbr', 'family', 'y']],\n                    left_on=['last_week_ds', 'store_nbr', 'family'], right_on=['ds', 'store_nbr', 'family'], how='left')[['id', 'y']].rename(columns={'y': 'sales'})\n\n# submission_df.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-02-21T13:59:57.886497Z","iopub.execute_input":"2022-02-21T13:59:57.886887Z","iopub.status.idle":"2022-02-21T13:59:58.72574Z","shell.execute_reply.started":"2022-02-21T13:59:57.886846Z","shell.execute_reply":"2022-02-21T13:59:58.724727Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Nice! The score on the leaderboard is <b>0.52245</b> which is not bad at all for a baseline model. I would assume that it performs better than the last year value model because the last available week value reflects more the recent data (such as trend or seasonality, especially weekly seasonality).","metadata":{}},{"cell_type":"markdown","source":"# 5. LSTM Model\n### 5.1 Training\nNow that we have a baseline model, let's try to beat that model using LSTMs.","metadata":{}},{"cell_type":"code","source":"# Parameters\nstart_training_ds = '2017-01-01'\ninput_width = 1*7\nlabel_width = 16\n\nMAX_EPOCHS = 50\nlearning_rate = 0.001\nscaling = ['standardisation', 'normalisation'][0]\n\n# Reshape the dataframe\ndf = data_di['train'][data_di['train'].ds >= start_training_ds].assign(key=data_di['train']['store_nbr'].astype('str') + '~' + data_di['train']['family'])\ndf = pd.pivot_table(df, values = 'y', index=['ds'], columns='key').reset_index()\ndate_time = df.ds\ndf = df.drop('ds', axis=1)\ndf = df.iloc[:, :df.shape[1]]\ncolumn_indices = {name: i for i, name in enumerate(df.columns)}\n\n# Split into train, val and test set\nn = len(df)\ntest_df = df[-label_width:]\nval_df = df[-(input_width + 2*label_width):-label_width]\ntrain_df = df[:-(input_width + label_width)]\n\nprint(\"Train set size: (%s, %s)\" %(train_df.shape[0], train_df.shape[1]))\nprint(\"Validation set size: (%s, %s)\"  %(test_df.shape[0], test_df.shape[1]))\nprint(\"Test set size: (%s, %s) \\n\"  %(val_df.shape[0], val_df.shape[1]))\n\nnum_features = df.shape[1]\n\n# Perform normalisation\nif scaling == 'standardisation':\n    train_mean = train_df.mean()\n    train_std = train_df.std()\n\n    train_df = (train_df - train_mean) / train_std\n    train_df = train_df.fillna(0)\n    val_df = (val_df - train_mean) / train_std\n    val_df = val_df.fillna(0)\n    test_df = (test_df - train_mean) / train_std\n    test_df = test_df.fillna(0)\n\n    train_df[train_df == np.inf] = 0\n    val_df[val_df == np.inf] = 0\n    test_df[test_df == np.inf] = 0\n\nelif scaling == 'normalisation':\n    scaler = MinMaxScaler(feature_range=(0,1))\n    train_df = pd.DataFrame(scaler.fit_transform(train_df), columns=train_df.columns)\n    val_df = pd.DataFrame(scaler.transform(val_df), columns=val_df.columns)\n    test_df = pd.DataFrame(scaler.transform(test_df), columns=test_df.columns)\n\nWindowGenerator.train = train\nWindowGenerator.val = val\nWindowGenerator.test = test\nWindowGenerator.example = example\nWindowGenerator.make_dataset = make_dataset\nWindowGenerator.split_window = split_window\nWindowGenerator.plot = plot\n\n# Generate windows for training batches\nwindow = WindowGenerator(input_width=input_width, label_width=label_width, shift=label_width,\n                         train_df=train_df, val_df=val_df, test_df=test_df)\n\nmodel = tf.keras.Sequential([\n    # Shape [batch, time, features] => [batch, lstm_units].\n    tf.keras.layers.Conv1D(filters=128, kernel_size=(input_width,), activation='relu'),\n    tf.keras.layers.LSTM(128, return_sequences=False),\n    tf.keras.layers.Dense(label_width*num_features,\n                          kernel_initializer=tf.initializers.zeros()),\n    tf.keras.layers.Reshape([label_width, num_features])\n])\n\nprint('Input shape:', window.example[0].shape)\nprint('Labels shape:', window.example[1].shape)\nprint('Output shape:', model(window.example[0]).shape)\n\nhistory = compile_and_fit(model, window, MAX_EPOCHS, learning_rate)","metadata":{"execution":{"iopub.status.busy":"2022-02-21T13:59:58.72701Z","iopub.execute_input":"2022-02-21T13:59:58.727278Z","iopub.status.idle":"2022-02-21T14:00:26.805592Z","shell.execute_reply.started":"2022-02-21T13:59:58.727236Z","shell.execute_reply":"2022-02-21T14:00:26.804848Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame(history.history).plot(figsize=(8,5))\nplt.title('Train and Val loss')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-21T14:00:26.806987Z","iopub.execute_input":"2022-02-21T14:00:26.807746Z","iopub.status.idle":"2022-02-21T14:00:27.054071Z","shell.execute_reply.started":"2022-02-21T14:00:26.8077Z","shell.execute_reply":"2022-02-21T14:00:27.053186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"window.plot(model, plot_col=random.choice(train_df.columns))","metadata":{"execution":{"iopub.status.busy":"2022-02-21T14:00:27.055274Z","iopub.execute_input":"2022-02-21T14:00:27.055974Z","iopub.status.idle":"2022-02-21T14:00:27.61457Z","shell.execute_reply.started":"2022-02-21T14:00:27.05592Z","shell.execute_reply":"2022-02-21T14:00:27.613713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 5.2 Cross validation\nWe can see from the two plots above that the neural network seems to be able to learn. Let's perform a cross validation to evaluate the results in details.","metadata":{}},{"cell_type":"code","source":"# Generate predictions for the test period taking the last values of the validation period\ny_hat = model.predict(val_df.values[-input_width:, :].reshape(1, input_width, num_features))\npredict_df = (pd.DataFrame(y_hat.reshape(label_width, num_features), columns=df.columns)*train_std + train_mean).assign(ds=date_time[-label_width:].values)\ncolumns_to_keep = [e for e in predict_df.columns if '~' in e or e == 'ds'] \npredict_df = predict_df[columns_to_keep]\npredict_df = predict_df.melt(id_vars =['ds'], value_vars =[c for c in predict_df.columns if c != 'ds'])\npredict_df[['store_nbr', 'family']] = predict_df.key.str.split('~', expand=True)\npredict_df = predict_df.rename(columns={'value': 'y_hat'})\npredict_df['store_nbr'] = predict_df.store_nbr.astype('int')\npredict_df.drop('key', axis=1, inplace=True)\n\npredict_df = pd.merge(data_di['train'].drop('id', axis=1), predict_df, on=['ds', 'store_nbr', 'family'], how='left')\npredict_df['y_hat'] = np.clip(predict_df.y_hat, 0, np.inf)\n\npredict_df.tail()","metadata":{"execution":{"iopub.status.busy":"2022-02-21T14:00:27.615923Z","iopub.execute_input":"2022-02-21T14:00:27.616183Z","iopub.status.idle":"2022-02-21T14:00:29.383243Z","shell.execute_reply.started":"2022-02-21T14:00:27.616152Z","shell.execute_reply":"2022-02-21T14:00:29.38254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's now compute the evaluation metric on the test period: \n<b> RMSLE </b> \n$=\\sqrt{ \\frac{1}{n} \\sum_{i=1}^n \\left(\\log (1 + \\hat{y}_i) - \\log (1 + y_i)\\right)^2}$\n\nNote that the cross-validation is performed from 2017-08-01 to 2017-08-15 to keep it simple. A more robust cross-validation would be to use a rolling window and compute the average of the metric over all those windows. See here for more details: https://robjhyndman.com/hyndsight/tscv/","metadata":{}},{"cell_type":"code","source":"predict_df['error'] = (np.log(1 + predict_df[~predict_df.y_hat.isnull()].y) - np.log(1 + predict_df[~predict_df.y_hat.isnull()].y_hat))**2\nprint(\"RMSLE: %s\" %rmsle(predict_df[~predict_df.y_hat.isnull()].y, predict_df[~predict_df.y_hat.isnull()].y_hat))","metadata":{"execution":{"iopub.status.busy":"2022-02-21T14:00:29.384273Z","iopub.execute_input":"2022-02-21T14:00:29.385082Z","iopub.status.idle":"2022-02-21T14:00:29.630153Z","shell.execute_reply.started":"2022-02-21T14:00:29.385046Z","shell.execute_reply":"2022-02-21T14:00:29.629441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 5.3 Investigation\nLooking at the results above, it seems that the LSTM model is unable to outperformed the baseline model at least on the cross-validation period. Let's try to understand where the weakness of the LSTM is coming from. A way to do that is to compute the evaluation metric for each of the time series in the test set and extract the highest values.","metadata":{}},{"cell_type":"code","source":"predict_df['key'] = predict_df['store_nbr'].astype(str) + '~' + predict_df['family'].astype(str)\nrmsle_per_ts_df = predict_df.groupby('key').agg({'error': 'sum'}).sort_values('error', ascending=False).reset_index()\nrmsle_per_ts_df.head(10)","metadata":{"execution":{"iopub.status.busy":"2022-02-21T14:00:29.631097Z","iopub.execute_input":"2022-02-21T14:00:29.631807Z","iopub.status.idle":"2022-02-21T14:00:32.309957Z","shell.execute_reply.started":"2022-02-21T14:00:29.631767Z","shell.execute_reply":"2022-02-21T14:00:32.308755Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"At a first look, <i>SCHOOL AND OFFICE SUPPLIES</i> family seems to have a major issue. Let's a take a closer look by visualising the Time Series.","metadata":{}},{"cell_type":"code","source":"plot_ds_range = ['2015-05-01', predict_df.ds.iloc[-1]]\nfig, ax = plt.subplots(figsize=(20, 5))\nunique_keys = set(zip(data_di['train'].store_nbr, data_di['train'].family))\nkey = random.choice(list(unique_keys))\nkey = [48, 'SCHOOL AND OFFICE SUPPLIES']\nts = predict_df[(predict_df.store_nbr == key[0]) & (predict_df.family == key[1])]\nts = ts[(ts.ds >= plot_ds_range[0]) & (ts.ds <= plot_ds_range[1])]\n\n# plt.plot(ts.ds, ts.onpromotion*np.median(ts.y), label='promotion')\nplt.plot(ts.ds, ts.y_hat, label='y_hat')\nplt.plot(ts.ds, ts.y, label='y')\nplt.title(\"Store: \" + str(key[0]) + \", Family: \" + key[1])\nplt.legend()\nax.set_xticks(np.array(ts.ds)[::100])\nax.tick_params(axis='x',rotation=45)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-21T14:00:32.312508Z","iopub.execute_input":"2022-02-21T14:00:32.312815Z","iopub.status.idle":"2022-02-21T14:00:33.655486Z","shell.execute_reply.started":"2022-02-21T14:00:32.312778Z","shell.execute_reply":"2022-02-21T14:00:33.654628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The issue is quite obvious. It looks like the LSTM model is not able to understand the yearly pattern. This is not a suprise as the model is trying to learn from the last 7 days to predict the next 14 days without any information of where we are in the year. I am currently working at injecting exogenous variables in the model in order to capture the seasonality patterns (such as day of the year, day of the week,...). If you have any recommendations on how to do that please let me know.\n\n# 6. Submission\n\nLet's now re-run the model to predict the leaderboard period - last 16 days of August.","metadata":{"execution":{"iopub.status.busy":"2022-02-17T16:58:37.187319Z","iopub.execute_input":"2022-02-17T16:58:37.187606Z","iopub.status.idle":"2022-02-17T16:58:37.564185Z","shell.execute_reply.started":"2022-02-17T16:58:37.187573Z","shell.execute_reply":"2022-02-17T16:58:37.563094Z"}}},{"cell_type":"code","source":"y_hat = model.predict(df.values[-input_width:, :].reshape(1, input_width, num_features))\npredict_df = (pd.DataFrame(y_hat.reshape(label_width, num_features), columns=df.columns)*train_std + train_mean).assign(ds=pd.date_range(date_time.iloc[-1] + timedelta(days=1), date_time.iloc[-1] + timedelta(days=label_width)))\n\npredict_df = predict_df.melt(id_vars =['ds'], value_vars =[c for c in predict_df.columns if c != 'ds'])\npredict_df[['store_nbr', 'family']] = predict_df.key.str.split('~', expand=True)\npredict_df = predict_df.rename(columns={'value': 'y_hat'})\npredict_df['store_nbr'] = predict_df.store_nbr.astype('int')\npredict_df.drop('key', axis=1, inplace=True)\n\npredict_df = pd.merge(data_di['test'], predict_df,  on=['ds', 'store_nbr', 'family'], how='left')\npredict_df['y_hat'] = np.clip(predict_df.y_hat, 0, np.inf)\n\npredict_df[['id', 'y_hat']].rename(columns={'y_hat': 'sales'}).to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-02-21T14:00:33.656934Z","iopub.execute_input":"2022-02-21T14:00:33.65755Z","iopub.status.idle":"2022-02-21T14:00:33.917423Z","shell.execute_reply.started":"2022-02-21T14:00:33.657493Z","shell.execute_reply":"2022-02-21T14:00:33.91668Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 7. Conclusion\n\nThis was a first attempt in order to use LSTMs for Time Series forecasting. The current LTSM model has a score <b>0.61</b> on the leaderboard (can vary depending on the run) and is unfortunately unable to beat the baseline model (<b>0.52</b>). My guess is that more information need to be feed into the model. To be continued...","metadata":{}},{"cell_type":"markdown","source":"### References\n- https://www.tensorflow.org/tutorials/structured_data/time_series#recurrent_neural_network\n- Interesting article about the performance of deep learning for forecasting: \n\"Do we really need Deep Learning for Time Series Forecasting?\" https://arxiv.org/abs/2101.02118","metadata":{}}]}