{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction \n`V1.0.1 - 2021-10-30`\n### Who am I\nJust a fellow Kaggle learner. Created this Notebook as practice and thought it could be useful to some others \n### Who is this for\nThis Notebook is for people that learn from examples. Forget the boring lectures and follow along for some fun/instructive time :)\n### What can I learn here\nYou learn all the basics needed to create a rudimentary Exploratory Data Analysdis and Multivariate LSTM Network. I go over a multitude of steps with explanations. Hopefully with these building blocks,\nyou can go ahead and build much more complex models.\n\n### Thins to remember\n+ Please Upvote/Like the Notebook so other people can learn from it\n+ Feel free to give any recommendations/changes. \n+ I will be continuously updating the notebook. Look forward to many more upcoming changes in the future.","metadata":{}},{"cell_type":"markdown","source":"# File Descriptions and Data Field Information\n## train.csv\n- The training data, comprising time series of features store_nbr, family, and onpromotion as well as the target sales.\n- store_nbr identifies the store at which the products are sold.\n- family identifies the type of product sold.\n- sales gives the total sales for a product family at a particular store at a given date. Fractional values are possible since products can be sold in fractional units (1.5 kg of cheese, for instance, as opposed to 1 bag of chips).\n- onpromotion gives the total number of items in a product family that were being promoted at a store at a given date.\n## test.csv\n- The test data, having the same features as the training data. You will predict the target sales for the dates in this file.\n- The dates in the test data are for the 15 days after the last date in the training data.\n## sample_submission.csv\n- A sample submission file in the correct format.\n## stores.csv\n- Store metadata, including city, state, type, and cluster.\n- cluster is a grouping of similar stores.\n## oil.csv\n- Daily oil price. Includes values during both the train and test data timeframes. (Ecuador is an oil-dependent country and it's economical health is highly vulnerable to shocks in oil prices.)\n## Transactions.csv\n- Daily transactions per store\n## holidays_events.csv\n- Holidays and Events, with metadata\n- NOTE: Pay special attention to the transferred column. A holiday that is transferred officially falls on that calendar day, but was moved to another date by the government. A transferred day is more like a normal day than a holiday. To find the day that it was actually celebrated, look for the corresponding row where type is Transfer. For example, the holiday Independencia de Guayaquil was transferred from 2012-10-09 to 2012-10-12, which means it was celebrated on 2012-10-12. Days that are type Bridge are extra days that are added to a holiday (e.g., to extend the break across a long weekend). These are frequently made up by the type Work Day which is a day not normally scheduled for work (e.g., Saturday) that is meant to payback the Bridge.\nAdditional holidays are days added a regular calendar holiday, for example, as typically happens around Christmas (making Christmas Eve a holiday).\n## Additional Notes\n- Wages in the public sector are paid every two weeks on the 15 th and on the last day of the month. Supermarket sales could be affected by this.\n- A magnitude 7.8 earthquake struck Ecuador on April 16, 2016. People rallied in relief efforts donating water and other first need products which greatly affected supermarket sales for several weeks after the earthquake.","metadata":{}},{"cell_type":"markdown","source":"# Imports\nFirst let us start by importing the relevant libraries that we need.","metadata":{}},{"cell_type":"code","source":"# Computational imports\nimport numpy as np   # Library for n-dimensional arrays\nimport pandas as pd  # Library for dataframes (structured data)\n\n# Helper imports\nimport os \nimport warnings\nimport pandas_datareader as web\nimport datetime as dt\n\n# ML/DL imports\nfrom keras.models import Sequential\nfrom keras.preprocessing.sequence import TimeseriesGenerator\nfrom keras.layers import LSTM, Dense, Dropout, RepeatVector, TimeDistributed\nfrom sklearn.preprocessing import MinMaxScaler, OrdinalEncoder\nfrom sklearn.metrics import mean_squared_error, mean_squared_log_error\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n\n# Plotting imports\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as dates\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport plotly.figure_factory as ff\nfrom plotly.subplots import make_subplots\nfrom plotly.offline import init_notebook_mode, iplot\n\n%matplotlib inline\nwarnings.filterwarnings(\"ignore\")\ninit_notebook_mode(connected=True)\n\n# Set seeds to make the experiment more reproducible.\nfrom numpy.random import seed\nseed(1)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Reading the Data\nLet's start by reading our data. We will store them in dataframes. ","metadata":{}},{"cell_type":"code","source":"path = '/kaggle/input/store-sales-time-series-forecasting/'\n\ntrain_data = pd.read_csv(path+'train.csv', index_col=0)\ntest_data = pd.read_csv(path+'test.csv', index_col=0)\ndata_oil = pd.read_csv(path+'oil.csv')\nsamp_subm = pd.read_csv(path+'sample_submission.csv')\ndata_holi = pd.read_csv(path+'holidays_events.csv')\ndata_store =  pd.read_csv(path+'stores.csv')\ndata_trans = pd.read_csv(path+'transactions.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Exploratory Data Analysis (EDA)\n## Exploring with Dataframe Analysis\nHere we will analyze each dataframe to to understand the type of data we are working with. To do this, we create a simple function that we can call whenver we want to do basic dataframe analysis.","metadata":{}},{"cell_type":"code","source":"def basic_eda(df):\n    print(\"-------------------------------TOP 5 RECORDS-----------------------------\")\n    print(df.head(5))\n    print(\"-------------------------------INFO--------------------------------------\")\n    print(df.info())\n    print(\"-------------------------------Describe----------------------------------\")\n    print(df.describe())\n    print(\"-------------------------------Columns-----------------------------------\")\n    print(df.columns)\n    print(\"-------------------------------Data Types--------------------------------\")\n    print(df.dtypes)\n    print(\"----------------------------Missing Values-------------------------------\")\n    print(df.isnull().sum())\n    print(\"----------------------------NULL values----------------------------------\")\n    print(df.isna().sum())\n    print(\"--------------------------Shape Of Data---------------------------------\")\n    print(df.shape)\n    print(\"============================================================================ \\n\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Litle bit of exploration of data\n\nprint(\"=================================Train Data=================================\")\nbasic_eda(train_data)\nprint(\"=================================Test data=================================\")\nbasic_eda(test_data)\nprint(\"=================================Holidays events=================================\")\nbasic_eda(data_holi)\nprint(\"=================================Transactions data=================================\")\nbasic_eda(data_trans)\nprint(\"=================================Stores data=================================\")\nbasic_eda(data_store)\nprint(\"=================================Oil data=================================\")\nbasic_eda(data_oil)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lucky for us, there seems not to be any missing values. This makes our lives easier.","metadata":{}},{"cell_type":"markdown","source":"## Exploring by plotting and analyzing graphs (using plotly)","metadata":{}},{"cell_type":"markdown","source":"### Sales Variation with store, family and clusters\nTo see the variation of sales per store, family and clusters in a subplot, we use plotly. ","metadata":{}},{"cell_type":"code","source":"# Creating one joined dataframe for visualization needs\ndf_visualization = train_data.merge(data_holi, on = 'date', how='left')\ndf_visualization = df_visualization.merge(data_oil, on = 'date', how='left')\ndf_visualization = df_visualization.merge(data_store, on = 'store_nbr', how='left')\ndf_visualization = df_visualization.merge(data_trans, on = ['date', 'store_nbr'], how='left')\ndf_visualization = df_visualization.rename(columns = {\"type_x\" : \"holiday_type\", \"type_y\" : \"store_type\"})\n\ndf_visualization['date'] = pd.to_datetime(df_visualization['date'])\ndf_visualization['year'] = df_visualization['date'].dt.year\ndf_visualization['month'] = df_visualization['date'].dt.month\ndf_visualization['week'] = df_visualization['date'].dt.isocalendar().week\ndf_visualization['quarter'] = df_visualization['date'].dt.quarter\ndf_visualization['day_of_week'] = df_visualization['date'].dt.day_name()\ndf_visualization[:3]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# data -------------------------------------------------------------------------------\ndf_store_sales = df_visualization.groupby('store_type').agg({\"sales\" : \"mean\"}).reset_index().sort_values(by='sales', ascending=False)\ndf_fam_sales = df_visualization.groupby('family').agg({\"sales\" : \"mean\"}).reset_index().sort_values(by='sales', ascending=False)[:10]\ndf_clus_sales = df_visualization.groupby('cluster').agg({\"sales\" : \"mean\"}).reset_index() \n\n# chart color -------------------------------------------------------------------------------\ndf_fam_sales['color'] = '#008000'\ndf_fam_sales['color'][3:] = '#00FF00'\ndf_clus_sales['color'] = '#00FF00'\n\n# chart -------------------------------------------------------------------------------\nfig = make_subplots(rows=2, cols=2, \n                    specs=[[{\"type\": \"bar\"}, {\"type\": \"pie\"}],\n                           [{\"colspan\": 2}, None]],\n                    column_widths=[0.7, 0.3], vertical_spacing=0, horizontal_spacing=0.02,\n                    subplot_titles=(\"Top 10 Highest Product Sales\", \"Highest Sales in Stores\", \"Clusters Vs Sales\"))\n\nfig.add_trace(go.Bar(x=df_fam_sales['sales'], y=df_fam_sales['family'], marker=dict(color= df_fam_sales['color']),\n                     name='Family', orientation='h'), \n                     row=1, col=1)\nfig.add_trace(go.Pie(values=df_store_sales['sales'], labels=df_store_sales['store_type'], name='Store type',\n                     marker=dict(colors=['#006400', '#008000','#228B22','#00FF00','#7CFC00','#00FF7F']), hole=0.7,\n                     hoverinfo='label+percent+value', textinfo='label'), \n                    row=1, col=2)\nfig.add_trace(go.Bar(x=df_clus_sales['cluster'], y=df_clus_sales['sales'], \n                     marker=dict(color= df_clus_sales['color']), name='Cluster'), \n                     row=2, col=1)\n\n# styling -------------------------------------------------------------------------------\nfig.update_yaxes(showgrid=False, ticksuffix=' ', categoryorder='total ascending', row=1, col=1)\nfig.update_xaxes(visible=False, row=1, col=1)\nfig.update_xaxes(tickmode = 'array', tickvals=df_clus_sales.cluster, ticktext=[i for i in range(1,17)], row=2, col=1)\nfig.update_yaxes(visible=False, row=2, col=1)\nfig.update_layout(height=500, bargap=0.2,\n                  margin=dict(b=0,r=20,l=20), xaxis=dict(tickmode='linear'),\n                  title_text=\"Average Sales Analysis\",\n                  template=\"plotly_white\",\n                  title_font=dict(size=25, color='#8a8d93', family=\"Lato, sans-serif\"),\n                  font=dict(color='#8a8d93'),\n                  hoverlabel=dict(bgcolor=\"#f2f2f2\", font_size=13, font_family=\"Lato, sans-serif\"),\n                  showlegend=False)\nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This allows us to get an initial understanding on how the sales varies depending on various variables.","metadata":{}},{"cell_type":"markdown","source":"### Sales variation by month and year\nTo see the variation of sales per month and year in one plot, we use plotly. ","metadata":{}},{"cell_type":"code","source":"# data \ndf_2013 = df_visualization[df_visualization['year']==2013][['month','sales']]\ndf_2013 = df_2013.groupby('month').agg({\"sales\" : \"mean\"}).reset_index().rename(columns={'sales':'s13'})\ndf_2014 = df_visualization[df_visualization['year']==2014][['month','sales']]\ndf_2014 = df_2014.groupby('month').agg({\"sales\" : \"mean\"}).reset_index().rename(columns={'sales':'s14'})\ndf_2015 = df_visualization[df_visualization['year']==2015][['month','sales']]\ndf_2015 = df_2015.groupby('month').agg({\"sales\" : \"mean\"}).reset_index().rename(columns={'sales':'s15'})\ndf_2016 = df_visualization[df_visualization['year']==2016][['month','sales']]\ndf_2016 = df_2016.groupby('month').agg({\"sales\" : \"mean\"}).reset_index().rename(columns={'sales':'s16'})\ndf_2017 = df_visualization[df_visualization['year']==2017][['month','sales']]\ndf_2017 = df_2017.groupby('month').agg({\"sales\" : \"mean\"}).reset_index()\ndf_2017_no = pd.DataFrame({'month': [9,10,11,12], 'sales':[0,0,0,0]})\ndf_2017 = df_2017.append(df_2017_no).rename(columns={'sales':'s17'})\ndf_year = df_2013.merge(df_2014,on='month').merge(df_2015,on='month').merge(df_2016,on='month').merge(df_2017,on='month')\n\n# top levels\ntop_labels = ['2013', '2014', '2015', '2016', '2017']\n\ncolors = ['#2EB62C', '#57C84D',\n          '#83D475', '#ABE098',\n          '#C5E8B7']\n\n# X axis value \ndf_year = df_year[['s13','s14','s15','s16','s17']].replace(np.nan,0)\nx_data = df_year.values\n\n# y axis value (Month)\ndf_2013['month'] =['Jan','Feb','Mar','Apr','Mai','Jun','Jul','Aug','Sep','Oct','Nov','Dec']\ny_data = df_2013['month'].tolist()\n\n# create plotly figure\nfig = go.Figure()\nfor i in range(0, len(x_data[0])):\n    for xd, yd in zip(x_data, y_data):\n        fig.add_trace(go.Bar(\n            x=[xd[i]], y=[yd],\n            orientation='h',\n            marker=dict(\n                color=colors[i],\n                line=dict(color='rgb(248, 248, 249)', width=1)\n            )\n        ))\n\nfig.update_layout(title='Avg Sales for each Year',\n    xaxis=dict(showgrid=False, \n               zeroline=False, domain=[0.15, 1]),\n    yaxis=dict(showgrid=False, showline=False,\n               showticklabels=False, zeroline=False),\n    barmode='stack',\n    plot_bgcolor='#fff', \n    paper_bgcolor='#fff',\n    margin=dict(l=0, r=50, t=100, b=10),\n    showlegend=False, \n)\n\nannotations = []\nfor yd, xd in zip(y_data, x_data):  \n    # labeling the y-axis\n    annotations.append(dict(xref='paper', yref='y',\n                            x=0.14, y=yd,\n                            xanchor='right',\n                            text=str(yd),\n                            font=dict(family='verdana', size=16,\n                                      color='rgb(67, 67, 67)'),\n                            showarrow=False, align='right'))\n    \n    # labeling the first Likert scale (on the top)\n    if yd == y_data[-1]:\n        annotations.append(dict(xref='x', yref='paper',\n                                x=xd[0] / 2, y=1.1,\n                                text=top_labels[0],\n                                font=dict(family='verdana ', size=16,\n                                          color='rgb(67, 67, 67)'),\n                                showarrow=False))\n        \n    space = xd[0]\n    for i in range(1, len(xd)):        \n            # labeling the Likert scale\n            if yd == y_data[-1]:\n                annotations.append(dict(xref='x', yref='paper',\n                                        x=space + (xd[i]/2), y=1.1,\n                                        text=top_labels[i],\n                                        font=dict(family='verdana ', size=16,\n                                                  color='rgb(67, 67, 67)'),\n                                        showarrow=False))\n            space += xd[i]\n            \nfig.update_layout(\n    annotations=annotations)\nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"With this graph we can see that the sales are highest generally for the month of december and they also increase through the years. This can be due to many reasons such as Christmas gifts.","metadata":{}},{"cell_type":"markdown","source":"### Analyzing the relationship of Sales and Transactions amount with Oil price","metadata":{}},{"cell_type":"code","source":"data_oil.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(data_oil.set_index('date').dcoilwtico, color='green', label=f\"Oil Price\")\nplt.title(\"Oil Price vs Days\")\nplt.xlabel(\"Days\")\nplt.ylabel(\"Oil Price\")\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We group by the date and find the mean valus for sales and transactions for those grouped dates.","metadata":{}},{"cell_type":"code","source":"train_data_per_date = train_data.groupby('date').agg({'sales': 'mean'}).reset_index()\ntrain_data_per_date['weekly_avg_sales'] = train_data_per_date['sales'].ewm(span=7, adjust=False).mean()\n\ntrain_data_per_date.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transactions_per_day = data_trans.groupby('date').agg({'transactions': 'mean'}).reset_index()\ntransactions_per_day['weekly_avg_transactions'] = transactions_per_day['transactions'].ewm(span=7, adjust=False).mean()\n\ntransactions_per_day.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let us compare sales and oil price","metadata":{}},{"cell_type":"code","source":"fig=make_subplots()\n\nfig.add_trace(go.Scatter(x=train_data_per_date.date,y=train_data_per_date.sales,name=\"Sales\"))\nfig.add_trace(go.Scatter(x=train_data_per_date.date,y=train_data_per_date.weekly_avg_sales,name=\"Weekly Sales\"))\n\n\nfig.add_trace(go.Scatter(x=data_oil.date,y=data_oil.dcoilwtico,name=\"Oil Price\"))\n\nfig.update_layout(autosize=True,width=900,height=500,title_text=\"Variation of Sales and Oil Price Through Time\")\nfig.update_xaxes(title_text=\"Days\")\nfig.update_yaxes(title_text=\"Prices\")\nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let us compare transactions and oil price","metadata":{}},{"cell_type":"code","source":"fig=make_subplots()\n\nfig.add_trace(go.Scatter(x=transactions_per_day.date,y=transactions_per_day.transactions,name=\"Transactions\"))\nfig.add_trace(go.Scatter(x=transactions_per_day.date,y=transactions_per_day.weekly_avg_transactions,name=\"Weekly Transactions\"))\n\nfig.add_trace(go.Scatter(x=data_oil.date,y=data_oil.dcoilwtico,name=\"Oil Price\"))\n\nfig.update_layout(autosize=True,width=900,height=500,title_text=\"Variation Transactions and Oil Price Through Time\")\nfig.update_xaxes(title_text=\"Days\")\nfig.update_yaxes(title_text=\"Prices\")\nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can also create a correlation matrix to see the correlation between the various variables.","metadata":{}},{"cell_type":"code","source":"data_oil['sales'] = train_data_per_date['sales']\ndata_oil['transactions'] = transactions_per_day['transactions']\n\ndata_oil.corr()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Analyzing the graphs and the correlation matrix, we get the understanding that there is no strong correlation between oil and transactions. There is slight inversion correlation between sales and oil price (representing loosely the economic status of a country)","metadata":{}},{"cell_type":"markdown","source":"# Training the model","metadata":{}},{"cell_type":"code","source":"path = '/kaggle/input/store-sales-time-series-forecasting/'\n\ntrain_data = pd.read_csv(path+'train.csv', index_col=0)\ntest_data = pd.read_csv(path+'test.csv', index_col=0)\ndata_oil = pd.read_csv(path+'oil.csv')\nsamp_subm = pd.read_csv(path+'sample_submission.csv')\ndata_holi = pd.read_csv(path+'holidays_events.csv')\ndata_store =  pd.read_csv(path+'stores.csv')\ndata_trans = pd.read_csv(path+'transactions.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Explore the training data","metadata":{}},{"cell_type":"code","source":"basic_eda(train_data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(min(train_data['date']))\nprint(max(train_data['date']))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's find the numerical and categorical columns. This is important if you want to later standardize your numerical data or Encode your categorical data.","metadata":{}},{"cell_type":"code","source":"object_cols = [cname for cname in train_data.columns \n               if train_data[cname].dtype == \"object\" \n               and cname != \"date\"]\n\nprint(\"Categorical variables:\")\nobject_cols ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_cols = [cname for cname in train_data.columns \n            if train_data[cname].dtype in ['int64', 'float64']]\n\nprint(\"Numerical variables:\")\nnum_cols ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_cols = num_cols + object_cols\nprint(all_cols)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Taking care of Categorical Features\nLet us transform the categorical columns into numerical ones so that we can use them as features.","metadata":{}},{"cell_type":"code","source":"ordinal_encoder = OrdinalEncoder()\ntrain_data[object_cols] = ordinal_encoder.fit_transform(train_data[object_cols])\ntrain_data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Scaling of Numerical Features\nWe have to standardize our numerical data so that our leaning algorithm can perform better.","metadata":{}},{"cell_type":"code","source":"scaler = MinMaxScaler(feature_range=(0,1))\n\nfor col in num_cols:\n    scaled_data = scaler.fit_transform(train_data[col].values.reshape(-1,1))\n    train_data[col] = pd.Series(scaled_data.flatten())\n\ntrain_data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Grouping the Data\nWe need to group the data by the dates. This will make the prediction much easier.","metadata":{}},{"cell_type":"code","source":"train_data = train_data.groupby(['date']).agg({'sales':'mean', 'onpromotion':'mean'})\ntrain_data.tail()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train = train_data.copy()\ny_train = train_data.sales.copy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Transforming the Input data into Time-Series data\nWe have to transform the training data into time-series accepte sequences to be then fed to our model. We have decided to use keras TimeseriesGenerator to create those sequences. You can also choose to seperate into sequences without using this function.","metadata":{}},{"cell_type":"code","source":"num_feature_input = len(x_train.columns)\nhistory_input = 30\n\n\"\"\"\nlength: Number of past time steps to be included, \nbatch_size: The amount of predicted days. Here we have 1 since we are trying to predict the next day using the last 30 days.\n\"\"\"\ngenerator = TimeseriesGenerator(x_train, y_train, length=history_input, batch_size = 1)\n\n# Print the first sequence, you should see 30 past day (x) for 1 predicted day (y)\nfor i in range(len(generator)):\n    x, y = generator[i]\n    print('%s => %s' % (x, y))\n    break","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(generator))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def Multi_Step_LSTM_model():\n    \n    # Use Keras sequential model\n    model = Sequential()    \n    \n    # First LSTM layer with Dropout regularisation; Set return_sequences to True to feed outputs to next layer\n    model.add(LSTM(units = 50, activation='relu', return_sequences = True, input_shape = (history_input, num_feature_input))) \n    model.add(Dropout(0.2))\n    \n    # Second LSTM layer with Dropout regularisation; Set return_sequences to True to feed outputs to next layer\n    model.add(LSTM(units = 50,  activation='relu', return_sequences = True))                                    \n    model.add(Dropout(0.2))\n    \n    # Final LSTM layer with Dropout regularisation; Set return_sequences to False since now we will be predicting with the output layer\n    model.add(LSTM(units = 50))\n    model.add(Dropout(0.2))\n    \n    # The output layer with linear activation to predict Open stock price\n    model.add(Dense(units=1, activation = \"linear\"))\n    \n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Multi_Step_LSTM_model()\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we set our compiler and our optimatization mechanism. We will be using the Adam optimazation method since it is widely used and performs much better than regular gradient descent.","metadata":{}},{"cell_type":"code","source":"model.compile(optimizer='adam', loss='mean_squared_error', metrics = ['accuracy'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.fit_generator(generator, steps_per_epoch=len(generator), epochs=20, verbose=2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Save and Load model if needed\nI have commented the two code blocks out. Uncomment and use if you need to:","metadata":{}},{"cell_type":"markdown","source":"a) save the model","metadata":{}},{"cell_type":"code","source":"# # serialize model to JSON\n# model_json = model.to_json()\n# with open(\"model2.json\", \"w\") as json_file:\n#     json_file.write(model_json)\n# # serialize weights to HDF5\n# model.save_weights(\"model2.h5\")\n# print(\"Saved model to disk\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"b) load the model","metadata":{}},{"cell_type":"code","source":"# # load json and create model\n# json_file = open('model.json', 'r')\n# loaded_model_json = json_file.read()\n# json_file.close()\n# loaded_model = model_from_json(loaded_model_json)\n# # load weights into new model\n# loaded_model.load_weights(\"model.h5\")\n# print(\"Loaded model from disk\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Predicting on Test data (WIP)\nHere we keep 30 past days from the training data to help us start predicting the test data.","metadata":{}},{"cell_type":"code","source":"# split a univariate sequence into samples\ndef split_sequence(data, days_past, days_future):\n    X, y = list(), list()\n    \n    for i in range(len(data)):        \n        # find the end of this pattern\n        end_ix = i + days_past\n        out_end_ix = end_ix + days_future\n        \n        # check if we are beyond the sequence\n        if out_end_ix > len(data):\n            break\n            \n        # gather input and output parts of the pattern\n        seq_x, seq_y = data[i:end_ix], data[end_ix:out_end_ix]\n        X.append(seq_x)\n        y.append(seq_y)\n        \n    return np.array(X), np.array(y)","metadata":{"execution":{"iopub.status.busy":"2021-10-30T21:48:53.156064Z","iopub.execute_input":"2021-10-30T21:48:53.156321Z","iopub.status.idle":"2021-10-30T21:48:53.162662Z","shell.execute_reply.started":"2021-10-30T21:48:53.156291Z","shell.execute_reply":"2021-10-30T21:48:53.161626Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"full_dataset = pd.concat([train_data, test_data], ignore_index=True, sort=False)\nfull_dataset = full_dataset.iloc[3000887-5:,:]\nfull_dataset","metadata":{"execution":{"iopub.status.busy":"2021-10-30T21:51:21.843362Z","iopub.execute_input":"2021-10-30T21:51:21.843904Z","iopub.status.idle":"2021-10-30T21:51:22.011209Z","shell.execute_reply.started":"2021-10-30T21:51:21.843864Z","shell.execute_reply":"2021-10-30T21:51:22.010468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"full_dataset = full_dataset.groupby(['date']).agg({'sales':'mean', 'onpromotion':'mean'})\nfull_dataset","metadata":{"execution":{"iopub.status.busy":"2021-10-30T21:50:20.407599Z","iopub.execute_input":"2021-10-30T21:50:20.408133Z","iopub.status.idle":"2021-10-30T21:50:20.421417Z","shell.execute_reply.started":"2021-10-30T21:50:20.408095Z","shell.execute_reply":"2021-10-30T21:50:20.420167Z"},"trusted":true},"execution_count":null,"outputs":[]}]}