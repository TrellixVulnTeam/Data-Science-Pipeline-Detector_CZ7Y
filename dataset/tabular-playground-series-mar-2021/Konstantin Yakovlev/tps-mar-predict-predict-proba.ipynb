{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os, sys, gc, warnings, random\n\n## Sklearn utils\nfrom sklearn.metrics import roc_auc_score\n\n## Turn off warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"########################### Initial Vars\n###########################################################\nTARGET    = 'target'   # Our Target\nSEED      = 42         # Base SEED\nPATH      = '../input/tabular-playground-series-mar-2021/'\n\ncat_cols = ['cat'+str(i) for i in range(19)]  # Categorial Columns\ncnt_cols = ['cont'+str(i) for i in range(11)] # Continuous Columns \n\nremove_features = ['id',TARGET] # Features that we will not use for training\ntrain_df = pd.read_csv(PATH+'train.csv')\nfor col in cat_cols:   \n    train_df[col] = train_df[col].astype('category')\n    \nfeatures_columns = [col for col in list(train_df) if col not in remove_features]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"########################### Models params\n###########################################################\nlgb_params = {\n                'boosting_type': 'gbdt',\n                'objective': 'binary',\n                'metric': 'auc',\n                'n_estimators': 200,\n                'learning_rate': 0.05,\n                'num_leaves': 2**7,\n                'min_data_in_leaf': 2**8,\n                'feature_fraction': 0.7,\n                'subsample': 0.7,\n                'subsample_freq': 1,\n                'early_stopping_rounds': 100,\n                'boost_from_average': True,\n                'seed': SEED,\n                'verbose': -1\n            }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# There are 2 ways to train lgb (same for xgb and catboost)\n# 1. create booster and call train\n# 2. use \"sklearn version\" and use fit","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## 1. Type 1\nimport lightgbm as lgb\ntrain_data = lgb.Dataset(train_df[features_columns], label=train_df[TARGET])\nestimator = lgb.train(\n                          lgb_params,\n                          train_data,\n                          valid_sets = [train_data],\n                          verbose_eval = 100,\n                        )\n\n# And we can call normal predict here\nprint(roc_auc_score(train_df[TARGET], estimator.predict(train_df[features_columns])))\n\n# Predict_proba will not even work this way\ntry:\n    print(estimator.predict_proba(train_df[features_columns]))\nexcept:\n    print('Ups...')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## 2. Type 2\n## Call \"sklean type API\" / or better call it wrapper\nfrom lightgbm import LGBMClassifier\nestimator = LGBMClassifier(**lgb_params)\nestimator.fit(train_df[features_columns], train_df[TARGET], \n              eval_set=(train_df[features_columns], train_df[TARGET]),\n              verbose = 100)\n\n# And here we need to call predict_proba\nprint(roc_auc_score(train_df[TARGET], estimator.predict_proba(train_df[features_columns])[:,1]))\n\n# Normal predict will not give right values\nprint(roc_auc_score(train_df[TARGET], estimator.predict(train_df[features_columns])))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}