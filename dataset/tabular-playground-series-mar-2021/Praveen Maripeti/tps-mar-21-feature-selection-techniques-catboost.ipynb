{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Problem description\n\nIn this challenge, we have to predict a binary target using a number of continuous and categorical features. There are 19 categorical features and 11 continuous features. Further, there are 300000 samples in training data and 200000 samples in test data."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Install Data Analysis Baseline Library for automated data analysis\n!pip install dabl ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Import necessary libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport dabl\nimport itertools\n\nfrom pandas_profiling import ProfileReport\n\n# Preprocessing\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.feature_selection import SelectKBest, chi2, mutual_info_classif\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\n# Classifiers\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\n\nfrom sklearn.metrics import roc_auc_score, plot_roc_curve, confusion_matrix \n\nimport warnings \nwarnings.filterwarnings('ignore') # silence warnings","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load data"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"../input/tabular-playground-series-mar-2021/train.csv\")\ntest = pd.read_csv(\"../input/tabular-playground-series-mar-2021/test.csv\")\nsample = pd.read_csv(\"../input/tabular-playground-series-mar-2021/sample_submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# EDA with DABL"},{"metadata":{"trusted":true},"cell_type":"code","source":"dabl.plot(train, \"target\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Encode categorical features"},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_features = [feat for feat in train.columns if feat[:3]=='cat'] # select categorical features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def check_new_labels_in_test(train_set, test_set, cat_features=cat_features):\n    \"\"\"\n    This function checks for any new labels in categorical features in test set which are not present in train set    \"\"\"\n    \n    df = pd.DataFrame()\n    for feat in cat_features:\n        set_tr = set(train_set[feat].values)\n        set_te = set(test_set[feat].values)\n        diff = set_te - set_tr    # get new labels present in test set and not in train set\n        df.loc[0, feat] = len(diff)\n    print(df)\n\ncheck_new_labels_in_test(train, test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are 8 new labels in test set of cat10 feature which are absent in train set. This needs to be fixed before using LabelEncoder to encode the categorical features."},{"metadata":{"trusted":true},"cell_type":"code","source":"def encode_cat_features(train_set, test_set, cat_features=cat_features): \n    \n    \"\"\"\n    This function fits LabelEncoder on train data and transforms train and test data. If a feature contains new labels in test data, \n    first the train and test data will be merged and LabelEncoder will be fitted on merged data \n    \"\"\"\n    le = LabelEncoder()\n\n    for feat in cat_features:\n        le.fit(train_set[feat])\n        train_set[feat] = le.transform(train_set[feat])\n        try:\n            test_set[feat] = le.transform(test_set[feat])\n        except ValueError:\n            train_set[feat] = le.inverse_transform(train_set[feat]) # get the labels back before merging train and test data\n            le.fit(pd.concat([train_set[feat], test_set[feat]], axis=0))\n            train_set[feat] = le.transform(train_set[feat])\n            test_set[feat] = le.transform(test_set[feat])\n    return train_set, test_set        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_enc, test_enc = encode_cat_features(train, test)\n#train_enc[cat_features] = train_enc[cat_features].astype('category') \ny = train_enc['target']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Selection of K top categorical features\n\nThe data contains 19 categorical features. Although we would prefer to have as many features as we can in the hope of getting a reasonably accurate model, it is often the case that the variance in target is better explained by only a subset of the features. I will perform feature selection of catgorical features based on mutual information between the features and target to see if it leads to any improvement in model performance over using all the features. "},{"metadata":{"trusted":true},"cell_type":"code","source":"def cat_feature_selection(X_cat=train_enc[cat_features], y=y, top_feats=8, print_fs_score=False, train_enc=train_enc):\n    \"\"\"\n    This function selects k top features based on mutual information between features and target\n    \"\"\"\n    fs_mutual_info = mutual_info_classif(X_cat, y, random_state=1)\n    if print_fs_score:\n        print(fs_mutual_info)\n    top_features = fs_mutual_info.argsort()[-top_feats:][::-1]\n    X_post_fs = train_enc.iloc[:, [i+1 for i in top_features]]\n    return X_post_fs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_cat_fs = cat_feature_selection(print_fs_score=True)\nX_cat_fs.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Feature selection based on mutual information corroborates the importance of categorical features detected in DABL analysis. Categorical features in decreasing order of importance as per both DABL and Mutual Info are the same: cat16, cat15, cat18, cat1 and so on. "},{"metadata":{},"cell_type":"markdown","source":"# Continuous features\n\nThere are 11 continous features. Let's plot a heatmap showing correlation of these features with target"},{"metadata":{"trusted":true},"cell_type":"code","source":"cont_features = [feat for feat in train.columns if feat[:4]=='cont']\nX_cont = pd.concat([train[cont_features], train[\"target\"]], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_heatmap(df, width, height):\n    \"\"\"\n    Plot heatmap of correlation matrix in specified height and width\n    \"\"\"\n    sns.set_style('whitegrid')\n    plt.subplots(figsize=(width, height))\n\n    mask = np.zeros_like(df.corr(), dtype=np.bool)\n    mask[np.triu_indices_from(mask)] = True\n\n    sns.heatmap(df.corr(), \n                cmap=sns.diverging_palette(250, 15, s=75, l=40,n=9, center=\"dark\"), \n                mask = mask, \n                annot=True, \n                center = 0)\n    plt.title(\"Correlation Heatmap\")\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_heatmap(X_cont, 18, 12)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is a moderate correlation between target and some features such as cont5, cont6 etc and very weak correlation between target and some other features like cont0, cont7 etc. There is also a problem of multicollinearity among the independent variables as some of them are highly correlated: cont1 & cont2, cont0 & cont10, cont7 & cont10 and so on. It is important to minimize or eliminate multicollinearity as it undermines the statistical power of the model. \n\nTo identify the degree of multi-collinearity in the data, I will use Variance Inflation Factor. VIF is equal to the ratio of the overall model variance to the variance of a model that includes only a single independent variable. This ratio is calculated for each independent variable. A high VIF indicates that the associated independent variable is highly collinear with the other variables in the model. A VIF of 1 indicates that the two variables are not correlated, 5 indicates moderate collinearity and 10 indicates high collinearity. I would try to achieve a VIF score below 10 for the remaining features after removing some highly correlated features."},{"metadata":{"trusted":true},"cell_type":"code","source":"vif = pd.DataFrame()\nvif[\"features\"] = train[cont_features].columns\nvif[\"vif_Factor\"] = [variance_inflation_factor(train[cont_features].values, i) for i in range(train[cont_features].shape[1])]\nvif","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is high correlation between independent variables as can be seen from the high VIF score of some features"},{"metadata":{"trusted":true},"cell_type":"code","source":"vif_feats = ['cont3','cont4','cont5','cont6','cont8', 'cont9']\nvif = pd.DataFrame()\nvif[\"features\"] = train[vif_feats].columns\nvif[\"vif_Factor\"] = [variance_inflation_factor(train[vif_feats].values, i) for i in range(train[vif_feats].shape[1])]\nvif","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This subset of features have better VIF scores than the complete set of features. I will see if this leads to an improvement in model performance."},{"metadata":{"trusted":true},"cell_type":"code","source":"X_cont_fs = pd.concat([train[vif_feats], train[\"target\"]],axis=1)\nplot_heatmap(X_cont_fs, 10, 7)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Modelling with all features"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Select independent and dependent variables\nX = train_enc.iloc[:,1:-1]\ny = train_enc.iloc[:,-1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"skf = StratifiedKFold(n_splits=2, shuffle=True, random_state=111) # split data into multiple folds for reliable results","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def run_model(model,skf=skf, X=X, y=y, get_pred=False, plot_cm=False, cmap=plt.cm.Reds, **params): \n    \"\"\"\n    This function trains a given model with chosen parameters on training data and generates predictions on validation data. \n    If plot_cm is True, it plots confusion matrix on full data.  \n    \"\"\"\n    cv_score = [] # container to compute mean cv scores\n    i = 1\n\n    for train_idx, val_idx in skf.split(X, y):  # stratified split of train data\n        print(f\"{i} of KFold {skf.n_splits}\")\n        X_train, X_val = X.loc[train_idx], X.loc[val_idx]\n        y_train, y_val = y.loc[train_idx], y.loc[val_idx]\n\n        modl = model(**params)\n        modl.fit(X_train, y_train)\n        score = roc_auc_score(y_val, modl.predict(X_val))\n        print(f\"ROC AUC Score: {score:.4f}\")\n        cv_score.append(score)\n        i+=1\n\n    print(f\"CV Scores: {[round(val, 4) for val in cv_score]} \\n Mean CV Score: {np.mean(cv_score):.4f}\")\n    \n    if plot_cm:\n        # plot confusion matrix\n        pred_all_y = modl.predict_proba(X)[:,1] # predict all targets for plotting confusion matrix \n        plt.figure(figsize=(8,5))\n        cm = confusion_matrix(y,np.where(pred_all_y > 0.5, 1, 0))\n        plt.imshow(cm, interpolation='nearest', cmap=cmap)\n        plt.title(f\"Confusion Matrix - {modl.__class__.__name__}\")\n        plt.colorbar()\n        classes = [0,1]\n        tick_marks = np.arange(len(classes))\n        plt.xticks(tick_marks, classes, rotation=0)\n        plt.yticks(tick_marks, classes)\n        plt.tight_layout()\n        plt.ylabel('True class')\n        plt.xlabel('Predicted class')\n        plt.grid(False)\n        thresh = cm.max() / 2.\n        for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n            plt.text(j, i, cm[i, j],\n                     horizontalalignment=\"center\",\n                     color=\"white\" if cm[i, j] > thresh else \"black\")\n        plt.show()        \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Logistic Regression with default parameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {'C':1}\nrun_model(LogisticRegression, plot_cm=True, **params)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# XGBoost with default parameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"run_model(XGBClassifier, params=None, plot_cm=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"XGBoost classifier has done a marginally better job classifying the target as compared to Logistic regression (both with default parameters)."},{"metadata":{},"cell_type":"markdown","source":"# CatBoostClassifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {'verbose':False}\nrun_model(CatBoostClassifier, plot_cm=True, **params)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see slight improvement in ROC AUC score with CatBoostClassifier over XGBClassifier"},{"metadata":{},"cell_type":"markdown","source":"# Applying feature selection for continuous and categorical features\n\nI will select a subset of features which are more associated with the target than the rest and train the model on these features. Applying mutual information for categorical features and variance inflation factor for continuous features in the previous stage, I have selected the top 14 features which explain the variance of target the most. Let's see how the results may vary when we use only 14 features instead of the initial 30 features."},{"metadata":{"trusted":true},"cell_type":"code","source":"X_fs = pd.concat([train_enc[X_cat_fs.columns], train_enc[vif_feats]],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {'verbose':False}\nrun_model(CatBoostClassifier, X=X_fs, plot_cm=True, **params) # using CatBoostClassifier as it gave best results","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"At the first glance, it is clear that ROC AUC score dropped after feature selection. However, the drop in score is a miniscule 0.0076 whereas we were able to reduce the number of features by 53.33% which is quite sigificant. This confirms our assumption with this dataset that only a subset of the given features mainly contribute to model's learning. The absence of the remaining features hardly had an effect. One can also compare the confusion matrices and see how similar they are."},{"metadata":{},"cell_type":"markdown","source":"# Model selection, training and submission of predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"# model will be trained on entire training set\nX_train = train_enc.iloc[:, 1:-1]\ny_train = train_enc['target']\n\nX_test = test_enc.iloc[:, 1:]\ntest_ids = test_enc['id']\n\n# Choosing CatBoostClassifier as it gave best results with default parameters\ncbc = CatBoostClassifier(verbose=False)\ncbc.fit(X_train, y_train)\ntest_pred = cbc.predict(X_test) # test predictions\n\n# generate solution for submission\nsub = pd.DataFrame()\nsub['id'] = test_ids\nsub['target'] = test_pred\nsub.to_csv('My submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Next steps\n\nIn the next segment, I will try hyperparamater tuning to improve the results...\n\nIf you like my work, kindly upvote. Thanks for reading through."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}