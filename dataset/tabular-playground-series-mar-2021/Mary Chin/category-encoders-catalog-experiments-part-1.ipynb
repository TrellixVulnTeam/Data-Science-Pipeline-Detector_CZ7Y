{"cells":[{"metadata":{},"cell_type":"markdown","source":"We shall explore 24 encoders from 4 libraries:\n\n| library | one-hot encoders | other simple encoders | contrast encoders | target/Bayesian encoders |\n| --- | --- | --- | --- | --- |\n| [sklearn.preprocessing](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing) | OneHotEncoder | LabelEncoder <br> OrdinalEncoder <br> LabelBinarizer | |\n| [category_encoders](https://contrib.scikit-learn.org/category_encoders) | OneHotEncoder | OrdinalEncoder <br> BinaryEncoder <br> BaseNEncoder <br> CountEncoder <br> HashingEncoder| HelmertEncoder <br> SumEncoder <br> BackwardDifferenceEncoder <br> PolynomialEncoder | TargetEncoder <br> MEstimateEncoder <br> WOEEncoder <br> JamesSteinEncoder <br> LeaveOneOutEncoder <br> CatBoostEncoder <br> GLMMEncoder |\n| [pandas](https://pandas.pydata.org) | get_dummies | factorize | | |\n| [keras.utils](https://keras.io/api/utils) | to_categorical | | | |\n\n<br>\nEncoders map the original categories (often dtype=string) to a set of representing values (often dtype=int for simple encoders; dtype=float for target encoders). This notebook walks through a tour of the encoders listed in the table, exploring each non-target encoder one by one, producing a comparison table at the end. Target encoders shall be explored in detail in a separate notebook.\n<br><br>\nWhen to use which encoder to solve what problems? There is a good guide here: [Encode Smarter: How to Easily Integrate Categorical Encoding into Your Machine Learning Pipeline](https://innovation.alteryx.com/encode-smarter)."},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"from sklearn import preprocessing\nfrom category_encoders import OrdinalEncoder, OneHotEncoder, BinaryEncoder, BaseNEncoder, CountEncoder, HashingEncoder\nfrom category_encoders import HelmertEncoder, SumEncoder, BackwardDifferenceEncoder, PolynomialEncoder\nfrom category_encoders import TargetEncoder, MEstimateEncoder, WOEEncoder, JamesSteinEncoder, LeaveOneOutEncoder, CatBoostEncoder, GLMMEncoder\nfrom keras import utils\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport warnings, gc, time\nwarnings.simplefilter('ignore') # once | error | always | default | module\n\n# We shall be compiling a summary table as we go along.\nsummary = pd.DataFrame({'inp2out_map': pd.Series(dtype=object),   # input-to-output map\n                        'nunique'    : pd.Series(dtype=int),      # number of unique (or distinct) values in output\n                        'unique'     : pd.Series(dtype='object'), # unique values in output\n                        'shape'      : pd.Series(dtype=int),      # rows-by-columns of output array\n                        'tictoc'     : pd.Series(dtype=int)})     # computation time i seconds\nsummary.index.name = 'encoder'\n# The grand summary is printed at the end of this notebook.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/tabular-playground-series-mar-2021/train.csv', index_col='id') # [['cat10', 'cat5', 'target']]\ntrain.sample(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Is encoding optional?\nNot always. Some packages can't digest string-type data without encoding. 'Donkey', 'horse' and 'mule', for instance, would not work whereas 0, 1 and 2 would.\n\nEven when the package can digest data without encoding, they sometimes learn encoded data better."},{"metadata":{},"cell_type":"markdown","source":"# Encoder types: a broad-stroke scan\nWe've got 2 dozen encoders here. Let's take an overview by trying to group them into families according to observable behaviors."},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# Would the output differ whether or not we supply the target as input?\n# Let's run a test with 10 encoders which optionally accept the target as input:\npick = train.columns[train.columns.str.startswith('cat')]\nfor ncoda in [OrdinalEncoder, HelmertEncoder, SumEncoder, OneHotEncoder, BinaryEncoder, BaseNEncoder, CountEncoder, BackwardDifferenceEncoder]:\n    tis = ncoda().fit_transform(train[pick])\n    tat = ncoda().fit_transform(train[pick], train['target']) \n#   Print 'True' if same; print 'False' otherwise\n    print((tis==tat).all().all(), ncoda)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Some encoders use the target for computing the output; they can't run without being given the target. These are the target encoders.\nfor ncoda in [TargetEncoder, MEstimateEncoder, WOEEncoder, JamesSteinEncoder, LeaveOneOutEncoder, CatBoostEncoder, GLMMEncoder]:\n    try:\n#       Run without train['target']:\n        tis = ncoda().fit_transform(train[pick])\n        print('Passed:', ncoda)    \n    except Exception as complaint:\n        print(complaint)\n        print('See, told ya it was going to break:', ncoda)    \n    gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's do a scan for encoders which output a column named, 'intercept', which suggests contrast encoding, which we will see in the last section.\nfor ncoda in [OrdinalEncoder, OneHotEncoder, BinaryEncoder, BaseNEncoder, CountEncoder, HashingEncoder,\n              HelmertEncoder, SumEncoder, BackwardDifferenceEncoder, PolynomialEncoder]:\n    out = ncoda().fit_transform(train[pick])\n    if 'intercept' in out.columns:\n        print(str(ncoda))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# One-to-one simple, target-independent encoders"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's zoom into a single column.\ntrain['cat10'].nunique(), train['cat10'].unique()\n# cat10 alone has 299 unique values altogether. This value in termed 'cardinality'.\n# This is an extreme case. Cardinalities are usually lower e.g. exam grades = A, B, C, D, E would have cardinality=5.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nfor which in [preprocessing.LabelEncoder, preprocessing.OrdinalEncoder, OrdinalEncoder,  # Section 1\n              preprocessing.OneHotEncoder, OneHotEncoder,                                # Section 2\n              preprocessing.LabelBinarizer, BinaryEncoder, BaseNEncoder,                 # Section 3\n              CountEncoder,                                                              # Section 4\n              HelmertEncoder, SumEncoder, BackwardDifferenceEncoder]:                    # Section 5\n    if which==preprocessing.OrdinalEncoder or which==preprocessing.OneHotEncoder: \n        inp = train['cat10'].values.reshape(-1, 1)\n    else:\n        inp = train['cat10']\n\n    tic = time.time()\n    if which==preprocessing.OneHotEncoder: \n        out = which(sparse=False).fit_transform(inp)\n    else:\n        out = which().fit_transform(inp)\n    tictoc = time.time() - tic\n\n    inp2out_map = pd.concat([pd.DataFrame({'inp': train['cat10']}, columns=['inp']),\n                             pd.DataFrame(out, index=train.index)], axis=1).drop_duplicates()\n    inp2out_map.set_index('inp', inplace=True, drop=True)\n    unik = np.unique(inp2out_map.values)\n#   Grab the label, apply some minor hiding cosmetics:\n    label = str(which).replace(\"<class '\", \"\").replace(\"'>\", \"\")\n    if inp2out_map.isnull().any().any():\n        print(label, \"doesn't map one-to-one\")\n    summary.loc[label] = inp2out_map, len(unik), unik, inp2out_map.shape, tictoc\ncolumns_show = ['nunique', 'unique', 'shape', 'tictoc']\nsummary[columns_show]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1. Label & Ordinal encoders\nFrom the table we find the first 3 rows:\n* sklearn.preprocessing._label.LabelEncoder\n* sklearn.preprocessing._encoders.OrdinalEncoder\n* category_encoders.ordinal.OrdinalEncoder\n\nrather similar to each other:\n* they all output 299 unique numbers, where 299 is the cardinality of the original input;\n* they all output a single column;\n* they basically do one-to-one mapping of the original input;\n* they run quickly compared to the rest."},{"metadata":{},"cell_type":"markdown","source":"## 1.1 LabelEncoder vs OrdinalEncoder\n* LabelEncoder encodes one variable at a time; meant for encoding target labels (as in classification problems). \n* OrdinalEncoder encodes multiple variables/columns at a time; meant for encoding features (plural).\n\nLet's see that in action:"},{"metadata":{"trusted":true},"cell_type":"code","source":"try:\n    out = LabelEncoder().fit_transform(train[['cat10', 'cat5']])\nexcept Exception as complaint:\n    print(complaint)\n    print('See, told ya it was going to break.')    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"out = OrdinalEncoder().fit_transform(train[['cat10', 'cat5']])\n# no complains","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1.2 pandas does label encoding too"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def redressOutput(out):\n    inp2out_map = pd.concat([pd.DataFrame({'inp': train['cat10']}, columns=['inp']),\n                             pd.DataFrame(out, index=train.index)], axis=1).drop_duplicates()\n    inp2out_map.set_index('inp', inplace=True, drop=True)\n    unik = np.unique(inp2out_map.values)\n    return inp2out_map, len(unik), unik, inp2out_map.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tic = time.time()\nout = pd.factorize(train['cat10'])[0]\ntictoc = time.time() - tic\nsummary.loc['pd.factorize'] = redressOutput(out) + (tictoc, )\n\nlabelordinal_encoders = ['sklearn.preprocessing._label.LabelEncoder',\n                         'sklearn.preprocessing._encoders.OrdinalEncoder',\n                         'category_encoders.ordinal.OrdinalEncoder',\n                         'pd.factorize']\nsummary.loc[labelordinal_encoders, columns_show ]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# like scikit's LabelEncoder, pd.factorize can only handle one column at a time\ntry:\n    out = pd.factorize(train[['cat10', 'cat5']])\nexcept Exception as complaint:\n    print(complaint)\n    print('See, told ya it was going to break.')    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. One-hot encoders\n## 2.1 by scikit-learn and catagory-encoders"},{"metadata":{"trusted":true},"cell_type":"code","source":"summary.loc[ summary.index.str.contains('OneHot') , columns_show ]\n# We've got two one-hot encoders so far. One from sklearn.preprocessing; another by category_encoders. Both work in a similar way. We can use either.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Compared to label and ordinal encoders, we find that with one-hot encoders:\n* ```nunique``` dropped from 299 to 2;\n* the number of columns increased from 1 to 299.\n\nLet's see how a one-hot encoder maps input to output:"},{"metadata":{"trusted":true},"cell_type":"code","source":"inp2out_map = summary.loc['category_encoders.one_hot.OneHotEncoder', 'inp2out_map']\ninp2out_map","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"One-hot encoding is thus name because for each row there is strictly one 1; all other columns must be zero. Let's do a quick check:"},{"metadata":{"trusted":true},"cell_type":"code","source":"for row_idx, row_data in inp2out_map.iterrows():\n    vcount = row_data.value_counts().sort_index()\n    if not (vcount==pd.Series({0: 298, 1: 1})).all():\n        print('oopsy')\n# Loop passes without any oopsy, confirming that each row had strictly 1 one and 298 zeros.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's take the chance to visualise the input-to-output mapping.\nplt.imshow(inp2out_map, cmap='gray'); plt.axis('equal'); _ = plt.axis('off')\n# black = zero; white = one. We find strictly 1 one on each row, zero everywhere else.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.2 by pandas"},{"metadata":{"trusted":true},"cell_type":"code","source":"tic = time.time()\nout = pd.get_dummies(train['cat10'])\ntictoc = time.time() - tic\nsummary.loc['pd.get_dummies'] = redressOutput(out) + (tictoc, )\n\nonehot_encoders = ['sklearn.preprocessing._encoders.OneHotEncoder',\n                   'category_encoders.one_hot.OneHotEncoder',\n                   'pd.get_dummies']\nsummary.loc[onehot_encoders, columns_show ]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.3 by keras\nBut with numeric input only. ```cat10``` is string, not numeric. We would need to first convert from string to numeric."},{"metadata":{"trusted":true},"cell_type":"code","source":"try:\n    utils.to_categorical(train['cat10'])\nexcept Exception as complaint:\n    print(complaint)\n    print('See, told ya it was going to break.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tic = time.time()\nborrow = preprocessing.LabelEncoder().fit_transform(train['cat10'])\nout = utils.to_categorical(borrow)\ntictoc = time.time() - tic\nsummary.loc['utils.to_categorical'] = redressOutput(out) + (tictoc, )\n\nonehot_encoders = ['sklearn.preprocessing._encoders.OneHotEncoder',\n                   'category_encoders.one_hot.OneHotEncoder',\n                   'pd.get_dummies',\n                   'utils.to_categorical']\nsummary.loc[onehot_encoders, columns_show ]\n# We have at our disposal 4 one-hot encoders by different libraries.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Warning\n```keras.utils.to_categorical``` doesn't work with negative input."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def redressOutput(out):\n    inp2out_map = pd.concat([pd.DataFrame({'inp': train['cat10']}, columns=['inp']),\n                             pd.DataFrame(out, index=train.index)], axis=1).drop_duplicates()\n    inp2out_map.set_index('inp', inplace=True, drop=True)\n    unik = np.unique(inp2out_map.values)\n    return inp2out_map, len(unik), unik, inp2out_map.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"inp = [0, 1, 2, 3, 4]\nout = utils.to_categorical(inp)\nlen(pd.DataFrame(out).drop_duplicates())\n# All good: 5 unique values in, 5 unique values out.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"inp = [-1, 0, 1, 2, 3]\nout = utils.to_categorical(inp)\nlen(pd.DataFrame(out).drop_duplicates())\n# 5 unique values in but just 4 out. What's happening here?","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"inp = [-2, -1, 0, 1, 2]\nout = utils.to_categorical(inp)\nlen(pd.DataFrame(out).drop_duplicates())\n# Now it's even worse: 5 unique values in, just 3 unique values out.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for before, after in zip (inp, out):\n    print(before, after)\n# This explains why. Negative values weren't mapped the way we thought. \n# -2 was mapped to the same outcome as 1. \n# -1 got mapped to the same outcome as 2.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. Binary & Base-N Encoders\nBase-N encoding is the superset of \n* binary encoding (N=2);\n* one-hot encoding (N=1).\nBy default ```category_encoders.BaseNEncoder``` takes N=2; the output is there for identical to ```category_encoders.BinaryEncoder```:"},{"metadata":{"trusted":true},"cell_type":"code","source":"summary.loc[ ['category_encoders.binary.BinaryEncoder', 'category_encoders.basen.BaseNEncoder'] ][ columns_show ]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tis = summary.loc['category_encoders.binary.BinaryEncoder', 'inp2out_map']\ntat = summary.loc['category_encoders.basen.BaseNEncoder', 'inp2out_map']\n(tis==tat).all().all()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### But do we really need 10 columns?\n2^8, 2^9 = 256, 512 so we should only need 9 columns to binary-encode 299 categories."},{"metadata":{"trusted":true},"cell_type":"code","source":"tis.apply(lambda x: np.unique(x))\n# Column cat10_0 is all zeros and is therefore redundant.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We can pass the option drop_invariant=True to avoid that redundancy.\nBaseNEncoder(drop_invariant=True).fit_transform(train['cat10'])\n# Now the redundant column disappears; we get 9 columns instead of 10.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4. Count Encoder"},{"metadata":{"trusted":true},"cell_type":"code","source":"summary.loc['category_encoders.count.CountEncoder', 'inp2out_map']\n# The count encoder seems to output all sorts of integers.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's take a look where those values come from. For sampling sake we take the last 3 values and try to derive them.\nout = CountEncoder().fit_transform(train['cat10'], train['target'])\ninp, out.tail(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Where did 3011 come from?\n(train['cat10']=='HC').sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Where did 565 come from?\n(train['cat10']=='BF').sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Where did 5917 come from?\n(train['cat10']=='LM').sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5. Contrast Encoders\nThese are [contrast encoders](https://stats.idre.ucla.edu/r/library/r-library-contrast-coding-systems-for-categorical-variables) characterised by the presence of an ```intercept``` in the output."},{"metadata":{},"cell_type":"markdown","source":"## 5.1 Helmert Encoder"},{"metadata":{"trusted":true},"cell_type":"code","source":"summary.loc[ 'category_encoders.helmert.HelmertEncoder', 'inp2out_map' ]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5.2 Sum Encoder"},{"metadata":{"trusted":true},"cell_type":"code","source":"inp2out_map = summary.loc[ 'category_encoders.sum_coding.SumEncoder', 'inp2out_map' ]\ninp2out_map","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"column_sum = inp2out_map.sum()\ncolumn_sum\n# This is the signature of sum encoding: except the ```intercept``` column all columns sum to zero.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"column_sum[ column_sum!= 0 ]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5.3 Backward-Difference Encoder"},{"metadata":{"trusted":true},"cell_type":"code","source":"summary.loc[ 'category_encoders.backward_difference.BackwardDifferenceEncoder', 'inp2out_map' ]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Grand summary"},{"metadata":{"trusted":true},"cell_type":"code","source":"summary[columns_show]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This notebook is getting a little long. We've covered simple, one-to-one mapping encoders. Let's do target encoders in another notebook!"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}