{"cells":[{"metadata":{},"cell_type":"markdown","source":"![](https://marychin.org/download/kaggle/tabmar.png)"},{"metadata":{},"cell_type":"markdown","source":"Update 10th March:\n* Plot ROC curves: \n* - manual back-of-envelop calculation (excellent refresher);\n* - sklearn.metrics.plot_roc_curve.\n* ```roc_auc_score``` with and without ```average='micro'``` option: no difference found.\n\nThis is the first walkthrough of the March Playground:\n* identifying troublesome features such as ```cat10```, ```cat5```, ```cat8```, ```cat7``` and others, which require handling;\n* running quick-and-dirty baselines (without parameters tweaking) using LightGBM, XGBoost, CatBoost and Random Forests;\n* looking at gains and feature rankings from LightGBM, XGBoost, CatBoost and Random Forests;\n* running BorutaShap, which reports each feature as either confirmed important, unimportant or tentative."},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_theme(style=\"whitegrid\")\nsns.set_palette('hot')\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score, confusion_matrix, plot_roc_curve\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.ensemble import RandomForestClassifier\nfrom lightgbm import LGBMClassifier\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier\nimport sys, glob, copy, warnings, time\nwarnings.simplefilter('ignore') # once | error | always | default | module\n\ninp = '/kaggle/input/tabular-playground-series-mar-2021/'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df, features = {}, {}\nprint('{:18s}{:>10s}{:>5s}{:>5s}'.format('FILE', 'ROWS', 'COLS', 'NULL'))\nfor file in glob.glob(f'{inp}/*.csv'):\n    label = file.split('/')[-1].split('.')[0]\n    df[label] = pd.read_csv(file, index_col='id')\n    features[label] = set(df[label].columns.to_list())\n    print('{:18s}{:10,d}{:5d}{:5d}'.format(label, *df[label].shape, df[label].isna().any().sum()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(df['sample_submission'].index == df['test'].index).all()\n# Straightforward if True.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features['train'] == features['test'].union(features['sample_submission'])\n# Straightforward if True.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['train'].sample(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Datatypes"},{"metadata":{"trusted":true},"cell_type":"code","source":"sr = pd.DataFrame(df['train'].dtypes, columns=['dtype'])\nfor dtype, dtype_data in sr.groupby('dtype'):\n    print('{:2d} columns of dtype {}\\n{}'.format(len(dtype_data), dtype, '='*10))\n    print(dtype_data.index.to_list(), '\\n')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['train']['target'].unique()\n# target is in fact categorical, not continuous.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['train'].describe(include='float')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['train'].describe(include='object')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features = {'cat': df['train'].columns[ df['train'].columns.str.startswith('cat') ].to_list(),\n            'con': df['train'].columns[ df['train'].columns.str.startswith('con') ].to_list(),\n            'num': df['train'].select_dtypes(include=[float, int]).columns.to_list()}\n# We are going to use these over and over; save us from having to do dot-columns again and again.\nfeatures","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Categories & encoding"},{"metadata":{"trusted":true},"cell_type":"code","source":"unik = {'train'    : {}, # to hold unique categorical values from train\n        'test'     : {}} # to hold unique categorical values from test\nprint('{:<8s}{} {}'.format('FEATURE', 'NUNIQUE', 'UNIQUE VALUES IN TRAIN'))\n# Print list of unique values starting from the lowest nunique, in that order. Features near the bottom are the troublesome ones.\nfor feature in df['train'][features['cat']].nunique().sort_values().index:\n    unik['train'].update({feature: set(sorted(df['train'][feature].unique()))})\n    unik['test'].update({feature: set(sorted(df['test'][feature].unique()))})\n    print('{:<8s}{:7d} {}'.format(feature, len(unik['train'][feature]), str(unik['train'][feature])))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('{:<8s}{:76s}'.format('FEATURE', 'UNIQUE VALUES IN TRAIN'))\nfor feature in features['cat']:\n    if unik['train'][feature]!=unik['test'][feature]:\n        print('in train but not in test:', feature, unik['train'][feature].difference(unik['test'][feature]))\n        print('in test but not in train:', feature, unik['test'][feature].difference(unik['train'][feature]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ncoda = OrdinalEncoder().fit(pd.concat([ df['train'][features['cat']], \n                             df['test'][features['cat']] ]))\n# For sanity check only; will be deleted real soon:\norig = copy.deepcopy(df)\nfor dataset in ['train', 'test']:\n    df[dataset][features['cat']] = ncoda.transform(df[dataset][features['cat']])\n    df[dataset][features['cat']] = df[dataset][features['cat']].astype(int)# .astype('category')\nncoda.categories_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Just a pedantic sanity check.\nassert (ncoda.inverse_transform(df['train'][features['cat']]) == orig['train'][features['cat']]).all().all()\nassert (ncoda.inverse_transform(df['test'][features['cat']]) == orig['test'][features['cat']]).all().all()\ndel orig   # Deleted as promised.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['train'].info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Distribution: categorical features by target"},{"metadata":{"trusted":true},"cell_type":"code","source":"valuecount2D = pd.DataFrame()\nfor nfeature, feature in enumerate(features['cat']):\n    tis = {'feature': feature}\n    for group, group_data in df['train'].groupby(feature):\n        tis['feature_category'] = group\n        if group_data['target'].value_counts().nunique()==1:\n            print(feature, group)\n        for tis['target'], tis['count'] in group_data['target'].value_counts().iteritems():\n            valuecount2D = pd.concat([valuecount2D, pd.DataFrame(tis, index=[f'{feature}_{group}_{tis[\"target\"]}'])])\n# valuecount2D.reset_index(drop=True, inplace=True)\nvaluecount2D.rename(columns={0: 'target=0', 1: 'target=1'}, inplace=True)\nvaluecount2D","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# sanity\nauto = valuecount2D.loc['cat18_3_1', 'count']\nmanual = len(df['train'].query('target==1 and cat18==3'))\nif auto==manual:\n    print('sane')\nelse:\n    print('insane')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols = 3\nrows = int(np.ceil(len(features['cat'])/cols))\nfig, ax = plt.subplots(rows, cols, figsize=(15, 7*rows), sharex=True)\n# As before, start with well-behaved features, with the problematic ones at the end, in that order.\nfor nfeature, feature in enumerate(df['train'][features['cat']].nunique().sort_values().index):\n    tis_ax = ax[nfeature//cols][nfeature%cols]\n    sns.barplot(data=valuecount2D.loc[valuecount2D['feature']==feature], \n                x='count', y='feature_category', hue='target', orient='h', ax=tis_ax, palette='hot')\n    tis_ax.set_title(feature)\n# As warned by earlier text output we find cat5 and cat10 screaming for attention.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Distribution: continuous features"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nplt.figure(figsize=(15, 5))\nsns.violinplot(data=df['train'][ features['con'] ])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tmp = df['train'][features['con']]\nxx = tmp.mean()\nyy = tmp.median()\nplt.figure(figsize=(10, 10))\nplt.plot([xx.min(), xx.max()], [yy.min(), yy.max()], 'y-.')\nplt.plot(xx, yy, '.r')\nfor x, y, z in zip(xx, yy, tmp):\n    plt.text(x+.005, y, z)\n_ = plt.axis('equal'); plt.xlabel('feature mean'); plt.ylabel('feature median')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Distribution: continuous features by target"},{"metadata":{"trusted":true},"cell_type":"code","source":"cols = 2\nrows = int(np.ceil(len(features['con'])/cols))\nfig, ax = plt.subplots(rows, cols, figsize= (15, 5*rows))\nfor nfeature, feature in enumerate(features['con']):\n    sns.histplot(data=df['train'], y=feature, hue='target', stat='density', ax=ax[nfeature//cols, nfeature%cols], palette='hot')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2D flood maps: how features pair cross-talk\nSeaborn has one-liners for this; but runs till eternity without returning. Here is therefore a dirty hack."},{"metadata":{"trusted":true},"cell_type":"code","source":"traintest = pd.concat([df['train'], df['test']])\nbinned = traintest[features['con']].apply(lambda x: pd.cut(x, bins=32, labels=False))\nplt.figure(figsize=(15, 15))\nnfeatures = len(features['con'])\nfor aa in range(1, nfeatures):\n    for bb in range(aa):\n        plt.subplot(nfeatures, nfeatures, aa*nfeatures + bb + 1)\n        sns.heatmap(binned.groupby(features['con'][aa]).apply(lambda x: x[features['con'][bb]].value_counts()).unstack(), \n                    square=True, cmap='hot', cbar=False, xticklabels=False, yticklabels=False)\n        plt.axis('off')\nfor tmp in range(1, nfeatures):\n    plt.subplot(nfeatures, nfeatures, nfeatures*tmp+1)\n    plt.axis('on'); plt.ylabel(features['con'][tmp])\nfor tmp in range(nfeatures-1):\n    plt.subplot(nfeatures, nfeatures, nfeatures*(nfeatures-1)+tmp+1)\n    plt.axis('on'); plt.xlabel(features['con'][tmp])\nfor tmp in range(1, nfeatures-1):\n    plt.subplot(nfeatures, nfeatures, nfeatures*(nfeatures-1)+tmp+1)\n    plt.ylabel('')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ncorr = traintest.corr()\ncorr.to_csv('corr.csv')\nplt.figure(figsize=(15, 15))\nsns.heatmap(corr, mask=np.triu(np.ones_like(corr, dtype=bool)), annot=True, fmt='.1f', linewidths=.5, square=True, cmap='hot', annot_kws={'size': 10}, cbar_kws={\"shrink\": .5})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"slimcorr = pd.Series(dtype=float)\nfor feature in corr.columns:\n    slimcorr.loc[feature] = corr[feature].sort_values()[-2]\nslimcorr.sort_values(ascending=False)\n# output reports no correlation too high; therefore too premature to drop any feature","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4 baselines before tuning"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataX = df['train'].copy()\ndatay = dataX.pop('target')\ntrainX, validX, trainy, validy = train_test_split(dataX, datay)\n\ndef trainNpredict(model):\n    tic = time.time()\n    pred = model.fit(trainX, trainy).predict_proba(validX)[:, 1]\n    roc_auc = roc_auc_score(validy, pred, average='micro')\n    print(\"roc_auc_score(validy, pred, average='micro') =\", roc_auc)\n    print(\"roc_auc_score(validy, pred) =\", roc_auc_score(validy, pred))\n#   plot ROC curve\n    return model, time.time()-tic, roc_auc\n\nmodel, tictoc, roc_auc = {}, pd.Series(dtype=float), pd.Series(dtype=float)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"label = 'rf'\nmodel[label], tictoc[label], roc_auc.loc[label] = trainNpredict(RandomForestClassifier(n_estimators=200, max_depth=7))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"label = 'lgb'\nmodel[label], tictoc[label], roc_auc.loc[label] = trainNpredict(LGBMClassifier(**{'is_unbalance': True}))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"label = 'xgb'\nscale_pos_weight = (df['train']['target']==0).sum() / (df['train']['target']==1).sum()\nmodel[label], tictoc[label], roc_auc.loc[label] = trainNpredict(XGBClassifier(**{'scale_pos_weight': scale_pos_weight}))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"label = 'cat'\nmodel[label], tictoc[label], roc_auc.loc[label] = trainNpredict(CatBoostClassifier(**{'scale_pos_weight': scale_pos_weight}))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"roc_auc.sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tictoc.sort_values()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Plot ROC curves"},{"metadata":{"trusted":true},"cell_type":"code","source":"# manual back-of-envelop calculation\nplt.figure(figsize=(7, 5))\nkolor = {'rf': 'r',\n         'lgb': 'g',\n         'xgb': 'b',\n         'cat': 'k'}\nfor k, v in model.items():\n    df['train'][k] = v.predict_proba(dataX)[:, 1]\n    for threshold in np.linspace(df['train'][k].min(), df['train'][k].max(), 100):\n        positive = df['train'][k]>threshold\n        true_positive = positive & (datay==1)\n        false_positive = positive & (datay==0)\n        plt.plot(false_positive.sum()/len(df['train']), true_positive.sum()/len(df['train']), '.', color=kolor[k])\n        plt.xlabel('false positives'); plt.ylabel('true ppsitives')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# auto: sklearn.metrics.plot_roc_curve\nfor k in model.keys():\n    plot_roc_curve(model[k], dataX, datay, color=kolor[k])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Pick the best baseline, submit and see"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['sample_submission']['target'] = model[roc_auc.idxmax()].predict_proba(df['test'])[:, 1]\ndf['sample_submission'].to_csv('submission.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Gain"},{"metadata":{"trusted":true},"cell_type":"code","source":"gain = pd.DataFrame(index=trainX.columns)\nfor treetype in model.keys():\n    gain[treetype] = model[treetype].feature_importances_\ngain.rank().astype(int).sort_values(by='lgb')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## BorutaShap"},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"if 'BorutaShap' not in sys.modules:\n    !pip install BorutaShap\nfrom BorutaShap import BorutaShap","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"Feature_Selector = BorutaShap(model=XGBClassifier(**{'tree_method':'gpu_hist'}), importance_measure='shap')   # importance_measure='gini'\n# Feature_Selector = BorutaShap(model=LGBMClassifier(), importance_measure='shap')\nFeature_Selector.fit(X=dataX, y=datay, n_trials=1000, verbose=False) # sample=False, train_or_test = 'test', normalize=True, verbose=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Feature_Selector.results_to_csv(filename='borutashap.csv')\nFeature_Selector.plot(which_features='all')  # X_size=15, figsize=(12,8), y_scale='log'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Feature_Selector.accepted","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Feature_Selector.features_to_remove","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}