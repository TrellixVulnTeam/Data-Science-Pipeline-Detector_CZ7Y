{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# This notebook got 0.8862 in one line of code (see below)\n##  Turn on the GPU Accelerator in this Notebook to get the fastest Results below using XGBoost","metadata":{}},{"cell_type":"markdown","source":"## Goal: Use Featurwiz to build a better ranking model in TPS\n1.  Big_Mart Sales Prediction Score: 1147  -- Rank 250 out of 41,361 = That's a Top <1% Rank!!\n1.  Loan Status Predictions Score 0.791  -- Rank 850 out of 67,424 - Top 1.25% Rank\n1.  Machine Hack Flight Ticket Score 0.9389 -- Rank 165 out of 2723 - Top 6% Rank!\n1.  Machine Hack Data Scientist Salary class Score 0.417 -- Rank 58 out of 1547 - Top 3.7% Rank! (Autoviml Score was 0.329 -- less than 0.417 of Featurewiz+Simple even though an NLP problem!)\n1.  MCHACK Book Price NLP Score 0.7336 -- Rank 104 Autoviml NLP problem and should have done better","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.dates as md\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.axes_grid1 import host_subplot\nimport mpl_toolkits.axisartist as AA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans\nfrom sklearn.covariance import EllipticEnvelope\nfrom mpl_toolkits.mplot3d import Axes3D\n%matplotlib inline","metadata":{"execution":{"iopub.status.busy":"2021-06-19T13:28:54.568144Z","iopub.execute_input":"2021-06-19T13:28:54.568613Z","iopub.status.idle":"2021-06-19T13:28:55.974418Z","shell.execute_reply.started":"2021-06-19T13:28:54.568508Z","shell.execute_reply":"2021-06-19T13:28:55.973413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Install Featurewiz Library to Get the Max Benefits","metadata":{}},{"cell_type":"code","source":"!pip install featurewiz","metadata":{"execution":{"iopub.status.busy":"2021-06-19T13:29:00.078375Z","iopub.execute_input":"2021-06-19T13:29:00.07878Z","iopub.status.idle":"2021-06-19T13:29:11.25965Z","shell.execute_reply.started":"2021-06-19T13:29:00.078717Z","shell.execute_reply":"2021-06-19T13:29:11.258109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import featurewiz as FW","metadata":{"execution":{"iopub.status.busy":"2021-06-19T13:29:45.267913Z","iopub.execute_input":"2021-06-19T13:29:45.268645Z","iopub.status.idle":"2021-06-19T13:29:55.76339Z","shell.execute_reply.started":"2021-06-19T13:29:45.268596Z","shell.execute_reply":"2021-06-19T13:29:55.762313Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from featurewiz import FE_kmeans_resampler, FE_find_and_cap_outliers, EDA_find_outliers\nfrom featurewiz import FE_convert_all_object_columns_to_numeric, split_data_n_ways, FE_create_categorical_feature_crosses\nfrom featurewiz import FE_create_time_series_features, FE_concatenate_multiple_columns\nfrom featurewiz import simple_XGBoost_model\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.set_option('display.float_format', lambda x: '%.3f' % x)\npd.set_option('display.max_columns', 500)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from load_kaggle import load_kaggle","metadata":{"execution":{"iopub.status.busy":"2021-06-19T13:31:14.630959Z","iopub.execute_input":"2021-06-19T13:31:14.631475Z","iopub.status.idle":"2021-06-19T13:31:14.651119Z","shell.execute_reply.started":"2021-06-19T13:31:14.63144Z","shell.execute_reply":"2021-06-19T13:31:14.649975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"subm, train, test = load_kaggle()\nprint(train.shape, test.shape)\ntrain.head(3)","metadata":{"execution":{"iopub.status.busy":"2021-06-19T13:31:19.628884Z","iopub.execute_input":"2021-06-19T13:31:19.629264Z","iopub.status.idle":"2021-06-19T13:31:23.462235Z","shell.execute_reply.started":"2021-06-19T13:31:19.629231Z","shell.execute_reply":"2021-06-19T13:31:23.461147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cats, ids, cont, _ = FW.EDA_classify_and_return_cols_by_type(train)\nids","metadata":{"execution":{"iopub.status.busy":"2021-06-19T13:32:51.36667Z","iopub.execute_input":"2021-06-19T13:32:51.367034Z","iopub.status.idle":"2021-06-19T13:32:54.190293Z","shell.execute_reply.started":"2021-06-19T13:32:51.367002Z","shell.execute_reply":"2021-06-19T13:32:54.189165Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"FW.FE_","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"### Drop the following categorical vars according to the Blog Post here ###\n###     https://www.kaggle.com/c/tabular-playground-series-mar-2021/discussion/224530  #####  \ndrop_cols = ['cat5', 'cat7', 'cat8', 'cat10']\nprint(len(drop_cols))\ndrop_cols","metadata":{"execution":{"iopub.status.busy":"2021-06-19T13:34:53.955295Z","iopub.execute_input":"2021-06-19T13:34:53.955647Z","iopub.status.idle":"2021-06-19T13:34:53.96616Z","shell.execute_reply.started":"2021-06-19T13:34:53.955616Z","shell.execute_reply":"2021-06-19T13:34:53.964984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target = 'target'\n#df[target] = (df[target] - np.mean(df[target]))/np.std(df[target])\n#train[target] = np.log(train[target].values)\nidcols = ['id']\nfeatures = [x for x in list(test) if x not in idcols+drop_cols]","metadata":{"execution":{"iopub.status.busy":"2021-06-19T13:34:58.704816Z","iopub.execute_input":"2021-06-19T13:34:58.705208Z","iopub.status.idle":"2021-06-19T13:34:58.711576Z","shell.execute_reply.started":"2021-06-19T13:34:58.70516Z","shell.execute_reply":"2021-06-19T13:34:58.710292Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = train[features+[target]]\ndf = train.copy(deep=True)\nprint(train.shape)\ntrain.head(1)","metadata":{"execution":{"iopub.status.busy":"2021-06-19T13:35:02.832079Z","iopub.execute_input":"2021-06-19T13:35:02.832524Z","iopub.status.idle":"2021-06-19T13:35:03.004479Z","shell.execute_reply.started":"2021-06-19T13:35:02.832476Z","shell.execute_reply":"2021-06-19T13:35:03.002658Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[target].hist()","metadata":{"execution":{"iopub.status.busy":"2021-06-19T13:35:27.163942Z","iopub.execute_input":"2021-06-19T13:35:27.164316Z","iopub.status.idle":"2021-06-19T13:35:27.405363Z","shell.execute_reply.started":"2021-06-19T13:35:27.164282Z","shell.execute_reply":"2021-06-19T13:35:27.403623Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = test[features]\nprint(test.shape)\ntest.head(1)","metadata":{"execution":{"iopub.status.busy":"2021-06-19T13:35:33.788434Z","iopub.execute_input":"2021-06-19T13:35:33.788821Z","iopub.status.idle":"2021-06-19T13:35:33.865205Z","shell.execute_reply.started":"2021-06-19T13:35:33.788788Z","shell.execute_reply":"2021-06-19T13:35:33.864083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### After dropping features, let's classify columns again\ncats, ids, cont, _ = FW.EDA_classify_and_return_cols_by_type(train)\ncats","metadata":{"execution":{"iopub.status.busy":"2021-06-19T14:26:00.533484Z","iopub.execute_input":"2021-06-19T14:26:00.533837Z","iopub.status.idle":"2021-06-19T14:26:02.69827Z","shell.execute_reply.started":"2021-06-19T14:26:00.533801Z","shell.execute_reply":"2021-06-19T14:26:02.697148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ls = ['interactions','groupby','target']","metadata":{"execution":{"iopub.status.busy":"2021-06-19T13:36:25.772647Z","iopub.execute_input":"2021-06-19T13:36:25.773067Z","iopub.status.idle":"2021-06-19T13:36:25.778495Z","shell.execute_reply.started":"2021-06-19T13:36:25.773022Z","shell.execute_reply":"2021-06-19T13:36:25.776677Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output = FW.featurewiz(train, target, corr_limit=0.70,\n                    verbose=2, sep=',', header=0, test_data=test,\n                    feature_engg='interactions', category_encoders='')","metadata":{"execution":{"iopub.status.busy":"2021-06-19T14:00:42.378187Z","iopub.execute_input":"2021-06-19T14:00:42.378629Z","iopub.status.idle":"2021-06-19T14:19:39.406462Z","shell.execute_reply.started":"2021-06-19T14:00:42.378594Z","shell.execute_reply":"2021-06-19T14:19:39.405374Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Just use this one line of code to get ~0.88 score in less than 2 mins!","metadata":{}},{"cell_type":"code","source":"trainm = output[0]\ntestm = output[1]\ntrainm.shape, testm.shape","metadata":{"execution":{"iopub.status.busy":"2021-06-19T14:19:58.480913Z","iopub.execute_input":"2021-06-19T14:19:58.481297Z","iopub.status.idle":"2021-06-19T14:19:58.488856Z","shell.execute_reply.started":"2021-06-19T14:19:58.481265Z","shell.execute_reply":"2021-06-19T14:19:58.487319Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainm.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-19T14:20:02.127296Z","iopub.execute_input":"2021-06-19T14:20:02.127696Z","iopub.status.idle":"2021-06-19T14:20:02.162421Z","shell.execute_reply.started":"2021-06-19T14:20:02.127651Z","shell.execute_reply":"2021-06-19T14:20:02.160806Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"traincat = FW.FE_create_categorical_feature_crosses(train, cats)\ntestcat = FW.FE_create_categorical_feature_crosses(test, cats)","metadata":{"execution":{"iopub.status.busy":"2021-06-19T14:29:47.033375Z","iopub.execute_input":"2021-06-19T14:29:47.033803Z","iopub.status.idle":"2021-06-19T14:33:13.03294Z","shell.execute_reply.started":"2021-06-19T14:29:47.033756Z","shell.execute_reply":"2021-06-19T14:33:13.03176Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"traincat.drop(cats, axis=1, inplace=True)\ntestcat.drop(cats, axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-19T14:34:01.882552Z","iopub.execute_input":"2021-06-19T14:34:01.882966Z","iopub.status.idle":"2021-06-19T14:34:06.377992Z","shell.execute_reply.started":"2021-06-19T14:34:01.882935Z","shell.execute_reply":"2021-06-19T14:34:06.376579Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"traincat.head(1)","metadata":{"execution":{"iopub.status.busy":"2021-06-19T14:35:32.300558Z","iopub.execute_input":"2021-06-19T14:35:32.300947Z","iopub.status.idle":"2021-06-19T14:35:32.327866Z","shell.execute_reply.started":"2021-06-19T14:35:32.300917Z","shell.execute_reply":"2021-06-19T14:35:32.326142Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'traincat' in traincat.columns.tolist()","metadata":{"execution":{"iopub.status.busy":"2021-06-19T14:36:38.761261Z","iopub.execute_input":"2021-06-19T14:36:38.761636Z","iopub.status.idle":"2021-06-19T14:36:38.770188Z","shell.execute_reply.started":"2021-06-19T14:36:38.761591Z","shell.execute_reply":"2021-06-19T14:36:38.76861Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#traincat.drop(['cont3',], axis=1, inplace=True)\ntestm.drop(['cont3',], axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-19T14:37:29.736521Z","iopub.execute_input":"2021-06-19T14:37:29.736962Z","iopub.status.idle":"2021-06-19T14:37:29.775515Z","shell.execute_reply.started":"2021-06-19T14:37:29.73693Z","shell.execute_reply":"2021-06-19T14:37:29.773803Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainm1 = trainm.join(traincat)\ntestm1 = testm.join(testcat)\nprint(trainm1.shape, testm1.shape)","metadata":{"execution":{"iopub.status.busy":"2021-06-19T14:37:31.163812Z","iopub.execute_input":"2021-06-19T14:37:31.164231Z","iopub.status.idle":"2021-06-19T14:37:32.577566Z","shell.execute_reply.started":"2021-06-19T14:37:31.164185Z","shell.execute_reply":"2021-06-19T14:37:32.576335Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feats = testm1.columns.tolist()","metadata":{"execution":{"iopub.status.busy":"2021-06-19T14:37:52.447242Z","iopub.execute_input":"2021-06-19T14:37:52.447628Z","iopub.status.idle":"2021-06-19T14:37:52.455721Z","shell.execute_reply.started":"2021-06-19T14:37:52.44759Z","shell.execute_reply":"2021-06-19T14:37:52.45424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_XGB=trainm1[feats]\nY_XGB=trainm1[target]\nX_XGB_test=testm1[feats]","metadata":{"execution":{"iopub.status.busy":"2021-06-19T14:58:16.796747Z","iopub.execute_input":"2021-06-19T14:58:16.797209Z","iopub.status.idle":"2021-06-19T14:58:17.771148Z","shell.execute_reply.started":"2021-06-19T14:58:16.797176Z","shell.execute_reply":"2021-06-19T14:58:17.770043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model, y_preds,y_probas = FW.simple_XGBoost_model(X_XGB=X_XGB, Y_XGB=Y_XGB,\n                                                  X_XGB_test=X_XGB_test, \n                               modeltype='Classification', log_y=False,\n                               GPU_flag=True, scaler=StandardScaler(), enc_method='label', verbose=1)","metadata":{"execution":{"iopub.status.busy":"2021-06-19T14:38:04.558685Z","iopub.execute_input":"2021-06-19T14:38:04.559034Z","iopub.status.idle":"2021-06-19T14:55:59.597292Z","shell.execute_reply.started":"2021-06-19T14:38:04.559004Z","shell.execute_reply":"2021-06-19T14:55:59.594792Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_preds[:,1]","metadata":{"execution":{"iopub.status.busy":"2021-06-19T14:56:16.313129Z","iopub.execute_input":"2021-06-19T14:56:16.313508Z","iopub.status.idle":"2021-06-19T14:56:16.325134Z","shell.execute_reply.started":"2021-06-19T14:56:16.313477Z","shell.execute_reply":"2021-06-19T14:56:16.323888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Base model above with no feature engg gets you ~0.88 score which is a very nice score.\nsubm[target] = y_preds[:,1]\nsubm.to_csv('submission.csv',index=False)\nsubm.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-19T14:56:17.490114Z","iopub.execute_input":"2021-06-19T14:56:17.490511Z","iopub.status.idle":"2021-06-19T14:56:18.175339Z","shell.execute_reply.started":"2021-06-19T14:56:17.490478Z","shell.execute_reply":"2021-06-19T14:56:18.173978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Use AutoViz to gain some insights - here's what I understood\nimportant vars - leave them as is\ncont1\ncont3\ncont5\ncont6\ncont8\n\nbin the following:\ncont0 4\ncont1 5\ncont3 2\ncont4 2\ncont6 3\ncont8 3\ncont10 10\n\ngroupby vars\ncont3 by cat2\ncont1 by cat4\ncont3 by cat4\n\n\ninteraction cat vars - feature crosses\ncat4 x cat18\ncat13 x cat4\ncat13 x cat2\n\ninteraction vars and then bin them\ncont3 x cont7\ncont3 x cont8\ncont3 x cont9\ncont3 x cont10\ncont4 x cont5\ncont4 x cont6\ncont4 x cont9\ncont4 x cont10\n\nBoolean cats - leave them as is\ncat0\ncat1\ncat12\ncat13\ncat14\ncat15\ncat16\n\nlog transform these\ncont5\ncont8\ncont7\n","metadata":{}},{"cell_type":"code","source":"#!pip install autoviz","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#from autoviz.AutoViz_Class import AutoViz_Class\n#AV = AutoViz_Class()\n#filename = \"\"\n#sep = \",\"\n#dft = AV.AutoViz(\n#    filename,\n#    sep=\",\",\n#    depVar=target,\n#    dfte=train,\n#    header=0,\n#    verbose=0,\n#    lowess=False,\n#    chart_format=\"svg\",\n#    max_rows_analyzed=30000,\n#    max_cols_analyzed=30,\n#)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature Engineering using Featurewiz begins here using insights from AutoViz above","metadata":{"trusted":true}},{"cell_type":"code","source":"### Step 1: we create numeric interaction variables first ###\nintxn_vars = [('cont3', 'cont7'),('cont3', 'cont8'),('cont3', 'cont9'),('cont3', 'cont10'),('cont4', 'cont5'),\n             ('cont4', 'cont6'),('cont4', 'cont9'),('cont4', 'cont10')]\ndef FE_create_interaction_vars(df, intxn_vars):\n    \"\"\"\n    This handy function creates interaction variables among pairs of numeric vars you send in.\n    Your input must be a dataframe and a list of tuples. Each tuple must contain a pair of variables.\n    All variables must be numeric. Double check your input before sending them in.\n    \"\"\"\n    df = df.copy(deep=True)\n    for (each_intxn1,each_intxn2)  in intxn_vars:\n        new_col = each_intxn1 + '_x_' + each_intxn2\n        try:\n            df[new_col] = df[each_intxn1] * df[each_intxn2]\n        except:\n            continue\n    return df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = FE_create_interaction_vars(train, intxn_vars)\ntest = FE_create_interaction_vars(test, intxn_vars)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### we must bin the above newly created discrete variables into 4 or 6 buckets. We will choose 6 for now\nintx_cols = train.columns.tolist()[-8:]\nintx_dict = dict(zip(intx_cols, [6]*8))\ntrain, test = FW.FE_discretize_numeric_variables(train,intx_dict,test=test, strategy='gaussian')\nprint(train.shape, test.shape)\ntrain.head(1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = [x for x in list(test) if x not in idcols]\nlen(preds)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_preds,y_probas = simple_XGBoost_model(X_XGB=train[preds], Y_XGB=train[target], X_XGB_test=test[preds], \n                               modeltype='Classification', log_y=False,\n                               GPU_flag=True, scaler=StandardScaler(), enc_method='label', verbose=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### The CV scores are less with new features. ####### So this is not worth adding these features\n### <Let us discard the new interaction variables and go back to the old train, test data > ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"subm, train, test = load_kaggle()\nprint(train.shape, test.shape)\ntrain.head(3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### step 2: we bin the following numeric variables using gaussian mixture models\nbin_these = {'cont0': 4, 'cont1': 5, 'cont3': 2, 'cont4': 2, 'cont6': 3, 'cont8': 3, 'cont10': 10}\ntrain, test = FW.FE_discretize_numeric_variables(train,bin_these,test=test, strategy='gaussian')\nprint(train.shape, test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = [x for x in list(test) if x not in idcols]\nlen(preds)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_preds,y_probas = simple_XGBoost_model(X_XGB=train[preds], Y_XGB=train[target], X_XGB_test=test[preds], \n                               modeltype='Classification', log_y=False,\n                               GPU_flag=True, scaler=StandardScaler(), enc_method='label', verbose=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### The CV scores are not bad - let's keep these binned variables and add to them in next steps ##","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### step 3: next we create feature crosses of these categorical variables ###\ntrain = FW.FE_create_categorical_feature_crosses(train, ['cat4','cat18','cat13','cat2'])\ntest = FW.FE_create_categorical_feature_crosses(test, ['cat4','cat18','cat13','cat2'])\nprint(train.shape, test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = [x for x in list(test) if x not in idcols]\nlen(preds)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_preds,y_probas = simple_XGBoost_model(X_XGB=train[preds], Y_XGB=train[target], X_XGB_test=test[preds], \n                               modeltype='Classification', log_y=False,\n                               GPU_flag=True, scaler=StandardScaler(), enc_method='label', verbose=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Absolutely no improvement - but we will keep these vars as long as performance is same! ####","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### step 4: create groupby aggregates of the following numerics \nagg_nums = ['cont1','cont3']\ngroupby_vars = ['cat2','cat4']\ntrain_add, test_add = FW.FE_add_groupby_features_aggregated_to_dataframe(train[agg_nums+groupby_vars], agg_types=['mean','std'],\n                                groupby_columns=groupby_vars,\n                                ignore_variables=[] , test=test[agg_nums+groupby_vars])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_copy = train.join(train_add.drop(groupby_vars+agg_nums, axis=1))\ntest_copy = test.join(test_add.drop(groupby_vars+agg_nums, axis=1))\nprint(train_copy.shape, test_copy.shape)\ntrain_copy.head(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = [x for x in list(test_copy) if x not in idcols]\nlen(preds)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_preds,y_probas = simple_XGBoost_model(X_XGB=train_copy[preds], Y_XGB=train[target], X_XGB_test=test_copy[preds], \n                               modeltype='Classification', log_y=False,\n                               GPU_flag=True, scaler=StandardScaler(), enc_method='label', verbose=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"train,_ = FW.FE_split_one_field_into_many(train, field='Product', splitter='-', filler='missing')\ntest,_ = FW.FE_split_one_field_into_many(test, field='Product', splitter='-', filler='missing')\ntrain.head(1)","metadata":{}},{"cell_type":"raw","source":"combs = ['Item_Category','Subcategory_1','Subcategory_2']\ntrain = FE_concatenate_multiple_columns(train, combs)\ntest = FE_concatenate_multiple_columns(test, combs)","metadata":{}},{"cell_type":"markdown","source":"train = FE_find_and_cap_outliers(train,[target], verbose=1)\n#test = FE_find_and_cap_outliers(test,nums,verbose=0)","metadata":{}},{"cell_type":"raw","source":"#output = split_data_n_ways(df,target, n_splits=2)","metadata":{}},{"cell_type":"markdown","source":"train = FE_create_time_series_features(train, 'Date')\ntest = FE_create_time_series_features(test, 'Date')\ntrain.head(1)","metadata":{}},{"cell_type":"code","source":"###### step 5: log transform these columns ##########\nlog_cols = {'cont5':'log', 'cont8':'log', 'cont7':'log'}\ntrain_copy = FW.FE_transform_numeric_columns(train_copy, log_cols)\ntest_copy = FW.FE_transform_numeric_columns(test_copy, log_cols)\ntrain_copy.head(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Lastly convert all object columns to numeric ############\ntrain_copy, test_copy = FE_convert_all_object_columns_to_numeric(train_copy,test_copy)\nprint(train_copy.shape, test_copy.shape)\ntrain_copy.head()","metadata":{"trusted":true}},{"cell_type":"markdown","source":"# Select the best features created using Featurewiz","metadata":{}},{"cell_type":"code","source":"train_best, test_best = FW.featurewiz(train_copy, target, test_data=test_copy,verbose=2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def left_subtract(l1,l2):\n    lst = []\n    for i in l1:\n        if i not in l2:\n            lst.append(i)\n    return lst\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cats = train_copy.select_dtypes(include=\"object\").columns.tolist()\nlen(cats)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sel_nums = ['cont6', 'cont9', 'cont0_discrete', 'cont1_discrete', 'cont6_discrete', 'cont10_discrete', 'cont1_by_cat4_mean', 'cont1_by_cat4_std', 'cont3_by_cat4_mean', 'cont3_by_cat4_std', 'cont3_by_cat2_mean', 'cont1_by_cat2_std', 'cont5', 'cont8_discrete', 'cont1', 'cont4', 'cont3_discrete', 'cont10']\npreds = sel_nums+cats\nprint(len(preds))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### using reduced list of variables, the score actually drops 2% points! wow #######\ny_preds,y_probas = simple_XGBoost_model(X_XGB=train_copy[preds], Y_XGB=train[target], X_XGB_test=test_copy[preds], \n                               modeltype='Classification', log_y=False,\n                               GPU_flag=True, scaler=StandardScaler(), enc_method='label', verbose=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"####\nm, feats, trainm, testm = Auto_ViML(train_copy[preds+[target]], target, test_copy[preds],\n                            sample_submission='',\n                            scoring_parameter='', KMeans_Featurizer=False,\n                            hyper_param='RS',feature_reduction=True,\n                             Boosting_Flag=True, Binning_Flag=False,\n                            Add_Poly=0, Stacking_Flag=True,Imbalanced_Flag=False,\n                            verbose=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_preds1 = testm['target_proba_1'].values\ny_preds1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"subm = test[idcols]\n#subm = pd.DataFrame()\nsubm[target] = y_preds1\nsubm.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"disto","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"subm.to_csv(target+'_Binary_Classification_submission2.csv',index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Autoviml got about 0.8746 in the Kaggle rankings. #######\n###  This is slightly lower than 0.8845 that Autoviml got a month ago but it is about same as featurewiz\n### The good news is that AutoviML and Featurewiz now produce results on a 300K dataset lightning fast\n### It takes less than 2 mins for Autoviml and Featurewiz to crunch this dataset! That's a huge leap.","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}