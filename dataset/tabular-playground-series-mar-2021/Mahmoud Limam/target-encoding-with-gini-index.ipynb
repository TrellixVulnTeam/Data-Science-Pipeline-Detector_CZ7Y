{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Hey there.**\n### In this notebook I experimented with a couple of encoding techniques that one can use when OHE increases dimensionality too much.","metadata":{}},{"cell_type":"markdown","source":"## Data:\n### The usual tabular playground series, March 2021. These TPS datasets are really good for experimentation.\n### 300000 samples, 31 predictor features, and a binary target.","metadata":{}},{"cell_type":"markdown","source":"## What I did:\n### First, I tried OHE.\nI ended up with more than 600 features.\nThe accuracy wasn't bad at all with LGBM, and it didn't overfit.\nI tried feature selection, but that considerably increased bias.\nPCA didn't do any good either, it only increased variance.\n### Then, I implemented frequency encoding.\nIt's when you replace each category in each categorical feature with its frequency.\nCategory Encoders (link at the end) is a library that implements count encoding, which is almost the same.\nBut I just hard coded it.\n### Finally, Target Encoding with Gini Index.\nTarget encoding is basically the replacement of each category with some kind of information that it tells about the target variable.\nFor continuous targets, it's usually the target mean for that category.\nFor categorical targets, the Category Encoders library uses, if I'm not mistaken, the posterior probability: P(target/category).\nI've got a categorical (binary) target here, but instead of the posterior probability, I used Gini index.\nI just felt like trying it out, and it did well.","metadata":{}},{"cell_type":"markdown","source":"##                                                                        بسم الله","metadata":{}},{"cell_type":"markdown","source":"## Importing and Exploring the Data. ","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data=pd.read_csv(\"../input/tabular-playground-series-mar-2021/train.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for col in data.columns[1:20]:\n    print(\"unique values in {}:\\n\".format(col),data[col].unique())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"OHE will blow this up to 600+ dimensions.","metadata":{}},{"cell_type":"markdown","source":"## One-Hot Encoding","metadata":{}},{"cell_type":"code","source":"ohd=pd.get_dummies(data, drop_first=True)\nohd.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ohd.drop(\"id\",axis=1,inplace=True)\ny=ohd[\"target\"]\nx=ohd.drop(\"target\", axis=1, inplace=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.15)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nss=StandardScaler()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_tr = ss.fit_transform(x_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I'll be using LGBM for this one as it's fast.","metadata":{}},{"cell_type":"code","source":"from lightgbm import LGBMClassifier\nlgbm=LGBMClassifier()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lgbm.fit(x_tr,y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_ts = ss.transform(x_test)\ny_pred_tr=lgbm.predict(x_tr)\ny_pred_ts=lgbm.predict(x_ts)\ntrain_acc=accuracy_score(y_train,y_pred_tr)\ntest_acc=accuracy_score(y_test,y_pred_ts)\nprint(\"LGBM Results with OHE:\")\nprint(\"training accuracy = {}\".format(train_acc))\nprint(\"testing accuracy = {}\".format(test_acc))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Surprisingly good results.  \nIt also didn't take long to train, but LGBM is fast.  \nIt would probably take much longer with other models.  ","metadata":{}},{"cell_type":"markdown","source":"### Would feature importance improve anything?","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport matplotlib.style as style\nstyle.use('seaborn-darkgrid')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feat_imp = lgbm.feature_importances_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(25,7.5))\nsns.barplot(x.columns, feat_imp ,palette=\"cool_r\")\nplt.title(\"Feature Impotances\",fontsize=40)\nplt.xlabel(\"Features\",fontsize=30)\nplt.ylabel(\"Importance\",fontsize=30)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So I'm going to set a threshhold here, and everything below it will be thrown away.  \nI tried with ","metadata":{}},{"cell_type":"code","source":"for t in [100,40,20,10,5,1]:\n    droplist=[]\n    for j in range(x.shape[1]):\n        if feat_imp[j]<t:\n            droplist.append(x.columns[j])\n    len(droplist)\n    x_sel=x.drop(droplist, axis=1, inplace=False)\n    print(\"Results for threshhold = {}\".format(t))\n    print(\"Shape of Dataframe: {}\".format(x_sel.shape))\n    x_train_sel, x_test_sel, y_train_sel, y_test_sel = train_test_split(x_sel,y,test_size=0.15)\n    x_tr_sel = ss.fit_transform(x_train_sel)\n    lgbm.fit(x_tr_sel,y_train_sel)\n    x_ts_sel = ss.transform(x_test_sel)\n    y_pred_tr=lgbm.predict(x_tr_sel)\n    y_pred_ts=lgbm.predict(x_ts_sel)\n    train_acc=accuracy_score(y_train,y_pred_tr)\n    test_acc=accuracy_score(y_test,y_pred_ts)\n    print(\"LGBM training accuracy = {}\".format(train_acc))\n    print(\"LGBM testing accuracy = {}\\n\".format(test_acc))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"BIAS.","metadata":{}},{"cell_type":"markdown","source":"### That didn't work. How about PCA?","metadata":{}},{"cell_type":"code","source":"from sklearn.decomposition import PCA\npca=PCA()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train_pca=pca.fit_transform(x_train)\nax=plt.figure(figsize=(25,20))\npca_features= list(range(0,pca.n_components_))\nsns.barplot(pca_features, pca.explained_variance_,palette=\"winter\")\nplt.title(\"Variation along PCA Components\", fontsize=40)\nplt.xlabel(\"Components\", fontsize=30)\nplt.ylabel(\"Variation\", fontsize=30)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Beyond about 200 components, pretty much all is noise.","metadata":{}},{"cell_type":"code","source":"for i in [15,35,100,200]:\n    pca=PCA(n_components=i)\n    x_tr = ss.fit_transform(x_train)\n    x_tr_pca=pca.fit_transform(x_tr)\n    lgbm.fit(x_tr_pca,y_train)\n    x_ts = ss.transform(x_test)\n    x_ts_pca=pca.fit_transform(x_ts)\n    y_pred_tr=lgbm.predict(x_tr_pca)\n    y_pred_ts=lgbm.predict(x_ts_pca)\n    train_acc=accuracy_score(y_train,y_pred_tr)\n    test_acc=accuracy_score(y_test,y_pred_ts)\n    print(\"PCA Components: {}\".format(i))\n    print(\"LGBM training accuracy = {}\".format(train_acc))\n    print(\"LGBM testing accuracy = {}\".format(test_acc))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"High variance here.","metadata":{}},{"cell_type":"markdown","source":"### Bottom line:\nTo be honest, I expected PCA and feature importance to improve results.  \nOHE certainly did increase dimensionality, but LGBM still did achieve reatively low bias and almost 0 variance.  \nLGBM also didn't take too long to train.  \nBut still, people often look for alternatives to OHE, and a smaller dataset takes less training time.  \nSo brace yourself, dear viewer, for frequency encoding!","metadata":{}},{"cell_type":"markdown","source":"### Frequency Encoding!\nYou just replace categories with their frequencies. Simple as that.  \nIt's the same as count encoding except that you need to divide counts by the total number of samples to get frequencies.  \nIf you plan to standardize your data then counts and frequencies would give the same results.  \nI didn't use the categorical encoders library, I felt like doing it myself.","metadata":{}},{"cell_type":"code","source":"frqdata=pd.DataFrame()\nfor col in data.columns[20:]:\n    frqdata[col]=data[col]\nfor col in data.columns[1:20]:\n    d=data[col].value_counts().to_dict()\n    frqdata[col]=data[col].map(d)/300000\nfrqdata.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"That's all there is to it.  \nNow we train a model.  \nI'll be using the SGD Classifier from sklearn.  \nIt's a fast linear model that uses stochastic gradient descent.  \nIt seemed interesting when I read about it so I felt like trying it out.  \nI also expect standardization to make a difference, and LGBM isn't affected by it whereas SGDC is.  \nPS: As it wouldn't make sense to compare two encoding techniques with different models, I'll also try LGBM.","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import SGDClassifier\nsgdc=SGDClassifier(max_iter=1000, tol=1e-3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_frq = frqdata.drop(\"target\",axis=1,inplace=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train_frq, x_test_frq, y_train_frq, y_test_frq = train_test_split(x_frq,y,test_size=0.15)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sgdc.fit(x_train_frq,y_train_frq)\ny_pred_tr=sgdc.predict(x_train_frq)\ny_pred_ts=sgdc.predict(x_test_frq)\ntrain_acc=accuracy_score(y_train_frq,y_pred_tr)\ntest_acc=accuracy_score(y_test_frq,y_pred_ts)\nprint(\"SGDClassifier results with Frequency/Count Encoding:\")\nprint(\"Without Feature Scaling:\")\nprint(\"training accuracy = {}\".format(train_acc))\nprint(\"testing accuracy = {}\".format(test_acc))\nx_train_frqs = ss.fit_transform(x_train_frq)\nsgdc.fit(x_train_frqs,y_train_frq)\ny_pred_tr=sgdc.predict(x_train_frqs)\nx_test_frqs = ss.fit_transform(x_test_frq)\ny_pred_ts=sgdc.predict(x_test_frqs)\ntrain_acc=accuracy_score(y_train_frq,y_pred_tr)\ntest_acc=accuracy_score(y_test_frq,y_pred_ts)\nprint(\"With Feature Scaling:\")\nprint(\"training accuracy = {}\".format(train_acc))\nprint(\"testing accuracy = {}\".format(test_acc))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Results are good, but I certainly didn't expect scaling to increase bias.  \nAnyways, let's try LGBM.","metadata":{}},{"cell_type":"code","source":"lgbm.fit(x_train_frq,y_train_frq)\ny_pred_tr=lgbm.predict(x_train_frq)\ny_pred_ts=lgbm.predict(x_test_frq)\ntrain_acc=accuracy_score(y_train_frq,y_pred_tr)\ntest_acc=accuracy_score(y_test_frq,y_pred_ts)\nprint(\"LGBM results with Frequency/Count Encoding:\")\nprint(\"training accuracy = {}\".format(train_acc))\nprint(\"testing accuracy = {}\".format(test_acc))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Pretty much the same results as with OHE, but less training time.","metadata":{}},{"cell_type":"markdown","source":"## Target Encoding with Gini Index!\nYou just replace each category with its Gini index. The gini index basically tells you how much information that category tells you about the target variable.  \nOne could use entropy instead of gini, but gini -from what I've read- is faster to compute.","metadata":{}},{"cell_type":"markdown","source":"So I just wrote a function (next cell) that implements this, with two options:  \n\n**Weighted**: If true, then each category will further be multiplied by its frequency.  \nThis could perhaps give better results as rare categories don't hold much information anyways.  \n\n**Standardize**: Name says it all.  \n\n### Feel free to copy and paste the following encoder in your work if you wish to use it, but please cite this notebook if you do.","metadata":{}},{"cell_type":"markdown","source":"##### PS: It only works for a binary target.  \n##### You would need to make some adjustments for multi-class targets.","metadata":{}},{"cell_type":"code","source":"def gini_encoder(column,target,weighted=False,standardize='None'):\n    unique=column.unique()\n    total=column.shape[0]\n    gini_ind=dict()\n    \n    for i in unique:\n        x_i = column[column==i]\n        i_total = x_i.shape[0]\n        x_i_yes = x_i[target==1]\n        yes_count = x_i_yes.shape[0]\n        x_i_no = x_i[target==0]\n        no_count = x_i_no.shape[0]\n        gini = 1 - (yes_count/i_total)**2 - (no_count/i_total)**2\n        if weighted==True:\n            gini *= i_total/total\n        gini_ind[i] = gini\n    \n    encoded = column.copy()\n    encoded = encoded.map(gini_ind)\n    \n    if standardize==\"mean\":\n        encoded = (encoded - encoded.mean())/encoded.std()\n    elif standardize==\"median\":\n        encoded = (encoded - encoded.median())/encoded.std()\n        \n    return encoded","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We'll start with LGBM:","metadata":{}},{"cell_type":"code","source":"gini_results_lgbm=pd.DataFrame(columns=[\"No Standardization\",\"Median Standardization\",\"Mean Standardization\"],\n                          index=[\"Training (Weighted)\",\"Testing (Weighted)\",\"Training (Unweighted)\",\"Testing (Unweighted)\"])\n\nweighted=[True,False]\nstandardize=[\"None\",\"median\",\"mean\"]\n\nfor w in weighted:\n    for s in standardize:\n        gini_data=pd.DataFrame()\n        for col in data.columns[20:]:\n            gini_data[col]=data[col]\n        for col in data.columns[1:20]:\n            gini_data[col]=gini_encoder(column=data[col],target=data[\"target\"], weighted=w, standardize=s)\n        x_gini=gini_data.drop(\"target\",axis=1)\n        x_train_gini, x_test_gini, y_train_gini, y_test_gini = train_test_split(x_gini,y,test_size=0.15)\n        lgbm.fit(x_train_gini,y_train_gini)\n        y_pred_tr=lgbm.predict(x_train_gini)\n        y_pred_ts=lgbm.predict(x_test_gini)\n        train_acc=accuracy_score(y_train_gini,y_pred_tr)\n        test_acc=accuracy_score(y_test_gini,y_pred_ts)\n        wi=weighted.index(w)\n        si=standardize.index(s)\n        \n        gini_results_lgbm.iloc[wi*2,si] = train_acc\n        gini_results_lgbm.iloc[1+wi*2,si] = test_acc","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"LGBM Results with Gini Target Encoding:\")\ngini_results_lgbm","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Surprisingly, weighting made no considerable difference.  \nSame with Standardization, but LGBM is already insensitive to scale.","metadata":{}},{"cell_type":"code","source":"gini_results_sgdc=pd.DataFrame(columns=[\"No Standardization\",\"Median Standardization\",\"Mean Standardization\"],\n                          index=[\"Training (Weighted)\",\"Testing (Weighted)\",\"Training (Unweighted)\",\"Testing (Unweighted)\"])\n\nweighted=[True,False]\nstandardize=[\"None\",\"median\",\"mean\"]\n\nfor w in weighted:\n    for s in standardize:\n        gini_data=pd.DataFrame()\n        for col in data.columns[20:]:\n            gini_data[col]=data[col]\n        for col in data.columns[1:20]:\n            gini_data[col]=gini_encoder(column=data[col],target=data[\"target\"], weighted=w, standardize=s)\n        x_gini=gini_data.drop(\"target\",axis=1)\n        x_train_gini, x_test_gini, y_train_gini, y_test_gini = train_test_split(x_gini,data[\"target\"],test_size=0.15)\n        sgdc.fit(x_train_gini,y_train_gini)\n        y_pred_tr=sgdc.predict(x_train_gini)\n        y_pred_ts=sgdc.predict(x_test_gini)\n        train_acc=accuracy_score(y_train_gini,y_pred_tr)\n        test_acc=accuracy_score(y_test_gini,y_pred_ts)\n        wi=weighted.index(w)\n        si=standardize.index(s)\n        \n        gini_results_sgdc.iloc[wi*2,si] = train_acc\n        gini_results_sgdc.iloc[1+wi*2,si] = test_acc","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"SGDClassifier Results with Gini Target Encoding:\")\ngini_results_sgdc","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Weighting only increased bias.  \nMedian Standardization slightly decreased variance.  ","metadata":{}},{"cell_type":"markdown","source":"### Well, there you go, 2 alternatives to OHE.  \n### Use them wisely.\n### There are several other techniques that you can implement with the following library: https://contrib.scikit-learn.org/category_encoders/","metadata":{}},{"cell_type":"markdown","source":"### سبحانك اللهم وبحمدك، أشهد أن لا إله إلا أنت، أستغفرك وأنوب إليك","metadata":{}}]}