{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Introduction"},{"metadata":{},"cell_type":"markdown","source":"This is a very basic notebook to show how can you further increase your score combining the predictions of all the models you have used in this competiton. In this notebook I have used the ***best*** predictions of XGBoost, Lgbm, Catboost, logisticRegression, ridgeClassifier and some other models."},{"metadata":{"id":"EkQreg7qsgr9","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np","execution_count":null,"outputs":[]},{"metadata":{"id":"ZO_LF1Urs-lw","outputId":"b0c087eb-1301-4888-a11f-201374227327","trusted":true},"cell_type":"code","source":"from google.colab import drive\ndrive.mount('/content/drive')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here the model predictions on the training set will be used as the features of the dataset for this second level classification and the labels will be the same."},{"metadata":{"id":"JrCmuQj4s4sa","trusted":true},"cell_type":"code","source":"train=pd.DataFrame({\n    'xgb':pd.read_csv('/content/drive/MyDrive/ML/xgb_train.csv')['target'],\n    'lr':pd.read_csv('/content/drive/MyDrive/ML/lr_train.csv')['target'],\n    'lgbm':pd.read_csv('/content/drive/MyDrive/ML/lgbm_train.csv')['target'],\n    'cat':pd.read_csv('/content/drive/MyDrive/ML/cat_train.csv')['target'],\n    'sgd':pd.read_csv('/content/drive/MyDrive/ML/sgd_train.csv')['target'],\n    'ridge':pd.read_csv('/content/drive/MyDrive/ML/ridge_train.csv')['target'],\n    'lgbm_nn':pd.read_csv('/content/drive/MyDrive/ML/train_lgbm_nn.csv')['target']\n})\nY=pd.read_csv('/content/drive/MyDrive/ML/train_TPSMar.csv')['target']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"These are the predictions on the test set by each of the models"},{"metadata":{"id":"5cUbtm3Tv5Kf","trusted":true},"cell_type":"code","source":"test=pd.DataFrame({\n    'xgb':pd.read_csv('/content/drive/MyDrive/ML/xgb_test.csv')['target'],\n    'lr':pd.read_csv('/content/drive/MyDrive/ML/lr_test.csv')['target'],\n    'lgbm':pd.read_csv('/content/drive/MyDrive/ML/lgbm_test.csv')['target'],\n    'cat':pd.read_csv('/content/drive/MyDrive/ML/cat_test.csv')['target'],\n    'sgd':pd.read_csv('/content/drive/MyDrive/ML/sgd_test.csv')['target'],\n    'ridge':pd.read_csv('/content/drive/MyDrive/ML/ridge_test.csv')['target'],\n    'lgbm_nn':pd.read_csv('/content/drive/MyDrive/ML/test_lgbm_nn.csv')['target']\n})","execution_count":null,"outputs":[]},{"metadata":{"id":"wYJU7sqfw4R4","trusted":true},"cell_type":"code","source":"test_id=pd.read_csv('/content/drive/MyDrive/ML/test_TPSMar.csv')['id']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I have used a basic 10 fold RidgeClassifier to make predictions on the second level test set which worked out to be the best for me. I did try using basic XGB, Lgbm, RandomForest but RidgeClassifier worked out to be the best of all those."},{"metadata":{"id":"L5ejHeX0ug_e","outputId":"0d774a4f-f172-467c-b9c6-3e94e43ecbd5","trusted":true},"cell_type":"code","source":"from sklearn.linear_model import RidgeClassifier\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold\n\nkf=StratifiedKFold(n_splits=10,shuffle=True,random_state=100)\npred=np.zeros((test.shape[0],))\n\nfor fold,(train_idx,valid_idx) in enumerate(kf.split(train,Y)):\n    X1,X2,Y1,Y2=train.iloc[train_idx],train.iloc[valid_idx],Y.iloc[train_idx],Y.iloc[valid_idx]\n    model=CalibratedClassifierCV(RidgeClassifier())\n    model.fit(X1,Y1)\n    score=roc_auc_score(model.predict(X2),Y2)\n    print(score,' ',fold)\n    pred+=model.predict_proba(test)[:,1]/10\n  ","execution_count":null,"outputs":[]},{"metadata":{"id":"kYc3VlquwwZa","trusted":true},"cell_type":"code","source":"pd.DataFrame({\n    'id':test_id,\n    'target':pred\n}).to_csv('stack.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"With stacking I have seen lot of improvement in the final score. My 10 fold Xgb best score was 0.89227 and 10 fold lgbm best score was around 0.89220. Combining these and some other basic models with a second level classification using RidgeClassifier turned out my score to be 0.89288. Try Stacking models if you see some improvement in your basic model score you will definetely score even more."},{"metadata":{},"cell_type":"markdown","source":"If there are any doubts with the code or something else please do comment and upvote the notebook if you found it useful."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}