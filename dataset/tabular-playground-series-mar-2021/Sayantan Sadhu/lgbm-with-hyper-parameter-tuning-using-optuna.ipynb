{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Introduction\nIn this notebook I will be creating a using lightgbm and then I will do hyperparameter tuning on that model to improve it ðŸ˜„. Initially with out the hyper parameter tuning, the model gives a score of around .77(approx) on submitting it to the competition. While, after doing the hyper parameter tuning, it gets a score of .88(appprox)ðŸ¤ž. I have used optuna for hyper parameter tunining. I will keep adding all the necessary links as well from which you can learn. \n\nThank you !! \nEnjoy :)"},{"metadata":{},"cell_type":"markdown","source":"## Importing the libraries and loading the data "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom pandas_profiling import ProfileReport\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn import metrics \nimport lightgbm as lgb\nimport optuna\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/tabular-playground-series-mar-2021/train.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv('../input/tabular-playground-series-mar-2021/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv('../input/tabular-playground-series-mar-2021/sample_submission.csv',index_col='id')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature Engineering and Feature Selection"},{"metadata":{},"cell_type":"markdown","source":"This section includes - \n1. Understanding and visualizing the data.\n2. Applying label encoder to the categorical features.\n3. Checking for missing values.\n4. Dropping some of the unimportant features. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Pandas Profiling on the training set. \nprof = ProfileReport(train)\nprof.to_notebook_iframe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Pandas Profiling on the test set\nprof = ProfileReport(test)\nprof.to_notebook_iframe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Label encoder\nfor c in train.columns:\n    if train[c].dtype=='object': \n        lbl = LabelEncoder()\n        lbl.fit(list(train[c].values) + list(test[c].values))\n        train[c] = lbl.transform(train[c].values)\n        test[c] = lbl.transform(test[c].values)\n        \ndisplay(train.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"#checking for missing values in training set\ntrain.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"#Checking for missing values in test set\ntest.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"#Removing unimportant columns\ntarget = train.pop('target')\ntrain.pop('id')\ntrain.info()\ntest.pop('id')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model Creation and Hyper Parameter Optimisation"},{"metadata":{},"cell_type":"markdown","source":"First I have used lightgbm to train the model and then I have used optuna to get the best hyper parameters for the model. After that I have created a simple model using optuna with defaults hyper parameters. The reason for doing so is to compare the roc auc score for both the models and how much improvement is seen from the initial model. \n\nLink to learn about optuna for lgbm - https://github.com/optuna/optuna/blob/master/examples/lightgbm/lightgbm_simple.py"},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef objective(trial,data=train, target = target):\n    X_train,X_test,y_train,y_test = train_test_split(train,target,train_size=0.9)\n    dtrain = lgb.Dataset(X_train, label=y_train)\n    param = {\n        'objective': 'binary',\n        'metric': 'binary_logloss',\n        'verbosity': -1,\n        'boosting_type': 'gbdt',\n        'lambda_l1': trial.suggest_loguniform('lambda_l1', 1e-8, 10.0),\n        'lambda_l2': trial.suggest_loguniform('lambda_l2', 1e-8, 10.0),\n        'num_leaves': trial.suggest_int('num_leaves', 2, 256),\n        'feature_fraction': trial.suggest_uniform('feature_fraction', 0.4, 1.0),\n        'bagging_fraction': trial.suggest_uniform('bagging_fraction', 0.4, 1.0),\n        'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n    }\n    model = lgb.train(param,dtrain)\n    y_pred = model.predict(X_test)\n    pred_labels = np.rint(y_pred)\n    auc_roc_score = roc_auc_score(y_test,pred_labels)\n    return auc_roc_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"study = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=50)\nprint('Number of finished trials:', len(study.trials))\nprint('Best trial:', study.best_trial.params)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params=study.best_params \n#params['n_estimators'] = 2000 \nparams['metric'] = 'roc_auc_score'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The dictionary params contains the best values for our hyper parameters which we found out using optuna"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train,X_test,y_train,y_test = train_test_split(train,target,train_size=0.9)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"First I have created a lgbm model in which I have not included the hyper parameters and then I have created another model where I have included the hyper parameter. And the hyper parameters have made a very slight improvement in the model but the slight improvement shows a significantly big jump in the leaderboard. "},{"metadata":{"trusted":true},"cell_type":"code","source":"from lightgbm import LGBMClassifier\nlgb = LGBMClassifier()\nlgb.fit(X_train,y_train)\ny_preds = lgb.predict(X_test)\nprint(metrics.roc_auc_score(y_preds,y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\ncheck = LGBMClassifier(**params)\ncheck.fit(X_train,y_train)\ny_preds = check.predict(X_test)\ntarget_names = [\"class 0 \",\"class 1\"]\nprint(metrics.roc_auc_score(y_preds,y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The roc_auc_score without the hyper parameter tuning is around 0.81645 and the roc_auc_score with hyper parameters is around 0.81780. And the difference in the roc_auc_score is 0.00135"},{"metadata":{"trusted":true},"cell_type":"code","source":"output = check.predict_proba(test)\nsubmission['target'] = output","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv('lgbm.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Thank you so much for your time, if you find it useful kindly upvote** :)"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}