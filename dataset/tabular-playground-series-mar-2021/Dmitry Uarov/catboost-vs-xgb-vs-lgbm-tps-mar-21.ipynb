{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport seaborn as sns\nimport optuna\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport os\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\nfrom catboost import CatBoostClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/tabular-playground-series-mar-2021/train.csv')\ntest = pd.read_csv('../input/tabular-playground-series-mar-2021/test.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**In tis notebook I want to check CatBoost, XGB and LGBM which is one the best with Optuna optimization with 30 trials. As a novice, I've heard a lot about how CatBoost is fast, and that XGB is the weapon of champions. Well, it's time to check it out in person.**"},{"metadata":{},"cell_type":"markdown","source":"# EDA"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are no NA values. It's good."},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are no NA values too"},{"metadata":{},"cell_type":"markdown","source":"Let's look the correlation between features"},{"metadata":{"trusted":true},"cell_type":"code","source":"trheat = train.drop('id', axis = 1)\nmatrix = np.triu(trheat.corr())\nplt.figure(figsize=(15, 10))\nsns.heatmap(trheat.corr(), annot = True, cmap = 'YlGn', fmt=\".2f\", mask = matrix, vmin = -1, vmax = 1, linewidths = 0.1, linecolor = 'white')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's look on count distribution in target"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set_style('whitegrid')\nsns.set_palette('Greens_r', 2)\nplt.figure(figsize=(15, 10))\nsns.countplot(x = 'target', data = train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = train.drop(['target', 'id'], axis = 1)\ny = train['target']\n\nID = test['id'] # for submission\ntest = test.drop('id', axis = 1)\n\nnum_cols = X.select_dtypes(include = 'number').columns.to_list() # numerical features\ncat_cols = X.select_dtypes(exclude = 'number').columns.to_list() # categorical features","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Encoding categorical fetaures"},{"metadata":{"trusted":true},"cell_type":"code","source":"cols = cat_cols + num_cols\nX_objs = len(X)\ndf = pd.concat(objs = [X[cols], test[cols]], axis = 0)\ndf = pd.concat(objs = [X[cols], test[cols]], axis = 0)\ndf = pd.get_dummies(df, columns = cat_cols)\nX = df[:X_objs]\ntest = df[X_objs:]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's see what we got"},{"metadata":{"trusted":true},"cell_type":"code","source":"X.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# CatBoost and optuna's optimization"},{"metadata":{"trusted":true},"cell_type":"code","source":"def objective(trial, data = X, target = y):\n    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.2, random_state = 0)\n\n    params = {\n        'max_depth': trial.suggest_int('max_depth', 3, 10),\n        'learning_rate': trial.suggest_categorical('learning_rate', [0.005, 0.02, 0.05, 0.08, 0.1]),\n        'n_estimators': trial.suggest_int('n_estimators', 50, 3000),\n        'max_bin': trial.suggest_int('max_bin', 200, 400),\n        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 1, 300),\n        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 0.0001, 1.0, log = True),\n        'subsample': trial.suggest_float('subsample', 0.1, 0.8),\n        'random_seed': 42,\n        'task_type': 'GPU',\n        'loss_function': 'Logloss',\n        'eval_metric': 'AUC',\n        'bootstrap_type': 'Poisson'\n    }\n    \n    model = CatBoostClassifier(**params)  \n    model.fit(X_train, y_train, eval_set = [(X_val,y_val)], early_stopping_rounds = 222, verbose = False)\n    y_pred = model.predict_proba(X_val)[:,1]\n    roc_auc = roc_auc_score(y_val, y_pred)\n\n    return roc_auc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nstudy = optuna.create_study(direction = 'maximize')\nstudy.optimize(objective, n_trials = 30)\nprint('Number of finished trials:', len(study.trials))\nprint('Best trial:', study.best_trial.params)\nprint('Best value:', study.best_value)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Visualize optuna's optimization CatBoost**"},{"metadata":{"trusted":true},"cell_type":"code","source":"optuna.visualization.plot_optimization_history(study)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"optuna.visualization.plot_param_importances(study)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Fit CatBoost model with best parametrs**"},{"metadata":{"trusted":true},"cell_type":"code","source":"paramsCB = study.best_trial.params\nparamsCB['task_type'] = 'GPU'\nparamsCB['loss_function'] = 'Logloss'\nparamsCB['eval_metric'] = 'AUC'\nparamsCB['random_seed'] = 42\nparamsCB['bootstrap_type'] = 'Poisson'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nfrom sklearn.model_selection import KFold\n\nfolds = KFold(n_splits = 10, shuffle = True, random_state = 42)\n\npredictions = np.zeros(len(test))\n\nfor fold, (trn_idx, val_idx) in enumerate(folds.split(X, y)):\n    \n    X_train, X_val = X.iloc[trn_idx], X.iloc[val_idx]\n    y_train, y_val = y.iloc[trn_idx], y.iloc[val_idx]\n\n    model = CatBoostClassifier(**paramsCB)\n   \n    model.fit(X_train, y_train, eval_set = [(X_val, y_val)], verbose = False, early_stopping_rounds = 222)\n    \n    predictions += model.predict_proba(test)[:,1] / folds.n_splits ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame({'id': ID, 'target': predictions})\nsubmission.to_csv('submissionCB.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# XGB and optuna's optimization"},{"metadata":{"trusted":true},"cell_type":"code","source":"def objective(trial, data = X, target = y):\n    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.2, random_state = 0)\n\n    params = {\n        'max_depth': trial.suggest_int('max_depth', 3, 10),\n        'learning_rate': trial.suggest_categorical('learning_rate', [0.005, 0.02, 0.05, 0.08, 0.1]),\n        'n_estimators': trial.suggest_int('n_estimators', 50, 3000),\n        'min_child_weight': trial.suggest_int('min_child_weight', 1, 300),\n        'gamma': trial.suggest_float('gamma', 0.0001, 1.0, log = True),\n        'alpha': trial.suggest_float('alpha', 0.0001, 10.0, log = True),\n        'lambda': trial.suggest_float('lambda', 0.0001, 10.0, log = True),\n        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.1, 0.8),\n        'subsample': trial.suggest_float('subsample', 0.1, 0.8),\n        'tree_method': 'gpu_hist',\n        'booster': 'gbtree',\n        'random_state': 42,\n        'use_label_encoder': False,\n        'eval_metric': 'auc'\n\n    }\n    \n    model = XGBClassifier(**params)  \n    model.fit(X_train, y_train, eval_set = [(X_val,y_val)], early_stopping_rounds = 222, verbose = False)\n    y_pred = model.predict_proba(X_val)[:,1]\n    roc_auc = roc_auc_score(y_val, y_pred)\n\n    return roc_auc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nstudy = optuna.create_study(direction = 'maximize')\nstudy.optimize(objective, n_trials = 30)\nprint('Number of finished trials:', len(study.trials))\nprint('Best trial:', study.best_trial.params)\nprint('Best value:', study.best_value)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Visualize optuna's optimization XGB**"},{"metadata":{"trusted":true},"cell_type":"code","source":"optuna.visualization.plot_optimization_history(study)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"optuna.visualization.plot_param_importances(study)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Fit XGB model with best parametrs**"},{"metadata":{"trusted":true},"cell_type":"code","source":"paramsXGB = study.best_trial.params\nparamsXGB['tree_method'] = 'gpu_hist'\nparamsXGB['booster'] = 'gbtree'\nparamsXGB['eval_metric'] = 'auc'\nparamsXGB['random_state'] = 42\nparamsXGB['use_label_encoder'] = False","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nfrom sklearn.model_selection import KFold\n\nfolds = KFold(n_splits = 10, shuffle = True, random_state = 42)\n\npredictions = np.zeros(len(test))\n\nfor fold, (trn_idx, val_idx) in enumerate(folds.split(X, y)):\n    \n    X_train, X_val = X.iloc[trn_idx], X.iloc[val_idx]\n    y_train, y_val = y.iloc[trn_idx], y.iloc[val_idx]\n\n    model = XGBClassifier(**paramsXGB)\n   \n    model.fit(X_train, y_train, eval_set = [(X_val, y_val)], eval_metric = 'auc', verbose = False, early_stopping_rounds = 222)\n    \n    predictions += model.predict_proba(test)[:,1] / folds.n_splits ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame({'id': ID, 'target': predictions})\nsubmission.to_csv('submissionXGB.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# LGBM and optuna's optimizaton"},{"metadata":{"trusted":true},"cell_type":"code","source":"def objective(trial, data = X, target = y):\n    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.2, random_state = 0)\n\n    params = {\n        'reg_alpha': trial.suggest_float('reg_alpha', 0.001, 10.0),\n        'reg_lambda': trial.suggest_float('reg_lambda', 0.001, 10.0),\n        'num_leaves': trial.suggest_int('num_leaves', 11, 333),\n        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n        'max_depth': trial.suggest_int('max_depth', 5, 20),\n        'learning_rate': trial.suggest_categorical('learning_rate', [0.01, 0.02, 0.05, 0.005, 0.1]),\n        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.1, 0.5),\n        'n_estimators': trial.suggest_int('n_estimators', 50, 3000),\n        'random_state': 42,\n        'boosting_type': 'gbdt',\n        'metric': 'AUC',\n        'device': 'gpu'\n    }\n    \n    model = LGBMClassifier(**params)  \n    model.fit(X_train, y_train, eval_set = [(X_val,y_val)], early_stopping_rounds = 222, verbose = False)\n    y_pred = model.predict_proba(X_val)[:,1]\n    roc_auc = roc_auc_score(y_val, y_pred)\n\n    return roc_auc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nstudy = optuna.create_study(direction = 'maximize')\nstudy.optimize(objective, n_trials = 30)\nprint('Number of finished trials:', len(study.trials))\nprint('Best trial:', study.best_trial.params)\nprint('Best value:', study.best_value)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Visualize optuna's optimization LGBM**"},{"metadata":{"trusted":true},"cell_type":"code","source":"optuna.visualization.plot_optimization_history(study)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"optuna.visualization.plot_param_importances(study)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Fit LGBM model with best parametrs**"},{"metadata":{"trusted":true},"cell_type":"code","source":"paramsLGBM = study.best_trial.params\nparamsLGBM['boosting_type'] = 'gbdt'\nparamsLGBM['metric'] = 'AUC'\nparamsLGBM['random_state'] = 42","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nfrom sklearn.model_selection import KFold\n\nfolds = KFold(n_splits = 10, shuffle = True, random_state = 42)\n\npredictions = np.zeros(len(test))\n\nfor fold, (trn_idx, val_idx) in enumerate(folds.split(X, y)):\n    \n    X_train, X_val = X.iloc[trn_idx], X.iloc[val_idx]\n    y_train, y_val = y.iloc[trn_idx], y.iloc[val_idx]\n\n    model = LGBMClassifier(**paramsLGBM)\n   \n    model.fit(X_train, y_train, eval_set = [(X_val, y_val)], eval_metric = 'auc', verbose = False, early_stopping_rounds = 222)\n    \n    predictions += model.predict_proba(test)[:,1] / folds.n_splits ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame({'id': ID, 'target': predictions})\nsubmission.to_csv('submissionLGBM.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conclusion"},{"metadata":{"trusted":true},"cell_type":"code","source":"results = pd.DataFrame([['CatBoost', 0.89144],\n                  ['XGB', 0.89414],\n                  ['LGBM', 0.89490]], \ncolumns = ['Algorithm', 'BestScore'])\nresults","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Well, based on the results in the table above, we can say that CatBoost has the lowest score. XGB showed an average result. LGBM has the best result (so that's why everyone uses it in TPS). Of course, only 30 trials were done with Optuna, and CatBoost may have better result, but this is just my little experiment, where all algorithms are on an equal conditions. Conclusion - if we want to win in TPS competitions on Kaggle and fight for every 0.001 or even 0.0001, then our choice slow, but true - LGBM.**"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}