{"cells":[{"metadata":{},"cell_type":"markdown","source":"# LightGBM with parameter tunning using Optuna"},{"metadata":{},"cell_type":"markdown","source":"This notebook is based on the one I used in February. It can help you get started with LGBM and Optuna, in case you have not used these technologies before. I believe there is a lot of room for improvement and I will work on it throughout the competition."},{"metadata":{},"cell_type":"markdown","source":"**VERSION 7**: I tried standardizing the continuous variables, but the results were a little worse.\n\n**VERSION 8**: The continuous variables are not standardized anymore. The categorical variables are now label encoded independntly in each column, since an \"A\" in column `cat0` does not necessarily mean the same thing as an \"A\" in the column `cat1`.\n\n**VERSION 10**: Added `cat_l2` to list of parameters being optimized.\n\n**VERSION 11**: Now Using LGBM to recognize categorical features by changing the variable type to \"category\". Parameter `cat_feature` removed from list."},{"metadata":{},"cell_type":"markdown","source":"# Load libraries and data"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom pathlib import Path\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import ParameterGrid\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import roc_auc_score\nfrom lightgbm import LGBMClassifier\nimport optuna\n        \ninput_path = Path('/kaggle/input/tabular-playground-series-mar-2021/')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(input_path / 'train.csv', index_col='id')\ntest = pd.read_csv(input_path / 'test.csv', index_col='id')\ntarget = train.pop('target')\nsubmission = pd.read_csv(input_path / 'sample_submission.csv', index_col='id')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Encode categorical variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_cols = [col for col in train.columns if 'cat' in col]\n\ntrain[cat_cols] = train[cat_cols].astype('category')\ntest[cat_cols] = test[cat_cols].astype('category')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data split and base model"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_valid, y_train, y_valid = train_test_split(train, target, test_size=0.1, random_state=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Last month we had a regression problem and now we have a classification one. So I'm using `LGBMClassifier` instead of `LGBMRegressor`. For the base model, we need only set our metric to `auc`."},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nmodel = LGBMClassifier(random_state=0, metric='auc')\nmodel.fit(X_train, y_train)\ny_pred = model.predict_proba(X_valid)[:,1]\nauc = roc_auc_score(y_valid, y_pred)\nprint('AUC =', f'{auc:0.5f}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The AUC is 0.89125, which is a little better than the score obtained with the [Getting Started Notebook](https://www.kaggle.com/inversion/get-started-mar-tabular-playground-competition) random forest model (0.87176)."},{"metadata":{},"cell_type":"markdown","source":"# Set objective function for Optuna with parameters and their ranges"},{"metadata":{},"cell_type":"markdown","source":"I am using the same parameters and ranges I did last time, but of course you can try to find more suitable ones."},{"metadata":{"trusted":true},"cell_type":"code","source":"def objective(trial):\n    params = {\n        'metric': 'auc',\n        'random_state': 0,\n        'n_estimators': trial.suggest_categorical('n_estimators', [1000]),\n        'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.1),\n        'reg_lambda': trial.suggest_float('reg_lambda', 0.001, 10.0),\n        'reg_alpha': trial.suggest_float('reg_alpha', 0.001, 10.0),\n        'colsample_bytree': trial.suggest_categorical('colsample_bytree', [0.1, 0.2, 0.3, 0.4, 0.5]),\n        'min_child_samples': trial.suggest_int('min_child_samples', 1, 300),\n        'max_depth': trial.suggest_int('max_depth', 6, 127),\n        'num_leaves': trial.suggest_int('num_leaves', 31, 128),\n        'cat_smooth': trial.suggest_int('cat_smooth', 10, 100),\n        'cat_l2': trial.suggest_int('cat_l2', 1, 20),\n    }\n    model = LGBMClassifier(**params) \n    model.fit(X_train, y_train, eval_set=[(X_valid,y_valid)], early_stopping_rounds=100, verbose=0)\n    y_pred = model.predict_proba(X_valid)[:,1]\n    auc = roc_auc_score(y_valid, y_pred)\n    \n    return auc","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In last month's competition, the metric used was the RMSE (root mean squared error), but this month it is the AUC (area under curve). So this time we want to **maximize** it. In [this notebook](https://www.kaggle.com/ekozyreff/tps-2021-03-roc-and-auc-tutorial) I wrote a brief tutorial about ROC and AUC."},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nstudy = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=0))\nstudy.optimize(objective, n_trials=100)\nprint('Number of finished trials:', len(study.trials))\nprint('Best parameters:', study.best_trial.params)\nprint('Best AUC:', study.best_trial.value)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Visualize optimization history"},{"metadata":{"trusted":true},"cell_type":"code","source":"optuna.visualization.plot_optimization_history(study)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Recover best parameters found and build final predictions"},{"metadata":{},"cell_type":"markdown","source":"Let's recover the best parameters for our model according to the optimization performed by Optuna. \n\nTo perform the final predictions, I increased the number of estimators to 20000, since this worked well in previous tests."},{"metadata":{"trusted":true},"cell_type":"code","source":"params = study.best_params\nparams['random_state'] = 0\nparams['metric'] = 'auc'\nparams['n_estimators'] = 20000","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finally, let us split the training data in 10 folds and build our final model."},{"metadata":{"trusted":true},"cell_type":"code","source":"n_folds = 10\nkf = KFold(n_splits=n_folds, shuffle=True, random_state=0)\ny_pred = np.zeros(test.shape[0])\n\nfor fold, (train_index, valid_index) in enumerate(kf.split(train, target)):\n    print(\"Running Fold {}\".format(fold + 1))\n    X_train, X_valid = pd.DataFrame(train.iloc[train_index]), pd.DataFrame(train.iloc[valid_index])\n    y_train, y_valid = target.iloc[train_index], target.iloc[valid_index]\n    model = LGBMClassifier(**params)\n    model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], early_stopping_rounds=1000, verbose=0)\n    print(\"  AUC: {}\".format(roc_auc_score(y_valid, model.predict_proba(X_valid)[:, 1])))\n    y_pred += model.predict_proba(test)[:,1]    \n\ny_pred /= n_folds\n\nprint(\"\")\nprint(\"Done!\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission['target'] = y_pred\nsubmission.to_csv('lgbm_optuna_enc.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"If you found this notebook helpful, please upvote üëç and also feel free to leave comments and suggestions below. Thanks!"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}