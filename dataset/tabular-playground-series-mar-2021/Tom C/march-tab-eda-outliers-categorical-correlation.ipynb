{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Exploratory Data Analysis (outliers, CDFs, categorical correlations)\n\nBelow is my exploratory analysis for the March tabular dataset.\n\nPlease let me know what you think in the comments and **upvote** if you find anything useful.\n\nThanks and enjoy!"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport scipy.stats as ss\n\nfrom sklearn.neighbors import LocalOutlierFactor\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import LinearRegression\n\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\n\nfrom scipy.stats import norm\nimport scipy.stats as st\n\n!pip install sklearn-contrib-py-earth\nfrom pyearth import Earth\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load the Data\n\nHere we will load the data into a pandas dataframe."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv('../input/tabular-playground-series-mar-2021/train.csv')\ntest_df = pd.read_csv('../input/tabular-playground-series-mar-2021/test.csv')\ndisplay(train_df.head())\ntrain_df.describe()\nprint(train_df.columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see here that we have quite a few categorical variables (especially compared to previous months) and 10 continuous variables."},{"metadata":{"trusted":true},"cell_type":"code","source":"cont_FEATURES = ['cont%d' % (i) for i in range(0, 11)]\ncat_FEATURES = ['cat%d' % (i) for i in range(0, 19)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Cleaning the Dataset\n\nFollowing the steps of the Machine Learning Checklist we will start by cleaning out invalid values and outliers from the dataset."},{"metadata":{"trusted":true},"cell_type":"markdown","source":" ### Examine the target"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(set(train_df['target'].values))\ntrain_df['target'].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So above we can see that we have a binary target and where the majority (almost 75%) of the values are 0. "},{"metadata":{},"cell_type":"markdown","source":"### Invalid Values"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we can see that there are no non-null values so there is nothing to remove here.\n\n### Outliers\n\nRemoving outliers is less of a science and more of an art form. So I will leave the choice up to you, but show you how to visualise these points.\n\nWe will add noise to the one dimensional features in order to \"explode\" the points out, helping us see the distributions and potential outliers.\n\nWe will consider a point to be an outlier if it is N standard deviations from the mean. N is defined as the threshold."},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_outliers(df, feature, threshold=3):\n    mean, std = np.mean(df), np.std(df)\n    z_score = np.abs((df-mean) / std)\n    good = z_score < threshold\n\n    print(f\"Rejection {(~good).sum()} points\")\n    visual_scatter = np.random.normal(size=df.size)\n    plt.scatter(df[good], visual_scatter[good], s=2, label=\"Good\", color=\"#4CAF50\")\n    plt.scatter(df[~good], visual_scatter[~good], s=8, label=\"Bad\", color=\"#F44336\")\n    plt.legend(loc='upper right')\n    plt.title(feature)\n    plt.show();\n    \n    return good","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature Outliers\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"for feature in cont_FEATURES:\n    plot_outliers(train_df[feature], feature)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see from the above that there are possibly some outliers for `cont8` that could be removed if you were struggling with the accuracy of your model."},{"metadata":{},"cell_type":"markdown","source":"# Analysing Distributions\n\nHere we will look at correlations between the features, distributions of the features.\n\nFirst let's check that each row has it's own unique id."},{"metadata":{"trusted":true},"cell_type":"code","source":"len(set(list(train_df['id'].values)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Continuous Variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"for feature in cont_FEATURES:\n    sns.violinplot(x=train_df[feature], inner='quartile', bw=0.1)\n    plt.title(feature)\n    plt.show();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for feature in cont_FEATURES:\n    sns.violinplot(x='target', y=feature, data=train_df, inner='quartile');\n    plt.title(feature)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above analysis we can see that most of the features have some variation depending on the target value, however this difference is subtle. Therefore no feature is going to be a silver bullet."},{"metadata":{},"cell_type":"markdown","source":"# Categorical Variables\n\nFirst let's look at what values the categorical variables can take."},{"metadata":{"trusted":true},"cell_type":"code","source":"for cat in cat_FEATURES:\n    values = train_df.groupby(cat)['id'].count().reset_index()\n    sns.barplot(x=cat, y='id', data=values)\n    plt.title(cat)\n    plt.show();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This quick piece of analysis shows us that some categorical features are binary, others have a large number of categories. We can also see that there is a lot of class imbalance in these features which could help us build a feature set to predict the target."},{"metadata":{"trusted":true},"cell_type":"code","source":"number_of_rows = train_df.shape[0]\nfor feature in cat_FEATURES:\n    percentage_common_category = train_df.groupby(feature)['id'].count().reset_index()\n    print(feature)\n    print(percentage_common_category['id'].max() / number_of_rows)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# TODO: Stacked bar chart to show the percentage of target 0, that have a label, and the percentage of target 1 that have a label\n# TODO: Like further down in this notebook https://www.kaggle.com/tsilveira/applying-heatmaps-for-categorical-data-analysis","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Empirical CDFs\n\nThe below graphs show us where the 10th/20th/..../90th percentiles lie for each of the features."},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_cdf(df, feature):\n    ps = 100 * st.norm.cdf(np.linspace(-4, 4, 10)) # The last number in this tuple is the number of percentiles\n    x_p = np.percentile(df, ps)\n\n    xs = np.sort(df)\n    ys = np.linspace(0, 1, len(df))\n\n    plt.plot(xs, ys * 100, label=\"ECDF\")\n    plt.plot(x_p, ps, label=\"Percentiles\", marker=\".\", ms=10)\n    plt.legend()\n    plt.ylabel(\"Percentile\")\n    plt.title(feature)\n    plt.show();\n\nfor feature in cont_FEATURES:\n    plot_cdf(train_df[feature], feature)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The majority of these continuous feature's CDFs are smooth and show a relatively even distribution of values. However we can see that `Cont3` and `Cont5` clearly have *steps* in their data that suggest that these features could be discretized."},{"metadata":{},"cell_type":"markdown","source":"# Correlation\n\nHere we can look at the correlation between the features and each other (and the target)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# This plots a matrix of correlations between all the features and the target\n# Note: I sometimes comment this out because it takes a few minutes to run and doesn't show any useful information.\n\n# pd.plotting.scatter_matrix(train_df, figsize=(10, 10));","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Continuous Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(10,10)) \nsns.heatmap(train_df.drop(columns=['id']).corr(), annot=True, cmap='viridis', fmt='0.2f', ax=ax)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It is promising to see relatively high numbers of correlations here. We can see some groups of features that could be suitable for PCA dimensionality reduction. For example `[cont1, cont2, cont3, cont7, cont8, cont9, cont10]`.\n"},{"metadata":{},"cell_type":"markdown","source":"### Categorical Features\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# I stole this method from here https://stackoverflow.com/questions/46498455/categorical-features-correlation/46498792#46498792\n\ndef cramers_v(confusion_matrix):\n    \"\"\" calculate Cramers V statistic for categorial-categorial association.\n        uses correction from Bergsma and Wicher,\n        Journal of the Korean Statistical Society 42 (2013): 323-328\n    \"\"\"\n    chi2 = ss.chi2_contingency(confusion_matrix)[0]\n    n = confusion_matrix.sum()\n    phi2 = chi2 / n\n    r, k = confusion_matrix.shape\n    phi2corr = max(0, phi2 - ((k-1)*(r-1))/(n-1))\n    rcorr = r - ((r-1)**2)/(n-1)\n    kcorr = k - ((k-1)**2)/(n-1)\n    return np.sqrt(phi2corr / min((kcorr-1), (rcorr-1)))\n\ncm = pd.DataFrame(columns=cat_FEATURES+['target'], index=cat_FEATURES+['target'])\n\nfor feature_1 in cat_FEATURES+['target']:\n    for feature_2 in cat_FEATURES+['target']:\n        confusion_matrix = pd.crosstab(train_df[feature_1], train_df[feature_2])\n        #print(feature)\n        #print(cramers_v(confusion_matrix.values))\n        cm.at[feature_1, feature_2] = float(cramers_v(confusion_matrix.values))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(25,10)) \nsns.heatmap(cm.astype(float).values, vmin=0, vmax=1, xticklabels=cat_FEATURES+['target'], yticklabels=cat_FEATURES+['target'], annot=True, ax=ax)\nplt.title('Categorical Features Correlation')\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above we can see correlations between the features with each other and even with the target. These can all help use decide whether to run dimensionality reduction such as Multiple Corresponance Analysis."},{"metadata":{},"cell_type":"markdown","source":"# Feature Engineering\n\nThis is still a work in progress but next I will perform some feature engineering based on the above findings (such as PCA for continuous features and MCA for categorical features)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# TODO: Feature engineering based on the above findings","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# TODO: PCA for continuous features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# TODO: MCA for categorical features\n# https://pypi.org/project/mca/","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}