{"cells":[{"metadata":{},"cell_type":"markdown","source":"LightGBM is a really convenient to use, fast to train and usually accurate implementation of boosted trees. Here I use optuna for hyperparameter search using Bayesian optimization methods, with 5-fold cross validation, to gain a fairly accurate model. Also I leverage some seemingly minor but very useful built in features of the LightGBM library to handle categorical variables."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"##Import the required packages. These include pandas, numpy,scikit-learn and optuna\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import KFold\nimport optuna\nimport optuna.integration.lightgbm as lgb\n\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Read the train and test csv's into variables and list out the column names"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/tabular-playground-series-mar-2021/train.csv')\ntest = pd.read_csv('/kaggle/input/tabular-playground-series-mar-2021/test.csv')\n\ntrain.columns.to_list()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Convert the categorical data into the category type such that lightgbm can handle the categorical variables. Unless you leverage learned embeddings for categorical variables, this fares better that one hot encoding or label encoding"},{"metadata":{"trusted":true},"cell_type":"code","source":"conts = ['cont0','cont1','cont2','cont3','cont4','cont5','cont6','cont7','cont8','cont9','cont10']\ncats = ['cat0','cat1','cat2','cat3','cat4','cat5','cat6','cat7','cat8','cat9','cat10','cat11','cat12','cat13','cat14','cat15','cat16','cat17','cat18','target']\nfor c in train.columns:\n    col_type = train[c].dtype\n    if col_type == 'object' or col_type.name == 'category':\n        train[c] = train[c].astype('category')\n\nfor c in test.columns:\n    col_type = test[c].dtype\n    if col_type == 'object' or col_type.name == 'category':\n        test[c] = test[c].astype('category')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Specify dependent and independent variables and create a lgb Dataset object"},{"metadata":{"trusted":true},"cell_type":"code","source":"X= train[['cont0','cont1','cont2','cont3','cont4','cont5','cont6','cont7','cont8','cont9','cont10','cat0','cat1','cat2','cat3','cat4','cat5','cat6','cat7','cat8','cat9','cat10','cat11','cat12','cat13','cat14','cat15','cat16','cat17','cat18']]\nY = train[['target']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Use the optuna lightGBM integration to do hyperparamater optimization with 5 fold cross validation. Make sure to pass in the argument 'auto' for categorical_feature for automated feature engineering for categorical input features."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold\nkfolds = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Notice how I've specified auc as the metric."},{"metadata":{"trusted":true},"cell_type":"code","source":"dtrain = lgb.Dataset(X,Y,categorical_feature = 'auto')\n\nparams = {\n    \"objective\": \"binary\",\n    \"metric\": \"auc\",\n    \"verbosity\": -1,\n    \"boosting_type\": \"gbdt\",\n}\n\ntuner = lgb.LightGBMTunerCV(\n    params, dtrain, verbose_eval=100, early_stopping_rounds=1000000, folds=kfolds\n)\n\ntuner.run()\n\nprint(\"Best score:\", tuner.best_score)\nbest_params = tuner.best_params\nprint(\"Best params:\", best_params)\nprint(\"  Params: \")\nfor key, value in best_params.items():\n    print(\"    {}: {}\".format(key, value))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Inspect the best score for AUC value"},{"metadata":{"trusted":true},"cell_type":"code","source":"tuner.best_score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Assign the best params to a variable"},{"metadata":{"trusted":true},"cell_type":"code","source":"params = tuner.best_params","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Use these parameters to train a LightGBM model on the entire training dataset \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"import lightgbm as lgb\nid_test = test.id.to_list()\nmodel = lgb.train(params, dtrain, num_boost_round=1000)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Predict on the test set and save file. Make sure you set index=False "},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test = test[['cont0','cont1','cont2','cont3','cont4','cont5','cont6','cont7','cont8','cont9','cont10','cat0','cat1','cat2','cat3','cat4','cat5','cat6','cat7','cat8','cat9','cat10','cat11','cat12','cat13','cat14','cat15','cat16','cat17','cat18']]\npreds = model.predict(X_test)\nresultf = pd.DataFrame()\nresultf['id'] = id_test\nresultf['target'] = preds\nresultf.to_csv('submission.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}