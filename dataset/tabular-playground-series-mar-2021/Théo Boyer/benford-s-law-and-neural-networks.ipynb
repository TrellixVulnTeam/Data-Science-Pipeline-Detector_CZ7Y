{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\"\"\"\nImports\n\"\"\"\n\nimport numpy as np\nimport pandas as pd\nfrom random import random, randint, sample\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.decomposition import PCA\nfrom copy import deepcopy\nimport math\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport matplotlib.pyplot as plt\n\nif torch.cuda.is_available():  \n    dev = \"cuda:0\" \nelse:  \n    dev = \"cpu\"\ndevice = torch.device(dev)\nCUDA_LAUNCH_BLOCKING=\"1\"","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Benford's law and Neural Networks\nThis notebook is an attempt to make use of the results of [this paper](https://arxiv.org/pdf/2102.03313.pdf). \nOne of the conclusions of the said paper is that the \\\\( MLH \\\\) is a good indicator of whether the network is overfitting or not.","metadata":{}},{"cell_type":"markdown","source":"# Benford's law\n> The leftmost non-zero digit’s occurrence in the observations\nof a population is not uniformly distributed for many datasets.\nInstead, it is log-uniform, with 1 occurring the maximum\nnumber of times, followed by 2, 3, ... 9. According to Benford’s Law, the probability for a sample having a significant digit d is given as follows:\n\n$$ P(d) = log_{10}(1 + \\frac{1}{d}), d = 1, 2, 3, ..., 9$$","metadata":{}},{"cell_type":"code","source":"d = torch.arange(1, 10, device=device)\nbenford_b10 = torch.log10(1 +1/d)\nplt.title(\"Benford's law for base 10\")\nplot = plt.plot(np.arange(1, 10), benford_b10.cpu().numpy())","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# MLH metric\nThe \\\\( MLH \\\\) metric is defined in the paper:\n> We devise a new metric, \\\\( MLH \\\\), that provides the measure\nof correlation between Benford’s Law and the distribution of\nsignificant digits of model weights.\nThis score indicates a strong correlation between the\nweights and BL and proves to be an essential metric for a\nwide range of neural networks, and is described in the following sections.\nThe proposed score, \\\\( MLH \\\\) based on the Pearson’s Correlation Coefficient is defined as follows:\n\n$$MLH(\\theta) = PearsonR(BinCount(\\theta), \\beta)$$\n\n> \\\\( BinCount(\\theta) \\\\) is the distribution of Significant Digits of network parameter set \\\\( \\theta \\\\). \\\\( \\beta \\\\) is the distribution defined by BL :\n\n\n$$BinCount(\\theta) = \\frac{[f_0, f_1, ..., f_1]}{D_{\\theta}}$$\n\n## Pearson's correlation coefficient\n\nPearson's correlation coefficient is basically the covariance normalized between -1 and 1.\nIts formula is:\n\n$$\nPearsonR(X, Y) = \\frac{cov(X, Y)}{\\sigma_X\\sigma_Y} \\\\\n$$","metadata":{}},{"cell_type":"markdown","source":"# KLDB Metric\n\nAs the MLH represents a certain kind of correlation, I would like to test another metric that measures a distance between two distributions. I'll refer as the \\\\( KLDB \\\\) the metric defined by:\n\n$$\n    KLDB(\\theta) = KLDivergence(BinCount(\\theta), \\beta)\n$$\n\n## KLDivergence\n\nThe KLDivergence is defined on Wikipedia as follows:\n\n> In mathematical statistics, the Kullback–Leibler divergence, \\\\( D_{\\text{KL}} \\\\) (also called relative entropy), is a measure of how one probability distribution is different from a second, reference probability distribution.\n\nIts formula is:\n\n$$\n     D_{\\text{KL}}(P\\parallel Q)=\\sum _{x\\in {\\mathcal {X}}}P(x)\\log \\left({\\frac {P(x)}{Q(x)}}\\right)\n$$\n\nSo the closest \\\\( KLDB \\\\) is to \\\\( 0 \\\\), the closest the distribution of the first significant digit in the neural network parameters is to Benford's law.","metadata":{}},{"cell_type":"code","source":"\"\"\"\n    MLH Metric\n\"\"\"\n\ndef pearsonR(X, Y):\n    \"\"\"\n        return the Pearson's correlation coefficient between variable X and Y\n    \"\"\"\n    x_centered = (X - torch.mean(X))\n    y_centered = (Y - torch.mean(Y))\n    \n    r = torch.sum(x_centered * y_centered)\n    r /= torch.sqrt(torch.sum(torch.square(x_centered))) * torch.sqrt(torch.sum(torch.square(y_centered)))\n    \n    return r\n\ndef BinCount(theta):\n    \"\"\"\n        Return the frequence of each digit's occurence as first significant digit in the parameter set defined by theta\n    \"\"\"\n    abs_scaled = torch.abs(theta) * 1e10\n    digits = (abs_scaled // (10 ** torch.log10(abs_scaled).long())).long()\n    val, counts = torch.unique(digits, return_counts=True)\n\n    freq = torch.zeros(9, device=device)\n    freq[val-1] = counts.float()\n    freq /= max(1, torch.numel(theta))\n\n    return freq\n    \n@torch.no_grad()\ndef MLH(theta):\n    \"\"\"\n        Return the MLH of a set of parameters\n    \"\"\"\n    # Security in case there are zeros\n    theta = theta[theta.abs() > 1e-9]\n    return pearsonR(BinCount(torch.flatten(theta)), benford_b10)\n\ndef KlDivergence(P, Q):\n    \"\"\"\n        Return the KL Divergence between two distributions\n    \"\"\"\n    ratio = P / Q\n    # Security in case there are zeros\n    ratio[ratio == 0.0] = 1e-7\n    return torch.sum(P * torch.log2(ratio))\n\n@torch.no_grad()\ndef KLDB(theta):\n    \"\"\"\n        Return the KLDB (KL Divergence between the distribution of first significant didgits of theta and benford's law)\n    \"\"\"\n    # Security in case there are zeros\n    theta = theta[theta.abs() > 1e-9]\n    return KlDivergence(BinCount(torch.flatten(theta)), benford_b10)\n    \n\ndef model_MLH(model):\n    \"\"\"\n        Return the MLH of a model\n    \"\"\"\n    params = []\n    for p in model.parameters():\n        params.append(torch.flatten(p.detach()))\n    params_flattened = torch.cat(params)\n    return MLH(params_flattened)\n\ndef model_KLDB(model):\n    \"\"\"\n        Return the KLDB of a model\n    \"\"\"\n    params = []\n    for p in model.parameters():\n        params.append(torch.flatten(p.detach()))\n    params_flattened = torch.cat(params)\n    return KLDB(params_flattened)\n\ndef model_benford_metrics(model):\n    \"\"\"\n        Return both the MLH and the KLDB of a model\n    \"\"\"\n    params = []\n    for p in model.parameters():\n        params.append(torch.flatten(p.detach()))\n    theta = torch.cat(params)\n    theta = theta[theta.abs() > 1e-9]\n    bc = BinCount(torch.flatten(theta))\n    mlh = pearsonR(bc, benford_b10)\n    kldb = KlDivergence(bc, benford_b10)\n    return mlh.item(), kldb.item()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Empirical test on datasets\n\nSo I'm gonna use different datasets and try to fit an autoencoder with cross-validation on them. I do that because it's a task that we can do for each dataset as long as it contains numbers. I'll try to select the best model once based on the validation loss, once based on the \\\\( MLH \\\\) score, and once based on the \\\\( KLDB \\\\) score. I'll also measure the performances of the model at the end of training without any selection. The hypothetical advantage to use MLH and KLDB is that it allows the training process to use all of the data.","metadata":{}},{"cell_type":"code","source":"\"\"\"\n    Data management functions\n\"\"\"\ndef process_data(csv_file):\n    \"\"\"\n        Take a csv_file, reads it and return the numerical normalized vaues\n    \"\"\"\n    data = pd.read_csv(csv_file)\n    final = pd.DataFrame()\n    for c in data.columns:\n        # We only care about numerical data, so skip the categorical features\n        if data[c].dtype != \"object\" and data[c].duplicated().any():\n            m = data[c].mean()\n            s = data[c].std()\n            final[c] = (data[c] - m) / (s if s > 0.0 else 1)\n    return final.values","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\n    Neural network auto-encoder functions\n\"\"\"\n\ndef get_model(data_shape, n_hidden_layers, n_components):\n    \"\"\"\n        Return two identical autoencoders that takes as input a data of size data_shape,\n        compress it to n_components, and reconstruct it to data_shape with n_hidden_layers\n        in both the encoder and decoder part\n    \"\"\"\n    layers = [nn.Flatten()]\n    a = data_shape\n    # We select the number of units linearly\n    units = np.linspace(float(data_shape), float(n_components), num=2 + n_hidden_layers)\n    \n    for b in units[1:]:\n        layers += [nn.Linear(int(a), int(b)), nn.ReLU()]\n        a = b\n        \n    for b in list(reversed(units))[1:]:\n        layers += [nn.Linear(int(a), int(b)), nn.ReLU()]\n        a = b\n        \n    layers = layers[:-1]\n    model = nn.Sequential(*layers)\n    # Name it to be able to find it later\n    model.name = 'AE_' + '-'.join(map(str, units.astype(np.uint))) + '|' + '-'.join(map(str, reversed(units.astype(np.uint))))\n    return deepcopy(model).to(device), deepcopy(model).to(device)\n\ndef build_models(data, n_max_layers=4, pca_prop=0.4, n_folds=4):\n    \"\"\"\n        Iterator that yields n_folds paris of models for each possible architecture until n_max_layers with \n        a hidden representation of the same size as a PCA requires to explain a fraction of \n        pca_prop of the variance of the data\n    \"\"\"\n    # Calculate the desired number of components (Just a way to account for the complexity of the data)\n    pca = PCA()\n    pca.fit(data)\n    n_components = np.argmax(np.cumsum(pca.explained_variance_ratio_) > pca_prop)\n    \n    data_shape = int(data.size / data.shape[0])\n    # Yield models\n    for i in range(n_max_layers):\n        for n in range(n_folds):\n            yield get_model(data_shape, i+1, n_components)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\n    Training functions\n\"\"\"\n\ndef train_val(model, data, n_steps=15000, batch_size=32, val_every=50):\n    \"\"\"\n        Train \"model\" on \"data\" for \"n_steps\" with \"batch_size\". A validation step is made every \"val_every\" steps.\n        Select the best model using validation loss.\n        Return the predictions of the best model on the test data and its kldb\n    \"\"\"\n    print(\"Training {}\".format(model.name))\n    # Separate validation and training\n    X_train, X_test = data\n    X_train, X_val, y_train, y_val = train_test_split(X_train, X_train, test_size=0.15)\n    \n    criterion = nn.MSELoss()\n    optimizer = torch.optim.Adam(model.parameters())\n    \n    idxs = []\n    loss_history = []\n    val_loss_history = []\n    best_val_loss = float(\"inf\")\n    X_val_batch = torch.tensor(X_val, device=device, dtype=torch.float).unsqueeze(1)\n    y_val_batch = torch.tensor(y_val, device=device, dtype=torch.long)\n    for i in range(n_steps):\n        # At end of epoch just loop\n        if not len(idxs):\n            idxs = np.random.permutation(np.arange(len(X_train)))\n        optimizer.zero_grad()\n        # Sample a batch\n        bidxs = idxs[:batch_size]\n        idxs = idxs[batch_size:]\n        \n        X_batch = torch.tensor(X_train[bidxs], device=device, dtype=torch.float)\n        y_batch = torch.tensor(y_train[bidxs], device=device, dtype=torch.float)\n\n        y_pred = model(X_batch)\n        # Optimisation\n        train_loss = criterion(y_pred, y_batch)\n        train_loss.backward()\n        loss_history.append(train_loss.item())\n        optimizer.step()\n\n        del X_batch\n        del y_batch\n        # Evaluation\n        with torch.no_grad():\n            y_pred = model(X_val_batch)\n            val_loss = criterion(y_pred, y_val_batch).item()\n        # Calculate the Benford metrics\n        mlh, kldb = model_benford_metrics(model)\n        print(\"Step {}: loss={:.4f} | val_loss={:.4f} | MLH={:.4f} | KLDB={:.4f}\".format(\n            i, train_loss, val_loss, mlh, kldb\n        ), end='\\r')\n        val_loss_history.append(val_loss)\n        # We save the model if it's better\n        if i%val_every==0 and val_loss < best_val_loss:\n            torch.save(model, \"best_acc.pt\")\n            best_val_loss = val_loss\n            best_val_kldb = kldb\n            \n    print(\"Last val loss={:.5f} | Best val loss={:.5f}                           \".format(\n        val_loss, best_val_loss\n    ))\n    model = torch.load(\"best_acc.pt\")\n    return model(torch.tensor(X_test, device=device, dtype=torch.float)), best_val_kldb\n            \ndef train_benford(model, data, n_steps=15000, batch_size=32):\n    \"\"\"\n        Train \"model\" on \"data\" for \"n_steps\" with \"batch_size\".\n        Select the best model using respectively MLH and KLDB.\n        Return the predictions of the best models and the last one on the test data\n    \"\"\"\n    print(\"Training {}\".format(model.name))\n    X_train, X_test = data\n    y_train = X_train\n    \n    criterion = nn.MSELoss()\n    optimizer = torch.optim.Adam(model.parameters())\n    \n    idxs = []\n    mlh_history = []\n    kldb_history = []\n    loss_history = []\n    best_mlh = -1\n    best_kldb = float(\"inf\")\n    for i in range(n_steps):\n        # At end of epoch just loop\n        if not len(idxs):\n            idxs = np.random.permutation(np.arange(len(X_train)))\n        optimizer.zero_grad()\n\n        # Sample a batch\n        bidxs = idxs[:batch_size]\n        idxs = idxs[batch_size:]\n\n        X_batch = torch.tensor(X_train[bidxs], device=device, dtype=torch.float)\n        y_batch = torch.tensor(y_train[bidxs], device=device, dtype=torch.float)\n\n        y_pred = model(X_batch)\n\n        # Optimisation\n        train_loss = criterion(y_pred, y_batch)\n        train_loss.backward()\n        loss_history.append(train_loss.item())\n        optimizer.step()\n\n        del X_batch\n        del y_batch\n\n        # Calculate the Benford metrics\n        mlh, kldb = model_benford_metrics(model)\n        print(\"Step {}: loss={:.5f} | MLH={:.5f} | KLDB={:.5f}\".format(\n            i, train_loss, mlh, kldb\n        ), end='\\r')\n        mlh_history.append(mlh)\n        kldb_history.append(kldb)\n        # Save the model if it's better according to the metrics\n        if mlh > best_mlh:\n            torch.save(model, \"best_mlh.pt\")\n            best_mlh = mlh\n            \n        if kldb < best_kldb:\n            torch.save(model, \"best_kldb.pt\")\n            best_kldb = kldb\n            \n    print(\"Last MLH={:.5f} | Best MLH={:.5f}                     \".format(\n        mlh, best_mlh)\n    )\n    print(\"Last KLDB={:.5f} | Best KLDB={:.5f}\".format(kldb, best_kldb))\n    X_t_test = torch.tensor(X_test, device=device, dtype=torch.float)\n    last_model_preds = model(X_t_test)\n    model = torch.load(\"best_mlh.pt\")\n    mlh_preds = model(X_t_test)\n    kldb = torch.load(\"best_kldb.pt\")\n    kldb_preds = model(X_t_test)\n    return mlh_preds, kldb_preds, last_model_preds","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\n    Experiment function\n\"\"\"\n\ndef experiment_on_dataset(csv_file, n_folds=3):\n    \"\"\"\n        Run the previously described experiment on the data in \"csv_file\" with \"n_folds\"\n    \"\"\"\n    print(\"Running on dataset:\",csv_file)\n    # Load data\n    all_data = process_data(csv_file)\n    # To save some values\n    df = pd.DataFrame(columns=['data', 'model', 'best_mlh', 'best_mlh_val_loss', 'best_kldb', 'best_kldb_val_loss', 'best_val_loss'])\n    criterion = nn.MSELoss()\n    # Histories\n    val_hist = []\n    best_val_kldb_hist = []\n    mlh_hist = []\n    kldb_hist = []\n    last_model_hist = []\n    best_val_kldb_buffer = []\n    val_preds = torch.zeros(all_data.shape, device=device, dtype=torch.float)\n    mlh_preds = torch.zeros(all_data.shape, device=device, dtype=torch.float)\n    kldb_preds = torch.zeros(all_data.shape, device=device, dtype=torch.float)\n    last_model_preds = torch.zeros(all_data.shape, device=device, dtype=torch.float)\n    ys = torch.tensor(all_data, device=device, dtype=torch.float)\n    # Iterate on the models\n    for i, models in enumerate(build_models(all_data, n_folds=n_folds)):\n        # Calculate current fold\n        fold = i%n_folds\n        if fold==0: # If it's the first one we should affect each example to a fold\n            folds = np.random.randint(0, n_folds, len(all_data))\n        # We split train and test data\n        data = (all_data[folds!=fold], all_data[folds==fold])\n        # Perform the training using the validation loss method\n        val_pred, best_val_kldb = train_val(models[0], data)\n        best_val_kldb_buffer.append(best_val_kldb)\n        # Perform the training using the Benford metrics\n        kldb_pred, mlh_pred, last_model_pred = train_benford(models[1], data)\n        # We save the predictions on the test set associated to the current fold\n        val_preds[folds==fold] = val_pred\n        kldb_preds[folds==fold] = kldb_pred\n        mlh_preds[folds==fold] = mlh_pred\n        last_model_preds[folds==fold] = last_model_pred\n        if fold == n_folds - 1: # When we did every fold\n            # Save things\n            best_val_kldb = sum(best_val_kldb_buffer) / len(best_val_kldb_buffer)\n            best_val_kldb_buffer = []\n            val_loss = criterion(val_preds, ys).item()\n            val_preds[:] = 0.0\n            mlh_loss = criterion(mlh_preds, ys).item()\n            mlh_preds[:] = 0.0\n            kldb_loss = criterion(kldb_preds, ys).item()\n            kldb_preds[:] = 0.0\n            last_model_loss = criterion(last_model_preds, ys).item()\n            last_model_preds[:] = 0.0\n            \n            val_hist.append(val_loss)\n            best_val_kldb_hist.append(best_val_kldb)\n            mlh_hist.append(mlh_loss)\n            kldb_hist.append(kldb_loss)\n            last_model_hist.append(last_model_loss)\n            print(\"Results: Val loss={:.4f} | MLH loss={:.4f} | KLDB loss={:.4f} | Best val loss KLDB={:.4f}\".format(\n                val_loss, mlh_loss, kldb_loss, best_val_kldb\n            ))\n    return val_hist, mlh_hist, kldb_hist, last_model_hist, best_val_kldb_hist, ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mnist_results = experiment_on_dataset(\"../input/digit-recognizer/train.csv\")\nplt.title(\"Score obtained on MNIST for different model selection metrics\")\nplt.plot(mnist_results[0], label=\"Validation loss\")\nplt.plot(mnist_results[1], label=\"MLH\")\nplt.plot(mnist_results[2], label=\"KLDB\")\nplt.plot(mnist_results[3], label=\"Last model\")\nplt.xlabel(\"Number of hidden layers\")\nplt.ylabel(\"Test loss\")\nplt.legend()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"doverfit_results = experiment_on_dataset(\"../input/older-dataset-for-dont-overfit-ii-challenge/train.csv\")\nplt.title(\"Score obtained on \\\"Don't overfit\\\" for different model selection metrics\")\nplt.plot(doverfit_results[0], label=\"Validation loss\")\nplt.plot(doverfit_results[1], label=\"MLH\")\nplt.plot(doverfit_results[2], label=\"KLDB\")\nplt.plot(doverfit_results[3], label=\"Last model\")\nplt.xlabel(\"Number of hidden layers\")\nplt.ylabel(\"Test loss\")\nplt.legend()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tabular_playground = experiment_on_dataset(\"../input/tabular-playground-series-mar-2021/train.csv\")\nplt.title(\"Score obtained on \\\"Tabular playground series march 2021\\\" for different model selection metrics\")\nplt.plot(tabular_playground[0], label=\"Validation loss\")\nplt.plot(tabular_playground[1], label=\"MLH\")\nplt.plot(tabular_playground[2], label=\"KLDB\")\nplt.plot(tabular_playground[3], label=\"Last model\")\nplt.xlabel(\"Number of hidden layers\")\nplt.ylabel(\"Test loss\")\nplt.legend()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see, it's not clear that selecting a model based on \\\\( MLH \\\\) or \\\\( KLDB \\\\) prevents overfitting. It's even unclear that using thoses metrics is better than taking a random model. Surely those metrics evolve in a certain way during training but it's probably something else.","metadata":{}},{"cell_type":"code","source":"plt.title(\"KLDB of the model selected by the validation loss method\")\nplt.plot(mnist_results[4], label=\"MNIST\")\nplt.plot(doverfit_results[4], label=\"Don't overfit\")\nplt.plot(tabular_playground[4], label=\"Tabular playground series march 2021\")\nplt.xlabel(\"Number of hidden layers\")\nplt.ylabel(\"KLDB\")\nplt.legend()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Also, it's clear that there is no absolute value of \\\\( MLH \\\\) or \\\\( KLDB \\\\) that defines a fitted model. \n\n**TODO**\n* Write an algorithm that scraps output files of public notebooks to see if they rank in the leaderboard is related to \\\\( MLH \\\\) or \\\\( KLDB \\\\)\n* Try to implement the results of [Opening the black box of Deep Neural Networks\nvia Information](https://arxiv.org/pdf/1703.00810.pdf) to see if \\\\( MLH \\\\) or \\\\( KLDB \\\\) have anything to do with the two phases that are discussed in the paper\n* Use it on other tasks than autoencoding (classification, generative process, reinforcement learning)\n* Use more Datasets for evidence","metadata":{}},{"cell_type":"markdown","source":"It's very likely that in this notebook there are some imprecision, spelling mistakes or things that are not correct. If you spot one of them, feel free to let me know in the comment section, I'll be happy to correct it !","metadata":{}},{"cell_type":"code","source":"raise Exception(\"for some reason if the notebook don't raise an error, then the latex is broken.\")","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]}]}