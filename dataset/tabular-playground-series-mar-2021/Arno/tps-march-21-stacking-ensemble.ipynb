{"cells":[{"metadata":{},"cell_type":"markdown","source":"# TPS March 21 - Stacking Ensemble"},{"metadata":{},"cell_type":"markdown","source":"Hello everyone. Let me share with you my approach to March 21 competition. It's a simple ensemble of four models (XGBoost, LightGBM, CatBoost and RidgeClassifier), whose individual predictions were then trained on a meta-classifier.\n\nAll individual models hyperparameters were obtained using Optuna.\n\nHuge thanks to all the participants who published awesome notebooks during the competition; these helped me to learn a lot about different topics such as stratified k-fold, and obviously stacking.\n\nSpecial thanks to Craig Thomas for his notebook which I've been greatly inspired by:\nhttps://www.kaggle.com/craigmthomas/tps-mar-2021-stacked-starter"},{"metadata":{},"cell_type":"markdown","source":"## Imports"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/tabular-playground-series-mar-2021/train.csv')\ntest = pd.read_csv('/kaggle/input/tabular-playground-series-mar-2021/test.csv')\nsub = pd.read_csv('/kaggle/input/tabular-playground-series-mar-2021/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train.drop('id', axis=1)\ntest = test.drop('id', axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"numerical = train.select_dtypes(exclude='object').columns\ncategorical = train.select_dtypes(include='object').columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Smooth Target Encoding"},{"metadata":{},"cell_type":"markdown","source":"From the different tests I've made, I've found that smooth target encoding works best with LightGBM. So I'll transform categorical data using this method, and then feed that into my LightGBM model.\n\nIf you want to learn more about smooth target encoding, I recommend this great link which explains clearly how it works:\nhttps://maxhalford.github.io/blog/target-encoding/"},{"metadata":{"trusted":true},"cell_type":"code","source":"def SmoothTarget(train, test, features, weight):\n    \n    mean_target = train['target'].mean()\n    \n    for col in features:\n        agg = train.groupby(col)['target'].agg(['count', 'mean'])\n        count = agg['count']\n        mean = agg['mean']\n        \n        smooth = (count*mean + weight*mean_target) / (count+weight)\n        \n        train[col] = train[col].map(smooth)\n        test[col] = test[col].map(smooth)\n    \n    return train, test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_se, test_se = train.copy(), test.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_se, test_se = SmoothTarget(train_se, test_se, categorical, 10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Leave-One-Out Encoding"},{"metadata":{},"cell_type":"markdown","source":"Again, after many tests, categorical data transformed using LeaveOneOut encoding gave me better results with XGBoost, so I'll use this method for my XGBoost model."},{"metadata":{"trusted":true},"cell_type":"code","source":"from category_encoders import LeaveOneOutEncoder","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_loo, test_loo = train.copy(), test.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in categorical:\n    loo = LeaveOneOutEncoder()\n    loo.fit(train_loo[col], train_loo['target'])\n    train_loo[col] = loo.transform(train_loo[col])\n    test_loo[col] = loo.transform(test_loo[col])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Creating Level 1 models"},{"metadata":{"trusted":true},"cell_type":"code","source":"target = train['target']\n\nfor dataframe in (train, train_se, train_loo):\n    dataframe = dataframe.drop('target', axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Cross Validation Function"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import auc, roc_auc_score, roc_curve, plot_roc_curve","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def KFoldROC(X, y, test_set, model, params, folds, eval_set_bool):\n\n    train_pred = np.zeros(len(train.index))\n    test_pred = np.zeros(len(test.index))\n    \n    roc_score = []\n    \n    \n    skf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=21)\n\n    for train_idx, test_idx in skf.split(X,y):\n        X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n        X_test, y_test = X.iloc[test_idx], y.iloc[test_idx]\n\n        clf = model(**params)\n        \n        if eval_set_bool == True:          \n            clf.fit(X_train, y_train, eval_set=[(X_test, y_test)], early_stopping_rounds=200, verbose=False)\n        else:\n            clf.fit(X_train, y_train)\n\n        train_pred[test_idx] = clf.predict_proba(X_test)[:, 1]\n        \n        test_pred += clf.predict_proba(test_set)[:, 1] / folds\n        \n        score = roc_auc_score(y_test, clf.predict_proba(X_test)[:, 1])\n        roc_score.append(score)\n    \n    overall_roc = roc_auc_score(target, train_pred)\n    \n    return clf, train_pred, test_pred, np.mean(roc_score), overall_roc","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I will be training a calibrated version of RidgeClassifier, which doesn't accept fit parameters such as eval_set. This is why I've added the eval_set_bool parameter to this cross-validation function. Depending on the model I'm training, it just allows me to specify whether the model accepts the eval_set parameter or not."},{"metadata":{},"cell_type":"markdown","source":"### XGB Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import XGBClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_params = {\n    'tree_method' : 'gpu_hist',\n    'eval_metric' : 'auc',\n    'verbosity' : 0,\n    'learning_rate': 0.011,\n     'n_estimators': 13278,\n     'max_depth': 21,\n     'reg_alpha': 7.369502726375538,\n     'gamma': 0.6911623139352171,\n     'reg_lambda': 4.4405272244246765,\n     'subsample': 0.8558774777122383,\n     'colsample_bytree': 0.17259675946606295,\n     'min_child_weight': 2.1918267231776003\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb, train_pred_xgb, test_pred_xgb, roc_xgb, overall_roc_xgb = KFoldROC(\n    train_loo, target, test_loo, XGBClassifier, xgb_params, 5, eval_set_bool=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(roc_xgb)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(overall_roc_xgb)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### LGBM Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"from lightgbm import LGBMClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lgb_params = {\n 'learning_rate' : 0.03,\n 'metric' : 'auc',\n 'n_estimators': 8511,\n 'num_leaves': 205,\n 'max_depth': 10,\n 'reg_alpha': 8.337753037902587,\n 'reg_lambda': 2.778797190184823,\n 'subsample': 0.593175849495612,\n 'colsample_bytree' : 0.4228037476166183,\n 'min_child_samples': 1592}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"light, train_pred_light, test_pred_light, roc_light, overall_roc_light = KFoldROC(\n    train_se, target, test_se, LGBMClassifier, lgb_params, 5, eval_set_bool=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(roc_light)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(overall_roc_light)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Calibrated Ridge Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.linear_model import RidgeClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ridge_params = {\n    'base_estimator' : RidgeClassifier(),\n    'cv' : 5   \n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ridge, train_pred_ridge, test_pred_ridge, roc_ridge, overall_roc_ridge = KFoldROC(\n    train_loo, target, test_loo, CalibratedClassifierCV, ridge_params, 5, eval_set_bool=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(roc_ridge)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(overall_roc_ridge)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### CatBoost Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"from catboost import CatBoostClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_params = {\n    'cat_features' : categorical,\n    'task_type' : 'GPU',\n    'grow_policy' : 'Depthwise',\n    'loss_function' : 'Logloss',\n    'eval_metric' : 'AUC',\n    'metric_period' : 500,\n    'learning_rate': 0.01,\n    'max_depth': 15,\n    'l2_leaf_reg': 2.998072993047546,\n    'num_boost_round': 5535,\n    'min_data_in_leaf': 296,\n    'bagging_temperature': 1.8002809995267188,\n    'penalties_coefficient': 3.2585922042596422\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cb, train_pred_cb, test_pred_cb, roc_cb, overall_roc_cb = KFoldROC(\n    train, target, test, CatBoostClassifier, cat_params, 5, eval_set_bool=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(roc_cb)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(overall_roc_cb)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Ensemble Predictions"},{"metadata":{},"cell_type":"markdown","source":"### Getting predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_predictions = pd.DataFrame(\n    [train_pred_xgb, train_pred_light, train_pred_ridge, train_pred_cb, target]).transpose()\n\ntrain_predictions.columns = ['XGB', 'LightGBM', 'Ridge', 'CatBoost', 'target']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_predictions = pd.DataFrame(\n    [test_pred_xgb, test_pred_light, test_pred_ridge, test_pred_cb]).transpose()\n\ntest_predictions.columns = ['XGB', 'LightGBM', 'Ridge', 'CatBoost']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since the code above takes a while to run, I save the train and test predictions to csv files, so that I can work with them in a new notebook specifically dedicated to ensembling methods (stacking of course, but also averaging or weighted averaging)."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_predictions.to_csv('train_predictions.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_predictions.to_csv('test_predictions.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Averaging"},{"metadata":{"trusted":true},"cell_type":"code","source":"average_pred = (train_pred_xgb + train_pred_light + train_pred_ridge + train_pred_cb) / 4","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(roc_auc_score(target, average_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"average_pred_test = (test_pred_xgb + test_pred_light + test_pred_ridge + test_pred_cb) / 4","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Weighted Average"},{"metadata":{},"cell_type":"markdown","source":"Weights were obtained using scipy.optimize on a different notebook. From the many tests I've made, the RidgeClassifier model only decreased the overall score, so I didn't include it in this prediction."},{"metadata":{"trusted":true},"cell_type":"code","source":"x = 0.33930655\ny = 0.34311931\nz = 0.31757414","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"w_avg_pred = train_pred_xgb*x + train_pred_light*y + train_pred_cb*z","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(roc_auc_score(target, w_avg_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"w_avg_pred_test = test_pred_xgb*x + test_pred_light*y + test_pred_cb*z","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Level 2 Classifier"},{"metadata":{},"cell_type":"markdown","source":"Finally, let's create a meta-classifier into which we'll feed our previous' models predictions."},{"metadata":{"trusted":true},"cell_type":"code","source":"X_2 = train_predictions.drop('target', axis=1)\ny_2 = train_predictions['target']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def KFoldROC_L2(X, y, model, folds):\n\n    train_pred = np.zeros(len(train.index))\n    test_pred = np.zeros(len(test.index))\n    \n    roc_score = []\n    \n    \n    skf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=21)\n\n    for train_idx, test_idx in skf.split(X,y):\n        X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n        X_test, y_test = X.iloc[test_idx], y.iloc[test_idx]\n\n        clf = CalibratedClassifierCV(model, cv=5)\n        clf.fit(X_train, y_train)\n\n        train_pred[test_idx] = clf.predict_proba(X_test)[:, 1]\n        \n        test_pred += clf.predict_proba(test_predictions)[:, 1] / folds\n        \n        score = roc_auc_score(y_test, clf.predict_proba(X_test)[:, 1])\n        roc_score.append(score)\n    \n    overall_roc = roc_auc_score(target, train_pred)\n    \n    return clf, train_pred, test_pred, np.mean(roc_score), overall_roc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"L2_clf, train_pred_L2, test_pred_L2, roc_L2, overall_roc_L2 = KFoldROC_L2(X_2, y_2, RidgeClassifier(), 10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(roc_L2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(overall_roc_L2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Submissions"},{"metadata":{},"cell_type":"markdown","source":"Now let's produce submissions for our Level 2 Model, Average and Weighted Average models."},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_id = sub['id']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_avg = pd.DataFrame(data=[sub_id, average_pred_test]).transpose()\nsub_avg.columns = ['id', 'target']\nsub_avg['id'] = sub['id'].astype('int64')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_wavg = pd.DataFrame(data=[sub_id, w_avg_pred_test]).transpose()\nsub_wavg.columns = ['id', 'target']\nsub_wavg['id'] = sub['id'].astype('int64')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_l2 = pd.DataFrame(data=[sub_id, test_pred_L2]).transpose()\nsub_l2.columns = ['id', 'target']\nsub_l2['id'] = sub['id'].astype('int64')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_avg.to_csv('Submission Average.csv', index=False)\nsub_wavg.to_csv('Submission Weighted Average.csv', index=False)\nsub_l2.to_csv('Submission Level 2.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}