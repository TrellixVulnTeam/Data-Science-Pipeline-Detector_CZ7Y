{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Training of a DNN Tabular model with categorical embeddings using FastAI"},{"metadata":{},"cell_type":"markdown","source":"FastAI provides a huge number of convenient functions on top of PyTorch for Deep Learning tasks. \n\nWithin this notebook, I'll quickly demonstrate a simple process that can be used to perform binary classification with a Deep Learning Tabular model that uses categorical embeddings and standardised numerical features as inputs."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import fastai\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport pandas as pd\nimport seaborn as sns\n\nfrom fastai.tabular.all import *","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"## 1. Load our data"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_dir = \"/kaggle/input/tabular-playground-series-mar-2021/\"\ntrain_df = pd.read_csv(os.path.join(data_dir, \"train.csv\"))\ntest_df = pd.read_csv(os.path.join(data_dir, \"test.csv\"))\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We've got a varied mix of categorical and numerical features.\n\nLets preprocess our data into a suitable form for training. We'll encode categorical variables, standardise numerical features, and fill missing values (if there are any) within the dataset. We can do this extremely easily using the TabulerPandas class, like so:"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['target'].value_counts().plot.bar()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['target'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our problem is a slightly imbalanced classification problem."},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"## 2. Data preprocessing and creation of dataloader"},{"metadata":{},"cell_type":"markdown","source":"Lets preprocess our data into a suitable form for training. We'll encode categorical variables, standardise numerical features, and fill missing values (if there are any) within the dataset. We can do this extremely easily using the TabulerPandas class, like so:"},{"metadata":{"trusted":true},"cell_type":"code","source":"processing_funcs = [Categorify, FillMissing, Normalize]\ncat_cols = [x for x in train_df.columns.values if x.startswith('cat')]\nnum_cols = [x for x in train_df.columns.values if x.startswith('cont')]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nn_df = TabularPandas(train_df, cat_names=cat_cols, cont_names=num_cols, procs=processing_funcs, y_names='target', y_block = CategoryBlock())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A key thing with tabular classification problems is to pass in y_block = CategoryBlock() above, since this will inform our model to perform classification rather than regression."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dl = nn_df.dataloaders(1024)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# preview some of our data from the dataloader\ntrain_dl.show_batch()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Its so easy you almost feel like you've cheated somehow!\n\nI must admit, this is something that put me off using FastAI initially, however after the pain and effort of doing all of this manually many times with Keras, Tensorflow and PyTorch imeplementations, the ease of this method is highly appreciated for quick experimentation and research.\n\nWe could also have performed exactly the same as above, but straight from TabularDataLoaders, like so:"},{"metadata":{"trusted":true},"cell_type":"code","source":"dls = TabularDataLoaders.from_df(train_df, path='.', y_names=\"target\",  \n                                 cat_names = cat_cols, \n                                 cont_names = num_cols, \n                                 procs=processing_funcs, \n                                 y_block = CategoryBlock())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"## 3. Production of our DNN model"},{"metadata":{},"cell_type":"markdown","source":"We're performing basic binary classification for this challenge, so we only need to inform our model that its output bounds lie between 0 and 1. This will create a sigmoid output layer, from which we can classify our targets as either 0 and 1 depending on the chosen threshold."},{"metadata":{"trusted":true},"cell_type":"code","source":"tab_learn = tabular_learner(dls, layers=[500, 250], metrics=[accuracy, error_rate, Recall(), Precision()])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can get a quick preview of our model before training:"},{"metadata":{"trusted":true},"cell_type":"code","source":"tab_learn.model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Its also helpful to find an appropriate learning rate for our model prior to training:"},{"metadata":{"trusted":true},"cell_type":"code","source":"tab_learn.lr_find()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tab_learn.fit_one_cycle(2, lr_max=5e-3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tab_learn.recorder.plot_loss()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"interpret = ClassificationInterpretation.from_learner(tab_learn)\ninterpret.plot_confusion_matrix()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"interpret.print_classification_report()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"## 4. Test set predictions"},{"metadata":{},"cell_type":"markdown","source":"Preprocess our test set and make predictions using our trained model:"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_dl = tab_learn.dls.test_dl(test_df)\ntest_dl.show_batch()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds, test_labels = tab_learn.get_preds(dl=test_dl)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We need to take the argmax of these resultant predictions in order to obtain the final hard class output labels. We'll do this, and then make a submission to the competition:"},{"metadata":{"trusted":true},"cell_type":"code","source":"final_preds = preds.numpy()\nfinal_preds = np.argmax(final_preds, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_df = pd.read_csv(os.path.join(data_dir, \"sample_submission.csv\"))\nsubmission_df['target'] = final_preds\nsubmission_df.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Overall, its amazing how easy this process is, especially when compared to doing all of the low-level features yourself."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}