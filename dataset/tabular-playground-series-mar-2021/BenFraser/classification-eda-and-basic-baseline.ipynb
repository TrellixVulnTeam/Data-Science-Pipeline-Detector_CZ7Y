{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Tabular Data Classification and Baseline with EDA"},{"metadata":{},"cell_type":"markdown","source":"**Table of Contents:**\n\n1. [Load Data and Inspect Top Level Features](#load)\n2. [Exploratory Data Analysis (EDA)](#eda)\n3. [Data Preparation and Preprocessing](#data-preprocessing)\n4. [Model Training and Evaluation](#model-training)\n    - 4.1. [Basic Analysis using Random Forest](#random-forest)\n    - 4.2 [CatBoost Classification model](#catboost)\n5. [Test set predictions](#test-predictions)"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install pydotplus","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import gc\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport pandas as pd\nimport seaborn as sns\n\nfrom catboost import CatBoostClassifier, cv, Pool\nfrom collections import defaultdict\n\nfrom imblearn.over_sampling import SMOTE\nfrom IPython.display import Image\nfrom pydotplus import graph_from_dot_data\n        \nfrom sklearn.decomposition import PCA\nfrom sklearn.tree import DecisionTreeClassifier, export_graphviz\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, BaggingClassifier\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\nfrom sklearn.model_selection import train_test_split, KFold, cross_val_score, cross_val_predict\nfrom sklearn.model_selection import cross_validate, StratifiedKFold\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, roc_curve, \\\n                            recall_score, confusion_matrix, classification_report, \\\n                            auc, precision_recall_curve\nfrom sklearn.linear_model import LinearRegression, Lasso, Ridge\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression, Perceptron, RidgeClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.svm import SVC","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"load\"></a>\n## 1. Load Data and inspect top level features"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_dir = \"/kaggle/input/tabular-playground-series-mar-2021/\"\ntrain_df = pd.read_csv(os.path.join(data_dir, \"train.csv\"))\ntest_df = pd.read_csv(os.path.join(data_dir, \"test.csv\"))\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.isna().sum().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Good, we have absolutely no null or missing values at all!"},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"eda\"></a>\n## 2. Exploratory Data Analysis (EDA)"},{"metadata":{},"cell_type":"markdown","source":"Since we've got a range of both categorical and numerical features we should briefly explore these for insights."},{"metadata":{},"cell_type":"markdown","source":"### 2.1 Analysis of Categorical Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"def custom_countplot(data_df, col_name, ax=None, annotate=True):\n    \"\"\" Plot seaborn countplot for selected dataframe col \"\"\"\n    c_plot = sns.countplot(x=col_name, data=data_df, ax=ax)\n    if annotate:\n        for g in c_plot.patches:\n            c_plot.annotate(f\"{g.get_height()}\",\n                            (g.get_x()+g.get_width()/3,\n                             g.get_height()+60))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can plot all of our categorical columns using this basic helper function:"},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_cols = [x for x in train_df.columns.values if x.startswith('cat')]\nn = len(cat_cols)\n\nprint(f\"Number of categorical columns: {n}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n = len(cat_cols)\n\nfig, axs = plt.subplots(5, 4, figsize=(18,10))\naxs = axs.flatten()\n\n# iterate through each col and plot\nfor i, col_name in enumerate(cat_cols):\n    custom_countplot(train_df, col_name, ax=axs[i], annotate=False)\n    axs[i].set_xlabel(f\"{col_name}\", weight = 'bold')\n    axs[i].set_ylabel('Count', weight='bold')\n    \n    # only apply y label to left-most plots\n    if (i not in [0, 4, 8, 12, 16]):\n        axs[i].set_ylabel('')\n        \nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets also look at our target output:"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['target'].value_counts().plot.bar()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['target'].value_counts(normalize=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So we've got a slight imbalance of data for our outputs. We could consider correcting this through the use of various imbalanced data techniques."},{"metadata":{"trusted":true},"cell_type":"code","source":"num_cols = [x for x in train_df.columns.values if x.startswith('cont')]\nnum_n = len(num_cols)\n\nprint(f\"Number of numerical columns: {num_n}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def target_boxplot(y_val_col, x_val_col, data_df, figsize=(9,6), ax=None, name=\"Boxplot\"):\n    \"\"\" Custom boxplot function - plot a chosen value against target x col \"\"\"\n    \n    if not ax:\n        fig, ax = plt.subplots(figsize=figsize)\n    b_plot = sns.boxplot(x=x_val_col, y=y_val_col, data=data_df, ax=ax)\n    \n    medians = data_df.groupby(x_val_col)[y_val_col].median()\n    vert_offset = data_df[y_val_col].median() * 0.05 \n    \n    for xtick in b_plot.get_xticks():\n        b_plot.text(xtick, medians[xtick] + vert_offset, medians[xtick], \n                horizontalalignment='center',size='small',color='w',weight='semibold')\n    \n    if not ax:\n        plt.title(f\"{name}\", weight='bold')\n        plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_n = len(num_cols)\n\nfig, axs = plt.subplots(3, 4, figsize=(16,9))\naxs = axs.flatten()\n\n# iterate through each col and plot\nfor i, col_name in enumerate(num_cols):\n    \n    target_boxplot(col_name, 'target', train_df, \n               name=f'{col_name}', ax=axs[i])\n    \n    axs[i].set_xlabel(f\"{col_name}\", weight = 'bold')\n    axs[i].set_ylabel('Value', weight='bold')\n    \n    # only apply y label to left-most plots\n    if (i not in [0, 5, ]):\n        axs[i].set_ylabel('')\n        \nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(train_df.loc[:20000, num_cols], height=3, plot_kws={'alpha':0.2})\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# find the correlation between our variables\ncorr = train_df.loc[:, num_cols].corr()\n\nplt.figure(figsize=(12,8))\nsns.heatmap(corr, annot=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have a few fairly strong correlations between our variables, which might be worth exploring further and seeing if we can reduce any unnecessary redundancy. For this it will be worth experimenting with some dimensionality reduction and/or feature augmentation techniques.\n\nDespite this, we dont have a huge number of numerical features, and so dont want to throw any useful information about our dependent output variable away if we can avoid it. Therefore, we will keep all features as they are during preprocessing for this notebook."},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"data-preprocessing\"></a>\n## 3. Preprocessing"},{"metadata":{},"cell_type":"markdown","source":"We need to suitably encode our categorical variables, and standardise (if required) our numerical features. In addition, we can also add some additional features as part of feature engineering if we're feeling curious. \n\nFrom a simplistic categorical encoding perspective, we can either encode our categorical features with appropriate integer labels, or perform one-hot encoding. The latter method will produce a much larger dataset, and hence introduced more complexity, but with tabular models this is often at the benefit of improved performance. This is not a simple rule however, and results will vary from problem to problem - thus, it is worth exploring both approaches and seeing which works best for your application."},{"metadata":{"trusted":true},"cell_type":"code","source":"class DataProcessor(object):\n    def __init__(self):\n        self.encoder = None\n        self.standard_scaler = None\n        self.num_cols = None\n        self.cat_cols = None\n        \n    def preprocess(self, data_df, train=True, one_hot_encode=False,\n                   add_pca_feats=False):\n        \"\"\" Preprocess train / test as required \"\"\"\n        \n        # if training, fit our transformers\n        if train:\n            self.train_ids = data_df.loc[:, 'id']\n            train_cats = data_df.loc[:, data_df.dtypes == object]\n            self.cat_cols = train_cats.columns\n            \n            # if selected, one hot encode our cat features\n            if one_hot_encode:\n                self.encoder = OneHotEncoder(handle_unknown='ignore')\n                oh_enc = self.encoder.fit_transform(train_cats).toarray()\n                train_cats_enc = pd.DataFrame(oh_enc, columns=self.encoder.get_feature_names())\n                self.final_cat_cols = list(train_cats_enc.columns)\n            \n            # otherwise just encode our cat feats with ints\n            else:\n                # encode all of our categorical variables\n                self.encoder = defaultdict(LabelEncoder)\n                train_cats_enc = train_cats.apply(lambda x: \n                                                  self.encoder[x.name].fit_transform(x))\n                self.final_cat_cols = list(self.cat_cols)\n            \n            \n            # standardise all numerical columns\n            train_num = data_df.loc[:, data_df.dtypes != object].drop(columns=['target', 'id'])\n            self.num_cols = train_num.columns\n            self.standard_scaler = StandardScaler()\n            train_num_std = self.standard_scaler.fit_transform(train_num)\n            \n            # add pca reduced num feats if selected, else just combine num + cat feats\n            if add_pca_feats:\n                pca_feats = self._return_num_pca(train_num_std)\n                self.final_num_feats = list(self.num_cols)+list(self.pca_cols)\n                \n                \n                X = pd.DataFrame(np.hstack((train_cats_enc, train_num_std, pca_feats)), \n                        columns=list(self.final_cat_cols)+list(self.num_cols)+list(self.pca_cols))\n            else:   \n                self.final_num_feats = list(self.num_cols)\n                X = pd.DataFrame(np.hstack((train_cats_enc, train_num_std)), \n                        columns=list(self.final_cat_cols)+list(self.num_cols))\n        \n        # otherwise, treat as test data\n        else:\n            # transform categorical and numerical data\n            self.test_ids = data_df.loc[:, 'id']\n            cat_data = data_df.loc[:, self.cat_cols]\n        \n            if one_hot_encode:\n                oh_enc = self.encoder.transform(cat_data).toarray()\n                cats_enc = pd.DataFrame(oh_enc, columns=self.encoder.get_feature_names())\n            else:\n                cats_enc = cat_data.apply(lambda x: self.encoder[x.name].transform(x))\n                \n            # transform test numerical data\n            num_data = data_df.loc[:, self.num_cols]\n            num_std = self.standard_scaler.transform(num_data)\n            \n            if add_pca_feats:\n                pca_feats = self._return_num_pca(num_std, train=False)\n                \n                X = pd.DataFrame(np.hstack((cats_enc, num_std, pca_feats)), \n                        columns=list(self.final_cat_cols)+list(self.num_cols)+list(self.pca_cols))\n            \n            else:\n                X = pd.DataFrame(np.hstack((cats_enc, num_std)), \n                        columns=list(self.final_cat_cols)+list(self.num_cols)) \n        return X\n    \n    def _return_num_pca(self, num_df, n_components=0.85, train=True):\n        \"\"\" return dim reduced numerical features using PCA \"\"\"\n        if train:\n            self.pca = PCA(n_components=n_components)\n            num_rd = self.pca.fit_transform(num_df)\n            \n            # create new col names for our reduced features\n            self.pca_cols = [f\"pca_{x}\" for x in range(num_rd.shape[1])]\n            \n        else:\n            num_rd = self.pca.transform(num_df)\n        \n        return pd.DataFrame(num_rd, columns=self.pca_cols)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets transform our data into a form suitable for training various models. This includes encoding our categorical variables, and standardising our numerical variables.\n\nWe can either encode our categorical feature values, or one-hot encode them. Our preprocessing function supports whichever we want, through simply setting the one_hot_encode argument as true (one-hot encoding) or false (simple numerical encoding). We obtain a larger number of feature columns if we one-hot encode, and therefore introduce more complexity. However, many models perform better with one-hot encoding, so it is worth trying both techniques for our range of models.\n\nWe'll be using mainly tree-based methods in this notebook, and as such one-hot encoding and simple encoding of features does not actually make any noticeable difference (as demonstrated through years of empirical research and comparisons). Therefore, we'll keep our dataset simpler and just use categorical encoding."},{"metadata":{"trusted":true},"cell_type":"code","source":"data_proc = DataProcessor()\n\n# advanced preprocessing- include pca feats + one hot encoding\nX_train_full = data_proc.preprocess(train_df, one_hot_encode=True, add_pca_feats=False)\ny_train_full = train_df.loc[:, 'target']\n\nX_test = data_proc.preprocess(test_df, train=False, one_hot_encode=True, add_pca_feats=False)\n\nprint(f\"X_train_full: {X_train_full.shape} \\\\ny_train_full: {y_train_full.shape}, \\nX_test: {X_test.shape}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets obtain a further split containing a validation and training split for model training, optimising and evaluation purposes:"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full, \n                                                  test_size=0.2, random_state=12, stratify=y_train_full)\nprint(f\"X_train_full: {X_train.shape} \\ny_train_full: {y_train.shape} \\nX_val: {X_val.shape}, \\ny_val: {y_val.shape}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"## 4. Exploring our dataset using different models"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"random-forest\"></a>\n### 4.1 Random Forest Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"def show_tree_graph(tree_model, feature_names):\n    \"\"\" Output a decision tree to notebook \"\"\"\n    draw_data = export_graphviz(tree_model, filled=True, \n                                rounded=True, feature_names=feature_names, \n                                out_file=None, rotate=True, class_names=True)\n    graph = graph_from_dot_data(draw_data)\n\n    return Image(graph.create_png())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_clf = RandomForestClassifier(n_estimators=100, max_depth=3)\nrf_clf.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"show_tree_graph(rf_clf.estimators_[0], list(X_train.columns))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets train our random forest again, but this time not limiting the depth of our trees:"},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_clf = RandomForestClassifier(n_estimators=100, n_jobs=-1)\n%time rf_clf.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The great thing with random forests is the ease of being able to see the relative importance of our features used for making predictions:"},{"metadata":{"trusted":true},"cell_type":"code","source":"def feature_importances(rf_model, dataframe):\n    \"\"\" Return dataframe of feat importances from random forest model \"\"\"\n    return pd.DataFrame({'columns' : dataframe.columns, \n                         'importance' : rf_model.feature_importances_}\n                       ).sort_values('importance', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"importances = feature_importances(rf_clf, X_train)\nTOP_N = 45\n\nplt.figure(figsize=(14,6))\nsns.barplot(x=\"columns\", y=\"importance\", data=importances[:TOP_N])\nplt.ylabel(\"Feature Importances\", weight='bold')\nplt.xlabel(\"Features\", weight='bold')\nplt.title(\"Random Forest Feature Importances\", weight='bold')\nplt.xticks(rotation=90)\nplt.show()\nprint(importances[:TOP_N])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets now make some predictions on our validation set to get a rough idea of the performance of our model:"},{"metadata":{"trusted":true},"cell_type":"code","source":"val_preds = rf_clf.predict(X_val)\nval_acc = accuracy_score(val_preds, y_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"Random Forest accuracy on validation set: {val_acc}\\n\")\nprint(classification_report(val_preds, y_val))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"These metrics are hard to appreciate from the values alone, however they do highlight a severe limitation of our model. Lets plot a confusion matrix, which will help illustrate what this is."},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_confusion_matrix(true_y, pred_y, title='Confusion Matrix', figsize=(8,6)):\n    \"\"\" Custom function for plotting a confusion matrix for predicted results \"\"\"\n    conf_matrix = confusion_matrix(true_y, pred_y)\n    conf_df = pd.DataFrame(conf_matrix, columns=np.unique(true_y), index = np.unique(true_y))\n    conf_df.index.name = 'Actual'\n    conf_df.columns.name = 'Predicted'\n    plt.figure(figsize = figsize)\n    plt.title(title)\n    sns.set(font_scale=1.4)\n    sns.heatmap(conf_df, cmap=\"Blues\", annot=True, \n                annot_kws={\"size\": 16}, fmt='g')\n    plt.show()\n    return","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_confusion_matrix(y_val, val_preds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_roc_curve(y_train, y_train_probs, y_val, y_val_probs, figsize=(8,8)):\n    \"\"\" Helper function to plot the ROC AUC from given labels \"\"\"\n    # obtain true positive and false positive rates for roc_auc\n    fpr, tpr, thresholds = roc_curve(y_train, y_train_probs[:, 1], pos_label=1)\n    roc_auc = auc(fpr, tpr)\n\n    # obtain true positive and false positive rates for roc_auc\n    val_fpr, val_tpr, val_thresholds = roc_curve(y_val, y_val_probs[:, 1], pos_label=1)\n    val_roc_auc = auc(val_fpr, val_tpr)\n\n    plt.figure(figsize=figsize)\n    plt.plot(fpr, tpr, label=f\"Train ROC AUC = {roc_auc}\", color='blue')\n    plt.plot(val_fpr, val_tpr, label=f\"Val ROC AUC = {val_roc_auc}\", color='red')\n    plt.plot([0,1], [0, 1], label=\"Random Guessing\", \n             linestyle=\":\", color='grey', alpha=0.6)\n    plt.plot([0, 0, 1], [0, 1, 1], label=\"Perfect Performance\", \n             linestyle=\"--\", color='black', alpha=0.6)\n    plt.xlabel(\"False Positive Rate\")\n    plt.ylabel(\"True Positive Rate\")\n    plt.title(\"Receiver Operating Characteristic\", weight='bold')\n    plt.legend(loc='best')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# obtain prediction probabilities for trg and val\ny_val_probs = rf_clf.predict_proba(X_val)\ny_trg_probs = rf_clf.predict_proba(X_train)\n\n# plot our ROC curve\nplot_roc_curve(y_train, y_trg_probs, y_val, y_val_probs)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The AUC is the metric we're trying to maximise for this competition, and therefore we should seek to obtain a model that scores particularly well in this regard.\n\nJust for interest, we can also inspect the precision-recall curve for our models:"},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_prec_rec_curve(y_train, y_train_probs, y_val, y_val_probs, figsize=(14,6)):\n    \"\"\" Helper function to plot the ROC AUC from given labels \"\"\"\n    # obtain true positive and false positive rates for roc_auc\n    prec, rec, thresholds = precision_recall_curve(y_train, \n                                                   y_train_probs[:, 1], \n                                                   pos_label=1)\n    prec_rec_auc = auc(rec, prec)\n\n    # obtain true positive and false positive rates for roc_auc\n    val_prec, val_rec, val_thresholds = precision_recall_curve(y_val, \n                                                               y_val_probs[:, 1], \n                                                               pos_label=1)\n    val_prec_rec_auc = auc(val_rec, val_prec)\n\n    plt.figure(figsize=figsize)\n    plt.plot(prec, rec, \n             label=f\"Train Precision-Recall AUC = {prec_rec_auc}\", color='blue')\n    plt.plot(val_prec, val_rec, \n             label=f\"Val Precision-Recall AUC = {val_prec_rec_auc}\", color='red')\n    plt.plot([0, 0, 1], [0, 1, 1], label=\"Perfect Performance\", \n             linestyle=\"--\", color='black', alpha=0.6)\n    plt.xlabel(\"Recall\")\n    plt.ylabel(\"Precision\")\n    plt.title(\"Precision-Recall Curve\", weight='bold')\n    plt.legend(loc='best')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot our precision recall curve\nplot_prec_rec_curve(y_train, y_trg_probs, y_val, y_val_probs)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"catboost\"></a>\n### 4.2 CatBoost Classifier"},{"metadata":{},"cell_type":"markdown","source":"Since training can take a long time on CPU, we want to ensure we select GPU as the task type for our catboost model:"},{"metadata":{"trusted":true},"cell_type":"code","source":"cb_learn_rate = 0.006\nn_iterations = 15000\nearly_stop_rounds = 400\n\ncb_params = {'iterations' : n_iterations, 'learning_rate' : cb_learn_rate, \n             'task_type' : 'GPU', 'random_seed' : 13, 'verbose' : 500}\n\n#cb_params = {'iterations' : n_iterations, 'learning_rate' : cb_learn_rate, \n#             'random_seed' : 13, 'verbose' : 500}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cb_clf = CatBoostClassifier(**cb_params)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A nice feature of CatBoost is the option of adding an interactive plot of training, which allows us to analyse the performance in real time:"},{"metadata":{"trusted":true},"cell_type":"code","source":"cb_clf.fit(X_train, y_train, eval_set=(X_val, y_val), \n           use_best_model=True, plot=True, \n           early_stopping_rounds=early_stop_rounds)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets make some predictions on the validation set and compare to our previous random forest model:"},{"metadata":{"trusted":true},"cell_type":"code","source":"val_preds = cb_clf.predict(X_val)\nval_acc = accuracy_score(val_preds, y_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"CatBoost accuracy on validation set: {val_acc}\\n\")\nprint(classification_report(val_preds, y_val))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_confusion_matrix(y_val, val_preds)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finally, lets look at the ROC AUC for the CatBoost model:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# obtain prediction probabilities for trg and val\ny_val_probs = cb_clf.predict_proba(X_val)\ny_trg_probs = cb_clf.predict_proba(X_train)\n\n# plot our ROC curve\nplot_roc_curve(y_train, y_trg_probs, y_val, y_val_probs)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"test-predictions\"></a>\n## 5. Test set predictions"},{"metadata":{},"cell_type":"markdown","source":"With some basic models under our belt, lets make some predictions on the test set and submit these:"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_preds = cb_clf.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_df = pd.read_csv(os.path.join(data_dir, \"sample_submission.csv\"))\nsubmission_df['target'] = test_preds\nsubmission_df.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}