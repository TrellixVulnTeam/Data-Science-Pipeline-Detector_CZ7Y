{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\nsns.set_style(\"darkgrid\")\n%matplotlib inline\n\n\nfrom sklearn.cluster import KMeans\nfrom category_encoders import LeaveOneOutEncoder\nfrom sklearn.preprocessing import LabelEncoder\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.metrics import roc_auc_score\n\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\n\nimport optuna\n\nimport eli5\nfrom eli5.sklearn import PermutationImportance","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load data"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_full = pd.read_csv('/kaggle/input/tabular-playground-series-mar-2021/train.csv', index_col='id')\nX_test = pd.read_csv('/kaggle/input/tabular-playground-series-mar-2021/test.csv', index_col='id')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Specify target"},{"metadata":{"trusted":true},"cell_type":"code","source":"target = \"target\"\nX_full.dropna(axis=0, subset=[target], inplace=True)\ny_full = X_full.pop(target)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Categorical features"},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_features = [col for col in X_full.columns if X_full[col].dtype == \"object\"]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Numerical features"},{"metadata":{"trusted":true},"cell_type":"code","source":"num_features = [col for col in X_full.columns if X_full[col].dtype in [\"int\", \"float\"]]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Make some EDA plots"},{"metadata":{"trusted":true},"cell_type":"code","source":"nc = 4\nnr = int(len(num_features)/nc+1)\n\nfig, axes = plt.subplots(nrows=nr, ncols=nc, figsize=(18,4*nr))\n\nfor count, feature in enumerate(num_features):\n    ks_score = stats.ks_2samp(X_full[feature], X_test[feature])[0]\n    i, j = count//nc, count%nc\n    sns.kdeplot(X_full[feature], color='Blue', ax=axes[i, j])\n    sns.kdeplot(X_test[feature], color='Red', ax=axes[i, j])\n\n    axes[i, j].legend([\"Train\", \"Test\"], facecolor=\"White\")\n    axes[i, j].set_title(f\"{feature} ks stat : {np.round(ks_score,3)}\")\n\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As can be seen, the distribution is quite even between train and test datasets for the numerical features. "},{"metadata":{},"cell_type":"markdown","source":"# Encode categorical features"},{"metadata":{"trusted":true},"cell_type":"code","source":"def loo_encode(X_full, X_test, column):\n    loo = LeaveOneOutEncoder()\n    new_feature = f\"{column}_loo\"\n    loo.fit(X_full[column], y_full)\n    X_full[new_feature] = loo.transform(X_full[column])\n    X_test[new_feature] = loo.transform(X_test[column])\n    return new_feature\n\nloo_features = []\nfor feature in cat_features:\n    loo_features.append(loo_encode(X_full, X_test, feature))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nc = 4\nnr = int(len(loo_features)/nc+1)\n\nfig, axes = plt.subplots(nrows=nr, ncols=nc, figsize=(18,4*nr))\n\nfor count, feature in enumerate(loo_features):\n    ks_score = stats.ks_2samp(X_full[feature], X_test[feature])[0]\n    i, j = count//nc, count%nc\n    sns.kdeplot(X_full[feature], color='Blue', ax=axes[i, j])\n    sns.kdeplot(X_test[feature], color='Red', ax=axes[i, j])\n\n    axes[i, j].legend([\"Train\", \"Test\"], facecolor=\"White\")\n    axes[i, j].set_title(f\"{feature} ks stat : {np.round(ks_score,3)}\")\n\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Also, the distribution is quite even between train and test datasets for the leave-one-out encoded categorical features."},{"metadata":{},"cell_type":"markdown","source":"# Label encode 'cat16'"},{"metadata":{"trusted":true},"cell_type":"code","source":"def label_encode(X_full, X_test, column):\n    le = LabelEncoder()\n    new_feature = f\"{column}_le\"\n    le.fit(X_full[column])\n    le.fit(X_full[column].unique().tolist() + X_test[column].unique().tolist())\n    X_full[new_feature] = le.transform(X_full[column])\n    X_test[new_feature] = le.transform(X_test[column])\n    return new_feature\n\nle_list = ['cat16']\nle_features = []\nfor feature in le_list:\n    le_features.append(label_encode(X_full, X_test, feature))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# K-cluster 'cat16_loo'"},{"metadata":{"trusted":true},"cell_type":"code","source":"clusters = [ \n    (\"cat16_loo\", 2)\n]\n\nkmeans_features = []\nfor var in clusters:\n    kmeans = KMeans(n_clusters=var[1])\n    X_full[f\"{var[0]}_kmeans\"] = kmeans.fit_predict( np.array(X_full[var[0]]).reshape(-1, 1) )\n    X_test[f\"{var[0]}_kmeans\"] = kmeans.predict( np.array(X_test[var[0]]).reshape(-1, 1) )\n    kmeans_features.append(f\"{var[0]}_kmeans\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Specify all features to use"},{"metadata":{"trusted":true},"cell_type":"code","source":"my_features = num_features + loo_features + le_features + kmeans_features","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Optuna hyperparameter optimization"},{"metadata":{"trusted":true},"cell_type":"code","source":"def objective(trial, data=X_full[my_features], target=y_full):\n    seed = 2021\n    split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=seed)\n\n    for train_index, valid_index in split.split(X_full[my_features], y_full):\n        X_train = X_full[my_features].iloc[train_index]\n        y_train = y_full.iloc[train_index]\n        X_valid = X_full[my_features].iloc[valid_index]\n        y_valid = y_full.iloc[valid_index]\n\n\n    lgbm_params = {\n        'reg_alpha': trial.suggest_float('reg_alpha', 0.001, 10.0),\n        'reg_lambda': trial.suggest_float('reg_lambda', 0.001, 10.0),\n        'num_leaves': trial.suggest_int('num_leaves', 11, 333),\n        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n        'max_depth': trial.suggest_int('max_depth', 5, 30),\n        'learning_rate': trial.suggest_categorical('learning_rate', [0.005, 0.01, 0.02, 0.05, 0.1]),\n        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.1, 0.5),\n        'n_estimators': trial.suggest_int('n_estimators', 100, 5000),\n        'random_state': seed,\n        'boosting_type': 'gbdt',\n        'metric': 'AUC',\n        #'device': 'gpu'\n    }\n    \n\n    model = LGBMClassifier(**lgbm_params)  \n    \n    model.fit(\n            X_train,\n            y_train,\n            early_stopping_rounds=100,\n            eval_set=[(X_valid, y_valid)],\n            verbose=False\n        )\n\n    y_valid_pred = model.predict_proba(X_valid)[:,1]\n    \n    roc_auc = roc_auc_score(y_valid, y_valid_pred)\n    \n    return roc_auc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#study = optuna.create_study(direction = 'maximize')\n#study.optimize(objective, n_trials = 10)\n#print('Number of finished trials:', len(study.trials))\n#print('Best trial:', study.best_trial.params)\n#print('Best value:', study.best_value)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Optuna visualization"},{"metadata":{"trusted":true},"cell_type":"code","source":"#optuna.visualization.plot_optimization_history(study)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#optuna.visualization.plot_param_importances(study)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Fit model with Optuna best parameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"seed = 2021\n#paramsLGBM = study.best_trial.params\nparamsLGBM = {'reg_alpha': 1.9553269755200153, \n              'reg_lambda': 6.667487742284949, \n              'num_leaves': 173, \n              'min_child_samples': 86, \n              'max_depth': 23, \n              'learning_rate': 0.01, \n              'colsample_bytree': 0.15433885172555964, \n              'n_estimators': 3473}\nparamsLGBM['boosting_type'] = 'gbdt'\nparamsLGBM['metric'] = 'AUC'\nparamsLGBM['random_state'] = seed\n\n\nsplit = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=seed)\nfor train_index, valid_index in split.split(X_full[my_features], y_full):\n    X_train = X_full[my_features].iloc[train_index]\n    y_train = y_full.iloc[train_index]\n    X_valid = X_full[my_features].iloc[valid_index]\n    y_valid = y_full.iloc[valid_index]\n\n\nlgbm_clf = LGBMClassifier(**paramsLGBM)\nlgbm_clf.fit(X_train[my_features], y_train, \n             early_stopping_rounds=100, \n             eval_set=[(X_valid, y_valid)], \n             verbose=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#perm = PermutationImportance(lgbm_clf, random_state=seed).fit(X_valid, y_valid)\n#eli5.show_weights(perm, feature_names = X_valid.columns.tolist())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Make predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_preds = lgbm_clf.predict_proba(X_test[my_features])[:,1]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Save predictions to file"},{"metadata":{"trusted":true},"cell_type":"code","source":"output = pd.DataFrame({'Id': X_test.index,\n                       target: test_preds})\noutput.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Acknowledgement"},{"metadata":{},"cell_type":"markdown","source":"This notebook is a combination of different ideas I have learnt from:\n* https://www.kaggle.com/craigmthomas/tps-mar-2021-stacked-starter/comments\n* https://www.kaggle.com/dmitryuarov/catboost-vs-xgb-vs-lgbm-tps-mar-21\n* And many other Kagglers"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}