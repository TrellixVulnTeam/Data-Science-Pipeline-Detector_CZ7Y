{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Complete Code, it helped me to get score 0.8897 on leaderboard"},{"metadata":{},"cell_type":"markdown","source":"# **Data Reading**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from tqdm.autonotebook import tqdm\nimport numpy as np # linear algebra\nimport pandas as pd","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/tabular-playground-series-mar-2021/train.csv')\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv('/kaggle/input/tabular-playground-series-mar-2021/test.csv')\ncat_test = pd.read_csv('/kaggle/input/tabular-playground-series-mar-2021/test.csv')\nsub = pd.read_csv('/kaggle/input/tabular-playground-series-mar-2021/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data encoding"},{"metadata":{"trusted":true},"cell_type":"code","source":"cols = [i for i in train.columns if train[i].dtypes=='object']\ncols","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in cols:\n    print(train[i].unique())\n    \n# we will lable them manualy like a =1 , b =2 so ab = 26+(1*2) = 28,  why we add so that they are above 26 as z ==26  so like that we will do it for all, yes it is to be done manually","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels = {}\nimport string\ntest_list = list(string.ascii_uppercase) \nprint(test_list)\nfor i,j in zip(range(1,27),test_list):\n    labels[j] = i\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for p,q in zip(range(1,27),test_list):\n    for i,j in zip(range(1,27),test_list):\n        labels[q+j] = 26+(p*i)\n\n# we are realy with out lables , lets change our value","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in tqdm(cols):\n    for p,q in labels.items():\n        train[i] = train[i].replace(p,q)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in tqdm(cols):\n    for p,q in labels.items():\n        test[i] = test[i].replace(p,q)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train.drop(columns='id')\ntest = test.drop(columns='id')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.groupby('target').nunique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Balancing"},{"metadata":{"trusted":true},"cell_type":"code","source":"from imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.over_sampling import RandomOverSampler\n\nX = train.drop(columns='target')\nY = train['target']\n\n# special case\nover = RandomOverSampler(sampling_strategy=0.5)\nunder = RandomOverSampler(sampling_strategy=0.8)\n\nx1, y1 = over.fit_resample(X,Y)\nc_x, c_y = under.fit_resample(x1, y1)\nprint(len(c_y))\n# # we will create four models ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Hypermeter Optimization"},{"metadata":{},"cell_type":"markdown","source":"Special thanks to [https://www.kaggle.com/haichaoshang/tps-mar2021-lgbm-optuna](http://)"},{"metadata":{"trusted":true},"cell_type":"code","source":"import optuna.integration.lightgbm as lgb\nimport optuna\nfrom sklearn.model_selection import train_test_split\nimport lightgbm\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom catboost import CatBoostClassifier\nfrom xgboost import XGBClassifier\nimport xgboost as xgb","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Here i have run it only for one trial, but actually for better score, i run it for 30 trials, you can tune it**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def objective(trial, data = X, target = Y):\n    X_train, X_val, y_train, y_val = train_test_split(X, Y,stratify=Y, test_size = 0.2, random_state = 0)\n\n    params = {\n        'max_depth': trial.suggest_int('max_depth', 3, 10),\n        'learning_rate': trial.suggest_categorical('learning_rate', [0.005, 0.02, 0.05, 0.08, 0.1]),\n        'n_estimators': trial.suggest_int('n_estimators', 50, 3000),\n        'max_bin': trial.suggest_int('max_bin', 200, 400),\n        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 1, 300),\n        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 0.0001, 1.0, log = True),\n        'subsample': trial.suggest_float('subsample', 0.1, 0.8),\n        'random_seed': 42,\n        'task_type': 'GPU',\n        'loss_function': 'Logloss',\n        'eval_metric': 'AUC',\n        'bootstrap_type': 'Poisson'\n    }\n    \n    model = CatBoostClassifier(**params)  \n    model.fit(X_train, y_train, eval_set = [(X_val,y_val)], early_stopping_rounds = 222, verbose = False)\n    y_pred = model.predict_proba(X_val)[:,1]\n    roc_auc = roc_auc_score(y_val, y_pred)\n\n    return roc_auc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"study = optuna.create_study(direction = 'maximize')\nstudy.optimize(objective, n_trials = 1)\nprint('Number of finished trials:', len(study.trials))\nprint('Best trial:', study.best_trial.params)\nprint('Best value:', study.best_value)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def objective(trial):\n    X_train, X_val, y_train, y_val = train_test_split(X, Y,stratify=Y, test_size = 0.2, random_state = 0)\n    params = {\n        \"verbosity\": 0,\n        \"objective\": \"binary:logistic\",\n        'max_depth': trial.suggest_int('max_depth', 3, 10),\n        \"eval_metric\": \"auc\",\n        \"eta\" : trial.suggest_float(\"eta\", 1e-8, 1.0, log=True),\n        \"booster\": trial.suggest_categorical(\"booster\", [\"dart\"]),\n        \"lambda\": trial.suggest_float(\"lambda\", 1e-8, 1.0, log=True),\n        \"alpha\": trial.suggest_float(\"alpha\", 1e-8, 1.0, log=True),\n        'tree_method' : 'gpu_hist',\n        'eval_metric': 'auc',\n        'random_seed': 42,\n    }\n\n    model = XGBClassifier(**params) \n    model.fit(X_train, y_train, eval_set = [(X_val,y_val)], early_stopping_rounds = 222, verbose = False)\n    y_pred = model.predict_proba(X_val)[:,1]\n    roc_auc = roc_auc_score(y_val, y_pred)\n    return roc_auc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"study = optuna.create_study(direction = 'maximize')\nstudy.optimize(objective, n_trials = 1)\nprint('Number of finished trials:', len(study.trials))\nprint('Best trial:', study.best_trial.params)\nprint('Best value:', study.best_value)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# optuna.visualization.plot_param_importances(study)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nfrom catboost import CatBoostClassifier\ndef scaling(X_train, X_test, trial):\n    sc = StandardScaler()\n    X_train = sc.fit_transform(X_train)\n    X_test = sc.transform(X_test)\n    test_data = sc.transform(trial)\n    return X_train, X_test, test_data\n\n\ndef lgbm_model(X_train, X_test, Y_train,Y_test,test_data):\n    \n    params =  {'reg_alpha': 9.883045220708368,\n         'reg_lambda': 1.8406970907815618e-05, \n         'num_leaves': 105, 'learning_rate': 0.0363323342120501,\n         'max_depth': 9, 'n_estimators': 7645, 'min_child_samples': 30, \n         'min_child_weight': 0.6126344551951962, 'subsample': 0.305413058674825,\n         'colsample_bytree': 0.9557961982690266, 'random_state': 2021,\n        'metric' : 'auc',\n        'device_type' : 'gpu'}\n    \n    model = lightgbm.LGBMClassifier(**params)\n    model.fit(X_train , Y_train , eval_set = [(X_test , Y_test)] , early_stopping_rounds = 200 , \\\n             verbose = False)\n    preds = model.predict_proba(X_test,num_iteration=model.best_iteration_)[:,1]\n    test_preds = model.predict_proba(test_data,num_iteration=model.best_iteration_)[:,1]\n    return preds,test_preds\n\n\n\n\ndef xgb_model(X_train, X_test, Y_train,Y_test,test_data):\n    \n    params =  {'max_depth': 8, 'eta': 0.1917070083625873, 'booster': 'dart', \n               'lambda': 0.003158888201683581, 'alpha': 0.16427957283436528,\n            'tree_method' : 'gpu_hist',\n        'eval_metric': 'auc',\n        'random_seed': 42 }\n    \n    model = XGBClassifier(**params)  \n    model.fit(X_train, Y_train, eval_set = [(X_test,Y_test)], early_stopping_rounds = 222, verbose = False)\n    preds = model.predict_proba(X_test)[:,1]\n    test_preds = model.predict_proba(test_data)[:,1]\n    return preds,test_preds","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Again runing for only 2 splits**"},{"metadata":{"trusted":true},"cell_type":"code","source":"score = []\nfinal = []\nkfold = StratifiedKFold(n_splits=2, random_state=24, shuffle=True)\n\nfor Train, Test in kfold.split(X,Y):\n    X_train, X_test = X.iloc[Train], X.iloc[Test]\n    Y_train, Y_test = Y.iloc[Train], Y.iloc[Test]\n#     X_train, X_test, test_data = scaling(X_train, X_test,test)\n    #====== MODEL =======================\n    params = {'max_depth': 6, 'learning_rate': 0.05, 'n_estimators': 2997, \n              'max_bin': 328, 'min_data_in_leaf': 28,\n              'l2_leaf_reg': 0.003018532125919831, \n              'subsample': 0.6347767905576908,\n             'random_seed': 42,\n        'task_type': 'GPU',\n             'eval_metric': 'AUC',\n             'bootstrap_type': 'Poisson'}\n    \n    model = CatBoostClassifier(**params)  \n    model.fit(X_train, Y_train, eval_set = [(X_test,Y_test)], early_stopping_rounds = 222, verbose = False)\n    preds = model.predict_proba(X_test)[:,1]\n    test_preds = model.predict_proba(test)[:,1]\n    \n    #====================================\n    roc = roc_auc_score(Y_test, preds)\n    print(roc)\n    score.append(roc)\n    final.append(test_preds)\naverage_score = np.mean(score)\nprint('The average roc is ', average_score)\n    \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score = []\nu_final = []\nkfold = StratifiedKFold(n_splits=2, random_state=24, shuffle=True)\n\nfor Train, Test in kfold.split(X , Y):\n    X_train, X_test = X.iloc[Train], X.iloc[Test]\n    Y_train, Y_test = Y.iloc[Train], Y.iloc[Test]\n#     X_train, X_test, test_data = scaling(X_train, X_test,test)\n    #====== MODEL =======================\n    preds,test_preds = lgbm_model(X_train, X_test, Y_train, Y_test,test)\n    #====================================\n    roc = roc_auc_score(Y_test, preds)\n    print(roc)\n    score.append(roc)\n    u_final.append(test_preds)\naverage_score = np.mean(score)\nprint('The average roc is ', average_score)\n    \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score = []\no_final = []\nkfold = StratifiedKFold(n_splits=2, random_state=24, shuffle=True)\n\nfor Train, Test in kfold.split(X , Y):\n    X_train, X_test = X.iloc[Train], X.iloc[Test]\n    Y_train, Y_test = Y.iloc[Train], Y.iloc[Test]\n#     X_train, X_test, test_data = scaling(X_train, X_test,test)\n    #====== MODEL =======================\n    preds,test_preds = xgb_model(X_train, X_test, Y_train, Y_test,test)\n    #====================================\n    roc = roc_auc_score(Y_test, preds)\n    print(roc)\n    score.append(roc)\n    o_final.append(test_preds)\naverage_score = np.mean(score)\nprint('The average roc is ', average_score)\n    \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"r1= np.mean(final,0)  # cat\nr2 = np.mean(o_final,0) #xgb\nr3 = np.mean(u_final,0)  # lgbm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"r_9 = r1*0.2+r2*0.2+r3*0.6\nr_9","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"r_8 = r1*0.3+r2*0.1+r3*0.6\nr_8","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"r = r_9*0.4+r_8*0.6\nr","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub['target'] = r","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.to_csv('sub.csv',index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}