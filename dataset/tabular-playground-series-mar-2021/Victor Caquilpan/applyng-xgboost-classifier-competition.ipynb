{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Application of XGBoost\n\nWe will perform a XGBoost model to generate an accuracy prediction to submit in the [Tabular Playground Series - Mar 2021 Competition](https://www.kaggle.com/c/tabular-playground-series-mar-2021). XGBoost is one of the most useful models in Kaggle and we go to probe in a competition. "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Import packages\nimport numpy as np # Handling matrices\nimport pandas as pd # Data processing\nimport matplotlib.pyplot as plt # Plotting\nimport seaborn as sns # Plotting \nfrom sklearn.preprocessing import OneHotEncoder, MinMaxScaler, OrdinalEncoder # Handling categorical data and normalization\nfrom sklearn.model_selection import train_test_split, cross_val_score # Split data in train and test and CV\nfrom sklearn.metrics import roc_auc_score,precision_score,confusion_matrix, accuracy_score, roc_curve, f1_score # Several useful metrics\nfrom xgboost import XGBClassifier # XGB model\nfrom sklearn.pipeline import Pipeline # Connect processes\nfrom sklearn.compose import ColumnTransformer # Capable apply transformer to columns\n\n# Set matplotlib configuration\n%matplotlib inline\nplt.style.use('seaborn')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1) Review and analysis of data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import data\ndata = pd.read_csv('../input/tabular-playground-series-mar-2021/train.csv')\nprint(\"This dataset contains: {} rows and {} columns\".format(data.shape[0],data.shape[1]))\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Review the type of each feature\ndata.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Count the type of features\ndata.dtypes.value_counts()\nprint('This dataset contains {} categorical features'.format(data.dtypes.value_counts()[0]))\nprint('This dataset contains {} numerical features'.format(data.dtypes.value_counts()[1]))\n\n# Id and target are the unique integer features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Analyse missing values\ndata.isna().sum()\n\n# Do not have missing values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Identify categorical features\ncat = (data.dtypes == 'object')\ncat_cols = list(cat[cat].index)\nprint(cat_cols)\n\n# Create a handful of plots\nfor cols in cat_cols:\n    plt.figure(figsize=(8,4));\n    sns.countplot(x = data[cols]);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a list of numerical_cols\nnumerical_cols = [cname for cname in data.columns if data[cname].dtype in ['float64']]\n\n# Also, we can see how numerical features are related with the target\ndata[numerical_cols].hist(bins=15, figsize=(20, 14), layout=(7, 3));","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that our categorical and numeric features have different behaviours. We have categorical features with low and high number of classes, while our numerical feature are different distributions. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Analyse our target colum\ndata['target'].hist(bins=15, figsize=(12,6));\n\n# We observe that our data is unbalanced. This is an important point.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2) Create a model\n\nUsing the recommendation given by the tutorial of [Intermediate ML](https://www.kaggle.com/alexisbcook/categorical-variables) of Kaggle, we apply different methods to categorical features, which have less than 12 unique values. For these features we will aply One-Hot Encoding, while features with 12 or more unique values, we will apply Ordinal Encoding. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Separate independent features of target\ny = data['target']\nX = data.drop(['id','target'],axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Divide data into training and validation subsets. We stratify data by output classes.\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, \n                                                                test_size=0.2,random_state = 123,stratify = y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Print proportion of entire dataset\nprint(\"Proportion of classes in entire data: \")\nprint(100. * y.value_counts() / len(y),\"\\n\")\n\n# Print proportion of train and test sets \nprint(\"Proportion of classes in train data: \")\nprint(100. * y_train.value_counts() / len(y_train),\"\\n\")\nprint(\"Proportion of classes in valid data: \")\nprint(100. * y_valid.value_counts() / len(y_valid))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Identify categorical columns with relatively low cardinality (low number of unique values)\ncategorical_cols_O = [cname for cname in X_train.columns if X_train[cname].nunique() < 12 and \n                    X_train[cname].dtype == \"object\"]\n\n# Identify categorical columns with high cardinality\ncategorical_cols_L = [cname for cname in X_train.columns if X_train[cname].nunique() >= 12 and \n                    X_train[cname].dtype == \"object\"]\n\n# Identify numerical columns\nnumerical_cols = [cname for cname in X_train.columns if X_train[cname].dtype in ['int64', 'float64']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Preprocessing\n\n# To categorical columns with low cardinality\ncategorical_O_transformer = OneHotEncoder(handle_unknown = 'ignore')\n\n# To categorical columns with high cardinality\ncategorical_L_transformer = OrdinalEncoder(handle_unknown = 'use_encoded_value',\n                                          unknown_value = -99)\n\n# To numerical columns\nnumerical_transformer = MinMaxScaler()\n\n# Bundle preprocessing for numerical and categorical data\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('cat_O', categorical_O_transformer, categorical_cols_O),\n        ('cat_L', categorical_L_transformer, categorical_cols_L),\n        ('num', numerical_transformer, numerical_cols)\n    ])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creation of a model\nmodel = XGBClassifier(n_estimators=1000, learning_rate=0.05, n_jobs=4,use_label_encoder = False,\n                     objective = \"binary:logistic\",eval_metric = \"auc\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Bundle preprocessing and modeling code in a pipeline\npipeline = Pipeline(steps=[('preprocessor', preprocessor),\n                              ('model', model)\n                             ])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Preprocessing of training data, fit model \npipeline.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Preprocessing of validation data, get predictions\ny_pred = pipeline.predict_proba(X_valid)\n\n# Consider our output has two columns (one per each class)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Function to plot ROC curve\ndef plot_roc_curve(fpr, tpr):\n    plt.plot(fpr, tpr, color='orange', label='ROC')\n    plt.plot([0, 1], [0, 1], color='darkblue', linestyle='--')\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver Operating Characteristic (ROC) Curve')\n    plt.legend()\n    plt.show()\n\n# Create plot\nfpr, tpr, thresholds = roc_curve(y_valid, y_pred[:, [1]])\nplot_roc_curve(fpr, tpr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create Confusion Matrix\npred_class = y_pred[:, [1]] > 0.5\npred_class = pred_class.astype(int)\ncm = confusion_matrix(y_valid, pred_class)\nprint(\"Confusion matrix: \\n\",cm,\"\\n\")\n\n# Get accuracy\naccuracy = round(accuracy_score(y_valid,pred_class),4)\nprint(\"Accuracy: {}\".format(accuracy),\"\\n\")\n\n# Get f1 score (it is required on the Task 1 of this dataset)\nf1 = f1_score(y_valid,pred_class)\nprint(\"F1: {}\".format(f1),\"\\n\")\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3) Use model in test set"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load test data\ntest = pd.read_csv(\"../input/tabular-playground-series-mar-2021/test.csv\")\ntest.head()\n\n# Remove id\nX_test = test.drop(\"id\",axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Prediction on the valid set\ntest_pred=pipeline.predict_proba(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4) Write results"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create submission file\noutput = test[['id']].copy()\npositive_class = test_pred[:,[1]]\n\noutput['target'] = pd.Series(positive_class.flatten(), index=output.index)\noutput.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# write csv\noutput.to_csv(\"submissionv_XGBoost.csv\",index = False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}