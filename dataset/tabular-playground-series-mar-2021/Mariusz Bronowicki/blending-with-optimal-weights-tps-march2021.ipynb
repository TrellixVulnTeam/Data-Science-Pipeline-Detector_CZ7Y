{"cells":[{"metadata":{},"cell_type":"markdown","source":"Hi Kagglers.\nIn this notebook I would like to improve my model performance with blending. It seems to me that in most competitions kagglers using average predictions or blending prediction with weights to get better results. I will blend results of 3 models with their defaults, use their predictions to find optimal weights and blend it."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv(\"/kaggle/input/tabular-playground-series-mar-2021/train.csv\")\ntest_df = pd.read_csv(\"/kaggle/input/tabular-playground-series-mar-2021/test.csv\")\nsample = pd.read_csv(\"/kaggle/input/tabular-playground-series-mar-2021/sample_submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_feats = [col for col in train_df.columns if col.startswith(\"cat\")]\nnum_feats = [col for col in train_df.columns if col.startswith(\"cont\")]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df['target'] = -1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_df = pd.concat([train_df, test_df])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dummies_df = pd.get_dummies(all_df[cat_feats], drop_first=True)\nnew_df = pd.concat([all_df['id'],dummies_df, all_df[num_feats], all_df['target']], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = new_df[new_df[\"target\"] != -1]\ntest = new_df[new_df[\"target\"] == -1]\ntest = test.drop(\"target\", axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.stats.outliers_influence import variance_inflation_factor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vif = pd.DataFrame()\nvif[\"variables\"] = num_feats\nvif[\"VIF\"] = [variance_inflation_factor(train[num_feats].values, i) for i in range(train[num_feats].shape[1])]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vif = vif.sort_values(by=[\"VIF\"], ascending=False)\nvif.style.background_gradient(cmap=\"magma\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train.sample(frac=1).reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = train['target'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"skf = StratifiedKFold(n_splits=5)\n\nfor fold, (train_idx, valid_idx) in enumerate(skf.split(train, y)):\n    train.loc[valid_idx, \"kfold\"] = fold","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def run_training(df, algo, fold, model_name, test):\n    df_train = df[df.kfold != fold].reset_index(drop=True)\n    df_valid = df[df.kfold == fold].reset_index(drop=True)\n    \n    xtrain = df_train.drop([\"id\", \"kfold\", \"target\"], axis=1).values\n    xvalid = df_valid.drop([\"id\", \"kfold\", \"target\"], axis=1).values\n    \n    ytrain = df_train.target.values\n    yvalid = df_valid.target.values\n    \n    model = algo\n    model.fit(xtrain, ytrain)\n    \n    preds = model.predict_proba(xvalid)[:, 1]\n    auc = roc_auc_score(yvalid, preds)\n    print(f\"fold={fold}, auc={auc}\")\n    \n    df_valid.loc[:, model_name] = preds\n    \n    sub_preds = model.predict_proba(test)[:, 1]\n    \n    return df_valid[[\"id\", \"kfold\", \"target\", model_name]], sub_preds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = RandomForestClassifier(n_estimators=200, verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df_sub = pd.DataFrame({\"id\": test[\"id\"].values})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dfs = []\ntest_temp = np.zeros(len(test))\n\nfor fold in range(5):\n    temp_df, test_preds = run_training(train, clf, fold, \"random_forest\",test.drop(\"id\", axis=1))\n    dfs.append(temp_df)\n    test_temp += test_preds\n    \ntest_df_sub[f\"forest_mean_fold\"] = test_temp / 5   \nfin_valid_df_rfc = pd.concat(dfs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fin_valid_df_rfc.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"roc_auc_score(fin_valid_df_rfc[\"target\"], fin_valid_df_rfc[\"random_forest\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr = LogisticRegression(max_iter=100000,verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dfs = []\ntest_temp = np.zeros(len(test))\n\nfor fold in range(5):\n    temp_df, test_preds = run_training(train, lr, fold, \"logisticRegression\",test.drop(\"id\", axis=1))\n    dfs.append(temp_df)\n    test_temp += test_preds\n    \ntest_df_sub[f\"lr_mean_fold\"] = test_temp / 5   \nfin_valid_df_lr = pd.concat(dfs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fin_valid_df_lr.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"roc_auc_score(fin_valid_df_lr[\"target\"], fin_valid_df_lr[\"logisticRegression\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import XGBClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb = XGBClassifier(use_label_encoder=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dfs = []\ntest_temp = np.zeros(len(test))\n\nfor fold in range(5):\n    temp_df, test_preds = run_training(train, xgb, fold, \"xgboost\",test.drop(\"id\", axis=1))\n    dfs.append(temp_df)\n    test_temp += test_preds\n    \ntest_df_sub[f\"xgb_mean_fold\"] = test_temp / 5   \nfin_valid_df_xgb = pd.concat(dfs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fin_valid_df_xgb.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"roc_auc_score(fin_valid_df_xgb[\"target\"], fin_valid_df_xgb[\"xgboost\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df_sub.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Blending - optimal weights"},{"metadata":{"trusted":true},"cell_type":"code","source":"from functools import partial\nfrom scipy.optimize import fmin","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class OptimizerAUC:\n    def __init__(self):\n        self.coef_ = 0\n        \n    def _auc(self, coef, X, y):\n        x_coef = X * coef\n        predictions = np.sum(x_coef, axis=1)\n        auc_score = roc_auc_score(y, predictions)\n        return -1.0 * auc_score\n    \n    def fit(self, X, y):\n        partial_loss = partial(self._auc, X=X, y=y)\n        init_coef = np.random.dirichlet(np.ones(X.shape[1]))\n        self.coef_ = fmin(partial_loss, init_coef, disp=True)\n    \n    def predict(self, X):\n        x_coef = X * self.coef_\n        predictions = np.sum(x_coef, axis=1)\n        return predictions      \n\n\ndef run_training2(pred_df, fold, col_names):\n\n    train_df = pred_df[pred_df.kfold !=fold].reset_index(drop=True)\n    valid_df = pred_df[pred_df.kfold == fold].reset_index(drop=True)\n    \n    xtrain = train_df[col_names].values\n    xvalid = valid_df[col_names].values\n    \n    ytrain = train_df.target.values\n    yvalid = valid_df.target.values\n    \n    opt = OptimizerAUC()\n    opt.fit(xtrain, ytrain)\n    preds = opt.predict(xvalid)\n    \n    auc = roc_auc_score(yvalid, preds)\n    print(f\"Fold={fold}, AUC={auc}\")\n    \n    return opt.coef_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = None\n\ndfs_list = [fin_valid_df_lr, fin_valid_df_rfc, fin_valid_df_xgb]\nfor i in range(len(dfs_list)):\n    if df is None:\n        df = dfs_list[i]\n    else:\n        df = df.merge(dfs_list[i], on=\"id\", how=\"left\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"targets = df.target.values\ncol_names = [\"logisticRegression\", \"random_forest\", \"xgboost\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"coefs = []\nfor j in range(5):\n    coefs.append(run_training2(df, j, col_names))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"coefs = np.array(coefs)\ncoefs_mean = np.mean(coefs, axis=0)\nprint(coefs_mean)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"col_names","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wt_avg = (\n    coefs_mean[0] * df.logisticRegression.values\n    + coefs_mean[1] * df.random_forest.values\n    + coefs_mean[2] * df.xgboost.values\n)\nprint(\"Optimal auc after finding coefs\")\nwt_auc = roc_auc_score(targets, wt_avg)\nprint(f\"Optimized weighted avg of auc: {wt_auc}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Submit blending predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df_sub","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wt_avg_blend = (\n    coefs_mean[0] * test_df_sub[\"lr_mean_fold\"].values\n    + coefs_mean[1] * test_df_sub[\"forest_mean_fold\"].values\n    + coefs_mean[2] + test_df_sub[\"xgb_mean_fold\"].values\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample['target'] = wt_avg_blend","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample.to_csv(\"blend_avg_weights_sub2.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Add more features"},{"metadata":{},"cell_type":"markdown","source":"Some suggest that adding predictions as new features can improve the model, let's test that."},{"metadata":{"trusted":true},"cell_type":"code","source":"col_names","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"col_names.append(\"id\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_train = train.merge(df[col_names], on=\"id\", how=\"left\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_test = test.merge(test_df_sub, on=\"id\", how=\"left\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb = XGBClassifier(use_label_encoder=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df_sub2 = pd.DataFrame()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dfs = []\ntest_temp = np.zeros(len(new_test))\n\nfor fold in range(5):\n    temp_df, test_preds = run_training(new_train, xgb, fold, \"xgboost\",new_test.drop(\"id\", axis=1))\n    dfs.append(temp_df)\n    test_temp += test_preds\n    \ntest_df_sub2[f\"xgb_mean_fold\"] = test_temp / 5   \nfin_valid_df_xgb = pd.concat(dfs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"roc_auc_score(fin_valid_df_xgb[\"target\"], fin_valid_df_xgb[\"xgboost\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df_sub2.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rfc = RandomForestClassifier(n_estimators=200, verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dfs = []\ntest_temp = np.zeros(len(new_test))\n\nfor fold in range(5):\n    temp_df, test_preds = run_training(new_train, rfc, fold, \"random_forest\",new_test.drop(\"id\", axis=1))\n    dfs.append(temp_df)\n    test_temp += test_preds\n    \ntest_df_sub2[\"rfc_mean_fold\"] = test_temp / 5   \nfin_valid_df_rfc = pd.concat(dfs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"roc_auc_score(fin_valid_df_rfc[\"target\"], fin_valid_df_rfc[\"random_forest\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df_sub2.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr = LogisticRegression(max_iter=100000,verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dfs = []\ntest_temp = np.zeros(len(test))\n\nfor fold in range(5):\n    temp_df, test_preds = run_training(new_train, lr, fold, \"logisticRegression\",new_test.drop(\"id\", axis=1))\n    dfs.append(temp_df)\n    test_temp += test_preds\n    \ntest_df_sub2[f\"lr_mean_fold\"] = test_temp / 5   \nfin_valid_df_lr = pd.concat(dfs)\n\nroc_auc_score(fin_valid_df_lr[\"target\"], fin_valid_df_lr[\"logisticRegression\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = None\n\ndfs_list = [fin_valid_df_lr, fin_valid_df_rfc, fin_valid_df_xgb]\nfor i in range(len(dfs_list)):\n    if df is None:\n        df = dfs_list[i]\n    else:\n        df = df.merge(dfs_list[i], on=\"id\", how=\"left\")\n        \ntargets = df.target.values\ncol_names = [\"logisticRegression\", \"random_forest\", \"xgboost\"]\n\ncoefs = []\nfor j in range(5):\n    coefs.append(run_training2(df, j, col_names))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"coefs = np.array(coefs)\ncoefs_mean = np.mean(coefs, axis=0)\nprint(coefs_mean)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wt_avg = (\n    coefs_mean[0] * df.logisticRegression.values\n    + coefs_mean[1] * df.random_forest.values\n    + coefs_mean[2] * df.xgboost.values\n)\nprint(\"Optimal auc after finding coefs\")\nwt_auc = roc_auc_score(targets, wt_avg)\nprint(f\"Optimized weighted avg of auc: {wt_auc}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df_sub2.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wt_avg_blend = (\n    coefs_mean[0] * test_df_sub[\"lr_mean_fold\"].values\n    + coefs_mean[1] * test_df_sub[\"forest_mean_fold\"].values\n    + coefs_mean[2] + test_df_sub[\"xgb_mean_fold\"].values\n)\n\nsample['target'] = wt_avg_blend\nsample.to_csv(\"blend_avg_weights_sub3.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Indeed, adding pedictions as new features can improve the model as we can see it in this notebook, however all those actions haven't improve my score in Kaggle competition LB. Can anyone explain that to me. Please leave feedback if you found it interesting."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}