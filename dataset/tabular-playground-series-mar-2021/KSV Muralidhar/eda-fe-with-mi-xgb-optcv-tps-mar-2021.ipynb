{"cells":[{"metadata":{},"cell_type":"markdown","source":"<h1>Tabular Playground Series - Mar 2021"},{"metadata":{},"cell_type":"markdown","source":"<section id = 'top'></section>\n<h2>\n    <ol>\n        <li><a href=#preda>Preliminary EDA</a></li>\n        <li><a href=#csep>Visualizing Class Separation</a></li>\n        <li><a href=#numdist>Distribution of Numeric Variables</a></li>\n        <li><a href=#numvstgt>Numeric Variables vs Target</a></li>\n        <li><a href=#catdist>Distribution of Categorical Variables</a></li>\n        <li><a href=#catrel>Categorical Variables vs Target</a></li>\n        <li><a href=#rare>Handling Rare Categories in Categorical Variables</a></li>\n        <li><a href=#tgtdist>Target Distribution</a></li>\n        <li><a href=#mutinfo>Mutual Information</a></li>\n        <li><a href=#base>Random Forest Baseline Model</a></li>\n        <li><a href=#nfc>New Feature Creation</a></li>\n        <li><a href=#build>Model Building</a></li>\n        <li><a href=#subm>Submission</a></li>\n    </ol></h2>\n        \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport scipy.stats as stats\nfrom sklearn.manifold import TSNE\nfrom scipy.stats import chi2_contingency \nfrom sklearn.preprocessing import OrdinalEncoder, MinMaxScaler, OneHotEncoder, MaxAbsScaler, LabelEncoder\nfrom sklearn.decomposition import PCA, TruncatedSVD\nfrom sklearn.feature_selection import mutual_info_classif, RFE\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\nfrom sklearn.model_selection import cross_validate, StratifiedKFold, train_test_split\nfrom imblearn import under_sampling\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.ensemble import EasyEnsembleClassifier, RUSBoostClassifier\nfrom sklearn.metrics import roc_auc_score\nimport xgboost as xgb\nfrom lightgbm import LGBMClassifier\nimport optuna\nfrom sklearn.pipeline import Pipeline\nfrom imblearn.pipeline import Pipeline as imbpipeline\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1><section id=\"preda\">1. Preliminary EDA&nbsp;&nbsp;<a href=#top>Top</a></section></h1>"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv('../input/tabular-playground-series-mar-2021/train.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>Dropping 'id' column"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.drop(columns='id',inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>Data type of all the variables looks good.</h3><br>\n<h2>Storing Categorical, Numeric and Target variables in separate variables for simpler EDA"},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical_variables = list(data.select_dtypes('object').columns)\n\nnumeric_variables = list(data.select_dtypes('float64').columns)\n\ntarget = list(data.select_dtypes('int64').columns)[0]\n\nprint(f'Categorical Variables ({len(categorical_variables)}):\\n{categorical_variables}\\n\\nNumeric Variables ({len(numeric_variables)}):\\n{numeric_variables}\\n\\nTarget:\\n{target}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe().T","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>No numeric variable has 0 as minimum, hence ruling out the presence of masked NaNs. No numeric varable has missing values</h3>"},{"metadata":{},"cell_type":"markdown","source":"<h2>Checking for zero/near-zero variance in relation with the median for a clearer picture"},{"metadata":{"trusted":true},"cell_type":"code","source":"data[numeric_variables].apply(lambda x: (x.std()/x.median())*100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>No numeric variable has zero/near-zero variance</h3><br>\n<h2>Checking for Missing Values"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.isnull().mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>No missing values</h3>"},{"metadata":{},"cell_type":"markdown","source":"<h1><section id='csep'>2. Visualizing Class Separation&nbsp;&nbsp;<a href=#top>Top</a></section></h1>"},{"metadata":{},"cell_type":"markdown","source":"<h2>We'll reduce the dimensionality of the data using PCA/SVD to get a rough idea on how the classes are separated.<br><br>First using ordinal encoding for categorical variables and using PCA"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = data.iloc[:,:-1].copy()\ny = data['target'].astype(np.int8).copy()\n\ncol_transform = ColumnTransformer(transformers=[['ordinal_encoder',OrdinalEncoder(),categorical_variables]],remainder='passthrough')\n\nX = col_transform.fit_transform(X).copy()\nmin_max = MinMaxScaler()\nX = min_max.fit_transform(X).copy()\n\npca = PCA(n_components=2,random_state=11)\nX_pca = pca.fit_transform(X).copy()\nX_pca = pd.DataFrame(X_pca,columns=[\"component_1\",\"component_2\"])\nX_pca['y'] = y\n\nnp.cumsum(pca.explained_variance_ratio_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(8,8))\nsns.scatterplot(data=X_pca,x='component_1',y=\"component_2\",hue='y',alpha=0.2,palette=['black','lightgray']);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>Though the explained variance of PCA is low, the plot above gives us a fair idea (not very much reliable) that the classes are fairly separated. Let's try SVD"},{"metadata":{},"cell_type":"markdown","source":"<h2>Let's one hot encode categorical variables and use Truncated SVD on the sparse data"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = data.iloc[:,:-1].copy()\ny = data['target'].astype(np.int8).copy()\n\ncol_transform = ColumnTransformer(transformers=[['ohe',OneHotEncoder(),categorical_variables]],remainder='passthrough')\n\nX = col_transform.fit_transform(X).copy()\nmax_abs = MaxAbsScaler()\nX = max_abs.fit_transform(X).copy()\n\ntsvd = TruncatedSVD(n_components=2,random_state=11)\nX_svd = tsvd.fit_transform(X).copy()\nX_svd = pd.DataFrame(X_svd,columns=[\"component_1\",\"component_2\"])\nX_svd['y'] = y\n\nnp.cumsum(tsvd.explained_variance_ratio_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(8,8))\nsns.scatterplot(data=X_svd,x='component_1',y=\"component_2\",hue='y',alpha=0.2,palette=['black','lightgray']);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>Both PCA and SVD show that the classes are fairly separated (though they are not reliable due to very low explained variance, they give us a fair idea). Trying it after feature engineering may result in better explained variance ratio."},{"metadata":{},"cell_type":"markdown","source":"<h1><section id='numdist'>3. Distribution of Numeric Variables&nbsp;&nbsp;<a href=#top>Top</a></section></h1>"},{"metadata":{"trusted":true},"cell_type":"code","source":"data[numeric_variables].plot(kind='kde',figsize=(15,20),subplots=True,layout=(6,2),color='black');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>Few variables have multiple peaks in their distribution and almost all of them look skewed. However, a statistical test can confirm these observations"},{"metadata":{},"cell_type":"markdown","source":"<h2>Normality Test"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"h0:Sample comes from a normal distribution\\nh1:Sample doesn't come from a normal distribution\\n\\n\")\n\nfor i in numeric_variables:\n    print(f\"{i}: {'Non-Gaussian' if (stats.normaltest(data[i])[1])<0.05 else 'Gaussian'}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2>Q-Q plots"},{"metadata":{"trusted":true},"cell_type":"code","source":"for n,i in enumerate(numeric_variables):\n    stats.probplot(data[i],plot=plt)\n    plt.title(i)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>No numeric variable has a Gaussian/Normal Distribution"},{"metadata":{},"cell_type":"markdown","source":"<h2>Box Plot for outlier detection"},{"metadata":{"trusted":true},"cell_type":"code","source":"data[numeric_variables].plot(kind='box',subplots=True,layout=(6,2),figsize=(15,20),color='black');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>Variables cont8, cont9, cont10 have outliers</h3><br>\n<h2>Skewness test"},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in numeric_variables:\n    print(f\"{i}: {'Skewed' if (stats.skewtest(data[i])[1])<0.05 else 'Not Skewed'}  {stats.skew(data[i])}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>cont7, cont8, cont9 & cont10 have slightly higher right-skewness. Other variables have slight skewness (cont5 is slightly left-skewed)"},{"metadata":{},"cell_type":"markdown","source":"<h2>Relationship among numeric variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(10,10))\nsns.heatmap(data[numeric_variables].corr(),mask=np.triu(data[numeric_variables].corr()),\n            annot=True,fmt='.2f',\n            cbar=False,cmap=['white'],linewidths=0.01,linecolor='black',square=True);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>Few numeric features are highly correlated with each other"},{"metadata":{},"cell_type":"markdown","source":"<h1><section id='numvstgt'>4. Numeric Variables vs Target&nbsp;&nbsp;<a href=#top>Top</a></section></h1>"},{"metadata":{"trusted":true},"cell_type":"code","source":"r = c = 0\nfig,ax = plt.subplots(6,2,figsize=(14,35))\nfor n,i in enumerate(numeric_variables):\n    med = data[[i,'target']].groupby('target').median().copy()\n    sns.violinplot(x=target,y=i,data=data,ax=ax[r,c],palette=[\"gray\",\"lightgray\"])\n    med.plot(ax=ax[r,c],color='black',linewidth=3,linestyle=\"--\",legend=False)\n    for x,y in zip(list(med.index),med[i]):\n        ax[r,c].text(x=x+0.05,y=y+0.01,s=np.round(y,2),fontsize=10,color='white',backgroundcolor='black')\n    ax[r,c].set_title(i.upper()+\" by \"+\"TARGET\")\n    c+=1\n    if (n+1)%2==0:\n        r+=1\n        c=0\nax[r,c].axis(\"off\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>Most of the continous variables have a similar distribution for both the classes of target variables. However, their median shows some sort of relationship with the target."},{"metadata":{},"cell_type":"markdown","source":"<h1><section id='catdist'>5. Distribution of Categorical Variables&nbsp;&nbsp;<a href=#top>Top</a></section></h1>"},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in categorical_variables:\n    val_cnt = pd.DataFrame(np.round(data[i].value_counts(normalize=True)*100,2))\n    val_cnt.columns = ['Proportion']\n    print(f'{i}: {data[i].nunique()} unique categories\\n{val_cnt}\\n')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2>Few categorical variables have high cardinality therefore, rare labels."},{"metadata":{},"cell_type":"markdown","source":"<h1><section id='catrel'>6. Categorical Variables vs Target&nbsp;&nbsp;<a href=#top>Top</a></section></h1>"},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in categorical_variables:\n    ct = pd.crosstab(columns=data[i],index=data[\"target\"])\n    stat, p, dof, expected = chi2_contingency(ct) \n    print(f\"\\n{'-'*len(f'Chi-Square test between {i} & Target')}\")\n    print(f'Chi-Square test between {i} & Target')\n    print(f\"{'-'*len(f'Chi-Square test between {i} & Target')}\")\n    print(f\"\\nH0: THERE IS NO RELATIONSHIP BETWEEN TARGET & {i.upper()}\\nH1: THERE IS RELATIONSHIP BETWEEN TARGET & {i.upper()}\")\n    print(f\"\\nP-VALUE: {np.round(p,2)}\")\n    print(\"REJECT H0\" if p<0.05 else \"FAILED TO REJECT H0\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"r = c = 0\nfig,ax = plt.subplots(7,2,figsize=(24,32))\nnew_cat = categorical_variables.copy()\nnew_cat.remove('cat3')\nnew_cat.remove('cat10')\nnew_cat.remove('cat5')\nnew_cat.remove('cat7')\nnew_cat.remove('cat8')\nfor n,i in enumerate(new_cat):\n    ct = pd.crosstab(columns=data[i],index=data['target'],normalize=\"columns\")\n    ct.T.plot(kind=\"bar\",stacked=True,color=[\"black\",\"gray\"],ax=ax[r,c])\n    ax[r,c].set_ylabel(\"% of observations\")\n    ax[r,c].set_xlabel(\"\")\n    ax[r,c].set_title(f'{i} vs Target')\n    c+=1\n    if (n+1)%2==0:\n        r+=1\n        c=0\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>Excluded few plots as they were cluttered."},{"metadata":{},"cell_type":"markdown","source":"<h1><section id='rare'>7. Handling Rare Categories in Categorical Variables&nbsp;&nbsp;<a href=#top>Top</a></section></h1>"},{"metadata":{},"cell_type":"markdown","source":"<h2>Identifying non-rare categories in category variables with different cut-offs for each variable and saving them in a csv."},{"metadata":{"trusted":true},"cell_type":"code","source":"non_rare = pd.DataFrame()\n\ncut_offs = [0.2,0.02,0.02,0.02,0.02,0.03,0.01,0.01,0.005,0.02,0.001,0.1,0.1,0.02,0.4,0.04,0.02,0.085,0.072]\n\nfor n,i in enumerate(categorical_variables):\n    var_dist = data[i].value_counts(normalize=True).copy()\n    non_rare = pd.concat([non_rare,pd.DataFrame({i:var_dist[var_dist>cut_offs[n]].index})],axis=1).copy()\n\nnon_rare.to_csv('./non_rare_categories.csv',index=False) #storing non-rare categories in a csv for further use\nnon_rare","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2>Plotting categorical variables before and after marking rare categories"},{"metadata":{"trusted":true},"cell_type":"code","source":"new_data = data.copy()\nfor i in non_rare.columns:\n    new_data.loc[(new_data[i].isin(non_rare[i]) == False), i] = \"Rare\"\n    \n    \n\nfor n,i in enumerate(non_rare.columns):\n    if i in ['cat5', 'cat7', 'cat8', 'cat10']:\n        continue\n    \n    cat_dist = data[i].value_counts().sort_values().copy()\n    cat_dist = np.round((cat_dist / cat_dist.sum()) * 100,1).copy()\n    fig,ax = plt.subplots(1,2,figsize=(15, len(cat_dist)/1.5))\n    cat_dist.plot(kind=\"barh\",ax=ax[0],sharey=False,title=i + \" Before Adding Rare Label\",color='black')\n    for n,j in enumerate(cat_dist.index):\n        ax[0].text(y=n,x=cat_dist[j]+0.1,s=str(cat_dist[j]) + \"%\")\n    \n    \n    new_cat_dist = new_data[i].value_counts().sort_values().copy()\n    new_cat_dist = np.round((new_cat_dist / new_cat_dist.sum()) * 100,1).copy()\n    new_cat_dist.plot(kind=\"barh\",ax=ax[1],sharey=False,title=i + \" After Adding Rare Label\",color='black')\n    for n,j in enumerate(new_cat_dist.index):\n        ax[1].text(y=n,x=new_cat_dist[j]+0.1,s=str(new_cat_dist[j]) + \"%\")\n    \n    \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>Plots of few variables with high cardinality as hidden"},{"metadata":{},"cell_type":"markdown","source":"<h3>Defining a function to mark rare labels"},{"metadata":{"trusted":true},"cell_type":"code","source":"def mark_rare_categories(df):\n    df = df.copy()\n    non_rare = pd.read_csv('./non_rare_categories.csv')\n    for i in non_rare.columns:\n        df.loc[(df[i].isin(non_rare[i]) == False), i] = \"Rare\"\n    return df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1><section id='tgtdist'>8. Target Distribution&nbsp;&nbsp;<a href=#top>Top</a></section></h1>"},{"metadata":{"trusted":true},"cell_type":"code","source":"data[target].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(data=data,x=target,palette=['black','gray']);\n\ntgt_cnt = data[target].value_counts()\n\ntgt_prop = np.round(data[target].value_counts(normalize=True)*100,1)\n\nplt.text(x=-0.2,y=tgt_cnt[0]+2500,s=f'{tgt_cnt[0]:,} ({tgt_prop[0]}%)')\n\nplt.text(x=0.8,y=tgt_cnt[1]+3000,s=f'{tgt_cnt[1]:,} ({tgt_prop[1]}%)');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>Class in imbalanced"},{"metadata":{},"cell_type":"markdown","source":"<h1><section id='mutinfo'>9. Mutual Information&nbsp;&nbsp;<a href=#top>Top</a></section></h1>"},{"metadata":{},"cell_type":"markdown","source":"<h3>Calculating mutual information scores with and without marking rare categories. This helps in understanding the effect of marking rare categories"},{"metadata":{"trusted":true},"cell_type":"code","source":"def mutual_info(df,mark_rare=False):\n    df = df.copy()\n    if mark_rare == True:\n        df = mark_rare_categories(df).copy()\n    \n    #X = df.iloc[:,:-1].copy()\n    X = df.drop(columns=['target']).copy()\n    y = df['target'].values.copy()\n\n    for i in categorical_variables:\n        le = LabelEncoder()\n        X[i] = le.fit_transform(X[i])\n\n    mutual_info = mutual_info_classif(X=X,y=y,discrete_features=(X.dtypes == np.int64),random_state=11)\n    mutual_info_df = pd.DataFrame({'feature':X.columns,'MI':mutual_info}).sort_values(by='MI',ascending=False)\n    mutual_info_df.sort_values(by='MI').plot(x='feature',y='MI',kind='barh',figsize=(15,12),color='black')\n    for n,k in enumerate(range((len(mutual_info_df)-1),-1,-1)):\n        plt.text(y=n-0.2,x=mutual_info_df.iloc[k,1],s=np.round(mutual_info_df.iloc[k,1],4))\n    plt.show()\n    if mark_rare == True:\n        mutual_info_df.columns = ['Feature','MI_mark_rare_T']\n    else:\n        mutual_info_df.columns = ['Feature','MI_mark_rare_F']\n    return mutual_info_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mi_mark_rare_F = mutual_info(data).copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mi_mark_rare_T = mutual_info(df=data,mark_rare=True).copy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>From the above MI plots, we need to assess if marking rare lables is resulting in any loss of information"},{"metadata":{"trusted":true},"cell_type":"code","source":"marked_rare_cols = mark_rare_categories(data).copy()\nmarked_rare_cols = (((marked_rare_cols[categorical_variables] == 'Rare').sum()).astype('bool')).reset_index()\nmarked_rare_cols.columns = ['Feature','is_marked_rare']\n\nmi_mark_rare_F = mi_mark_rare_F.merge(mi_mark_rare_T,on='Feature',how='left').copy()\nmi_mark_rare_F = mi_mark_rare_F.merge(marked_rare_cols,on='Feature',how='left').copy()\n\nmi_mark_rare_F['mi_percent_change'] = (mi_mark_rare_F['MI_mark_rare_T'] / mi_mark_rare_F['MI_mark_rare_F'] -1)*100\nmi_mark_rare_F","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2>cat16, cat15 and cat18 have the highest MI scores"},{"metadata":{},"cell_type":"markdown","source":"<h3>MARKING RARE LABELS IS RESULTING IN REDUCED MI SCORE. REDUCING THE CUT-OFF FOR MARKING RARE LABELS IS IMPROVING THE MI SCORE. HENCE, THE MORE VARIABLES MARKED AS RARE, LOWER WILL BE THE MI SCORE."},{"metadata":{},"cell_type":"markdown","source":"<h1><section id='base'>10. Random Forest Baseline Model&nbsp;&nbsp;<a href=#top>Top</a></section></h1>"},{"metadata":{},"cell_type":"markdown","source":"<h3>Building RF with and without marking rare categories helps to identify the effect of marking rare categories on a model' performance"},{"metadata":{"trusted":true},"cell_type":"code","source":"def prepare_x_y(df,mark_rare=False):\n    df = df.copy()\n    if mark_rare == True:\n        df = mark_rare_categories(df).copy()\n\n    #X = df.iloc[:,:-1].copy()\n    X = df.drop(columns=['target']).copy()\n    y = df['target'].copy()\n\n    col_transform = ColumnTransformer(transformers=[['ordinal_encoder',OrdinalEncoder(),categorical_variables]],\n                                      remainder='passthrough')\n\n    X = col_transform.fit_transform(X).copy()\n    return (X,y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2>RF without marking rare labels"},{"metadata":{"trusted":true},"cell_type":"code","source":"X,y = prepare_x_y(data)\n\nkfold = StratifiedKFold(n_splits=5,shuffle=True,random_state=11)\n\nrf = RandomForestClassifier(random_state=11)\n\n#cv = cross_validate(estimator=rf,X=X,y=y,cv=kfold,n_jobs=-1,verbose=11,scoring='roc_auc',return_train_score=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"cv['train_score']<br>\narray([1., 1., 1., 1., 1.])<br><br>\ncv['test_score']<br>\narray([0.88257123, 0.88632475, 0.88164209, 0.88505834, 0.88238509])\n\nnp.mean(cv['test_score'])<br>\n0.8835963\n"},{"metadata":{},"cell_type":"markdown","source":"<h2>RF with rare labels marked"},{"metadata":{"trusted":true},"cell_type":"code","source":"X,y = prepare_x_y(data,mark_rare=True)\n\nkfold = StratifiedKFold(n_splits=5,shuffle=True,random_state=11)\n\nrf = RandomForestClassifier(random_state=11)\n\n#cv = cross_validate(estimator=rf,X=X,y=y,cv=kfold,n_jobs=-1,verbose=11,scoring='roc_auc',return_train_score=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"cv['train_score']<br>\narray([1., 1., 1., 1., 1.])<br><br>\n\ncv['test_score']<br>\narray([0.88190414, 0.88619879, 0.88114262, 0.88497447, 0.88220613])<br>\n\nnp.mean(cv['test_score'])<br>\n0.8832852299999999<br>\n\n<h2>Marking rare labels showed no difference in model' performance\n"},{"metadata":{},"cell_type":"markdown","source":"<h1><section id='nfc'>11. New Feature Creation&nbsp;&nbsp;<a href=#top>Top</a></section></h1>"},{"metadata":{},"cell_type":"markdown","source":"<h3>cat16, cat15, cat18 and cat1 have the highest MI scores. Hence, we'll combine them to create new features."},{"metadata":{"trusted":true},"cell_type":"code","source":"data['c15_16'] = data['cat15'] + '-' + data['cat16']\ndata['c15_18'] = data['cat15'] + '-' + data['cat18']\ndata['c16_18'] = data['cat16'] + '-' + data['cat18']\ndata['c15_16_18'] = data['cat15'] +  '-' + data['cat16'] + '-' + data['cat18']\n\n\n#NEW\ndata['c15_1'] = data['cat15'] + '-' + data['cat1']\ndata['c16_1'] = data['cat16'] + '-' + data['cat1']\ndata['c18_1'] = data['cat18'] + '-' + data['cat1']\n\ndata['c15_16_1'] = data['cat15'] +  '-' + data['cat16'] + '-' + data['cat1']\ndata['c15_18_1'] = data['cat15'] +  '-' + data['cat18'] + '-' + data['cat1']\ndata['c16_18_1'] = data['cat16'] +  '-' + data['cat18'] + '-' + data['cat1']\ndata['c15_16_18_1'] = data['cat15'] +  '-' + data['cat16'] +  '-' + data['cat18'] + '-' + data['cat1']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#categorical_variables.extend(['c15_16','c15_18','c16_18','c15_16_18'])\ncategorical_variables.extend(['c15_16','c15_18','c16_18','c15_16_18','c15_1','c16_1','c18_1',\n                            'c15_16_1','c15_18_1','c16_18_1','c15_16_18_1'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>Calculating MI scores by including the newly added features"},{"metadata":{"trusted":true},"cell_type":"code","source":"mi_mark_rare_F = mutual_info(data).copy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>All the newly added features have high MI scores, but their individual score is close to 0. Hence, adding them may not improve the model' performance to a great extent. However, adding them and employing a feature selection method might work."},{"metadata":{},"cell_type":"markdown","source":"<h2>Vanilla RF on data with newly added features"},{"metadata":{"trusted":true},"cell_type":"code","source":"X,y = prepare_x_y(data)\n\nkfold = StratifiedKFold(n_splits=5,shuffle=True,random_state=11)\n\nrf = RandomForestClassifier(random_state=11)\n\n#cv = cross_validate(estimator=rf,X=X,y=y,cv=kfold,n_jobs=-1,verbose=11,scoring='roc_auc',return_train_score=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"cv['train_score']<br>\narray([1., 1., 1., 1., 1.])<br><br>\n\ncv['test_score']<br>\narray([0.88089911, 0.88598508, 0.88015966, 0.88502024, 0.88199494])<br>\n\nnp.mean(cv['test_score'])<br>\n0.8828118051395147<br>"},{"metadata":{},"cell_type":"markdown","source":"<h3>Adding New features has not improved the performance of the model. However, hyperparameter tuning may help"},{"metadata":{},"cell_type":"markdown","source":"<h1><section id='build'>12. Model Building&nbsp;&nbsp;<a href=#top>Top</a></section></h1>"},{"metadata":{},"cell_type":"markdown","source":"<h1>Vanilla XGB"},{"metadata":{},"cell_type":"markdown","source":"<h3>scale_pos_weight = no. of neg samples / no. of pos samples"},{"metadata":{"trusted":true},"cell_type":"code","source":"# X = data.drop(columns=['target']).copy()\n# y = data['target'].copy()\n\n# col_transform = ColumnTransformer(transformers=[['ordinal_encoder',OrdinalEncoder(handle_unknown=\"use_encoded_value\",unknown_value=-1),categorical_variables]],\n#                                    remainder='passthrough')\n\n# X = col_transform.fit_transform(X).copy()\n\n# kfold = StratifiedKFold(n_splits=5,shuffle=True,random_state=11)\n\n# final_model = xgb.XGBClassifier(random_state=11,\n#                                 eval_metric='auc',\n#                                 tree_method='gpu_hist',\n#                                 n_jobs=-1,\n#                                 use_label_encoder=False,\n#                                 scale_pos_weight=2.78)\n\n# # cv = cross_validate(estimator=final_model,X=X,y=y,cv=kfold,n_jobs=-1,verbose=11,scoring='roc_auc',return_train_score=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"cv['train_score']<br>\narray([0.91757861, 0.91684923, 0.91634894, 0.91599782, 0.91700579])<br><br>\n\ncv['test_score']<br>\narray([0.88787838, 0.89218227, 0.88705092, 0.89156957, 0.8876197 ])<br>\n\nnp.mean(cv['test_score'])<br>\n0.8892601661007467<br>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# final_model.fit(X,y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2>Trying OptunaSearchCV (ExperimentalWarning: OptunaSearchCV is still experimental phase)"},{"metadata":{"trusted":true},"cell_type":"code","source":"def model_train_test(X,y,estimator,param_distributions,n_trials,n_splits):\n    \n    kfold = StratifiedKFold(n_splits=n_splits,shuffle=True,random_state=11)\n    \n    optuna_search = optuna.integration.OptunaSearchCV(estimator=estimator,\n                                                  param_distributions=param_distributions,\n                                                  cv=kfold,\n                                                  scoring='roc_auc',\n                                                  n_jobs=-1,\n                                                  n_trials=n_trials,\n                                                  random_state=11,\n                                                  verbose=1)\n    \n    optuna_search.fit(X,y)\n    \n    result = {'optuna_search':optuna_search,\n              'best_params':optuna_search.best_params_,\n              'best_score':optuna_search.best_score_}\n    \n    return result","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2>XGB + Optuna + RFE (Used as Final Model)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#scale_pos_weight = no. of neg samples / no. of pos samples\n\nX = data.drop(columns=['target']).copy()\ny = data['target'].copy()\n\ncol_transform = ColumnTransformer(transformers=[['ordinal_encoder',\n                                                 OrdinalEncoder(handle_unknown=\"use_encoded_value\",unknown_value=-1),\n                                                 categorical_variables]],\n                                  remainder='passthrough')\n\nX = col_transform.fit_transform(X).copy()\n\nxgb_classifier = xgb.XGBClassifier(random_state=11,\n                                   eval_metric='auc',\n                                   tree_method='gpu_hist',\n                                   n_jobs=-1,\n                                   use_label_encoder=False,\n                                   scale_pos_weight=2.8)\n\n\npipeline = Pipeline(steps=[['rfe',RFE(estimator=xgb_classifier)],\n                           ['classifier',xgb_classifier]])\n\n\n\n\nparam_distributions = {'rfe__n_features_to_select':optuna.distributions.IntUniformDistribution(20, X.shape[1]),\n                       'classifier__n_estimators':optuna.distributions.IntUniformDistribution(100, 700),\n                       'classifier__max_depth':optuna.distributions.IntUniformDistribution(3, 15),\n                       'classifier__reg_lambda':optuna.distributions.LogUniformDistribution(500, 100000),\n                       'classifier__colsample_bylevel':optuna.distributions.LogUniformDistribution(0.5,1)\n                      }\n\n    \n#result = model_train_test(X,y,pipeline,param_distributions,300,3)\n\n#result","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"'best_params': {'rfe__n_features_to_select': 41,<br>\n  'classifier__n_estimators': 670,<br>\n  'classifier__max_depth': 10,<br>\n  'classifier__reg_lambda': 9738.763400431715,<br>\n  'classifier__colsample_bylevel': 0.6323793308207895},<br>\n 'best_score': 0.8926917680409953}"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = data.drop(columns=['target']).copy()\ny = data['target'].copy()\n\ncol_transform = ColumnTransformer(transformers=[['ordinal_encoder',\n                                                 OrdinalEncoder(handle_unknown=\"use_encoded_value\",unknown_value=-1),\n                                                 categorical_variables]],\n                                  remainder='passthrough')\n\nX = col_transform.fit_transform(X).copy()\n\nxgb_classifier = xgb.XGBClassifier(random_state=11,\n                                   eval_metric='auc',\n                                   tree_method='gpu_hist',\n                                   n_jobs=-1,\n                                   use_label_encoder=False,\n                                   scale_pos_weight=2.8,\n                                   n_estimators=670,\n                                   max_depth=10,\n                                   reg_lambda=9738.763400431715,\n                                   colsample_bylevel=0.6323793308207895)\n\n\nfinal_model = Pipeline(steps=[['rfe',RFE(n_features_to_select=41,\n                                         estimator=xgb.XGBClassifier(\n                                         random_state=11,\n                                         eval_metric='auc',\n                                         tree_method='gpu_hist',\n                                         n_jobs=-1,\n                                         use_label_encoder=False,\n                                         scale_pos_weight=2.8))],\n                           ['classifier',xgb_classifier]])\n\nfinal_model.fit(X,y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2>LGBMClassifier using same hyperparameters tuned for XGB (Need to train)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# #scale_pos_weight = no. of neg samples / no. of pos samples\n\n# X = data.drop(columns=['target']).copy()\n# y = data['target'].copy()\n\n# col_transform = ColumnTransformer(transformers=[['ordinal_encoder',\n#                                                  OrdinalEncoder(handle_unknown=\"use_encoded_value\",unknown_value=-1),\n#                                                  categorical_variables]],\n#                                   remainder='passthrough')\n\n# X = col_transform.fit_transform(X).copy()\n\n# lgb_classifier = LGBMClassifier(random_state=11,\n#                                 metric='auc',\n#                                 device_type='gpu',\n#                                 n_jobs=-1,\n#                                 scale_pos_weight=2.8)\n\n\n# pipeline = Pipeline(steps=[['rfe',RFE(estimator=lgb_classifier)],\n#                            ['classifier',lgb_classifier]])\n\n\n\n\n# param_distributions = {'rfe__n_features_to_select':optuna.distributions.IntUniformDistribution(10, X.shape[1]),\n#                        'classifier__n_estimators':optuna.distributions.IntUniformDistribution(100, 700),\n#                        'classifier__max_depth':optuna.distributions.IntUniformDistribution(2, 15),\n#                        'classifier__reg_lambda':optuna.distributions.LogUniformDistribution(500, 100000),\n#                        'classifier__feature_fraction_bynode':optuna.distributions.LogUniformDistribution(0.5,1)\n#                       }\n\n    \n# #result = model_train_test(X,y,pipeline,param_distributions,150,3)\n\n# #result","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2>VotingClassifier (Not yet trained)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# X = data.drop(columns=['target']).copy()\n# y = data['target'].copy()\n\n# col_transform = ColumnTransformer(transformers=[['ordinal_encoder',\n#                                                  OrdinalEncoder(handle_unknown=\"use_encoded_value\",unknown_value=-1),\n#                                                  categorical_variables]],\n#                                   remainder='passthrough')\n\n# X = col_transform.fit_transform(X).copy()\n\n\n# xgb_model = xgb.XGBClassifier(random_state=11,\n#                                    eval_metric='auc',\n#                                    tree_method='gpu_hist',\n#                                    n_jobs=-1,\n#                                    use_label_encoder=False,\n#                                    scale_pos_weight=2.8,\n#                                    n_estimators=636,\n#                                    max_depth=8,\n#                                    reg_lambda=10495.566650340597,\n#                                    colsample_bylevel=0.8940079532042917)\n\n# lgb_model = LGBMClassifier(random_state=11,\n#                                 metric='auc',\n#                                 device_type='gpu',\n#                                 n_jobs=-1,\n#                                 scale_pos_weight=2.8,\n#                                 n_estimators=550,\n#                                 max_depth=13,\n#                                 num_leaves = 2^(13),\n#                                 reg_lambda=789.5237167418961,\n#                                 feature_fraction_bynode=0.6711243271129161\n#                                )\n\n# voting_classifier = VotingClassifier(estimators = [('xgb',xgb_model),('lgb',lgb_model)],\n#                                     voting='soft')\n\n# voting_classifier.fit(X,y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# final_model = voting_classifier","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2>EasyEnsembleClassifier (no improvement in submission score)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# X = data.drop(columns=['target']).copy()\n# y = data['target'].copy()\n\n# col_transform = ColumnTransformer(transformers=[['ordinal_encoder',\n#                                                  OrdinalEncoder(handle_unknown=\"use_encoded_value\",unknown_value=-1),\n#                                                  categorical_variables]],\n#                                   remainder='passthrough')\n\n# X = col_transform.fit_transform(X).copy()\n\n# xgb_classifier = xgb.XGBClassifier(random_state=11,\n#                                    eval_metric='auc',\n#                                    tree_method='gpu_hist',\n#                                    n_jobs=-1,\n#                                    use_label_encoder=False,\n#                                    n_estimators=636,\n#                                    max_depth=8,\n#                                    reg_lambda=10495.566650340597,\n#                                    colsample_bylevel=0.8940079532042917)\n\n# kfold = StratifiedKFold(n_splits=10,shuffle=True,random_state=11)\n\n# model = EasyEnsembleClassifier(base_estimator = xgb_classifier,\n#                                random_state=11,\n#                                verbose=11,\n#                                n_jobs=-1)\n    \n# cv = cross_validate(estimator=model,X=X,y=y,cv=kfold,n_jobs=-1,verbose=11,scoring='roc_auc',return_train_score=True)\n    \n    \n# print(f'\\ntrain_scores\\n{cv[\"train_score\"]}\\nmean train score\\n{np.mean(cv[\"train_score\"])}' + \n#      f'\\ntest_scores\\n{cv[\"test_score\"]}\\nmean test score\\n{np.mean(cv[\"test_score\"])}')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"train_scores\n[0.90357855 0.90318018 0.90274322 0.90319348 0.90365007 0.90314774\n 0.90269818 0.90347099 0.90326053 0.90343125]<br><br>\nmean train score<br>\n0.9032354202610001<br><br>\ntest_scores<br>\n[0.88959868 0.89332921 0.8974062  0.89291028 0.88805739 0.8933892\n 0.89770895 0.89028771 0.89207586 0.89045199]<br><br>\nmean test score<br>\n0.892521546427905\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# final_model = EasyEnsembleClassifier(base_estimator = xgb_classifier,\n#                                random_state=11,\n#                                verbose=11,\n#                                n_jobs=-1)\n\n# final_model.fit(X,y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2>Evaluating few under-sampling techniques (Not yet trained)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# X = data.drop(columns=['target']).copy()\n# y = data['target'].copy()\n\n# col_transform = ColumnTransformer(transformers=[['ordinal_encoder',\n#                                                  OrdinalEncoder(handle_unknown=\"use_encoded_value\",unknown_value=-1),\n#                                                  categorical_variables]],\n#                                   remainder='passthrough')\n\n# X = col_transform.fit_transform(X).copy()\n\n# xgb_classifier = xgb.XGBClassifier(random_state=11,\n#                                    eval_metric='auc',\n#                                    tree_method='gpu_hist',\n#                                    n_jobs=-1,\n#                                    use_label_encoder=False,\n#                                    n_estimators=426,\n#                                    max_depth=7,\n#                                    reg_lambda=4905.085039913591)\n\n# under_samplers = [under_sampling.TomekLinks(n_jobs=-1),\n#                   under_sampling.CondensedNearestNeighbour(random_state=11,n_jobs=-1),\n#                   under_sampling.EditedNearestNeighbours(n_jobs=-1),\n#                   under_sampling.RepeatedEditedNearestNeighbours(n_jobs=-1),\n#                   under_sampling.NearMiss(n_jobs=-1),\n#                   under_sampling.NeighbourhoodCleaningRule(n_jobs=-1)]\n\n# for i in under_samplers:\n    \n#     kfold = StratifiedKFold(n_splits=3,shuffle=True,random_state=11)\n    \n    \n#     model = imbpipeline(steps=[['under_sampler',i],\n#                                ['classifier',xgb_classifier]])\n    \n#     cv = cross_validate(estimator=model,X=X,y=y,cv=kfold,n_jobs=-1,verbose=11,scoring='roc_auc')\n    \n    \n#     print(f'{i}\\ntest_scores\\n{cv[\"test_score\"]}\\nmean test score\\n{np.mean(cv[\"test_score\"])}')\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1>WORK IN PROGRESS"},{"metadata":{},"cell_type":"markdown","source":"<h1><section id='subm'>13. Submission&nbsp;&nbsp;<a href=#top>Top</a></section></h1>"},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv('../input/tabular-playground-series-mar-2021/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_ids = test['id'].copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.drop(columns='id',inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical_variables = list(test.select_dtypes('object').columns)\n\nnumeric_variables = list(test.select_dtypes('float64').columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'Categorical Variables ({len(categorical_variables)}):\\n{categorical_variables}\\n\\nNumeric Variables ({len(numeric_variables)}):\\n{numeric_variables}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['c15_16'] = test['cat15'] + '-' + test['cat16']\ntest['c15_18'] = test['cat15'] + '-' +  test['cat18']\ntest['c16_18'] = test['cat16'] + '-' +  test['cat18']\ntest['c15_16_18'] = test['cat15'] + '-' +  test['cat16'] + '-' +  test['cat18']\n\n#NEW\ntest['c15_1'] = test['cat15'] + '-' + test['cat1']\ntest['c16_1'] = test['cat16'] + '-' + test['cat1']\ntest['c18_1'] = test['cat18'] + '-' + test['cat1']\n\ntest['c15_16_1'] = test['cat15'] +  '-' + test['cat16'] + '-' + test['cat1']\ntest['c15_18_1'] = test['cat15'] +  '-' + test['cat18'] + '-' + test['cat1']\ntest['c16_18_1'] = test['cat16'] +  '-' + test['cat18'] + '-' + test['cat1']\ntest['c15_16_18_1'] = test['cat15'] +  '-' + test['cat16'] +  '-' + test['cat18'] + '-' + test['cat1']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#categorical_variables.extend(['c15_16','c15_18','c16_18','c15_16_18'])\ncategorical_variables.extend(['c15_16','c15_18','c16_18','c15_16_18','c15_1','c16_1','c18_1',\n                              'c15_16_1','c15_18_1','c16_18_1','c15_16_18_1'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>Count of rows in test data with categories not in training data"},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in categorical_variables:\n    print(f'{i}\\n{np.sum(test[i].isin(data[i])==False)}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = col_transform.transform(test).copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction = final_model.predict_proba(test)[:,1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame({'id':test_ids,'target':prediction})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv('submission_11.csv',index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}