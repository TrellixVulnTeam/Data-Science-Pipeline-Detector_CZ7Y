{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport missingno as msno\nfrom scipy.stats import skew\nfrom sklearn.decomposition import PCA\nfrom sklearn.ensemble import RandomForestClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score,roc_curve\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom functools import partial\nimport optuna","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df=pd.read_csv('../input/tabular-playground-series-mar-2021/train.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Checking for null values\n* missingno is a library which heps in visualizing missing values in our dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"msno.matrix(df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### From above we infer that no column has any missing vlaue"},{"metadata":{},"cell_type":"markdown","source":"## **Exploring Categorical Columns**"},{"metadata":{},"cell_type":"markdown","source":"### Checking for number of unique values per categorical column"},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_cols=df.select_dtypes('object')\n\nprint('Number of unique values for each categorical feature \\n')\nfor col in cat_cols:\n    print(f'{col} : {df[col].nunique()}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Cat10 has the maximum number of unique values followed by cat5. Let's plot a pie chart illustrating the top 3 maximum value counts in these categorical features"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"explode=[0.1,0.1,0.1]\ncmap=plt.get_cmap('Paired')\ncolors=[cmap(i) for i in np.linspace(0,1,3)]\n\ncols=['cat5','cat10']\nfig=plt.figure(figsize=(15,15))\nfor i,col in enumerate(cols):\n    fig.add_subplot(1,2,i+1)\n    fig.set_size_inches(12,11)\n    pie=df[col].value_counts().head(3).plot.pie(shadow=True,\n                                           autopct='%1.1f%%',\n                                           explode=explode,\n                                           pctdistance=0.5,\n                                           colors=colors,\n                                           textprops={'fontsize':14})\n    plt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### From above plots we infer that :\n*  For cat5 BI has the most value counts.\n* For cat10 however,all values have almost equal value counts."},{"metadata":{},"cell_type":"markdown","source":"## **Exploring Numerical Columns**"},{"metadata":{},"cell_type":"markdown","source":"### Histogram and box plot below show distributions of all numerical columns."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"num_cols=df.select_dtypes(['float64'])\nplt.style.use('seaborn-whitegrid')\n\nfor col in num_cols:\n    fig,ax=plt.subplots(2,1,sharex=True,\n                       gridspec_kw={'height_ratios':(0.25,0.75)})\n    fig.set_size_inches(7,6)\n    sns.boxplot(x=col,data=df,ax=ax[0])\n    sns.histplot(x=col,data=df,ax=ax[1])\n    ax[0].set_xlabel(col,fontsize=14)\n    ax[1].set_xlabel(col,fontsize=14)\n    ax[1].set_ylabel('Count',fontsize=14)\n    ax[0].set_yticks([])\n    sns.despine(ax=ax[1])\n    sns.despine(ax=ax[0],left=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Below heatmap illustrates correlation analysis of  a feature with the other features"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#dropping the id column since it won't be of much help in finding the correlation.\n\ncorr=df.drop('id',axis=1).corr(method='pearson')\nplt.figure(figsize=(10,8))\nplt.title('Correlation Analysis',fontsize=16)\nplt.xticks(rotation=90,fontsize=14)\nplt.yticks(fontsize=14)\n\nsns.heatmap(corr,annot=True,fmt='0.1f',\n            robust=True,cmap='coolwarm')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### cont7 and cont10 have a strong positive correlation while the same case is with cont7 and cont0. Lastly cont2 and cont1 have a correlation of 0.86. Below are the scatterplots for these features."},{"metadata":{"trusted":true},"cell_type":"code","source":"col1=['cont0','cont7','cont10','cont1']\ncol2=['cont10','cont0','cont7','cont2']\n\nfig=plt.figure()\nplt.style.use('seaborn-darkgrid')\nfor i in range(4):\n    fig.add_subplot(4,2,i+1)\n    fig.set_size_inches(10,12)\n    sns.scatterplot(x=col1[i],y=col2[i],data=df,\n                    alpha=0.1,edgecolor='none')\n    plt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Checking skewness of numerical columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in num_cols:\n    print(col)\n    print('Skewness :',np.round(skew(df[col]),3))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols=['cont8','cont9','cont10']\nfor col in cols:\n    df[col]=np.log(df[col])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Below we check the target feature and plot a count plot. We observe that class 0 has a greater count than that of class 1"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(6,6))\nsns.countplot(data=df,x='target')\nplt.xlabel('target',fontsize=14)\nplt.ylabel('Count',fontsize=14)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"le=LabelEncoder()\nfor col in cat_cols:\n    df[col]=le.fit_transform(df[col])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y=df.target.values\nX=df.drop(['id','target'],axis=1).values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Hyperparameter Optimization using Optuna\n* Below we fit and train a LGBMClassifier using StratifiedKFold. \n* Lastly we return the mean of the AUC score obtained as a result of 5 splits"},{"metadata":{"trusted":true},"cell_type":"code","source":"def optimize(trial,x,y):\n    \n    num_iterations=trial.suggest_int('num_iterations',100,1500)\n    max_depth=trial.suggest_int('max_depth',3,15)\n    num_leaves=trial.suggest_int('num_leaves',10,100)\n    min_data_in_leaf=trial.suggest_int('min_data_in_leaf',1,100)\n    min_sum_hessian_in_leaf=trial.suggest_int('min_sum_hessian_in_leaf',1,200)\n    feature_fraction=trial.suggest_uniform('feature_fraction',1e-5,1.0)\n    bagging_fraction=trial.suggest_uniform('bagging_fraction',1e-5,1.0)\n    bagging_freq=trial.suggest_int('bagging_freq',1,10)\n    lambda_l1=trial.suggest_uniform('lambda_l1',1e-5,5.0)\n    lambda_l2=trial.suggest_uniform('lambda_l2',1e-5,10)\n   \n    model=LGBMClassifier(\n        num_iterations=num_iterations,\n        max_depth=max_depth,\n        num_leaves=num_leaves,\n        min_data_in_leaf=min_data_in_leaf,\n        min_sum_hessian_in_leaf= min_sum_hessian_in_leaf,\n        feature_fraction=feature_fraction,\n        bagging_fraction=bagging_fraction,\n        bagging_freq=bagging_freq,\n        lambda_l1=lambda_l1,\n        lambda_l2=lambda_l2\n    )\n    kf=StratifiedKFold(n_splits=5)\n    AUC=[]\n    for idx in kf.split(X=x,y=y):\n        train_idx,test_idx=idx[0],idx[1]\n        x_train,y_train=x[train_idx],y[train_idx]\n        x_test,y_test=x[test_idx],y[test_idx]\n       \n        model.fit(x_train,y_train)\n        preds=model.predict_proba(x_test)[:,1]\n        fold_auc=roc_auc_score(y_test,preds)\n        AUC.append(fold_auc)\n        \n    return -1*np.mean(AUC)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"#optimization_function=partial(optimize,x=X,y=y)\n#study=optuna.create_study(direction='minimize')\n\n#study.optimize(optimization_function,n_trials=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"lgbm=LGBMClassifier(\n     num_iterations=1091,\n     max_depth=13,\n     num_leaves=22,\n     min_data_in_leaf=82,\n     min_sum_hessian_in_leaf=42,\n     feature_fraction=0.1631559284100434,\n     bagging_fraction=0.38583663547224584,\n     bagging_freq=7,\n     lambda_l1=0.054607760008535275,\n     lambda_l2=0.4441933265076425,\n     )\n\nX_train,X_val,y_train,y_val=train_test_split(X,y,test_size=0.3,stratify=y)\nlgbm.fit(X_train,y_train)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lgbm_probs=lgbm.predict_proba(X_val)\n\n# Plotting the roc curve for probability prediction\nplt.style.use('seaborn-whitegrid')\nn_probs=[0 for _ in range(len(y_val))]\nlgbm_probs=lgbm_probs[:,1]\nns_auc=roc_auc_score(y_val,n_probs)\nlgbm_auc=roc_auc_score(y_val,lgbm_probs)\nprint('ROC AUC:%.3f' %(lgbm_auc))\nns_fpr,ns_tpr,_=roc_curve(y_val,n_probs)\nlgbm_fpr,lgbm_tpr,_=roc_curve(y_val,lgbm_probs)\nplt.figure(figsize=(9,7))\nplt.plot(ns_fpr,ns_tpr,linestyle='--',label='No skill')\nplt.plot(lgbm_fpr,lgbm_tpr,'g-',linewidth=2.3,label='positive outcome')\nplt.xlabel('False Positive Rate',fontsize=14)\nplt.ylabel('True positive rate',fontsize=14)\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_=df.copy()\ndf_=df_.drop(['id','target'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature Importance Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"x=pd.DataFrame(lgbm.feature_importances_)\nx.columns=['Feature Importance']\nx.index=df_.columns\nx=x.sort_values(by='Feature Importance',ascending=False)\n\nplt.style.use('default')\nplt.figure(figsize=(7,5))\nsns.barplot(x='Feature Importance',y=x.index,data=x)\nplt.xlabel('Feature Importance',fontsize=14)\nplt.ylabel('Feature',fontsize=14)\nplt.title('Feature Imporatnce Analysis',fontweight='bold',fontsize=10)\nyticks=plt.yticks(fontsize=8)\nxticks=plt.xticks(fontsize=8)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test=pd.read_csv('../input/tabular-playground-series-mar-2021/test.csv')\nX_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in cat_cols:\n    X_test[col],_=X_test[col].factorize()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test['target']=lgbm.predict_proba(X_test.drop('id',axis=1))[:,1]\nsubmission=pd.DataFrame({'id':X_test['id'],'target':X_test['target']})\nsubmission.to_csv('my_submission.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}