{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport xgboost as xgb\nimport sklearn.metrics as sklm\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Functions"},{"metadata":{"trusted":true},"cell_type":"code","source":"ver = 7\n\ndef score_model(probs, threshold):\n    return np.array([1 if x > threshold else 0 for x in probs])\n\ndef print_metrics(labels, probs, threshold):\n    scores = score_model(probs, threshold)\n    metrics = sklm.precision_recall_fscore_support(labels, scores)\n    logloss = sklm.log_loss(labels, probs)\n    conf = sklm.confusion_matrix(labels, scores)\n    print('                 Confusion matrix')\n    print('                 Score positive    Score negative')\n    print('Actual positive    %6d' % conf[0,0] + '             %5d' % conf[0,1])\n    print('Actual negative    %6d' % conf[1,0] + '             %5d' % conf[1,1])\n    print('')\n    print(f\"logloss: {logloss}\")\n    print('Accuracy        %0.2f' % sklm.accuracy_score(labels, scores))\n    print('AUC             %0.2f' % sklm.roc_auc_score(labels, probs))\n    print('Macro precision %0.2f' % float((float(metrics[0][0]) + float(metrics[0][1]))/2.0))\n    print('Macro recall    %0.2f' % float((float(metrics[1][0]) + float(metrics[1][1]))/2.0))\n    print(' ')\n    print('           Positive      Negative')\n    print('Num case   %6d' % metrics[3][0] + '        %6d' % metrics[3][1])\n    print('Precision  %6.2f' % metrics[0][0] + '        %6.2f' % metrics[0][1])\n    print('Recall     %6.2f' % metrics[1][0] + '        %6.2f' % metrics[1][1])\n    print('F1         %6.2f' % metrics[2][0] + '        %6.2f' % metrics[2][1])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"../input/ncaam2020/df.csv\")\ndftest = pd.read_csv(\"../input/ncaam2020/dftest.csv\")\ndfsubmit = pd.read_csv(\"../input/ncaam2020/submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Features and labels"},{"metadata":{"trusted":true},"cell_type":"code","source":"label = \"Pred\"\n\nfeatures = [\"t1_N_win_perc\",\"t2_N_win_perc\",\"t1_rank\",\"t2_rank\",\"t1_win_perc\",\"t2_win_perc\",\n            \"t1_outscore\",\"t2_outscore\",\"t1_outscored\",\"t2_outscored\",\n            \"t1_points_avg\",\"t2_points_avg\",\"t1_fg_perc\",\"t2_fg_perc\",\n            \"t1_fg3_perc\",\"t2_fg3_perc\",\"t1_ft_perc\",\"t2_ft_perc\",\n            \"t1_or_avg\",\"t1_dr_avg\",\"t2_or_avg\",\"t2_dr_avg\",\"t1_ast_avg\",\"t2_ast_avg\",\n            \"t1_stl_avg\",\"t2_stl_avg\",\"t1_blk_avg\",\"t2_blk_avg\"]\n\ndf[features].head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Filter out first games"},{"metadata":{"trusted":true},"cell_type":"code","source":"filternum = 0\ndf = df.loc[(df[\"t1_games\"] > filternum) & (df[\"t2_games\"] > filternum)]\nprint(f\"Shape: {df.shape}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Split"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = df[features]\nX_test = dftest[features]\ny_train = df[label]\ny_test = dftest[label]\n\ndtrain = xgb.DMatrix(X_train, label=y_train)\ndtest = xgb.DMatrix(X_test, label=y_test)\ndsubmit = xgb.DMatrix(dfsubmit[features])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Cross validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"num_round = 999\nparam = {\"max_depth\":9,\n         \"min_child_weight\":2,\n         \"subsample\":0.8,\n         \"colsample_bytree\":0.6,\n         \"eta\":0.01,\n         \"eval_metric\":\"logloss\",\n         \"objective\":\"binary:logistic\",\n         \"seed\":1,\n         \"verbosity\":1}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scores = xgb.cv(param,\n                dtrain,\n                num_boost_round=num_round,\n                nfold=2,\n                metrics=\"logloss\",\n                verbose_eval=True,\n                seed=1)\nscores.to_csv(f\"scores_{ver}.csv\")\nscores","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_round = scores[\"test-logloss-mean\"].idxmin() + 1\nnum_round","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train and test"},{"metadata":{"trusted":true},"cell_type":"code","source":"bst = xgb.train(param, dtrain, num_round)\n# make prediction\npreds = bst.predict(dtest)\nprint_metrics(y_test, preds, 0.5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Predict"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predict\npreds_proba = bst.predict(dsubmit)\npreds_class = score_model(preds_proba, 0.5)\nprint(f\"class = {preds_class[:20]}\")\nprint(f\"proba = {preds_proba[:20]}\")\n\ndf = pd.DataFrame({\"ID\":list(dfsubmit[\"ID\"]),\"Pred\":[i for i in preds_proba],\"Class\":preds_class})\n\n# Submit\ndf.to_csv(f\"sumbit{ver}_full.csv\", index=False)\ndf[[\"ID\",\"Pred\"]].to_csv(f\"sumbit{ver}.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature importances"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.DataFrame(bst.get_score(), index=[\"Score\"])\ns = df.loc[\"Score\"].sort_values(ascending=False)\ndf = pd.DataFrame(s)\ndf.to_csv(f\"feature_importances_{ver}.csv\")\ndf","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}