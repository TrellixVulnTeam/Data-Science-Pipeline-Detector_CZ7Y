{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install -U kaggle_environments cpprb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import gc\nfrom multiprocessing import set_start_method, cpu_count, Process, Event, SimpleQueue\nimport time\n\nimport numpy as np\nimport tensorflow as tf\nimport cpprb # Replay Buffer Library: https://ymd_h.gitlab.io/cpprb/\nfrom tqdm.notebook import tqdm\n\nfrom kaggle_environments.envs.hungry_geese.hungry_geese import Observation, Configuration, Action, row_col\nfrom kaggle_environments import make\n\n# %load_ext tensorboard\n# %tensorboard --logdir logs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Global config\nRIGHT = 0\nGO = 1\nLEFT = 2\n\nGOOSE = -1\nFOOD = 1\n\nact_shape = 3\n\nWIDTH = 11\nHEIGHT = 7\n\nxc = WIDTH//2 + 1\nyc = HEIGHT//2 + 1\n\ncode2dir = {0:'EAST', 1:'NORTH', 2:'WEST', 3:'SOUTH'}\n\ndir2code = {\"EAST\":0, \"NORTH\": 1, \"WEST\":2, \"SOUTH\": 3}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_model():\n    model = tf.keras.Sequential([tf.keras.layers.Dense(100,activation=\"relu\",input_shape=(WIDTH*HEIGHT,)),\n                                 tf.keras.layers.Dense(100,activation=\"relu\"),\n                                 tf.keras.layers.Dense(100,activation=\"relu\"),\n                                 tf.keras.layers.Dense(act_shape)])\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def Q_func(model,obs,act):\n    return tf.reduce_sum(model(obs) * tf.one_hot(act,depth=3), axis=1)\n\ndef Q1_func(model,next_obs,rew,done):\n    gamma = 0.99\n    return gamma*tf.reduce_max(model(next_obs),axis=1)*(1.0-done) + rew\n\n#@tf.function\ndef train_then_absTD(model,target,obs,act,rew,next_obs,done,weights):\n    with tf.GradientTape() as tape:\n        tape.watch(model.trainable_weights)\n        Q = Q_func(model,obs,act)\n        yQ1_r = Q1_func(target,next_obs,rew,done)\n        TD_square = tf.square(Q - yQ1_r)\n        weighted_loss = tf.reduce_mean(TD_square * weights)\n\n    grad = tape.gradient(weighted_loss,model.trainable_weights)\n    opt.apply_gradients(zip(grad,model.trainable_weights))\n\n    Qnew = Q_func(model,obs,act)\n    return tf.abs(Qnew - yQ1_r)\n\n#@tf.function\ndef abs_TD(model,target,obs,act,rew,next_obs,done):\n    Q = Q_func(model,obs,act)\n    yQ1_r = Q1_func(target,next_obs,rew,done)\n    return tf.abs(Q - yQ1_r)   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def pos(index):\n    return index%WIDTH, index//WIDTH\n\ndef centering(z,dz,Z):\n    z += dz\n    if z < 0:\n        z += Z\n    elif Z >= Z:\n        z -= Z\n    return z\n    \n\ndef encode_board(obs,act=\"NORTH\",idx=0):\n    \"\"\"\n    Player goose is always set at the center\n    \"\"\"\n    board = np.zeros((WIDTH,HEIGHT))\n\n    if len(obs[\"geese\"][idx]) == 0:\n        return board\n        \n    x0, y0 = pos(obs[\"geese\"][idx][0])\n    dx = xc - x0\n    dy = yc - y0\n    \n    for goose in obs[\"geese\"]:\n        for g in goose:\n            x, y = pos(g)\n            \n            x = centering(x,dx,WIDTH)\n            y = centering(y,dy,HEIGHT)\n                \n            board[x,y] = GOOSE\n            \n    for food in obs[\"food\"]:\n        x, y = pos(food)\n        \n        x = centering(x,dx,WIDTH)\n        y = centering(y,dy,HEIGHT)\n        \n        board[x,y] = FOOD\n        \n    board[xc,yc] = dir2code[act]\n    \n    return board","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_obs_action(model,states,idx=0, train=False):\n    eps = 0.3\n\n    act = states[idx][\"action\"]\n\n    if states[idx][\"status\"] != \"ACTIVE\":\n        return None, act\n    \n    board = encode_board(states[0][\"observation\"],act=act,idx=idx)\n    \n    # e-greedy\n    if train and np.random.random() < eps:\n        turn = np.random.randint(3)\n    else:\n        turn = int(tf.math.argmax(tf.squeeze(model(board.reshape(1,-1))))) - 1\n\n    new_act = dir2code[act] + turn\n    if new_act < 0:\n        new_act += 4\n    elif new_act >= 4:\n        new_act -= 4\n    return board, code2dir[new_act]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_buffer(buffer_size,env_dict,alpha):\n    return cpprb.MPPrioritizedReplayBuffer(buffer_size,env_dict,alpha=alpha)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def explorer(global_rb,env_dict,is_training_done,queue):\n    local_buffer_size = int(1e+2)\n    local_rb = cpprb.ReplayBuffer(local_buffer_size+4,env_dict)\n\n    model = create_model()\n    target = tf.keras.models.clone_model(model)\n    env = make(\"hungry_geese\", debug=False)\n    \n    states = env.reset(4)\n    while not is_training_done.is_set():\n        if not queue.empty():\n            w,wt = queue.get()\n            model.set_weights(w)\n            target.set_weights(wt)\n\n        board_act = [get_obs_action(model,states,i,train=True) for i in range(4)]\n\n        states = env.step([a for b,a in board_act])\n\n        for i, (b, a) in enumerate(board_act):\n            if b is None:\n                continue\n\n            local_rb.add(obs=b.ravel(),\n                         act=dir2code[a],\n                         next_obs=encode_board(states[0][\"observation\"],act=a,idx=i).ravel(),\n                         rew=states[i][\"reward\"],\n                         done=(states[i][\"status\"] == \"ACTIVE\"))\n\n        if all(s[\"status\"] != \"ACTIVE\" for s in states):\n            states = env.reset(4)\n            local_rb.on_episode_end()\n\n        if local_rb.get_stored_size() >= local_buffer_size:\n            sample = local_rb.get_all_transitions()\n            global_rb.add(**sample,\n                          priorities=abs_TD(model,target,\n                                            tf.constant(sample[\"obs\"]),\n                                            tf.constant(sample[\"act\"].ravel()),\n                                            tf.constant(sample[\"rew\"].ravel()),\n                                            tf.constant(sample[\"next_obs\"]),\n                                            tf.constant(sample[\"done\"].ravel())))\n            local_rb.clear()            ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\n# Training\nn_warming = 100\nn_train_step = int(1e+5)\nbatch_size = 64\n\nwriter = tf.summary.create_file_writer(\"./logs\")\n\n# Replay Buffer \nbuffer_size = 10e+5\nenv_dict = {\"obs\": {\"shape\": (WIDTH*HEIGHT)},\n            \"act\": {\"dtype\": int},\n            \"next_obs\": {\"shape\": (WIDTH*HEIGHT)},\n            \"rew\": {},\n            \"done\": {}}\nalpha = 0.5\nrb = create_buffer(buffer_size, env_dict,alpha)\n\n# Model\ntarget_update = 50\n\n\nmodel = create_model()\ntarget = tf.keras.models.clone_model(model)\n\nopt = tf.keras.optimizers.Adam()\n\n# Ape-X\nexplorer_update_freq = 100\nn_explorer = cpu_count() - 1\n\n\nis_training_done = Event()\nis_training_done.clear()\n\nqs = [SimpleQueue() for _ in range(n_explorer)]\nps = [Process(target=explorer,\n              args=[rb,env_dict,is_training_done,q])\n      for q in qs]\n\nfor p in ps:\n    p.start()\n\nprint(\"warm-up\")\nwhile rb.get_stored_size() < n_warming:\n    time.sleep(1)\n\n\nprint(\"training\")\n    \nepoch = 0\nfor i in tqdm(range(n_train_step)):        \n    sample = rb.sample(batch_size,beta=0.4)\n    \n    absTD = train_then_absTD(model,target,\n                             tf.constant(sample[\"obs\"]),\n                             tf.constant(sample[\"act\"].ravel()),\n                             tf.constant(sample[\"rew\"].ravel()),\n                             tf.constant(sample[\"next_obs\"]),\n                             tf.constant(sample[\"done\"].ravel()),\n                             tf.constant(sample[\"weights\"].ravel()))\n    rb.update_priorities(sample[\"indexes\"],absTD)\n        \n    if i % target_update == 0:\n        target.set_weights(model.get_weights())\n        \n    if i % explorer_update_freq == 0:\n        w = model.get_weights()\n        wt = target.get_weights()\n        for q in qs:\n            q.put((w,wt))\n\n    \nis_training_done.set()\n\nmodel.save(\"model\")\n\nfor p in ps:\n    p.join()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_env = make(\"hungry_geese\", debug=True)\n\nstates = test_env.reset(4)\n\nwhile any(s[\"status\"] == \"ACTIVE\" for s in states):\n    board_act = [get_obs_action(model,states,i) for i in range(4)]\n    states = test_env.step([a for b,a in board_act])\n\ntest_env.render(mode='ipython')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}