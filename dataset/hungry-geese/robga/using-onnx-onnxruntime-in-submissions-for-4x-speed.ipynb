{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# What is OnnxRunTime?\n\n[ONNX Runtime](https://www.onnxruntime.ai/) or ORT is a cross-platform inference and training machine-learning accelerator.\n\nML engines like Torch have exporters to onnx format, like [torch.onnx.export()](https://pytorch.org/tutorials/advanced/super_resolution_with_onnxruntime.html). It is easy to convert your torch or other framework weights to onnx weights.\n\n# Why should I care?\n\nKaggle Simulations allow a set time for initiation and per step. Similarly, other kaggle kernel-only submission competitions.\n\n**ORT can enable speed ups from 1.5-10x**. The example in this notebook is for Hungry Geese Kaggle Simulation where we show **onnx inference takes just 25% of torch inference**. Plus, no need to wait for torch or tf to import.\n\n# Is it legal?\n\nKaggle does not have onnxruntime installed in its base kernel environments or allow external pip installations in submission environments. In order to include it in your submission, you'll need bundle it up in a tar.gz submission.\n\nA typical kaggle rule is that \"During the evaluation of an episode your Submission may not pull in or use any information external to the Submission and Environment and may not send any information out.\" \n\nBut this does not preclude the inclusion of external libraries in your submission, such as RL frameworks and other helper tools. The rule is meant to stop data ingress and egress from the evaluation environment.\n\n# Going forward\n\nAs far as we are aware, the advantage in using onnx in kaggle simulations and kaggle kernel competitions has not been covered in notebooks and this is the first description of usage in a competition for an advantage. We expect it to become widespread.\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-29T13:48:38.564215Z","iopub.execute_input":"2021-07-29T13:48:38.564633Z","iopub.status.idle":"2021-07-29T13:48:39.310358Z","shell.execute_reply.started":"2021-07-29T13:48:38.564595Z","shell.execute_reply":"2021-07-29T13:48:39.309172Z"}}},{"cell_type":"markdown","source":"# Hungry Geese agent with Torch inference\n\nThe test function in this notebook is the Hungry Geese agent from the excellent notebook [Smart Geese Trained by Reinforcement Learning](https://www.kaggle.com/yuricat/smart-geese-trained-by-reinforcement-learning) by [yuricat](https://www.kaggle.com/yuricat) and [kayazuki](https://www.kaggle.com/kyazuki).\n\nWe'll create it and run it a number of times to see how fast it is.\n","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport os\nimport sys\nfrom time import perf_counter\nfrom kaggle_environments import evaluate, make","metadata":{"execution":{"iopub.status.busy":"2021-07-29T14:33:37.094626Z","iopub.execute_input":"2021-07-29T14:33:37.095032Z","iopub.status.idle":"2021-07-29T14:33:37.582223Z","shell.execute_reply.started":"2021-07-29T14:33:37.094931Z","shell.execute_reply":"2021-07-29T14:33:37.581227Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"HRL_TORCH_FILE = '../input/handyrlbin/handyrl.bin'\n\ntorch_timings = []\nonnx_timings = []","metadata":{"execution":{"iopub.status.busy":"2021-07-29T14:33:38.605272Z","iopub.execute_input":"2021-07-29T14:33:38.605643Z","iopub.status.idle":"2021-07-29T14:33:38.609977Z","shell.execute_reply.started":"2021-07-29T14:33:38.605609Z","shell.execute_reply":"2021-07-29T14:33:38.608979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TorusConv2d(nn.Module):\n    def __init__(self, input_dim, output_dim, kernel_size, bn):\n        super().__init__()\n        self.edge_size = (kernel_size[0] // 2, kernel_size[1] // 2)\n        self.conv = nn.Conv2d(input_dim, output_dim, kernel_size=kernel_size)\n        self.bn = nn.BatchNorm2d(output_dim) if bn else None\n\n    def forward(self, x):\n        h = torch.cat([x[:,:,:,-self.edge_size[1]:], x, x[:,:,:,:self.edge_size[1]]], dim=3)\n        h = torch.cat([h[:,:,-self.edge_size[0]:], h, h[:,:,:self.edge_size[0]]], dim=2)\n        h = self.conv(h)\n        h = self.bn(h) if self.bn is not None else h\n        return h\n\n\nclass GeeseNet(nn.Module):\n    def __init__(self):\n        super().__init__()\n        layers, filters = 12, 32\n        self.conv0 = TorusConv2d(17, filters, (3, 3), True)\n        self.blocks = nn.ModuleList([TorusConv2d(filters, filters, (3, 3), True) for _ in range(layers)])\n        self.head_p = nn.Linear(filters, 4, bias=False)\n        self.head_v = nn.Linear(filters * 2, 1, bias=False)\n\n    def forward(self, x):\n        h = F.relu_(self.conv0(x))\n        for block in self.blocks:\n            h = F.relu_(h + block(h))\n        h_head = (h * x[:,:1]).view(h.size(0), h.size(1), -1).sum(-1)\n        h_avg = h.view(h.size(0), h.size(1), -1).mean(-1)\n        p = self.head_p(h_head)\n        v = torch.tanh(self.head_v(torch.cat([h_head, h_avg], 1)))\n\n        return {'policy': p, 'value': v}\n\n\n# Input for Neural Network\n\ndef make_input(obses):\n    b = np.zeros((17, 7 * 11), dtype=np.float32)\n    obs = obses[-1]\n\n    for p, pos_list in enumerate(obs['geese']):\n        # head position\n        for pos in pos_list[:1]:\n            b[0 + (p - obs['index']) % 4, pos] = 1\n        # tip position\n        for pos in pos_list[-1:]:\n            b[4 + (p - obs['index']) % 4, pos] = 1\n        # whole position\n        for pos in pos_list:\n            b[8 + (p - obs['index']) % 4, pos] = 1\n            \n    # previous head position\n    if len(obses) > 1:\n        obs_prev = obses[-2]\n        for p, pos_list in enumerate(obs_prev['geese']):\n            for pos in pos_list[:1]:\n                b[12 + (p - obs['index']) % 4, pos] = 1\n\n    # food\n    for pos in obs['food']:\n        b[16, pos] = 1\n\n    return b.reshape(-1, 7, 11)\n\n\n# Load PyTorch Model\n\n\nmodel = GeeseNet()\nstate_dict = torch.load(HRL_TORCH_FILE)['model_state_dict']\nmodel.load_state_dict(state_dict)\nmodel.eval()\n\n# Main Function of Agent\n\nobses = []\n\ndef agent(obs, _):\n    global torch_timings\n\n    obses.append(obs)\n    x = make_input(obses)\n\n    start_time = perf_counter()\n\n    with torch.no_grad():\n        xt = torch.from_numpy(x).unsqueeze(0)\n        o = model(xt)\n    p = o['policy'].squeeze(0).detach().numpy()\n\n    torch_timings.append(perf_counter() - start_time)\n\n    actions = ['NORTH', 'SOUTH', 'WEST', 'EAST']\n    return actions[np.argmax(p)]\n","metadata":{"execution":{"iopub.status.busy":"2021-07-29T14:33:41.461057Z","iopub.execute_input":"2021-07-29T14:33:41.461718Z","iopub.status.idle":"2021-07-29T14:33:41.507261Z","shell.execute_reply.started":"2021-07-29T14:33:41.461666Z","shell.execute_reply":"2021-07-29T14:33:41.506519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for _ in range(100): # run 100 times\n    env = make(\"hungry_geese\", debug=False)\n    env.reset()\n    output = env.run([agent, agent, agent, agent])","metadata":{"execution":{"iopub.status.busy":"2021-07-29T14:33:54.752638Z","iopub.execute_input":"2021-07-29T14:33:54.753145Z","iopub.status.idle":"2021-07-29T14:34:10.589848Z","shell.execute_reply.started":"2021-07-29T14:33:54.753098Z","shell.execute_reply":"2021-07-29T14:34:10.588999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'Torch mean step time is {np.mean(torch_timings)} ms across {len(torch_timings)} steps')","metadata":{"execution":{"iopub.status.busy":"2021-07-29T14:34:10.591431Z","iopub.execute_input":"2021-07-29T14:34:10.592043Z","iopub.status.idle":"2021-07-29T14:34:10.599039Z","shell.execute_reply.started":"2021-07-29T14:34:10.591998Z","shell.execute_reply":"2021-07-29T14:34:10.597962Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Creating the onnx weights\n\nConverting torch weights to onnx weights is a breeze.\n","metadata":{"execution":{"iopub.status.busy":"2021-07-29T14:16:09.808122Z","iopub.execute_input":"2021-07-29T14:16:09.80854Z","iopub.status.idle":"2021-07-29T14:16:09.814487Z","shell.execute_reply.started":"2021-07-29T14:16:09.80849Z","shell.execute_reply":"2021-07-29T14:16:09.813066Z"}}},{"cell_type":"code","source":"!pip install onnxruntime","metadata":{"execution":{"iopub.status.busy":"2021-07-29T14:34:13.463432Z","iopub.execute_input":"2021-07-29T14:34:13.46379Z","iopub.status.idle":"2021-07-29T14:34:20.541124Z","shell.execute_reply.started":"2021-07-29T14:34:13.463762Z","shell.execute_reply":"2021-07-29T14:34:20.540233Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = torch.randn(4, 17, 7, 11, requires_grad=True) # network shape\n    \ntorch.onnx.export(model,     # model being run\n  x,                         # model input (or a tuple for multiple inputs)\n  f\"handyrl.onnx\",           # where to save the model (can be a file or file-like object)\n  export_params=True,        # store the trained parameter weights inside the model file\n  opset_version=12,          # the ONNX version to export the model to\n  do_constant_folding=True,  # whether to execute constant folding for optimization\n  input_names = ['input'],   # the model's input names\n  output_names = ['output'], # the model's output names\n  dynamic_axes={'input' : {0 : 'batch_size'},    \n                'output' : {0 : 'batch_size'}})        \n","metadata":{"execution":{"iopub.status.busy":"2021-07-29T14:34:20.543657Z","iopub.execute_input":"2021-07-29T14:34:20.543961Z","iopub.status.idle":"2021-07-29T14:34:23.326852Z","shell.execute_reply.started":"2021-07-29T14:34:20.543929Z","shell.execute_reply":"2021-07-29T14:34:23.325793Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Hungry Geese agent with ORT inference\n\nWe no longer need to include a model or import torch","metadata":{"execution":{"iopub.status.busy":"2021-07-29T14:16:55.384989Z","iopub.execute_input":"2021-07-29T14:16:55.385374Z","iopub.status.idle":"2021-07-29T14:16:55.390574Z","shell.execute_reply.started":"2021-07-29T14:16:55.385342Z","shell.execute_reply":"2021-07-29T14:16:55.389465Z"}}},{"cell_type":"code","source":"os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n\nimport onnxruntime\n\nHRL_ONNX_FILE = 'handyrl.onnx'\n\nopts = onnxruntime.SessionOptions()\nopts.inter_op_num_threads = 1\nopts.intra_op_num_threads = 1\nopts.execution_mode = onnxruntime.ExecutionMode.ORT_SEQUENTIAL\n\nhandyrl_session = onnxruntime.InferenceSession(HRL_ONNX_FILE, sess_options=opts)\n\nobses = []\n\ndef agent(obs, _):\n    global torch_timings\n    obses.append(obs)\n    x = make_input(obses)\n\n    start_time = perf_counter()\n\n    ort_inputs = {handyrl_session.get_inputs()[0].name: np.expand_dims(x, axis=0)}\n    p = handyrl_session.run(None, ort_inputs)[0][0]\n    \n    onnx_timings.append(perf_counter() - start_time)\n\n    actions = ['NORTH', 'SOUTH', 'WEST', 'EAST']\n    return actions[np.argmax(p)]","metadata":{"execution":{"iopub.status.busy":"2021-07-29T14:34:29.46402Z","iopub.execute_input":"2021-07-29T14:34:29.46437Z","iopub.status.idle":"2021-07-29T14:34:29.532679Z","shell.execute_reply.started":"2021-07-29T14:34:29.464341Z","shell.execute_reply":"2021-07-29T14:34:29.531672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for _ in range(100): # run 100 times\n    env = make(\"hungry_geese\", debug=False)\n    env.reset()\n    output = env.run([agent, agent, agent, agent])","metadata":{"execution":{"iopub.status.busy":"2021-07-29T14:34:33.242529Z","iopub.execute_input":"2021-07-29T14:34:33.24287Z","iopub.status.idle":"2021-07-29T14:34:40.25242Z","shell.execute_reply.started":"2021-07-29T14:34:33.242842Z","shell.execute_reply":"2021-07-29T14:34:40.251447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'Torch mean step time is {np.mean(torch_timings)} ms across {len(torch_timings)} steps')\nprint(f'onnx mean step time is {np.mean(onnx_timings)} ms across {len(onnx_timings)} steps')\nprint(f'onnx is {np.mean(torch_timings)/np.mean(onnx_timings)} faster than torch for this task')","metadata":{"execution":{"iopub.status.busy":"2021-07-29T14:34:40.253958Z","iopub.execute_input":"2021-07-29T14:34:40.254353Z","iopub.status.idle":"2021-07-29T14:34:40.262412Z","shell.execute_reply.started":"2021-07-29T14:34:40.254311Z","shell.execute_reply":"2021-07-29T14:34:40.261358Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}