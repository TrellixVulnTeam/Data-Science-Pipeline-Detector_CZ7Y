{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"https://towardsdatascience.com/deep-q-learning-tutorial-mindqn-2a4c855abffc","metadata":{}},{"cell_type":"code","source":"from kaggle_environments.envs.hungry_geese.hungry_geese import Observation,\\\nConfiguration, Action, row_col, adjacent_positions, translate, min_distance,random_agent, GreedyAgent\n\nfrom kaggle_environments import make\nimport numpy as np\nfrom random import choice\n\nimport tensorflow as tf\nfrom tensorflow import keras\nimport numpy as np\nimport random\nfrom collections import deque\nimport time","metadata":{"execution":{"iopub.status.busy":"2021-05-21T18:01:07.081049Z","iopub.execute_input":"2021-05-21T18:01:07.081422Z","iopub.status.idle":"2021-05-21T18:01:08.694892Z","shell.execute_reply.started":"2021-05-21T18:01:07.081372Z","shell.execute_reply":"2021-05-21T18:01:08.694043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ACTIONS = [e.name for e in Action]\nACTIONS = ['NORTH', 'SOUTH', 'WEST', 'EAST']","metadata":{"execution":{"iopub.status.busy":"2021-05-21T18:01:08.702878Z","iopub.execute_input":"2021-05-21T18:01:08.70314Z","iopub.status.idle":"2021-05-21T18:01:08.70724Z","shell.execute_reply.started":"2021-05-21T18:01:08.703114Z","shell.execute_reply":"2021-05-21T18:01:08.706065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"env = make(\"hungry_geese\")\ndisplay(env.reset())\n\ntrainer = env.train([None, \"greedy\", \"greedy\", \"greedy\"])\ntrainer.reset()","metadata":{"execution":{"iopub.status.busy":"2021-05-21T18:01:08.708897Z","iopub.execute_input":"2021-05-21T18:01:08.709356Z","iopub.status.idle":"2021-05-21T18:01:08.770971Z","shell.execute_reply.started":"2021-05-21T18:01:08.709321Z","shell.execute_reply":"2021-05-21T18:01:08.770164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef DQNet():\n    model = tf.keras.Sequential()\n    model.add(tf.keras.Input(shape=(7,11,17)))\n    model.add(tf.keras.layers.Conv2D(filters=128, kernel_size=(3,5), activation='relu'))\n    model.add(tf.keras.layers.Conv2D(filters=256, kernel_size=3, activation='relu'))\n    model.add(tf.keras.layers.Conv2D(filters=128, kernel_size=3, activation='relu'))\n#     model.add(tf.keras.layers.GlobalAveragePooling2D())\n    model.add(tf.keras.layers.Flatten())\n    model.add(tf.keras.layers.Dense(128, activation='relu'))\n    model.add(tf.keras.layers.Dense(32, activation='relu'))\n    model.add(tf.keras.layers.Dense(4, activation='linear'))\n    \n    model.compile(loss=tf.keras.losses.Huber(), optimizer=tf.keras.optimizers.Adam(), metrics=['accuracy'])\n    \n    return model\n\nmodel = DQNet()\n# model.build(input_shape=(None, 7, 11, 17))\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2021-05-21T18:01:08.772247Z","iopub.execute_input":"2021-05-21T18:01:08.77255Z","iopub.status.idle":"2021-05-21T18:01:09.747747Z","shell.execute_reply.started":"2021-05-21T18:01:08.77252Z","shell.execute_reply":"2021-05-21T18:01:09.746923Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def centerize(b):\n    dy, dx = np.where(b[0])\n    if len(dy) == 0 and len(dx) == 0:\n        return b\n    centerize_y = (np.arange(0,7)-3+dy[0])%7\n    centerize_x = (np.arange(0,11)-5+dx[0])%11\n    \n    b = b[:, centerize_y,:]\n    b = b[:, :,centerize_x]\n    \n    return b\n\ndef make_input(obs, prev_obs):\n#     print(obs)\n    b = np.zeros((17, 7 * 11), dtype=np.float32)\n#     obs = obses[-1]\n\n    for p, pos_list in enumerate(obs['geese']):\n        # head position\n        for pos in pos_list[:1]:\n            b[0 + (p - obs['index']) % 4, pos] = 1\n        # tip position\n        for pos in pos_list[-1:]:\n            b[4 + (p - obs['index']) % 4, pos] = 1\n        # whole position\n        for pos in pos_list:\n            b[8 + (p - obs['index']) % 4, pos] = 1\n            \n    # previous head position\n    if prev_obs is not None:\n        obs_prev = prev_obs\n        for p, pos_list in enumerate(obs_prev['geese']):\n            for pos in pos_list[:1]:\n                b[12 + (p - obs['index']) % 4, pos] = 1\n\n    # food\n    for pos in obs['food']:\n        b[16, pos] = 1\n        \n    b = b.reshape(-1, 7, 11)\n    b = centerize(b)\n    b = np.transpose(b, (1,2,0))\n\n    return b","metadata":{"execution":{"iopub.status.busy":"2021-05-21T18:01:09.749767Z","iopub.execute_input":"2021-05-21T18:01:09.750119Z","iopub.status.idle":"2021-05-21T18:01:09.762035Z","shell.execute_reply.started":"2021-05-21T18:01:09.750083Z","shell.execute_reply":"2021-05-21T18:01:09.761103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train(env, replay_memory, model, target_model, done):\n    \n    learning_rate = 0.7 # Learning rate\n    discount_factor = 0.618\n    batch_size = 512 * 2\n\n    MIN_REPLAY_SIZE = 3000\n    if len(replay_memory) < MIN_REPLAY_SIZE:\n        return\n\n    \n    mini_batch = random.sample(replay_memory, batch_size)\n    \n    rewards = np.array([transition[2] for transition in mini_batch])\n    \n    current_states = np.array([transition[0] for transition in mini_batch])\n    current_qs_list = model.predict(current_states)\n    \n    \n    new_current_states = np.array([transition[3] for transition in mini_batch])\n    \n#     non_final_states = np.array([transition[3] for transition in mini_batch if not transition[4]])\n#     non_final_index_mask = np.array([False if transition[4] else True  for transition in mini_batch])\n    \n    \n#     future_qs_list = tf.zeros(batch_size)\n    \n    future_qs_list = target_model.predict(new_current_states)\n    \n#     next_state_reward = future_qs_list*discount_factor + rewards\n\n    X = []\n    Y = []\n    for index, (observation, action, reward, new_observation, done) in enumerate(mini_batch):\n        if not done:\n            max_future_q = reward + discount_factor * np.max(future_qs_list[index])\n        else:\n            max_future_q = reward\n\n#         current_qs = current_qs_list[index]\n        current_qs = [0]*4\n        current_qs[action] = (1 - learning_rate) * current_qs[action] + learning_rate * max_future_q\n\n        X.append(observation)\n        Y.append(current_qs)\n#     print(Y)\n    model.fit(np.array(X), np.array(Y), batch_size=batch_size, verbose=0, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2021-05-21T18:01:09.763362Z","iopub.execute_input":"2021-05-21T18:01:09.763857Z","iopub.status.idle":"2021-05-21T18:01:09.778315Z","shell.execute_reply.started":"2021-05-21T18:01:09.763818Z","shell.execute_reply":"2021-05-21T18:01:09.777269Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def main():\n    trainer = env.train([None, \"greedy\", \"greedy\", \"greedy\"])\n    trainer.reset()\n    is_render = False\n    \n    epsilon = 1 # Epsilon-greedy algorithm in initialized at 1 meaning every step is random at the start\n    max_epsilon = 1 # You can't explore more than 100% of the time\n    min_epsilon = 0.01 # At a minimum, we'll always explore 1% of the time\n    decay = 0.01\n    \n    # An episode a full game\n    train_episodes = 15000\n    test_episodes = 100\n\n    # 1. Initialize the Target and Main models\n    # Main Model (updated every 4 steps)\n    model = DQNet()\n    # Target Model (updated every 100 steps)\n    target_model = DQNet()\n    target_model.set_weights(model.get_weights())\n\n    replay_memory = deque(maxlen=20000)\n\n    target_update_counter = 0\n\n    # X = states, y = actions\n    X = []\n    y = []\n    \n#     observation_list = []\n    steps_to_update_target_model = 0\n    prev_obs = None\n\n    for episode in range(train_episodes):\n        total_training_rewards = 0\n        observation = trainer.reset()\n#         observation_list = []\n        prev_obs = None\n        done = False\n#         print(\"***********New Eposide **************\")\n        while not done:\n#             observation_list.append(observation)\n#             prev_obs = observation\n            encoded_observation = make_input(observation, prev_obs)\n            \n            steps_to_update_target_model += 1\n            if is_render:\n                env.render(mode=\"ipython\", width=500, height=450)\n\n            random_number = np.random.rand()\n            # 2. Explore using the Epsilon Greedy Exploration Strategy\n            if random_number <= epsilon or len(replay_memory) < 5000:\n                # Explore\n                g_agent = GreedyAgent(Configuration({'rows': 7, 'columns': 11}))\n                action = g_agent(Observation(observation))\n                \n                action = ACTIONS.index(action)\n            else:\n                # Exploit best known action\n                # model dims are (batch, env.observation_space.n)\n                encoded_reshaped = encoded_observation.reshape(-1,7,11,17)\n                \n                predicted = model.predict(encoded_reshaped)\n                action = np.argmax(predicted)\n#             print(action)\n            new_observation, reward, done, info = trainer.step(ACTIONS[action])\n            if done and reward == 0:\n#                 print(\"*****************Done*******************\")\n                reward = -1000\n                \n            prev_obs = observation\n            new_encoded_observation = make_input(new_observation, prev_obs)\n            replay_memory.append([encoded_observation, action, reward, new_encoded_observation, done])\n\n            # 3. Update the Main Network using the Bellman Equation\n            if steps_to_update_target_model % 4 == 0 or done:\n#                 print(\"***********Train************\")\n                train(env, replay_memory, model, target_model, done)\n            \n            observation = new_observation\n            total_training_rewards += reward\n\n            if done:\n#                 print('Total training rewards: {} after n steps = {} with final reward = {}'.format(total_training_rewards, episode, reward))\n                total_training_rewards += 1\n\n                if steps_to_update_target_model >= 100:\n                    print('Copying main network weights to the target network weights')\n                    target_model.set_weights(model.get_weights())\n                    steps_to_update_target_model = 0\n                break\n\n        epsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp(-decay * episode)\n        \n        if episode % 300 == 0:\n            model.save('my_model.h5')\n#     env.close()\n    model.save('my_model.h5')\n\nif __name__ == '__main__':\n    main()","metadata":{"execution":{"iopub.status.busy":"2021-05-21T18:01:09.77945Z","iopub.execute_input":"2021-05-21T18:01:09.779902Z","iopub.status.idle":"2021-05-21T18:03:57.097515Z","shell.execute_reply.started":"2021-05-21T18:01:09.779863Z","shell.execute_reply":"2021-05-21T18:03:57.09477Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# obs_list = []\n# obs = trainer.reset()\n# obs_list.append(obs)\n# model = tf.keras.models.load_model('my_model.h5')\n\n# done = False\n# total_reward = 0\n# while not done:\n#     obs_list.append(obs)\n#     encoded_observation = make_input(obs_list)\n#     encoded_reshaped = encoded_observation.reshape(-1,7,11,17)\n                \n#     predicted = model.predict(encoded_reshaped)\n# #     print(predicted)\n#     action = np.argmax(predicted)\n# #     print(action)\n#     new_obs, reward, done, info = trainer.step(ACTIONS[action])\n# #     env.render(mode=\"ipython\", width=500, height=450)\n    \n#     obs = new_obs\n#     total_reward += reward\n    \n#     if done:\n#         print(f'Total reward: {total_reward}')\n        \n   \n","metadata":{"execution":{"iopub.status.busy":"2021-05-21T18:04:17.445197Z","iopub.execute_input":"2021-05-21T18:04:17.445647Z","iopub.status.idle":"2021-05-21T18:04:17.451685Z","shell.execute_reply.started":"2021-05-21T18:04:17.445606Z","shell.execute_reply":"2021-05-21T18:04:17.450675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile main.py\n\nimport sys\nfrom pathlib import Path\nimport numpy as np\nimport tensorflow as tf\n\np = Path('/kaggle_simulations/agent/')\nif p.exists():\n    sys.path.append(str(p))\nelse:\n    p = Path('__file__').resolve().parent","metadata":{"execution":{"iopub.status.busy":"2021-05-21T18:04:18.302314Z","iopub.execute_input":"2021-05-21T18:04:18.302658Z","iopub.status.idle":"2021-05-21T18:04:18.311026Z","shell.execute_reply.started":"2021-05-21T18:04:18.302628Z","shell.execute_reply":"2021-05-21T18:04:18.307277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile -a main.py\n\nimport tensorflow as tf\nimport numpy as np\nimport random\n\ndef centerize(b):\n    dy, dx = np.where(b[0])\n    if len(dy) == 0 and len(dx) == 0:\n        return b\n    \n    centerize_y = (np.arange(0,7)-3+dy[0])%7\n    centerize_x = (np.arange(0,11)-5+dx[0])%11\n    \n    b = b[:, centerize_y,:]\n    b = b[:, :,centerize_x]\n    \n    return b\n\ndef make_input(obses):\n    b = np.zeros((17, 7 * 11), dtype=np.float32)\n    obs = obses[-1]\n\n    for p, pos_list in enumerate(obs['geese']):\n        # head position\n        for pos in pos_list[:1]:\n            b[0 + (p - obs['index']) % 4, pos] = 1\n        # tip position\n        for pos in pos_list[-1:]:\n            b[4 + (p - obs['index']) % 4, pos] = 1\n        # whole position\n        for pos in pos_list:\n            b[8 + (p - obs['index']) % 4, pos] = 1\n            \n    # previous head position\n    if len(obses) > 1:\n        obs_prev = obses[-2]\n        for p, pos_list in enumerate(obs_prev['geese']):\n            for pos in pos_list[:1]:\n                b[12 + (p - obs['index']) % 4, pos] = 1\n\n    # food\n    for pos in obs['food']:\n        b[16, pos] = 1\n        \n    b = b.reshape(-1, 7, 11)\n    b = centerize(b)\n    b = np.transpose(b, (1,2,0))\n\n    return b\n\nmodel = tf.keras.models.load_model(str(p/'my_model.h5'))\n\nobses = []\n\ndef agent(obs_dict, config_dict):\n    obses.append(obs_dict)\n\n    X_test = make_input(obses)\n#     X_test = np.transpose(X_test, (1,2,0))\n    X_test = X_test.reshape(-1,7,11,17) # channel last.\n    \n    # avoid suicide\n#     obstacles = X_test[:,:,:,[8,9,10,11,12]].max(axis=3) - X_test[:,:,:,[4,5,6,7]].max(axis=3) # body + opposite_side - my tail\n#     obstacles = np.array([obstacles[0,2,5], obstacles[0,4,5], obstacles[0,3,4], obstacles[0,3,6]])\n    \n    y_pred = model.predict(X_test) \n\n    \n    actions = ['NORTH', 'SOUTH', 'WEST', 'EAST']\n    return actions[np.argmax(y_pred)]","metadata":{"execution":{"iopub.status.busy":"2021-05-21T18:04:18.8497Z","iopub.execute_input":"2021-05-21T18:04:18.850117Z","iopub.status.idle":"2021-05-21T18:04:18.856253Z","shell.execute_reply.started":"2021-05-21T18:04:18.850084Z","shell.execute_reply":"2021-05-21T18:04:18.855264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from kaggle_environments import make\nenv = make(\"hungry_geese\", debug=True)\n\nenv.reset()\nenv.run(['main.py', 'main.py','main.py','main.py'])\nenv.render(mode=\"ipython\", width=500, height=450)","metadata":{"execution":{"iopub.status.busy":"2021-05-21T18:04:19.609621Z","iopub.execute_input":"2021-05-21T18:04:19.61Z","iopub.status.idle":"2021-05-21T18:04:29.187611Z","shell.execute_reply.started":"2021-05-21T18:04:19.609968Z","shell.execute_reply":"2021-05-21T18:04:29.186609Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!tar -czf submission.tar.gz main.py my_model.h5","metadata":{"execution":{"iopub.status.busy":"2021-05-21T18:03:57.105309Z","iopub.status.idle":"2021-05-21T18:03:57.105876Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# str(p/'model')","metadata":{"execution":{"iopub.status.busy":"2021-05-21T18:03:57.107087Z","iopub.status.idle":"2021-05-21T18:03:57.107679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}