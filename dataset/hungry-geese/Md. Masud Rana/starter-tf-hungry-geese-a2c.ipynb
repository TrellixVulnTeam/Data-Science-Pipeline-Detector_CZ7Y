{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from kaggle_environments.envs.hungry_geese.hungry_geese import Observation,\\\nConfiguration, Action, row_col, adjacent_positions, translate, min_distance,random_agent, GreedyAgent\n\nfrom kaggle_environments import make\nimport numpy as np\nfrom random import choice, shuffle\n\nimport tensorflow as tf\nfrom tensorflow import keras\nimport numpy as np\nimport random\nfrom collections import deque\nimport time\n\nfrom matplotlib import pyplot as plt\nfrom tensorflow.keras import layers\nfrom typing import Any, List, Sequence, Tuple\n\nimport collections\nimport statistics\nimport tqdm\neps = np.finfo(np.float32).eps.item()","metadata":{"execution":{"iopub.status.busy":"2021-05-22T19:47:48.58683Z","iopub.execute_input":"2021-05-22T19:47:48.587215Z","iopub.status.idle":"2021-05-22T19:47:48.593687Z","shell.execute_reply.started":"2021-05-22T19:47:48.587183Z","shell.execute_reply":"2021-05-22T19:47:48.592927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ACTIONS = [e.name for e in Action]\nACTIONS = ['NORTH', 'SOUTH', 'WEST', 'EAST']\n\nenv = make(\"hungry_geese\")\ndisplay(env.reset())\n\ntrainer = env.train([None, \"greedy\", \"greedy\", \"greedy\"])\nobs = trainer.reset()\nPREV_GEESE = obs['geese']\n\nTRAINER = trainer\n\ndisplay(PREV_GEESE)\n\nseed = 42\n# env.seed(seed)\ntf.random.set_seed(seed)\nnp.random.seed(seed)","metadata":{"execution":{"iopub.status.busy":"2021-05-22T19:47:48.983584Z","iopub.execute_input":"2021-05-22T19:47:48.984087Z","iopub.status.idle":"2021-05-22T19:47:49.030819Z","shell.execute_reply.started":"2021-05-22T19:47:48.984055Z","shell.execute_reply":"2021-05-22T19:47:49.030011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ActorCritic(tf.keras.Model):\n    \"\"\"Combined actor-critic network.\"\"\"\n\n    def __init__(\n        self, \n        num_actions: int = 4):\n        \"\"\"Initialize.\"\"\"\n        super().__init__()\n        \n        self.conv1 = tf.keras.layers.Conv2D(filters=128, kernel_size=(3,5))\n        self.bn1 = tf.keras.layers.BatchNormalization()\n        self.conv2 = tf.keras.layers.Conv2D(filters=256, kernel_size=(3,3))\n        self.bn2 = tf.keras.layers.BatchNormalization()\n        self.conv3 = tf.keras.layers.Conv2D(filters=128, kernel_size=(3,3))\n        self.bn3 = tf.keras.layers.BatchNormalization()\n        \n        self.flatter = tf.keras.layers.Flatten()\n        \n        self.fc1 = tf.keras.layers.Dense(128)\n        self.bn4 = tf.keras.layers.BatchNormalization()\n\n        self.actor = tf.keras.layers.Dense(num_actions)\n        self.critic = tf.keras.layers.Dense(1)\n        \n        \n        self.relu = tf.keras.layers.ReLU()\n\n    def call(self, inputs: tf.Tensor) -> Tuple[tf.Tensor, tf.Tensor]:\n        x = self.relu (self.bn1(self.conv1(inputs)))\n        x = self.relu (self.bn2(self.conv2(x)))\n        x = self.relu (self.bn3(self.conv3(x)))\n        x = self.flatter(x)\n        common = self.relu (self.bn4(self.fc1(x)))\n        \n        return self.actor(common), self.critic(common)","metadata":{"execution":{"iopub.status.busy":"2021-05-22T19:47:49.362625Z","iopub.execute_input":"2021-05-22T19:47:49.363162Z","iopub.status.idle":"2021-05-22T19:47:49.375087Z","shell.execute_reply.started":"2021-05-22T19:47:49.36313Z","shell.execute_reply":"2021-05-22T19:47:49.374167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = ActorCritic()\nmodel.build(input_shape=(None, 7, 11, 17))\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2021-05-22T19:47:49.74533Z","iopub.execute_input":"2021-05-22T19:47:49.746018Z","iopub.status.idle":"2021-05-22T19:47:49.848071Z","shell.execute_reply.started":"2021-05-22T19:47:49.745968Z","shell.execute_reply":"2021-05-22T19:47:49.846997Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def centerize(b):\n    dy, dx = np.where(b[0])\n    if len(dy) == 0 and len(dx) == 0:\n        return b\n    centerize_y = (np.arange(0,7)-3+dy[0])%7\n    centerize_x = (np.arange(0,11)-5+dx[0])%11\n    \n    b = b[:, centerize_y,:]\n    b = b[:, :,centerize_x]\n    \n    return b\n\ndef make_state(obs, prev_obs):\n#     print(obs)\n    b = np.zeros((17, 7 * 11), dtype=np.float32)\n#     obs = obses[-1]\n\n    for p, pos_list in enumerate(obs['geese']):\n        # head position\n        for pos in pos_list[:1]:\n            b[0 + (p - obs['index']) % 4, pos] = 1\n        # tip position\n        for pos in pos_list[-1:]:\n            b[4 + (p - obs['index']) % 4, pos] = 1\n        # whole position\n        for pos in pos_list:\n            b[8 + (p - obs['index']) % 4, pos] = 1\n            \n    # previous head position\n    if prev_obs:\n#         print(prev_obs)\n#         prev_geese = prev_geese.numpy()\n#         obs_prev = obses[-2]\n        for p, pos_list in enumerate(prev_obs['geese']):\n            for pos in pos_list[:1]:\n                b[12 + (p - obs['index']) % 4, pos] = 1\n\n    # food\n    for pos in obs['food']:\n        b[16, pos] = 1\n        \n    b = b.reshape(-1, 7, 11)\n    b = centerize(b)\n    b = np.transpose(b, (1,2,0))\n\n    return b","metadata":{"execution":{"iopub.status.busy":"2021-05-22T19:47:50.117604Z","iopub.execute_input":"2021-05-22T19:47:50.118007Z","iopub.status.idle":"2021-05-22T19:47:50.131996Z","shell.execute_reply.started":"2021-05-22T19:47:50.11797Z","shell.execute_reply":"2021-05-22T19:47:50.130721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class RLAgent:\n    def __init__(self, net, stochastic):\n        self.prev_obs = None\n        self.net = net\n        self.stochastic = stochastic\n\n    def raw_outputs(self, state):\n        state = tf.expand_dims(state, 0)\n        logits, values = self.net(state, training=False)\n        logits = tf.nn.softmax(logits)\n        \n        logits = tf.squeeze(logits)\n        values = tf.squeeze(values)\n        if self.stochastic:\n            # get probabilities\n#             probs = tf.math.exp(logits)\n#             probs = tf.nn.softmax(logits).numpy()\n#             # convert 2 numpy\n            probs = logits.numpy()\n#             probs /= probs.sum()\n            \n#             print(probs)\n\n            action = np.random.choice(range(4), p=probs) \n#             action = tf.random.categorical(logits, 1)[0, 0]\n#             logits = np.squeeze(logits)\n#             print(f\"act: {action}\")\n        else:\n#             logits = np.squeeze(logits)\n#             probs = tf.nn.softmax(logits)\n            action = tf.math.argmax(logits)\n#             print(f'Act2 : {action}')\n#         logits = np.squeeze(logits)\n        return action, logits[action], values\n\n    def __call__(self, observation, configuration):\n        if observation['step'] == 0:\n            self.prev_obs = None\n        state = make_state(observation, self.prev_obs)\n        action, _, _ = self.raw_outputs(state)\n        self.prev_obs =  observation\n        return ACTIONS[action]","metadata":{"execution":{"iopub.status.busy":"2021-05-22T19:47:50.49247Z","iopub.execute_input":"2021-05-22T19:47:50.492853Z","iopub.status.idle":"2021-05-22T19:47:50.502207Z","shell.execute_reply.started":"2021-05-22T19:47:50.492806Z","shell.execute_reply":"2021-05-22T19:47:50.501144Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = ActorCritic()\nmodel.build(input_shape=(None, 7, 11, 17))\n\nplayer = RLAgent(model, True)\n\nopponent = RLAgent(model, True)\n\nplayers = [None]+[opponent]*3\n\ntrainer = env.train(players)\nobs = trainer.reset()\n\nplayer.raw_outputs(make_state(obs, obs))","metadata":{"execution":{"iopub.status.busy":"2021-05-22T19:47:50.872744Z","iopub.execute_input":"2021-05-22T19:47:50.873226Z","iopub.status.idle":"2021-05-22T19:47:51.001114Z","shell.execute_reply.started":"2021-05-22T19:47:50.873188Z","shell.execute_reply":"2021-05-22T19:47:51.000023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# trainer.reset()\n# for x in range(30):\n#     obs, r, d, i = trainer.step(ACTIONS[1])\n#     if d:\n#         display(x)\n#         break\n#     env.render(mode=\"ipython\", width=500, height=450)","metadata":{"execution":{"iopub.status.busy":"2021-05-22T19:47:51.323216Z","iopub.execute_input":"2021-05-22T19:47:51.32359Z","iopub.status.idle":"2021-05-22T19:47:51.32797Z","shell.execute_reply.started":"2021-05-22T19:47:51.323554Z","shell.execute_reply":"2021-05-22T19:47:51.326452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_rank(obs, prev_obs):\n    geese = obs['geese']\n    index = obs['index']\n    player_len = len(geese[index])\n    survivors = [i for i in range(len(geese)) if len(geese[i]) > 0]\n    if index in survivors: # if our player survived in the end, its rank is given by its length in the last state\n        return sum(len(x) >= player_len for x in geese) # 1 is the best, 4 is the worst\n    # if our player is dead, consider lengths in penultimate state\n    geese = prev_obs['geese']\n    index = prev_obs['index']\n    player_len = len(geese[index])\n    rank_among_lost = sum(len(x) >= player_len for i, x in enumerate(geese) if i not in survivors)\n    return rank_among_lost + len(survivors)\n    \ndef get_rewards(env_reward, obs, prev_obs, done):\n    geese = prev_obs['geese']\n    index = prev_obs['index']\n    step  = prev_obs['step']\n    if done:\n        rank = get_rank(obs, prev_obs)\n        r = (200, -0.25*200, -0.75*200, -1*200)[rank - 1]\n        died_from_hunger = ((step + 1) % 40 == 0) and (len(geese[index]) == 1)\n        r += -200 if died_from_hunger else 0 # int(rank == 1) # huge penalty for dying from hunger and huge award for the win\n    else:\n        if step == 0:\n            env_reward -= 1 # somehow initial step is a special case\n#         r1 = 0\n        r = max(0.5 * (env_reward - 1), 0) # food reward\n    return r","metadata":{"execution":{"iopub.status.busy":"2021-05-22T19:47:51.841006Z","iopub.execute_input":"2021-05-22T19:47:51.841397Z","iopub.status.idle":"2021-05-22T19:47:51.853179Z","shell.execute_reply.started":"2021-05-22T19:47:51.841365Z","shell.execute_reply":"2021-05-22T19:47:51.851859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def greedy_action(model, observation, state):\n    # Explore\n    g_agent = GreedyAgent(Configuration({'rows': 7, 'columns': 11}))\n    action = g_agent(Observation(observation))\n\n    action = ACTIONS.index(action)\n    \n    state = tf.expand_dims(state, 0)\n    logits, values = model(state, training=False)\n    logits = tf.nn.softmax(logits)\n    \n    logits = tf.squeeze(logits)\n    values = tf.squeeze(values)\n    \n    return action, logits[action], values\n    ","metadata":{"execution":{"iopub.status.busy":"2021-05-22T19:47:52.405508Z","iopub.execute_input":"2021-05-22T19:47:52.405881Z","iopub.status.idle":"2021-05-22T19:47:52.413243Z","shell.execute_reply.started":"2021-05-22T19:47:52.405847Z","shell.execute_reply":"2021-05-22T19:47:52.411992Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def run_episode(\n    model,\n    player,\n    env,\n    players,\n    data_buffer,\n    gamma,\n    episode\n    ):\n    \"\"\"Runs a single episode to collect training data.\"\"\"\n    \n    GREEDY_EPISODES = 50\n    if episode < GREEDY_EPISODES:\n        trainer = env.train([None, 'greedy', 'greedy', 'greedy'])\n    else:\n        shuffle(players)\n        trainer = env.train(players)\n        \n    observation = trainer.reset()\n\n    done = False\n    \n    initial_state = make_state(observation, None)\n\n    initial_state_shape = initial_state.shape\n    state = initial_state\n    \n    episod_rewards = []\n    episod_dones = []\n   \n    while not done:\n        prev_observation = observation\n        \n#         print(observation)\n        \n        if episode < GREEDY_EPISODES:\n            action, logit, value = greedy_action(model, observation, state)\n        else:\n            action, logit, value = player.raw_outputs(state)\n            \n\n        \n        data_buffer['states'].append(state)\n        data_buffer['logits'].append(logit)\n        data_buffer['values'].append(value)\n        data_buffer['actions'].append(action)\n\n        # Apply action to the environment to get next state and reward\n        observation, reward, done, _ = trainer.step(ACTIONS[action])\n        reward = get_rewards(reward, observation, prev_observation, done)\n        \n        episod_rewards.append(reward)\n        episod_dones.append(done)\n        \n        state = make_state(observation, prev_observation)\n    \n    data_buffer['rewards'] += episod_rewards\n    data_buffer['returns'] += get_expected_return(episod_rewards, gamma)\n    data_buffer['dones'] += episod_dones\n        \n    return data_buffer","metadata":{"execution":{"iopub.status.busy":"2021-05-22T19:47:52.986346Z","iopub.execute_input":"2021-05-22T19:47:52.986852Z","iopub.status.idle":"2021-05-22T19:47:52.997704Z","shell.execute_reply.started":"2021-05-22T19:47:52.986809Z","shell.execute_reply":"2021-05-22T19:47:52.996814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def collect_data(\n    model,\n    env,\n    min_data_threshold,\n    gamma,\n    episode,\n                ):\n    data_buffer = {\n        'states': [],\n        'actions': [],\n        'logits': [],\n        'values': [],\n        'rewards': [],\n        'dones' : [],\n        'returns': [],\n    }\n    \n    player = RLAgent(model, stochastic=True)\n    opponents = [RLAgent(model, stochastic=False) for _ in range(3)]\n    \n    while len(data_buffer['states']) < min_data_threshold :\n        run_episode(model, player, env, players=[None] + opponents,\n                                  data_buffer=data_buffer, gamma = gamma, episode = episode)\n        \n#         for key, values in data_buffer.items():\n#             print(f\" || {key}: {len(values)}\", end='')\n#         print()\n    \n    return data_buffer\n        ","metadata":{"execution":{"iopub.status.busy":"2021-05-22T19:47:53.610746Z","iopub.execute_input":"2021-05-22T19:47:53.611189Z","iopub.status.idle":"2021-05-22T19:47:53.619109Z","shell.execute_reply.started":"2021-05-22T19:47:53.611153Z","shell.execute_reply":"2021-05-22T19:47:53.61775Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_expected_return(\n    rewards,\n    gamma: float, \n    standardize: bool = False) :\n    \"\"\"Compute expected returns per timestep.\"\"\"\n\n    returns = []\n\n    # Start from the end of `rewards` and accumulate reward sums\n    # into the `returns` array\n    \n    discounted_sum = 0.0\n    for reward in rewards[::-1]:\n        discounted_sum = reward + gamma * discounted_sum\n        returns.append(discounted_sum)\n    \n    returns = np.array(returns[::-1])\n    if standardize:\n        returns = ((np.array(returns) - np.mean(returns)) / \n                   (np.std(returns) + eps))\n\n    return returns.tolist()","metadata":{"execution":{"iopub.status.busy":"2021-05-22T19:47:54.205041Z","iopub.execute_input":"2021-05-22T19:47:54.205404Z","iopub.status.idle":"2021-05-22T19:47:54.212185Z","shell.execute_reply.started":"2021-05-22T19:47:54.205373Z","shell.execute_reply":"2021-05-22T19:47:54.211062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"huber_loss = tf.keras.losses.Huber(reduction=tf.keras.losses.Reduction.SUM)\n\ndef compute_loss(\n    action_probs: tf.Tensor,\n    action_probs_old: tf.Tensor,\n    values: tf.Tensor,  \n    advantage: tf.Tensor,\n    action_indices: tf.Tensor,\n    returns:tf.Tensor,\n    actions_one_hot:tf.Tensor,\n    clip_ratio: float = 1e-30) -> tf.Tensor:\n    \"\"\"Computes the combined actor-critic loss.\"\"\"\n    \n#     adv = returns - values\n#     action_indices = tf.cast(action_indices, tf.int32)\n#     logp = tf.gather_nd(action_probs,action_indices)\n    \n#     ratio = tf.math.log((logp - action_probs_old)+eps)\n#     ratio = logp - action_probs_old\n#     ratio = tf.clip_by_value(logp, 1e-12, 1e12)\n#     ratio = tf.math.log(ratio)\n    \n#     clip_adv = tf.clip_by_value(ratio, 1 - clip_ratio, 1 + clip_ratio) * advantage\n    \n#     actor_loss = - tf.math.reduce_mean(tf.math.minimum(ratio * advantage, clip_adv))\n    \n    \n#     actor_loss = - tf.math.reduce_mean(ratio * adv)\n    \n#     actor_loss = tf.keras.losses.CategoricalCrossentropy()(actions_one_hot, action_probs)\n\n#     critic_loss = huber_loss(values, returns)\n\n    advantage = returns - values\n#     print(advantage)\n\n    action_log_probs = tf.math.log(action_probs)\n    actor_loss = - tf.math.reduce_sum(action_log_probs * advantage)\n\n    critic_loss = huber_loss(values, returns)\n\n    return  critic_loss + actor_loss\n\nclass GeeseLoss(tf.keras.losses.Loss):\n    def call(self, y_true, y_pred):\n        logits = y_pred[0]\n        values = y_pred[1]\n        \n        logit_old = y_true[0]\n#         values_old = y_true[0]\n        advantage = y_true[1][0]\n        \n        actions = y_true[1][1]\n        returns = y_true[1][2]\n        \n        return compute_loss(logits, logit_old, values, advantage, actions, returns)","metadata":{"execution":{"iopub.status.busy":"2021-05-22T19:47:54.778981Z","iopub.execute_input":"2021-05-22T19:47:54.779357Z","iopub.status.idle":"2021-05-22T19:47:54.789423Z","shell.execute_reply.started":"2021-05-22T19:47:54.779325Z","shell.execute_reply":"2021-05-22T19:47:54.78846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def model_fit(X, Y, batch_size, epochs, optimizer):\n    train_dataset = tf.data.Dataset.from_tensor_slices((X, Y))\n    train_dataset = train_dataset.batch(batch_size, drop_remainder=True)\n#     drop_remainder=True\n    \n    for epoch in range(epochs):\n#         print(\"\\nStart of epoch %d\" % (epoch,))\n        lossess = 0.0\n\n        # Iterate over the batches of the dataset.\n        for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n            \n            logits_old, advantages, actions , returns = y_batch_train\n#             print(actions)\n            action_indices = [[i, a] for i,a in enumerate(actions)]\n            action_one_hot=tf.one_hot(actions, 4)\n#             returns = tf.expand_dims(returns, 0)\n#             print(actions)\n#             print(returns)\n            \n#             print(np.sum(x_batch_train))\n            \n            # Open a GradientTape to record the operations run\n            # during the forward pass, which enables auto-differentiation.\n            with tf.GradientTape() as tape:\n\n                # Run the forward pass of the layer.\n                # The operations that the layer applies\n                # to its inputs are going to be recorded\n                # on the GradientTape.\n                logits, values = model(x_batch_train, training=True)  # Logits for this minibatch\n                logits = tf.nn.softmax(logits)\n                values = tf.squeeze(values)\n                \n#                 print(values)\n                \n                action_indices = tf.cast(action_indices, tf.int32)\n                logp = tf.gather_nd(logits,action_indices)\n                \n                action_probs, values, returns = [\n                        tf.expand_dims(x, 1) for x in [logp, values, returns]] \n                    \n#                 print(action_probs.shape)\n#                 print(values.shape)\n#                 print(returns.shape)\n                \n                \n#                 values = tf.squeeze(values)\n                \n                # Compute the loss value for this minibatch.\n                loss_value = compute_loss(action_probs, logits_old, values,  advantages, action_indices, returns, action_one_hot)\n\n            # Use the gradient tape to automatically retrieve\n            # the gradients of the trainable variables with respect to the loss.\n            grads = tape.gradient(loss_value, model.trainable_weights)\n\n            # Run one step of gradient descent by updating\n            # the value of the variables to minimize the loss.\n            optimizer.apply_gradients(zip(grads, model.trainable_weights))\n            \n            lossess += float(loss_value)\n#             print(values)\n#             print(returns)\n#             print(action_probs)\n\n            # Log every 200 batches.\n#             if step % 1 == 0:\n#                 print(\n#                     \"Training loss (for one batch) at step %d: %.4f\"\n#                     % (step, float(loss_value))\n#                 )\n#                 print(\"Seen so far: %s samples\" % ((step + 1) * batch_size))\n        \n        print(f'Epoch: {epoch} || losses : {lossess}')","metadata":{"execution":{"iopub.status.busy":"2021-05-22T19:47:55.360805Z","iopub.execute_input":"2021-05-22T19:47:55.361183Z","iopub.status.idle":"2021-05-22T19:47:55.372375Z","shell.execute_reply.started":"2021-05-22T19:47:55.361151Z","shell.execute_reply":"2021-05-22T19:47:55.37156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_model(data_buffer, model, optimizer):\n    X = np.array(data_buffer['states'])\n    advantage = np.array(data_buffer['returns']) - np.array(data_buffer['values'])\n    \n#     action_indices = [[i, a] for i,a in enumerate(data_buffer['actions'])]\n    \n    Y = ( np.array(data_buffer['logits'], dtype=np.float32),\n         np.array(advantage,  dtype=np.float32),\n         np.array(data_buffer['actions'], dtype=np.int32),\n         np.array(data_buffer['returns'], dtype=np.float32))\n    \n#     print(Y)\n    \n    batch_size = 256\n    epochs = 2\n    \n    model_fit(X, Y, batch_size, epochs, optimizer)\n    \n    \n#     model.fit(X, Y, batch_size=batch_size, verbose=1, shuffle=True, epochs=1)\n         \n         \n    ","metadata":{"execution":{"iopub.status.busy":"2021-05-22T19:47:55.932401Z","iopub.execute_input":"2021-05-22T19:47:55.933075Z","iopub.status.idle":"2021-05-22T19:47:55.939636Z","shell.execute_reply.started":"2021-05-22T19:47:55.933035Z","shell.execute_reply":"2021-05-22T19:47:55.938823Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n\n\ndef train_step(\n    model: tf.keras.Model,\n    optimizer,\n    env,\n    gamma: float, \n    min_data_threshold: int,\n    episode: int) -> tf.Tensor:\n    \"\"\"Runs a model training step.\"\"\"\n\n    \n    data_buffer = collect_data(model, env, min_data_threshold, gamma, episode)\n    \n#     for key, values in data_buffer.items():\n#         print(f\" || {key}: {len(values)}\", end='')\n#     print()\n    \n    train_model(data_buffer, model, optimizer)\n    \n#     print(data_buffer['rewards'])\n    episode_reward = np.sum(data_buffer['rewards'])\n\n    return episode_reward","metadata":{"execution":{"iopub.status.busy":"2021-05-22T19:47:56.502936Z","iopub.execute_input":"2021-05-22T19:47:56.503453Z","iopub.status.idle":"2021-05-22T19:47:56.508796Z","shell.execute_reply.started":"2021-05-22T19:47:56.503422Z","shell.execute_reply":"2021-05-22T19:47:56.507996Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\nmin_episodes_criterion = 1000\nmax_episodes = 1000\nmax_steps_per_episode = 100\nmin_data_threshold = 1030\n\n# Cartpole-v0 is considered solved if average reward is >= 195 over 100 \n# consecutive trials\nreward_threshold = 1000\nrunning_reward = 0\n\n# Discount factor for future rewards\ngamma = 0.8\noptimizer=tf.keras.optimizers.Adam(learning_rate=1e-5)\n# optimizer=tf.keras.optimizers.SGD(learning_rate=0.001)\n\nmodel = ActorCritic(4)\n# model.compile(loss=GeeseLoss(), optimizer=tf.keras.optimizers.Adam(lr=0.0004), metrics=None)\n# Keep last episodes reward\nepisodes_reward: collections.deque = collections.deque(maxlen=min_episodes_criterion)\n\nwith tqdm.trange(max_episodes) as t:\n  for i in t:\n#     initial_state = make_state(trainer.reset())\n#     initial_state = tf.constant(initial_state, dtype=tf.float32)\n    episode_reward = int(train_step(model,optimizer, env, gamma, min_data_threshold, episode=i))\n\n    episodes_reward.append(episode_reward)\n    running_reward = statistics.mean(episodes_reward)\n\n    t.set_description(f'Episode {i}')\n    t.set_postfix(\n        episode_reward=episode_reward, running_reward=running_reward)\n\n    # Show average episode reward every 10 episodes\n    if i % 50 == 0:\n        model.save('my_model', save_format='tf')\n#       pass # print(f'Episode {i}: average reward: {avg_reward}')\n\n    if running_reward > reward_threshold and i >= min_episodes_criterion:  \n        break\n\nprint(f'\\nSolved at episode {i}: average reward: {running_reward:.2f}!')","metadata":{"execution":{"iopub.status.busy":"2021-05-22T19:47:57.07831Z","iopub.execute_input":"2021-05-22T19:47:57.078843Z","iopub.status.idle":"2021-05-22T20:18:17.970375Z","shell.execute_reply.started":"2021-05-22T19:47:57.078811Z","shell.execute_reply":"2021-05-22T20:18:17.969315Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.save('my_model', save_format='tf')\n# model = keras.models.load_model('my_model')\n","metadata":{"execution":{"iopub.status.busy":"2021-05-22T20:18:17.972239Z","iopub.execute_input":"2021-05-22T20:18:17.972654Z","iopub.status.idle":"2021-05-22T20:18:20.243681Z","shell.execute_reply.started":"2021-05-22T20:18:17.972609Z","shell.execute_reply":"2021-05-22T20:18:20.242603Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"env.reset()\nplayers = [RLAgent(model, False) for _ in range(4)]\nenv.run(players)\nenv.render(mode=\"ipython\", width=500, height=450)","metadata":{"execution":{"iopub.status.busy":"2021-05-22T20:18:20.258184Z","iopub.execute_input":"2021-05-22T20:18:20.258578Z","iopub.status.idle":"2021-05-22T20:18:29.49428Z","shell.execute_reply.started":"2021-05-22T20:18:20.258547Z","shell.execute_reply":"2021-05-22T20:18:29.492099Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"a = np.array([[0.1, 0.4, 0.3,0.2]])\nx = tf.random.categorical(a, 1)\nprint(x)\nprint(x[0,0])","metadata":{"execution":{"iopub.status.busy":"2021-05-22T20:18:29.497099Z","iopub.execute_input":"2021-05-22T20:18:29.497704Z","iopub.status.idle":"2021-05-22T20:18:29.510479Z","shell.execute_reply.started":"2021-05-22T20:18:29.49763Z","shell.execute_reply":"2021-05-22T20:18:29.50829Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"a = tf.convert_to_tensor(np.array([[0,2], [1,0], [2,3]]))\nb = tf.convert_to_tensor(np.array([[1,2,0.3,4], [4,5,6,7], [8,9,10,11]]))\n\nc = tf.gather_nd(b, a)\nprint(c-c)","metadata":{"execution":{"iopub.status.busy":"2021-05-22T20:18:29.512642Z","iopub.execute_input":"2021-05-22T20:18:29.513209Z","iopub.status.idle":"2021-05-22T20:18:29.530779Z","shell.execute_reply.started":"2021-05-22T20:18:29.513171Z","shell.execute_reply":"2021-05-22T20:18:29.529032Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}