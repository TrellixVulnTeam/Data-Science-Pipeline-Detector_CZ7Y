{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Simulations Episode Scraper Match Downloader","metadata":{}},{"cell_type":"markdown","source":"This notebook downloads episodes using Kaggle's GetEpisodeReplay API and the [Meta Kaggle](https://www.kaggle.com/kaggle/meta-kaggle) dataset.\n\nMeta Kaggle is refreshed daily, and sometimes fails a daily refresh. That's OK, Goose keeps well for 24hr.\n\nWhy download replays?\n- Train your ML/RL model\n- Inspect the performance of yours and others agents\n- To add to your ever growing json collection \n\nOnly one scraping strategy is implemented: For each top scoring submission, download all missing matches, move on to next submission.\n\nOther scraping strategies can be implemented, but not here. Like download max X matches per submission or per team per day, or ignore certain teams or ignore where some scores < X, or only download some teams.\n\nPlease let me know of any bugs. It's new, and my goose may be cooked.\n\nTodo:\n- Add teamid's once meta kaggle add them (a few days away)","metadata":{}},{"cell_type":"code","source":"!find ../input/fork-of-simulations-episode-scraper-match-download/ -name \"*.json\" -print0 | xargs -0 -I {} cp {} ./","metadata":{"execution":{"iopub.status.busy":"2021-07-25T15:05:25.470554Z","iopub.execute_input":"2021-07-25T15:05:25.470975Z","iopub.status.idle":"2021-07-25T15:09:57.506273Z","shell.execute_reply.started":"2021-07-25T15:05:25.470885Z","shell.execute_reply":"2021-07-25T15:09:57.504345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os\nimport requests\nimport json\nimport datetime\nimport time\nimport collections","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-07-25T15:09:57.508829Z","iopub.execute_input":"2021-07-25T15:09:57.509198Z","iopub.status.idle":"2021-07-25T15:09:57.515151Z","shell.execute_reply.started":"2021-07-25T15:09:57.509163Z","shell.execute_reply":"2021-07-25T15:09:57.514244Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## You should configure these to your needs. Choose one of ...\n# 'hungry-geese', 'rock-paper-scissors', santa-2020', 'halite', 'google-football'\nCOMP = 'hungry-geese'\nMAX_CALLS_PER_DAY = 3600 # Kaggle says don't do more than 3600 per day and 1 per second\nLOWEST_SCORE_THRESH = 1220","metadata":{"execution":{"iopub.status.busy":"2021-07-25T15:09:57.521921Z","iopub.execute_input":"2021-07-25T15:09:57.522337Z","iopub.status.idle":"2021-07-25T15:09:57.532477Z","shell.execute_reply.started":"2021-07-25T15:09:57.522308Z","shell.execute_reply":"2021-07-25T15:09:57.531313Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ROOT =\"../working/\"\nMETA = \"../input/meta-kaggle/\"\nMATCH_DIR = '../working/'\nbase_url = \"https://www.kaggle.com/requests/EpisodeService/\"\nget_url = base_url + \"GetEpisodeReplay\"\nBUFFER = 1\nCOMPETITIONS = {\n    'hungry-geese': 25401,\n    'rock-paper-scissors': 22838,\n    'santa-2020': 24539,\n    'halite': 18011,\n    'google-football': 21723\n}","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-07-25T15:09:57.533785Z","iopub.execute_input":"2021-07-25T15:09:57.534235Z","iopub.status.idle":"2021-07-25T15:09:57.546377Z","shell.execute_reply.started":"2021-07-25T15:09:57.534204Z","shell.execute_reply":"2021-07-25T15:09:57.545125Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load Episodes\nepisodes_df = pd.read_csv(META + \"Episodes.csv\")\n\n# Load EpisodeAgents\nepagents_df = pd.read_csv(META + \"EpisodeAgents.csv\")\n\nprint(f'Episodes.csv: {len(episodes_df)} rows')\nprint(f'EpisodeAgents.csv: {len(epagents_df)} rows')","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-07-25T15:09:57.547933Z","iopub.execute_input":"2021-07-25T15:09:57.548273Z","iopub.status.idle":"2021-07-25T15:10:38.407942Z","shell.execute_reply.started":"2021-07-25T15:09:57.548242Z","shell.execute_reply":"2021-07-25T15:10:38.40695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get top scoring submissions# Get top scoring submissions\nmax_df = (epagents_df.sort_values(by=['EpisodeId'], ascending=False).groupby('SubmissionId').head(1).drop_duplicates().reset_index(drop=True))\nmax_df = max_df[max_df.UpdatedScore>=LOWEST_SCORE_THRESH]\nmax_df = pd.merge(left=episodes_df, right=max_df, left_on='Id', right_on='EpisodeId')\nsub_to_score_top = pd.Series(max_df.UpdatedScore.values,index=max_df.SubmissionId).to_dict()\nprint(f'{len(sub_to_score_top)} submissions with score over {LOWEST_SCORE_THRESH}')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-07-25T15:10:38.409436Z","iopub.execute_input":"2021-07-25T15:10:38.409753Z","iopub.status.idle":"2021-07-25T15:10:43.803064Z","shell.execute_reply.started":"2021-07-25T15:10:38.409723Z","shell.execute_reply":"2021-07-25T15:10:43.801784Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get episodes for these submissions\nsub_to_episodes = collections.defaultdict(list)\nfor key, value in sorted(sub_to_score_top.items(), key=lambda kv: kv[1], reverse=True):\n    eps = sorted(epagents_df[epagents_df['SubmissionId'].isin([key])]['EpisodeId'].values,reverse=True)\n    sub_to_episodes[key] = eps\ncandidates = len(set([item for sublist in sub_to_episodes.values() for item in sublist]))\nprint(f'{candidates} episodes for these {len(sub_to_score_top)} submissions')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-07-25T15:10:43.804655Z","iopub.execute_input":"2021-07-25T15:10:43.804974Z","iopub.status.idle":"2021-07-25T15:10:47.863527Z","shell.execute_reply.started":"2021-07-25T15:10:43.804947Z","shell.execute_reply":"2021-07-25T15:10:47.862341Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"global num_api_calls_today\nnum_api_calls_today = 0\nall_files = []\nfor root, dirs, files in os.walk(MATCH_DIR, topdown=False):\n    all_files.extend(files)\nseen_episodes = [int(f.split('.')[0]) for f in all_files \n                      if '.' in f and f.split('.')[0].isdigit() and f.split('.')[1] == 'json']\nremaining = np.setdiff1d([item for sublist in sub_to_episodes.values() for item in sublist],seen_episodes)\nprint(f'{len(remaining)} of these {candidates} episodes not yet saved')\nprint('Total of {} games in existing library'.format(len(seen_episodes)))","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-07-25T15:10:47.864873Z","iopub.execute_input":"2021-07-25T15:10:47.865198Z","iopub.status.idle":"2021-07-25T15:10:48.01104Z","shell.execute_reply.started":"2021-07-25T15:10:47.865169Z","shell.execute_reply":"2021-07-25T15:10:48.010092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def saveEpisode(epid):\n    # request\n    re = requests.post(get_url, json = {\"EpisodeId\": int(epid)})\n        \n    # save replay\n    with open(MATCH_DIR + '{}.json'.format(epid), 'w') as f:\n        f.write(re.json()['result']['replay'])","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-06-03T12:05:46.617232Z","iopub.execute_input":"2021-06-03T12:05:46.617556Z","iopub.status.idle":"2021-06-03T12:05:46.622773Z","shell.execute_reply.started":"2021-06-03T12:05:46.617529Z","shell.execute_reply":"2021-06-03T12:05:46.621852Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"r = BUFFER;\n\nstart_time = datetime.datetime.now()\nse=0\nfor key, value in sorted(sub_to_score_top.items(), key=lambda kv: kv[1], reverse=True):\n    if num_api_calls_today<=MAX_CALLS_PER_DAY:\n        print('')\n        remaining = sorted(np.setdiff1d(sub_to_episodes[key],seen_episodes), reverse=True)\n        print(f'submission={key}, LB={\"{:.0f}\".format(value)}, matches={len(set(sub_to_episodes[key]))}, still to save={len(remaining)}')\n        \n        for epid in remaining:\n            if epid not in seen_episodes and num_api_calls_today<=MAX_CALLS_PER_DAY:\n                saveEpisode(epid); \n                r+=1;\n                se+=1\n                try:\n                    size = os.path.getsize(MATCH_DIR+'{}.json'.format(epid)) / 1e6\n                    print(str(num_api_calls_today) + f': saved episode #{epid}')\n                    seen_episodes.append(epid)\n                    num_api_calls_today+=1\n                except:\n                    print('  file {}.json did not seem to save'.format(epid))    \n                if r > (datetime.datetime.now() - start_time).seconds:\n                    time.sleep( r - (datetime.datetime.now() - start_time).seconds)\n            if num_api_calls_today>(min(3600,MAX_CALLS_PER_DAY)):\n                break\nprint('')\nprint(f'Episodes saved: {se}')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-06-03T12:05:46.623891Z","iopub.execute_input":"2021-06-03T12:05:46.624258Z","iopub.status.idle":"2021-06-03T12:07:46.734794Z","shell.execute_reply.started":"2021-06-03T12:05:46.624224Z","shell.execute_reply":"2021-06-03T12:07:46.732682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}