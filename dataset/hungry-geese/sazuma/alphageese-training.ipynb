{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport os\nimport random\nfrom tqdm.notebook import tqdm\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.optim as optim","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-24T13:54:26.593496Z","iopub.execute_input":"2021-07-24T13:54:26.593774Z","iopub.status.idle":"2021-07-24T13:54:27.643984Z","shell.execute_reply.started":"2021-07-24T13:54:26.593706Z","shell.execute_reply":"2021-07-24T13:54:27.643145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def seed_everything(seed_value):\n    random.seed(seed_value)\n    np.random.seed(seed_value)\n    torch.manual_seed(seed_value)\n    os.environ['PYTHONHASHSEED'] = str(seed_value)\n    \n    if torch.cuda.is_available(): \n        torch.cuda.manual_seed(seed_value)\n        torch.cuda.manual_seed_all(seed_value)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = True\n\nseed = 42\nseed_everything(seed)","metadata":{"execution":{"iopub.status.busy":"2021-07-24T13:54:27.645638Z","iopub.execute_input":"2021-07-24T13:54:27.64599Z","iopub.status.idle":"2021-07-24T13:54:27.708724Z","shell.execute_reply.started":"2021-07-24T13:54:27.645953Z","shell.execute_reply":"2021-07-24T13:54:27.707956Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class GeeseDataset(Dataset):\n    def __init__(self, states, actions, rewards):\n        self.states = states\n        self.actions = actions\n        self.rewards = rewards\n        \n    def __len__(self):\n        return len(self.states)\n\n    def __getitem__(self, idx):\n        state, action, reward = self.states[idx], self.actions[idx], self.rewards[idx]\n        # vertical flip\n        if torch.rand(1) > 0.5:\n            state = np.flip(state, axis=2).copy()\n            action = action[[0,1,3,2]]\n\n        # horizontal flip\n        if torch.rand(1) > 0.5:\n            state = np.flip(state, axis=1).copy()\n            action = action[[1,0,2,3]]\n            \n        action = action.argmax()\n        \n        return state, action, reward","metadata":{"execution":{"iopub.status.busy":"2021-07-24T13:54:27.710429Z","iopub.execute_input":"2021-07-24T13:54:27.710893Z","iopub.status.idle":"2021-07-24T13:54:27.718612Z","shell.execute_reply.started":"2021-07-24T13:54:27.710846Z","shell.execute_reply":"2021-07-24T13:54:27.717885Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Neural Network for Hungry Geese\nclass TorusConv2d(nn.Module):\n    def __init__(self, input_dim, output_dim, kernel_size, bn):\n        super().__init__()\n        self.edge_size = (kernel_size[0] // 2, kernel_size[1] // 2)\n        self.conv = nn.Conv2d(input_dim, output_dim, kernel_size=kernel_size)\n        self.bn = nn.BatchNorm2d(output_dim) if bn else None\n\n    def forward(self, x):\n        h = torch.cat([x[:,:,:,-self.edge_size[1]:], x, x[:,:,:,:self.edge_size[1]]], dim=3)\n        h = torch.cat([h[:,:,-self.edge_size[0]:], h, h[:,:,:self.edge_size[0]]], dim=2)\n        h = self.conv(h)\n        h = self.bn(h) if self.bn is not None else h\n        return h\n\nclass GeeseNet(nn.Module):\n    def __init__(self):\n        super().__init__()\n        layers, filters = 12, 32\n        self.conv0 = TorusConv2d(17, filters, (3, 3), True)\n        self.blocks = nn.ModuleList([TorusConv2d(filters, filters, (3, 3), True) for _ in range(layers)])\n        self.head_p = nn.Linear(filters, 4, bias=False)\n        self.head_v = nn.Linear(filters * 2, 1, bias=False)\n\n    def forward(self, x):\n        h = F.relu_(self.conv0(x))\n        for block in self.blocks:\n            h = F.relu_(h + block(h))\n        h_head = (h * x[:,:1]).view(h.size(0), h.size(1), -1).sum(-1)\n        h_avg = h.view(h.size(0), h.size(1), -1).mean(-1)\n        p = self.head_p(h_head)\n        v = torch.tanh(self.head_v(torch.cat([h_head, h_avg], 1)))\n        return p, v.squeeze(dim=1)","metadata":{"execution":{"iopub.status.busy":"2021-07-24T13:54:27.720063Z","iopub.execute_input":"2021-07-24T13:54:27.72061Z","iopub.status.idle":"2021-07-24T13:54:27.734862Z","shell.execute_reply.started":"2021-07-24T13:54:27.720571Z","shell.execute_reply":"2021-07-24T13:54:27.734047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_model(num_epochs):\n    dataloader = DataLoader(\n        GeeseDataset(\n            np.load('../input/alphageese-trajectories/states.npy'), \n            np.load('../input/alphageese-trajectories/actions.npy'), \n            np.load('../input/alphageese-trajectories/rewards.npy')\n        ), \n        batch_size=2048, \n        shuffle=True, \n        drop_last=True\n    )\n    model = GeeseNet()\n    model.cuda()\n    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n    data_size = len(dataloader.dataset)\n    \n    for epoch in range(num_epochs):\n        epoch_ploss = 0.0\n        epoch_vloss = 0.0\n        epoch_acc = 0\n\n        for item in tqdm(dataloader, leave=False):\n            states = item[0].cuda().float()\n            actions = item[1].cuda().long()\n            rewards = item[2].cuda().float()\n\n            optimizer.zero_grad()\n\n            policy, value = model(states)\n            policy_loss = F.cross_entropy(policy, actions)\n            value_loss = F.mse_loss(value, rewards)\n            _, preds = torch.max(policy, 1)\n            \n            (policy_loss + value_loss).backward()\n            optimizer.step()\n\n            epoch_ploss += policy_loss.item() * len(policy)\n            epoch_vloss += value_loss.item() * len(policy)\n            epoch_acc += torch.sum(preds == actions.data)\n\n        epoch_ploss = epoch_ploss / data_size\n        epoch_vloss = epoch_vloss / data_size\n        epoch_acc = epoch_acc.double() / data_size\n\n        print('Epoch {}/{} | Policy Loss: {:.4f} | Value Loss: {:.4f} | Acc: {:.4f}'.format(\n            epoch + 1, num_epochs, epoch_ploss, epoch_vloss, epoch_acc))\n        if (epoch + 1) % 5 == 0:\n            torch.save(model.state_dict(), f'alphageese_epoch{epoch + 1}.pth')","metadata":{"execution":{"iopub.status.busy":"2021-07-24T13:54:27.736103Z","iopub.execute_input":"2021-07-24T13:54:27.736516Z","iopub.status.idle":"2021-07-24T13:54:27.74828Z","shell.execute_reply.started":"2021-07-24T13:54:27.736483Z","shell.execute_reply":"2021-07-24T13:54:27.747566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_model(num_epochs=30)","metadata":{"execution":{"iopub.status.busy":"2021-07-24T13:54:27.749489Z","iopub.execute_input":"2021-07-24T13:54:27.749854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}