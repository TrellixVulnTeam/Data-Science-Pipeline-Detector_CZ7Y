{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport sys\nimport PIL.Image\n\nimport tensorflow as tf\nimport logging\n\nfrom sklearn import preprocessing\nimport random\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom kaggle_environments import evaluate, make\nfrom kaggle_environments.envs.hungry_geese.hungry_geese import *","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"seed=123\ntf.compat.v1.set_random_seed(seed)\nsession_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\nsess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\ntf.compat.v1.keras.backend.set_session(sess)\nlogging.disable(sys.maxsize)\nglobal agent_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"env = make(\"hungry_geese\", debug=True)\nenv.run([\"random\",\"random\"])\nenv.render(mode=\"ipython\",width=800, height=600)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"env.configuration","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"env.specification","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"env.specification.reward","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"env.specification.action","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"env.specification.observation","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%writefile simple_agent.py\nbots_stats = {}\n\n\n\nfrom kaggle_environments.envs.hungry_geese.hungry_geese import *\nfrom random import choice, sample\n\n\ndef simple_agent(observation, configuration):\n    observation = Observation(observation)\n    configuration = Configuration(configuration)\n    rows, columns = configuration.rows, configuration.columns\n\n    food = observation.food\n    geese = observation.geese\n    opponents = [\n        goose\n        for index, goose in enumerate(geese)\n        if index != observation.index and len(goose) > 0\n    ]\n\n    # Don't move adjacent to any heads\n    head_adjacent_positions = {\n        opponent_head_adjacent\n        for opponent in opponents\n        for opponent_head in [opponent[0]]\n        for opponent_head_adjacent in adjacent_positions(opponent_head, rows, columns)\n    }\n    # Don't move into any bodies\n    bodies = {position for goose in geese for position in goose[0:-1]}\n    # Don't move into tails of heads that are adjacent to food\n    tails = {\n        opponent[-1]\n        for opponent in opponents\n        for opponent_head in [opponent[0]]\n        if any(\n            adjacent_position in food\n            # Head of opponent is adjacent to food so tail is not safe\n            for adjacent_position in adjacent_positions(opponent_head, rows, columns)\n        )\n    }\n\n    # Move to the closest food\n    position = geese[observation.index][0]\n    actions = {\n        action: min_distance(new_position, food, columns)\n        for action in Action\n        for new_position in [translate(position, action, columns, rows)]\n        if (\n            new_position not in head_adjacent_positions and\n            new_position not in bodies and\n            new_position not in tails\n        )\n    }\n\n    if any(actions):\n        return min(actions, key=actions.get).name\n\n    return random_agent()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"trainer = env.train([None, \"random\"])\nobservation = trainer.reset()\nwhile not env.done:\n    my_action = simple_agent(observation, env.configuration)\n    print(\"My Action\", my_action)\n    observation = trainer.step(my_action)[0]\n    print(\"Reward gained\",observation.geese[0][0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"env.render(mode=\"ipython\",width=800, height=600)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def ActorModel(num_actions,in_):\n    common = tf.keras.layers.Dense(128, activation='tanh')(in_)\n    common = tf.keras.layers.Dense(32, activation='tanh')(common)\n    common = tf.keras.layers.Dense(num_actions, activation='softmax')(common)\n    \n    return common","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def CriticModel(in_):\n    common = tf.keras.layers.Dense(128)(in_)\n    common = tf.keras.layers.ReLU()(common)\n    common = tf.keras.layers.Dense(32)(common)\n    common = tf.keras.layers.ReLU()(common)\n    common = tf.keras.layers.Dense(1)(common)\n    \n    return common","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"input_ = tf.keras.layers.Input(shape=[2, 1,])\nmodel = tf.keras.Model(inputs=input_, outputs=[ActorModel(4,input_),CriticModel(input_)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"optimizer = tf.keras.optimizers.Adam(lr=7e-4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"huber_loss = tf.keras.losses.Huber()\naction_probs_history = []\ncritic_value_history = []\nrewards_history = []\nrunning_reward = 0\nepisode_count = 0\nnum_actions = 4\neps = np.finfo(np.float32).eps.item()\ngamma = 0.99  # Discount factor for past rewards\nenv = make(\"hungry_geese\", debug=True)\ntrainer = env.train([None,\"random\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"le = preprocessing.LabelEncoder()\nlabel_encoded = le.fit_transform(['NORTH', 'EAST', 'SOUTH', 'WEST'])\nlabel_encoded","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"   \n# Will keep track of whether a ship is collecting halite or carrying cargo to a shipyard\ngeese_states = {}\ngeese_ = 0\ndef update_L1():\n    global geese_\n    geese_+=1\n    \n# Returns the commands we send to our ships and shipyards\ndef advanced_agent(obs, config, action):\n    global geese_\n    observation = Observation(obs)\n    configuration = Configuration(config)\n    rows, columns = configuration.rows, configuration.columns\n\n    food = observation.food\n    geese = observation.geese\n    opponents = [\n        goose\n        for index, goose in enumerate(geese)\n        if index != observation.index and len(goose) > 0\n    ]\n    act = le.inverse_transform([action])[0]\n    \n    \n\n\n\n    # Don't move adjacent to any heads\n    head_adjacent_positions = {\n        opponent_head_adjacent\n        for opponent in opponents\n        for opponent_head in [opponent[0]]\n        for opponent_head_adjacent in adjacent_positions(opponent_head, rows, columns)\n    }\n    # Don't move into any bodies\n    bodies = {position for goose in geese for position in goose[0:-1]}\n    # Don't move into tails of heads that are adjacent to food\n    tails = {\n        opponent[-1]\n        for opponent in opponents\n        for opponent_head in [opponent[0]]\n        if any(\n            adjacent_position in food\n            # Head of opponent is adjacent to food so tail is not safe\n            for adjacent_position in adjacent_positions(opponent_head, rows, columns)\n        )\n    }\n\n    # Move to the closest food\n    position = geese[observation.index][0]\n    actions = {\n        action: min_distance(new_position, food, columns)\n        for action in Action\n        for new_position in [translate(position, action, columns, rows)]\n        if (\n            new_position not in head_adjacent_positions and\n            new_position not in bodies and\n            new_position not in tails\n        )\n    }\n    update_L1()\n    if act:\n        return act\n    elif any(actions):\n        return min(actions, key=actions.get).name\n\n    return random_agent()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainer = env.train([None, \"random\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"while not env.done:    \n    state = trainer.reset()\n    episode_reward = 0\n    with tf.GradientTape() as tape:\n        for timestep in range(1,env.configuration.episodeSteps+200):\n            # of the agent in a pop up window.\n            state_ = tf.convert_to_tensor(state.geese)\n            state_ = tf.expand_dims(state_, 0)\n            # Predict action probabilities and estimated future rewards\n            # from environment state\n            action_probs, critic_value = model(state_)\n            critic_value_history.append(critic_value[0, 0])\n            \n            # Sample action from action probability distribution\n            action = np.random.choice(num_actions, p=np.squeeze(action_probs)[0])\n            action_probs_history.append(tf.math.log(action_probs[0,action-1:action]))\n            \n            # Apply the sampled action in our environment\n            action = advanced_agent(state, env.configuration, action)\n            state = trainer.step(action)[0]\n            gain=state.geese[0][0]/5000\n            rewards_history.append(gain)\n            episode_reward += gain\n            \n            if env.done:\n                state = trainer.reset() \n        # Update running reward to check condition for solving\n        running_reward = 0.05 * episode_reward + (1 - 0.05) * running_reward\n\n        # Calculate expected value from rewards\n        # - At each timestep what was the total reward received after that timestep\n        # - Rewards in the past are discounted by multiplying them with gamma\n        # - These are the labels for our critic\n        returns = []\n        discounted_sum = 0\n        for r in rewards_history[::-1]:\n            discounted_sum = r + gamma * discounted_sum\n            returns.insert(0, discounted_sum)\n        # Normalize\n        returns = np.array(returns)\n        returns = (returns - np.mean(returns)) / (np.std(returns) + eps)\n        returns = returns.tolist()\n        # Calculating loss values to update our network\n        history = zip(action_probs_history, critic_value_history, returns)\n        actor_losses = []\n        critic_losses = []\n        for log_prob, value, ret in history:\n            # At this point in history, the critic estimated that we would get a\n            # total reward = `value` in the future. We took an action with log probability\n            # of `log_prob` and ended up recieving a total reward = `ret`.\n            # The actor must be updated so that it predicts an action that leads to\n            # high rewards (compared to critic's estimate) with high probability.\n            diff = ret - value\n            actor_losses.append(-log_prob * diff)  # actor loss\n\n            # The critic must be updated so that it predicts a better estimate of\n            # the future rewards.\n            critic_losses.append(\n                huber_loss(tf.expand_dims(value, 0), tf.expand_dims(ret, 0))\n            )\n        # Backpropagation\n        loss_value = sum(actor_losses) + sum(critic_losses)\n        grads = tape.gradient(loss_value, model.trainable_variables)\n        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n        \n        # Clear the loss and reward history\n        action_probs_history.clear()\n        critic_value_history.clear()\n        rewards_history.clear()\n        \n    # Log details\n    episode_count += 1\n    if episode_count % 10 == 0:\n        template = \"running reward: {:.2f} at episode {}\"\n        print(template.format(running_reward, episode_count))\n\n    if running_reward > 550:  # Condition to consider the task solved\n        print(\"Solved at episode {}!\".format(episode_count))\n        break","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"while not env.done:\n    state_ = tf.convert_to_tensor(state.geese)\n    state_ = tf.expand_dims(state_, 0)\n    action_probs, critic_value = model(state_)\n    critic_value_history.append(critic_value[0, 0])\n    action = np.random.choice(num_actions, p=np.squeeze(action_probs)[0])\n    action_probs_history.append(tf.math.log(action_probs[0, action]))\n    action = advanced_agent(state, env.configuration, action)\n    state = trainer.step(action)[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"env.render(mode=\"ipython\",width=800, height=600)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}