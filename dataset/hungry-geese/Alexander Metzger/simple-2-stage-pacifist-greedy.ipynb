{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Play default agents\nTo get a sense of how the game works, 4 default agents are set to play against each other: 2 greedy agents and 2 random agents.","metadata":{}},{"cell_type":"code","source":"from kaggle_environments.envs.hungry_geese.hungry_geese import greedy_agent, random_agent\nfrom kaggle_environments import evaluate, make\nenv = make(\"hungry_geese\")\nenv.reset()\nenv.run([random_agent, random_agent, greedy_agent, greedy_agent])\nenv.render(mode=\"ipython\", width=800, height=700)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Simple Agent: Improved Greedy with 2 stages \n\nSimilar to the code for the default Greedy agent, this agent chooses from moves that don't go adjacent to any heads and don't go into any bodies. It then prioritizes the moves as follows:\n\n1. Early game: lay low and maximize distance to the nearest opponent (to avoid getting killed early). Only seek out food greedily when smaller than 3 (avoid starving).\n\n2. Late game: greedily go for food to gain a higher reward (in case many geese survive, the fattest goose wins). Late game is decided based on an estimate of how many time steps are needed to catch up to the longest opponent and limited to a reasonable survivable length of 35.","metadata":{}},{"cell_type":"code","source":"%%writefile laylow.py\n\nfrom kaggle_environments.envs.hungry_geese.hungry_geese import Observation, Configuration, Action, row_col, adjacent_positions, translate, min_distance\nimport numpy as np\nfrom random import choice\n\ndef min_distance_opponent(position, opponents, columns): # same as builtin min_distance_food() but for opponent heads instead of food\n    row, column = row_col(position, columns)\n    return min(abs(row - opp_r) + abs(column - opp_c) for opponent in opponents for opponent_head in [opponent[0]] for opp_r, opp_c in [row_col(opponent_head, columns)])\n\nclass LayLowAgent:\n    def __init__(self, configuration: Configuration):\n        self.configuration = configuration\n        self.last_action = None\n        self.stepsleft = 200 # keep track of how many steps left in game to change to greedy late game\n\n    def __call__(self, observation: Observation):\n        rows, columns = self.configuration.rows, self.configuration.columns\n\n        food = observation.food\n        geese = observation.geese\n        opponents = [\n            goose\n            for index, goose in enumerate(geese)\n            if index != observation.index and len(goose) > 0\n        ]\n\n        # Don't move adjacent to any heads\n        head_adjacent_positions = {\n            opponent_head_adjacent\n            for opponent in opponents\n            for opponent_head in [opponent[0]]\n            for opponent_head_adjacent in adjacent_positions(opponent_head, columns, rows)\n        }\n        # Don't move into any bodies\n        bodies = {position for goose in geese for position in goose}\n        \n        position = geese[observation.index][0] \n\n        self.stepsleft -= 1\n        longestOpp = max([len(opponent) for opponent in opponents])\n        aggressCrit = min(max(longestOpp-len(geese[observation.index]),0)*12,40) # define a time step for changing to greedy based on how much longer longest oppenent is\n        if len(geese[observation.index]) < 3 or self.stepsleft < aggressCrit and len(geese[observation.index]) < 35: \n            # Move to the closest food\n            actions = {\n                action: min_distance(new_position, food, columns)\n                for action in Action\n                for new_position in [translate(position, action, columns, rows)]\n                if (\n                    new_position not in head_adjacent_positions and\n                    new_position not in bodies and\n                    (self.last_action is None or action != self.last_action.opposite())\n                )\n            }\n            action = min(actions, key=actions.get) if any(actions) else choice([action for action in Action])\n            self.last_action = action\n            return action.name\n        else:\n            # stay away from enemies\n            actions = {\n                action: min_distance_opponent(new_position, opponents, columns)\n                for action in Action\n                for new_position in [translate(position, action, columns, rows)]\n                if (\n                    new_position not in head_adjacent_positions and\n                    new_position not in bodies and\n                    (self.last_action is None or action != self.last_action.opposite())\n                )\n            }\n            action = max(actions, key=actions.get) if any(actions) else choice([action for action in Action]) # choose action based on farthest position from closest enemy\n            self.last_action = action\n            return action.name\n\n\ncached_agents = {}\n\ndef agent(obs, config):\n    index = obs[\"index\"]\n    if index not in cached_agents:\n        cached_agents[index] = LayLowAgent(Configuration(config))\n    return cached_agents[index](Observation(obs))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Debug game \nConfirm the agent works","metadata":{}},{"cell_type":"code","source":"from kaggle_environments.envs.hungry_geese.hungry_geese import greedy_agent, random_agent\nfrom kaggle_environments import evaluate, make\nenv = make(\"hungry_geese\", debug=True)\nenv.reset()\nenv.run([greedy_agent, 'laylow.py'])\nenv.render(mode=\"ipython\", width=500, height=400)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Evaluate Performance\nEvaluate the percentage of games won when two agents play against each other using the get_win_percentages() function from this https://www.kaggle.com/pierretihon/so-far-so-goose-simple-model-based-on-points\n<br> Compares with the default greedy agent and itself too to get an idea of performance over 200 iterations.","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\ndef get_win_percentages(agent1, agent2, n_rounds=200):\n    # Agent 1 goes first (roughly) half the time          \n    outcomes = evaluate(\"hungry_geese\", [agent1, agent2], num_episodes=n_rounds//2)\n    # Agent 2 goes first (roughly) half the time      \n    outcomes += [[b,a] for [a,b] in evaluate(\"hungry_geese\", [agent2, agent1], num_episodes= n_rounds-n_rounds//2)]\n    p1_wins = [True for [a,b] in outcomes if a>b]\n    p2_wins = [True for [a,b] in outcomes if b>a]\n    print(\"Agent 1 Win Percentage:\", np.round(len(p1_wins)/len(outcomes), 6)*100)\n    print(\"Agent 2 Win Percentage:\", np.round(len(p2_wins)/len(outcomes), 6)*100)\n    print(\"Number of Invalid Plays by Agent 1:\", outcomes.count([None, 0]))\n    print(\"Number of Invalid Plays by Agent 2:\", outcomes.count([0, None]))\n    print(outcomes)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"get_win_percentages(greedy_agent, greedy_agent)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"get_win_percentages('laylow.py', 'laylow.py')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"get_win_percentages(greedy_agent, 'laylow.py')","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}