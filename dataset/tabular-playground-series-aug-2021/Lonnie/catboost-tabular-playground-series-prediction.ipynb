{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# CatBoost Tabular Playground Series Prediction","metadata":{"execution":{"iopub.status.busy":"2021-08-15T08:15:20.439714Z","iopub.execute_input":"2021-08-15T08:15:20.440096Z","iopub.status.idle":"2021-08-15T08:15:20.44817Z","shell.execute_reply.started":"2021-08-15T08:15:20.44005Z","shell.execute_reply":"2021-08-15T08:15:20.446912Z"}}},{"cell_type":"markdown","source":"## Summary\nIn this notebook, I will use CatBoost Regressor to solve Tabular Playground Series Prediction. I will try hyperparameter searching and K-Fold Alogorithm to see if this can have an impact on test results.","metadata":{}},{"cell_type":"markdown","source":"## Import Packages","metadata":{}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split","metadata":{"execution":{"iopub.status.busy":"2021-08-26T03:57:31.69112Z","iopub.execute_input":"2021-08-26T03:57:31.691698Z","iopub.status.idle":"2021-08-26T03:57:31.696607Z","shell.execute_reply.started":"2021-08-26T03:57:31.691649Z","shell.execute_reply":"2021-08-26T03:57:31.695603Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Common Functions","metadata":{}},{"cell_type":"code","source":"def submit(model, test_features, test_ids, filename):\n    loss_pred = model.predict(test_features)\n    submission = pd.DataFrame({\"id\": test_ids, \"loss\": loss_pred.reshape(-1)})\n    submission.to_csv(filename, index=False)","metadata":{"execution":{"iopub.status.busy":"2021-08-26T03:57:31.698109Z","iopub.execute_input":"2021-08-26T03:57:31.698389Z","iopub.status.idle":"2021-08-26T03:57:31.7107Z","shell.execute_reply.started":"2021-08-26T03:57:31.698363Z","shell.execute_reply":"2021-08-26T03:57:31.709514Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load datasets","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/tabular-playground-series-aug-2021/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/tabular-playground-series-aug-2021/test.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-08-26T03:57:31.712819Z","iopub.execute_input":"2021-08-26T03:57:31.713135Z","iopub.status.idle":"2021-08-26T03:57:39.955778Z","shell.execute_reply.started":"2021-08-26T03:57:31.713105Z","shell.execute_reply":"2021-08-26T03:57:39.954709Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## EDA","metadata":{}},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-26T03:57:39.957498Z","iopub.execute_input":"2021-08-26T03:57:39.957949Z","iopub.status.idle":"2021-08-26T03:57:39.992774Z","shell.execute_reply.started":"2021-08-26T03:57:39.957917Z","shell.execute_reply":"2021-08-26T03:57:39.991864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.shape","metadata":{"execution":{"iopub.status.busy":"2021-08-26T03:57:39.99409Z","iopub.execute_input":"2021-08-26T03:57:39.994621Z","iopub.status.idle":"2021-08-26T03:57:39.999827Z","shell.execute_reply.started":"2021-08-26T03:57:39.994566Z","shell.execute_reply":"2021-08-26T03:57:39.99907Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.describe().transpose()","metadata":{"execution":{"iopub.status.busy":"2021-08-26T03:57:40.001061Z","iopub.execute_input":"2021-08-26T03:57:40.001578Z","iopub.status.idle":"2021-08-26T03:57:41.280421Z","shell.execute_reply.started":"2021-08-26T03:57:40.00151Z","shell.execute_reply":"2021-08-26T03:57:41.279465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There isn't an obvious correlation between features and target values.","metadata":{}},{"cell_type":"code","source":"corr_score = train.corr()","metadata":{"execution":{"iopub.status.busy":"2021-08-26T03:57:41.281815Z","iopub.execute_input":"2021-08-26T03:57:41.2824Z","iopub.status.idle":"2021-08-26T03:57:48.400425Z","shell.execute_reply.started":"2021-08-26T03:57:41.282343Z","shell.execute_reply":"2021-08-26T03:57:48.39963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corr_score[\"loss\"].sort_values(ascending=False)","metadata":{"execution":{"iopub.status.busy":"2021-08-26T03:57:48.401646Z","iopub.execute_input":"2021-08-26T03:57:48.402129Z","iopub.status.idle":"2021-08-26T03:57:48.411508Z","shell.execute_reply.started":"2021-08-26T03:57:48.40208Z","shell.execute_reply":"2021-08-26T03:57:48.410332Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Preprocessing","metadata":{}},{"cell_type":"markdown","source":"## Drop id column","metadata":{}},{"cell_type":"code","source":"train.pop(\"id\")\ntest_ids = test.pop(\"id\")","metadata":{"execution":{"iopub.status.busy":"2021-08-26T03:57:48.415005Z","iopub.execute_input":"2021-08-26T03:57:48.415436Z","iopub.status.idle":"2021-08-26T03:57:48.429713Z","shell.execute_reply.started":"2021-08-26T03:57:48.41539Z","shell.execute_reply":"2021-08-26T03:57:48.428543Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_mean = train.mean()\ntrain_std = train.std()","metadata":{"execution":{"iopub.status.busy":"2021-08-26T03:57:48.431084Z","iopub.execute_input":"2021-08-26T03:57:48.431463Z","iopub.status.idle":"2021-08-26T03:57:48.5642Z","shell.execute_reply.started":"2021-08-26T03:57:48.431432Z","shell.execute_reply":"2021-08-26T03:57:48.5632Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_targets_mean = train_mean.pop(\"loss\")\ntrain_targets_std = train_std.pop(\"loss\")","metadata":{"execution":{"iopub.status.busy":"2021-08-26T03:57:48.565466Z","iopub.execute_input":"2021-08-26T03:57:48.565807Z","iopub.status.idle":"2021-08-26T03:57:48.570363Z","shell.execute_reply.started":"2021-08-26T03:57:48.565778Z","shell.execute_reply":"2021-08-26T03:57:48.569403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Train Validation Split","metadata":{}},{"cell_type":"code","source":"validation_split = 0.2","metadata":{"execution":{"iopub.status.busy":"2021-08-26T03:57:48.571477Z","iopub.execute_input":"2021-08-26T03:57:48.571769Z","iopub.status.idle":"2021-08-26T03:57:48.585369Z","shell.execute_reply.started":"2021-08-26T03:57:48.571743Z","shell.execute_reply":"2021-08-26T03:57:48.584303Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_features, validation_features = train_test_split(train, test_size=validation_split)","metadata":{"execution":{"iopub.status.busy":"2021-08-26T03:57:48.586722Z","iopub.execute_input":"2021-08-26T03:57:48.587047Z","iopub.status.idle":"2021-08-26T03:57:48.815438Z","shell.execute_reply.started":"2021-08-26T03:57:48.587017Z","shell.execute_reply":"2021-08-26T03:57:48.814317Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_targets, validation_targets = train_features.pop(\"loss\"),  validation_features.pop(\"loss\")","metadata":{"execution":{"iopub.status.busy":"2021-08-26T03:57:48.816661Z","iopub.execute_input":"2021-08-26T03:57:48.816944Z","iopub.status.idle":"2021-08-26T03:57:48.825041Z","shell.execute_reply.started":"2021-08-26T03:57:48.816917Z","shell.execute_reply":"2021-08-26T03:57:48.823784Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data Scaling","metadata":{}},{"cell_type":"code","source":"should_scale = False\nif should_scale == True:\n    train_features = (train_features - train_mean) / train_std\n    validation_features = (validation_features - train_mean) / train_std\n    test_features = (test - train_mean) / train_std\n    print(test_features.head())\n    print(train_features.head())\n    print(validation_features.head())\nelse:\n     test_features = test","metadata":{"execution":{"iopub.status.busy":"2021-08-26T03:57:48.8269Z","iopub.execute_input":"2021-08-26T03:57:48.82738Z","iopub.status.idle":"2021-08-26T03:57:49.241509Z","shell.execute_reply.started":"2021-08-26T03:57:48.827332Z","shell.execute_reply":"2021-08-26T03:57:49.240676Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Development\n### Using Catboost","metadata":{"execution":{"iopub.status.busy":"2021-08-15T07:48:00.638201Z","iopub.execute_input":"2021-08-15T07:48:00.63858Z","iopub.status.idle":"2021-08-15T07:48:00.643438Z","shell.execute_reply.started":"2021-08-15T07:48:00.63855Z","shell.execute_reply":"2021-08-15T07:48:00.64239Z"}}},{"cell_type":"code","source":"import catboost\nimport time\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nbegin = time.time()\nparameters = {\n    \"depth\": [6, 7, 8],\n    \"learning_rate\": [0.08, 0.1],\n    \"iterations\": [300, 350], \n}\ndef train_catboost(hyperparameters, X_train, X_val, y_train, y_val):\n    keys = hyperparameters.keys()\n    best_index = {key:0 for key in keys}\n    best_cat = None\n    best_score = 10e8\n    for (index, key) in enumerate(keys):\n        print(\"Find best parameter for %s\" %(key))\n        items = hyperparameters[key]\n        best_parameter = None\n        temp_best = 10e8\n        for (key_index, item) in enumerate(items):\n            iterations = hyperparameters[\"iterations\"][best_index[\"iterations\"]] if key != \"iterations\" else item\n            learning_rate = hyperparameters[\"learning_rate\"][best_index[\"learning_rate\"]] if key != \"learning_rate\" else item\n            depth = hyperparameters[\"depth\"][best_index[\"depth\"]] if key != \"depth\" else item\n            print(\"Train with iterations: %d learning_rate: %.2f depth:%d\"%(iterations, learning_rate, depth))\n            cat = catboost.CatBoostRegressor(\n                iterations = iterations, \n                learning_rate = learning_rate,\n                depth = depth\n            )\n            cat.fit(X_train, y_train, verbose=False)\n            y_pred = cat.predict(X_val)\n            score = np.sqrt(mean_squared_error(y_val, y_pred))\n            print(\"RMSE: %.2f\"%(score))\n            if score < temp_best:\n                temp_best = score\n                best_index[key] = key_index\n                best_parameter = item\n            if score < best_score:\n                best_score = score\n                best_cat = cat\n        print(\"Best Parameter for %s: \"%(key), best_parameter)\n    best_parameters = {\n        \"iterations\": hyperparameters[\"iterations\"][best_index[\"iterations\"]],\n        \"learning_rate\": hyperparameters[\"learning_rate\"][best_index[\"learning_rate\"]],\n        \"depth\": hyperparameters[\"depth\"][best_index[\"depth\"]]\n    }\n    return best_cat, best_score, best_parameters\nbest_cat, best_score, best_parameters = train_catboost(parameters, train_features, validation_features, train_targets, validation_targets)\nprint(\"Best CatBoost Model: \", best_cat)\nprint(\"Best MAE: \", best_score)\nelapsed = time.time() - begin \nprint(\"Elapsed time: \", elapsed)\nsubmit(best_cat, test_features, test_ids, \"submission.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-08-26T03:57:49.242657Z","iopub.execute_input":"2021-08-26T03:57:49.243077Z","iopub.status.idle":"2021-08-26T04:00:12.616042Z","shell.execute_reply.started":"2021-08-26T03:57:49.243046Z","shell.execute_reply":"2021-08-26T04:00:12.614791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I will apply K-Fold alogorithm to best Model for training. The result looks good, sometime I can get 7.82, howerver when I submit the results, the scores are about 7.9, and just litte difference between different fold of data.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import KFold\nfold = 1\nfor train_indices, val_indices in KFold(n_splits=5, shuffle=True).split(train):\n    print(\"Training with Fold %d\"%(fold))\n    X_train = train.iloc[train_indices]\n    X_val = train.iloc[val_indices]\n    y_train = X_train.pop(\"loss\")\n    y_val = X_val.pop(\"loss\")\n    if should_scale:\n        X_train = (X_train - train_mean) / train_std\n        X_val = (X_val - train_mean) / train_std\n    cat = catboost.CatBoostRegressor(\n        iterations = best_parameters[\"iterations\"], \n        learning_rate = best_parameters[\"learning_rate\"],\n        depth = best_parameters[\"depth\"]\n    )\n    cat.fit(X_train, y_train, verbose=False)\n    y_pred = cat.predict(X_val)\n    score = np.sqrt(mean_squared_error(y_val, y_pred))\n    print(\"RMSE: %.2f\"%(score))\n    submit(cat, test_features, test_ids, \"submission_fold%d.csv\"%(fold))\n    fold += 1","metadata":{"execution":{"iopub.status.busy":"2021-08-26T04:00:12.617855Z","iopub.execute_input":"2021-08-26T04:00:12.618335Z","iopub.status.idle":"2021-08-26T04:01:58.404432Z","shell.execute_reply.started":"2021-08-26T04:00:12.618284Z","shell.execute_reply":"2021-08-26T04:01:58.403191Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Conclusion\nHyper Parameter Searching don't affect the result too much. K-Fold Alogrithm has obvious impact on validation dataset, but doesn't have an obvious impact on test result.","metadata":{"execution":{"iopub.status.busy":"2021-08-26T04:08:16.587401Z","iopub.execute_input":"2021-08-26T04:08:16.587846Z","iopub.status.idle":"2021-08-26T04:08:16.594261Z","shell.execute_reply.started":"2021-08-26T04:08:16.587811Z","shell.execute_reply":"2021-08-26T04:08:16.592989Z"}}},{"cell_type":"markdown","source":"## Your upvote can encourage me updating notebooks on Kaggle, if you like my work, give me an upvote.","metadata":{}}]}