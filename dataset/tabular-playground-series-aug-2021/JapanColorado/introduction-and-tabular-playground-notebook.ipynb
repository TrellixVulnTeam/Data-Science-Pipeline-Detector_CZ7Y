{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Intro","metadata":{}},{"cell_type":"code","source":"import this","metadata":{"execution":{"iopub.status.busy":"2022-05-12T23:15:09.795687Z","iopub.execute_input":"2022-05-12T23:15:09.795943Z","iopub.status.idle":"2022-05-12T23:15:09.826272Z","shell.execute_reply.started":"2022-05-12T23:15:09.795873Z","shell.execute_reply":"2022-05-12T23:15:09.825598Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Hello, my name is Russell(JapanColorado), I'm new to the field of machine learning, and I'm excited to learn all that I can! I've always been interested in math and programming, and so machine learning/AI seemed liked the natural next step. To learn machine learning, I'm following Aurélien Géron's *Hands-on Machine Learing with Scikit-Learn, Keras and Tensorflow*, as well as a number of Kaggle Learn courses, competitions, and other similair MOOCs. I'm excited to start my machine learning journey, as well as interacting with the community! This notebook is serving as both an introduction, as well as the exercise for chapter 2 of Hands-on ML, so without further ado, let's jump into the code!","metadata":{}},{"cell_type":"markdown","source":"btw, some of you might wonder why I'm using the August 2021 Tabular Playground data as my first notebook, and the motivation is to save the titanic competition for after the chapter on classifiers, and the housing prices one for once I know more advanced algorithims and basic EDA(since there are no column labels in the tabular playground data, there's no real need for EDA beyond a correlation matrix or so(of course, if I'm wrong about that, please correct me.)) I'm following *Hands-on Machine Learing with Scikit-Learn, Keras and Tensorflow*'s machine learning checklist. And since I'm only on chapter 2, some of the algorithims and data inspection/preperation will be quite basic. Any feedback for improvement is much appreciated!","metadata":{}},{"cell_type":"markdown","source":"OK, now for the code!","metadata":{}},{"cell_type":"markdown","source":"## Setup(De facto Step 1)","metadata":{}},{"cell_type":"code","source":"%matplotlib inline\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler, Normalizer\nfrom sklearn.model_selection import cross_val_score, train_test_split, GridSearchCV, RandomizedSearchCV\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import LinearRegression,SGDRegressor,ElasticNet,Lasso\nfrom sklearn.svm import LinearSVR,SVR\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import mean_squared_error\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-12T23:15:09.827631Z","iopub.execute_input":"2022-05-12T23:15:09.82795Z","iopub.status.idle":"2022-05-12T23:15:11.134987Z","shell.execute_reply.started":"2022-05-12T23:15:09.827914Z","shell.execute_reply":"2022-05-12T23:15:11.134049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"P.S.A. I'm skipping Step 1(Frame the Problem), as the data is artificially generated, with no real practical use beyond machine learning practice.","metadata":{}},{"cell_type":"markdown","source":"## **Step Two:** Get the Data","metadata":{}},{"cell_type":"markdown","source":"First things first, we have to load the data:","metadata":{}},{"cell_type":"code","source":"train_data = pd.read_csv(\"/kaggle/input/tabular-playground-series-aug-2021/train.csv\", index_col=\"id\")\ntest_data = pd.read_csv(\"/kaggle/input/tabular-playground-series-aug-2021/test.csv\", index_col=\"id\")\ntrain_data.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-12T23:15:11.136407Z","iopub.execute_input":"2022-05-12T23:15:11.136824Z","iopub.status.idle":"2022-05-12T23:15:21.151799Z","shell.execute_reply.started":"2022-05-12T23:15:11.13678Z","shell.execute_reply":"2022-05-12T23:15:21.151157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Step Two:** Explore the Data","metadata":{}},{"cell_type":"markdown","source":"In this step, we'll do some basic data exploration, see if there are any categorical values or missing values, graph correlations, etc.","metadata":{}},{"cell_type":"code","source":"s = (train_data.dtypes == 'object')\nobject_cols = list(s[s].index)\n\nprint(train_data.info())\nprint([col for col in train_data.columns if train_data[col].isnull().any()])\nprint(object_cols)\nprint(train_data.shape)","metadata":{"execution":{"iopub.status.busy":"2022-05-12T23:15:21.154142Z","iopub.execute_input":"2022-05-12T23:15:21.154665Z","iopub.status.idle":"2022-05-12T23:15:21.213975Z","shell.execute_reply.started":"2022-05-12T23:15:21.154627Z","shell.execute_reply":"2022-05-12T23:15:21.213285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Seems pretty simple. No missing values, no categorical features. That means the most we'll probably have to do for data preperation is normalization/standardization. Alright, let's look at some graphs!","metadata":{}},{"cell_type":"code","source":"train_data.hist(bins=50,figsize=(35,30))\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-12T23:15:21.215256Z","iopub.execute_input":"2022-05-12T23:15:21.215486Z","iopub.status.idle":"2022-05-12T23:15:38.208125Z","shell.execute_reply.started":"2022-05-12T23:15:21.215455Z","shell.execute_reply":"2022-05-12T23:15:38.205134Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Hmm...There seems to be a wide variety of graph types and shapes, as well as scaling, ranging from the ten thousands, all the way down to decimals. My guess is that standardization will preform better, considering the wide range and the number of outliers, but we can try normalization as well. Most notably, loss(the target feature) seems to be very tail heavy. Do I know how to fix that? Not really! Let's move on to the correlation matrix!","metadata":{}},{"cell_type":"code","source":"corr_matrix = train_data.corr()\ncorr_matrix[\"loss\"].sort_values(ascending=False)","metadata":{"execution":{"iopub.status.busy":"2022-05-12T23:15:38.209086Z","iopub.execute_input":"2022-05-12T23:15:38.20933Z","iopub.status.idle":"2022-05-12T23:15:45.45357Z","shell.execute_reply.started":"2022-05-12T23:15:38.209293Z","shell.execute_reply":"2022-05-12T23:15:45.45266Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Again, there seems to be alot of variety, and no features are strongly correlated. It seems like the best solution will probably be throwing a bunch of algorithims at the problem and seeing what sticks, since there is no practical way(to my knowledge) to do feature engineering. Let's go ahead and seperate the data into X and y, as well as make a pipeline for normalization and standardization, then we'll start with models.","metadata":{}},{"cell_type":"code","source":"y = train_data.loss\nX = train_data.drop(\"loss\", axis=1)\n\nnorm_pipeline = Pipeline([\n    (\"normalization\", Normalizer())\n])\nstan_pipeline = Pipeline([\n    (\"stanardization\", StandardScaler())\n])","metadata":{"execution":{"iopub.status.busy":"2022-05-12T23:15:45.455118Z","iopub.execute_input":"2022-05-12T23:15:45.455422Z","iopub.status.idle":"2022-05-12T23:15:45.52006Z","shell.execute_reply.started":"2022-05-12T23:15:45.455384Z","shell.execute_reply":"2022-05-12T23:15:45.518939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Alright, now for the fun part!","metadata":{}},{"cell_type":"markdown","source":"## **Step 3:** Shortlist Promising Models","metadata":{}},{"cell_type":"markdown","source":"Alright, for this step, my plan is to write a function that will train many different models using standard parameters on both normalized and standardized data, see which data preforms best, which models preform best, and hopefully find some models that will preform well after further tuning. Let's get started!","metadata":{}},{"cell_type":"code","source":"def quick_model_eval(models,preprocessor,random_state=69,X=X,y=y):\n    for model in models:\n        model.random_state = random_state\n        model_pipeline = Pipeline([\n            (\"preprocessor\", preprocessor),\n            (\"model\", model)\n        ])\n        score = -1 * cross_val_score(model_pipeline,X,y,cv=3,n_jobs=-1, scoring=\"neg_root_mean_squared_error\")\n        print(f\"The {model} scored: {score}\")\n        print(f\"The {model}'s mean score was: {score.mean()}\")","metadata":{"execution":{"iopub.status.busy":"2022-05-12T23:15:45.521946Z","iopub.execute_input":"2022-05-12T23:15:45.522453Z","iopub.status.idle":"2022-05-12T23:15:45.53147Z","shell.execute_reply.started":"2022-05-12T23:15:45.522408Z","shell.execute_reply":"2022-05-12T23:15:45.530534Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Alright, let's try it out!","metadata":{}},{"cell_type":"code","source":"#quick_model_eval([RandomForestRegressor()],stan_pipeline)","metadata":{"execution":{"iopub.status.busy":"2022-05-12T23:15:45.53587Z","iopub.execute_input":"2022-05-12T23:15:45.536467Z","iopub.status.idle":"2022-05-12T23:15:45.542553Z","shell.execute_reply.started":"2022-05-12T23:15:45.536385Z","shell.execute_reply":"2022-05-12T23:15:45.541863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Alright, so cross validation is taking TOO LONG. Linear regression was fine, but doing the function on a random forest never finish and I waited 33 minutes. If it can't handle a random forest with standard parameters, it certainly won't be able to hand XGBoost(If anyone knows a reason why cross validation might be failing beyond the amount of data, let me know). For thoroughness here's the function on linear regression:","metadata":{}},{"cell_type":"code","source":"quick_model_eval([LinearRegression()],stan_pipeline)","metadata":{"execution":{"iopub.status.busy":"2022-05-12T23:15:45.544092Z","iopub.execute_input":"2022-05-12T23:15:45.544354Z","iopub.status.idle":"2022-05-12T23:15:53.090105Z","shell.execute_reply.started":"2022-05-12T23:15:45.54432Z","shell.execute_reply":"2022-05-12T23:15:53.089302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"And while that takes not too long, as I said, it probably won't be able to handle complex algorithims like XGBoost. So let's implement a new function but with holdout validation instead!","metadata":{}},{"cell_type":"code","source":"def quicker_model_eval(models,preprocessor,random_state=69,X=X,y=y):\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2)\n    X_train_trans = preprocessor.fit_transform(X_train)\n    X_vaild_trans = preprocessor.transform(X_valid)\n    for model in models:\n        model.random_state = random_state\n        model.fit(X_train_trans,y_train)\n        predictions = model.predict(X_vaild_trans)\n        score = mean_squared_error(predictions,y_valid, squared=False)#squared=False makes it RMSE\n        print(f\"The {model} scored: {score}\")","metadata":{"execution":{"iopub.status.busy":"2022-05-12T23:15:53.091878Z","iopub.execute_input":"2022-05-12T23:15:53.092137Z","iopub.status.idle":"2022-05-12T23:15:53.098371Z","shell.execute_reply.started":"2022-05-12T23:15:53.092103Z","shell.execute_reply":"2022-05-12T23:15:53.097714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Alright, let's try this out. This time, I'm going to start with linear regressions.","metadata":{}},{"cell_type":"code","source":"quicker_model_eval([LinearRegression()],stan_pipeline)","metadata":{"execution":{"iopub.status.busy":"2022-05-12T23:15:53.099941Z","iopub.execute_input":"2022-05-12T23:15:53.100517Z","iopub.status.idle":"2022-05-12T23:15:55.345316Z","shell.execute_reply.started":"2022-05-12T23:15:53.10048Z","shell.execute_reply":"2022-05-12T23:15:55.344418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Ok, so that was alot quicker, and the results are pretty similar to cross validation's. Let's try out a random forest.","metadata":{}},{"cell_type":"code","source":"#quicker_model_eval([RandomForestRegressor(random_state=69)],stan_pipeline)","metadata":{"execution":{"iopub.status.busy":"2022-05-12T23:15:55.346483Z","iopub.execute_input":"2022-05-12T23:15:55.34673Z","iopub.status.idle":"2022-05-12T23:15:55.352745Z","shell.execute_reply.started":"2022-05-12T23:15:55.346694Z","shell.execute_reply":"2022-05-12T23:15:55.351436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So interestingly, this is still taking quite a while. I looked around on stackoverflow, and I think I've figured out why. I haven't constrained the algorithim, so since it is an ensemble method and is training on quite a large amount of data(250000 x 101 data points, including targets), the problem is that for each tree it has to train on all of that data. So I'm going to try my original cross validation algorithim as well as my new holdout validation one on a random forest with a fixed number of trees(eg. 5) and use that as quick shortlisting, then find the optimal number of trees later. Let's try it!","metadata":{}},{"cell_type":"code","source":"quicker_model_eval([RandomForestRegressor(n_estimators=5)],stan_pipeline)","metadata":{"execution":{"iopub.status.busy":"2022-05-12T23:15:55.353642Z","iopub.execute_input":"2022-05-12T23:15:55.353927Z","iopub.status.idle":"2022-05-12T23:19:39.430653Z","shell.execute_reply.started":"2022-05-12T23:15:55.353889Z","shell.execute_reply":"2022-05-12T23:19:39.428805Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#quick_model_eval([RandomForestRegressor(random_state=69, n_estimators=5)],stan_pipeline)","metadata":{"execution":{"iopub.status.busy":"2022-05-12T23:19:39.432126Z","iopub.execute_input":"2022-05-12T23:19:39.432398Z","iopub.status.idle":"2022-05-12T23:19:39.436085Z","shell.execute_reply.started":"2022-05-12T23:19:39.432361Z","shell.execute_reply":"2022-05-12T23:19:39.435377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Alright, so the holdout validation function took alot less time(~10 minutes), while the cross validation never finished. I think we can go ahead and try this out on some promising models!(btw, following sklearn's algorithim cheat sheet, but also trying out some of my own choice :P)","metadata":{}},{"cell_type":"code","source":"quicker_model_eval([\n    Lasso(),\n    SGDRegressor(),\n    ElasticNet(),\n    RandomForestRegressor(n_estimators=5),\n    XGBRegressor(tree_method='gpu_hist', gpu_id=0),\n    LinearSVR(),\n    #SVR(),\n    KNeighborsRegressor()], stan_pipeline)","metadata":{"execution":{"iopub.status.busy":"2022-05-12T23:19:39.437536Z","iopub.execute_input":"2022-05-12T23:19:39.438057Z","iopub.status.idle":"2022-05-12T23:27:50.531347Z","shell.execute_reply.started":"2022-05-12T23:19:39.438022Z","shell.execute_reply":"2022-05-12T23:27:50.530434Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Alright, so it looks like results are quite varied across the board. Some of the best preformers were Lasso, SGD, ElasticNet, and XGBoost, and this is going to be my shortlist. While I could definitly make the random forest perform much better, the amount of time it takes to train, as well as the fact that it can't utilize gpu means I'm not going to include it in my shortlist. Meanwhile, LinearSVR and KNeighbors both preformed terribly, and the regular SVR hasn't finished training. Alright, I think we should look at the variables that Lasso, SGD, and ElasticNet prioritized, then we can move on to model tuning. Oh wait, these models were defined for the function...Let me redefine the function to print out the coefficients of each model(btw, if anyone thinks I should have tried a different model, let me know! Feedback is always welcome).","metadata":{}},{"cell_type":"code","source":"def quicker_model_eval_coef(models,preprocessor,random_state=69,X=X,y=y):\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2)\n    X_train_trans = preprocessor.fit_transform(X_train)\n    X_vaild_trans = preprocessor.transform(X_valid)\n    for model in models:\n        model.random_state = random_state\n        model.fit(X_train_trans,y_train)\n        predictions = model.predict(X_vaild_trans)\n        score = mean_squared_error(predictions,y_valid, squared=False)#squared=False makes it RMSE\n        coefficients = pd.Series(model.coef_, index=X.columns)\n        print(f\"The {model} scored: {score}\")\n        print(f\"The {model}'s coefficients:\\n{coefficients}, with {sum(coefficients != 0)} picked, and the other {sum(coefficients == 0)} rejected\\n\")\n        #Credit to Alexandru Papiu for his wonderful notebook on Regularized Linear Models for housing, which showed me that you could look at the coefficients for linear models","metadata":{"execution":{"iopub.status.busy":"2022-05-12T23:27:50.532888Z","iopub.execute_input":"2022-05-12T23:27:50.533291Z","iopub.status.idle":"2022-05-12T23:27:50.541948Z","shell.execute_reply.started":"2022-05-12T23:27:50.533253Z","shell.execute_reply":"2022-05-12T23:27:50.54119Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"quicker_model_eval_coef([\n    Lasso(),\n    SGDRegressor(),\n    ElasticNet()], stan_pipeline)","metadata":{"execution":{"iopub.status.busy":"2022-05-12T23:27:50.543275Z","iopub.execute_input":"2022-05-12T23:27:50.543689Z","iopub.status.idle":"2022-05-12T23:27:57.064603Z","shell.execute_reply.started":"2022-05-12T23:27:50.543649Z","shell.execute_reply":"2022-05-12T23:27:57.063773Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So interestingly, the lasso and elastic net models have 0 features picked. Whether this means that they just supply a dummy value, or something else is going on is beyond me. If anyone has any feedback on why this might be, please let me know. Oddities asides, let's move on to model tuning!","metadata":{}},{"cell_type":"markdown","source":"## **Step 4:** Fine-Tune the System","metadata":{}},{"cell_type":"markdown","source":"Alright, now it's time to try and boost the preformace of the shortlisted models. Each has at least a couple hyperparameters that can be tuned(especially XGBoost), and let's see if we can get performance a *bit* higher(Keep in mind that the top score is 7.7955, so there's probably not going to be any dramatic increase). Alright! Let' start by identifying what hyperparameterse each learning algorithim has; we can do this through sklearn's get_params() method:","metadata":{}},{"cell_type":"code","source":"models = [Lasso(), SGDRegressor(), ElasticNet(), XGBRegressor(tree_method='gpu_hist', gpu_id=0)]\nfor model in models:\n    print(f\"{model.get_params()}\\n\")","metadata":{"execution":{"iopub.status.busy":"2022-05-12T23:27:57.066028Z","iopub.execute_input":"2022-05-12T23:27:57.066342Z","iopub.status.idle":"2022-05-12T23:27:57.074539Z","shell.execute_reply.started":"2022-05-12T23:27:57.066303Z","shell.execute_reply":"2022-05-12T23:27:57.073639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Alright, so my goal is to have about ~2-3 hyperparameters per algorithim. What I've narrowed down is this:\n* Lasso:\n  * Alpha(Float)\n  * Fit Intercept(Bool)\n* SGDRegressor:\n  * Alpha(Float)\n  * Fit Intercept(Bool)\n  * Early Stopping(Bool)\n* ElasticNet:\n  * Alpha(Float)\n  * Fit Intercept(Bool)\n* XGBoost:\n  * N Estimators(Int)\n  * Early Stopping Rounds(Int)\n  * Learning Rate(Float)","metadata":{}},{"cell_type":"markdown","source":"My plan is to define a parameter tuner function to find the hyperparameters. Let's also not forget to treat the normalization and standardization pipelines as hyperparameters. Let's go ahead and start!","metadata":{}},{"cell_type":"code","source":"lasso_params = [\n    {\"fit_intercept\":[True, False],\"alpha\":[0.0001, 0.001, 0.01, 0.1, 1, 10]}\n]\n\nSGD_params = [\n    {\"fit_intercept\":[True, False],\"alpha\":[0.0001, 0.001, 0.01, 0.1, 1, 10], \"early_stopping\":[True, False]}\n]\n\nelastic_params = [\n    {\"fit_intercept\":[True, False],\"alpha\":[0.0001, 0.001, 0.01, 0.1, 1, 10]}\n]\n\nXGB_params = [\n    {\"n_estimators\":[5, 10, 20, 30, 50, 100, 1000, 5000],\"early_stopping_rounds\":[3, 5, 10],\"learning_rate\":[0.0001, 0.001, 0.01, 0.1, 0.3, 0.7]}\n]\n\ndef model_param_tuner(model, preprocessor, tuner, params, random_state=69, cv=5, X=X, y=y):\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2)\n    X_train_trans = preprocessor.fit_transform(X_train)\n    X_vaild_trans = preprocessor.transform(X_valid)\n    model.random_state = random_state\n    tuned_model = tuner(model, params, cv=cv,\n                       scoring='neg_mean_squared_error')\n    tuned_model.fit(X_train_trans,y_train)\n    print(f\"The {model}'s ideal parameters were: {tuned_model.best_params_}\")\n    print(f\"The preprocessor was: {preprocessor}\")\n    print(f\"These were the scores: \\n\")\n    cv_results = tuned_model.cv_results_\n    for mean_score, params in zip(cv_results[\"mean_test_score\"],cv_results[\"params\"]):\n        print(np.sqrt(-mean_score), params)\n    predictions = tuned_model.predict(X_vaild_trans)\n    score = mean_squared_error(predictions, y_valid, squared=False)\n    print(f\"The {model}'s validation score: {score}\")","metadata":{"execution":{"iopub.status.busy":"2022-05-12T23:40:06.403098Z","iopub.execute_input":"2022-05-12T23:40:06.403807Z","iopub.status.idle":"2022-05-12T23:40:06.419661Z","shell.execute_reply.started":"2022-05-12T23:40:06.40377Z","shell.execute_reply":"2022-05-12T23:40:06.418898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's go ahead and test it:","metadata":{}},{"cell_type":"code","source":"model_param_tuner(Lasso(), stan_pipeline, GridSearchCV, lasso_params)","metadata":{"execution":{"iopub.status.busy":"2022-05-12T23:27:57.10552Z","iopub.execute_input":"2022-05-12T23:27:57.107128Z","iopub.status.idle":"2022-05-12T23:28:26.559202Z","shell.execute_reply.started":"2022-05-12T23:27:57.107084Z","shell.execute_reply":"2022-05-12T23:28:26.558412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Alright, it works! Let's go ahead and run it on all the models with both normalization and standardization pipelines:","metadata":{}},{"cell_type":"code","source":"for processor in [stan_pipeline, norm_pipeline]:\n    model_param_tuner(Lasso(), processor, RandomizedSearchCV, lasso_params)\n    model_param_tuner(SGDRegressor(), processor, RandomizedSearchCV, SGD_params)\n    model_param_tuner(ElasticNet(), processor, RandomizedSearchCV, elastic_params)\n    model_param_tuner(XGBRegressor(tree_method=\"gpu_hist\", gpu_id=0), processor, RandomizedSearchCV, XGB_params)","metadata":{"execution":{"iopub.status.busy":"2022-05-12T23:28:26.564937Z","iopub.execute_input":"2022-05-12T23:28:26.567107Z","iopub.status.idle":"2022-05-12T23:40:06.372406Z","shell.execute_reply.started":"2022-05-12T23:28:26.567059Z","shell.execute_reply":"2022-05-12T23:40:06.371653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As expected, standardization preformed slightly better than normilization, with XGBoost(unsurprisingly) preforming the best. Let's go ahead and submit the results! I'll be retuning the XGBRegressor model on GridSearchCV and standardization for submission.","metadata":{}},{"cell_type":"code","source":"def model_param_tuner_returned(model, preprocessor, tuner, params, test_set=test_data, random_state=69, cv=5, X=X, y=y):\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2)\n    X_train_trans = preprocessor.fit_transform(X_train)\n    X_vaild_trans = preprocessor.transform(X_valid)\n    model.random_state = random_state\n    tuned_model = tuner(model, params, cv=cv,\n                       scoring='neg_mean_squared_error')\n    tuned_model.fit(X_train_trans,y_train)\n    print(f\"The {model}'s ideal parameters were: {tuned_model.best_params_}\")\n    predictions = tuned_model.predict(X_vaild_trans)\n    score = mean_squared_error(predictions, y_valid, squared=False)\n    print(f\"The {model}'s validation score: {score}\")\n    return tuned_model\nxgb_tuned = model_param_tuner_returned(XGBRegressor(tree_method=\"gpu_hist\", gpu_id=0), stan_pipeline, GridSearchCV, XGB_params)","metadata":{"execution":{"iopub.status.busy":"2022-05-13T01:42:07.173214Z","iopub.execute_input":"2022-05-13T01:42:07.173505Z","iopub.status.idle":"2022-05-13T03:05:47.70325Z","shell.execute_reply.started":"2022-05-13T01:42:07.173474Z","shell.execute_reply":"2022-05-13T03:05:47.70256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submissions = pd.read_csv(\"/kaggle/input/tabular-playground-series-aug-2021/sample_submission.csv\")\nsubmissions[\"loss\"] = xgb_tuned.predict(test_data)\nsubmissions.to_csv(\"submission.csv\", index = False)\nsubmissions.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-13T03:17:29.844841Z","iopub.execute_input":"2022-05-13T03:17:29.845394Z","iopub.status.idle":"2022-05-13T03:17:33.407164Z","shell.execute_reply.started":"2022-05-13T03:17:29.845356Z","shell.execute_reply":"2022-05-13T03:17:33.405695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Conclusion**","metadata":{}},{"cell_type":"markdown","source":"Thanks to everyone who got through this notebook! I know my code is hard to read, convoluted, and doesn't follow DRY(Don't Repeat Yourself), but it was still fun to start writing some code, and expirementing along the way. I hope to keep improving, and this is defenitly not the last you'll see of me on Kaggle. Again, thanks, and I can't wait to keep on improving! Feedback is welcome, and I'll try to implement suggestions in futre code. Thanks!","metadata":{}}]}