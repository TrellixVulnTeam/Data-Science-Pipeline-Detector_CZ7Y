{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# üé∞ Comparison of LightAutoML & h2o.ai & FLAML\n![](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSl0cUeLOZ9Q5buRejZWlnKuj8wBPq4zei8o6L7uSuYCGY2C72bVwHi4hDxrrdYC8wtaqs&usqp=CAU)\n\n* The main idea of this kernel is to compare the LightAutoML, h2o.ai and FLAML AutoMl algorithms in terms of setup and this competition performance (RMSE).\n* All models are submitted to the competition and results are shared at the end of each model in this notebook.üèµ \n\n* In all three cases I set timeout as (1200 seconds~20 min) for better model development among time. \n* This notebook idea is mainly based on valuable notebook written early by @andreshg with link [here](https://www.kaggle.com/andreshg/automl-libraries-comparison) where he compared the effectiveness of 7 different AutoML models by their competition performance.\n","metadata":{}},{"cell_type":"code","source":"# Standard libraries\nimport os\nimport time\nimport numpy as np\nimport pandas as pd","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install scikit-learn --upgrade\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import StandardScaler","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv('../input/tabular-playground-series-aug-2021/train.csv')\ntest_df = pd.read_csv('../input/tabular-playground-series-aug-2021/test.csv')\nsample_submission = pd.read_csv('../input/tabular-playground-series-aug-2021/sample_submission.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = train_df.drop(['id', 'loss'], axis=1)\ny = train_df['loss'].values\nX_test = test_df.drop(['id'], axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scaler = StandardScaler()\nX = scaler.fit_transform(X)\nX_test = scaler.transform(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target = train_df['loss']\ntrain_df.drop(['id'], axis=1, inplace=True)\ntest_df.drop(['id'], axis=1, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\nss = StandardScaler()\nfeatures = [f'f{i}' for i in range(100)]\ntrain_df[features] = ss.fit_transform(train_df[features])\ntest_df[features] = ss.transform(test_df[features])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# LightAutoML \n\n*LightAutoML project from Sberbank AI Lab AutoML group is the framework for automatic classification and regression model creation.*\n\n* Thank you for this valuable detailed notebook for the usage of LightAutoML, credits to the author @alexryzhkov\n[Aug21 LightAutoML starter notebook](https://www.kaggle.com/alexryzhkov/aug21-lightautoml-starter)\n\n[For detailed LightAutoML documentation](https://lightautoml.readthedocs.io/en/latest/)\n\n[For Github repository](https://github.com/sberbank-ai-lab/LightAutoML)","metadata":{}},{"cell_type":"code","source":"!pip install -U lightautoml","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from lightautoml.automl.presets.tabular_presets import TabularAutoML, TabularUtilizedAutoML\nfrom lightautoml.tasks import Task\n\nimport torch","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"N_THREADS = 4 \nN_FOLDS = 5\nRANDOM_STATE = 42\nTEST_SIZE = 0.2\nTIMEOUT = 1200\n\nnp.random.seed(RANDOM_STATE)\ntorch.set_num_threads(N_THREADS)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\ndef rmse(y_true, y_pred, **kwargs):\n    return mean_squared_error(y_true, y_pred, squared = False, **kwargs)\n\ntask = Task('reg', metric = rmse)\n\nroles = {'target': 'loss',\n        'drop': ['id']}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n**These parameters below are gathered from my own notebook with using Optuna Parameter Tuning. I recommend you to see and upvote my notebook if you are interested.**\n\n**ü§ñ Optuna Tuning with XGBoost+CatBoost+LGBM [Link](https://www.kaggle.com/tolgakurtulus/optuna-tuning-with-xgboost-catboost-lgbm)**\n\n","metadata":{}},{"cell_type":"code","source":"lgb_params = {\n    'metric': 'RMSE',\n    'lambda_l1': 0.1912487104284709,\n    'lambda_l2': 0.06374015849652141,\n    'num_leaves': 53, \n    'learning_rate': 0.10398927752362405,\n    'feature_fraction': 0.8612490357778249,\n    'bagging_fraction': 0.8969003388461672,\n    'bagging_freq': 0,\n    'min_child_samples': 95,\n    'num_threads': 8\n}\n\n\ncb_params = {\n     #'iterations': 8908,\n     'od_wait': 1707,\n     'learning_rate': 0.010395447212764725,\n     #'reg_lambda': 99.12580252995424,\n     'subsample': 0.9982266060286022,\n     'random_strength': 17.782673214289556,\n     'min_data_in_leaf': 12,\n     'leaf_estimation_iterations': 3,\n     'loss_function': 'RMSE',\n     'eval_metric': 'RMSE',\n     'bootstrap_type': 'Bernoulli',\n     'leaf_estimation_method': 'Newton',\n     'random_seed': 42,\n     'thread_count': 4\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time \nautoml = TabularAutoML(task = task, \n                       timeout = TIMEOUT,\n                       cpu_limit = N_THREADS,\n                       reader_params = {'n_jobs': N_THREADS, 'cv': N_FOLDS, 'random_state': RANDOM_STATE},\n                       general_params = {'use_algos': [['linear_l2', 'cb', 'lgb', 'lgb_tuned']]},\n                       lgb_params = {'default_params': lgb_params, 'freeze_defaults': True}, # LGBM params\n                       cb_params = {'default_params': cb_params, 'freeze_defaults': True}, # CatBoost params\n                       verbose = 2\n                      )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\noof_pred = automl.fit_predict(train_df,  roles = roles)\ntest_pred = automl.predict(test_df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission['loss'] = test_pred.data[:, 0]\nsample_submission.to_csv('lightautomlsubmission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**# LightAutoML Competition Submission Score is 7.89745**","metadata":{}},{"cell_type":"markdown","source":"# H2o.ai AutoML\n\n![](https://docs.h2o.ai/h2o/latest-stable/h2o-docs/_static/logo.png)\n\n*H2O is an open source, in-memory, distributed, fast, and scalable machine learning and predictive analytics platform that allows you to build machine learning models on big data and provides easy productionalization of those models in an enterprise environment.*\n\n* Mainly inferred from main documentation on (h2o.ai) for this regression problem. [For detailed documentation](https://docs.h2o.ai/h2o/latest-stable/h2o-docs/index.html)","metadata":{}},{"cell_type":"code","source":"import h2o\nfrom h2o.automl import H2OAutoML","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"h2o.init()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ntrain_hf = h2o.H2OFrame(train_df.copy())\ntest_hf = h2o.H2OFrame(test_df.copy())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\naml = H2OAutoML(seed=2021, max_runtime_secs=1200, sort_metric = \"RMSE\")\n\naml.train(x = train_hf.columns, y = 'loss',training_frame = train_hf)\n\n# View the AutoML Leaderboard\nlb = aml.leaderboard\nlb.head(rows=lb.nrows)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\npreds = aml.predict(test_hf)\npreds_df = h2o.as_list(preds)\npreds_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission['loss'] = preds_df['predict']\nsample_submission.to_csv('h2o_automl_submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**# H2o.ai Competition Submission Score is 7.91883**","metadata":{}},{"cell_type":"markdown","source":"# FLAML: Fast and Lightweight AutoML by Microsoft Research\n\n![](https://raw.githubusercontent.com/microsoft/FLAML/main/docs/images/FLAML.png)\n\n*FLAML is a lightweight Python library that finds accurate machine learning models automatically, efficiently and economically. It frees users from selecting learners and hyperparameters for each learner. It is fast and economical. The simple and lightweight design makes it easy to extend, such as adding customized learners or metrics. FLAML is powered by a new, cost-effective hyperparameter optimization and learner selection method invented by Microsoft Research*\n\n* Mainly inferred from main documentation on Github repository for this regression problem. [For detailed documentation](https://github.com/microsoft/FLAML)","metadata":{}},{"cell_type":"code","source":"!pip install -U flaml","metadata":{"_kg_hide-input":false,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from flaml import AutoML","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = train_df.drop(['loss'], axis=1)\ny = train_df['loss'].values","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\n# Initialize an AutoML instance\nautoml = AutoML()\n\n# Specify automl goal and constraint\nautoml_settings = {\n    \"time_budget\": 1200,\n    \"metric\": 'rmse',\n    \"task\": 'regression',\n    \"seed\": 2021,\n    \"log_file_name\": 'tpsaug21log.log', \n}\n\n# Train with labeled input data\nautoml.fit(X_train=X , y_train=y,\n                        **automl_settings)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Retrieve best config and best learner\nprint('Best ML leaner:', automl.best_estimator)\nprint('Best hyperparmeter config:', automl.best_config)\nprint('Best accuracy on validation data: {0:.4g}'.format(1-automl.best_loss))\nprint('Training duration of best run: {0:.4g} s'.format(automl.best_config_train_time))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nypred = automl.predict(test_df.values)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ypred","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission['loss'] = ypred\nsample_submission.to_csv('microsoft_flaml_submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**# FLAML Competition Submission score 7.89733**","metadata":{}},{"cell_type":"markdown","source":"# Results\n\n**# LightAutoML Competition Submission Score is 7.88525**\n\n**# H2o.ai Competition Submission Score is 7.95611**\n\n**# FLAML Competition Submission score 7.89733**","metadata":{}},{"cell_type":"markdown","source":"> ***Thank you for reading my notebook. Please don't forget to comment & upvote! ü•≥ü§©ü§ì***","metadata":{}}]}