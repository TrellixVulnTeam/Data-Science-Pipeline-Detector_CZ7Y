{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Tabular Playground Series (August 2021)\n- This notebook covers my code for the Tabular Playground Series - August challenge, which can be found [here](https://www.kaggle.com/c/tabular-playground-series-aug-2021)\n- In this notebook, I have used various EDA techniques, which includes:\n    - PCC (Pearson Correlation Coefficient), I have simply eliminated all those features having PCC with 'loss' less than abs(0.005)\n    - Using Standard Scaler for the Standardization of all the features\n    - PCA (Principal Component Analysis), but it didn't gave any improvement in the results, so didn't used it in the final submission\n- As for the training part, I used various models, which includes\n    - Gradient Boosted Decision Tree (GBDT)\n    - Linear Regression (LR)\n    - Histogram Gradient Boosted Regressor\n    - Cat Boost Regressor\n- If you liked my work, do upvote it :)","metadata":{}},{"cell_type":"markdown","source":"# Installing & Importing Packages","metadata":{}},{"cell_type":"code","source":"!pip install catboost","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-08-17T16:40:48.506769Z","iopub.execute_input":"2021-08-17T16:40:48.507155Z","iopub.status.idle":"2021-08-17T16:40:55.913435Z","shell.execute_reply.started":"2021-08-17T16:40:48.50707Z","shell.execute_reply":"2021-08-17T16:40:55.912294Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom tqdm import tqdm\nfrom sklearn.experimental import enable_hist_gradient_boosting \nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import HistGradientBoostingRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom catboost import CatBoostRegressor\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-17T16:40:55.916247Z","iopub.execute_input":"2021-08-17T16:40:55.916593Z","iopub.status.idle":"2021-08-17T16:40:56.49701Z","shell.execute_reply.started":"2021-08-17T16:40:55.916558Z","shell.execute_reply":"2021-08-17T16:40:56.49572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Importing the Dataset","metadata":{}},{"cell_type":"code","source":"df_train = pd.read_csv(\"../input/tabular-playground-series-aug-2021/train.csv\")\ndf_test = pd.read_csv(\"../input/tabular-playground-series-aug-2021/test.csv\")\ndf_sub = pd.read_csv(\"../input/tabular-playground-series-aug-2021/sample_submission.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-08-17T16:40:56.499105Z","iopub.execute_input":"2021-08-17T16:40:56.499561Z","iopub.status.idle":"2021-08-17T16:41:04.530567Z","shell.execute_reply.started":"2021-08-17T16:40:56.499516Z","shell.execute_reply":"2021-08-17T16:41:04.529568Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df_train.shape)\ndf_train.info(verbose=True, null_counts=True)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-08-17T16:41:04.531867Z","iopub.execute_input":"2021-08-17T16:41:04.532146Z","iopub.status.idle":"2021-08-17T16:41:04.619542Z","shell.execute_reply.started":"2021-08-17T16:41:04.532118Z","shell.execute_reply":"2021-08-17T16:41:04.61878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df_test.shape)\ndf_test.info(verbose=True, null_counts=True)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-08-17T16:41:04.620539Z","iopub.execute_input":"2021-08-17T16:41:04.620945Z","iopub.status.idle":"2021-08-17T16:41:04.678397Z","shell.execute_reply.started":"2021-08-17T16:41:04.620916Z","shell.execute_reply":"2021-08-17T16:41:04.677688Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Keeping a separator variable and the target variable\nsep = df_train.shape[0]\nY = df_train[\"loss\"]\n\n# Dropping the IDs and the target variable\ndf_train.drop([\"id\", \"loss\"], axis=1, inplace=True)\ndf_test.drop([\"id\"], axis=1, inplace=True)\n\n# Concatenating the datasets for pre-processing\ndf = pd.concat([df_train, df_test], axis=0)\n\nprint(df.shape, Y.shape, sep)","metadata":{"execution":{"iopub.status.busy":"2021-08-17T16:41:04.679541Z","iopub.execute_input":"2021-08-17T16:41:04.679844Z","iopub.status.idle":"2021-08-17T16:41:05.008519Z","shell.execute_reply.started":"2021-08-17T16:41:04.679815Z","shell.execute_reply":"2021-08-17T16:41:05.007384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Visualizing & Pre-processing the Dataset\n- From the above code cells, we can see that all the features are numerical, and corresponding to every feature, all the values are non-null.","metadata":{}},{"cell_type":"code","source":"# Plotting the Distribution of 'Loss'\nplt.hist(Y, 50, density=True, facecolor='g')\nplt.title('Distribution of Loss')\nplt.grid(True)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-17T16:41:05.009734Z","iopub.execute_input":"2021-08-17T16:41:05.01003Z","iopub.status.idle":"2021-08-17T16:41:05.262186Z","shell.execute_reply.started":"2021-08-17T16:41:05.009999Z","shell.execute_reply":"2021-08-17T16:41:05.261166Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We are trying to find PCC (Pearson Correlation Coefficient) between features\n# So that, we can eliminate some of the redundant features. But for plotting the\n# correlation matrix, we will use the training set only.\n\n# Getting the train set\ndf_train = df.iloc[ : sep, : ]\ndf_train = df_train.assign(loss = pd.Series(Y))\nprint(df_train.shape)\n\n# Calculating the PCC\ncor_mat = df_train.corr(method='pearson', min_periods=50)\nprint(cor_mat.shape)\n\n# Number of variables having abs(PCC) with 'loss', less than or equal to 0.005\n# We will simply eliminate those features, as they are related with the 'loss', to the minimum extent\nred_fea = []\nfor i, pcc in enumerate(cor_mat['loss']):\n    if(-0.005 <= pcc and pcc <= 0.005):\n        red_fea.append(cor_mat.index[i])","metadata":{"execution":{"iopub.status.busy":"2021-08-17T16:41:05.264743Z","iopub.execute_input":"2021-08-17T16:41:05.265025Z","iopub.status.idle":"2021-08-17T16:41:12.553043Z","shell.execute_reply.started":"2021-08-17T16:41:05.264998Z","shell.execute_reply":"2021-08-17T16:41:12.551717Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Dropping all the Redundant features\ndf.drop(red_fea, axis=1, inplace=True)\nprint(df.shape)","metadata":{"execution":{"iopub.status.busy":"2021-08-17T16:41:12.555453Z","iopub.execute_input":"2021-08-17T16:41:12.555934Z","iopub.status.idle":"2021-08-17T16:41:12.842765Z","shell.execute_reply.started":"2021-08-17T16:41:12.555886Z","shell.execute_reply":"2021-08-17T16:41:12.841589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Splitting the df back into df_train and df_test\ndf_train = df.iloc[ :sep, : ]\ndf_test = df.iloc[sep: , : ]\nprint(df_train.shape, df_test.shape)","metadata":{"execution":{"iopub.status.busy":"2021-08-17T16:41:12.843971Z","iopub.execute_input":"2021-08-17T16:41:12.844255Z","iopub.status.idle":"2021-08-17T16:41:12.85349Z","shell.execute_reply.started":"2021-08-17T16:41:12.844227Z","shell.execute_reply":"2021-08-17T16:41:12.852357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scaler = StandardScaler()\ndf_train = scaler.fit_transform(df_train)\ndf_test = scaler.transform(df_test)\nprint(df_train.shape, df_test.shape)","metadata":{"execution":{"iopub.status.busy":"2021-08-17T16:41:12.854892Z","iopub.execute_input":"2021-08-17T16:41:12.855184Z","iopub.status.idle":"2021-08-17T16:41:13.349111Z","shell.execute_reply.started":"2021-08-17T16:41:12.855157Z","shell.execute_reply":"2021-08-17T16:41:13.34823Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Dimensionality Reduction using PCA\n# pca = PCA(n_components=None)\n# df_train = pca.fit_transform(df_train)\n# df_test = pca.transform(df_test)\n# print(df_train.shape, df_test.shape)","metadata":{"execution":{"iopub.status.busy":"2021-08-17T16:41:13.350114Z","iopub.execute_input":"2021-08-17T16:41:13.350525Z","iopub.status.idle":"2021-08-17T16:41:13.353378Z","shell.execute_reply.started":"2021-08-17T16:41:13.350488Z","shell.execute_reply":"2021-08-17T16:41:13.352705Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training the Model","metadata":{}},{"cell_type":"code","source":"# Defining the Custom Metric\ndef rmse(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))","metadata":{"execution":{"iopub.status.busy":"2021-08-17T16:41:13.354662Z","iopub.execute_input":"2021-08-17T16:41:13.354982Z","iopub.status.idle":"2021-08-17T16:41:13.368911Z","shell.execute_reply.started":"2021-08-17T16:41:13.354953Z","shell.execute_reply":"2021-08-17T16:41:13.367876Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Splitting the df_train into train & val sets\nX_train, X_val, y_train, y_val = train_test_split(df_train, Y, test_size=0.1, random_state=42)\nprint(X_train.shape, X_val.shape, y_train.shape, y_val.shape)","metadata":{"execution":{"iopub.status.busy":"2021-08-17T16:41:13.370433Z","iopub.execute_input":"2021-08-17T16:41:13.370801Z","iopub.status.idle":"2021-08-17T16:41:13.611828Z","shell.execute_reply.started":"2021-08-17T16:41:13.370766Z","shell.execute_reply":"2021-08-17T16:41:13.61074Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Gradient Boosting Regressor Model\n# lr, nes, mss, ss = 1, 50, 15, 1\n# gbr = GradientBoostingRegressor(\n#     learning_rate=lr, n_estimators=nes, min_samples_split=mss, \n#     subsample=ss, verbose=1\n# )\n# gbr.fit(X_train, y_train)\n# y_pred = gbr.predict(X_val)\n# print(rmse(y_val, y_pred))","metadata":{"execution":{"iopub.status.busy":"2021-08-17T16:41:13.613385Z","iopub.execute_input":"2021-08-17T16:41:13.613879Z","iopub.status.idle":"2021-08-17T16:41:13.618561Z","shell.execute_reply.started":"2021-08-17T16:41:13.613832Z","shell.execute_reply":"2021-08-17T16:41:13.6174Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Linear Regression\n# lr = LinearRegression(normalize=True)\n# lr.fit(X_train, y_train)\n# y_pred = lr.predict(X_val)\n# print(rmse(y_val, y_pred))","metadata":{"execution":{"iopub.status.busy":"2021-08-17T16:41:13.61988Z","iopub.execute_input":"2021-08-17T16:41:13.620185Z","iopub.status.idle":"2021-08-17T16:41:13.634436Z","shell.execute_reply.started":"2021-08-17T16:41:13.620156Z","shell.execute_reply":"2021-08-17T16:41:13.633145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Histogram Gradient Boosting Regressor\n# lr, mi, md = 0.05, 700, 22\n# hgbr = HistGradientBoostingRegressor(\n#     learning_rate = lr, max_iter= mi, \n#     max_depth = md, verbose=1,\n# )\n# hgbr.fit(X_train, y_train)\n# y_pred_train = hgbr.predict(X_train)\n# y_pred_val = hgbr.predict(X_val)\n# print(\"RMSE on Training Dataset \", rmse(y_train, y_pred_train))\n# print(\"RMSE on Validation Dataset \", rmse(y_val, y_pred_val))","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-08-17T16:41:13.636007Z","iopub.execute_input":"2021-08-17T16:41:13.636384Z","iopub.status.idle":"2021-08-17T16:41:13.647796Z","shell.execute_reply.started":"2021-08-17T16:41:13.636302Z","shell.execute_reply":"2021-08-17T16:41:13.646489Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Cat Boosting Regressor\nitr, lr, d = 50, 0.5, 4\ncbr = CatBoostRegressor(\n    iterations = itr, learning_rate = lr, depth = d,\n    custom_metric = 'RMSE', verbose = 1\n)\ncbr.fit(X_train, y_train)\ny_pred_train = cbr.predict(X_train)\ny_pred_val = cbr.predict(X_val)\nprint(\"RMSE on Training Dataset \", rmse(y_train, y_pred_train))\nprint(\"RMSE on Validation Dataset \", rmse(y_val, y_pred_val))","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-08-17T16:41:13.649465Z","iopub.execute_input":"2021-08-17T16:41:13.649847Z","iopub.status.idle":"2021-08-17T16:41:31.435553Z","shell.execute_reply.started":"2021-08-17T16:41:13.649815Z","shell.execute_reply":"2021-08-17T16:41:31.434533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submitting the Predictions","metadata":{}},{"cell_type":"code","source":"# Training the model on the entire df_train\nmodel = CatBoostRegressor(\n    iterations = itr, learning_rate = lr, depth = d,\n    custom_metric = 'RMSE', verbose = 1\n)\nmodel.fit(df_train, Y)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-08-17T16:41:31.437117Z","iopub.execute_input":"2021-08-17T16:41:31.437546Z","iopub.status.idle":"2021-08-17T16:41:33.981748Z","shell.execute_reply.started":"2021-08-17T16:41:31.437501Z","shell.execute_reply":"2021-08-17T16:41:33.980754Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_test = cbr.predict(df_test)\ndf_sub['loss'] = y_test\nprint(df_sub.shape)","metadata":{"execution":{"iopub.status.busy":"2021-08-17T16:41:33.983075Z","iopub.execute_input":"2021-08-17T16:41:33.983355Z","iopub.status.idle":"2021-08-17T16:41:34.027961Z","shell.execute_reply.started":"2021-08-17T16:41:33.983327Z","shell.execute_reply":"2021-08-17T16:41:34.025608Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_sub.to_csv(\"submission.csv\", index = False)","metadata":{"execution":{"iopub.status.busy":"2021-08-17T16:41:34.029063Z","iopub.execute_input":"2021-08-17T16:41:34.029336Z","iopub.status.idle":"2021-08-17T16:41:34.574008Z","shell.execute_reply.started":"2021-08-17T16:41:34.02931Z","shell.execute_reply":"2021-08-17T16:41:34.573013Z"},"trusted":true},"execution_count":null,"outputs":[]}]}