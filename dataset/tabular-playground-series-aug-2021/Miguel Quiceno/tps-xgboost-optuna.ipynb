{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Data processing\n%matplotlib inline\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\n\n# Machine Learning\nimport optuna\nimport xgboost as xgb\nfrom optuna.samplers import TPESampler\nfrom sklearn.preprocessing import RobustScaler, StandardScaler, MinMaxScaler\nfrom sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, KFold\nfrom sklearn.metrics import mean_squared_error","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-12T14:36:21.816923Z","iopub.execute_input":"2021-08-12T14:36:21.817357Z","iopub.status.idle":"2021-08-12T14:36:23.303787Z","shell.execute_reply.started":"2021-08-12T14:36:21.817266Z","shell.execute_reply":"2021-08-12T14:36:23.302892Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_dir = Path('../input/tabular-playground-series-aug-2021/')\ntrain_df = pd.read_csv(input_dir / 'train.csv')\ntest_df = pd.read_csv(input_dir / 'test.csv')\nsample_submission = pd.read_csv(input_dir / 'sample_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2021-08-12T14:36:41.335336Z","iopub.execute_input":"2021-08-12T14:36:41.33583Z","iopub.status.idle":"2021-08-12T14:36:51.3465Z","shell.execute_reply.started":"2021-08-12T14:36:41.335784Z","shell.execute_reply":"2021-08-12T14:36:51.345619Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-12T14:37:01.544081Z","iopub.execute_input":"2021-08-12T14:37:01.544486Z","iopub.status.idle":"2021-08-12T14:37:01.587911Z","shell.execute_reply.started":"2021-08-12T14:37:01.54445Z","shell.execute_reply":"2021-08-12T14:37:01.58695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-12T14:37:04.38667Z","iopub.execute_input":"2021-08-12T14:37:04.387014Z","iopub.status.idle":"2021-08-12T14:37:04.418772Z","shell.execute_reply.started":"2021-08-12T14:37:04.386982Z","shell.execute_reply":"2021-08-12T14:37:04.417423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-12T14:37:06.956758Z","iopub.execute_input":"2021-08-12T14:37:06.957112Z","iopub.status.idle":"2021-08-12T14:37:06.96706Z","shell.execute_reply.started":"2021-08-12T14:37:06.957079Z","shell.execute_reply":"2021-08-12T14:37:06.965906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = train_df.drop(['id', 'loss'], axis=1).values\ny = train_df['loss'].values\nX_test = test_df.drop(['id'], axis=1).values","metadata":{"execution":{"iopub.status.busy":"2021-08-12T14:37:28.660877Z","iopub.execute_input":"2021-08-12T14:37:28.66126Z","iopub.status.idle":"2021-08-12T14:37:28.855491Z","shell.execute_reply.started":"2021-08-12T14:37:28.661223Z","shell.execute_reply":"2021-08-12T14:37:28.854601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# I've found many using MinMaxScaling but I've personally had better results with StandardScaling\nscaler = StandardScaler()\nX = scaler.fit_transform(X)\nX_test = scaler.transform(X_test)","metadata":{"execution":{"iopub.status.busy":"2021-08-12T14:37:36.852999Z","iopub.execute_input":"2021-08-12T14:37:36.853426Z","iopub.status.idle":"2021-08-12T14:37:37.457081Z","shell.execute_reply.started":"2021-08-12T14:37:36.853392Z","shell.execute_reply":"2021-08-12T14:37:37.456209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_min = y.min()\ny_max = y.max()\n\n# While it's probably rare that values will fall outside the y-min-max range, we should probably do it anyway.\ndef my_rmse(y_true, y_hat):\n    y_true[y_true < y_min] = y_min\n    y_true[y_true > y_max] = y_max\n    \n    y_true[y_hat < y_min] = y_min\n    y_true[y_hat > y_max] = y_max\n    \n    return mean_squared_error(y_true, y_hat, squared=False)","metadata":{"execution":{"iopub.status.busy":"2021-08-12T14:38:10.549981Z","iopub.execute_input":"2021-08-12T14:38:10.550359Z","iopub.status.idle":"2021-08-12T14:38:10.556324Z","shell.execute_reply.started":"2021-08-12T14:38:10.550329Z","shell.execute_reply":"2021-08-12T14:38:10.555235Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def objective(trial):\n    # Split the train data for each trial.\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, stratify=y, test_size=0.4)\n\n    param_grid = {\n        'max_depth': trial.suggest_int('max_depth', 6, 10), # Extremely prone to overfitting!\n        'n_estimators': trial.suggest_int('n_estimators', 400, 4000, 400), # Extremely prone to overfitting!\n        'eta': trial.suggest_float('eta', 0.007, 0.013), # Most important parameter - the learning rate!\n        'subsample': trial.suggest_discrete_uniform('subsample', 0.5, 0.9, 0.1),\n        'colsample_bytree': trial.suggest_discrete_uniform('colsample_bytree', 0.5, 0.9, 0.1),\n        'min_child_weight': trial.suggest_int('min_child_weight', 5, 20), # I've had trouble with LB score until tuning this.\n        'reg_lambda': trial.suggest_int('reg_lambda', 1, 50), # L2 regularization\n        'reg_alpha': trial.suggest_int('reg_alpha', 0, 50), # L1 regularization\n    } \n    \n    reg = xgb.XGBRegressor(\n        # These parameters should help with trial speed.\n        tree_method='gpu_hist',\n        predictor='gpu_predictor',\n        n_jobs=4,\n        **param_grid\n    )\n    \n    reg.fit(X_train, y_train,\n            eval_set=[(X_valid, y_valid)], eval_metric='rmse',\n            verbose=False)\n\n    # Returns the best RMSE for the trial.\n    # Readers may want to try returning a cross validation score here.\n    return my_rmse(y_valid, reg.predict(X_valid))","metadata":{"execution":{"iopub.status.busy":"2021-08-12T14:38:34.682765Z","iopub.execute_input":"2021-08-12T14:38:34.683164Z","iopub.status.idle":"2021-08-12T14:38:34.691018Z","shell.execute_reply.started":"2021-08-12T14:38:34.683105Z","shell.execute_reply":"2021-08-12T14:38:34.690199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_time = 1 * 10 * 60 # Train for up to ten minutes.\nstudy = optuna.create_study(direction='minimize', sampler=TPESampler(), study_name='XGBRegressor')\nstudy.optimize(objective, timeout=train_time)\n\nprint('Number of finished trials: ', len(study.trials))\nprint('Best trial:')\ntrial = study.best_trial\n\nprint('\\tValue: {}'.format(trial.value))\nprint('\\tParams: ')\nfor key, value in trial.params.items():\n    print('\\t\\t{}: {}'.format(key, value))","metadata":{"execution":{"iopub.status.busy":"2021-08-12T14:38:41.418657Z","iopub.execute_input":"2021-08-12T14:38:41.419075Z","iopub.status.idle":"2021-08-12T14:48:57.790789Z","shell.execute_reply.started":"2021-08-12T14:38:41.419033Z","shell.execute_reply":"2021-08-12T14:48:57.789619Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fetch the best trial parameters and set some settings for the KFold predictions.\nxgb_params = trial.params\nxgb_params['tree_method'] = 'gpu_hist'\nxgb_params['predictor'] = 'gpu_predictor'\nxgb_params['n_jobs'] = 4\n\nn_splits = 10\ntest_preds = None\nkf_rmse = []\n\nfor fold, (train_idx, valid_idx) in enumerate(KFold(n_splits=n_splits, shuffle=True).split(X, y)):\n    # Fetch the train-validation indices.\n    X_train, y_train = X[train_idx], y[train_idx]\n    X_valid, y_valid = X[valid_idx], y[valid_idx]\n    \n    # Create and fit a new model using the best parameters.\n    model = xgb.XGBRegressor(**xgb_params)\n    model.fit(X_train, y_train,\n            eval_set=[(X_valid, y_valid)],\n            eval_metric='rmse', verbose=False)\n    \n    # Validation predictions.\n    valid_pred = model.predict(X_valid)\n    rmse = my_rmse(y_valid, valid_pred)\n    print(f'Fold {fold+1}/{n_splits} RMSE: {rmse:.4f}')\n    kf_rmse.append(rmse)\n    \n    # Use the model trained for 1/n_splits of the output predictions.\n    if test_preds is None:\n        test_preds = model.predict(X_test)\n    else:\n        # This is kind of naughty for numerical accuracy (may overflow on other problems) but slightly quicker.\n        test_preds += model.predict(X_test)\n\ntest_preds /= n_splits\nprint(f'Average KFold RMSE: {np.mean(np.array(kf_rmse)):.5f}')","metadata":{"execution":{"iopub.status.busy":"2021-08-12T14:49:21.5487Z","iopub.execute_input":"2021-08-12T14:49:21.549027Z","iopub.status.idle":"2021-08-12T14:54:18.597376Z","shell.execute_reply.started":"2021-08-12T14:49:21.548996Z","shell.execute_reply":"2021-08-12T14:54:18.595816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_preds[test_preds < y_min] = y_min\ntest_preds[test_preds > y_max] = y_max\nsample_submission['loss'] = test_preds\nsample_submission.to_csv('submission.csv', index=False)\nsample_submission","metadata":{"execution":{"iopub.status.busy":"2021-08-11T17:28:00.412451Z","iopub.execute_input":"2021-08-11T17:28:00.412767Z","iopub.status.idle":"2021-08-11T17:28:00.803793Z","shell.execute_reply.started":"2021-08-11T17:28:00.412738Z","shell.execute_reply":"2021-08-11T17:28:00.802808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}