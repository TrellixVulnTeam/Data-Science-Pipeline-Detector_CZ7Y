{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Tabular Playground series - August 2021\n\nThe dataset is used for this competition is synthetic, but based on a real dataset and generated using a [CTGAN](https://github.com/sdv-dev/CTGAN). The original dataset deals with calculating the loss associated with a loan defaults. Although the features are anonymized, they have properties relating to real-world features.","metadata":{}},{"cell_type":"markdown","source":"## Baseline model\nChecking the sample_submission.csv within the public leaderboard shows a Root Mean Squared Error score of 10.53201. Aim is to perform initial EDA and build a few baseline models and begin to perform hyperparameter tuning.","metadata":{}},{"cell_type":"code","source":"# Import libraries\nimport numpy as np\nimport pandas as pd\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Review files in the folder\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Switch on setting to allow all outputs to be displayed\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-30T09:11:45.589104Z","iopub.execute_input":"2021-08-30T09:11:45.589515Z","iopub.status.idle":"2021-08-30T09:11:46.520658Z","shell.execute_reply.started":"2021-08-30T09:11:45.589432Z","shell.execute_reply":"2021-08-30T09:11:46.520009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EDA","metadata":{}},{"cell_type":"code","source":"# Create the initial training DataFrame\ntrain = pd.read_csv('../input/tabular-playground-series-aug-2021/train.csv')\ntest = pd.read_csv('../input/tabular-playground-series-aug-2021/test.csv')","metadata":{"execution":{"iopub.status.busy":"2021-08-30T09:11:46.523697Z","iopub.execute_input":"2021-08-30T09:11:46.524191Z","iopub.status.idle":"2021-08-30T09:11:55.899958Z","shell.execute_reply.started":"2021-08-30T09:11:46.524157Z","shell.execute_reply":"2021-08-30T09:11:55.899073Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Review the first five observations\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-30T09:11:55.903485Z","iopub.execute_input":"2021-08-30T09:11:55.903913Z","iopub.status.idle":"2021-08-30T09:11:55.946468Z","shell.execute_reply.started":"2021-08-30T09:11:55.90387Z","shell.execute_reply":"2021-08-30T09:11:55.945454Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check the test dataset\ntest.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-30T09:11:55.951194Z","iopub.execute_input":"2021-08-30T09:11:55.951546Z","iopub.status.idle":"2021-08-30T09:11:55.979331Z","shell.execute_reply.started":"2021-08-30T09:11:55.951509Z","shell.execute_reply":"2021-08-30T09:11:55.978624Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Drop the id variable\nX = train.drop(['id', 'loss'], axis=1)\ny = train['loss']\ntest_id = test['id']\ntest_x = test.drop(['id'], axis=1)\n\n# Review train and test after dropping id\nX.head()\ntest_x.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-30T09:11:55.980215Z","iopub.execute_input":"2021-08-30T09:11:55.98056Z","iopub.status.idle":"2021-08-30T09:11:56.164173Z","shell.execute_reply.started":"2021-08-30T09:11:55.980534Z","shell.execute_reply":"2021-08-30T09:11:56.163233Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Shape of the dataframe\nprint(train.shape)\n# Find the number of rows within a dataframe\nprint(len(train))\n# Extracting information from the shape tuple\nprint(f'Number of rows: {train.shape[0]} \\nNumber of columns: {train.shape[1]}')","metadata":{"execution":{"iopub.status.busy":"2021-08-30T09:11:56.165381Z","iopub.execute_input":"2021-08-30T09:11:56.165897Z","iopub.status.idle":"2021-08-30T09:11:56.172256Z","shell.execute_reply.started":"2021-08-30T09:11:56.165857Z","shell.execute_reply":"2021-08-30T09:11:56.171397Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Review the high level summary details for each variable\ntrain.describe()","metadata":{"execution":{"iopub.status.busy":"2021-08-30T09:11:56.174117Z","iopub.execute_input":"2021-08-30T09:11:56.174393Z","iopub.status.idle":"2021-08-30T09:11:57.329328Z","shell.execute_reply.started":"2021-08-30T09:11:56.174367Z","shell.execute_reply":"2021-08-30T09:11:57.328548Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Variable types\ntrain.dtypes.value_counts()\n# All variables are numeric so don't have to worry about working with strings","metadata":{"execution":{"iopub.status.busy":"2021-08-30T09:11:57.33038Z","iopub.execute_input":"2021-08-30T09:11:57.330721Z","iopub.status.idle":"2021-08-30T09:11:57.33834Z","shell.execute_reply.started":"2021-08-30T09:11:57.330695Z","shell.execute_reply":"2021-08-30T09:11:57.337391Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Missing values review","metadata":{}},{"cell_type":"code","source":"# Proportion of missing values by column\ndef isnull_prop(df):\n    total_rows = df.shape[0]\n    missing_val_dict = {}\n    for col in df.columns:\n        missing_val_dict[col] = [df[col].isnull().sum(), (df[col].isnull().sum() / total_rows)]\n    return missing_val_dict\n\n# Apply the missing value method\nnull_dict = isnull_prop(train)","metadata":{"execution":{"iopub.status.busy":"2021-08-30T09:11:57.339397Z","iopub.execute_input":"2021-08-30T09:11:57.339637Z","iopub.status.idle":"2021-08-30T09:11:57.487582Z","shell.execute_reply.started":"2021-08-30T09:11:57.339614Z","shell.execute_reply":"2021-08-30T09:11:57.486722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a dataframe of the missing value information\ndf_missing = pd.DataFrame.from_dict(null_dict, orient=\"index\", columns=['missing', 'miss_percent'])\ndf_missing.loc[(df_missing['missing'] > 0)]","metadata":{"execution":{"iopub.status.busy":"2021-08-30T09:11:57.488676Z","iopub.execute_input":"2021-08-30T09:11:57.488926Z","iopub.status.idle":"2021-08-30T09:11:57.513609Z","shell.execute_reply.started":"2021-08-30T09:11:57.488901Z","shell.execute_reply":"2021-08-30T09:11:57.512715Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Target variable - Loss","metadata":{}},{"cell_type":"code","source":"# Method - review the distribution of the target variable\ndef sns_displot(df, col):\n    # set the histogram, mean and median\n    sns.displot(df[col], kde=False)\n    plt.axvline(x=df[col].mean(), linewidth=3, color='g', label=\"mean\", alpha=0.5)\n    plt.axvline(x=df[col].median(), linewidth=3, color='y', label=\"median\", alpha=0.5)\n\n    # set title, legends and labels\n    plt.xlabel(f'{col}')\n    plt.ylabel(\"Count\")\n    plt.title(f'Distribution of {col}', size=14)\n    plt.legend([\"mean\", \"median\"]);\n\n    print(f'Mean {col} value {df[col].mean()} \\n Median {col} value {df[col].median()} \\n Min {col} value {df[col].min()} \\n Max {col} value {df[col].max()}')","metadata":{"execution":{"iopub.status.busy":"2021-08-30T09:11:57.514721Z","iopub.execute_input":"2021-08-30T09:11:57.514951Z","iopub.status.idle":"2021-08-30T09:11:57.520916Z","shell.execute_reply.started":"2021-08-30T09:11:57.514929Z","shell.execute_reply":"2021-08-30T09:11:57.520041Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns_displot(train, 'loss')","metadata":{"execution":{"iopub.status.busy":"2021-08-30T09:11:57.522457Z","iopub.execute_input":"2021-08-30T09:11:57.522837Z","iopub.status.idle":"2021-08-30T09:11:58.291957Z","shell.execute_reply.started":"2021-08-30T09:11:57.522708Z","shell.execute_reply":"2021-08-30T09:11:58.291062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There is a positive skew present as the mean is greater than the median. A large proportion of the values are zero so not all rows have experienced the same loss. ","metadata":{}},{"cell_type":"code","source":"# Lets understand the common values\nprint(f'Average rate of zero: {train.loc[(train.loss == 0), \"loss\"].count() / train.shape[0]}')\nprint(f'{train.loss.value_counts()}')","metadata":{"execution":{"iopub.status.busy":"2021-08-30T09:11:58.293083Z","iopub.execute_input":"2021-08-30T09:11:58.293334Z","iopub.status.idle":"2021-08-30T09:11:58.305867Z","shell.execute_reply.started":"2021-08-30T09:11:58.29331Z","shell.execute_reply":"2021-08-30T09:11:58.304647Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Before we get into Model Predictions we need to understand which independent variables help to predict the binary loss classifier. By converting the target variable to a binary classifier first we can explore which models help to predict loss before we aim to predict the loss value","metadata":{}},{"cell_type":"markdown","source":"# Dimension Reduction - Binary classifier","metadata":{}},{"cell_type":"code","source":"# Import modules\n# Preprocessing\nfrom sklearn.preprocessing import scale\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Classifiers\n# from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\n\n# Hyperparameter tuning\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import randint\n\n# Performance metrics\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import roc_auc_score\n\n# Dimension Reduction techniques\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_selection import RFE\nfrom yellowbrick.model_selection import feature_importances\nfrom sklearn.feature_selection import SelectFromModel","metadata":{"execution":{"iopub.status.busy":"2021-08-30T09:44:24.726887Z","iopub.execute_input":"2021-08-30T09:44:24.727293Z","iopub.status.idle":"2021-08-30T09:44:24.734694Z","shell.execute_reply.started":"2021-08-30T09:44:24.727259Z","shell.execute_reply":"2021-08-30T09:44:24.733722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lets create the binary y_target variable\ny_target = np.where(y > 0, 1, 0)\ny_target.view()\nprint(f'Proportion of loss values {np.average(y_target)}')","metadata":{"execution":{"iopub.status.busy":"2021-08-30T09:12:57.465284Z","iopub.execute_input":"2021-08-30T09:12:57.465588Z","iopub.status.idle":"2021-08-30T09:12:57.474188Z","shell.execute_reply.started":"2021-08-30T09:12:57.465561Z","shell.execute_reply":"2021-08-30T09:12:57.473536Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sc = StandardScaler()\nX_scaled = sc.fit_transform(X)\n\npca = PCA(n_components=10)\nX_pca = pca.fit_transform(X_scaled)","metadata":{"execution":{"iopub.status.busy":"2021-08-30T09:14:32.595732Z","iopub.execute_input":"2021-08-30T09:14:32.596172Z","iopub.status.idle":"2021-08-30T09:14:35.880957Z","shell.execute_reply.started":"2021-08-30T09:14:32.596134Z","shell.execute_reply":"2021-08-30T09:14:35.880172Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# define the pipeline\nsteps = [\n        ('scaler', StandardScaler()),\n        ('pca', PCA(n_components=10)), \n        ('m', LogisticRegression())\n]\nmodel = Pipeline(steps=steps)\n# evaluate model\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\nn_scores = cross_val_score(model, X, y_target, scoring='accuracy', cv=cv, n_jobs=-1)\n# report performance\nprint('Accuracy: %.3f (%.3f)' % (np.mean(n_scores), np.std(n_scores)))","metadata":{"execution":{"iopub.status.busy":"2021-08-30T09:20:12.208819Z","iopub.execute_input":"2021-08-30T09:20:12.209334Z","iopub.status.idle":"2021-08-30T09:20:47.073573Z","shell.execute_reply.started":"2021-08-30T09:20:12.209292Z","shell.execute_reply":"2021-08-30T09:20:47.072698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lets try a Random Forest\nrf = RandomForestClassifier(n_estimators=100, max_depth=3,\n                                 bootstrap=True, n_jobs=-1,\n                                 random_state=0)\nsc = StandardScaler()\nX_scaled = sc.fit_transform(X)\n\nrf.fit(X_scaled, y_target)\n\nfeature_imp = pd.Series(rf.feature_importances_, \n                        index=X.columns).sort_values(ascending=False)\n\nprint('Feature importances: ', rf.feature_importances_)\nprint(sns.barplot(x=feature_imp, y=feature_imp.index))\nplt.xlabel('Feature Importance Score', fontsize=12)\nplt.ylabel('Features', fontsize=12)\nplt.title(\"Visualizing Important Features\", fontsize=15, pad=15)","metadata":{"execution":{"iopub.status.busy":"2021-08-30T10:49:28.160382Z","iopub.execute_input":"2021-08-30T10:49:28.16076Z","iopub.status.idle":"2021-08-30T10:50:05.098206Z","shell.execute_reply.started":"2021-08-30T10:49:28.160728Z","shell.execute_reply":"2021-08-30T10:50:05.09702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lets put a threshold on the feature importance score\nselector = SelectFromModel(rf, threshold=0.05)\nfeatures_important = selector.fit_transform(X, y_target)\n\nX_vars = X.loc[:, selector.get_support()]\nX_vars.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-30T10:56:01.759695Z","iopub.execute_input":"2021-08-30T10:56:01.760086Z","iopub.status.idle":"2021-08-30T10:56:37.044859Z","shell.execute_reply.started":"2021-08-30T10:56:01.760052Z","shell.execute_reply":"2021-08-30T10:56:37.043846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lets try reviewing the data with the standard scaler switched off\n# Lets try a Random Forest\nrf = RandomForestClassifier(n_estimators=100, max_depth=3,\n                                 bootstrap=True, n_jobs=-1,\n                                 random_state=0)\n\nrf.fit(X, y_target)\n\nfeature_imp = pd.Series(rf.feature_importances_, \n                        index=X.columns).sort_values(ascending=False)\n\nprint('Feature importances: ', rf.feature_importances_)\nprint(sns.barplot(x=feature_imp, y=feature_imp.index))\nplt.xlabel('Feature Importance Score', fontsize=12)\nplt.ylabel('Features', fontsize=12)\nplt.title(\"Visualizing Important Features\", fontsize=15, pad=15)","metadata":{"execution":{"iopub.status.busy":"2021-08-30T11:09:18.437134Z","iopub.execute_input":"2021-08-30T11:09:18.437525Z","iopub.status.idle":"2021-08-30T11:09:55.151307Z","shell.execute_reply.started":"2021-08-30T11:09:18.43749Z","shell.execute_reply":"2021-08-30T11:09:55.150281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lets put a threshold on the feature importance score\nselector = SelectFromModel(rf, threshold=0.05)\nfeatures_important = selector.fit_transform(X, y_target)\n\nX_vars1 = X.loc[:, selector.get_support()]\nX_vars1.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-30T11:09:55.152585Z","iopub.execute_input":"2021-08-30T11:09:55.152838Z","iopub.status.idle":"2021-08-30T11:10:30.631603Z","shell.execute_reply.started":"2021-08-30T11:09:55.152812Z","shell.execute_reply":"2021-08-30T11:10:30.630917Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create the train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X_vars1, \n                                                    y, \n                                                    test_size=0.3, \n                                                    random_state=42)\n# X_train, X_test, y_train, y_test = train_test_split(X_vars, \n#                                                     y, \n#                                                     test_size=0.3, \n#                                                     random_state=42)","metadata":{"execution":{"iopub.status.busy":"2021-08-30T11:13:25.655059Z","iopub.execute_input":"2021-08-30T11:13:25.655416Z","iopub.status.idle":"2021-08-30T11:13:25.691595Z","shell.execute_reply.started":"2021-08-30T11:13:25.655384Z","shell.execute_reply":"2021-08-30T11:13:25.690605Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model analysis - Negative Binomial","metadata":{}},{"cell_type":"code","source":"# Create the evaluation metric - RMSE\nfrom sklearn.metrics import mean_squared_error as mse\n\ndef rmse(actual, predicted):\n    mse_val = mse(actual, predicted)\n    return np.sqrt(mse_val)","metadata":{"execution":{"iopub.status.busy":"2021-08-30T11:12:51.420155Z","iopub.execute_input":"2021-08-30T11:12:51.420566Z","iopub.status.idle":"2021-08-30T11:12:51.425524Z","shell.execute_reply.started":"2021-08-30T11:12:51.420536Z","shell.execute_reply":"2021-08-30T11:12:51.424493Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#train the NB2 model on the training data set\nimport statsmodels.api as sm\n\nneg_bin = sm.GLM(y_train, X_train,family=sm.families.NegativeBinomial()).fit()\n\n#print the training summary\nprint(neg_bin.summary())","metadata":{"execution":{"iopub.status.busy":"2021-08-30T11:13:29.358577Z","iopub.execute_input":"2021-08-30T11:13:29.35895Z","iopub.status.idle":"2021-08-30T11:13:30.25237Z","shell.execute_reply.started":"2021-08-30T11:13:29.358919Z","shell.execute_reply":"2021-08-30T11:13:30.251331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lets predict with a negative binomial model\ny_pred = np.round(neg_bin.predict(X_test))\nrmse(y_test, y_pred)","metadata":{"execution":{"iopub.status.busy":"2021-08-30T11:13:33.994163Z","iopub.execute_input":"2021-08-30T11:13:33.994502Z","iopub.status.idle":"2021-08-30T11:13:34.013138Z","shell.execute_reply.started":"2021-08-30T11:13:33.994474Z","shell.execute_reply":"2021-08-30T11:13:34.012019Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred.view()\ny_test.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-30T11:13:36.524063Z","iopub.execute_input":"2021-08-30T11:13:36.524428Z","iopub.status.idle":"2021-08-30T11:13:36.535342Z","shell.execute_reply.started":"2021-08-30T11:13:36.524395Z","shell.execute_reply":"2021-08-30T11:13:36.534387Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model analysis - Zero Inflated Poisson regression","metadata":{}},{"cell_type":"markdown","source":"It might make more sense to split the challenge into two separate issues. First predict if there was a loss or not. Assign 0 to loss values of 0 and 1 to all other values. Then a second element of the task would be to predict the loss for values greater than zero.\n***\n1. The first challenge would be a binary logistic regression task\n2. Perform a poisson regression to predict the losses greater than 0\n***\nThis task can be achieved by using the Zero-Inflated Poisson Regression","metadata":{}},{"cell_type":"code","source":"# Train the Zero Inflated Poisson model\nzip_reg = sm.ZeroInflatedPoisson(endog=y_train, exog=X_train, exog_infl=X_train, inflation='logit').fit()\nprint(zip_reg.summary())","metadata":{"execution":{"iopub.status.busy":"2021-08-30T11:13:53.533741Z","iopub.execute_input":"2021-08-30T11:13:53.534177Z","iopub.status.idle":"2021-08-30T11:14:13.075723Z","shell.execute_reply.started":"2021-08-30T11:13:53.53414Z","shell.execute_reply":"2021-08-30T11:14:13.074712Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Poisson","metadata":{}},{"cell_type":"code","source":"# Poisson Regression\nfrom sklearn.linear_model import PoissonRegressor\n\npoisson_glm = Pipeline([\n    (\"scaler\", StandardScaler()),\n    (\"regressor\", PoissonRegressor(alpha=1e-12, max_iter=300))\n])\npoisson_glm.fit(X_train, y_train)\ny_pred = poisson_glm.predict(X_test)\nrmse(y_test, y_pred)","metadata":{"execution":{"iopub.status.busy":"2021-08-30T11:18:52.514663Z","iopub.execute_input":"2021-08-30T11:18:52.515053Z","iopub.status.idle":"2021-08-30T11:18:52.749171Z","shell.execute_reply.started":"2021-08-30T11:18:52.515021Z","shell.execute_reply":"2021-08-30T11:18:52.748025Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Predictions\nzip_predictions = zip_reg.predict(X_test,exog_infl=X_test)\npredicted_counts=np.round(zip_predictions)\nprint(f'RMSE : {rmse(y_test, predicted_counts)}')","metadata":{"execution":{"iopub.status.busy":"2021-08-30T11:14:18.541485Z","iopub.execute_input":"2021-08-30T11:14:18.541825Z","iopub.status.idle":"2021-08-30T11:14:18.569051Z","shell.execute_reply.started":"2021-08-30T11:14:18.541796Z","shell.execute_reply":"2021-08-30T11:14:18.568039Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Work to do to get this working correctly\n# fig, axes = plt.subplots(nrows=2, ncols=4, figsize=(16, 6), sharey=True)\n# fig.subplots_adjust(bottom=0.2)\n# n_bins = 20\n# for row_idx, label, df in zip(range(2),\n#                               [\"train\", \"test\"],\n#                               [y_train, y_test]):\n#     df.hist(bins=np.linspace(-1, 30, n_bins),\n#                          ax=axes[row_idx, 0])\n\n#     axes[row_idx, 0].set_title(\"Data\")\n#     axes[row_idx, 0].set_yscale('log')\n#     axes[row_idx, 0].set_xlabel(\"y (observed Frequency)\")\n#     axes[row_idx, 0].set_ylim([1e1, 5e5])\n#     axes[row_idx, 0].set_ylabel(label + \" samples\")\n\n#     for idx, model in enumerate([dummy, ridge, poisson_glm]):\n#         y_pred = model.predict(X_test)\n\n#         pd.Series(y_pred).hist(bins=np.linspace(-1, 4, n_bins),\n#                                ax=axes[row_idx, idx+1])\n#         axes[row_idx, idx + 1].set(\n#             title=model[-1].__class__.__name__,\n#             yscale='log',\n#             xlabel=\"y_pred (predicted expected Loss)\"\n#         )\n# plt.tight_layout();","metadata":{"execution":{"iopub.status.busy":"2021-08-24T08:09:39.209985Z","iopub.execute_input":"2021-08-24T08:09:39.210386Z","iopub.status.idle":"2021-08-24T08:09:39.215199Z","shell.execute_reply.started":"2021-08-24T08:09:39.210351Z","shell.execute_reply":"2021-08-24T08:09:39.214043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Make submission","metadata":{}},{"cell_type":"code","source":"# Create model submission method using the test_x and test_id variables\ndef submission(model, csv_name):\n    pred = model.predict(test_x)\n    df = pd.DataFrame(data={'id': test_id, 'loss': pred})\n    df = df.set_index('id')\n    return df.to_csv(f\"Submission_file_{csv_name}.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-08-30T11:14:27.71482Z","iopub.execute_input":"2021-08-30T11:14:27.715153Z","iopub.status.idle":"2021-08-30T11:14:27.719452Z","shell.execute_reply.started":"2021-08-30T11:14:27.71512Z","shell.execute_reply":"2021-08-30T11:14:27.71876Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create the code for the ZIP prediction\ntest_x_new = test_x.loc[:, X_train.columns]","metadata":{"execution":{"iopub.status.busy":"2021-08-30T11:15:12.85449Z","iopub.execute_input":"2021-08-30T11:15:12.855139Z","iopub.status.idle":"2021-08-30T11:15:12.861892Z","shell.execute_reply.started":"2021-08-30T11:15:12.855086Z","shell.execute_reply":"2021-08-30T11:15:12.861089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# submission(neg_bin, \"neg_bin_glm\")","metadata":{"execution":{"iopub.status.busy":"2021-08-30T11:16:16.202787Z","iopub.execute_input":"2021-08-30T11:16:16.203409Z","iopub.status.idle":"2021-08-30T11:16:16.207513Z","shell.execute_reply.started":"2021-08-30T11:16:16.203358Z","shell.execute_reply":"2021-08-30T11:16:16.206478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Scaled datasets\ndef submission_scaled(model, csv_name):\n    pred = model.predict(test_x_new)\n    df = pd.DataFrame(data={'id': test_id, 'loss': pred})\n    df = df.set_index('id')\n    return df.to_csv(f\"Submission_file_{csv_name}.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-08-30T11:15:27.565626Z","iopub.execute_input":"2021-08-30T11:15:27.566215Z","iopub.status.idle":"2021-08-30T11:15:27.570654Z","shell.execute_reply.started":"2021-08-30T11:15:27.566178Z","shell.execute_reply":"2021-08-30T11:15:27.569987Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# submission_scaled(neg_bin, \"neg_bin\")\n# submission_scaled(zip_reg, \"zip_reg\")\nsubmission_scaled(poisson_glm, \"poisson\")","metadata":{"execution":{"iopub.status.busy":"2021-08-30T11:19:27.493267Z","iopub.execute_input":"2021-08-30T11:19:27.493882Z","iopub.status.idle":"2021-08-30T11:19:27.93341Z","shell.execute_reply.started":"2021-08-30T11:19:27.493844Z","shell.execute_reply":"2021-08-30T11:19:27.932574Z"},"trusted":true},"execution_count":null,"outputs":[]}]}