{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## [TPS-AUG] Simple EDA\n\nThis time, it's a competition with so many features.\n\nFor a fun contest, I lightly conducted EDA.\n\n- No missing value.\n- There are 100 numerical continuous features.\n- The target variable loss ranges from 0 to 42 for a total of 43 discrete values. However, this is a regression problem and it is OK to submit as decimal values.","metadata":{}},{"cell_type":"markdown","source":"## Load Data & Library\n\nLet's load a library for basic data.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-04T13:20:33.053935Z","iopub.execute_input":"2021-08-04T13:20:33.054337Z","iopub.status.idle":"2021-08-04T13:20:34.007042Z","shell.execute_reply.started":"2021-08-04T13:20:33.054256Z","shell.execute_reply":"2021-08-04T13:20:34.00623Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# matplotlib setting\nmpl.rcParams['figure.dpi'] = 200\nmpl.rcParams['axes.spines.top'] = False\nmpl.rcParams['axes.spines.right'] = False","metadata":{"execution":{"iopub.status.busy":"2021-08-04T13:20:34.008576Z","iopub.execute_input":"2021-08-04T13:20:34.009215Z","iopub.status.idle":"2021-08-04T13:20:34.01433Z","shell.execute_reply.started":"2021-08-04T13:20:34.009164Z","shell.execute_reply":"2021-08-04T13:20:34.013246Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('../input/tabular-playground-series-aug-2021/train.csv')\ntest = pd.read_csv('../input/tabular-playground-series-aug-2021/test.csv')\nsample_submission = pd.read_csv('../input/tabular-playground-series-aug-2021/sample_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2021-08-04T13:20:34.016167Z","iopub.execute_input":"2021-08-04T13:20:34.016615Z","iopub.status.idle":"2021-08-04T13:20:45.204797Z","shell.execute_reply.started":"2021-08-04T13:20:34.016573Z","shell.execute_reply":"2021-08-04T13:20:45.20391Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'Train Shape :  {train.shape}')\nprint(f'Test Shape :  {test.shape}')","metadata":{"execution":{"iopub.status.busy":"2021-08-04T13:20:45.206231Z","iopub.execute_input":"2021-08-04T13:20:45.206514Z","iopub.status.idle":"2021-08-04T13:20:45.211304Z","shell.execute_reply.started":"2021-08-04T13:20:45.206487Z","shell.execute_reply":"2021-08-04T13:20:45.210247Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target = train['loss']\ntrain.drop(['id'], axis=1, inplace=True)\ntest.drop(['id'], axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-08-04T13:20:45.212484Z","iopub.execute_input":"2021-08-04T13:20:45.212786Z","iopub.status.idle":"2021-08-04T13:20:45.34746Z","shell.execute_reply.started":"2021-08-04T13:20:45.212757Z","shell.execute_reply":"2021-08-04T13:20:45.346485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-04T13:20:45.349027Z","iopub.execute_input":"2021-08-04T13:20:45.349446Z","iopub.status.idle":"2021-08-04T13:20:45.391141Z","shell.execute_reply.started":"2021-08-04T13:20:45.349404Z","shell.execute_reply":"2021-08-04T13:20:45.389876Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.info()","metadata":{"execution":{"iopub.status.busy":"2021-08-04T13:20:45.392706Z","iopub.execute_input":"2021-08-04T13:20:45.393117Z","iopub.status.idle":"2021-08-04T13:20:45.413942Z","shell.execute_reply.started":"2021-08-04T13:20:45.393084Z","shell.execute_reply":"2021-08-04T13:20:45.412802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(1, 1, figsize=(17, 8))\n\ntarget_cnt = train['loss'].value_counts().sort_index()\n\nax.bar(target_cnt.index, target_cnt, color=['#d4dddd' if i%2==0 else '#fafafa' for i in range(9)],\n       width=0.55, \n       edgecolor='black', \n       linewidth=0.7)\n\nax.margins(0.02, 0.05)\n\nfor i in range(20):\n    ax.annotate(f'{target_cnt[i]/len(train)*100:.3}', xy=(i, target_cnt[i]+1000),\n                   va='center', ha='center',\n               )\n\nax.set_title('Target Distribution', weight='bold', fontsize=15)\nax.grid(axis='y', linestyle='-', alpha=0.4)\n\nfig.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-04T13:20:45.417952Z","iopub.execute_input":"2021-08-04T13:20:45.418485Z","iopub.status.idle":"2021-08-04T13:20:46.146524Z","shell.execute_reply.started":"2021-08-04T13:20:45.418436Z","shell.execute_reply":"2021-08-04T13:20:46.1458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- There are a total of 43 discrete losses.\n- The top 12 distributions account for 80% of the total.\n- All except the order of 2 and 1 are in increasing order.","metadata":{}},{"cell_type":"code","source":"target_cnt_df = pd.DataFrame(target_cnt)\ntarget_cnt_df['ratio(%)'] = target_cnt_df/target_cnt.sum()*100\ntarget_cnt_df.sort_values('ratio(%)', ascending=False, inplace=True)\ntarget_cnt_df['cummulated_sum(%)'] = target_cnt_df['ratio(%)'].cumsum()\ntarget_cnt_df.style.bar(subset=['cummulated_sum(%)'], color='#205ff2')","metadata":{"execution":{"iopub.status.busy":"2021-08-04T13:20:46.147778Z","iopub.execute_input":"2021-08-04T13:20:46.148138Z","iopub.status.idle":"2021-08-04T13:20:46.21405Z","shell.execute_reply.started":"2021-08-04T13:20:46.14811Z","shell.execute_reply":"2021-08-04T13:20:46.213106Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Statistics Check\n\nThe scale of this data is really diverse.\n\nIt doesn't matter if you use a tree-based model, but scaling is essential by default.","metadata":{}},{"cell_type":"code","source":"train.describe().T.style.bar(subset=['mean'], color='#205ff2')\\\n                            .background_gradient(subset=['std'], cmap='Reds')\\\n                            .background_gradient(subset=['50%'], cmap='coolwarm')","metadata":{"execution":{"iopub.status.busy":"2021-08-04T13:20:46.215149Z","iopub.execute_input":"2021-08-04T13:20:46.215415Z","iopub.status.idle":"2021-08-04T13:20:47.441391Z","shell.execute_reply.started":"2021-08-04T13:20:46.21539Z","shell.execute_reply":"2021-08-04T13:20:47.440351Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- There is data without decimal point.\n- The range of data is diverse.","metadata":{}},{"cell_type":"markdown","source":"## Discrete Features\n\nSome data are found to have no decimal point.","metadata":{}},{"cell_type":"code","source":"discrete_features = []\n\nfor col in train.columns:\n    if np.array_equal(train[col].values, train[col].values.astype(int)):\n        discrete_features.append(col)\n\nprint(f'Total {len(discrete_features)} : ')\nprint(discrete_features)","metadata":{"execution":{"iopub.status.busy":"2021-08-04T13:20:47.442515Z","iopub.execute_input":"2021-08-04T13:20:47.442813Z","iopub.status.idle":"2021-08-04T13:20:47.518195Z","shell.execute_reply.started":"2021-08-04T13:20:47.442786Z","shell.execute_reply":"2021-08-04T13:20:47.517191Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A total of 6 features have no decimal point.\n\n- `f1`\n- `f16`\n- `f27`\n- `f55`\n- `f60`\n- `f86`\n","metadata":{}},{"cell_type":"code","source":"for dcol in discrete_features:\n    print(f'{dcol} unique value : {train[dcol].nunique()}')","metadata":{"execution":{"iopub.status.busy":"2021-08-04T13:20:47.519495Z","iopub.execute_input":"2021-08-04T13:20:47.519815Z","iopub.status.idle":"2021-08-04T13:20:47.587455Z","shell.execute_reply.started":"2021-08-04T13:20:47.519785Z","shell.execute_reply":"2021-08-04T13:20:47.586517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"While the total number of data is 250000, most of the data in `f16` and `f60` are confirmed as continuous with different values, but the remaining `f1`, `f27`, `f55`, and `f86` look relatively categorical.\n\nLooking at f1 and f86 with a small number of unique values:\nFor the relationship with the loss, we averaged after groupby.","metadata":{}},{"cell_type":"code","source":"f1_loss = train.groupby(['f1'])['loss'].mean().sort_values()\nprint((f1_loss==0).sum())","metadata":{"execution":{"iopub.status.busy":"2021-08-04T13:20:47.588544Z","iopub.execute_input":"2021-08-04T13:20:47.588856Z","iopub.status.idle":"2021-08-04T13:20:47.600909Z","shell.execute_reply.started":"2021-08-04T13:20:47.588829Z","shell.execute_reply":"2021-08-04T13:20:47.599878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(1, 1, figsize=(20, 6))\n\nax.bar(range(len(f1_loss)), f1_loss, alpha=0.7, color='lightgray', label='Test Dataset')\nax.set_yticks(range(0, 20, 3))\nax.margins(0.01)\nax.grid(axis='y', linestyle='--', zorder=5)\nax.set_title('Average of loss grouped by f1', loc='left', fontweight='bold')\nax.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-04T13:20:47.602298Z","iopub.execute_input":"2021-08-04T13:20:47.602717Z","iopub.status.idle":"2021-08-04T13:20:48.893895Z","shell.execute_reply.started":"2021-08-04T13:20:47.602674Z","shell.execute_reply":"2021-08-04T13:20:48.892575Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Depending on the value of f1, we can check the imbalance of loss.\n- In 5 cases, we confirmed that the loss is all 0.","metadata":{"execution":{"iopub.status.busy":"2021-08-01T01:59:51.82913Z","iopub.execute_input":"2021-08-01T01:59:51.829478Z","iopub.status.idle":"2021-08-01T01:59:51.83573Z","shell.execute_reply.started":"2021-08-01T01:59:51.829447Z","shell.execute_reply":"2021-08-01T01:59:51.834345Z"}}},{"cell_type":"code","source":"f86_loss = train.groupby(['f86'])['loss'].mean().sort_values()\nprint((f86_loss==0).sum())","metadata":{"execution":{"iopub.status.busy":"2021-08-04T13:20:48.895374Z","iopub.execute_input":"2021-08-04T13:20:48.895822Z","iopub.status.idle":"2021-08-04T13:20:48.910057Z","shell.execute_reply.started":"2021-08-04T13:20:48.895769Z","shell.execute_reply":"2021-08-04T13:20:48.908898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(1, 1, figsize=(20, 6))\n\nax.bar(range(len(f86_loss)), f86_loss, alpha=0.7, color='lightgray', label='Test Dataset')\nax.set_yticks(range(0, 20, 3))\nax.margins(0.01)\nax.grid(axis='y', linestyle='--', zorder=5)\nax.set_title('Average of loss grouped by f86', loc='left', fontweight='bold')\nax.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-04T13:20:48.911468Z","iopub.execute_input":"2021-08-04T13:20:48.911879Z","iopub.status.idle":"2021-08-04T13:20:50.229616Z","shell.execute_reply.started":"2021-08-04T13:20:48.911846Z","shell.execute_reply":"2021-08-04T13:20:50.228621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- I noticed that there is an imbalance, though not as much as f1.","metadata":{}},{"cell_type":"markdown","source":"## Scaling\n\nExcept for tree-based models, you need to scale the data.\n\nBefore visualization, we will adjust the line and proceed with the visualization.","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\nss = StandardScaler()\nfeatures = [f'f{i}' for i in range(100)]\ntrain[features] = ss.fit_transform(train[features])\ntest[features] = ss.transform(test[features])","metadata":{"execution":{"iopub.status.busy":"2021-08-04T13:20:50.231067Z","iopub.execute_input":"2021-08-04T13:20:50.231637Z","iopub.status.idle":"2021-08-04T13:21:02.312152Z","shell.execute_reply.started":"2021-08-04T13:20:50.231594Z","shell.execute_reply":"2021-08-04T13:21:02.311169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Target & Feature Relation\n\nAs the value of targets increases, the mean moves away from zero.","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(1,1, figsize=(12, 7))\nsns.heatmap(train.groupby('loss').mean().sort_index(),\n            square=True, center=0, linewidth=1,\n            cmap=sns.diverging_palette(240, 10, as_cmap=True),\n            cbar=False, \n           )\n\nax.set_title('Mean : Group by Target(Loss)',loc='left')\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's take a look by adjusting the range of expression.","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(1,1, figsize=(12, 7))\nsns.heatmap(train.groupby('loss').mean().sort_index(),\n            square=True, vmin=-0.5, vmax=0.5, center=0, linewidth=1,\n            cmap=sns.diverging_palette(240, 10, as_cmap=True),\n            cbar=False, \n           )\n\nax.set_title('Mean : Group by Target(Loss)',loc='left')\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There seems to be an increasing trend and a decreasing trend.","metadata":{}},{"cell_type":"markdown","source":"## Feature Distribution","metadata":{}},{"cell_type":"code","source":"fig, axes = plt.subplots(10,10,figsize=(12, 12))\naxes = axes.flatten()\n\nfor idx, ax in enumerate(axes):\n    sns.kdeplot(data=train, x=f'f{idx}', \n                fill=True, \n                ax=ax)\n    sns.kdeplot(data=test, x=f'f{idx}', \n                fill=True, \n                ax=ax)\n    ax.set_xticks([])\n    ax.set_yticks([])\n    ax.set_xlabel('')\n    ax.set_ylabel('')\n    ax.spines['left'].set_visible(False)\n    ax.set_title(f'f{idx}', loc='right', weight='bold', fontsize=10)\n\nfig.supxlabel('Average by class (by feature)', ha='center', fontweight='bold')\n\nfig.tight_layout()\nplt.show()\n    \n    ","metadata":{"execution":{"iopub.status.busy":"2021-08-04T13:21:02.313468Z","iopub.execute_input":"2021-08-04T13:21:02.313782Z","iopub.status.idle":"2021-08-04T13:22:54.752231Z","shell.execute_reply.started":"2021-08-04T13:21:02.313751Z","shell.execute_reply":"2021-08-04T13:22:54.747612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It's scaled up, but it's a pretty interesting aspect of the data.\n\nIt is safe to assume that the distributions of train and test are almost the same.","metadata":{}},{"cell_type":"markdown","source":"## Correlation\n\nIt can be seen that most of the correlations are close to zero.","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(1, 1, figsize=(12 , 12))\n\ncorr = train.corr()\n\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\nsns.heatmap(corr, ax=ax,\n        square=True, center=0, linewidth=1,\n        cmap=sns.diverging_palette(240, 10, as_cmap=True),\n        cbar_kws={\"shrink\": .82},    \n        mask=mask\n       ) \n\nax.set_title(f'Correlation', loc='left', fontweight='bold')     \n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-04T13:22:54.75351Z","iopub.status.idle":"2021-08-04T13:22:54.754152Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}