{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-29T21:42:09.307481Z","iopub.execute_input":"2021-08-29T21:42:09.307849Z","iopub.status.idle":"2021-08-29T21:42:09.316929Z","shell.execute_reply.started":"2021-08-29T21:42:09.307818Z","shell.execute_reply":"2021-08-29T21:42:09.315802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv(\"/kaggle/input/tabular-playground-series-aug-2021/train.csv\")\ntrain_df.info()","metadata":{"execution":{"iopub.status.busy":"2021-08-29T21:42:09.318617Z","iopub.execute_input":"2021-08-29T21:42:09.319273Z","iopub.status.idle":"2021-08-29T21:42:16.112285Z","shell.execute_reply.started":"2021-08-29T21:42:09.319231Z","shell.execute_reply":"2021-08-29T21:42:16.111367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub_test_df = pd.read_csv(\"/kaggle/input/tabular-playground-series-aug-2021/test.csv\")\nsub_test_df.info()","metadata":{"execution":{"iopub.status.busy":"2021-08-29T21:42:16.113914Z","iopub.execute_input":"2021-08-29T21:42:16.114414Z","iopub.status.idle":"2021-08-29T21:42:19.731316Z","shell.execute_reply.started":"2021-08-29T21:42:16.114376Z","shell.execute_reply":"2021-08-29T21:42:19.729836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-29T21:42:19.732845Z","iopub.execute_input":"2021-08-29T21:42:19.733125Z","iopub.status.idle":"2021-08-29T21:42:19.767151Z","shell.execute_reply.started":"2021-08-29T21:42:19.733098Z","shell.execute_reply":"2021-08-29T21:42:19.766067Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.describe()","metadata":{"execution":{"iopub.status.busy":"2021-08-29T21:42:19.768738Z","iopub.execute_input":"2021-08-29T21:42:19.769119Z","iopub.status.idle":"2021-08-29T21:42:20.800923Z","shell.execute_reply.started":"2021-08-29T21:42:19.769075Z","shell.execute_reply":"2021-08-29T21:42:20.799992Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(figsize=(36,36))\nax = sns.heatmap(train_df.corr(), vmax=.04)","metadata":{"execution":{"iopub.status.busy":"2021-08-29T21:42:20.802455Z","iopub.execute_input":"2021-08-29T21:42:20.802851Z","iopub.status.idle":"2021-08-29T21:42:31.257517Z","shell.execute_reply.started":"2021-08-29T21:42:20.802813Z","shell.execute_reply":"2021-08-29T21:42:31.256663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"top_corrs = train_df.corr().unstack().sort_values(ascending=False)","metadata":{"execution":{"iopub.status.busy":"2021-08-29T21:42:31.259829Z","iopub.execute_input":"2021-08-29T21:42:31.26018Z","iopub.status.idle":"2021-08-29T21:42:37.448328Z","shell.execute_reply.started":"2021-08-29T21:42:31.260141Z","shell.execute_reply":"2021-08-29T21:42:37.447461Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"top_corrs[top_corrs < 1].plot()","metadata":{"execution":{"iopub.status.busy":"2021-08-29T21:42:37.45018Z","iopub.execute_input":"2021-08-29T21:42:37.450544Z","iopub.status.idle":"2021-08-29T21:42:37.623654Z","shell.execute_reply.started":"2021-08-29T21:42:37.450505Z","shell.execute_reply":"2021-08-29T21:42:37.62266Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rstate = 3495743\ndf = train_df.copy()\n#split into train validation test for ML\ntrain_df = df.sample(frac=.8, random_state=rstate)\nvalid_df = df[~df.id.isin(train_df.id)].sample(frac=.5,random_state=rstate)\ntest_df = df[~df.id.isin(train_df.id.to_list()+valid_df.id.to_list())]\n\ntrain_df.shape, valid_df.shape, test_df.shape","metadata":{"execution":{"iopub.status.busy":"2021-08-29T21:42:37.625283Z","iopub.execute_input":"2021-08-29T21:42:37.625687Z","iopub.status.idle":"2021-08-29T21:42:37.998541Z","shell.execute_reply.started":"2021-08-29T21:42:37.625648Z","shell.execute_reply":"2021-08-29T21:42:37.997575Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.id.isin(test_df.id).sum(),train_df.id.isin(valid_df.id).sum()","metadata":{"execution":{"iopub.status.busy":"2021-08-29T21:42:38.000035Z","iopub.execute_input":"2021-08-29T21:42:38.000378Z","iopub.status.idle":"2021-08-29T21:42:38.017786Z","shell.execute_reply.started":"2021-08-29T21:42:38.000341Z","shell.execute_reply":"2021-08-29T21:42:38.0168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn \nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.preprocessing import MinMaxScaler","metadata":{"execution":{"iopub.status.busy":"2021-08-29T21:42:38.019265Z","iopub.execute_input":"2021-08-29T21:42:38.019656Z","iopub.status.idle":"2021-08-29T21:42:39.224328Z","shell.execute_reply.started":"2021-08-29T21:42:38.019617Z","shell.execute_reply":"2021-08-29T21:42:39.223525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndevice ","metadata":{"execution":{"iopub.status.busy":"2021-08-29T21:42:39.225615Z","iopub.execute_input":"2021-08-29T21:42:39.225956Z","iopub.status.idle":"2021-08-29T21:42:39.285316Z","shell.execute_reply.started":"2021-08-29T21:42:39.22592Z","shell.execute_reply":"2021-08-29T21:42:39.284503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class KaggleTabularDataset(Dataset):\n    def __init__(self,df,scaler=None):\n        self.df = df\n        if scaler:\n            self.scaler = scaler\n            self.data_label = self.scaler.transform(self.df.iloc[:,1:-1].values)\n        else:\n            self.scaler = MinMaxScaler(feature_range=(-1,1))\n            self.data_label = self.scaler.fit_transform(self.df.iloc[:,1:-1].values)\n            \n        self.id = self.df.iloc[:,0].values\n        self.data = self.data_label[:,:]\n        self.label = self.df.iloc[:,-1:].values # self.data_label[:,-2:-1]\n        \n    def __len__(self):\n        return  self.data.shape[0]\n\n    def __getitem__(self, idx):\n        X = torch.tensor(self.data[idx]).float().to(device)\n        y = torch.tensor(self.label[idx]).float().to(device)\n        return X, y","metadata":{"execution":{"iopub.status.busy":"2021-08-29T21:42:39.286763Z","iopub.execute_input":"2021-08-29T21:42:39.287496Z","iopub.status.idle":"2021-08-29T21:42:39.297215Z","shell.execute_reply.started":"2021-08-29T21:42:39.287448Z","shell.execute_reply":"2021-08-29T21:42:39.296215Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = KaggleTabularDataset(train_df)\nvalid_data = KaggleTabularDataset(valid_df, scaler=train_data.scaler)\ntest_data = KaggleTabularDataset(test_df, scaler=train_data.scaler)\n","metadata":{"execution":{"iopub.status.busy":"2021-08-29T21:42:39.29861Z","iopub.execute_input":"2021-08-29T21:42:39.299216Z","iopub.status.idle":"2021-08-29T21:42:39.592615Z","shell.execute_reply.started":"2021-08-29T21:42:39.299175Z","shell.execute_reply":"2021-08-29T21:42:39.591758Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub_test_df['loss'] = np.nan\nsub_test_data = KaggleTabularDataset(sub_test_df, scaler=train_data.scaler)","metadata":{"execution":{"iopub.status.busy":"2021-08-29T21:42:39.593818Z","iopub.execute_input":"2021-08-29T21:42:39.594155Z","iopub.status.idle":"2021-08-29T21:42:39.748023Z","shell.execute_reply.started":"2021-08-29T21:42:39.594118Z","shell.execute_reply":"2021-08-29T21:42:39.7471Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# this is one way to define a network\nclass Net(torch.nn.Module):\n    def __init__(self, n_feature, n_shrink, n_hidden,  n_output):\n        super(Net, self).__init__()\n        self.hidden1 = nn.Sequential(\n            torch.nn.Linear(n_feature, n_shrink),   # hidden layer\n            #nn.Dropout(p=0.02),\n            nn.LayerNorm(n_shrink),\n            nn.ReLU(),\n            torch.nn.Linear(n_shrink, n_hidden),   # hidden layer\n            #nn.Dropout(p=0.02),\n            nn.LayerNorm(n_hidden),\n            nn.ReLU()\n        )\n\n        self.combined = torch.nn.Linear(n_hidden, n_hidden)\n        self.predict = torch.nn.Linear(n_hidden, n_output)   # output layer\n\n    def forward(self, x):\n        x = self.hidden1(x)  # activation function for hidden layer\n        x = self.combined(x)\n        x = self.predict(x)*40             # linear output\n        return x","metadata":{"execution":{"iopub.status.busy":"2021-08-29T21:42:39.749382Z","iopub.execute_input":"2021-08-29T21:42:39.749769Z","iopub.status.idle":"2021-08-29T21:42:39.756885Z","shell.execute_reply.started":"2021-08-29T21:42:39.749726Z","shell.execute_reply":"2021-08-29T21:42:39.756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class FixedAttention(torch.nn.Module):\n    def __init__(self, n_feature):\n        super(FixedAttention, self).__init__()\n        self.Q = nn.parameter.Parameter(torch.rand((n_feature,50)))\n        self.wV = nn.parameter.Parameter(torch.rand((n_feature,50)))\n        self.alpha = None\n        self.softmax = nn.Softmax(1)\n        self.d_k = torch.sqrt(torch.tensor(n_feature).float())\n                              \n    def forward(self, x):\n        self.alpha = self.softmax(torch.matmul(x,self.Q)/self.d_k)\n        V = torch.matmul(x,self.wV)\n        return torch.sum(self.alpha*V,1).unsqueeze(-1)\n\nclass AutoQueryNetV1(torch.nn.Module):\n    def __init__(self, n_feature, n_head, n_hidden, n_output):\n        super(AutoQueryNetV1, self).__init__()\n        \n        self.multi_heads = nn.ModuleList([FixedAttention(n_feature).to(device) for _ in range(n_head)])\n        self.combined = torch.nn.Linear(n_head, n_hidden)\n        self.predict = torch.nn.Linear(n_hidden, n_output)   # output layer\n        \n    def forward(self, x):\n        x = torch.cat([head(x) for head in self.multi_heads],-1)\n        #print(x.shape)\n        x = self.combined(x)\n        x = self.predict(x)*10 \n        return x","metadata":{"execution":{"iopub.status.busy":"2021-08-29T21:42:39.758341Z","iopub.execute_input":"2021-08-29T21:42:39.758926Z","iopub.status.idle":"2021-08-29T21:42:39.77155Z","shell.execute_reply.started":"2021-08-29T21:42:39.758885Z","shell.execute_reply":"2021-08-29T21:42:39.770689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Norm(nn.Module):\n    def __init__(self, d_model, eps = 1e-6):\n        super().__init__()\n    \n        self.size = d_model\n        # create two learnable parameters to calibrate normalisation\n        self.alpha = nn.Parameter(torch.ones(self.size))\n        self.bias = nn.Parameter(torch.zeros(self.size))\n        self.eps = eps\n        \n    def forward(self, x):\n        norm = self.alpha * (x - x.mean(dim=-1, keepdim=True)) \\\n        / (x.std(dim=-1, keepdim=True) + self.eps) + self.bias\n        return norm\n    \nclass MultiHeadAttention(torch.nn.Module):\n    def __init__(self, n_feature,n_head, n_attn):\n        super(MultiHeadAttention, self).__init__()\n        condition = (n_attn / n_head) % 2 == 0 and n_attn % n_head == 0 and n_attn / n_head >= 20\n        if not condition:\n            raise AssertionError()\n        self.n_feature = n_feature\n        self.n_head = n_head\n        self.n_attn = n_attn\n        self.Q = nn.parameter.Parameter(torch.rand((n_head, int(n_attn/n_head), n_feature)))#.to(device)\n        self.wV = nn.parameter.Parameter(torch.rand((n_feature,n_attn)))#.to(device)\n        self.wK = nn.parameter.Parameter(torch.rand((n_feature,n_feature)))#.to(device)\n        \n        self.alpha = None\n        self.softmax = nn.Softmax(1)\n        self.d_k = torch.sqrt(torch.tensor(n_feature/n_head).float()).to(device)\n                              \n    def forward(self, x):\n        K = torch.matmul(x, self.wK)\n        QK_T = torch.matmul(self.Q,K.T)\n        self.alpha = self.softmax(QK_T/self.d_k).permute(2,0,1)\n        V = torch.matmul(x,self.wV).reshape(-1,self.n_head,int(self.n_attn/self.n_head))\n        #print(V.shape,self.alpha.shape)\n        return torch.sum(self.alpha*V,2)\n\nclass AutoQueryNetV2(torch.nn.Module):\n    def __init__(self, n_feature, n_head, n_attn, n_hidden, n_output):\n        super(AutoQueryNetV2, self).__init__()\n\n        self.multi_heads = MultiHeadAttention(n_feature, n_head, n_attn)\n        self.norm1 = Norm(n_head)\n        self.combined = torch.nn.Linear(n_head, n_hidden)\n        self.norm2 = nn.LayerNorm(n_hidden)\n        self.predict = torch.nn.Linear(n_hidden, n_output)   # output layer\n        \n        \n    def forward(self, x):\n        x = self.multi_heads(x)\n        x = self.norm1(x)\n        #print(x.shape)\n        x = self.combined(x)\n        x = self.norm2(x)\n        x = self.predict(x)*10 \n        return x","metadata":{"execution":{"iopub.status.busy":"2021-08-29T21:42:39.773098Z","iopub.execute_input":"2021-08-29T21:42:39.773565Z","iopub.status.idle":"2021-08-29T21:42:39.792076Z","shell.execute_reply.started":"2021-08-29T21:42:39.773524Z","shell.execute_reply":"2021-08-29T21:42:39.79093Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Attention(torch.nn.Module):\n    def __init__(self, n_feature):\n        super(Attention, self).__init__()\n        n_latent = 16\n        self.wQ = nn.parameter.Parameter(torch.rand((n_feature,n_latent)))\n        self.wK = nn.parameter.Parameter(torch.rand((n_feature,n_latent)))\n        self.wV = nn.parameter.Parameter(torch.rand((n_feature,n_latent)))\n        \n        self.alpha = None\n        self.softmax = nn.Softmax(1)\n        self.d_k = torch.sqrt(torch.tensor(n_feature).float())\n                              \n    def forward(self, x):\n        Q = torch.matmul(x, self.wQ) # (b_sz,n_fea)x(n_fea,latent_attn_dim) -> (b_sz,latent_attn_dim)\n        K = torch.matmul(x, self.wK) # (b_sz,n_fea)x(n_fea,latent_attn_dim) -> (b_sz,latent_attn_dim)\n        self.alpha = self.softmax(Q*K/self.d_k) # (b_sz,latent_attn_dim)\n        V = torch.matmul(x,self.wV) # (b_sz,n_fea)x(n_fea,latent_attn_dim) -> (b_sz,latent_attn_dim)\n        return torch.sum(self.alpha*V,1).unsqueeze(-1)\n\nclass AutoQueryNetV3(torch.nn.Module):\n    def __init__(self, n_feature, n_head, n_hidden, n_output):\n        super(AutoQueryNetV3, self).__init__()\n        \n        self.multi_heads = nn.ModuleList([Attention(n_feature).to(device) for _ in range(n_head)])\n        self.combined = torch.nn.Linear(n_head, n_hidden)\n        self.predict = torch.nn.Linear(n_hidden, n_output)   # output layer\n        \n    def forward(self, x):\n        x = torch.cat([head(x) for head in self.multi_heads],-1)\n        #print(x.shape)\n        x = self.combined(x)\n        x = self.predict(x)*10 \n        return x","metadata":{"execution":{"iopub.status.busy":"2021-08-29T21:56:53.263249Z","iopub.execute_input":"2021-08-29T21:56:53.263605Z","iopub.status.idle":"2021-08-29T21:56:53.276223Z","shell.execute_reply.started":"2021-08-29T21:56:53.263558Z","shell.execute_reply":"2021-08-29T21:56:53.274881Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def RMSELoss(yhat,y):\n    return torch.sqrt(torch.mean((yhat-y)**2))","metadata":{"execution":{"iopub.status.busy":"2021-08-29T21:56:53.917113Z","iopub.execute_input":"2021-08-29T21:56:53.917442Z","iopub.status.idle":"2021-08-29T21:56:53.921955Z","shell.execute_reply.started":"2021-08-29T21:56:53.917409Z","shell.execute_reply":"2021-08-29T21:56:53.920745Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def training_loop(data, model, val_data=None, t_data=None, verbose=2, num_epoch=25, patient=7):\n    batch_loss = []\n    epoch_loss = []\n    min_loss = np.inf\n    pat_ct = 0\n    \n    model.train()\n    for epoch in range(num_epoch):\n        pat_ct+=1\n        data_iterator = iter(data)\n        n_batch = len(data_iterator)\n        for batch,(X,y) in enumerate(data_iterator):\n            output = model(X)\n            loss = loss_func(output, y) \n\n            \n            optimizer.zero_grad()   # clear gradients for next train\n            loss.backward()         # backpropagation, compute gradients\n            optimizer.step()        # apply gradients\n            batch_loss+=[loss.item()]\n            \n            if batch%int(n_batch/5) == 0 and verbose>1:\n                print(f\"batch {batch+1}/{n_batch} loss: {sum(batch_loss)/len(batch_loss)}\")\n            \n        epoch_loss+=batch_loss\n        batch_loss = []\n        \n        print(f\"epoch {epoch+1}/{num_epoch} loss: {sum(epoch_loss)/len(epoch_loss)}\")\n        if val_data:\n            eval_loss = model_eval(val_data, model)\n            if t_data:\n                model_eval(t_data, model, test=True)\n            if min_loss > eval_loss:\n                print('saving best model')\n                min_loss = eval_loss\n                torch.save(model, 'best_model')\n                pat_ct = 0\n                \n        if pat_ct == patient: #each patient is an epoch that no best model according to val data loss\n            print('ran out of patient, loading best model')\n            model = torch.load('best_model')\n            return model\n        else:\n            print(f'patient {pat_ct}/{patient}')\n            \n    return model\n            \n\n                \ndef model_eval(data, model, test=False):\n    batch_loss = []\n    \n    with torch.no_grad():\n        model.eval()\n        data_iterator = iter(data)\n\n        for batch,(X,y) in enumerate(data_iterator):\n            output = model(X)\n            loss = loss_func(output, y)\n            batch_loss+=[loss.item()]\n\n    print(f\"{'test' if test else 'validation'} loss: {sum(batch_loss)/len(batch_loss)}\")\n    return sum(batch_loss)/len(batch_loss)","metadata":{"execution":{"iopub.status.busy":"2021-08-29T21:56:54.13662Z","iopub.execute_input":"2021-08-29T21:56:54.136952Z","iopub.status.idle":"2021-08-29T21:56:54.148763Z","shell.execute_reply.started":"2021-08-29T21:56:54.136921Z","shell.execute_reply":"2021-08-29T21:56:54.147866Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def flatten(t):\n    return [item for sublist in t for item in sublist]","metadata":{"execution":{"iopub.status.busy":"2021-08-29T21:56:54.348479Z","iopub.execute_input":"2021-08-29T21:56:54.348833Z","iopub.status.idle":"2021-08-29T21:56:54.35283Z","shell.execute_reply.started":"2021-08-29T21:56:54.348802Z","shell.execute_reply":"2021-08-29T21:56:54.351932Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#model = Net(n_feature=100, n_shrink=50, n_hidden=20, n_output=1)     # define the network\n\n# print(net)  # net architecture\n# model = Net(n_feature=100, n_shrink=50, n_hidden=20, n_output=1).to(device)\nloss_func = RMSELoss#torch.nn.MSELoss()  # this is for regression mean squared loss\n#model = AutoQueryNetV1(100, 10, 64, 1).to(device)\n#model = AutoQueryNetV2(100, 10, 200, 64, 1).to(device)\nmodel = AutoQueryNetV3(100, 32, 64, 1).to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\nprint(model)\n[p.shape for p in model.parameters()]","metadata":{"execution":{"iopub.status.busy":"2021-08-29T21:57:02.311337Z","iopub.execute_input":"2021-08-29T21:57:02.311752Z","iopub.status.idle":"2021-08-29T21:57:02.348632Z","shell.execute_reply.started":"2021-08-29T21:57:02.311706Z","shell.execute_reply":"2021-08-29T21:57:02.347525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_sz = 100\ndataset = DataLoader(train_data, batch_size=batch_sz, shuffle=True)\nvalid_dataset = DataLoader(valid_data, batch_size=10000, shuffle=False)\ntest_dataset = DataLoader(test_data, batch_size=10000, shuffle=False)\n\nmodel = training_loop(dataset, model, val_data=valid_dataset,t_data=test_dataset, verbose=2)","metadata":{"execution":{"iopub.status.busy":"2021-08-29T21:57:03.519847Z","iopub.execute_input":"2021-08-29T21:57:03.520172Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_eval(valid_dataset, model),model_eval(test_dataset, model, test=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with torch.no_grad():\n    model.eval()\n    test_df.loss.iloc[:100].plot.bar(figsize=(36,6))\n    pd.DataFrame(model(test_data[:][0]).cpu().detach().numpy()).iloc[:100].plot.bar(figsize=(36,6))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sLoader = DataLoader(sub_test_data, batch_size=1000, shuffle=False)\n\noutputs = []\nwith torch.no_grad():\n    for sX,sy in iter(sLoader):\n        model.eval()\n        batch_out = model(sX).cpu().detach().numpy()\n        outputs+=[batch_out]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub_list = np.vstack(outputs)[:,0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub = pd.read_csv('/kaggle/input/tabular-playground-series-aug-2021/sample_submission.csv')\nsub.loss = sub_list\nsub","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub.to_csv('submission_07.csv',index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}