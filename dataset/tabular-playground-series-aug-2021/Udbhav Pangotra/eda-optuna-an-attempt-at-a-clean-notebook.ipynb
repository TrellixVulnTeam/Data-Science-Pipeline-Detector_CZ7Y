{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# We have a ton of features in this competition, should be fun! \n\n### Initial Observations \n- No missing value.\n- There are 100 numerical continuous features.\n- The target variable loss ranges from 0 to 42 for a total of 43 discrete values. \n- However, this is a regression problem and it is OK to submit as decimal values. ***But can we do a regression + classification?***","metadata":{}},{"cell_type":"markdown","source":"#### I have divided this Notebook in two parts:\n- EDA\n- AutoML (I am just learning this)","metadata":{}},{"cell_type":"markdown","source":"# Importing Libraries and Data for the EDA ","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns \n# matplotlib setting\nmpl.rcParams['figure.dpi'] = 200\nmpl.rcParams['axes.spines.top'] = False\nmpl.rcParams['axes.spines.right'] = False\ntrain = pd.read_csv('../input/tabular-playground-series-aug-2021/train.csv')\ntest = pd.read_csv('../input/tabular-playground-series-aug-2021/test.csv')\nsample_submission = pd.read_csv('../input/tabular-playground-series-aug-2021/sample_submission.csv')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-31T07:09:09.391643Z","iopub.execute_input":"2021-08-31T07:09:09.392004Z","iopub.status.idle":"2021-08-31T07:09:19.762968Z","shell.execute_reply.started":"2021-08-31T07:09:09.391922Z","shell.execute_reply":"2021-08-31T07:09:19.762142Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'Train Shape :  {train.shape}')\nprint(f'Test Shape :  {test.shape}')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-31T07:09:19.764569Z","iopub.execute_input":"2021-08-31T07:09:19.764925Z","iopub.status.idle":"2021-08-31T07:09:19.771Z","shell.execute_reply.started":"2021-08-31T07:09:19.764861Z","shell.execute_reply":"2021-08-31T07:09:19.769997Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Observations: \n- We have 101 columns + 1 target column\n- We have a total data of 250k for the train data and 150k for the test data","metadata":{}},{"cell_type":"code","source":"target = train['loss']\ntrain.drop(['id'], axis=1, inplace=True)\ntest.drop(['id'], axis=1, inplace=True)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-31T07:42:25.872967Z","iopub.execute_input":"2021-08-31T07:42:25.873331Z","iopub.status.idle":"2021-08-31T07:42:25.972792Z","shell.execute_reply.started":"2021-08-31T07:42:25.8733Z","shell.execute_reply":"2021-08-31T07:42:25.971961Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Having a look at the top 2 rows of the train and the test data","metadata":{}},{"cell_type":"code","source":"train.head(2)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-31T07:42:26.650283Z","iopub.execute_input":"2021-08-31T07:42:26.650599Z","iopub.status.idle":"2021-08-31T07:42:26.684458Z","shell.execute_reply.started":"2021-08-31T07:42:26.65057Z","shell.execute_reply":"2021-08-31T07:42:26.683696Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.head(2)","metadata":{"execution":{"iopub.status.busy":"2021-08-31T07:42:26.951684Z","iopub.execute_input":"2021-08-31T07:42:26.951943Z","iopub.status.idle":"2021-08-31T07:42:26.97734Z","shell.execute_reply.started":"2021-08-31T07:42:26.951918Z","shell.execute_reply":"2021-08-31T07:42:26.976448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Info about the train and the test data","metadata":{}},{"cell_type":"code","source":"train.info()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-31T07:42:27.506162Z","iopub.execute_input":"2021-08-31T07:42:27.506477Z","iopub.status.idle":"2021-08-31T07:42:27.52685Z","shell.execute_reply.started":"2021-08-31T07:42:27.50644Z","shell.execute_reply":"2021-08-31T07:42:27.525838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.info(max_cols=10)","metadata":{"execution":{"iopub.status.busy":"2021-08-31T07:42:27.817415Z","iopub.execute_input":"2021-08-31T07:42:27.817659Z","iopub.status.idle":"2021-08-31T07:42:27.831334Z","shell.execute_reply.started":"2021-08-31T07:42:27.817636Z","shell.execute_reply":"2021-08-31T07:42:27.830342Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Now since we are done with the inital data exploration, Let's have a look at the Target Variable Distribution to get an understanding of how the target values are spread ","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(1, 1, figsize=(17, 8))\n\ntarget_cnt = train['loss'].value_counts().sort_index()\n\nax.bar(target_cnt.index, target_cnt, color=['#799EFF' if i%2==0 else '#CCDAFF' for i in range(9)],\n       width=0.55, \n       edgecolor='black', \n       linewidth=0.7)\n\nax.margins(0.02, 0.05)\n\nfor i in range(10):\n    ax.annotate(f'{target_cnt[i]/len(train)*100:.3}', xy=(i, target_cnt[i]+1000),\n                   va='center', ha='center',\n               )\n\nax.set_title('Target Distribution', weight='bold', fontsize=15)\nax.grid(axis='y', linestyle='-', alpha=0.4)\n\nfig.tight_layout()\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-31T07:42:28.334415Z","iopub.execute_input":"2021-08-31T07:42:28.334723Z","iopub.status.idle":"2021-08-31T07:42:28.928006Z","shell.execute_reply.started":"2021-08-31T07:42:28.334693Z","shell.execute_reply":"2021-08-31T07:42:28.927166Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Observations\n- There are a total of 43 discrete losses.\n- The top 12 distributions account for 80% of the total.\n- All except the order of 2 and 1 are in increasing order.","metadata":{}},{"cell_type":"code","source":"target_cnt_df = pd.DataFrame(target_cnt)\ntarget_cnt_df['ratio(%)'] = target_cnt_df/target_cnt.sum()*100\ntarget_cnt_df.sort_values('ratio(%)', ascending=False, inplace=True)\ntarget_cnt_df['cummulated_sum(%)'] = target_cnt_df['ratio(%)'].cumsum()\ntarget_cnt_df.style.bar(subset=['cummulated_sum(%)'], color='#CCDAFF').background_gradient(subset=['ratio(%)'], cmap='binary')\n# target_cnt_df.style.bar(subset=['ratio(%)'], color='#799EFF')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-31T07:42:28.929265Z","iopub.execute_input":"2021-08-31T07:42:28.929598Z","iopub.status.idle":"2021-08-31T07:42:28.998214Z","shell.execute_reply.started":"2021-08-31T07:42:28.929562Z","shell.execute_reply":"2021-08-31T07:42:28.997502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Statistics Check\nThe scale of this data is really diverse. Which makes me think that scaling should be done in this case. Usually we don't need to scaled data if we're using a tree-based model but it is important in case the data is as diverse as this here! \n","metadata":{}},{"cell_type":"code","source":"train.describe()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-31T07:42:29.400858Z","iopub.execute_input":"2021-08-31T07:42:29.401223Z","iopub.status.idle":"2021-08-31T07:42:30.334255Z","shell.execute_reply.started":"2021-08-31T07:42:29.401189Z","shell.execute_reply":"2021-08-31T07:42:30.333478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Observations:\n- The data is very diverse as you can see \n- Feature 0 and 4 can show it very easily, if you do deep dive you will see feature 16,52,60,75,91 also showing very high values and also very high standard deviation \n- There seem to be some data points with discrete values (integer values)\n - f1\n - f16\n - f27\n - f55\n - f60\n - f86\n","metadata":{}},{"cell_type":"markdown","source":"### A deeper dive into these 6 features with discrete values","metadata":{}},{"cell_type":"code","source":"discrete_features = []\n\nfor col in train.columns:\n    if np.array_equal(train[col].values, train[col].values.astype(int)):\n        discrete_features.append(col)\n\nprint(f'Total {len(discrete_features)} : ')\nfor dcol in discrete_features:\n    print(f'{dcol} unique value : {train[dcol].nunique()}')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-31T07:42:30.450844Z","iopub.execute_input":"2021-08-31T07:42:30.451114Z","iopub.status.idle":"2021-08-31T07:42:30.566744Z","shell.execute_reply.started":"2021-08-31T07:42:30.451088Z","shell.execute_reply":"2021-08-31T07:42:30.565747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Observations:\n- While the total number of data is 250000, most of the data in f16 and f60 are confirmed as continuous with different values\n- But the remaining f1, f27, f55, and f86 look relatively categorical.\n\n#### Looking at f1 and f86 with a small number of unique values: For the relationship with the loss, we averaged after groupby.","metadata":{}},{"cell_type":"code","source":"f1_loss = train.groupby(['f1'])['loss'].mean().sort_values()\nfig, ax = plt.subplots(1, 1, figsize=(20, 6))\n\nax.bar(range(len(f1_loss)), f1_loss, alpha=0.7, color='#799EFF', label='Train Dataset')\nax.set_yticks(range(0, 20, 3))\nax.margins(0.01)\nax.grid(axis='y', linestyle='--', zorder=5)\nax.set_title('Average of loss grouped by f1', loc='left', fontweight='bold')\nax.legend()\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-31T07:42:31.219685Z","iopub.execute_input":"2021-08-31T07:42:31.220008Z","iopub.status.idle":"2021-08-31T07:42:32.233527Z","shell.execute_reply.started":"2021-08-31T07:42:31.219977Z","shell.execute_reply":"2021-08-31T07:42:32.232707Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Observations\n- Depending on the value of f1, we can check the imbalance of loss.\n- In 5 cases, we confirmed that the loss is all 0.","metadata":{}},{"cell_type":"code","source":"f86_loss = train.groupby(['f86'])['loss'].mean().sort_values()\nfig, ax = plt.subplots(1, 1, figsize=(20, 6))\n\nax.bar(range(len(f86_loss)), f86_loss, alpha=0.7, color='#799EFF', label='Train Dataset')\nax.set_yticks(range(0, 20, 3))\nax.margins(0.01)\nax.grid(axis='y', linestyle='--', zorder=5)\nax.set_title('Average of loss grouped by f86', loc='left', fontweight='bold')\nax.legend()\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-31T07:42:32.23494Z","iopub.execute_input":"2021-08-31T07:42:32.235349Z","iopub.status.idle":"2021-08-31T07:42:33.241063Z","shell.execute_reply.started":"2021-08-31T07:42:32.23531Z","shell.execute_reply":"2021-08-31T07:42:33.240106Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Scaling the data","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\nss = StandardScaler()\nfeatures = [f'f{i}' for i in range(100)]\ntrain[features] = ss.fit_transform(train[features])\ntest[features] = ss.transform(test[features])","metadata":{"execution":{"iopub.status.busy":"2021-08-31T07:42:33.242734Z","iopub.execute_input":"2021-08-31T07:42:33.243084Z","iopub.status.idle":"2021-08-31T07:42:43.868491Z","shell.execute_reply.started":"2021-08-31T07:42:33.243034Z","shell.execute_reply":"2021-08-31T07:42:43.867582Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Target & Feature Relation\n- As the value of targets increases, the mean moves away from zero.","metadata":{}},{"cell_type":"code","source":"from matplotlib.pyplot import cm\nfig, ax = plt.subplots(1,1, figsize=(12, 7))\nsns.heatmap(train.groupby('loss').mean().sort_index(),\n            square=True, vmin=-0.5, vmax=0.5, center=0, linewidth=1,\n            cmap=sns.diverging_palette(240, 220, as_cmap=True),\n            cbar=False, \n           )\n\nax.set_title('Mean : Group by Target(Loss)',loc='left')\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-31T07:42:43.869874Z","iopub.execute_input":"2021-08-31T07:42:43.870221Z","iopub.status.idle":"2021-08-31T07:42:44.885775Z","shell.execute_reply.started":"2021-08-31T07:42:43.870184Z","shell.execute_reply":"2021-08-31T07:42:44.884698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Distribution","metadata":{}},{"cell_type":"code","source":"fig, axes = plt.subplots(10,10,figsize=(12, 12))\naxes = axes.flatten()\n\nfor idx, ax in enumerate(axes):\n    sns.kdeplot(data=train, x=f'f{idx}', \n                fill=True,color = '#799EFF',\n                ax=ax)\n#     sns.kdeplot(data=test, x=f'f{idx}', \n#                 fill=True,color = 'grey',\n#                 ax=ax)\n    ax.set_xticks([])\n    ax.set_yticks([])\n    ax.set_xlabel('')\n    ax.set_ylabel('')\n    ax.spines['left'].set_visible(False)\n    ax.set_title(f'f{idx}', loc='right', weight='bold', fontsize=10)\n\nfig.supxlabel('Average by class (by feature) Train Dataset', ha='center', fontweight='bold')\n\nfig.tight_layout()\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-31T07:42:44.887441Z","iopub.execute_input":"2021-08-31T07:42:44.887709Z","iopub.status.idle":"2021-08-31T07:44:41.70692Z","shell.execute_reply.started":"2021-08-31T07:42:44.887683Z","shell.execute_reply":"2021-08-31T07:44:41.706077Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\nfig = plt.figure(figsize = (15, 60))\nfor i in range(len(train.columns.tolist()[:100])):\n    plt.subplot(20,5,i+1)\n    sns.set_style(\"white\")\n    plt.title(train.columns.tolist()[:100][i], size = 12, fontname = 'monospace')\n    a = sns.boxplot(train[train.columns.tolist()[:100][i]], linewidth = 2,color = '#799EFF',saturation=1)\n    plt.ylabel('')\n    plt.xlabel('')\n    plt.xticks(fontname = 'monospace')\n    plt.yticks([])\n    for j in ['right', 'left', 'top']:\n        a.spines[j].set_visible(False)\n        a.spines['bottom'].set_linewidth(1.2)\n        \nfig.tight_layout(h_pad = 3)\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-31T07:44:41.708345Z","iopub.execute_input":"2021-08-31T07:44:41.708862Z","iopub.status.idle":"2021-08-31T07:45:03.204728Z","shell.execute_reply.started":"2021-08-31T07:44:41.708823Z","shell.execute_reply":"2021-08-31T07:45:03.203958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Observations:\n- It's scaled up, but it's a pretty interesting aspect of the data.\n\n- It is safe to assume that the distributions of train and test are almost the same.","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(1, 1, figsize=(12 , 12))\n\ncorr = train.corr()\n\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\nsns.heatmap(corr, ax=ax,\n        square=True, center=0, linewidth=1,\n        cmap=sns.diverging_palette(240, 220, as_cmap=True),\n        cbar_kws={\"shrink\": .82},    \n        mask=mask\n       ) \n\nax.set_title(f'Correlation', loc='left', fontweight='bold',)     \n\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-31T07:45:03.205888Z","iopub.execute_input":"2021-08-31T07:45:03.206375Z","iopub.status.idle":"2021-08-31T07:45:11.264977Z","shell.execute_reply.started":"2021-08-31T07:45:03.206337Z","shell.execute_reply":"2021-08-31T07:45:11.263785Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Observations:\n- Most correlations are close to 0","metadata":{}},{"cell_type":"markdown","source":"# Now we do the Modelling ","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.model_selection import train_test_split\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\nfrom catboost import CatBoostRegressor\nimport optuna\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold\ntrain = pd.read_csv(r'../input/tabular-playground-series-aug-2021/train.csv')\ntest = pd.read_csv(r'../input/tabular-playground-series-aug-2021/test.csv')\nsub = pd.read_csv(r'../input/tabular-playground-series-aug-2021/sample_submission.csv')\ny = train['loss']\ntrain.drop('loss',axis=1,inplace=True)\nfeatures = []\nfor feature in train.columns:\n    features.append(feature)\n# print(features)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-31T08:16:04.180845Z","iopub.execute_input":"2021-08-31T08:16:04.181275Z","iopub.status.idle":"2021-08-31T08:16:18.130033Z","shell.execute_reply.started":"2021-08-31T08:16:04.181185Z","shell.execute_reply":"2021-08-31T08:16:18.129094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Min Max Scaler","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nmm = MinMaxScaler()\ntrain[features] = mm.fit_transform(train[features])\ntest[features] = mm.transform(test[features])\nX = train","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-31T08:16:18.131727Z","iopub.execute_input":"2021-08-31T08:16:18.132087Z","iopub.status.idle":"2021-08-31T08:16:28.307804Z","shell.execute_reply.started":"2021-08-31T08:16:18.132047Z","shell.execute_reply":"2021-08-31T08:16:28.306955Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# LightGBM","metadata":{}},{"cell_type":"code","source":"def fit_lgb(trial, x_train, y_train, x_test, y_test):\n    params = {\n        'reg_alpha' : trial.suggest_loguniform('reg_alpha' , 0.47 , 0.5),\n        'reg_lambda' : trial.suggest_loguniform('reg_lambda' , 0.32 , 0.33),\n        'num_leaves' : trial.suggest_int('num_leaves' , 50 , 70),\n        'learning_rate' : trial.suggest_uniform('learning_rate' , 0.03 , 0.04),\n        'max_depth' : trial.suggest_int('max_depth', 30 , 40),\n        'n_estimators' : trial.suggest_int('n_estimators', 100 , 6100),\n        'min_child_weight' : trial.suggest_loguniform('min_child_weight', 0.015 , 0.02),\n        'subsample' : trial.suggest_uniform('subsample' , 0.9 , 1.0), \n        'colsample_bytree' : trial.suggest_loguniform('colsample_bytree', 0.52 , 1),\n        'min_child_samples' : trial.suggest_int('min_child_samples', 76, 80),\n        'metric' : 'rmse',\n        'device_type' : 'gpu',\n    }\n    \n    \n    model = LGBMRegressor(**params, random_state=2021)\n    model.fit(x_train, y_train,eval_set=[(x_test,y_test)], early_stopping_rounds=150, verbose=False)\n    \n    y_train_pred = model.predict(x_train)\n    \n    y_test_pred = model.predict(x_test)\n    y_train_pred = np.clip(y_train_pred, 0.1, None)\n    y_test_pred = np.clip(y_test_pred, 0.1, None)\n    \n    log = {\n        \"train rmse\": mean_squared_error(y_train, y_train_pred,squared=False),\n        \"valid rmse\": mean_squared_error(y_test, y_test_pred,squared=False)\n    }\n    \n    return model, log","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-31T08:16:28.309671Z","iopub.execute_input":"2021-08-31T08:16:28.310034Z","iopub.status.idle":"2021-08-31T08:16:28.31894Z","shell.execute_reply.started":"2021-08-31T08:16:28.309996Z","shell.execute_reply":"2021-08-31T08:16:28.317997Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def objective(trial):\n    rmse = 0\n    x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.15)\n    model, log = fit_lgb(trial, x_train, y_train, x_test, y_test)\n    rmse += log['valid rmse']\n        \n    return rmse","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-31T08:16:28.32048Z","iopub.execute_input":"2021-08-31T08:16:28.321004Z","iopub.status.idle":"2021-08-31T08:16:28.332636Z","shell.execute_reply.started":"2021-08-31T08:16:28.32097Z","shell.execute_reply":"2021-08-31T08:16:28.33183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lgb_params = {'reg_alpha': 0.4972562469417825, 'reg_lambda': 0.3273637203281044, \n          'num_leaves': 50, 'learning_rate': 0.032108486615557354, \n          'max_depth': 40, 'n_estimators': 4060, \n          'min_child_weight': 0.0173353329222102,\n          'subsample': 0.9493343850444064, \n          'colsample_bytree': 0.5328221263825876, 'min_child_samples': 80,'device':'gpu'}\nlgb_params","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-31T08:16:28.333872Z","iopub.execute_input":"2021-08-31T08:16:28.334299Z","iopub.status.idle":"2021-08-31T08:16:28.345519Z","shell.execute_reply.started":"2021-08-31T08:16:28.334263Z","shell.execute_reply":"2021-08-31T08:16:28.344671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def cross_val(X, y, model, params, folds=10):\n\n    kf = KFold(n_splits=folds, shuffle=True, random_state=2021)\n    for fold, (train_idx, test_idx) in enumerate(kf.split(X)):\n        print(f\"Fold: {fold}\")\n        x_train, y_train = X.values[train_idx], y.values[train_idx]\n        x_test, y_test = X.values[test_idx], y.values[test_idx]\n\n        alg = model(**params,random_state = 2021)\n        alg.fit(x_train, y_train,\n                eval_set=[(x_test, y_test)],\n                early_stopping_rounds=400,\n                verbose=False)\n        pred = alg.predict(x_test)\n        error = mean_squared_error(y_test, pred,squared = False)\n        print(f\" mean_squared_error: {error}\")\n        print(\"-\"*50)\n    \n    return alg","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-31T08:16:28.346751Z","iopub.execute_input":"2021-08-31T08:16:28.347191Z","iopub.status.idle":"2021-08-31T08:16:28.356235Z","shell.execute_reply.started":"2021-08-31T08:16:28.347154Z","shell.execute_reply":"2021-08-31T08:16:28.355487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lgb_model = cross_val(X, y, LGBMRegressor, lgb_params)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-31T08:16:28.357517Z","iopub.execute_input":"2021-08-31T08:16:28.357879Z","iopub.status.idle":"2021-08-31T08:22:19.856933Z","shell.execute_reply.started":"2021-08-31T08:16:28.357844Z","shell.execute_reply":"2021-08-31T08:22:19.855986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# XGBoost","metadata":{}},{"cell_type":"code","source":"def fit_xgb(trial, x_train, y_train, x_test, y_test):\n    params = {\n        'tweedie_variance_power': trial.suggest_discrete_uniform('tweedie_variance_power', 1.0, 2.0, 0.1),\n        'max_depth': trial.suggest_int('max_depth', 6, 10), # Extremely prone to overfitting!\n        'n_estimators': trial.suggest_int('n_estimators', 400, 4000, 400), # Extremely prone to overfitting!\n        'eta': trial.suggest_float('eta', 0.007, 0.013), # Most important parameter.\n        'subsample': trial.suggest_discrete_uniform('subsample', 0.2, 0.9, 0.1),\n        'colsample_bytree': trial.suggest_discrete_uniform('colsample_bytree', 0.2, 0.9, 0.1),\n        'colsample_bylevel': trial.suggest_discrete_uniform('colsample_bylevel', 0.2, 0.9, 0.1),\n        'min_child_weight': trial.suggest_loguniform('min_child_weight', 1e-4, 1e4), # I've had trouble with LB score until tuning this.\n        'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-4, 1e4), # L2 regularization\n        'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-4, 1e4), # L1 regularization\n        'gamma': trial.suggest_loguniform('gamma', 1e-4, 1e4)\n    } \n    \n    \n    model = XGBRegressor(**params,tree_method='gpu_hist', random_state=2021)\n    model.fit(x_train, y_train,eval_set=[(x_test,y_test)], early_stopping_rounds=150, verbose=False)\n    \n    y_train_pred = model.predict(x_train)\n    \n    y_test_pred = model.predict(x_test)\n    y_train_pred = np.clip(y_train_pred, 0.1, None)\n    y_test_pred = np.clip(y_test_pred, 0.1, None)\n    \n    log = {\n        \"train rmse\": mean_squared_error(y_train, y_train_pred,squared=False),\n        \"valid rmse\": mean_squared_error(y_test, y_test_pred,squared=False)\n    }\n    \n    return model, log","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-31T08:22:19.860966Z","iopub.execute_input":"2021-08-31T08:22:19.861249Z","iopub.status.idle":"2021-08-31T08:22:19.872569Z","shell.execute_reply.started":"2021-08-31T08:22:19.861219Z","shell.execute_reply":"2021-08-31T08:22:19.871633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def objective(trial):\n    rmse = 0\n    x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.15)\n    model, log = fit_xgb(trial, x_train, y_train, x_test, y_test)\n    rmse += log['valid rmse']\n        \n    return rmse","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-31T08:22:19.874321Z","iopub.execute_input":"2021-08-31T08:22:19.874693Z","iopub.status.idle":"2021-08-31T08:22:19.885177Z","shell.execute_reply.started":"2021-08-31T08:22:19.874656Z","shell.execute_reply":"2021-08-31T08:22:19.88428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgb_params = {'tweedie_variance_power': 2.0,\n 'max_depth': 9,\n 'n_estimators': 4000,\n 'eta': 0.01200085275863839,\n 'subsample': 0.8,\n 'colsample_bytree': 0.7,\n 'colsample_bylevel': 0.4,\n 'min_child_weight': 2.824928835841522,\n 'reg_lambda': 67.43522142240646,\n 'reg_alpha': 0.00012103217663028774,\n 'gamma': 0.012432559904494572,'tree_method':'gpu_hist'}\nxgb_params","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-31T08:22:19.886519Z","iopub.execute_input":"2021-08-31T08:22:19.886946Z","iopub.status.idle":"2021-08-31T08:22:19.896957Z","shell.execute_reply.started":"2021-08-31T08:22:19.886906Z","shell.execute_reply":"2021-08-31T08:22:19.895903Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgb_model = cross_val(X, y, XGBRegressor, xgb_params)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-31T08:22:19.898266Z","iopub.execute_input":"2021-08-31T08:22:19.898823Z","iopub.status.idle":"2021-08-31T08:35:53.015059Z","shell.execute_reply.started":"2021-08-31T08:22:19.898787Z","shell.execute_reply":"2021-08-31T08:35:53.014332Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# CatBoost","metadata":{}},{"cell_type":"code","source":"def fit_cat(trial, x_train, y_train, x_test, y_test):\n    params = {'iterations':trial.suggest_int(\"iterations\", 1000, 20000),\n              'od_wait':trial.suggest_int('od_wait', 500, 2000),\n              'task_type':\"GPU\",\n              'eval_metric':'RMSE',\n              'learning_rate' : trial.suggest_uniform('learning_rate', 0.03 , 0.04),\n              'reg_lambda': trial.suggest_loguniform('reg_lambda', 0.32 , 0.33),\n              'subsample': trial.suggest_uniform('subsample',0.9,1.0),\n              'random_strength': trial.suggest_uniform('random_strength',10,50),\n              'depth': trial.suggest_int('depth',1,15),\n              'min_data_in_leaf': trial.suggest_int('min_data_in_leaf',1,30),\n              'leaf_estimation_iterations': trial.suggest_int('leaf_estimation_iterations',1,15),\n               }\n    \n    \n    model = CatBoostRegressor(**params,task_type='GPU', random_state=2021)\n    model.fit(x_train, y_train,eval_set=[(x_test,y_test)], early_stopping_rounds=150, verbose=False)\n    \n    y_train_pred = model.predict(x_train)\n    \n    y_test_pred = model.predict(x_test)\n    y_train_pred = np.clip(y_train_pred, 0.1, None)\n    y_test_pred = np.clip(y_test_pred, 0.1, None)\n    \n    log = {\n        \"train rmse\": mean_squared_error(y_train, y_train_pred,squared=False),\n        \"valid rmse\": mean_squared_error(y_test, y_test_pred,squared=False)\n    }\n    \n    return model, log","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-31T08:35:53.018129Z","iopub.execute_input":"2021-08-31T08:35:53.019729Z","iopub.status.idle":"2021-08-31T08:35:53.029508Z","shell.execute_reply.started":"2021-08-31T08:35:53.019694Z","shell.execute_reply":"2021-08-31T08:35:53.028655Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def objective(trial):\n    rmse = 0\n    x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.15)\n    model, log = fit_cat(trial, x_train, y_train, x_test, y_test)\n    rmse += log['valid rmse']\n        \n    return rmse","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-31T08:35:53.032469Z","iopub.execute_input":"2021-08-31T08:35:53.03275Z","iopub.status.idle":"2021-08-31T08:35:53.041636Z","shell.execute_reply.started":"2021-08-31T08:35:53.032714Z","shell.execute_reply":"2021-08-31T08:35:53.040787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cat_params = {'iterations': 1224,\n 'od_wait': 1243,\n 'learning_rate': 0.03632022350716054,\n 'reg_lambda': 0.3257139588327784,\n 'subsample': 0.9741256425198503,\n 'random_strength': 41.06792107841663,\n 'depth': 12,\n 'min_data_in_leaf': 27,\n 'leaf_estimation_iterations': 10,'task_type':'GPU'}\ncat_params","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-31T08:35:53.043017Z","iopub.execute_input":"2021-08-31T08:35:53.043397Z","iopub.status.idle":"2021-08-31T08:35:53.05699Z","shell.execute_reply.started":"2021-08-31T08:35:53.043363Z","shell.execute_reply":"2021-08-31T08:35:53.05574Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cat_model = cross_val(X, y, CatBoostRegressor, cat_params)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-31T08:35:53.058411Z","iopub.execute_input":"2021-08-31T08:35:53.059181Z","iopub.status.idle":"2021-08-31T08:51:51.200188Z","shell.execute_reply.started":"2021-08-31T08:35:53.059138Z","shell.execute_reply":"2021-08-31T08:51:51.199277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cat = CatBoostRegressor(**cat_params)\nlgb = LGBMRegressor(**lgb_params)\nxgb = XGBRegressor(**xgb_params)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-31T08:51:51.201453Z","iopub.execute_input":"2021-08-31T08:51:51.201945Z","iopub.status.idle":"2021-08-31T08:51:51.206414Z","shell.execute_reply.started":"2021-08-31T08:51:51.201906Z","shell.execute_reply":"2021-08-31T08:51:51.205424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import VotingRegressor\nfolds = KFold(n_splits = 10, random_state = 2021, shuffle = True)\n\npredictions = np.zeros(len(test))\n\nfor fold, (trn_idx, val_idx) in enumerate(folds.split(X)):\n    print(f\"Fold: {fold}\")\n    X_train, X_val = X.values[trn_idx], X.values[val_idx]\n    y_train, y_val = y.values[trn_idx], y.values[val_idx]\n\n    model = VotingRegressor(\n            estimators = [\n                ('lgbm', lgb),\n                ('xgb', xgb)\n            ],\n            weights = [0.15, 0.65]\n        )\n   \n    model.fit(X_train, y_train)\n    pred = model.predict(X_val)\n    error = mean_squared_error(y_val, pred,squared = False)\n    print(f\" mean_squared_error: {error}\")\n    print(\"-\"*50)\n    \n    predictions += model.predict(test) / folds.n_splits","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-31T08:51:51.20786Z","iopub.execute_input":"2021-08-31T08:51:51.208405Z","iopub.status.idle":"2021-08-31T09:36:47.296429Z","shell.execute_reply.started":"2021-08-31T08:51:51.208362Z","shell.execute_reply":"2021-08-31T09:36:47.295661Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"sub['loss'] = lgb_model.predict(test)\nsub.to_csv(f'lgb.csv',index = False)\n\nsub['loss'] = xgb_model.predict(test)\nsub.to_csv(f'xgb.csv',index = False)\n\nsub['loss'] = cat_model.predict(test)\nsub.to_csv(f'cat.csv',index = False)\n\nsub['loss'] = predictions\nsub.to_csv(f'vote.csv',index = False)","metadata":{"execution":{"iopub.status.busy":"2021-08-31T09:36:47.297763Z","iopub.execute_input":"2021-08-31T09:36:47.298348Z","iopub.status.idle":"2021-08-31T09:37:15.846064Z","shell.execute_reply.started":"2021-08-31T09:36:47.298296Z","shell.execute_reply":"2021-08-31T09:37:15.845172Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Credits to the codes that have helped me make this notebook: \n- [Notebook by Subin An ](https://www.kaggle.com/subinium/tps-aug-simple-eda)\n- [Notebook by BIZEN](https://www.kaggle.com/hiro5299834/tps-aug-2021-lgbm-xgb-catboost)","metadata":{}},{"cell_type":"code","source":"%%html\n<marquee style='width: 90% ;height:70%; color: #799EFF ;'>\n    <b> Do UPVOTE if you like my work, I will be adding some more content to this kernel :) </b></marquee>","metadata":{"execution":{"iopub.status.busy":"2021-08-31T09:37:15.847184Z","iopub.execute_input":"2021-08-31T09:37:15.847512Z","iopub.status.idle":"2021-08-31T09:37:15.856711Z","shell.execute_reply.started":"2021-08-31T09:37:15.847479Z","shell.execute_reply":"2021-08-31T09:37:15.855794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}