{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Libraries","metadata":{}},{"cell_type":"code","source":"import math\nimport json\nimport os\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm import tqdm\nimport xgboost as xgb\nimport lightgbm as lgbm\nimport optuna.integration.lightgbm as lgbo\nimport scipy.stats as stats\nfrom sklearn.model_selection import StratifiedKFold, KFold, train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.preprocessing import FunctionTransformer, PowerTransformer, QuantileTransformer, OneHotEncoder\nfrom sklearn.ensemble import BaggingRegressor, ExtraTreesRegressor, GradientBoostingRegressor, RandomForestRegressor, StackingRegressor, AdaBoostRegressor\nfrom sklearn.svm import SVR\nimport statsmodels.api as sm\nimport catboost\nimport optuna\n\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nfrom tensorflow import keras\nfrom tensorflow.keras import callbacks\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import activations,callbacks\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras import initializers\nfrom tensorflow.keras import regularizers\nfrom tensorflow.keras import metrics\nfrom keras.models import Model\n\nimport multiprocessing\n\nsns.set_theme()\nsns.set_palette(palette = \"rainbow\")\n\ngpu_available = tf.test.is_gpu_available()\n%matplotlib inline","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-30T09:32:39.648557Z","iopub.execute_input":"2021-08-30T09:32:39.648928Z","iopub.status.idle":"2021-08-30T09:32:52.777831Z","shell.execute_reply.started":"2021-08-30T09:32:39.648896Z","shell.execute_reply":"2021-08-30T09:32:52.776718Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('../input/tabular-playground-series-aug-2021/train.csv')\ntrain = train.set_index('id')\ntarget = train['loss']\ntrain = train.drop('loss', axis=1)\ntest = pd.read_csv('../input/tabular-playground-series-aug-2021/test.csv')\ntest = test.set_index('id')\npreds = pd.read_csv('../input/tabular-playground-series-aug-2021/sample_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2021-08-30T09:32:52.782218Z","iopub.execute_input":"2021-08-30T09:32:52.782571Z","iopub.status.idle":"2021-08-30T09:33:02.775613Z","shell.execute_reply.started":"2021-08-30T09:32:52.782541Z","shell.execute_reply":"2021-08-30T09:33:02.774465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Scale Data","metadata":{}},{"cell_type":"code","source":"oh = OneHotEncoder()\n\ntrain_oh = oh.fit_transform(train[[\"f1\", \"f86\"]]).toarray()\ntest_oh = oh.transform(test[[\"f1\", \"f86\"]]).toarray()\n\nf1_map = { v: i for i, v in enumerate(train[\"f1\"].unique()) }\nf86_map = { v: i for i, v in enumerate(train[\"f86\"].unique()) }\n\ntrain[\"f1\"] = train[\"f1\"].map(lambda v: f1_map[v])\ntrain[\"f86\"] = train[\"f86\"].map(lambda v: f86_map[v])\n\ntest[\"f1\"] = test[\"f1\"].map(lambda v: f1_map[v])\ntest[\"f86\"] = test[\"f86\"].map(lambda v: f86_map[v])\n\ntrain_cat = train[[\"f1\", \"f86\"]].values\ntest_cat = test[[\"f1\", \"f86\"]].values","metadata":{"execution":{"iopub.status.busy":"2021-08-30T09:33:02.778127Z","iopub.execute_input":"2021-08-30T09:33:02.778506Z","iopub.status.idle":"2021-08-30T09:33:05.849714Z","shell.execute_reply.started":"2021-08-30T09:33:02.778476Z","shell.execute_reply":"2021-08-30T09:33:05.848638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"original_skew = train.skew()\n\nbest = { col: (\"og\", original_skew[col]) for col in train.columns }\ntransformers = {\n    \"log1p\": lambda x: x if 0.0 in x else FunctionTransformer(np.log1p).fit_transform(x),\n    \"square\": lambda x: FunctionTransformer(np.square).fit_transform(x),\n    \"cube\": lambda x: FunctionTransformer(lambda x: x**3).fit_transform(x),\n    \"box-cox\": lambda x: PowerTransformer(method = \"box-cox\", standardize=True).fit_transform(x.reshape(-1,1)).reshape(-1) if min(x) > 0 else x,\n    \"yeojohn\": lambda x: PowerTransformer(standardize=True).fit_transform(x.reshape(-1,1)).reshape(-1)\n}\n\nfor name, transformer in transformers.items():\n    for col in train.columns:\n        v_trans = transformer(train[col].values)\n        if np.isfinite(v_trans).all():\n            df_trans = pd.DataFrame(v_trans)\n            trans_skew = df_trans.skew()[0]\n            if abs(trans_skew) < abs(best[col][1]):\n                best[col] = (name, trans_skew)\n\nfor col, (method, _) in best.items():\n    if method != \"og\":\n        train[col] = transformers[method](train[col].values).reshape(-1)\n        test[col] = transformers[method](test[col].values).reshape(-1)\n\nprint(sum(original_skew), sum(train.skew()))","metadata":{"execution":{"iopub.status.busy":"2021-08-30T09:33:05.85178Z","iopub.execute_input":"2021-08-30T09:33:05.852219Z","iopub.status.idle":"2021-08-30T09:35:22.613917Z","shell.execute_reply.started":"2021-08-30T09:33:05.852155Z","shell.execute_reply":"2021-08-30T09:35:22.612324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sc = QuantileTransformer(output_distribution=\"normal\")\n\ntrain_sc = sc.fit_transform(train)\ntest_sc = sc.transform(test)","metadata":{"execution":{"iopub.status.busy":"2021-08-30T09:35:22.615719Z","iopub.execute_input":"2021-08-30T09:35:22.616156Z","iopub.status.idle":"2021-08-30T09:35:35.628608Z","shell.execute_reply.started":"2021-08-30T09:35:22.616112Z","shell.execute_reply":"2021-08-30T09:35:35.627471Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_sc = pd.DataFrame(train_sc, columns=[ \"f{}\".format(i) for i in range(100) ])\ntest_sc = pd.DataFrame(test_sc, columns=[ \"f{}\".format(i) for i in range(100) ])\n\ntrain_c = pd.DataFrame(train_sc, columns=[ \"f{}\".format(i) for i in range(100) ])\ntest_c  = pd.DataFrame(test_sc, columns=[ \"f{}\".format(i) for i in range(100) ])\n\ntrain_oh = pd.DataFrame(train_oh, columns=[ \"oh_{}\".format(i) for i in range(train_oh.shape[1]) ])\ntest_oh  = pd.DataFrame(test_oh, columns=[ \"oh_{}\".format(i) for i in range(test_oh.shape[1]) ])\n\ntrain_cat = pd.DataFrame(train_cat, columns=[\"f1_c\", \"f86_c\"])\ntest_cat  = pd.DataFrame(test_cat, columns=[\"f1_c\", \"f86_c\"])","metadata":{"execution":{"iopub.status.busy":"2021-08-30T09:35:35.63032Z","iopub.execute_input":"2021-08-30T09:35:35.630793Z","iopub.status.idle":"2021-08-30T09:35:35.647081Z","shell.execute_reply.started":"2021-08-30T09:35:35.630739Z","shell.execute_reply":"2021-08-30T09:35:35.645837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.concat([train_sc, train_oh], axis=1)\ntest  = pd.concat([test_sc, test_oh],   axis=1)\n\ntrain_c = pd.concat([train_c, train_cat], axis=1)\ntest_c  = pd.concat([test_sc, test_cat],  axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-08-30T09:35:35.648803Z","iopub.execute_input":"2021-08-30T09:35:35.649582Z","iopub.status.idle":"2021-08-30T09:35:39.637246Z","shell.execute_reply.started":"2021-08-30T09:35:35.649529Z","shell.execute_reply":"2021-08-30T09:35:39.636095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del train_sc\ndel test_sc\n\ndel train_oh\ndel test_oh\n\ndel train_cat\ndel test_cat","metadata":{"execution":{"iopub.status.busy":"2021-08-30T09:35:39.640618Z","iopub.execute_input":"2021-08-30T09:35:39.641061Z","iopub.status.idle":"2021-08-30T09:35:39.723884Z","shell.execute_reply.started":"2021-08-30T09:35:39.641014Z","shell.execute_reply":"2021-08-30T09:35:39.722404Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Models","metadata":{}},{"cell_type":"code","source":"FOLDS = 20\n\nprint(\"Using {} Folds, Calibraiton Train Size of {}, Calibraiton Test Size of {}\".format(FOLDS, 1 - 1 / FOLDS, 1 / FOLDS))","metadata":{"execution":{"iopub.status.busy":"2021-08-30T09:35:39.726292Z","iopub.execute_input":"2021-08-30T09:35:39.727002Z","iopub.status.idle":"2021-08-30T09:35:39.737924Z","shell.execute_reply.started":"2021-08-30T09:35:39.72695Z","shell.execute_reply":"2021-08-30T09:35:39.736316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Split Train/Test for Hyperparameter Search","metadata":{}},{"cell_type":"code","source":"cal_X_train, cal_X_val, cal_y_train, cal_y_val = train_test_split(train, target, random_state=0, stratify=target, test_size=1 / FOLDS)\ncal_X_train_c, cal_X_val_c, cal_y_train_c, cal_y_val_c = train_test_split(train_c, target, random_state=0, stratify=target, test_size=1 / FOLDS)\nlen(cal_X_train), len(cal_X_val)","metadata":{"execution":{"iopub.status.busy":"2021-08-30T09:35:39.739813Z","iopub.execute_input":"2021-08-30T09:35:39.740124Z","iopub.status.idle":"2021-08-30T09:35:42.710531Z","shell.execute_reply.started":"2021-08-30T09:35:39.740093Z","shell.execute_reply":"2021-08-30T09:35:42.709375Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def score_model(mod, X, y):\n    y_pred = mod.predict(X)\n    return np.sqrt(mean_squared_error(y, y_pred))","metadata":{"execution":{"iopub.status.busy":"2021-08-30T09:35:42.712166Z","iopub.execute_input":"2021-08-30T09:35:42.712638Z","iopub.status.idle":"2021-08-30T09:35:42.718974Z","shell.execute_reply.started":"2021-08-30T09:35:42.712593Z","shell.execute_reply":"2021-08-30T09:35:42.717358Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cal_preds = pd.DataFrame()\nrmse_dict = {}\nfold_preds = { i: pd.DataFrame() for i in range(FOLDS) }\nfold_test_preds = { i: pd.DataFrame() for i in range(FOLDS) }","metadata":{"execution":{"iopub.status.busy":"2021-08-30T09:35:42.721156Z","iopub.execute_input":"2021-08-30T09:35:42.722007Z","iopub.status.idle":"2021-08-30T09:35:42.744955Z","shell.execute_reply.started":"2021-08-30T09:35:42.721959Z","shell.execute_reply":"2021-08-30T09:35:42.743733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Xgb","metadata":{}},{"cell_type":"markdown","source":"### Hyperparameter Optimisation","metadata":{}},{"cell_type":"code","source":"if False:\n    def objective(trial):\n        param = {\n            \"n_estimators\": trial.suggest_int(\"n_estimators\", 200, 2000, 100),\n            \"subsample\": trial.suggest_discrete_uniform(\"subsample\", 0.6, 1, 0.1),\n            \"colsample_bytree\": trial.suggest_discrete_uniform(\"colsample_bytree\", 0.6, 1, 0.1),\n            \"eta\": trial.suggest_loguniform(\"eta\", 1e-3, 0.1),\n            \"reg_alpha\": trial.suggest_int(\"reg_alpha\", 1, 50),\n            \"reg_lambda\": trial.suggest_int(\"reg_lambda\", 5, 100),\n            \"max_depth\": trial.suggest_int(\"max_depth\", 5, 20),\n            \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 5, 20),\n            \"random_state\": 42,\n            \"learning_rate\": 0.05,\n            \"tree_method\": \"gpu_hist\" if gpu_available else \"hist\",\n        }\n\n        model = xgb.XGBRegressor(**param)\n\n        model.fit(cal_X_train, cal_y_train, eval_set=[(cal_X_val, cal_y_val)], verbose=False, eval_metric=\"rmse\", early_stopping_rounds=5)\n\n        return score_model(model, cal_X_val, cal_y_val)\n\n    study = optuna.create_study(direction=\"minimize\")\n    study.optimize(objective, n_trials=400, timeout=None)\n    print(\"Number of finished trials: {}\".format(len(study.trials)))\n\n    print(\"Best trial:\")\n    trial = study.best_trial\n\n    print(\"  Value: {}\".format(trial.value))\n\n    params = trial.params\n\n    with open(\"xgb.json\", \"w\") as file:\n        file.write(json.dumps(params, indent=4))\nelse:\n    params = {\n        \"n_estimators\": 900,\n        \"subsample\": 0.6,\n        \"colsample_bytree\": 0.6,\n        \"eta\": 0.0036403005445866375,\n        \"reg_alpha\": 26,\n        \"reg_lambda\": 100,\n        \"max_depth\": 5,\n        \"min_child_weight\": 13\n     }\n\nparams[\"tree_method\"] = \"gpu_hist\" if gpu_available else \"hist\"\nparams[\"random_state\"] = 42\nparams[\"learning_rate\"] = 0.05\nparams","metadata":{"execution":{"iopub.status.busy":"2021-08-30T08:56:44.415218Z","iopub.execute_input":"2021-08-30T08:56:44.415589Z","iopub.status.idle":"2021-08-30T08:56:44.431251Z","shell.execute_reply.started":"2021-08-30T08:56:44.415558Z","shell.execute_reply":"2021-08-30T08:56:44.430299Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Stack Calibration Model","metadata":{}},{"cell_type":"code","source":"model = xgb.XGBRegressor(**params)\nmodel.fit(cal_X_train, cal_y_train, eval_set=[(cal_X_val, cal_y_val)], verbose=False, eval_metric=\"rmse\", early_stopping_rounds=100)\n\ncal_preds = pd.concat([cal_preds, pd.DataFrame(model.predict(cal_X_val)[:, np.newaxis], columns=[\"xgb\"])], axis=1)\n\nscore_model(model, cal_X_val, cal_y_val)","metadata":{"execution":{"iopub.status.busy":"2021-08-30T08:48:24.376755Z","iopub.execute_input":"2021-08-30T08:48:24.377079Z","iopub.status.idle":"2021-08-30T08:49:14.188848Z","shell.execute_reply.started":"2021-08-30T08:48:24.377049Z","shell.execute_reply":"2021-08-30T08:49:14.188178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### KFold Prediction","metadata":{}},{"cell_type":"code","source":"for fold, (trn_idx, val_idx) in enumerate(KFold(n_splits=20, random_state=42, shuffle=True).split(train, target)):\n    print(\"Fold :\", fold + 1)\n\n    fold_X_train, fold_y_train = train.iloc[trn_idx], target.iloc[trn_idx]\n    fold_X_test,  fold_y_test  = train.iloc[val_idx], target.iloc[val_idx]\n\n    model = xgb.XGBRegressor(**params)\n    model.fit(fold_X_train, fold_y_train, eval_set=[(fold_X_test, fold_y_test)], verbose=False, eval_metric=\"rmse\")\n\n    fold_preds[fold] = pd.concat([fold_preds[fold], pd.DataFrame(model.predict(fold_X_test)[:, np.newaxis], columns=[\"xgb\"])], axis=1)\n    fold_test_preds[fold] = pd.concat([fold_test_preds[fold], pd.DataFrame(model.predict(test)[:, np.newaxis], columns=[\"xgb\"])], axis=1)\n\n    score = score_model(model, fold_X_test, fold_y_test)\n    rmse_dict[\"xgb_\" + str(fold + 1)] = score\n\n    ax = xgb.plot_importance(model)\n    plt.show()\n\n    print('#### fold #########', score)","metadata":{"execution":{"iopub.status.busy":"2021-08-25T14:57:25.628892Z","iopub.execute_input":"2021-08-25T14:57:25.629393Z","iopub.status.idle":"2021-08-25T14:57:45.388357Z","shell.execute_reply.started":"2021-08-25T14:57:25.629362Z","shell.execute_reply":"2021-08-25T14:57:45.385225Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## LightGBM","metadata":{}},{"cell_type":"markdown","source":"### Hyperparameter Optimisation","metadata":{}},{"cell_type":"code","source":"if False:\n    params = { \"objective\": \"mean_squared_error\", \"metric\": \"rmse\", \"device\": \"gpu\" if gpu_available else \"cpu\" }\n    \n    lgb_cal_X_train_c = cal_X_train_c.drop(\"f1_c\", axis=1).drop(\"f86_c\", axis=1)\n    \n    lgb_cal_X_val_c = cal_X_val_c.drop(\"f1_c\", axis=1).drop(\"f86_c\", axis=1)\n\n    lgb_train = lgbm.Dataset(lgb_cal_X_train_c, cal_y_train_c)\n    lgb_valid = lgbm.Dataset(lgb_cal_X_val_c, cal_y_val_c)\n\n    model = lgbo.train(params, lgb_train, valid_sets=[lgb_valid], verbose_eval=False, num_boost_round=100, early_stopping_rounds=5)\n\n    params = model.params\n\n    with open(\"lgbm.json\", \"w\") as file:\n        file.write(json.dumps(params, indent=4))\nelse:\n    params = {\n        \"objective\": \"mean_squared_error\",\n        \"metric\": \"rmse\", \n        \"device\": \"gpu\" if gpu_available else \"cpu\",\n        \"feature_pre_filter\": False, \n        \"lambda_l1\": 1.4627327010463796e-08, \n        \"lambda_l2\": 2.749104514966133e-08, \n        \"num_leaves\": 24, \n        \"feature_fraction\": 0.62, \n        \"bagging_fraction\": 1.0, \n        \"bagging_freq\": 0, \n        \"min_child_samples\": 100\n    }\n\nparams[\"learning_rate\"] = 0.006\nparams[\"num_iterations\"] = 80000\nparams","metadata":{"execution":{"iopub.status.busy":"2021-08-30T09:35:42.746849Z","iopub.execute_input":"2021-08-30T09:35:42.747427Z","iopub.status.idle":"2021-08-30T09:35:42.769255Z","shell.execute_reply.started":"2021-08-30T09:35:42.747383Z","shell.execute_reply":"2021-08-30T09:35:42.767699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Stack Calibration Model","metadata":{}},{"cell_type":"code","source":"lgb_cal_X_train_c = cal_X_train_c.drop(\"f1_c\", axis=1).drop(\"f86_c\", axis=1)\nlgb_cal_X_val_c = cal_X_val_c.drop(\"f1_c\", axis=1).drop(\"f86_c\", axis=1)\n\nlgb_train = lgbm.Dataset(lgb_cal_X_train_c, cal_y_train_c)\nlgb_valid = lgbm.Dataset(lgb_cal_X_val_c, cal_y_val_c)\n\nmodel = lgbm.train(params, lgb_train, valid_sets=[lgb_valid], verbose_eval=False, early_stopping_rounds=100)\n\ncal_preds = pd.concat([cal_preds, pd.DataFrame(model.predict(lgb_cal_X_val_c)[:, np.newaxis], columns=[\"lgbm\"])], axis=1)\n\nscore_model(model, lgb_cal_X_val_c, cal_y_val_c)","metadata":{"execution":{"iopub.status.busy":"2021-08-30T09:35:42.770935Z","iopub.execute_input":"2021-08-30T09:35:42.771514Z","iopub.status.idle":"2021-08-30T09:37:05.874568Z","shell.execute_reply.started":"2021-08-30T09:35:42.77147Z","shell.execute_reply":"2021-08-30T09:37:05.873511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### KFold Prediction","metadata":{}},{"cell_type":"code","source":"for fold, (trn_idx, val_idx) in enumerate(KFold(n_splits=20, random_state=42, shuffle=True).split(train, target)):\n    print(\"Fold :\", fold + 1)\n\n    # create dataset\n    fold_X_train, fold_y_train = train_c.iloc[trn_idx], target.iloc[trn_idx]\n    fold_X_test,  fold_y_test  = train_c.iloc[val_idx], target.iloc[val_idx]\n    \n    lgb_fold_X_train = fold_X_train.drop(\"f1_c\", axis=1).drop(\"f86_c\", axis=1)\n    \n    lgb_fold_X_test = fold_X_test.drop(\"f1_c\", axis=1).drop(\"f86_c\", axis=1)\n    \n    lgb_test_c = test_c.drop(\"f1_c\", axis=1).drop(\"f86_c\", axis=1)\n\n    lgb_train = lgbm.Dataset(lgb_fold_X_train, fold_y_train)\n    lgb_valid = lgbm.Dataset(lgb_fold_X_test, fold_y_test)\n\n    model = lgbm.train(params, lgb_train, valid_sets=[lgb_valid], verbose_eval=False, early_stopping_rounds=100)\n\n    fold_preds[fold] = pd.concat([fold_preds[fold], pd.DataFrame(model.predict(lgb_fold_X_test)[:, np.newaxis], columns=[\"lgbm\"])], axis=1)\n    fold_test_preds[fold] = pd.concat([fold_test_preds[fold], pd.DataFrame(model.predict(lgb_test_c)[:, np.newaxis], columns=[\"lgbm\"])], axis=1)\n\n    score = score_model(model, lgb_fold_X_test, fold_y_test)\n    rmse_dict[\"lgbm_\" + str(fold + 1)] = score\n\n    ax = lgbm.plot_importance(model, figsize=(15,15))\n    plt.show()\n\n    print('#### fold #########', score)","metadata":{"execution":{"iopub.status.busy":"2021-08-25T17:16:55.443987Z","iopub.execute_input":"2021-08-25T17:16:55.444435Z","iopub.status.idle":"2021-08-25T17:32:44.157392Z","shell.execute_reply.started":"2021-08-25T17:16:55.444389Z","shell.execute_reply":"2021-08-25T17:32:44.154241Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Catboost","metadata":{}},{"cell_type":"code","source":"if False:\n    def objective(trial):\n        param = {\n            \"iterations\": trial.suggest_int(\"iterations\", 800, 4000, 100),\n            \"learning_rate\": trial.suggest_discrete_uniform(\"learning_rate\", 0.02, 0.08, 0.01),\n            \"depth\": trial.suggest_int(\"depth\", 2, 16, 2),\n            \"l2_leaf_reg\": trial.suggest_discrete_uniform(\"l2_leaf_reg\", 0.2, 4, 0.1),\n            \"random_strength\": trial.suggest_discrete_uniform(\"random_strength\", 0.5, 2, 0.5),\n        }\n\n        model = catboost.CatBoostRegressor(\n            **param, \n            thread_count=4,\n            eval_metric=\"RMSE\", \n            loss_function=\"RMSE\",\n            grow_policy='Depthwise',\n            leaf_estimation_method='Newton', \n            bootstrap_type='Bernoulli',\n            task_type=\"GPU\" if gpu_available else \"CPU\",\n            early_stopping_rounds=5,\n            random_state=42\n        )\n        \n        cat_train = catboost.Pool(cal_X_train_c, cal_y_train_c, [\"f1_c\", \"f86_c\"])\n        cat_valid = catboost.Pool(cal_X_val_c, cal_y_val_c, [\"f1_c\", \"f86_c\"])\n        \n        model.fit(cat_train, eval_set=[cat_valid], verbose=False)\n\n        return np.sqrt(mean_squared_error(cal_y_val_c, model.predict(cat_valid)))\n\n    study = optuna.create_study(direction=\"minimize\")\n    study.optimize(objective, n_trials=400, timeout=None)\n    print(\"Number of finished trials: {}\".format(len(study.trials)))\n\n    print(\"Best trial:\")\n    trial = study.best_trial\n\n    print(\"  Value: {}\".format(trial.value))\n\n    params = trial.params\n\n    with open(\"catboost.json\", \"w\") as file:\n        file.write(json.dumps(params, indent=4))\nelse:\n    params = {\n        \"iterations\": 2700,\n        \"learning_rate\": 0.08,\n        \"depth\": 4,\n        \"l2_leaf_reg\": 1.0,\n        \"random_strength\": 2.0\n    }\n\nparams","metadata":{"execution":{"iopub.status.busy":"2021-08-30T09:41:28.225369Z","iopub.execute_input":"2021-08-30T09:41:28.225789Z","iopub.status.idle":"2021-08-30T09:41:28.248652Z","shell.execute_reply.started":"2021-08-30T09:41:28.225757Z","shell.execute_reply":"2021-08-30T09:41:28.247113Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = catboost.CatBoostRegressor(\n    **params, \n    thread_count=4,\n    eval_metric=\"RMSE\", \n    loss_function=\"RMSE\",\n    grow_policy='Depthwise',\n    leaf_estimation_method='Newton', \n    bootstrap_type='Bernoulli',\n    task_type=\"GPU\" if gpu_available else \"CPU\",\n    early_stopping_rounds=100,\n    random_state=42\n)\n\ncat_train = catboost.Pool(cal_X_train_c, cal_y_train_c, [\"f1_c\", \"f86_c\"])\ncat_valid = catboost.Pool(cal_X_val_c, cal_y_val_c, [\"f1_c\", \"f86_c\"])\n\nmodel.fit(cat_train, eval_set=[cat_valid], verbose=False)\n\ncal_preds = pd.concat([cal_preds, pd.DataFrame(model.predict(cat_valid)[:, np.newaxis], columns=[\"catboost\"])], axis=1)\n\nscore_model(model, cat_valid, cal_y_val)","metadata":{"execution":{"iopub.status.busy":"2021-08-30T09:41:30.29903Z","iopub.execute_input":"2021-08-30T09:41:30.29943Z","iopub.status.idle":"2021-08-30T09:43:04.521416Z","shell.execute_reply.started":"2021-08-30T09:41:30.299398Z","shell.execute_reply":"2021-08-30T09:43:04.520267Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for fold, (trn_idx, val_idx) in enumerate(KFold(n_splits=20, random_state=42, shuffle=True).split(train, target)):\n    print(\"Fold :\", fold + 1)\n\n    # create dataset\n    fold_X_train, fold_y_train = train_c.iloc[trn_idx], target.iloc[trn_idx]\n    fold_X_test,  fold_y_test  = train_c.iloc[val_idx], target.iloc[val_idx]\n\n    model = catboost.CatBoostRegressor(\n        **params, \n        thread_count=4,\n        eval_metric=\"RMSE\", \n        loss_function=\"RMSE\",\n        grow_policy='Depthwise',\n        leaf_estimation_method='Newton', \n        bootstrap_type='Bernoulli',\n        task_type=\"GPU\" if gpu_available else \"CPU\",\n        early_stopping_rounds=100,\n        random_state=42\n    )\n    \n    cat_train = catboost.Pool(fold_X_train, fold_y_train, cat_features=[\"f1_c\", \"f86_c\"])\n    cat_valid = catboost.Pool(fold_X_test, fold_y_test, cat_features=[\"f1_c\", \"f86_c\"])\n    cat_test  = catboost.Pool(test_c, cat_features=[\"f1_c\", \"f86_c\"])\n\n    model.fit(cat_train, eval_set=[cat_valid], verbose=False)\n\n    fold_preds[fold] = pd.concat([fold_preds[fold], pd.DataFrame(model.predict(cat_valid)[:, np.newaxis], columns=[\"catboost\"])], axis=1)\n    fold_test_preds[fold] = pd.concat([fold_test_preds[fold], pd.DataFrame(model.predict(cat_test)[:, np.newaxis], columns=[\"catboost\"])], axis=1)\n\n    score = score_model(model, cat_valid, fold_y_test)\n    rmse_dict[\"cat_\" + str(fold + 1)] = score\n\n    print('#### fold #########', score)","metadata":{"execution":{"iopub.status.busy":"2021-08-25T16:54:30.096486Z","iopub.execute_input":"2021-08-25T16:54:30.098674Z","iopub.status.idle":"2021-08-25T17:00:10.420683Z","shell.execute_reply.started":"2021-08-25T16:54:30.098638Z","shell.execute_reply":"2021-08-25T17:00:10.419262Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## ATTN-DNN","metadata":{}},{"cell_type":"markdown","source":"### Model Definition","metadata":{}},{"cell_type":"code","source":"def keras_model(block_activation: str, final_activation: str, kernel_initializer: str, embedding_width_1: int, embedding_width_2: int, lr: int, blocks: int, dropout: float, **kwargs):\n    width = train_c.shape[1] - 2 + embedding_width_1 + embedding_width_2\n    \n    def attn(inp):\n        k = layers.Dense(width, kernel_initializer = kernel_initializer)(inp)\n        q = layers.Dense(width, kernel_initializer = kernel_initializer)(inp)\n        v = layers.Dense(width, kernel_initializer = kernel_initializer)(inp)\n\n        k_r = layers.Reshape((width, 1))(k)\n        q_r = layers.Reshape((width, 1))(q)\n        v_r = layers.Reshape((width, 1))(v)\n\n        a = layers.Attention(use_scale=True, dropout=dropout)([ q, v, k ])\n        a = layers.Reshape((width,))(a)\n        a = layers.LayerNormalization()(a)\n\n        return a\n\n    def block(inp):\n        x = layers.Dense(width, kernel_initializer = kernel_initializer)(inp)\n        x = layers.Activation(block_activation)(x)\n        x = layers.Add()([x, inp])\n        x = layers.LayerNormalization()(x)\n\n        return x\n\n    inp_s = layers.Input(shape=(train_c.shape[1] - 2,))\n    inp_c_1 = layers.Input(shape=(1,))\n    inp_c_2 = layers.Input(shape=(1,))\n    \n    e1 = layers.Flatten()(layers.Embedding(cal_X_train_c[\"f1_c\"].nunique(), embedding_width_1)(inp_c_1))\n    e2 = layers.Flatten()(layers.Embedding(cal_X_train_c[\"f86_c\"].nunique(), embedding_width_2)(inp_c_2))\n    \n    x = layers.Concatenate()([inp_s, e1, e2])\n    \n    for _ in range(blocks):\n        i = x\n        x = attn(x)\n        x = block(x)\n        x = layers.Add()([ x, i ])\n\n    x = layers.Dense(1, activation=final_activation, kernel_initializer =\"lecun_normal\")(x)\n\n    model = keras.Model(inputs=[inp_s, inp_c_1, inp_c_2], outputs=x)\n    model.compile(optimizer=Adam(lr=lr), loss=\"mse\", metrics=[metrics.RootMeanSquaredError()])\n\n    return model","metadata":{"execution":{"iopub.status.busy":"2021-08-25T15:28:36.808388Z","iopub.execute_input":"2021-08-25T15:28:36.808875Z","iopub.status.idle":"2021-08-25T15:28:36.828749Z","shell.execute_reply.started":"2021-08-25T15:28:36.808835Z","shell.execute_reply":"2021-08-25T15:28:36.826102Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Hyperparameter Optimisation","metadata":{}},{"cell_type":"code","source":"if True:\n    def objective(trial):\n        params = {\n            \"blocks\": trial.suggest_int(\"blocks\", 4, 16),\n            \"dropout\": trial.suggest_float(\"dropout\", 0, 0.5),\n            \"lr\": trial.suggest_loguniform(\"lr\", 1e-5, 1e-3),\n            \"block_activation\": trial.suggest_categorical(\"block_activation\", [\"elu\", \"relu\"]),\n            \"final_activation\": trial.suggest_categorical(\"final_activation\", [\"elu\", \"relu\"]),\n            \"kernel_initializer\": trial.suggest_categorical(\"kernel_initializer\", [\"lecun_normal\", \"random_normal\", \"random_uniform\", \"glorot_uniform\"]),\n            \"embedding_width_1\": trial.suggest_int(\"embedding_width_1\", 16, 64, 8),\n            \"embedding_width_2\": trial.suggest_int(\"embedding_width_2\", 16, 64, 8)\n        }\n\n        model = keras_model(**params)\n\n        earlyStopping = callbacks.EarlyStopping(min_delta=0.001, patience=10, verbose=0)\n        checkpoint = callbacks.ModelCheckpoint(\"/tmp/checkpoint\", monitor=\"val_root_mean_squared_error\", mode=\"min\", save_best_only=True, save_weights_only=True)\n        history = model.fit(\n            [ cal_X_train_c.drop(\"f1_c\", axis=1).drop(\"f86_c\", axis=1), cal_X_train_c[[\"f1_c\"]], cal_X_train_c[[\"f86_c\"]] ], \n            cal_y_train_c, \n            validation_data=([ cal_X_val_c.drop(\"f1_c\", axis=1).drop(\"f86_c\", axis=1), cal_X_val_c[[\"f1_c\"]], cal_X_val_c[[\"f86_c\"]] ], cal_y_val_c), \n            batch_size=2048, \n            epochs=100, \n            callbacks=[earlyStopping, checkpoint],\n            verbose=0\n        )\n        model.load_weights(\"/tmp/checkpoint\")\n\n        return score_model(model, [ cal_X_val_c.drop(\"f1_c\", axis=1).drop(\"f86_c\", axis=1), cal_X_val_c[[\"f1_c\"]], cal_X_val_c[[\"f86_c\"]] ], cal_y_val_c)\n\n    study = optuna.create_study(direction=\"minimize\")\n    study.optimize(objective, n_trials=100, timeout=None)\n    trial = study.best_trial\n    params = trial.params\n\n    with open(\"attn_dnn.json\", \"w\") as file:\n        file.write(json.dumps(params, indent=4))\nelse:\n    params = {\n        \"blocks\": 5,\n        \"dropout\": 0.46778133370223896,\n        \"lr\": 0.0008267100960741205,\n        \"block_activation\": \"relu\",\n        \"final_activation\": \"elu\",\n        \"kernel_initializer\": \"random_uniform\",\n        \"embedding_width_1\": 64,\n        \"embedding_width_2\": 64\n    }\n\nparams","metadata":{"execution":{"iopub.status.busy":"2021-08-25T15:28:39.36905Z","iopub.execute_input":"2021-08-25T15:28:39.369397Z","iopub.status.idle":"2021-08-25T15:28:39.391389Z","shell.execute_reply.started":"2021-08-25T15:28:39.369367Z","shell.execute_reply":"2021-08-25T15:28:39.390053Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Stack Calibration Model","metadata":{}},{"cell_type":"code","source":"model = keras_model(**params)\n\nearlyStopping = callbacks.EarlyStopping(min_delta=0.001, patience=10, verbose=0)\ncheckpoint = callbacks.ModelCheckpoint(\"/tmp/checkpoint\", monitor=\"val_root_mean_squared_error\", mode=\"min\", save_best_only=True, save_weights_only=True)\nhistory = model.fit(\n    [ cal_X_train_c.drop(\"f1_c\", axis=1).drop(\"f86_c\", axis=1), cal_X_train_c[[\"f1_c\"]], cal_X_train_c[[\"f86_c\"]] ],\n    cal_y_train_c, \n    validation_data=([ cal_X_val_c.drop(\"f1_c\", axis=1).drop(\"f86_c\", axis=1), cal_X_val_c[[\"f1_c\"]], cal_X_val_c[[\"f86_c\"]] ], cal_y_val_c),\n    batch_size=2048, \n    epochs=400, \n    callbacks=[earlyStopping, checkpoint],\n    verbose=0\n)\nmodel.load_weights(\"/tmp/checkpoint\")\n\ncal_preds = pd.concat([cal_preds, pd.DataFrame(model.predict([ cal_X_val_c.drop(\"f1_c\", axis=1).drop(\"f86_c\", axis=1), cal_X_val_c[[\"f1_c\"]], cal_X_val_c[[\"f86_c\"]] ]), columns=[\"attn_dnn\"])], axis=1)\n\nscore_model(model, [ cal_X_val_c.drop(\"f1_c\", axis=1).drop(\"f86_c\", axis=1), cal_X_val_c[[\"f1_c\"]], cal_X_val_c[[\"f86_c\"]] ], cal_y_val_c)","metadata":{"execution":{"iopub.status.busy":"2021-08-25T15:28:55.958352Z","iopub.execute_input":"2021-08-25T15:28:55.958753Z","iopub.status.idle":"2021-08-25T15:29:10.035361Z","shell.execute_reply.started":"2021-08-25T15:28:55.958719Z","shell.execute_reply":"2021-08-25T15:29:10.034291Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### KFold Prediction","metadata":{}},{"cell_type":"code","source":"for fold, (trn_idx, val_idx) in enumerate(KFold(n_splits=20, random_state=42, shuffle=True).split(train_c, target)):\n    print(\"Fold :\", fold + 1)\n\n    # create dataset\n    fold_X_train, fold_y_train = train_c.iloc[trn_idx], target.iloc[trn_idx]\n    fold_X_test,  fold_y_test  = train_c.iloc[val_idx], target.iloc[val_idx]\n    \n    model = keras_model(**params)\n\n    earlyStopping = callbacks.EarlyStopping(min_delta=0.001, patience=10, verbose=0)\n    checkpoint = callbacks.ModelCheckpoint(\"/tmp/checkpoint\", monitor=\"val_root_mean_squared_error\", mode=\"min\", save_best_only=True, save_weights_only=True)\n    history = model.fit(\n        [ fold_X_train.drop(\"f1_c\", axis=1).drop(\"f86\", axis=1), fold_X_train[[\"f1_c\"]], fold_X_train[[\"f86_c\"]] ], \n        fold_y_train, \n        validation_data=([ fold_X_test.drop(\"f1_c\", axis=1).drop(\"f86_c\", axis=1), fold_X_test[[\"f1_c\"]], fold_X_test[[\"f86_c\"]] ], fold_y_test), \n        batch_size=2048, \n        epochs=400, \n        callbacks=[earlyStopping, checkpoint],\n        verbose=0\n    )\n    model.load_weights(\"/tmp/checkpoint\")\n\n    fold_preds[fold] = pd.concat([fold_preds[fold], pd.DataFrame(model.predict([ fold_X_test.drop(\"f1_c\", axis=1).drop(\"f86_c\", axis=1), fold_X_test[[\"f1_c\"]], fold_X_test[[\"f86_c\"]] ]), columns=[\"attndnn\"])], axis=1)\n    fold_test_preds[fold] = pd.concat([fold_test_preds[fold], pd.DataFrame(model.predict([ test_c.drop(\"f1_c\", axis=1).drop(\"f86_c\", axis=1), test_c[[\"f1_c\"]], test_c[[\"f86_c\"]] ]), columns=[\"attndnn\"])], axis=1)\n    \n    score = score_model(model, [ fold_X_test.drop(\"f1_c\", axis=1).drop(\"f86_c\", axis=1), fold_X_test[[\"f1_c\"]], fold_X_test[[\"f86_c\"]] ], fold_y_test)\n    rmse_dict[\"attn_dnn_\" + str(fold + 1)] = score\n\n    print('#### fold #########', score)","metadata":{"execution":{"iopub.status.busy":"2021-08-25T15:29:34.780994Z","iopub.execute_input":"2021-08-25T15:29:34.781362Z","iopub.status.idle":"2021-08-25T15:34:08.413127Z","shell.execute_reply.started":"2021-08-25T15:29:34.781326Z","shell.execute_reply":"2021-08-25T15:34:08.411017Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## DNN","metadata":{}},{"cell_type":"markdown","source":"### Model Definition","metadata":{}},{"cell_type":"code","source":"def keras_model(block_activation: str, final_activation: str, embedding_width_1: int, embedding_width_2: int, lr: int, blocks: int, dropout: float):\n    width = train_c.shape[1] - 2 + embedding_width_1 + embedding_width_2\n    \n    def block(inp):\n        x = layers.Dense(width, kernel_initializer =\"random_uniform\")(inp)\n        x = layers.Dropout(dropout)(x)\n        x = layers.BatchNormalization()(x)\n        x = layers.Activation(block_activation)(x)\n        x = layers.Dense(width, kernel_initializer =\"random_uniform\")(x)\n        x = layers.Dropout(dropout)(x)\n        x = layers.BatchNormalization()(x)\n        x = layers.Activation(block_activation)(x)\n        x = layers.Dense(width, kernel_initializer =\"random_uniform\")(x)\n        x = layers.Dropout(dropout)(x)\n        x = layers.BatchNormalization()(x)\n        x = layers.Activation(block_activation)(x)\n        x = layers.Add()([x, inp])\n\n        return x\n\n    inp_s = layers.Input(shape=(train_c.shape[1] - 2,))\n    inp_c_1 = layers.Input(shape=(1,))\n    inp_c_2 = layers.Input(shape=(1,))\n    \n    e1 = layers.Flatten()(layers.Embedding(cal_X_train_c[\"f1_c\"].nunique(), embedding_width_1)(inp_c_1))\n    e2 = layers.Flatten()(layers.Embedding(cal_X_train_c[\"f86_c\"].nunique(), embedding_width_2)(inp_c_2))\n    \n    x = layers.Concatenate()([inp_s, e1, e2])\n\n    for _ in range(blocks):\n        x = block(x)\n\n    x = layers.Dense(1, activation=final_activation, kernel_initializer =\"random_uniform\")(x)\n\n    model = keras.Model(inputs=[inp_s, inp_c_1, inp_c_2], outputs=x)\n    model.compile(optimizer=Adam(lr=lr), loss=\"mse\", metrics=[metrics.RootMeanSquaredError()])\n\n    return model","metadata":{"execution":{"iopub.status.busy":"2021-08-25T15:34:12.443758Z","iopub.execute_input":"2021-08-25T15:34:12.444113Z","iopub.status.idle":"2021-08-25T15:34:12.461157Z","shell.execute_reply.started":"2021-08-25T15:34:12.44408Z","shell.execute_reply":"2021-08-25T15:34:12.459772Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Hyperparameter Optimisation","metadata":{}},{"cell_type":"code","source":"if False:\n    def objective(trial):\n        params = {\n            \"blocks\": trial.suggest_int(\"blocks\", 8, 32),\n            \"dropout\": trial.suggest_float(\"dropout\", 0, 0.5),\n            \"lr\": trial.suggest_loguniform(\"lr\", 1e-5, 1e-3),\n            \"block_activation\": trial.suggest_categorical(\"block_activation\", [\"elu\", \"relu\"]),\n            \"final_activation\": trial.suggest_categorical(\"final_activation\", [\"elu\", \"relu\"]),\n            \"embedding_width_1\": trial.suggest_int(\"embedding_width_1\", 16, 64, 8),\n            \"embedding_width_2\": trial.suggest_int(\"embedding_width_2\", 16, 64, 8)\n        }\n\n        model = keras_model(**params)\n\n        earlyStopping = callbacks.EarlyStopping(min_delta=0.001, patience=10, verbose=0)\n        checkpoint = callbacks.ModelCheckpoint(\"/tmp/checkpoint\", monitor=\"val_root_mean_squared_error\", mode=\"min\", save_best_only=True, save_weights_only=True)\n        history = model.fit(\n            [ cal_X_train_c.drop(\"f1_c\", axis=1).drop(\"f86_c\", axis=1), cal_X_train_c[[\"f1_c\"]], cal_X_train_c[[\"f86_c\"]] ], \n            cal_y_train_c, \n            validation_data=([ cal_X_val_c.drop(\"f1_c\", axis=1).drop(\"f86_c\", axis=1), cal_X_val_c[[\"f1_c\"]], cal_X_val_c[[\"f86_c\"]] ], cal_y_val_c), \n            batch_size=2048, \n            epochs=100, \n            callbacks=[earlyStopping, checkpoint],\n            verbose=0\n        )\n        model.load_weights(\"/tmp/checkpoint\")\n\n        return score_model(model, [ cal_X_val_c.drop(\"f1_c\", axis=1).drop(\"f86_c\", axis=1), cal_X_val_c[[\"f1_c\"]], cal_X_val_c[[\"f86_c\"]] ], cal_y_val_c)\n\n    study = optuna.create_study(direction=\"minimize\")\n    study.optimize(objective, n_trials=100, timeout=None)\n    trial = study.best_trial\n    params = trial.params\n    \n    with open(\"dnn.json\", \"w\") as file:\n        file.write(json.dumps(params, indent=4))\nelse:\n    params = {\n        'blocks': 11,\n        'dropout': 0.07658614707982882, \n        'lr': 1.7141256541284283e-05,\n        'block_activation': 'elu', \n        'final_activation': 'relu', \n        'embedding_width_1': 24,\n        'embedding_width_2': 16\n    }\n\nparams","metadata":{"execution":{"iopub.status.busy":"2021-08-25T15:34:21.353095Z","iopub.execute_input":"2021-08-25T15:34:21.353709Z","iopub.status.idle":"2021-08-25T15:34:21.372769Z","shell.execute_reply.started":"2021-08-25T15:34:21.353658Z","shell.execute_reply":"2021-08-25T15:34:21.371643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Stack Calibration Model","metadata":{}},{"cell_type":"code","source":"model = keras_model(**params)\n\nearlyStopping = callbacks.EarlyStopping(min_delta=0.001, patience=10, verbose=0)\ncheckpoint = callbacks.ModelCheckpoint(\"/tmp/checkpoint\", monitor=\"val_root_mean_squared_error\", mode=\"min\", save_best_only=True, save_weights_only=True)\nhistory = model.fit(\n    [ cal_X_train_c.drop(\"f1_c\", axis=1).drop(\"f86_c\", axis=1), cal_X_train_c[[\"f1_c\"]], cal_X_train_c[[\"f86_c\"]] ],\n    cal_y_train_c, \n    validation_data=([ cal_X_val_c.drop(\"f1_c\", axis=1).drop(\"f86_c\", axis=1), cal_X_val_c[[\"f1_c\"]], cal_X_val_c[[\"f86_c\"]] ], cal_y_val_c),\n    batch_size=2048, \n    epochs=400, \n    callbacks=[earlyStopping, checkpoint],\n    verbose=0\n)\nmodel.load_weights(\"/tmp/checkpoint\")\n\ncal_preds = pd.concat([cal_preds, pd.DataFrame(model.predict([ cal_X_val_c.drop(\"f1_c\", axis=1).drop(\"f86_c\", axis=1), cal_X_val_c[[\"f1_c\"]], cal_X_val_c[[\"f86_c\"]] ]), columns=[\"dnn\"])], axis=1)\n\nscore_model(model, [ cal_X_val_c.drop(\"f1_c\", axis=1).drop(\"f86_c\", axis=1), cal_X_val_c[[\"f1_c\"]], cal_X_val_c[[\"f86_c\"]] ], cal_y_val_c)","metadata":{"execution":{"iopub.status.busy":"2021-08-25T15:34:31.015394Z","iopub.execute_input":"2021-08-25T15:34:31.015777Z","iopub.status.idle":"2021-08-25T15:34:32.603692Z","shell.execute_reply.started":"2021-08-25T15:34:31.015746Z","shell.execute_reply":"2021-08-25T15:34:32.60232Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### KFold Prediction","metadata":{}},{"cell_type":"code","source":"for fold, (trn_idx, val_idx) in enumerate(KFold(n_splits=20, random_state=42, shuffle=True).split(train, target)):\n    print(\"Fold :\", fold + 1)\n\n    # create dataset\n    fold_X_train, fold_y_train = train_c.iloc[trn_idx], target.iloc[trn_idx]\n    fold_X_test,  fold_y_test  = train_c.iloc[val_idx], target.iloc[val_idx]\n    \n    model = keras_model(**params)\n\n    earlyStopping = callbacks.EarlyStopping(min_delta=0.001, patience=10, verbose=0)\n    checkpoint = callbacks.ModelCheckpoint(\"/tmp/checkpoint\", monitor=\"val_root_mean_squared_error\", mode=\"min\", save_best_only=True, save_weights_only=True)\n    history = model.fit(\n        [ fold_X_train.drop(\"f1_c\", axis=1).drop(\"f86_c\", axis=1), fold_X_train[[\"f1_c\"]], fold_X_train[[\"f86_c\"]] ], \n        fold_y_train, \n        validation_data=([ fold_X_test.drop(\"f1_c\", axis=1).drop(\"f86_c\", axis=1), fold_X_test[[\"f1_c\"]], fold_X_test[[\"f86_c\"]] ], fold_y_test), \n        batch_size=2048, \n        epochs=400, \n        callbacks=[earlyStopping, checkpoint],\n        verbose=0\n    )\n    model.load_weights(\"/tmp/checkpoint\")\n\n    fold_preds[fold] = pd.concat([fold_preds[fold], pd.DataFrame(model.predict([ fold_X_test.drop(\"f1_c\", axis=1).drop(\"f86_c\", axis=1), fold_X_test[[\"f1_c\"]], fold_X_test[[\"f86_c\"]] ]), columns=[\"dnn\"])], axis=1)\n    fold_test_preds[fold] = pd.concat([fold_test_preds[fold], pd.DataFrame(model.predict([ test_c.drop(\"f1_c\", axis=1).drop(\"f86_c\", axis=1), test_c[[\"f1_c\"]], test_c[[\"f86_c\"]] ]), columns=[\"dnn\"])], axis=1)\n\n    score = score_model(model, [ fold_X_test.drop(\"f1_c\", axis=1).drop(\"f86_c\", axis=1), fold_X_test[[\"f1_c\"]], fold_X_test[[\"f86_c\"]] ], fold_y_test)\n    rmse_dict[\"dnn_s_\" + str(fold + 1)] = score\n\n    print('#### fold #########', score)","metadata":{"execution":{"iopub.status.busy":"2021-08-25T14:40:32.846705Z","iopub.execute_input":"2021-08-25T14:40:32.847073Z","iopub.status.idle":"2021-08-25T14:42:19.547504Z","shell.execute_reply.started":"2021-08-25T14:40:32.847036Z","shell.execute_reply":"2021-08-25T14:42:19.545149Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Stack","metadata":{}},{"cell_type":"code","source":"preds_dict = {}","metadata":{"execution":{"iopub.status.busy":"2021-08-25T17:36:16.755447Z","iopub.execute_input":"2021-08-25T17:36:16.755854Z","iopub.status.idle":"2021-08-25T17:36:16.760583Z","shell.execute_reply.started":"2021-08-25T17:36:16.755821Z","shell.execute_reply":"2021-08-25T17:36:16.759067Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corr = cal_preds.corr()\nmask = np.triu(np.ones_like(corr, dtype=bool))\nf, ax = plt.subplots(figsize=(11, 9))\nsns.heatmap(\n    corr,\n    mask=mask, \n    vmax=.3,\n    center=0,\n    square=True, linewidths=.5, cbar_kws={\"shrink\": .5})","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stack_X = pd.concat([cal_X_val_c.reset_index(), cal_preds], axis=1)\nstack_y = cal_y_val_c\n\nstack_cal_X_train, stack_cal_X_val, stack_cal_y_train, stack_cal_y_val = train_test_split(stack_X, stack_y, random_state=0, test_size=1 / FOLDS)\nlen(stack_cal_X_train), len(stack_cal_X_val)","metadata":{"execution":{"iopub.status.busy":"2021-08-25T17:36:16.98373Z","iopub.execute_input":"2021-08-25T17:36:16.984093Z","iopub.status.idle":"2021-08-25T17:36:17.012783Z","shell.execute_reply.started":"2021-08-25T17:36:16.984061Z","shell.execute_reply":"2021-08-25T17:36:17.011364Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params = { \"objective\": \"mean_squared_error\", \"metric\": \"rmse\", \"device\": \"cpu\" }\n\nlgb_train = lgbm.Dataset(stack_cal_X_train.drop(\"f1_c\", axis=1).drop(\"f86_c\", axis=1), stack_cal_y_train)\nlgb_valid = lgbm.Dataset(stack_cal_X_val.drop(\"f1_c\", axis=1).drop(\"f86_c\", axis=1), stack_cal_y_val)\n\nmodel = lgbo.train(params, lgb_train, valid_sets=[lgb_valid], verbose_eval=False, num_boost_round=100, early_stopping_rounds=5)\n\nparams = model.params\n\ndel params[\"early_stopping_round\"]\nparams[\"learning_rate\"] = 0.006\nparams[\"num_iterations\"] = 4500\n\nwith open(\"stack_lgbm.json\", \"w\") as file:\n    file.write(json.dumps(params, indent=4))\n\nparams","metadata":{"execution":{"iopub.status.busy":"2021-08-25T17:32:52.270304Z","iopub.execute_input":"2021-08-25T17:32:52.270716Z","iopub.status.idle":"2021-08-25T17:34:00.041476Z","shell.execute_reply.started":"2021-08-25T17:32:52.270685Z","shell.execute_reply":"2021-08-25T17:34:00.040023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for fold, (trn_idx, val_idx) in enumerate(KFold(n_splits=20, random_state=42, shuffle=True).split(train_c, target)):\n    print(\"Fold :\", fold + 1)\n    \n    fold_X, fold_y = train_c.iloc[val_idx], target.iloc[val_idx]\n    \n    fold_X = pd.concat((fold_X.drop(\"f1_c\", axis=1).drop(\"f86_c\", axis=1).reset_index(), fold_preds[fold]), axis=1)\n\n    lgb_train = lgbm.Dataset(fold_X, fold_y)\n    model = lgbm.train(params, lgb_train, verbose_eval=False)\n    \n    ax = lgbm.plot_importance(model, figsize=(15,15))\n    plt.show()\n    \n    preds_pre = pd.concat((test_c.drop(\"f1_c\", axis=1).drop(\"f86_c\", axis=1).reset_index(), fold_test_preds[fold]), axis=1)\n    \n    preds_dict[\"stack_\" + str(fold + 1)] = model.predict(preds_pre)","metadata":{"execution":{"iopub.status.busy":"2021-08-25T17:35:37.729519Z","iopub.execute_input":"2021-08-25T17:35:37.729877Z","iopub.status.idle":"2021-08-25T17:35:45.759087Z","shell.execute_reply.started":"2021-08-25T17:35:37.729845Z","shell.execute_reply":"2021-08-25T17:35:45.756601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Performance Analysis","metadata":{}},{"cell_type":"code","source":"keys = list(rmse_dict.keys())\nvals = [rmse_dict[k] for k in keys]\nplt.figure(figsize=(15, 10))\ng = sns.barplot(x=keys, y=vals)\ng.set_ylim(7.5, 8.2)\nNone","metadata":{"execution":{"iopub.status.busy":"2021-08-25T17:32:44.167419Z","iopub.status.idle":"2021-08-25T17:32:44.168149Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Ensemble","metadata":{}},{"cell_type":"code","source":"total_preds = np.zeros(shape=preds.shape[0])\nfor i, pred in enumerate(preds_dict.values()):\n    pred_ = pd.read_csv('../input/tabular-playground-series-aug-2021/sample_submission.csv')\n    pred_.loss = pred\n    pred_.to_csv('submission_'+ str(i) +'.csv', index=False)\n    \n    total_preds += pred\n\ntotal_preds /= len(preds_dict.keys())\nsub = pd.read_csv('../input/tabular-playground-series-aug-2021/sample_submission.csv')\nsub.loss = total_preds\nsub.to_csv('submission.csv',index=False)\nsub","metadata":{"execution":{"iopub.status.busy":"2021-08-25T17:32:44.169882Z","iopub.status.idle":"2021-08-25T17:32:44.170761Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}