{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom fastai.tabular.all import *\nimport optuna\nfrom optuna.integration import FastAIV2PruningCallback\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-22T02:42:05.071325Z","iopub.execute_input":"2021-08-22T02:42:05.071748Z","iopub.status.idle":"2021-08-22T02:42:08.784873Z","shell.execute_reply.started":"2021-08-22T02:42:05.071659Z","shell.execute_reply":"2021-08-22T02:42:08.783648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Please upvote if you found this useful ","metadata":{}},{"cell_type":"markdown","source":"# Get data and split into continuous and categorical variables\n\nNote that for this dataset we'll treat all the features as **continuous**","metadata":{}},{"cell_type":"code","source":"path = Path('/kaggle/input/tabular-playground-series-aug-2021')\nPath.BASE_PATH = path\n\ndf = pd.read_csv(path/'train.csv', low_memory=False)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-22T02:42:08.788552Z","iopub.execute_input":"2021-08-22T02:42:08.789055Z","iopub.status.idle":"2021-08-22T02:42:19.354991Z","shell.execute_reply.started":"2021-08-22T02:42:08.789026Z","shell.execute_reply":"2021-08-22T02:42:19.353907Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dep_var = 'loss'\ncont, cat = cont_cat_split(df, dep_var=dep_var)\ncont.remove('id')\nlen(cont), len(cat)","metadata":{"execution":{"iopub.status.busy":"2021-08-22T02:42:19.357632Z","iopub.execute_input":"2021-08-22T02:42:19.358228Z","iopub.status.idle":"2021-08-22T02:42:19.427282Z","shell.execute_reply.started":"2021-08-22T02:42:19.358186Z","shell.execute_reply":"2021-08-22T02:42:19.425983Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Helper functions\n\nWe'll use RMSE (Root Mean Squared Error) for the loss function.\nLet's also make helper functions to create `DataLoaders` and to train our Neural Net (NN)","metadata":{}},{"cell_type":"code","source":"def rmse(y_hat, y):\n    \"\"\"\n    Root Mean Squared Error\n    Note: the competition does not divide by N so we don't here\n    \"\"\"\n    return torch.sqrt(torch.mean((y_hat - y)**2))","metadata":{"execution":{"iopub.status.busy":"2021-08-22T02:42:19.429659Z","iopub.execute_input":"2021-08-22T02:42:19.430146Z","iopub.status.idle":"2021-08-22T02:42:19.437793Z","shell.execute_reply.started":"2021-08-22T02:42:19.430103Z","shell.execute_reply":"2021-08-22T02:42:19.436286Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_dls(df, bs = 2048, valid_pct = 0.25):\n    to = TabularPandas(df, \n                  procs=[Normalize],\n                  y_names=dep_var,\n                  cont_names=cont,\n                  cat_names=cat,\n                  splits=RandomSplitter(valid_pct)(range_of(df)),\n                  reduce_memory=False)\n    return to.dataloaders(bs=bs)","metadata":{"execution":{"iopub.status.busy":"2021-08-22T02:42:19.439698Z","iopub.execute_input":"2021-08-22T02:42:19.440284Z","iopub.status.idle":"2021-08-22T02:42:19.448975Z","shell.execute_reply.started":"2021-08-22T02:42:19.440241Z","shell.execute_reply":"2021-08-22T02:42:19.447651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"early_stopping_cbs = [\n    EarlyStoppingCallback(monitor='valid_loss', patience=2, min_delta=0.01),\n]\n\ndef train_nn(dls, layers, ps, wd, y_range_eps, cbs):\n    learn = tabular_learner(dls, layers=layers,\n                            #procs=[Normalize],\n                            y_range=(dls.train.y.min() - y_range_eps, dls.train.y.max() + y_range_eps),\n                            config={'ps': ps, 'act_cls': nn.LeakyReLU(inplace=True)}, \n                            wd=wd, \n                            loss_func=rmse)\n    suggested_lr = learn.lr_find(show_plot=False)\n    learn.fit_one_cycle(2, 1e-3, cbs=early_stopping_cbs + cbs)\n    learn.fit_one_cycle(55, 1e-3 / 100, cbs=early_stopping_cbs + cbs)\n    return learn","metadata":{"execution":{"iopub.status.busy":"2021-08-22T02:42:19.450622Z","iopub.execute_input":"2021-08-22T02:42:19.451294Z","iopub.status.idle":"2021-08-22T02:42:19.462675Z","shell.execute_reply.started":"2021-08-22T02:42:19.45125Z","shell.execute_reply":"2021-08-22T02:42:19.461491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Optuna Hyperparameter Tuning\nI just learned about Optuna and it's been a game changer. Now I don't have to painstakingly fiddle with hyperparameters. Instead I just specify a range of values and Optuna takes care of the rest. \n\nIt's honestly so convenient it almost feels like a cheat code. ","metadata":{}},{"cell_type":"markdown","source":"Notice how in our `objective` function, we always decrease the layer sizes. This is because it doesn't really make sense to increase layer_sizes through the network. Increasing layer sizes means we're asking the model to generate more numbers from less numbers. Instead what makes sense is to have the layer sizes __decrease__ because then we're asking the model to take a large number of features and turn them into a smaller, more feature-rich representation. ","metadata":{}},{"cell_type":"code","source":"dls = create_dls(df, bs=8192, valid_pct=0.5)\n\ndef objective(trial: optuna.Trial, y_range_eps: float):\n    num_layers = trial.suggest_int('num_layers', 1, 3)\n    wd = trial.suggest_float('wd', 0.0, 1.0)\n    layers = [\n        trial.suggest_categorical('layer_0', [512, 256, 128]),\n        trial.suggest_categorical('layer_1', [512, 256, 128]),\n        trial.suggest_categorical('layer_2', [512, 256, 128]),\n    ]\n    ps = [\n        trial.suggest_discrete_uniform('dropout_0', 0.0, 0.95, 0.05),\n        trial.suggest_discrete_uniform('dropout_1', 0.0, 0.95, 0.05),\n        trial.suggest_discrete_uniform('dropout_2', 0.0, 0.95, 0.05),\n    ]\n    learn = train_nn(dls, layers[:num_layers], ps[:num_layers], wd, \n                     y_range_eps=y_range_eps,\n                     cbs=[FastAIV2PruningCallback(trial, monitor='valid_loss',)])\n    return learn.validate()[0]","metadata":{"execution":{"iopub.status.busy":"2021-08-22T02:56:21.07205Z","iopub.execute_input":"2021-08-22T02:56:21.072427Z","iopub.status.idle":"2021-08-22T02:56:22.404382Z","shell.execute_reply.started":"2021-08-22T02:56:21.072395Z","shell.execute_reply":"2021-08-22T02:56:22.402849Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Comment out to save time\n\n# timeout = 3600 * 2\n# study = optuna.create_study(direction='minimize',\n#                            pruner=optuna.pruners.MedianPruner(n_warmup_steps=4, interval_steps=2))\n# study.optimize(lambda trial: objective(trial, y_range_eps=0.00),\n#                n_trials=35, timeout=timeout)","metadata":{"_kg_hide-output":false,"scrolled":true,"execution":{"iopub.status.busy":"2021-08-22T02:56:22.408142Z","iopub.execute_input":"2021-08-22T02:56:22.408995Z","iopub.status.idle":"2021-08-22T03:04:20.092999Z","shell.execute_reply.started":"2021-08-22T02:56:22.408876Z","shell.execute_reply":"2021-08-22T03:04:20.091505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optuna.visualization.plot_optimization_history(study)","metadata":{"execution":{"iopub.status.busy":"2021-08-22T03:04:33.115064Z","iopub.execute_input":"2021-08-22T03:04:33.115432Z","iopub.status.idle":"2021-08-22T03:04:33.142326Z","shell.execute_reply.started":"2021-08-22T03:04:33.115383Z","shell.execute_reply":"2021-08-22T03:04:33.141032Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optuna.visualization.plot_param_importances(study)","metadata":{"execution":{"iopub.status.busy":"2021-08-22T03:04:33.877653Z","iopub.execute_input":"2021-08-22T03:04:33.878054Z","iopub.status.idle":"2021-08-22T03:04:35.318522Z","shell.execute_reply.started":"2021-08-22T03:04:33.878022Z","shell.execute_reply":"2021-08-22T03:04:35.317096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Number of finished trials:', len(study.trials))\nprint('Best trial parameters:', study.best_trial.params)\nprint('Best score:', study.best_value)","metadata":{"execution":{"iopub.status.busy":"2021-08-22T03:04:41.414148Z","iopub.execute_input":"2021-08-22T03:04:41.414521Z","iopub.status.idle":"2021-08-22T03:04:41.431604Z","shell.execute_reply.started":"2021-08-22T03:04:41.414489Z","shell.execute_reply":"2021-08-22T03:04:41.430259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Optuna Results\n\nFor this dataset I have not been able to get below ~7.90 on the validation set using a NN; and not below 7.94 on the final evaluation set used by kaggle.\n\nIn a way this is expected, as XGBRegressor and CatBoostRegressor will often perform better on tabular datasets. Exactly why NN's cannot achieve similar performance is something I'm still looking into. With XGB- and CatBoost- Regressors, I've been able to achieve 7.87 but still cannot hit the SOTA 7.85. If you have any suggestions, tips, or comments, please share them below! ","metadata":{}},{"cell_type":"markdown","source":"# Train Model using Optuna Hyperparameters","metadata":{}},{"cell_type":"code","source":"learn = train_nn(dls, layers=[512, 256],\n                 ps=[0.6, 0.7], wd=0.373, y_range_eps=0.0, cbs=[])","metadata":{"execution":{"iopub.status.busy":"2021-08-22T03:06:58.920401Z","iopub.execute_input":"2021-08-22T03:06:58.920794Z","iopub.status.idle":"2021-08-22T03:07:10.861617Z","shell.execute_reply.started":"2021-08-22T03:06:58.920747Z","shell.execute_reply":"2021-08-22T03:07:10.860168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create Submission","metadata":{}},{"cell_type":"code","source":"test_df = pd.read_csv(path/'test.csv')\ntest_dl = learn.dls.test_dl(test_df)\ntest_dl.show_batch()","metadata":{"execution":{"iopub.status.busy":"2021-08-22T03:07:10.863764Z","iopub.execute_input":"2021-08-22T03:07:10.86434Z","iopub.status.idle":"2021-08-22T03:07:15.052686Z","shell.execute_reply.started":"2021-08-22T03:07:10.864278Z","shell.execute_reply":"2021-08-22T03:07:15.05157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds, _ = learn.get_preds(dl=test_dl)\npreds = preds.numpy().flatten()","metadata":{"execution":{"iopub.status.busy":"2021-08-22T03:07:15.055105Z","iopub.execute_input":"2021-08-22T03:07:15.055571Z","iopub.status.idle":"2021-08-22T03:07:15.490723Z","shell.execute_reply.started":"2021-08-22T03:07:15.055526Z","shell.execute_reply":"2021-08-22T03:07:15.489611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df = pd.DataFrame({'id': test_df.id, 'loss': preds})\nsubmission_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-22T03:07:15.492629Z","iopub.execute_input":"2021-08-22T03:07:15.493139Z","iopub.status.idle":"2021-08-22T03:07:15.509153Z","shell.execute_reply.started":"2021-08-22T03:07:15.493098Z","shell.execute_reply":"2021-08-22T03:07:15.507834Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-08-22T03:07:15.511005Z","iopub.execute_input":"2021-08-22T03:07:15.511506Z","iopub.status.idle":"2021-08-22T03:07:15.930748Z","shell.execute_reply.started":"2021-08-22T03:07:15.51145Z","shell.execute_reply":"2021-08-22T03:07:15.929615Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Please upvote if you found this useful","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}