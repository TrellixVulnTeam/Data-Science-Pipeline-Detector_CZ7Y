{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import accuracy_score\nimport random\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\ntrain = pd.read_csv('../input/tabular-playground-series-aug-2021/train.csv')\ntest = pd.read_csv('../input/tabular-playground-series-aug-2021/test.csv')\n\ntrain_id = train[\"id\"]\n\ntrain.drop(\"id\", axis = 1, inplace = True)\ntrain.head(1)","metadata":{"execution":{"iopub.status.busy":"2021-08-19T16:46:21.045512Z","iopub.execute_input":"2021-08-19T16:46:21.046171Z","iopub.status.idle":"2021-08-19T16:46:33.973118Z","shell.execute_reply.started":"2021-08-19T16:46:21.046041Z","shell.execute_reply":"2021-08-19T16:46:33.972226Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Machine learning with random forest as a baseline.\nSince there is a lot of data and it takes too much time to calculate, we will approach by extracting training data.\n（It takes a lot of time to calculate here, so I will extract the data and proceed.）","metadata":{}},{"cell_type":"code","source":"#RandomForestRegressor\nsample_1 = [random.randint(0, len(train)) for i in range(1000)]\nsample_2 = [random.randint(0, len(train)) for i in range(2000)]\nsample_3 = [random.randint(0, len(train)) for i in range(3000)]\n\nsample = [sample_1, sample_2, sample_3]\n\nfig = plt.figure(figsize = [14,6])\nfor i in range(len(sample)):\n    X, y = train.iloc[sample[i],:-1], train.iloc[sample[i],-1]\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n\n    forest = RandomForestRegressor(random_state = 1, n_jobs = -1)\n    forest.fit(X_train, y_train)\n    y_train_pred = forest.predict(X_train)\n    y_test_pred = forest.predict(X_test)\n\n    print(\"rmse train:{:.2f} / test:{:.2f}\".format(np.sqrt(mean_squared_error(y_train, y_train_pred)), np.sqrt(mean_squared_error(y_test, y_test_pred))))\n    print(\"r2_score train:{:.2f} / test:{:.2f}\".format(r2_score(y_train, y_train_pred), r2_score(y_test, y_test_pred)))\n\n    fig.add_subplot(1, 3, i+1)\n    plt.scatter(y_train, y_train_pred, label = \"train\")\n    plt.scatter(y_test, y_test_pred, label = \"test\")\n    plt.xlabel(\"raw data\")\n    plt.ylabel(\"predict data\")\n    plt.xlim(0,40)\n    plt.ylim(0,40)\n    plt.grid()","metadata":{"execution":{"iopub.status.busy":"2021-08-19T16:46:33.974434Z","iopub.execute_input":"2021-08-19T16:46:33.974704Z","iopub.status.idle":"2021-08-19T16:46:54.482337Z","shell.execute_reply.started":"2021-08-19T16:46:33.974678Z","shell.execute_reply":"2021-08-19T16:46:54.481275Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The large loss value is completely unpredictable.\n（Is the score the same as the one with all average values?）\n\nSubmit score when learning with all data:8.02\n\nSince the original theme is the default prediction of loans, I would like to predict even those with a lot of losses.\nTry another Algols.\n","metadata":{}},{"cell_type":"code","source":"#LinearRegression\nrandom.seed(0)\nsample_1 = [random.randint(0, len(train)) for i in range(1000)]\nsample_2 = [random.randint(0, len(train)) for i in range(2000)]\nsample_3 = [random.randint(0, len(train)) for i in range(3000)]\n\nsample = [sample_1, sample_2, sample_3]\n\npipe_lr = make_pipeline(StandardScaler(),\n                        LinearRegression())\n\nfig = plt.figure(figsize = [14,6])\nfor i in range(len(sample)):\n    X, y = train.iloc[sample[i],:-1], train.iloc[sample[i],-1]\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n\n    pipe_lr.fit(X_train, y_train)\n    y_train_pred = pipe_lr.predict(X_train)\n    y_test_pred = pipe_lr.predict(X_test)\n\n    print(\"rmse train:{:.2f} / test:{:.2f}\".format(np.sqrt(mean_squared_error(y_train, y_train_pred)), np.sqrt(mean_squared_error(y_test, y_test_pred))))\n    print(\"r2_score train:{:.2f} / test:{:.2f}\".format(r2_score(y_train, y_train_pred), r2_score(y_test, y_test_pred)))\n\n    fig.add_subplot(1, 3, i+1)\n    plt.scatter(y_train, y_train_pred, label = \"train\")\n    plt.scatter(y_test, y_test_pred, label = \"test\")\n    plt.xlabel(\"raw data\")\n    plt.ylabel(\"predict data\")\n    plt.xlim(0,40)\n    plt.ylim(0,40)\n    plt.grid()","metadata":{"execution":{"iopub.status.busy":"2021-08-19T16:46:54.484594Z","iopub.execute_input":"2021-08-19T16:46:54.484997Z","iopub.status.idle":"2021-08-19T16:46:55.547309Z","shell.execute_reply.started":"2021-08-19T16:46:54.484956Z","shell.execute_reply":"2021-08-19T16:46:55.546616Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The tendency is the same as randomforestregressor, and the value with large loss cannot be predicted.\n\nSubmit score when learning with all data ：7.93","metadata":{}},{"cell_type":"code","source":"#KneighborsRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\n\nrandom.seed(0)\nsample_1 = [random.randint(0, len(train)) for i in range(1000)]\nsample_2 = [random.randint(0, len(train)) for i in range(2000)]\nsample_3 = [random.randint(0, len(train)) for i in range(3000)]\n\nsample = [sample_1, sample_2, sample_3]\n\n\n\nn = [3,7,20, 50]\nk = 0\nfor j in range(len(n)):\n    fig = plt.figure(figsize = [24,18])\n    for i in range(len(sample)):\n        X, y = train.iloc[sample[i],:-1], train.iloc[sample[i],-1]\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n        pipe_lr = make_pipeline(StandardScaler(),\n                                KNeighborsRegressor(n_neighbors = n[j]))\n        pipe_lr.fit(X_train, y_train)\n        y_train_pred = pipe_lr.predict(X_train)\n        y_test_pred = pipe_lr.predict(X_test)\n        \n        print(\"neighbor:{:.2f} \".format(n[j]))\n        print(\"rmse train:{:.2f} / test:{:.2f}\".format(np.sqrt(mean_squared_error(y_train, y_train_pred)), np.sqrt(mean_squared_error(y_test, y_test_pred))))\n        print(\"r2_score train:{:.2f} / test:{:.2f}\".format(r2_score(y_train, y_train_pred), r2_score(y_test, y_test_pred)))\n        print(\"-\"*50)\n\n        fig.add_subplot(5, 3, k+1)\n        plt.scatter(y_train, y_train_pred, label = \"train\")\n        plt.scatter(y_test, y_test_pred, label = \"test\")\n        plt.xlabel(\"raw data\")\n        plt.ylabel(\"predict data\")\n        plt.xlim(0,40)\n        plt.ylim(0,40)\n        plt.grid()\n        k+=1","metadata":{"execution":{"iopub.status.busy":"2021-08-19T16:46:55.55038Z","iopub.execute_input":"2021-08-19T16:46:55.550744Z","iopub.status.idle":"2021-08-19T16:47:07.07818Z","shell.execute_reply.started":"2021-08-19T16:46:55.550716Z","shell.execute_reply":"2021-08-19T16:47:07.077129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The tendency is the same as randomforestregressor, and the value with large loss cannot be predicted.\n\nSubmit score when learning with all data ：\n","metadata":{}},{"cell_type":"code","source":"sns.distplot(train[\"loss\"])","metadata":{"execution":{"iopub.status.busy":"2021-08-19T16:47:07.079823Z","iopub.execute_input":"2021-08-19T16:47:07.080257Z","iopub.status.idle":"2021-08-19T16:47:08.699118Z","shell.execute_reply.started":"2021-08-19T16:47:07.080206Z","shell.execute_reply":"2021-08-19T16:47:08.698136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Can't predict high loss due to bias in training data? Try to cut loss 0 at 1/2.\nTry this idea in randomforestregressor for the time being.","metadata":{}},{"cell_type":"code","source":"#RandomForestRegressor delete the value of 0 loss at 1/2\ntmp_0 = train[train[\"loss\"] ==0]\ntmp_1 = train[train[\"loss\"] !=0]\n\nn_len = len(tmp_0) // 2\ntmp_0 = tmp_0.iloc[0:n_len]\n\ntrain_del = pd.concat([tmp_0, tmp_1], axis = 0)\n# train_del = tmp_1\n\n#RandomForestRegressor\nsample_1 = [random.randint(0, len(train_del)) for i in range(1000)]\nsample_2 = [random.randint(0, len(train_del)) for i in range(2000)]\nsample_3 = [random.randint(0, len(train_del)) for i in range(3000)]\n\nsample = [sample_1, sample_2, sample_3]\n\nfig = plt.figure(figsize = [14,6])\nfor i in range(len(sample)):\n    X, y = train_del.iloc[sample[i],:-1], train_del.iloc[sample[i],-1]\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n\n    forest = RandomForestRegressor(random_state = 1, n_jobs = -1)\n    forest.fit(X_train, y_train)\n    y_train_pred = forest.predict(X_train)\n    y_test_pred = forest.predict(X_test)\n\n    print(\"rmse train:{:.2f} / test:{:.2f}\".format(np.sqrt(mean_squared_error(y_train, y_train_pred)), np.sqrt(mean_squared_error(y_test, y_test_pred))))\n    print(\"r2_score train:{:.2f} / test:{:.2f}\".format(r2_score(y_train, y_train_pred), r2_score(y_test, y_test_pred)))\n\n    fig.add_subplot(1, 3, i+1)\n    plt.scatter(y_train, y_train_pred, label = \"train\")\n    plt.scatter(y_test, y_test_pred, label = \"test\")\n    plt.xlabel(\"raw data\")\n    plt.ylabel(\"predict data\")\n    plt.xlim(0,40)\n    plt.ylim(0,40)\n    plt.grid()","metadata":{"execution":{"iopub.status.busy":"2021-08-19T16:47:08.700347Z","iopub.execute_input":"2021-08-19T16:47:08.700647Z","iopub.status.idle":"2021-08-19T16:47:27.260726Z","shell.execute_reply.started":"2021-08-19T16:47:08.700619Z","shell.execute_reply":"2021-08-19T16:47:27.260092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I deleted 0 losses　1/2, but it didn't work well.\n\nSubmit score when learning with all data ：8.11","metadata":{}},{"cell_type":"code","source":"#RandomForestRegressor delete the value of loss range(0 <10)\ntmp_10 = train[train[\"loss\"] >=10]\n\ntrain_del = tmp_10\n\n#RandomForestRegressor\nrandom.seed(0)\nsample_1 = [random.randint(0, len(train_del)) for i in range(1000)]\nsample_2 = [random.randint(0, len(train_del)) for i in range(2000)]\nsample_3 = [random.randint(0, len(train_del)) for i in range(3000)]\n\nsample = [sample_1, sample_2, sample_3]\n\nfig = plt.figure(figsize = [14,6])\nfor i in range(len(sample)):\n    X, y = train_del.iloc[sample[i],:-1], train_del.iloc[sample[i],-1]\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n\n    forest = RandomForestRegressor(random_state = 1, n_jobs = -1)\n    forest.fit(X_train, y_train)\n    y_train_pred = forest.predict(X_train)\n    y_test_pred = forest.predict(X_test)\n\n    print(\"rmse train:{:.2f} / test:{:.2f}\".format(np.sqrt(mean_squared_error(y_train, y_train_pred)), np.sqrt(mean_squared_error(y_test, y_test_pred))))\n    print(\"r2_score train:{:.2f} / test:{:.2f}\".format(r2_score(y_train, y_train_pred), r2_score(y_test, y_test_pred)))\n\n    fig.add_subplot(1, 3, i+1)\n    plt.scatter(y_train, y_train_pred, label = \"train\")\n    plt.scatter(y_test, y_test_pred, label = \"test\")\n    plt.xlabel(\"raw data\")\n    plt.ylabel(\"predict data\")\n    plt.xlim(0,40)\n    plt.ylim(0,40)\n    plt.grid()","metadata":{"execution":{"iopub.status.busy":"2021-08-19T16:47:27.262279Z","iopub.execute_input":"2021-08-19T16:47:27.262649Z","iopub.status.idle":"2021-08-19T16:47:42.67245Z","shell.execute_reply.started":"2021-08-19T16:47:27.262622Z","shell.execute_reply":"2021-08-19T16:47:42.671356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since the loss of 10 or less has been deleted, naturally there is no predicted value of 10 or less.\nSubmit score when learning with all data ：13.1\nInterestingly, the small value of loss can be predicted before the data is deleted. And, as usual, the large value of loss cannot be predicted.\n\nLooking at the leaderboard, about 7.8 is the best. Should I go to the person who accurately predicts the smaller loss?","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}