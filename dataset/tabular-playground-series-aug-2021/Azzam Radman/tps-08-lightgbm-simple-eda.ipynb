{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# import standard libraries\nimport pandas as pd\nimport numpy as np\nimport lightgbm as lgbm\nimport optuna\nimport matplotlib.pyplot as plt\nfrom sklearn import model_selection, metrics\nimport seaborn as sns\n\n# show all columns\npd.set_option('max_columns', None)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-04T02:31:34.611941Z","iopub.execute_input":"2021-08-04T02:31:34.612383Z","iopub.status.idle":"2021-08-04T02:31:38.069821Z","shell.execute_reply.started":"2021-08-04T02:31:34.612293Z","shell.execute_reply":"2021-08-04T02:31:38.068585Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# read the data\ntrain = pd.read_csv('../input/tabular-playground-series-aug-2021/train.csv')\ntest = pd.read_csv('../input/tabular-playground-series-aug-2021/test.csv')\nsample = pd.read_csv('../input/tabular-playground-series-aug-2021/sample_submission.csv')\n\n# drop id columns from train and test sets\ntrain = train.drop('id', axis=1)\ntest = test.drop('id', axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-08-04T02:33:22.327356Z","iopub.execute_input":"2021-08-04T02:33:22.327943Z","iopub.status.idle":"2021-08-04T02:33:34.027622Z","shell.execute_reply.started":"2021-08-04T02:33:22.327898Z","shell.execute_reply":"2021-08-04T02:33:34.026424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Simple EDA","metadata":{}},{"cell_type":"code","source":"# histogramse for all variables with KDE\nplt.figure(figsize=(24, 6*(104/4)))\nfor i in range(len(train.columns.tolist())):\n    plt.subplot(26, 4, i+1)\n    if i <= 99:\n        sns.histplot(train[f'f{i}'], kde=True)\n    else:\n        sns.histplot(train['loss'], kde=True)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-04T02:33:35.96459Z","iopub.execute_input":"2021-08-04T02:33:35.965021Z","iopub.status.idle":"2021-08-04T02:37:10.355205Z","shell.execute_reply.started":"2021-08-04T02:33:35.964982Z","shell.execute_reply":"2021-08-04T02:37:10.354297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# correlation matrix with heat map\ncorr = train.corr()\nplt.figure(figsize=(20, 20))\nsns.heatmap(corr)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-04T02:37:10.356618Z","iopub.execute_input":"2021-08-04T02:37:10.35721Z","iopub.status.idle":"2021-08-04T02:37:19.39489Z","shell.execute_reply.started":"2021-08-04T02:37:10.357169Z","shell.execute_reply":"2021-08-04T02:37:19.394046Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# the ten highest correlated features for each feature\ncols = train.columns.tolist()\nfor col in cols:\n    print(col)\n    print(corr[col].sort_values(ascending=False)[1:11])\n    print('=======================')","metadata":{"execution":{"iopub.status.busy":"2021-08-04T02:37:19.396576Z","iopub.execute_input":"2021-08-04T02:37:19.397035Z","iopub.status.idle":"2021-08-04T02:37:19.586167Z","shell.execute_reply.started":"2021-08-04T02:37:19.397001Z","shell.execute_reply":"2021-08-04T02:37:19.584963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# extract X and y for training set\nX = train.drop('loss', axis=1).values\ny = train['loss'].values","metadata":{"execution":{"iopub.status.busy":"2021-08-04T02:37:19.587756Z","iopub.execute_input":"2021-08-04T02:37:19.588112Z","iopub.status.idle":"2021-08-04T02:37:19.733997Z","shell.execute_reply.started":"2021-08-04T02:37:19.58808Z","shell.execute_reply":"2021-08-04T02:37:19.732797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# LightGBM with Optimized Hyperparameters","metadata":{}},{"cell_type":"code","source":"# optimized hyperparameters\nparams = {\n        \"min_child_weight\": 638.7295413674256,\n        \"num_leaves\": 32,\n        \"reg_alpha\": 0.7635991288488166,\n        \"reg_lambda\": 93.08626337603258\n        }\n\n# construct the model\nmodel= lgbm.LGBMRegressor(\n                       **params,\n                       objective='rmse',\n                       metric='rmse',\n                       subsample=0.7,\n                       learning_rate=0.03,\n                       n_estimators=10000,\n                       n_jobs=-1\n                       )\n\n# construct KFold cross validation\nn_splits=5\nkf = model_selection.KFold(n_splits=n_splits)\n\n# initiate lists to save folds scores\nscores_train = []\nscores_valid = []\n\n# initiate zeros array for test data predictions\npreds_test_array = np.zeros((test.shape[0], ))\n\n# KFold cross validation \nfor fold, (train_idx, valid_idx) in enumerate(kf.split(X)):\n\n    print(f\"Fold {fold+1} -------------->\")\n    x_train, y_train = X[train_idx], y[train_idx]\n    x_valid, y_valid = X[valid_idx], y[valid_idx]\n\n    y_train_log = y_train\n    y_valid_log = y_valid\n    \n    # fit the model\n    model.fit(\n            x_train, y_train_log,\n            eval_set=[(x_valid,y_valid_log)],\n            verbose=100,\n            early_stopping_rounds=100\n            )\n\n    # clip the results so that the minimum and maximum values are 0 and 50, respectively\n    preds_train = np.clip(model.predict(x_train), 0, 50)\n    preds_valid = np.clip(model.predict(x_valid), 0, 50)\n    preds_test = np.clip(model.predict(test), 0, 50)\n    \n    # add the predictions of each fold to the array\n    preds_test_array += preds_test / n_splits\n    \n    # find both train and test rsme and observe if there is overfitting\n    score_train = np.sqrt(metrics.mean_squared_error(y_train, preds_train))\n    score_valid = np.sqrt(metrics.mean_squared_error(y_valid, preds_valid))\n    \n    # print the fold score\n    print(score_valid)\n    \n    # append the fold score\n    scores_train.append(score_train)\n    scores_valid.append(score_valid)\n\nprint('Mean train score =', np.mean(scores_train), 'STD train =', np.std(scores_train, ddof=1))\nprint('Mean valid score =', np.mean(scores_valid), 'STD valid =', np.std(scores_valid, ddof=1))\n\n# populate the submission dataframe\nsample.iloc[:, 1] = preds_test_array\nsample.to_csv('lgbm_base_model_submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-08-04T02:37:19.735417Z","iopub.execute_input":"2021-08-04T02:37:19.735745Z","iopub.status.idle":"2021-08-04T02:42:25.458386Z","shell.execute_reply.started":"2021-08-04T02:37:19.735713Z","shell.execute_reply":"2021-08-04T02:42:25.457148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Train and validation RMSE's are so close, hence the odds of overfitting is small.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}