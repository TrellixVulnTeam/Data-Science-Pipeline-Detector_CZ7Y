{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Data processing\n%matplotlib inline\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\n\n# Machine Learning\nimport optuna\nfrom catboost import CatBoostRegressor\nfrom optuna.samplers import TPESampler\nfrom sklearn.preprocessing import RobustScaler, StandardScaler, MinMaxScaler\nfrom sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, KFold\nfrom sklearn.metrics import mean_squared_error","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-12T16:50:21.955197Z","iopub.execute_input":"2021-08-12T16:50:21.955625Z","iopub.status.idle":"2021-08-12T16:50:23.608637Z","shell.execute_reply.started":"2021-08-12T16:50:21.955539Z","shell.execute_reply":"2021-08-12T16:50:23.607689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_dir = Path('../input/tabular-playground-series-aug-2021/')\ntrain_df = pd.read_csv(input_dir / 'train.csv')\ntest_df = pd.read_csv(input_dir / 'test.csv')\nsample_submission = pd.read_csv(input_dir / 'sample_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2021-08-12T16:50:26.626242Z","iopub.execute_input":"2021-08-12T16:50:26.626615Z","iopub.status.idle":"2021-08-12T16:50:37.215281Z","shell.execute_reply.started":"2021-08-12T16:50:26.626565Z","shell.execute_reply":"2021-08-12T16:50:37.214468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-12T16:50:51.635333Z","iopub.execute_input":"2021-08-12T16:50:51.635709Z","iopub.status.idle":"2021-08-12T16:50:51.675738Z","shell.execute_reply.started":"2021-08-12T16:50:51.635676Z","shell.execute_reply":"2021-08-12T16:50:51.674699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-12T16:50:53.185521Z","iopub.execute_input":"2021-08-12T16:50:53.185844Z","iopub.status.idle":"2021-08-12T16:50:53.215919Z","shell.execute_reply.started":"2021-08-12T16:50:53.185816Z","shell.execute_reply":"2021-08-12T16:50:53.21453Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-12T16:50:54.828035Z","iopub.execute_input":"2021-08-12T16:50:54.828356Z","iopub.status.idle":"2021-08-12T16:50:54.836299Z","shell.execute_reply.started":"2021-08-12T16:50:54.828325Z","shell.execute_reply":"2021-08-12T16:50:54.8355Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = train_df.drop(['id', 'loss'], axis=1).values\ny = train_df['loss'].values\nX_test = test_df.drop(['id'], axis=1).values","metadata":{"execution":{"iopub.status.busy":"2021-08-12T16:50:57.088952Z","iopub.execute_input":"2021-08-12T16:50:57.08926Z","iopub.status.idle":"2021-08-12T16:50:57.28878Z","shell.execute_reply.started":"2021-08-12T16:50:57.08923Z","shell.execute_reply":"2021-08-12T16:50:57.287951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# I've found many using MinMaxScaling but I've personally had better results with StandardScaling\nscaler = StandardScaler()\nX = scaler.fit_transform(X)\nX_test = scaler.fit_transform(X_test)","metadata":{"execution":{"iopub.status.busy":"2021-08-12T16:50:59.756979Z","iopub.execute_input":"2021-08-12T16:50:59.757288Z","iopub.status.idle":"2021-08-12T16:51:00.455027Z","shell.execute_reply.started":"2021-08-12T16:50:59.75726Z","shell.execute_reply":"2021-08-12T16:51:00.45416Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_min = y.min()\ny_max = y.max()\n\n# While it's probably rare that values will fall outside the y-min-max range, we should probably do it anyway.\ndef my_rmse(y_true, y_hat):\n    y_true[y_true < y_min] = y_min\n    y_true[y_true > y_max] = y_max\n    \n    y_true[y_hat < y_min] = y_min\n    y_true[y_hat > y_max] = y_max\n    \n    return mean_squared_error(y_true, y_hat, squared=False)","metadata":{"execution":{"iopub.status.busy":"2021-08-12T16:51:02.761779Z","iopub.execute_input":"2021-08-12T16:51:02.762081Z","iopub.status.idle":"2021-08-12T16:51:02.768198Z","shell.execute_reply.started":"2021-08-12T16:51:02.762054Z","shell.execute_reply":"2021-08-12T16:51:02.767031Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Some optimal parameters used from:\nhttps://www.kaggle.com/somayyehgholami/1-tps-aug-21-xgboost-catboost\n\nGo throw an upvote!","metadata":{}},{"cell_type":"code","source":"def objective(trial):\n    # Split the train data for each trial.\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, stratify=y, test_size=0.4)\n\n    param_grid = {\n        'depth': trial.suggest_int('depth', 6, 10), # Extremely prone to overfitting!\n        'iterations': trial.suggest_int('iterations', 400, 4000, 400), # Extremely prone to overfitting!\n        'learning_rate': trial.suggest_float('learning_rate', 0.02, 0.03), # Most important parameter - the learning rate!\n        'random_strength': trial.suggest_discrete_uniform('random_strength', 1.0, 2.0, 0.1),\n        'l2_leaf_reg': trial.suggest_int('l2_leaf_reg', 1, 50), # L2 regularization\n    } \n    \n    reg = CatBoostRegressor(\n        grow_policy='Depthwise',\n        leaf_estimation_method='Newton', \n        bootstrap_type='Bernoulli',\n        thread_count=4,\n        loss_function='RMSE',\n        eval_metric='RMSE',\n        od_type='Iter',\n        task_type='GPU',\n        verbose=False,\n        early_stopping_rounds=400,\n        **param_grid\n    )\n    \n    reg.fit(X_train, y_train, verbose=False)\n\n    return my_rmse(y_valid, reg.predict(X_valid))","metadata":{"execution":{"iopub.status.busy":"2021-08-12T16:55:01.730833Z","iopub.execute_input":"2021-08-12T16:55:01.731151Z","iopub.status.idle":"2021-08-12T16:55:01.739597Z","shell.execute_reply.started":"2021-08-12T16:55:01.731123Z","shell.execute_reply":"2021-08-12T16:55:01.73857Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_time = 1 * 10 * 60 # Train for up to ten minutes.\nstudy = optuna.create_study(direction='minimize', sampler=TPESampler(), study_name='CatBoost')\nstudy.optimize(objective, timeout=train_time)\n\nprint('Number of finished trials: ', len(study.trials))\nprint('Best trial:')\ntrial = study.best_trial\n\nprint('\\tValue: {}'.format(trial.value))\nprint('\\tParams: ')\nfor key, value in trial.params.items():\n    print('\\t\\t{}: {}'.format(key, value))","metadata":{"execution":{"iopub.status.busy":"2021-08-12T16:55:14.899956Z","iopub.execute_input":"2021-08-12T16:55:14.900372Z","iopub.status.idle":"2021-08-12T17:06:01.438174Z","shell.execute_reply.started":"2021-08-12T16:55:14.900332Z","shell.execute_reply":"2021-08-12T17:06:01.437321Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fetch the best trial parameters and set some settings for the KFold predictions.\ncatb_params = trial.params\ncatb_params['grow_policy'] = 'Depthwise'\ncatb_params['leaf_estimation_method'] = 'Newton'\ncatb_params['bootstrap_type'] = 'Bernoulli'\ncatb_params['thread_count'] = 4\ncatb_params['loss_function'] = 'RMSE'\ncatb_params['eval_metric'] = 'RMSE'\ncatb_params['od_type'] = 'Iter'\ncatb_params['task_type'] = 'GPU'\ncatb_params['early_stopping_rounds'] = 400\n\nn_splits = 10\ntest_preds = None\nkf_rmse = []\n\nfor fold, (train_idx, valid_idx) in enumerate(KFold(n_splits=n_splits, shuffle=True).split(X, y)):\n    # Fetch the train-validation indices.\n    X_train, y_train = X[train_idx], y[train_idx]\n    X_valid, y_valid = X[valid_idx], y[valid_idx]\n    \n    # Create and fit a new model using the best parameters.\n    model = CatBoostRegressor(**catb_params)\n    model.fit(X_train, y_train, verbose=False)\n    \n    # Validation predictions.\n    valid_pred = model.predict(X_valid)\n    rmse = my_rmse(y_valid, valid_pred)\n    print(f'Fold {fold+1}/{n_splits} RMSE: {rmse:.4f}')\n    kf_rmse.append(rmse)\n    \n    # Use the model trained for 1/n_splits of the output predictions.\n    if test_preds is None:\n        test_preds = model.predict(X_test)\n    else:\n        # This is kind of naughty for numerical accuracy (may overflow on other problems) but slightly quicker.\n        test_preds += model.predict(X_test)\n\ntest_preds /= n_splits\nprint(f'Average KFold RMSE: {np.mean(np.array(kf_rmse)):.5f}')","metadata":{"execution":{"iopub.status.busy":"2021-08-12T17:30:15.329485Z","iopub.execute_input":"2021-08-12T17:30:15.329801Z","iopub.status.idle":"2021-08-12T17:41:18.335254Z","shell.execute_reply.started":"2021-08-12T17:30:15.329773Z","shell.execute_reply":"2021-08-12T17:41:18.334176Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_preds[test_preds < y_min] = y_min\ntest_preds[test_preds > y_max] = y_max\nsample_submission['loss'] = test_preds\nsample_submission.to_csv('submission.csv', index=False)\nsample_submission","metadata":{"execution":{"iopub.status.busy":"2021-08-12T17:41:40.043172Z","iopub.execute_input":"2021-08-12T17:41:40.043536Z","iopub.status.idle":"2021-08-12T17:41:40.526358Z","shell.execute_reply.started":"2021-08-12T17:41:40.043502Z","shell.execute_reply":"2021-08-12T17:41:40.525292Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}