{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport random\nfrom catboost import CatBoostRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nimport optuna\n\n# Pandas setting to display more dataset rows and columns\npd.set_option('display.max_rows', 150)\npd.set_option('display.max_columns', 500)\npd.set_option('display.max_colwidth', None)\npd.set_option('display.float_format', lambda x: '%.5f' % x)\n\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-02T15:24:37.346024Z","iopub.execute_input":"2021-08-02T15:24:37.346433Z","iopub.status.idle":"2021-08-02T15:24:39.182751Z","shell.execute_reply.started":"2021-08-02T15:24:37.346346Z","shell.execute_reply":"2021-08-02T15:24:39.181199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Data import**","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/tabular-playground-series-aug-2021/train.csv\", low_memory=False)#, nrows=10000)\n# train[\"date_time\"] = pd.to_datetime(train[\"date_time\"], format=\"%Y-%m-%d %H:%M:%S\")\ntest = pd.read_csv(\"/kaggle/input/tabular-playground-series-aug-2021/test.csv\", low_memory=False)\n# test[\"date_time\"] = pd.to_datetime(test[\"date_time\"], format=\"%Y-%m-%d %H:%M:%S\")\ntrain.info(memory_usage=\"deep\")","metadata":{"execution":{"iopub.status.busy":"2021-08-02T15:24:39.184838Z","iopub.execute_input":"2021-08-02T15:24:39.185177Z","iopub.status.idle":"2021-08-02T15:24:54.427623Z","shell.execute_reply.started":"2021-08-02T15:24:39.18514Z","shell.execute_reply":"2021-08-02T15:24:54.42596Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.info(memory_usage=\"deep\")","metadata":{"execution":{"iopub.status.busy":"2021-08-02T15:24:54.42934Z","iopub.execute_input":"2021-08-02T15:24:54.429717Z","iopub.status.idle":"2021-08-02T15:24:54.445094Z","shell.execute_reply.started":"2021-08-02T15:24:54.429681Z","shell.execute_reply":"2021-08-02T15:24:54.443999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head(10)","metadata":{"execution":{"iopub.status.busy":"2021-08-02T15:24:54.446635Z","iopub.execute_input":"2021-08-02T15:24:54.447262Z","iopub.status.idle":"2021-08-02T15:24:54.522335Z","shell.execute_reply.started":"2021-08-02T15:24:54.447218Z","shell.execute_reply":"2021-08-02T15:24:54.521585Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **EDA**","metadata":{}},{"cell_type":"code","source":"# Colors to be used for plots\ncolors = [\"lightcoral\", \"sandybrown\", \"darkorange\", \"mediumseagreen\",\n          \"lightseagreen\", \"cornflowerblue\", \"mediumpurple\", \"palevioletred\",\n          \"lightskyblue\", \"sandybrown\", \"yellowgreen\", \"indianred\",\n          \"lightsteelblue\", \"mediumorchid\", \"deepskyblue\"]","metadata":{"execution":{"iopub.status.busy":"2021-08-02T15:24:54.523598Z","iopub.execute_input":"2021-08-02T15:24:54.524152Z","iopub.status.idle":"2021-08-02T15:24:54.529032Z","shell.execute_reply.started":"2021-08-02T15:24:54.524112Z","shell.execute_reply":"2021-08-02T15:24:54.527853Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(5, 5))\npie = ax.pie([len(train), len(test)],\n             labels=[\"Train dataset\", \"Test dataset\"],\n             colors=[\"salmon\", \"teal\"],\n             textprops={\"fontsize\": 15},\n             autopct='%1.1f%%')\nax.axis(\"equal\")\nax.set_title(\"Dataset length comparison\", fontsize=18)\nfig.set_facecolor('white')\nplt.show();","metadata":{"execution":{"iopub.status.busy":"2021-08-02T15:24:54.530387Z","iopub.execute_input":"2021-08-02T15:24:54.530826Z","iopub.status.idle":"2021-08-02T15:24:54.661878Z","shell.execute_reply.started":"2021-08-02T15:24:54.530778Z","shell.execute_reply":"2021-08-02T15:24:54.661094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.describe().T","metadata":{"execution":{"iopub.status.busy":"2021-08-02T15:24:54.663052Z","iopub.execute_input":"2021-08-02T15:24:54.663369Z","iopub.status.idle":"2021-08-02T15:24:55.702409Z","shell.execute_reply.started":"2021-08-02T15:24:54.663337Z","shell.execute_reply":"2021-08-02T15:24:55.701592Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.isna().sum().sum(), test.isna().sum().sum()","metadata":{"execution":{"iopub.status.busy":"2021-08-02T15:24:55.705545Z","iopub.execute_input":"2021-08-02T15:24:55.705861Z","iopub.status.idle":"2021-08-02T15:24:55.792624Z","shell.execute_reply.started":"2021-08-02T15:24:55.70581Z","shell.execute_reply":"2021-08-02T15:24:55.791491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are no missing value in the both datasets.\n\nLets check target distribution.","metadata":{}},{"cell_type":"code","source":"train[\"loss\"].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-08-02T15:24:55.795124Z","iopub.execute_input":"2021-08-02T15:24:55.795524Z","iopub.status.idle":"2021-08-02T15:24:55.808297Z","shell.execute_reply.started":"2021-08-02T15:24:55.795483Z","shell.execute_reply":"2021-08-02T15:24:55.807421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(16, 8))\n\nbars = ax.bar(train[\"loss\"].value_counts().sort_index().index,\n              train[\"loss\"].value_counts().sort_index().values,\n              color=colors,\n              edgecolor=\"black\")\nax.set_title(\"Loss (target) distribution\", fontsize=20, pad=15)\nax.set_ylabel(\"Amount of values\", fontsize=14, labelpad=15)\nax.set_xlabel(\"Loss (target) value\", fontsize=14, labelpad=10)\nax.bar_label(bars, [f\"{x:2.2f}%\" for x in train[\"loss\"].value_counts().sort_index().values/(len(train)/100)],\n                 padding=5, fontsize=10, rotation=90)\nax.margins(0.025, 0.12)\nax.grid(axis=\"y\")\n\nplt.show();","metadata":{"execution":{"iopub.status.busy":"2021-08-02T15:24:55.809651Z","iopub.execute_input":"2021-08-02T15:24:55.810167Z","iopub.status.idle":"2021-08-02T15:24:56.341405Z","shell.execute_reply.started":"2021-08-02T15:24:55.810125Z","shell.execute_reply":"2021-08-02T15:24:56.340591Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lets check feature values distribution in the both datasets.","metadata":{}},{"cell_type":"code","source":"df = pd.concat([train.drop([\"id\", \"loss\"], axis=1), test.drop(\"id\", axis=1)], axis=0)\ncolumns = df.columns.values\n\ncols = 3\nrows = len(columns) // cols + 1\n\nfig, axs = plt.subplots(ncols=cols, nrows=rows, figsize=(16,100), sharex=False)\n\nplt.subplots_adjust(hspace = 0.3)\ni=0\n\nfor r in np.arange(0, rows, 1):\n    for c in np.arange(0, cols, 1):\n        if i >= len(columns):\n            axs[r, c].set_visible(False)\n        else:\n            hist1 = axs[r, c].hist(train[columns[i]].values,\n                                   range=(df[columns[i]].min(),\n                                          df[columns[i]].max()),\n                                   bins=40,\n                                   color=\"deepskyblue\",\n                                   edgecolor=\"black\",\n                                   alpha=0.7,\n                                   label=\"Train Dataset\")\n            hist2 = axs[r, c].hist(test[columns[i]].values,\n                                   range=(df[columns[i]].min(),\n                                          df[columns[i]].max()),\n                                   bins=40,\n                                   color=\"palevioletred\",\n                                   edgecolor=\"black\",\n                                   alpha=0.7,\n                                   label=\"Test Dataset\")\n            axs[r, c].set_title(columns[i], fontsize=14, pad=5)\n            axs[r, c].tick_params(axis=\"y\", labelsize=13)\n            axs[r, c].tick_params(axis=\"x\", labelsize=13)\n            axs[r, c].grid(axis=\"y\")\n            axs[r, c].legend(fontsize=13)\n                                  \n        i+=1\n#plt.suptitle(\"Feature values distribution in both datasets\", y=0.99)\nplt.show();","metadata":{"execution":{"iopub.status.busy":"2021-08-02T15:24:56.342652Z","iopub.execute_input":"2021-08-02T15:24:56.343032Z","iopub.status.idle":"2021-08-02T15:25:29.521626Z","shell.execute_reply.started":"2021-08-02T15:24:56.342995Z","shell.execute_reply":"2021-08-02T15:25:29.520851Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The datasets are pretty well balanced.","metadata":{}},{"cell_type":"code","source":"train.nunique().sort_values().head()","metadata":{"execution":{"iopub.status.busy":"2021-08-02T15:25:29.52268Z","iopub.execute_input":"2021-08-02T15:25:29.523009Z","iopub.status.idle":"2021-08-02T15:25:30.990526Z","shell.execute_reply.started":"2021-08-02T15:25:29.522977Z","shell.execute_reply":"2021-08-02T15:25:30.989484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As you can see, f1 feature has the smallest amount of unique values - 289. So I don't think any feature should be treated as categorical.\n\nLets look at feature correlation.","metadata":{}},{"cell_type":"code","source":"# Plot dataframe\ndf = train.drop(\"id\", axis=1).corr().round(5)\n\n# Mask to hide upper-right part of plot as it is a duplicate\nmask = np.zeros_like(df)\nmask[np.triu_indices_from(mask)] = True\n\n# Making a plot\nplt.figure(figsize=(16,16))\nax = sns.heatmap(df, annot=False, mask=mask, cmap=\"RdBu\", annot_kws={\"weight\": \"bold\", \"fontsize\":13})\nax.set_title(\"Feature correlation heatmap\", fontsize=17)\nplt.setp(ax.get_xticklabels(), rotation=90, ha=\"right\",\n         rotation_mode=\"anchor\", weight=\"normal\")\nplt.setp(ax.get_yticklabels(), weight=\"normal\",\n         rotation_mode=\"anchor\", rotation=0, ha=\"right\")\nplt.show();","metadata":{"execution":{"iopub.status.busy":"2021-08-02T15:25:30.992107Z","iopub.execute_input":"2021-08-02T15:25:30.992483Z","iopub.status.idle":"2021-08-02T15:25:38.514645Z","shell.execute_reply.started":"2021-08-02T15:25:30.992446Z","shell.execute_reply":"2021-08-02T15:25:38.513799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As you can see, the correlation is between ~0.03 and ~0.03 which is pretty small. So the features are weakly correlated. \n\nThere are some features with relatively low correlation with target value even comparing with other features:","metadata":{}},{"cell_type":"code","source":"df[(df[\"loss\"]>-0.001) & (df[\"loss\"]<0.001)][\"loss\"]","metadata":{"execution":{"iopub.status.busy":"2021-08-02T15:25:38.515854Z","iopub.execute_input":"2021-08-02T15:25:38.516316Z","iopub.status.idle":"2021-08-02T15:25:38.537704Z","shell.execute_reply.started":"2021-08-02T15:25:38.516269Z","shell.execute_reply":"2021-08-02T15:25:38.536742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lets visualize each feature vs loss.","metadata":{}},{"cell_type":"code","source":"columns = train.drop([\"id\", \"loss\"], axis=1).columns.values\n\ncols = 4\nrows = len(columns) // cols + 1\n\nfig, axs = plt.subplots(ncols=cols, nrows=rows, figsize=(16,100), sharex=False)\n\nplt.subplots_adjust(hspace = 0.3)\ni=0\n\nfor r in np.arange(0, rows, 1):\n    for c in np.arange(0, cols, 1):\n        if i >= len(columns):\n            axs[r, c].set_visible(False)\n        else:\n            scatter = axs[r, c].scatter(train[columns[i]].values,\n                                        train[\"loss\"],\n                                        color=random.choice(colors))\n            axs[r, c].set_title(columns[i], fontsize=14, pad=5)\n            axs[r, c].tick_params(axis=\"y\", labelsize=11)\n            axs[r, c].tick_params(axis=\"x\", labelsize=11)\n                                  \n        i+=1\n#plt.suptitle(\"Features vs loss\", y=0.99)\nplt.show();","metadata":{"execution":{"iopub.status.busy":"2021-08-02T15:25:38.539049Z","iopub.execute_input":"2021-08-02T15:25:38.539373Z","iopub.status.idle":"2021-08-02T15:26:28.619368Z","shell.execute_reply.started":"2021-08-02T15:25:38.539341Z","shell.execute_reply":"2021-08-02T15:26:28.618451Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Data preparation**","metadata":{}},{"cell_type":"code","source":"# Calculating edges of target bins to be used for stratified split\ntarget_bin_edges = np.histogram_bin_edges(train[\"loss\"], bins=10)\ntarget_bin_edges[0] = -np.inf\ntarget_bin_edges[-1] = np.inf\ntarget_bins = pd.cut(train[\"loss\"], target_bin_edges, labels=np.arange(10))\ntarget_bins.value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-08-02T15:26:28.620782Z","iopub.execute_input":"2021-08-02T15:26:28.621142Z","iopub.status.idle":"2021-08-02T15:26:28.645748Z","shell.execute_reply.started":"2021-08-02T15:26:28.621107Z","shell.execute_reply":"2021-08-02T15:26:28.644864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Scaling data to [0, 1] range\n# x_scaler = MinMaxScaler()\n# X = pd.DataFrame(x_scaler.fit_transform(train.drop([\"id\", \"loss\"], axis=1)), columns=train.drop([\"id\", \"loss\"], axis=1).columns)\n# X_test = pd.DataFrame(x_scaler.transform(test.drop(\"id\", axis=1)), columns=test.drop([\"id\"], axis=1).columns)\n# y_scaler = MinMaxScaler()\n# y = pd.Series(y_scaler.fit_transform(np.array(train[\"loss\"].copy()).reshape(-1,1)).flatten())\n# # y = train[\"loss\"].copy()","metadata":{"execution":{"iopub.status.busy":"2021-08-02T15:26:28.647136Z","iopub.execute_input":"2021-08-02T15:26:28.647477Z","iopub.status.idle":"2021-08-02T15:26:28.651321Z","shell.execute_reply.started":"2021-08-02T15:26:28.647443Z","shell.execute_reply":"2021-08-02T15:26:28.650194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Scaling data\nx_scaler = StandardScaler()\nX = pd.DataFrame(x_scaler.fit_transform(train.drop([\"id\", \"loss\"], axis=1)), columns=train.drop([\"id\", \"loss\"], axis=1).columns)\nX_test = pd.DataFrame(x_scaler.transform(test.drop(\"id\", axis=1)), columns=test.drop([\"id\"], axis=1).columns)\n\ny = train[\"loss\"].copy()","metadata":{"execution":{"iopub.status.busy":"2021-08-02T15:26:28.653145Z","iopub.execute_input":"2021-08-02T15:26:28.653489Z","iopub.status.idle":"2021-08-02T15:26:29.567538Z","shell.execute_reply.started":"2021-08-02T15:26:28.653455Z","shell.execute_reply":"2021-08-02T15:26:29.566685Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def add_new_features(df):\n#     \"\"\"\n#     Adds custom features to a given dataframe\n#     \"\"\"\n#     df_copy = df.copy()\n#     df[\"custom_feat_0\"] = df_copy.sum(axis=1)\n#     df[\"custom_feat_1\"] = df_copy.mean(axis=1)  \n#     df[\"custom_feat_2\"] = df_copy.min(axis=1)\n#     df[\"custom_feat_3\"] = df_copy.max(axis=1)\n#     df[\"custom_feat_4\"] = df_copy.median(axis=1)\n\n# #     df[\"custom_feat_5\"] = (df_copy > 0).astype(int).sum(axis=1)\n# #     df[\"custom_feat_6\"] = (df_copy < 0).astype(int).sum(axis=1)\n# #     df[\"custom_feat_7\"] = df_copy.kurt(axis=1)\n# #     df[\"custom_feat_8\"] = df_copy.mad(axis=1)\n# #     df[\"custom_feat_9\"] = df_copy.var(axis=1)\n# #     df[\"custom_feat_10\"] = df_copy.std(axis=1)\n# #     df[\"custom_feat_11\"] = df_copy.skew(axis=1)\n# #     df[\"custom_feat_12\"] = df_copy.abs().sum(axis=1)\n    \n#     return df","metadata":{"execution":{"iopub.status.busy":"2021-08-02T15:26:29.568918Z","iopub.execute_input":"2021-08-02T15:26:29.569267Z","iopub.status.idle":"2021-08-02T15:26:29.574515Z","shell.execute_reply.started":"2021-08-02T15:26:29.569227Z","shell.execute_reply":"2021-08-02T15:26:29.573597Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Adding new features to the datasets\n# X = add_new_features(X.copy())\n# X_test = add_new_features(X_test.copy())","metadata":{"execution":{"iopub.status.busy":"2021-08-02T15:26:29.57613Z","iopub.execute_input":"2021-08-02T15:26:29.576554Z","iopub.status.idle":"2021-08-02T15:26:29.583366Z","shell.execute_reply.started":"2021-08-02T15:26:29.576518Z","shell.execute_reply":"2021-08-02T15:26:29.582592Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X.describe()","metadata":{"execution":{"iopub.status.busy":"2021-08-02T15:26:29.584741Z","iopub.execute_input":"2021-08-02T15:26:29.585082Z","iopub.status.idle":"2021-08-02T15:26:30.71727Z","shell.execute_reply.started":"2021-08-02T15:26:29.585057Z","shell.execute_reply":"2021-08-02T15:26:30.716511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test.describe()","metadata":{"execution":{"iopub.status.busy":"2021-08-02T15:26:30.718384Z","iopub.execute_input":"2021-08-02T15:26:30.718691Z","iopub.status.idle":"2021-08-02T15:26:31.405667Z","shell.execute_reply.started":"2021-08-02T15:26:30.718665Z","shell.execute_reply":"2021-08-02T15:26:31.404701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y.min(), y.max()","metadata":{"execution":{"iopub.status.busy":"2021-08-02T15:26:31.407064Z","iopub.execute_input":"2021-08-02T15:26:31.407402Z","iopub.status.idle":"2021-08-02T15:26:31.414431Z","shell.execute_reply.started":"2021-08-02T15:26:31.407369Z","shell.execute_reply":"2021-08-02T15:26:31.413547Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Hyperparameters optimization**","metadata":{}},{"cell_type":"code","source":"def train_model_optuna(trial, X_train, X_valid, y_train, y_valid):\n    \"\"\"\n    A function to train a model using different hyperparamerters combinations provided by Optuna. \n    Loss of validation data predictions is returned to estimate hyperparameters effectiveness.\n    \"\"\"\n    preds = 0\n    \n        \n    #A set of hyperparameters to optimize by optuna\n   \n    cb_params = {\n             \"iterations\": trial.suggest_categorical('iterations', [10000]),\n             \"learning_rate\": trial.suggest_float('learning_rate', 0.01, 1.0),\n             \"loss_function\": 'RMSE',\n             \"eval_metric\": 'RMSE',\n             \"l2_leaf_reg\": trial.suggest_float('l2_leaf_reg', 0.00001, 10),\n             \"bagging_temperature\": trial.suggest_float('bagging_temperature', 0.0, 10.0),\n             \"random_strength\": trial.suggest_float('random_strength', 1.0, 2.0),\n             \"depth\": trial.suggest_int('depth', 1, 16),\n             \"grow_policy\": trial.suggest_categorical(\"grow_policy\", [\"SymmetricTree\", \"Depthwise\", \"Lossguide\"]),\n             \"leaf_estimation_method\": trial.suggest_categorical(\"leaf_estimation_method\", [\"Newton\", \"Gradient\"]),#, \"Exact\"]),\n             \"od_type\": \"Iter\",\n             \"early_stopping_rounds\": 100,\n             \"border_count\": 254,\n             \"use_best_model\": True,\n\n# #                  \"max_leaves\": trial.suggest_int('max_leaves', 1, 64),\n# #                  \"task_type\": \"GPU\",\n                }\n\n\n    model = CatBoostRegressor(**cb_params, random_state=42, thread_count=4)\n    model.fit(\n                X_train, y_train,\n                eval_set=(X_valid, y_valid),\n                verbose=False,\n            )\n    print(f\"Trees: {model.tree_count_}\")\n    oof = model.predict(X_valid)\n    oof[oof<0] = 0\n    \n    return np.sqrt(mean_squared_error(y_valid, oof))","metadata":{"execution":{"iopub.status.busy":"2021-08-02T15:26:31.415891Z","iopub.execute_input":"2021-08-02T15:26:31.416246Z","iopub.status.idle":"2021-08-02T15:26:31.427256Z","shell.execute_reply.started":"2021-08-02T15:26:31.416211Z","shell.execute_reply":"2021-08-02T15:26:31.42621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The code below is commented in order to save runtime.","metadata":{}},{"cell_type":"code","source":"# %%time\n# # Splitting data into train and valid folds using target bins for stratification\n# split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n# for train_idx, valid_idx in split.split(X, target_bins):\n#     X_train, X_valid = X.loc[train_idx], X.loc[valid_idx]\n#     y_train, y_valid = y.loc[train_idx], y.loc[valid_idx]\n# # Setting optuna verbosity to show only warning messages\n# # If the line is uncommeted each iteration results will be shown\n# # optuna.logging.set_verbosity(optuna.logging.WARNING)\n\n# study = optuna.create_study(direction='minimize')\n# study.optimize(lambda trial: train_model_optuna(trial, X_train, X_valid,\n#                                                     y_train, y_valid),\n#                n_trials = 100)\n\n# # Showing optimization results\n# print('Number of finished trials:', len(study.trials))\n# print('Best trial parameters:', study.best_trial.params)\n# print('Best score:', study.best_value)","metadata":{"execution":{"iopub.status.busy":"2021-08-02T15:26:31.428651Z","iopub.execute_input":"2021-08-02T15:26:31.429037Z","iopub.status.idle":"2021-08-02T15:26:31.438977Z","shell.execute_reply.started":"2021-08-02T15:26:31.429001Z","shell.execute_reply":"2021-08-02T15:26:31.438189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Model training**","metadata":{}},{"cell_type":"code","source":"# Hyperparameters optimized by Optuna\n\n\ncb_params = {'iterations': 10000,\n             'learning_rate': 0.00929418593306697,\n             'l2_leaf_reg': 7.1557353016594405,\n             'bagging_temperature': 9.924411556321033,\n             'random_strength': 1.5202844820405794,\n             'depth': 7,\n             'grow_policy': 'Depthwise',\n             'leaf_estimation_method': 'Newton'}\n# cb_params = {'iterations': 10000,\n#              'learning_rate': 0.010526847803225213,\n#              'l2_leaf_reg': 7.578784014838337,\n#              'bagging_temperature': 0.5813008056988401,\n#              'random_strength': 1.7705601193056997,\n#              'depth': 7,\n#              'grow_policy': 'Depthwise',\n#              'leaf_estimation_method': 'Gradient'}\n# cb_params = {'iterations': 10000,\n#              'learning_rate': 0.030108080370377578,\n#              'l2_leaf_reg': 4.606620127979116,\n#              'bagging_temperature': 7.518949583881732,\n#              'random_strength': 1.060436568484918,\n#              'depth': 2,\n#              'grow_policy': 'Depthwise',\n#              'leaf_estimation_method': 'Gradient'}","metadata":{"execution":{"iopub.status.busy":"2021-08-02T15:26:31.443482Z","iopub.execute_input":"2021-08-02T15:26:31.443751Z","iopub.status.idle":"2021-08-02T15:26:31.451016Z","shell.execute_reply.started":"2021-08-02T15:26:31.443728Z","shell.execute_reply":"2021-08-02T15:26:31.450269Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nsplits = 10\nskf = StratifiedKFold(n_splits=splits, shuffle=True, random_state=42)\noof_preds = np.zeros((X.shape[0],))\npreds = 0\nmodel_fi = 0\ntotal_mean_rmse = 0\n\nfor num, (train_idx, valid_idx) in enumerate(skf.split(X, target_bins)):\n    X_train, X_valid = X.loc[train_idx], X.loc[valid_idx]\n    y_train, y_valid = y.loc[train_idx], y.loc[valid_idx]\n    model = CatBoostRegressor(random_state=42,\n                             thread_count=4,\n                             verbose=False,\n                             loss_function='RMSE',\n                             eval_metric='RMSE',\n                             od_type=\"Iter\",\n                             early_stopping_rounds=500,\n                             use_best_model=True,\n                             **cb_params)\n    model.fit(X_train, y_train,\n              eval_set=(X_valid, y_valid),\n              verbose=False)\n    preds += model.predict(X_test) / splits\n    model_fi += model.feature_importances_\n    oof_preds[valid_idx] = model.predict(X_valid)\n    oof_preds[oof_preds < 0] = 0\n#     fold_rmse = np.sqrt(mean_squared_error(y_scaler.inverse_transform(np.array(y_valid).reshape(-1,1)), y_scaler.inverse_transform(np.array(oof_preds[valid_idx]).reshape(-1,1))))\n    fold_rmse = np.sqrt(mean_squared_error(y_valid, oof_preds[valid_idx]))\n    print(f\"Fold {num} RMSE: {fold_rmse}\")\n#         print(f\"Trees: {model.tree_count_}\")\n    total_mean_rmse += fold_rmse / splits\nprint(f\"\\nOverall RMSE: {total_mean_rmse}\")    ","metadata":{"execution":{"iopub.status.busy":"2021-08-02T15:27:35.777302Z","iopub.execute_input":"2021-08-02T15:27:35.777674Z","iopub.status.idle":"2021-08-02T15:41:49.871832Z","shell.execute_reply.started":"2021-08-02T15:27:35.777642Z","shell.execute_reply":"2021-08-02T15:41:49.870934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Feature importances**","metadata":{}},{"cell_type":"code","source":"df = pd.DataFrame(columns=[\"Feature\", \"Importance\"])\ndf[\"Feature\"] = X.columns\ndf[\"Importance\"] = model_fi / model_fi.sum()\ndf.sort_values(\"Importance\", axis=0, ascending=False, inplace=True)\n\nx = np.arange(0, len(df[\"Feature\"]))\nheight = 0.4\n\nfig, ax = plt.subplots(figsize=(16, 30))\nbars1 = ax.barh(x, df[\"Importance\"], height=height,\n                color=\"mediumorchid\", edgecolor=\"black\")\nax.set_title(\"Feature importances\", fontsize=30, pad=15)\nax.set_ylabel(\"Feature names\", fontsize=20, labelpad=15)\nax.set_xlabel(\"Feature importance\", fontsize=20, labelpad=15)\nax.set_yticks(x)\nax.set_yticklabels(df[\"Feature\"], fontsize=15)\nax.tick_params(axis=\"x\", labelsize=15)\nax.grid(axis=\"x\")\nax2 = ax.secondary_xaxis('top')\nax2.set_xlabel(\"Feature importance\", fontsize=20, labelpad=15)\nax2.tick_params(axis=\"x\", labelsize=15)\nplt.margins(0.04, 0.01)\nplt.gca().invert_yaxis()","metadata":{"execution":{"iopub.status.busy":"2021-08-02T15:26:31.473923Z","iopub.status.idle":"2021-08-02T15:26:31.47462Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Submission**","metadata":{}},{"cell_type":"code","source":"predictions = pd.DataFrame()\npredictions[\"id\"] = test[\"id\"]\npredictions[\"loss\"] = preds\n\npredictions.to_csv('submission.csv', index=False, header=predictions.columns)\npredictions.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-02T15:26:31.47568Z","iopub.status.idle":"2021-08-02T15:26:31.476409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}