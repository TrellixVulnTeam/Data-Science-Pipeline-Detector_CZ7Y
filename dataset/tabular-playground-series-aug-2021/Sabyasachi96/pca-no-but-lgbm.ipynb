{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This notebook is a continuation of TPS-Aug-Random Forest(https://www.kaggle.com/sabyasachi96/tps-aug-random-forest). \n* Recap: Well in the last notebook, the results were not that promising and also the runtime of Random Forest was too long. \n    * Error = 6.1751\n    * Run time = 170 min\n* My thoughts were, since the dataset is too huge a PCA can help improve the Random Forest predictions, but things didn't turn out the way i wanted to. \n","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport seaborn as sns\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport lightgbm as lgb","metadata":{"execution":{"iopub.status.busy":"2021-08-18T06:42:11.056347Z","iopub.execute_input":"2021-08-18T06:42:11.056812Z","iopub.status.idle":"2021-08-18T06:42:12.079627Z","shell.execute_reply.started":"2021-08-18T06:42:11.056758Z","shell.execute_reply":"2021-08-18T06:42:12.078464Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv(\"../input/tabular-playground-series-aug-2021/train.csv\")\ntest = pd.read_csv(\"../input/tabular-playground-series-aug-2021/test.csv\")\nsubmission = pd.read_csv(\"../input/tabular-playground-series-aug-2021/sample_submission.csv\")\n\n# I'll remove the ID column of both train and test, cause they are of no use here\ntrain.drop(columns = 'id', inplace = True)\ntest.drop(columns = 'id', inplace =True)\n\n# Check\nprint('train columns', train.columns)\nprint('test columns', test.columns)","metadata":{"execution":{"iopub.status.busy":"2021-08-18T05:35:46.0753Z","iopub.execute_input":"2021-08-18T05:35:46.075786Z","iopub.status.idle":"2021-08-18T05:35:52.67415Z","shell.execute_reply.started":"2021-08-18T05:35:46.075755Z","shell.execute_reply":"2021-08-18T05:35:52.673186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inference From the last notebook\n\nWell the features were too less correlated, both positively and negatively with each other and the target.\n* conducted Random forest with error 6.17 but long runtime","metadata":{}},{"cell_type":"markdown","source":"# Approach PCA","metadata":{}},{"cell_type":"markdown","source":"With the help of PCA i'll reduce the dimensions and then check their correlations before modelling and predicting","metadata":{}},{"cell_type":"code","source":"# Splitting\nfrom sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split( train.iloc[: ,:-1], train.iloc[:,-1], \n                                                    test_size=0.2, random_state=2)\nprint(\"X-train shape\", x_train.shape, \"\\nY-test shape\" ,y_test.shape)\n\n# Standard scaling\nfrom sklearn.preprocessing import StandardScaler\n\nss = StandardScaler()\nx_train = pd.DataFrame(ss.fit_transform(x_train))\nx_test = pd.DataFrame(ss.fit_transform(x_test))","metadata":{"execution":{"iopub.status.busy":"2021-08-18T05:38:47.53223Z","iopub.execute_input":"2021-08-18T05:38:47.532758Z","iopub.status.idle":"2021-08-18T05:38:48.485192Z","shell.execute_reply.started":"2021-08-18T05:38:47.532713Z","shell.execute_reply":"2021-08-18T05:38:48.484245Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# PCA\nfrom sklearn.decomposition import PCA\npca = PCA()\npca_train = pca.fit_transform(x_train)","metadata":{"execution":{"iopub.status.busy":"2021-08-18T05:40:39.723453Z","iopub.execute_input":"2021-08-18T05:40:39.723822Z","iopub.status.idle":"2021-08-18T05:40:40.897239Z","shell.execute_reply.started":"2021-08-18T05:40:39.723789Z","shell.execute_reply":"2021-08-18T05:40:40.896222Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now according to literature, i'll get maximum variance explained by first few Principle components. Thus haven't put any cap on the number of PC's. I'll know how many PC's needed by plotting cumulative variance ","metadata":{}},{"cell_type":"code","source":"explained_variance = pca.explained_variance_ratio_","metadata":{"execution":{"iopub.status.busy":"2021-08-18T06:17:41.30326Z","iopub.execute_input":"2021-08-18T06:17:41.303591Z","iopub.status.idle":"2021-08-18T06:17:41.307933Z","shell.execute_reply.started":"2021-08-18T06:17:41.303562Z","shell.execute_reply":"2021-08-18T06:17:41.306863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(explained_variance)\nplt.xlabel('Number of components')\nplt.ylabel('Explained variance')\nplt.title(\"Scree Plot\");","metadata":{"execution":{"iopub.status.busy":"2021-08-18T06:18:05.750949Z","iopub.execute_input":"2021-08-18T06:18:05.751279Z","iopub.status.idle":"2021-08-18T06:18:05.903939Z","shell.execute_reply.started":"2021-08-18T06:18:05.751252Z","shell.execute_reply":"2021-08-18T06:18:05.90295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(np.cumsum(explained_variance))\nplt.xlabel('Number of components')\nplt.ylabel('Cumulative explained variance')\nplt.title(\"Counting PC's\");","metadata":{"execution":{"iopub.status.busy":"2021-08-18T06:18:10.646509Z","iopub.execute_input":"2021-08-18T06:18:10.646851Z","iopub.status.idle":"2021-08-18T06:18:10.785509Z","shell.execute_reply.started":"2021-08-18T06:18:10.646822Z","shell.execute_reply":"2021-08-18T06:18:10.784567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Inference\nWell the above images are disheartening, i was expecting plots that could help me getting the exact number of components. But the data lacked collinearity to a great extent and thus PCA will not be that helpful i am assuming. \n* Still I performed the Random forest on colab with the PCA data, I am not  running the code here, since it will a long time. I'll add the error value and the run time. ","metadata":{}},{"cell_type":"code","source":"\"\"\"\n# Import the model we are using\nfrom sklearn.ensemble import RandomForestRegressor\n\n# Initantiate model with 500 decision trees\nrf = RandomForestRegressor(n_estimators=500, max_depth = 20, \n                           min_samples_split = 50, min_samples_leaf = 10,\n                           verbose = 1)\n\n# Train the model on training data\nrf.fit(pca_train, y_train)\n# Use the forest's predict method on the test data\ny_pred = rf.predict(pca_test)\n# Calculate the absolute errors\nerrors = abs(y_pred - y_test)\n# Print out the mean absolute error (mae)\nprint('Base line Mean Absolute Error', round(np.mean(errors), 4), 'degrees.')\n\n\"\"\"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Error ~ 6.245;  Runtime - 122 min","metadata":{}},{"cell_type":"markdown","source":"# Model With Lgbm","metadata":{}},{"cell_type":"code","source":"# Splitting\nfrom sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split( train.iloc[: ,:-1], train.iloc[:,-1], \n                                                    test_size=0.2, random_state=2)\nprint(\"X-train shape\", x_train.shape, \"\\nY-test shape\" ,y_test.shape)","metadata":{"execution":{"iopub.status.busy":"2021-08-18T06:39:48.480258Z","iopub.execute_input":"2021-08-18T06:39:48.480607Z","iopub.status.idle":"2021-08-18T06:39:48.854033Z","shell.execute_reply.started":"2021-08-18T06:39:48.480576Z","shell.execute_reply":"2021-08-18T06:39:48.853076Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"P.S. I have changed the test size from 0.4 to 0.2, just to make the prediction better","metadata":{}},{"cell_type":"code","source":"model = lgb.LGBMRegressor(num_iteration = 800,  learning_rate=0.03, n_estimators=150, \n                           max_depth = 5, random_state = 10 )\nmodel.fit(x_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2021-08-18T06:42:18.761737Z","iopub.execute_input":"2021-08-18T06:42:18.762081Z","iopub.status.idle":"2021-08-18T06:42:47.296486Z","shell.execute_reply.started":"2021-08-18T06:42:18.762051Z","shell.execute_reply":"2021-08-18T06:42:47.295676Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The above parameters were not randomly picked, rather its an outcome of multiple trial and errors. Since it's my first time with LGBM, next time onwards i'll be using a automated way like a loop or something else. Please suggest if you have anything. ","metadata":{}},{"cell_type":"code","source":"# Use the forest's predict method on the test data\ny_pred = model.predict(x_test)\n# Calculate the absolute errors\nerrors = abs(y_pred - y_test)\n# Print out the mean absolute error (mae)\nprint('Base line Mean Absolute Error', round(np.mean(errors), 4), 'degrees.')\n# Print score top check overfiting\nprint(\"train Score -\", model.score(x_train,y_train))\nprint(\"test Score -\", model.score(x_test,y_test))","metadata":{"execution":{"iopub.status.busy":"2021-08-18T06:43:22.619926Z","iopub.execute_input":"2021-08-18T06:43:22.620276Z","iopub.status.idle":"2021-08-18T06:43:29.826514Z","shell.execute_reply.started":"2021-08-18T06:43:22.620248Z","shell.execute_reply":"2021-08-18T06:43:29.825852Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Well the error value reduced from 6.17 to 6.16, not too great, but something atleast. \n* P.s. Do suggest other methods for a beginner to try out in such datasets or cases.","metadata":{}},{"cell_type":"code","source":"submission['loss'] = model.predict(test)\nsubmission.to_csv('submission.csv', index = False)","metadata":{"execution":{"iopub.status.busy":"2021-08-18T06:48:57.442338Z","iopub.execute_input":"2021-08-18T06:48:57.442706Z","iopub.status.idle":"2021-08-18T06:49:01.434508Z","shell.execute_reply.started":"2021-08-18T06:48:57.442666Z","shell.execute_reply":"2021-08-18T06:49:01.433765Z"},"trusted":true},"execution_count":null,"outputs":[]}]}