{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## TPS August 2021","metadata":{}},{"cell_type":"markdown","source":"This notebook is mostly based on the following notebook for Optuna tuning and KFold CV.\nhttps://www.kaggle.com/michael127001/xgbregressor-with-optuna-tuning/notebook\n\nThe idea to use yeo-johnson transformation came from this discussion.\nhttps://www.kaggle.com/c/tabular-playground-series-aug-2021/discussion/266321\n\nI'm very new at dealing with high dimensional features, so please don't hesitate to give some input :)","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport sklearn as sk\nimport optuna\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-31T06:25:23.303374Z","iopub.execute_input":"2021-08-31T06:25:23.303711Z","iopub.status.idle":"2021-08-31T06:25:24.919345Z","shell.execute_reply.started":"2021-08-31T06:25:23.303681Z","shell.execute_reply":"2021-08-31T06:25:24.918342Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Read in the train and test data while making id the index\ntrain_data = pd.read_csv('../input/tabular-playground-series-aug-2021/train.csv', index_col=['id'])\ntest_data = pd.read_csv('../input/tabular-playground-series-aug-2021/test.csv', index_col=['id'])\ntrain_data","metadata":{"execution":{"iopub.status.busy":"2021-08-31T06:25:24.921031Z","iopub.execute_input":"2021-08-31T06:25:24.921592Z","iopub.status.idle":"2021-08-31T06:25:35.440098Z","shell.execute_reply.started":"2021-08-31T06:25:24.921546Z","shell.execute_reply":"2021-08-31T06:25:35.439219Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\nfrom math import sqrt\n\n# sklearn doesn't have an rmse function, so we define it here\ndef my_rmse(y_actual, y_predicted):\n    return mean_squared_error(y_actual, y_predicted, squared=False)","metadata":{"execution":{"iopub.status.busy":"2021-08-31T06:25:35.442129Z","iopub.execute_input":"2021-08-31T06:25:35.442648Z","iopub.status.idle":"2021-08-31T06:25:35.448052Z","shell.execute_reply.started":"2021-08-31T06:25:35.442607Z","shell.execute_reply":"2021-08-31T06:25:35.446831Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## EDA","metadata":{}},{"cell_type":"code","source":"# Wide range of values\ntrain_data.describe(include='all')","metadata":{"execution":{"iopub.status.busy":"2021-08-30T06:45:27.730742Z","iopub.status.idle":"2021-08-30T06:45:27.731264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check for null values (blank array = no null values)\n[null for null in train_data.isnull().sum() if null != 0]","metadata":{"execution":{"iopub.status.busy":"2021-08-30T06:45:27.732911Z","iopub.status.idle":"2021-08-30T06:45:27.733687Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.columns","metadata":{"execution":{"iopub.status.busy":"2021-08-30T06:45:27.735586Z","iopub.status.idle":"2021-08-30T06:45:27.736378Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Separate columns into features and label\nfeatures = [feature for feature in train_data.columns if feature.startswith('f')]\nlabel = ['loss']","metadata":{"execution":{"iopub.status.busy":"2021-08-31T06:25:48.042654Z","iopub.execute_input":"2021-08-31T06:25:48.043048Z","iopub.status.idle":"2021-08-31T06:25:48.048654Z","shell.execute_reply.started":"2021-08-31T06:25:48.043017Z","shell.execute_reply":"2021-08-31T06:25:48.047469Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Distributions of all features (many contain right skew)\nfeature_histograms = train_data[features].hist(figsize = (120, 160), bins=50, grid = False, xlabelsize=8, ylabelsize=8, layout = (101,4))","metadata":{"execution":{"iopub.status.busy":"2021-08-30T06:45:27.741139Z","iopub.status.idle":"2021-08-30T06:45:27.74195Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Distribution of loss\nloss_histogram = train_data[label].hist(figsize=(8,6), bins=10, grid=False, xlabelsize=8, ylabelsize=8)","metadata":{"execution":{"iopub.status.busy":"2021-08-30T06:45:27.743933Z","iopub.status.idle":"2021-08-30T06:45:27.744745Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# No correlation between features\ncorrelations = train_data.corr()\nsns.heatmap(data=correlations)","metadata":{"execution":{"iopub.status.busy":"2021-08-30T06:45:27.74659Z","iopub.status.idle":"2021-08-30T06:45:27.747403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data cleaning","metadata":{}},{"cell_type":"code","source":"X = train_data[features].values\ny = train_data[label].values\nX_test = test_data.values","metadata":{"execution":{"iopub.status.busy":"2021-08-31T06:25:50.69507Z","iopub.execute_input":"2021-08-31T06:25:50.69552Z","iopub.status.idle":"2021-08-31T06:25:50.865747Z","shell.execute_reply.started":"2021-08-31T06:25:50.695482Z","shell.execute_reply":"2021-08-31T06:25:50.864718Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Scale feature data to be roughly the same range\n# Only fit to train data to avoid data leakage\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nscaler.fit(X)\nX = scaler.transform(X)\nX_test = scaler.transform(X_test)\n\nX_test","metadata":{"execution":{"iopub.status.busy":"2021-08-30T06:53:08.221938Z","iopub.execute_input":"2021-08-30T06:53:08.222416Z","iopub.status.idle":"2021-08-30T06:53:09.061597Z","shell.execute_reply.started":"2021-08-30T06:53:08.222368Z","shell.execute_reply":"2021-08-30T06:53:09.060272Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import PowerTransformer\n\n# Use yeo johnson to deal with skewed data (also deals with negative values)\n# Only fit to train data to avoid data leakage\npt = PowerTransformer(method='yeo-johnson', standardize=False)\npt.fit(X)\nX = pt.transform(X)\nX_test = pt.transform(X_test)\n\nX","metadata":{"execution":{"iopub.status.busy":"2021-08-31T06:28:05.07317Z","iopub.execute_input":"2021-08-31T06:28:05.073525Z","iopub.status.idle":"2021-08-31T06:28:55.255149Z","shell.execute_reply.started":"2021-08-31T06:28:05.073494Z","shell.execute_reply":"2021-08-31T06:28:55.25333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"unskewed_train_data = pd.DataFrame(data=X, columns=features)\nunskewed_train_data","metadata":{"execution":{"iopub.status.busy":"2021-08-30T06:54:52.30444Z","iopub.execute_input":"2021-08-30T06:54:52.30531Z","iopub.status.idle":"2021-08-30T06:54:52.378327Z","shell.execute_reply.started":"2021-08-30T06:54:52.305222Z","shell.execute_reply":"2021-08-30T06:54:52.377304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Feature distributions after fixing skew (More but not all follow normal distribution)\nunskewed_features = unskewed_train_data.hist(figsize = (120, 160), bins=50, grid = False, xlabelsize=8, ylabelsize=8, layout = (101,4))","metadata":{"execution":{"iopub.status.busy":"2021-08-30T06:54:52.380347Z","iopub.execute_input":"2021-08-30T06:54:52.380759Z","iopub.status.idle":"2021-08-30T06:55:34.655667Z","shell.execute_reply.started":"2021-08-30T06:54:52.380712Z","shell.execute_reply":"2021-08-30T06:55:34.654612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Hyperparameter Tuning + Modelling","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom xgboost import XGBRegressor\n\n# Define objective (what to optimize)\ndef objective(trial):\n    # Split dataset for each trial (larger test size to prevent overfitting)\n    X_train, X_val, y_train, y_val = train_test_split(X,y, stratify=y, test_size=0.4)\n    \n    # Specify ranges of hyperparameters to try \n    param_grid = {\n        'n_estimators': trial.suggest_int('n_estimators',400,4000,400),\n        'max_depth': trial.suggest_int('max_depth',6,10),\n        'eta': trial.suggest_float('eta', 0.007,0.01),\n        'subsample': trial.suggest_discrete_uniform('subsample', 0.2,0.9,0.1),\n        'colsample_bylevel': trial.suggest_discrete_uniform('colsample_bylevel', 0.2, 0.9, 0.1),\n        'colsample_bytree': trial.suggest_discrete_uniform('colsample_bytree', 0.2, 0.9, 0.1),\n        'tweedie_variance_power': trial.suggest_discrete_uniform('tweedie_variance_power', 1.0,2.0,0.1),\n        #'booster': trial.suggest_categorical('booster', ['gbtree', 'gblinear', 'dart']),\n        'gamma': trial.suggest_loguniform('gamma', 1e-4,1e4),\n        'min_child_weight': trial.suggest_loguniform('min_child_weight', 1e-4,1e4),\n        'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-4,1e4),\n        'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-4,1e4),\n    }\n    \n    # Apparently this set of parameters makes processing faster\n    reg = XGBRegressor(\n        objective = 'reg:tweedie',\n        tree_method = 'gpu_hist',\n        predictor = 'gpu_predictor',\n        # Split job for 4 CPUs\n        n_jobs=4,\n        # Use set of params generated by param grid\n        **param_grid\n    )\n    \n    reg.fit(X_train, y_train, eval_set=[(X_val, y_val)], eval_metric='rmse', verbose=False)\n    \n    return my_rmse(y_val, reg.predict(X_val))","metadata":{"execution":{"iopub.status.busy":"2021-08-30T06:55:34.660423Z","iopub.execute_input":"2021-08-30T06:55:34.66274Z","iopub.status.idle":"2021-08-30T06:55:34.678451Z","shell.execute_reply.started":"2021-08-30T06:55:34.662689Z","shell.execute_reply":"2021-08-30T06:55:34.676837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Study is Optuna's object to call functions on\n# direction set to minimize as we want to lower rmse, sampler set to TPESampler for predicting one label\nstudy = optuna.create_study(direction='minimize', sampler=optuna.samplers.TPESampler(), study_name='XGBRegressor')\n# Try different sets of hyperparameters set in objective and timeout at 1 hour (60s*60min) \nstudy.optimize(objective, timeout=60*60)\n\n# trial object holds best params\ntrial = study.best_trial\nprint('Best root mean squared error: {}'.format(trial.value))\nprint('Best trial\\'s parameters: ')\nfor key, value in trial.params.items():\n    print('{}: {}'.format(key, value))","metadata":{"execution":{"iopub.status.busy":"2021-08-30T06:55:34.683739Z","iopub.execute_input":"2021-08-30T06:55:34.687225Z","iopub.status.idle":"2021-08-30T06:58:26.502764Z","shell.execute_reply.started":"2021-08-30T06:55:34.687177Z","shell.execute_reply":"2021-08-30T06:58:26.501687Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optuna.visualization.plot_optimization_history(study)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# See which hyperparameters are more important to tweak suggested ranges \noptuna.visualization.plot_param_importances(study)","metadata":{"execution":{"iopub.status.busy":"2021-08-30T06:58:26.504444Z","iopub.execute_input":"2021-08-30T06:58:26.504908Z","iopub.status.idle":"2021-08-30T06:58:26.970643Z","shell.execute_reply.started":"2021-08-30T06:58:26.504858Z","shell.execute_reply":"2021-08-30T06:58:26.969581Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## KFold Cross Validation on Best Model","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import KFold\n\nbest_params = trial.params\nbest_params['objective'] = 'reg:tweedie'\nbest_params['tree_method'] = 'gpu_hist'\nbest_params['predictor'] = 'gpu_predictor'\nbest_params['n_jobs'] = 4\n\ntest_preds = None\nrmse_list = []\nfold_num = 1\n\n# Use KFold CV to avoid overfitting\nfor train_index, val_index in KFold(n_splits=10, shuffle=True).split(X, y):\n    X_train, y_train = X[train_index], y[train_index]\n    X_val, y_val = X[val_index], y[val_index]\n    \n    model = XGBRegressor(**best_params)\n    model.fit(X_train, y_train, eval_set=[(X_val, y_val)], eval_metric='rmse', verbose=False)\n    \n    val_predictions = model.predict(X_val)\n    rmse_val = my_rmse(y_val, val_predictions)\n    print('Fold {fold_num} RMSE: {rmse:.4f}'.format(fold_num=fold_num, rmse=rmse_val))\n    rmse_list.append(rmse_val)\n    fold_num += 1\n    \n    # Use each fold's model to predict test values and add them to test_preds\n    if test_preds is None:\n        test_preds = model.predict(X_test)\n    else:\n        test_preds += model.predict(X_test)\n\n# Get average of predictions from KFold CV for submission\ntest_preds /= fold_num\nprint('Average KFold rmse: {avg_rmse:.4f}'.format(avg_rmse = np.mean(np.array(rmse_list))))","metadata":{"execution":{"iopub.status.busy":"2021-08-30T07:13:06.464727Z","iopub.execute_input":"2021-08-30T07:13:06.465167Z","iopub.status.idle":"2021-08-30T07:14:46.344672Z","shell.execute_reply.started":"2021-08-30T07:13:06.465121Z","shell.execute_reply":"2021-08-30T07:14:46.343591Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Submission","metadata":{}},{"cell_type":"code","source":"submission_df = pd.DataFrame(data=test_preds, columns=['loss'])\nsubmission_df.index = test_data.index\nsubmission_df","metadata":{"execution":{"iopub.status.busy":"2021-08-30T07:00:08.135298Z","iopub.status.idle":"2021-08-30T07:00:08.135941Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df.to_csv('TPS August Submission')","metadata":{"execution":{"iopub.status.busy":"2021-08-30T07:00:08.137151Z","iopub.status.idle":"2021-08-30T07:00:08.138023Z"},"trusted":true},"execution_count":null,"outputs":[]}]}