{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<h1 style='color:white; background:#663399; border:0'><center> TPS-Aug: EDA, Baselines (XGB, Keras NN)</center></h1>\n\n![](https://storage.googleapis.com/kaggle-competitions/kaggle/26480/logos/header.png?t=2021-04-09-00-57-05)\n\n<a id=\"section-start\"></a>\n\nThe goal of these competitions is to provide a fun, and approachable for anyone, tabular dataset. These competitions will be great for people looking for something in between the Titanic Getting Started competition and a Featured competition. If you're an established competitions master or grandmaster, these probably won't be much of a challenge for you. We encourage you to avoid saturating the leaderboard.\n\nThe dataset is used for this competition is synthetic, but based on a real dataset and generated using a CTGAN. The original dataset deals with calculating the loss associated with a loan defaults. Although the features are anonymized, they have properties relating to real-world features.\n\nFor this competition, you will be predicting a `target loss` based on a number of feature columns given in the data. The ground truth loss is integer valued, although predictions can be continuous.\n\n### See also my previous TPS works:\n- [TPS-July: EDA, Baseline Analysis (XGBRegressor)](https://www.kaggle.com/maksymshkliarevskyi/tps-july-eda-baseline-analysis-xgbregressor)\n- [TPS-Jun: starting point (EDA, Baseline, CV)](https://www.kaggle.com/maksymshkliarevskyi/tps-jun-starting-point-eda-baseline-cv)","metadata":{}},{"cell_type":"code","source":"!pip install /kaggle/input/adjusttext\n!pip install /kaggle/input/bioinfokit","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-08-01T02:48:50.334798Z","iopub.execute_input":"2021-08-01T02:48:50.335199Z","iopub.status.idle":"2021-08-01T02:49:47.956092Z","shell.execute_reply.started":"2021-08-01T02:48:50.335101Z","shell.execute_reply":"2021-08-01T02:49:47.955109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# for feature importance study\nimport eli5\nfrom eli5.sklearn import PermutationImportance\nfrom pdpbox import pdp\nimport shap\n\n# for PCA\nfrom bioinfokit.visuz import cluster\nfrom sklearn.decomposition import PCA\n\n# ML\nfrom sklearn.model_selection import train_test_split, KFold, GridSearchCV\nfrom sklearn.preprocessing import StandardScaler\nfrom xgboost import XGBRegressor\nimport os\nimport tensorflow as tf\nimport keras\nfrom tensorflow.keras import models, layers, callbacks\nimport tensorflow_addons as tfa\nfrom keras.utils.vis_utils import plot_model\nfrom keras import backend as K\nimport gc\n\n# Reproducability\ndef set_seed(seed = 0):\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n    print('*** --- Set seed \"%i\" --- ***' %seed)\n\n# Custom theme\nplt.style.use('fivethirtyeight')\n\nfigure = {'dpi': '200'}\nfont = {'family': 'serif'}\ngrid = {'linestyle': ':', 'alpha': .9}\naxes = {'titlecolor': 'black', 'titlesize': 20, 'titleweight': 'bold',\n        'labelsize': 12, 'labelweight': 'bold'}\n\nplt.rc('font', **font)\nplt.rc('figure', **figure)\nplt.rc('grid', **grid)\nplt.rc('axes', **axes)\n\nmy_colors = ['#DC143C', '#FF1493', '#FF7F50', '#FFD700', '#32CD32', \n             '#4ddbff', '#1E90FF', '#663399', '#708090']\n\ncaption = \"Â© maksymshkliarevskyi\"\n\n# Show our custom palette\nsns.palplot(sns.color_palette(my_colors))\nplt.title('Custom palette')\nplt.text(6.9, 0.75, caption, size = 8)\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-01T02:49:47.959756Z","iopub.execute_input":"2021-08-01T02:49:47.960024Z","iopub.status.idle":"2021-08-01T02:49:57.809652Z","shell.execute_reply.started":"2021-08-01T02:49:47.959994Z","shell.execute_reply":"2021-08-01T02:49:57.808741Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"First, let's load the data and take a look at basic statistics.","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('../input/tabular-playground-series-aug-2021/train.csv', \n                    index_col = 0)\ntest = pd.read_csv('../input/tabular-playground-series-aug-2021/test.csv', \n                   index_col = 0)\nss = pd.read_csv('../input/tabular-playground-series-aug-2021/sample_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2021-08-01T02:49:57.811463Z","iopub.execute_input":"2021-08-01T02:49:57.811794Z","iopub.status.idle":"2021-08-01T02:50:07.814098Z","shell.execute_reply.started":"2021-08-01T02:49:57.81176Z","shell.execute_reply":"2021-08-01T02:50:07.813269Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2 style='color:white; background:#663399; border:0'><center>EDA</center></h2>\n\n[**Back to the start**](#section-start)","metadata":{}},{"cell_type":"code","source":"train.describe().T.style.background_gradient(subset = ['count'], cmap = 'viridis') \\\n    .bar(subset = ['mean', '50%'], color = my_colors[6]) \\\n    .bar(subset = ['std'], color = my_colors[0])","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-01T02:50:07.815578Z","iopub.execute_input":"2021-08-01T02:50:07.815897Z","iopub.status.idle":"2021-08-01T02:50:08.83005Z","shell.execute_reply.started":"2021-08-01T02:50:07.815864Z","shell.execute_reply":"2021-08-01T02:50:08.829263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.describe().T.style.background_gradient(subset = ['count'], cmap = 'viridis') \\\n    .bar(subset = ['mean', '50%'], color = my_colors[6]) \\\n    .bar(subset = ['std'], color = my_colors[0])","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-01T02:50:08.831149Z","iopub.execute_input":"2021-08-01T02:50:08.831492Z","iopub.status.idle":"2021-08-01T02:50:09.530609Z","shell.execute_reply.started":"2021-08-01T02:50:08.831459Z","shell.execute_reply":"2021-08-01T02:50:09.529822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Wow, f60 feature has very big values.","metadata":{}},{"cell_type":"code","source":"dtypes = train.dtypes.value_counts().reset_index()\n\nplt.figure(figsize = (12, 1))\nplt.title('Data types\\n')\nplt.barh(str(dtypes.iloc[0, 0]), dtypes.iloc[0, 1],\n         label = str(dtypes.iloc[0, 0]), color = my_colors[4])\nplt.legend(loc = 'upper center', ncol = 3, fontsize = 13,\n           bbox_to_anchor = (0.5, 1.45), frameon = False)\nplt.yticks('')\nplt.text(85, -0.9, caption, size = 8)\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-01T02:50:09.531848Z","iopub.execute_input":"2021-08-01T02:50:09.532195Z","iopub.status.idle":"2021-08-01T02:50:09.673157Z","shell.execute_reply.started":"2021-08-01T02:50:09.532146Z","shell.execute_reply":"2021-08-01T02:50:09.672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have 250000 training and 150000 test observations. All our data is in float32 format.\n\nBefore we continue, let's pull the target feature into the separate variable.","metadata":{}},{"cell_type":"code","source":"loss = train.loss\n\ntrain.drop(['loss'], axis = 1, inplace = True)","metadata":{"execution":{"iopub.status.busy":"2021-08-01T02:50:09.674584Z","iopub.execute_input":"2021-08-01T02:50:09.674945Z","iopub.status.idle":"2021-08-01T02:50:09.742606Z","shell.execute_reply.started":"2021-08-01T02:50:09.674906Z","shell.execute_reply":"2021-08-01T02:50:09.741067Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It's important to see if our data has missing values.","metadata":{}},{"cell_type":"code","source":"# Concatenate train and test datasets\nall_data = pd.concat([train, test], axis = 0)\n\n# columns with missing values\ncols_with_na = all_data.isna().sum()[all_data.isna().sum() > 0].sort_values(ascending = False)\ncols_with_na","metadata":{"execution":{"iopub.status.busy":"2021-08-01T02:50:09.746315Z","iopub.execute_input":"2021-08-01T02:50:09.746572Z","iopub.status.idle":"2021-08-01T02:50:10.100771Z","shell.execute_reply.started":"2021-08-01T02:50:09.746544Z","shell.execute_reply":"2021-08-01T02:50:10.099728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As in the previous competitions, our data has no missing values. Now, let's look at the feature distributions.","metadata":{}},{"cell_type":"code","source":"print('Train data')\nfig = plt.figure(figsize = (15, 120))\nfor idx, i in enumerate(train.columns):\n    fig.add_subplot(np.ceil(len(train.columns)/4), 4, idx+1)\n    train.iloc[:, idx].hist(bins = 20)\n    plt.title(i)\nplt.text(10, -8000, caption, size = 12)\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-01T02:50:10.102808Z","iopub.execute_input":"2021-08-01T02:50:10.103163Z","iopub.status.idle":"2021-08-01T02:50:27.514837Z","shell.execute_reply.started":"2021-08-01T02:50:10.103127Z","shell.execute_reply":"2021-08-01T02:50:27.513376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize = (15, 5))\nloss.hist(bins = 20)\nplt.title('loss')\nplt.text(37, -12000, caption, size = 12)\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-01T02:50:27.515918Z","iopub.execute_input":"2021-08-01T02:50:27.516164Z","iopub.status.idle":"2021-08-01T02:50:27.924111Z","shell.execute_reply.started":"2021-08-01T02:50:27.516138Z","shell.execute_reply":"2021-08-01T02:50:27.92299Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Test data')\nfig = plt.figure(figsize = (15, 120))\nfor idx, i in enumerate(test.columns):\n    fig.add_subplot(np.ceil(len(test.columns)/4), 4, idx+1)\n    test.iloc[:, idx].hist(bins = 20)\n    plt.title(i)\nplt.text(10, -8000, caption, size = 12)\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-01T02:50:27.925821Z","iopub.execute_input":"2021-08-01T02:50:27.926239Z","iopub.status.idle":"2021-08-01T02:50:47.234121Z","shell.execute_reply.started":"2021-08-01T02:50:27.926198Z","shell.execute_reply":"2021-08-01T02:50:47.232451Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We should also look at the correlation between features.","metadata":{}},{"cell_type":"code","source":"corr = train.corr()\nmask = np.triu(np.ones_like(corr, dtype = bool))\n\nplt.figure(figsize = (15, 15))\nplt.title('Corelation matrix')\nsns.heatmap(corr, mask = mask, cmap = 'Spectral_r', linewidths = .5)\nplt.text(106, 106, caption, size = 8)\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-01T02:50:47.235565Z","iopub.execute_input":"2021-08-01T02:50:47.235954Z","iopub.status.idle":"2021-08-01T02:50:54.475919Z","shell.execute_reply.started":"2021-08-01T02:50:47.235911Z","shell.execute_reply":"2021-08-01T02:50:54.475097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corr = test.corr()\nmask = np.triu(np.ones_like(corr, dtype = bool))\n\nplt.figure(figsize = (15, 15))\nplt.title('Corelation matrix')\nsns.heatmap(corr, mask = mask, cmap = 'Spectral_r', linewidths = .5)\nplt.text(106, 106, caption, size = 8)\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-01T02:50:54.477307Z","iopub.execute_input":"2021-08-01T02:50:54.477823Z","iopub.status.idle":"2021-08-01T02:50:59.65272Z","shell.execute_reply.started":"2021-08-01T02:50:54.477785Z","shell.execute_reply":"2021-08-01T02:50:59.651868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"All features are weakly correlated.\n\n<h2 style='color:white; background:#663399; border:0'><center>XGBRegressor Baseline</center></h2>\n\n[**Back to the start**](#section-start)\n\nAt first, we'll train a very simple basic XGBRegressor.","metadata":{}},{"cell_type":"code","source":"# Create data sets for training (85%) and validation (15%)\nX_train, X_valid, y_train, y_valid = train_test_split(train, loss, \n                                                      test_size = 0.15,\n                                                      random_state = 0)","metadata":{"execution":{"iopub.status.busy":"2021-08-01T02:50:59.654227Z","iopub.execute_input":"2021-08-01T02:50:59.654778Z","iopub.status.idle":"2021-08-01T02:50:59.867196Z","shell.execute_reply.started":"2021-08-01T02:50:59.654741Z","shell.execute_reply":"2021-08-01T02:50:59.866263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def root_mean_squared_error(y_true, y_pred):\n    from sklearn.metrics import mean_squared_error\n    from math import sqrt\n    return sqrt(mean_squared_error(y_true, y_pred))","metadata":{"execution":{"iopub.status.busy":"2021-08-01T02:50:59.868605Z","iopub.execute_input":"2021-08-01T02:50:59.868954Z","iopub.status.idle":"2021-08-01T02:50:59.873605Z","shell.execute_reply.started":"2021-08-01T02:50:59.868917Z","shell.execute_reply":"2021-08-01T02:50:59.8728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The basic model\nparams = {'random_state': 0,\n          'predictor': 'gpu_predictor',\n          'tree_method': 'gpu_hist',\n          'eval_metric': 'logloss'}\n\nmodel = XGBRegressor(**params)\n\nmodel.fit(X_train, y_train, verbose = False)\n\npreds = model.predict(X_valid)\nprint('Valid RMSE of the basic model: {}'.format(root_mean_squared_error(y_valid, preds)))","metadata":{"execution":{"iopub.status.busy":"2021-08-01T02:50:59.874926Z","iopub.execute_input":"2021-08-01T02:50:59.875534Z","iopub.status.idle":"2021-08-01T02:51:02.105921Z","shell.execute_reply.started":"2021-08-01T02:50:59.875497Z","shell.execute_reply":"2021-08-01T02:51:02.104949Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"explainer = shap.TreeExplainer(model)\nshap_values = explainer.shap_values(X_valid)\n\nshap.summary_plot(shap_values, X_valid)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-01T02:51:02.107376Z","iopub.execute_input":"2021-08-01T02:51:02.10797Z","iopub.status.idle":"2021-08-01T02:51:12.648878Z","shell.execute_reply.started":"2021-08-01T02:51:02.107931Z","shell.execute_reply":"2021-08-01T02:51:12.647918Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2 style='color:white; background:#663399; border:0'><center>Keras NN Baseline</center></h2>\n\n[**Back to the start**](#section-start)\n\nIn this step, we'll train our baseline keras NN. We'll use the residual network from this excellent notebook [tabular residual network](https://www.kaggle.com/oxzplvifi/tabular-residual-network).\n\nLet's define architecture.","metadata":{}},{"cell_type":"code","source":"def root_mean_squared_error(y_true, y_pred):\n    return K.sqrt(K.mean(K.square(y_pred - y_true))) \n\ndef create_model(shape = train.shape[1]):\n    inputs = layers.Input(shape = (shape))\n\n    hidden = layers.Dropout(0.25)(inputs)\n    hidden = tfa.layers.WeightNormalization(layers.Dense(units = 128, activation = 'relu'))(hidden)\n\n    output = layers.Dropout(0.25)(layers.Concatenate()([inputs, hidden]))\n    output = tfa.layers.WeightNormalization(layers.Dense(units = 64, activation = 'relu'))(output) \n\n    output = layers.Dropout(0.25)(layers.Concatenate()([inputs, hidden, output]))\n    output = tfa.layers.WeightNormalization(layers.Dense(units = 32, activation = 'relu'))(output) \n    output = layers.Dense(1)(output)\n\n    model = keras.Model(inputs = inputs, outputs = output, name = \"res_nn_model\")\n\n    model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = 0.003),\n                  loss = root_mean_squared_error)\n    return model","metadata":{"execution":{"iopub.status.busy":"2021-08-01T02:51:12.650354Z","iopub.execute_input":"2021-08-01T02:51:12.650671Z","iopub.status.idle":"2021-08-01T02:51:12.66106Z","shell.execute_reply.started":"2021-08-01T02:51:12.650637Z","shell.execute_reply":"2021-08-01T02:51:12.660341Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Before starting the training process, let's visualize our architecture.","metadata":{}},{"cell_type":"code","source":"model = create_model()\n\nplot_model(model, to_file = 'model_plot.png', show_shapes = True, show_layer_names = True)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-01T02:51:12.662435Z","iopub.execute_input":"2021-08-01T02:51:12.663009Z","iopub.status.idle":"2021-08-01T02:51:19.477213Z","shell.execute_reply.started":"2021-08-01T02:51:12.662974Z","shell.execute_reply":"2021-08-01T02:51:19.476253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scaler = StandardScaler()\ntrain = scaler.fit_transform(train)\ntest = scaler.transform(test)\n\n# Create data sets for training (85%) and validation (15%)\nX_train, X_valid, y_train, y_valid = train_test_split(train, np.float32(loss), \n                                                      test_size = 0.15,\n                                                      random_state = 0)","metadata":{"execution":{"iopub.status.busy":"2021-08-01T02:51:19.479064Z","iopub.execute_input":"2021-08-01T02:51:19.479742Z","iopub.status.idle":"2021-08-01T02:51:20.513884Z","shell.execute_reply.started":"2021-08-01T02:51:19.479699Z","shell.execute_reply":"2021-08-01T02:51:20.512967Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"set_seed()\n\nmodel = create_model()\n\nearly_stopping = callbacks.EarlyStopping(\n    patience = 7,\n    min_delta = 0.00001,\n    restore_best_weights = True,\n    monitor = \"val_loss\")\nplateau = callbacks.ReduceLROnPlateau(\n    factor = 0.6,                                     \n    patience = 3,                                   \n    min_delt = 0.00001,                                \n    verbose = 0) \n\nhistory = model.fit(X_train, y_train,\n                    batch_size = 64,\n                    epochs = 30,\n                    validation_data = (X_valid, y_valid),\n                    callbacks = [early_stopping, plateau],\n                    verbose = 0)\n\nmin_loss = round(np.min(history.history['val_loss']), 5)\nmin_loss_epoch = np.argmin(history.history['val_loss']) + 1\nprint('Best valid RMSE at the epoch #{} : {}'.format(min_loss_epoch, min_loss))\n\nfig, ax = plt.subplots(figsize = (20, 4))\nsns.lineplot(x = history.epoch, y = history.history['loss'])\nsns.lineplot(x = history.epoch, y = history.history['val_loss'])\nax.set_title('Learning Curve (Loss) (Best value: {})'.format(min_loss))\nax.set_ylabel('Loss')\nax.set_xlabel('Epoch')\nax.legend(['train', 'test'], loc='best')\nplt.show()\n\ndel X_train, X_valid, y_train, y_valid, train\ngc.collect()","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2021-08-01T02:51:20.515381Z","iopub.execute_input":"2021-08-01T02:51:20.515731Z","iopub.status.idle":"2021-08-01T02:54:07.809887Z","shell.execute_reply.started":"2021-08-01T02:51:20.515694Z","shell.execute_reply":"2021-08-01T02:54:07.808752Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The loss of the NN is slightly fewer. But we used very simple sighting models. There is still a lot of work to be done with the selection of optimal parameters.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"section-8\"></a>\n<h2 style='color:white; background:#008294; border:0'><center>Test prediction</center></h2>\n\n[**Back to the table of contents**](#section-start)\n\nLet's take a look at our predictions and prepare them for submission.","metadata":{}},{"cell_type":"code","source":"ss.loss = model.predict(test, batch_size = 8)\nss","metadata":{"execution":{"iopub.status.busy":"2021-08-01T02:55:03.320627Z","iopub.execute_input":"2021-08-01T02:55:03.320959Z","iopub.status.idle":"2021-08-01T02:55:39.429362Z","shell.execute_reply.started":"2021-08-01T02:55:03.320929Z","shell.execute_reply":"2021-08-01T02:55:39.428224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ss.to_csv('submission.csv', index = False)","metadata":{"execution":{"iopub.status.busy":"2021-08-01T02:56:01.749066Z","iopub.execute_input":"2021-08-01T02:56:01.749432Z","iopub.status.idle":"2021-08-01T02:56:02.131697Z","shell.execute_reply.started":"2021-08-01T02:56:01.749401Z","shell.execute_reply":"2021-08-01T02:56:02.130796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2 style='color:white; background:#663399; border:0'><center>WORK IN PROGRESS...</center></h2>\n\n[**Back to the start**](#section-start)","metadata":{}}]}