{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%config Completer.use_jedi = False","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-20T17:11:01.666144Z","iopub.execute_input":"2021-08-20T17:11:01.666542Z","iopub.status.idle":"2021-08-20T17:11:01.683235Z","shell.execute_reply.started":"2021-08-20T17:11:01.666509Z","shell.execute_reply":"2021-08-20T17:11:01.682076Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import imshow, imread\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans\nfrom sklearn.mixture import GaussianMixture\n\nimport scipy.stats as stats\n\nimport lightgbm as lgb\nimport warnings","metadata":{"execution":{"iopub.status.busy":"2021-08-20T17:11:01.713596Z","iopub.execute_input":"2021-08-20T17:11:01.713952Z","iopub.status.idle":"2021-08-20T17:11:01.720732Z","shell.execute_reply.started":"2021-08-20T17:11:01.713923Z","shell.execute_reply":"2021-08-20T17:11:01.719144Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"R_SEED = 37","metadata":{"execution":{"iopub.status.busy":"2021-08-20T17:11:01.738124Z","iopub.execute_input":"2021-08-20T17:11:01.738513Z","iopub.status.idle":"2021-08-20T17:11:01.742906Z","shell.execute_reply.started":"2021-08-20T17:11:01.738472Z","shell.execute_reply":"2021-08-20T17:11:01.741746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submit = True # for some testing","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_ex = pd.read_csv('/kaggle/input/tabular-playground-series-aug-2021/sample_submission.csv')\ntrain_df = pd.read_csv('/kaggle/input/tabular-playground-series-aug-2021/train.csv')\ntest_df = pd.read_csv('/kaggle/input/tabular-playground-series-aug-2021/test.csv')","metadata":{"execution":{"iopub.status.busy":"2021-08-20T17:11:01.7634Z","iopub.execute_input":"2021-08-20T17:11:01.763758Z","iopub.status.idle":"2021-08-20T17:11:09.58425Z","shell.execute_reply.started":"2021-08-20T17:11:01.763727Z","shell.execute_reply":"2021-08-20T17:11:09.58302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Main idea\nIf we merge train data with test data and perform series of transformation on them, maybe we create additional bond between them. Just an idea, let's see what will happen.","metadata":{}},{"cell_type":"code","source":"targets_df = train_df[['loss']].copy()\ntrain_df.drop(['id', 'loss'], axis=1, inplace=True) \ntest_df.drop(['id'], axis=1, inplace=True) ","metadata":{"execution":{"iopub.status.busy":"2021-08-20T17:11:09.585776Z","iopub.execute_input":"2021-08-20T17:11:09.586078Z","iopub.status.idle":"2021-08-20T17:11:09.700365Z","shell.execute_reply.started":"2021-08-20T17:11:09.586048Z","shell.execute_reply":"2021-08-20T17:11:09.699011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_df = pd.concat([train_df, test_df])\n# 1-------------------vvv","metadata":{"execution":{"iopub.status.busy":"2021-08-20T17:11:09.702196Z","iopub.execute_input":"2021-08-20T17:11:09.702613Z","iopub.status.idle":"2021-08-20T17:11:09.92114Z","shell.execute_reply.started":"2021-08-20T17:11:09.702571Z","shell.execute_reply":"2021-08-20T17:11:09.920064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"warnings.filterwarnings(\"ignore\")\n\nfig = plt.figure(figsize = (30,60))\nax = fig.gca()\nhist = all_df.hist(bins = 50, layout = (20,5), color='k', alpha=0.5,  ax = ax)","metadata":{"execution":{"iopub.status.busy":"2021-08-20T17:11:09.923177Z","iopub.execute_input":"2021-08-20T17:11:09.923503Z","iopub.status.idle":"2021-08-20T17:11:37.423017Z","shell.execute_reply.started":"2021-08-20T17:11:09.923473Z","shell.execute_reply":"2021-08-20T17:11:37.422014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### step by step\nI know that kmeans is not very good for this, but, its fast enough for just a try.","metadata":{}},{"cell_type":"code","source":"# warnings.filterwarnings(\"ignore\")\ndef plot_fea_hist(fea_name):\n    fig = plt.figure(figsize = (10, 10))\n    ax = fig.gca()\n    hist = all_df[fea_name].hist(bins=150, ax = ax)","metadata":{"execution":{"iopub.status.busy":"2021-08-20T17:11:37.424196Z","iopub.execute_input":"2021-08-20T17:11:37.424499Z","iopub.status.idle":"2021-08-20T17:11:37.42971Z","shell.execute_reply.started":"2021-08-20T17:11:37.42447Z","shell.execute_reply":"2021-08-20T17:11:37.42873Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### KMeans","metadata":{}},{"cell_type":"code","source":"def plot_kmeans(data, labels, no_of_cl, fea_name, ax):\n    ax.hist(data, 100, density = True)\n    for cl in range(no_of_cl):\n        ax.hist(data[labels == cl], 1, density = True, alpha = 0.5)\n    ax.set_title(fea_name)\n#     plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-20T17:11:37.430856Z","iopub.execute_input":"2021-08-20T17:11:37.431144Z","iopub.status.idle":"2021-08-20T17:11:37.443508Z","shell.execute_reply.started":"2021-08-20T17:11:37.431117Z","shell.execute_reply":"2021-08-20T17:11:37.442432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# guesswork\nfor_kmeans = [('f2', 2), ('f3', 3), ('f7', 3), ('f11', 3), ('f12', 2), ('f14', 2), ('f18', 2), ('f19', 2), ('f20', 4), ('f24', 3), \n              ('f26', 2), ('f27', 2), ('f32', 3), ('f34', 2), ('f38', 2), ('f48', 2), ('f50', 3), ('f57', 3), ('f67', 3), ('f76', 3),\n             ('f80', 2), ('f86', 4), ('f93', 2), ('f94', 2)]\n\nfig, axes = plt.subplots(nrows = 6, ncols = 4, figsize=(20, 30))\ni = 1\nfor f, n_clusters in for_kmeans:\n#     print(str(i) + ' of ' + str(len(for_kmeans)))\n    \n    # KMeans\n    data = all_df[[f]].values\n    km = KMeans(n_clusters = n_clusters, n_init = 50)\n    km.fit(data)\n    k_clus = km.labels_\n    \n    # print(km.cluster_centers_)\n    # print(pd.value_counts(km.labels_))\n\n    ax = axes[(i-1) // 4, (i-1) % 4]\n    plot_kmeans(data, k_clus, n_clusters, f, ax)\n\n    i += 1\n    \n    # # one_h_clus = np.zeros((k_clus.size, k_clus.max()+1))\n    # # one_h_clus[np.arange(k_clus.size), k_clus] = 1\n    # # for i in range(n_clusters):\n    # #     all_df['clus_' + str(i)] = one_h_clus[:,i]\n\n#     all_df[f + '_clus'] = k_clus\n    _dist = km.transform(data)\n    _dict = {f + '_dist_from_' + str(i): _dist[:,i] for i in range(n_clusters)}\n    for k, v in _dict.items():\n        all_df[k] = v\n#     del all_df[f]\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-20T17:11:37.444936Z","iopub.execute_input":"2021-08-20T17:11:37.445237Z","iopub.status.idle":"2021-08-20T17:13:31.534785Z","shell.execute_reply.started":"2021-08-20T17:11:37.445208Z","shell.execute_reply":"2021-08-20T17:13:31.533755Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I created some new features as distance from cluster centroids for specified original features.","metadata":{}},{"cell_type":"markdown","source":"#### GaussianMixture","metadata":{}},{"cell_type":"code","source":"def plot_gmm(model, data, fea_name, ax):\n    weights = model.weights_\n    means = model.means_\n    covars = model.covariances_\n\n    n, bins, patches = ax.hist(data, 100, density = True, alpha = 0.2, color = 'k')\n    x = np.arange(np.min(data), np.max(data), (np.max(data) - np.min(data)) / 100)\n    for i in range(len(weights)):\n        ax.plot(x, weights[i] * stats.norm.pdf(x,means[i],np.sqrt(covars[i])[0]), alpha = 0.7, linewidth = 3)\n    ax.set_title(fea_name)\n#     plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-20T17:13:31.537806Z","iopub.execute_input":"2021-08-20T17:13:31.538412Z","iopub.status.idle":"2021-08-20T17:13:31.546866Z","shell.execute_reply.started":"2021-08-20T17:13:31.538349Z","shell.execute_reply":"2021-08-20T17:13:31.545932Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# guesswork\nfor_gmm = [('f2', 2), ('f4', 3), ('f6', 2), ('f7', 4), ('f8', 2), ('f9', 2), ('f11', 3), ('f12', 2), ('f14', 3), ('f15', 3),\n          ('f16', 5), ('f18', 3), ('f19', 3), ('f21', 2), ('f22', 3), ('f24', 3), ('f26', 2), ('f28', 3), ('f30', 3), ('f32', 3),\n          ('f34', 2), ('f35', 3), ('f37', 3), ('f38', 3), ('f39', 3), ('f41', 3), ('f52', 3), ('f64', 3), ('f67', 3), ('f68', 2),\n          ('f69', 2), ('f73', 3), ('f75', 3), ('f76', 3), ('f77', 2), ('f79', 2), ('f80', 2), ('f81', 3), ('f85', 3), ('f89', 3),\n          ('f91', 3), ('f93', 3), ('f94', 3)]\n\nfig, axes = plt.subplots(nrows = 11, ncols = 4, figsize=(20, 50))\n\ni = 1\nfor f, n_clusters in for_gmm:\n#     print(str(i) + ' of ' + str(len(for_gmm)))\n    \n    # GMM\n    data = all_df[[f]].values\n    \n    gm = GaussianMixture(n_components = n_clusters, n_init = 5)\n    gm.fit(data)\n    k_clus_1 = gm.predict(data)\n    k_clus_2 = gm.predict_proba(data)\n\n    ax = axes[(i-1) // 4, (i-1) % 4]\n    plot_gmm(gm, data, f + '_clus_gmm', ax)\n    i += 1\n    \n    all_df[f + '_clus_gmm'] = k_clus_1\n    for j in range(len(k_clus_2[0])):\n        all_df[f + '_clus_gmm_' + str(j)] = k_clus_2[:, j]\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-20T17:13:31.548907Z","iopub.execute_input":"2021-08-20T17:13:31.549333Z","iopub.status.idle":"2021-08-20T17:24:32.272457Z","shell.execute_reply.started":"2021-08-20T17:13:31.549287Z","shell.execute_reply":"2021-08-20T17:24:32.2717Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"gmm features represent probability that value of original feature belongs to certain distribution","metadata":{}},{"cell_type":"code","source":"all_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-20T17:24:32.273507Z","iopub.execute_input":"2021-08-20T17:24:32.273926Z","iopub.status.idle":"2021-08-20T17:24:32.304498Z","shell.execute_reply.started":"2021-08-20T17:24:32.273884Z","shell.execute_reply":"2021-08-20T17:24:32.303784Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# lightGBM will work same without this, but I try to transform data as much as I can before splitting them on trainset and testset\nall_df_normalized = StandardScaler().fit_transform(all_df)\nall_df = pd.DataFrame(all_df_normalized, columns=all_df.columns)","metadata":{"execution":{"iopub.status.busy":"2021-08-20T17:24:32.305482Z","iopub.execute_input":"2021-08-20T17:24:32.305874Z","iopub.status.idle":"2021-08-20T17:24:36.631645Z","shell.execute_reply.started":"2021-08-20T17:24:32.305833Z","shell.execute_reply":"2021-08-20T17:24:36.630618Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# KMeans on whole dataset\nn_clusters = 2 # ?\n\ndata = all_df.values\nkm = KMeans(n_clusters = n_clusters, n_init = 50)\nkm.fit(data)\nk_clus = km.labels_\nprint(pd.value_counts(km.labels_))\n\n_dist = km.transform(data)\n_dict = {'all_dist_from_' + str(i): _dist[:,i] for i in range(n_clusters)}\nfor k, v in _dict.items():\n    all_df[k] = v","metadata":{"execution":{"iopub.status.busy":"2021-08-20T17:24:36.63288Z","iopub.execute_input":"2021-08-20T17:24:36.633191Z","iopub.status.idle":"2021-08-20T17:26:51.006326Z","shell.execute_reply.started":"2021-08-20T17:24:36.633159Z","shell.execute_reply":"2021-08-20T17:26:51.004951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 1-------------------^^^\ntrain_df, test_df = all_df.iloc[:train_df.shape[0],:].copy(), all_df.iloc[-test_df.shape[0]:,:].copy()","metadata":{"execution":{"iopub.status.busy":"2021-08-20T17:26:51.008406Z","iopub.execute_input":"2021-08-20T17:26:51.008855Z","iopub.status.idle":"2021-08-20T17:26:51.336919Z","shell.execute_reply.started":"2021-08-20T17:26:51.008811Z","shell.execute_reply":"2021-08-20T17:26:51.336086Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if submit:\n    X = train_df.copy()\n    y = targets_df[['loss']].copy()\nelse:\n    np.random.seed(R_SEED)\n    msk = np.random.rand(len(train_df)) < 0.9\n    X = train_df[msk].copy()\n    my_X = train_df[~msk].copy()\n    y = targets_df[msk].copy()\n    my_y = targets_df[~msk].copy()","metadata":{"execution":{"iopub.status.busy":"2021-08-20T17:26:51.343239Z","iopub.execute_input":"2021-08-20T17:26:51.343801Z","iopub.status.idle":"2021-08-20T17:26:52.356882Z","shell.execute_reply.started":"2021-08-20T17:26:51.343758Z","shell.execute_reply":"2021-08-20T17:26:52.355786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# qq_df = pd.concat([X, y], axis=1, join='inner')\n\n# corr = qq_df.corr()\n\n# mask = np.zeros_like(corr)\n# mask[np.triu_indices_from(mask)] = True\n\n# fig = plt.figure(figsize = (30, 25))\n# sns.heatmap(corr, cmap=\"flare\", mask=mask)\n# plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-20T17:26:52.357935Z","iopub.execute_input":"2021-08-20T17:26:52.358203Z","iopub.status.idle":"2021-08-20T17:26:52.361692Z","shell.execute_reply.started":"2021-08-20T17:26:52.358177Z","shell.execute_reply":"2021-08-20T17:26:52.360751Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def softmax(x):\n    e_x = np.exp(x - np.max(x))\n    return e_x / e_x.sum(axis = 0)","metadata":{"execution":{"iopub.status.busy":"2021-08-20T17:26:52.362727Z","iopub.execute_input":"2021-08-20T17:26:52.362992Z","iopub.status.idle":"2021-08-20T17:26:52.374636Z","shell.execute_reply.started":"2021-08-20T17:26:52.362965Z","shell.execute_reply":"2021-08-20T17:26:52.373479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_fea_imp(model, model_name):\n    print('Plotting feature importances...')\n    fea_imp = pd.DataFrame({'imp': model.feature_importances_, 'col': train_df.columns})\n    fea_imp = fea_imp.sort_values(['imp', 'col'], ascending=[True, False])#.iloc[-10:]\n    fea_imp.plot(kind='barh', x='col', y='imp', figsize=(20, 70), legend=None)\n    plt.title('%s - Feature Importance' % (model_name))\n    plt.ylabel('Features')\n    plt.xlabel('Importance')","metadata":{"execution":{"iopub.status.busy":"2021-08-20T17:41:38.345119Z","iopub.execute_input":"2021-08-20T17:41:38.345622Z","iopub.status.idle":"2021-08-20T17:41:38.35192Z","shell.execute_reply.started":"2021-08-20T17:41:38.345586Z","shell.execute_reply":"2021-08-20T17:41:38.350918Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params_loss = {\n                'n_estimators': 150000, # there is early_stopping\n                'learning_rate': 0.002,\n                'min_child_samples': 500,\n                'feature_fraction': 0.35,\n                'bagging_fraction': 0.8,\n                'bagging_freq': 1,\n                }\n\nlgbm_reg = lgb.LGBMRegressor(\n                            **params_loss, \n                            objective='rmse',\n                            metric='rmse',\n                            n_jobs=-1\n                            )","metadata":{"execution":{"iopub.status.busy":"2021-08-20T18:28:14.456277Z","iopub.execute_input":"2021-08-20T18:28:14.456733Z","iopub.status.idle":"2021-08-20T18:28:14.53277Z","shell.execute_reply.started":"2021-08-20T18:28:14.456636Z","shell.execute_reply":"2021-08-20T18:28:14.531346Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### interesting\nprediction_2 is really weird, but on testing it gives best blend score (next cell)","metadata":{}},{"cell_type":"code","source":"import time\nstart_time = time.time()\n\n_target = 'loss'\n\npred = []\nval_rmse = []\nall_rmse = 0\n\nprint(X.shape)\n\nk = 5\n\nkfolds = KFold(n_splits = k, shuffle = True, random_state = R_SEED) # \n_k = 1\nfor train_index, test_index in kfolds.split(X):\n    print('------------------------------------------------k: ', _k, 'of', k)\n    X_train, X_val = X.iloc[train_index], X.iloc[test_index]\n    y_train, y_val = y.iloc[train_index], y.iloc[test_index]\n\n    lgbm_reg.fit(\n        X_train, \n        y_train,\n        eval_set = [(X_val, y_val)],\n        verbose = 500,\n        early_stopping_rounds = 3000,\n#             callbacks = [lgb.reset_parameter(learning_rate = [0.005] * 500 + [0.001] * 29500)]\n    )\n\n    f_pred = lgbm_reg.predict(X_val)\n    curr_rmse = mean_squared_error(y_val[_target].values, f_pred, squared = False)\n    print('curr_rmse: ', curr_rmse, 'fold: ',_k, 'of', k)\n    _k += 1\n    val_rmse.append(curr_rmse)\n\n    all_rmse += curr_rmse\n\n    _p = lgbm_reg.predict(my_X if not submit else test_df)\n    pred.append(_p)\n\n    if not submit:\n        test_rmse = mean_squared_error(my_y, _p, squared = False)\n        print('test_rmse: ', test_rmse)\n\n    print(\"Execution time: \", time.time() - start_time, \"secs\")\n    \n#     plot_fea_imp(lgbm_reg, 'LGBMRegressor')\n\nprint('all_rmse: ', all_rmse / len(pred))\n\n    \npred1 = np.sum(pred, axis = 0) / len(pred)\npred2 = np.transpose(np.matmul(np.transpose(pred), softmax(val_rmse)))\npred3 = np.transpose(np.matmul(np.transpose(pred), softmax(np.squeeze(np.full((1, len(pred)), 100)) - val_rmse)))\n\nif not submit:\n    end_rmse1 = mean_squared_error(my_y, pred1, squared = False)\n    print('end_rmse1: ', end_rmse1)\n    end_rmse2 = mean_squared_error(my_y, pred2, squared = False)\n    print('end_rmse2: ', end_rmse2)\n    end_rmse3 = mean_squared_error(my_y, pred3, squared = False)\n    print('end_rmse3: ', end_rmse3)\nelse:\n    submission_1 = submission_ex[['id']].copy()\n    submission_2 = submission_ex[['id']].copy()\n    submission_3 = submission_ex[['id']].copy()\n    submission_1[_target] = pred1\n    submission_2[_target] = pred2\n    submission_3[_target] = pred3\n    submission_1.to_csv('submission_1.csv', index=False)\n    submission_2.to_csv('submission_2_p.csv', index=False)\n    submission_3.to_csv('submission_3.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-08-20T18:27:51.633484Z","iopub.execute_input":"2021-08-20T18:27:51.633828Z"},"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}