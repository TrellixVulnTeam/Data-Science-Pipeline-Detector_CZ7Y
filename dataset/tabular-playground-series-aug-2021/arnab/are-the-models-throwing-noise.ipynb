{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### I have already shown ([here](https://www.kaggle.com/arnabbiswas1/classification-approach-lgbm-log-loss/comments)) that classification approach is not working well with this data. \n\n### But, since the target variable \"loss\" consists of 43 discrete integer values, we can create a **confusion matrix using the out of fold predictions** generated during cross validation. \n\nTo do this, I have loaded out-of-fold predictions and test predictions (i.e. prediction on test data) generated by my vanilla XGBoost Regressor (with StratifiedKFold). OOF score for this submission was 7.86117 and public LB score was 7.88749. \n\nNow, to create a confusion matrix, I need the OOF predictions to be of integer type (discrete values between 0 to 42). But, the predictions made by the regressor is float in nature. To handle this, I have rounded off the OOF predictions to the nearest integer values.\n\nReference Discussion: https://www.kaggle.com/c/tabular-playground-series-aug-2021/discussion/263862","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport seaborn as sns\n\nfrom sklearn.metrics import confusion_matrix, mean_squared_error","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-12T05:59:09.498567Z","iopub.execute_input":"2021-08-12T05:59:09.498991Z","iopub.status.idle":"2021-08-12T05:59:10.711168Z","shell.execute_reply.started":"2021-08-12T05:59:09.498906Z","shell.execute_reply":"2021-08-12T05:59:10.710101Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_confusion_matrix(cm_array, labels, figsize):\n    df_cm = pd.DataFrame(cm_array, index=[i for i in labels], columns=[i for i in labels])\n    plt.figure(figsize=figsize)\n    sns.heatmap(df_cm, annot=True, fmt='d')\n    plt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-12T05:59:10.712667Z","iopub.execute_input":"2021-08-12T05:59:10.712994Z","iopub.status.idle":"2021-08-12T05:59:10.718577Z","shell.execute_reply.started":"2021-08-12T05:59:10.712963Z","shell.execute_reply":"2021-08-12T05:59:10.717451Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Read the data","metadata":{}},{"cell_type":"code","source":"# Train and Test Data\ntrain_df = pd.read_csv('../input/tabular-playground-series-aug-2021/train.csv', index_col=\"id\")\ntest_df = pd.read_csv('../input/tabular-playground-series-aug-2021/test.csv', index_col=\"id\")\n\n# OOF Predictions from my submission\ndf_oof = pd.read_csv(\"../input/tpsaug2021oof785946/oof_xgb_benchmark_Stratifiedkfold_0804_1945_7.86117.csv\", index_col=\"id\")\n# Predictions on test data from my submission\ndf_submission = pd.read_csv(\"/kaggle/input/tpsaug2021oof785946/sub_xgb_benchmark_Stratifiedkfold_0804_1945_7.86117.csv\", index_col=\"id\")","metadata":{"execution":{"iopub.status.busy":"2021-08-12T05:59:10.720581Z","iopub.execute_input":"2021-08-12T05:59:10.720975Z","iopub.status.idle":"2021-08-12T05:59:22.552932Z","shell.execute_reply.started":"2021-08-12T05:59:10.720879Z","shell.execute_reply":"2021-08-12T05:59:22.551947Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Round off the OOF predictions to the nearest integer values & create a new column called loss_round. Now, there are two columns:\n- loss: OOF Prediction by the XGB Regressor\n- loss_round: Rounded values of the OOF predictions","metadata":{}},{"cell_type":"code","source":"df_oof = df_oof.rename(columns={\"0\": \"loss\"})\ndf_oof[\"loss_round\"] = df_oof.loss.round()\ndf_oof.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-12T05:59:22.55444Z","iopub.execute_input":"2021-08-12T05:59:22.554735Z","iopub.status.idle":"2021-08-12T05:59:22.578618Z","shell.execute_reply.started":"2021-08-12T05:59:22.554707Z","shell.execute_reply":"2021-08-12T05:59:22.577303Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Plot the confusion matrix using true value of \"loss\" from train data and OOF predictions (rounded values)","metadata":{}},{"cell_type":"code","source":"labels = list(np.sort(train_df.loss.unique()))\ncm_array = confusion_matrix(y_true=train_df.loss, y_pred=df_oof.loss_round, labels=labels)\n\nplot_confusion_matrix(cm_array, labels, figsize=(30, 20))","metadata":{"scrolled":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-12T05:59:22.579854Z","iopub.execute_input":"2021-08-12T05:59:22.58014Z","iopub.status.idle":"2021-08-12T05:59:30.177325Z","shell.execute_reply.started":"2021-08-12T05:59:22.580112Z","shell.execute_reply":"2021-08-12T05:59:30.176214Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"If the image is not displayed clearly, check it [here](https://imgur.com/a/obqT6yM).\n\nThe confusion matrix looks surprising. Are not we supposed to see lot of large numbers along the diagonal of the matrix? Here, most of the elements across the diagonal are zero. That means most of the classes (0 to 42) are NOT predicted properly. \n\nOn a closer look:\n- All instances of loss=0 are predicted as values between 1 to 17\n- All instances of loss=1 are predicted as values between 1 to 17 (Only one instance of 1 has been predicted as 1)\n- All instances of loss=7 are predicted as values between 2 to 15 (Only one instance of 1 has been predicted as 1)\n.....\n- All instances of loss=38 are predicted as values between 5 to 11 (None of the instances of 17 has been predicted as 17)\n\nand so on\n\nOverall, \n- whatever be the real value, the predicted values are lying between 1 to 17.\n- the higher the real value, the lesser is the spread of predicted values (some kind of Normal distribution?)\n\n#### Let's check the RMSE value for the rounded version of OOF predictions.","metadata":{}},{"cell_type":"code","source":"np.sqrt(mean_squared_error(y_true=train_df.loss, y_pred=df_oof.loss_round))","metadata":{"execution":{"iopub.status.busy":"2021-08-12T05:59:30.178642Z","iopub.execute_input":"2021-08-12T05:59:30.178942Z","iopub.status.idle":"2021-08-12T05:59:30.191054Z","shell.execute_reply.started":"2021-08-12T05:59:30.178912Z","shell.execute_reply":"2021-08-12T05:59:30.189782Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Let's check the RMSE value for the original version of OOF predictions.","metadata":{}},{"cell_type":"code","source":"np.sqrt(mean_squared_error(y_true=train_df.loss, y_pred=df_oof.loss))","metadata":{"execution":{"iopub.status.busy":"2021-08-12T05:59:30.192519Z","iopub.execute_input":"2021-08-12T05:59:30.192865Z","iopub.status.idle":"2021-08-12T05:59:30.202499Z","shell.execute_reply.started":"2021-08-12T05:59:30.192834Z","shell.execute_reply":"2021-08-12T05:59:30.200866Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### As we can see they are actaully pretty close. So, by rounding off the OOF predictions, I have not introduced lot of error.","metadata":{}},{"cell_type":"markdown","source":"#### Let me plot histograms of the following:\n\n- True value of OOF from train data (Red)\n- OOF Predictions (Green)\n- Predictions made using test data (Blue)\n\n#### From the figure below, it's very clear that **the model is not actually predicting anything close to the true value**\n\n## Update (12th August 2021)\nBased on Patrick's (@paddykb) comment, I have actually plotted the mean of the above three as well. They are so close that we can't distinguish the values.","metadata":{}},{"cell_type":"code","source":"figsize = (20, 10)\nbins=50\ntrain_df.loss.plot(\n    kind=\"hist\",\n    figsize=figsize,\n    label=\"loss_on_true\",\n    bins=bins,\n    alpha=0.4,\n    color=\"red\",\n    title=f\"\",\n)\ndf_oof.loss.plot(\n    kind=\"hist\",\n    figsize=figsize,\n    label=\"prediction_on_oof\",\n    bins=bins,\n    alpha=0.4,\n    color=\"darkgreen\",\n)\ndf_submission.loss.plot(\n    kind=\"hist\",\n    figsize=figsize,\n    label=\"prediction_on_test\",\n    bins=bins,\n    alpha=0.4,\n    color=\"blue\",\n)\n\nplt.axvline(train_df.loss.mean(), color='red', linestyle='solid', linewidth=1, alpha=0.3, label=\"mean of loss_on_true\")\nplt.axvline(df_oof.loss.mean(), color='darkgreen', linestyle='dotted', linewidth=2.5, alpha=0.3, label=\"mean of prediction_on_oof\")\nplt.axvline(df_submission.loss.mean(), color='blue', linestyle=':', linewidth=1, alpha=0.3, label=\"mean of prediction_on_test\")\n\nplt.legend()\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-12T05:59:30.204954Z","iopub.execute_input":"2021-08-12T05:59:30.205603Z","iopub.status.idle":"2021-08-12T05:59:30.991272Z","shell.execute_reply.started":"2021-08-12T05:59:30.205557Z","shell.execute_reply":"2021-08-12T05:59:30.990321Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Mean of loss in train data (True target): {train_df.loss.mean()}\")\nprint(f\"Mean of loss in OOF predictions : {df_oof.loss.mean()}\")\nprint(f\"Mean of loss in prediction in test : {df_submission.loss.mean()}\")","metadata":{"execution":{"iopub.status.busy":"2021-08-12T05:59:30.992859Z","iopub.execute_input":"2021-08-12T05:59:30.993429Z","iopub.status.idle":"2021-08-12T05:59:31.002202Z","shell.execute_reply.started":"2021-08-12T05:59:30.993384Z","shell.execute_reply":"2021-08-12T05:59:31.001264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### As we can see that the difference are on the third decimal point. I am not sure if we can call the difference as statistically significant or not.\n\n#### The model which I have used is a vanilla LGBM Regressor. I am wondering about the means for the models which are on the top of the LB (Please feel free to use this kernel and do a quick calculation)","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}}]}