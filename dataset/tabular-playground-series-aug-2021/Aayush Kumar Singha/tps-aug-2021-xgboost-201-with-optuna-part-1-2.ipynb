{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This is a follow up notebook of [XGBoost 101 - Baseline](https://www.kaggle.com/aayush26/tps-aug-2021-xgboost-101-baseline).\n\n\nThe scope of this notebook is to perform hyperparameter tuning and store the best hyperparameters found for XGB model. \n\nThe stored file can later be loaded into another notebook by using `Kaggle Add data` or directly copy-paste the best params displayed in the output cell.\n\nCheck [[TPS Aug 2021] XGBoost 201 - with Optuna Part 2/2](https://www.kaggle.com/aayush26/tps-aug-2021-xgboost-201-with-optuna-part-2-2/notebook), where I am loading the best hyperparameters acquired from this notebook.\n\n### Pre-requisite\n1. [GPU version] Change the accelerator to GPU in order to be able to exexute this notebook.\n2. [CPU version] Delete the following hyper params present in getXgbHyperparametersgetXgbHyperparameters(): `\"tree_method\": \"gpu_hist\"` and `\"booster\": 'gbtree'`","metadata":{}},{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport optuna\nfrom xgboost import XGBRegressor\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom optuna.samplers import TPESampler\nfrom sklearn.model_selection import KFold\noptuna.logging.set_verbosity(optuna.logging.WARNING)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-12T15:40:12.209125Z","iopub.execute_input":"2021-08-12T15:40:12.209513Z","iopub.status.idle":"2021-08-12T15:40:13.757896Z","shell.execute_reply.started":"2021-08-12T15:40:12.209431Z","shell.execute_reply":"2021-08-12T15:40:13.757068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load dataset","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('../input/tabular-playground-series-aug-2021/train.csv')\nprint('train shape:',train.shape)","metadata":{"execution":{"iopub.status.busy":"2021-08-12T15:40:18.835422Z","iopub.execute_input":"2021-08-12T15:40:18.835774Z","iopub.status.idle":"2021-08-12T15:40:25.253901Z","shell.execute_reply.started":"2021-08-12T15:40:18.835716Z","shell.execute_reply":"2021-08-12T15:40:25.25238Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train data\nX_train = train.drop(columns = ['loss','id'])\ny_train = train['loss'].values","metadata":{"execution":{"iopub.status.busy":"2021-08-12T15:40:25.255538Z","iopub.execute_input":"2021-08-12T15:40:25.255968Z","iopub.status.idle":"2021-08-12T15:40:25.326372Z","shell.execute_reply.started":"2021-08-12T15:40:25.25593Z","shell.execute_reply":"2021-08-12T15:40:25.325393Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Setup XGB hyperparameters for experiment","metadata":{}},{"cell_type":"code","source":"def getXgbHyperparameters(trial):\n    xgb_param = {\n            \"tree_method\": \"gpu_hist\",\n            'n_estimators': trial.suggest_int('n_estimators', 500, 2000, 100),\n            \"booster\": 'gbtree',\n            \"reg_lambda\": trial.suggest_int(\"reg_lambda\", 1, 100),\n            \"reg_alpha\": trial.suggest_int(\"reg_alpha\", 1, 100),\n            \"subsample\": trial.suggest_float(\"subsample\", 0.2, 1.0, step=0.1),\n            \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.2, 1.0, step=0.1),\n            \"max_depth\": trial.suggest_int(\"max_depth\", 3, 9),\n            \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 2, 10),\n            \"learning_rate\": 0.01,\n            \"gamma\": trial.suggest_float(\"gamma\", 0, 20)\n        }\n    return xgb_param","metadata":{"execution":{"iopub.status.busy":"2021-08-12T15:40:28.357004Z","iopub.execute_input":"2021-08-12T15:40:28.357371Z","iopub.status.idle":"2021-08-12T15:40:28.364135Z","shell.execute_reply.started":"2021-08-12T15:40:28.357336Z","shell.execute_reply":"2021-08-12T15:40:28.363102Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Define objective function","metadata":{}},{"cell_type":"code","source":"def objective(trial, X, y):\n    \n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=.3, random_state=1337)\n    \n    xgb_param = getXgbHyperparameters(trial)\n\n    eval_set = [(X_valid, y_valid)]\n    \n    fit_params = dict(eval_set=eval_set, \n                      eval_metric='rmse', \n                      early_stopping_rounds=100, \n                      verbose=False)\n    \n    xgb_regressor = XGBRegressor(**xgb_param)\n\n    # Fit/predict\n    xgb_regressor = xgb_regressor.fit(X_train, y_train)\n    preds = xgb_regressor.predict(X_valid)\n    \n    # Compute rmse\n    rmse = np.sqrt(mean_squared_error(y_valid, preds))\n    \n    return rmse","metadata":{"execution":{"iopub.status.busy":"2021-08-12T15:41:11.46933Z","iopub.execute_input":"2021-08-12T15:41:11.46965Z","iopub.status.idle":"2021-08-12T15:41:11.475839Z","shell.execute_reply.started":"2021-08-12T15:41:11.469621Z","shell.execute_reply":"2021-08-12T15:41:11.474846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Custom logging callback function","metadata":{}},{"cell_type":"code","source":"# Callback function to print log messages when the best trial is updated\n\ndef logging_callback(study, frozen_trial):\n    previous_best_value = study.user_attrs.get(\"previous_best_value\", None)\n    if previous_best_value != study.best_value:\n        study.set_user_attr(\"previous_best_value\", study.best_value)\n        print(\n            \"Trial {} finished with best value: {}. \".format(\n            frozen_trial.number,\n            frozen_trial.value\n            )\n        )","metadata":{"execution":{"iopub.status.busy":"2021-08-12T15:40:39.1095Z","iopub.execute_input":"2021-08-12T15:40:39.10988Z","iopub.status.idle":"2021-08-12T15:40:39.11557Z","shell.execute_reply.started":"2021-08-12T15:40:39.109848Z","shell.execute_reply":"2021-08-12T15:40:39.114772Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Initiate experiment to find best hyperparameters","metadata":{}},{"cell_type":"code","source":"%%time\n\nstudy = optuna.create_study(sampler=TPESampler(seed=1337), direction='minimize', study_name='xgb')\nfunc = lambda trial: objective(trial, X_train, y_train)\n\nstudy.optimize(func, timeout=60*180, callbacks=[logging_callback]) # timeout = seconds * minutes. Longer timeout will tend to lead to better hyperparameter tuning.","metadata":{"execution":{"iopub.status.busy":"2021-08-12T15:41:15.68602Z","iopub.execute_input":"2021-08-12T15:41:15.686365Z","iopub.status.idle":"2021-08-12T15:46:46.897602Z","shell.execute_reply.started":"2021-08-12T15:41:15.686333Z","shell.execute_reply":"2021-08-12T15:46:46.896894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"\\tBest value (rmse): {study.best_value:.5f}\")\nprint(f\"\\tBest params:\")\nfor key, value in study.best_params.items():\n    print(f\"\\t\\t{key}: {value}\")","metadata":{"execution":{"iopub.status.busy":"2021-08-12T15:47:24.898279Z","iopub.execute_input":"2021-08-12T15:47:24.898604Z","iopub.status.idle":"2021-08-12T15:47:24.905754Z","shell.execute_reply.started":"2021-08-12T15:47:24.898574Z","shell.execute_reply":"2021-08-12T15:47:24.904919Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Save the study containing the best hyperparameters for the XBG model","metadata":{}},{"cell_type":"code","source":"import joblib\n\njoblib.dump(study, \"xgb_study.pkl\")","metadata":{"execution":{"iopub.status.busy":"2021-08-12T15:47:36.847967Z","iopub.execute_input":"2021-08-12T15:47:36.848339Z","iopub.status.idle":"2021-08-12T15:47:36.865883Z","shell.execute_reply.started":"2021-08-12T15:47:36.848307Z","shell.execute_reply":"2021-08-12T15:47:36.864824Z"},"trusted":true},"execution_count":null,"outputs":[]}]}