{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport warnings\n\n#graphs and plots\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#model building + optimization\nimport lightgbm as lgb\nfrom sklearn.model_selection import train_test_split, KFold, cross_val_score\nfrom bayes_opt import BayesianOptimization\nfrom skopt import BayesSearchCV\nimport time\nimport sys\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-05T17:05:29.717597Z","iopub.execute_input":"2021-08-05T17:05:29.718018Z","iopub.status.idle":"2021-08-05T17:05:29.731702Z","shell.execute_reply.started":"2021-08-05T17:05:29.717986Z","shell.execute_reply":"2021-08-05T17:05:29.730286Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# LGBM + Bayesian Optimization\nAfter running this once and it not going to plan, I am running it again after going over the code more closely. I still don't think this method is that good so any comments and criticisms for improvement are welcome.\n","metadata":{}},{"cell_type":"markdown","source":"## Data Importing","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('../input/tabular-playground-series-aug-2021/train.csv', index_col = 'id')\ntest = pd.read_csv('../input/tabular-playground-series-aug-2021/test.csv', index_col = 'id')\nsample = pd.read_csv('../input/tabular-playground-series-aug-2021/sample_submission.csv', index_col = 'id')\ntest.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-05T17:06:25.490122Z","iopub.execute_input":"2021-08-05T17:06:25.49052Z","iopub.status.idle":"2021-08-05T17:06:33.47143Z","shell.execute_reply.started":"2021-08-05T17:06:25.49049Z","shell.execute_reply":"2021-08-05T17:06:33.470399Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# LGBM Baseline\n\nBaseline score with no optimization was 7.95860\nWe will now use Bayesian optimization to try to increase that in a short amount of time.\n### Data preprocessing\nSince we know from EDA (as seen here <a href=\"https://www.kaggle.com/subinium/tps-aug-simple-eda\">Some awesome EDA not by me</a>), that there are no missing data points, and we are assuming there are no categorical variables (though there are quite a few non-float ones), the data preprocessing is quite simple. ","metadata":{}},{"cell_type":"code","source":"X = train.drop(['loss'], axis = 1)\ny = train['loss']\n\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.2, random_state=42)\nX_train.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-05T17:06:40.061558Z","iopub.execute_input":"2021-08-05T17:06:40.061986Z","iopub.status.idle":"2021-08-05T17:06:40.427083Z","shell.execute_reply.started":"2021-08-05T17:06:40.061949Z","shell.execute_reply":"2021-08-05T17:06:40.425878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Parameter Optimization\nThis method has been inspired by : <a href=\"https://medium.com/analytics-vidhya/hyperparameters-optimization-for-lightgbm-catboost-and-xgboost-regressors-using-bayesian-6e7c495947a9\">Hyperparameters Optimization for LightGBM, CatBoost and XGBoost Regressors using Bayesian Optimization.</a>","metadata":{}},{"cell_type":"code","source":"%%time\n\ndef bayes_parameter_opt_lgb(X, y, init_round=15, opt_round=25, n_folds=3, random_seed=42, n_estimators = 10000, output_process=False):\n    #prep data\n    train_data = lgb.Dataset(X, label = y, free_raw_data=False)\n    \n    #parameters\n    def lgb_eval(learning_rate, num_leaves, feature_fraction, bagging_fraction, max_depth):\n        params = {'application':'regression','metric':'rmse', 'boosting':'gbdt', 'num_iterations': 5000, 'early_stopping_rounds': 500 }\n        params['learning_rate'] = max(min(learning_rate, 1), 0)\n        params[\"num_leaves\"] = int(round(num_leaves))\n        params['feature_fraction'] = max(min(feature_fraction, 1), 0)\n        params['bagging_fraction'] = max(min(bagging_fraction, 1), 0)\n        params['max_depth'] = int(round(max_depth))\n\n        \n        \n        \n        cv_result = lgb.cv(params, train_data, nfold=n_folds, seed=random_seed, stratified = True, verbose_eval=200, metrics=['rmse'])\n        return max(cv_result['rmse-mean'])\n    \n    lgbBO = BayesianOptimization(lgb_eval,{'learning_rate': (0.01, 0.5),\n                                            'num_leaves': (10, 200),\n                                            'feature_fraction': (0.1, 1.0),\n                                            'bagging_fraction': (0.1, 1.0),\n                                            'max_depth': (1, 30),\n                                           }, random_state=42)\n    \n    #n_iter: How many steps of bayesian optimization you want to perform. The more steps the more likely to find a good maximum you are.\n    #init_points: How many steps of random exploration you want to perform. Random exploration can help by diversifying the exploration space.   \n    \n    lgbBO.maximize(init_points=init_round, n_iter=opt_round)\n    return lgbBO.max['params']\n\nopt_params = bayes_parameter_opt_lgb(X_train, y_train, init_round=5, opt_round=10, n_folds=3, random_seed=42,n_estimators=10000)","metadata":{"execution":{"iopub.status.busy":"2021-08-05T19:54:37.557145Z","iopub.execute_input":"2021-08-05T19:54:37.557575Z","iopub.status.idle":"2021-08-05T20:03:54.543288Z","shell.execute_reply.started":"2021-08-05T19:54:37.557541Z","shell.execute_reply":"2021-08-05T20:03:54.542338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Optimal parameters","metadata":{}},{"cell_type":"code","source":"# We have optimal parameters so now we need to do a cv test with the optimal parameterts\nopt_params","metadata":{"execution":{"iopub.status.busy":"2021-08-05T20:04:30.823801Z","iopub.execute_input":"2021-08-05T20:04:30.824363Z","iopub.status.idle":"2021-08-05T20:04:30.830169Z","shell.execute_reply.started":"2021-08-05T20:04:30.824299Z","shell.execute_reply":"2021-08-05T20:04:30.829307Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"op = {'task':'train','application':'regression','metric':'rmse', 'boosting':'gbdt', 'num_iterations': 5000, 'early_stopping_rounds': 500,\n    'bagging_fraction': 0.3253848734026177,\n 'feature_fraction': 0.30860836505782663,\n 'learning_rate': 0.08689261851358254,\n 'max_depth': 2,\n 'num_leaves': 179\n     }\n","metadata":{"execution":{"iopub.status.busy":"2021-08-05T20:04:54.976666Z","iopub.execute_input":"2021-08-05T20:04:54.977091Z","iopub.status.idle":"2021-08-05T20:04:54.98301Z","shell.execute_reply.started":"2021-08-05T20:04:54.977053Z","shell.execute_reply":"2021-08-05T20:04:54.981863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Modeling\nUsing the optimal parameters detailed above we train our LGB model before using them to predict from the test set and submitting","metadata":{}},{"cell_type":"code","source":"trn_data = lgb.Dataset(X_train, label = y_train)\nval_data = lgb.Dataset(X_val, label = y_val, reference = trn_data)\nregressor = lgb.train(op, trn_data, verbose_eval = 50, valid_sets = val_data)","metadata":{"execution":{"iopub.status.busy":"2021-08-05T20:04:58.302911Z","iopub.execute_input":"2021-08-05T20:04:58.303336Z","iopub.status.idle":"2021-08-05T20:05:20.044196Z","shell.execute_reply.started":"2021-08-05T20:04:58.303298Z","shell.execute_reply":"2021-08-05T20:05:20.043076Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = regressor.predict(test)","metadata":{"execution":{"iopub.status.busy":"2021-08-05T20:06:07.66023Z","iopub.execute_input":"2021-08-05T20:06:07.660663Z","iopub.status.idle":"2021-08-05T20:06:16.493284Z","shell.execute_reply.started":"2021-08-05T20:06:07.660627Z","shell.execute_reply":"2021-08-05T20:06:16.492339Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub = pd.DataFrame(index=test.index)\nsub['loss'] = y_pred\nsub.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-05T20:06:22.00194Z","iopub.execute_input":"2021-08-05T20:06:22.002337Z","iopub.status.idle":"2021-08-05T20:06:22.018245Z","shell.execute_reply.started":"2021-08-05T20:06:22.002296Z","shell.execute_reply":"2021-08-05T20:06:22.016358Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub.describe()","metadata":{"execution":{"iopub.status.busy":"2021-08-05T20:06:25.985433Z","iopub.execute_input":"2021-08-05T20:06:25.985858Z","iopub.status.idle":"2021-08-05T20:06:26.010786Z","shell.execute_reply.started":"2021-08-05T20:06:25.985824Z","shell.execute_reply":"2021-08-05T20:06:26.009659Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub.to_csv('submission.csv')","metadata":{"execution":{"iopub.status.busy":"2021-08-05T20:06:29.95457Z","iopub.execute_input":"2021-08-05T20:06:29.955127Z","iopub.status.idle":"2021-08-05T20:06:30.556191Z","shell.execute_reply.started":"2021-08-05T20:06:29.955088Z","shell.execute_reply":"2021-08-05T20:06:30.554911Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}