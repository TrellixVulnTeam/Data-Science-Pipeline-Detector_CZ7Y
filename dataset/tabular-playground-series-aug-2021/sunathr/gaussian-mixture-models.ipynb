{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"In this notebook, I tried to use the concept of Gaussian Mixture Models to get the relationship between each feature and the final loss. This was used to predict the loss for the test set.\n\nStarting with the typical data imports:","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom pandas import DataFrame, Series\nimport sklearn \nimport os\n'''for dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))'''\nX=pd.read_csv('/kaggle/input/tabular-playground-series-aug-2021/train.csv')\nY=X['loss']\nX.drop(['id','loss'],axis=1,inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we see the training data set","metadata":{}},{"cell_type":"code","source":"X.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-06T13:04:29.453575Z","iopub.execute_input":"2021-08-06T13:04:29.45399Z","iopub.status.idle":"2021-08-06T13:04:29.490867Z","shell.execute_reply.started":"2021-08-06T13:04:29.453956Z","shell.execute_reply":"2021-08-06T13:04:29.48947Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we can find how the final loss varies with each of the hundred features. I checked the relationship between each feature and the final loss for the first one thousand training samples.","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nfig, axs = plt.subplots(10, 10,figsize=[100, 100])\nfor i in range(10):\n    for j in range(10):\n        axs[i,j].plot(X[X.columns[(10*i)+j]][0:1000],Y[0:1000])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Interesting! It would appear that there are definite trends and shapes. I was curious on whether a similar relationship is displayed across the entire dataset. So I selected another subset of a thousand training samples (from 15000-16000) and checked.","metadata":{}},{"cell_type":"code","source":"fig, axs = plt.subplots(10, 10,figsize=[100, 100])\nfor i in range(10):\n    for j in range(10):\n        axs[i,j].plot(X[X.columns[(10*i)+j]][15000:16000],Y[15000:16000])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Surprisingly (or not surprisingly), the features show a very similar trend and distribution across the dataset with respect to their relationship with the final loss value.\n\nBuilding on this, I tried to fit a Gaussian Mixture Model onto the relationship between each feature and the final loss. I found that setting the number of components to be the maximum loss value was helpful in modelling a suitable distribution.","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\nfrom sklearn.mixture import GaussianMixture\nmodel_list=[]\nfor i in range(10):\n    for j in range(10):\n        s=GaussianMixture(n_components=int(max(Y[0:1000])), random_state=10).fit(np.vstack(X[X.columns[(10*i)+j]][0:1000]),np.vstack(Y[0:1000]))\n        model_list.append(s)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To find the best weightage to give to each of the one hundred predictions, I checked the best value to divide the sum of predictions such that mean squared error was minimum. The validation set was the dataset sampled from indices 1000-2000.","metadata":{}},{"cell_type":"code","source":"s=np.zeros((1,1000))\nfor i in range(100):\n    s=s+model_list[i].predict(np.vstack(X[X.columns[i]][1000:2000]))\nmis=[]\nfor i in range(1,1000):\n    r=s/i\n    rms=mean_squared_error(r[0],Y[1000:2000], squared=False)\n    mis.append(rms)\nprint(mis.index(min(mis)))\nprint(min(mis))","metadata":{"execution":{"iopub.status.busy":"2021-08-06T12:57:11.743611Z","iopub.execute_input":"2021-08-06T12:57:11.744017Z","iopub.status.idle":"2021-08-06T12:57:12.736527Z","shell.execute_reply.started":"2021-08-06T12:57:11.743984Z","shell.execute_reply":"2021-08-06T12:57:12.735086Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It appears that dividing by 279 yields the lowest RMS value. Applying this information to a larger subset of the test data also yields similar RMS.","metadata":{}},{"cell_type":"code","source":"s=np.zeros((1,10000))\nfor i in range(100):\n    s=s+model_list[i].predict(np.vstack(X[X.columns[i]][3000:13000]))\ns=s/279\nrms=mean_squared_error(s[0],Y[3000:13000], squared=False)\nprint(rms)","metadata":{"execution":{"iopub.status.busy":"2021-08-06T12:58:07.489633Z","iopub.execute_input":"2021-08-06T12:58:07.490041Z","iopub.status.idle":"2021-08-06T12:58:11.733754Z","shell.execute_reply.started":"2021-08-06T12:58:07.490008Z","shell.execute_reply":"2021-08-06T12:58:11.73253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Okay, time to implement this idea for the entire training and test data. Let's train 100 GMMs on the entire training set.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.mixture import GaussianMixture\nmodel_list=[]\nfor i in range(10):\n    for j in range(10):\n        s=GaussianMixture(n_components=int(max(Y[0:250000])), random_state=5).fit(np.vstack(X[X.columns[(10*i)+j]][0:250000]),np.vstack(Y[0:250000]))\n        model_list.append(s)\n        print(str(10*i+j)) #This could take a while to run, so this print statement serves as a timer","metadata":{"execution":{"iopub.status.busy":"2021-08-06T13:05:00.391524Z","iopub.execute_input":"2021-08-06T13:05:00.391945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let's sum up every model's predictions of the test set and divide by 279. We can place the final result into a dataframe 'concated'.","metadata":{}},{"cell_type":"code","source":"s=np.zeros((1,150000))\nX_test=pd.read_csv('/kaggle/input/tabular-playground-series-aug-2021/test.csv')\nids=X_test['id'][0:150000]\nX_test.drop(['id'],axis=1,inplace=True)\nfor i in range(100):\n    s=s+model_list[i].predict(np.vstack(X_test[X_test.columns[i]][0:150000]))\n    print(i)\ns=s/279\nap=pd.DataFrame(s[0])\nconcated=pd.concat([ids,ap],axis=1)\nconcated.rename(columns={0:'loss'},inplace=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In the final step, we convert the dataframe to a csv.","metadata":{}},{"cell_type":"code","source":"concated.to_csv('gaussian_sub',index=False)","metadata":{},"execution_count":null,"outputs":[]}]}