{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Random Forest and XGBoost on Tabular Playground August 2021\nIn this notebook, I will train an XGBoost and a Random Forest model on tabular data and evaluate their accuracies.","metadata":{}},{"cell_type":"markdown","source":"## Library and Data Imports\nI'll start with the basic library and data imports.","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-18T01:44:42.10108Z","iopub.execute_input":"2021-08-18T01:44:42.101826Z","iopub.status.idle":"2021-08-18T01:44:42.110974Z","shell.execute_reply.started":"2021-08-18T01:44:42.101785Z","shell.execute_reply":"2021-08-18T01:44:42.109946Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Loading in the train and test data\ntrain_data = pd.read_csv('../input/tabular-playground-series-aug-2021/train.csv')\ntest_data = pd.read_csv('../input/tabular-playground-series-aug-2021/test.csv')\n\nX = train_data.iloc[:,1:101]\ny = train_data['loss']\nX.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-18T01:44:42.139615Z","iopub.execute_input":"2021-08-18T01:44:42.139989Z","iopub.status.idle":"2021-08-18T01:44:50.051686Z","shell.execute_reply.started":"2021-08-18T01:44:42.139957Z","shell.execute_reply":"2021-08-18T01:44:50.050558Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Attempt to use feature engineering with mutual info regression, but this took up too much space\n# from sklearn.feature_selection import mutual_info_regression\n\n# # Function code from Mutual Information lesson in Kaggle Feature Engineering course\n# def make_mi_scores(X, y, discrete_features):\n#     '''create series for the mutual info scores for each feature'''\n#     mi_scores = mutual_info_regression(X, y, discrete_features=discrete_features)\n#     mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n#     mi_scores = mi_scores.sort_values(ascending=False)\n#     return mi_scores\n\n# # Remove the discrete features and find the MI scores\n# X = X.loc[:,X.dtypes == float]\n# mi_scores = make_mi_scores(X, y, False)\n# mi_scores.head(20)","metadata":{"execution":{"iopub.status.busy":"2021-08-18T01:44:50.053133Z","iopub.execute_input":"2021-08-18T01:44:50.053427Z","iopub.status.idle":"2021-08-18T01:44:50.058143Z","shell.execute_reply.started":"2021-08-18T01:44:50.053398Z","shell.execute_reply":"2021-08-18T01:44:50.056961Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size = 0.2, random_state = 1)\nprint(len(X_train), len(X_valid), len(y_train), len(y_valid))","metadata":{"execution":{"iopub.status.busy":"2021-08-18T01:44:50.060069Z","iopub.execute_input":"2021-08-18T01:44:50.060379Z","iopub.status.idle":"2021-08-18T01:44:50.332843Z","shell.execute_reply.started":"2021-08-18T01:44:50.060349Z","shell.execute_reply":"2021-08-18T01:44:50.331751Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training the Model\nHere I'll import and train the Random Forest and XGBoost models on the tabular data.","metadata":{}},{"cell_type":"code","source":"# models to be used\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBRegressor\n# metric for calculating accuracy\nfrom sklearn.metrics import mean_squared_error","metadata":{"execution":{"iopub.status.busy":"2021-08-18T01:44:50.334341Z","iopub.execute_input":"2021-08-18T01:44:50.334707Z","iopub.status.idle":"2021-08-18T01:44:50.340214Z","shell.execute_reply.started":"2021-08-18T01:44:50.334676Z","shell.execute_reply":"2021-08-18T01:44:50.339138Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# creating the RandomForestRegressor model with 500 estimators and 4 jobs (for faster processing)\nrf_model = RandomForestRegressor(n_estimators = 500, n_jobs = 4, random_state = 0)\nrf_model.fit(X_train, y_train)\n# Rounding predictions\npredictions_rf = rf_model.predict(X_valid).round()\nprint(\"Random Forest error: \" + str(mean_squared_error(y_valid, predictions_rf)))","metadata":{"execution":{"iopub.status.busy":"2021-08-18T01:44:50.341614Z","iopub.execute_input":"2021-08-18T01:44:50.341926Z","iopub.status.idle":"2021-08-18T04:52:08.511005Z","shell.execute_reply.started":"2021-08-18T01:44:50.341895Z","shell.execute_reply":"2021-08-18T04:52:08.509264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Non-rounding predictions\npredictions_rf = rf_model.predict(X_valid)\nprint(\"Random Forest error: \" + str(mean_squared_error(y_valid, predictions_rf)))","metadata":{"execution":{"iopub.status.busy":"2021-08-18T04:52:08.514511Z","iopub.execute_input":"2021-08-18T04:52:08.514988Z","iopub.status.idle":"2021-08-18T04:52:16.771122Z","shell.execute_reply.started":"2021-08-18T04:52:08.514933Z","shell.execute_reply":"2021-08-18T04:52:16.770032Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# creating the XGBRegressor model with 500 estimators, a learning_rate of 0.05,\n# 5 rounds for early stopping, and 4 jobs (for faster processing)\nxgb_model = XGBRegressor(n_estimators = 500, learning_rate = 0.05, n_jobs = 4, random_state = 0)\nxgb_model.fit(X_train, y_train,\n              early_stopping_rounds = 5,\n              eval_set = [(X_valid, y_valid)],\n              verbose = False)\n# Rounding predictions\npredictions_xgb = xgb_model.predict(X_valid).round()\nprint(\"XGBoost Error: \" + str(mean_squared_error(y_valid, predictions_xgb)))","metadata":{"execution":{"iopub.status.busy":"2021-08-18T04:52:16.774854Z","iopub.execute_input":"2021-08-18T04:52:16.775215Z","iopub.status.idle":"2021-08-18T05:00:39.959442Z","shell.execute_reply.started":"2021-08-18T04:52:16.775182Z","shell.execute_reply":"2021-08-18T05:00:39.958583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Non-rounding predictions\npredictions_xgb = xgb_model.predict(X_valid)\nprint(\"XGBoost Error: \" + str(mean_squared_error(y_valid, predictions_xgb)))","metadata":{"execution":{"iopub.status.busy":"2021-08-18T05:00:39.962659Z","iopub.execute_input":"2021-08-18T05:00:39.963309Z","iopub.status.idle":"2021-08-18T05:00:40.134156Z","shell.execute_reply.started":"2021-08-18T05:00:39.963265Z","shell.execute_reply":"2021-08-18T05:00:40.133308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The better model seems to be the **XGBoost model** with non-rounding, so I will use that for the final submission.","metadata":{}},{"cell_type":"markdown","source":"## Final Training and Submission\nHere I'll do the final training and submission with the better of the two models (XGBoostRegressor).","metadata":{}},{"cell_type":"code","source":"# create and train the final model\nfinal_model = XGBRegressor(n_estimators = 500, learning_rate = 0.05, n_jobs = 4, random_state = 0)\nfinal_model.fit(X, y, verbose = False)","metadata":{"execution":{"iopub.status.busy":"2021-08-18T05:00:40.135763Z","iopub.execute_input":"2021-08-18T05:00:40.136364Z","iopub.status.idle":"2021-08-18T05:23:43.966775Z","shell.execute_reply.started":"2021-08-18T05:00:40.136315Z","shell.execute_reply":"2021-08-18T05:23:43.965179Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_predictions = final_model.predict(test_data.iloc[:,1:])\nsubmission = pd.DataFrame({\"id\": list(range(250000,400000)),\n                          \"loss\": final_predictions})\nsubmission.to_csv(\"submission.csv\", index=False, header=True)\nprint(\"Final submission created!\")","metadata":{"execution":{"iopub.status.busy":"2021-08-18T06:03:17.971678Z","iopub.execute_input":"2021-08-18T06:03:17.972095Z","iopub.status.idle":"2021-08-18T06:03:18.049004Z","shell.execute_reply.started":"2021-08-18T06:03:17.972005Z","shell.execute_reply":"2021-08-18T06:03:18.047329Z"},"trusted":true},"execution_count":null,"outputs":[]}]}