{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n! pip install rich\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom rich import print\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-16T11:15:33.75766Z","iopub.execute_input":"2021-08-16T11:15:33.758265Z","iopub.status.idle":"2021-08-16T11:15:41.019213Z","shell.execute_reply.started":"2021-08-16T11:15:33.758207Z","shell.execute_reply":"2021-08-16T11:15:41.018218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2 style=\"color:red;\"><center>Objecives</center></h2>\nTo determine the function that fits the data and predictes loss for new data.","metadata":{}},{"cell_type":"code","source":"#imports\nimport numpy as np\nimport pandas as pd\n\n#Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#processing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler","metadata":{"execution":{"iopub.status.busy":"2021-08-16T11:15:41.020818Z","iopub.execute_input":"2021-08-16T11:15:41.021111Z","iopub.status.idle":"2021-08-16T11:15:42.27104Z","shell.execute_reply.started":"2021-08-16T11:15:41.021079Z","shell.execute_reply":"2021-08-16T11:15:42.269832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2 style=\"color:red;\"><center>Understanding Data</center></h2>","metadata":{}},{"cell_type":"code","source":"#Loading the data\ndtrain = pd.read_csv(\"/kaggle/input/tabular-playground-series-aug-2021/train.csv\")\ndtest = pd.read_csv(\"/kaggle/input/tabular-playground-series-aug-2021/test.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-08-16T11:15:42.273098Z","iopub.execute_input":"2021-08-16T11:15:42.273416Z","iopub.status.idle":"2021-08-16T11:15:53.60028Z","shell.execute_reply.started":"2021-08-16T11:15:42.273384Z","shell.execute_reply":"2021-08-16T11:15:53.599126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Training dataframe\ndtrain.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-16T11:15:53.601907Z","iopub.execute_input":"2021-08-16T11:15:53.602233Z","iopub.status.idle":"2021-08-16T11:15:53.649972Z","shell.execute_reply.started":"2021-08-16T11:15:53.602202Z","shell.execute_reply":"2021-08-16T11:15:53.648954Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"description = dtrain.describe()\ndescription.applymap(\"{0:.2f}\".format)","metadata":{"execution":{"iopub.status.busy":"2021-08-16T11:15:53.651334Z","iopub.execute_input":"2021-08-16T11:15:53.651651Z","iopub.status.idle":"2021-08-16T11:15:54.947746Z","shell.execute_reply.started":"2021-08-16T11:15:53.651617Z","shell.execute_reply":"2021-08-16T11:15:54.946346Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Observe that the features in the dataset are not in a specifi range so we require feature scaling so that our algorithms converge efficiently.","metadata":{}},{"cell_type":"code","source":"#list of features\nfeatures = dtrain.columns[1:-1]\n\ninfo_dtrain = dtrain.dtypes\ninfo_dtest = dtest.dtypes\n\n#exclude the id column\nint_features = list(filter(lambda x: (x[1]=='int64'), zip(dtrain.columns, info_dtrain)))[1:]\nprint(int_features)","metadata":{"execution":{"iopub.status.busy":"2021-08-16T11:15:54.949266Z","iopub.execute_input":"2021-08-16T11:15:54.949718Z","iopub.status.idle":"2021-08-16T11:15:54.962955Z","shell.execute_reply.started":"2021-08-16T11:15:54.949667Z","shell.execute_reply":"2021-08-16T11:15:54.961654Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The loss is an integer between 0 and 42.\n\nSix features are purely integer in the above features.","metadata":{}},{"cell_type":"code","source":"#Principal Component Analysis of Data\n#Before we proceed we will drop the id and loss columns\n#and split the set for validation with 30% test size.\nx_train, x_test, y_train, y_test = train_test_split(dtrain[features], dtrain.loss, test_size=0.3, random_state=0)\n\n#Standard Scaler\nscaler = StandardScaler()\n\n# Fit on training set\nscaler.fit(x_train)\n\n# Apply transform to both the training set and the test set.\nx_train = scaler.transform(x_train)\nx_test = scaler.transform(x_test)\nx_dtest = scaler.transform(dtest.drop([\"id\"], axis=1))","metadata":{"execution":{"iopub.status.busy":"2021-08-16T11:15:54.96446Z","iopub.execute_input":"2021-08-16T11:15:54.964912Z","iopub.status.idle":"2021-08-16T11:15:56.052563Z","shell.execute_reply.started":"2021-08-16T11:15:54.964861Z","shell.execute_reply":"2021-08-16T11:15:56.051498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3 style=\"color:red;\"><center>Reducing Dimension PCA</center></h3>","metadata":{}},{"cell_type":"code","source":"#PCA\n\ndef reduce_dimension(array, dim=2):\n    \"\"\"\n    Defining the outpout size for pca\n    \"\"\"\n    \n    #Dimension Reduction\n    pca = PCA(n_components=dim)\n    \n    #fit to the train set\n    pca.fit(array)\n    \n    #return the pca object\n    return pca\n\n#Call reduce dimension on fatures for reduction to 2 features\npca = reduce_dimension(x_train)\nx_train_pca = pca.transform(x_train)\n\n#validation set\ny_pca = pca.transform(x_test)\n\n#Actual test set\nxtest_pca = pca.transform(x_dtest)","metadata":{"execution":{"iopub.status.busy":"2021-08-16T11:15:56.055124Z","iopub.execute_input":"2021-08-16T11:15:56.055504Z","iopub.status.idle":"2021-08-16T11:15:58.295786Z","shell.execute_reply.started":"2021-08-16T11:15:56.055468Z","shell.execute_reply":"2021-08-16T11:15:58.294654Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Build the dataframe from pca arrays\ndtrain_pca = pd.DataFrame(np.column_stack((x_train_pca, y_train)), columns=[\"x_pca\", \"y_pca\", \"loss\"])\ndtest_pca = pd.DataFrame(xtest_pca, columns=[\"test_x\", \"test_y\"])\ndtrain_pca[\"loss\"]=dtrain_pca[\"loss\"].astype(int)\nprint(dtrain_pca.head())\nprint(dtest_pca.head())","metadata":{"execution":{"iopub.status.busy":"2021-08-16T11:15:58.298388Z","iopub.execute_input":"2021-08-16T11:15:58.299239Z","iopub.status.idle":"2021-08-16T11:15:58.333857Z","shell.execute_reply.started":"2021-08-16T11:15:58.299185Z","shell.execute_reply":"2021-08-16T11:15:58.33269Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\n\nrows_to_plot = 10000\n\n#Visualizing the 2D data obtaibed through PCA\nfig = plt.figure(figsize=(24, 8))\nfig.suptitle(\"Training and test set distributions\")\n\nax = [fig.add_subplot(1, 3, 1), fig.add_subplot(1, 3, 2) ,fig.add_subplot(1, 3, 3, projection=\"3d\")]\n\n#using sequential colomap for loss\nax[2].scatter(dtrain_pca[\"x_pca\"][:rows_to_plot], dtrain_pca[\"y_pca\"][:rows_to_plot], c=dtrain_pca[\"loss\"][:rows_to_plot], cmap=\"inferno\")\nax[2].set_title(\"3D Plot\")\n\ng1 = sns.kdeplot(x=\"x_pca\",y=\"y_pca\",data=dtrain_pca[:rows_to_plot],palette=\"hls\", ax=ax[0])\nax[0].set_title(\"Train Data\")\n\ng2 = sns.kdeplot(x=xtest_pca[:, :1].reshape(1, -1)[0][:rows_to_plot],y=xtest_pca[:, 1:].reshape(1, -1)[0][:rows_to_plot], ax=ax[1])\nax[1].set_title(\"Test Data\");\n\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-16T11:15:58.335815Z","iopub.execute_input":"2021-08-16T11:15:58.336662Z","iopub.status.idle":"2021-08-16T11:16:17.952007Z","shell.execute_reply.started":"2021-08-16T11:15:58.3366Z","shell.execute_reply":"2021-08-16T11:16:17.950874Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Both the train data and have similar distributions on pca with 2 components. So we will have less headache while trying to optimize.","metadata":{}},{"cell_type":"code","source":"#Visualizing the 2D data obtaibed through PCA\ng3 = sns.FacetGrid(dtrain_pca,hue=\"loss\", palette=\"hls\",height=8)\ng3.map(sns.scatterplot, \"x_pca\",\"y_pca\").add_legend();","metadata":{"execution":{"iopub.status.busy":"2021-08-16T11:16:17.95333Z","iopub.execute_input":"2021-08-16T11:16:17.953624Z","iopub.status.idle":"2021-08-16T11:16:23.040436Z","shell.execute_reply.started":"2021-08-16T11:16:17.953596Z","shell.execute_reply":"2021-08-16T11:16:23.039333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This is not only for beautification purpose but also observe that we have maximum loss at the center and it decreases as we move further. So in 3 dimensions we would have a mountain shaped distributions. We probably have a name for it.","metadata":{}},{"cell_type":"code","source":"g3 = sns.FacetGrid(dtrain_pca, palette=\"hls\",height=8, aspect=2)\ng3.map(sns.kdeplot, \"x_pca\", color=\"g\")\ng3.map(sns.kdeplot, \"y_pca\", color=\"b\")\ng3.map(sns.kdeplot, \"loss\", color=\"r\")\ng3.set(xticks=range(-10, 20, 2))","metadata":{"execution":{"iopub.status.busy":"2021-08-16T11:16:23.042019Z","iopub.execute_input":"2021-08-16T11:16:23.042646Z","iopub.status.idle":"2021-08-16T11:16:26.336022Z","shell.execute_reply.started":"2021-08-16T11:16:23.042597Z","shell.execute_reply":"2021-08-16T11:16:26.334825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Observe the distribution we will comback to it.","metadata":{}},{"cell_type":"markdown","source":"# Linear Regression","metadata":{}},{"cell_type":"code","source":"#Invovking linear model\nfrom sklearn.linear_model import LinearRegression\nlinear_model = LinearRegression().fit(x_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2021-08-16T11:16:26.337549Z","iopub.execute_input":"2021-08-16T11:16:26.337999Z","iopub.status.idle":"2021-08-16T11:16:26.821162Z","shell.execute_reply.started":"2021-08-16T11:16:26.337951Z","shell.execute_reply":"2021-08-16T11:16:26.820252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Cross Validation\npredicted_test_loss = linear_model.predict(x_test)","metadata":{"execution":{"iopub.status.busy":"2021-08-16T11:16:26.824792Z","iopub.execute_input":"2021-08-16T11:16:26.825113Z","iopub.status.idle":"2021-08-16T11:16:26.843162Z","shell.execute_reply.started":"2021-08-16T11:16:26.825083Z","shell.execute_reply":"2021-08-16T11:16:26.841453Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import mean_squared_error as RMSE\n\ndef rmse_plot(y_true, y_pred):\n    rmse = np.sqrt(RMSE(y_true,y_pred))\n    return rmse\n\ndef rmse_plots(y_test, predicted_test_loss):\n    rmse = np.sqrt((1/len(y_test))*(sum(y_test-predicted_test_loss)))\n    fig = plt.figure()\n    plt.plot(y_test, predicted_test_loss)","metadata":{"execution":{"iopub.status.busy":"2021-08-16T11:16:26.845049Z","iopub.execute_input":"2021-08-16T11:16:26.845549Z","iopub.status.idle":"2021-08-16T11:16:26.856497Z","shell.execute_reply.started":"2021-08-16T11:16:26.845503Z","shell.execute_reply":"2021-08-16T11:16:26.853463Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#RMSE ERROR\nprint(f\"The Error for the train set during is observed to be {rmse_plot(y_train, linear_model.predict(x_train))}\")\nprint(f\"The Error for the test set during cross vaidation is observed to be {rmse_plot(y_test, predicted_test_loss)}\")","metadata":{"execution":{"iopub.status.busy":"2021-08-16T11:16:26.858222Z","iopub.execute_input":"2021-08-16T11:16:26.858872Z","iopub.status.idle":"2021-08-16T11:16:26.91091Z","shell.execute_reply.started":"2021-08-16T11:16:26.858815Z","shell.execute_reply":"2021-08-16T11:16:26.909774Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"RMSE is still \"high\" very much. We need to bring it down looking at the principal components we will require some non-linear function.","metadata":{}},{"cell_type":"code","source":"#Now fit the data on PCA\nlinear_model_pca = LinearRegression().fit(x_train_pca, y_train)\npredicted_test_loss_pca = linear_model_pca.predict(y_pca)\nprint(f\"The Error for the test set during cross vaidation on pca is observed to be {rmse_plot(y_test, predicted_test_loss_pca)}\")","metadata":{"execution":{"iopub.status.busy":"2021-08-16T11:16:26.912617Z","iopub.execute_input":"2021-08-16T11:16:26.913326Z","iopub.status.idle":"2021-08-16T11:16:27.020437Z","shell.execute_reply.started":"2021-08-16T11:16:26.913271Z","shell.execute_reply":"2021-08-16T11:16:27.019249Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Bayesian Ridge","metadata":{}},{"cell_type":"code","source":"#Invovking linear model Ridge\nfrom sklearn.linear_model import BayesianRidge\nbayesridge = BayesianRidge(verbose=True).fit(x_train, y_train)\npredicted_loss_ridge = bayesridge.predict(x_test)\nprint(f\"The Error for the test set during cross vaidation with Bayesian Ridge is observed to be {rmse_plot(y_test, predicted_loss_ridge)}\")","metadata":{"execution":{"iopub.status.busy":"2021-08-16T11:16:27.022532Z","iopub.execute_input":"2021-08-16T11:16:27.023449Z","iopub.status.idle":"2021-08-16T11:16:27.929863Z","shell.execute_reply.started":"2021-08-16T11:16:27.023388Z","shell.execute_reply":"2021-08-16T11:16:27.928555Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Predicting on the unseen value we don't know loss here\ny_predicted = bayesridge.predict(x_dtest)\nresult_f0 = pd.DataFrame({\"id\":dtest[\"id\"],\"loss\":y_predicted})\ndtest_pca[\"loss\"] = y_predicted","metadata":{"execution":{"iopub.status.busy":"2021-08-16T11:16:27.931546Z","iopub.execute_input":"2021-08-16T11:16:27.932352Z","iopub.status.idle":"2021-08-16T11:16:27.965464Z","shell.execute_reply.started":"2021-08-16T11:16:27.9323Z","shell.execute_reply":"2021-08-16T11:16:27.963973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result_f0.to_csv(\"./submit_bayes.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2021-08-16T11:16:27.967206Z","iopub.execute_input":"2021-08-16T11:16:27.968081Z","iopub.status.idle":"2021-08-16T11:16:28.563314Z","shell.execute_reply.started":"2021-08-16T11:16:27.968025Z","shell.execute_reply":"2021-08-16T11:16:28.562479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission 1\n\nSubmission 1 ends with bayes ridge with a score of 7.94038 on 33% dataset.\nI'm trying to understand what went not so good.","metadata":{}},{"cell_type":"markdown","source":"Bad score larger than previous 7.94038.","metadata":{}},{"cell_type":"markdown","source":"We see that loss predicted is offset by certain offset by certain amount. Hints at case of bias. We will add more !!!","metadata":{}},{"cell_type":"code","source":"# import xgboost as xgb\n# xgb_model = xgb.XGBRegressor(reg_lambda=0.5,\n#                              max_depth =10,\n#                              minimum_child_weight=2\n#                              objective=\"reg:squarederror\",\n#                              scale_pos_weights=0.5,\n#                              random_state=42)\n\n# xgb_model.fit(x_train, y_train, eval_metric='rmse')\n\n# y_pred = xgb_model.predict(x_train)\n\n# rmse_error_train = rmse_plot(y_train, y_pred)\n\n# print(rmse_error_train)","metadata":{"execution":{"iopub.status.busy":"2021-08-16T11:16:28.564609Z","iopub.execute_input":"2021-08-16T11:16:28.565101Z","iopub.status.idle":"2021-08-16T11:16:28.569611Z","shell.execute_reply.started":"2021-08-16T11:16:28.565066Z","shell.execute_reply":"2021-08-16T11:16:28.568538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from scipy.stats import loguniform\n# from sklearn.model_selection import RandomizedSearchCV, RepeatedKFold\n# from sklearn.linear_model import Ridge\n# space = dict()\n# space['solver'] = ['svd', 'cholesky', 'lsqr', 'sag']\n# space['alpha'] = loguniform(1e-5, 100)\n# space['fit_intercept'] = [True, False]\n# space['normalize'] = [True, False]\n# model = Ridge()\n# # define evaluation\n# cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n# # define search\n# search = RandomizedSearchCV(model, space, n_iter=500, scoring='neg_mean_absolute_error', n_jobs=-1, cv=cv, random_state=1)\n# # execute search\n# result = search.fit(x_t, y_train)\n# # summarize result\n# print('Best Score: %s' % result.best_score_)\n# print('Best Hyperparameters: %s' % result.best_params_)","metadata":{"execution":{"iopub.status.busy":"2021-08-16T11:16:28.57081Z","iopub.execute_input":"2021-08-16T11:16:28.571107Z","iopub.status.idle":"2021-08-16T11:16:28.584481Z","shell.execute_reply.started":"2021-08-16T11:16:28.571079Z","shell.execute_reply":"2021-08-16T11:16:28.583481Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# y_pred_test = xgb_model.predict(x_test)\n\n# rmse_plot(y_test, y_pred_test)","metadata":{"execution":{"iopub.status.busy":"2021-08-16T11:16:28.588387Z","iopub.execute_input":"2021-08-16T11:16:28.588835Z","iopub.status.idle":"2021-08-16T11:16:28.60532Z","shell.execute_reply.started":"2021-08-16T11:16:28.588802Z","shell.execute_reply":"2021-08-16T11:16:28.60428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# y_pred_dtest = xgb_model.predict(x_dtest)","metadata":{"execution":{"iopub.status.busy":"2021-08-16T11:16:28.606875Z","iopub.execute_input":"2021-08-16T11:16:28.60721Z","iopub.status.idle":"2021-08-16T11:16:28.618955Z","shell.execute_reply.started":"2021-08-16T11:16:28.607179Z","shell.execute_reply":"2021-08-16T11:16:28.617924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# g3 = sns.FacetGrid(pd.DataFrame(np.column_stack((xtest_pca, y_pred_dtest)), columns=[\"test_x\",\"test_y\", \"predicted_loss\"]), palette=\"hls\",height=8, aspect=2)\n# g3.map(sns.kdeplot, \"test_x\", color=\"g\")\n# g3.map(sns.kdeplot, \"test_y\", color=\"b\")\n# g3.map(sns.kdeplot, \"predicted_loss\", color=\"r\")\n# g3.set(xticks=range(-10, 20, 2))","metadata":{"execution":{"iopub.status.busy":"2021-08-16T11:16:28.620359Z","iopub.execute_input":"2021-08-16T11:16:28.620671Z","iopub.status.idle":"2021-08-16T11:16:28.6318Z","shell.execute_reply.started":"2021-08-16T11:16:28.62064Z","shell.execute_reply":"2021-08-16T11:16:28.630732Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}