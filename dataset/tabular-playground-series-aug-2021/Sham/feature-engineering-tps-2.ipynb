{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<h1 style=\"color: blue\"> TPS August </h1>\n\n\n","metadata":{}},{"cell_type":"markdown","source":"## [Contents](#h1)\n* <p style=\"color:violet\"> Data and EDA</p>\n* <p style=\"color:violet\"> Plotting Functions</p>\n* <p style=\"color:violet\"> Running Base Model</p>\n* <p style=\"color:violet\"> Engineer Features</p>\n* <p style=\"color:violet\">Optuna and XGB </p>","metadata":{}},{"cell_type":"markdown","source":"## Data and EDA","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport os\nimport numpy as np\n\ndef load_data(path):\n    \"\"\"\n    Takes path oof csv file\n    and returns dataframe object.\n    \"\"\"\n    try:\n        data = pd.read_csv(path)\n        return data\n    except:\n        print(\"Not a valid file\")\n\nclass Dataframe(pd.DataFrame):\n    \n    def __init__(self, dataframe):\n        super().__init__(dataframe)\n\n       \n    def feature_columns(self, drop_col=list()):\n        \"\"\"\n        Extract feature columns from dataframe\n        with reference to drop_col\n        \"\"\"\n        return [col for col in self.drop(drop_col, axis=1).columns]\n    \n    def int_feature_dtypes(self):\n        \"\"\"\n        Get integer dtypes\n        pandas has a method already for this\n        df[df.dtypes==dtype]\n        \"\"\"\n        \n        return list(filter(lambda x: True if x[1]==np.dtype('int64') else False, zip(self.columns,self.dtypes)))\n    \n    def pos_features(self):\n        \"\"\"\n        Get positive features\n        from the data.\n        \"\"\"\n        return list(filter(lambda x: all(df_train[x]>0), df_train.columns[:-1]))\n    ","metadata":{"execution":{"iopub.status.busy":"2021-08-28T18:45:51.225681Z","iopub.execute_input":"2021-08-28T18:45:51.226055Z","iopub.status.idle":"2021-08-28T18:45:51.234557Z","shell.execute_reply.started":"2021-08-28T18:45:51.226024Z","shell.execute_reply":"2021-08-28T18:45:51.233638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#path\npath = \"/kaggle/input/tabular-playground-series-aug-2021/\"\n\n#training and test files location\ncsv_files = [os.path.join(path, x) for x in [\"train.csv\", \"test.csv\"]]\n\n#tuple object of csv files\ntables = (load_data(csv) for csv in csv_files)","metadata":{"execution":{"iopub.status.busy":"2021-08-28T18:45:51.23954Z","iopub.execute_input":"2021-08-28T18:45:51.239861Z","iopub.status.idle":"2021-08-28T18:45:51.256044Z","shell.execute_reply.started":"2021-08-28T18:45:51.239831Z","shell.execute_reply":"2021-08-28T18:45:51.255272Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create Class instance \ndf_train, df_test = tables\ndf_train = Dataframe(df_train)\ndf_test = Dataframe(df_test)\ndf_train.columns","metadata":{"execution":{"iopub.status.busy":"2021-08-28T18:45:51.257869Z","iopub.execute_input":"2021-08-28T18:45:51.258325Z","iopub.status.idle":"2021-08-28T18:46:03.166425Z","shell.execute_reply.started":"2021-08-28T18:45:51.258287Z","shell.execute_reply":"2021-08-28T18:46:03.165569Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#check shapes \nprint(df_train.shape, df_test.shape)\n\n#get all names of feature columns\nfeatures = df_train.feature_columns(drop_col=['id','loss'])","metadata":{"execution":{"iopub.status.busy":"2021-08-28T18:46:03.168468Z","iopub.execute_input":"2021-08-28T18:46:03.169121Z","iopub.status.idle":"2021-08-28T18:46:03.264184Z","shell.execute_reply.started":"2021-08-28T18:46:03.169075Z","shell.execute_reply":"2021-08-28T18:46:03.261949Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Total number of features\n\ndf_train.drop([\"id\"], axis=1, inplace=True)\nprint(len(features))\n\n#Purely Integer Features\n\ninteger_features = df_train.int_feature_dtypes()\nprint(*integer_features,sep='\\n')","metadata":{"execution":{"iopub.status.busy":"2021-08-28T18:46:03.26945Z","iopub.execute_input":"2021-08-28T18:46:03.270138Z","iopub.status.idle":"2021-08-28T18:46:03.358158Z","shell.execute_reply.started":"2021-08-28T18:46:03.270056Z","shell.execute_reply":"2021-08-28T18:46:03.357307Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Strictly Positive Features\n\npos_features = df_train.pos_features()\nprint(pos_features)","metadata":{"execution":{"iopub.status.busy":"2021-08-28T18:46:03.36188Z","iopub.execute_input":"2021-08-28T18:46:03.363921Z","iopub.status.idle":"2021-08-28T18:46:04.618473Z","shell.execute_reply.started":"2021-08-28T18:46:03.363879Z","shell.execute_reply":"2021-08-28T18:46:04.617451Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Plotting Functions","metadata":{}},{"cell_type":"code","source":"#visualization libraries\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{"execution":{"iopub.status.busy":"2021-08-28T18:46:04.619833Z","iopub.execute_input":"2021-08-28T18:46:04.62018Z","iopub.status.idle":"2021-08-28T18:46:05.355932Z","shell.execute_reply.started":"2021-08-28T18:46:04.620139Z","shell.execute_reply":"2021-08-28T18:46:05.355103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#plots in rows and cols\ndef plot_dics():\n    \n    sns_distributions={'kde':sns.kdeplot,'histplot':sns.histplot}\n    return sns_distributions\n\ndef skew_plot(skew_values):\n    \"\"\"\n    Plots skew from a series of skew values.\n    \"\"\"\n    skew_values_positive = skew_values[skew_values>0]\n    skew_values_negative = skew_values[skew_values<0]\n    skew_values_zero = skew_values[skew_values==0]\n    row_size = len(skew_values)\n    index_list = dict(map(lambda x: (x[1], x[0]),enumerate(skew_values.index)))\n    \n    #plot values positive values\n    plt.scatter(x=[index_list[key] for key in skew_values_positive.index], y=skew_values_positive,color='blue')\n    plt.plot(range(row_size), np.ones(row_size), color ='blue')\n    \n    #plot negative values\n    plt.scatter(x=[index_list[key] for key in skew_values_negative.index], y=skew_values_negative,color='red')\n    plt.plot(range(row_size), -1*np.ones(row_size), color ='red')\n    \n    plt.scatter(x=[index_list[key] for key in skew_values_zero.index], y=skew_values_zero,color='green')\n    plt.plot(range(row_size), np.zeros(row_size), color ='green')\n    plt.xticks(np.arange(0, len(skew_values), 3), skew_values.index[::3]); #set ticks\n    plt.title(\"Scatter Plot of Skew values for each column\")\n    plt.grid()\n\ndef legend(sns_plot,value):\n    sns_plot.legend(f\"skew:{value:.2f}\", loc=\"best\")\n    \ndef dist_plots(sns_plot, dataframe, features, figsize=(6, 6), rows=1, cols=1,sharex=False, colors=False):\n    \"\"\"\n    Plots histplots and kernel density estimators using seaborn\n    \"\"\"\n    fig, ax = plt.subplots(nrows=rows, ncols=cols, figsize=figsize,sharex=sharex)\n    feature_index = 0\n    if colors:\n        colors = sns.color_palette(n_colors=(rows*cols))\n    else :\n        colors = [None]*(rows*cols)\n    try:\n        if rows*cols >= len(features):\n            if rows > 1 and cols > 1:\n                for i in range(rows):\n                    for j in range(cols):\n                        if feature_index <= len(features)-1:\n                            g = sns_plot(dataframe[features[feature_index]], ax=ax[i][j], color=colors[feature_index])\n                            feature_index+=1\n                        else:\n                            break          \n            elif rows > 1 and cols == 1:\n                 for i in range(rows):\n                        if feature_index <= len(features)-1:\n                            g = sns_plot(dataframe[features[feature_index]], ax=ax[i], color=colors[feature_index])\n                            feature_index += 1\n                        else:\n                            break  \n            elif rows and cols:\n                g = sns_plot(dataframe[features[feature_index]], ax=ax, color=colors[feature_index])\n                \n        else:\n            raise ValueError()\n    except ValueError:\n        raise ValueError(\"rows*cols should be greater than or equal to the number of features\")\n        ","metadata":{"execution":{"iopub.status.busy":"2021-08-28T18:46:05.35845Z","iopub.execute_input":"2021-08-28T18:46:05.35884Z","iopub.status.idle":"2021-08-28T18:46:05.375973Z","shell.execute_reply.started":"2021-08-28T18:46:05.35879Z","shell.execute_reply":"2021-08-28T18:46:05.374991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns_hist = plot_dics()['kde']\ndist_plots(sns_hist, df_train, list(map(lambda x:x[0], integer_features)), figsize=(12, 8),rows=3, cols=2, colors=True)","metadata":{"execution":{"iopub.status.busy":"2021-08-28T18:46:05.377884Z","iopub.execute_input":"2021-08-28T18:46:05.378259Z","iopub.status.idle":"2021-08-28T18:46:12.934338Z","shell.execute_reply.started":"2021-08-28T18:46:05.378221Z","shell.execute_reply":"2021-08-28T18:46:12.933448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The loss ditribution of loss is asymmetric and so are many features we will normalize them.","metadata":{}},{"cell_type":"markdown","source":"### Skew Plots","metadata":{}},{"cell_type":"code","source":"#get all skewed values\nskew_values=df_train.skew()\nprint(\"=============\")\n\n#print columns with skew values greater than 1\nprint(*skew_values[skew_values>1].index, sep=\" \")\n\n#print columns with skew values less than 1\nprint(\"=============\")\nprint(*skew_values[skew_values<-1].index, sep=\" \")","metadata":{"execution":{"iopub.status.busy":"2021-08-28T18:46:12.935693Z","iopub.execute_input":"2021-08-28T18:46:12.936216Z","iopub.status.idle":"2021-08-28T18:46:13.334194Z","shell.execute_reply.started":"2021-08-28T18:46:12.936173Z","shell.execute_reply":"2021-08-28T18:46:13.332704Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12, 8))\nskew_plot(skew_values)","metadata":{"execution":{"iopub.status.busy":"2021-08-28T18:46:13.335539Z","iopub.execute_input":"2021-08-28T18:46:13.335945Z","iopub.status.idle":"2021-08-28T18:46:13.687014Z","shell.execute_reply.started":"2021-08-28T18:46:13.335903Z","shell.execute_reply":"2021-08-28T18:46:13.685999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Regression","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n#Shape before the split\nprint(df_train.shape)\n\n# Split the set in train, cross-validation, test set\nX_train,X_,y_train, y_ = train_test_split(df_train.drop([\"loss\"],axis=1),df_train[\"loss\"], test_size=0.3, shuffle=True, random_state=23)\nX_cv, X_test, y_cv, y_test = train_test_split(X_,y_, test_size=1/2,shuffle=True, random_state=100)\n\n#Trainging set\nprint(f\"Training Size: {X_train.shape}\")\n\n#cross-validation set\nprint(f\"CV Size: {X_cv.shape}\")\n\n#test test\nprint(f\"Test Size: {X_test.shape}\")","metadata":{"execution":{"iopub.status.busy":"2021-08-28T18:46:13.688513Z","iopub.execute_input":"2021-08-28T18:46:13.688894Z","iopub.status.idle":"2021-08-28T18:46:14.251399Z","shell.execute_reply.started":"2021-08-28T18:46:13.688854Z","shell.execute_reply":"2021-08-28T18:46:14.250495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Scaling Data","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler, Normalizer, RobustScaler, PowerTransformer\n\n# all transformers\ndef transform_data(df, transformer_name='standardscaler'):\n    transformer_dict = {\"standardscaler\":StandardScaler,\n                        \"normalizer\":Normalizer,\n                        \"gaussian\":PowerTransformer,\n                        \"robustscaler\": RobustScaler}\n    return transformer_dict[transformer_name]().fit_transform(df)\n\ntemp_list = []\nfor df in [X_train, X_cv, X_test]:\n    temp_list.append(transform_data(transform_data(df, transformer_name=\"standardscaler\"), transformer_name=\"normalizer\"))","metadata":{"execution":{"iopub.status.busy":"2021-08-28T18:46:14.253655Z","iopub.execute_input":"2021-08-28T18:46:14.254343Z","iopub.status.idle":"2021-08-28T18:46:15.122648Z","shell.execute_reply.started":"2021-08-28T18:46:14.254289Z","shell.execute_reply":"2021-08-28T18:46:15.121676Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Unpack the values\nX_train_gaussian,X_cv_gaussian, X_test_gaussian = temp_list","metadata":{"execution":{"iopub.status.busy":"2021-08-28T18:46:15.123987Z","iopub.execute_input":"2021-08-28T18:46:15.124324Z","iopub.status.idle":"2021-08-28T18:46:15.127707Z","shell.execute_reply.started":"2021-08-28T18:46:15.12429Z","shell.execute_reply":"2021-08-28T18:46:15.126905Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train_gaussian = pd.DataFrame(X_train_gaussian, columns=df_train.columns[:-1])\ndf_cv_gaussian = pd.DataFrame(X_cv_gaussian, columns=df_train.columns[:-1])\ndf_test_gaussian = pd.DataFrame(X_test_gaussian, columns=df_train.columns[:-1])\n\n#After transform skewed values\nprint(\"=============\")\nplt.figure(figsize=(12, 8))\nskew_values_gaussian = df_train_gaussian.skew()\nskew_plot(skew_values_gaussian)","metadata":{"execution":{"iopub.status.busy":"2021-08-28T18:46:15.12947Z","iopub.execute_input":"2021-08-28T18:46:15.13Z","iopub.status.idle":"2021-08-28T18:46:15.75407Z","shell.execute_reply.started":"2021-08-28T18:46:15.129962Z","shell.execute_reply":"2021-08-28T18:46:15.75229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Plots after scaling","metadata":{}},{"cell_type":"code","source":"dist_plots(sns_hist, df_test_gaussian, list(map(lambda x:x[0], integer_features[:-1])), figsize=(12, 8),rows=3, cols=2, colors=True)","metadata":{"execution":{"iopub.status.busy":"2021-08-28T18:46:15.755431Z","iopub.execute_input":"2021-08-28T18:46:15.755807Z","iopub.status.idle":"2021-08-28T18:46:17.538585Z","shell.execute_reply.started":"2021-08-28T18:46:15.755768Z","shell.execute_reply":"2021-08-28T18:46:17.537771Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Running Base Model","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import mean_squared_error, r2_score\n\n#classical\nfrom sklearn.linear_model import Ridge, BayesianRidge, LinearRegression, Lasso\n\n#Experimental feature in scikit-learn\nfrom sklearn.experimental import enable_hist_gradient_boosting\n\n#Ensemble regressors\nfrom sklearn.ensemble import RandomForestRegressor, StackingRegressor, HistGradientBoostingRegressor\nimport xgboost as xgb\n\nmodel_dict = {\"ridge\":Ridge, \"bayesridge\":BayesianRidge,\n                  \"linearreg\":LinearRegression, \"lasso\":Lasso}\n\n#Fitting without any parameters adjustment\ndef fit_model(X, y, model=\"linearreg\"):\n    model = model_dict[model]()\n    model.fit(X, y)\n    return model\n\n#predict function\ndef predict_y(X, model):\n    y_pred = model.predict(X)\n    return y_pred\n  \n# root mean square error  \ndef rmse(y_true, y_pred):\n    return np.sqrt(mean_squared_error(y_true, y_pred))\n\n# Coefficient Histogram and line plots\ndef coefficient_plot(model_name,models,ax, color):\n    for model in models:\n        ax[0].plot(model.coef_, linestyle='-', color=color[models.index(model)])\n    ax[0].legend(model_name)\n    for model in models:\n        ax[1].bar([list(model.coef_).index(val) for val in list(model.coef_)], model.coef_, color=color[models.index(model)],label=features)\n    ax[1].legend(model_name)","metadata":{"execution":{"iopub.status.busy":"2021-08-28T18:46:17.539857Z","iopub.execute_input":"2021-08-28T18:46:17.540188Z","iopub.status.idle":"2021-08-28T18:46:17.777048Z","shell.execute_reply.started":"2021-08-28T18:46:17.540159Z","shell.execute_reply":"2021-08-28T18:46:17.776197Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def run_base_model(X_train, y_train, X_test, y_test):\n    \"\"\"\n    Run base models and select the best model\n    \"\"\"\n    error_train = None\n    error_rmse = None\n    base_model = None\n    for key in model_dict.keys():\n        model_error = 0\n        _=20\n        model_name = model_dict[key].__name__\n        print(\"=\"*_+f\"{model_name}\"+\"=\"*(abs(len(model_name)-_)), end=\"\\n\")\n        model = fit_model(X_train, y_train, model=key)\n        \n        model_error = rmse(predict_y(X_train, model), y_train)   \n        if error_train is None:\n            error_train = model_error\n        elif error_train > model_error:\n            base_model = model\n            error_train = model_error\n            print(f\"Low error found {model_error}--{model_name}\")\n        print(f\"Train Root Mean Squared Error: {model_error}\", end=\"\\n\")\n        print(\"\\n\")\n    \n    print(\"Base Model with low error on training set \"+str(base_model).replace(\"()\",\"\"))\n    print(\"\\n\")\n    print(\"Base Model Prediction on Cross Validation Set\")\n    y_pred_cv = predict_y(X_test, base_model)\n    error_rmse = rmse(y_pred_cv, y_test)\n    print(f\"Prediction Root Mean Squared Error: {error_rmse}\")\n    \n    return base_model, error_train\nbase_model, error_train = run_base_model(X_train_gaussian, y_train, X_cv_gaussian, y_cv)\nerror_train","metadata":{"execution":{"iopub.status.busy":"2021-08-28T18:46:17.778345Z","iopub.execute_input":"2021-08-28T18:46:17.778694Z","iopub.status.idle":"2021-08-28T18:46:20.27035Z","shell.execute_reply.started":"2021-08-28T18:46:17.778656Z","shell.execute_reply":"2021-08-28T18:46:20.269412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib as mpl\nsns.set_style(\"ticks\")\nfig_coef, ax_coef = plt.subplots(nrows=1, ncols=2, figsize=(24, 8))\ncolormap = mpl.cm.Dark2.colors\nmodel_names = [str(base_model).replace(\"()\",\"\")]\ncoefficient_plot(model_names, [base_model], ax_coef, colormap)","metadata":{"execution":{"iopub.status.busy":"2021-08-28T18:46:20.271649Z","iopub.execute_input":"2021-08-28T18:46:20.272192Z","iopub.status.idle":"2021-08-28T18:46:20.855826Z","shell.execute_reply.started":"2021-08-28T18:46:20.272151Z","shell.execute_reply":"2021-08-28T18:46:20.85321Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"a = list(zip(features,base_model.coef_))\nfig_bar, ax_bar = plt.subplots( figsize=(12, 8))\na.sort(key=lambda x: x[1])\na=dict(a[::-1])\nsns.barplot(y=list(a.values()), x=list(a.keys()), ax=ax_bar)\nax_bar.set_title(\"Feature Coefficients\");\nax_bar.set_xticks(range(0, len(a.keys()), 3))\nax_bar.set_xticklabels(list(a.keys())[::3]);\ndrop_features = list(filter(lambda x: True if -0.05<=a[x]<=0.05 else False, a.keys()))\nprint(drop_features)","metadata":{"execution":{"iopub.status.busy":"2021-08-28T18:46:20.859354Z","iopub.execute_input":"2021-08-28T18:46:20.859619Z","iopub.status.idle":"2021-08-28T18:46:21.736114Z","shell.execute_reply.started":"2021-08-28T18:46:20.859589Z","shell.execute_reply":"2021-08-28T18:46:21.735067Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n#Predicting on actual test set\nreal_test = transform_data(transform_data(df_test.drop([\"id\"],axis=1), transformer_name=\"standardscaler\"),transformer_name=\"normalizer\")\nreal_test.shape","metadata":{"execution":{"iopub.status.busy":"2021-08-28T18:46:21.738233Z","iopub.execute_input":"2021-08-28T18:46:21.73861Z","iopub.status.idle":"2021-08-28T18:46:22.262443Z","shell.execute_reply.started":"2021-08-28T18:46:21.738571Z","shell.execute_reply":"2021-08-28T18:46:22.2614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def submission(y_pred, col, file_name):\n    \"\"\"\n    Submission Function \n    with filename\n    \"\"\"\n    data = pd.DataFrame({\"id\":col, \"loss\":y_pred})\n    sub_files = []\n    for dirname,_, filenames in os.walk(\"./\"):\n            for file in filenames:\n                if file.endswith(\".csv\"):\n                    sub_files.append(file)\n    if len(sub_files)>0:\n        new_n = str(int(sub_files[-1].split(\".\")[0][-1])+1)\n    else:\n        new_n=1\n    data.to_csv(f\"./{file_name}{new_n}.csv\", index=False)            ","metadata":{"execution":{"iopub.status.busy":"2021-08-28T18:46:22.263879Z","iopub.execute_input":"2021-08-28T18:46:22.264247Z","iopub.status.idle":"2021-08-28T18:46:22.271192Z","shell.execute_reply.started":"2021-08-28T18:46:22.264207Z","shell.execute_reply":"2021-08-28T18:46:22.270071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#using LR\ny_pred_real = base_model.predict(real_test)\nsubmission(y_pred_real, df_test[\"id\"], \"bayes\")","metadata":{"execution":{"iopub.status.busy":"2021-08-28T18:46:22.2726Z","iopub.execute_input":"2021-08-28T18:46:22.273043Z","iopub.status.idle":"2021-08-28T18:46:22.811319Z","shell.execute_reply.started":"2021-08-28T18:46:22.273005Z","shell.execute_reply":"2021-08-28T18:46:22.810434Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lowest error on 25% of data: 7.94038 using bayesridge using standardscaler with no tuning\nHighest error on 25% of data: 7.9433 using robustscaler and normazlier","metadata":{}},{"cell_type":"code","source":"# dist_plots(sns_hist, , [\"predicted_loss\",\"test_loss\"], rows=2, cols=1, colors=True)\n# sns_plot(dataframe[features[feature_index]], ax=ax[i][j], color=colors[feature_index])\nfig,ax = plt.subplots(figsize=(16, 8))\ng1 = sns.kdeplot( \"predicted_loss\", data = pd.DataFrame({\"predicted_loss\":base_model.predict(X_cv_gaussian), \"test_loss\":y_cv}), color='red', ax = ax)\ng2 = sns.kdeplot(\"test_loss\", data=pd.DataFrame({\"predicted_loss\":base_model.predict(X_cv_gaussian), \"test_loss\":y_cv}), color='blue', ax=ax)\n\nax.grid(linestyle='--', linewidth=2)\nax.legend([\"predicted loss\", \"actual loss\"]);\nax.set_title(\"Predicted loss vs actual loss density\");\n              ","metadata":{"execution":{"iopub.status.busy":"2021-08-28T18:46:22.812598Z","iopub.execute_input":"2021-08-28T18:46:22.813018Z","iopub.status.idle":"2021-08-28T18:46:23.5055Z","shell.execute_reply.started":"2021-08-28T18:46:22.81298Z","shell.execute_reply":"2021-08-28T18:46:23.504673Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Engineering","metadata":{}},{"cell_type":"code","source":"def square_root(col_list, data):\n    \n    #add square root columns\n    new_data = data.copy()\n    for col in col_list:\n        new_data[\"sqrt_\"+col]= np.sqrt(data[col])\n    return new_data\n\ndef square(col_list, data):\n    \n    #add squares for col\n    new_data = data.copy()\n    for col in col_list:\n        new_data[\"sqr\"+col]= (data[col])*(data[col])\n    return new_data\n\ndef reciprocals(col_list, data):\n    #add reciprocals for col_list\n    new_data = data.copy()\n    for col in col_list:\n        new_data[\"rec\"+col]= 1/(abs(data[col])+1)                      \n    return new_data\n\ndef product_col(data):\n    #all product gives one column\n    new_data = data.copy()\n    temp = data[data.columns[0]]\n    for col in data.columns:\n        temp *= data[col]\n    new_data[\"all_product\"] = temp\n    return new_data\n    \n\ndef update_sets(data_list):\n    temp_list = []\n    for df in data_list:\n        temp_list.append(square(df.columns, data=df))\n    return temp_list","metadata":{"execution":{"iopub.status.busy":"2021-08-28T18:46:23.506902Z","iopub.execute_input":"2021-08-28T18:46:23.507247Z","iopub.status.idle":"2021-08-28T18:46:23.515661Z","shell.execute_reply.started":"2021-08-28T18:46:23.507211Z","shell.execute_reply":"2021-08-28T18:46:23.514621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_f1 = reciprocals(features, data=square(features, data=X_train))\nX_cv_f1 = reciprocals(features, data= square(features, data=X_cv))\nX_test_f1 = reciprocals(features, data=square(features, data=X_test))\ntemp_list1 = []\nfor df in [X_train_f1, X_cv_f1, X_test_f1]:\n    temp_list1.append(transform_data(transform_data(df, transformer_name=\"standardscaler\"), transformer_name=\"normalizer\"))","metadata":{"execution":{"iopub.status.busy":"2021-08-28T18:46:23.517141Z","iopub.execute_input":"2021-08-28T18:46:23.517752Z","iopub.status.idle":"2021-08-28T18:46:28.119118Z","shell.execute_reply.started":"2021-08-28T18:46:23.517701Z","shell.execute_reply":"2021-08-28T18:46:28.118201Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_sqrt_gaussian, X_cv_sqrt_gaussian, X_test_sqrt_gaussian = temp_list1","metadata":{"execution":{"iopub.status.busy":"2021-08-28T18:46:28.120519Z","iopub.execute_input":"2021-08-28T18:46:28.1209Z","iopub.status.idle":"2021-08-28T18:46:28.125455Z","shell.execute_reply.started":"2021-08-28T18:46:28.12086Z","shell.execute_reply":"2021-08-28T18:46:28.124342Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"base_model_v1, error_v1 = run_base_model(X_train_sqrt_gaussian, y_train, X_cv_sqrt_gaussian, y_cv)","metadata":{"execution":{"iopub.status.busy":"2021-08-28T18:46:28.126982Z","iopub.execute_input":"2021-08-28T18:46:28.127523Z","iopub.status.idle":"2021-08-28T18:46:39.068632Z","shell.execute_reply.started":"2021-08-28T18:46:28.127483Z","shell.execute_reply":"2021-08-28T18:46:39.067654Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Observe that the same linear regression gives a error of 7.783425 on validation set with added features","metadata":{}},{"cell_type":"code","source":"fig_v1,ax_v1 = plt.subplots(figsize=(16, 8))\ng1 = sns.kdeplot( \"predicted_loss\", data = pd.DataFrame({\"predicted_loss\":base_model_v1.predict(X_cv_sqrt_gaussian), \"cv_loss\":y_cv}),\n                 color='red', ax = ax_v1)\ng2 = sns.kdeplot(\"cv_loss\", data=pd.DataFrame({\"predicted_loss\":base_model_v1.predict(X_cv_sqrt_gaussian), \"cv_loss\":y_cv}),\n                 color='blue', ax=ax_v1)\n\nax_v1.grid(linestyle='--', linewidth=2)\nax_v1.legend([\"predicted loss\", \"actual loss\"]);\nax_v1.set_title(\"Predicted loss vs actual loss density\");\n              ","metadata":{"execution":{"iopub.status.busy":"2021-08-28T18:46:39.070134Z","iopub.execute_input":"2021-08-28T18:46:39.070744Z","iopub.status.idle":"2021-08-28T18:46:39.800579Z","shell.execute_reply.started":"2021-08-28T18:46:39.070685Z","shell.execute_reply":"2021-08-28T18:46:39.799743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"real_test_v1 = reciprocals(features, data=square(features, data=df_test.drop([\"id\"],axis=1)))\nreal_test_v1 = transform_data(transform_data(real_test_v1, transformer_name=\"standardscaler\"),transformer_name=\"normalizer\")\nreal_test_v1.shape","metadata":{"execution":{"iopub.status.busy":"2021-08-28T18:46:39.801868Z","iopub.execute_input":"2021-08-28T18:46:39.802383Z","iopub.status.idle":"2021-08-28T18:46:42.360171Z","shell.execute_reply.started":"2021-08-28T18:46:39.802339Z","shell.execute_reply":"2021-08-28T18:46:42.359334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#using linear regression with squared features\ny_pred_real = base_model_v1.predict(real_test_v1)\nsubmission(y_pred_real, df_test[\"id\"], \"submission\")","metadata":{"execution":{"iopub.status.busy":"2021-08-28T18:46:42.361494Z","iopub.execute_input":"2021-08-28T18:46:42.361871Z","iopub.status.idle":"2021-08-28T18:46:42.933107Z","shell.execute_reply.started":"2021-08-28T18:46:42.361832Z","shell.execute_reply":"2021-08-28T18:46:42.932159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"NICE! Adding Square and reciprocal terms Terms brought the error down by 1.36% that is 92439 previous error was 94038.","metadata":{}},{"cell_type":"markdown","source":"## Optuna and XGB","metadata":{}},{"cell_type":"code","source":"#Method was taken from a kaggle notebook from last feb i don't rememeber the link\ndef objective(trial):\n    \n    train_x, test_x, train_y, test_y = X_train_sqrt_gaussian, X_cv_sqrt_gaussian, y_train, y_cv\n    param = {\n        'tree_method':'gpu_hist',  # this parameter means using the GPU when training our model to speedup the training process\n        'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n        'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n        'colsample_bytree': trial.suggest_categorical('colsample_bytree', [0.3,0.4,0.5,0.6,0.7,0.8,0.9, 1.0]),\n        'subsample': trial.suggest_categorical('subsample', [0.4,0.5,0.6,0.7,0.8,1.0]),\n        'learning_rate': trial.suggest_categorical('learning_rate', [0.008,0.009,0.01,0.012,0.014,0.016,0.018, 0.02]),\n        'n_estimators': 4000,\n        'max_depth': trial.suggest_categorical('max_depth', [5,7,9,11,13,15,17,20]),\n        'random_state': trial.suggest_categorical('random_state', [24, 48,90, 100, 120]),\n        'min_child_weight': trial.suggest_int('min_child_weight', 1, 300),\n    }\n    model = xgb.XGBRegressor(**param)  \n    \n    model.fit(train_x,train_y,eval_set=[(test_x,test_y)],early_stopping_rounds=100,verbose=False)\n    \n    preds = model.predict(test_x)\n    \n    rmse = mean_squared_error(test_y, preds,squared=False)\n    \n    return rmse","metadata":{"execution":{"iopub.status.busy":"2021-08-28T18:48:46.143481Z","iopub.execute_input":"2021-08-28T18:48:46.143837Z","iopub.status.idle":"2021-08-28T18:48:46.153431Z","shell.execute_reply.started":"2021-08-28T18:48:46.143802Z","shell.execute_reply":"2021-08-28T18:48:46.152447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Best Parameters Obtained\n\n{\"lambda\": 0.04001717971729262, \"alpha\": 0.001537777755465056, \"colsample_bytree\": 0.5, \"subsample\": 0.7, \"learning_rate\": 0.009, \"max_depth\": 9, \"random_state\": 48, \"min_child_weight\": 282}","metadata":{}},{"cell_type":"code","source":"# ! pip install optuna\nimport optuna\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=50)\nprint('Number of finished trials:', len(study.trials))\nprint('Best trial:', study.best_trial.params)","metadata":{"execution":{"iopub.status.busy":"2021-08-28T18:48:48.667581Z","iopub.execute_input":"2021-08-28T18:48:48.66803Z","iopub.status.idle":"2021-08-28T19:35:34.279005Z","shell.execute_reply.started":"2021-08-28T18:48:48.667991Z","shell.execute_reply":"2021-08-28T19:35:34.277858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"7.8306 on CV with hundred estimators","metadata":{}},{"cell_type":"code","source":"import json\nwith open(\"./best_param_120RS.json\", \"w\") as f:\n    json.dump(study.best_trial.params, f)\nstudy.best_trial.params","metadata":{"execution":{"iopub.status.busy":"2021-08-28T19:37:17.842448Z","iopub.execute_input":"2021-08-28T19:37:17.842802Z","iopub.status.idle":"2021-08-28T19:37:17.852038Z","shell.execute_reply.started":"2021-08-28T19:37:17.842768Z","shell.execute_reply":"2021-08-28T19:37:17.850826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import RepeatedKFold, KFold\n\nhp = study.best_trial.params\n\nhp[\"tree_method\"]=\"gpu_hist\"\nhp[\"verbosity\"] = 0\nhp[\"objective\"] = \"reg:squarederror\"\nhp[\"n_estimators\"] = 4000\n\ntrain = pd.concat([pd.DataFrame(X_train_sqrt_gaussian, columns=X_train_f1.columns,index=X_train_f1.index),\n                   pd.DataFrame(X_cv_sqrt_gaussian, columns=X_cv_f1.columns,index=X_cv_f1.index)])\n                   \ndf_y_train = pd.concat([y_train,y_cv])\ntest = pd.DataFrame(real_test_v1, columns=X_train_f1.columns)\n\ny_preds = np.zeros(test.shape[0])\nkf = KFold(n_splits=5,random_state=48,shuffle=True)\nrmse_error=[]  # list contains rmse for each fold\nn=0\nfor trn_idx, test_idx in kf.split(train, df_y_train):\n    \n    X_tr,X_val=train.iloc[trn_idx],train.iloc[test_idx]\n    y_tr,y_val=df_y_train.iloc[trn_idx],df_y_train.iloc[test_idx]\n    \n    model_xgb = xgb.XGBRegressor(**hp)\n    model_xgb.fit(X_tr,y_tr,eval_set=[(X_val,y_val)],early_stopping_rounds=100,verbose=False)\n    \n    y_preds+=model_xgb.predict(test)/kf.n_splits\n    \n    rmse_error.append(mean_squared_error(y_val, model_xgb.predict(X_val), squared=False))\n    \n    print(\"Fold\", n+1,rmse_error[n])\n    \n    n+=1","metadata":{"execution":{"iopub.status.busy":"2021-08-28T19:37:52.323531Z","iopub.execute_input":"2021-08-28T19:37:52.323919Z","iopub.status.idle":"2021-08-28T19:42:16.772431Z","shell.execute_reply.started":"2021-08-28T19:37:52.323887Z","shell.execute_reply":"2021-08-28T19:42:16.771521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission(y_preds, df_test[\"id\"], \"1_submission\")","metadata":{"execution":{"iopub.status.busy":"2021-08-28T19:42:28.986625Z","iopub.execute_input":"2021-08-28T19:42:28.987002Z","iopub.status.idle":"2021-08-28T19:42:29.447351Z","shell.execute_reply.started":"2021-08-28T19:42:28.986968Z","shell.execute_reply":"2021-08-28T19:42:29.446449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optuna.visualization.plot_optimization_history(study)","metadata":{"execution":{"iopub.status.busy":"2021-08-28T19:43:10.354828Z","iopub.execute_input":"2021-08-28T19:43:10.355235Z","iopub.status.idle":"2021-08-28T19:43:10.525419Z","shell.execute_reply.started":"2021-08-28T19:43:10.355188Z","shell.execute_reply":"2021-08-28T19:43:10.524504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optuna.visualization.plot_parallel_coordinate(study)","metadata":{"execution":{"iopub.status.busy":"2021-08-28T19:43:19.929256Z","iopub.execute_input":"2021-08-28T19:43:19.929588Z","iopub.status.idle":"2021-08-28T19:43:20.01867Z","shell.execute_reply.started":"2021-08-28T19:43:19.929556Z","shell.execute_reply":"2021-08-28T19:43:20.017903Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_xgb.save_model(\"./xgb_model_Rs120\")","metadata":{"execution":{"iopub.status.busy":"2021-08-28T19:43:26.557049Z","iopub.execute_input":"2021-08-28T19:43:26.55738Z","iopub.status.idle":"2021-08-28T19:43:26.58825Z","shell.execute_reply.started":"2021-08-28T19:43:26.557351Z","shell.execute_reply":"2021-08-28T19:43:26.58715Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optuna.importance.get_param_importances(study)","metadata":{"execution":{"iopub.status.busy":"2021-08-28T19:44:14.895954Z","iopub.execute_input":"2021-08-28T19:44:14.89629Z","iopub.status.idle":"2021-08-28T19:44:16.721558Z","shell.execute_reply.started":"2021-08-28T19:44:14.896259Z","shell.execute_reply":"2021-08-28T19:44:16.720707Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}