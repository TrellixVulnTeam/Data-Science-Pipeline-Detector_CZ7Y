{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport warnings\nwarnings.filterwarnings(action='ignore')\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TRAIN_DATA_PATH = '/kaggle/input/covid19-global-forecasting-week-5/train.csv'\nTEST_DATA_PATH = '/kaggle/input/covid19-global-forecasting-week-5/test.csv'\nPOPULATION_DATA_PATH = '/kaggle/input/covid19-global-forecasting-locations-population/locations_population.csv'\nTARGET_DATE = 21","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"CAL_TYPE = {'County':'category', 'Province_State':'category', 'Country_Region':'category', 'Id': 'int32', \\\n            'Population':'int64', 'Weight':'float32', 'Date':'str', 'Target':'str'}\nCAL_DATA = ['id', 'week', 'day', 'month']\nINPUT_DATA = ['target_before', 'target_3']\ndef make_dataset():\n    train_data = pd.read_csv(TRAIN_DATA_PATH, dtype=CAL_TYPE)\n    train_data['Date'] = pd.to_datetime(train_data['Date'])\n    train_data['week'] = train_data['Date'].dt.weekday\n    train_data['day'] = train_data['Date'].dt.day\n    train_data['month'] = train_data['Date'].dt.month\n    train_data['id'] = train_data['Country_Region'].str.cat(train_data['Province_State'], sep=' ', na_rep='')\n    train_data['id'] = train_data['id'].str.cat(train_data['County'], sep=' ', na_rep='')\n    train_data['id'] = train_data['id'].astype('category')\n    train_data['id'] = train_data['id'].cat.codes.astype('int16')\n    c_train_data = train_data[train_data['Target'] == 'ConfirmedCases']\n    f_train_data = train_data[train_data['Target'] == 'Fatalities']\n    \n    c_train_data['target_before'] = c_train_data.groupby(by='id')['TargetValue'].shift(periods=1)\n    c_train_data['target_7'] = c_train_data.groupby(by='id')['target_before'].rolling(7).mean().reset_index(0, drop=True)\n    c_train_data['target_5'] = c_train_data.groupby(by='id')['target_before'].rolling(5).mean().reset_index(0, drop=True)\n    c_train_data['target_3'] = c_train_data.groupby(by='id')['target_before'].rolling(3).mean().reset_index(0, drop=True)\n    \n    f_train_data['target_before'] = f_train_data.groupby(by='id')['TargetValue'].shift(periods=1)\n    f_train_data['target_7'] = f_train_data.groupby(by='id')['target_before'].rolling(7).mean().reset_index(0, drop=True)\n    f_train_data['target_5'] = f_train_data.groupby(by='id')['target_before'].rolling(5).mean().reset_index(0, drop=True)\n    f_train_data['target_3'] = f_train_data.groupby(by='id')['target_before'].rolling(3).mean().reset_index(0, drop=True)\n    \n#     train_data.dropna(inplace=True)\n#     train_data.dropna(subset=['target_before'], inplace=True)\n    f_train_data.dropna(subset=['target_7'], inplace=True)\n    c_train_data.dropna(subset=['target_7'], inplace=True)\n    return c_train_data, f_train_data\n    \ndef make_X(df, batch_size):\n    X = {'inputs': df[INPUT_DATA].to_numpy().reshape((batch_size, TARGET_DATE,3))}\n    for i, v in enumerate(CAL_DATA):\n            X[v] = df[[v]].to_numpy().reshape((batch_size, TARGET_DATE, 1))\n    return X\n\ndef make_X2(df):\n    X = {'inputs': df[INPUT_DATA].to_numpy()}\n    for i, v in enumerate(CAL_DATA):\n            X[v] = df[[v]].to_numpy()\n    return X","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"c_train_data, f_train_data = make_dataset()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"c_train_data[c_train_data['TargetValue'] < 0].id.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"c_train_data[c_train_data['id'] == 11]['target_3'].plot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"c_train_data[c_train_data['id'] == 11]['TargetValue'].plot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from datetime import datetime\nfrom datetime import timedelta\nimport tensorflow.keras\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Dense, Input, GRU, Masking, Permute, Concatenate, BatchNormalization, Flatten, Embedding, TimeDistributed, Reshape, Dropout, Concatenate\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.regularizers import l2\nfrom tensorflow.keras import backend as K\nimport tensorflow as tf\nfrom tqdm import tqdm\nimport pandas as pd\nimport numpy as np\nimport gzip\nimport pickle\n\ndef pinball_loss(true, pred):\n    assert pred.shape[0]==true.shape[0]\n    tau = K.constant(np.array([0.05, 0.5, 0.95]))\n    loss1 = (true[:, 0:1] - pred) *  tau\n    loss2 = (pred - true[:, 0:1]) * (1 - tau)\n    print(true[:, 1].shape, K.mean(loss1).shape, pred.shape)\n    print((true[:, 1] * K.mean(loss1, axis=1)).shape, loss1[pred <= true[:, 0:1]].shape)\n    loss1 = K.mean(true[:, 1] * K.mean(loss1[pred <= true[:, 0:1]]))/3\n    loss2 = K.mean(true[:, 1] * K.mean(loss2[pred > true[:, 0:1]]))/3\n    return (loss1 + loss2) / 2\n\ndef pinball_loss1(true, pred):\n    assert pred.shape[0]==true.shape[0]\n    loss1 = (true[:, 0:1] - pred) *  0.05\n    loss2 = (pred - true[:, 0:1]) * (1 - 0.05)\n    loss1 = K.clip(loss1, 0, K.max(loss1))\n    loss2 = K.clip(loss2, 0, K.max(loss2))\n    loss = loss1 + loss2\n    print(loss.shape, pred.shape)\n    loss = K.mean(true[:, 1:2] * loss)\n    return loss\ndef pinball_loss2(true, pred):\n    assert pred.shape[0]==true.shape[0]\n    loss1 = (true[:, 0] - pred)# *  0.5\n    loss2 = (pred - true[:, 0])# * (1 - 0.5)\n    loss1 = K.clip(loss1, 0, K.max(loss1))\n    loss2 = K.clip(loss2, 0, K.max(loss2))\n    loss = loss1 + loss2\n    loss = K.mean(true[:, 1] * loss)\n    return loss\ndef pinball_loss3(true, pred):\n    assert pred.shape[0]==true.shape[0]\n    loss1 = (true[:, 0:1] - pred) *  0.95\n    loss2 = (pred - true[:, 0:1]) * (1 - 0.95)\n    loss1 = K.clip(loss1, 0, K.max(loss1))\n    loss2 = K.clip(loss2, 0, K.max(loss2))\n    loss = loss1 + loss2\n    loss = K.mean(true[:, 1:2] * loss)\n    return loss\n\ndef rmse(true, pred):\n    loss1 = (true[:, 0:1] - pred) \n    loss2 = (pred - true[:, 0:1])\n    loss1_005 = loss1 * 0.05\n    loss1_005 = K.clip(loss1_005, 0, K.max(loss1_005))\n    loss2_005 = loss2 * 0.95\n    loss2_005 = K.clip(loss2_005, 0, K.max(loss2_005))\n    loss_005 = loss1_005 + loss2_005\n    \n    loss1_05 = loss1 * 0.5\n    loss1_05 = K.clip(loss1_05, 0, K.max(loss1_05))\n    loss2_05 = loss2 * 0.5\n    loss2_05 = K.clip(loss2_05, 0, K.max(loss2_05))\n    loss_05 = loss1_05 + loss2_05\n    \n    loss1_095 = loss1 * 0.95\n    loss1_095 = K.clip(loss1_095, 0, K.max(loss1_095))\n    loss2_095 = loss2 * 0.05\n    loss2_095 = K.clip(loss2_095, 0, K.max(loss2_095))\n    loss_095 = loss1_095 + loss2_095\n    \n    loss = K.mean(true[:, 1:2] * ((loss_005 + loss_05 + loss_095) / 3))\n    \n#     loss1 = K.clip(loss1, 0, K.max(loss1))\n#     loss2 = K.clip(loss2, 0, K.max(loss2))\n    \n    return loss\n#     return K.mean(K.abs(true[:,0:1] - pred) * true[:, 1:2])\n\n\ndef simple_model(input_size, days=21, batch_size=2**14, epochs=200, lr=1e-3):\n    \n    inputs = Input(shape=(input_size), name='inputs')    \n    \n    id_input = Input(shape=(1,), name='id')\n    wday_input = Input(shape=(1,), name='week')\n    day_input = Input(shape=(1,), name='day')\n    month_input = Input(shape=(1,), name='month')\n    \n    id_emb = Flatten()(Embedding(3464, 3)(id_input))\n    wday_emb = Flatten()(Embedding(8, 1)(wday_input))\n    day_emb = Flatten()(Embedding(32, 3)(day_input))\n    month_emb = Flatten()(Embedding(13, 2)(month_input))\n    \n#     x = Concatenate(-1)([inputs, id_emb, wday_emb, day_emb, month_emb])\n    x = Concatenate(-1)([inputs, id_emb, day_emb, month_emb])\n\n    x = Dense(128, activation='relu')(x)\n    x = BatchNormalization()(x)\n    x = Dropout(0.25)(x)\n    x = Dense(64, activation='relu')(x)\n    x = BatchNormalization()(x)\n    x = Dropout(0.25)(x)\n    x = Dense(32, activation='relu')(x)        \n    outputs1 = Dense(1, activation='linear')(x) \n#     outputs2 = Dense(1, activation='linear')(x) \n#     outputs3 = Dense(1, activation='linear')(x) \n#     print(outputs.shape)\n\n#     outputs1 = outputs1 * 46197 - 10034\n#     outputs2 = outputs2 * 46197 - 10034\n#     outputs3 = outputs3 * 46197 - 10034\n    \n    input_dic = {\n        'inputs': inputs, 'week': wday_input, 'month': month_input, \n        'day': day_input, 'id': id_input,\n\n    }\n\n    optimizer = Adam(lr=lr, name='adam')\n    model = Model(input_dic, outputs1, name='gru_network')\n    model.compile(optimizer=optimizer, loss=rmse)\n#     model.compile(optimizer=optimizer, loss=pinball_loss2)\n#     model.compile(optimizer=optimizer, loss=['mse', 'mse', 'mse'])\n    return model\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# make_ID_list(c_train_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# f_model = attention_model(3,)\n# c_model = attention_model(3,)\n# c_gen = DataGenerator(c_train_data, 2**10, make_ID_list(c_train_data))\n# f_gen = DataGenerator(f_train_data, 2**10, make_ID_list(f_train_data))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# b = make_X2(c_train_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Normalization\nfrom sklearn.preprocessing import MinMaxScaler\nmin_max_scaler = MinMaxScaler()\n# fitted = min_max_scaler.fit(c_train_data[INPUT_DATA + ['TargetValue']])\n# print(fitted.data_max_)\nc_train_data_norm = c_train_data.copy()\n# c_train_data_norm[INPUT_DATA + ['TargetValue']] = min_max_scaler.transform(c_train_data[INPUT_DATA + ['TargetValue']])\n# output = min_max_scaler.transform(output)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"c_s_model = simple_model(2,)\nmodel_path = './cv19_predict5.h5'  # '{epoch:02d}-{val_loss:.4f}.h5'\ncb_checkpoint = ModelCheckpoint(filepath=model_path, monitor='val_loss', verbose=1, save_best_only=True)\nearly_stopping = EarlyStopping(patience=10)\nhist = c_s_model.fit(make_X2(c_train_data_norm), c_train_data_norm[['TargetValue', 'Weight']].values, batch_size=2**14, epochs=100, validation_split=0.2, callbacks=[cb_checkpoint, early_stopping], shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# c_s_e_model = esemble_model(3,)\n# model_path = './e_cv19_predict5.h5'  # '{epoch:02d}-{val_loss:.4f}.h5'\n# cb_checkpoint = ModelCheckpoint(filepath=model_path, monitor='val_loss', verbose=1, save_best_only=True)\n# early_stopping = EarlyStopping(patience=10)\n# hist = c_s_e_model.fit(make_X2(c_train_data_norm), [c_train_data_norm[['TargetValue', 'Weight']].values], batch_size=256, epochs=100, validation_split=0.2, callbacks=[cb_checkpoint, early_stopping], shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"c_s_model.load_weights('./cv19_predict5.h5')\ntest_result = c_s_model.predict(make_X2(c_train_data_norm))\n# print(test_result, c_train_data_norm['TargetValue'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from matplotlib import pyplot as plt\n# plt.plot(test_result[0].reshape(349763))\nplt.plot(test_result.reshape(len(c_train_data_norm)), alpha=0.5)\n# plt.plot(test_result[2].reshape(349763))\nplt.plot(c_train_data_norm['TargetValue'].reset_index(drop=True), alpha=0.5)\n# plt.ylim(-100, 100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plt.plot(c_train_data_norm['TargetValue'])\n# plt.show()\nplt.plot(c_train_data_norm['TargetValue'].reset_index(drop=True))\n# plt.ylim(-100, 100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"min_max_scaler_f = MinMaxScaler()\n# fitted = min_max_scaler_f.fit(f_train_data[INPUT_DATA + ['TargetValue']])\n# print(fitted.data_max_)\nf_train_data_norm = f_train_data.copy()\n# f_train_data_norm[INPUT_DATA + ['TargetValue']] = min_max_scaler.transform(f_train_data[INPUT_DATA + ['TargetValue']])\n# output = min_max_scaler.transform(output)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f_s_model = simple_model(2,)\nmodel_path = './cv19_predict_f.h5'  # '{epoch:02d}-{val_loss:.4f}.h5'\ncb_checkpoint = ModelCheckpoint(filepath=model_path, monitor='val_loss', verbose=1, save_best_only=True)\nearly_stopping = EarlyStopping(patience=10)\nhist = f_s_model.fit(make_X2(f_train_data_norm), [f_train_data_norm[['TargetValue', 'Weight']].values], batch_size=2**14, epochs=100, validation_split=0.2, callbacks=[cb_checkpoint, early_stopping])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"c_train_data, f_train_data = make_dataset()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data = pd.read_csv(TEST_DATA_PATH, dtype=CAL_TYPE)\ntest_data['Date'] = pd.to_datetime(test_data['Date'])\ntest_data['Date'] = pd.to_datetime(test_data['Date'])\ntest_data['week'] = test_data['Date'].dt.weekday\ntest_data['day'] = test_data['Date'].dt.day\ntest_data['month'] = test_data['Date'].dt.month\ntest_first_date = test_data['Date'].unique()[0]\nexist_data_c = c_train_data[c_train_data['Date'] >= test_first_date]\nexist_data_f = f_train_data[f_train_data['Date'] >= test_first_date]\n# test_data['Target']\ntarget_list = np.zeros(len(test_data))\n# test_data['target_before']\ntarget_before_list = np.zeros(len(test_data))\n# make id\ntest_data['id'] = test_data['Country_Region'].str.cat(test_data['Province_State'], sep=' ', na_rep='')\ntest_data['id'] = test_data['id'].str.cat(test_data['County'], sep=' ', na_rep='')\ntest_data['id'] = test_data['id'].astype('category')\ntest_data['id'] = test_data['id'].cat.codes.astype('int16')\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"c_test_data = test_data[test_data.Target == 'ConfirmedCases']\nf_test_data = test_data[test_data.Target == 'Fatalities']\n# c_test_data['target_7'] = c_test_data.groupby(by='id')['target_before'].rolling(7).mean().reset_index(0, drop=True)\ndate_list = c_test_data.sort_values(by = 'Date').Date.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def test_seq(date_list, test_data, test_model, min_max_scaler):\n    test_data.loc[test_data.index, 'target_before'] = test_data.groupby(by='id')['TargetValue'].shift(periods=1)\n    test_data.loc[test_data.index, 'target_7'] = test_data.groupby(by='id')['target_before'].rolling(7).mean().reset_index(0, drop=True)\n    test_data.loc[test_data.index, 'target_3'] = test_data.groupby(by='id')['target_before'].rolling(3).mean().reset_index(0, drop=True)\n    group_by_test = test_data.groupby(by='Date')\n    result_data = []\n    for data in tqdm(date_list, position=0):\n        tmp_test_data = group_by_test.get_group(data)\n#         if True:\n        if np.sum(pd.isnull(tmp_test_data.TargetValue)) != 0:\n#             print(tmp_test_data[['target_before', 'target_7', 'target_3']])\n#             tmp_test_data.loc[tmp_test_data.index, INPUT_DATA + ['TargetValue']] = min_max_scaler.transform(tmp_test_data[INPUT_DATA + ['TargetValue']])\n#             print(make_X2(tmp_test_data))\n            pre_data = test_model.predict_on_batch(make_X2(tmp_test_data))\n#             print(pre_data)\n#             tmp_test_data['TargetValue_1'] = pre_data[0].numpy()\n#             tmp_test_data['TargetValue_3'] = pre_data[2].numpy()\n            tmp_test_data['TargetValue'] = pre_data.numpy().reshape(len(tmp_test_data))\n#             tmp_test_data[INPUT_DATA + ['TargetValue_1']] = min_max_scaler.inverse_transform(tmp_test_data[INPUT_DATA + ['TargetValue_1']])\n#             tmp_test_data[INPUT_DATA + ['TargetValue_3']] = min_max_scaler.inverse_transform(tmp_test_data[INPUT_DATA + ['TargetValue_3']])\n#             tmp_test_data[INPUT_DATA + ['TargetValue']] = min_max_scaler.inverse_transform(tmp_test_data[INPUT_DATA + ['TargetValue']])\n            test_data.loc[tmp_test_data.index, 'TargetValue'] = tmp_test_data['TargetValue']\n#             print(tmp_test_data[['TargetValue_1', 'TargetValue_3', 'TargetValue']])\n            for idata in tmp_test_data.itertuples():\n                result_data.append([str(int(idata.ForecastId)) + '_0.05', idata.TargetValue])\n                result_data.append([str(int(idata.ForecastId)) + '_0.5', idata.TargetValue])\n                result_data.append([str(int(idata.ForecastId)) + '_0.95', idata.TargetValue])\n            test_data = test_data.sort_values(by=['id', 'Date'])\n            test_data['target_before'] = test_data.groupby(by='id')['TargetValue'].shift(periods=1)\n            test_data['target_7'] = test_data.groupby(by='id')['target_before'].rolling(7).mean().reset_index(0, drop=True)\n            test_data['target_5'] = test_data.groupby(by='id')['target_before'].rolling(5).mean().reset_index(0, drop=True)\n            test_data['target_3'] = test_data.groupby(by='id')['target_before'].rolling(3).mean().reset_index(0, drop=True)\n            group_by_test = test_data.groupby(by='Date')    \n            \n        else:\n            for idata in tmp_test_data.itertuples():\n                result_data.append([str(int(idata.ForecastId)) + '_0.05', idata.TargetValue])\n                result_data.append([str(int(idata.ForecastId)) + '_0.5', idata.TargetValue])\n                result_data.append([str(int(idata.ForecastId)) + '_0.95', idata.TargetValue])\n    return pd.DataFrame(result_data, columns=['ForecastId_Quantile', 'TargetValue'])\n            \n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tt = pd.merge(c_test_data[['ForecastId', 'County', 'Province_State', 'Country_Region',\n       'Population', 'Weight', 'Date', 'Target', 'week', 'day', 'month', 'id',]], c_train_data, how='outer', on=[ 'County', 'Province_State', 'Country_Region', 'Population', 'Weight', 'Date', 'Target', 'week', 'day', 'month', 'id'])\nc_s_model.load_weights('./cv19_predict5.h5')\n# tt=tt.sort_values(by='Date')\nc_result = test_seq(date_list, tt.sort_values(by=['id', 'Date']), c_s_model, min_max_scaler)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"c_result","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tt = pd.merge(f_test_data[['ForecastId', 'County', 'Province_State', 'Country_Region',\n       'Population', 'Weight', 'Date', 'Target', 'week', 'day', 'month', 'id',]], f_train_data, how='outer', on=[ 'County', 'Province_State', 'Country_Region', 'Population', 'Weight', 'Date', 'Target', 'week', 'day', 'month', 'id'])\nf_s_model.load_weights('./cv19_predict_f.h5')\n# tt=tt.sort_values(by='Date')\nf_result = test_seq(date_list, tt.sort_values(by=['id', 'Date']), f_s_model, min_max_scaler_f)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result = pd.concat([c_result, f_result])\nresult.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result[result['ForecastId_Quantile'] == '207_0.05']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# tt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}