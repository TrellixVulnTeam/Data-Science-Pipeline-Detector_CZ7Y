{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import math\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.cluster import KMeans\n\n\ndef rate(frame, key, target, new_target_name=\"rate\"):\n    import numpy as np\n\n\n    corrections = 0\n    group_keys = frame[ key].values.tolist()\n    target = frame[target].values.tolist()\n    rate=[1.0 for k in range (len(target))]\n\n    for i in range(1, len(group_keys) ):\n        previous_group = group_keys[i - 1]\n        current_group = group_keys[i]\n\n        previous_value = target[i - 1]\n        current_value = target[i]\n         \n        if current_group == previous_group:\n                if previous_value!=0.0:\n                     rate[i]=current_value/previous_value\n\n                 \n        rate[i] =max(1,rate[i] )#correct negative values\n\n    frame[new_target_name] = np.array(rate)\n\n  \ndef get_cumulative(frame, key, target, new_target_name=\"diff\"):\n    \n    corrections = 0\n    group_keys = frame[ key].values.tolist()\n    target = frame[target].values.tolist()\n\n    for i in range(1, len(group_keys) ):\n        previous_group = group_keys[i - 1]\n        current_group = group_keys[i]\n\n        previous_value = target[i - 1]\n        current_value = target[i]\n         \n        if current_group == previous_group:\n                target[i]=previous_value + current_value\n          \n        target[i] =max(0,target[i] )\n\n    frame[new_target_name] = np.array(target)    \n    \n        \ndef get_difference(frame, key, target, new_target_name=\"diff\"):\n    import numpy as np\n\n\n    corrections = 0\n    group_keys = frame[ key].values.tolist()\n    target = frame[target].values.tolist()\n    rate=[0 for k in range (len(target))]\n\n    for i in range(1, len(group_keys) ):\n        previous_group = group_keys[i - 1]\n        current_group = group_keys[i]\n\n        previous_value = target[i - 1]\n        current_value = target[i]\n         \n        if current_group == previous_group:\n                rate[i]=max(0,current_value-previous_value)\n          \n        rate[i] =max(0,rate[i] )\n\n    frame[new_target_name] = np.array(rate)\n   \n\ndef get_time(frame, key, new_target_name=\"time\"):\n\n    corrections = 0\n    group_keys = frame[ key].values.tolist()\n    rate=[1 for k in range (len(group_keys))]\n    time_counter=1\n    for i in range(1, len(group_keys) ):\n        previous_group = group_keys[i - 1]\n        current_group = group_keys[i]\n         \n        if current_group == previous_group:\n                time_counter+=1\n                rate[i]=time_counter\n        else :\n                time_counter=1\n                rate[i]=time_counter                \n\n    frame[new_target_name] = np.array(rate)\n\ndef get_x_day_min_max_avg(frame, key, targetcol,window=7, new_target_name=\"window\"):\n\n    corrections = 0\n    group_keys = frame[ key].values.tolist()\n    tar = frame[ targetcol].values.tolist()\n    \n    max_values=[0.0 for k in range (len(group_keys))]\n    min_values=[0.0 for k in range (len(group_keys))]\n    mean_values=[0.0 for k in range (len(group_keys))] \n    \n    time_counter=1\n    this_group_stats=[]    \n    this_group_stats=[tar[0]]\n        \n    max_values[0]=np.max(this_group_stats[max(0,len(this_group_stats)-window) :len(this_group_stats)] )\n    min_values[0]=np.min(this_group_stats[max(0,len(this_group_stats)-window) :len(this_group_stats)] )\n    mean_values[0]=np.mean(this_group_stats[max(0,len(this_group_stats)-window) :len(this_group_stats)] )      \n    \n    for i in range(1, len(group_keys) ):\n        previous_group = group_keys[i - 1]\n        current_group = group_keys[i]\n        this_target=tar[i]\n         \n        if current_group == previous_group:\n                this_group_stats.append(this_target)\n                time_counter+=1\n                max_values[i]=np.max(this_group_stats[max(0,len(this_group_stats)-window) :len(this_group_stats)] )\n                min_values[i]=np.min(this_group_stats[max(0,len(this_group_stats)-window) :len(this_group_stats)] )\n                mean_values[i]=np.mean(this_group_stats[max(0,len(this_group_stats)-window) :len(this_group_stats)] )   \n                \n        else :\n            \n                this_group_stats=[]\n                this_group_stats.append(this_target)                \n                time_counter=1\n                \n                \n                max_values[i]=np.max(this_group_stats[max(0,len(this_group_stats)-window) :len(this_group_stats)] )\n                min_values[i]=np.min(this_group_stats[max(0,len(this_group_stats)-window) :len(this_group_stats)] )\n                mean_values[i]=np.mean(this_group_stats[max(0,len(this_group_stats)-window) :len(this_group_stats)] )               \n\n    frame[new_target_name+\"_max\" ] = np.array(max_values)    \n    frame[new_target_name+\"_min\"  ] = np.array(min_values) \n    frame[new_target_name+\"_mean\"  ] = np.array(mean_values) \n    \ndef get_difference_special(frame, key, target, new_target_name=\"diff\"):\n    import numpy as np\n\n\n    corrections = 0\n    group_keys = frame[ key].values.tolist()\n    target = frame[target].values.tolist()\n    rate=[0 for k in range (len(target))]\n    equal_counter=0\n    for i in range(1, len(group_keys) ):\n        previous_group = group_keys[i - 1]\n        current_group = group_keys[i]\n\n        previous_value = target[i - 1]\n        current_value = target[i]\n         \n        if current_group == previous_group:\n                rate[i]=max(0,current_value-previous_value)\n                if equal_counter==0: \n                    rate[i-1]=rate[i]\n                equal_counter+=1\n        else :\n            equal_counter=0\n        rate[i] =max(0,rate[i] )\n\n            \n    frame[new_target_name] = np.array(rate) \n\ndef get_data_by_key(dataframe, key, key_value, fields=None):\n    mini_frame=dataframe[dataframe[key]==key_value]\n    if not fields is None:                \n        mini_frame=mini_frame[fields].values\n        \n    return mini_frame\n\n#pinball loss with 3 different predictions for different quantiles\ndef pinball_loss_many_marios(ytrue,pred_05,pred_50,pred_95, weight):\n    assert len(ytrue)==len(pred_05)==len(pred50)==len(pred95)==len(weight)\n    total_pin_ball=0.0\n    total_count=0.0\n    for i in range(len(ytrue)):\n        error_05=0.0\n        error_50=0.0\n        error_95=0.0\n        total_count+=1.\n        #####0.05 quantile\n        if ytrue[i]>=pred_05[i]:\n            error_05+=0.05*(ytrue[i]-pred_05[i])\n        else :\n            error_05+=(1-0.05)*(pred_05[i]-ytrue[i])\n        #####0.50 quantile        \n        if ytrue[i]>=pred_50[i]:\n            error_50+=0.5*(ytrue[i]-pred_50[i])\n        else :\n            error_50+=(1-0.5)*(pred_50[i]-ytrue[i])  \n        #####0.95 quantile             \n        if ytrue[i]>=pred_95[i]:\n            error_95+=0.95*(ytrue[i]-pred_95[i])\n        else :\n            error_95+=(1-0.95)*(pred_95[i]-ytrue[i]) \n        \n        total_pin_ball+=weight[i]*(error_05 + error_50 + error_95)/3.\n    return   total_pin_ball/ total_count \n\n#pinball loss assuming prediction is the same for all .05,.5,0.95    \ndef pinball_loss_single(ytrue,pred, weight):\n    assert len(ytrue)==len(pred)==len(weight)\n    total_pin_ball=0.0\n    total_count=0.0\n    for i in range(len(ytrue)):\n        error_05=0.0\n        error_50=0.0\n        error_95=0.0\n        total_count+=1.\n        #####0.05 quantile\n        if ytrue[i]>=pred[i]:\n            error_05+=0.05*(ytrue[i]-pred[i])\n        else :\n            error_05+=(1-0.05)*(pred[i]-ytrue[i])\n        #####0.50 quantile        \n        if ytrue[i]>=pred[i]:\n            error_50+=0.5*(ytrue[i]-pred[i])\n        else :\n            error_50+=(1-0.5)*(pred[i]-ytrue[i])  \n        #####0.95 quantile             \n        if ytrue[i]>=pred[i]:\n            error_95+=0.95*(ytrue[i]-pred[i])\n        else :\n            error_95+=(1-0.95)*(pred[i]-ytrue[i]) \n        \n        total_pin_ball+=weight[i]*(error_05 + error_50 + error_95)/3.\n    return   total_pin_ball/ total_count  \n\n#pinball loss assuming prediction is the same for all .05,.5,0.95    \ndef pinball_loss_single_with_t(ytrue,pred, weight, t=0.05):\n    assert len(ytrue)==len(pred)==len(weight)\n    total_pin_ball=0.0\n    total_count=0.0\n    for i in range(len(ytrue)):\n        error=0.0\n        total_count+=1.\n        #####0.05 quantile\n        if t==0.05:\n            if ytrue[i]>=pred[i]:\n                error+=0.05*(ytrue[i]-pred[i])\n            else :\n                error+=(1-0.05)*(pred[i]-ytrue[i])\n        elif t==0.5:\n            #####0.50 quantile        \n            if ytrue[i]>=pred[i]:\n                error+=0.5*(ytrue[i]-pred[i])\n            else :\n                error+=(1-0.5)*(pred[i]-ytrue[i])  \n        elif t==0.95:\n            #####0.95 quantile     \n            if ytrue[i]>=pred[i]:\n                error+=0.95*(ytrue[i]-pred[i])\n            else :\n                error+=(1-0.95)*(pred[i]-ytrue[i])\n\n        total_pin_ball+=weight[i]*(error)\n        \n    return   total_pin_ball/ total_count  \n    \ndef fix_target(frame, key, target, new_target_name=\"target\"):\n    import numpy as np\n\n    corrections = 0\n    group_keys = frame[ key].values.tolist()\n    target = frame[target].values.tolist()\n\n    for i in range(1, len(group_keys) - 1):\n        previous_group = group_keys[i - 1]\n        current_group = group_keys[i]\n\n        target[i] =max(0,target[i] )#correct negative values\n\n    frame[new_target_name] = np.array(target)\n    \n\ndirectory=\"/kaggle/input/covid19-global-forecasting-week-5/\"\nmodel_directory=\"/kaggle/input/model-v9-2/model\"\ntrain_file=\"train.csv\"\ntest_file=\"test.csv\"\n\nbagging=1\nminimum_count_for_rate_model=2000\n\nextra_stable_columns=None\ngeo_dir=None\nextra_stable_columns=None\ngroup_by_columns=None\ngroup_names=None\ntrain_frame_supplamanteary=None\n\n\n##################load data and bring them into previous competition format################\n\nuse_external=True\nexternal_file=\"/kaggle/input/extra-day-11/extra_data_11_5_2020_v2.csv\" #use external data from https://www.worldometers.info/coronavirus/#countries\n#data obtained with the scraper: https://www.kaggle.com/philippsinger/covid-w5-worldometer-scraper\n\nholdder_cumulative={}\nholdder={}\n\ntrain=pd.read_csv(directory + train_file, parse_dates=[\"Date\"] , engine=\"python\")\ntest=pd.read_csv(directory + test_file, parse_dates=[\"Date\"], engine=\"python\")\n\ntrain.head()\n\ntrain_fatalities=train[train.Target==\"Fatalities\"]\ntrain_confirmed=train[train.Target==\"ConfirmedCases\"]\n\nif use_external:\n    extra_data=pd.read_csv(external_file)\n    extra_data[\"key\"]=extra_data[[\"County\",\"Province_State\",\"Country_Region\"]].apply(\n        lambda row: str(row[0]) + \"_\" + str(row[1])+ \"_\" + str(row[2]),axis=1)\n    extra_train_fatalities=extra_data[extra_data.Target==\"Fatalities\"]\n    extra_train_confirmed=extra_data[extra_data.Target==\"ConfirmedCases\"]\n    print (\"extra_train_fatalities shape\", extra_train_fatalities.shape)\n    print (\"extra_train_confirmed shape\", extra_train_confirmed.shape) \n    holdder={}\n    for f in [extra_train_confirmed,extra_train_fatalities]:\n        keysss=f[\"key\"].values\n        valuess=f[\"TargetValue\"].values\n        for g in range (len(keysss)):\n            if keysss[g] in holdder:\n                holdder[keysss[g]].append(valuess[g])\n                assert len(holdder[keysss[g]])==2\n            else :\n                holdder[keysss[g]]=[valuess[g]]\n        \n            \n\ntrain_fatalities.columns=[\"Id_Fatalities\",\"County\",\"Province_State\",\"Country_Region\",\n                          \"Population\",\"Weight_Fatalities\",\"Date\",\"Target\",\"diff_Fatalities\"]\n\ntrain_confirmed.columns=[\"Id_ConfirmedCases\",\"County\",\"Province_State\",\"Country_Region\",\n                         \"Population\",\"Weight_ConfirmedCases\",\"Date\",\"Target\",\"diff_ConfirmedCases\"]\n\ntrain_confirmed.drop(\"Population\", inplace=True, axis=1)\n\ntrain_fatalities.drop(\"Target\", axis=1, inplace=True)\ntrain_confirmed.drop(\"Target\", axis=1, inplace=True)\n\ntrain=pd.merge(train_confirmed, train_fatalities, how=\"left\", left_on=[\"County\",\"Province_State\",\"Country_Region\",\"Date\"],\n              right_on=[\"County\",\"Province_State\",\"Country_Region\",\"Date\"])\n\ntrain=train[[\"Id_Fatalities\",\"Id_ConfirmedCases\",\"Date\",\"County\",\"Province_State\",\"Country_Region\",\n                          \"Population\",\"Weight_ConfirmedCases\",\"Weight_Fatalities\",\"diff_ConfirmedCases\",\"diff_Fatalities\"]]\n\n\ntrain[\"key\"]=train[[\"County\",\"Province_State\",\"Country_Region\"]].apply(lambda row: str(row[0]) + \"_\" + str(row[1])+ \"_\" + str(row[2]),axis=1)\n\n\nif use_external:\n    for country_key,arra in holdder.items():\n        print (\" insterting key %s\" % (country_key))\n        confirmed_diff_hold,fatalities_diff_hold=holdder[country_key]\n        mini_train=train[train.key==country_key]#.values.tolist()\n        #print(mini_train.head(10))\n        #fat_values=train[train.key==country_key,\"diff_Fatalities\"].values.tolist()  \n        conf_values=mini_train[\"diff_ConfirmedCases\"].values.tolist()         \n        fat_values=mini_train[\"diff_Fatalities\"].values.tolist()  \n        \n        for j in range (1, len(conf_values)):\n            conf_values[j-1]= conf_values[j]\n            fat_values[j-1]= fat_values[j]\n            \n        conf_values[-1]=confirmed_diff_hold    \n        fat_values[-1]=fatalities_diff_hold  \n        \n\n        train.loc[train.key==country_key, 'diff_ConfirmedCases'] = np.array(conf_values)\n        train.loc[train.key==country_key, 'diff_Fatalities'] =np.array(fat_values)\n\n    \nget_time(train, \"key\", new_target_name=\"time\") #get time\n\n\nfix_target(train, \"key\", \"diff_ConfirmedCases\", new_target_name=\"diff_ConfirmedCases\")\nfix_target(train, \"key\", \"diff_Fatalities\", new_target_name=\"diff_Fatalities\")\n\nget_cumulative(train, \"key\", \"diff_ConfirmedCases\", new_target_name=\"ConfirmedCases\")\nget_cumulative(train, \"key\", \"diff_Fatalities\", new_target_name=\"Fatalities\")\n\nif use_external:\n    last_date=train[train[\"Date\"]== train[\"Date\"].max()]\n    all_keys=last_date[\"key\"].values.tolist()\n    all_con=last_date[\"ConfirmedCases\"].values.tolist()\n    all_fat=last_date[\"Fatalities\"].values.tolist()\n    holdder_cumulative={}\n    for jj in range (len(all_keys)):\n        if all_keys[jj] in holdder:\n            print (\" adding %s to cumulative holder \" %(all_keys[jj]))\n            holdder_cumulative[all_keys[jj]]=[all_con[jj] ,all_fat[jj] ]\n    print (\" there are %d elements in the extra data \" %(len(holdder_cumulative)))\n    assert (len(holdder_cumulative))==len(holdder)\n\n        \n\nrate(train, \"key\", \"ConfirmedCases\", new_target_name=\"rate_\" +\"ConfirmedCases\" )\nrate(train, \"key\", \"Fatalities\", new_target_name=\"rate_\" +\"Fatalities\" )\n\ntrain[\"dow\"]=train[\"Date\"].dt.dayofweek\nprint(train[\"dow\"].head(10))\n\n\n#target1=\"ConfirmedCases\"\n#target2=\"Fatalities\"\n#key=\"key\"\n\n#train.to_csv(directory + \"train_reshapedv2.csv\", index=False)\n\n\nmax_train_date=train[\"Date\"].max()\nmax_test_date=test[\"Date\"].max()\nmin_test_date=test[\"Date\"].min()\nhorizon=  (max_test_date-max_train_date).days\nprint (\"horizon\", int(horizon))\nprint (\"max train date\", max_train_date)\nprint (\"max test date\", max_test_date)\nprint (\"min test date\", min_test_date)\n\ntest_fatalities=test[test.Target==\"Fatalities\"]\ntest_confirmed=test[test.Target==\"ConfirmedCases\"]\n\ntest_fatalities.columns=[\"Id_Fatalities\",\"County\",\"Province_State\",\"Country_Region\",\n                          \"Population\",\"Weight_Fatalities\",\"Date\",\"Target_Fatalities\"]\n\ntest_confirmed.columns=[\"Id_ConfirmedCases\",\"County\",\"Province_State\",\"Country_Region\",\n                         \"Population\",\"Weight_ConfirmedCases\",\"Date\",\"Target_ConfirmedCases\"]\n\ntest_confirmed.drop(\"Population\", inplace=True, axis=1)\n\ntest=pd.merge(test_confirmed, test_fatalities, how=\"left\", left_on=[\"County\",\"Province_State\",\"Country_Region\",\"Date\"],\n              right_on=[\"County\",\"Province_State\",\"Country_Region\",\"Date\"])\n\n\ntest=test[[\"Id_Fatalities\",\"Id_ConfirmedCases\",\"Date\",\"County\",\"Province_State\",\"Country_Region\",\n                          \"Population\",\"Weight_ConfirmedCases\",\"Weight_Fatalities\",\"Target_ConfirmedCases\",\"Target_Fatalities\"]]                         \n\ntest[\"key\"]=test[[\"County\",\"Province_State\",\"Country_Region\"]].apply(lambda row: str(row[0]) + \"_\" + str(row[1])+ \"_\" + str(row[2]),axis=1)\n\n\n#test.to_csv(directory + \"test_reshaped.csv\", index=False)\n\nkey=\"key\"\nunique_keys=train[key].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_lags(rate_array, current_index, size=20):\n    lag_confirmed_rate=[-1 for k in range(size)]\n    for j in range (0, size):\n        if current_index-j>=0:\n            lag_confirmed_rate[j]=rate_array[current_index-j]\n        else :\n            break\n    return lag_confirmed_rate\n\ndef days_ago_thresold_hit(full_array, indx, thresold):\n        days_ago_confirmed_count_10=-1\n        if full_array[indx]>thresold: # if currently the count of confirmed is more than 10\n            for j in range (indx,-1,-1):\n                entered=False\n                if full_array[j]<=thresold:\n                    days_ago_confirmed_count_10=abs(j-indx)\n                    entered=True\n                    break\n                if entered==False:\n                    days_ago_confirmed_count_10=100 #this value would we don;t know it cross 0      \n        return days_ago_confirmed_count_10 \n    \n    \ndef ewma_vectorized(data, alpha):\n    sums=sum([ (alpha**(k+1))*data[k] for  k in range(len(data)) ])\n    counts=sum([ (alpha**(k+1)) for  k in range(len(data)) ])\n    return sums/counts\n\ndef generate_ma_std_window(rate_array, current_index, size=20, window=3):\n    ma_rate_confirmed=[-1 for k in range(size)]\n    std_rate_confirmed=[-1 for k in range(size)] \n    \n    for j in range (0, size):\n        if current_index-j>=0:\n            ma_rate_confirmed[j]=np.mean(rate_array[max(0,current_index-j-window+1 ):current_index-j+1])\n            std_rate_confirmed[j]=np.std(rate_array[max(0,current_index-j-window+1 ):current_index-j+1])           \n        else :\n            break\n    return ma_rate_confirmed, std_rate_confirmed\n\ndef generate_ewma_window(rate_array, current_index, size=20, window=3, alpha=0.05):\n    ewma_rate_confirmed=[-1 for k in range(size)]\n\n    \n    for j in range (0, size):\n        if current_index-j>=0:\n            ewma_rate_confirmed[j]=ewma_vectorized(rate_array[max(0,current_index-j-window+1 ):current_index-j+1, ], alpha)           \n        else :\n            break\n    \n    #print(ewma_rate_confirmed)\n    return ewma_rate_confirmed\n\n\ndef get_target(rate_col, indx, horizon=33, average=3, use_hard_rule=False):\n    target_values=[-1 for k in range(horizon)]\n    cou=0\n    for j in range(indx+1, indx+1+horizon):\n        if j<len(rate_col):\n            if average==1:\n                target_values[cou]=rate_col[j]\n            else :\n                if use_hard_rule and j +average <=len(rate_col) :\n                     target_values[cou]=np.mean(rate_col[j:j +average])\n                else :\n                    target_values[cou]=np.mean(rate_col[j:min(len(rate_col),j +average)])                   \n            cou+=1\n        else :\n            break\n    return target_values\n\ndef get_target_count(rate_col, indx, horizon=33, average=3, use_hard_rule=False):\n    target_values=[-1 for k in range(horizon)]\n    cou=0\n    for j in range(indx+1, indx+1+horizon):\n        if j<len(rate_col):\n            if average==1:\n                target_values[cou]=rate_col[j]\n            else :\n                if use_hard_rule and j +average <=len(rate_col) :\n                     target_values[cou]=np.mean(rate_col[j:j +average])\n                else :\n                    target_values[cou]=np.mean(rate_col[j:min(len(rate_col),j +average)])\n                   \n            cou+=1\n        else :\n            break\n    return target_values\n\n\ndef dereive_features(frame, confirmed, fatalities, rate_confirmed, rate_fatalities, count_confirmed, count_fatalities ,\n                     population,weight_confirmed,weight_fatalities,day_of_week,time,\n                     horizon ,size=20, windows=[3,7], days_back_confimed=[1,10,100], days_back_fatalities=[1,2,10], \n                    extra_data=None, groups_data=None, windows_group=[3,7], size_group=20,\n                    days_back_confimed_group=[1,10,100]):\n    targets=[]\n    if not extra_data is None:\n        assert len(extra_stable_columns)==extra_data.shape[1]\n        \n    if not groups_data is None:\n        assert len(group_names)==groups_data.shape[1]        \n    names=[]    \n    names=[\"lag_confirmed_rate\" + str(k+1) for k in range (28)]\n    names+=[\"lag_confirmed_count\" + str(k+1) for k in range (28)]    \n    for day in days_back_confimed:\n        names+=[\"days_ago_confirmed_count_\" + str(day) ]\n    for window in windows:        \n        names+=[\"ma\" + str(window) + \"_rate_confirmed\" + str(k+1) for k in range (size)]\n        #names+=[\"std\" + str(window) + \"_rate_confirmed\" + str(k+1) for k in range (size)] \n        #names+=[\"ewma\" + str(window) + \"_rate_confirmed\" + str(k+1) for k in range (size)]         \n        names+=[\"ma\" + str(window) + \"_count_confirmed\" + str(k+1) for k in range (size)]        \n        \n    names+=[\"lag_fatalities_rate\" + str(k+1) for k in range (28)]\n    names+=[\"lag_fatalities_count\" + str(k+1) for k in range (28)]    \n    \n    for day in days_back_fatalities:\n        names+=[\"days_ago_fatalitiescount_\" + str(day) ]    \n    for window in windows:        \n        names+=[\"ma\" + str(window) + \"_rate_fatalities\" + str(k+1) for k in range (size)]\n        #names+=[\"std\" + str(window) + \"_rate_fatalities\" + str(k+1) for k in range (size)]  \n        #names+=[\"ewma\" + str(window) + \"_rate_fatalities\" + str(k+1) for k in range (size)] \n        names+=[\"ma\" + str(window) + \"_count_fatalities\" + str(k+1) for k in range (size)]\n        \n    names+=[\"confirmed_level\"]\n    names+=[\"fatalities_level\"]  \n    \n    names+=[\"weight_confirmed\"]\n    names+=[\"weight_fatalities\"]     \n    names+=[\"population\"]  \n    names+=[\"dowthis\"]      \n    names+=[\"timethis\"]          \n\n    \n    if not extra_data is None: \n        names+=[k for k in extra_stable_columns]\n    if not groups_data is None:  \n         for gg in range (groups_data.shape[1]):\n             #names+=[\"lag_rate_group_\"+ str(gg+1) + \"_\" + str(k+1) for k in range (size_group)]    \n             for day in days_back_confimed_group:\n                names+=[\"days_ago_grooupcount_\" + str(gg+1) + \"_\" + str(day) ]             \n             for window in windows_group:        \n                names+=[\"ma_group_\" + str(gg+1) + \"_\" + str(window) + \"_rate_\" + str(k+1) for k in range (size_group)]\n                #names+=[\"std_group_\" + str(gg+1)+ \"_\" + str(window) + \"_rate_\" + str(k+1) for k in range (size_group)]  \n                #names+=[\"ewma_group_\" + str(gg+1) + \"_\" + str(window) + \"_rate_\" + str(k+1) for k in range (size)]  \n\n            \n    names+=[\"confirmed_plus\" + str(k+1) for k in range (horizon)]    \n    names+=[\"fatalities_plus\" + str(k+1) for k in range (horizon)]  \n    names+=[\"confirmed_count_plus\" + str(k+1) for k in range (horizon)]    \n    names+=[\"fatalities_count_plus\" + str(k+1) for k in range (horizon)]      \n    #names+=[\"current_confirmed\"]\n    #names+=[\"current_fatalities\"]    \n    \n    features=[]\n    for i in range (len(confirmed)):\n        row_features=[]\n        #####################lag_confirmed_rate       \n        lag_confirmed_rate=get_lags(rate_confirmed, i, size=28)\n        lag_confirmed_count=get_lags(count_confirmed, i, size=28)        \n        row_features+=lag_confirmed_rate\n        row_features+=lag_confirmed_count        \n        #####################days_ago_confirmed_count_10\n        for day in days_back_confimed:\n            days_ago_confirmed_count_10=days_ago_thresold_hit(confirmed, i, day)               \n            row_features+=[days_ago_confirmed_count_10] \n        #####################ma_rate_confirmed       \n        #####################std_rate_confirmed \n        for window in windows:\n            ma3_rate_confirmed,std3_rate_confirmed= generate_ma_std_window(rate_confirmed, i, size=size, window=window)\n            row_features+= ma3_rate_confirmed   \n            #row_features+= std3_rate_confirmed          \n            #ewma3_rate_confirmed=generate_ewma_window(rate_confirmed, i, size=size, window=window, alpha=0.05)\n            #row_features+= ewma3_rate_confirmed \n            ma3_count_confirmed,std3_count_confirmed= generate_ma_std_window(count_confirmed, i, size=size, window=window)  \n            row_features+= ma3_count_confirmed               \n        #####################lag_fatalities_rate   \n        lag_fatalities_rate=get_lags(rate_fatalities, i, size=28)\n        lag_fatalities_count=get_lags(count_fatalities, i, size=28)        \n        row_features+=lag_fatalities_rate\n        row_features+=lag_fatalities_count        \n        #####################days_ago_confirmed_count_10\n        for day in days_back_fatalities:\n            days_ago_fatalitiescount_2=days_ago_thresold_hit(fatalities, i, day)               \n            row_features+=[days_ago_fatalitiescount_2]     \n        #####################ma_rate_fatalities       \n        #####################std_rate_fatalities \n        for window in windows:        \n            ma3_rate_fatalities,std3_rate_fatalities= generate_ma_std_window(rate_fatalities, i, size=size, window=window)\n            row_features+= ma3_rate_fatalities             \n            #row_features+= std3_rate_fatalities  \n            #ewma3_rate_fatalities=generate_ewma_window(rate_fatalities, i, size=size, window=window, alpha=0.05)\n            #row_features+= ewma3_rate_fatalities\n            ma3_count_fatalities,std3_count_fatalities= generate_ma_std_window(count_fatalities, i, size=size, window=window)\n            row_features+= ma3_count_fatalities               \n            \n        ##################confirmed_level\n        confirmed_level=0\n        \n        \"\"\"\n        if confirmed[i]>0 and confirmed[i]<1000:\n            confirmed_level= confirmed[i]\n        else :\n            confirmed_level=2000\n        \"\"\"   \n        confirmed_level= confirmed[i]\n        row_features+=[confirmed_level]\n        \n        ##################fatalities_is_level\n        fatalities_is_level=0\n        \"\"\"\n        if fatalities[i]>0 and fatalities[i]<100:\n            fatalities_is_level= fatalities[i]\n        else :\n            fatalities_is_level=200            \n        \"\"\"\n        fatalities_is_level= fatalities[i]\n        \n        row_features+=[fatalities_is_level] \n        \n        conf_weight=  weight_confirmed[i]\n        fat_weight=  weight_fatalities[i]        \n        current_population=population[i]\n        doweek=day_of_week[i]\n        tim=time[i]\n        \n        \n                \n        row_features+=[conf_weight]\n        row_features+=[fat_weight]\n        row_features+=[current_population]\n        row_features+=[doweek]\n        row_features+=[tim]        \n        \n        if not extra_data is None:    \n            row_features+=extra_data[i].tolist()\n            \n        if not groups_data is None:  \n          for gg in range (groups_data.shape[1]): \n             ## lags per group\n             this_group=groups_data[:,gg].tolist()\n             lag_group_rate=get_lags(this_group, i, size=size_group)\n             #row_features+=lag_group_rate           \n             #####################days_ago_confirmed_count_10\n             for day in days_back_confimed_group:\n                days_ago_groupcount_2=days_ago_thresold_hit(this_group, i, day)               \n                row_features+=[days_ago_groupcount_2]     \n             #####################ma_rate_fatalities       \n             #####################std_rate_fatalities \n             for window in windows_group:        \n                ma3_rate_group,std3_rate_group= generate_ma_std_window(this_group, i, size=size_group, window=window)\n                row_features+= ma3_rate_group   \n                #row_features+= std3_rate_group             \n            \n            \n        #######################confirmed_plus target\n        confirmed_plus=get_target(rate_confirmed, i, horizon=horizon)\n        row_features+= confirmed_plus          \n        #######################fatalities_plus target\n        fatalities_plus=get_target(rate_fatalities, i, horizon=horizon)\n        row_features+= fatalities_plus \n            \n        #######################confirmed_plus target count\n        confirmed_plus=get_target(count_confirmed, i, horizon=horizon)\n        row_features+= confirmed_plus          \n        #######################fatalities_plus target\n        fatalities_plus=get_target(count_fatalities, i, horizon=horizon)\n        row_features+= fatalities_plus         \n        \n        \n        ##################current_confirmed\n        #row_features+=[confirmed[i]]\n        ##################current_fatalities\n        #row_features+=[fatalities[i]]        \n        \n          \n\n        \n        features.append(row_features)\n        \n    new_frame=pd.DataFrame(data=features, columns=names).reset_index(drop=True)\n    frame=frame.reset_index(drop=True)\n    frame=pd.concat([frame, new_frame], axis=1)\n    #print(frame.shape)\n    return frame\n    \n    \ndef feature_engineering_for_single_key(frame, group, key, horizon=33, size=20, windows=[3,7], \n                                       days_back_confimed=[1,10,100], days_back_fatalities=[1,2,10],\n                                      extra_stable_=None, group_nams=None,windows_group=[3,7], \n                                       size_group=20, days_back_confimed_group=[1,10,100]):\n    \n    mini_frame=get_data_by_key(frame, group, key, fields=None)\n    \n    mini_frame_with_features=dereive_features(mini_frame, mini_frame[\"ConfirmedCases\"].values,\n                                              mini_frame[\"Fatalities\"].values, mini_frame[\"rate_ConfirmedCases\"].values, \n                                               mini_frame[\"rate_Fatalities\"].values, mini_frame[\"diff_ConfirmedCases\"].values, \n                                               mini_frame[\"diff_Fatalities\"].values,mini_frame[\"Population\"].values\n                                              ,mini_frame[\"Weight_ConfirmedCases\"].values,mini_frame[\"Weight_Fatalities\"].values,\n                                              mini_frame[\"dow\"].values,mini_frame[\"time\"].values, horizon ,size=size, windows=windows,\n                                              days_back_confimed=days_back_confimed, days_back_fatalities=days_back_fatalities,\n                                              extra_data=mini_frame[extra_stable_].values if not extra_stable_ is None else None,\n                                              groups_data=mini_frame[group_nams].values if not group_nams is None else None,\n                                              windows_group=windows_group, size_group=size_group, \n                                              days_back_confimed_group=days_back_confimed_group)\n    #print (mini_frame_with_features.shape[0])\n    return mini_frame_with_features\n\nsize=10\nwindows=[3]\ndays_back_confimed=[1,5,10,20,50,100,500]\ndays_back_fatalities=[1,2,5,10,20,50,200]\n\nsize_group=10\nwindows_group=[3,5]\ndays_back_confimed_group=[1,10,100]\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tqdm import tqdm\ntrain_frame=[]\nsize=10\nwindows=[3]\ndays_back_confimed=[1,5,10,20,50,100,250,500,1000]\ndays_back_fatalities=[1,2,5,10,20,50]\n\nsize_group=10\nwindows_group=[3]\ndays_back_confimed_group=[1,10,100]\n\n\nprint (\"total unique keys \", len(train['key'].unique()))\nfor unique_k in tqdm(unique_keys):\n    mini_frame=feature_engineering_for_single_key(train, key, unique_k, horizon=horizon, size=size, \n                                                  windows=windows, days_back_confimed=days_back_confimed,\n                                                  days_back_fatalities=days_back_fatalities,\n                                                  extra_stable_=extra_stable_columns if extra_stable_columns is not None and len(extra_stable_columns)>0 else None,\n                                     group_nams=group_names,windows_group=windows_group, \n                                     size_group=size_group, days_back_confimed_group=days_back_confimed_group\n                                                 ).reset_index(drop=True) \n    #print (mini_frame.shape[0])\n    train_frame.append(mini_frame)\n    \ntrain_frame = pd.concat(train_frame, axis=0).reset_index(drop=True)\n#train_frame.to_csv(directory +\"all\" + \".csv\", index=False)\nnew_unique_keys=train_frame['key'].unique()\nprint (\"total unique new keys\" , len(new_unique_keys))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import lightgbm as lgb\nfrom sklearn.linear_model import Ridge\nfrom sklearn.externals import joblib\n\ndef bagged_set_trainc(X_ts,y_cs,wts, seed, estimators,xtest, xt=None,yt=None, output_name=None):\n   #print (type(yt))\n   # create array object to hold predictions \n  \n   baggedpred=np.array([ 0.0 for d in range(0, xtest.shape[0])]) \n   #print (y_cs[:10])\n   #print (yt[:10])  \n\n   #loop for as many times as we want bags\n   for n in range (0, estimators):\n       \n       params = {'objective': 'mae',\n                'metric': 'mae',\n                'boosting': 'gbdt',\n                'learning_rate': 0.005, #change here    \n                'drop_rate':0.005,\n                'alpha': 0.95, \n                'skip_drop':0.6,\n                'max_drop':2,                \n                'uniform_drop':True,               \n                'verbose': -1,    \n                'num_leaves': 40, # ~18    \n                'bagging_fraction': 0.9,    \n                'bagging_freq': 1,    \n                'bagging_seed': seed + n,    \n                'feature_fraction': 0.8,    \n                'feature_fraction_seed': seed + n,    \n                'min_data_in_leaf': 10, #30, #56, # 10-50    \n                'max_bin': 100, # maybe useful with overfit problem    \n                'max_depth':20,                   \n                #'reg_lambda': 10,    \n                'reg_alpha':1,    \n                'lambda_l2': 10,\n                #'categorical_feature':'2', # because training data is extremely unbalanced                     \n                'num_threads':38\n                }\n       d_train = lgb.Dataset(X_ts,y_cs, weight=wts, free_raw_data=False)#np.log1p(\n       if not type(yt) is type(None):           \n           d_cv = lgb.Dataset(xt,yt, free_raw_data=False, reference=d_train)#, reference=d_train\n           model = lgb.train(params,d_train,num_boost_round=500,\n                             valid_sets=d_cv,\n\n                             verbose_eval=500 ) #1000                        \n           \n       else :\n           #d_cv = lgb.Dataset(xt, free_raw_data=False, categorical_feature=\"2\")  \n           model = lgb.train(params,d_train,num_boost_round=500) #1000                              \n           #importances=model.feature_importance('gain')\n           #print(importances)\n       preds=model.predict(xtest)               \n       # update bag's array\n       baggedpred+=preds\n       #np.savetxt(\"preds_lgb\" + str(n)+ \".csv\",baggedpred)   \n       #if n%5==0:\n           #print(\"completed: \" + str(n)  )                 \n\n       if not output_name is None:\n            joblib.dump((model), output_name+ \"_\" +str(n))\n\n   # divide with number of bags to create an average estimate  \n   baggedpred/= estimators\n     \n   return baggedpred\n\n\ndef predict(xtest, estimators ,input_name=None):\n   #print (type(yt))\n   # create array object to hold predictions \n  \n   baggedpred=np.array([ 0.0 for d in range(0, xtest.shape[0])]) \n   for n in range (0, estimators):    \n       print(\"loading model %s\"% (input_name+ \"_\" +str(n)))\n       model=  joblib.load( input_name+ \"_\" +str(n)) \n       preds=model.predict(xtest)               \n       baggedpred+=preds\n   baggedpred/= estimators\n\n   return baggedpred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"names=[]\nnames+=[\"lag_confirmed_rate\" + str(k+1) for k in [6,13,20]]\nnames+=[\"lag_confirmed_count\" + str(k+1) for k in [6,13,20]]\nfor day in days_back_confimed:\n    names+=[\"days_ago_confirmed_count_\" + str(day) ]\nfor window in windows:        \n    names+=[\"ma\" + str(window) + \"_rate_confirmed\" + str(k+1) for k in range (size)]\n    #names+=[\"std\" + str(window) + \"_rate_confirmed\" + str(k+1) for k in range (size)] \n    #names+=[\"ewma\" + str(window) + \"_rate_confirmed\" + str(k+1) for k in range (size)]         \n    names+=[\"ma\" + str(window) + \"_count_confirmed\" + str(k+1) for k in range (size)]  \n\nnames+=[\"lag_fatalities_rate\" + str(k+1) for k in [6,13,20]]\nnames+=[\"lag_fatalities_count\" + str(k+1) for k in [6,13,20]]\nfor day in days_back_fatalities:\n    names+=[\"days_ago_fatalitiescount_\" + str(day) ]    \nfor window in windows:        \n    names+=[\"ma\" + str(window) + \"_rate_fatalities\" + str(k+1) for k in range (size)]\n    #names+=[\"std\" + str(window) + \"_rate_fatalities\" + str(k+1) for k in range (size)]  \n    #names+=[\"ewma\" + str(window) + \"_rate_fatalities\" + str(k+1) for k in range (size)]    \n    names+=[\"ma\" + str(window) + \"_count_fatalities\" + str(k+1) for k in range (size)]  \n      \n#names+=[\"confirmed_level\"]\n#names+=[\"fatalities_level\"]  \nnames+=[\"dowthis\"]  \n#names+=[\"timethis\"]\n#names+=[\"weight_confirmed\"]\n#names+=[\"weight_fatalities\"]     \n#names+=[\"population\"]  \n\nif not extra_stable_columns is None and len(extra_stable_columns)>0: \n    names+=[k for k in extra_stable_columns]  \n    \nif not group_names is None:  \n     for gg in range (len(group_names)):\n         #names+=[\"lag_rate_group_\"+ str(gg+1) + \"_\" + str(k+1) for k in range (size_group)]    \n         for day in days_back_confimed_group:\n            names+=[\"days_ago_grooupcount_\" + str(gg+1) + \"_\" + str(day) ]             \n         for window in windows_group:        \n            names+=[\"ma_group_\" + str(gg+1) + \"_\" + str(window) + \"_rate_\" + str(k+1) for k in range (size_group)]\n            #names+=[\"std_group_\" + str(gg+1)+ \"_\" + str(window) + \"_rate_\" + str(k+1) for k in range (size_group)]  \n            #names+=[\"ewma_group_\" + str(gg+1) + \"_\" + str(window) + \"_rate_\" + str(k+1) for k in range (size)] ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##############################################################################################################\n############################ THIS PART IS THE TRAINING CODE - COMMENT IT OUT TO MODEL IT. ####################\n############################ HERE I USE COUNT ON DIFFERENCES MODEL TO ESTIMATE CONFIRMED AND FAT.#############\n############################ BEAR IN MIND , PAST EXPERIENCE HAS SHOWN THAT RESULTS MIGHT BE ##################\n############################ SLIGHTLY DIFFERENT THAN MY CURRENT UPLOADED ONE (HOPEFULLY NOT BY MUCH)  ########\n##############################################################################################################\n\n\"\"\"\n\n#################Full model\n\ntr_frame=train_frame\n\n    \ntarget_confirmed=[\"confirmed_count_plus\" + str(k+1) for k in range (horizon)]    \ntarget_fatalities=[\"fatalities_count_plus\" + str(k+1) for k in range (horizon)] \nweight_confirmed=\"timethis\"    \nweight_fatalities=\"timethis\"\nseed=1412\n\ntarget_confirmed_train=tr_frame[target_confirmed].values\nprint (\"  original shape of train is {}  \".format( target_confirmed_train.shape) )\n\nweight_confirmed_train=tr_frame[weight_confirmed].values\nprint (\"  original shape of weight confirmed for train is {}  \".format( weight_confirmed_train.shape) )\n\ntarget_fatalities_train=tr_frame[target_fatalities].values\nprint (\"  original shape of train fatalities is {}  \".format( target_fatalities_train.shape) )\n\nweight_fatalities_train=tr_frame[weight_fatalities].values\nprint (\"  original shape of weight fatalities for train is {}  \".format( weight_fatalities_train.shape) )\n\nfeatures_train=tr_frame[names].values   \ncurrent_fatalities_train=tr_frame[\"Fatalities\"].values\ncurrent_confirmed_train=tr_frame[\"ConfirmedCases\"].values\n \nprint(\"features_train.shape\", features_train.shape)    \n\nfeatures_cv=[]\nname_cv=[]\nstandard_confirmed_cv=[]\nstandard_fatalities_cv=[]\nnames_=tr_frame[\"key\"].values\ntraining_horizon=int(features_train.shape[0]/len(unique_keys)) \nprint(\"training horizon = \",training_horizon)\nfor dd in range(training_horizon-1,features_train.shape[0],training_horizon):\n    features_cv.append(features_train[dd])\n    name_cv.append(names_[dd])\n    standard_confirmed_cv.append(current_confirmed_train[dd])\n    standard_fatalities_cv.append(current_fatalities_train[dd])\n    print (name_cv[-1], standard_confirmed_cv[-1], standard_fatalities_cv[-1])\n    \n\ncurrent_confirmed_train_index=[k for k in range(len(current_confirmed_train)) if current_confirmed_train[k]>0]\ntarget_confirmed_train=target_confirmed_train[current_confirmed_train_index]\ntarget_fatalities_train=target_fatalities_train[current_confirmed_train_index] \n\nweight_confirmed_train=weight_confirmed_train[current_confirmed_train_index]\nweight_fatalities_train=weight_fatalities_train[current_confirmed_train_index] \n\nfeatures_train=features_train[current_confirmed_train_index]         \ncurrent_confirmed_train=current_confirmed_train[current_confirmed_train_index]\ncurrent_fatalities_train=current_fatalities_train[current_confirmed_train_index]  \n    \nfeatures_cv=np.array(features_cv)\n\noveral_rmsle_metric_confirmed=0.0\n\nfor j in range (horizon):\n    this_target=target_confirmed_train[:,j]\n    index_positive=[k for k in range(len(this_target)) if this_target[k]!=-1]\n    this_features=features_train[index_positive]\n    this_target=this_target[index_positive]\n    this_weight=np.log(weight_confirmed_train[index_positive] +2.)    \n    #this_weight=weight_confirmed_train[index_positive]\n    #this_weight=np.log(standard_confirmed_train[index_positive]+2.)\n    #this_weight=[1. for k in range(len(this_weight))]\n    this_features_cv=features_cv                          \n\n    preds=bagged_set_trainc(this_features,this_target,this_weight, seed, bagging,features_cv, xt=None,yt=None, output_name=model_directory +\"confirmedc\"+ str(j))\n    print (\" modelling count confirmed, case %d, original train %d, and after %d, original cv %d and after %d \"%(\n    j,target_confirmed_train.shape[0],this_target.shape[0],this_features_cv.shape[0],this_features_cv.shape[0])) \n\n\nfor j in range (horizon):\n    this_target=target_fatalities_train[:,j]\n    index_positive=[k for k in range(len(this_target)) if this_target[k]!=-1]\n    this_features=features_train[index_positive]\n    this_target=this_target[index_positive]\n    \n    this_weight=np.log(weight_fatalities_train[index_positive] +2.)  \n    \n    #this_weight=np.log(standard_confirmed_train[index_positive]+2.)\n    #this_weight=[1. for k in range(len(this_weight))]\n    \n    this_features_cv=features_cv\n                             \n    preds=bagged_set_trainc(this_features,this_target,this_weight, seed, bagging,features_cv, xt=None,yt=None, output_name=model_directory +\"fatalc\"+ str(j))\n    print (\" modelling count fatalities, case %d, original train %d, and after %d, original cv %d and after %d \"%(\n    j,target_confirmed_train.shape[0],this_target.shape[0],this_features_cv.shape[0],this_features_cv.shape[0]))\n    \n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.stats import pearsonr\ncut_off_count=1.2\n###prediction part ##############\n\ndef decay_4_first_10_then_1_f(array):\n    arr=[k for k in array]\n    for j in range(len(array)):\n        if j<10:\n            arr[j]*=1./4.\n        else :\n            arr[j]=0\n    return arr\n\n\t\ndef decay_16_first_10_then_1_f(array):\n    arr=[k for k in array]\n    for j in range(len(array)):\n        if j<10:\n            arr[j]*=1./16.\n        else :\n            arr[j]=0\n    return arr\n            \ndef decay_2_f(array):\n    arr=[k for k in array]    \n    for j in range(len(array)):\n            arr[j]*=1./2.\n    return arr \n\ndef decay_4_f(array):\n    arr=[k for k in array]    \n    for j in range(len(array)):\n            arr[j]*=1./4.\n    return arr \n\t\ndef acceleratorx2_f(array):\n    arr=[k for k in array]    \n    for j in range(len(array)):\n            arr[j]*=2.\n    return arr \n\n\ndef decay_1_5_f(array):\n    arr=[k for k in array]    \n    for j in range(len(array)):\n            arr[j]*=1./1.5\n    return arr          \n\ndef decay_1_2_f(array):\n    arr=[k for k in array]    \n    for j in range(len(array)):\n            arr[j]*=1./1.2\n    return arr   \n\ndef decay_1_1_f(array):\n    arr=[k for k in array]    \n    for j in range(len(array)):\n            arr[j]*=1./1.1\n    return arr   \n         \ndef stay_same_f(array):\n    arr=[0.0 for k in array]      \n    return arr   \n\ndef decay_2_last_12_linear_inter_f(array):\n    arr=[k for k in array]\n    for j in range(len(array)):\n        arr[j]*=1/2.\n    arr12= arr[-12]\n\n    for j in range(0, 12):\n        arr[len(arr)-12 +j]= arr12/(j+1.)\n    return arr\n\ndef decay_1_5_last_12_linear_inter_f(array):\n    arr=[k for k in array]\n    for j in range(len(array)):\n        arr[j]*=1/(1.5)\n    arr12= arr[-12]\n\n    for j in range(0, 12):\n        arr[len(arr)-12 +j]= arr12/(j+1.)\n    return arr\n\n\n\ndef decay_1_2_last_12_linear_inter_f(array):\n    arr=[k for k in array]\n    for j in range(len(array)):\n        arr[j]*=1./(1.2)\n    arr12= arr[-12]\n\n    for j in range(0, 12):\n        arr[len(arr)-12 +j]= arr12/(j+1.)\n    return arr\n\n\n\ndef decay_4_last_12_linear_inter_f(array):\n    arr=[k for k in array]\n    for j in range(len(array)):\n        arr[j]*=1/4.\n    arr12= arr[-12]\n\n    for j in range(0, 12):\n        arr[len(arr)-12 +j]= arr12/(j+1.)\n    return arr\n\n\ndef decay_8_last_12_linear_inter_f(array):\n    arr=[k for k in array]\n    for j in range(len(array)):\n        arr[j]*=1/8.\n    arr12= arr[-12]\n\n    for j in range(0, 12):\n        arr[len(arr)-12 +j]= arr12/(j+1.)\n    return arr\n\n\n\n\ndef linear_last_12_f(array):\n    arr=[k for k in array]\n    for j in range(len(array)):\n        arr[j]=array[j]\n    arr12=  arr[-12] \n    \n    for j in range(0, 12):\n        arr[len(arr)-12 +j]= arr12/(j+1.)\n    return arr\n    \ndef add_3_contsants_f(array,constants):\n    arr=[k for k in array]    \n    for j in range(0,min(10,len(array))):\n        arr[j]=array[j]+ constants[0]\n    for j in range(10,min(20,len(array))):\n        arr[j]=array[j]+ constants[1]    \n    for j in range(20,len(array)):\n        arr[j]=array[j]+ constants[2]  \n    return arr\n\ndef revert_preds(array):\n    arr=[k for k in array] \n    for jjj in range (1,len(array)):\n        arr[jjj]=array[jjj-1]\n    return arr\n    \nstay_same=[\"nan_nan_Belize\",\n\"nan_Falkland Islands (Malvinas)_United Kingdom\",\n\"nan_Greenland_Denmark\",\n\"nan_nan_Suriname\",\n\"nan_nan_Papua New Guinea\",\n\"nan_Saint Barthelemy_France\",\n\"nan_Anguilla_United Kingdom\",\n\"nan_Faroe Islands_Denmark\",\n\"nan_nan_Diamond Princess\",\n\"nan_Saint Pierre and Miquelon_France\",\n\"nan_nan_MS Zaandam\"]\n\nnot_china=[\"nan_Hong Kong_China\",\n           \"nan_Jilin_China\",\n           \"nan_Shanghai_China\",\n           \"nan_nan_China\"]\n    \ntr_frame=train_frame\n\nfeatures_train=tr_frame[names].values   \n\nstandard_confirmed_train=tr_frame[\"ConfirmedCases\"].values\nstandard_fatalities_train=tr_frame[\"Fatalities\"].values\ncurrent_confirmed_train=tr_frame[\"ConfirmedCases\"].values\n \n\n#based on differences between aggraged level of state level and county level\nUS_County_decays=[1.12928721917076,\n1.11319717445063,\n1.09019080003495,\n1.09054639211022,\n1.07359162437952,\n1.07758690695603,\n1.09228026420265,\n1.08177697041925,\n1.03976897078037,\n1.02818087417532,\n1.03918061105423,\n1.03200772948294,\n1.02445390740797,\n1.02074697729396,\n1.03411094916523,\n1.0062638939838,\n0.994894354473368,\n0.962558969500773,\n0.972769268084967,\n0.963989662013557,\n0.994878533190241,\n0.973785778384885,\n0.955063495068311,\n0.937809259916995,\n0.938370164891967,\n0.894556518582016,\n0.889929316241106,\n0.87077864868053,\n0.857970890863051,\n0.828321911237346,\n0.801852381304044,\n]    \n    \n#obtained from average per count plus constant    \nUS_OVERRIDE=[[\n23207.3722652086,\n23889.0306567728,\n24430.0431046741,\n24466.7567050266,\n23713.2400041044,\n22608.4892577587,\n21367.8534988397,\n20769.6129680705,\n21228.1211938592,\n21495.4428971699,\n22035.5166697628,\n21125.9867035126,\n20181.3684696248,\n18494.0407094816,\n18475.5718884067,\n19215.0940461046,\n19333.1028894944,\n19522.3401300248,\n19166.0663535153,\n18096.9669398416,\n16937.4222363175,\n16947.6467406821,\n16357.855664555,\n15976.4969540291,\n16624.4012014047,\n16262.3505500094,\n15364.4351409369,\n15028.6796673928,\n15579.9384603638,\n15834.0289720947,\n16576.2953662852,\n\n], [1521.00845305084,\n1566.97981581732,\n1590.42795692813,\n1431.97778497973,\n1281.31188429921,\n1151.84665265908,\n1198.1294341396,\n1311.54799108804,\n1356.57819142736,\n1271.28355785146,\n1227.7373880933,\n1099.79166367917,\n1033.79403187646,\n958.252997644407,\n1046.47668605323,\n1085.99235244191,\n1081.7845211432,\n994.467577598198,\n897.908986384629,\n852.023641179723,\n879.298706602438,\n883.484565921627,\n972.507881529548,\n922.139598187947,\n893.864912358786,\n901.67568945017,\n783.837418771163,\n793.314828070173,\n896.614556007508,\n859.957640953242,\n770.734101867327,\n] ]\n\n\n#ghana reports 0 every 2 days\nGHANA_OVERRIDE=[\n    [437.34,\n160.085898221233,\n423.982956408698,\n197.208012040989,\n438.1867611832,\n160.685861259004,\n383.667040128953,\n175.753495639198,\n400.74307732034,\n188.782063124033,\n479.921308802172,\n205.837541082642,\n462.821368862156,\n344.595696311912,\n461.519828860741,\n397.104823526226,\n521.899614517621,\n502.391633158483,\n481.702595518469,\n467.449979112643,\n560.96931390999,\n455.537034350064,\n482.244071286786,\n506.221918800896,\n510.67633991002,\n462.067712758548,\n383.930145751492,\n363.267022670611,\n453.059337065173,\n324.008740298506,\n299.563588083208,\n\n] , [1.17583524490721,\n0,\n1.26096966594506,\n0,\n1.45773722679996,\n0,\n1.31769888587172,\n0,\n1.90002768724863,\n0,\n1.97527592437864,\n1.75370174412068,\n1.72406661083427,\n1.65081144481032,\n1.8676119135745,\n2.12479552032021,\n2.8060755390374,\n2.38864420453819,\n1.90622435354147,\n1.93737239215752,\n1.82606349042584,\n2.51189161572323,\n2.64862312509722,\n2.93841977514869,\n2.33613023947002,\n1.73078769935274,\n1.63014931330317,\n1.19478903202637,\n1.92652108876484,\n1.89717476536816,\n1.04840010811724,\n] ]\n    \nadd_3_constants={}\n#manual adjustements\nadd_3_constants[\"nan_nan_Belgium\"]=[ [100,150,70] , [10,15,10]   ]\nadd_3_constants[\"nan_Ontario_Canada\"]=[ [100,70,50] , [20,15,10]   ]\nadd_3_constants[\"nan_nan_Canada\"]=[ [0,100,100] , [0,0,0]   ]\nadd_3_constants[\"nan_nan_Germany\"]=[ [0,0,0] , [0,0,0]   ] \nadd_3_constants[\"nan_Quebec_Canada\"]=[ [100,200,150] , [20,20,20]   ] \nadd_3_constants[\"nan_nan_Indonesia\"]=[ [50,50,50] , [0,0,0]   ]\nadd_3_constants[\"nan_nan_Iran\"]=[ [100,200,300] , [0,1,10]   ]\nadd_3_constants[\"nan_nan_Italy\"]=[ [100,200,100] , [0,50,0]   ]\nadd_3_constants[\"nan_nan_Romania\"]=[ [40,70,70] , [5,5,5] ] \nadd_3_constants[\"nan_nan_Spain\"]=[ [500,400,300] , [50,30,0] ] \nadd_3_constants[\"nan_nan_Sweden\"]=[ [100,70,50] , [0,5,5] ] \nadd_3_constants[\"nan_nan_Turkey\"]=[ [100,50,150] , [0,20,10] ]\nadd_3_constants[\"nan_nan_Portugal\"]=[ [0,0,0] , [0,0,0] ]\nadd_3_constants[\"nan_nan_Kenya\"]=[ [0,0,0] , [0,0,0]   ]\nadd_3_constants[\"nan_nan_United Kingdom\"]=[ [1200,1200,1000] , [0,10,10]   ]\nadd_3_constants[\"nan_nan_Denmark\"]=[ [10,10,20] , [2,1,1]   ] \nadd_3_constants[\"nan_nan_Netherlands\"]=[ [50,70,40] , [0,5,5]   ]\nadd_3_constants[\"nan_nan_Poland\"]=[ [70,70,70] , [0,0,0]   ] \nadd_3_constants[\"nan_nan_Ukraine\"]=[ [50,50,20] , [0,0,0] ] \nadd_3_constants[\"nan_Arizona_US\"]=[ [50,50,100] , [5,2,1] ] \nadd_3_constants[\"Los Angeles_California_US\"]=[ [50,100,100] , [0,0,5] ] \nadd_3_constants[\"Santa Barbara_California_US\"]=[ [0,0,0] , [0,0,0] ] \nadd_3_constants[\"nan_California_US\"]=[ [200,200,200] , [0,0,0]   ]\nadd_3_constants[\"nan_Colorado_US\"]=[ [70,100,100] , [0,0,0]   ]\nadd_3_constants[\"nan_Florida_US\"]=[ [100,100,100] , [0,0,0]   ]\nadd_3_constants[\"nan_Georgia_US\"]=[ [150,150,110] , [0,0,0]   ]\nadd_3_constants[\"Cook_Illinois_US\"]=[ [200,200,400] , [0,0,10]   ]\nadd_3_constants[\"nan_Illinois_US\"]=[ [0,100,200] , [0,0,10]   ]\nadd_3_constants[\"nan_Indiana_US\"]=[ [50,100,70] , [0,0,10]   ]\nadd_3_constants[\"nan_Louisiana_US\"]=[ [50,100,70] , [0,0,10]   ]\nadd_3_constants[\"Prince George's_Maryland_US\"]=[ [0,0,50] , [0,0,1]   ]\nadd_3_constants[\"nan_Maryland_US\"]=[ [100,150,200] , [0,0,0]   ]\nadd_3_constants[\"nan_Massachusetts_US\"]=[ [0,100,200] , [5,10,10] ] \nadd_3_constants[\"nan_Michigan_US\"]=[ [0,100,50] , [0,0,0] ] \nadd_3_constants[\"nan_Minnesota_US\"]=[ [100,120,110] , [0,0,0] ] \nadd_3_constants[\"nan_New Jersey_US\"]=[ [100,200,150] , [0,0,0] ] \nadd_3_constants[\"New York_New York_US\"]=[ [100,100,100] , [0,0,0] ] \nadd_3_constants[\"nan_New York_US\"]=[ [300,500,500] , [0,0,50] ] \nadd_3_constants[\"nan_North Carolina_US\"]=[ [100,250,250] , [0,0,50] ] \nadd_3_constants[\"nan_Ohio_US\"]=[ [100,150,100] , [5,10,5]   ] \nadd_3_constants[\"nan_Pennsylvania_US\"]=[ [200,200,200] , [0,0,0] ] \nadd_3_constants[\"Philadelphia_Pennsylvania_US\"]=[ [100,100,100] , [0,0,0] ] \nadd_3_constants[\"nan_Tennessee_US\"]=[ [50,50,70] , [0,0,0] ] \nadd_3_constants[\"nan_Texas_US\"]=[ [300,200,500] , [0,0,0]   ]\nadd_3_constants[\"nan_Virginia_US\"]=[ [100,200,300] , [0,0,0]   ]\nadd_3_constants[\"nan_Wisconsin_US\"]=[ [30,50,50] , [0,0,0]   ]\nadd_3_constants[\"nan_Iowa_US\"]=[ [0,50,50] , [0,0,0]   ]\nadd_3_constants[\"nan_Kansas_US\"]=[ [0,0,50] , [0,0,0]   ]\nadd_3_constants[\"Middlesex_Massachusetts_US\"]=[ [50,50,50] , [0,0,0]   ]\nadd_3_constants[\"Hudson_New Jersey_US\"]=[ [0,20,20] , [0,0,0]   ]\nadd_3_constants[\"nan_Washington_US\"]=[ [0,20,40] , [0,0,0]   ]\nadd_3_constants[\"nan_Nebraska_US\"]=[ [0,0,0] , [0,0,0]   ]\nadd_3_constants[\"Suffolk_New York_US\"]=[ [0,0,50] , [0,0,0]   ]\nadd_3_constants[\"nan_Rhode Island_US\"]=[ [10,20,30] , [0,0,0] ] \nadd_3_constants[\"nan_nan_Serbia\"]=[ [20,30,20] , [0,0,0] ] \nadd_3_constants[\"nan_nan_Egypt\"]=[ [100,150,170] , [0,0,0] ] \n\n\nfor cn,vall in add_3_constants.items():\n    if cn not in new_unique_keys:\n        raise Exception(\"%s not in unique keys\"%(cn))\n        \n\n###################### HERE #################\n#add_3_constants={}\n\nprint(\" len add_3_constants\", len(add_3_constants))\n\n\nfeatures_cv=[]\nname_cv=[]\nstandard_confirmed_cv=[]\nstandard_fatalities_cv=[]\nnames_=tr_frame[\"key\"].values\ntraining_horizon=int(features_train.shape[0]/len(unique_keys)) \nprint(\"training horizon = \",training_horizon)\nfor dd in range(training_horizon-1,features_train.shape[0],training_horizon):\n    features_cv.append(features_train[dd])\n    name_cv.append(names_[dd])\n    standard_confirmed_cv.append(standard_confirmed_train[dd])\n    standard_fatalities_cv.append(standard_fatalities_train[dd])\n    print (name_cv[-1], standard_confirmed_cv[-1], standard_fatalities_cv[-1])\n\n\nfeatures_cv=np.array(features_cv)\npreds_confirmed_cv=np.zeros((features_cv.shape[0],horizon))\npreds_confirmed_standard_cv=np.zeros((features_cv.shape[0],horizon))\npreds_confirmed_non_cumulative_cv=np.zeros((features_cv.shape[0],horizon))\n\npreds_fatalities_cv=np.zeros((features_cv.shape[0],horizon))\npreds_fatalities_standard_cv=np.zeros((features_cv.shape[0],horizon))\npreds_fatalities_non_cumulative_cv=np.zeros((features_cv.shape[0],horizon))\n\noveral_rmsle_metric_confirmed=0.0\n\nfor j in range (preds_confirmed_cv.shape[1]):\n\n    this_features_cv=features_cv                          \n    preds=predict(features_cv,bagging, input_name=model_directory +\"confirmedc\"+ str(j))\n    preds_confirmed_cv[:,j]=preds\n    print (\" modelling confirmed, case %d, , original cv %d and after %d \"%(j,this_features_cv.shape[0],this_features_cv.shape[0])) \n\npredictions=[] \nfor ii in range (preds_confirmed_cv.shape[0]):\n    current_prediction=standard_confirmed_cv[ii]\n    #if current_prediction==0 :\n        #current_prediction=0.1   \n    this_preds=preds_confirmed_cv[ii].tolist()\n    name=name_cv[ii]\n    reserve=this_preds[0]\n    #overrides\n    \n    if use_external and name in holdder :\n        print(\" name %s is for confirmed\" %(name))\n        this_preds=revert_preds(this_preds)\n    \n    \n    \n    if name in add_3_constants:\n         print(\"name \" , name , \" is in 3 constants\")\n        \n         this_preds=add_3_contsants_f(this_preds,add_3_constants[name][0])        \n        \n    if name in stay_same or (\"China\" in name and name not in not_china):\n\n         this_preds=stay_same_f(this_preds)\n\n    if name==\"nan_nan_US\":\n        print (\" ======= overriding USA Conf ========== \")\n        this_preds=US_OVERRIDE[0]\n        \n    if name==\"nan_nan_Ghana\":\n        print (\" ======= overriding Ghana Fat ========== \")\n        this_preds=GHANA_OVERRIDE[0]  \n        \n    \n    for j in range (preds_confirmed_cv.shape[1]):\n                this_pr=max(0,this_preds[j])\n                \n                if j>18:\n                   previous_pr= max(0,this_preds[j-1])\n                   if  previous_pr>0.1:\n                        if this_pr/previous_pr>cut_off_count:\n                            this_pr=np.mean(this_preds[j-8:j+1])\n                            this_preds[j]=this_pr\n           \n                #usa counties overrides            \n                if \"_US\" in name and \"nan_\" not in name and name not in add_3_constants and j>23:\n                     this_preds[j]= this_preds[j]*US_County_decays[j]\n                   \n                if use_external and j==0 and name in holdder :    \n                    current_prediction+=max(0,holdder[name][0])\n                    preds_confirmed_standard_cv[ii][j]=current_prediction\n                    preds_confirmed_non_cumulative_cv[ii][j]=max(0,holdder[name][0])                \n                else:\n                    current_prediction+=max(0,this_preds[j])\n                    preds_confirmed_standard_cv[ii][j]=current_prediction\n                    preds_confirmed_non_cumulative_cv[ii][j]=max(0,this_preds[j])\n\nfor j in range (preds_confirmed_cv.shape[1]):\n\n    this_features_cv=features_cv\n                             \n    preds=predict(features_cv,bagging, input_name=model_directory +\"fatalc\"+ str(j))\n    preds_fatalities_cv[:,j]=preds\n    print (\" modelling fatalities, case %d, original cv %d and after %d \"%( j,this_features_cv.shape[0],this_features_cv.shape[0])) \n\npredictions=[]\nfor ii in range (preds_fatalities_cv.shape[0]):\n    current_prediction=standard_fatalities_cv[ii]\n        \n    this_preds=preds_fatalities_cv[ii].tolist()\n    name=name_cv[ii]\n    reserve=this_preds[0]\n    #overrides\n   \n    \n    if use_external and name in holdder :\n        print(\" name %s is for fatalities\" %(name))        \n        this_preds=revert_preds(this_preds)\n\n    ####fatality special\n    if name in add_3_constants:\n         print(\"name \" , name , \" is in 3 constants FATALITIES\")        \n         this_preds=add_3_contsants_f(this_preds,add_3_constants[name][1])             \n        \n    if name in stay_same or (\"China\" in name and name not in not_china):\n         this_preds=stay_same_f(this_preds)\n           \n    if name==\"nan_nan_US\":\n        print (\" ======= overriding USA Fat ========== \")\n        this_preds=US_OVERRIDE[1] \n        \n    if name==\"nan_nan_Ghana\":\n        print (\" ======= overriding Ghana Fat ========== \")\n        this_preds=GHANA_OVERRIDE[1]         \n       \n        \n    for j in range (preds_fatalities_cv.shape[1]):\n                    this_pr=max(0,this_preds[j])\n                    if j>18:\n                       previous_pr= max(0,this_preds[j-1])\n                       if  previous_pr>0.1:\n                            if this_pr/previous_pr>cut_off_count:\n                                this_pr=np.mean(this_preds[j-8:j+1])\n                                this_preds[j]=this_pr    \n                                \n                    if use_external and j==0 and name in holdder :    \n                        current_prediction+=max(0,holdder[name][1])\n                        preds_confirmed_standard_cv[ii][j]=current_prediction\n                        preds_confirmed_non_cumulative_cv[ii][j]=max(0,holdder[name][1])                                  \n                    else :\n                        current_prediction+=max(0,this_preds[j])\n                        preds_fatalities_standard_cv[ii][j]=current_prediction\n                        preds_fatalities_non_cumulative_cv[ii][j]=max(0,this_preds[j])   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#### rate modelling\n\nimport lightgbm as lgb\nfrom sklearn.linear_model import Ridge\nfrom sklearn.externals import joblib\n\ndef bagged_set_train_rate(X_ts,y_cs,wts, seed, estimators,xtest, xt=None,yt=None, output_name=None):\n   #print (type(yt))\n   # create array object to hold predictions \n  \n   baggedpred=np.array([ 0.0 for d in range(0, xtest.shape[0])]) \n   #print (y_cs[:10])\n   #print (yt[:10])  \n\n   #loop for as many times as we want bags\n   for n in range (0, estimators):\n       ###\n       params = {'objective': 'rmse',\n                'metric': 'rmse',\n                'boosting': 'gbdt',\n                'learning_rate': 0.02, #change here    \n                'drop_rate':0.01,\n                #'alpha': 0.99, \n                'skip_drop':0.6,\n                'uniform_drop':True,               \n                'verbose': -1,    \n                'num_leaves': 40, # ~18    \n                'bagging_fraction': 0.9,    \n                'bagging_freq': 1,    \n                'bagging_seed': seed + n,    \n                'feature_fraction': 0.8,    \n                'feature_fraction_seed': seed + n,    \n                'min_data_in_leaf': 10, #30, #56, # 10-50    \n                'max_bin': 100, # maybe useful with overfit problem    \n                'max_depth':20,                   \n                #'reg_lambda': 10,    \n                'reg_alpha':1,    \n                'lambda_l2': 10,\n                #'categorical_feature':'2', # because training data is extremely unbalanced                     \n                'num_threads':38\n                }\n       d_train = lgb.Dataset(X_ts,y_cs, weight=wts, free_raw_data=False)#np.log1p(\n       if not type(yt) is type(None):           \n           d_cv = lgb.Dataset(xt,yt, free_raw_data=False, reference=d_train)#, reference=d_train\n           model = lgb.train(params,d_train,num_boost_round=500,\n                             valid_sets=d_cv,\n\n                             verbose_eval=50 ) #1000                        \n           \n       else :\n           #d_cv = lgb.Dataset(xt, free_raw_data=False, categorical_feature=\"2\")  \n           model = lgb.train(params,d_train,num_boost_round=500) #1000                              \n           #importances=model.feature_importance('gain')\n           #print(importances)\n       preds=model.predict(xtest)               \n       # update bag's array\n       baggedpred+=preds\n       #np.savetxt(\"preds_lgb\" + str(n)+ \".csv\",baggedpred)   \n       #if n%5==0:\n           #print(\"completed: \" + str(n)  )                 \n\n       if not output_name is None:\n            joblib.dump((model), output_name+ \"_\" +str(n))\n\n   # divide with number of bags to create an average estimate  \n   baggedpred/= estimators\n     \n   return baggedpred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"names_rate=[]\nnames_rate+=[\"lag_confirmed_rate\" + str(k+1) for k in [6,13,20]]\nnames_rate+=[\"lag_confirmed_count\" + str(k+1) for k in [6,13,20]]\n\nnames_rate+=[\"lag_fatalities_rate\" + str(k+1) for k in [6,13,20]]\nnames_rate+=[\"lag_fatalities_count\" + str(k+1) for k in [6,13,20]]\n\nfor day in days_back_confimed:\n    names_rate+=[\"days_ago_confirmed_count_\" + str(day) ]\nfor window in windows:        \n    names_rate+=[\"ma\" + str(window) + \"_rate_confirmed\" + str(k+1) for k in range (size)]\n    #names_rate+=[\"std\" + str(window) + \"_rate_confirmed\" + str(k+1) for k in range (size)] \n    #names_rate+=[\"ewma\" + str(window) + \"_rate_confirmed\" + str(k+1) for k in range (size)]         \n    #names_rate+=[\"ma\" + str(window) + \"_count_confirmed\" + str(k+1) for k in range (size)]  \n\n#names_rate+=[\"lag_fatalities_rate\" + str(k+1) for k in range (size)]\nfor day in days_back_fatalities:\n    names_rate+=[\"days_ago_fatalitiescount_\" + str(day) ]    \nfor window in windows:        \n    names_rate+=[\"ma\" + str(window) + \"_rate_fatalities\" + str(k+1) for k in range (size)]\n    #names_rate+=[\"std\" + str(window) + \"_rate_fatalities\" + str(k+1) for k in range (size)]  \n    #names_rate+=[\"ewma\" + str(window) + \"_rate_fatalities\" + str(k+1) for k in range (size)]    \n    #names_rate+=[\"ma\" + str(window) + \"_count_fatalities\" + str(k+1) for k in range (size)]    \n#names_rate+=[\"confirmed_level\"]\n#names_rate+=[\"fatalities_level\"]  \nnames_rate+=[\"dowthis\"] \n#names_rate+=[\"timethis\"]\n\n#names_rate+=[\"weight_confirmed\"]\n#names_rate+=[\"weight_fatalities\"]     \n#names_rate+=[\"population\"]     \nif not extra_stable_columns is None and len(extra_stable_columns)>0: \n    names_rate+=[k for k in extra_stable_columns]  \n    \nif not group_names is None:  \n     for gg in range (len(group_names)):\n         #names_rate+=[\"lag_rate_group_\"+ str(gg+1) + \"_\" + str(k+1) for k in range (size_group)]    \n         for day in days_back_confimed_group:\n            names_rate+=[\"days_ago_grooupcount_\" + str(gg+1) + \"_\" + str(day) ]             \n         for window in windows_group:        \n            names_rate+=[\"ma_group_\" + str(gg+1) + \"_\" + str(window) + \"_rate_\" + str(k+1) for k in range (size_group)]\n            #names_rate+=[\"std_group_\" + str(gg+1)+ \"_\" + str(window) + \"_rate_\" + str(k+1) for k in range (size_group)]  \n            #names_rate+=[\"ewma_group_\" + str(gg+1) + \"_\" + str(window) + \"_rate_\" + str(k+1) for k in range (size)]   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##############################################################################################################\n############################ THIS PART IS THE TRAINING CODE - COMMENT IT OUT TO MODEL IT. ####################\n############################ HERE I USE to MODEL GROWTH RATE OF CONFIRMED AND FAT. ###########################\n############################ BEAR IN MIND , PAST EXPERIENCE HAS SHOWN THAT RESULTS MIGHT BE ##################\n############################ SLIGHTLY DIFFERENT THAN MY CURRENT UPLOADED ONE (HOPEFULLY NOT BY MUCH)  ########\n##############################################################################################################\n\n\"\"\"\ntr_frame=train_frame\n\n\t\ntarget_confirmed_rate=[\"confirmed_plus\" + str(k+1) for k in range (horizon)]    \ntarget_fatalities_rate=[\"fatalities_plus\" + str(k+1) for k in range (horizon)] \nweight_confirmed=\"timethis\"    \nweight_fatalities=\"timethis\"\nseed=1412\n\ntarget_confirmed_train=tr_frame[target_confirmed_rate].values\nprint (\"  original shape of train is {}  \".format( target_confirmed_train.shape) )\n\nweight_confirmed_train=tr_frame[weight_confirmed].values\nprint (\"  original shape of weight confirmed for train is {}  \".format( weight_confirmed_train.shape) )\n\ntarget_fatalities_train=tr_frame[target_fatalities_rate].values\nprint (\"  original shape of train fatalities is {}  \".format( target_fatalities_train.shape) )\n\nweight_fatalities_train=tr_frame[weight_fatalities].values\nprint (\"  original shape of weight fatalities for train is {}  \".format( weight_fatalities_train.shape) )\n\n\nstandard_confirmed_train=tr_frame[\"ConfirmedCases\"].values\nstandard_fatalities_train=tr_frame[\"Fatalities\"].values\ncurrent_confirmed_train=tr_frame[\"ConfirmedCases\"].values\n\nfeatures_train=tr_frame[names_rate].values   \nprint(\"features_train.shape\", features_train.shape)\n\n\nfeatures_cv=[]\nname_cv=[]\nstandard_confirmed_cv=[]\nstandard_fatalities_cv=[]\nnames_rate_=tr_frame[\"key\"].values\ntraining_horizon=int(features_train.shape[0]/len(unique_keys)) \nprint(\"training horizon = \",training_horizon)\nfor dd in range(training_horizon-1,features_train.shape[0],training_horizon):\n    features_cv.append(features_train[dd])\n    name_cv.append(names_rate_[dd])\n    standard_confirmed_cv.append(standard_confirmed_train[dd])\n    standard_fatalities_cv.append(standard_fatalities_train[dd])\n    print (name_cv[-1], standard_confirmed_cv[-1], standard_fatalities_cv[-1])\n    \n \n    \ncurrent_confirmed_train=[k for k in range(len(current_confirmed_train)) if current_confirmed_train[k]>0]\ntarget_confirmed_train=target_confirmed_train[current_confirmed_train]\ntarget_fatalities_train=target_fatalities_train[current_confirmed_train] \n\nweight_confirmed_train=weight_confirmed_train[current_confirmed_train_index]\nweight_fatalities_train=weight_fatalities_train[current_confirmed_train_index] \n\nfeatures_train=features_train[current_confirmed_train]         \nstandard_confirmed_train=standard_confirmed_train[current_confirmed_train]\nstandard_fatalities_train=standard_fatalities_train[current_confirmed_train]  \n    \nfeatures_cv=np.array(features_cv)\n\n\noveral_rmsle_metric_confirmed=0.0\n\nfor j in range (horizon):\n    this_target=target_confirmed_train[:,j]\n    index_positive=[k for k in range(len(this_target)) if this_target[k]!=-1]\n    this_features=features_train[index_positive]\n    this_target=this_target[index_positive]\n    \n    this_weight=np.log(weight_confirmed_train[index_positive] +2.)     \n    \n    #this_weight=weight_confirmed_train[index_positive]    \n    #this_weight=np.log(standard_confirmed_train[index_positive]+2.)\n    #this_weight=[1. for k in range(len(this_weight))]\n    this_features_cv=features_cv                          \n\n    preds=bagged_set_train_rate(this_features,this_target,this_weight, seed, bagging,features_cv, xt=None,yt=None, output_name=model_directory +\"confirmed\"+ str(j))\n    print (\" modelling confirmed, case %d, original train %d, and after %d, original cv %d and after %d \"%(\n    j,target_confirmed_train.shape[0],this_target.shape[0],this_features_cv.shape[0],this_features_cv.shape[0])) \n\n\n\nfor j in range (horizon):\n    this_target=target_fatalities_train[:,j]\n    index_positive=[k for k in range(len(this_target)) if this_target[k]!=-1]\n    this_features=features_train[index_positive]\n    this_target=this_target[index_positive]\n    this_weight=np.log(weight_fatalities_train[index_positive] +2.)      \n    #this_weight=weight_fatalities_train[index_positive]        \n    #this_weight=np.log(standard_confirmed_train[index_positive]+2.)\n    #this_weight=[1. for k in range(len(this_weight))]\n    \n    this_features_cv=features_cv\n                             \n    preds=bagged_set_train_rate(this_features,this_target,this_weight, seed, bagging,features_cv, xt=None,yt=None, output_name=model_directory +\"fatal\"+ str(j))\n    print (\" modelling fatalities, case %d, original train %d, and after %d, original cv %d and after %d \"%(\n    j,target_confirmed_train.shape[0],this_target.shape[0],this_features_cv.shape[0],this_features_cv.shape[0])) \n    \n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def decay_4_first_10_then_1_f(array):\n    arr=[1.0 for k in range(len(array))]\n    for j in range(len(array)):\n        if j<10:\n            arr[j]=1. + (max(1,array[j])-1.)/4.\n        else :\n            arr[j]=1.\n    return arr\n\t\ndef decay_16_first_10_then_1_f(array):\n    arr=[1.0 for k in range(len(array))]\n    for j in range(len(array)):\n        if j<10:\n            arr[j]=1. + (max(1,array[j])-1.)/16.\n        else :\n            arr[j]=1.\n    return arr\t\n            \ndef decay_2_f(array):\n    arr=[1.0 for k in range(len(array))]    \n    for j in range(len(array)):\n            arr[j]=1. + (max(1,array[j])-1.)/2.\n    return arr \n\ndef decay_4_f(array):\n    arr=[1.0 for k in range(len(array))]    \n    for j in range(len(array)):\n            arr[j]=1. + (max(1,array[j])-1.)/4.\n    return arr \t\n\t\ndef acceleratorx2_f(array):\n    arr=[1.0 for k in range(len(array))]    \n    for j in range(len(array)):\n            arr[j]=1. + (max(1,array[j])-1.)*2.\n    return arr \n\n\n\ndef decay_1_5_f(array):\n    arr=[1.0 for k in range(len(array))]    \n    for j in range(len(array)):\n            arr[j]=1. + (max(1,array[j])-1.)/1.5\n    return arr            \n\ndef decay_1_2_f(array):\n    arr=[1.0 for k in range(len(array))]    \n    for j in range(len(array)):\n            arr[j]=1. + (max(1,array[j])-1.)/1.2\n    return arr            \n  \n\ndef decay_1_1_f(array):\n    arr=[1.0 for k in range(len(array))]    \n    for j in range(len(array)):\n            arr[j]=1. + (max(1,array[j])-1.)/1.1\n    return arr            \n      \n         \ndef stay_same_f(array):\n    arr=[1.0 for k in range(len(array))]      \n    for j in range(len(array)):\n        arr[j]=1.\n    return arr   \n\ndef decay_2_last_12_linear_inter_f(array):\n    arr=[1.0 for k in range(len(array))]\n    for j in range(len(array)):\n        arr[j]=1. + (max(1,array[j])-1.)/2.\n    arr12= (max(1,arr[-12])-1.)/12. \n\n    for j in range(0, 12):\n        arr[len(arr)-12 +j]= max(1, 1 + ( (arr12*12) - (j+1)*arr12 ))\n    return arr\n\ndef decay_1_2_last_12_linear_inter_f(array):\n    arr=[1.0 for k in range(len(array))]\n    for j in range(len(array)):\n        arr[j]=1. + (max(1,array[j])-1.)/1.2\n    arr12= (max(1,arr[-12])-1.)/12. \n\n    for j in range(0, 12):\n        arr[len(arr)-12 +j]= max(1, 1 + ( (arr12*12) - (j+1)*arr12 ))\n    return arr\n\ndef decay_1_5_last_12_linear_inter_f(array):\n    arr=[1.0 for k in range(len(array))]\n    for j in range(len(array)):\n        arr[j]=1. + (max(1,array[j])-1.)/1.5\n    arr12= (max(1,arr[-12])-1.)/12. \n\n    for j in range(0, 12):\n        arr[len(arr)-12 +j]= max(1, 1 + ( (arr12*12) - (j+1)*arr12 ))\n    return arr\n\n\n\ndef decay_4_last_12_linear_inter_f(array):\n    arr=[1.0 for k in range(len(array))]\n    for j in range(len(array)):\n        arr[j]=1. + (max(1,array[j])-1.)/4.\n    arr12= (max(1,arr[-12])-1.)/12. \n\n    for j in range(0, 12):\n        arr[len(arr)-12 +j]= max(1, 1 + ( (arr12*12) - (j+1)*arr12 ))\n    return arr\n\ndef decay_8_last_12_linear_inter_f(array):\n    arr=[1.0 for k in range(len(array))]\n    for j in range(len(array)):\n        arr[j]=1. + (max(1,array[j])-1.)/8.\n    arr12= (max(1,arr[-12])-1.)/12. \n\n    for j in range(0, 12):\n        arr[len(arr)-12 +j]= max(1, 1 + ( (arr12*12) - (j+1)*arr12 ))\n    return arr\n\n\n\ndef linear_last_12_f(array):\n    arr=[1.0 for k in range(len(array))]\n    for j in range(len(array)):\n        arr[j]=max(1,array[j])\n    arr12= (max(1,arr[-12])-1.)/12. \n    \n    for j in range(0, 12):\n        arr[len(arr)-12 +j]= max(1, 1 + ( (arr12*12) - (j+1)*arr12 ))\n    return arr\n\n\ndef add_rate(array, rate):\n    arr=[k for k in array]    \n    for j in range(len(array)):\n            arr[j]+=rate\n    return arr \n\ndef revert_preds(array):\n    arr=[k for k in array] \n    for jjj in range (1,len(array)):\n        arr[jjj]=array[jjj-1]\n    return arr\n\n\ntr_frame=train_frame\n\nfeatures_train=tr_frame[names_rate].values   \n\nstandard_confirmed_train=tr_frame[\"ConfirmedCases\"].values\nstandard_fatalities_train=tr_frame[\"Fatalities\"].values\ncurrent_confirmed_train=tr_frame[\"ConfirmedCases\"].values\n\n \nadd_constant_rate={}    \nadd_constant_rate[\"nan_nan_Brazil\"]=[0.003,0.004]\nadd_constant_rate[\"nan_nan_Chile\"]=[0.002,0.0001]\nadd_constant_rate[\"nan_nan_Colombia\"]=[0.002,0.0001]\nadd_constant_rate[\"nan_nan_Dominican Republic\"]=[0.006,0.0001]\nadd_constant_rate[\"nan_nan_India\"]=[0.015,0.001]\nadd_constant_rate[\"nan_nan_El Salvador\"]=[0.012,0.001]\nadd_constant_rate[\"nan_nan_Kuwait\"]=[0.01,0.01]\nadd_constant_rate[\"nan_nan_Mexico\"]=[0.01,0.002]\nadd_constant_rate[\"nan_nan_Pakistan\"]=[0.01,0.002]\nadd_constant_rate[\"nan_nan_Peru\"]=[0.0025,0.001]\nadd_constant_rate[\"nan_nan_Qatar\"]=[0.0055,0.001]\nadd_constant_rate[\"nan_nan_Russia\"]=[0.012,0.003]\nadd_constant_rate[\"nan_nan_Saudi Arabia\"]=[0.003,0.001]\nadd_constant_rate[\"nan_nan_Singapore\"]=[0.005,0.001]\nadd_constant_rate[\"nan_nan_South Africa\"]=[0.007,0.002]\nadd_constant_rate[\"nan_nan_Nigeria\"]=[0.005,0.0001]\nadd_constant_rate[\"nan_nan_United Arab Emirates\"]=[0.01,0.02]\nadd_constant_rate[\"nan_Alabama_US\"]=[0.001,0.001]\nadd_constant_rate[\"nan_Connecticut_US\"]=[0.001,0.001]\nadd_constant_rate[\"nan_nan_Kazakhstan\"]=[0.001,0.001]\nadd_constant_rate[\"nan_nan_Ecuador\"]=[0.0001,0.0001]\nadd_constant_rate[\"nan_nan_Honduras\"]=[0.005,0.0001]\nadd_constant_rate[\"nan_nan_Guinea-Bissau\"]=[0.0001,0.0001]\n\n\nfor cn,vall in add_constant_rate.items():\n    if cn not in new_unique_keys:\n        raise Exception(\"%s not in unique keys\"%(cn))\n\n####### HERE UNC #######\n#add_constant_rate={}  \n\nprint (\"len(add_constant_rate)\", len(add_constant_rate))\n        \n    \n    \nfeatures_cv=[]\nname_cv=[]\nstandard_confirmed_cv=[]\nstandard_fatalities_cv=[]\nnames_rate_=tr_frame[\"key\"].values\ntraining_horizon=int(features_train.shape[0]/len(unique_keys)) \nprint(\"training horizon = \",training_horizon)\nfor dd in range(training_horizon-1,features_train.shape[0],training_horizon):\n    features_cv.append(features_train[dd])\n    name_cv.append(names_rate_[dd])\n    standard_confirmed_cv.append(standard_confirmed_train[dd])\n    standard_fatalities_cv.append(standard_fatalities_train[dd])\n    print (name_cv[-1], standard_confirmed_cv[-1], standard_fatalities_cv[-1])\n    \n \nfeatures_cv=np.array(features_cv)\npreds_confirmed_rate_cv=np.zeros((features_cv.shape[0],horizon))\npreds_confirmed_cumulative_rate_cv=np.zeros((features_cv.shape[0],horizon))\n\npreds_fatalities_rate_cv=np.zeros((features_cv.shape[0],horizon))\npreds_fatalities_cumulative_rate_cv=np.zeros((features_cv.shape[0],horizon))\n\noveral_rmsle_metric_confirmed=0.0\n\nfor j in range (preds_confirmed_rate_cv.shape[1]):\n\n    this_features_cv=features_cv                          \n\n    preds=predict(features_cv,bagging, input_name=model_directory +\"confirmed\"+ str(j))\n    preds_confirmed_rate_cv[:,j]=preds\n    print (\" modelling confirmed, case %d, , original cv %d and after %d \"%(j,this_features_cv.shape[0],this_features_cv.shape[0])) \n\npredictions=[] \nfor ii in range (preds_confirmed_rate_cv.shape[0]):\n    current_prediction=standard_confirmed_cv[ii]\n    if current_prediction==0 :\n        current_prediction=0.1   \n    this_preds=preds_confirmed_rate_cv[ii].tolist()\n    name=name_cv[ii]\n    reserve=this_preds[0]\n    #overrides     \n    if use_external and name in holdder_cumulative :\n        print(\" name %s is for confirmed rate\" %(name))\n        this_preds=revert_preds(this_preds)    \n    \n    if name in add_constant_rate:\n        this_preds=add_rate(this_preds, add_constant_rate[name][0])\n        \n    for j in range (preds_confirmed_rate_cv.shape[1]):\n        \n                if use_external and j==0 and name in holdder_cumulative:\n                    current_prediction=holdder_cumulative[name][0]\n                    preds_confirmed_cumulative_rate_cv[ii][j]=current_prediction                                     \n                else:\n                    current_prediction*=max(1,this_preds[j])\n                    preds_confirmed_cumulative_rate_cv[ii][j]=current_prediction\n\n\nfor j in range (preds_confirmed_rate_cv.shape[1]):\n\n    this_features_cv=features_cv\n                             \n    preds=predict(features_cv,bagging, input_name=model_directory +\"fatal\"+ str(j))\n    preds_fatalities_rate_cv[:,j]=preds\n    print (\" modelling fatalities, case %d, original cv %d and after %d \"%( j,this_features_cv.shape[0],this_features_cv.shape[0])) \n\n    \npredictions=[]\nfor ii in range (preds_fatalities_rate_cv.shape[0]):\n    current_prediction=standard_fatalities_cv[ii]\n        \n    this_preds=preds_fatalities_rate_cv[ii].tolist()\n    name=name_cv[ii]\n    reserve=this_preds[0]\n    #overrides\n    if use_external and name in holdder_cumulative :\n        print(\" name %s is for confirmed rate\" %(name))\n        this_preds=revert_preds(this_preds)    \n    \n    if name in add_constant_rate:\n        this_preds=add_rate(this_preds, add_constant_rate[name][1])       \n  \n    for j in range (preds_fatalities_rate_cv.shape[1]):\n                if current_prediction==0 and  preds_confirmed_cumulative_rate_cv[ii][j]>400 :#(preds_confirmed_cumulative_rate_cv[ii][j]>400 or \"Malta\" in name or \"Somalia\" in name):\n                    current_prediction=1.\n                    \n                if use_external and j==0 and name in holdder_cumulative:\n                    current_prediction=holdder_cumulative[name][1]\n                    preds_fatalities_cumulative_rate_cv[ii][j]=current_prediction      \n                else:    \n                    current_prediction*=max(1,this_preds[j])\n                    preds_fatalities_cumulative_rate_cv[ii][j]=current_prediction","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nkey_to_confirmed={}\nkey_to_fatality={}\nkey_to_confirmed_count={}\nkey_to_fatality_count={}\n\nkey_to_confirmed_cumulative_rate={}\nkey_to_fatality_cumulative_rate={}\nkey_to_confirmed_rate={}\nkey_to_fatality_rate={}\n\nprint(len(features_cv), len(name_cv),len(standard_confirmed_cv),len(standard_fatalities_cv)) \nprint(preds_confirmed_cv.shape,\n      preds_confirmed_standard_cv.shape,\n      preds_fatalities_cv.shape,\n      preds_fatalities_standard_cv.shape,\n     \n      preds_confirmed_rate_cv.shape,\n      preds_confirmed_cumulative_rate_cv.shape,\n      preds_fatalities_rate_cv.shape,\n      preds_fatalities_cumulative_rate_cv.shape     \n     \n     ) \nfor j in range (len(name_cv)):\n    \n    key_to_confirmed_count[name_cv[j]]=preds_confirmed_non_cumulative_cv[j,:].tolist()\n    key_to_fatality_count[name_cv[j]]=preds_fatalities_non_cumulative_cv[j,:].tolist()\n    key_to_confirmed[name_cv[j]]  =preds_confirmed_standard_cv[j,:].tolist()  \n    key_to_fatality[name_cv[j]]=preds_fatalities_standard_cv[j,:].tolist()  \n    \n    key_to_confirmed_cumulative_rate[name_cv[j]]=preds_confirmed_cumulative_rate_cv[j,:].tolist()\n    key_to_fatality_cumulative_rate[name_cv[j]]=preds_fatalities_cumulative_rate_cv[j,:].tolist()\n    key_to_confirmed_rate[name_cv[j]] =preds_confirmed_rate_cv[j,:].tolist()  \n    key_to_fatality_rate[name_cv[j]]=preds_fatalities_rate_cv[j,:].tolist()  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_new=train[[\"Date\",\"key\",\"rate_ConfirmedCases\",\"rate_Fatalities\",\"ConfirmedCases\",\"Fatalities\",\"diff_ConfirmedCases\",\"diff_Fatalities\"]]\n\ntrain_new[\"ConfirmedCasescumrate\"]=train_new[\"ConfirmedCases\"].values\ntrain_new[\"Fatalitiescumrate\"]=train_new[\"Fatalities\"].values\n\ntest_new_count=pd.merge(test,train_new, how=\"left\", left_on=[\"key\",\"Date\"], right_on=[\"key\",\"Date\"] ).reset_index(drop=True)\n\ntest_new_count","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def fillin_columns(frame,key_column, original_name, training_horizon, test_horizon, unique_values, key_to_values):\n    keys=frame[key_column].values\n    original_values=frame[original_name].values.tolist()\n    print(len(keys), len(original_values), training_horizon ,test_horizon,len(key_to_values))\n    \n    for j in range(unique_values):\n        current_index=(j * (training_horizon +test_horizon )) +training_horizon \n        current_key=keys[current_index]\n        values=key_to_values[current_key]\n        co=0\n        for g in range(current_index, current_index + test_horizon):\n            original_values[g]=values[co]\n            co+=1\n    \n    frame[original_name]=original_values\n \n\nall_days=int(test_new_count.shape[0]/len(unique_keys))\n\ntr_horizon=all_days-horizon\nprint(all_days,tr_horizon, horizon )\n\nfillin_columns(test_new_count,\"key\", 'ConfirmedCases', tr_horizon, horizon, len(unique_keys), key_to_confirmed)    \nfillin_columns(test_new_count,\"key\", 'Fatalities', tr_horizon, horizon, len(unique_keys), key_to_fatality)   \nfillin_columns(test_new_count,\"key\", 'diff_ConfirmedCases', tr_horizon, horizon, len(unique_keys), key_to_confirmed_count)   \nfillin_columns(test_new_count,\"key\", 'diff_Fatalities', tr_horizon, horizon, len(unique_keys), key_to_fatality_count)   \n\nfillin_columns(test_new_count,\"key\", 'ConfirmedCasescumrate', tr_horizon, horizon, len(unique_keys), key_to_confirmed_cumulative_rate)    \nfillin_columns(test_new_count,\"key\", 'Fatalitiescumrate', tr_horizon, horizon, len(unique_keys), key_to_fatality_cumulative_rate)   \nfillin_columns(test_new_count,\"key\", 'rate_ConfirmedCases', tr_horizon, horizon, len(unique_keys), key_to_confirmed_rate)   \nfillin_columns(test_new_count,\"key\", 'rate_Fatalities', tr_horizon, horizon, len(unique_keys), key_to_fatality_rate)   \n\n\ntest_new_count","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#####create count_difference from rate model\n\n#get_difference(test_new_count, \"key\", \"ConfirmedCasescumrate\", new_target_name=\"diff_ConfirmedCasescumrate\" )\n#get_difference(test_new_count, \"key\", 'Fatalitiescumrate', new_target_name=\"diff_Fatalitiescumrate\" )\n######################## HERE COMM ##################\nget_difference(test_new_count, \"key\", \"ConfirmedCasescumrate\", new_target_name=\"diff_ConfirmedCasescumrate\" )#_special\nget_difference(test_new_count, \"key\", 'Fatalitiescumrate', new_target_name=\"diff_Fatalitiescumrate\" )#_special\n\n\n#test_new_count['final_diff_ConfirmedCases']=test_new_count['diff_ConfirmedCases'].values *0.5 + test_new_count['diff_ConfirmedCasescumrate'].values *0.5\n#test_new_count['final_diff_Fatalities']=test_new_count['diff_Fatalities'].values *0.5 + test_new_count['diff_Fatalitiescumrate'].values *0.5\n\n#test_new_count['final_cumulative_ConfirmedCases']=test_new_count['ConfirmedCases'].values*0.5+test_new_count['ConfirmedCasescumrate'].values*0.5\n#test_new_count['final_cumulative_Fatalities']=test_new_count['Fatalities'].values*0.5+test_new_count['Fatalitiescumrate'].values*0.5\n\ntest_new_count['final_diff_ConfirmedCases']=test_new_count['diff_ConfirmedCases'].values \ntest_new_count['final_diff_Fatalities']=test_new_count['diff_Fatalities'].values \n\ntest_new_count['final_cumulative_ConfirmedCases']=test_new_count['ConfirmedCases'].values\ntest_new_count['final_cumulative_Fatalities']=test_new_count['Fatalities'].values\n\n\n#test_new_count['final_diff_ConfirmedCases']=test_new_count['diff_ConfirmedCasescumrate'].values\n#test_new_count['final_diff_Fatalities']=test_new_count['diff_Fatalitiescumrate'].values\n\n#test_new_count['final_cumulative_ConfirmedCases']=test_new_count['ConfirmedCasescumrate'].values\n#test_new_count['final_cumulative_Fatalities']=test_new_count['Fatalitiescumrate'].values\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"###find all_countries that in the last day have count more than minimum\ntrain_again=pd.read_csv(directory + \"train.csv\", parse_dates=[\"Date\"] , engine=\"python\")\ntrain_again[\"key\"]=train_again[[\"County\",\"Province_State\",\"Country_Region\"]].apply(\n    lambda row: str(row[0]) + \"_\" + str(row[1])+ \"_\" + str(row[2]),axis=1)\n\nmax_train_new=train_again[\"Date\"].max()\nprint(\"last date in the training data is %s\" %(max_train_new))\ntrain_again_mini=train_again[train_again.Date == max_train_new].copy()\nprint (\" shape of frame using only date {} is {}\".format(max_train_new,train_again_mini.shape))\n#flitered framed based on counts of confirmed and \n\ntrain_again_mini_filtered=train_again_mini[train_again_mini.Target==\"ConfirmedCases\"]\nprint (\" shape of frame using only date {}  and confirmed cases is {}\".format(max_train_new,train_again_mini_filtered.shape))\ntrain_again_mini_filtered=train_again_mini_filtered[train_again_mini_filtered.TargetValue>minimum_count_for_rate_model]\nprint (\" shape of frame using only date {}  and confirmed cases higher than {} is {}\".format(\n    max_train_new,minimum_count_for_rate_model,train_again_mini_filtered.shape))\nunique_countrie_for_rate_model=train_again_mini_filtered[\"key\"].unique()\n################## HERE COM #################### \nunique_countrie_for_rate_model=[cn for  cn,vall in add_constant_rate.items()]\n \nprint (\" there are {} unique countries that on the last training date {} had a count of more than {}\".format(\n    len(unique_countrie_for_rate_model),max_train_new, minimum_count_for_rate_model))\nprint(unique_countrie_for_rate_model)\n\ntest_new_count.loc[test_new_count.key.isin(unique_countrie_for_rate_model), 'final_diff_ConfirmedCases'] = test_new_count.loc[test_new_count.key.isin(unique_countrie_for_rate_model), 'diff_ConfirmedCasescumrate']\ntest_new_count.loc[test_new_count.key.isin(unique_countrie_for_rate_model), 'final_diff_Fatalities'] = test_new_count.loc[test_new_count.key.isin(unique_countrie_for_rate_model), 'diff_Fatalitiescumrate']\n\ntest_new_count.loc[test_new_count.key.isin(unique_countrie_for_rate_model), 'final_cumulative_ConfirmedCases'] = test_new_count.loc[test_new_count.key.isin(unique_countrie_for_rate_model), 'ConfirmedCasescumrate']\ntest_new_count.loc[test_new_count.key.isin(unique_countrie_for_rate_model), 'final_cumulative_Fatalities'] = test_new_count.loc[test_new_count.key.isin(unique_countrie_for_rate_model), 'Fatalitiescumrate']\n\n#create time variable\nget_time(test_new_count, \"key\", new_target_name=\"t\") #get time\ntime=test_new_count[\"t\"].values.tolist()\nprint (time[:40])\nfor jjj in range(len(time)):\n    if time[jjj]<15:\n        time[jjj]=1\n    else :\n        time[jjj]=time[jjj]-14\nprint (time[:40])\ntest_new_count[\"t\"]=np.array(time)\n\nget_x_day_min_max_avg(test_new_count,  \"key\", 'final_diff_ConfirmedCases',window=7, new_target_name=\"final_diff_ConfirmedCases_window\")\nget_x_day_min_max_avg(test_new_count,  \"key\", 'final_diff_Fatalities',window=7, new_target_name=\"final_diff_Fatalities_window\")\n\n\n\ntest_new_count['final_diff_ConfirmedCases_05']=test_new_count['final_diff_ConfirmedCases_window_min'].values*(\n    ((test_new_count['t'].values*10.85600625)*0.00001) +0.207181222)\n\ntest_new_count['final_diff_ConfirmedCases_95']=test_new_count['final_diff_ConfirmedCases_window_max'].values*(\n    ((test_new_count['t'].values*0.832347133)*0.042631428) +1.325626509)\n\n\n\ntest_new_count['final_diff_Fatalities_05']=test_new_count['final_diff_Fatalities_window_min'].values*(\n    ((test_new_count['t'].values*11.17489827)*0.00001) +0.271907924)\n\ntest_new_count['final_diff_Fatalities_95']=test_new_count['final_diff_Fatalities_window_max'].values*(\n    ((test_new_count['t'].values*0.00001)*0.001111873) +1.771248526)\n\n#test_new_count['final_diff_ConfirmedCases']=test_new_count['final_diff_ConfirmedCases'].values * 0.9\n#test_new_count['final_diff_Fatalities']=test_new_count['final_diff_Fatalities'].values *0.9\n\n\ntest_new_count.loc[test_new_count['final_diff_ConfirmedCases']<0.5, \"final_diff_ConfirmedCases\"]=0.0\ntest_new_count.loc[test_new_count['final_diff_Fatalities']<0.5,'final_diff_Fatalities']=0.0\n\ntest_new_count.loc[test_new_count['final_diff_ConfirmedCases_05']<0.5, \"final_diff_ConfirmedCases_05\"]=0.0\ntest_new_count.loc[test_new_count['final_diff_Fatalities_05']<0.5,'final_diff_Fatalities_05']=0.0\n\n\n\ntest_new_count\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_new_count['final_diff_ConfirmedCases'].head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\ntaus=[\"0.05\",\"0.5\",\"0.95\"]\ntaus+=taus\n\n_preds=['final_diff_ConfirmedCases_05','final_diff_ConfirmedCases','final_diff_ConfirmedCases_95'] +['final_diff_Fatalities_05','final_diff_Fatalities','final_diff_Fatalities_95']\nassert(len(taus))==6==len(_preds)\n\nsubmission=[]\n\nfor j in range(len(taus)):\n    ids=\"\"\n    if \"Fatalities\" in _preds[j]:\n         ids=\"Id_Fatalities\"\n    elif\"ConfirmedCases\" in _preds[j]:\n         ids=\"Id_ConfirmedCases\"\n    else :\n        raise Exception(\" name not identified in title\")\n    print (\" adding %s with pred name %s for tau %s\"%(ids,_preds[j],taus[j]))\n        \n    mini_frame=test_new_count[[ids,_preds[j]]]\n    mini_frame[\"id\"]=mini_frame[ids].values\n    mini_frame[ids]=mini_frame[ids].apply(lambda x: str(x) +\"_\" + taus[j])\n    mini_frame=mini_frame.reset_index(drop=True)\n    mini_frame.columns=[\"ForecastId_Quantile\",\"TargetValue\",\"id\"]\n    submission.append(mini_frame)\n    \nsubmission=pd.concat(submission)\nsubmission = submission.sort_values([\"id\",\"ForecastId_Quantile\"], ascending = (True, True))\n\nsubmission.drop(\"id\", inplace=True, axis=1)\n\nsubmission.to_csv( \"submission.csv\", index=False)\n\nsubmission","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}