{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.ensemble import RandomForestRegressor \nimport warnings\nwarnings.filterwarnings('ignore')\n\ndf=pd.read_csv('/kaggle/input/covid19-global-forecasting-week-5/train.csv', index_col='Id')\ndtest=pd.read_csv('/kaggle/input/covid19-global-forecasting-week-5/test.csv', index_col='ForecastId')\n\n# dividing the datasets into ConfirmedCases and Fatalities\n#dcc=df.loc[df['Target']=='ConfirmedCases']\n#dfat=df.loc[df['Target']=='Fatalities']\n#tdcc=dtest.loc[dtest['Target']=='ConfirmedCases']\n#tdfat=dtest.loc[dtest['Target']=='Fatalities']\n\n#Drop Target column \n#dcc.drop('Target', axis=1, inplace=True)\n#dfat.drop('Target', axis=1, inplace=True)\n#tdcc.drop('Target', axis=1, inplace=True)\n#tdfat.drop('Target', axis=1, inplace=True)\n\n#create y and drop TargetValue from dcc and dfat\ny=df.TargetValue\n#yfat=dfat.TargetValue\ndf.drop('TargetValue', axis=1, inplace=True)\n#dfat.drop('TargetValue', axis=1, inplace=True)\ndf['check']=1\ndtest['check']=2\ncombo=pd.concat([df,dtest])\n#dfat['check']=1\n#tdfat['check']=2\n#combo_fat=pd.concat([dfat,tdfat])\n\n#Add democracy indices for each country from Democracy dataset\ndemocracy=pd.read_csv('/kaggle/input/covid19-week-5-supplementary-datasets/Democracy.csv')\ndemo2018=democracy.loc[democracy['time']==2018]\ndemo2019=democracy.loc[democracy['time']==2019]\n\ndemo2018.drop(['geo','time','Electoral pluralism index (EIU)','Political participation index(EIU)',\n              'Political culture index (EIU)','Change in democracy index (EIU)'], axis=1, inplace=True)\ndemo2018=demo2018.rename(columns={\"name\": \"Country_Region\", \"Democracy index (EIU)\": \"di18\", \n                         \"Government index (EIU)\": \"gi18\",\"Civil liberties index (EIU)\": \"cli18\"})\ndemo2019.drop(['geo','time','Electoral pluralism index (EIU)','Political participation index(EIU)',\n              'Political culture index (EIU)','Change in democracy index (EIU)'], axis=1, inplace=True)\ndemo2019=demo2019.rename(columns={\"name\": \"Country_Region\", \"Democracy index (EIU)\": \"di19\", \n                         \"Government index (EIU)\": \"gi19\",\"Civil liberties index (EIU)\": \"cli19\"})\ncombo=combo.merge(demo2018, how='left', on=['Country_Region'])\ncombo=combo.merge(demo2019, how='left', on=['Country_Region'])\n#combo_fat=combo_fat.merge(demo2018, how='left', on=['Country_Region'])\n#combo_fat=combo_fat.merge(demo2019, how='left', on=['Country_Region'])\n\n#Add economy indicators for each country from 2019 economy dataset\neconomy=pd.read_excel('/kaggle/input/covid19-week-5-supplementary-datasets/economy.xlsx')\n\necon_GDP_ConstPr=economy.loc[(economy['Subject Descriptor']=='Gross domestic product, constant prices')]\necon_GDP_CurrPr=economy.loc[economy['Subject Descriptor']=='Gross domestic product, current prices']\necon_GDP_Capita=economy.loc[economy['Subject Descriptor']=='Gross domestic product per capita, constant prices']\necon_Inf_ConstPr=economy.loc[economy['Subject Descriptor']=='Inflation, average consumer prices']\necon_Inf_eoP=economy.loc[economy['Subject Descriptor']=='Inflation, end of period consumer prices']\necon_unemp=economy.loc[economy['Subject Descriptor']=='Unemployment rate']\n\necon_GDP_ConstPr.drop(['Subject Descriptor','Units'], axis=1, inplace=True)\necon_GDP_ConstPr=econ_GDP_ConstPr.rename(columns={\"Country\": \"Country_Region\", 2019: \"GDP_ConstPr19\"})\necon_GDP_CurrPr.drop(['Subject Descriptor','Units'], axis=1, inplace=True)\necon_GDP_CurrPr=econ_GDP_CurrPr.rename(columns={\"Country\": \"Country_Region\", 2019: \"GDP_CurrPr19\"})\necon_GDP_Capita.drop(['Subject Descriptor','Units'], axis=1, inplace=True)\necon_GDP_Capita=econ_GDP_Capita.rename(columns={\"Country\": \"Country_Region\", 2019: \"GDP_Capita19\"})\necon_Inf_ConstPr.drop(['Subject Descriptor','Units'], axis=1, inplace=True)\necon_Inf_ConstPr=econ_Inf_ConstPr.rename(columns={\"Country\": \"Country_Region\", 2019: \"Inf_ConstPr19\"})\necon_Inf_eoP.drop(['Subject Descriptor','Units'], axis=1, inplace=True)\necon_Inf_eoP=econ_Inf_eoP.rename(columns={\"Country\": \"Country_Region\", 2019: \"Inf_eoP19\"})\necon_unemp.drop(['Subject Descriptor','Units'], axis=1, inplace=True)\necon_unemp=econ_unemp.rename(columns={\"Country\": \"Country_Region\", 2019: \"unemp19\"})\n\ncombo=combo.merge(econ_GDP_ConstPr, how='left', on=['Country_Region'])\ncombo=combo.merge(econ_GDP_CurrPr, how='left', on=['Country_Region'])\ncombo=combo.merge(econ_GDP_Capita, how='left', on=['Country_Region'])\ncombo=combo.merge(econ_Inf_ConstPr, how='left', on=['Country_Region'])\ncombo=combo.merge(econ_Inf_eoP, how='left', on=['Country_Region'])\ncombo=combo.merge(econ_unemp, how='left', on=['Country_Region'])\n\n#combo_fat=combo_fat.merge(econ_GDP_ConstPr, how='left', on=['Country_Region'])\n#combo_fat=combo_fat.merge(econ_GDP_CurrPr, how='left', on=['Country_Region'])\n#combo_fat=combo_fat.merge(econ_GDP_Capita, how='left', on=['Country_Region'])\n#combo_fat=combo_fat.merge(econ_Inf_ConstPr, how='left', on=['Country_Region'])\n#combo_fat=combo_fat.merge(econ_Inf_eoP, how='left', on=['Country_Region'])\n#combo_fat=combo_fat.merge(econ_unemp, how='left', on=['Country_Region'])\n\n#Add population density for each country \ndensity=pd.read_excel('/kaggle/input/covid19-week-5-supplementary-datasets/population_density.xlsx')\ncombo=combo.merge(density, how='left', on=['Country_Region'])\n#combo_fat=combo_fat.merge(density, how='left', on=['Country_Region'])\n\n#Add Sanitation Services for each country from WHO 2017 WASH report (the latest from the data repository) \nSan=pd.read_csv('/kaggle/input/covid19-week-5-supplementary-datasets/San_service.csv')\ncombo=combo.merge(San, how='left', on=['Country_Region'])\n#combo_fat=combo_fat.merge(San, how='left', on=['Country_Region'])\n\n#Add Mortality due to unsafe WASH services for each country from WHO 2016 WASH report (the latest from the data repository) \nMort=pd.read_csv('/kaggle/input/covid19-week-5-supplementary-datasets/Mort_unsafe.csv')\ncombo=combo.merge(Mort, how='left', on=['Country_Region'])\n#combo_fat=combo_fat.merge(Mort, how='left', on=['Country_Region'])\n\n#Impute Mort by median()\ncombo['Mor_unsafeWASH16']=combo['Mor_unsafeWASH16'].fillna(combo['Mor_unsafeWASH16'].median())\n#combo_fat['Mor_unsafeWASH16']=combo_fat['Mor_unsafeWASH16'].fillna(combo_fat['Mor_unsafeWASH16'].median())\n#Impute demo2018 and demo2019 indicators by min()\ncombo['di18']=combo['di18'].fillna(combo['di18'].min())\n#combo_fat['di18']=combo_fat['di18'].fillna(combo_fat['di18'].min())\ncombo['di19']=combo['di19'].fillna(combo['di19'].min())\n#combo_fat['di19']=combo_fat['di19'].fillna(combo_fat['di19'].min())\ncombo['gi18']=combo['gi18'].fillna(combo['gi18'].min())\n#combo_fat['gi18']=combo_fat['gi18'].fillna(combo_fat['gi18'].min())\ncombo['gi19']=combo['gi19'].fillna(combo['gi19'].min())\n#combo_fat['gi19']=combo_fat['gi19'].fillna(combo_fat['gi19'].min())\ncombo['cli18']=combo['cli18'].fillna(combo['cli18'].min())\n#combo_fat['cli18']=combo_fat['cli18'].fillna(combo_fat['cli18'].min())\ncombo['cli19']=combo['cli19'].fillna(combo['cli19'].min())\n#combo_fat['cli19']=combo_fat['cli19'].fillna(combo_fat['cli19'].min())\n#Impute San by max() - cruise ships\ncombo['Basic_San_service17']=combo['Basic_San_service17'].fillna(combo['Basic_San_service17'].max())\n#combo_fat['Basic_San_service17']=combo_fat['Basic_San_service17'].fillna(combo_fat['Basic_San_service17'].max())\ncombo['Safe_San_service17']=combo['Safe_San_service17'].fillna(combo['Safe_San_service17'].max())\n#combo_fat['Safe_San_service17']=combo_fat['Safe_San_service17'].fillna(combo_fat['Safe_San_service17'].max())\n#Impute econ by 0\ncombo['GDP_ConstPr19']=combo['GDP_ConstPr19'].fillna(0)\n#combo_fat['GDP_ConstPr19']=combo_fat['GDP_ConstPr19'].fillna(0)\ncombo['GDP_CurrPr19']=combo['GDP_CurrPr19'].fillna(0)\n#combo_fat['GDP_CurrPr19']=combo_fat['GDP_CurrPr19'].fillna(0)\ncombo['GDP_Capita19']=combo['GDP_Capita19'].fillna(0)\n#combo_fat['GDP_Capita19']=combo_fat['GDP_Capita19'].fillna(0)\ncombo['Inf_ConstPr19']=combo['Inf_ConstPr19'].fillna(0)\n#combo_fat['Inf_ConstPr19']=combo_fat['Inf_ConstPr19'].fillna(0)\ncombo['Inf_eoP19']=combo['Inf_eoP19'].fillna(0)\n#combo_fat['Inf_eoP19']=combo_fat['Inf_eoP19'].fillna(0)\ncombo['unemp19']=combo['unemp19'].fillna(0)\n#combo_fat['unemp19']=combo_fat['unemp19'].fillna(0)\n#Impute density by max()\ncombo['density_19']=combo['density_19'].fillna(combo['density_19'].max())\n#combo_fat['density_19']=combo_fat['density_19'].fillna(combo_cc['density_19'].max())\ncombo['density_20']=combo['density_20'].fillna(combo['density_20'].max())\n#combo_fat['density_20']=combo_fat['density_20'].fillna(combo_cc['density_20'].max())\n\n#Impute County and Province_State by 'A'\ncombo['County']=combo['County'].fillna('A')\n#combo_fat['County']=combo_fat['County'].fillna('A')\ncombo['Province_State']=combo['Province_State'].fillna('A')\n#combo_fat['Province_State']=combo_fat['Province_State'].fillna('A')\n\n#Managing Date by taking only the MM-DD\ndef date_split(date):\n    d=date.str.split('-', n=1, expand=True)\n    return d[1]\ncombo['MM_DD']= date_split(combo['Date'])\n#combo_fat['MM_DD']= date_split(combo_fat['Date'])\n\n#Managing Location\ncombo['Location']=combo['County'] + combo['Province_State'] + combo['Country_Region']\n#combo_fat['Location']=combo_fat['County'] + combo_fat['Province_State'] + combo_fat['Country_Region']\n\n#Label Encode Date\nle=LabelEncoder()\ncombo['Target']=le.fit_transform(combo['Target'])\ncombo['MM_DD']=le.fit_transform(combo['MM_DD'])\n#combo_fat['MM_DD']=le.fit_transform(combo_fat['MM_DD'])\ncombo['Location']=le.fit_transform(combo['Location'])\n#combo_fat['Location']=le.fit_transform(combo_fat['Location'])\n\n#Drop repeated columns like Date\ncombo.drop(['County','Province_State','Country_Region','Date'], axis=1, inplace=True)\n#combo_fat.drop(['County','Province_State','Country_Region','Date'], axis=1, inplace=True)\n\n#Divide combo to train and test dataframes and drop 'check' column\ndf_cc=combo[combo['check']==1]\ndtest_cc=combo[combo['check']==2]\n#df_fat=combo_fat[combo_fat['check']==1]\n#dtest_fat=combo_fat[combo_fat['check']==2]\n\n#Separate 'Weight' in separate df and drop it from the originals\n#w_cc=df_cc['Weight']\n#wtest_cc=dtest_cc['Weight']\n#w_fat=df_fat['Weight']\n#wtest_fat=dtest_fat['Weight']\ndf_cc.drop('check', axis=1, inplace=True)\ndtest_cc.drop('check', axis=1, inplace=True)\n#df_fat.drop(['Weight', 'check'], axis=1, inplace=True)\n#dtest_fat.drop(['Weight', 'check'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_cc.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dtest_cc.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(2):\n    X_train1, X_valid1, y_train1, y_valid1=train_test_split(df_cc, y, train_size=0.8, \n                                                            test_size=0.2, random_state=8)\n    et=ExtraTreesRegressor(n_estimators=10, random_state=14)\n    p=et.fit(X_train1, y_train1).predict(X_valid1)\n    print(i,'----', mean_absolute_error(y_valid1, p))\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pf1=et.fit(df_cc,y).predict(dtest_cc)\npf_out=pd.DataFrame({'Id': dtest_cc.index, 'TargetValue':pf1})\n\nq=pf_out.groupby(['Id'])['TargetValue'].quantile(q=0.05).reset_index()\na=pf_out.groupby(['Id'])['TargetValue'].quantile(q=0.5).reset_index()\nz=pf_out.groupby(['Id'])['TargetValue'].quantile(q=0.95).reset_index()\n\nq.columns=['Id', '0.05']\na.columns=['Id', '0.5']\nz.columns=['Id', '0.95']\nq=pd.concat([q,a['0.5'],z['0.95']], 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"s={}\nfor i in range(len(q)):\n    s[str(i+1)+'_'+'0.05']=q['0.05'][i]\n    s[str(i+1)+'_'+'0.5']=q['0.5'][i]\n    s[str(i+1)+'_'+'0.95']=q['0.95'][i]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission=pd.DataFrame(s.items(), columns=['ForecastId_Quantile', 'TargetValue'])\nsubmission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}