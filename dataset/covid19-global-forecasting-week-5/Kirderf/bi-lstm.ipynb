{"cells":[{"metadata":{},"cell_type":"markdown","source":"Based on this kernel https://www.kaggle.com/avirl3364/lstm-covid19, credit to @avirl3364.\nI changed it to Bi-LSTM with Mish and Swish and some other minor modifications.\nIt's also config. to be tested on public LB, with no leak.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install tensorflow_addons","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.layers import LSTM, Dropout, Dense, GRU\nfrom tensorflow.keras import regularizers\nfrom tensorflow.keras.models import Sequential\nfrom sklearn.preprocessing import Normalizer, MinMaxScaler, LabelEncoder\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom tensorflow.keras import metrics\nfrom sklearn.utils import shuffle\nfrom tensorflow.keras.layers import *\nimport tensorflow_addons as tfa\nfrom tensorflow.keras import losses, models, optimizers\n\nSEED = 321\ndef seed_everything(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    tf.random.set_seed(seed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = pd.read_csv('../input/covid19-global-forecasting-week-5/train.csv')\ndf_test = pd.read_csv('../input/covid19-global-forecasting-week-5/test.csv')\n\n\ntest_date_min = df_test['Date'].min()\ntest_date_max = df_test['Date'].max()\ndf_train = df_train[df_train['Date']<test_date_min]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['Date'].max()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from datetime import date\n\nd0 = date(2020,1,23)\nd1 = date(2020,4,26)\ndelta = d1 - d0\nnumber = delta.days+1\nnumber","execution_count":null,"outputs":[]},{"metadata":{"id":"Qjor0jHDihhG","outputId":"014d27b3-6bcb-494b-ed09-4f238b1516a6","trusted":true},"cell_type":"code","source":"\n\ntrain_data = df_train.drop(\n    ['Id', 'County', 'Province_State', 'Country_Region'], axis=1)\n#test_data = df_test.drop(\n#    ['County', 'Province_State', 'Country_Region'], axis=1)\ntrain_data.set_index('Date', inplace=True)\n#test_data.set_index('Date', inplace=True)\n\ntrain_confirm = train_data[train_data['Target'] == 'ConfirmedCases']\ntrain_confirm = train_confirm.drop(['Target'], axis = 1)\ntrain_confirm['TargetValue'] = np.where(train_confirm['TargetValue'] <=0, 0, train_confirm['TargetValue'])\n#print(train_confirm)\n\nX = train_confirm.iloc[:, 0: 4].to_numpy()\nY = train_data.iloc[:, 4: 5].to_numpy()\n\n# MinMaxScaling\n\nsc_pop = MinMaxScaler(feature_range=(0, 1))\nsc_tg = MinMaxScaler(feature_range=(0, 1))\nX[:, 0:1] = sc_pop.fit_transform(X[:, 0:1])\nX[:, 2:3] = sc_tg.fit_transform(X[:, 2:3])\n\n\nX = X.reshape(-1,number,3)\nprint(X.shape)\n#print(df_train.dtypes)\n#print(X)\n\ndef multivariate_data(dataset, target, start_index, end_index, time_step) :\n\tdata=list()\n\tlabel =list()\n\n\tstart_index = start_index + time_step\n\tfor i in range(start_index, end_index) :\n\t\tindices = range(i-time_step, i)\n\t\tdata.append(dataset[indices])\n\t\tlabel.append(target[i])\n\n\treturn np.array(data), np.array(label)\n\ntime_step = 40\npartition = number-4-time_step\nX_train, Y_train = multivariate_data(X[0,:,:], X[0,:,2], 0, number, time_step)\n\nfor i in range(1,3463) :\n\tX_dummy, Y_dummy = multivariate_data(X[i,:,:], X[i,:,2], 0, number, time_step)\n\tX_train = np.concatenate((X_train, X_dummy), axis = 0)\n\tY_train = np.concatenate((Y_train, Y_dummy), axis = 0)\n\nprint(X_train.shape)\nprint(Y_train.shape)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def swishE(x):\n   beta = 1.75 #1, 1.5 or 2\n   return beta * x * tf.keras.backend.sigmoid(x)\n\ndef swish(x):\n    return x * tf.keras.backend.sigmoid(x)\n\ndef phrishII(x):\n    return x*tf.keras.backend.tanh(1.75 * x * tf.keras.backend.sigmoid(x))\ndef phrishI(x):\n    return x*tf.keras.backend.tanh(x * tf.keras.backend.sigmoid(x))\n\ndef mish(x):\n    return x*tf.keras.backend.tanh(tf.keras.backend.softplus(x))\n\ndef gelu_new(x):\n    \"\"\"Gaussian Error Linear Unit.\n    This is a smoother version of the RELU.\n    Original paper: https://arxiv.org/abs/1606.08415\n    Args:\n        x: float Tensor to perform activation.\n    Returns:\n        `x` with the GELU activation applied.\n    \"\"\"\n    cdf = 0.5 * (1.0 + tf.tanh(\n        (np.sqrt(2 / np.pi) * (x + 0.044715 * tf.pow(x, 3)))))\n    return x * cdf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del regressor, checkpoint, es, ReduceLROnPlateau,regr1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def regr1():\n        \n    inp = Input(shape = (X_train.shape[1], 3))\n    x = tf.keras.layers.Bidirectional(LSTM(units = 32, return_sequences = True,input_shape = (X_train.shape[1], 3)))(inp)\n    x = Activation(mish)(x)\n    x = Dropout(0.3)(x)\n    x = tf.keras.layers.Bidirectional(LSTM(units = 32, return_sequences = True))(x)\n    x = Activation(mish)(x)\n    x = Dropout(0.3)(x)\n    x = tf.keras.layers.Bidirectional(GRU(units = 32))(x)\n    x = Activation(mish)(x)\n    x = Dropout(0.3)(x)\n    out = Dense(1, activation = swish, name = 'out')(x)\n    \n    model = models.Model(inputs = inp, outputs = out)\n    \n    #opt1 = tf.keras.optimizers.RMSprop(lr=0.0001, rho=0.9, epsilon=1e-08, decay=0.0)#'RMSprop'\n    opt = 'RMSprop'\n    #opt2 = tfa.optimizers.AdamW(lr=0.001,weight_decay=5.5e-6)\n    #opt = tfa.optimizers.Lookahead(tfa.optimizers.AdamW(lr=0.001,weight_decay=0.006), sync_period=5, slow_step_size=0.5)\n    #opt = tfa.optimizers.Lookahead(opt1, sync_period=5, slow_step_size=0.5)\n    #opt = tfa.optimizers.SWA(opt)\n\n    model.compile(optimizer = opt, loss = 'mean_squared_error')\n    return model\n\nregressor = regr1()\nregressor.summary()\ntf.keras.utils.plot_model(\n    regressor, to_file='model1.png', show_shapes=True, show_layer_names=True,\n    rankdir='TB', expand_nested=True, dpi=96\n)","execution_count":null,"outputs":[]},{"metadata":{"id":"hwu7cQK8m-gM","outputId":"3f789cfc-6aff-4eba-a007-516250b0e5ec","trusted":true},"cell_type":"code","source":"checkpoint = tf.keras.callbacks.ModelCheckpoint(\"model_1_.h5\".format(i), monitor='val_loss', verbose=1, save_best_only=True,save_weights_only=True, mode='min')\nes = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, verbose=1, mode='min', restore_best_weights=True)\nReduceLROnPlateau = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss',mode='min', patience=2, verbose=1, factor=0.5, min_lr=0.00001)\nregressor.fit(X_train, Y_train, epochs = 100, batch_size = 64, validation_split=0.05, shuffle=True,callbacks=[checkpoint,es])\nregressor.load_weights(\"model_1_.h5\")\n","execution_count":null,"outputs":[]},{"metadata":{"id":"iI9K_pj_uZgh","outputId":"7b85e40d-340e-4378-ab8c-d75811c8086e","trusted":true},"cell_type":"code","source":"test_data = df_test.drop(\n    ['ForecastId','County', 'Province_State', 'Country_Region'], axis=1)\ntest_data.set_index('Date', inplace=True)\ntest_confirm = test_data[test_data['Target'] == 'ConfirmedCases']\ntest_confirm = test_confirm.drop(['Target'], axis = 1)\n\nx_test = test_confirm.iloc[:, 0: 4].values\n(a,b) = x_test.shape\nx_test[:, 0:1] = sc_pop.fit_transform(x_test[:, 0:1])\n\nX_modify = np.zeros(shape = (a,b+1))\nX_modify[:,:-1] = x_test\nX_modify = X_modify.reshape(-1,45,3)\n\nX_t1 = X_modify[0,:,:]\nX_t2 = X[0,:,:]\nX_t3 = np.concatenate((X_t2,X_t1), axis = 0)\nst_index = number - time_step\nX_t3 = X_t3[st_index:160,:]\nX_test1, Y_d1 = multivariate_data(X_t3, X_t3[:,2], 0, time_step + 45, time_step)\nprint(X_test1.shape)\n\nfor i in range (1,3463) :\n\tx_t1 = X_modify[i,:,:]\n\tX_t2 = X[i,:,:]\n\tX_t3 = np.concatenate((X_t2,X_t1), axis = 0)\n\tX_t3 = X_t3[st_index:160,:]\n\tX_test_dummy, Y_d1 = multivariate_data(X_t3, X_t3[:,2], 0, time_step + 45, time_step)\n\tX_test1 = np.concatenate((X_test1, X_test_dummy), axis =0)\n\npredicted_test = regressor.predict(X_test1)\npredicted_test = sc_tg.inverse_transform(predicted_test)\nprint(predicted_test)","execution_count":null,"outputs":[]},{"metadata":{"id":"gpcJSpuQONhX","outputId":"6f795a07-b81f-47d4-ea77-7f7ad2e77ba4","trusted":true},"cell_type":"code","source":"pred_test_flat = predicted_test.flatten()\npred_test_flat = pred_test_flat.astype(int)","execution_count":null,"outputs":[]},{"metadata":{"id":"Jaibu4K6Fsa2","outputId":"b67c5eba-1936-4e40-e1bb-ad3f8f876cde","trusted":true},"cell_type":"code","source":"pred_test_flat.shape","execution_count":null,"outputs":[]},{"metadata":{"id":"rXqV91fYDInj","outputId":"4d5aa824-f690-46a1-9526-a449a9934c22","trusted":true},"cell_type":"code","source":"df = pd.DataFrame(pred_test_flat)\nmy_list = [*range(2,935011,6)]\ndf['Id_1'] = my_list\ndf.rename(columns = {0 : 'Predicted_Results'}, inplace = True)\ndf['Predicted_Results'] = np.where(df['Predicted_Results'] <0, 0, df['Predicted_Results'])\ndf","execution_count":null,"outputs":[]},{"metadata":{"id":"1a4tdGPjJdr0","outputId":"3c32c811-e294-414c-ef04-bbfef53335de","trusted":true},"cell_type":"code","source":"s1 = df['Predicted_Results']\nl1 = s1.tolist()\nl2 = [i*2 for i in l1]\nl1 = [i*0.95 for i in l2]\nl2 = [round(i) for i in l1]\ndf2 = pd.DataFrame(l2)\n\nmy_list = [*range(3,935011,6)]\ndf2['Id_1'] = my_list\ndf2.rename(columns = {0 : 'Predicted_Results'}, inplace = True)\n\ns1 = df['Predicted_Results']\nl1 = s1.tolist()\nl2 = [i*2 for i in l1]\nl1 = [i*0.05 for i in l2]\nl2 = [round(i) for i in l1]\ndf3 = pd.DataFrame(l2)\n\nmy_list = [*range(1,935011,6)]\ndf3['Id_1'] = my_list\ndf3.rename(columns = {0 : 'Predicted_Results'}, inplace = True)\n\nresult_confirmed_cases = pd.concat([df, df2, df3])\nresult_confirmed_cases.sort_values(by = ['Id_1'], inplace = True)\nresult_confirmed_cases","execution_count":null,"outputs":[]},{"metadata":{"id":"ZPx5IlOwO3V2","outputId":"179fc672-c5e1-479a-c853-1e17f53cb0b9","trusted":true},"cell_type":"code","source":"train_confirm1 = train_data[train_data['Target'] == 'Fatalities']\ntrain_confirm1 = train_confirm1.drop(['Target'], axis = 1)\ntrain_confirm1['TargetValue'] = np.where(train_confirm1['TargetValue'] <=0, 0, train_confirm1['TargetValue'])\n#print(train_confirm)\n\nX1 = train_confirm1.iloc[:, 0: 4].to_numpy()\n\n# MinMaxScaling\n\nX1[:, 0:1] = sc_pop.fit_transform(X1[:, 0:1])\nX1[:, 2:3] = sc_tg.fit_transform(X1[:, 2:3])\n\n\nX1 = X1.reshape(-1,number,3)\nprint(X1.shape)\n#print(df_train.dtypes)\n#print(X)\n\ndef multivariate_data(dataset, target, start_index, end_index, time_step) :\n\tdata=list()\n\tlabel =list()\n\n\tstart_index = start_index + time_step\n\tfor i in range(start_index, end_index) :\n\t\tindices = range(i-time_step, i)\n\t\tdata.append(dataset[indices])\n\t\tlabel.append(target[i])\n\n\treturn np.array(data), np.array(label)\n\nX_train1, Y_train1 = multivariate_data(X1[0,:,:], X1[0,:,2], 0, number, time_step)\n\nfor i in range(1,3463) :\n\tX_dummy, Y_dummy = multivariate_data(X1[i,:,:], X1[i,:,2], 0, number, time_step)\n\tX_train1 = np.concatenate((X_train1, X_dummy), axis = 0)\n\tY_train1 = np.concatenate((Y_train1, Y_dummy), axis = 0)\n\nprint(X_train1.shape)\nprint(Y_train1.shape)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del regr1,regressor, checkpoint, es, ReduceLROnPlateau","execution_count":null,"outputs":[]},{"metadata":{"id":"-gCVzZrVQPbG","outputId":"0c0fb13a-02e2-4d89-8081-0feb18ba5765","trusted":true},"cell_type":"code","source":"def regr2():\n        \n    inp = Input(shape = (X_train.shape[1], 3))\n    x = tf.keras.layers.Bidirectional(LSTM(units = 32, return_sequences = True,input_shape = (X_train.shape[1], 3)))(inp)\n    x = Activation(mish)(x)\n    x = Dropout(0.3)(x)\n    x = tf.keras.layers.Bidirectional(LSTM(units = 32, return_sequences = True))(x)\n    x = Activation(mish)(x)\n    x = Dropout(0.3)(x)\n    x = tf.keras.layers.Bidirectional(GRU(units = 32))(x)\n    x = Activation(mish)(x)\n    x = Dropout(0.3)(x)\n    out = Dense(1, activation = swish, name = 'out')(x)\n    \n    model = models.Model(inputs = inp, outputs = out)\n    \n    #opt1 = tf.keras.optimizers.RMSprop(lr=0.0001, rho=0.9, epsilon=1e-08, decay=0.0)#'RMSprop'\n    opt = 'RMSprop'\n    #opt2 = tfa.optimizers.AdamW(lr=0.001,weight_decay=5.5e-6)\n    #opt = tfa.optimizers.Lookahead(tfa.optimizers.AdamW(lr=0.001,weight_decay=0.006), sync_period=5, slow_step_size=0.5)\n    #opt = tfa.optimizers.Lookahead(opt1, sync_period=5, slow_step_size=0.5)\n    #opt = tfa.optimizers.SWA(opt)\n\n    model.compile(optimizer = opt, loss = 'mean_squared_error')\n    return model\n\nregressor1 = regr2()\nregressor1.summary()\ntf.keras.utils.plot_model(\n    regressor1, to_file='model2.png', show_shapes=True, show_layer_names=True,\n    rankdir='TB', expand_nested=True, dpi=96\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"checkpoint = tf.keras.callbacks.ModelCheckpoint(\"model_2_.h5\".format(i), monitor='val_loss', verbose=1, save_best_only=True,save_weights_only=True, mode='min')\nes = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, verbose=1, mode='min', restore_best_weights=True)\nReduceLROnPlateau = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss',mode='min', patience=2, verbose=1, factor=0.5, min_lr=0.00001)\nregressor1.fit(X_train, Y_train, epochs = 100, batch_size = 64, validation_split=0.05, shuffle=True,callbacks=[checkpoint,es])\nregressor1.load_weights(\"model_2_.h5\")\n","execution_count":null,"outputs":[]},{"metadata":{"id":"kbO5iDXgQdaI","outputId":"4e7ea164-b52b-4a5b-dd27-95437acb6d5f","trusted":true},"cell_type":"code","source":"test_data = df_test.drop(\n    ['ForecastId','County', 'Province_State', 'Country_Region'], axis=1)\ntest_data.set_index('Date', inplace=True)\ntest_confirm = test_data[test_data['Target'] == 'Fatalities']\ntest_confirm = test_confirm.drop(['Target'], axis = 1)\n\nx_test = test_confirm.iloc[:, 0: 4].values\n(a,b) = x_test.shape\nx_test[:, 0:1] = sc_pop.fit_transform(x_test[:, 0:1])\n\nX_modify = np.zeros(shape = (a,b+1))\nX_modify[:,:-1] = x_test\nX_modify = X_modify.reshape(-1,45,3)\n\nX_t1 = X_modify[0,:,:]\nX_t2 = X[0,:,:]\nX_t3 = np.concatenate((X_t2,X_t1), axis = 0)\nst_index = number - time_step\nX_t3 = X_t3[st_index:160,:]\nX_test1, Y_d1 = multivariate_data(X_t3, X_t3[:,2], 0, time_step + 45, time_step)\n\nfor i in range (1,3463) :\n\tx_t1 = X_modify[i,:,:]\n\tX_t2 = X[i,:,:]\n\tX_t3 = np.concatenate((X_t2,X_t1), axis = 0)\n\tX_t3 = X_t3[st_index:160,:]\n\tX_test_dummy, Y_d1 = multivariate_data(X_t3, X_t3[:,2], 0, time_step + 45, time_step)\n\tX_test1 = np.concatenate((X_test1, X_test_dummy), axis =0)\n\npredicted_test = regressor1.predict(X_test1)\npredicted_test = sc_tg.inverse_transform(predicted_test)\nprint(predicted_test)","execution_count":null,"outputs":[]},{"metadata":{"id":"vsLtn9BZRWHt","outputId":"7915b827-ebb3-43e5-b959-a1a252971860","trusted":true},"cell_type":"code","source":"pred_test_flat = predicted_test.flatten()\npred_test_flat = pred_test_flat.astype(int)\ndf4 = pd.DataFrame(pred_test_flat)\nmy_list = [*range(5,935011,6)]\ndf4['Id_1'] = my_list\ndf4.rename(columns = {0 : 'Predicted_Results'}, inplace = True)\ndf4['Predicted_Results'] = np.where(df4['Predicted_Results'] <0, 0, df4['Predicted_Results'])\ndf4","execution_count":null,"outputs":[]},{"metadata":{"id":"0C5a7e69R-w1","outputId":"243dfb18-e455-416e-c303-e06aa9cc2ad8","trusted":true},"cell_type":"code","source":"s1 = df4['Predicted_Results']\nl1 = s1.tolist()\nl2 = [i*2 for i in l1]\nl1 = [i*0.95 for i in l2]\nl2 = [round(i) for i in l1]\ndf5 = pd.DataFrame(l2)\n\nmy_list = [*range(6,935011,6)]\ndf5['Id_1'] = my_list\ndf5.rename(columns = {0 : 'Predicted_Results'}, inplace = True)\n\ns1 = df4['Predicted_Results']\nl1 = s1.tolist()\nl2 = [i*2 for i in l1]\nl1 = [i*0.05 for i in l2]\nl2 = [round(i) for i in l1]\ndf6 = pd.DataFrame(l2)\n\nmy_list = [*range(4,935011,6)]\ndf6['Id_1'] = my_list\ndf6.rename(columns = {0 : 'Predicted_Results'}, inplace = True)\n\nresult_fatalities = pd.concat([df4, df5, df6])\nresult_fatalities.sort_values(by = ['Id_1'], inplace = True)\nresult_fatalities","execution_count":null,"outputs":[]},{"metadata":{"id":"Zg1TyfvZTHQG","outputId":"ca21c143-5456-49b0-d15e-f8c26b95718a","trusted":true},"cell_type":"code","source":"result_total = pd.concat([result_confirmed_cases, result_fatalities])\nresult_total.sort_values(by = ['Id_1'], inplace = True)\nresult_total","execution_count":null,"outputs":[]},{"metadata":{"id":"yLU5x6FKTioo","outputId":"53ead31f-55d6-43d7-a4e0-a2d5faa7a932","trusted":true},"cell_type":"code","source":"df_submission = pd.read_csv('../input/covid19-global-forecasting-week-5/submission.csv')\nmy_list = [*range(1,935011)]\ndf_submission['Id'] = my_list \nfinal_result = pd.merge(df_submission,result_total, left_on = 'Id', right_on ='Id_1', how = 'inner')\nfinal_result.drop(['TargetValue','Id','Id_1'], axis =1, inplace = True)\nfinal_result.rename({'Predicted_Results' : 'TargetValue'}, axis = 1, inplace = True)\nfinal_result","execution_count":null,"outputs":[]},{"metadata":{"id":"5rtEWSTTX4m_","trusted":true},"cell_type":"code","source":"final_result.to_csv('submission.csv', index = False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}