{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"\nimport pandas as pd\nfrom sklearn import preprocessing\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom sklearn.ensemble import GradientBoostingRegressor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"path = \"/kaggle/input/covid19-global-forecasting-week-5/\"\ncpc = ['County','Province_State','Country_Region']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#Load the data\ndftrain = pd.read_csv(\"/kaggle/input/covid19-global-forecasting-week-5/train.csv\")\ndftest = pd.read_csv(\"/kaggle/input/covid19-global-forecasting-week-5/test.csv\")\ndfsub = pd.read_csv(\"/kaggle/input/covid19-global-forecasting-week-5/submission.csv\")\ndftr = pd.read_csv(path+\"train.csv\" , parse_dates = ['Date'])\ndftt = pd.read_csv(path+\"test.csv\" , parse_dates = ['Date'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trmindate = dftr['Date'].min()\ndftr['ndays']= (dftr['Date'] - trmindate).dt.days\ndftt['ndays'] = (dftt['Date'] - trmindate).dt.days","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#dftr[(dftr['Date'] == \"2020-04-27\") & (dftr['Country_Region'] == 'Afghanistan')]\n# df_train['Date'] = pd.to_datetime(df_train['Date'], infer_datetime_format=True)\n# df_test['Date'] = pd.to_datetime(df_test['Date'], infer_datetime_format=True)\n# df_train['Date'] = df_train['Date'].apply(lambda s: time.mktime(s.timetuple()))\n# df_test['Date'] = df_test['Date'].apply(lambda s: time.mktime(s.timetuple()))\n# min_timestamp = np.min(df_train['Date'])\n# df_train['Date'] = df_train['Date'].apply(lambda s: (s - min_timestamp) / 86400.0)\n# df_test['Date'] = df_test['Date'].apply(lambda s: (s - min_timestamp) / 86400.0)\n# df_train.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dftr[dftr['Country_Region'] == 'Afghanistan']['Weight'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dftr.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dftr[cpc] = dftr[cpc].fillna('')\ndftt[cpc] = dftt[cpc].fillna('')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dftt.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dftr['Date'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dftr['Loc'] = dftr['Country_Region'] + ' ' + dftr['Province_State'] + ' ' +  dftr['County']\ndftr['Loc'] = dftr['Loc'].str.strip()\ndftr['Loc'].value_counts()\ndftt['Loc'] = dftt['Country_Region'] + ' ' + dftt['Province_State'] + ' ' +  dftt['County']\ndftt['Loc'] = dftt['Loc'].str.strip()\ndftt['Loc'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# dftr['Datei'] = dftr.Date.dt.strftime(\"%m%d\")\n# dftr['Datei'] = dftr['Datei'].astype(int)\n# dftt['Datei'] = dftt.Date.dt.strftime(\"%m%d\")\n# dftt['Datei'] = dftt['Datei'].astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dftr.shape, dftt.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_columns = ['Id','Loc','Population','Weight','Date','ndays','Target','TargetValue']\ntest_columns = ['ForecastId','Loc','Population','Weight','Date','ndays','Target']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dftr1= dftr[train_columns]\ndftt1 = dftt[test_columns]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"le = preprocessing.LabelEncoder()\ndftr1.Loc = le.fit_transform(dftr1.Loc)\ndftt1.Loc = le.fit_transform(dftt1.Loc)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dftrc = dftr1[dftr1.Target == 'ConfirmedCases']\ndftrc = dftrc.drop('Target', axis=1)\ndftrc = dftrc.rename(columns={'TargetValue':'ConfirmedCases'})\n\ndftrf = dftr1[dftr1.Target == 'Fatalities']\ndftrf = dftrf.drop('Target',axis=1)\ndftrf = dftrf.rename(columns={'TargetValue':'Fatalities'})\n\ndfttc = dftt1[dftt1.Target == 'ConfirmedCases']\ndfttc = dfttc.drop('Target', axis=1)\n\ndfttf = dftt1[dftt1.Target == 'Fatalities']\ndfttf = dfttf.drop('Target',axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Xtrainc = dftrc[['Loc','Population','Weight','ndays']]\nyc = dftrc.ConfirmedCases\nXtrainf = dftrf[['Loc','Population','Weight','ndays']]\nyf = dftrf.Fatalities","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"XttFCid = dfttc['ForecastId']\nXttFFid = dfttf['ForecastId']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"XttC = dfttc[['Loc','Population','Weight','ndays']]\nXttF = dfttf[['Loc','Population','Weight','ndays']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xoutc = pd.DataFrame({'ForecastId': [], 'ConfirmedCases': []})\nxoutf = pd.DataFrame({'ForecastId': [], 'Fatalities': []})\nl_alpha = 0.05\nm_alpha = 0.50\nu_alpha = 0.95","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"l_modelc = GradientBoostingRegressor(loss=\"quantile\", alpha=l_alpha,max_depth = 6,\n                                     learning_rate=0.2, random_state=47,n_estimators=100)\nm_modelc = GradientBoostingRegressor(loss=\"quantile\", alpha=m_alpha, max_depth = 6,\n                                     learning_rate=0.2, random_state=47,n_estimators=100)\nu_modelc = GradientBoostingRegressor(loss=\"quantile\", alpha=u_alpha,max_depth = 6, \n                                     learning_rate=0.2, random_state=47,n_estimators=100)\n        \nl_modelc.fit(Xtrainc, yc)\nm_modelc.fit(Xtrainc, yc)\nu_modelc.fit(Xtrainc, yc)\n\nl_predict = l_modelc.predict(XttC)\nm_predict = m_modelc.predict(XttC)\nu_predict = u_modelc.predict(XttC)\nl_data = pd.DataFrame({'ForecastId': [str(i)+'_0.05' for i in XttFCid.values.tolist()], 'ConfirmedCases': l_predict})\nl_data['ConfirmedCases'] = 1.0\nxoutc = xoutc.append(l_data)\nm_data = pd.DataFrame({'ForecastId': [str(i)+'_0.5' for i in XttFCid.values.tolist()], 'ConfirmedCases': m_predict})\nxoutc = xoutc.append(m_data)\nu_data = pd.DataFrame({'ForecastId': [str(i)+'_0.95' for i in XttFCid.values.tolist()], 'ConfirmedCases': u_predict})\nxoutc = xoutc.append(u_data)\n\nxoutc.reset_index(inplace=True)\nxoutc = xoutc.rename(columns={'ConfirmedCases':'PValue'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#l_data.head()\n#u_data['ConfirmedCases'].value_counts()\n# cols = ['T1','T2','T3','T4']\n# df[df[cols] < 0] = -5\n#l_data[l_data['ConfirmedCases'] == 0.0] =1.0\n#l_data['ConfirmedCases'] = 1.0\nl_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"l_modelf = GradientBoostingRegressor(loss=\"quantile\", alpha=l_alpha,max_depth = 6,\n                                     learning_rate=0.1, random_state=47,n_estimators=100)\nm_modelf = GradientBoostingRegressor(loss=\"quantile\", alpha=m_alpha, max_depth = 6,\n                                     learning_rate=0.1, random_state=47,n_estimators=100)\nu_modelf = GradientBoostingRegressor(loss=\"quantile\", alpha=u_alpha, max_depth = 6,\n                                     learning_rate=0.1, random_state=47,n_estimators=100)\nl_modelf.fit(Xtrainf, yf)\nm_modelf.fit(Xtrainf, yf)\nu_modelf.fit(Xtrainf, yf)\n\nl_predictf = l_modelf.predict(XttF)\nm_predictf = m_modelf.predict(XttF)\nu_predictf = u_modelf.predict(XttF)\nl_dataf = pd.DataFrame({'ForecastId': [str(i)+'_0.05' for i in XttFFid.values.tolist()], 'Fatalities': l_predictf})\nnum = l_dataf._get_numeric_data()\nnum[num<0] = 0\nxoutf = xoutf.append(l_dataf)\nm_dataf = pd.DataFrame({'ForecastId': [str(i)+'_0.5' for i in XttFFid.values.tolist()], 'Fatalities': m_predictf})\nxoutf = xoutf.append(m_dataf)\nu_dataf = pd.DataFrame({'ForecastId': [str(i)+'_0.95' for i in XttFFid.values.tolist()], 'Fatalities': u_predictf})\nxoutf = xoutf.append(u_dataf)\n\nxoutf.reset_index(inplace=True)\nxoutf = xoutf.rename(columns={'Fatalities':'PValue'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"l_dataf['Fatalities'].value_counts()\n#df.loc[df.Weight == \"155\", \"Name\"] = \"John\"\n#num = l_dataf._get_numeric_data()\n#num[num<0] = 0\n#xoutf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dfs = pd.concat([xoutc, xoutf], ignore_index=True)\ndfs = dfs.drop('index', axis=1)\ndfs = dfs.rename(columns={'ForecastId':'ForecastId_Quantile'})\ndfsss = dfsub.merge(dfs, on='ForecastId_Quantile', how='inner')\ndfsss = dfsss.drop(\"TargetValue\", axis=1)\ndfsss = dfsss.rename(columns = ({'PValue':'TargetValue'}))\ndfsss['TargetValue'] = dfsss['TargetValue'].astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submit =dfsss.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}