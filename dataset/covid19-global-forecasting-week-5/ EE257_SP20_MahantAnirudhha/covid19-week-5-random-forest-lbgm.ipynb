{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import preprocessing\nimport time\nfrom datetime import datetime\nfrom scipy import integrate, optimize\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# ML libraries\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom xgboost import plot_importance, plot_tree\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom sklearn import linear_model\nfrom sklearn.preprocessing import scale\nimport sklearn.linear_model as skl_lm\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nfrom lightgbm import LGBMRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom pandas import DataFrame\n\n\n#Libraries to import\n\nimport datetime as dt\nimport requests\nimport sys\nfrom itertools import chain\nimport plotly_express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn import metrics\nimport xgboost as xgb\nfrom xgboost import XGBRegressor\nfrom xgboost import plot_importance, plot_tree\nfrom sklearn.model_selection import GridSearchCV","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ntest = pd.read_csv(\"../input/covid19-global-forecasting-week-5/test.csv\")\ntrain = pd.read_csv(\"../input/covid19-global-forecasting-week-5/train.csv\")\n\ndisplay(train.head())\ndisplay(train.describe())\ntrain.info()\n\nprint(\"Number of Country_Region: \", train['Country_Region'].nunique())\nprint(\"Dates go from day\", max(train['Date']), \"to day\", min(train['Date']), \", a total of\", train['Date'].nunique(), \"days\")\nprint(\"Countries with Province/State informed: \", train.loc[train['Province_State']!='None']['Country_Region'].unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_date_min = train['Date'].min()\ntrain_date_max = train['Date'].max()\nprint('Minimum date from training set: {}'.format(train_date_min))\nprint('Maximum date from training set: {}'.format(train_date_max))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_date_min = test['Date'].min()\ntest_date_max = test['Date'].max()\nprint('Minimum date from test set: {}'.format(test_date_min))\nprint('Maximum date from test set: {}'.format(test_date_max))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ** **Visualization"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.pie(train, values='TargetValue', names='Target')\nfig.update_traces(textposition='inside')\nfig.update_layout(uniformtext_minsize=12, uniformtext_mode='hide')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.pie(train, values='TargetValue', names='Country_Region')\nfig.update_traces(textposition='inside')\nfig.update_layout(uniformtext_minsize=12, uniformtext_mode='hide')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.pie(train, values='Population', names='Country_Region')\nfig.update_traces(textposition='inside')\nfig.update_layout(uniformtext_minsize=12, uniformtext_mode='hide')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> **Feature Selection**"},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_matrix = train.corr()     #computing correlation between features and output\nprint(corr_matrix)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Using Pearson Correlation\nplt.figure(figsize=(12,10))\ncor = train.corr()\nsns.heatmap(cor, annot=True, cmap=plt.cm.Reds)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"Province_State\"] = train.Province_State.fillna(0)\ntest[\"Province_State\"] = test.Province_State.fillna(0)\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"total = len(train[\"Country_Region\"])\nprint(total)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(0,total):\n    if(train.Province_State[i] == 0):\n        train.Province_State[i] = train.Country_Region[i]\ntrain.head()\n        \n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"total_test = len(test[\"Province_State\"])\nfor i in range(0,total_test):\n    if(test.Province_State[i] == 0):\n        test.Province_State[i] = test.Country_Region[i]\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"t2\"] = train.Target.factorize()[0]\ntest[\"t2\"] = test.Target.factorize()[0]\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> **Modeling**"},{"metadata":{"trusted":true},"cell_type":"code","source":"label = preprocessing.LabelEncoder()\ntrain.Country_Region = label.fit_transform(train.Country_Region)\ntrain.Province_State = label.fit_transform(train.Province_State)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.Country_Region = label.fit_transform(test.Country_Region)\ntest.Province_State = label.fit_transform(test.Province_State)\ntest.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import OrdinalEncoder\n\ndef create_features(df):\n    df['day'] = df['Date'].dt.day\n    df['month'] = df['Date'].dt.month\n    df['dayofweek'] = df['Date'].dt.dayofweek\n    df['dayofyear'] = df['Date'].dt.dayofyear\n    df['quarter'] = df['Date'].dt.quarter\n    df['weekofyear'] = df['Date'].dt.weekofyear\n    return df\n\ndef train_dev_split(df, days):\n    #Last days data as dev set\n    date = df['Date'].max() - dt.timedelta(days=days)\n    return df[df['Date'] <= date], df[df['Date'] > date]\n\ntest_date_min = test['Date'].min()\ntest_date_max = test['Date'].max()\n\ndef avoid_data_leakage(df, date=test_date_min):\n    return df[df['Date']<date]\n\ndef to_integer(dt_time):\n    return 10000*dt_time.year + 100*dt_time.month + dt_time.day\n\ntrain['Date']=pd.to_datetime(train['Date'])\ntest['Date']=pd.to_datetime(test['Date'])\n\ntest['Date']=test['Date'].dt.strftime(\"%m%d\").astype(int)\ntrain['Date']=train['Date'].dt.strftime(\"%m%d\").astype(int)\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = train.drop(['Id','County','Target', 'TargetValue'],axis=1)\nx_test = test.drop(['ForecastId','County','Target'],axis=1)\ny = train[\"TargetValue\"]\nx.head ()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nlin_reg=LinearRegression()\nlin_reg.fit(x,y)\nprint(lin_reg.intercept_)\nprint(lin_reg.coef_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(train, x_vars=['Population','Weight','Date'], y_vars='TargetValue', size=7, aspect=0.7, kind='reg')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import PolynomialFeatures\npoly_reg2=PolynomialFeatures(degree=2)\nX_poly=poly_reg2.fit_transform(x)\nlin_reg_2=LinearRegression()\nlin_reg_2.fit(X_poly,y)\n\nprint(\"Coefficients of polynimial(degree2) are\", lin_reg_2.coef_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"poly_reg3=PolynomialFeatures(degree=3)\nX_poly3=poly_reg3.fit_transform(x)\nlin_reg_3=LinearRegression()\nlin_reg_3.fit(X_poly3,y)\nprint(\"Coefficients of polynimial(degree3) are\", lin_reg_3.coef_)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Random Forest Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor \nmodel = RandomForestRegressor(n_jobs=-1)\nestimators = [10,50,100]\nscores = []\nfor n in estimators:\n    model.set_params(n_estimators=n)\n    model.fit(x,y)\n    scores.append(model.score(x,y))\nplt.title(\"Effect of n_estimators\")\nplt.xlabel(\"n_estimator\")\nplt.ylabel(\"score\")\nplt.plot(estimators, scores)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So Random Forest with n=100 has 96% accuracy which is the best estimator. "},{"metadata":{"trusted":true},"cell_type":"code","source":"main_model=RandomForestRegressor(n_estimators=100, n_jobs=-1)\nmain_model.fit(x,y)\ny_pred = main_model.predict(x)\ny_pred = np.round(y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mae = mean_squared_error(y_pred,y)\nprint(\"The mean absolute error is =\", mae, \"Training Error\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lbg = LGBMRegressor(n_estimators = 5000, learning_rate = 1.1,  random_state = 42 , max_depth = 18)\nlbg.fit(x,y)\nyp = lbg.predict(x)\nyp = np.round(yp)\nm = mean_squared_error(yp,y)\nprint(\"The mean absolute error is =\", m, \"Training Error\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"output = lbg.predict(x_test)\noutput = np.round(output)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.DataFrame()\nf = test[\"ForecastId\"]\nf = f.astype(int)\ndf.insert(0,\"Id\",f,False)\ndf.insert(1,\"TargetValue\",output,False)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"q=df.groupby(['Id'])['TargetValue'].quantile(q=0.05).reset_index()\nw=df.groupby(['Id'])['TargetValue'].quantile(q=0.5).reset_index()\ne=df.groupby(['Id'])['TargetValue'].quantile(q=0.95).reset_index()\n\nq.columns=['Id','q0.05']\nw.columns=['Id','q0.5']\ne.columns=['Id','q0.95']\nq=pd.concat([q,w['q0.5'],e['q0.95']],1)\nq['q0.05']=q['q0.05'].clip(0,10000)\nq['q0.5']=q['q0.5'].clip(0,10000)\nq['q0.95']=q['q0.95'].clip(0,10000)\nq['Id'] =q['Id']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub=pd.melt(q, id_vars=['Id'], value_vars=['q0.05','q0.5','q0.95'])\nsub['variable']=sub['variable'].str.replace(\"q\",\"\", regex=False)\nsub['ForecastId_Quantile']=sub['Id'].astype(str)+'_'+sub['variable']\nsub['TargetValue']=sub['value']\nsub=sub[['ForecastId_Quantile','TargetValue']]\nsub.reset_index(drop=True,inplace=True)\nsub.to_csv(\"submission.csv\",index=False)\nsub.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}