{"cells":[{"metadata":{"pycharm":{"is_executing":false},"trusted":true},"cell_type":"code","source":"%matplotlib inline\nimport pandas as pd\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"\npd.set_option('display.max_columns', 99)\npd.set_option('display.max_rows', 99)\nimport os\nimport numpy as np\nfrom matplotlib import pyplot as plt\nfrom tqdm import tqdm\nimport plotly.express as px\nfrom sklearn import metrics\nimport datetime as dt\nfrom functools import partial\nimport lightgbm as lgb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start_time = dt.datetime.now()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"VERSION = 11\nCOMP_DIR = '../input/covid19-global-forecasting-week-5/'\nGEO_PATH = '../input/covid19belugaw5/geo.csv'\nCLOSEST_PATH = '../input/covid19belugaw5/closest.csv'\n\n\nWORK_DIR = '/kaggle/working/'\nLOG_FILE = f'{WORK_DIR}paropt_lgb_v{VERSION}.csv'\nFIMP_FILE = f'{WORK_DIR}fimp_lgb_v{VERSION}.csv'\nFEATURE_FILE_PATH = f'{WORK_DIR}features_v{VERSION}.csv'\nPREDS_PATH = f'{WORK_DIR}predictions_v{VERSION}/'\nif not os.path.exists(PREDS_PATH):\n    os.makedirs(PREDS_PATH)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.listdir(WORK_DIR)\nos.listdir(COMP_DIR)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TRAIN_START = '2020-03-31'\nTRAIN_END = '2020-05-11'\nCV = 5\nRELOAD_FEATURES = True\nPRECISION = 2\n\n\ndef w5_loss(preds, data, q=0.5):\n    y_true = data.get_label()\n    weights = data.get_weight()\n\n    diff = (y_true - preds) * weights\n    gt_is_higher = np.sum(diff[diff >= 0] * q)\n    gt_is_lower = np.sum(- diff[diff < 0] * (1 - q))\n\n    return 'w5', (gt_is_higher + gt_is_lower) / len(preds), False","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def process_train():\n    train = pd.read_csv(COMP_DIR + 'train.csv')\n    train = train.fillna('')\n    train['Location'] = train.Country_Region + '-' + train.Province_State + '-' + train.County\n    train = train.drop(columns=['Id', 'Country_Region', 'Province_State', 'County'])\n\n    pop = train.groupby('Location')[['Population', 'Weight']].mean().reset_index()\n    pop['cv'] = np.random.randint(0, CV, len(pop))\n\n    confirmed = train.loc[train.Target == 'ConfirmedCases',\n                          ['Location', 'Date', 'TargetValue']].copy()\n    confirmed.columns = ['Location', 'Date', 'Confirmed']\n    fatalities = train.loc[train.Target == 'Fatalities',\n                           ['Location', 'Date', 'TargetValue']].copy()\n    fatalities.columns = ['Location', 'Date', 'Fatalities']\n    targets = pd.merge(confirmed, fatalities, on=['Location', 'Date'])\n    targets = targets.merge(pop, on='Location')\n\n    targets['DateTime'] = pd.to_datetime(targets.Date)\n    targets['DaysTillEnd'] = (targets.DateTime.max() - targets.DateTime).dt.days + 1\n    targets['DayOfWeek'] = targets.DateTime.dt.weekday\n    targets['USCounty'] = 1 * (~targets.Location.str.endswith('-'))\n\n    targets.Confirmed = targets.Confirmed.clip(0, None)\n    targets.Fatalities = targets.Fatalities.clip(0, None)\n    print(targets.max())\n    return targets","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def extract_timeseries_features(single_location):\n    df = single_location.copy()\n    df = df.sort_values(by='Date')\n    for target in ['Confirmed', 'Fatalities']:\n        df[f'{target}CumSum'] = df[target].cumsum()\n        for dow in range(7):\n            df[f'{target}DOW{dow}'] = ((df.DayOfWeek == dow) * df[target]).cumsum() / (df[target].cumsum() + 1)\n        for k in [3, 7, 14, 21]:\n            df[f'{target}RollingMean{k}'] = df[target].rolling(k).mean()\n            df[f'{target}RollingMean{k}PerK'] = df[target].rolling(k).mean() / df.Population * 10000\n        df[f'{target}RollingStd21'] = df[target].rolling(21).std().round(0)\n        df[f'{target}DaysSince10'] = (df[f'{target}CumSum'] > 10).cumsum()\n        df[f'{target}DaysSince100'] = (df[f'{target}CumSum'] > 100).cumsum()\n\n        df[f'{target}RollingMeanDiff2w'] = df[f'{target}RollingMean7'] / (df[f'{target}RollingMean14'] + 1) - 1\n        df[f'{target}RollingMeanDiff3w'] = df[f'{target}RollingMean7'] / (df[f'{target}RollingMean21'] + 1) - 1\n\n    df['DeathRate'] = 100 * df.FatalitiesCumSum.clip(0, None) / (df.ConfirmedCumSum.clip(0, None) + 1)\n    df['DeathRateRolling3w'] = 100 * df.FatalitiesRollingMean7.clip(0, None) / (\n            df.ConfirmedRollingMean21.clip(0, None) + 1)\n    df['ConfirmedPerK'] = 1000 * df.ConfirmedCumSum.clip(0, None) / df.Population\n    df['FatalitiesPerK'] = 1000 * df.FatalitiesCumSum.clip(0, None) / df.Population\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_nearby_features(features, rank):\n    closest = pd.read_csv(CLOSEST_PATH)\n\n    to_aggregate = ['ConfirmedCumSum',\n                    'ConfirmedRollingMean21',\n                    'ConfirmedRollingMean14',\n                    'ConfirmedRollingMean7',\n\n                    'FatalitiesCumSum',\n                    'FatalitiesRollingMean21',\n                    'FatalitiesRollingMean14',\n                    'FatalitiesRollingMean7']\n\n    subset = features[['Date', 'Location', 'Population'] + to_aggregate].copy()\n    subset = subset.rename(columns={'Location': 'Location_1'})\n\n    nearby = features[['Date', 'Location']].merge(closest[closest.Rank <= rank], on='Location')\n    nearby = nearby.merge(subset, on=['Date', 'Location_1'])\n\n    nearby_sum = nearby.groupby(['Date', 'Location']).sum()\n    nearby_mean = nearby.groupby(['Date', 'Location'])[['distance']].mean().round(0)\n    for c in to_aggregate:\n        nearby_sum[f'Nearby{rank}{c}'] = 1000 * nearby_sum[c] / nearby_sum['Population']\n\n    nearby_features = pd.merge(nearby_sum, nearby_mean, on=['Date', 'Location'])\n    nearby_features = nearby_features.rename(columns={'distance_y': f'Nearby{rank}Distance'})\n    return nearby_features[[f for f in nearby_features.columns if f.startswith('Nearby')]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"targets = process_train()\n\nif os.path.exists(FEATURE_FILE_PATH) and RELOAD_FEATURES:\n    features = pd.read_csv(FEATURE_FILE_PATH)\nelse:\n    features = []\n    for loc, df in tqdm(targets.groupby('Location')):\n        df = extract_timeseries_features(df)\n        features.append(df)\n    features = pd.concat(features)\n\n    geo = pd.read_csv(GEO_PATH)\n    features = features.merge(geo, on='Location')\n    for rank in [5, 10, 20]:\n        nearby_features = get_nearby_features(features, rank)\n        features = features.merge(nearby_features, on=['Date', 'Location'])\n\n    to_log = ['ConfirmedCumSum', 'Population', 'FatalitiesCumSum']\n    for c in to_log:\n        features.loc[:, c] = np.log(features[c].values + 1).round(2)\n\n    features.loc[:, 'DeathRate'] = features.loc[:, 'DeathRate'].clip(0, 50)\n    features.loc[:, 'DeathRateRolling3w'] = features.loc[:, 'DeathRateRolling3w'].clip(0, 50)\n\n    round_1_digit = [\n        'ConfirmedRollingMean21', 'ConfirmedRollingMean14', 'ConfirmedRollingMean7', 'ConfirmedRollingMean3',\n    ]\n    for c in round_1_digit:\n        features.loc[:, c] = features[c].round(1)\n\n    features = features.round(PRECISION)\n    features.to_csv(FEATURE_FILE_PATH, index=False)\n\nfeatures = features[features.Location != 'US--']\n# Remove Public LB Future\nfeatures = features[features.Date < TRAIN_END]\nprint(f'Features: {features.shape}')\nprint(f'Features: {features.count()}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def apply_lgb(features, target, q, params, k, num_round=1000):\n    features['TARGET'] = features.groupby('Location')[target].shift(-k)\n    do_not_use = [\n                     'Location', 'Date', 'TARGET', 'Weight', 'cv', 'DaysTillEnd', 'DateTime'\n                 ] + ['Confirmed', 'Fatalities']\n    feature_names = [f for f in features.columns if f not in do_not_use]\n\n    print(features.columns)\n    print(len(feature_names), feature_names)\n\n    train = features.loc[(~features.TARGET.isna()) & (features.Date > TRAIN_START)]\n    test = features.loc[(features.TARGET.isna()) & (features.Date > TRAIN_START)]\n    print(train.shape, test.shape)\n\n    test.loc[:, 'PREDICTION'] = 0\n    train.loc[:, 'PREDICTION'] = 0\n    feature_importances = []\n    for cv in range(CV):\n        tr = train[train.cv != cv]\n        val = train[train.cv == cv]\n\n        train_set = lgb.Dataset(tr[feature_names], label=tr.TARGET, weight=tr.Weight / tr.DaysTillEnd ** 0.2)\n        valid_set = lgb.Dataset(val[feature_names], label=val.TARGET, weight=val.Weight / val.DaysTillEnd ** 0.2)\n\n        model = lgb.train(params, train_set, num_round, valid_sets=[train_set, valid_set],\n                          early_stopping_rounds=50, feval=partial(w5_loss, q=q))\n\n        train.loc[train.cv == cv, 'PREDICTION'] = model.predict(val[feature_names])\n        test.loc[test.cv == cv, 'PREDICTION'] = model.predict(test.loc[test.cv == cv, feature_names])\n\n        fimp = pd.DataFrame({'f': feature_names, 'imp': model.feature_importance()})\n        feature_importances.append(fimp)\n\n    _, error, _ = w5_loss(\n        train.PREDICTION,\n        lgb.Dataset(train[feature_names], label=train.TARGET, weight=train.Weight / train.DaysTillEnd ** 0.2),\n        q\n    )\n    feature_importances = pd.concat(feature_importances)\n    feature_importances = feature_importances.groupby('f').sum().reset_index().sort_values(by='imp', ascending=False)\n    feature_importances['target'] = target\n    feature_importances['k'] = k\n    feature_importances['q'] = q\n\n    train_preds = train[['Date', 'Location', 'PREDICTION']]\n    test_preds = test[['Date', 'Location', 'PREDICTION']]\n    test_preds['target'] = target\n    test_preds['k'] = k\n    test_preds['q'] = q\n    return error, train_preds, test_preds, feature_importances","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"for i in range(1):\n    for target in ['Confirmed', 'Fatalities']:\n        for k in range(1, 15):\n            for q in [0.05, 0.5, 0.95]:\n                params = dict(\n                    objective='quantile',\n                    alpha=q,\n                    metric='custom',\n                    max_depth=np.random.choice([6, 8, 10, 15, 20]),\n                    learning_rate=np.random.choice([0.025, 0.05, 0.1]),\n                    feature_fraction=np.random.choice([0.5, 0.6, 0.7, 0.8]),\n                    bagging_freq=np.random.choice([2, 3, 5]),\n                    bagging_fraction=np.random.choice([0.7, 0.8]),\n                    min_data_in_leaf=np.random.choice([5, 10]),\n                    num_leaves=np.random.choice([127, 255]),\n                    verbosity=0,\n                    n_jobs=4\n                )\n\n                num_round = 10\n\n                start = dt.datetime.now()\n                error, train_preds, test_preds, feature_importances = apply_lgb(\n                    features, target, q, params, k, num_round=num_round)\n                print(error)\n                print(train_preds.shape, test_preds.shape)\n\n                preds_file_name = PREDS_PATH + f'preds_{target}_{q}_{k}_{error.round(5)}.csv'\n                test_preds.to_csv(preds_file_name, index=False)\n\n                end = dt.datetime.now()\n                print('Finished', end, (end - start).seconds, 's')\n\n                result = [\n                             target, q, k, error, (end - start).seconds\n                         ] + list(params.values())\n                columns = [\n                              'target', 'q', 'k', 'error', 'train_time'\n                          ] + list(params.keys())\n                result_df = pd.DataFrame([result], columns=columns)\n                if os.path.exists(LOG_FILE):\n                    result_df.to_csv(LOG_FILE, index=False, sep=';', mode='a', header=False)\n                else:\n                    result_df.to_csv(LOG_FILE, index=False, sep=';')\n\n                if os.path.exists(FIMP_FILE):\n                    feature_importances.to_csv(FIMP_FILE, index=False, mode='a', header=False)\n                else:\n                    feature_importances.to_csv(FIMP_FILE, index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"end_time = dt.datetime.now()\nprint('Finished', end_time, (end_time - start_time).seconds, 's')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}