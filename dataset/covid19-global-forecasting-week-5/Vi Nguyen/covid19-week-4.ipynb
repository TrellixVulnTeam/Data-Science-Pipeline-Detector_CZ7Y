{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"## Load in data\n## Imports\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ntrain = pd.read_csv('/kaggle/input/covid19-global-forecasting-week-4/train.csv', index_col = 'Id')\ntest = pd.read_csv('/kaggle/input/covid19-global-forecasting-week-4/test.csv', index_col = 'ForecastId')\n\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Combine 2 province and country columns\n\ntrain['Province_State'].fillna(\" \", inplace = True)\ntrain.rename(columns = {'Country_Region': 'Country'}, inplace = True)\ntrain['Country_Region'] = train.apply(lambda x: (x['Country'] + \"_\" + str(x['Province_State'] )) \n                                              if x['Province_State'] != \" \"\n                                              else x['Country'], axis = 1)\n\ntest['Province_State'].fillna(\" \", inplace = True)\ntest.rename(columns = {'Country_Region': 'Country'}, inplace = True)\ntest['Country_Region'] = test.apply(lambda x: (x['Country'] + \"_\" + str(x['Province_State']) \n                                              if x['Province_State'] != \" \"\n                                              else x['Country']), axis = 1)\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Change data types\n\n#train['Date'] = pd.to_datetime(train['Date'])\n#test['Date'] = pd.to_datetime(test['Date'])\n\nprint('There are ' + str(len(train['Country_Region'].unique())) + ' countries reported in this dataset')\nprint('Date from ' + str(min(train['Date'])) + '. And ends on ' + str(max(train['Date'])))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## EXPLORATORY ANALYSIS AND VISUALIZATION"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Plot world cases and fatalities\n\ntot_cases = train.groupby(['Date'], as_index = False).sum()\n\nplt.figure(figsize = (10,8))\nax = sns.lineplot(x = 'Date', y = 'ConfirmedCases', data = tot_cases, ci = False)\nsns.lineplot(x = 'Date', y = 'Fatalities', data = tot_cases, ax = ax)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def show_values_on_bars(axs):\n    def _show_on_single_plot(ax):        \n        for p in ax.patches:\n            _x = p.get_x() + p.get_width() / 2\n            _y = p.get_y() + p.get_height()\n            value = '{:.2f}'.format(p.get_height())\n            ax.text(_x, _y, value, ha=\"center\") \n\n    if isinstance(axs, np.ndarray):\n        for idx, ax in np.ndenumerate(axs):\n            _show_on_single_plot(ax)\n    else:\n        _show_on_single_plot(axs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gb_cntry = train.groupby(['Country', 'Date'], as_index = False).sum()\ngb_cntry['Mortality_rate'] = gb_cntry['Fatalities'] / gb_cntry['ConfirmedCases'] \ngb_cntry['Mortality_rate'].fillna(0, inplace = True)\n\n## Plot 5 countries with the most cases as of last day in training set\n\nplt.figure(figsize = (10, 8))\nax = sns.barplot(x = 'Country', y = 'ConfirmedCases',\n            data = gb_cntry[gb_cntry['Date'] == max(gb_cntry['Date'])].sort_values(['ConfirmedCases'], ascending = False)[:5])\nplt.title('Number of COVID19 cases in top5 most infected countries')\n\nshow_values_on_bars(ax)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Plot the mortality rates in top3 countries\n## Notice: different y-scales\n\nfig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize = (20,8))\n\ngb_cntry[gb_cntry['Country'] == 'US'].plot(x = 'Date', y = 'Mortality_rate', ax = ax1)\nax1.set_title('Mortality rate in the US')\n\ngb_cntry[gb_cntry['Country'] == 'Spain'].plot(x = 'Date', y = 'Mortality_rate', ax = ax2, color = 'r')\nax2.set_title('Mortality rate in Spain')\n\ngb_cntry[gb_cntry['Country'] == 'United Kingdom'].plot(x = 'Date', y = 'Mortality_rate', ax = ax3, color = 'g')\nax3.set_title('Mortality rate in UK')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Plot the number of cases in top3 countries\n\nfig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize = (20,8), sharey = 'all')\n\ngb_cntry[gb_cntry['Country'] == 'US'].plot(x = 'Date', y = 'ConfirmedCases', ax = ax1)\nax1.set_title('Number of cases in the US')\n\ngb_cntry[gb_cntry['Country'] == 'Spain'].plot(x = 'Date', y = 'ConfirmedCases', ax = ax2, color = 'r')\nax2.set_title('Number of cases in Italy')\n\ngb_cntry[gb_cntry['Country'] == 'United Kingdom'].plot(x = 'Date', y = 'ConfirmedCases', ax = ax3, color = 'g')\nax3.set_title('Number of cases in UK')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## MACHINE LEARNING"},{"metadata":{"trusted":true},"cell_type":"code","source":"## EXTERNAL DATA\n\ndf_temp = pd.read_csv('/kaggle/input/weather-data-for-covid19-data-analysis/training_data_with_weather_info_week_4.csv')\ndf_temp.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_temp.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Match df_temp with train data\n\ndf_temp['country+province'] = df_temp['country+province'].apply(lambda x: x[:-1] if x[-1] == '-' else x.replace('-', '_'))\nset(df_temp['country+province'].unique()) - set(train['Country_Region'].unique()) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Rename df_temp columns\n\ndf_temp.rename(columns = {'Country_Region': 'Country'}, inplace = True)\ndf_temp.rename(columns = {'country+province': 'Country_Region'}, inplace = True)\n\ndf_temp.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Merge df_temp and train data\n\ndf_temp1 = df_temp[['Date', 'Country_Region', 'temp', 'rh', 'wdsp']]\n\ntrain1 = train.copy().reset_index()\n\ndata = train1.merge(df_temp1, how = 'left', on = ['Country_Region', 'Date'], left_index = True)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Does not cover all dates in test set\n\nprint(df_temp['Date'].max())\nprint(test['Date'].min())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Function to extract day, week, and num of day since first day recorded\n\nimport re\n\ndef add_datepart (df, field_name):\n    fld = df[field_name]\n    targ_pre = re.sub('[Dd]ate$', '', field_name)\n    for n in ('Year', 'Month', 'Week', 'Day'):\n        df[targ_pre + n] = getattr(fld.dt, n.lower())\n    df[targ_pre + 'Elapsed'] = (fld - fld.min()).dt.days","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Take a subset of the data for testing model purposes\n\ndata['Date'] = pd.to_datetime(data['Date'])\nus_va = data[data['Country_Region'] == 'US_Virginia']\n\nprint(f'Size of dataset {len(us_va)}')\nadd_datepart(us_va, 'Date')\nus_va = us_va.set_index('Date')\nus_va","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# us_va['Prev_cases'] = [us_va['ConfirmedCases'][n-1] for n in range(us_va.shape[0])]\n# us_va['Prev_cases'][0] = 0.0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Transform X and fill in NAs\n\n\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\n\n\nclass ColumnChooser (BaseEstimator, TransformerMixin):\n    \n    def __init__ (self, columns):\n        self.columns = columns\n    def fit(self, X, y = None):\n        return self\n    def transform(self, X, y = None):\n        return X[self.columns]\n    \n\n\npre_pipeline = Pipeline([\n    ('choose_cols', ColumnChooser(['temp', 'rh', 'wdsp', 'Elapsed'])),\n    ('fill_na', SimpleImputer(strategy = 'median')),\n    ('scaler', StandardScaler())\n])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = pre_pipeline.fit_transform(us_va)\ny_cases = us_va['ConfirmedCases']\ny_fatal = us_va['Fatalities']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Linear Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\n\nlm = LinearRegression()\nlm.fit(X_train, y_cases)\nlm.score(X_train, y_cases)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def display_scores(scores):\n    print('Scores: ', scores)\n    print('Mean: ', scores.mean())\n    print('Standard deviation: ', scores.std())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\n\nlm_pred = lm.predict(X_train)\nprint(np.sqrt(mean_squared_error(y_cases, lm_pred)))\n\nfrom sklearn.model_selection import cross_val_score, cross_val_predict\n\nlm_scores = cross_val_score(lm, X_train, y_cases, cv = 10, scoring = 'neg_mean_squared_error')\ndisplay_scores(np.sqrt(-lm_scores))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lm_fatal = LinearRegression()\nlm_fatal.fit(X_train, y_fatal)\nlm_fatal_pred = lm_fatal.predict(X_train)\nprint(np.sqrt(mean_squared_error(us_va['Fatalities'], lm_fatal_pred)))\n\nlm_fatal_scores = cross_val_score(lm_fatal, X_train, y_fatal, cv = 10, scoring = 'neg_mean_squared_error')\ndisplay_scores(np.sqrt(-lm_fatal_scores))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### SGDRegression"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import SGDRegressor\n\nsgd = SGDRegressor(max_iter = 2000)\nsgd.fit(X_train, y_cases)\nsgd_pred = sgd.predict(X_train)\nprint(np.sqrt(mean_squared_error(y_cases, sgd_pred)))\n\nsgd_scores = cross_val_score(sgd, X_train, y_cases, cv = 10, scoring = 'neg_mean_squared_error')\ndisplay_scores(np.sqrt(-sgd_scores))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sgd_fatal = SGDRegressor()\nsgd_fatal.fit(X_train, y_fatal)\nsgd_fatal_pred = sgd_fatal.predict(X_train)\nprint(np.sqrt(mean_squared_error(y_fatal, sgd_fatal_pred)))\n\nsgd_fatal_scores = cross_val_score(sgd, X_train, y_fatal, cv = 10, scoring = 'neg_mean_squared_error')\ndisplay_scores(np.sqrt(-sgd_fatal_scores))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import math\n\nfor i in zip(y_fatal, lm_fatal_pred):\n    print(f'Actual: {i[0]}, Predicted: {i[1]}, Off_by: {i[0] - i[1]}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"cases_pred = []\nfatal_pred = []\n\nfor i in data['Country_Region'].unique():\n    \n    area = data[data['Country_Region'] == i]\n    \n    add_datepart(area, 'Date')\n    area = area.set_index('Date')\n    area['Prev_cases'] = [area['ConfirmedCases'][n-1] for n in range(area.shape[0])]\n    area['Prev_cases'][0] = 0.0\n    \n    X_train = pre_pipeline.fit_transform(area[['Prev_cases', 'rh', 'temp', 'Elapsed']])\n    y_cases = area['ConfirmedCases']\n    y_fatal = area['Fatalities']\n    \n    lm_cases = LinearRegression()\n    lm_cases.fit(X_train, y_cases)\n    lm_cases_pred = lm_cases.predict(X_train).tolist()\n    cases_pred += lm_cases_pred\n    \n    lm_fatal = LinearRegression()\n    lm_fatal.fit(X_train, y_fatal)\n    lm_fatal_pred = lm_fatal.predict(X_train).tolist()\n    fatal_pred += lm_fatal_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"actual = []\nfor i in data['Country_Region'].unique():\n    new = data[data['Country_Region'] == i]['ConfirmedCases'].tolist()\n    actual += new","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cases_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cases_pred = [0 if i < 0 else math.floor(i) for i in cases_pred]\nfatal_pred = [0 if i < 0 else math.floor(i) for i in fatal_pred]\n\ndata['Predicted cases'] = cases_pred\ndata['Predicted fatalities'] = fatal_pred\ndata","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.sqrt(mean_squared_error(data['ConfirmedCases'], data['Predicted cases']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in zip(actual[:100], cases_pred[:100]):\n    print(f'Actual: {i[0]}, Predicted: {i[1]}, Off_by: {i[0] - i[1]}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.sqrt(mean_squared_error(actual, cases_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"test1 = test.merge(df_temp1, how = 'left', on = ['Country_Region', 'Date'])\ntest1['Date'] = pd.to_datetime(test1['Date'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test1[['temp','rh','wdsp']].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_cases_pred = []\nfinal_fatal_pred = []\n\nfor i in test1['Country_Region'].unique():\n    \n    area = test1[test1['Country_Region'] == i]\n    \n    add_datepart(area, 'Date')\n    area = area.set_index('Date')\n    area['Prev_cases'] = [area['ConfirmedCases'][n-1] for n in range(area.shape[0])]\n    area['Prev_cases'][0] = 0.0\n    \n    X_train = pre_pipeline.fit_transform(area[['Prev_cases', 'rh', 'temp', 'Elapsed']])\n    y_cases = area['ConfirmedCases']\n    y_fatal = area['Fatalities']\n    \n    lm_cases = LinearRegression()\n    lm_cases.fit(X_train, y_cases)\n    lm_cases_pred = lm_cases.predict(X_train).tolist()\n    final_cases_pred += lm_cases_pred\n    \n    lm_fatal = LinearRegression()\n    lm_fatal.fit(X_train, y_fatal)\n    lm_fatal_pred = lm_fatal.predict(X_train).tolist()\n    final_fatal_pred += lm_fatal_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}