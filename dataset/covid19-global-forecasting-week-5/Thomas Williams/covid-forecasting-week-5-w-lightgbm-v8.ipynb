{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","id":"h3-EY6eHSM1z","trusted":false},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n#from google.colab import drive\n#drive.mount('/content/gdrive')\n#os.chdir('/content/gdrive/My Drive/kaggle/covid-19/forecasting/week5')\nfor dirname, _, filenames in os.walk('../input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"id":"MDWBS_peSM13"},"cell_type":"markdown","source":"# 0. Discussion of approach"},{"metadata":{"id":"8TzThsirSM14"},"cell_type":"markdown","source":"From the CDC:  https://www.cdc.gov/coronavirus/2019-ncov/hcp/clinical-guidance-management-patients.html\n\n<b>Clinical Progression</b>\n\nAmong patients who developed severe disease, the medium time to dyspnea ranged from 5 to 8 days, the <b>median time to acute respiratory distress syndrome (ARDS) ranged from 8 to 12 days</b>, and the <b>median time to ICU admission ranged from 10 to 12 days</b>.<sup>5,6,10,11</sup> Clinicians should be aware of the potential for some patients to rapidly deteriorate one week after illness onset. Among all hospitalized patients, a range of 26% to 32% of patients were admitted to the ICU.<sup>6,8,11</sup> Among all patients, a range of 3% to 17% developed ARDS compared to a range of 20% to 42% for hospitalized patients and 67% to 85% for patients admitted to the ICU.<sup>1,4-6,8,11</sup> Mortality among patients admitted to the ICU ranges from 39% to 72% depending on the study.<sup>5,8,10,11</sup> The <b>median length of hospitalization among survivors was 10 to 13 days</b>.<sup>1,6,8</sup>\n\n<p>1. Guan WJ, Ni ZY, Hu Y, et al. Clinical Characteristics of Coronavirus Disease 2019 in China. The New England journal of medicine. 2020.</p>\n<p>4. Chen N, Zhou M, Dong X, et al. Epidemiological and clinical characteristics of 99 cases of 2019 novel coronavirus pneumonia in Wuhan, China: a descriptive study. Lancet (London, England). 2020;395(10223):507-513.</p>\n<p>5. Huang C, Wang Y, Li X, et al. Clinical features of patients infected with 2019 novel coronavirus in Wuhan, China. Lancet (London, England). 2020;395(10223):497-506.</p>\n<p>6. Wang D, Hu B, Hu C, et al. Clinical Characteristics of 138 Hospitalized Patients With 2019 Novel Coronavirus-Infected Pneumonia in Wuhan, China. Jama. 2020.</p>\n<p>8. Wu C, Chen X, Cai Y, et al. Risk Factors Associated With Acute Respiratory Distress Syndrome and Death in Patients With Coronavirus Disease 2019 Pneumonia in Wuhan, China. JAMA Intern Med. 2020.</p>\n<p>10. Yang X, Yu Y, Xu J, et al. Clinical course and outcomes of critically ill patients with SARS-CoV-2 pneumonia in Wuhan, China: a single-centered, retrospective, observational study. The Lancet Respiratory medicine. 2020.</p>\n<p>11. Zhou F, Yu T, Du R, et al. Clinical course and risk factors for mortality of adult inpatients with COVID-19 in Wuhan, China: a retrospective cohort study. Lancet (London, England). 2020.</p>\n\n<p><b>Lag data by 7, 10, 14 and 21 days</b></p>\n<p><b>Fatalities should lag cases by about 14 days</b></p>\n<p><b>See if data aligns with these patterns, or is something changing?  Does geography matter?</b></p>\n<p> Week 5 has started!  Rework this notebook, understand what will work with new metric & daily submission</p>"},{"metadata":{"id":"_P9EtpbzSM14"},"cell_type":"markdown","source":"# 1. Import required libraries"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","id":"Kx5ukcDuSM15","trusted":false},"cell_type":"code","source":"import datetime\nfrom enum import Enum\nimport pdb\nimport lightgbm as lgb\nfrom lightgbm import LGBMRegressor\n#import xgboost as xgb\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_log_error\nfrom sklearn.preprocessing import MinMaxScaler, LabelEncoder\nfrom sklearn.model_selection import train_test_split, cross_val_score, KFold\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"id":"_w2_VAoISM18"},"cell_type":"markdown","source":"# 2. Define functions to simplify processing"},{"metadata":{"id":"fmHz5U73SM19","trusted":false},"cell_type":"code","source":"class ModelType(Enum):\n    XGBoost = 1\n    LightGBM = 2\n\nmodel_type = ModelType.LightGBM\n\nverbose = True\nloop_logic = True    # loop on country/state/county in main program\nscale_data = False   # scale data with MinMaxScaler\nuse_base_model = False  # use base model throughout\none_hot_encode = False  # use one-hot encoding or label to integer encoding\nconfidence_levels = [0.05, 0.5, 0.95]\nall_lags = [7, 14, 21]\nestimators=2500\nglobal_label_encoder = LabelEncoder()\n\nlabel_encoders = {}\n\n# only submit predictions up to last date in test set during public leaderboard period\npublic_leaderboard_end_date = None # '5/10/2020' ","execution_count":null,"outputs":[]},{"metadata":{"id":"mDZFfDIXSM1_","trusted":false},"cell_type":"code","source":"def fix_data_issues(df):\n    df['Province_State'] = np.where(df['Province_State'].isnull(), df['Country_Region'], df['Province_State']) \n    df['County'] = np.where(df['County'].isnull(), df['Province_State'], df['County']) ","execution_count":null,"outputs":[]},{"metadata":{"id":"3mUPkjlNSM2C","trusted":false},"cell_type":"code","source":"def get_test_train_for_country_state_county(one_hot_encode_flag, df_train, df_test, country, state, county):\n    if one_hot_encode_flag == True:\n        cs_train = df_train[(df_train['Country_Region_'+country] == 1) & (df_train['Province_State_'+state] == 1)  & (df_train['County_'+county] == 1) ]  \n        cs_test = df_test[(df_test['Country_Region_'+country] == 1) & (df_test['Province_State_'+state] == 1) & (df_test['County_'+county] == 1) ]\n    else:\n        cs_train = df_train[(df_train['Country_Region'] == country) & (df_train['Province_State'] == state)  & (df_train['County'] == county) ]\n        cs_test = df_test[(df_test['Country_Region'] == country) & (df_test['Province_State'] == state) & (df_test['County'] == county) ] \n    \n    return (cs_train, cs_test)","execution_count":null,"outputs":[]},{"metadata":{"id":"ymvhQNVqSM2F","trusted":false},"cell_type":"code","source":"def add_features(df, default_value=None):\n    # add log of cases / fatalities values\n    if default_value == None:\n        log_cases = np.log1p(df.loc[(df['Target']=='ConfirmedCases'), 'TargetValue'])\n        log_fatal = np.log1p(df.loc[(df['Target']=='Fatalities'), 'TargetValue'])\n        df.insert(4, 'LogCases', log_cases)\n        df.insert(5, 'LogFatal', log_fatal)\n        df.insert(5, 'LogPopulation', np.log1p(df['Population']) )\n        df['LogCases'].fillna(0)\n        df['LogFatal'].fillna(0)\n        df['LogPopulation'].fillna(0)\n    else:\n        df.insert(4, 'LogCases', default_value)\n        df.insert(5, 'LogFatal', default_value)\n        df.insert(5, 'LogPopulation', default_value) \n        \n        \n    \n    #lag_target(df, all_lags, ['ConfirmedCases', 'Fatalities'], 6)\n\n    #df_group = df.groupby(['Country_Region', 'Province_State'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def lag_target(df, lags, target_labels, insert_after, all_zeros=False):\n    # dataframe dummy columns should still have original names at this point, no one-hot encoding yet    \n    country_groups = df.groupby(['Country_Region', 'Province_State', 'County']).groups\n    df_country_list = pd.DataFrame.from_dict(list(country_groups))\n    unique_country_list = df_country_list[0].unique()\n    \n    lag_columns = {}\n    \n#    pdb.set_trace()\n    \n    for label in target_labels:\n        for lag in lags:\n            lag_columns[lag] = \"{0}_Lag_{1:.0f}\".format(label, lag)\n            if not lag_columns[lag] in df.columns:\n                df.insert(insert_after, lag_columns[lag], 0)\n\n    for country in unique_country_list:\n        country_states = df_country_list[(df_country_list[0] == country)][1].values\n        for state in country_states:\n            state_counties = df_country_list[(df_country_list[0] == country) & (df_country_list[1] == state)][2].values\n            for county in state_counties:\n                for label in target_labels:\n                    geog_filter = (df_train['Country_Region']==country) & (df_train['Province_State']==state) & (df_train['County']==county) & (df_train['Target']==label)\n#                    print(\"Generating lags for {0}, country={1}, state={2}, county={3}, target={4}.\".format(lag_columns[lag], country, state, county, label))\n                    for lag in lags:\n                        if all_zeros == False:\n                            df_train.loc[geog_filter, lag_columns[lag]] = df_train.loc[geog_filter, \"TargetValue\"].shift(lag)\n                        else:\n                            df_train.loc[geog_filter, lag_columns[lag]] = 0","execution_count":null,"outputs":[]},{"metadata":{"id":"lrfoIKu8SM2I","trusted":false},"cell_type":"code","source":"def transform_dates(df):\n    dates = pd.to_datetime(df['Date']) \n    min_dates = dates.min()\n#    df['Date_Days_Since_Pandemic_Start'] = (dates - min_dates).dt.days\n    df.insert(len(df.columns)-2,'Date_Year', dates.dt.year)\n    df.insert(len(df.columns)-2,'Date_Month', dates.dt.month)\n    df.insert(len(df.columns)-2,'Date_Day', dates.dt.day)\n    df.insert(len(df.columns)-2,'Date_Week', dates.dt.week)\n    df.insert(len(df.columns)-2,'Date_DayofWeek', dates.dt.dayofweek)\n    df.insert(len(df.columns)-2,'Date_DayofYear', dates.dt.dayofyear)\n    df.insert(len(df.columns)-2,'Date_WeekofYear', dates.dt.weekofyear)\n    df.insert(len(df.columns)-2,'Date_Quarter', dates.dt.quarter)\n    df.drop(['Date'], axis=1, inplace=True)   # remove the date column, no longer needed","execution_count":null,"outputs":[]},{"metadata":{"id":"4J7IfkWHSM2L","trusted":false},"cell_type":"code","source":"def setup_df_encode_and_dates(df, encode_flag, dummy_cols, target_cols=[]):\n    # move country in front of province/state\n    enc_df = df.copy()\n    \n    # Find out how to move columns - only the 4 columns listed below are moved\n\n    cols = list(enc_df.columns)\n    a, b = cols.index('Province_State'), cols.index('Country_Region')\n    cols[b], cols[a] = cols[a], cols[b]\n    enc_df = enc_df[cols]\n    \n#    enc_df = enc_df[[enc_df.columns[0], enc_df.columns[2], enc_df.columns[1],enc_df.columns[3]]]  # 1st column named differently in train vs test\n    \n    if encode_flag == True:\n        enc_df = pd.get_dummies(enc_df, columns=dummy_cols)  # one-hot encoding\n#        dummy_df = pd.get_dummies(enc_df, columns=dummy_cols)  # one-hot encoding\n#        enc_df = pd.concat([enc_df, dummy_df], axis=1)\n#        enc_df.drop(dummy_cols, axis=1)\n        enc_df = enc_df[[col for col in enc_df if col not in target_cols] + target_cols]\n    else:\n        for dum_col in dummy_cols:\n            label_encoders[dum_col] = LabelEncoder()\n            enc_df[dum_col] = label_encoders[dum_col].fit_transform(enc_df[dum_col])   # label encoding\n\n    # extract date parts / date descriptors (week, quarter, etc.).  Remove original date variable as it can't be used by NN\n    transform_dates(enc_df)\n\n    # make sure added feature columns are moved to encoded df\n    \n    \n#    for col in target_cols:\n#        enc_df[col] = df[col]\n\n    return(enc_df)","execution_count":null,"outputs":[]},{"metadata":{"id":"PysUaMc7SM2W","trusted":false},"cell_type":"code","source":"def prepare_train_set(df_train):\n    # break out main body of train set and separate the target variables out\n    df_cases = df_train[df_train['Target'] == 'ConfirmedCases']\n    df_fatal = df_train[df_train['Target'] == 'Fatalities']\n    \n    train_x_cases, train_target_cases = df_cases.iloc[:, :-2], df_cases.iloc[:, -1]\n    train_x_fatal, train_target_fatal = df_fatal.iloc[:, :-2], df_fatal.iloc[:, -1]\n    \n    return(train_x_cases, train_x_fatal, train_target_cases, train_target_fatal)","execution_count":null,"outputs":[]},{"metadata":{"id":"n5QPXLxuSM2b","trusted":false},"cell_type":"code","source":"def model_and_predict(model, X, y, test, conf_levels, estimators=5000):\n    if verbose == True:\n        print(\"Initial model ID in model_and_predict: {0}\".format(id(model)))\n    if model != None:\n        run_model = model\n        if verbose == True:\n            print(\"Running with model id #{0}\".format(id(model)))\n\n    else:\n#        run_model = xgb.XGBRegressor(objective='reg:squarederror', n_estimators= estimators)\n        if model_type == ModelType.XGBoost:\n            run_model = xgb.XGBRegressor(objective='reg:squarederror', n_estimators= estimators)\n        else:\n            #[LightGBM] [Warning] Unknown parameter: loss\n            #[LightGBM] [Warning] min_data_in_leaf is set=500, min_child_samples=2 will be ignored. Current value: min_data_in_leaf=500\n            #[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n\n            run_model = LGBMRegressor(num_leaves = 85, learning_rate =10**-1.89,n_estimators=100,\n                                      min_sum_hessian_in_leaf=(10**-4.1),min_child_samples =2,\n                                      subsample =0.97,subsample_freq=10,\n                                      colsample_bytree = 0.68,reg_lambda=10**1.4,random_state=1234,n_jobs=4)\n        if verbose == True:\n            print(\"Running with new model\")\n\n    if verbose == True:\n        print(\"Model ID in model_and_predict: {0}\".format(id(run_model)))\n        \n    #initial training on 80%/20% train/test split \n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=12345)\n\n    X_train_weights = X_train['Weight']\n    X_test_weights = X_test['Weight']\n\n    X_no_weights = X.drop(['Weight'], axis=1)  # drop Weight column from X for final training\n    test_no_weights = test.drop(['Weight'], axis=1)  # drop Weight column from test for final training\n    \n    # drop weight columns from X and test \n    X_train.drop(['Weight'], axis=1, inplace=True)  # drop Weight column from train set\n    X_test.drop(['Weight'], axis=1, inplace=True)  # drop Weight column from test set\n\n    \n#    model = model.fit(X_train, y_train)\n    if model_type == ModelType.XGBoost:\n        run_model.fit(X_train, y_train)\n        y_train_pred = run_model.predict(X_train)\n        y_test_pred = run_model.predict(X_test)\n    else:\n#        pdb.set_trace()\n#        run_model.fit(X_train, y_train, eval_set=[(X_test, y_test)], eval_metric='l1', early_stopping_rounds=5)\n#        run_model.fit(X_train, y_train)\n        lgb_train = lgb.Dataset(X_train, y_train)\n        quantile_models = {}\n        y_preds = []\n        y_train_preds = []\n        lgb_parms = run_model.get_params()\n        lgb_parms.pop('silent')\n        lgb_parms.pop('importance_type')\n        lgb_parms.pop('min_child_weight')        \n        for cl in conf_levels:\n            lgb_parms['verbose'] = -1\n            lgb_parms['alpha'] = cl\n            curr_model = lgb.train(lgb_parms, lgb_train)\n            quantile_models[cl] = curr_model\n            \n            y_train_pred = curr_model.predict(X_train)\n            y_train_preds.append(y_train_pred)\n            \n            y_test_pred = curr_model.predict(X_test)\n            \n            # now predict using the trained model on all of the test rows\n            \n#            lgb_full = lgb.Dataset(X_no_weights, y)\n#            full_model = lgb.train(lgb_parms, lgb_full)\n            y_train_full = curr_model.predict(X_no_weights)    # predict with full train set w/o Weight column\n            y_pred = curr_model.predict(test_no_weights)\n#            y_pred[y_pred < 0] = 0\n            y_preds.append(y_pred)\n            \n    # compute pinball loss here.  \n    pb_loss = weighted_pinball_loss(y_train, y_train_preds, X_train_weights, conf_levels)\n\n    \n    return(y_preds, pb_loss)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def compute_cl_loss(y, y_hat, weights, tau):\n    return((weights * (tau * (y - y_hat) + (1 - tau) * (y_hat - y))))","execution_count":null,"outputs":[]},{"metadata":{"id":"FJQbEYcTSM2S","trusted":false},"cell_type":"code","source":"def weighted_pinball_loss(y, y_hat_arrays, w, tau_list):\n    Nf = len(y)\n    Nt = len(tau_list)\n    \n#    w_conf = 1/(np.ln(y.size + 1))\n#    w_fatal = 1/(10*np.ln(y.size + 1))\n\n    \n\n    score = (1/Nf) * np.sum([np.sum((1/Nt) * (w * np.maximum(tau * (y - y_hat), (1 - tau) * (y_hat - y)))) for y_hat, tau in zip(y_hat_arrays, tau_list)  ])\n    \n    return(score)","execution_count":null,"outputs":[]},{"metadata":{"id":"76PGeMq7SM2d","trusted":false},"cell_type":"code","source":"def show_results(model):\n    # Code based on \"Selecting Optimal Parameters for XGBoost Model Training\" by Andrej Baranovskij (Medium)\n    results = model.evals_result()\n    epochs = len(results['validation_0']['error'])\n    x_axis = range(0, epochs)\n    # plot log loss\n    fig, ax = plt.subplots()\n    ax.plot(x_axis, results['validation_0']['logloss'], label='Train')\n    ax.plot(x_axis, results['validation_1']['logloss'], label='Test')\n    ax.legend()\n    plt.ylabel('Log Loss')\n    plt.title('XGBoost Log Loss')\n    plt.show()\n    # plot classification error\n    fig, ax = pyplot.subplots()\n    ax.plot(x_axis, results['validation_0']['error'], label='Train')\n    ax.plot(x_axis, results['validation_1']['error'], label='Test')\n    ax.legend()\n    plt.ylabel('Classification Error')\n    plt.title('XGBoost Classification Error')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"mFbH5qWdSM2f","trusted":false},"cell_type":"code","source":"def fit_models_and_train(country, state, county, model, train, test, conf_levels):\n    X_cases, X_fatal, y_cases, y_fatal = prepare_train_set(train)\n    \n    X_cases.drop(['Id'], axis=1, inplace=True)    # remove the Id column from the training set to avoid leakage\n    X_fatal.drop(['Id'], axis=1, inplace=True)    # remove the Id column from the training set to avoid leakage\n\n    X_cases_weights = X_cases['Weight']\n    X_fatal_weights = X_fatal['Weight']\n    \n    forecast_IDs = test.iloc[:,0]  # save the ForecastId column\n    \n    test_cases = test[test['Target'] == 'ConfirmedCases']\n    test_fatal = test[test['Target'] == 'Fatalities']\n\n    cases_forecast_IDs = test_cases.ForecastId  # save the ForecastId column for cases\n    fatal_forecast_IDs = test_fatal.ForecastId  # save the ForecastId column for fatalities\n\n\n    test_cases_no_id = test_cases.iloc[:, 1:]   # use the rest of the test set without the ForecastId column\n    test_fatal_no_id = test_fatal.iloc[:, 1:]   # use the rest of the test set without the ForecastId column\n\n    test_cases_no_id.drop(['Target'], axis=1, inplace=True)  # drop Target column from test set\n    test_fatal_no_id.drop(['Target'], axis=1, inplace=True)  # drop Target column from test set\n\n    country_text = decode_label('Country_Region', country)\n    state_text = decode_label('Province_State', state)\n    county_text = decode_label('County', county)\n    \n    # apply scaling to train and test set\n    if scale_data == True:\n        scaler = MinMaxScaler()\n        X = scaler.fit_transform(X.values)\n        test_cases_no_id = scaler.transform(test_cases_no_id.values)\n        test_fatal_no_id = scaler.transform(test_fatal_no_id.values)\n\n#    pdb.set_trace()\n\n    if verbose == True:\n        print(\"Predicting cases.\")\n        \n    y_cases_pred, cases_pb_loss = model_and_predict(model, X_cases, y_cases, test_cases_no_id, conf_levels)   # prior version: estimators = 10000, trying default of 2000\n    if verbose == True:\n        print(\"Country {0}, state {1}, county {2}: cases pinball loss score: {3:0.2f}.\".format(country_text, state_text, county_text, cases_pb_loss))\n\n#   pdb.set_trace()\n    \n##    X_train, X_test, y_train, y_test = train_test_split(X, y_fatal, test_size=0.2, random_state=12345)\n    if verbose == True:\n        print(\"Predicting fatalities.\")\n        \n    y_fatal_pred, fatal_pb_loss = model_and_predict(model, X_fatal, y_fatal, test_fatal_no_id, conf_levels)\n    if verbose == True:\n        print(\"Country {0}, state {1}, county {2}: fatalities pinball loss score: {3:0.2f}.\".format(country_text, state_text, county_text, fatal_pb_loss))\n\n#    5/9:  Make sure preds are in proper format for submission file\n\n    preds = pd.DataFrame(forecast_IDs)\n\n# Stitch the predictions back together\n\n#    preds['ConfirmedCases'] = y_cases_pred\n#    preds['Fatalities'] = y_fatal_pred\n\n\n#    pdb.set_trace()\n\n    y_cases_with_index = []\n    for i in range(len(cases_forecast_IDs)):\n        conf_group = []\n        for j in range(len(conf_levels)):\n            conf_group.append([ cases_forecast_IDs[cases_forecast_IDs.index[i]], conf_levels[j], y_cases_pred[j][i]])\n        y_cases_with_index.append(conf_group)\n\n    y_fatal_with_index = []\n    for i in range(len(fatal_forecast_IDs)):\n        conf_group = []\n        for j in range(len(conf_levels)):\n            conf_group.append([ fatal_forecast_IDs[fatal_forecast_IDs.index[i]], conf_levels[j], y_fatal_pred[j][i]])\n        y_fatal_with_index.append(conf_group)\n\n\n    preds = [ y_cases_with_index, y_fatal_with_index ]\n\n    return(preds)","execution_count":null,"outputs":[]},{"metadata":{"id":"le2Uji5ySM2i","trusted":false},"cell_type":"code","source":"def cv_model(country, state, train, test):\n    X, y_cases, y_fatal = prepare_train_set(train)\n    X = X.drop(['Id'], axis=1)    # remove the Id column from the training set to avoid leakage\n\n#    forecast_IDs = test.iloc[:,0]  # save the ForecastId column\n\n    X_test = test.iloc[:, 1:]   # use the rest of the test set without the ForecastId column\n\n    data_train_cases_matrix = xgb.DMatrix(data=X, label=y_cases)\n    data_train_fatal_matrix = xgb.DMatrix(data=X, label=y_fatal)\n    \n#    scores = cross_val_score(model, X, y_cases,cv=5, scoring='accuracy')\n#    print(\"Country {0}, state {1}: cases mean cross-validation score: {2:0.2f}.\".format(country, state, scores.mean()))\n\n    cv_results_cases = xgb.cv(dtrain=data_train_cases_matrix, params=parms, nfold=3, num_boost_round=50,\n                   early_stopping_rounds=50,metrics=\"rmse\",as_pandas=True,seed=12345)\n    \n    print(\"Cases RMSE: {0:.2f}.\".format(cv_results_cases['test-rmse-mean'].tail(1).values[0]))\n    \n    cv_results_fatal = xgb.cv(dtrain=data_train_fatal_matrix, params=parms, nfold=3, num_boost_round=50,\n                   early_stopping_rounds=50,metrics=\"rmse\",as_pandas=True,seed=12345)        \n\n    print(\"Fatalities RMSE: {0:.2f}.\".format(cv_results_fatal['test-rmse-mean'].tail(1).values[0]))\n\n#    scores = cross_val_score(model, X, y_fatal,cv=5, scoring='accuracy')\n#    print(\"Country {0}, state {1}: fatalities mean cross-validation score: {2:0.2f}.\".format(country, state, scores.mean()))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def decode_label(column, label):\n    decoded_label = label     # return original label if no decoding to be done\n    \n    if one_hot_encode == False:\n        if label != \"All\":\n            decoded_label = label_encoders[column].inverse_transform([label])\n        \n    return(decoded_label)","execution_count":null,"outputs":[]},{"metadata":{"id":"lnM7uceSSM2Y","trusted":false},"cell_type":"code","source":"def prepare_submission(df_all_preds):\n    formatted_preds = [ [\"{0:.0f}_{1}\".format(row[0], row[1] ), row[2]] for row in df_all_preds[['ForecastId', 'Quantile', 'TargetValue']].values ]\n    pd.DataFrame(formatted_preds).to_csv('submission.csv', header=['ForecastId_Quantile', 'TargetValue'], index=False)","execution_count":null,"outputs":[]},{"metadata":{"id":"CQf_5PzhSM2l"},"cell_type":"markdown","source":"# 3. Obtain data"},{"metadata":{"id":"PUWMG9trSM2l"},"cell_type":"markdown","source":"Note: Data has changed, now multiple target values + quantile loss.\nWill try separate predictions for cases / fatal but this will be time-consuming.\nConsider multiple target regression methods.  \n\nSee https://scikit-learn.org/stable/modules/multiclass.html\n\nMultioutput regression: predicts multiple numerical properties for each sample. Each property is a numerical variable and the number of properties to be predicted for each sample is greater than or equal to 2. Some estimators that support multioutput regression are faster than just running n_output estimators.\n\nFor example, prediction of both wind speed and wind direction, in degrees, using data obtained at a certain location. Each sample would be data obtained at one location and both wind speed and directtion would be output for each sample.\n\nValid representation of multilabel y is dense matrix of shape (n_samples, n_classes) of floats. A column wise concatenation of continuous variables.\n\n\n"},{"metadata":{"id":"XrcoSARvSM2l","trusted":false},"cell_type":"code","source":"# Get the training data\ndf_train = pd.read_csv('../input/covid19-global-forecasting-week-5/train.csv')","execution_count":null,"outputs":[]},{"metadata":{"id":"yyMOM-3nSM2r","trusted":false},"cell_type":"code","source":"# Get the test data\ndf_test = pd.read_csv('../input/covid19-global-forecasting-week-5/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"id":"MKOD8SLeSM2t"},"cell_type":"markdown","source":"# 4. Perform Exploratory Data Analysis and Feature Extraction"},{"metadata":{"id":"Kw8mHyZPSM2t","trusted":false},"cell_type":"code","source":"# Exploratory data analysis\ndf_train.shape","execution_count":null,"outputs":[]},{"metadata":{"id":"pac_cFIeSM2w","trusted":false},"cell_type":"code","source":"df_test.shape","execution_count":null,"outputs":[]},{"metadata":{"id":"-_9M5NyCSM2z","trusted":false},"cell_type":"code","source":"# Fix any known data issues in train and test sets\nfix_data_issues(df_train)\nfix_data_issues(df_test)","execution_count":null,"outputs":[]},{"metadata":{"id":"WQ-9qeafSM21","trusted":false},"cell_type":"code","source":"# Save original train/test data in case we need it later\ndf_train_original = df_train.copy()\ndf_test_original = df_test.copy()\ndf_train_original['Datetime'] = pd.to_datetime(df_train_original['Date'])\ndf_test_original['Datetime'] = pd.to_datetime(df_test_original['Date'])","execution_count":null,"outputs":[]},{"metadata":{"id":"J0fGeSj1SM23","trusted":false},"cell_type":"code","source":"# remove overlap dates from train set\nif not (public_leaderboard_end_date == None):\n    date_filter = df_train[pd.to_datetime(df_train.Date) > pd.to_datetime(public_leaderboard_end_date)].index\n    df_train.drop(date_filter, inplace=True)  # remove for final submissions","execution_count":null,"outputs":[]},{"metadata":{"id":"khA7nlomSM26","scrolled":true,"trusted":false},"cell_type":"code","source":"df_train[pd.to_datetime(df_train.Date) > pd.to_datetime('2020/04/01')]","execution_count":null,"outputs":[]},{"metadata":{"id":"Z0lVVf61SM28","scrolled":true,"trusted":false},"cell_type":"code","source":"df_train_original.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"4lGCIau0SM2-","trusted":false},"cell_type":"code","source":"# add features\n#add_features(df_train)\n#add_features(df_test, 0)","execution_count":null,"outputs":[]},{"metadata":{"id":"2CGUWmJ5SM3A","scrolled":false,"trusted":false},"cell_type":"code","source":"df_train.head(92)","execution_count":null,"outputs":[]},{"metadata":{"id":"s9KHQBN9SM3D","scrolled":true,"trusted":false},"cell_type":"code","source":"df_test.head(92)","execution_count":null,"outputs":[]},{"metadata":{"id":"xOUQRXmiSM3G"},"cell_type":"markdown","source":"# 5. Define base model and parameters"},{"metadata":{"id":"49GVKdkkSM3G","trusted":false},"cell_type":"code","source":"if model_type == ModelType.XGBoost:\n    parms = {'loss': 'quantile', 'objective' :'reg:squarederror', 'colsample_bytree' : 0.4, 'learning_rate' : 0.01,\n                    'max_depth' : 5, 'reg_alpha' : 0.3, 'n_estimators' : 2000 }\nelse:\n    if loop_logic == False:\n        parms = {'verbose' : - 1, 'objective' :'quantile',  'max_depth' : 8, 'num_leaves' : 50, 'colsample_bytree' : 0.4, 'learning_rate' : 10**-1.89, \n                         'reg_alpha' : 0.3, 'n_estimators' : 1000, 'min_sum_hessian_in_leaf' : (10**-4.1), \n                         'min_child_samples' : 2, 'subsample' : 0.97, 'subsample_freq' : 10, 'min_data_in_leaf' : 500, \n                         'colsample_bytree' : 0.68, 'reg_lambda' : 10**1.4, 'random_state' : 1234, 'n_jobs': 4 \n                }\n    else:\n        parms = {'verbose' : - 1, 'objective' :'quantile',  'max_depth' : 8, 'num_leaves' : 50, 'colsample_bytree' : 0.4, 'learning_rate' : 10**-1.89, \n                         'reg_alpha' : 0.3, 'n_estimators' : 1000, 'min_sum_hessian_in_leaf' : (10**-4.1), \n                         'min_child_samples' : 2, 'subsample' : 0.97, 'subsample_freq' : 10, \n                         'colsample_bytree' : 0.68, 'reg_lambda' : 10**1.4, 'random_state' : 1234, 'n_jobs': 4 \n                }\n   \nnum_round = 2","execution_count":null,"outputs":[]},{"metadata":{"id":"QjYVIAAoSM3I","trusted":false},"cell_type":"code","source":"# define base XGBoost parameters and model for predictions\n#base_model = xgb.XGBRegressor(objective='reg:squarederror', \n#                         colsample_bytree=0.4, \n#                         learning_rate=0.01,\n#                         max_depth=15, \n#                         reg_alpha=0.3,\n#                         n_estimators= estimators)\n\nif model_type == ModelType.XGBoost:\n    base_model = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=estimators, random_state=12345, max_depth=15)\n#    base_model = xgb.XGBRegressor(objective='reg:squarederror', \n#                             colsample_bytree=0.4, \n#                             learning_rate=0.01,\n#                             max_depth=15, \n#                             reg_alpha=0.3,\n#                             n_estimators= estimators)\nelse:\n    base_model = LGBMRegressor(objective='quantile', num_leaves = 85, learning_rate =10**-1.89,n_estimators=100,\n                               min_sum_hessian_in_leaf=(10**-4.1),min_child_samples =2,subsample =0.97,subsample_freq=10,\n                               colsample_bytree = 0.68,reg_lambda=10**1.4,random_state=1234,n_jobs=4, verbose=-1)","execution_count":null,"outputs":[]},{"metadata":{"id":"4GTQEinJSM3K","trusted":false},"cell_type":"code","source":"print(\"Model ID: {0}\".format(id(base_model)))","execution_count":null,"outputs":[]},{"metadata":{"id":"AODdIBJ3SM3M"},"cell_type":"markdown","source":"# 6. Train models, predict outcomes"},{"metadata":{"id":"cDYaVbBGSM3N","scrolled":false,"trusted":false},"cell_type":"code","source":"# Logic influenced by Anshul Sharma's \"COVID19-Explained through Visualizations\" notebook,\n# RanjitKS's \"20 lines; XGBoost; No Leaks; Best Score\" and others:\n\n# Set up one-hot encoding to avoid possible leakage from LabelEncoder values (alphabetical ordering of geographies, etc.)\n\n# Possible improvements:\n#  - try time lags and other time-series adjustments\n#  - try geog, political, transportation, cultural data to enhance model fit\n\n# get country / state list. If one-hot encoded, Train dataframe will have one column per country/state combination\n# If label encoded, original columns will have a numeric value instead of text country/state name\n\n#pdb.set_trace()\n\ndf_train_dd = setup_df_encode_and_dates(df_train, one_hot_encode, ['Country_Region', 'Province_State', 'County'], ['Target', 'TargetValue'])\ndf_test_dd = setup_df_encode_and_dates(df_test, one_hot_encode, ['Country_Region', 'Province_State', 'County'])\n\nif one_hot_encode == True:\n    country_groups = df_train_original.groupby(['Country_Region', 'Province_State', 'County']).groups\nelse:\n    country_groups = df_train_dd.groupby(['Country_Region', 'Province_State', 'County']).groups\n    \ndf_country_list = pd.DataFrame.from_dict(list(country_groups))\ntrain_country_list = df_country_list[0].unique()\n\n#pdb.set_trace()\n\n#df_preds = pd.DataFrame({'TargetValue': []})\n\n\nif (loop_logic == True): \n    preds = [[],[]]\n    # loop over states within countries\n    print(\"Starting forecasting for {0} countries.\".format(len(train_country_list)))\n    for country in train_country_list:\n        country_text = decode_label('Country_Region', country)\n        print(\"Starting country {0}.\".format(country_text))\n\n        # Get list of states/provinces (if any) for the current country \n        country_states = np.unique(df_country_list[(df_country_list[0] == country)][1].values)\n        \n        for state in country_states:\n            state_text = decode_label('Province_State', state)\n            print(\"Starting state {0}.\".format(state_text))\n            # Get list of counties (if any) in current state\n            state_counties = np.unique(df_country_list[(df_country_list[0] == country) & (df_country_list[1] == state)][2].values)\n            for county in state_counties:\n        #        pdb.set_trace()\n                county_text = decode_label('County', county)\n                print(\"Starting county {0}.\".format(county_text))\n                # get train / test data for current state/province/county\n                curr_cs_train, curr_cs_test = get_test_train_for_country_state_county(one_hot_encode, df_train_dd, df_test_dd, country, state, county)\n\n                # train model for each state/province/county combination\n                # predict county's values (if country values not broken out by county/state/province, county == state == country)\n                curr_preds = fit_models_and_train(country, state, county, base_model if use_base_model==True else None, curr_cs_train, curr_cs_test, confidence_levels)\n\n#                preds = [ np.round(pred_array, 5) for pred_array in preds ]  # round predictions to 5 decimal places\n                \n#                pdb.set_trace()\n        \n#                cases_test_ids = curr_cs_test[curr_cs_test['Target']=='ConfirmedCases']['ForecastId']\n#                fatal_test_ids = curr_cs_test[curr_cs_test['Target']=='Fatalities']['ForecastId']\n#                cases_pred_dict = { cases_test_ids : preds[0]}\n#                fatal_pred_dics = { fatal_test_ids : preds[1]}\n\n#                pdb.set_trace()\n\n                for i in range(0, len(curr_preds[0])):\n                    preds[0].append(curr_preds[0][i])\n                    preds[1].append(curr_preds[1][i])\n                \n#                # add results to list of predictions\n#                df_preds = pd.concat([df_preds, cases_pred_dict], axis=0)\n#                df_preds = pd.concat([df_preds, fatal_pred_dict], axis=0)\n\n    #        show_results(base_model)\n        print(\"Country {0} complete.\".format(country))\nelse:\n    print(\"Starting forecasting for all {0} countries.\".format(len(train_country_list)))\n    preds = fit_models_and_train(\"All\", \"All\", \"All\", base_model if use_base_model==True else None, df_train_dd, df_test_dd, confidence_levels)\n#    df_preds = pd.concat([df_preds, preds], axis=0)\n    \nprint(\"All countries complete.\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"all_preds = preds[0][0] + preds[1][0]    #  cases and fatalities\nfor i in range(1, len(preds[0])):         #### check if range 1 is correct start #####\n    all_preds += preds[0][i] + preds[1][i]\n#    preds = preds.fillna(0.0).astype('int32')\n  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df_all_preds = pd.DataFrame(all_preds, columns=['ForecastId','Quantile', 'TargetValue'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df_all_preds['TargetValue'].clip(lower=0, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df_all_preds.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"if not public_leaderboard_end_date is None:\n    date_cutoff_forecast_ids = df_test[(df_test_original.Datetime > pd.to_datetime(public_leaderboard_end_date))].ForecastId\n    df_all_preds.loc[df_all_preds['ForecastId'].isin(date_cutoff_forecast_ids), 'TargetValue'] = 1","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":false},"cell_type":"code","source":"df_all_preds[df_all_preds['ForecastId'] == 31]","execution_count":null,"outputs":[]},{"metadata":{"id":"1N60OSovSM3h"},"cell_type":"markdown","source":"# 7. Analyze results to improve model"},{"metadata":{"id":"nFAcR2CiSM3j","trusted":false},"cell_type":"code","source":"if loop_logic == False:\n    if model_type == ModelType.XGBoost:\n        xgb.plot_importance(base_model)\n        plt.rcParams['figure.figsize'] = [40,40]\n        plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 8. Prepare submission file for Kaggle"},{"metadata":{"trusted":false},"cell_type":"code","source":"prepare_submission(df_all_preds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"colab":{"name":"COVID Forecasting Week 5 w_LightGBM.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"}},"nbformat":4,"nbformat_minor":4}