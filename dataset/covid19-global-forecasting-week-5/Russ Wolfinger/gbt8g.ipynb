{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":false},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os, gc\nimport lightgbm as lgb\nimport xgboost as xgb\nimport catboost as ctb\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nfrom datetime import date, datetime, timedelta\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\nfrom sklearn.neighbors import KNeighborsRegressor, NearestNeighbors\nfrom sklearn.linear_model import Ridge\nfrom scipy.optimize import nnls\npd.set_option('display.max_columns', 10)\npd.set_option('display.max_rows', 10)\nnp.set_printoptions(precision=6, suppress=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# note: clip, dow, lat lon, drop fb and extra, knn on scaled y, reduced cats, mad\nmname = 'gbt8g'\npath = '/kaggle/input/gbt8gx/'\npathk = '/kaggle/input/covid19-global-forecasting-week-5/'\nnhorizon = 31\nnhorizon = 31\nwin = 1\nskip = 0\nkv = [6,11]\nval_scheme = 'forward'\nprev_test = False\nblend = False\nfit_cum = False\ntrain_full = True\nsave_data = False\n\nbooster = ['lgb']\n# booster = ['lgb','xgb','ctb','rdg']\n# booster = ['cas']\n\nblender = []\n# blender = ['nq0j_updated','kaz0z']\n\nquant = [0.05, 0.5, 0.95]\nqlab = ['q05','q50','q95']\nnq = len(quant)\n\n# for nq final day adjustment\n# when validating make this the first validation day\n# for final fitting with nhorizon = 30, make it today\nTODAY = '2020-05-11'\n\nteams = []\n\n# if using updated daily data, also update time-varying external data\n# in COVID-19, covid-19-data, covid-tracking-data, git pull origin master \n# ecdc wget https://opendata.ecdc.europa.eu/covid19/casedistribution/csv\n# weather: https://www.kaggle.com/davidbnn92/weather-data/output?scriptVersionId=31103959\n# google trends: pytrends0d.ipynb\n# data scraped from https://www.worldometers.info/coronavirus/, including past daily snapshots\n# download html for final day (country and us states) at 22:00 UTC and run wm0d.ipynb first","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"code","source":"train = pd.read_csv(pathk+'train.csv')\n\n# helper lists\nynames = ['ConfirmedCases', 'Fatalities']\nyv = ['y0','y1']\nyvs = ['y0_scaled','y1_scaled']\nny = len(ynames)\ncp = ['Country_Region','Province_State','County']\ncpd = cp + ['Date']\n\n# from kaz\n# train[\"key\"] = train[[\"Province_State\",\"Country_Region\"]].apply(lambda row: \\\n#                                                 str(row[0]) + \"_\" + str(row[1]),axis=1)\n\n# fill missing provinces with blanks, must also do this with external data before merging\n# need to fillna so groupby works\ntrain[cp] = train[cp].fillna('')\ntrain = train.sort_values(cpd).reset_index(drop=True)\n\ntrain","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# having trouble with pivot so doing this\nt0 = train.loc[train.Target=='ConfirmedCases'].reset_index(drop=True)\nt0 = t0.rename(mapper={'TargetValue':'ConfirmedCases'}, axis=1)\nt0['Fatalities'] = train.loc[train.Target=='Fatalities','TargetValue'].values\nt0.drop('Target',axis=1,inplace=True)\ntrain = t0\ntrain","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# pivot to create two target columns as in past weeks, allows feature engineering to stay the same\n# remember to multiply weight by 10 when computing pinball loss for fatalities\n# train.loc[train.Target=='Fatalities','Weight'] = train.loc[train.Target=='ConfirmedCases','Weight'].values \n# train","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# t = train.set_index(cpd+['Population'])\n# t.drop('Id',axis=1,inplace=True)\n# t","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# # t = pd.pivot(train, index = cpd+['Population'], columns = 'Target', \\\n# #                            values = 'TargetValue').reset_index()\n# t = pd.pivot(t, columns = 'Target', \\\n#                            values = 'TargetValue')\n# t","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# use previous week test set in order to compare with previous week leaderboard\nif prev_test:\n    test = pd.read_csv('../'+pw+'/test.csv')\n    ss = pd.read_csv('../'+pw+'/submission.csv')\nelse:\n    test = pd.read_csv(pathk+'test.csv')\n    ss = pd.read_csv(pathk+'submission.csv')\n\n# from kaz\n# test[\"key\"] = test[[\"Province_State\",\"Country_Region\"]].apply(lambda row: \\\n#                                             str(row[0]) + \"_\" + str(row[1]),axis=1)\n\ntest[cp] = test[cp].fillna('')\ntest","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# having trouble with pivot so doing this\nt1 = test.loc[test.Target=='ConfirmedCases'].reset_index(drop=True)\nt1.drop('Target',axis=1,inplace=True)\ntest = t1\ntest","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# tmax and dmax are the last day of training\ntmax = train.Date.max()\ndmax = datetime.strptime(tmax,'%Y-%m-%d').date()\nprint(tmax, dmax)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"fmax = test.Date.max()\nfdate = datetime.strptime(fmax,'%Y-%m-%d').date()\nfdate","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"tmin = train.Date.min()\nfmin = test.Date.min()\ntmin, fmin","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"dmin = datetime.strptime(tmin,'%Y-%m-%d').date()\nprint(dmin)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# prepare for concat\ntrain = train.merge(test[cpd+['ForecastId']], how='left', on=cpd)\ntrain['ForecastId'] = train['ForecastId'].fillna(0).astype(int)\n\ntest['Id'] = test.ForecastId + train.Id.max()\ntest['ConfirmedCases'] = np.nan\ntest['Fatalities'] = np.nan\n\n# storage for predictions\nfor i in range(ny):\n    for q in range(nq):\n        train[yv[i]+'_pred_'+qlab[q]] = np.nan\n        # use zeros here instead of nans so monotonic adjustment fills final dates if necessary\n        test[yv[i]+'_pred_'+qlab[q]] = 0.0","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# concat non-overlapping part of test to train for feature engineering\nd = pd.concat([train,test[test.Date > train.Date.max()]],sort=True).reset_index(drop=True)\nd","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# day counter\nd['day'] = pd.to_datetime(d['Date']).dt.dayofyear\nd['day'] -= d['day'].min()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# override weights, adjust US due to roll-ups\n# d['Weight'] = 0.1\n# d.loc[d.Country_Region=='US','Weight'] = 0.1/3.0","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# reduce data to US county only\n# d = d.loc[(d.Country_Region=='US') & (d.County!='')]\n# d = d.loc[(d.Country_Region!='US') | (d.County=='')]\n# d","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"d['Date'].value_counts().std()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# fill missing province with blank, must also do this with external data before merging\nd[cp] = d[cp].fillna('')\n\n# create single location variable\nd['Loc'] = d['Country_Region'] + ' ' + d['Province_State'] + ' ' + d['County']\nd.loc[:,'Loc'] = d['Loc'].str.strip()\nd['Loc'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# previous test set, drop new regions in order to compare with previous week leaderboard\nif prev_test:\n    test2 = pd.read_csv('../'+pw+'/test.csv')\n    test2[cp] = test2[cp].fillna('')\n    test2 = test2.drop(['ForecastId','Date'], axis=1).drop_duplicates()\n    test2\n    d = d.merge(test2, how='inner', on=cp)\n    d.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# sort by location, date\nd = d.sort_values(['Loc','Date']).reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"ynames","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# target creation and cleaning\nyvc = []\n# qs = [0.95]\nfor i in range(ny):\n    v = yv[i]\n    d[v] = d[ynames[i]]\n    # crude fix:  replace negative targets with zeros\n    # should prolly use negatives to reduce previous positives, see Spain and France\n    d.loc[d[v] < 0, v] = 0\n    # create log1p cum counts as in weeks 1-4\n    vc = v+'cum'\n    d[vc] = d.groupby('Loc')[v].cumsum()\n    # d[vc] = np.log1p(d[vc])\n    yvc.append(vc)\n#     da = d.groupby(['Loc']).agg({v:{ \"min\" , \"max\", \"median\"} } )\n#     da.columns = ['_'.join(c) for c in da.columns]\n#     da = da.reset_index()\n#     d = d.merge(da, how='left', on='Loc')\n#     d[v+'_min'] = d.groupby(['Loc'])[v].apply(lambda x: x.min())\n#     d[v+'_med'] = d.groupby(['Loc'])[v].apply(lambda x: x.median())\n#     d[v+'_max'] = d.groupby(['Loc'])[v].apply(lambda x: x.max())\n#     d[v+'_range'] = d[v+'_max'] - d[v+'_min'] \n    # robust center\n    # d[v] = d[v] - d[v+'_median']\n    # robust center and scale \n    # d[v] = (d[v] - d[v+'_median'])/(1e-8 + d[v+'_range']) \n            \n# enforce monotonicity, roughly cleans some data errors\n# d[yv] = d.groupby(cp)[yv].cummax()\n\nprint(d[yv+yvc].describe())","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# find max of smooth and compute scale factors\ndef ewma(x, com):\n    return pd.Series.ewm(x, com=com).mean()\n\nw = 7\nws = '_w'+str(w)\n\nfor i in range(ny):\n    yw = yv[i]+ws\n    d[yw] = d.groupby('Loc')[yv[i]].transform(lambda x: ewma(x,w))\n    # print(d[yw].describe())\n\n    d1 = d.sort_values(['Loc', yw,'Date'], ascending=[True,False,True])\n    # print(d1.head())\n\n    d2 = d1.groupby('Loc').first()\n    # print(d2.head())\n\n    d2 = d2[['day',yw]].reset_index()\n    d2.columns = ['Loc','day_of_max'+str(i),  yw+'_max']\n    d2\n\n    d = d.merge(d2, how='left', on='Loc')\n    # print(d.shape)\n\n    d[yw+'_d1'] = d.groupby('Loc')[yw].transform(lambda x: x.diff(1))\n    d1 = d[d.day==d['day_of_max'+str(i)]]\n    d1 = d1.sort_values([yw+'_d1'], ascending=False)\n    # print(d1.head(n=10))\n\n    domax = d1['day'].max()\n    print(domax)\n\n    d['scale_factor'+str(i)] = 1.0 - np.maximum(10.0 - (domax - d['day_of_max'+str(i)]), 0.0)/20.0\n    print(d['scale_factor'+str(i)].describe())\n    print(d.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# d.groupby('Loc').agg({})\nd.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"d.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# d.groupby('Loc')['y0'].quantile(0.95)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# d.loc[d.Loc=='US',['Date','y0','y0cum','y1','y1cum']][95:105]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# d.loc[d.Loc=='Spain',['Date','y0','y0cum','y1','y1cum']][95:105]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"d['dow'] = pd.to_datetime(d.Date).dt.dayofweek","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"d['dow'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# d.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# merge fbprophet features\nfbf = []\n# gd = ['Loc','Date']\n# for i in range(ny):\n#     fb = pd.read_parquet(path+'fbprophet0d'+str(i)+'.pq')\n#     fb.drop([f for f in fb.columns if f.startswith('mult')], axis=1, inplace=True)\n#     fbf = fbf + [f for f in fb.columns if f not in gd]\n#     fb.loc[:,'Date'] = fb.Date.astype(str)\n#     fb = fb.sort_values(gd).reset_index(drop=True)\n#     d = d.merge(fb, how='left', on=gd)\n#     print(d.shape)\n# print(fbf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# # data scraped from https://www.worldometers.info/coronavirus/, including past daily snapshots\n# # download html for final day (country and us states) at 22:00 UTC and run wm0d.ipynb first\n# wmf = []\n# wm = pd.read_csv('wmc.csv')\n# wm[cp] = wm[cp].fillna('')\n# # 12 new features, all log1p transformed, must be lagged\n# wmf = [c for c in wm.columns if c not in cpd]\n\n# # since wm leads by a day, shift the date to make it contemporaneous\n# wmax = wm.Date.max()\n# wmax = datetime.strptime(wmax,'%Y-%m-%d').date()\n# woff = (dmax - wmax).days\n# print(dmax, wmax, woff)\n# wm1 = wm.copy()\n# wm1['Date'] = (pd.to_datetime(wm1.Date) + timedelta(woff)).dt.strftime('%Y-%m-%d')\n\n# wm1.Date.value_counts()[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# wm1['Date'].max()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# d = d.merge(wm1, how='left', on=cpd)\n# print(d.shape)\n# d[wmf].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# # google trends\n# gt = pd.read_csv(path+'google_trends.csv')\n# gt[cp] = gt[cp].fillna('')\n# gt","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# # since trends data lags behind a day or two, shift the date to make it contemporaneous\n# gmax = gt.Date.max()\n# gmax = datetime.strptime(gmax,'%Y-%m-%d').date()\n# goff = (dmax - gmax).days\n# print(dmax, gmax, goff)\n# gt['Date'] = (pd.to_datetime(gt.Date) + timedelta(goff)).dt.strftime('%Y-%m-%d')\n# gt['google_covid'] = gt['coronavirus'] + gt['covid-19'] + gt['covid19']\n# gt.drop(['coronavirus','covid-19','covid19'], axis=1, inplace=True)\n# google = ['google_covid']\n# gt","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# d = d.merge(gt, how='left', on=['Country_Region','Province_State','Date'])\n# d","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# d['google_covid'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# # merge country info\n# country = pd.read_csv(path+'covid19countryinfo2.csv')\n# # country[\"pop\"] = country[\"pop\"].str.replace(\",\",\"\").astype(float)\n# country","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# country.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# # first merge by country\n# d = d.merge(country.loc[country.medianage.notnull(),['country','pop','testpop','medianage']],\n#             how='left', left_on='Country_Region', right_on='country')\n# d","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# # then merge by province\n# c1 = country.loc[country.medianage.isnull(),['country','pop','testpop']]\n# print(c1.shape)\n# c1.columns = ['Province_State','pop1','testpop1']\n# # d.update(c1)\n# d = d.merge(c1,how='left',on='Province_State')\n# d.loc[d.pop1.notnull(),'pop'] = d.loc[d.pop1.notnull(),'pop1']\n# d.loc[d.testpop1.notnull(),'testpop'] = d.loc[d.testpop1.notnull(),'testpop1']\n# d.drop(['pop1','testpop1'], axis=1, inplace=True)\n# print(d.shape)\n# print(d.loc[(d.Date=='2020-03-25') & (d['Province_State']=='New York')])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# covid tracking data\n# testing data time series, us states only, would love to have this for all countries\nct = pd.read_csv(path+'states_daily_4pm_et.csv')\nsi = pd.read_csv(path+'states_info.csv')\nsi = si.rename(columns={'name':'Province_State'})\nct = ct.merge(si[['state','Province_State']], how='left', on='state')\nct['Date'] = ct['date'].apply(str).transform(lambda x: '-'.join([x[:4], x[4:6], x[6:]]))\nct.loc[ct.Province_State=='US Virgin Islands','Province_State'] = 'Virgin Islands'\nct.loc[ct.Province_State=='District Of Columbia','Province_State'] = 'District of Columbia'\npd.set_option('display.max_rows', 20)\nct\n# ct = ct['Date','state','total']","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"ckeep = ['positiveIncrease','negativeIncrease','totalTestResultsIncrease']\n# for c in ckeep: ct[c] = np.log1p(ct[c])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"d = d.merge(ct[['Province_State','Date']+ckeep], how='left',\n            on=['Province_State','Date'])\nd","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# covid tracking data\n# testing data time series, us rollup\nct = pd.read_csv(path+'us_daily.csv')\nct['Date'] = ct['date'].apply(str).transform(lambda x: '-'.join([x[:4], x[4:6], x[6:]]))\nct = ct[['Date']+ckeep]\nckeep1 = [c+'1' for c in ckeep]\nct.columns = ['Date'] + ckeep1\nct['Country_Region'] = 'US'\nct['Province_State'] = ''\nct['County'] = ''\nct","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# update ckeep for us rollup\nd = d.merge(ct, how='left', on=cpd)\nd.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# update\nd.loc[d.Loc=='US',ckeep] = d.loc[d.Loc=='US',ckeep1]\nd.drop(ckeep1, axis=1, inplace=True)\nd.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# # approximate county tracking values using ratios from training\n# cs = ['Country_Region','Province_State']\n# d['csum'] = d.groupby(cs)['ConfirmedCases'].transform(lambda x:x.sum())\n# d['fsum'] = d.groupby(cs)['Fatalities'].transform(lambda x:x.sum())\n# d['cfrac'] = d.ConfirmedCases / d.csum\n# d['ffrac'] = d.Fatalities / d.fsum \n# for c in ckeep: d[c] *= d.cfrac\n# ckeep = ckeep + ['cfrac','ffrac']","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# # weather data from from davide bonine\n# w = pd.read_csv(path+'training_data_with_weather_info_week_4.csv')\n# w.drop(['Id','ConfirmedCases','Fatalities','country+province','day_from_jan_first'], axis=1, inplace=True)\n# w[cp] = w[cp].fillna('')\n# wf = list(w.columns[5:])\n# w","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# w.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# # replace values\n# w['ah'] = w['ah'].replace(to_replace={np.inf:np.nan})\n# w['wdsp'] = w['wdsp'].replace(to_replace={999.9:np.nan})\n# w['prcp'] = w['prcp'].replace(to_replace={99.99:np.nan})\n# w.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# w[['Country_Region','Province_State']].nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# w[['Country_Region','Province_State']].drop_duplicates().shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# # since weather data may lag behind a day or two, adjust the date to make it contemporaneous\n# wmax = w.Date.max()\n# wmax = datetime.strptime(wmax,'%Y-%m-%d').date()\n# woff = (dmax - wmax).days\n# print(dmax, wmax, woff)\n# w['Date'] = (pd.to_datetime(w.Date) + timedelta(woff)).dt.strftime('%Y-%m-%d')\n# w","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# merge Lat and Long for all times and the time-varying weather data based on date\ngeo = pd.read_csv(path+'geo_all.csv')\ngeo.loc[:,cp] = geo[cp].fillna('') \ngeo","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"d = d.merge(geo, how='left', on=cp)\nd.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# # combine ecdc and nytimes data as extra y0 and y1\n# ecdc = pd.read_csv(path+'ecdc.csv', encoding = 'latin')\n# ecdc","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# # https://opendata.ecdc.europa.eu/covid19/casedistribution/csv\n# ecdc['Date'] = pd.to_datetime(ecdc[['year','month','day']]).dt.strftime('%Y-%m-%d')\n# ecdc = ecdc.rename(mapper={'countriesAndTerritories':'Country_Region'}, axis=1)\n# ecdc['Country_Region'] = ecdc['Country_Region'].replace('_',' ',regex=True)\n# ecdc['Province_State'] = ''\n# ecdc['County'] = ''\n\n# ecdc['extra_y0'] = ecdc.cases\n# ecdc['extra_y1'] = ecdc.deaths\n\n# # ecdc['cc'] = ecdc.groupby(cp)['cases'].cummax()\n# # ecdc['extra_y0'] = np.log1p(ecdc.cc)\n# # ecdc['cd'] = ecdc.groupby(cp)['deaths'].cummax()\n# # ecdc['extra_y1'] = np.log1p(ecdc.cd)\n\n# ecdc = ecdc[cpd + ['extra_y0','extra_y1']]\n# ecdc[::63]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# ecdc = ecdc[(ecdc.Date >= '2020-01-22')]\n# ecdc","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# # new york times data\n# # https://github.com/nytimes/covid-19-data\n# # full us roll-up\n# n0 = pd.read_csv(path+'covid-19-data/us.csv')\n# n0['extra_y0'] = n0.cases.diff(1)\n# n0['extra_y1'] = n0.deaths.diff(1)\n# n0['Country_Region'] = 'US'\n# n0['Province_State'] = ''\n# n0['County'] = ''\n# n0 = n0.rename(mapper={'date':'Date'},axis=1)\n# n0.drop(['cases','deaths'],axis=1,inplace=True)\n# n0","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# # new york times data\n# # https://github.com/nytimes/covid-19-data\n# # us state-level\n# n1 = pd.read_csv(path+'covid-19-data/us-states.csv')\n# n1 = n1.sort_values(['state','date']).reset_index(drop=True)\n# n1['extra_y0'] = n1.groupby('state')['cases'].diff(1)\n# n1['extra_y1'] = n1.groupby('state')['deaths'].diff(1)\n# n1['Country_Region'] = 'US'\n# n1['County'] = ''\n# n1 = n1.rename(mapper={'date':'Date','state':'Province_State'},axis=1)\n# n1.drop(['fips','cases','deaths'],axis=1,inplace=True)\n# n1","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# # new york times data\n# # https://github.com/nytimes/covid-19-data\n# # us county-level\n# n2 = pd.read_csv(path+'covid-19-data/us-counties.csv')\n# n2 = n2.sort_values(['state','county','date']).reset_index(drop=True)\n# n2['extra_y0'] = n2.groupby(['state','county'])['cases'].diff(1)\n# n2['extra_y1'] = n2.groupby(['state','county'])['deaths'].diff(1)\n# n2['Country_Region'] = 'US'\n# n2 = n2.rename(mapper={'date':'Date','state':'Province_State','county':'County'},axis=1)\n# n2.drop(['fips','cases','deaths'],axis=1,inplace=True)\n# # fix for new york city, 4 out of 5 boroughs are left missing\n# n2.loc[n2.County=='New York City','County'] = 'New York'\n# n2.loc[n2.County=='New York']","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# n2.sort_values('extra_y0', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# extra = pd.concat([ecdc,n0,n1,n2], sort=True)\n# extra","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# d = d.merge(extra, how='left', on=cpd)\n# d","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# # enforce monotonicity\n# d = d.sort_values(['Loc','Date']).reset_index(drop=True)\n# for y in yv:\n#     ey = 'extra_'+y\n#     d[ey] = d[ey].fillna(0.)\n#     d[ey] = d.groupby('Loc')[ey].cummax()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# d[['y0','y1','extra_y0','extra_y1']].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# # impute us state data prior to march 10\n# for i in range(ny):\n#     ei = 'extra_'+yv[i]\n#     qm = (d.Country_Region == 'US') & (d.Date < '2020-03-10') & (d[ei].notnull())\n#     print(i,sum(qm))\n#     d.loc[qm,yv[i]] = d.loc[qm,ei]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# d[['y0','y1']].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# plt.plot(d.loc[d.Province_State=='New York','y0'])\ndq = (d.Country_Region=='US') & (d.Province_State=='') & (d.County=='')\nplt.plot(d.loc[dq,['y0','Date']].set_index('Date'))\n# plt.plot(d.loc[dq,['extra_y0','Date']].set_index('Date'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"d.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# log rates\nd['rate0'] = np.log1p(d.y0) - np.log(d['Population'])\nd['rate1'] = np.log1p(d.y1) - np.log(d['Population'])\nd[['rate0','rate1']].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# # recovered data from hopkins, https://github.com/CSSEGISandData/COVID-19\n# recovered = pd.read_csv(path+'COVID-19/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_recovered_global.csv')\n# recovered = recovered.rename(mapper={'Country/Region':'Country_Region','Province/State':'Province_State'}, axis=1)\n# recovered[cp] = recovered[cp].fillna('')\n# recovered = recovered.drop(['Lat','Long'], axis=1)\n# recovered","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# # replace US row with identical rows for every US state\n# usp = d.loc[d.Country_Region=='US','Province_State'].unique()\n# print(usp, len(usp))\n# rus = recovered[recovered.Country_Region=='US']\n# rus","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# rus = rus.reindex(np.repeat(rus.index.values,len(usp)))\n# rus.loc[:,'Province_State'] = usp\n# rus","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# recovered =  recovered[recovered.Country_Region!='US']\n# recovered = pd.concat([recovered,rus]).reset_index(drop=True)\n# recovered","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# # melt and merge\n# rm = pd.melt(recovered, id_vars=cp, var_name='d', value_name='recov')\n# rm","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# rm['Date'] = pd.to_datetime(rm.d)\n# rm.drop('d',axis=1,inplace=True)\n# rm['Date'] = rm['Date'].dt.strftime('%Y-%m-%d')\n# rm","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# d = d.merge(rm, how='left', on=['Country_Region','Province_State','Date'])\n# d","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# d['recov'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# # approximate US state recovery via proportion of confirmed cases\n# d['ccsum'] = d.groupby(['Country_Region','Date'])['ConfirmedCases'].transform(lambda x: x.sum())\n# d.loc[d.Country_Region=='US','recov'] = d.loc[d.Country_Region=='US','recov'] * \\\n#                                         d.loc[d.Country_Region=='US','ConfirmedCases'] / \\\n#                                         (d.loc[d.Country_Region=='US','ccsum'] + 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# d.loc[:,'recov'] = np.log1p(d.recov)\n# # d.loc[:,'recov'] = d['recov'].fillna(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# # enforce monotonicity\n# d = d.sort_values(['Loc','Date']).reset_index(drop=True)\n# d['recov'] = d['recov'].fillna(0.)\n# d['recov'] = d.groupby('Loc')['recov'].cummax()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# d.loc[d.Province_State=='North Carolina','recov'][45:55]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"d = d.sort_values(['Loc','Date']).reset_index(drop=True)\nd.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"d['Date'] = pd.to_datetime(d['Date'])\nd['Date'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# days since beginning\n# basedate = train['Date'].min()\n# train['dint'] = train.apply(lambda x: (x.name.to_datetime() - basedate).days, axis=1)\nd['dint'] = (d['Date'] - d['Date'].min()).dt.days\nd['dint'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"d.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# reference days since exp(j)th occurrence\nfor i in range(ny):\n    \n    for j in range(3):\n\n        ij = str(i)+'_'+str(j)\n        \n        cut = 2**j if i==0 else j\n        \n        qd1 = (d[yvc[i]] > cut) & (d[yvc[i]].notnull())\n        d1 = d.loc[qd1,['Loc','dint']]\n        # d1.shape\n        # d1.head()\n\n        # get min for each location\n        d1['dmin'] = d1.groupby('Loc')['dint'].transform(lambda x: x.min())\n        # dintmax = d1['dint'].max()\n        # print(i,j,'dintmax',dintmax)\n        # d1.head()\n\n        d1.drop('dint',axis=1,inplace=True)\n        d1 = d1.drop_duplicates()\n        d = d.merge(d1,how='left',on=['Loc'])\n \n        # if dmin is missing then the series had no occurrences in the training set\n        # go ahead and assume there will be one at the beginning of the test period\n        # the average time between first occurrence and first death is 14 days\n        # if j==0: d[dmi] = d[dmi].fillna(dintmax + 1 + i*14)\n\n        # ref day is days since dmin, must clip at zero to avoid leakage\n        d['ref_day'+ij] = np.clip(d.dint - d.dmin, 0, None)\n        d['ref_day'+ij] = d['ref_day'+ij].fillna(0)\n        d.drop('dmin',axis=1,inplace=True)\n\n        # asymptotic curve may bin differently\n        d['recip_day'+ij] = 1 / (1 + (1 + d['ref_day'+ij])**(-1.0))\n    \n\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"d['dint'].value_counts().std()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"d[[f for f in d.columns if f.startswith('ref_day')]].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"d[['scale_factor0','y0_w7_max']].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# diffs and rolling means\n# note lags are taken dynamically at run time\ne = 1\n# r = 5\nr = 7\nwr = [2]\n# wr = [2,5]\ngb = d.groupby('Loc')\n\nfor i in range(ny):\n    yi = 'y'+str(i)\n    yis = yi+'_scaled'\n    dd = '_d'+str(e)\n    rr = '_r'+str(r)\n    \n    for w in wr:\n        ww = '_w'+str(w)\n        d[yi+ww] = gb[yi].transform(lambda x: ewma(x,w))\n        d['rate'+str(i)+ww] = gb['rate'+str(i)].transform(lambda x: ewma(x,w))\n        \n    # scale target by smooth max + scale factor penalty for recent max\n    d[yis] = d['scale_factor'+str(i)] * d[yi+ww] / (1e-4 + d[yi+'_w7_max'])\n    print(d[yis].describe())\n    \n    for j in range(7):\n        d[yi+'_d'+str(1+j)] = gb[yi].transform(lambda x: x.diff(1+j))\n        d[yi+'_l'+str(1+j)] = gb[yi].transform(lambda x: x.shift(1+j))\n        d[yis+'_d'+str(1+j)] = gb[yis].transform(lambda x: x.diff(1+j))\n        d[yis+'_l'+str(1+j)] = gb[yis].transform(lambda x: x.shift(1+j))\n    \n    d[yi+rr] = gb[yi].transform(lambda x: x.rolling(r).mean())\n    d[yis+rr] = gb[yis].transform(lambda x: x.rolling(r).mean())\n        \n    d['rate'+str(i)+dd] = gb['rate'+str(i)].transform(lambda x: x.diff(e))\n    d['rate'+str(i)+rr] = gb['rate'+str(i)].transform(lambda x: x.rolling(r).mean())\n    \n#     d['extra_y'+str(i)+dd] = gb['extra_y'+str(i)].transform(lambda x: x.diff(e))\n#     d['extra_y'+str(i)+rr] = gb['extra_y'+str(i)].transform(lambda x: x.rolling(r).mean())\n#     d['extra_y'+str(i)+ww] = gb['extra_y'+str(i)].transform(lambda x: ewma(x,w))\n        \n# vlist = ['recov'] + google + wf\n\n# for v in vlist:\n#     d[v+dd] = gb[v].transform(lambda x: x.diff(e))\n#     d[v+rr] = gb[v].transform(lambda x: x.rolling(r).mean())\n#     d[v+ww] = gb[v].transform(lambda x: ewma(x,w))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"d['y0'+ww].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# compute nearest neighbors\nregions = d[['Loc','lat','lon']].drop_duplicates('Loc').reset_index(drop=True)\nregions","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# regions.to_csv('regions.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# knn max features\nk = kv[0]\nnn = NearestNeighbors(k)\nnn.fit(regions[['lat','lon']])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# first matrix is distances, second indices to nearest neighbors including self\n# note two cruise ships have identical lat, lon values\nknn = nn.kneighbors(regions[['lat','lon']])\nknn","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"ns = d['Loc'].nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# time series matrix\nky = d['y0'].values.reshape(ns,-1)\nprint(ky.shape)\n\nprint(ky[0])\n\n# use knn indices to create neighbors\nknny = ky[knn[1]]\nprint(knny.shape)\n\nknny = knny.transpose((0,2,1)).reshape(-1,k)\nprint(knny.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# knn max features\nnk = len(kv)\nkp = []\nkd = []\nns = regions.shape[0]\nfor k in kv:\n    nn = NearestNeighbors(k)\n    nn.fit(regions[['lat','lon']])\n    knn = nn.kneighbors(regions[['lat','lon']])\n    kp.append('knn'+str(k)+'_')\n    kd.append('kd'+str(k)+'_')\n    for i in range(ny):\n        yis = 'y'+str(i)+'_scaled'\n        kc = kp[-1]+yis\n        # time series matrix\n        ky = d[yis].values.reshape(ns,-1)\n        # use knn indices to create neighbor matrix\n        km = ky[knn[1]].transpose((0,2,1)).reshape(-1,k)\n        \n        # take maximum value over all neighbors to approximate spreading\n        d[kc] = np.amax(km, axis=1)\n        print(d[kc].describe())\n        print()\n        \n        # distance to max\n        kc = kd[-1]+yis\n        ki = np.argmax(km, axis=1).reshape(ns,-1)\n        kw = np.zeros_like(ki).astype(float)\n        # inefficient indexing, surely some way to do it faster\n        for j in range(ns): \n            kw[j] = knn[0][j,ki[j]]\n        d[kc] = kw.flatten()\n        print(d[kc].describe())\n        print()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# correlations for knn features\ncols = []\nfor i in range(ny):\n    yi = yv[i]\n    yis = yi+'_scaled'\n    cols.append(yi)\n    cols.append(yis)\n    for k in kp:\n        cols.append(k+yis)\nd.loc[:,cols].corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# smooth knn features\nfor i in range(ny):\n    yi = 'y'+str(i)\n    yis = yi+'_scaled'\n    dd = '_d'+str(e)\n    rr = '_r'+str(r)\n    \n    for k in kp:\n        d[k+yis+dd] = gb[k+yis].transform(lambda x: x.diff(e))\n        d[k+yis+rr] = gb[k+yis].transform(lambda x: x.rolling(r).mean())\n        d[k+yis+ww] = gb[k+yis].transform(lambda x: ewma(x,w))\n\n    for k in kd:\n        d[k+yis+dd] = gb[k+yis].transform(lambda x: x.diff(e))\n        d[k+yis+rr] = gb[k+yis].transform(lambda x: x.rolling(r).mean())\n        d[k+yis+ww] = gb[k+yis].transform(lambda x: ewma(x,w))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# remove US data before 2020-03-10, instead might should impute from ny times\n# must do this after knn because it assumes balanced data\n# note also 4/5 NY boroughs are all 0s:  Bronx, Queens, Kings, Richmond\nd = d[(d.Country_Region!='US') | (d.Date >= '2020-03-10')]\n# d = d[d.Date >= '2020-03-10']\nprint(d.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# drop leading zeros, must do after knn\nqr = (d.ref_day0_0 > 0) | (d.Date >= '2020-04-06')\nd = d[qr]\nprint(d.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# final sort before training\nd = d.sort_values(['Loc','dint']).reset_index(drop=True)\nd.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# # save data'\n# if save_data:\n#     fname = mname + '_data.csv'\n#     d.to_csv(fname, index=False)\n#     print(fname, d.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# # range of dates for training\n# # dates = d[~d.y0.isnull()]['Date'].drop_duplicates()\n# dates = d[d.y0.notnull()]['Date'].drop_duplicates()\n# dates","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# initial continuous and categorical features\n# dogs = tfeats\n# ref_day0_0 is no longer leaky since every location has at least one confirmed case\n# dogs = ['ref_day0_0']\ndogs = ['Population','Weight','lat','lon']\n# dogs = ['Population','Weight','dow']\n# cats = ['Loc','dow','continent']\n# cats = ['dow','continent']\ncats = ['Country_Region','Province_State','dow','continent']\n# cats = []\nprint(dogs, len(dogs))\nprint(cats, len(cats))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# one-hot encode categorical features\nohef = []\nfor i,c in enumerate(cats):\n    print(c, d[c].nunique())\n    ohe = pd.get_dummies(d[c], prefix=c)\n    ohec = [f.translate({ord(c): \"_\" for c in \" !@#$%^&*()[]{};:,./<>?\\|`~-=_+\"}) for f in list(ohe.columns)]\n    ohe.columns = ohec\n    d = pd.concat([d,ohe],axis=1)\n    ohef = ohef + ohec","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# d['Loc_US_North_Carolina'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# d['Loc_US_Colorado'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# must start cas server from gevmlax02 before running this cell\n# ssh rdcgrd001 /opt/vb025/laxnd/TKGrid/bin/caslaunch stat -mode mpp -cfg /u/sasrdw/config.lua\nif 'cas' in booster:\n    from swat import *\n    s = CAS('rdcgrd001.unx.sas.com', 16695)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# boosting hyperparameters\nparams = {}\n\n# # from vopani\n# SEED = 345\n# LGB_PARAMS = {\"objective\": \"regression\",\n#               \"num_leaves\": 5,\n#               \"learning_rate\": 0.013,\n#               \"bagging_fraction\": 0.91,\n#               \"feature_fraction\": 0.81,\n#               \"reg_alpha\": 0.13,\n#               \"reg_lambda\": 0.13,\n#               \"metric\": \"rmse\",\n#               \"seed\": SEED\n#              }\n\n# from oscii\nSEED = 42\nLGB_PARAMS = {\n    'objective': 'quantile',\n    # 'metric': 'rmse',\n    'metric': 'quantile',\n    'alpha': 0.5,\n    'num_leaves': 8,\n    'min_data_in_leaf': 5,  # 42,\n    'max_depth': 8,\n    'learning_rate': 0.02,\n    'boosting': 'gbdt',\n    'bagging_freq': 5,  # 5\n    'bagging_fraction': 0.8,  # 0.5,\n    'feature_fraction': 0.8201,\n    'bagging_seed': SEED,\n    'reg_alpha': 1,  # 1.728910519108444,\n    'reg_lambda': 4.9847051755586085,\n    'random_state': SEED,\n    'min_gain_to_split': 0.02,  # 0.01077313523861969,\n    'min_child_weight': 5,  # 19.428902804238373,\n    # 'num_threads': 6,\n    # 'device_type': 'gpu'\n}\n\nparams[('lgb','y0')] = LGB_PARAMS\nparams[('lgb','y1')] = LGB_PARAMS\n# params[('lgb','y0')] = {'lambda_l2': 1.9079933811271934, 'max_depth': 5}\n# params[('lgb','y1')] = {'lambda_l2': 1.690407455211948, 'max_depth': 3}\nparams[('xgb','y0')] = {'lambda_l2': 1.9079933811271934, 'max_depth': 5}\nparams[('xgb','y1')] = {'lambda_l2': 1.690407455211948, 'max_depth': 3}\nparams[('ctb','y0')] = {'l2_leaf_reg': 1.9079933811271934, 'max_depth': 5}\nparams[('ctb','y1')] = {'l2_leaf_reg': 1.690407455211948, 'max_depth': 3}\n\n# fix number of estimators to avoid overfitting with early stopping\n# index by target and quantile, based on previous early stopping results in iallv\nlgb_nest = np.zeros((2,3)).astype(int)\nlgb_nest[0,0] = 1000\nlgb_nest[1,0] = 1000\nlgb_nest[0,1] = 400\nlgb_nest[1,1] = 475\nlgb_nest[0,2] = 315\nlgb_nest[1,2] = 540","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# booster = ['rdg','lgb','xgb','ctb']\n# booster = ['lgb','xgb']","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# weighted quantile loss, used to compute total pinball loss\ndef wqloss(y,p,q,w):\n    e = y-p\n    a,s = np.average(np.maximum(q*e, (q-1)*e), weights=w, returned=True)\n    # divide by length instead of sum of weights\n    loss = a*s/len(y)\n    return loss\n    \n# def quantile_loss(q, y_p, y):\n#     e = y_p-y\n#     return tf.keras.backend.mean(tf.keras.backend.maximum(q*e, (q-1)*e))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# main training and validation loop\nnb = len(booster)\nnls = np.zeros((nhorizon//win,ny,nq,nb))\nrallv = np.zeros((nhorizon//win,ny,nq,nb))\niallv = np.zeros((nhorizon//win,ny,nq,nb)).astype(int)\nyallv = []\npallv = []\nimps = []\nMAD_FACTOR = 0.5\n \n# loop over horizons\nfor horizon in range(win,nhorizon+1,win):\n    \n    print()\n    gc.collect()\n    \n    hs = str(horizon)\n    if horizon < 10: hs = '0' + hs\n    z = horizon // win - 1\n    \n    # build lists of features\n    lags = []\n    # must lag reference days to avoid validation leakage\n    for i in range(ny):\n        for j in range(3):\n            # omit ref_day0_0 since it is no longer leaky\n            # if (i > 0) | (j > 0): lags.append('ref_day'+str(i)+'_'+str(j))\n            lags.append('ref_day'+str(i)+'_'+str(j))\n            \n    # lag all time-varying features\n    for i in range(ny):\n        yi = 'y'+str(i)\n        yis = yi + '_scaled'\n        lags.append(yi)\n        lags.append(yis)\n#         lags.append('extra_'+yi)\n        lags.append('rate'+str(i))\n        for j in range(5):\n            lags.append(yi+'_d'+str(1+j))\n            lags.append(yi+'_l'+str(1+j))\n            lags.append(yis+'_d'+str(1+j))\n            lags.append(yis+'_l'+str(1+j))\n#         lags.append('extra_'+yi+dd)\n        lags.append('rate'+str(i)+dd)\n        lags.append(yi+rr)\n        lags.append(yis+rr)\n#         lags.append('extra_'+yi+rr)\n        lags.append('rate'+str(i)+rr)\n    \n        for w in wr:\n            lags.append(yi+ww)\n            lags.append('rate'+str(i)+ww)\n        \n#         lags.append('extra_'+yi+ww)\n    \n        for k in kp:\n            lags.append(k+yis)\n            lags.append(k+yis+dd)\n            lags.append(k+yis+rr)\n            lags.append(k+yis+ww)\n        for k in kd:\n            lags.append(k+yis)\n            lags.append(k+yis+dd)\n            lags.append(k+yis+rr)\n            lags.append(k+yis+ww)\n       \n#     lags.append('recov')\n    \n#     lags = lags + wmf + google + wf + ckeep\n\n    lags = lags + fbf + ckeep\n    \n#     cinfo = ['pop', 'tests', 'testpop', 'density', 'medianage',\n#        'urbanpop', 'hospibed', 'smokers']\n\n\n    cinfo0 = []\n    cinfo1 = []\n    \n#     cinfo0 = ['testpop']\n#     cinfo1 = ['testpop','medianage']\n    \n    f0 = dogs + lags + cinfo0 + ohef\n    f1 = dogs + lags + cinfo1 + ohef\n    \n    # remove some features based on validation experiments\n#     f0 = [f for f in f0 if not f.startswith('knn11') and not f.startswith('kd') \\\n#          and not f.startswith('rate') and not f.endswith(dd) and not f.endswith(rr)]\n\n#     f0 = [f for f in f0 if not f.startswith('knn11') and not f.startswith('kd11')]\n#     f1 = [f for f in f1 if not f.startswith('knn6') and not f.startswith('kd6')]\n    \n    # remove any duplicates\n    # f0 = list(set(f0))\n    # f1 = list(set(f1))\n    \n    features = []\n    features.append(f0)\n    features.append(f1)\n    \n    nf = []\n    for i in range(ny):\n        nf.append(len(features[i]))\n        # print(nf[i], features[i][:10])\n     \n    if val_scheme == 'forward':\n        # ddate is the last day of training for validation\n        # training data stays constant\n        ddate = dmax - timedelta(days=nhorizon)\n        qtrain = d['Date'] <= ddate.isoformat()\n        # validation day moves forward\n        vdate0 = ddate + timedelta(days=horizon-win)\n        vdate = ddate + timedelta(days=horizon)\n        qval = (vdate0.isoformat() < d['Date']) & (d['Date'] <= vdate.isoformat())\n        # lag day is last day of training\n        ddate0 = ddate - timedelta(days=win)\n        qvallag = (ddate0.isoformat() < d['Date']) & (d['Date'] <= ddate.isoformat())\n        # for saving predictions into main table\n        qsave = qval\n    else: \n        # ddate is the last day of training for validation\n        # training data moves backwards\n        ddate = dmax - timedelta(days=horizon)\n        qtrain = d['Date'] <= ddate.isoformat()\n        # validate using the last day with data\n        # validation day stays constant\n        vdate = dmax\n        qval = d['Date'] == vdate.isoformat()\n        # lag day is last day of training\n        qvallag = d['Date'] == ddate.isoformat()\n        # for saving predictions into table, expected rise going backwards\n        sdate = dmax - timedelta(days=horizon-1)\n        qsave = d['Date'] == sdate.isoformat()\n\n    x_train = d[qtrain].copy()\n    x_val = d[qval].copy()\n    \n    # y training data\n    y_train = []\n    yd_train = [] \n    w_train = []\n    y_val = []\n    y_val2 = []\n    yd_val = []\n    w_val = []\n    for i in range(ny):\n        y_train.append(d.loc[qtrain,yv[i]+'_scaled'].values)\n        y_val2.append(d.loc[qval,yv[i]+'_scaled'].values)            \n        y_val.append(d.loc[qval,yv[i]+'_w2'].values)\n        yd_train.append(d.loc[qtrain,yvc[i]].values)\n        yd_val.append(d.loc[qval,yvc[i]].values)\n        \n        # fatality weight is 10x larger\n        wm = 9*i + 1.0\n        w_train.append(wm * d.loc[qtrain,'Weight'].values)\n        w_val.append(wm * d.loc[qval,'Weight'].values)\n        \n\n    yallv.append(y_val)\n    \n    # lag time-varying features\n    x_train.loc[:,lags] = x_train.groupby('Loc')[lags].transform(lambda x: x.shift(horizon))\n    x_val.loc[:,lags] = d.loc[qvallag,lags].values\n    \n    # paulo mad features\n    for i in range(ny):\n        for x in [x_train, x_val]:\n            x['avg_diff_'+yvs[i]] = (x[yvs[i]] - x[yvs[i]+'_l3']) / 3\n            x['mad_'+yvs[i]] = x[yvs[i]] + horizon * x['avg_diff_'+yvs[i]] - \\\n                (1 - MAD_FACTOR) * x['avg_diff_'+yvs[i]] * \\\n                np.sum([j for j in range(horizon)]) / nhorizon\n            \n        features[i] = features[i] + ['avg_diff_'+yvs[0], 'mad_'+yvs[0]] + \\\n                                    ['avg_diff_'+yvs[1], 'mad_'+yvs[1]]\n            \n    print()\n    print(horizon, 'x_train', x_train.shape)\n    print(horizon, 'x_val', x_val.shape)\n    \n    if train_full:\n        \n        qfull = (d['Date'] <= tmax)\n        \n        tdate0 = dmax + timedelta(days=horizon-win)\n        tdate = dmax + timedelta(days=horizon)\n        qtest = (tdate0.isoformat() < d['Date']) & (d['Date'] <= tdate.isoformat())\n                \n        dmax0 = dmax - timedelta(days=win)\n        qtestlag = (dmax0.isoformat() < d['Date']) & (d['Date'] <= dmax.isoformat())\n    \n        x_full = d[qfull].copy()\n\n        y_full = []\n        yd_full = []\n        w_full = []\n        for i in range(ny):\n            y_full.append(d.loc[qfull,yv[i]+'_scaled'].values)\n            yd_full.append(d.loc[qfull,yvc[i]].values)\n            wm = 9*i + 1.0\n            w_full.append(wm * d.loc[qfull,'Weight'].values)\n                    \n        x_test = d[qtest].copy()\n        # y_fulllag = [d.loc[qtestlag,'y0'].values, d.loc[qtestlag,'y1'].values]\n        \n        # lag features\n        x_full.loc[:,lags] = x_full.groupby('Loc')[lags].transform(lambda x: x.shift(horizon))\n        x_test.loc[:,lags] = d.loc[qtestlag,lags].values\n\n        # paulo mad features\n        for i in range(ny):\n            for x in [x_full, x_test]:\n                x['avg_diff_'+yvs[i]] = (x[yvs[i]] - x[yvs[i]+'_l3']) / 3\n                x['mad_'+yvs[i]] = x[yvs[i]] + horizon * x['avg_diff_'+yvs[i]] - \\\n                    (1 - MAD_FACTOR) * x['avg_diff_'+yvs[i]] * \\\n                    np.sum([j for j in range(horizon)]) / nhorizon\n        \n        print(horizon, 'x_full', x_full.shape)\n        print(horizon, 'x_test', x_test.shape)\n\n    train_set = []\n    val_set = []\n    ny = len(y_train)\n\n#     for i in range(ny):\n#         train_set.append(xgb.DMatrix(x_train[features[i]], y_train[i]))\n#         val_set.append(xgb.DMatrix(x_val[features[i]], y_val[i]))\n\n    gc.collect()\n\n    # loop over multiple targets\n    mod = []\n    pred = []\n    rez = []\n    iters = []\n    \n    for i in range(ny):\n#     for i in range(1):\n        print()\n        print('*'*40)\n        print(f'horizon {horizon} {yv[i]} {ynames[i]} {vdate}')\n        print('*'*40)\n        \n        # x_train[features[i]] = x_train[features[i]].fillna(0)\n        # x_val[features[i]] = x_val[features[i]].fillna(0)\n        \n        # use catboost only for y1\n        # nb = 2 if i==0 else 3\n       \n        # matrices to store predictions\n        vpm = np.zeros((x_val.shape[0],nq,nb))\n        if train_full:\n            tpm = np.zeros((x_test.shape[0],nq,nb))\n         \n        # loop over quantiles\n        for q in range(nq):    \n        \n            print()\n            print('quantile',quant[q])\n            ql = qlab[q]\n        \n            # loop over boosters\n            for b in range(nb):\n                \n                params[(booster[b],yv[i])]['alpha'] = quant[q]\n                restore_features = False\n\n                # loop over validation or full training\n                for tset in ['val','full']:\n\n                    if (tset=='full') & (not train_full): continue\n\n                    if tset=='full':\n                        print()\n                        print(f'{booster[b]} training with full data and predicting', tdate.isoformat())\n                        # params[(booster[b],yv[i])]['n_estimators'] = iallv[horizon//win-1,i,q,b]\n\n#                     else:\n#                         params[(booster[b],yv[i])]['n_estimators'] = 10000\n                        \n                    # scikit interface automatically uses best model for predictions\n                    # params[(booster[b],yv[i])]['n_estimators'] = 5000\n\n                    kwargs = {'verbose':1000}\n                    if booster[b]=='lgb':\n                        params[(booster[b],yv[i])]['n_estimators'] = lgb_nest[i,q]\n                        model = lgb.LGBMRegressor(**params[(booster[b],yv[i])]) \n                    elif booster[b]=='xgb':\n                         #params[(booster[b],yv[i])]['n_estimators'] = 75 if i==0 else 50\n                        params[(booster[b],yv[i])]['base_score'] = np.mean(y_train[i])\n                        model = xgb.XGBRegressor(**params[(booster[b],yv[i])])\n                    elif booster[b]=='ctb':\n                         #params[(booster[b],yv[i])]['n_estimators'] = 400 if i==0 else 350\n                        # change feature list for categorical features\n                        features_save = features[i].copy()\n                        features[i] = [f for f in features[i] if not f.startswith('Loc_')] + ['Loc']\n                        params[(booster[b],yv[i])]['cat_features'] = ['Loc']\n                        restore_features = True\n                        model = ctb.CatBoostRegressor(**params[(booster[b],yv[i])])\n                    elif booster[b]=='rdg':\n                        # alpha from cpmp\n                        model = Ridge(alpha=3, fit_intercept=True)\n                        kwargs = {}\n                    else:\n                        raise ValueError(f'Unrecognized booster {booster[b]}')\n\n                    if tset=='val':\n                        xtrn = x_train[features[i]].copy()\n                        xval = x_val[features[i]].copy()\n                        ytrn = y_train[i]\n                        yval = y_val[i]\n                        wtrn = w_train[i]\n                        wval = w_val[i]\n                        eset = [(xtrn, ytrn), (xval, y_val2[i])] \n                        esw = [wtrn,wval]      \n                    else:\n                        xtrn = x_full[features[i]].copy()\n                        ytrn = y_full[i]\n                        xtest = x_test[features[i]].copy()\n                        wtrn = w_full[i]\n                        eset = [(xtrn, ytrn)]\n                        esw = [wtrn]\n\n\n                    if booster[b]=='rdg':\n                        s = StandardScaler()\n                        xtrn = s.fit_transform(xtrn)\n                        xtrn = np.nan_to_num(xtrn)\n                        xtrn = pd.DataFrame(xtrn, columns=features[i])\n                        if tset=='val':\n                            xval = s.transform(xval)\n                            xval = pd.DataFrame(xval, columns=features[i])\n                            xval = np.nan_to_num(xval)\n\n                    # fit y\n                    model.fit(xtrn, ytrn, sample_weight=wtrn,\n                          eval_set=eset, eval_sample_weight=esw,\n                          # early_stopping_rounds=50,\n                          **kwargs\n                    )\n\n                    if tset=='val':\n                        vp = model.predict(xval)\n                        vp = vp * x_val[yv[i]+'_w7_max'] / x_val['scale_factor'+str(i)]\n                        vp = vp.clip(0,None)\n                        val_score = wqloss(yval, vp, quant[q], wval)\n                        print(f'wqloss {quant[q]} {val_score:.6f}')\n                    else:\n                        tp = model.predict(xtest)\n                        tp = tp * x_test[yv[i]+'_w7_max'] / x_test['scale_factor'+str(i)]\n                        tp = tp.clip(0,None)\n\n                    mod.append(model)\n                    \n#                     if tset=='val':\n#                         iallv[horizon//win-1,i,q,b] = model._best_iteration if booster[b]=='lgb' else \\\n#                                                       model.best_iteration if booster[b]=='xgb' else \\\n#                                                       model.best_iteration_\n\n                    # fit cum y\n                    if fit_cum:\n                        # kwargs = {'verbose':100}\n                        if booster[b]=='lgb':\n                            # params[(booster[b],yv[i])]['n_estimators'] = 125 if i==0 else 75\n                            model = lgb.LGBMRegressor(**params[(booster[b],yv[i])]) \n                        elif booster[b]=='xgb':\n                            # params[(booster[b],yv[i])]['n_estimators'] = 75 if i==0 else 30\n                            params[(booster[b],yv[i])]['base_score'] = np.mean(yd_train[i])\n                            model = xgb.XGBRegressor(**params[(booster[b],yv[i])])\n                        elif booster[b]=='ctb':\n                            # params[(booster[b],yv[i])]['n_estimators'] = 400 if i==0 else 200\n                            # hack for categorical features, ctb must be last in booster list\n                            # features[i] = [f for f in features[i] if not f.startswith('Loc_')] + ['Loc']\n                            # params[(booster[b],yv[i])]['cat_features'] = ['Loc']\n                            model = ctb.CatBoostRegressor(**params[(booster[b],yv[i])])\n                        elif booster[b]=='rdg':\n                            # alpha from cpmp\n                            model = Ridge(alpha=3, fit_intercept=True)\n                            kwargs = {}\n                        else:\n                            raise ValueError(f'Unrecognized booster {booster[b]}')\n\n                        model.fit(xtrn, yd_train[i], sample_weight=w_train[i],\n                              eval_set=[(xtrn, yd_train[i]), (xval, yd_val[i])],\n                              eval_sample_weight=[w_train[i],w_val[i]],\n                              # early_stopping_rounds=50,\n                              **kwargs\n                        )\n\n                        vpd = model.predict(xval)\n                        vpd = np.clip(vpd,0,None)\n                        # vpd = y_vallag[i] + vpd\n\n                        # blend two predictions based on horizon\n                        # alpha = 0.1 + 0.8*(horizon-1)/29\n                        alpha = 1.0\n                        vp = alpha*vp + (1-alpha)*vpd\n\n                        mod.append(model)\n\n\n                    gain = np.abs(model.coef_) if booster[b]=='rdg' else model.feature_importances_\n            #         gain = model.get_score(importance_type='gain')\n            #         split = model.get_score(importance_type='weight')   \n                #     gain = model.feature_importance(importance_type='gain')\n                #     split = model.feature_importance(importance_type='split').astype(float)  \n                #     imp = pd.DataFrame({'feature':features,'gain':gain,'split':split})\n                    imp = pd.DataFrame({'feature':features[i],'gain':gain})\n            #         imp = pd.DataFrame({'feature':features[i]})\n            #         imp['gain'] = imp['feature'].map(gain)\n            #         imp['split'] = imp['feature'].map(split)\n\n                    imp.set_index(['feature'],inplace=True)\n\n                    imp.gain /= np.sum(imp.gain)\n            #         imp.split /= np.sum(imp.split)\n\n                    imp.sort_values(['gain'], ascending=False, inplace=True)\n\n                    print()\n                    print(imp.head(n=10))\n                    # print(imp.shape)\n\n                    imp.reset_index(inplace=True)\n                    imp['horizon'] = horizon\n                    imp['target'] = yv[i]\n                    imp['set'] = 'valid'\n                    imp['booster'] = booster[b]\n\n                    imps.append(imp)\n\n                    # make sure horizon 1 prediction is not smaller than first lag\n                    # because we know series is monotonic\n                    # if horizon==1+skip:\n                    if False:\n                        a = np.zeros((len(vp),2))\n                        a[:,0] = vp\n                        # note yv is lagged here\n                        a[:,1] = x_val[yv[i]].values\n                        vp = np.nanmax(a,axis=1)\n\n                    if tset=='val':\n                        val_score = wqloss(yval, vp, quant[q], wval)\n                        vpm[:,q,b] = vp\n\n                        print()\n                        print(f'{booster[b]} validation wqloss {quant[q]} {val_score:.6f}')\n                        rallv[horizon//win-1,i,q,b] = val_score\n\n                    else:\n                        tpm[:,q,b] = tp\n\n\n                    gc.collect()\n    \n                \n                # restore feature list\n                if restore_features:\n                    features[i] = features_save\n                    restore_features = False\n                \n    #         # concat team predictions\n    #         if len(tfeats[i]):\n    #             vpm = np.concatenate([vpm,d.loc[qval,tfeats[i]].values], axis=1)\n    #             if train_full:\n    #                 tpm = np.concatenate([tpm,d.loc[qtest,tfeats[i]].values], axis=1)\n\n            # nonnegative least squares to estimate ensemble weights\n            # x, rnorm = nnls(vpm, y_val[i])\n\n            # smooth weights by shrinking towards all equal\n            # x = (x + np.ones(3)/3.)/2\n\n            nm = vpm.shape[-1]\n            x = np.ones(nm)/nm\n\n            # simple averaging to avoid overfitting\n            # drop ridge from y0\n    #         if i==0:\n    #             x = np.array([1., 1., 1., 0.])/3.\n    #         else:\n    #             nm = vpm.shape[1]\n    #             x = np.ones(nm)/nm\n\n    #         # drop catboost from y0\n    #         if i == 0:  \n    #             x = np.array([0.5, 0.5, 0.0])\n    #         else: \n    #             nm = vpm.shape[1]\n    #             x = np.ones(nm)/nm\n\n            # smooth weights with rolling mean, ewma\n            # alpha = 0.1\n            # if horizon-skip > 1: x = alpha * x + (1 - alpha) * nls[horizon-skip-2,i]\n\n            nls[horizon//win-1,i,q] = x\n\n            val_pred = np.matmul(vpm[:,q], x)\n            if train_full:\n                test_pred = np.matmul(tpm[:,q], x)\n\n            # save validation and test predictions back into main table\n            d.loc[qsave,yv[i]+'_pred_'+ql] = val_pred\n            if train_full:\n                d.loc[qtest,yv[i]+'_pred_'+ql] = test_pred\n                \n#             d.loc[qsave,yv[i]+'_pred_'+ql] = val_pred + x_val[yv[i]+'_median'].values\n#             if train_full:\n#                 d.loc[qtest,yv[i]+'_pred_'+ql] = test_pred + x_test[yv[i]+'_median'].values\n\n            # ensemble validation score\n            # val_score = np.sqrt(rnorm/vpm.shape[0])\n            val_score = wqloss(y_val[i], val_pred, q, w_val[i])\n\n            rez.append(val_score)\n            pred.append(val_pred)\n\n    pallv.append(pred)\n    \n#     # construct strings of nnls weights for printing\n#     w0 = ''\n#     w1 = ''\n#     for b in range(nb+tf2):\n#         w0 = w0 + f' {nls[horizon-skip-1,0,b]:.2f}'\n#         w1 = w1 + f' {nls[horizon-skip-1,1,b]:.2f}'\n        \n#     print()\n#     print('         Validation RMSLE  ', ' '.join(booster), ' '.join(tfeats[0]))\n#     print(f'{ynames[0]} \\t {rez[0]:.6f}  ' + w0)\n#     print(f'{ynames[1]} \\t {rez[1]:.6f}  ' + w1)\n#     print(f'Mean \\t \\t {np.mean(rez):.6f}')\n\n#     # break down RMSLE by day\n#     rp = np.zeros((2,7))\n#     for i in range(ny):\n#         for di in range(50,57):\n#             j = di - 50\n#             qf = x_val.dint == di\n#             rp[i,j] = np.sqrt(mean_squared_error(pred[i][qf], y_val[i][qf]))\n#             print(i,di,f'{rp[i,j]:.6f}')\n#         print(i,f'{np.mean(rp[i,:]):.6f}')\n#         plt.plot(rp[i])\n#         plt.title(ynames[i] + ' RMSLE')\n#         plt.show()\n        \n    # plot actual vs predicted\n    plt.figure(figsize=(10, 15))\n    for q in range(nq):\n        for i in range(ny):\n            idx = ny*q+i+1\n            plt.subplot(nq,ny,idx)\n            # plt.plot([0, 12], [0, 12], 'black')\n            plt.plot(np.log1p(pred[i*nq+q]), np.log1p(y_val[i]), '.')\n            plt.xlabel('Predicted')\n            plt.ylabel('Actual')\n            plt.title(ynames[i] + ' ' + qlab[q])\n            plt.grid()\n    plt.show()\n    \n    print()\n    print(f'Weighted Pinball Loss for Validation Horizon {horizon}')\n    print(f'    {np.mean(rallv[horizon//win-1,0].flatten()):.5f} {ynames[0]}')\n    print(f'    {np.mean(rallv[horizon//win-1,1].flatten()):.5f} {ynames[1]}')\n    print(f'    {np.mean(rallv[horizon//win-1].flatten()):.5f} Average')\n    print()\n\n# save one big table of importances\nimpall = pd.concat(imps)\n\n# remove number suffixes from lag names to aid in analysis\n# impall['feature1'] = impall['feature'].replace(to_replace='lag..', value='lag', regex=True)\n\nos.makedirs('imp', exist_ok=True)\nfname = 'imp/' + mname + '_imp.csv'\nimpall.to_csv(fname, index=False)\nprint()\nprint(fname, impall.shape)\n\n# save scores and weights\nos.makedirs('rez', exist_ok=True)\nfname = 'rez/' + mname+'_rallv.npy'\nnp.save(fname, rallv)\nprint(fname, rallv.shape)\n\nfname = 'rez/' + mname+'_nnls.npy'\nnp.save(fname, nls)\nprint(fname, nls.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"if 'cas' in booster: s.shutdown()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"np.mean(iallv, axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# plot pinball loss, note this is on smoothed target so is better than original\nplt.figure(figsize=(10, 4))\nfor i in range(ny):\n    plt.subplot(1,2,1+i)\n    plt.plot(np.mean(rallv[:,i],axis=1).flatten())\n    plt.title(ynames[i] + ' Weighted Pinball Loss vs Horizon')\n    plt.grid()\n    plt.legend(booster)\n    \n#     plt.subplot(2,2,3+i)\n#     plt.plot(nls[:,i])\n#     plt.title(ynames[i] + ' Ensemble Weights')\n#     plt.grid()\n#     plt.legend(booster+tfeats[i])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# weighted pinball loss with smooth target, better than with original target\nnp.mean(rallv.flatten())","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"d['Weight'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# # take differences and compute weighted pinball loss\n# # skip over first horizon so difference is not nan\n# vdate0 = ddate + timedelta(2+skip)\n# vdate1 = ddate + timedelta(nhorizon)\n# qval = (vdate0.isoformat() <= d['Date']) & (d['Date'] <= vdate1.isoformat())\n# for i in range(ny):\n#     d[yv[i]+'_pdiff'] = d.groupby('Loc')[yv[i]+'_pred'].transform(lambda x: x.diff())\n#     # exponentiate if original target is on log scale\n#     # d[yv[i]+'_exp'] = d.groupby('Loc')[yv[i]+'_pred'].transform(lambda x: np.expm1(x))\n#     # d[yv[i]+'_pdiff'] = d.groupby('Loc')[yv[i]+'_exp'].transform(lambda x: x.diff())\n#     vp = d.loc[qval,yv[i]+'_pdiff']\n#     vy = d.loc[qval,ynames[i]]\n#     vw = d.loc[qval,'Weight']\n#     a,s = np.average(np.abs(vp-vy), weights=vw, returned=True)\n#     if i==1: a *= 10\n#     print(a*s/len(vy))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# break down validation pinball loss for each location\nlocs = d.loc[:,['Loc','Weight']].drop_duplicates().reset_index(drop=True)\nwqa = []\nfor i in range(ny):\n    for q in range(nq):\n        wq = yv[i]+'_wqloss_'+qlab[q]\n        locs[wq] = np.nan\n        wqa.append(wq)\n\n# locs = x_val.copy().reset_index(drop=True)\n# print(locs.shape)\n# y_truea = []\n# y_preda = []\n\nvdate0 = ddate + timedelta(days=1+skip)\nvdate1 = ddate + timedelta(days=nhorizon)\nqv = (vdate0.isoformat() <= d['Date']) & (d['Date'] <= vdate1.isoformat())\n\n# print(f'# {mname}')\nfor j,loc in enumerate(locs['Loc']):\n    if j % 1000 == 0: print(j)\n    qvl = qv & (d['Loc']==loc)\n    w = d.loc[qvl,'Weight']\n    for i in range(ny):\n        wm = i*9 + 1\n        for q in range(nq):\n            # y = d.loc[qvl,yv[i]].values\n            y = d.loc[qvl,ynames[i]].values\n            p = d.loc[qvl,yv[i]+'_pred_'+qlab[q]]\n            wq = yv[i]+'_wqloss_'+qlab[q]\n            locs.loc[locs.Loc==loc,wq] = wqloss(y,p,quant[q],wm*w)\n            \n        # make each series monotonic increasing\n        # for j in range(y_pred.shape[1]): \n        #     y_pred[:,j] = np.maximum.accumulate(y_pred[:,j])\n        # copy updated predictions into main table\nlocs","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# overall validation pinball loss\nnp.mean(locs[wqa].values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"np.mean(locs.loc[locs.Loc!='US',wqa].values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"np.mean(locs.loc[locs.Loc=='US',wqa].values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"os.makedirs('locs',exist_ok=True)\nfname = 'locs/' + mname + '_locs.csv'\nlocs.to_csv(fname, index=False)\nprint(fname, locs.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"yv[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"np.nansum(np.abs(d[[yv[0]]].values.flatten() - d.ConfirmedCases.values.flatten()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# # enforce monotonicity of forecasts in test set after last date in training\n# if train_full:\n#     # loc = d['Loc'].unique()\n#     locs1 = d['Loc'].drop_duplicates()\n#     for loc in locs1:\n#         # q = (d.Loc==loc) & (d.ForecastId > 0)\n#         q = (d.Loc==loc) & (d.Date > tmax)\n#         # if skip, fill in last observed value\n#         if skip: qs0 = (d.Loc==loc) & (d.Date == dmax.isoformat())\n#         for yi in yv:\n#             yp = yi+'_pred'\n#             d.loc[q,yp] = np.maximum.accumulate(d.loc[q,yp])\n#             if skip:\n#                 for j in range(skip):\n#                     qs1 = (d.Loc==loc) & (d.Date == (dmax + timedelta(1+j)).isoformat())\n#                     d.loc[qs1,yp] = d.loc[qs0,yi].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# sort to find worst predictions of y0\nlocs = locs.sort_values('y0_wqloss_q50', ascending=False)\nlocs[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# plot worst fits \nfor j in range(10):\n    plt.figure(figsize=(14,4))\n    lj = locs.index[j]\n    loc = locs.loc[lj,'Loc']\n    qvl = (d['Loc']==loc) & (d['Date'] <= tmax)\n    for i in range(ny):\n        plt.subplot(1,2,i+1)\n        plt.plot(d.loc[qvl,[ynames[i],'Date']].set_index('Date'))\n        plt.plot(d.loc[qvl,[yv[i]+'_w2','Date']].set_index('Date'))\n        plt.plot(d.loc[qvl,[yv[i]+'_w7','Date']].set_index('Date'))\n        # plt.plot(d.loc[qvl,[yv[i]+'_w5','Date']].set_index('Date'))\n        for q in range(nq):\n            plt.plot(d.loc[qvl,[yv[i]+'_pred_'+qlab[q],'Date']].set_index('Date'))\n\n    #     qvl = qv & (d['Loc']==loc)\n    #     y = d.loc[qvl,'y0'].values\n    #     plt.plot(y)\n    #     for q in range(nq):\n    #         p = d.loc[qvl,'y0_pred_'+qlab[q]].values\n    #         if quant[q] == 0.5: plt.plot(p, c='r')\n    #         else: plt.plot(p, '--', c='r')\n\n        plt.xticks([])\n        plt.title(loc + ' ' + ynames[i])\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"'dow' in features[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# plot individual location\nloc = 'US North Carolina'\n# loc = 'US North Carolina Wake'\n# loc = 'US Ohio'\n# loc = 'US Ohio Morrow'\nqvl = (d['Loc']==loc) & (d['Date'] <= tmax)\nplt.figure(figsize=(14,6))\nfor i in range(ny):\n    plt.subplot(1,2,i+1)\n    plt.plot(d.loc[qvl,[ynames[i],'Date']].set_index('Date'))\n    for q in range(nq):\n        plt.plot(d.loc[qvl,[yv[i]+'_pred_'+qlab[q],'Date']].set_index('Date'))\n\n    plt.xticks([])\n    plt.title(loc + ' ' + ynames[i])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# d.loc[d.Loc=='US','Loc_US'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# sort to find worst predictions of y0\nlocs = locs.sort_values('y1_wqloss_q50', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# # plot worst fits of y1\n# i = 1\n# for j in range(5):\n#     lj = locs.index[j]\n#     loc = locs.loc[lj,'Loc']\n#     qvl = (d['Loc']==loc) & (d['Date'] <= tmax)\n#     plt.plot(d.loc[qvl,[ynames[i],'Date']].set_index('Date'))\n#     for q in range(nq):\n#         plt.plot(d.loc[qvl,[yv[i]+'_pred_'+qlab[q],'Date']].set_index('Date'))\n\n# #     qvl = qv & (d['Loc']==loc)\n# #     y = d.loc[qvl,'y0'].values\n# #     plt.plot(y)\n# #     for q in range(nq):\n# #         p = d.loc[qvl,'y0_pred_'+qlab[q]].values\n# #         if quant[q] == 0.5: plt.plot(p, c='r')\n# #         else: plt.plot(p, '--', c='r')\n     \n#     plt.xticks([])\n#     plt.title(loc + ' ' + ynames[i])\n#     plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# compute public lb score\nif not prev_test:\n    ys = ['CC','FT']\n    print(f'# {ddate.isoformat()} {fmin} {tmax} {mname}')\n    qvl = (d.Date >= '2020-04-27') & (d.Date <= tmax)\n    w = d.loc[qvl,'Weight']\n    a = []\n    for i in range(ny):\n        wm = i*9 + 1\n        for q in range(nq):\n            # y = d.loc[qvl,yv[i]].values\n            y = d.loc[qvl,ynames[i]].values\n            p = d.loc[qvl,yv[i]+'_pred_'+qlab[q]]\n            loss = wqloss(y,p,quant[q],wm*w)\n            print(f'# {ys[i]} \\t {quant[q]} \\t {loss:.5f}')\n            a.append(loss)\n            \n    print()\n    print(f'# {ys[0]} \\t {np.mean(a[:3]):.5f}')\n    print(f'# {ys[1]} \\t {np.mean(a[3:]):.5f}')\n    print(f'# Avg \\t {np.mean(a):.5f}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# # compute public lb score\n# if not prev_test:\n#     # q = (d.Date >= fmin) & (d.Date > ddate.isoformat()) & (d.Date <= tmax)\n#     q = (d.Date >= '2020-04-02') & (d.Date <= tmax)\n#     # q = (d.Date >= tmax) & (d.Date <= tmax)\n#     print(f'# {fmin} {ddate.isoformat()} {tmax} {sum(q)//ns} {mname}')\n#     s0 = np.sqrt(mean_squared_error(d.loc[q,'y0r'],d.loc[q,'y0_pred']))\n#     s1 = np.sqrt(mean_squared_error(d.loc[q,'y1r'],d.loc[q,'y1_pred']))\n#     print(f'# CC \\t {s0:.6f}')\n#     print(f'# Fa \\t {s1:.6f}')\n#     print(f'# Mean \\t {(s0+s1)/2:.6f}')\n    \n#     s0 = np.sqrt(mean_squared_error(d.loc[q,'y0r'],d.loc[q,'y0_preda']))\n#     s1 = np.sqrt(mean_squared_error(d.loc[q,'y1r'],d.loc[q,'y1_preda']))\n#     print()\n#     print(f'# CC \\t {s0:.6f}')\n#     print(f'# Fa \\t {s1:.6f}')\n#     print(f'# Mean \\t {(s0+s1)/2:.6f}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# nnls to estimate blending weights\nif blend:\n    print('blending with',blender)\n    sub = d.loc[d.ForecastId > 0, ['ForecastId','ConfirmedCases','Fatalities',\n                                   'y0','y1','y0_preda','y1_preda','Date','dint']]\n    sub['dint'] = sub['dint'] - sub['dint'].min()\n    # original data, nonmonotonic in some places\n    sub['y0r'] = np.log1p(sub.ConfirmedCases)\n    sub['y1r'] = np.log1p(sub.Fatalities)\n    sub['ConfirmedCases'] = sub.ConfirmedCases.astype(float)\n    sub['Fatalities'] = sub.Fatalities.astype(float)\n\n    print(sub.shape)\n    print(sub['dint'].describe())\n    hmax = np.max(sub.dint.values) + 1\n    print(hmax)\n    \n    # add nq\n    bs = pd.read_csv('sub/'+blender[0]+'.csv')\n    print(bs.shape)\n    bs['nq0'] = np.log1p(bs.ConfirmedCases)\n    bs['nq1'] = np.log1p(bs.Fatalities)\n    bs.drop(['ConfirmedCases','Fatalities'],axis=1,inplace=True)\n    sub = sub.merge(bs, how='left', on='ForecastId')\n    sub['nq0'] = sub['nq0'].fillna(sub['y0'])\n    sub['nq1'] = sub['nq1'].fillna(sub['y1'])\n\n    # add kaz\n    bs = pd.read_csv('sub/'+blender[1]+'.csv')\n    print(bs.shape)\n    bs['kaz0'] = np.log1p(bs.ConfirmedCases)\n    bs['kaz1'] = np.log1p(bs.Fatalities)\n    bs.drop(['ConfirmedCases','Fatalities'],axis=1,inplace=True)\n    sub = sub.merge(bs, how='left', on='ForecastId')\n    \n    for i in range(ny): sub[mname+str(i)] = sub[yv[i]+'_preda']\n        \n    # qv = (sub.Date >= '2020-04-09') & (sub.Date <= tmax)\n    qv = (sub.Date > tmax)\n    a = sub[qv].copy()\n\n#     # intercept estimate is 0\n#     # a['intercept0'] = 1.0\n#     # a['intercept1'] = 1.0\n#     # m = ['intercept','b0g','v0e','o0e',mname]\n#     # m = ['kaz',mname]\n#     m = ['nq','kaz',mname]\n#     print(m)\n#     n = a.shape[0]\n#     wt= np.zeros((2,len(m)))\n#     s = 0\n#     for i in range(ny):\n#         mi = [c+str(i) for c in m]\n#         wt[i], rnorm = nnls(a[mi].values, a[yv[i]+'r'].values)\n#         r = rnorm/np.sqrt(n)\n#         print(i, wt[i], f'{sum(wt[i]):.6f}', f'{r:.6f}')\n#         s += 0.5*r\n#     print(f'{s:.6f}')\n#     print()\n    print(a[['nq0','kaz0',mname+'0','nq1','kaz1',mname+'1']].corr())","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"d['ForecastId'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# create submission\nss = []\nqf = d.ForecastId > 0\nfor q in range(nq):\n    for i in range(ny):\n        yp = yv[i]+'_pred_'+qlab[q]\n        s = d.loc[qf,['ForecastId',yp]]\n        s = s.rename(mapper={yp:'TargetValue'}, axis=1)\n        s['Target'] = ynames[i]\n        s['Quantile'] = quant[q]\n        if i==1: s['ForecastId'] += 1\n        s['ForecastId_Quantile'] = s.ForecastId.astype(str) + '_' + s.Quantile.astype(str)\n        s = s.reset_index(drop=True)\n        ss.append(s)\ns","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sub = pd.concat(ss).sort_values(['ForecastId','Quantile'])\nsub","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"os.makedirs('sub', exist_ok=True)\nsub1 = sub[['ForecastId_Quantile','TargetValue']]\nfname = 'sub/'+mname+'.csv'\nsub1.to_csv(fname, index=False)\nprint(fname, sub1.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# # create blended submission, set weights by hand after looking at validation nnls\n# if blend:\n#     # blend\n#     sub['ConfirmedCases'] = np.expm1(0.997*(0.5 * sub['nq0'] + \\\n#                                             0.2 * sub['kaz0'] + \\\n#                                             0.3 * sub['y0_preda']))\n#     sub['Fatalities'] = np.expm1(0.996*(0.666666 * sub['nq1'] + \\\n#                                         0.133333 * sub['kaz1'] + \\\n#                                         0.2      * sub['y1_preda']))\n            \n# else:\n#     # create submission without any blending with others\n#     sub = d.loc[d.ForecastId > 0, ['ForecastId','y0_pred','y1_pred']]\n#     print(sub.shape)\n\n#     sub['ConfirmedCases'] = np.expm1(sub['y0_preda'])\n#     sub['Fatalities'] = np.expm1(sub['y1_preda'])    \n\n# sub0 = sub.copy()\n# print(sub0.shape)\n# sub = sub[['ForecastId','ConfirmedCases','Fatalities']]\n\n# os.makedirs('sub',exist_ok=True)\n# fname = 'sub/' + mname + '.csv'\n# sub.to_csv(fname, index=False)\n# print(fname, sub.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# sub.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# # final day adjustment as per northquay\n# pname = mname\n# # pred = sub.copy()\n# pred = pd.read_csv('sub/' + mname + '.csv')\n\n# # pname = 'kaz0m'\n# # pred = pd.read_csv('../week3/sub/'+pname+'.csv')\n\n# pred_orig = pred.copy()\n\n# if prev_test:\n#     test = pd.read_csv('../'+pw+'/test.csv')\n# else:\n#     test = pd.read_csv('test.csv')\n\n# test[cp] = test[cp].fillna('')\n\n# # test.Date = pd.to_datetime(test.Date)\n# # train.Date = pd.to_datetime(train.Date)\n\n# # TODAY = datetime.datetime(  *datetime.datetime.today().timetuple()[:3] )\n# # TODAY = date(2020, 4, 7)\n\n# # shift day back one to match wm adjustment\n\n# print(TODAY)\n\n# final_day = wm[wm.Date == TODAY].copy()\n# final_day['cases_final'] = np.expm1(final_day.TotalCases)\n# final_day['cases_chg'] = np.expm1(final_day.NewCases)\n# final_day['deaths_final'] = np.expm1(final_day.TotalDeaths)\n# final_day['deaths_chg'] = np.expm1(final_day.NewDeaths)\n\n\n# # test.rename(columns={'Country_Region': 'Country'}, inplace=True)\n# # test['Place'] = test.Country +  test.Province_State.fillna(\"\")\n\n# # final_day = pd.read_excel(path + '../week3/nq/' + 'final_day.xlsx')\n# # final_day = final_day.iloc[1:, :5]\n# # final_day = final_day.fillna(0)\n# # final_day.columns = ['Country', 'cases_final', 'cases_chg', \n# #                      'deaths_final', 'deaths_chg']\n\n# final_day = final_day[['Country_Region','Province_State','cases_final','cases_chg',\n#                       'deaths_final','deaths_chg']].fillna(0)\n# # final_day = final_day.drop('Date', axis=1).reset_index(drop=True)\n# final_day = final_day.sort_values('cases_final', ascending=False)\n\n# print()\n# print('final_day')\n# print(final_day.head(n=10), final_day.shape)\n\n# # final_day.Country.replace({'Taiwan': 'Taiwan*',\n# #                            'S. Korea': 'Korea, South',\n# #                            'Myanmar': 'Burma',\n# #                            'Vatican City': 'Holy See',\n# #                            'Ivory Coast':  \"Cote d'Ivoire\",\n                        \n# #                           },\n# #                          inplace=True)\n\n\n# pred = pd.merge(pred, test, how='left', on='ForecastId')\n# print()\n# print('pred')\n# print(pred.head(n=10), pred.shape)\n\n# # pred = pd.merge(pred, test[test.Province_State.isnull()], how='left', on='ForecastId')\n\n# # compare = pd.merge(pred[pred.Date == TODAY], final_day, on= [ 'Country'],\n# #                            validate='1:1')\n\n# compare = pd.merge(pred[pred.Date == TODAY], final_day, on=cp, validate='1:1')\n\n# compare['c_li'] = np.round(np.log(compare.cases_final + 1) - np.log(compare.ConfirmedCases + 1), 2)\n# compare['f_li'] = np.round(np.log(compare.deaths_final + 1) - np.log(compare.Fatalities + 1), 2)\n\n# print()\n# print('compare')\n# print(compare.head(n=10), compare.shape)\n# print(compare.describe())\n\n# # compare[compare.c_li > 0.3][['Country', 'ConfirmedCases', 'Fatalities',\n# #                                         'cases_final', 'cases_chg',\n# #                                     'deaths_final', 'deaths_chg',\n# #                                             'c_li', 'f_li']]\n\n# # compare[compare.c_li > 0.15][['Country', 'ConfirmedCases', 'Fatalities',\n# #                                         'cases_final', 'cases_chg',\n# #                                     'deaths_final', 'deaths_chg',\n# #                                             'c_li', 'f_li']]\n\n# # compare[compare.f_li > 0.3][['Country', 'ConfirmedCases', 'Fatalities',\n# #                                         'cases_final', 'cases_chg',\n# #                                     'deaths_final', 'deaths_chg',\n# #                                             'c_li', 'f_li']]\n\n\n# # compare[compare.f_li > 0.15][['Country', 'ConfirmedCases', 'Fatalities',\n# #                                         'cases_final', 'cases_chg',\n# #                                     'deaths_final', 'deaths_chg',\n# #                                             'c_li', 'f_li']]\n\n# # compare[compare.c_li < -0.15][['Country', 'ConfirmedCases', 'Fatalities',\n# #                                         'cases_final', 'cases_chg',\n# #                                     'deaths_final', 'deaths_chg',\n# #                                             'c_li', 'f_li']]\n\n# # compare[compare.f_li < -0.2][['Country', 'ConfirmedCases', 'Fatalities',\n# #                                         'cases_final', 'cases_chg',\n# #                                     'deaths_final', 'deaths_chg',\n# #                                             'c_li', 'f_li']]\n\n# fixes = pd.merge(pred[pred.Date >= TODAY], \n#                      compare[cp + ['c_li', 'f_li']], on=cp)\n\n\n# fixes['c_li'] = np.where( fixes.c_li < 0,\n#                              0,\n#                                  fixes.c_li)\n# fixes['f_li'] = np.where( fixes.f_li < 0,\n#                              0,\n#                                  fixes.f_li)\n\n# fixes['total_fixes'] = fixes.c_li**2 + fixes.f_li**2\n\n# print()\n# print('most fixes')\n# print(fixes.groupby(cp).last().sort_values(['total_fixes','Date'], ascending = False).head(n=10))\n\n# # adjustment\n# fixes['Fatalities'] = np.round(np.exp((np.log(fixes.Fatalities + 1) + fixes.f_li))-1, 3)\n# fixes['ConfirmedCases'] = np.round(np.exp((np.log(fixes.ConfirmedCases + 1) + fixes.c_li))-1, 3)\n\n\n# fix_ids = fixes.ForecastId.unique()\n# len(fix_ids)\n\n# cols = ['ForecastId', 'ConfirmedCases', 'Fatalities']\n\n\n# fixed = pd.concat((pred.loc[~pred.ForecastId.isin(fix_ids),cols],\n#     fixes[cols])).sort_values('ForecastId')\n\n\n# # fixed.head()\n# # fixed.tail()\n\n# # len(pred_orig)\n# # len(fixed)\n\n# fname = 'sub/' + pname + '_updated.csv'\n# fixed.to_csv(fname, index=False)\n# print(fname, fixed.shape)\n# fixed.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# !wget https://web.archive.org/web/20200408225104/https://www.worldometers.info/coronavirus/country/us/ -O us0408.html\n\n# us = pd.read_html('us0408.html')\n# len(us)\n\n# # us[0].sort_values('USAState')\n# us[0].sort_values('TotalCases', ascending=False)[2:12]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# !wget https://web.archive.org/web/20200408234045/https://www.worldometers.info/coronavirus/country/us/ -O us0408.html\n\n# us = pd.read_html('us0408.html')\n# len(us)\n\n# # us[0].sort_values('USAState')\n# us[0].sort_values('TotalCases', ascending=False)[2:12]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# compare[compare.Country_Region=='US'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# sum(qv)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# # merge final predictions back into main table\n# sub1 = fixed.copy()\n# for i in range(ny): \n#     mi = mname + str(i)\n#     if mi in d.columns: d.drop(mi, axis=1, inplace=True)\n#     sub1[mi] = np.log1p(sub1[ynames[i]])\n#     sub1.drop(ynames[i],axis=1,inplace=True)\n# d = d.merge(sub1, how='left', on='ForecastId')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# fixed.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# # compute public lb score after averaging with others\n# if not prev_test:\n#     # q = (d.Date >= fmin) & (d.Date > ddate.isoformat()) & (d.Date <= tmax)\n#     q = (d.Date >= '2020-04-02') & (d.Date <= tmax)\n#     # q = (d.Date >= tmax) & (d.Date <= tmax)\n#     print(f'# {fmin} {ddate.isoformat()} {tmax} {sum(q)/ns} {mname}')\n#     s0 = np.sqrt(mean_squared_error(d.loc[q,'y0r'],d.loc[q,mname+'0']))\n#     s1 = np.sqrt(mean_squared_error(d.loc[q,'y1r'],d.loc[q,mname+'1']))\n#     print(f'# CC \\t {s0:.6f}')\n#     print(f'# Fa \\t {s1:.6f}')\n#     print(f'# Mean \\t {(s0+s1)/2:.6f}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# 2020-04-02 2020-03-15 2020-04-14 13.0 gbt5c\n# CC \t 0.536442\n# Fa \t 0.402339\n# Mean \t 0.469391","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# 2020-04-02 2020-03-15 2020-04-14 13.0 gbt5b\n# CC \t 0.536442\n# Fa \t 0.402339\n# Mean \t 0.469391","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# # check submissions\n# if prev_test:\n#     snames = nqs + ['nq0a', 'kaz','kaz0f','kaz0h','kaz0i','kaz0j', 'kaz0k', \n#                     'kaz0m','kaz0m_updated',\n#                     'gbt3l','gbt3n',mname,mname+'_updated']\n# #     snames = ['nq','kaz','kaz0f','kaz0h','kaz0i','kaz0j', 'kaz0k', 'kaz0m',\n# #               'beluga0g',\n# #               'vopani','jeremiah0a','jeremiah0b',\n# #               'kgmon','isaac0a','oscii','oscii0g','pdd',\n# #               'gbt3l','gbt3n',mname]\n# #     snames = ['kaz','kaz0f','kaz0g']\n\n#     for j,s in enumerate(snames):\n#         if s != mname:\n#             fname = '../'+pw+'/sub/'+s+'.csv'\n#             if not os.path.exists(fname): fname = 'sub/'+s+'.csv'\n#             sub = pd.read_csv(fname)\n#             for i in range(ny): sub[s+str(i)] = np.log1p(sub[ynames[i]])\n#             sub = sub.drop(['ConfirmedCases','Fatalities'], axis=1)\n#             # print(sub.head(),sub.shape)\n\n#             v = d.merge(sub, how='left', on='ForecastId')\n#         else:\n#             v = d.copy()\n\n#         qv = (v.Date >= '2020-04-09') & (v.Date <= tmax)\n#         v = v[qv]\n#         v['y0r'] = np.log1p(v.ConfirmedCases)\n#         v['y1r'] = np.log1p(v.Fatalities)\n#         # print(v.shape)\n#         cc = np.sqrt(mean_squared_error(v.y0r,v[s+'0']))\n#         fa = np.sqrt(mean_squared_error(v.y1r,v[s+'1']))\n#         print()\n#         print(f'{s} CC   {cc:.6f}')\n#         print(f'{s} Fa   {fa:.6f}')\n#         print(f'{s} Mean {(cc+fa)/2:.6f}')\n        \n# else:\n#     snames = ['nq0b', 'nq0g','nq0g_updated', 'nq0i','nq0i_updated',\n#               'kaz0n', 'kaz0o', 'kaz0q', 'kaz0r', 'kaz0s', 'kaz0t',\n#                mname, mname+'_updated']\n\n#     for j,s in enumerate(snames):\n#         if s != mname:\n#             fname = 'sub/'+s+'.csv'\n#             if not os.path.exists(fname): fname = '../'+pw+'/sub/'+s+'.csv'\n#             sub = pd.read_csv(fname)\n#             for i in range(ny): sub[s+str(i)] = np.log1p(sub[ynames[i]])\n#             sub = sub.drop(['ConfirmedCases','Fatalities'], axis=1)\n#             # print(sub.head(),sub.shape)\n\n#             v = d.merge(sub, how='left', on='ForecastId')\n#         else:\n#             v = d.copy()\n\n#         qv = (v.Date >= '2020-04-09') & (v.Date <= tmax)\n#         v = v[qv]\n#         v['y0r'] = np.log1p(v.ConfirmedCases)\n#         v['y1r'] = np.log1p(v.Fatalities)\n#         # print(v.shape)\n#         cc = np.sqrt(mean_squared_error(v.y0r,v[s+'0']))\n#         fa = np.sqrt(mean_squared_error(v.y1r,v[s+'1']))\n#         print()\n#         print(f'{s} CC   {cc:.6f}')\n#         print(f'{s} Fa   {fa:.6f}')\n#         print(f'{s} Mean {(cc+fa)/2:.6f}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# gbt5b CC   0.582307\n# gbt5b Fa   0.445966\n# gbt5b Mean 0.514136\n\n# gbt5b_updated CC   0.582307\n# gbt5b_updated Fa   0.445966\n# gbt5b_updated Mean 0.514136","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# gbt5b CC   0.175608\n# gbt5b Fa   0.123974\n# gbt5b Mean 0.149791\n\n# gbt5b_updated CC   0.175608\n# gbt5b_updated Fa   0.123974\n# gbt5b_updated Mean 0.149791","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# gbt5a CC   0.152043\n# gbt5a Fa   0.157290\n# gbt5a Mean 0.154666\n\n# gbt5a_updated CC   0.145772\n# gbt5a_updated Fa   0.154880\n# gbt5a_updated Mean 0.150326","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"wqa","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# save oof predictions\novars = ['Id','ForecastId','Country_Region','Province_State','County','Loc','Date',\n         'y0_pred_q05', 'y0_pred_q50', 'y0_pred_q95',\n         'y0_pred_q05', 'y0_pred_q50', 'y0_pred_q95']\nqd = (d.Date >= '2020-04-27') & (d.Date <= '2020-05-22')\noof = d.loc[qd,ovars]\n# oof = oof.rename(mapper={'y0_pred':mname+'0','y1_pred':mname+'1'}, axis=1)\nos.makedirs('oof',exist_ok=True)\nfname = 'oof/' + mname + '_oof.csv'\noof.to_csv(fname, index=False)\nprint(fname, oof.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"if save_data:\n    os.makedirs('data',exist_ok=True)\n    fname = 'data/' + mname + '_d.csv'\n    d.to_csv(fname, index=False)\n    print(fname, d.shape)\n    \n    fname = 'data/' + mname + '_x_train.csv'\n    x_train.to_csv(fname, index=False)\n    print(fname, d.shape)\n    \n    fname = 'data/' + mname + '_x_val.csv'\n    x_val.to_csv(fname, index=False)\n    print(fname, d.shape)\n    \n    fname = 'data/' + mname + '_x_full.csv'\n    x_full.to_csv(fname, index=False)\n    print(fname, d.shape)\n    \n    fname = 'data/' + mname + '_x_test.csv'\n    x_test.to_csv(fname, index=False)\n    print(fname, d.shape)\n    \n#     fname = 'data/' + mname + '_y_train.csv'\n#     y_train[0].to_csv(fname, index=False)\n#     print(fname, d.shape)\n    \n#     fname = 'data/' + mname + '_y_val.csv'\n#     y_val[0].to_csv(fname, index=False)\n#     print(fname, d.shape)\n    \n#     fname = 'data/' + mname + '_y_full.csv'\n#     y_full[0].to_csv(fname, index=False)\n#     print(fname, d.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# set(features[i]) - set(lags)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# set(lags) - set(features[i])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"len(features[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# pd.set_option('display.max_rows', 150)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# # q = (d.Date >= '2020-04-02') & (d.Loc=='Cabo Verde')\n# # q = (d.Date >= '2020-04-02') & (d.Loc=='Congo (Brazzaville)')\n# q = (d.Date >= '2020-04-02') & (d.Loc=='Somalia')\n# d.loc[q,['Date','ForecastId','y0','y1','y0r','y1r',\n#                         mname + str(0),mname+str(1)]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# most fixes\n#                                     ForecastId  ConfirmedCases   Fatalities  \\\n# Country_Region      Province_State                                            \n# Cabo Verde                                1591       83.069233     2.661215   \n# Congo (Brazzaville)                       3827      188.486683    10.366824   \n# Jamaica                                   6364      310.090653     9.060071   \n# Slovakia                                  9374     2407.449830    17.087912   \n# Netherlands         Aruba                 7869      207.263786     1.729323   \n# Timor-Leste                              10019       15.586367     0.892582   \n# Tanzania                                  9933      250.855338    11.648181   \n# Somalia                                   9460      213.936255    15.164921   \n# Gabon                                     5375      169.672938     2.876582   \n# US                  Maryland             11137    24302.549228  1205.949083   \n\n#                                           Date  c_li  f_li  total_fixes  \n# Country_Region      Province_State                                       \n# Cabo Verde                          2020-05-14  0.49  0.00       0.2401  \n# Congo (Brazzaville)                 2020-05-14  0.28  0.00       0.0784  \n# Jamaica                             2020-05-14  0.18  0.03       0.0333  \n# Slovakia                            2020-05-14 -0.00  0.17       0.0289  \n# Netherlands         Aruba           2020-05-14  0.00  0.14       0.0196  \n# Timor-Leste                         2020-05-14  0.13  0.00       0.0169  \n# Tanzania                            2020-05-14  0.12  0.02       0.0148  \n# Somalia                             2020-05-14  0.02  0.10       0.0104  \n# Gabon                               2020-05-14  0.09  0.00       0.0081  \n# US                  Maryland        2020-05-14  0.04  0.08       0.0080  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# # plot actual and predicted curves over time for specific locations\n# # locs = ['China Tibet','China Xinjiang','China Hong Kong', 'China Macau',\n# #         'Spain','Italy','India',\n# #         'US Washington','US New York','US California',\n# #         'US North Carolina','US Ohio']\n# # xlab = ['03-12','03-18','03-25','04-01','04-08','04-15','04-22']\n# # plot all locations\n# locs = d['Loc'].drop_duplicates()\n# for loc in locs:\n#     plt.figure(figsize=(14,2))\n    \n#     # fig, ax = plt.subplots()\n#     # fig.autofmt_xdate()\n    \n#     for i in range(ny):\n    \n#         plt.subplot(1,2,i+1)\n#         plt.plot(d.loc[d.Loc==loc,[yv[i],'Date']].set_index('Date'))\n#         plt.plot(d.loc[d.Loc==loc,[mname + str(i),'Date']].set_index('Date'))\n#         # plt.plot(d.loc[d.Loc==loc,[yv[i]+'_pred','Date']].set_index('Date'))\n#         # plt.plot(d.loc[d.Loc==loc,[yv[i]]])\n#         # plt.plot(d.loc[d.Loc==loc,[yv[i]+'_pred']])\n#         # plt.xticks(np.arange(len(xlab)), xlab, rotation=-45)\n#         # plt.xticks(np.arange(12), calendar.month_name[3:5], rotation=20)\n#         # plt.xticks(rotation=-45)\n#         plt.xticks([])\n#         plt.title(loc + ' ' + ynames[i])\n       \n#     plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# fixed.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}