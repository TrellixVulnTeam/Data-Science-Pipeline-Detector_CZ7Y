{"cells":[{"metadata":{},"cell_type":"markdown","source":"# The Story of COVID-19 in World and Time Forecasting in Turkey"},{"metadata":{},"cell_type":"markdown","source":"**Coronavirus is a large family of viruses. This is a disease that was detected in 1960, with several varieties. The virus, which is seen mostly in animals, has also been seen in humans for the first time. The current outbreak first appeared in Wuhan, China, in December 2019.The best way to prevent and slow down transmission is be well informed about the COVID-19 virus, the disease it causes and how it spreads. Protect yourself and others from infection by washing your hands or using an alcohol based rub frequently and not touching your face.**\n\n**The COVID-19 virus spreads primarily through droplets of saliva or discharge from the nose when an infected person coughs or sneezes, so it’s important that you also practice respiratory etiquette (for example, by coughing into a flexed elbow).At this time, there are no specific vaccines or treatments for COVID-19. However, there are many ongoing clinical trials evaluating potential treatments. WHO will continue to provide updated information as soon as clinical findings become available.**\n\n<img src=\"https://pbs.twimg.com/media/EWjM_DBWAAESbWc.jpg\">"},{"metadata":{},"cell_type":"markdown","source":"# Table of Contents\n\n* [World COVID-19 Cases](#section-one)\n* [Global Deaths Heat Map](#section-two)\n* [Active, Recovered, Deaths in Hotspot Countries](#section-three)\n* [US Heatmap(Confirmed Cases)](#section-four)\n* [Average Age Distribution of Cases in Countries](#section-four-1)\n* [Turkey](#section-five)\n    * [Top 5 Cities with the Highest Number of Cases](#section-five-one)\n    * [Turkey Heatmap (Number of Case)](#section-five-two)\n    * [10 Cities with the Lowest Number of Cases](#section-five-three)\n* [Turkey COVID-19 Forecasting](#section-six)\n     * [Confirmed Case in Time Intervals](#section-six-one)\n     * [Fatalities Case in Time Intervals](#section-six-two)\n     * [Time Series Model](#section-six-four)\n         * [Importing Libraries](#section-six-four-1)\n         * [Prepearing Data](#section-six-four-2)\n         * [Spliting Data for Training and Validation](#section-six-four-3)\n         * [Determine Rolling Stats](#section-six-four-4)\n         * [Check for Stationary](#section-six-four-5)\n         * [Log scale tranformation](#section-six-four-6)\n         * [Exponential Decay Transformation](#section-six-four-7)\n         * [ADCF Test](#section-six-four-8)\n         * [Time Shift Transformation](#section-six-four-9)\n         * [Decomposition](#section-six-four-10)\n         * [Building Model](#section-six-four-11)\n         * [Prediction & Reverse Transformations](#section-six-four-12)\n         * [Validation](#section-six-four-13)\n         * [Test Forecasting](#section-six-four-14)\n         * [ARIMA PDQ Param Tuning](#section-six-four-15) \n* [REFERENCES](#section-five)"},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport datetime\nimport requests\nimport warnings\nimport random\nimport squarify\nimport matplotlib\nimport seaborn as sns\nimport matplotlib as mpl\nimport plotly.offline as py\nimport plotly_express as px\nfrom sklearn.svm import SVR\nimport statsmodels.api as sm\nfrom functools import partial\nfrom fbprophet import Prophet\nfrom tqdm.notebook import tqdm\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\nfrom lightgbm import LGBMRegressor\nfrom scipy.optimize import minimize\nfrom sklearn.pipeline import Pipeline\nfrom statsmodels.tsa.arima_model import ARIMA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nfrom fbprophet.plot import plot_plotly, add_changepoints_to_plot\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom statsmodels.tsa.stattools import adfuller, acf, pacf,arma_order_select_ic\n\nfrom IPython.display import Image\nwarnings.filterwarnings('ignore')\n%matplotlib inline\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section-one\"></a>\n# World COVID-19 Cases"},{"metadata":{},"cell_type":"markdown","source":"#### Multiple Data Source:\n\n* COVID19 Global Forecasting (Week 5)\n* COVID-19 in Turkey\n* COVID-19 useful features by country\n* COVID19 Daily Updates\n* Novel Corona Virus 2019 Dataset\n* Number of Covid-19 cases in the cities of Turkey\n* Python Folium Country Boundaries\n* Turkey Geoplot\n\nI received the \"World COVID-19 Cases\" from the following github links to study its worldwide spread and effects. It contains all cases until 23/9/2020.\nhttps://github.com/CSSEGISandData/COVID-19/tree/master/csse_covid_19_data"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"## Weights -> In the previous weeks (wk 2/3/4) of the competition phase weight was assigned according to the population of the region specified\n\nconfirmed_df = pd.read_csv('https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv')\ndeaths_df = pd.read_csv('https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_deaths_global.csv')\nrecovered_df = pd.read_csv('https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_recovered_global.csv')\nlatest_data = pd.read_csv('https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_daily_reports/04-04-2020.csv')\n\nworld_confirmed = confirmed_df[confirmed_df.columns[-1:]].sum()\nworld_recovered = recovered_df[recovered_df.columns[-1:]].sum()\nworld_deaths = deaths_df[deaths_df.columns[-1:]].sum()\nworld_active = world_confirmed - (world_recovered - world_deaths)\n\nlabels = ['Active','Recovered','Deceased']\nsizes = [world_active,world_recovered,world_deaths]\ncolor= ['red','green','black']\nexplode = []\n\nfor i in labels:\n    explode.append(0.05)\n\nplt.figure(figsize= (15,10))\nplt.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=8, explode =explode,colors = color)\ncentre_circle = plt.Circle((0,0),0.70,fc='white')\n\nfig = plt.gcf()\nfig.gca().add_artist(centre_circle)\nplt.title('World COVID-19 Cases',fontsize = 20)\nplt.axis('equal')  \nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section-two\"></a>\n## Global Deaths Heat Map"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"## DATA READING\n\ndf_deaths = pd.read_csv('https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_deaths_global.csv')\ndf_covid19 = pd.read_csv(\"https://raw.githubusercontent.com/CSSEGISandData/COVID-19/web-data/data/cases_country.csv\")\ndf_confirmed = pd.read_csv('https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv')\n\n## PRE-PROCESSING\n\ndf_confirmed = df_confirmed.rename(columns={\"Province/State\":\"state\",\"Country/Region\": \"country\"})\ndf_covid19 = df_covid19.drop([\"People_Tested\",\"People_Hospitalized\",\"UID\",\"ISO3\",\"Mortality_Rate\"],axis =1)\ndf_deaths = df_deaths.rename(columns={\"Province/State\":\"state\",\"Country/Region\": \"country\"})\ndf_covid19 = df_covid19.rename(columns={\"Country_Region\": \"country\"})\ndf_covid19[\"Active\"] = df_covid19[\"Confirmed\"]-df_covid19[\"Recovered\"]-df_covid19[\"Deaths\"]\n\n# Changing the conuntry names as required by pycountry_convert Lib\ndf_deaths.loc[df_deaths['country'] == \"US\", \"country\"] = \"USA\"\ndf_deaths.loc[df_deaths['country'] == 'Korea, South', \"country\"] = 'South Korea'\ndf_deaths.loc[df_deaths['country'] == 'Taiwan*', \"country\"] = 'Taiwan'\ndf_deaths.loc[df_deaths['country'] == 'Congo (Kinshasa)', \"country\"] = 'Democratic Republic of the Congo'\ndf_deaths.loc[df_deaths['country'] == \"Cote d'Ivoire\", \"country\"] = \"Côte d'Ivoire\"\ndf_deaths.loc[df_deaths['country'] == \"Reunion\", \"country\"] = \"Réunion\"\ndf_deaths.loc[df_deaths['country'] == 'Congo (Brazzaville)', \"country\"] = 'Republic of the Congo'\ndf_deaths.loc[df_deaths['country'] == 'Bahamas, The', \"country\"] = 'Bahamas'\ndf_deaths.loc[df_deaths['country'] == 'Gambia, The', \"country\"] = 'Gambia'\n\ncountries = np.asarray(df_confirmed[\"country\"])\ncountries1 = np.asarray(df_covid19[\"country\"])\n# Continent_code to Continent_names\ncontinents = {\n    'NA': 'North America',\n    'SA': 'South America', \n    'AS': 'Asia',\n    'OC': 'Australia',\n    'AF': 'Africa',\n    'EU' : 'Europe',\n    'na' : 'Others'}\n\n\n# Defininng Function for getting continent code for country.\ndef country_to_continent_code(country):\n    try:\n        return pc.country_alpha2_to_continent_code(pc.country_name_to_country_alpha2(country))\n    except :\n        return 'na'\n\n#Collecting Continent Information\ndf_deaths.insert(2,\"continent\",  [continents[country_to_continent_code(country)] for country in countries[:]])\ndf_covid19.insert(1,\"continent\",  [continents[country_to_continent_code(country)] for country in countries1[:]])\n\ndf_deaths[df_deaths[\"continent\" ]== 'Others']\ndf_deaths = df_deaths.replace(np.nan, '', regex=True)\n\ndf_countries_cases = df_covid19.copy().drop(['Lat','Long_','continent','Last_Update'],axis =1)\ndf_countries_cases.index = df_countries_cases[\"country\"]\ndf_countries_cases = df_countries_cases.drop(['country'],axis=1)\n\ndf_countries_cases.fillna(0,inplace=True)\n\n## VISUALIZATION\n\ntemp_df = pd.DataFrame(df_countries_cases['Deaths'])\ntemp_df = temp_df.reset_index()\nfig = px.choropleth(temp_df, locations=\"country\",\n                    color=np.log10(temp_df[\"Deaths\"]+1), \n                    hover_name=\"country\", \n                    hover_data=[\"Deaths\"],\n                    color_continuous_scale=px.colors.sequential.Plasma,locationmode=\"country names\")\nfig.update_geos(fitbounds=\"locations\", visible=False)\nfig.update_coloraxes(colorbar_title=\"Deaths (Log Scale)\",colorscale=\"Reds\")\n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Top 10 countries (Deaths)"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"f = plt.figure(figsize=(10,5))\nf.add_subplot(111)\n\nplt.axes(axisbelow=True)\nplt.barh(df_countries_cases.sort_values('Deaths')[\"Deaths\"].index[-10:],df_countries_cases.sort_values('Deaths')[\"Deaths\"].values[-10:],color=\"crimson\")\nplt.tick_params(size=5,labelsize = 13)\nplt.xlabel(\"Deaths Cases\",fontsize=18)\nplt.title(\"Top 10 Countries (Deaths Cases)\",fontsize=20)\nplt.grid(alpha=0.3,which='both')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section-three\"></a>\n## Active, Recovered, Deaths in Hotspot Countries"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"hotspots = ['China','Germany','Iran','Italy','Spain','US','Korea, South','France','Turkey','United Kingdom','India']\ndates = list(confirmed_df.columns[4:])\ndates = list(pd.to_datetime(dates))\ndates_india = dates[8:]\n\ndf1 = confirmed_df.groupby('Country/Region').sum().reset_index()\ndf2 = deaths_df.groupby('Country/Region').sum().reset_index()\ndf3 = recovered_df.groupby('Country/Region').sum().reset_index()\n\nglobal_confirmed = {}\nglobal_deaths = {}\nglobal_recovered = {}\nglobal_active= {}\n\nfor country in hotspots:\n    k =df1[df1['Country/Region'] == country].loc[:,'1/30/20':]\n    global_confirmed[country] = k.values.tolist()[0]\n\n    k =df2[df2['Country/Region'] == country].loc[:,'1/30/20':]\n    global_deaths[country] = k.values.tolist()[0]\n\n    k =df3[df3['Country/Region'] == country].loc[:,'1/30/20':]\n    global_recovered[country] = k.values.tolist()[0]\n    \nfor country in hotspots:\n    k = list(map(int.__sub__, global_confirmed[country], global_deaths[country]))\n    global_active[country] = list(map(int.__sub__, k, global_recovered[country]))\n    \nfig = plt.figure(figsize= (15,25))\nplt.suptitle('Active, Recovered, Deaths in Hotspot Countries and India as of May 15',fontsize = 13,y=1.0)\n#plt.legend()\nk=0\nfor i in range(1,12):\n    ax = fig.add_subplot(6,2,i)\n    ax.xaxis.set_major_formatter(mdates.DateFormatter('%d-%b'))\n    ax.bar(dates_india,global_active[hotspots[k]],color = 'red',alpha = 0.6,label = 'Active');\n    ax.bar(dates_india,global_recovered[hotspots[k]],color='green',label = 'Recovered');\n    ax.bar(dates_india,global_deaths[hotspots[k]],color='black',label = 'Death');   \n    plt.title(hotspots[k])\n    handles, labels = ax.get_legend_handles_labels()\n    fig.legend(handles, labels, loc='upper left')\n    k=k+1\n\nplt.tight_layout(pad=3.0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section-four\"></a>\n## US Heatmap (Confirmed Cases)"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"us = latest_data.loc[latest_data['Country_Region'] == 'US']\nus.drop('Admin2', axis=1, inplace=True)\n\nfrom urllib.request import urlopen\nimport json\nwith urlopen('https://raw.githubusercontent.com/plotly/datasets/master/geojson-counties-fips.json') as response:\n    counties = json.load(response)\n\nus_min = us[\"Confirmed\"].min()\nus_mean = us[\"Confirmed\"].mean()\nus_max = us[\"Confirmed\"].max()\nus_med = us[\"Confirmed\"].median()\n\nfig = px.choropleth_mapbox(us, geojson=counties, locations=\"FIPS\", color='Confirmed',\n                           hover_name=\"Province_State\",\n                           color_continuous_scale=\"OrRd\",\n                           range_color=(us_med,us_mean),\n                           mapbox_style=\"carto-positron\",\n                           zoom=3, center = {\"lat\": 37.0902, \"lon\": -95.7129},\n                           opacity=0.4,\n                           labels={'Confirmed':'Confirmed Case Number'}\n                          )\nfig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section-four-1\"></a>\n## Average Age Distribution of Cases in Countries"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"cluster_data = pd.read_csv(\"../input/covid19-useful-features-by-country/Countries_usefulFeatures.csv\")\nage_df = cluster_data[[\"Country_Region\",\"Mean_Age\"]]\nsns.distplot(a=age_df['Mean_Age'], kde=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section-five\"></a>\n# Turkey"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/covid19-global-forecasting-week-5/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/covid19-global-forecasting-week-5/test.csv\")\n\ntrain_copy = train.copy()\ntest_copy = test.copy()\n\ntrain['day']=pd.to_datetime(train.Date,format='%Y-%m-%d').dt.day\ntrain['month']=pd.to_datetime(train.Date,format='%Y-%m-%d').dt.month\n\ntest['day']=pd.to_datetime(test.Date,format='%Y-%m-%d').dt.day\ntest['month']=pd.to_datetime(test.Date,format='%Y-%m-%d').dt.month\n\ntrain.columns = map(str.lower, train.columns)\ntrain = train.rename(columns = {'county': 'country', 'province_state': 'state', 'country_region': 'region', 'target': 'case', 'targetvalue':'case_value'}, inplace = False)\n\ntc_data = pd.read_csv(\"../input/number-of-cases-in-the-city-covid19-turkey/number_of_cases_in_the_city.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section-five-one\"></a>\n### Top 5 Cities with the Highest Number of Cases"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"tc_list = list(range(1, 82))\ntc_data.insert(0, \"id\", tc_list, True) \n\nimport plotly.express as px\n\nmore_case = tc_data.sort_values(by='Number of Case', ascending=False)\n\nfig = px.pie(\n    more_case.head(5),\n    values = \"Number of Case\",\n    names = \"Province\",\n    color_discrete_sequence = px.colors.sequential.RdBu)\n\nfig.update_traces(textposition=\"inside\", textinfo=\"percent+label\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section-five-two\"></a>\n### Turkey Heatmap (Number of Case)"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import plotly.express as px\n\n# loading Turkey's geoplot json file\nfrom urllib.request import urlopen\nimport json\nwith open(\"../input/geoplot/tr-cities-utf8.json\") as f:\n    cities = json.load(f)\n\nmini = tc_data[\"Number of Case\"].min()\naverage = tc_data[\"Number of Case\"].mean()\n#tc_data.drop('id', axis=1, inplace=True)\n    \nfig = px.choropleth_mapbox(tc_data, geojson=cities, locations=tc_data.id, color=(tc_data[\"Number of Case\"]),\n                           hover_name=\"Province\",\n                           range_color= (mini,average),\n                           color_continuous_scale='amp',\n                           mapbox_style=\"carto-positron\",\n                           zoom=4, opacity=0.7,center = {\"lat\": 38.963745, \"lon\": 35.243322},\n                           labels={'color':'Number of Case'})\n\nfig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section-five-three\"></a>\n### 10 Cities with the Lowest Number of Cases"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"less_case = tc_data.sort_values(by='Number of Case', ascending=True)\n\nfig = px.bar(\n    less_case.head(10),\n    x = \"Province\",\n    y = \"Number of Case\")\nfig.update_layout(barmode=\"group\")\nfig.update_traces(marker_color='rosybrown')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Fatalities vs Confirmed Cases"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"import matplotlib as mpl\n\ntc = train.loc[train.region == 'Turkey']\n\ntc.drop('country', axis=1, inplace=True)\ntc.drop('state', axis=1, inplace=True)\ntc.drop('region', axis=1, inplace=True)\ntc.drop('population', axis=1, inplace=True)\n\ntc_1=tc['case_value'].groupby(tc['case']).sum()\n\nfatal_tc=tc[tc['case']=='Fatalities']\nconf_tc=tc[tc['case']=='ConfirmedCases']\n\nlabels =[tc_1.index[0],tc_1.index[1]]\nsizes = [tc_1[0],tc_1[1]]\nexplode = (0, 0.08)  \nplt.figure(figsize = (8,8))\n\nplt.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%',textprops={'fontsize': 14},startangle=90)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section-six\"></a>\n# Turkey COVID-19 Forecasting"},{"metadata":{},"cell_type":"markdown","source":"### Plotting the Features to see trends\n* Covid cases have strong daily and monthly properties.\n\n#### Concepts:\n* Trend: As the name suggests trend depicts the variation in the output as time increases.It is often non-linear. Sometimes we will refer to trend as “changing direction” when it might go from an increasing trend to a decreasing trend.\n\n* Level: It basically depicts baseline value for the time series.\n\n* Seasonal: As its name depicts it shows the repeated pattern over time. In layman terms, it shows the seasonal variation of data over time.\n\n* Noise: It is basically external noises that vary the data randomly."},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section-six-one\"></a>\n### Confirmed Case in Time Intervals"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"px.line(data_frame=conf_tc, x=\"date\", y='case_value',hover_name=\"case\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Implications and Causation:\n* The first confirmed cases in Turkey were found Covidien-19 on March 11.\n* Before that, on February 3, Turkey has announced that it stop all flights from China.\n* Turkey on February 29, announced that flights with Italy, South Korea and Iraq were mutually suspended.\n* Soon, the Iraqi border was also closed. The ministry also established field hospitals close to the Iraqi and Iranian borders.\n* On March 11, Health Minister Fahrettin Koca announced that a Turkish man caught the virus while traveling to Europe was the country's first coronavirus case. The patient was isolated to a hospital and family members of the patient were observed.\n* During the month of March, many activities such as sports leagues, horse races, barbecues in gardens, parks and recreation areas were stopped.<br>\n\n#### Why It Increased?\n* It peaked on April 11.\n* As a result of incomplete reporting of case numbers, people did not take the disease seriously.\n* The fact that the virus load is very high indicates that these people stay together in large numbers and for a long time in closed spaces and therefore are exposed to very intense / large amount of virus attacks and that there is a lot of inter-human contact in these places. The amount of virus you come into contact with is important in terms of how you will overcome the disease.\n* People with chronic diseases (obesity, inactivity is also considered a chronic disease) could not protect themselves and got infected.\n* Effective drugs were not in use.\n* The increased hospital load could not be balanced.\n* People did not listen to the Stay At Home call and continued on domestic travel. Necessary measures could not be taken early for jobs and schools, the interaction continued for a long time.\n------------------------------------------\n\n### Çıkarımlar ve Nedensellik:\n* Türkiye'de ilk teyit edilen covid-19 vakası 11 Mart'ta bulundu.\n* Ondan önce 3 Şubat'ta Türkiye, Çin'den gelen tüm uçuşları durdurduğunu açıkladı. \n* 29 Şubatta Türkiye; İtalya, Güney Kore ve Irak ile uçuşların karşılıklı olarak durdurulduğunu açıkladı.\n* Kısa süre sonra Irak sınırı da kapatıldı. Bakanlık ayrıca Irak ve İran sınırlarına yakın saha hastaneleri kurdu.\n* 11 Mart'ta Sağlık Bakanı Fahrettin Koca, Avrupa'ya seyahat ederken virüse yakalanan bir Türk erkeğin ülkenin ilk koronavirüs vakası olduğunu açıkladı. Hasta bir hastaneye tecrit edildi ve hastanın aile üyeleri gözlem altına alındı.\n* Mart ayı süresince spor ligleri, at yarışları, bahçe, park ve mesire alanlarında mangal yakılması gibi birçok aktivite durduruldu.<br>\n\n#### Neden Arttı? \n* **11 Nisan'da** pik yaptı.\n* **Vaka sayılarının eksik bildirimi** sonucunda,insanlar hastalığı ciddiye almadı.\n* **Virüs yükünün çok yüksek olması,** bu insanlarımızın kapalı mekânlarda çok sayıda bir arada ve uzun süre kaldıklarını ve bu nedenle çok yoğun / çok miktarda virüs saldırısına maruz kaldıklarını ve bu mekânlarda insanlar arası temasın çok olduğunu gösteriyor. Temasa geldiğiniz virüs miktarı hastalığı nasıl atlatacağınız açısından önemli.\n* **Kronik hastalıkları** olan (obezlik, hareketsizlik de bir kronik hastalık sayılır) insanların kendilerini koruyamayıp virüs kaptılar.\n* **Etkili ilaçlar** devrede değildi.\n* **Artan hastane yükü** dengelenemedi.\n* İnsanlar **Evde Kalın** çağrısını dinlemedi ve yurtiçi seyehatlare devam ettiler. İş ve okullar için erkenden gerekli önlem alınamadı, uzun bir süre etkileşim devam etti.\n"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section-six-two\"></a>\n### Fatalities Case in Time Intervals"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"fig = px.line(data_frame=fatal_tc, x=\"date\", y='case_value',hover_name=\"case\", color_discrete_map={'case_value': 'red'})\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* The first fatalities case of covid-19 in Turkey was found on 17 March"},{"metadata":{},"cell_type":"markdown","source":"### Changes in the Number of Cases by Months"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"with plt.style.context('fivethirtyeight'):\n    dategroup=tc.groupby('month').mean()\n    fig, ax = plt.subplots(figsize=(20,6))\n    ax.xaxis.set(ticks=range(0,13)) # Manually set x-ticks\n    dategroup['case_value'].plot(x=tc.month)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* In this graph, we ca see that the cases peaked in April."},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section-six-four\"></a>\n# TIME SERIES MODEL"},{"metadata":{},"cell_type":"markdown","source":"1. How can techniques we will use to create the model help us deepen the understandings from EDA?\n    - While our human eyes used to focus more on what happened already,machine may look beyond to the future objectively and systematically.\n2. How much worthy or useful is it to predict the number of infections or deceased victims?\n    - It could help decision makers select more efficient and timely measures to tackle the new cases\n    - It might be helpful for individuals to know how much we need to be serious on preventing efforts\n3. What could be fundamental or neccessary too for anyone in the storm of pandemic?\n    - A general forecasting model not only for Turkey but for all countries (especially for the ones showing the early pattern of spreading.)\n    - Ways of allocating necessary medical resources nationwide or even worldwide (e.g. ventilator) (from the places with the decreasing trend to those with the increasing one.)\n-----\n1. Modeli oluştururken kullanacağımız teknikler, EDA'yı derinleştirmemize nasıl yardımcı olabilir?\n     - İnsan gözü önceden olanlara daha çok odaklanırken, oluşturacağımız modelle geleceğin ötesine nesnel ve sistematik olarak bakabiliriz.\n2. Enfektelerin veya ölen kurbanların sayısını tahmin etmek ne kadar değerli veya yararlıdır?\n     - Karar vericilerin yeni vakaların üstesinden gelmek için daha verimli ve zamanında önlemler seçmelerine yardımcı olabilir.\n     - Bireylerin hastalığı önleme konusundaki çabalarının ne kadar ciddi olması gerektiğini bilmelerinde faydalı olabilir.\n3. Pandemi sürecinde olan herhangi biri için temel veya gerekli olanlar nedir, ne olabilir?\n     - Sadece Türkiye için değil, tüm ülkeler için (özellikle erken yayılma modelini gösteren ülkeler için) genel bir tahmin modeli sunar.\n     - Ulusal ve hatta dünya çapında gerekli tıbbi kaynakları tahsis etme yolları konusunda yardımcı olabilir. (örn. test kiti)\n    "},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section-six-four-1\"></a>\n### Importing Libraries"},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"from pandas import Series\nfrom math import sqrt\n\n# metrics\nfrom sklearn.metrics import mean_squared_error\nimport statsmodels.api as sm\n\n# forecasting model\nfrom statsmodels.tsa.api import ExponentialSmoothing, SimpleExpSmoothing, Holt\nfrom statsmodels.tsa.arima_model import ARIMA\n\n# for analysis\nfrom statsmodels.tsa.stattools import adfuller\nfrom statsmodels.tsa.stattools import acf, pacf\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom shapely.geometry import LineString\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import plot\nimport seaborn as sns\nfrom matplotlib.pylab import rcParams\nrcParams['figure.figsize'] = 12, 7\n\nfrom IPython.display import display, HTML\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section-six-four-2\"></a>\n### Prepearing Data"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"train_original=pd.read_csv('../input/covid19-global-forecasting-week-5/train.csv')\ntest_original=pd.read_csv('../input/covid19-global-forecasting-week-5/test.csv')\ntrain_original.sample(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Data Cleaning and Generating Date:"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# Train Data Cleaning\ntrain_original=train_original.drop([\"County\"], axis=1)\ntrain_original=train_original.drop([\"Province_State\"], axis=1)\ntrain_original=train_original.drop([\"Population\"], axis=1)\ntrain_original=train_original.drop([\"Weight\"], axis=1)\ntrain_original=train_original.drop([\"Id\"], axis=1)\n\ntrain_original = pd.DataFrame(train_original[(train_original['Country_Region'] == 'Turkey') & (train_original['Target'] == 'ConfirmedCases')])\ntrain_original=train_original.drop([\"Country_Region\"], axis=1)\ntrain_original=train_original.drop([\"Target\"], axis=1)\n\n# Test Data Cleaning\ntest_original=test_original.drop([\"County\"], axis=1)\ntest_original=test_original.drop([\"Province_State\"], axis=1)\ntest_original=test_original.drop([\"Population\"], axis=1)\ntest_original=test_original.drop([\"Weight\"], axis=1)\n\ntest_original = pd.DataFrame(test_original[(test_original['Country_Region'] == 'Turkey') & (test_original['Target'] == 'ConfirmedCases')])\ntest_original=test_original.drop([\"Country_Region\"], axis=1)\ntest_original=test_original.drop([\"Target\"], axis=1)\n\ntest_original.dropna(inplace=True)\ntest_original.dropna(inplace=True)\ntest_original.drop(test_original.tail(1).index, inplace=True)\n\ntrain_df=train_original.copy()\ntest_df=test_original.copy()\n\ntrain_original['Date']=pd.to_datetime(train_original.Date, format='%Y/%m/%d')\ntest_original['Date']=pd.to_datetime(test_original.Date, format='%Y/%m/%d')\ntrain_df['Date']=pd.to_datetime(train_df.Date, format='%Y/%m/%d')\ntest_df['Date']=pd.to_datetime(test_df.Date, format='%Y/%m/%d')\n\n# generate day, month, year feature\nfor i in (train_original, test_original, train_df, test_df):\n    i['year']=i.Date.dt.year\n    i['month']=i.Date.dt.month\n    i['day']=i.Date.dt.day\n    i['hour']=i.Date.dt.hour\n    \n# sampling for daily basis\ntrain_df.index=train_df.Date\ntest_df.index=test_df.Date\n\ntrain_df=train_df.resample('D').mean()\ntest_df=test_df.resample('D').mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head(3) #Last Version","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section-six-four-3\"></a>\n### Spliting Data for Training and Validation"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"train=train_df.loc['2020-04-11':'2020-05-16']\nvalid=train_df.loc['2020-05-17':'2020-07-10']\nplt.figure(figsize=(20,7))\n\ntrain.TargetValue.plot(label='Train data')\nvalid.TargetValue.plot(label='Valid data')\nplt.legend(loc='best')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section-six-four-4\"></a>\n### Determine Rolling Stats"},{"metadata":{},"cell_type":"markdown","source":"* A rolling analysis of a time series model is often used to assess the model's stability over time. \n\n-----\n\n* Bir zaman serisi modelinin rolling analizi, genellikle modelin zaman içindeki kararlılığını değerlendirmek için kullanılır."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"rolmean=train.TargetValue.rolling(window=7).mean() #for 7 days -> roll.mean: pencere gezdirip ortalama alma\nrolstd=train.TargetValue.rolling(window=7).std()\nrolmean.dropna(inplace=True)\nrolstd.dropna(inplace=True)\n\nplt.figure(figsize=(17,7))\nrolmean.plot(label='Rolmean', color='green')\nrolstd.plot(label='rolstd')\ntrain.TargetValue.plot(label='Train')\nplt.legend(loc='best')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section-six-four-5\"></a>\n### Check for Stationary"},{"metadata":{},"cell_type":"markdown","source":"#### What does it mean for data to be stationary?\n\n- The mean of the series should not be a function of time. We will see if this series is stationary by examining the Decomposition result in Trend.\n\n#### Why is this important? \n- When running a linear regression the assumption is that all of the observations are all independent of each other. In a time series, however, we know that observations are time dependent. It turns out that a lot of nice results that hold for independent random variables (law of large numbers and central limit theorem to name a couple) hold for stationary random variables. So by making the data stationary, we can actually apply regression techniques to this time dependent variable.\n\n- There are two ways you can check the stationarity of a time series. The first is by looking at the data. By visualizing the data it should be easy to identify a changing mean or variation in the data. For a more accurate assessment there is the Dickey-Fuller test. I won’t go into the specifics of this test, but if the ‘Test Statistic’ is greater than the ‘Critical Value’ than the time series is stationary. Below is code that will help you visualize the time series and test for stationarity.\n-----\n- Veriyi durağan(stationary) hale getirerek, regresyon tekniklerini verilen zamana bağlı değişkene gerçekten de uygulayabiliriz.\n<img src=\"https://miro.medium.com/max/390/0*3XXCQed3bPHrD1lt.png\">"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"dftest=adfuller(train.TargetValue, autolag='AIC')\ndfout=pd.Series(dftest[0:4], index=['Test statistics', 'p-value', '#Lags used', 'Number of observation used'])\nfor key, val in dftest[4].items():\n    dfout['Critical value (%s)'%key]=val\n\nprint(dfout)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* The smaller p-value, the more likely it's stationary. Here our p-value is 0.603053. It's actually not that bad, if we use a 5% Critical Value(CV), this series would be considered stationary. But as we just visually found an upward trend, we want to be more strict, we use 1% CV.\n* To get a stationary data, there's many techiniques. We can use log, differencing etc..."},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section-six-four-6\"></a>\n### Log scale tranformation\n\n* Plot your graph of the data against time. If it looks like the variation increases with the level of the series, take logs. Otherwise model the original data.\n* If it seems to be linear, than no need for logs."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# estimating trend\ntrain_count_log=np.log(train.TargetValue)\n\n# make TS to be stationary\nmoving_avg=train_count_log.rolling(window=7).mean()\nmoving_std=train_count_log.rolling(window=7).std()\nplt.figure(figsize=(17,7))\n\ntrain_count_log.plot(label='Log Scale')\nmoving_avg.plot(label='moving_avg')\nmoving_std.plot(label='moving_std')\n\nplt.legend(loc='best')","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# Varyasyonun yüksek olduğu yer.\ndif_log=train_count_log-moving_avg\ndif_log.dropna(inplace=True)\ndif_log.plot()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def test_stationary(timeseries):\n    # determine roling stats\n    mov_avg=timeseries.rolling(window=7).mean()\n    mov_std=timeseries.rolling(window=7).std()\n    #plot rolling stats\n    plt.figure(figsize=(12,7))\n    timeseries.plot(label='Original')\n    mov_avg.plot(label='Mov avg')\n    mov_std.plot(label='Mov std')\n    plt.legend(loc='best')\n    plt.title('Rolling mean & standard deviation')\n    \n    # dickey-fuller test\n    print('Result of Dickey-fuller test')\n    dftest=adfuller(timeseries, autolag='AIC')\n    dfout=pd.Series(dftest[:4], index=['Test stats', 'p-value', '#Lag used', 'Number of observation used'])\n    for key, val in dftest[4].items():\n        dfout['Critical value (%s)'%key]=val\n    print(dfout)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_stationary(dif_log)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After log transformation, our p-value is extremely small. Thus this series is very likely to be stationary."},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section-six-four-7\"></a>\n### Exponential Decay Transformation"},{"metadata":{},"cell_type":"markdown","source":"* Exponential Decay Transformation is a time series forecasting method for univariate data that can be extended to support data with a systematic trend or seasonal component. Specifically, past observations are weighted with a geometrically decreasing ratio.\n\n------\n\n* Exponential Decay Transformation, verileri sistematik bir eğilim veya mevsimsel bileşenle desteklemek için genişletilebilen tek değişkenli veriler için bir zaman serisi tahmin yöntemidir. Spesifik olarak, geçmiş gözlemler geometrik olarak azalan bir oranla ağırlıklandırılır."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,7))\nedw_avg=train_count_log.ewm(halflife=7, min_periods=0, adjust=True).mean()\ntrain_count_log.plot(label='Log scale')\nedw_avg.plot(label='Exponential Decay Weight MA')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section-six-four-8\"></a>\n### ADCF Test"},{"metadata":{},"cell_type":"markdown","source":"* Augmented Dickey–Fuller test is the most accepted determination of stationarity in the literature and it is accepted as the most valid test in determining stationarity in time series.\n\n------\n\n* Augmented Dickey–Fuller testi, literatürde en çok kabul gören durağanlık tespitidir ve zaman serisi konusunda da durağanlığın tespitinde en geçerli test olarak kabul edilmiştir (Enders, 1995)."},{"metadata":{},"cell_type":"markdown","source":"The partial autocorrelation at lag k is the correlation that results after removing the effect of any correlations due to the terms at shorter lags.\n\n#### Autoregression Intuition\n- Consider a time series that was generated by an autoregression (AR) process with a lag of k.\n\n- We know that the ACF describes the autocorrelation between an observation and another observation at a prior time step that includes direct and indirect dependence information.\n\n- This means we would expect the ACF for the AR(k) time series to be strong to a lag of k and the inertia of that relationship would carry on to subsequent lag values, trailing off at some point as the effect was weakened.\n\n- We know that the PACF only describes the direct relationship between an observation and its lag. This would suggest that there would be no correlation for lag values beyond k.\n\n- This is exactly the expectation of the ACF and PACF plots for an AR(k) process."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"dif_edw=train_count_log-edw_avg\ndif_edw = dif_edw.replace([np.inf, -np.inf], np.nan)\ndif_edw.dropna(inplace=True)\ntest_stationary(dif_edw)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section-six-four-9\"></a>\n### Time Shift Transformation"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"dif_shift=train_count_log-train_count_log.shift()\ndif_shift = dif_shift.replace([np.inf, -np.inf], np.nan)\ndif_shift.dropna(inplace=True)\ntest_stationary(dif_shift)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section-six-four-10\"></a>\n### Decomposition\n\n- To start with, we want to decompose the data to seperate the trend. Since we have 3 months of confirmed case data, we would expect there's a  monthly or weekly pattern. Let's use a function in statsmodels to help us find it."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"decom=seasonal_decompose(dif_edw, freq=3)\n\ntrend=decom.trend\nseasonal=decom.seasonal\nresidual=decom.resid\n\nfig=plt.figure(figsize=(15,8))\nplt.subplot(211)\ntrain_count_log.plot(label='Original')\nplt.title(\"Original\")\nplt.subplot(212)\ntrend.plot(label='Trend')\nplt.title(\"Trend\")\n\n'''\nplt.subplot(413)\nseasonal.plot(label='Seasonal')\nplt.title(\"Seasonal\")\nplt.subplot(414)\nresidual.plot(label='Residual')\nplt.title(\"Residual\")\nfig.tight_layout()\n'''\n\ndecom_log_data=residual\ndecom_log_data = decom_log_data.replace([np.inf, -np.inf], np.nan)\ndecom_log_data.dropna(inplace=True)\n#test_stationary(decom_log_data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section-six-four-11\"></a>\n### Building Model"},{"metadata":{},"cell_type":"markdown","source":"#### How to determine p, d, q\n- In our case, we see the first order differencing make the ts stationary. \n\n- AR model might be investigated first with lag length selected from the PACF or via empirical investigation. In our case, it's clearly that within 4 lags the AR is significant. Which means, we can use AR = 4\n\n- To avoid the potential for incorrectly specifying the MA order (in the case where the MA is first tried then the MA order is being set to 0), it may often make sense to extend the lag observed from the last significant term in the PACF.\n\n- What is interesting is that when the AR model is appropriately specified, the the residuals from this model can be used to directly observe the uncorrelated error. This residual can be used to further investigate alternative MA and ARMA model specifications directly by regression.\n\n- Assuming an AR(s) model were computed, then I would suggest that the next step in identification is to estimate an MA model with s-1 lags in the uncorrelated errors derived from the regression. The parsimonious MA specification might be considered and this might be compared with a more parsimonious AR specification. Then ARIMA models might also be analysed.\n-----\n#### Özbağlanımlı Model (Autoregressive model):\n- İstatistik, ekonometri ve sinyal işlemede, otoregresif bir model bir tür rasgele sürecin temsilidir; bu haliyle, doğa, ekonomi, vb. zamana göre belirli zamanla değişen süreçleri tanımlamak için kullanılır\n\n#### Moving-average model\n* Zaman serisi analizinde, hareketli ortalama süreci olarak da bilinen hareketli ortalama modeli, tek değişkenli zaman serilerini modellemek için yaygın bir yaklaşımdır. Hareketli ortalama modeli, çıktı değişkeninin doğrusal olarak stokastik bir terimin mevcut ve çeşitli geçmiş değerlerine bağlı olduğunu belirtir. ARIMA ise ikisinin birleşimidir."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"## AR MODEL:\n\ntrain_count_log = train_count_log.replace([np.inf, -np.inf], np.nan)\ntrain_count_log.dropna(inplace=True)\nmodel=ARIMA(train_count_log, order=(4,1,0))\nresults_AR=model.fit(disp=0)\n\n'''\nThe Residual sum of Squares (RSS) is defined as below and is used in the Least Square Method \nin order to estimate the regression coefficient.\nThe smallest residual sum of squares is equivalent to the largest r squared.\nThe deviance calculation is a generalization of residual sum of squares.\nSquared loss = (y−y^)2\n'''\n\nplt.figure(figsize=(18,6))\ndif_edw.plot(label='Exponentian Decay Differentiation')\nresults_AR.fittedvalues.dropna(inplace=True)\nresults_AR.fittedvalues.plot(label='Results AR')\ndf=pd.concat([results_AR.fittedvalues, dif_edw], axis=1).dropna()\nplt.title('AR MODEL /RSS: %.4f'%sum((df[0]-df['TargetValue'])**2))","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"## MA MODEL\nmodel=ARIMA(train_count_log, order=(2,1,1))\nresults_MA=model.fit(disp=0)\n\nplt.figure(figsize=(18,6))\ndif_edw.plot(label='Exponentian Decay Differentiation')\nresults_MA.fittedvalues.dropna(inplace=True)\nresults_MA.fittedvalues.plot(label='Results AR')\ndf=pd.concat([results_MA.fittedvalues, dif_edw], axis=1).dropna()\nplt.title('MA MODEL /RSS: %.4f'%sum((df[0]-df['TargetValue'])**2))","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"## ARIMA MODEL\nmodel=ARIMA(train_count_log, order=(4,1,2))\nresults_ARIMA=model.fit(disp=0)\n\nplt.figure(figsize=(18,6))\ndif_edw.plot(label='Exponentian Decay Differentiation')\nresults_ARIMA.fittedvalues.dropna(inplace=True)\nresults_ARIMA.fittedvalues.plot(label='Results AR')\ndf=pd.concat([results_ARIMA.fittedvalues, dif_edw], axis=1).dropna()\nplt.title('ARIMA MODEL /RSS: %.4f'%sum((df[0]-df['TargetValue'])**2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### ARIMA Pros:\n\n* Intepretability: Each coefficient means a specific thing ts key elements understanding: the concept of lags, and error lag terms are very unique, ARIMA gave a comprehensive cover on them. So even in the future I want to try some other regression model. I would add the lag terms and consider the error term.\n\n#### ARIMA Cons:\n\n* Inefficiency: ARIMA needs to be run on each time series, since we have 500 store/item combinations, it needs to run 500 times. Every time we want to forecast the future, say on Jan 2, 2018, we want to forecast next 90 days. We need to re-run ARIMA.\n\n-----\n\n#### ARIMA Artıları:\n\n* Anlaşılabilirlik: Her bir katsayı, belirli bir şey anlamına gelir ve temel unsurların anlaşılması, gecikme kavramı ve hata gecikme terimleri çok benzersizdir, ARIMA bunları kapsamlı bir şekilde ele alır. Bu yüzden gelecekte bile başka bir regresyon modeli denemek istiyorsak gecikme katsayılarını ekler ve hata metriklerini dikkate alabiliriz.\n\n#### ARIMA Eksileri:\n\n* Verimsizlik: ARIMA'nın her zaman serisinde çalıştırılması gerekir, çünkü örneğin 500 mağaza / ürün kombinasyonumuz olduğundan, 500 kez çalıştırılması gerekir. Geleceği tahmin etmek istersek, örneğin önümüzdeki 90 günü tahmin etmek istediğimizde ARIMA'yı yeniden çalıştırmamız gerekiyor."},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section-six-four-12\"></a>\n### Prediction & Reverse Transformations"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# using AR model\npred_ar_dif=pd.Series(results_AR.fittedvalues, copy=True)\npred_ar_dif_cumsum=pred_ar_dif.cumsum()\n\npred_ar_log=pd.Series(train_count_log.iloc[0], index=train_count_log.index)\npred_ar_log=pred_ar_log.add(pred_ar_dif_cumsum, fill_value=0)\npred_ar_log.head()\n\n# inverse of log is exp\npred_ar=np.exp(pred_ar_log)\nplt.figure(figsize=(12,7))\ntrain.TargetValue.plot(label='Train')\npred_ar.plot(label='Pred')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section-six-four-13\"></a>\n### Validation "},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def validation(order):\n    # forecasting for validation\n    valid_count_log=list(np.log(valid.TargetValue).values)\n    history = list(train_count_log.values)\n    model = ARIMA(history, order=order)\n    model_fit = model.fit(disp=0)\n    output = model_fit.forecast(steps=len(valid))\n    mse = mean_squared_error(valid_count_log, output[0])\n    rmse = np.sqrt(mse)\n    print('Test MSE: %.3f' % mse)\n    print('Test RMSE: %.3f' % rmse)\n    \n    fig=plt.figure(figsize=(12,7))\n    # reverse transform\n    pred=np.exp(output[0])\n    pred=pd.Series(pred, index=valid.index)\n    valid.TargetValue.plot(label='Valid')\n    pred.plot(label='Pred')\n    plt.legend(loc='best')\n    \n    fig=plt.figure(figsize=(18,7))\n    train.TargetValue.plot(label='Train')\n    valid.TargetValue.plot(label='Valid')\n    pred.plot(label='Pred', color='black')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"validation((2,1,2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section-six-four-14\"></a>\n### Test Forecasting"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"'''def arima_predict_hourly(data, arima_order):\n    # forecasting for testing (Daily based forecasting)\n    \n    history = data\n    model = ARIMA(history, order=arima_order)\n    model_fit = model.fit(disp=0)\n    output = model_fit.forecast(steps=len(test_original))\n\n    submit=test_original.copy()\n    submit.index=submit.ID\n    submit['Count']=np.exp(output[0])\n    submit.drop(['Unnamed: 0','ID','Datetime','year','month','day','hour'], axis=1, inplace=True)\n    \n    # plot result\n    plt.figure(figsize=(12,7))\n    train_original.index=train_original.Datetime\n    submit.index=test_original.Datetime\n\n    train_original.TargetValue.plot(label='Train')\n    submit.TargetValue.plot(label='Pred')\n    return submit'''\n\nfrom pandas import DataFrame\n# forecasting for testing (daily based forecasting)\n\nh = list(np.log(train_original.TargetValue).values)\nhistory = DataFrame(h,columns=['values'])\nhistory = history.replace([np.inf, -np.inf], np.nan)\nhistory.fillna(0, inplace=True)\n\nmodel = ARIMA(history, order=(2,0,1))\nmodel_fit = model.fit(disp=0)\noutput = model_fit.forecast(steps=len(test_original))\n\nsubmit=test_original.copy()\nsubmit.index=submit.ForecastId\nsubmit['TargetValue']=np.exp(output[0])\nsubmit.drop(['ForecastId','Date','year','month','day','hour'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# plot result\nplt.figure(figsize=(18,7))\ntrain_original.index=train_original.Date\nsubmit.index=test_original.Date\n\ntrain_original.TargetValue.plot(label='Train')\nsubmit.TargetValue.plot(label='Pred')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section-six-four-15\"></a>\n### ARIMA PDQ Param Tuning"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# evaluate an ARIMA model for a given order (p,d,q)\ndef evaluate_arima_model(arima_order):\n    # forecasting for validation\n    valid_count_log=list(np.log(valid.TargetValue).values)\n    history = list(train_count_log.values)\n    model = ARIMA(history, order=arima_order)\n    model_fit = model.fit(disp=0)\n    output = model_fit.forecast(steps=len(valid))\n    mse = mean_squared_error(valid_count_log, output[0])\n    rmse = np.sqrt(mse)\n#     print('Test MSE: %.3f' % mse)\n#     print('Test RMSE: %.3f' % rmse)\n    return mse\n\n\n# evaluate combinations of p, d and q values for an ARIMA model\ndef evaluate_models(p_values, d_values, q_values):\n    best_score, best_cfg = float(\"inf\"), None\n    for p in p_values:\n        for d in d_values:\n            for q in q_values:\n                order = (p,d,q)\n                try:\n                    mse = evaluate_arima_model(order)\n                    if mse < best_score:\n                        best_score, best_cfg = mse, order\n                    print('ARIMA%s MSE=%.3f' % (order,mse))\n                except:\n                    continue\n    print('Best ARIMA%s MSE=%.3f' % (best_cfg, best_score))","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# evaluate parameters\np_values = [0, 1, 2, 4, 6, 8]\nd_values = range(0, 3)\nq_values = range(0, 3)\nwarnings.filterwarnings(\"ignore\")\nevaluate_models(p_values, d_values, q_values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ARIMA PDQ Param Tuning said that BEST ARIMA -> (4,1,0)\nvalidation((4,1,0))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Further steps:\n* Improving the current model by using different techniques and based on different metrics.\n* To make longer term predictions with a better understanding of certain concepts.\n* With the arrival of new data, to make more detailed estimates based on city / region.\n-----\n#### İleriki adımlar:\n* Daha farklı teknikler kullanılıp ve farklı metrikler baz alınarak şuanki modelin iyileştirilmesi.\n* Belli kavramların daha iyi anlaşılmasıyla birlikte daha uzun vadeli tahminler yapmak.\n* Yeni dataların gelmesiyle de birlikte şehir/bölge bazlı daha detaylı tahminler yapmak."},{"metadata":{},"cell_type":"markdown","source":"### TO SEE GLOBAL FORECASTING, PLEASE GO PART.2 -> https://www.kaggle.com/thepinokyo/esin-part-2-covid19"},{"metadata":{},"cell_type":"markdown","source":"# STAY AT HOME AND DO KAGGLE\n<img src=\"https://i.hizliresim.com/dOUeA3.jpg\">"},{"metadata":{},"cell_type":"markdown","source":"### REFERENCES\n* https://github.com/CSSEGISandData/COVID-19/tree/master/csse_covid_19_data\n* https://www.who.int/health-topics/coronavirus#tab=tab_1\n* https://www.stat.tamu.edu/~jnewton/stat626/topics/topics/topic5.pdf\n* https://tr.wikipedia.org/wiki/Türkiye%27de_COVID-19_pandemisi_zaman_çizelgesi\n* https://www.herkesebilimteknoloji.com/yazarlar/orhan-bursali/turkiyede-covid-19-olum-oranlari-neden-artti\n* https://machinelearningmastery.com/gentle-introduction-autocorrelation-partial-autocorrelation/\n* https://www.researchgate.net/post/How_does_one_determine_the_values_for_ARp_and_MAq\n* https://stats.stackexchange.com/questions/281666/how-does-acf-pacf-identify-the-order-of-ma-and-ar-terms/281726#281726\n* https://stats.stackexchange.com/questions/134487/analyse-acf-and-pacf-plots?rq=1"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}