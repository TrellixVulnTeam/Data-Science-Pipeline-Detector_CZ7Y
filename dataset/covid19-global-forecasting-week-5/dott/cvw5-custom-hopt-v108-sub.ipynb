{"cells":[{"metadata":{},"cell_type":"markdown","source":"WM data based on our public kernel https://www.kaggle.com/philippsinger/covid-w5-worldometer-scraper"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"import os\nimport datetime\nimport numpy as np\nimport pandas as pd\nimport time\nimport lightgbm as lgb\nfrom sklearn.metrics import mean_squared_error\nfrom tqdm.notebook import tqdm\n\npd.set_option('display.max_rows', 100)\n\npath = '../input/covid19-global-forecasting-week-5/'\ntrain = pd.read_csv(path + 'train.csv')\ntest  = pd.read_csv(path + 'test.csv')\nsub   = pd.read_csv(path + 'submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_extra = pd.read_csv(\"../input/wmscraperfinal2/train_extra.csv\")\ntrain_extra.TargetValue.isna().sum()\ntrain_extra.loc[train_extra['Province_State'].isnull(), 'Province_State'] = 'N/A'\ntrain_extra.loc[train_extra['County'].isnull(), 'County'] = 'N/A'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_extra2 = pd.read_csv(\"../input/wmscraperfinal3/extra_data_11_5_2020_v2.csv\")\ntrain_extra2.loc[train_extra2['Province_State'].isnull(), 'Province_State'] = 'N/A'\ntrain_extra2.loc[train_extra2['County'].isnull(), 'County'] = 'N/A'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_extra.loc[~train_extra[\"Country_Region\"].isin(train_extra2[\"Country_Region\"]), \"TargetValue\"] = np.nan","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(train_extra)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_extra.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(train_extra2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_extra = train_extra.merge(train_extra2[[\"County\", \"Province_State\", \"Country_Region\", \"Target\", \"TargetValue\"]], on=[\"County\", \"Province_State\", \"Country_Region\", \"Target\"], how=\"left\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_extra[(train_extra.TargetValue_x != train_extra.TargetValue_y) & (~train_extra.TargetValue_y.isna())]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_extra[\"TargetValue\"] = train_extra[\"TargetValue_y\"]\ndel train_extra[\"TargetValue_x\"]\ndel train_extra[\"TargetValue_y\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_extra.TargetValue.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.concat([train, train_extra], axis=0)\n\ntrain.groupby(\"Date\")[\"Id\"].count().unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train.sort_values([\"Country_Region\", \"Province_State\", \"County\", \"Date\", \"Target\"]).reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['Date'] = train['Date'].apply(lambda x: (datetime.datetime.strptime(x, '%Y-%m-%d')))\ntest['Date'] = test['Date'].apply(lambda x: (datetime.datetime.strptime(x, '%Y-%m-%d')))\n\ntrain['days'] = (train['Date'].dt.date - train['Date'].dt.date.min()).dt.days\ntest['days'] = (test['Date'].dt.date - train['Date'].dt.date.min()).dt.days\n\ntrain.loc[train['Province_State'].isnull(), 'Province_State'] = 'N/A'\ntest.loc[test['Province_State'].isnull(), 'Province_State'] = 'N/A'\n\ntrain.loc[train['County'].isnull(), 'County'] = 'N/A'\ntest.loc[test['County'].isnull(), 'County'] = 'N/A'\n\ntrain['Area'] = train['Country_Region'] + '_' + train['Province_State'] + '_' + train['County']\ntest['Area'] = test['Country_Region'] + '_' + test['Province_State'] + '_' + test['County']\n\nprint(train['Date'].max())\nprint(test['Date'].min())\nprint(train['days'].max())\n\nAREAS = np.sort(train['Area'].unique())\n#VAL_LEN = 28\n#TRAIN_N = train['days'].max() - VAL_LEN\n\nTRAIN_N = 95 + 14\nVAL_LEN = train['days'].max() - TRAIN_N + 1\nprint(TRAIN_N)\n\nprint(train[train['days'] < TRAIN_N]['Date'].max())\nprint()\nprint(train[train['days'] >= TRAIN_N]['Date'].min())\nprint(train[train['days'] >= TRAIN_N]['Date'].max())\n\nTAU_LIST = [0.05, 0.5, 0.95]\n\ntest_orig = test.copy()\ntrain.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[train[\"Country_Region\"]==\"Mexico\"].tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_c_raw = train[train['Target'] == 'ConfirmedCases'].pivot(index='Area', columns='days', values='TargetValue').sort_index().values\ntrain_f_raw = train[train['Target'] == 'Fatalities'].pivot(index='Area', columns='days', values='TargetValue').sort_index().values\n\n#train_c = np.clip(train_c_raw, , None)\n#train_f = np.clip(train_f_raw, 0, None)\n\ntrain_c = train_c_raw\ntrain_f = train_f_raw\n\nweights_c = train[train['Target'] == 'ConfirmedCases'].groupby('Area')['Weight'].mean().sort_index().values.reshape(-1,1)\nweights_f = train[train['Target'] == 'Fatalities'].groupby('Area')['Weight'].mean().sort_index().values.reshape(-1,1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_c","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"idx_nan = np.isnan(train_c[:,-1])\ntrain_c[idx_nan] = np.roll(train_c[idx_nan], 1, axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_c","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"idx_nan = np.isnan(train_f[:,-1])\ntrain_f[idx_nan] = np.roll(train_f[idx_nan], 1, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_f","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train_c = train_c[:,1:]\n# train_f = train_f[:,1:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_c = np.nan_to_num(train_c)\ntrain_f = np.nan_to_num(train_f)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_c = train_c\nX_f = train_f","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def pinball_loss_single(ytrue, pred, weight, tau=0.5):\n    cond = (ytrue >= pred).astype(int)\n    error = np.sum(weight * (ytrue - pred) * cond * tau) - \\\n            np.sum(weight * (ytrue - pred) * (1-cond) * (1-tau))\n    return error / ytrue.shape[0] / ytrue.shape[1]\n\ndef pinball_loss_many(ytrue, preds, weight, tau=[0.05, 0.50, 0.95]):\n    return np.mean([pinball_loss_single(ytrue, preds[i], weight, t) for i,t in enumerate(tau)])\n\n\nfrom hyperopt import hp, space_eval, fmin, tpe, Trials, rand\nimport multiprocessing as mp\nfrom hyperopt.pyll.base import scope\nfrom joblib import Parallel, delayed\n\nclass ZmodelBase():\n    def __init__(self, loss_fun):\n        self.loss_fun = loss_fun\n        self.space = {}\n    \n    def _predict(self, params, X, horizon):\n        pass\n    \n    def _objective(self, params, X, horizon):\n        preds = self._predict(params, X[:,:-horizon], horizon)\n        loss = self.loss_fun(X[:, -horizon:], preds[:, -horizon:])\n        return loss\n\n    def opt(self, X, valid_horizon=14, rstate=42, max_trials=30, overrides={}):\n        for key, value in overrides.items():\n            self.space[key] = value\n        trials = Trials()\n        rstate = np.random.RandomState(rstate)\n\n        \n        best = fmin(lambda p: self._objective(p, X, valid_horizon),\n                    self.space,\n                    algo=tpe.suggest,\n                    max_evals=max_trials,\n                    trials=trials,\n                    rstate=rstate,\n                    show_progressbar=False,\n                    verbose=0)\n        self.best_params = space_eval(self.space, best)\n        self.best_loss = self._objective(self.best_params, X, valid_horizon)\n        \n        print(\"best loss\", self.best_loss)\n        \n        return self\n    \n    def predict(self, X, test_horizon=50):\n        return self._predict(self.best_params, X, test_horizon)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from hyperopt import hp, space_eval, fmin, tpe, Trials, rand\nimport multiprocessing as mp\nfrom hyperopt.pyll.base import scope\nfrom joblib import Parallel, delayed\nimport time\n\nclass Zmodel1(ZmodelBase):\n    def __init__(self, loss_fun):\n        super(Zmodel1, self).__init__(loss_fun)\n        self.space = {\n            \"min cases for local growth\": hp.choice(\"min cases for local growth\", [0,10,100,500]),\n            \"N days for local growth\": scope.int(hp.quniform('N days for local growth', 1, 25, 1)),\n            \"growth default\": hp.quniform('growth default', -0.1, 0.1, 0.01),\n\n            #\"N days for start\": scope.int(hp.quniform('N days for start', 1, 25, 1)),\n            \"N days for start\": hp.choice('N days for start', [1,2,3,4,5,6,7,8,9,12,13,14,15,16,19,20,21,22,23]),\n            #\"start function\": hp.choice(\"start function\", [np.min, np.mean, np.max]),\n            \"quantile\": hp.quniform('quantile', 0, 1, 0.1),\n\n            \"growth factor\": hp.quniform('growth factor', 0.4, 1.0, 0.01),\n            \"growth scale factor\": hp.quniform('growth scale factor', 0.5, 10.0, 0.1),\n            \n            \"delta factor\": hp.quniform('delta factor', 0.9, 0.99, 0.001),\n            \"growth multiplier\": hp.quniform('growth multiplier', 0.0, 1.0, 0.01),\n        }\n    \n    def _predict(self, params, X, horizon):\n        gr_base = []\n        gr_base_factor = []\n\n        #X = X.copy()\n        \n        X = np.clip(X, 0, None)\n    \n#         X_shift = np.roll(X, 1, axis=1)\n#         X[np.where(X<0)] = X_shift[np.where(X<0)]\n\n        threshold = params['min cases for local growth']\n        num_days = params['N days for local growth']\n        \n        check = (X > threshold).sum(axis=1) > num_days\n        d = np.mean(np.diff(X[check], axis=1)[-num_days:])\n        d += np.abs(d) * params['growth multiplier']\n        \n        gr_base = np.zeros(X.shape[0])\n        gr_base[~check] = params['growth default']\n        gr_base[check] = d\n\n#         for i in range(X.shape[0]):\n#             temp = X[i,:]\n            \n#             if check[i]:\n#                 d = np.mean(np.diff(temp[temp > threshold])[-num_days:])\n#                 d += np.abs(d) * params['growth multiplier']\n#                 gr_base.append(d)\n#             else:\n#                 gr_base.append(params['growth default'])\n\n#         gr_base = np.array(gr_base)\n        preds = X.copy()\n\n        deltas = []\n        for i in range(horizon):\n            if i == 0:\n                pr_base = np.quantile(preds[:, -params['N days for start']:], axis=1, q=params['quantile'])\n            else:\n                pr_base = deltas[-1]\n            #delta = pr_base + gr_base * params['growth factor']\n            #delta = np.clip(delta, 0, None) * params['delta factor']\n            \n            delta = np.clip(pr_base, 0, None) * params['delta factor'] + params['growth scale factor'] * gr_base * params['growth factor'] ** i\n            #delta = np.clip(delta, 0, None) * params['delta factor']\n            \n            deltas.append(delta)\n        \n        deltas = np.vstack(deltas).T\n        preds = np.hstack((preds, deltas))\n\n        return preds\n    \ns = time.time()\n\nVAL_LEN = 7\n    \noverrides = [{},{},{}]\nzmodels_c = Parallel(n_jobs=1)(delayed(lambda t: \\\n                Zmodel1(loss_fun = lambda X, Y: pinball_loss_single(X, Y, weights_c, tau=t)).\\\n                 opt(train_c[:, :], valid_horizon=VAL_LEN, max_trials=1000, overrides=overrides[i]))(tau) for i, tau in enumerate(TAU_LIST))\n\noverrides = [{},{},{}]\nzmodels_f = Parallel(n_jobs=1)(delayed(lambda t: \\\n                Zmodel1(loss_fun = lambda X, Y: pinball_loss_single(X, Y, weights_f, tau=t)).\\\n                 opt(train_f[:, :], valid_horizon=VAL_LEN, max_trials=1000, overrides=overrides[i]))(tau) for i, tau in enumerate(TAU_LIST))\n\n\npreds_c = [m.predict(train_c[:, :-VAL_LEN]) for m,tau in zip(zmodels_c, TAU_LIST)]\npreds_f = [m.predict(train_f[:, :-VAL_LEN]) for m,tau in zip(zmodels_f, TAU_LIST)]\n\n\nfor i, tau in enumerate(TAU_LIST):\n    print(pinball_loss_single(train_c[:, -VAL_LEN:], preds_c[i][:, train_c.shape[1]-VAL_LEN:train_c.shape[1]], weights_c, tau=tau))\n\nloss_c = pinball_loss_many(train_c[:, -VAL_LEN:], [p[:, train_c.shape[1]-VAL_LEN:train_c.shape[1]] for p in preds_c], weights_c)\nprint()\nprint(loss_c)\nprint()\n\n\nfor i, tau in enumerate(TAU_LIST):\n    print(pinball_loss_single(train_f[:, -VAL_LEN:], preds_f[i][:, train_c.shape[1]-VAL_LEN:train_c.shape[1]], weights_f, tau=tau))\n\nloss_f = pinball_loss_many(train_f[:, -VAL_LEN:], [p[:, train_c.shape[1]-VAL_LEN:train_c.shape[1]] for p in preds_f], weights_f)\nprint()\nprint(loss_f)\nprint()\n\nprint((loss_c + loss_f) / 2)\n\n#print()\n#print(time.time() - s)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds_c = [m.predict(train_c[:, :]) for m,tau in zip(zmodels_c, TAU_LIST)]\npreds_f = [m.predict(train_f[:, :]) for m,tau in zip(zmodels_f, TAU_LIST)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#shift back\n\nfor i in range(len(preds_c)):\n    preds_c[i][idx_nan,:] = np.roll(preds_c[i][idx_nan,:], -1, axis=1)\n    preds_f[i][idx_nan] = np.roll(preds_f[i][idx_nan], -1, axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for m in zmodels_c:\n    print(m.best_params)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for m in zmodels_f:\n    print(m.best_params)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nplt.style.use(['default'])\nfig = plt.figure(figsize = (20, 8))\n\n#for col in ['red', 'grey', 'green', 'purple', 'black', 'yellow', 'blue']:\nfor col in ['red', 'grey', 'green', 'purple']:\n    #idx = np.random.choice(range(len(AREAS)), 1)[0]\n    #idx = np.random.choice(np.where([x for x in AREAS if not 'US' in x])[0])\n    idx = np.random.choice(np.where(train_c[:,-1] > 100)[0])\n    plt.plot(train_c[idx], label=AREAS[idx], color=col)\n    for i in range(3):\n        plt.plot(preds_c[i][idx], linestyle='--', color=col)\n\nplt.title(\"Cases\")\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nplt.style.use(['default'])\nfig = plt.figure(figsize = (20, 8))\n\n#for col in ['red', 'grey', 'green', 'purple', 'black', 'yellow', 'blue']:\nfor col in ['red', 'grey', 'green', 'purple']:\n    #idx = np.random.choice(range(len(AREAS)), 1)[0]\n    idx = np.random.choice(np.where([x for x in AREAS if not 'US' in x])[0])\n    plt.plot(train_f[idx], label=AREAS[idx], color=col)\n    for i in range(3):\n        plt.plot(preds_f[i][idx], linestyle='--', color=col)\n\nplt.title(\"Fatalities\")\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = sub.copy()\nsubmission['TargetValue'] = -1\n\nfor tgt, ar, q in \\\n        [('ConfirmedCases', preds_c[i], tau) for i, tau in enumerate(TAU_LIST)] + \\\n        [('Fatalities',     preds_f[i], tau) for i, tau in enumerate(TAU_LIST)]:\n    \n    temp = pd.DataFrame(np.clip(ar, 0, None))\n    temp['Area'] = AREAS\n    temp = temp.melt(id_vars='Area', var_name='days', value_name=\"value\")\n    temp['Target'] = tgt\n\n    temp = test_orig.merge(temp, how='left', left_on=['Area', 'days', 'Target'], right_on=['Area', 'days', 'Target'])[['ForecastId', 'Target', 'value']]\n    temp['ForecastId_Quantile'] = temp['ForecastId'].apply(lambda x: str(x) + '_' + str(q))\n    temp = temp[temp['Target'] == tgt]\n    submission = submission.merge(temp[['ForecastId_Quantile', 'value']], how='left', left_on=['ForecastId_Quantile'], right_on=['ForecastId_Quantile'])\n\n    cond = ~submission['value'].isnull()\n    submission.loc[cond, 'TargetValue'] = submission.loc[cond, 'value']\n    del submission['value']\n\nprint(submission.shape)\nprint((submission['TargetValue'] < 0).sum())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"}},"nbformat":4,"nbformat_minor":4}