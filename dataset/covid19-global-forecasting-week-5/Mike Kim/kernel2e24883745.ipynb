{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom xgboost import XGBRegressor\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\n\nPREFIX = dirname + \"/\"\nprint(\"\")\nprint(PREFIX)\n#\"/kaggle/input/covid19-global-forecasting-week-3/\"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"\n\nimport pandas as pd\nimport numpy as np\n\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom datetime import timedelta, datetime\nfrom collections import defaultdict\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import Ridge\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import LabelBinarizer\nfrom sklearn.linear_model import LinearRegression, BayesianRidge\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.pipeline import Pipeline\n\n\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.compose import TransformedTargetRegressor\nfrom sklearn.base import TransformerMixin\nclass DenseTransformer(TransformerMixin):\n\n    def fit(self, X, y=None, **fit_params):\n        return self\n\n    def transform(self, X, y=None, **fit_params):\n        return X.todense()\n\npd.options.display.float_format = '{:.2f}'.format\n\n#/kaggle/input/covid19-global-forecasting-week-2/\n#PREFIX = \"/home/mikeskim/Desktop/covid5/data/\"\nprint(\"done\")\n\n#import sklearn as sk\n#print(sk.__version__)\n\n\n# In[2]:\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv(PREFIX+\"train.csv\", parse_dates=[\"Date\"])\nprint(train_df.describe())\ntrain_df['TargetValue'].clip(0, inplace=True)\nprint(train_df.describe())\nMIN_DATE_TRAIN = train_df[\"Date\"].min()\ntest_df = pd.read_csv(PREFIX+\"test.csv\", parse_dates=[\"Date\"])\nsub_df = pd.read_csv(PREFIX+\"submission.csv\")\n\ntrain_df.fillna(\"_\", inplace=True)\ntest_df.fillna(\"_\", inplace=True)\n\ntrain_df['region_tuple'] = train_df[['County','Province_State','Country_Region']].sum(axis=1)\ntest_df['region_tuple'] = test_df[['County','Province_State','Country_Region']].sum(axis=1)\ntrain_vc = train_df['region_tuple'].value_counts()\npopulation_vc = train_df['Population'].map(np.log).round(0).value_counts()\ntrain_weights = train_df['Weight'].tolist()\ntrain_population = train_df['Population'].tolist()\n\ndef create_days(df):\n    df = df.copy()\n    df['target_dummy'] = 0\n    df['region_tuple'] = df['region_tuple'].map(train_vc)\n    df['pop_vc'] = df['Population'].map(np.log).round(0).map(population_vc)\n    df.loc[df['Target']=='Fatalities','target_dummy']=1\n    df['Days'] = (df[\"Date\"]-MIN_DATE_TRAIN).dt.days.astype(int)\n    df['Days2'] = df['Days']**0.5\n    df['Days3'] = df['Days'].map(np.log1p)\n    df['weekday'] = df[\"Date\"].dt.dayofweek\n    df['weekday_str'] = df['weekday'].astype(str)\n    df['Weight'] =df['Weight']*df['Days']\n    return df\n\ntrain_df = create_days(train_df)\ntest_df = create_days(test_df)\ntrain_days = train_df['Days'].tolist()\n\n    \n#   df['Days'] = (df[\"Date\"]-MIN_DATE_TRAIN).dt.days.astype(int)\n#    df['Days2'] = df['Days'].pow(2)\n#   df['weekday'] = df[\"Date\"].dt.dayofweek\n#ef combine_regions(df):\n#   df = df.copy()\n#   df['C_P_S_C_R'] = df['County']+df['Province_State']+df['Country_Region'].astype(str)\n#   return \n\n#rain_df = combine_regions(train_df)\n#est_df = combine_regions(test_df)\nprint(MIN_DATE_TRAIN)\n\n\n# In[3]:\n\n\nsub_df.head(3)\n\n\n# In[4]:\n\n\ntrain_df_Id = train_df['Id'].tolist()\ntrain_df.drop([\"Id\",\"Date\"], axis=1, inplace=True)\n\ntrain_df.head(3)\n\n\n# In[5]:\n\n\n\n#        model = make_pipeline(PolynomialFeatures(2), BayesianRidge()) #Ridge() worse\n#        model.fit(prior_df[[\"ConfirmedCases\",\"Fatalities\",\"Days\"]].shift(periods=2).dropna(),\n#                  prior_df[[\"ConfirmedCases\"]].shift(periods=-2).dropna().values.reshape(-1,))  \n#        preds = model.predict(prior_df[[\"ConfirmedCases\",\"Fatalities\",\"Days\"]])\n#        features_list += [preds[-1]]\n\n\n# In[6]:\n\n\ntest_df_ForecastId = test_df['ForecastId'].tolist()\ntest_df.drop([\"ForecastId\",\"Date\"], axis=1, inplace=True)\ntest_df.head(3)\n\n\n# In[7]:\n\n\ntrain_df.dtypes\n\n\n# In[8]:\n\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nnumeric_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='median')),\n    ('scaler', StandardScaler()),\n                      ('interactions', PolynomialFeatures(2)),\n])\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n\n\n# In[9]:\n\n\nnumeric_features = train_df.select_dtypes(include=['int64', 'float64']).drop(['TargetValue'], axis=1).columns\ncategorical_features = train_df.select_dtypes(include=['object']).columns\nprint(numeric_features)\nprint(categorical_features)\nfrom sklearn.compose import ColumnTransformer\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numeric_transformer, numeric_features),\n        ('cat', categorical_transformer, categorical_features)])\n\n\n# In[10]:\n\n\n#https://medium.com/vickdata/a-simple-guide-to-scikit-learn-pipelines-4ac0d974bdcf\n\ntransformed_ridge =  TransformedTargetRegressor(regressor=Ridge(alpha=0.1), # Lasso(alpha=0.01),\n                                                func=np.log1p,\n                                                inverse_func=np.expm1)\n\nbr = Pipeline(steps=[('preprocessor', preprocessor),\n  #                    ('interactions', PolynomialFeatures(2)),\n #                        ('to_dense', DenseTransformer()), \n                      ('regressor', transformed_ridge)])\n\nfeatures_list = test_df.columns.tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tw_list = [\n           np.ones(len(train_weights)), \n           np.array(train_population),\n           np.array(train_weights),\n           np.array(train_days),\n           np.array(train_weights)*np.array(train_population)*np.array(train_days),    \n           np.array(train_weights)*np.array(train_population),\n           np.array(train_weights)*(np.array(train_population)**0.5),\n           np.array(train_population)*np.array(train_days),\n           np.array(train_weights)*np.array(train_days),\n           np.array(train_weights)*(np.array(train_days)**0.5),\n           np.array(train_weights)*(np.array(train_days)**2),\n          ]\npreds_list = []\nfor tw in tw_list:\n    br.fit(train_df[features_list], \n           train_df[\"TargetValue\"],\n           regressor__sample_weight=tw)\n    preds = br.predict(test_df[features_list])\n    preds_list.append(preds)\n\n\npreds_df = pd.DataFrame(preds_list).T\npreds_df.head(3)\n\noutput_df = pd.DataFrame()\noutput_df['min_pred'] = preds_df.min(axis=1)\noutput_df['median_pred'] = preds_df.median(axis=1)\noutput_df['max_pred'] = preds_df.max(axis=1)\noutput_df.head(3)\n\nsub_list = output_df.values.flatten().tolist()\nsub_list[0:5]\n\nsub_df['TargetValue'] = sub_list\nsub_df['TargetValue'] = sub_df['TargetValue'].clip(0)\nsub_df.to_csv('submission.csv', index=False)\nprint(sub_df.describe())\nsub_df.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}