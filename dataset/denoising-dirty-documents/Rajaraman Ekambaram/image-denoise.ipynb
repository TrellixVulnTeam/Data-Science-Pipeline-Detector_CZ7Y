{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Remove noisy background from images/documents using Autoencoders, Tensorflow v2 and Keras"},{"metadata":{},"cell_type":"markdown","source":"![](https://osclasspoint.com/kaggle/autoencoder.png)"},{"metadata":{},"cell_type":"markdown","source":"# Import libraries and data\nFirst load libraries we need for our work. We need multiple libraries to be able to unzip files, work with directories, sklearn, tensorflow..."},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport zipfile\nimport os\nimport cv2\n\nfrom sklearn.model_selection import train_test_split\n\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, UpSampling2D, Dropout, BatchNormalization, Input\nfrom tensorflow.keras.callbacks import EarlyStopping\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"What kind of data/files we have there?"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we have data zipped, we will have to work in /kaggle/working/ directory to unzip images here."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# path to zipped & working directories\npath_zip = '/kaggle/input/denoising-dirty-documents/'\npath = '/kaggle/working/'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# unzip files first to working directory\n# We could use also unzipped data source, but why not to learn something new?\nwith zipfile.ZipFile(path_zip + 'train.zip', 'r') as zip_ref:\n    zip_ref.extractall(path)\n\nwith zipfile.ZipFile(path_zip + 'test.zip', 'r') as zip_ref:\n    zip_ref.extractall(path)  \n    \nwith zipfile.ZipFile(path_zip + 'train_cleaned.zip', 'r') as zip_ref:\n    zip_ref.extractall(path)  \n    \nwith zipfile.ZipFile(path_zip + 'sampleSubmission.csv.zip', 'r') as zip_ref:\n    zip_ref.extractall(path)  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For later use, we will store image names into list, so we can draw them simply."},{"metadata":{"trusted":true},"cell_type":"code","source":"# store image names in list for later use\ntrain_img = sorted(os.listdir(path + '/train'))\ntrain_cleaned_img = sorted(os.listdir(path + '/train_cleaned'))\ntest_img = sorted(os.listdir(path + '/test'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data preparation\nNext step is to define function to process images and then store this images in list. As there is not as many data, we do not need to work in batches."},{"metadata":{"trusted":true},"cell_type":"code","source":"# prepare function\ndef process_image(path):\n    img = cv2.imread(path)\n    img = np.asarray(img, dtype=\"float32\")\n    img = cv2.resize(img, (540, 420))\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    img = img/255.0\n    img = np.reshape(img, (420, 540, 1))\n    \n    return img","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Reshape images and put them into list."},{"metadata":{"trusted":true},"cell_type":"code","source":"# preprocess images\ntrain = []\ntrain_cleaned = []\ntest = []\n\nfor f in sorted(os.listdir(path + 'train/')):\n    train.append(process_image(path + 'train/' + f))\n\nfor f in sorted(os.listdir(path + 'train_cleaned/')):\n    train_cleaned.append(process_image(path + 'train_cleaned/' + f))\n   \nfor f in sorted(os.listdir(path + 'test/')):\n    test.append(process_image(path + 'test/' + f))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Exploratory data analysis\nNot too much to look there, but just quickly look on train images and their cleaned version. This is what we put into model to learn how to clean noise from background."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,25))\nfor i in range(0,8,2):\n    plt.subplot(4,2,i+1)\n    plt.xticks([])\n    plt.yticks([])\n    plt.imshow(train[i][:,:,0], cmap='gray')\n    plt.title('Noise image: {}'.format(train_img[i]))\n    \n    plt.subplot(4,2,i+2)\n    plt.xticks([])\n    plt.yticks([])\n    plt.imshow(train_cleaned[i][:,:,0], cmap='gray')\n    plt.title('Denoised image: {}'.format(train_img[i]))\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Split data\nIn this step we convert lists to numpy arrays and split dataset into train and validation in ration 85% train, 15% test."},{"metadata":{"trusted":true},"cell_type":"code","source":"# convert list to numpy array\nX_train = np.asarray(train)\nY_train = np.asarray(train_cleaned)\nX_test = np.asarray(test)\n\nX_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.15)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I have tested many different models and this one brought me to best results (val_loss < 0.0004). Increasing filters, adding another convolutional layer, batch normalization or drop out did not helped to get better score."},{"metadata":{"trusted":true},"cell_type":"code","source":"def model():\n    input_layer = Input(shape=(420, 540, 1))  # we might define (None,None,1) here, but in model summary dims would not be visible\n    \n    # encoding\n    x = Conv2D(64, (3, 3), activation='relu', padding='same')(input_layer)\n    x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n    x = BatchNormalization()(x)\n\n    x = MaxPooling2D((2, 2), padding='same')(x)\n    \n    x = Dropout(0.5)(x)\n\n    # decoding\n    x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n    x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n    x = BatchNormalization()(x)\n\n    x = UpSampling2D((2, 2))(x)\n\n    output_layer = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n    model = Model(inputs=[input_layer], outputs=[output_layer])\n    model.compile(optimizer='adam' , loss='mean_squared_error', metrics=['mae'])\n\n    return model\n\n\nmodel = model()\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train model\nVerbose is going to be set to 0 to avoid filling output with hundreds of lines from training. We will run 300 epochs having early stopping set to 20 (if val loss does not drop in 20 epochs, it will stop)."},{"metadata":{},"cell_type":"markdown","source":"Let's store history of model as well, so we can plot loss (rmse) and mae."},{"metadata":{"trusted":true},"cell_type":"code","source":"callback = EarlyStopping(monitor='loss', patience=30)\nhistory = model.fit(X_train, Y_train, validation_data = (X_val, Y_val), epochs=600, batch_size=24, verbose=0, callbacks=[callback])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Plot error evolution on epochs"},{"metadata":{},"cell_type":"markdown","source":"You may notice jump in error after approx. 10 epoch that is pretty important, but enought epochs flatten this to almost 0."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check how loss & mae went down\nepoch_loss = history.history['loss']\nepoch_val_loss = history.history['val_loss']\nepoch_mae = history.history['mae']\nepoch_val_mae = history.history['val_mae']\n\nplt.figure(figsize=(20,6))\nplt.subplot(1,2,1)\nplt.plot(range(0,len(epoch_loss)), epoch_loss, 'b-', linewidth=2, label='Train Loss')\nplt.plot(range(0,len(epoch_val_loss)), epoch_val_loss, 'r-', linewidth=2, label='Val Loss')\nplt.title('Evolution of loss on train & validation datasets over epochs')\nplt.legend(loc='best')\n\nplt.subplot(1,2,2)\nplt.plot(range(0,len(epoch_mae)), epoch_mae, 'b-', linewidth=2, label='Train MAE')\nplt.plot(range(0,len(epoch_val_mae)), epoch_val_mae, 'r-', linewidth=2,label='Val MAE')\nplt.title('Evolution of MAE on train & validation datasets over epochs')\nplt.legend(loc='best')\n\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Evaluation\nIn this step we will \"predict\", or better say clean test images and check how well model works."},{"metadata":{"trusted":true},"cell_type":"code","source":"# predict/clean test images\nY_test = model.predict(X_test, batch_size=16)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now compare noisy (left) and denoised test images (right). Our model has done great job with denoising!"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,25))\nfor i in range(0,8,2):\n    plt.subplot(4,2,i+1)\n    plt.xticks([])\n    plt.yticks([])\n    plt.imshow(X_test[i][:,:,0], cmap='gray')\n    plt.title('Noisy image: {}'.format(test_img[i]))\n    \n    plt.subplot(4,2,i+2)\n    plt.xticks([])\n    plt.yticks([])\n    plt.imshow(Y_test[i][:,:,0], cmap='gray')\n    plt.title('Denoised by autoencoder: {}'.format(test_img[i]))\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Submission\nIt's time to contest! Let's submit our data."},{"metadata":{},"cell_type":"markdown","source":"Honestly, this way to submit results was really weird for me :-/"},{"metadata":{"trusted":true},"cell_type":"code","source":"# it will take a while!\nids = []\nvals = []\nfor i, f in enumerate(test_img):\n    file = path + 'test/' + f\n    imgid = int(f[:-4])\n    img = cv2.imread(file, 0)\n    img_shape = img.shape\n    # print('Processing image: {} \\tinto size: {}'.format(f, img_shape))    # uncomment to see progress\n    preds_reshaped = cv2.resize(Y_test[i], (img_shape[1], img_shape[0]))\n\n    for r in range(img_shape[0]):\n        for c in range(img_shape[1]):\n            ids.append(str(imgid)+'_'+str(r + 1)+'_'+str(c + 1))\n            vals.append(preds_reshaped[r, c])\n\nsubmission = pd.DataFrame({'id': ids, 'value': vals})\nsubmission.to_csv('submission.csv',index = False)\n\nprint('Results saved to submission.csv!')\n\n# quick check if length of IDs is OK\n# we should get there number 14230080\nprint('Length of IDs: {}'.format(len(ids)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check first few rows of submission\nmy_submission = pd.read_csv('submission.csv')\nmy_submission.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# cleanup working directory\nimport shutil\nshutil.rmtree(path + 'train/')\nshutil.rmtree(path + 'test/')\nshutil.rmtree(path + 'train_cleaned/')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conclusion\nWe've created autoencoder using tensorflow v2 and keras that can very successfully remove background and noise from documents. Next step could me create algorithm that will be able to extract words out of cleaned sheets ;-)"},{"metadata":{},"cell_type":"markdown","source":"I have really enjoyed this task, hope you did as well!"},{"metadata":{},"cell_type":"markdown","source":"## If you liked this notebook, please UPVOTE\n## Thanks !"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}