{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<h1>Denoising using MPRNet</h1>\n\nMost of code adapted from: https://github.com/swz30/MPRNet \n\nModel from               : https://github.com/swz30/MPRNet \n\nDataset from             : https://github.com/kwcckw/shabby_data_normal_quality and https://www.kaggle.com/c/denoising-dirty-documents","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-26T12:55:36.644874Z","iopub.execute_input":"2022-05-26T12:55:36.645273Z","iopub.status.idle":"2022-05-26T12:55:38.158128Z","shell.execute_reply.started":"2022-05-26T12:55:36.645193Z","shell.execute_reply":"2022-05-26T12:55:38.157191Z"}}},{"cell_type":"markdown","source":"# <h1> MPRNet repo installation </h1>","metadata":{}},{"cell_type":"code","source":"# clone MPRNet repo\n\n!git clone https://github.com/swz30/MPRNet","metadata":{"execution":{"iopub.status.busy":"2022-06-19T04:29:24.44734Z","iopub.execute_input":"2022-06-19T04:29:24.447975Z","iopub.status.idle":"2022-06-19T04:29:26.485268Z","shell.execute_reply.started":"2022-06-19T04:29:24.447933Z","shell.execute_reply":"2022-06-19T04:29:26.484354Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# install scheduler\n\n\n%cd /kaggle/working/MPRNet/pytorch-gradual-warmup-lr\n\n!python setup.py install\n\nfrom warmup_scheduler import GradualWarmupScheduler","metadata":{"execution":{"iopub.status.busy":"2022-06-19T04:29:29.055079Z","iopub.execute_input":"2022-06-19T04:29:29.056001Z","iopub.status.idle":"2022-06-19T04:29:32.919951Z","shell.execute_reply.started":"2022-06-19T04:29:29.055959Z","shell.execute_reply":"2022-06-19T04:29:32.918952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import libraries from MPR repo\n\n!pip install natsort\n%cd /kaggle/working/MPRNet/Denoising\n\nimport utils\nimport losses","metadata":{"execution":{"iopub.status.busy":"2022-06-19T04:29:33.095372Z","iopub.execute_input":"2022-06-19T04:29:33.096654Z","iopub.status.idle":"2022-06-19T04:29:45.642572Z","shell.execute_reply.started":"2022-06-19T04:29:33.096606Z","shell.execute_reply":"2022-06-19T04:29:45.641674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# set working dorectory\n\n%cd /kaggle/working/","metadata":{"execution":{"iopub.status.busy":"2022-06-19T04:29:47.02311Z","iopub.execute_input":"2022-06-19T04:29:47.023462Z","iopub.status.idle":"2022-06-19T04:29:47.029038Z","shell.execute_reply.started":"2022-06-19T04:29:47.02343Z","shell.execute_reply":"2022-06-19T04:29:47.028117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import libraries\n\nimport os\n\ngpus = ','.join([str(i) for i in [0,1,2,3]])\nos.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = gpus\n\nimport torch\ntorch.backends.cudnn.benchmark = True\n\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\n\nimport random\nimport time\nimport numpy as np\n\nfrom tqdm import tqdm\nfrom pdb import set_trace as stx\n\nfrom torch.utils.data import Dataset\nimport torch\nfrom PIL import Image\nimport torchvision.transforms.functional as TF\n\nfrom natsort import natsorted\nfrom glob import glob\nfrom skimage import img_as_ubyte\nimport cv2","metadata":{"execution":{"iopub.status.busy":"2022-06-19T04:29:48.879855Z","iopub.execute_input":"2022-06-19T04:29:48.88047Z","iopub.status.idle":"2022-06-19T04:29:49.193811Z","shell.execute_reply.started":"2022-06-19T04:29:48.880432Z","shell.execute_reply":"2022-06-19T04:29:49.19287Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# set random seeds\n\nnp.random.seed(0)\ntorch.manual_seed(0)\ntorch.cuda.manual_seed_all(0)\nrandom.seed(0)","metadata":{"execution":{"iopub.status.busy":"2022-06-19T04:29:52.582973Z","iopub.execute_input":"2022-06-19T04:29:52.58353Z","iopub.status.idle":"2022-06-19T04:29:52.590771Z","shell.execute_reply.started":"2022-06-19T04:29:52.583496Z","shell.execute_reply":"2022-06-19T04:29:52.589764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <h1> Define training parameters </h1>","metadata":{}},{"cell_type":"code","source":"# create directories\n\ntrain_save_dir = \"./checkpoints\"\n\n# train & validate\ntrain_dir = '/kaggle/working/train/'\ntrain_input_path = '/kaggle/working/train/input/'\ntrain_target_path = '/kaggle/working/train/target/'\n\nval_dir = '/kaggle/working/val/'\nval_input_path = '/kaggle/working/val/input/'\nval_target_path = '/kaggle/working/val/target/'\n\n\nos.makedirs(train_input_path)\nos.makedirs(train_target_path)\n\nos.makedirs(val_input_path)\nos.makedirs(val_target_path)\n\n\n# move data into the correct dirs\n# train\n!cp -ar /kaggle/input/shabby-data-normal-quality/images_normal_quality/cropped/train/* /kaggle/working/train/input/\n!cp -ar /kaggle/input/shabby-data-normal-quality/images_normal_quality/cropped/train_cleaned/* /kaggle/working/train/target/\n\n# validate\n!cp -ar /kaggle/input/shabby-data-normal-quality/images_normal_quality/cropped/validate/* /kaggle/working/val/input/\n!cp -ar /kaggle/input/shabby-data-normal-quality/images_normal_quality/cropped/validate_cleaned/* /kaggle/working/val/target/\n\n\n# test\ntest_dir = \"/kaggle/input/augmented-shabby-images/Datasets/test/input\"\ntest_out_dir = \"/kaggle/working/output\"\n","metadata":{"execution":{"iopub.status.busy":"2022-06-19T04:29:54.466957Z","iopub.execute_input":"2022-06-19T04:29:54.467528Z","iopub.status.idle":"2022-06-19T04:30:17.027956Z","shell.execute_reply.started":"2022-06-19T04:29:54.46749Z","shell.execute_reply":"2022-06-19T04:30:17.026814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Add new data\n\nos.makedirs('/kaggle/working/denoising-dirty-documents/')\n\n!unzip /kaggle/input/denoising-dirty-documents/train_cleaned.zip -d /kaggle/working/denoising-dirty-documents/\n!unzip /kaggle/input/denoising-dirty-documents/train.zip -d /kaggle/working/denoising-dirty-documents/\n\n!cp -ar /kaggle/working/denoising-dirty-documents/train/* /kaggle/working/train/input/\n!cp -ar /kaggle/working/denoising-dirty-documents/train_cleaned/* /kaggle/working/train/target/","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# set configs and training params\n\nstart_epoch = 1\nmode = \"Denoising\"\nsession = \"MPRNet\"\n\ninitial_lr = 4e-04\nmin_lr = 1e-06\nepoch_num = 50\nbatch_size=8\npatch_size = 400\ntrain_resume = 0","metadata":{"execution":{"iopub.status.busy":"2022-06-19T04:34:26.307503Z","iopub.execute_input":"2022-06-19T04:34:26.307875Z","iopub.status.idle":"2022-06-19T04:34:26.312674Z","shell.execute_reply.started":"2022-06-19T04:34:26.307842Z","shell.execute_reply":"2022-06-19T04:34:26.311884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Display some of the training images\n\nfrom glob import glob\nfrom matplotlib import pyplot as plt\n\nnoisy_sample = glob(train_input_path+'/*.png')\nclean_sample = glob(train_target_path+'/*.png')\n\nfor i, (noisy_path, clean_path) in enumerate(zip(noisy_sample, clean_sample)):\n    img_noisy = cv2.imread(noisy_path, cv2.IMREAD_GRAYSCALE)\n    img_clean = cv2.imread(clean_path, cv2.IMREAD_GRAYSCALE)\n    \n    plt.figure()\n    plt.subplot(121)\n    plt.imshow(img_noisy,cmap='gray')\n    plt.subplot(122)\n    plt.imshow(img_clean,cmap='gray')\n\n    if i >5:\n        break","metadata":{"execution":{"iopub.status.busy":"2022-06-19T06:31:40.112345Z","iopub.execute_input":"2022-06-19T06:31:40.112712Z","iopub.status.idle":"2022-06-19T06:31:41.877018Z","shell.execute_reply.started":"2022-06-19T06:31:40.112665Z","shell.execute_reply":"2022-06-19T06:31:41.876248Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <h1> Create Dataloader </h1>","metadata":{}},{"cell_type":"code","source":"# create dataloader\n\ndef is_image_file(filename):\n    return any(filename.endswith(extension) for extension in ['jpeg', 'JPEG', 'jpg', 'png', 'JPG', 'PNG', 'gif'])\n\nclass DataLoaderTrain(Dataset):\n    def __init__(self, rgb_dir, img_options=None):\n        super(DataLoaderTrain, self).__init__()\n\n        inp_files = sorted(os.listdir(os.path.join(rgb_dir, 'input')))\n        tar_files = sorted(os.listdir(os.path.join(rgb_dir, 'target')))\n\n        self.inp_filenames = [os.path.join(rgb_dir, 'input', x)  for x in inp_files if is_image_file(x)]\n        self.tar_filenames = [os.path.join(rgb_dir, 'target', x) for x in tar_files if is_image_file(x)]\n\n        self.img_options = img_options\n        self.sizex       = len(self.tar_filenames)  # get the size of target\n\n        self.ps = self.img_options['patch_size']\n\n    def __len__(self):\n        return self.sizex\n\n    def __getitem__(self, index):\n        index_ = index % self.sizex\n        ps = self.ps\n\n        inp_path = self.inp_filenames[index_]\n        tar_path = self.tar_filenames[index_]\n\n        inp_img = Image.open(inp_path)\n        tar_img = Image.open(tar_path)\n\n        inp_img = np.array(inp_img)\n        tar_img = np.array(tar_img)\n\n        w,h= inp_img.shape[:2]\n        if w!=patch_size or h!=patch_size:\n            inp_img = cv2.resize(inp_img, (patch_size, patch_size), interpolation = cv2.INTER_AREA)\n            tar_img = cv2.resize(tar_img, (patch_size, patch_size), interpolation = cv2.INTER_AREA)\n            w,h= inp_img.shape[:2]\n        \n        if len(inp_img.shape)>2:\n            inp_img = cv2.cvtColor(inp_img, cv2.COLOR_RGB2GRAY)\n            inp_img.reshape(h,w,1)\n        else:\n            inp_img.reshape(h,w,1)\n\n        if len(tar_img.shape)>2:\n            tar_img = cv2.cvtColor(tar_img, cv2.COLOR_RGB2GRAY)\n            tar_img.reshape(h,w,1)\n        else:\n            tar_img.reshape(h,w,1)\n\n        inp_img = torch.from_numpy(inp_img.astype('float')/255).reshape(1, patch_size, patch_size).type(torch.FloatTensor)\n        tar_img = torch.from_numpy(tar_img.astype('float')/255).reshape(1, patch_size, patch_size).type(torch.FloatTensor)\n        \n        aug    = 0\n\n        # Data Augmentations\n        if aug==1:\n            inp_img = inp_img.flip(1)\n            tar_img = tar_img.flip(1)\n        elif aug==2:\n            inp_img = inp_img.flip(2)\n            tar_img = tar_img.flip(2)\n        elif aug==3:\n            inp_img = torch.rot90(inp_img,dims=(1,2))\n            tar_img = torch.rot90(tar_img,dims=(1,2))\n        elif aug==4:\n            inp_img = torch.rot90(inp_img,dims=(1,2), k=2)\n            tar_img = torch.rot90(tar_img,dims=(1,2), k=2)\n        elif aug==5:\n            inp_img = torch.rot90(inp_img,dims=(1,2), k=3)\n            tar_img = torch.rot90(tar_img,dims=(1,2), k=3)\n        elif aug==6:\n            inp_img = torch.rot90(inp_img.flip(1),dims=(1,2))\n            tar_img = torch.rot90(tar_img.flip(1),dims=(1,2))\n        elif aug==7:\n            inp_img = torch.rot90(inp_img.flip(2),dims=(1,2))\n            tar_img = torch.rot90(tar_img.flip(2),dims=(1,2))\n\n        filename = os.path.splitext(os.path.split(tar_path)[-1])[0]\n\n        return tar_img, inp_img, filename\n\nclass DataLoaderVal(Dataset):\n    def __init__(self, rgb_dir, img_options=None, rgb_dir2=None):\n        super(DataLoaderVal, self).__init__()\n\n        inp_files = sorted(os.listdir(os.path.join(rgb_dir, 'input')))\n        tar_files = sorted(os.listdir(os.path.join(rgb_dir, 'target')))\n\n        self.inp_filenames = [os.path.join(rgb_dir, 'input', x)  for x in inp_files if is_image_file(x)]\n        self.tar_filenames = [os.path.join(rgb_dir, 'target', x) for x in tar_files if is_image_file(x)]\n\n        self.img_options = img_options\n        self.sizex       = len(self.tar_filenames)  # get the size of target\n\n        self.ps = self.img_options['patch_size']\n\n    def __len__(self):\n        return self.sizex\n\n    def __getitem__(self, index):\n        index_ = index % self.sizex\n        ps = self.ps\n\n        inp_path = self.inp_filenames[index_]\n        tar_path = self.tar_filenames[index_]\n\n        inp_img = Image.open(inp_path)\n        tar_img = Image.open(tar_path)\n\n        inp_img = np.array(inp_img)\n        tar_img = np.array(tar_img)\n\n        w,h= inp_img.shape[:2]\n        if w!=patch_size or h!=patch_size:\n            inp_img = cv2.resize(inp_img, (patch_size, patch_size), interpolation = cv2.INTER_AREA)\n            tar_img = cv2.resize(tar_img, (patch_size, patch_size), interpolation = cv2.INTER_AREA)\n            w,h= inp_img.shape[:2]\n        \n        if len(inp_img.shape)>2:\n            inp_img = cv2.cvtColor(inp_img, cv2.COLOR_RGB2GRAY)\n            inp_img.reshape(h,w,1)\n        else:\n            inp_img.reshape(h,w,1)\n\n        if len(tar_img.shape)>2:\n            tar_img = cv2.cvtColor(tar_img, cv2.COLOR_RGB2GRAY)\n            tar_img.reshape(h,w,1)\n        else:\n            tar_img.reshape(h,w,1)\n\n\n        inp_img = torch.from_numpy(inp_img.astype('float')/255).reshape(1, patch_size, patch_size).type(torch.FloatTensor)\n        tar_img = torch.from_numpy(tar_img.astype('float')/255).reshape(1, patch_size, patch_size).type(torch.FloatTensor)\n\n        filename = os.path.splitext(os.path.split(tar_path)[-1])[0]\n\n        return tar_img, inp_img, filename\n\nclass DataLoaderTest(Dataset):\n    def __init__(self, inp_dir, img_options):\n        super(DataLoaderTest, self).__init__()\n\n        inp_files = sorted(os.listdir(inp_dir))\n        self.inp_filenames = [os.path.join(inp_dir, x) for x in inp_files if is_image_file(x)]\n\n        self.inp_size = len(self.inp_filenames)\n        self.img_options = img_options\n\n    def __len__(self):\n        return self.inp_size\n\n    def __getitem__(self, index):\n\n        path_inp = self.inp_filenames[index]\n        filename = os.path.splitext(os.path.split(path_inp)[-1])[0]\n        inp = Image.open(path_inp)\n\n        inp = np.array(inp)\n\n        ys,xs= inp.shape[:2]\n        if len(inp.shape)>2:\n            inp = cv2.cvtColor(inp, cv2.COLOR_RGB2GRAY)\n            inp.reshape(ys,xs,1)\n        else:\n            inp.reshape(ys,xs,1)\n                \n        inp = torch.from_numpy(inp.astype('float')/255).reshape(1, patch_size, patch_size).type(torch.FloatTensor) \n            \n        return inp, filename\n\n\ndef get_training_data(rgb_dir, img_options):\n    assert os.path.exists(rgb_dir)\n    return DataLoaderTrain(rgb_dir, img_options)\n\ndef get_validation_data(rgb_dir, img_options):\n    assert os.path.exists(rgb_dir)\n    return DataLoaderVal(rgb_dir, img_options)\n\ndef get_test_data(rgb_dir, img_options):\n    assert os.path.exists(rgb_dir)\n    return DataLoaderTest(rgb_dir, img_options)\n","metadata":{"execution":{"iopub.status.busy":"2022-06-19T04:30:49.894439Z","iopub.execute_input":"2022-06-19T04:30:49.894889Z","iopub.status.idle":"2022-06-19T04:30:50.035462Z","shell.execute_reply.started":"2022-06-19T04:30:49.894849Z","shell.execute_reply":"2022-06-19T04:30:50.034447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <h1> Create Model </h1>","metadata":{}},{"cell_type":"code","source":"# create model\n\n##########################################################################\ndef conv(in_channels, out_channels, kernel_size, bias=False, stride = 1):\n    return nn.Conv2d(\n        in_channels, out_channels, kernel_size,\n        padding=(kernel_size//2), bias=bias, stride = stride)\n\n\n##########################################################################\n## Channel Attention Layer\nclass CALayer(nn.Module):\n    def __init__(self, channel, reduction=16, bias=False):\n        super(CALayer, self).__init__()\n        # global average pooling: feature --> point\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        # feature channel downscale and upscale --> channel weight\n        self.conv_du = nn.Sequential(\n                nn.Conv2d(channel, channel // reduction, 1, padding=0, bias=bias),\n                nn.ReLU(inplace=True),\n                nn.Conv2d(channel // reduction, channel, 1, padding=0, bias=bias),\n                nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        y = self.avg_pool(x)\n        y = self.conv_du(y)\n        return x * y\n\n\n##########################################################################\n## Channel Attention Block (CAB)\nclass CAB(nn.Module):\n    def __init__(self, n_feat, kernel_size, reduction, bias, act):\n        super(CAB, self).__init__()\n        modules_body = []\n        modules_body.append(conv(n_feat, n_feat, kernel_size, bias=bias))\n        modules_body.append(act)\n        modules_body.append(conv(n_feat, n_feat, kernel_size, bias=bias))\n\n        self.CA = CALayer(n_feat, reduction, bias=bias)\n        self.body = nn.Sequential(*modules_body)\n\n    def forward(self, x):\n        res = self.body(x)\n        res = self.CA(res)\n        res += x\n        return res\n\n##########################################################################\n## Supervised Attention Module\nclass SAM(nn.Module):\n    def __init__(self, n_feat, kernel_size, bias):\n        super(SAM, self).__init__()\n        self.conv1 = conv(n_feat, n_feat, kernel_size, bias=bias)\n        self.conv2 = conv(n_feat, 3, kernel_size, bias=bias)\n        self.conv3 = conv(3, n_feat, kernel_size, bias=bias)\n\n    def forward(self, x, x_img):\n        x1 = self.conv1(x)\n        img = self.conv2(x) + x_img\n        x2 = torch.sigmoid(self.conv3(img))\n        x1 = x1*x2\n        x1 = x1+x\n        return x1, img\n\n##########################################################################\n## U-Net\n\nclass Encoder(nn.Module):\n    def __init__(self, n_feat, kernel_size, reduction, act, bias, scale_unetfeats, csff):\n        super(Encoder, self).__init__()\n\n        self.encoder_level1 = [CAB(n_feat,                     kernel_size, reduction, bias=bias, act=act) for _ in range(2)]\n        self.encoder_level2 = [CAB(n_feat+scale_unetfeats,     kernel_size, reduction, bias=bias, act=act) for _ in range(2)]\n        self.encoder_level3 = [CAB(n_feat+(scale_unetfeats*2), kernel_size, reduction, bias=bias, act=act) for _ in range(2)]\n\n        self.encoder_level1 = nn.Sequential(*self.encoder_level1)\n        self.encoder_level2 = nn.Sequential(*self.encoder_level2)\n        self.encoder_level3 = nn.Sequential(*self.encoder_level3)\n\n        self.down12  = DownSample(n_feat, scale_unetfeats)\n        self.down23  = DownSample(n_feat+scale_unetfeats, scale_unetfeats)\n\n        # Cross Stage Feature Fusion (CSFF)\n        if csff:\n            self.csff_enc1 = nn.Conv2d(n_feat,                     n_feat,                     kernel_size=1, bias=bias)\n            self.csff_enc2 = nn.Conv2d(n_feat+scale_unetfeats,     n_feat+scale_unetfeats,     kernel_size=1, bias=bias)\n            self.csff_enc3 = nn.Conv2d(n_feat+(scale_unetfeats*2), n_feat+(scale_unetfeats*2), kernel_size=1, bias=bias)\n\n            self.csff_dec1 = nn.Conv2d(n_feat,                     n_feat,                     kernel_size=1, bias=bias)\n            self.csff_dec2 = nn.Conv2d(n_feat+scale_unetfeats,     n_feat+scale_unetfeats,     kernel_size=1, bias=bias)\n            self.csff_dec3 = nn.Conv2d(n_feat+(scale_unetfeats*2), n_feat+(scale_unetfeats*2), kernel_size=1, bias=bias)\n\n    def forward(self, x, encoder_outs=None, decoder_outs=None):\n        enc1 = self.encoder_level1(x)\n        if (encoder_outs is not None) and (decoder_outs is not None):\n            enc1 = enc1 + self.csff_enc1(encoder_outs[0]) + self.csff_dec1(decoder_outs[0])\n\n        x = self.down12(enc1)\n\n        enc2 = self.encoder_level2(x)\n        if (encoder_outs is not None) and (decoder_outs is not None):\n            enc2 = enc2 + self.csff_enc2(encoder_outs[1]) + self.csff_dec2(decoder_outs[1])\n\n        x = self.down23(enc2)\n\n        enc3 = self.encoder_level3(x)\n        if (encoder_outs is not None) and (decoder_outs is not None):\n            enc3 = enc3 + self.csff_enc3(encoder_outs[2]) + self.csff_dec3(decoder_outs[2])\n\n        return [enc1, enc2, enc3]\n\nclass Decoder(nn.Module):\n    def __init__(self, n_feat, kernel_size, reduction, act, bias, scale_unetfeats):\n        super(Decoder, self).__init__()\n\n        self.decoder_level1 = [CAB(n_feat,                     kernel_size, reduction, bias=bias, act=act) for _ in range(2)]\n        self.decoder_level2 = [CAB(n_feat+scale_unetfeats,     kernel_size, reduction, bias=bias, act=act) for _ in range(2)]\n        self.decoder_level3 = [CAB(n_feat+(scale_unetfeats*2), kernel_size, reduction, bias=bias, act=act) for _ in range(2)]\n\n        self.decoder_level1 = nn.Sequential(*self.decoder_level1)\n        self.decoder_level2 = nn.Sequential(*self.decoder_level2)\n        self.decoder_level3 = nn.Sequential(*self.decoder_level3)\n\n        self.skip_attn1 = CAB(n_feat,                 kernel_size, reduction, bias=bias, act=act)\n        self.skip_attn2 = CAB(n_feat+scale_unetfeats, kernel_size, reduction, bias=bias, act=act)\n\n        self.up21  = SkipUpSample(n_feat, scale_unetfeats)\n        self.up32  = SkipUpSample(n_feat+scale_unetfeats, scale_unetfeats)\n\n    def forward(self, outs):\n        enc1, enc2, enc3 = outs\n        dec3 = self.decoder_level3(enc3)\n\n        x = self.up32(dec3, self.skip_attn2(enc2))\n        dec2 = self.decoder_level2(x)\n\n        x = self.up21(dec2, self.skip_attn1(enc1))\n        dec1 = self.decoder_level1(x)\n\n        return [dec1,dec2,dec3]\n\n##########################################################################\n##---------- Resizing Modules ----------    \nclass DownSample(nn.Module):\n    def __init__(self, in_channels,s_factor):\n        super(DownSample, self).__init__()\n        self.down = nn.Sequential(nn.Upsample(scale_factor=0.5, mode='bilinear', align_corners=False),\n                                  nn.Conv2d(in_channels, in_channels+s_factor, 1, stride=1, padding=0, bias=False))\n\n    def forward(self, x):\n        x = self.down(x)\n        return x\n\nclass UpSample(nn.Module):\n    def __init__(self, in_channels,s_factor):\n        super(UpSample, self).__init__()\n        self.up = nn.Sequential(nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n                                nn.Conv2d(in_channels+s_factor, in_channels, 1, stride=1, padding=0, bias=False))\n\n    def forward(self, x):\n        x = self.up(x)\n        return x\n\nclass SkipUpSample(nn.Module):\n    def __init__(self, in_channels,s_factor):\n        super(SkipUpSample, self).__init__()\n        self.up = nn.Sequential(nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n                                nn.Conv2d(in_channels+s_factor, in_channels, 1, stride=1, padding=0, bias=False))\n\n    def forward(self, x, y):\n        x = self.up(x)\n        x = x + y\n        return x\n\n##########################################################################\n## Original Resolution Block (ORB)\nclass ORB(nn.Module):\n    def __init__(self, n_feat, kernel_size, reduction, act, bias, num_cab):\n        super(ORB, self).__init__()\n        modules_body = []\n        modules_body = [CAB(n_feat, kernel_size, reduction, bias=bias, act=act) for _ in range(num_cab)]\n        modules_body.append(conv(n_feat, n_feat, kernel_size))\n        self.body = nn.Sequential(*modules_body)\n\n    def forward(self, x):\n        res = self.body(x)\n        res += x\n        return res\n\n##########################################################################\nclass ORSNet(nn.Module):\n    def __init__(self, n_feat, scale_orsnetfeats, kernel_size, reduction, act, bias, scale_unetfeats, num_cab):\n        super(ORSNet, self).__init__()\n\n        self.orb1 = ORB(n_feat+scale_orsnetfeats, kernel_size, reduction, act, bias, num_cab)\n        self.orb2 = ORB(n_feat+scale_orsnetfeats, kernel_size, reduction, act, bias, num_cab)\n        self.orb3 = ORB(n_feat+scale_orsnetfeats, kernel_size, reduction, act, bias, num_cab)\n\n        self.up_enc1 = UpSample(n_feat, scale_unetfeats)\n        self.up_dec1 = UpSample(n_feat, scale_unetfeats)\n\n        self.up_enc2 = nn.Sequential(UpSample(n_feat+scale_unetfeats, scale_unetfeats), UpSample(n_feat, scale_unetfeats))\n        self.up_dec2 = nn.Sequential(UpSample(n_feat+scale_unetfeats, scale_unetfeats), UpSample(n_feat, scale_unetfeats))\n\n        self.conv_enc1 = nn.Conv2d(n_feat, n_feat+scale_orsnetfeats, kernel_size=1, bias=bias)\n        self.conv_enc2 = nn.Conv2d(n_feat, n_feat+scale_orsnetfeats, kernel_size=1, bias=bias)\n        self.conv_enc3 = nn.Conv2d(n_feat, n_feat+scale_orsnetfeats, kernel_size=1, bias=bias)\n\n        self.conv_dec1 = nn.Conv2d(n_feat, n_feat+scale_orsnetfeats, kernel_size=1, bias=bias)\n        self.conv_dec2 = nn.Conv2d(n_feat, n_feat+scale_orsnetfeats, kernel_size=1, bias=bias)\n        self.conv_dec3 = nn.Conv2d(n_feat, n_feat+scale_orsnetfeats, kernel_size=1, bias=bias)\n\n    def forward(self, x, encoder_outs, decoder_outs):\n        x = self.orb1(x)\n        x = x + self.conv_enc1(encoder_outs[0]) + self.conv_dec1(decoder_outs[0])\n\n        x = self.orb2(x)\n        x = x + self.conv_enc2(self.up_enc1(encoder_outs[1])) + self.conv_dec2(self.up_dec1(decoder_outs[1]))\n\n        x = self.orb3(x)\n        x = x + self.conv_enc3(self.up_enc2(encoder_outs[2])) + self.conv_dec3(self.up_dec2(decoder_outs[2]))\n\n        return x\n\n\n##########################################################################\nclass MPRNet(nn.Module):\n    def __init__(self, in_c=1, out_c=1, n_feat=12, scale_unetfeats=8, scale_orsnetfeats=6, num_cab=4, kernel_size=3, reduction=4, bias=False):\n    # def __init__(self, in_c=3, out_c=3, n_feat=80, scale_unetfeats=48, scale_orsnetfeats=32, num_cab=8, kernel_size=3, reduction=4, bias=False):\n        super(MPRNet, self).__init__()\n\n        act=nn.PReLU()\n        self.shallow_feat1 = nn.Sequential(conv(in_c, n_feat, kernel_size, bias=bias), CAB(n_feat,kernel_size, reduction, bias=bias, act=act))\n        self.shallow_feat2 = nn.Sequential(conv(in_c, n_feat, kernel_size, bias=bias), CAB(n_feat,kernel_size, reduction, bias=bias, act=act))\n        self.shallow_feat3 = nn.Sequential(conv(in_c, n_feat, kernel_size, bias=bias), CAB(n_feat,kernel_size, reduction, bias=bias, act=act))\n\n        # Cross Stage Feature Fusion (CSFF)\n        self.stage1_encoder = Encoder(n_feat, kernel_size, reduction, act, bias, scale_unetfeats, csff=False)\n        self.stage1_decoder = Decoder(n_feat, kernel_size, reduction, act, bias, scale_unetfeats)\n\n        self.stage2_encoder = Encoder(n_feat, kernel_size, reduction, act, bias, scale_unetfeats, csff=True)\n        self.stage2_decoder = Decoder(n_feat, kernel_size, reduction, act, bias, scale_unetfeats)\n\n        self.stage3_orsnet = ORSNet(n_feat, scale_orsnetfeats, kernel_size, reduction, act, bias, scale_unetfeats, num_cab)\n\n        self.sam12 = SAM(n_feat, kernel_size=1, bias=bias)\n        self.sam23 = SAM(n_feat, kernel_size=1, bias=bias)\n\n        self.concat12  = conv(n_feat*2, n_feat, kernel_size, bias=bias)\n        self.concat23  = conv(n_feat*2, n_feat+scale_orsnetfeats, kernel_size, bias=bias)\n        self.tail     = conv(n_feat+scale_orsnetfeats, out_c, kernel_size, bias=bias)\n\n    def forward(self, x3_img):\n        # Original-resolution Image for Stage 3\n        H = x3_img.size(2)\n        W = x3_img.size(3)\n\n        # Multi-Patch Hierarchy: Split Image into four non-overlapping patches\n\n        # Two Patches for Stage 2\n        x2top_img  = x3_img[:,:,0:int(H/2),:]\n        x2bot_img  = x3_img[:,:,int(H/2):H,:]\n\n        # Four Patches for Stage 1\n        x1ltop_img = x2top_img[:,:,:,0:int(W/2)]\n        x1rtop_img = x2top_img[:,:,:,int(W/2):W]\n        x1lbot_img = x2bot_img[:,:,:,0:int(W/2)]\n        x1rbot_img = x2bot_img[:,:,:,int(W/2):W]\n\n        ##-------------------------------------------\n        ##-------------- Stage 1---------------------\n        ##-------------------------------------------\n        ## Compute Shallow Features\n        x1ltop = self.shallow_feat1(x1ltop_img)\n        x1rtop = self.shallow_feat1(x1rtop_img)\n        x1lbot = self.shallow_feat1(x1lbot_img)\n        x1rbot = self.shallow_feat1(x1rbot_img)\n\n        ## Process features of all 4 patches with Encoder of Stage 1\n        feat1_ltop = self.stage1_encoder(x1ltop)\n        feat1_rtop = self.stage1_encoder(x1rtop)\n        feat1_lbot = self.stage1_encoder(x1lbot)\n        feat1_rbot = self.stage1_encoder(x1rbot)\n\n        ## Concat deep features\n        feat1_top = [torch.cat((k,v), 3) for k,v in zip(feat1_ltop,feat1_rtop)]\n        feat1_bot = [torch.cat((k,v), 3) for k,v in zip(feat1_lbot,feat1_rbot)]\n\n        ## Pass features through Decoder of Stage 1\n        res1_top = self.stage1_decoder(feat1_top)\n        res1_bot = self.stage1_decoder(feat1_bot)\n\n        ## Apply Supervised Attention Module (SAM)\n        x2top_samfeats, stage1_img_top = self.sam12(res1_top[0], x2top_img)\n        x2bot_samfeats, stage1_img_bot = self.sam12(res1_bot[0], x2bot_img)\n\n        ## Output image at Stage 1\n        stage1_img = torch.cat([stage1_img_top, stage1_img_bot],2) \n        ##-------------------------------------------\n        ##-------------- Stage 2---------------------\n        ##-------------------------------------------\n        ## Compute Shallow Features\n        x2top  = self.shallow_feat2(x2top_img)\n        x2bot  = self.shallow_feat2(x2bot_img)\n\n        ## Concatenate SAM features of Stage 1 with shallow features of Stage 2\n        x2top_cat = self.concat12(torch.cat([x2top, x2top_samfeats], 1))\n        x2bot_cat = self.concat12(torch.cat([x2bot, x2bot_samfeats], 1))\n\n        ## Process features of both patches with Encoder of Stage 2\n        feat2_top = self.stage2_encoder(x2top_cat, feat1_top, res1_top)\n        feat2_bot = self.stage2_encoder(x2bot_cat, feat1_bot, res1_bot)\n\n        ## Concat deep features\n        feat2 = [torch.cat((k,v), 2) for k,v in zip(feat2_top,feat2_bot)]\n\n        ## Pass features through Decoder of Stage 2\n        res2 = self.stage2_decoder(feat2)\n\n        ## Apply SAM\n        x3_samfeats, stage2_img = self.sam23(res2[0], x3_img)\n\n\n        ##-------------------------------------------\n        ##-------------- Stage 3---------------------\n        ##-------------------------------------------\n        ## Compute Shallow Features\n        x3     = self.shallow_feat3(x3_img)\n\n        ## Concatenate SAM features of Stage 2 with shallow features of Stage 3\n        x3_cat = self.concat23(torch.cat([x3, x3_samfeats], 1))\n\n        x3_cat = self.stage3_orsnet(x3_cat, feat2, res2)\n\n        stage3_img = self.tail(x3_cat)\n\n        return [stage3_img+x3_img, stage2_img, stage1_img]\n\n###########################################################################","metadata":{"execution":{"iopub.status.busy":"2022-06-19T04:31:01.244356Z","iopub.execute_input":"2022-06-19T04:31:01.244919Z","iopub.status.idle":"2022-06-19T04:31:01.326645Z","shell.execute_reply.started":"2022-06-19T04:31:01.244879Z","shell.execute_reply":"2022-06-19T04:31:01.325859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <h1> Run training for model </h1>","metadata":{}},{"cell_type":"code","source":"# run training\n\nresult_dir = os.path.join(train_save_dir, mode, 'results', session)\nmodel_dir  = os.path.join(train_save_dir, mode, 'models',  session)\n\nutils.mkdir(result_dir)\nutils.mkdir(model_dir)\n\n\n######### Model ###########\nmodel_restoration = MPRNet()\nmodel_restoration.cuda()\n\ndevice_ids = [i for i in range(torch.cuda.device_count())]\nif torch.cuda.device_count() > 1:\n  print(\"\\n\\nLet's use\", torch.cuda.device_count(), \"GPUs!\\n\\n\")\n\n\nnew_lr = initial_lr \n\noptimizer = optim.Adam(model_restoration.parameters(), lr=new_lr, betas=(0.9, 0.999),eps=1e-8, weight_decay=1e-8)\n\n\n######### Scheduler ###########\nwarmup_epochs = 3\nscheduler_cosine = optim.lr_scheduler.CosineAnnealingLR(optimizer, epoch_num-warmup_epochs+40, eta_min=min_lr)\nscheduler = GradualWarmupScheduler(optimizer, multiplier=1, total_epoch=warmup_epochs, after_scheduler=scheduler_cosine)\nscheduler.step()\n\n######### Resume ###########\nif train_resume:\n    path_chk_rest    = utils.get_last_path(model_dir, '_latest.pth')\n    utils.load_checkpoint(model_restoration,path_chk_rest)\n    start_epoch = utils.load_start_epoch(path_chk_rest) + 1\n    utils.load_optim(optimizer, path_chk_rest)\n\n    for i in range(1, start_epoch):\n        scheduler.step()\n    new_lr = scheduler.get_lr()[0]\n    print('------------------------------------------------------------------------------')\n    print(\"==> Resuming Training with learning rate:\", new_lr)\n    print('------------------------------------------------------------------------------')\n\nif len(device_ids)>1:\n    model_restoration = nn.DataParallel(model_restoration, device_ids = device_ids)\n\n######### Loss ###########\ncriterion = losses.CharbonnierLoss()\n\n######### DataLoaders ###########\ntrain_dataset = get_training_data(train_dir, {'patch_size':patch_size})\ntrain_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, drop_last=False, pin_memory=True)\n\nval_dataset = get_validation_data(val_dir, {'patch_size':patch_size})\nval_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=False, drop_last=False, pin_memory=True)\n\nprint('===> Start Epoch {} End Epoch {}'.format(start_epoch,epoch_num + 1))\nprint('===> Loading datasets')\n\nbest_psnr = 0\nbest_epoch = 0\nbest_iter = 0\n\neval_now = 143\nprint(f\"\\nEval after every {eval_now} Iterations !!!\\n\")\nmixup = utils.MixUp_AUG()\n\nfor epoch in range(start_epoch, epoch_num + 1):\n    epoch_start_time = time.time()\n    epoch_loss = 0\n    train_id = 1\n\n    model_restoration.train()\n    for i, data in enumerate(tqdm(train_loader), 0):\n\n        # zero_grad\n        for param in model_restoration.parameters():\n            param.grad = None\n\n        target = data[0].cuda()\n        input_ = data[1].cuda()\n\n        if epoch>5:\n            target, input_ = mixup.aug(target, input_)\n\n        restored = model_restoration(input_)\n\n        # Compute loss at each stage\n        crits = []\n        for j in range(len(restored)):\n            crits.append(criterion(torch.clamp(restored[j],0,1),target))\n        loss = torch.stack(crits, dim=0).sum(dim=0).sum(dim=0)\n        # loss = np.sum([criterion(torch.clamp(restored[j],0,1),target) for j in range(len(restored))])\n\n        loss.backward()\n        optimizer.step()\n        epoch_loss +=loss.item()\n\n        #### Evaluation ####\n        if i%eval_now==0 and i>0:\n            model_restoration.eval()\n            psnr_val_rgb = []\n            for ii, data_val in enumerate((val_loader), 0):\n                target = data_val[0].cuda()\n                input_ = data_val[1].cuda()\n\n                with torch.no_grad():\n                    restored = model_restoration(input_)\n                restored = restored[0]\n\n                for res,tar in zip(restored,target):\n                    psnr_val_rgb.append(utils.torchPSNR(res, tar))\n\n            psnr_val_rgb  = torch.stack(psnr_val_rgb).mean().item()\n\n            if psnr_val_rgb > best_psnr:\n                best_psnr = psnr_val_rgb\n                best_epoch = epoch\n                best_iter = i\n                torch.save({'epoch': epoch, \n                            'state_dict': model_restoration.state_dict(),\n                            'optimizer' : optimizer.state_dict()\n                            }, os.path.join(model_dir,\"model_best.pth\"))\n\n            print(\"[epoch %d it %d PSNR: %.4f --- best_epoch %d best_iter %d Best_PSNR %.4f]\" % (epoch, i, psnr_val_rgb, best_epoch, best_iter, best_psnr))\n\n            torch.save({'epoch': epoch, \n                        'state_dict': model_restoration.state_dict(),\n                        'optimizer' : optimizer.state_dict()\n                        }, os.path.join(model_dir,f\"model_epoch_{epoch}.pth\")) \n\n            model_restoration.train()\n\n    scheduler.step()\n\n    print(\"------------------------------------------------------------------\")\n    print(\"Epoch: {}\\tTime: {:.4f}\\tLoss: {:.4f}\\tLearningRate {:.6f}\".format(epoch, time.time()-epoch_start_time, epoch_loss, scheduler.get_lr()[0]))\n    print(\"------------------------------------------------------------------\")\n\n    torch.save({'epoch': epoch, \n                'state_dict': model_restoration.state_dict(),\n                'optimizer' : optimizer.state_dict()\n                }, os.path.join(model_dir,\"model_latest.pth\")) ","metadata":{"execution":{"iopub.status.busy":"2022-06-19T04:34:33.62978Z","iopub.execute_input":"2022-06-19T04:34:33.630223Z","iopub.status.idle":"2022-06-19T06:10:49.944191Z","shell.execute_reply.started":"2022-06-19T04:34:33.630186Z","shell.execute_reply":"2022-06-19T06:10:49.943375Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# misc functions\n\ndef save_img(filepath, img):\n    cv2.imwrite(filepath,cv2.cvtColor(img, cv2.COLOR_RGB2BGR))\n\ndef load_checkpoint(model, weights):\n    checkpoint = torch.load(weights)\n    try:\n        model.load_state_dict(checkpoint[\"state_dict\"])\n    except:\n        state_dict = checkpoint[\"state_dict\"]\n        new_state_dict = OrderedDict()\n        for k, v in state_dict.items():\n            name = k[7:] # remove `module.`\n            new_state_dict[name] = v\n        model.load_state_dict(new_state_dict)\n","metadata":{"execution":{"iopub.status.busy":"2022-06-19T06:15:09.219983Z","iopub.execute_input":"2022-06-19T06:15:09.220594Z","iopub.status.idle":"2022-06-19T06:15:09.226935Z","shell.execute_reply.started":"2022-06-19T06:15:09.220556Z","shell.execute_reply":"2022-06-19T06:15:09.225777Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <h1> Predict clean image </h1>","metadata":{}},{"cell_type":"code","source":"# run model and get cleaned images\n\ntask    = \"Denoising\"\ninp_dir = test_dir\nout_dir = test_out_dir\n\nos.makedirs(out_dir, exist_ok=True)\n\nfiles = natsorted(glob(os.path.join(inp_dir, '*.jpg'))\n                + glob(os.path.join(inp_dir, '*.png')))\n\nif len(files) == 0:\n    raise Exception(f\"No files found at {inp_dir}\")\n\n# Load corresponding model architecture and weights\nmodel = MPRNet()\nmodel.cuda()\n\n\nweights =  \"./checkpoints/Denoising/models/MPRNet/model_latest.pth\"\nload_checkpoint(model, weights)\nmodel.eval()\n\nimg_multiple_of = 2\n\nfor file_ in files:\n    img = Image.open(file_).convert('L')\n    input_ = TF.to_tensor(img).unsqueeze(0).cuda()\n\n    # Pad the input if not_multiple_of 8\n    h,w = input_.shape[2], input_.shape[3]\n    H,W = ((h+img_multiple_of)//img_multiple_of)*img_multiple_of, ((w+img_multiple_of)//img_multiple_of)*img_multiple_of\n    padh = H-h if h%img_multiple_of!=0 else 0\n    padw = W-w if w%img_multiple_of!=0 else 0\n    input_ = F.pad(input_, (0,padw,0,padh), 'reflect')\n\n    with torch.no_grad():\n        restored = model(input_)\n    restored = restored[0]\n    restored = torch.clamp(restored, 0, 1)\n\n    # Unpad the output\n    restored = restored[:,:,:h,:w]\n\n    restored = restored.permute(0, 2, 3, 1).cpu().detach().numpy()\n    restored = img_as_ubyte(restored[0])\n\n    f = os.path.splitext(os.path.split(file_)[-1])[0]\n    save_img((os.path.join(out_dir, f+'.png')), restored)\n\nprint(f\"Files saved at {out_dir}\")","metadata":{"execution":{"iopub.status.busy":"2022-06-19T06:15:47.924404Z","iopub.execute_input":"2022-06-19T06:15:47.925378Z","iopub.status.idle":"2022-06-19T06:16:07.764457Z","shell.execute_reply.started":"2022-06-19T06:15:47.925328Z","shell.execute_reply":"2022-06-19T06:16:07.762971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Display output\n\nfrom matplotlib import pyplot as plt\n\nn = 0\nfor file_ in files:\n    img_noisy = np.array(Image.open(file_).convert('L'))\n    img_clean = cv2.imread(out_dir+'/'+os.path.basename(file_), cv2.IMREAD_GRAYSCALE)\n    \n    plt.figure()\n    plt.subplot(121)\n    plt.imshow(img_noisy,cmap='gray')\n    plt.subplot(122)\n    plt.imshow(img_clean,cmap='gray')\n    n+=1\n    if n >5:\n        break","metadata":{"execution":{"iopub.status.busy":"2022-06-19T06:27:27.759829Z","iopub.execute_input":"2022-06-19T06:27:27.760366Z","iopub.status.idle":"2022-06-19T06:27:29.322278Z","shell.execute_reply.started":"2022-06-19T06:27:27.76033Z","shell.execute_reply":"2022-06-19T06:27:29.321535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <h1> Get submission file </h1>","metadata":{}},{"cell_type":"code","source":"# create submission file\n\ncleaned_images_dir = out_dir\n\ndef select_pixels(img):\n    y,x = img.shape\n\n    pixels = list()\n\n    for i in range(10000):\n        pixel = (random.randrange(y), random.randrange(x))\n\n        if pixel not in pixels:\n            pixels.append(pixel)\n\n    return pixels\n\n\nrandom.seed(0)\n\ncleaned_images = sorted(os.listdir(cleaned_images_dir))\n\nwith open(\"submission.csv\", \"w\") as submission_file:\n    submission_file.write(\"id,predicted\\n\")\n\n    print(\"Processing images...\")\n    filenum = 1\n    for image in tqdm(cleaned_images):\n        \n\n        img = cv2.imread(\"{}/{}\".format(cleaned_images_dir, image), cv2.IMREAD_GRAYSCALE)\n        pixels = select_pixels(img)\n\n        for pixel in pixels:\n            y,x = pixel\n            submission_file.write(\"{}_{}_{},{}\\n\".format(filenum, y, x, img[y][x]/255.0))\n\n        filenum += 1\n","metadata":{"execution":{"iopub.status.busy":"2022-06-19T06:18:27.512922Z","iopub.execute_input":"2022-06-19T06:18:27.513814Z","iopub.status.idle":"2022-06-19T06:24:27.019983Z","shell.execute_reply.started":"2022-06-19T06:18:27.513768Z","shell.execute_reply":"2022-06-19T06:24:27.019125Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get submission file\n\nfrom IPython.display import FileLink\n\nFileLink(r'submission.csv')","metadata":{"execution":{"iopub.status.busy":"2022-06-19T06:24:30.260275Z","iopub.execute_input":"2022-06-19T06:24:30.261092Z","iopub.status.idle":"2022-06-19T06:24:30.267018Z","shell.execute_reply.started":"2022-06-19T06:24:30.261054Z","shell.execute_reply":"2022-06-19T06:24:30.266087Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get trained model (optional)\n\n!zip -r model_latest.zip ./checkpoints/Denoising/models/MPRNet/model_latest.pth\n\nFileLink(r'model_latest.zip')","metadata":{"execution":{"iopub.status.busy":"2022-06-19T06:17:03.240331Z","iopub.execute_input":"2022-06-19T06:17:03.240729Z","iopub.status.idle":"2022-06-19T06:17:04.12939Z","shell.execute_reply.started":"2022-06-19T06:17:03.240674Z","shell.execute_reply":"2022-06-19T06:17:04.12829Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get cleaned image  (optional)\n\n!zip -r output.zip ./output\n\nFileLink(r'output.zip')","metadata":{"execution":{"iopub.status.busy":"2022-06-19T06:16:49.904166Z","iopub.execute_input":"2022-06-19T06:16:49.905115Z","iopub.status.idle":"2022-06-19T06:16:52.366332Z","shell.execute_reply.started":"2022-06-19T06:16:49.905068Z","shell.execute_reply":"2022-06-19T06:16:52.365217Z"},"trusted":true},"execution_count":null,"outputs":[]}]}