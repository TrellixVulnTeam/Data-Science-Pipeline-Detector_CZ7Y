{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport zipfile, os, cv2\nfrom tqdm.auto import tqdm\nimport tensorflow as tf\n\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras import Input\nfrom tensorflow.keras import layers, callbacks, utils\n\nimport imgaug as ia\nfrom imgaug import augmenters as iaa\n\nsns.set_style('darkgrid')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"# Data Preparation"},{"metadata":{"trusted":true},"cell_type":"code","source":"path_zip = '../input/denoising-dirty-documents/'\npath = '/kaggle/working/'\n\nwith zipfile.ZipFile(path_zip + 'train.zip', 'r') as zip_ref:\n    zip_ref.extractall(path)\n\nwith zipfile.ZipFile(path_zip + 'test.zip', 'r') as zip_ref:\n    zip_ref.extractall(path)  \n    \nwith zipfile.ZipFile(path_zip + 'train_cleaned.zip', 'r') as zip_ref:\n    zip_ref.extractall(path)  \n    \nwith zipfile.ZipFile(path_zip + 'sampleSubmission.csv.zip', 'r') as zip_ref:\n    zip_ref.extractall(path)\n    \ntrain_img = sorted(os.listdir(path + '/train'))\ntrain_cleaned_img = sorted(os.listdir(path + '/train_cleaned'))\ntest_img = sorted(os.listdir(path + '/test'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class config():\n    IMG_SIZE = (420, 540)\n\nimgs = [cv2.imread(path + 'train/' + f) for f in sorted(os.listdir(path + 'train/'))]\nprint('Median Dimensions:', np.median([len(img) for img in imgs]), np.median([len(img[0]) for img in imgs]))\ndel imgs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def process_image(path):\n    img = cv2.imread(path)\n    img = np.asarray(img, dtype=\"float32\")\n    img = cv2.resize(img, config.IMG_SIZE[::-1])\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    img = img/255.0\n    img = np.reshape(img, (*config.IMG_SIZE, 1))\n    \n    return img","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = []\ntrain_cleaned = []\ntest = []\n\nfor f in sorted(os.listdir(path + 'train/')):\n    train.append(process_image(path + 'train/' + f))\n\nfor f in sorted(os.listdir(path + 'train_cleaned/')):\n    train_cleaned.append(process_image(path + 'train_cleaned/' + f))\n    \nfor f in sorted(os.listdir(path + 'test/')):\n    test.append(process_image(path + 'test/' + f))\n    \ntrain = np.asarray(train)\ntrain_cleaned = np.asarray(train_cleaned)\ntest = np.asarray(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape, train_cleaned.shape, test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(4, 2, figsize=(15,25))\nfor i in range(4):\n    ax[i][0].imshow(tf.squeeze(train[i]), cmap='gray')\n    ax[i][0].set_title('Noise image: {}'.format(train_img[i]))\n    \n    ax[i][1].imshow(tf.squeeze(train_cleaned[i]), cmap='gray')\n    ax[i][1].set_title('Denoised image: {}'.format(train_img[i]))\n    \n    ax[i][0].get_xaxis().set_visible(False)\n    ax[i][0].get_yaxis().set_visible(False)\n    ax[i][1].get_xaxis().set_visible(False)\n    ax[i][1].get_yaxis().set_visible(False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Image Augmentation"},{"metadata":{"trusted":true},"cell_type":"code","source":"def augment_pipeline(pipeline, images, seed=19):\n    ia.seed(seed)\n    processed_images = images.copy()\n    for step in pipeline:\n        temp = np.array(step.augment_images(images))\n        processed_images = np.append(processed_images, temp, axis=0)\n    return(processed_images)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rotate90 = iaa.Rot90(1) # rotate image 90 degrees\nrotate180 = iaa.Rot90(2) # rotate image 180 degrees\nrotate270 = iaa.Rot90(3) # rotate image 270 degrees\nrandom_rotate = iaa.Rot90((1,3)) # randomly rotate image from 90,180,270 degrees\nperc_transform = iaa.PerspectiveTransform(scale=(0.02, 0.1)) # Skews and transform images without black bg\nrotate10 = iaa.Affine(rotate=(10)) # rotate image 10 degrees\nrotate10r = iaa.Affine(rotate=(-10)) # rotate image 30 degrees in reverse\ncrop = iaa.Crop(px=(5, 32)) # Crop between 5 to 32 pixels\nhflip = iaa.Fliplr(1) # horizontal flips for 100% of images\nvflip = iaa.Flipud(1) # vertical flips for 100% of images\ngblur = iaa.GaussianBlur(sigma=(1, 1.5)) # gaussian blur images with a sigma of 1.0 to 1.5\nmotionblur = iaa.MotionBlur(8) # motion blur images with a kernel size 8\n\nseq_rp = iaa.Sequential([\n    iaa.Rot90((1,3)), # randomly rotate image from 90,180,270 degrees\n    iaa.PerspectiveTransform(scale=(0.02, 0.1)) # Skews and transform images without black bg\n])\n\nseq_cfg = iaa.Sequential([\n    iaa.Crop(px=(5, 32)), # crop images from each side by 5 to 32px (randomly chosen)\n    iaa.Fliplr(0.5), # horizontally flip 50% of the images\n    iaa.GaussianBlur(sigma=(0, 1.5)) # blur images with a sigma of 0 to 1.5\n])\n\nseq_fm = iaa.Sequential([\n    iaa.Flipud(1), # vertical flips all the images\n    iaa.MotionBlur(k=6) # motion blur images with a kernel size 6\n])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pipeline = [\n    rotate90, rotate180, rotate270, hflip, vflip\n]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"processed_train = augment_pipeline(pipeline, train)\nprocessed_train_cleaned = augment_pipeline(pipeline, train_cleaned)\n\nprocessed_train.shape, processed_train_cleaned.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Autoencoder"},{"metadata":{"trusted":true},"cell_type":"code","source":"class ConvSkipConnection(layers.Layer):\n    \"\"\"Implementation of Skip Connection for Convolution Layer\n\n    Args:\n        num_filters                   (int): the number of output filters in the convolution, default: 32\n        kernel_size (int/tuple of two ints): the height and width of the 2D convolution window,\n                single integer specifies the same value for both dimensions, default: 3\n        activation       (keras Activation): activation to be applied, default: relu\n        batch_normalization          (bool): whether to use Batch Normalization, default: False\n        dropout                     (float): the dropout rate, default: 0\n        kwargs          (keyword arguments): the arguments for Convolution Layer\n    \"\"\"\n\n    def __init__(\n        self,\n        num_filters,\n        kernel_size=3,\n        activation=\"relu\",\n        batch_normalization=False,\n        dropout=0,\n        **kwargs\n    ):\n        super().__init__()\n        self.num_filters = num_filters\n        self.kernel_size = kernel_size\n        self.activation = activation\n        self.batch_normalization = batch_normalization\n        self.dropout = dropout\n        self.kwargs = kwargs\n\n    def __call__(self, inputs):\n        x = inputs\n\n        skip_connection = layers.Conv2D(\n            self.num_filters, self.kernel_size, padding=\"same\", **self.kwargs\n        )(x)\n        if self.batch_normalization:\n            skip_connection = layers.BatchNormalization()(skip_connection)\n        skip_connection = layers.Activation(self.activation)(skip_connection)\n\n        skip_connection = layers.Conv2D(\n            self.num_filters, self.kernel_size, padding=\"same\", **self.kwargs\n        )(skip_connection)\n\n        x = layers.add([skip_connection, x])\n        if self.batch_normalization:\n            x = layers.BatchNormalization()(x)\n        x = layers.Activation(self.activation)(x)\n        if self.dropout > 0:\n            x = layers.Dropout(self.dropout)(x)\n        return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"inputs = Input(shape=(*config.IMG_SIZE, 1))\n\nx = layers.Conv2D(48, (1, 1), padding='same', activation='relu')(inputs)\nx = ConvSkipConnection(48, batch_normalization=True, dropout=0.2)(x)\nx = layers.MaxPooling2D((2, 2), padding='same')(x)\n\nx = layers.Conv2D(96, (1, 1), padding='same', activation='relu')(x)\nx = ConvSkipConnection(96, batch_normalization=True, dropout=0.2)(x)\nx = layers.Conv2D(48, (1, 1), padding='same', activation='relu')(x)\n\nx = layers.UpSampling2D((2, 2))(x)\nx = ConvSkipConnection(48, batch_normalization=True, dropout=0.2)(x)\noutputs = layers.Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n\nautoencoder = Model(inputs=inputs, outputs=outputs)\nautoencoder.compile(optimizer='adam', loss='mean_squared_error', metrics=['mean_absolute_error'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"autoencoder.summary()\nutils.plot_model(autoencoder, show_shapes=True, expand_nested=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"es = callbacks.EarlyStopping(\n    monitor='loss', patience=30, verbose=1, restore_best_weights=True\n)\n\nrlp = callbacks.ReduceLROnPlateau(\n    monitor='loss', factor=0.5, patience=5, min_lr=1e-15, mode='min', verbose=1\n)\n\n\nhistory =  autoencoder.fit(\n    processed_train, processed_train_cleaned,  \n    shuffle=True, callbacks=[es, rlp], epochs=500, batch_size=12\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(20, 6))\npd.DataFrame(history.history).iloc[:, :-1].plot(ax=ax)\ndel history","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Evaluation"},{"metadata":{"trusted":true},"cell_type":"code","source":"decoded_imgs = autoencoder(train[:4]).numpy()\n    \nfig, ax = plt.subplots(4, 2, figsize=(15,25))\nfor i in range(4):\n    ax[i][0].imshow(tf.squeeze(train_cleaned[i]), cmap='gray')\n    ax[i][0].set_title('Denoised image: {}'.format(train_img[i]))\n    \n    ax[i][1].imshow(tf.squeeze(decoded_imgs[i]), cmap='gray')\n    ax[i][1].set_title('Predicted image: {}'.format(train_img[i]))\n    \n    ax[i][0].get_xaxis().set_visible(False)\n    ax[i][0].get_yaxis().set_visible(False)\n    ax[i][1].get_xaxis().set_visible(False)\n    ax[i][1].get_yaxis().set_visible(False)    \n\ndel decoded_imgs    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ids = []\nvals = []\nfor i, f in tqdm(enumerate(test_img)):\n    file = path + 'test/' + f\n    imgid = int(f[:-4])\n    img = cv2.imread(file, 0)\n    img_shape = img.shape\n    decoded_img = np.squeeze(autoencoder(test[i:i+1]).numpy())\n    preds_reshaped = cv2.resize(decoded_img, (img_shape[1], img_shape[0]))\n\n    for r in range(img_shape[0]):\n        for c in range(img_shape[1]):\n            ids.append(str(imgid)+'_'+str(r + 1)+'_'+str(c + 1))\n            vals.append(preds_reshaped[r, c])\n\nprint('Length of IDs: {}'.format(len(ids)))            \npd.DataFrame({'id': ids, 'value': vals}).to_csv('submission.csv',index=False)\nprint('Results saved to submission.csv!')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}