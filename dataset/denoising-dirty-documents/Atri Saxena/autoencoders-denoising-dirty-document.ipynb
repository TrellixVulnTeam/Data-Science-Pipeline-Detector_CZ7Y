{"cells":[{"metadata":{"_uuid":"77fc2e352e7af100d42e1712afe22e90703c3e2c"},"cell_type":"markdown","source":"Hello Kagglers, This is the simple implementation of AutoEncoders in Keras. On [denoising-dirty-documents](https://www.kaggle.com/c/denoising-dirty-documents) dataset. \n![](https://media.giphy.com/media/NL6i0bK8omoMM/giphy.gif)"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nfrom pathlib import Path\nimport glob\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom skimage.io import imread, imshow, imsave\nfrom keras.preprocessing.image import load_img, array_to_img, img_to_array\nfrom keras.models import Sequential, Model\nfrom keras.layers import Dense, Conv2D, MaxPooling2D, UpSampling2D, Flatten, Input\nfrom keras.optimizers import Adam, Adadelta, Adagrad\nfrom keras import backend as K\nfrom sklearn.model_selection import train_test_split\nnp.random.seed(111)\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"input_dir  = Path('../input/')\ntrain = input_dir / 'train/'\ntrain_cleaned = input_dir / 'train_cleaned/'\ntest = input_dir / 'test/'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c04bfea03e72409e89e66d501ca1a10fee421336","scrolled":true},"cell_type":"code","source":"import glob\ntrain_images = glob.glob('../input/train/*.png')\ntrain_labels = glob.glob('../input/train_cleaned/*.png')\ntest_images = glob.glob('../input/test/*.png')\n\nprint(\"Total number of images in the training set: \", len(train_images))\nprint(\"Total number of cleaned images found: \", len(train_labels))\nprint(\"Total number of samples in the test set: \", len(test_images))\n\n# Lets' plot a few samples\n# First row will be raw data, second row will be the corresponding cleaned images\nsamples = train_images[:3] + train_labels[:3]\n\nf, ax = plt.subplots(2, 3, figsize=(20,10))\nfor i, img in enumerate(samples):\n    img = imread(img)\n    ax[i//3, i%3].imshow(img, cmap='gray')\n    ax[i//3, i%3].axis('off')\nplt.show()    ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a481d4b1dc205cf4beaced0ed123a9b845850241"},"cell_type":"markdown","source":"### AutoEncoder\nAn autoencoder is a type of artificial neural network used to learn efficient data codings in an unsupervised manner. The aim of an autoencoder is to learn a representation (encoding) for a set of data, typically for the purpose of dimensionality reduction. Recently, the autoencoder concept has become more widely used for learning generative models of data.\n![](https://cdn-images-1.medium.com/max/1600/1*44eDEuZBEsmG_TCAKRI3Kw@2x.png)\n\n[Read More here](https://towardsdatascience.com/deep-inside-autoencoders-7e41f319999f)"},{"metadata":{"trusted":true,"_uuid":"d030ee260028d51b2b7f212179896fde7e009461","collapsed":true},"cell_type":"code","source":"def build_autoenocder():\n    input_img = Input(shape=(420,540,1), name='image_input')\n    \n    #enoder \n    x = Conv2D(32, (3,3), activation='relu', padding='same', name='Conv1')(input_img)\n    x = MaxPooling2D((2,2), padding='same', name='pool1')(x)\n   \n    #decoder\n\n    x = Conv2D(32, (3,3), activation='relu', padding='same', name='Conv2')(x)\n    x = UpSampling2D((2,2), name='upsample3')(x)\n    x = Conv2D(1, (3,3), activation='sigmoid', padding='same', name='Conv3')(x)\n    \n    #model\n    autoencoder = Model(inputs=input_img, outputs=x)\n    autoencoder.compile(optimizer='Adagrad', loss='binary_crossentropy')\n    return autoencoder","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e7adf4ed73f6d998426fb047798f33b35c9ed312","scrolled":true},"cell_type":"code","source":"autoencoder = build_autoenocder()\nautoencoder.summary()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"07f2431ef520d1cf996f3402f152f816d38b186c"},"cell_type":"markdown","source":"The dataset is small, so we can actually store the numpy arrays of images and corresponding cleaned images numpy arrays into two numpy arrays."},{"metadata":{"trusted":true,"_uuid":"cf7ee653068dcb19ec39475c0a20c9ba08b088bf"},"cell_type":"code","source":"X = []\nY = []\n\nfor img in train_images:\n    img = load_img(img, grayscale=True,target_size=(420,540))\n    img = img_to_array(img).astype('float32')/255.\n    X.append(img)\n\nfor img in train_labels:\n    img = load_img(img, grayscale=True,target_size=(420,540))\n    img = img_to_array(img).astype('float32')/255.\n    Y.append(img)\n\n\nX = np.array(X)\nY = np.array(Y)\n\nprint(\"Size of X : \", X.shape)\nprint(\"Size of Y : \", Y.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"506bbc5c40037d967a468ce689842ee0e3e0e5cb"},"cell_type":"code","source":"# Split the dataset into training and validation. Always set the random state!!\nX_train, X_valid, y_train, y_valid = train_test_split(X, Y, test_size=0.1, random_state=111)\nprint(\"Total number of training samples: \", X_train.shape)\nprint(\"Total number of validation samples: \", X_valid.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"326d371a643d75137599479b04953f47b23cf8a6"},"cell_type":"code","source":"# Train your model\nautoencoder.fit(X_train, y_train, epochs=30, batch_size=8, validation_data=(X_valid, y_valid))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"f5ed7e984be573ebcc18febed4e8b9c6b33cb1e1"},"cell_type":"code","source":"sample_test = load_img(test_images[10], grayscale=True, target_size=(420,540))\nsample_test = img_to_array(sample_test)\nsample_test_img = sample_test.astype('float32')/255.\nsample_test_img = np.expand_dims(sample_test, axis=0)\n\n# Get the predition\npredicted_label = np.squeeze(autoencoder.predict(sample_test_img))\n\nf, ax = plt.subplots(1,2, figsize=(10,8))\nax[0].imshow(np.squeeze(sample_test), cmap='gray')\nax[1].imshow(np.squeeze(predicted_label.astype('int8')), cmap='gray')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"208db6e20291d04891fc5881e159675975a3f68a"},"cell_type":"markdown","source":"**Upvote the kernel if you liked it**. Also, if you found anything wrong in the notebook or if you want to suggest something as an improvement, please do share that in the comments section. I hope you enjoyed the kernel."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}