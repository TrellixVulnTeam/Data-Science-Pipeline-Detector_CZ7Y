{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install git+https://www.github.com/keras-team/keras-contrib.git","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# example of defining a 70x70 patchgan discriminator model\nfrom keras.optimizers import Adam\nfrom keras.initializers import RandomNormal\nfrom keras.models import Model\nfrom keras.models import Input\nfrom keras.layers import Conv2D\nfrom keras.layers import LeakyReLU\nfrom keras.layers import Activation\nfrom keras.layers import Concatenate\nfrom keras.layers import BatchNormalization\nfrom keras_contrib.layers.normalization.instancenormalization import InstanceNormalization\nfrom keras.utils.vis_utils import plot_model\nfrom keras.initializers import RandomNormal\nfrom keras.layers import Conv2DTranspose\nimport cv2\nimport glob\nimport numpy as np\nimport zipfile\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nfrom numpy.random import randn\nfrom numpy.random import randint\nfrom numpy import zeros\nfrom numpy import ones\nfrom numpy import asarray\nfrom matplotlib import pyplot\nfrom random import random\nimport tensorflow as tf\nfrom keras.models import load_model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# define the discriminator model\ndef define_discriminator(image_shape):\n    # weight initialization\n    init = RandomNormal(stddev=0.02)\n    # source image input\n    in_image = Input(shape=image_shape)\n    # C64\n    d = Conv2D(64, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(in_image)\n    d = LeakyReLU(alpha=0.2)(d)\n    # C128\n    d = Conv2D(128, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n    d = InstanceNormalization(axis=-1)(d)\n    d = LeakyReLU(alpha=0.2)(d)\n    # C256\n    d = Conv2D(256, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n    d = InstanceNormalization(axis=-1)(d)\n    d = LeakyReLU(alpha=0.2)(d)\n    # C512\n    d = Conv2D(512, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n    d = InstanceNormalization(axis=-1)(d)\n    d = LeakyReLU(alpha=0.2)(d)\n    # second last output layer\n    d = Conv2D(512, (4,4), padding='same', kernel_initializer=init)(d)\n    d = InstanceNormalization(axis=-1)(d)\n    d = LeakyReLU(alpha=0.2)(d)\n    # patch output\n    patch_out = Conv2D(1, (4,4), padding='same', kernel_initializer=init)(d)\n    # define model\n    model = Model(in_image, patch_out)\n    # compile model\n    model.compile(loss='mse', optimizer=Adam(lr=0.0002, beta_1=0.5), loss_weights=[0.5])\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# generator a resnet block\ndef resnet_block(n_filters, input_layer):\n    # weight initialization\n    init = RandomNormal(stddev=0.02)\n    # first layer convolutional layer\n    g = Conv2D(n_filters, (3,3), padding='same', kernel_initializer=init)(input_layer)\n    g = InstanceNormalization(axis=-1)(g)\n    g = Activation('relu')(g)\n    # second convolutional layer\n    g = Conv2D(n_filters, (3,3), padding='same', kernel_initializer=init)(g)\n    g = InstanceNormalization(axis=-1)(g)\n    # concatenate merge channel-wise with input layer\n    g = Concatenate()([g, input_layer])\n    return g","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# example of an encoder-decoder generator for the cyclegan\n# define the standalone generator model\ndef define_generator(image_shape=(256,256,3), n_resnet=9):\n    # weight initialization\n    init = RandomNormal(stddev=0.02)\n    # image input\n    in_image = Input(shape=image_shape)\n    # c7s1-64\n    g = Conv2D(64, (7,7), padding='same', kernel_initializer=init)(in_image)\n    g = InstanceNormalization(axis=-1)(g)\n    g = Activation('relu')(g)\n    # d128\n    g = Conv2D(128, (3,3), strides=(2,2), padding='same', kernel_initializer=init)(g)\n    g = InstanceNormalization(axis=-1)(g)\n    g = Activation('relu')(g)\n    # d256\n    g = Conv2D(256, (3,3), strides=(2,2), padding='same', kernel_initializer=init)(g)\n    g = InstanceNormalization(axis=-1)(g)\n    g = Activation('relu')(g)\n    # R256\n    for _ in range(n_resnet):\n        g = resnet_block(256, g)\n    # u128\n    g = Conv2DTranspose(128, (3,3), strides=(2,2), padding='same', kernel_initializer=init)(g)\n    g = InstanceNormalization(axis=-1)(g)\n    g = Activation('relu')(g)\n    # u64\n    g = Conv2DTranspose(64, (3,3), strides=(2,2), padding='same', kernel_initializer=init)(g)\n    g = InstanceNormalization(axis=-1)(g)\n    g = Activation('relu')(g)\n    # c7s1-3\n    g = Conv2D(3, (7,7), padding='same', kernel_initializer=init)(g)\n    g = InstanceNormalization(axis=-1)(g)\n    out_image = Activation('tanh')(g)\n    # define model\n    model = Model(in_image, out_image)\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# define a composite model for updating generators by adversarial and cycle loss\ndef define_composite_model(g_model_1, d_model, g_model_2, image_shape):\n    # ensure the model we're updating is trainable\n    g_model_1.trainable = True\n    # mark discriminator as not trainable\n    d_model.trainable = False\n    # mark other generator model as not trainable\n    g_model_2.trainable = False\n    # discriminator element\n    input_gen = Input(shape=image_shape)\n    gen1_out = g_model_1(input_gen)\n    output_d = d_model(gen1_out)\n    # identity element\n    input_id = Input(shape=image_shape)\n    output_id = g_model_1(input_id)\n    # forward cycle\n    output_f = g_model_2(gen1_out)\n    # backward cycle\n    gen2_out = g_model_2(input_id)\n    output_b = g_model_1(gen2_out)\n    # define model graph\n    model = Model([input_gen, input_id], [output_d, output_id, output_f, output_b])\n    # define optimization algorithm configuration\n    opt = Adam(lr=0.0002, beta_1=0.5)\n    # compile model with weighting of least squares loss and L1 loss\n    model.compile(loss=['mse', 'mae', 'mae', 'mae'], loss_weights=[1, 5, 10, 10], optimizer=opt)\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train cyclegan models\ndef train_model(d_model_A, d_model_B, g_model_AtoB, g_model_BtoA, c_model_AtoB, c_model_BtoA, dataset):\n    # define properties of the training run\n    n_epochs, n_batch, = 250, 1\n    # determine the output square shape of the discriminator\n    n_patch = d_model_A.output_shape[1]\n    # unpack dataset\n    trainA, trainB = dataset\n    # prepare image pool for fakes\n    poolA, poolB = list(), list()\n    # calculate the number of batches per training epoch\n    bat_per_epo = int(len(trainA) / n_batch)\n    # calculate the number of training iterations\n    n_steps = bat_per_epo * n_epochs\n    # manually enumerate epochs\n    for i in range(n_steps):\n        # select a batch of real samples\n        X_realA, y_realA = generate_real_samples(trainA, n_batch, n_patch)\n        X_realB, y_realB = generate_real_samples(trainB, n_batch, n_patch)\n        # generate a batch of fake samples\n        X_fakeA, y_fakeA = generate_fake_samples(g_model_BtoA, X_realB, n_patch)\n        X_fakeB, y_fakeB = generate_fake_samples(g_model_AtoB, X_realA, n_patch)\n        # update fakes from pool\n        X_fakeA = update_image_pool(poolA, X_fakeA)\n        X_fakeB = update_image_pool(poolB, X_fakeB)\n        # update generator B->A via adversarial and cycle loss\n        g_loss2, _, _, _, _  = c_model_BtoA.train_on_batch([X_realB, X_realA], [y_realA, X_realA, X_realB, X_realA])\n        # update discriminator for A -> [real/fake]\n        dA_loss1 = d_model_A.train_on_batch(X_realA, y_realA)\n        dA_loss2 = d_model_A.train_on_batch(X_fakeA, y_fakeA)\n        # update generator A->B via adversarial and cycle loss\n        g_loss1, _, _, _, _ = c_model_AtoB.train_on_batch([X_realA, X_realB], [y_realB, X_realB, X_realA, X_realB])\n        # update discriminator for B -> [real/fake]\n        dB_loss1 = d_model_B.train_on_batch(X_realB, y_realB)\n        dB_loss2 = d_model_B.train_on_batch(X_fakeB, y_fakeB)\n        # summarize performance\n        print('>%d, dA[%.3f,%.3f] dB[%.3f,%.3f] g[%.3f,%.3f]' % (i+1, dA_loss1,dA_loss2, dB_loss1,dB_loss2, g_loss1,g_loss2))\n   \n    filename_g_model_AtoB_model = 'generator_model_%s_.h5' % ('g_model_AtoB')\n    g_model_AtoB.save(filename_g_model_AtoB_model)\n    \n    filename_g_model_BtoA_model = 'generator_model_%s_.h5' % ('g_model_BtoA')\n    g_model_BtoA.save(filename_g_model_BtoA_model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# select a batch of random samples, returns images and target\ndef generate_real_samples(dataset, n_samples, patch_shape):\n    # choose random instances\n    # print(dataset.shape[0])\n    ix = randint(0, dataset.shape[0], n_samples)\n    # retrieve selected images\n    X = dataset[ix]\n    # generate 'real' class labels (1)\n    y = ones((n_samples, patch_shape, patch_shape, 1))\n    return X, y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# update image pool for fake images\ndef update_image_pool(pool, images, max_size=50):\n    selected = list()\n    for image in images:\n        if len(pool) < max_size:\n            # stock the pool\n            pool.append(image)\n            selected.append(image)\n        elif random() < 0.5:\n            # use image, but don't add it to the pool\n            selected.append(image)\n        else:\n            # replace an existing image and use replaced image\n            ix = randint(0, len(pool))\n            selected.append(pool[ix])\n            pool[ix] = image\n    return asarray(selected)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# generate a batch of images, returns images and targets\ndef generate_fake_samples(g_model, dataset, patch_shape):\n    # generate fake instance\n    X = g_model.predict(dataset)\n    # create 'fake' class labels (0)\n    y = zeros((len(X), patch_shape, patch_shape, 1))\n    return X, y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# input shape\nimage_shape = (256,256,3)\n# generator: A -> B\ng_model_AtoB = define_generator(image_shape)\n# generator: B -> A\ng_model_BtoA = define_generator(image_shape)\n# discriminator: A -> [real/fake]\nd_model_A = define_discriminator(image_shape)\n# discriminator: B -> [real/fake]\nd_model_B = define_discriminator(image_shape)\n# composite: A -> B -> [real/fake, A]\nc_model_AtoB = define_composite_model(g_model_AtoB, d_model_B, g_model_BtoA, image_shape)\n# composite: B -> A -> [real/fake, B]\nc_model_BtoA = define_composite_model(g_model_BtoA, d_model_A, g_model_AtoB, image_shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# path to zipped & working directories\npath_zip = '/kaggle/input/denoising-dirty-documents/'\npath = '/kaggle/working/'\n\n# unzip files first to working directory\n# We could use also unzipped data source, but why not to learn something new?\nwith zipfile.ZipFile(path_zip + 'train.zip', 'r') as zip_ref:\n    zip_ref.extractall(path)\n\nwith zipfile.ZipFile(path_zip + 'test.zip', 'r') as zip_ref:\n    zip_ref.extractall(path)  \n    \nwith zipfile.ZipFile(path_zip + 'train_cleaned.zip', 'r') as zip_ref:\n    zip_ref.extractall(path)  \n    \nwith zipfile.ZipFile(path_zip + 'sampleSubmission.csv.zip', 'r') as zip_ref:\n    zip_ref.extractall(path)  \n\n# For later use, we will store image names into list, so we can draw them simply.\n\n# store image names in list for later use\ntrain_img = sorted(os.listdir(path + '/train'))\ntrain_cleaned_img = sorted(os.listdir(path + '/train_cleaned'))\ntest_img = sorted(os.listdir(path + '/test'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# prepare function\ndef process_image(path):\n    img = cv2.imread(path)\n    img = np.asarray(img, dtype=\"float32\")\n    img = cv2.resize(img, (256, 256))\n    img = img / 255.0\n    return img","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# preprocess images\ntrain = []\ntrain_cleaned = []\ntest = []\n\nfor f in sorted(os.listdir(path + 'train/')):\n    train.append(process_image(path + 'train/' + f))\n\nfor f in sorted(os.listdir(path + 'train_cleaned/')):\n    train_cleaned.append(process_image(path + 'train_cleaned/' + f))\n   \nfor f in sorted(os.listdir(path + 'test/')):\n    test.append(process_image(path + 'test/' + f))\n       \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,25))\nfor i in range(0, 8, 2):\n    plt.subplot(4, 2, i + 1)\n    plt.xticks([])\n    plt.yticks([])\n    plt.imshow(train[i])\n    plt.title('Noise image: {}'.format(train_img[i]))\n    \n    plt.subplot(4, 2, i + 2)\n    plt.xticks([])\n    plt.yticks([])\n    plt.imshow(train_cleaned[i])\n    plt.title('Denoised image: {}'.format(train_img[i]))\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# load a dataset as a list of two numpy arrays\n\ntrain_dirty = np.asarray(train, dtype=np.float32)\ntrain_cleaned = np.asarray(train_cleaned, dtype=np.float32)\n\ndataset = [train_dirty, train_cleaned]\ntrain_model(d_model_A, d_model_B, g_model_AtoB, g_model_BtoA, c_model_AtoB, c_model_BtoA, dataset)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# load model\nmodel = load_model('./generator_model_g_model_AtoB_.h5',\ncustom_objects={'InstanceNormalization': InstanceNormalization})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_images = np.asarray(test, dtype=np.float32)\nprint(test_images.shape)\n\nresult_images = model.predict(test_images)\n\nplt.figure(figsize=(15,25))\nfor i in range(0, 8, 2):\n    plt.subplot(4, 2, i + 1)\n    plt.xticks([])\n    plt.yticks([])\n    plt.imshow(test[i])\n    plt.title('Noise image')\n    \n    plt.subplot(4, 2, i + 2)\n    plt.xticks([])\n    plt.yticks([])\n    plt.imshow(result_images[i])\n    plt.title('Denoised image')\n\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}