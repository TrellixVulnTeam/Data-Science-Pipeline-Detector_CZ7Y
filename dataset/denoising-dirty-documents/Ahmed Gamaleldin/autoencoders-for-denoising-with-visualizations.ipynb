{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport os\nimport glob\nfrom pathlib import Path\n\nfrom PIL import Image\nimport cv2\n\n# Python ≥3.5 is required\nimport sys\nassert sys.version_info >= (3, 5)\n\n# Scikit-Learn ≥0.20 is required\nimport sklearn\nassert sklearn.__version__ >= \"0.20\"\nfrom sklearn.model_selection import train_test_split\n\n# TensorFlow ≥2.0 is required\nimport tensorflow as tf\nfrom tensorflow import keras\nassert tf.__version__ >= \"2.0\"\nfrom keras.models import Sequential, Model\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D,MaxPooling2D, BatchNormalization, UpSampling2D, Input, ZeroPadding2D, Cropping2D\nfrom keras.preprocessing.image import load_img, array_to_img, img_to_array\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline \n\n# to make this notebook's output stable across runs\nnp.random.seed(42)\ntf.random.set_seed(42)\n\n# To plot pretty figures\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nmpl.rc('axes', labelsize=14)\nmpl.rc('xtick', labelsize=12)\nmpl.rc('ytick', labelsize=12)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define paths in the fancy way, after all we have pathlib now. No more os.path.join...whatever!!\ninput_dir  = Path('/kaggle/input/denoising-dirty-documents')\ntrain = input_dir / 'train.zip'\ntrain_cleaned = input_dir / 'train_cleaned.zip'\ntest = input_dir / 'test.zip'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# importing required modules \nfrom zipfile import ZipFile \n\ndef extract_zip_file(file_names):\n    # opening the zip file in READ mode \n    for file in file_names:\n        with ZipFile(file, 'r') as zip: \n            zip.extractall() \n\n# train and test are zipped folders. we need to unzip, save them for further processing\nextract_zip_file([train, train_cleaned, test])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# save directories of each image in a list using glob library\ntrain_images = sorted(glob.glob('train/*.png'))\ntrain_cleaned_images = sorted(glob.glob('train_cleaned/*.png'))\ntest_images = sorted(glob.glob('test/*.png'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#  convert images to arrays for training\ndef convert_imgs_to_array(images_folder, test=False):\n    images = []\n    for img_dir in images_folder:\n        # use keras built-in libraries load_img and im_to_array\n        image = load_img(img_dir, color_mode='grayscale', target_size=(258, 540, 1))\n        image = img_to_array(image).astype('float32') / 255.0\n        images.append(image)\n    return np.asarray(images)\n\nX_train_full = convert_imgs_to_array(train_images)\nY_train_full = convert_imgs_to_array(train_cleaned_images)\n#X_test = convert_imgs_to_array(test_images)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X_train_full.shape)\nprint(Y_train_full.shape)\n#print(X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#split into training and validation\nX_train, x_val, Y_train, y_val = train_test_split(X_train_full, Y_train_full,\n                                                  test_size=0.3, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X_train.shape)\nprint(x_val.shape)\nprint(Y_train.shape)\nprint(y_val.shape)\n#print(X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot random documents from training samples and their labels\ndef plot_documents(nrows=3, ncols=2):\n    selection = np.random.choice(len(X_train), size=(nrows*ncols), replace=False)\n    images = np.asarray(train_images)[selection]\n    cleaned_images = np.asarray(train_cleaned_images)[selection]\n    fig, axes = plt.subplots(figsize=(nrows*20, ncols*30), nrows=nrows, ncols=ncols)\n    fig.subplots_adjust(hspace = .05, wspace=.05)\n    axes = axes.ravel()\n    for img, img_cleaned, i in zip(images, cleaned_images, range(0, nrows*ncols, 2)):\n        axes[i].imshow(cv2.imread(img, cv2.IMREAD_GRAYSCALE), cmap='gray')\n        axes[i+1].imshow(cv2.imread(img_cleaned, cv2.IMREAD_GRAYSCALE), cmap='gray')\n        axes[i].axis('off')\n        axes[i+1].axis('off')\n        \nplot_documents()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# try that later\n'''\ndata_augmentation = keras.preprocessing.image.ImageDataGenerator(\n        rotation_range=5,  # randomly rotate images in the range (degrees, 0 to 180)\n        zoom_range = 0.1, # Randomly zoom image \n        width_shift_range=0.05,  # randomly shift images horizontally (fraction of total width)\n        height_shift_range=0.05)  # randomly shift images vertically (fraction of total height)\n\ndata_augmentation.fit(X_train)\n\n# plot random samples with keypoints from training\ndef plot_documents_after_augmentation(nrows=4, ncols=2):\n    for X_batch, _ in data_augmentation.flow(X_train, Y_train, batch_size=nrows*ncols):\n        print(X_batch.shape)\n        fig, axes = plt.subplots(figsize=(nrows*20, ncols*30), nrows=nrows, ncols=ncols)\n        fig.subplots_adjust(hspace=.05, wspace=.05)\n        \n        # create a grid of 3x3 images\n        for i, axes in zip(range(0, nrows*ncols), axes.ravel()):\n            axes.imshow(X_batch[i].reshape(258, 540), cmap='gray')\n        break\n   # for img, img_cleaned, i in zip(images, cleaned_images, range(0, nrows*ncols, 2)):\n    #    axes[i].imshow(cv2.imread(img, cv2.IMREAD_GRAYSCALE), cmap='gray')\n     #   axes[i+1].imshow(cv2.imread(img_cleaned, cv2.IMREAD_GRAYSCALE), cmap='gray')\n      #  axes[i].axis('off')\n       # axes[i+1].axis('off')\n        \nplot_documents_after_augmentation()\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\n# CNN model architecture (all these parameters might be tuned to achieve better results)\nfrom functools import partial\n\nDefaultConv2D = partial(Conv2D, activation='relu', padding='SAME')\n\nmodel = Sequential([\n    # encoder\n    DefaultConv2D(filters=64, kernel_size=3, input_shape=[258,540,1]),\n    BatchNormalization(),\n    MaxPooling2D((2, 2)),\n    DefaultConv2D(filters=32, kernel_size=3),\n    BatchNormalization(),\n    MaxPooling2D((2, 2)),\n    # decoder\n    DefaultConv2D(filters=32, kernel_size=3),\n    BatchNormalization(),\n    UpSampling2D((2,2)),\n    DefaultConv2D(filters=64, kernel_size=3),\n    BatchNormalization(),\n    UpSampling2D((2,2)),\n    DefaultConv2D(filters=1, kernel_size=3, activation='sigmoid'),\n    ZeroPadding2D((1,0))\n])\n\n# show model architecture\nmodel.summary()\n'''\n\n'''\n# CNN model architecture (all these parameters might be tuned to achieve better results)\nfrom functools import partial\n\nDefaultConv2D = partial(Conv2D, activation='relu', padding='SAME')\n\nmodel = Sequential([\n    # input layer\n    BatchNormalization(input_shape=(258, 540, 1)),\n    DefaultConv2D(64, (5, 5), kernel_initializer='he_normal'),\n    MaxPooling2D(pool_size=(2, 2)),\n    Dropout(0.2),\n    # layer 2\n    DefaultConv2D(32, (5, 5)),\n    MaxPooling2D(pool_size=(2, 2)),\n    Dropout(0.2),\n    # layer 3\n    DefaultConv2D(16, (5, 5)),\n    UpSampling2D((2, 2)),\n    Dropout(0.2),\n    # layer 4\n    DefaultConv2D(32, (3, 3)),\n    UpSampling2D((2, 2)),\n    Dropout(0.2),\n    # layer 5\n    DefaultConv2D(64, (3, 3)),\n    Dropout(0.2),\n    DefaultConv2D(1, (3, 3)),\n    ZeroPadding2D((1,0))\n])\n\nmodel.summary()\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets' define our autoencoder now\ndef build_autoenocder():\n    input_img = Input(shape=(None,None,1), name='image_input')\n    \n    #enoder \n    x = Conv2D(32, (3,3), activation='relu', padding='same', name='Conv1')(input_img)\n    x = Conv2D(64, (3,3), activation='relu', padding='same', name='Conv2')(x)\n    x = MaxPooling2D((2,2), padding='same', name='pool1')(x)\n    #x = MaxPooling2D((2,2), padding='same', name='pool2')(x)\n    \n    #decoder\n    x = Conv2D(64, (3,3), activation='relu', padding='same', name='Conv3')(x)\n    x = Conv2D(32, (3,3), activation='relu', padding='same', name='Conv4')(x)\n    x = UpSampling2D((2,2), name='upsample2')(x)\n    x = Conv2D(1, (3,3), activation='sigmoid', padding='same', name='Conv5')(x)\n    \n    #model\n    autoencoder = Model(inputs=input_img, outputs=x)\n    return autoencoder\n\nmodel = build_autoenocder()\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(optimizer='adam', loss='mse', metrics=['mae'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# another method is to use LearningRateScheduler, reduce the learning rate by 10% every epoch\n# annealer = LearningRateScheduler(lambda x: 1e-3 * 0.9 ** x)\nK = keras.callbacks\nreduce_lr = K.ReduceLROnPlateau(monitor='val_mae', patience=7,\n                                             verbose=1, factor=0.1, min_lr=0.00001)\n\nearly_stopping = K.EarlyStopping(monitor='val_mae', patience=20, restore_best_weights=True,\n                                 verbose=1, mode='auto')\n#checkpointer = K.ModelCheckpoint(filepath = 'best_model.hdf5', monitor='val_mae',\n                                 #verbose=1, save_weights_only=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"epochs = 600\nbatch_size = 2\n'''\nhistory = model.fit_generator(data_augmentation.flow(X_train, Y_train, batch_size=batch_size),\n                              epochs=epochs, validation_data=(x_val, y_val), \n                              steps_per_epoch=X_train.shape[0] // batch_size,\n                              shuffle=True, verbose=1,\n                              callbacks=[reduce_lr, early_stopping])\n'''\nhistory = model.fit(X_train, Y_train, validation_data=(x_val, y_val),\n                   batch_size=batch_size, epochs=epochs, shuffle=True,\n                   callbacks=[reduce_lr])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_loss, final_mae = model.evaluate(x_val, y_val, verbose=1)\nprint(\"Final loss: {0:.4f}, final mae: {1:.4f}\".format(final_loss, final_mae))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot the loss and accuracy curves for training and validation \nfig, ax = plt.subplots(2,1)\nax[0].plot(history.history['loss'], color='b', label=\"Training loss\")\nax[0].plot(history.history['val_loss'], color='r', label=\"validation loss\",axes =ax[0])\nlegend = ax[0].legend(loc='best', shadow=True)\n\nax[1].plot(history.history['mae'], color='b', label=\"Training mae\")\nax[1].plot(history.history['val_mae'], color='r',label=\"Validation mae\")\nlegend = ax[1].legend(loc='best', shadow=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot random samples with keypoints from training\ndef plot_documents_after_training(nrows=3, ncols=2):\n    selection = np.random.choice(len(test_images), size=(nrows*ncols), replace=False)\n    images = np.asarray(test_images)[selection]\n    #tested_images = X_test[selection]\n    #predicted_images = model.predict(tested_images)\n    fig, axes = plt.subplots(figsize=(nrows*20, ncols*30), nrows=nrows, ncols=ncols)\n    fig.subplots_adjust(hspace=.05, wspace=.05)\n    axes = axes.ravel()\n    for img, i in zip(images, range(0, nrows*ncols, 2)):\n        original_img = cv2.imread(img, cv2.IMREAD_GRAYSCALE)\n        img = load_img(img, color_mode='grayscale', target_size=(original_img.shape[0], original_img.shape[1], 1))\n        img = img_to_array(img).astype('float32') / 255.0\n        predicted_img = model.predict(np.expand_dims(img, axis=0))\n        axes[i].imshow(array_to_img(img), cmap='gray')\n        axes[i+1].imshow(array_to_img(predicted_img[0]), cmap='gray')\n        axes[i].axis('off')\n        axes[i+1].axis('off')\n        \nplot_documents_after_training()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# save the weights to prevent training every time you open the kernel\nmodel.save_weights(\"model.h5\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# after loading, you have to compile the model\nmodel.load_weights('/kaggle/input/denoising-dirty-documents2/model.h5')\nmodel.compile(optimizer='adam', loss='mse',\n             metrics=['mae'])\n\nfinal_loss, final_mae = model.evaluate(x_val, y_val, verbose=0)\nprint(\"Final loss: {0:.4f}, final mae: {1:.4f}\".format(final_loss, final_mae))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict_images(test_images):\n    results = []\n    for img in test_images:\n        original_img = cv2.imread(img, cv2.IMREAD_GRAYSCALE)\n        img = load_img(img, color_mode='grayscale', target_size=(original_img.shape[0], original_img.shape[1], 1))\n        img = img_to_array(img).astype('float32') / 255.0\n        predicted_img = model.predict(np.expand_dims(img, axis=0))\n        results.append(predicted_img)\n    return results\n\nresults = predict_images(test_images)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(results))\nprint(results[2].shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ids = []\nvalues = []\n\n# extract image ids from image file names ('train/509.png --> '509')\ndef split_numbers(s):\n    head = s.split('.')[0].split('/')[1]\n    return head\n\nfor i, pred in enumerate(results):\n    print('Predicted image shape: ', pred.shape)\n    img_dir = test_images[i]\n    img_id = split_numbers(img_dir)   \n\n    for j in range(pred.shape[2]):\n        for k in range(pred.shape[1]):\n            values.append(pred[0][k][j].item())\n            ids.append(img_id + '_' + str(k+1) + '_' + str(j+1))\n    print(\"Processed: {}\".format(img_id))   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame({'id': ids, 'value': values}).to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"my_submission = pd.read_csv(\"submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"my_submission.head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}