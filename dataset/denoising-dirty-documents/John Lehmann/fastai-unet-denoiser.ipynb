{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Denoising Document Backgrounds with Fastai Unet\n\n## Purpose\n\nThe purpose of the notebook is to demonstrate the Fastai Library to perform background removal in images using the Unet.  Most of this code comes from his [Super Resolution Notebook](https://github.com/fastai/course-v3/blob/master/nbs/dl1/lesson7-superres.ipynb) which is exlained in this [fast.ai course video](https://www.youtube.com/watch?time_continue=4745&v=9spwoDYwW_I). This was adapted from another [unfinished notebook](https://www.kaggle.com/hahmed747/background-removal-using-fastai-unet-learner).\n\n## Opportunities\n\nThere are a few reasons why this setup as-is won't perform that well:\n* The pretrained vgg model is using photos, not text\n* The pretrained vgg model is using color images, not black and white\n* Insufficient experimentation of hyperparameters\n* The values have to get clipped to 0,1; seems like a bad sign that they are out of that range"},{"metadata":{},"cell_type":"markdown","source":"# Setup"},{"metadata":{"trusted":true},"cell_type":"code","source":"%reload_ext autoreload\n%autoreload 2\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pathlib\nimport fastai\nfrom fastai import *\nfrom fastai.vision import *\nfrom fastai.callbacks import *\nfrom fastai.utils.mem import *\n\nfrom torchvision.models import vgg16_bn\nfrom subprocess import check_output","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Import"},{"metadata":{"trusted":true},"cell_type":"code","source":"input_path = Path('/kaggle/input/denoising-dirty-documents')\nitems = list(input_path.glob(\"*.zip\"))\nprint([x for x in items])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import zipfile\n\nfor item in items:\n    print(item)\n    with zipfile.ZipFile(str(item), \"r\") as z:\n        z.extractall(\".\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bs, size = 4, 128\narch = models.resnet34\npath_train = Path(\"train\")\npath_train_cleaned = Path(\"train_cleaned\")\npath_test = Path(\"test\")\npath_submission = Path(\"submission\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Processing"},{"metadata":{"trusted":true},"cell_type":"code","source":"src = ImageImageList.from_folder(path_train).split_by_rand_pct(0.2, seed=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_data(src, bs, size):\n    data = (\n        src.label_from_func(lambda x: path_train_cleaned / x.name)\n           .transform(get_transforms(max_zoom=2.), size=size, tfm_y=True)\n           .databunch(bs=bs)       \n           .normalize(imagenet_stats, do_y=True)\n    )\n    data.c = 3\n    return data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = get_data(src, bs, size)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# Show some validation examples\ndata.show_batch(ds_type=DatasetType.Valid, rows=2, figsize=(5, 5), title=\"Some image\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature Loss"},{"metadata":{"trusted":true},"cell_type":"code","source":"t = data.valid_ds[0][1].data\nt = torch.stack([t,t])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def gram_matrix(x):\n    n,c,h,w = x.size()\n    x = x.view(n, c, -1)\n    return (x @ x.transpose(1,2))/(c*h*w)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"base_loss = F.l1_loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vgg_m = vgg16_bn(True).features.cuda().eval()\nrequires_grad(vgg_m, False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Show the layers before all pooling layers, which turn out to be ReLU activations.\n# This is right before the grid size changes in the VGG model, which we are using\n# for feature generation.\nblocks = [i-1 for i,o in enumerate(children(vgg_m)) if isinstance(o,nn.MaxPool2d)]\nblocks, [vgg_m[i] for i in blocks]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class FeatureLoss(nn.Module):\n    def __init__(self, m_feat, layer_ids, layer_wgts):\n        \"\"\" m_feat is the pretrained model \"\"\"\n        super().__init__()\n        self.m_feat = m_feat\n        self.loss_features = [self.m_feat[i] for i in layer_ids]\n        # hooking grabs intermediate layers\n        self.hooks = hook_outputs(self.loss_features, detach=False)\n        self.wgts = layer_wgts\n        self.metric_names = ['pixel',] + [f'feat_{i}' for i in range(len(layer_ids))\n              ] + [f'gram_{i}' for i in range(len(layer_ids))]\n\n    def make_features(self, x, clone=False):\n        self.m_feat(x)\n        return [(o.clone() if clone else o) for o in self.hooks.stored]\n    \n    def forward(self, input, target):\n        # get features for target\n        out_feat = self.make_features(target, clone=True)\n        # features for input\n        in_feat = self.make_features(input)\n        # calc l1 pixel loss\n        self.feat_losses = [base_loss(input,target)]\n        # get l1 loss from all the block activations\n        self.feat_losses += [base_loss(f_in, f_out)*w\n                             for f_in, f_out, w in zip(in_feat, out_feat, self.wgts)]\n        self.feat_losses += [base_loss(gram_matrix(f_in), gram_matrix(f_out))*w**2 * 5e3\n                             for f_in, f_out, w in zip(in_feat, out_feat, self.wgts)]\n        # so we can show all the layer loss amounts\n        self.metrics = dict(zip(self.metric_names, self.feat_losses))\n        return sum(self.feat_losses)\n    \n    def __del__(self): self.hooks.remove()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feat_loss = FeatureLoss(vgg_m, blocks[2:5], [5,15,2])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train"},{"metadata":{"trusted":true},"cell_type":"code","source":"wd = 1e-3\nlearn = unet_learner(data, arch, wd=wd, loss_func=feat_loss, callback_fns=LossMetrics,\n                     blur=True, norm_type=NormType.Weight)\ngc.collect();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.model_dir = Path('models').absolute()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.lr_find()\nlearn.recorder.plot()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"print(f\"Validation set size: {len(data.valid_ds.items)}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr = 1e-3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def do_fit(save_name, lrs=slice(lr), pct_start=0.9):\n    learn.fit_one_cycle(10, lrs, pct_start=pct_start)\n    learn.save(save_name)\n    learn.show_results(rows=1, imgsize=5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"do_fit('1a', slice(lr*10))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.unfreeze()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"do_fit('1b', slice(1e-5, lr))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Increase resolution of the images.\ndata = get_data(src, 12, size*2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.data = data\nlearn.freeze()\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.load('1b');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"do_fit('2a')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.unfreeze()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"do_fit('2b', slice(1e-6,1e-4), pct_start=0.3)\n\n# save entire configuration\n#learn.export(file = model_path)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Test on a Validation Example"},{"metadata":{"trusted":true},"cell_type":"code","source":"fn = data.valid_ds.x.items[10]; fn","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img = open_image(fn); img.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"p,img_pred,b = learn.predict(img)\nshow_image(img, figsize=(8,5), interpolation='nearest');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Image(img_pred).show(figsize=(8,5))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Processing Test Set\n\nWe could add the test set to the original DataBunch with `add_test_folder`, \n([as discussed](https://forums.fast.ai/t/beginner-question-how-to-predict-on-test-set/31179)) however this encounters a number of problems. Namely, we want to process\nour entire test images without cropping transformations, but they have difference\nsizes which means a DataBunch (minibatches) can't be used.  \n\nThere is an option to assign a test set when calling `load_learn` but this\nhad problems as well.\n\nWe go with the simple inefficient route of processing them one by one.  This does require a [beautiful hack](https://forums.fast.ai/t/segmentation-mask-prediction-on-different-input-image-sizes/44389), however which is the first line below."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Turn off resizing transformations for inference time.\n# https://forums.fast.ai/t/segmentation-mask-prediction-on-different-input-image-sizes/44389\nlearn.data.single_ds.tfmargs['size'] = None","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_images = ImageImageList.from_folder(path_test)\nprint(test_images)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img = test_images[0]\nimg.show()\nimg.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"p, img_pred, b = learn.predict(img)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def rgb2gray(_img):\n    \"\"\" Convert from 3 channels to 1 channel \"\"\"\n    from skimage.color import rgb2gray as _rgb2gray\n\n    # Rotate channels dimension to the end, per skimage's expectations\n    _img_pred_np = _img.permute(1, 2, 0).numpy()\n    _img_pred_2d = Tensor(_rgb2gray(_img_pred_np))\n    # Add the channel dimension back\n    _img_pred = _img_pred_2d.unsqueeze(0)\n    return _img_pred\n  \nImage(rgb2gray(img_pred)).show(figsize=(8,5))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def write_image(fname, _img_tensor):\n    _img_tensor = (_img_tensor * 255).to(dtype=torch.uint8)\n    imwrite(path_submission/fname, _img_tensor.squeeze().numpy())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import csv\nfrom imageio import imread, imwrite\n\npath_submission.mkdir(exist_ok=True)\n\nwith Path('submission.csv').open('w', encoding='utf-8', newline='') as outf:\n    writer = csv.writer(outf)\n    writer.writerow(('id', 'value'))\n    for i, fname in enumerate(path_test.glob(\"*.png\")):\n        img = open_image(fname)\n        img_id = int(fname.name[:-4])\n        print('Processing: {} '.format(img_id))\n        # Predictions\n        p, img_pred, b = learn.predict(img)\n        # Convert to grayscale and clip out of range values.\n        img_2d = rgb2gray(img_pred).clamp(0, 1)\n        # Write an image file for examination.\n        write_image(fname.name, img_2d)\n        # Write to the submission file, in a very inefficient way.\n        for r in range(img_2d.shape[1]):\n            for c in range(img_2d.shape[2]):\n                id = str(img_id)+'_'+str(r + 1)+'_'+str(c + 1)\n                val = img_2d[0, r, c].item()\n                writer.writerow((id, val))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}