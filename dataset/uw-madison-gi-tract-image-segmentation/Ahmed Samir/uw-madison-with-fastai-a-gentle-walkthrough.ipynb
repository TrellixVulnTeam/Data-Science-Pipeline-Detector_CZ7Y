{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Outline\n\n* [Introduction](#intro)\n* [What is this competition about?](#what)\n* [Date Preparation](#data)\n    - [Fastai's High Level Segmentation API](#data-one)\n    - [Fastai's Mid Level Segmentation API (Data Block)](#data-two)\n    - [Fastai low level API](#data-three)\n    - [2.5D Data with Fastai low level API](#data-four)\n    - [2.5D Data pre-padded with fastai augmentations and random resized crops](#data-five)\n    - [2.5D Data with albumentations](#data-six)\n* [Training](#train)\n    - [Metrics and Losses](#train-one)\n    - [Using SMP models with Fastai](#train-two)\n    - [Using Fastai's DynamicUnet with timm encoder?](#train-three)\n* [Inference](#infer)","metadata":{}},{"cell_type":"markdown","source":"<a id=\"intro\"></a>\n\n# Introduction\n\nIn this notebook, I'll go through the creating a pipeline for this competition step by step using fastai, and I'll incorporate the various methods that participants have shared during the past also using fastai.\n\nIf you are beginner don't worry, I'll try to walk you through easy steps to understand what is going on. And if you are here just to see how specific ideas are implemented using fastai's api, like multi-channels 2.5D data or custom albumentations augmentation, then I hope you find what you are looking for in the right depth that you need.\n\n<a id=\"what\"></a>\n# What is this competition about?\n\nFor this part, I won't reinvent the wheel trying to do an in-depth EDA when other kagglers have made tremendous work. So please check the following notebooks to get an understanding of the problem we are adressing and the data of the competition.\n\n[UWM - GI Tract Image Segmentation - EDA by Darien Schettler](https://www.kaggle.com/code/dschettler8845/uwm-gi-tract-image-segmentation-eda)\n\n[⚕️ AW-Madison: EDA & In Depth Mask Exploration by Andrada Olteanu](https://www.kaggle.com/code/andradaolteanu/aw-madison-eda-in-depth-mask-exploration)","metadata":{}},{"cell_type":"markdown","source":"<a id=\"data\"></a>\n# Data Preparation\n\n#### What are we supposed to do in this step exactly?\nWe know that the competition provides a dataframe so let's take a look at that.","metadata":{}},{"cell_type":"code","source":"from fastai.vision.all import *\n\ntrain = pd.read_csv('../input/uw-madison-gi-tract-image-segmentation/train.csv', low_memory=False)\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-23T06:05:58.876755Z","iopub.execute_input":"2022-06-23T06:05:58.877098Z","iopub.status.idle":"2022-06-23T06:06:02.724382Z","shell.execute_reply.started":"2022-06-23T06:05:58.877024Z","shell.execute_reply":"2022-06-23T06:06:02.723588Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In order to get our pipeline working, we need each slice to have it's three classes segmentations (if they exist) in one row. We can do this using pandas.","metadata":{}},{"cell_type":"code","source":"train_df = train.pivot(index='id', columns='class', values='segmentation').reset_index()\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-23T06:06:02.726334Z","iopub.execute_input":"2022-06-23T06:06:02.726924Z","iopub.status.idle":"2022-06-23T06:06:02.916262Z","shell.execute_reply.started":"2022-06-23T06:06:02.726886Z","shell.execute_reply":"2022-06-23T06:06:02.915349Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### What is next?\n\nWe need to get our data to work with one of fastai's api levels. \n\n## Fastai's High Level Segmentation API\n\nTo get this one working we need 3 things:\n1. `path`: A Path object where the images reside\n2. `fnames`: The filenames of the images\n3. `label_func`: A function through which we can load the masks","metadata":{}},{"cell_type":"code","source":"# The Path objects are really useful\n# You can read about them in here https://docs.python.org/3/library/pathlib.html\npath = Path('../input/uw-madison-gi-tract-image-segmentation/train')\n\n# To get the filenames we'll use a nice helper function provided by fastai called `get_image_files` \n# that collects the filenames of all images within a Path\nfnames = get_image_files(path)\n\n# Let's inspect these so far\npath, fnames","metadata":{"execution":{"iopub.status.busy":"2022-06-23T06:06:02.918143Z","iopub.execute_input":"2022-06-23T06:06:02.918977Z","iopub.status.idle":"2022-06-23T06:06:06.913208Z","shell.execute_reply.started":"2022-06-23T06:06:02.918926Z","shell.execute_reply":"2022-06-23T06:06:06.912434Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that within `fnames` is a `Path` object for each image. Now let's move to create `label_func`, but before we do it let's think about what we need out of it.\n\n1. The function should take an image `fname` as an input which is a `Path` object\n2. Using this `Path`, it should construct a mask using from the rle format provided within `train_df`\n\nThat's a high level of what needs to be done, but it needs to be dissected a bit to understand it, especially the second point, let's reiterate more extensively over that.\n\n1. The function should find a way to map the `Path` object to the it's corresponding row in `train_df`\n2. Then it should convert the rle formatted masks into masks that can be overalyed with an image\n\n#### Let's address the first part. \n\n#### How can we map a `Path` object be used to a row in the dataframe?\n\nA great feature of `Path` than we can use is accessing it's parts using an attribute `parts`.","metadata":{}},{"cell_type":"code","source":"fnames[0].parts","metadata":{"execution":{"iopub.status.busy":"2022-06-23T06:06:06.915124Z","iopub.execute_input":"2022-06-23T06:06:06.915647Z","iopub.status.idle":"2022-06-23T06:06:06.922072Z","shell.execute_reply.started":"2022-06-23T06:06:06.915611Z","shell.execute_reply":"2022-06-23T06:06:06.92114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now all we need to do is to construct the id in the dataframe of that slice from it's parts.","metadata":{}},{"cell_type":"code","source":"# This is how an id looks in the dataframe\ntrain_df.id[0]","metadata":{"execution":{"iopub.status.busy":"2022-06-23T06:06:06.923491Z","iopub.execute_input":"2022-06-23T06:06:06.924015Z","iopub.status.idle":"2022-06-23T06:06:06.933859Z","shell.execute_reply.started":"2022-06-23T06:06:06.923979Z","shell.execute_reply":"2022-06-23T06:06:06.933181Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fnames[0].parts[5] + '_' + fnames[0].parts[7][:10]","metadata":{"execution":{"iopub.status.busy":"2022-06-23T06:06:06.935417Z","iopub.execute_input":"2022-06-23T06:06:06.935949Z","iopub.status.idle":"2022-06-23T06:06:06.944448Z","shell.execute_reply.started":"2022-06-23T06:06:06.935916Z","shell.execute_reply":"2022-06-23T06:06:06.9436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### And I think we got that one right. Now let's wrap into a function to use it later.","metadata":{}},{"cell_type":"code","source":"def get_slice_id(fname):\n    return fname.parts[5] + '_' + fname.parts[7][:10]","metadata":{"execution":{"iopub.status.busy":"2022-06-23T06:06:06.946046Z","iopub.execute_input":"2022-06-23T06:06:06.946431Z","iopub.status.idle":"2022-06-23T06:06:06.952077Z","shell.execute_reply.started":"2022-06-23T06:06:06.946395Z","shell.execute_reply":"2022-06-23T06:06:06.951196Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now since we have a way to map the `Path` to the dataframe, we need a function to convert the rle formatted masks into a mask array, and to do that we'll use a helper function.","metadata":{}},{"cell_type":"code","source":"# ref: https://www.kaggle.com/paulorzp/run-length-encode-and-decode\n# modified from: https://www.kaggle.com/inversion/run-length-decoding-quick-start\ndef rle_decode(mask_rle, shape, color=1):\n    \"\"\" TBD\n    \n    Args:\n        mask_rle (str): run-length as string formated (start length)\n        shape (tuple of ints): (height,width) of array to return \n    \n    Returns: \n        Mask (np.array)\n            - 1 indicating mask\n            - 0 indicating background\n\n    \"\"\"\n    # Split the string by space, then convert it into a integer array\n    s = np.array(mask_rle.split(), dtype=int)\n\n    # Every even value is the start, every odd value is the \"run\" length\n    starts = s[0::2] - 1\n    lengths = s[1::2]\n    ends = starts + lengths\n\n    # The image image is actually flattened since RLE is a 1D \"run\"\n    if len(shape)==3:\n        h, w, d = shape\n        img = np.zeros((h * w, d), dtype=np.float32)\n    else:\n        h, w = shape\n        img = np.zeros((h * w,), dtype=np.float32)\n\n    # The color here is actually just any integer you want!\n    for lo, hi in zip(starts, ends):\n        img[lo : hi] = color\n        \n    # Don't forget to change the image back to the original shape\n    return img.reshape(shape)","metadata":{"execution":{"iopub.status.busy":"2022-06-23T06:06:06.954009Z","iopub.execute_input":"2022-06-23T06:06:06.954448Z","iopub.status.idle":"2022-06-23T06:06:06.963788Z","shell.execute_reply.started":"2022-06-23T06:06:06.954407Z","shell.execute_reply":"2022-06-23T06:06:06.962873Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we have a way to map the slice fname to it's row, and we have a way to decode rle masks into arrays, all we need to do is to make a function that wraps all of this to output a mask for a slice fname. \n\nFor the sake of simplicity let's test it with one one mask class in order to see how we are doing, then we can move on to use all classes.","metadata":{}},{"cell_type":"code","source":"# Let's start with on slice fname\nfname = fnames[0]\n\n# First we need to get the slice row\nslice_id = get_slice_id(fname)\nslice_row = train_df.query('id == @slice_id')\n\n# Then we need to extract the slice height and width which are provided in the fname last part\nh, w = map(lambda x: int(x), fname.parts[-1].split('_')[2:4])\nh, w","metadata":{"execution":{"iopub.status.busy":"2022-06-23T06:06:06.965272Z","iopub.execute_input":"2022-06-23T06:06:06.965651Z","iopub.status.idle":"2022-06-23T06:06:06.989714Z","shell.execute_reply.started":"2022-06-23T06:06:06.965618Z","shell.execute_reply":"2022-06-23T06:06:06.988999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Now we create an empty numpy array with the dimensions of the slice and decode the rle mask into it\nmask = np.zeros((h, w, 1))\n\n# If the segmentation mask is not null then we shall populate the mask\nif not np.isnan(slice_row['large_bowel'].item()):\n    mask[:, :, 1] = rle_decode(slice_row['large_bowel'].item(), shape=(h, w), color=255)","metadata":{"execution":{"iopub.status.busy":"2022-06-23T06:06:06.993515Z","iopub.execute_input":"2022-06-23T06:06:06.994275Z","iopub.status.idle":"2022-06-23T06:06:07.000253Z","shell.execute_reply.started":"2022-06-23T06:06:06.994237Z","shell.execute_reply":"2022-06-23T06:06:06.999491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### I guess this now works, let's package it into a function and test making a dataloader.","metadata":{}},{"cell_type":"code","source":"def label_func(fname):\n    # First we need to get the slice row\n    slice_id = get_slice_id(fname)\n    slice_row = train_df.query('id == @slice_id')\n    \n    # Then we need to extract the slice width and height which are provided in the fname last part\n    # Typically the height is the first part of a dimension, but for some reason the slices have\n    # widths provided first\n    w, h = map(lambda x: int(x), fname.parts[-1].split('_')[2:4])\n    \n    # Create mask array (It needs to have 3 channels but fastai will only keep the first one anyways)\n    mask = np.zeros((h, w, 3), dtype=np.uint8)\n    \n    # If the segmentation mask is str\n    if isinstance(slice_row['large_bowel'].item(), str):\n        # Right now the pixel having the mask will have a code of one\n        mask[:, :, 0] = rle_decode(slice_row['large_bowel'].item(), shape=(h, w), color=1)\n            \n    return mask","metadata":{"execution":{"iopub.status.busy":"2022-06-23T06:06:07.003092Z","iopub.execute_input":"2022-06-23T06:06:07.003682Z","iopub.status.idle":"2022-06-23T06:06:07.010962Z","shell.execute_reply.started":"2022-06-23T06:06:07.003648Z","shell.execute_reply":"2022-06-23T06:06:07.010196Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# I have added Resize transformation in order to be able to collate batch of slices with different dimensions together\n# It isn't the most optimal way to do it like this and we'll see why as we go\ndls = SegmentationDataLoaders.from_label_func(path, fnames, label_func, item_tfms=Resize(224))","metadata":{"execution":{"iopub.status.busy":"2022-06-23T06:06:07.012391Z","iopub.execute_input":"2022-06-23T06:06:07.012918Z","iopub.status.idle":"2022-06-23T06:06:10.229809Z","shell.execute_reply.started":"2022-06-23T06:06:07.012881Z","shell.execute_reply":"2022-06-23T06:06:10.228904Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### And now let's visualize a batch to see what's going on...","metadata":{}},{"cell_type":"code","source":"dls.show_batch()","metadata":{"execution":{"iopub.status.busy":"2022-06-23T06:06:10.23102Z","iopub.execute_input":"2022-06-23T06:06:10.231889Z","iopub.status.idle":"2022-06-23T06:06:11.979237Z","shell.execute_reply.started":"2022-06-23T06:06:10.231858Z","shell.execute_reply":"2022-06-23T06:06:11.978429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"And voila, we've got ourselves a dataloader that can load single slices with their masks and can overlay them into visulaizations with little effort.\n\n#### However, now you might wonder about the blueness of the plots, and what if we want to add the other masks? And that's exactly correct to think about!\n\nWhile this dataloader served as an intro to see how we can hack a way to make a segmentation dataloader, it still isn't the best way. You can probably still train a model using this dataloader and even make a submission than can get you a decent score, given that you added the rest of the masks in the same channel and treated the problem as multi-class.\n\nIn order to do this, we only need to customize our `label_func` to add the rest of the masks in the same way we added the `large_bowel` mask, but we'll face the issue of overlapping masks, and then we can also just keep one the masks at random just to make this pipeline work.","metadata":{}},{"cell_type":"code","source":"def label_func(fname):\n    # First we need to get the slice row\n    slice_id = get_slice_id(fname)\n    slice_row = train_df.query('id == @slice_id')\n    \n    # Then we need to extract the slice width and height which are provided in the fname last part\n    # Typically the height is the first part of a dimension, but for some reason the slices have\n    # widths provided first\n    w, h = map(lambda x: int(x), fname.parts[-1].split('_')[2:4])\n    \n    # Create mask array (It needs to have 3 channels but fastai will only keep the first one anyways)\n    mask = np.zeros((h, w, 3), dtype=np.uint8)\n    \n    # If the segmentation mask is str\n    # Each mask should have it's own code (color) where fastai will use them for identification\n    if isinstance(slice_row['large_bowel'].item(), str):\n        mask[:, :, 0] = rle_decode(slice_row['large_bowel'].item(), shape=(h, w), color=1)\n        \n    if isinstance(slice_row['small_bowel'].item(), str):\n        mask[:, :, 0] = rle_decode(slice_row['small_bowel'].item(), shape=(h, w), color=2)\n        \n    # In case of overlap of first two masks\n    mask = np.where(mask == 3, 1, mask)\n        \n    if isinstance(slice_row['stomach'].item(), str):\n        mask[:, :, 0] = rle_decode(slice_row['stomach'].item(), shape=(h, w), color=3)\n        \n    # In case of overlap with one of the two masks\n    mask = np.where(mask == 4, 1, mask)\n    mask = np.where(mask == 5, 2, mask)\n        \n    return mask","metadata":{"execution":{"iopub.status.busy":"2022-06-23T06:06:11.98028Z","iopub.execute_input":"2022-06-23T06:06:11.980688Z","iopub.status.idle":"2022-06-23T06:06:11.992785Z","shell.execute_reply.started":"2022-06-23T06:06:11.980645Z","shell.execute_reply":"2022-06-23T06:06:11.991884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let's make a dataloader.","metadata":{}},{"cell_type":"code","source":"codes = {1: 'large_bowel', 2: 'small_bowel', 3: 'stomach'}\ndls = SegmentationDataLoaders.from_label_func(path, fnames, label_func, codes=codes, item_tfms=Resize(224))","metadata":{"execution":{"iopub.status.busy":"2022-06-23T06:06:11.994706Z","iopub.execute_input":"2022-06-23T06:06:11.995303Z","iopub.status.idle":"2022-06-23T06:06:12.276308Z","shell.execute_reply.started":"2022-06-23T06:06:11.995264Z","shell.execute_reply":"2022-06-23T06:06:12.275559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dls.show_batch()","metadata":{"execution":{"iopub.status.busy":"2022-06-23T06:06:12.278165Z","iopub.execute_input":"2022-06-23T06:06:12.279074Z","iopub.status.idle":"2022-06-23T06:06:13.867759Z","shell.execute_reply.started":"2022-06-23T06:06:12.279032Z","shell.execute_reply":"2022-06-23T06:06:13.866994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since there is no legend in these plots, we can't see the different masks, so let's take a look at one batch.","metadata":{}},{"cell_type":"code","source":"x, y = dls.one_batch()","metadata":{"execution":{"iopub.status.busy":"2022-06-23T06:06:13.868885Z","iopub.execute_input":"2022-06-23T06:06:13.869814Z","iopub.status.idle":"2022-06-23T06:06:14.971776Z","shell.execute_reply.started":"2022-06-23T06:06:13.869777Z","shell.execute_reply":"2022-06-23T06:06:14.970834Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's take a look at each count value using `torch.bincount`","metadata":{}},{"cell_type":"code","source":"torch.bincount(y.flatten())","metadata":{"execution":{"iopub.status.busy":"2022-06-23T06:06:14.973117Z","iopub.execute_input":"2022-06-23T06:06:14.973766Z","iopub.status.idle":"2022-06-23T06:06:15.014382Z","shell.execute_reply.started":"2022-06-23T06:06:14.973713Z","shell.execute_reply":"2022-06-23T06:06:15.013253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that there are four values, the first is for the background, and the rest are for the 3 masks.\n\nNow let's get it on the next part.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"data-two\"></a>\n## Fastai's Mid Level Segmentation API (Data Block)\n\nThrough this level, we can make some customizations that we weren't able to do in the previous level.\n1. We can load each slice in one channel\n2. We can load the masks in multiple-channels \n3. We can customize the batch visualization\n\n### How does it work?\n\nIt works really similar to `SegmentationDataLoaders`, in that we need to provide a set of `fnames` and a `label_func`, but we need to provide the `blocks` which will process our `fnames` and `label_func` output.\n\nWe'll also provide the type of splitter for train and validation.\n\nLet's think about that for a moment, each `fname` is basically an image file that needs to be loaded using `PIL` for example, and each label function is an array that should be loaded also using `PIL`.\n\nLucky for us, fastai comes with ImageBlock and MaskBlock that carries that from us.\n\nLet's now adjust the `label_func` to take the 3 masks into separate channels.","metadata":{}},{"cell_type":"code","source":"def label_func(fname):\n    # First we need to get the slice row\n    slice_id = get_slice_id(fname)\n    slice_row = train_df.query('id == @slice_id')\n    \n    # Then we need to extract the slice width and height which are provided in the fname last part\n    # Typically the height is the first part of a dimension, but for some reason the slices have\n    # widths provided first\n    w, h = map(lambda x: int(x), fname.parts[-1].split('_')[2:4])\n    \n    # Create mask array (It needs to have 3 channels but fastai will only keep the first one anyways)\n    mask = np.zeros((h, w, 3), dtype=np.uint8)\n    \n    # If the segmentation mask is str\n    # Each mask should have it's own code (color) where fastai will use them for identification\n    if isinstance(slice_row['large_bowel'].item(), str):\n        mask[:, :, 0] = rle_decode(slice_row['large_bowel'].item(), shape=(h, w), color=255)\n        \n    if isinstance(slice_row['small_bowel'].item(), str):\n        mask[:, :, 1] = rle_decode(slice_row['small_bowel'].item(), shape=(h, w), color=255)\n        \n    if isinstance(slice_row['stomach'].item(), str):\n        mask[:, :, 2] = rle_decode(slice_row['stomach'].item(), shape=(h, w), color=255)\n        \n    return mask\n\n","metadata":{"execution":{"iopub.status.busy":"2022-06-23T06:06:15.019061Z","iopub.execute_input":"2022-06-23T06:06:15.019459Z","iopub.status.idle":"2022-06-23T06:06:15.041203Z","shell.execute_reply.started":"2022-06-23T06:06:15.019421Z","shell.execute_reply":"2022-06-23T06:06:15.039564Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dblock = DataBlock(blocks=(ImageBlock(), MaskBlock()),\n                           splitter=RandomSplitter(0.2),\n                           get_y=label_func,\n                           item_tfms=Resize(224))\n\ndls = dblock.dataloaders(fnames)\n\ndls.show_batch()","metadata":{"execution":{"iopub.status.busy":"2022-06-23T06:06:15.049455Z","iopub.execute_input":"2022-06-23T06:06:15.05299Z","iopub.status.idle":"2022-06-23T06:06:17.108893Z","shell.execute_reply.started":"2022-06-23T06:06:15.052947Z","shell.execute_reply":"2022-06-23T06:06:17.107924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"First of all, the blue tint of the plots is caused by the background of the mask, as the majority of pixels don't have any label. So this can be fixed if turned the mask into 3 channels.\n\nNow fastai doesn't give us a Block that can make mutli channel masks out of the box, but we can one ourselves, or....\n\nWe can hack the `MaskBlock` itself. Let's take a look at what `MaskBlock` does.","metadata":{}},{"cell_type":"code","source":"MaskBlock??","metadata":{"execution":{"iopub.status.busy":"2022-06-23T06:06:17.110154Z","iopub.execute_input":"2022-06-23T06:06:17.111027Z","iopub.status.idle":"2022-06-23T06:06:17.171512Z","shell.execute_reply.started":"2022-06-23T06:06:17.110989Z","shell.execute_reply":"2022-06-23T06:06:17.170763Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"What we see in code is fastai `TransformBlock`, which handles out of the box transformations handling in the datablock api, but it doesn't seem to the problem.\n\nWe need to understand what the datablock api does for the mask, and here it is:\n1. First the datablock takes an `fname` and loads its image and mask\n2. It converts both of them mandatory into a Tensor using the `ToTensor` class that has specific methods image and masks\n3. Then it goes on with other transformations including conversion of image tensor types to floats and added any augmentations or normalization\n\nThe problem must lie with `ToTensor`, and after taking a look in the fastai source code, I found out that `ToTensor` converts masks to single channel by default, and all we need to do is to make them take multi channel.","metadata":{}},{"cell_type":"code","source":"# This was the code available in fastai\n@ToTensor\ndef encodes(self, o:PILMask): return o._tensor_cls(image2tensor(o)[0])\n\n# And this is how we customize it to suit our needs\n@ToTensor\ndef encodes(self, o:PILMask): return o._tensor_cls(image2tensor(o))","metadata":{"execution":{"iopub.status.busy":"2022-06-23T06:06:17.172846Z","iopub.execute_input":"2022-06-23T06:06:17.17334Z","iopub.status.idle":"2022-06-23T06:06:17.179029Z","shell.execute_reply.started":"2022-06-23T06:06:17.173295Z","shell.execute_reply":"2022-06-23T06:06:17.177902Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let's test this.","metadata":{}},{"cell_type":"code","source":"dblock = DataBlock(blocks=(ImageBlock, MaskBlock),\n                           splitter=RandomSplitter(0.2),\n                           get_y=label_func,\n                           item_tfms=Resize(224))\n\ndls = dblock.dataloaders(fnames)\n\ndls.show_batch()","metadata":{"execution":{"iopub.status.busy":"2022-06-23T06:06:17.180696Z","iopub.execute_input":"2022-06-23T06:06:17.181157Z","iopub.status.idle":"2022-06-23T06:06:19.014473Z","shell.execute_reply.started":"2022-06-23T06:06:17.181124Z","shell.execute_reply":"2022-06-23T06:06:19.01349Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Voila, now we have 3 masks represented in the plots, and the slices themselves are better, but need to correct those with high contrast.\n\nSo what we'll do for this is add a method that loads the `fname` into an array and preprocesses it before passing it to the `ImageBlock`.","metadata":{}},{"cell_type":"code","source":"def load_image(fname):\n    img = np.array(Image.open(fname))\n    img = np.interp(img, [np.min(img), np.max(img)], [0,255])\n    return img","metadata":{"execution":{"iopub.status.busy":"2022-06-23T06:06:19.015667Z","iopub.execute_input":"2022-06-23T06:06:19.016309Z","iopub.status.idle":"2022-06-23T06:06:19.022284Z","shell.execute_reply.started":"2022-06-23T06:06:19.016273Z","shell.execute_reply":"2022-06-23T06:06:19.021417Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dblock = DataBlock(blocks=(ImageBlock, MaskBlock),\n                           splitter=RandomSplitter(0.2),\n                           get_x=load_image,\n                           get_y=label_func,\n                           item_tfms=Resize(224))\n\ndls = dblock.dataloaders(fnames)\n\ndls.show_batch()","metadata":{"execution":{"iopub.status.busy":"2022-06-23T06:06:19.023841Z","iopub.execute_input":"2022-06-23T06:06:19.024227Z","iopub.status.idle":"2022-06-23T06:06:21.129179Z","shell.execute_reply.started":"2022-06-23T06:06:19.024191Z","shell.execute_reply":"2022-06-23T06:06:21.128382Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now all we need to do is just customize the `cmap` of the plots, and for that we'll customize the `show_batch` method.\n\nThe `typedispatch` decorator together with the type hints will dispatch this method whenever we call `dls.show_batch` with the specified types, which are `TensorImage` and `TensorMask`.","metadata":{}},{"cell_type":"code","source":"import matplotlib.patches as mpatches\n\n@typedispatch\ndef show_batch(x:TensorImage, y:TensorMask, samples, ctxs=None, max_n=6, nrows=None, ncols=2, figsize=None, **kwargs):\n    if figsize is None: figsize = (ncols*3, max_n//ncols * 3)\n    if ctxs is None: ctxs = get_grid(max_n, nrows=nrows, ncols=ncols, figsize=figsize)\n    for i,ctx in enumerate(ctxs): \n        x_i = x[i] / x[i].max()\n        show_image(x_i, ctx=ctx, cmap='gray', **kwargs)\n        show_image(y[i], ctx=ctx, cmap='Spectral_r', alpha=0.35, **kwargs)\n        red_patch = mpatches.Patch(color='red', label='large_bowel')\n        green_patch = mpatches.Patch(color='green', label='small_bowel')\n        blue_patch = mpatches.Patch(color='blue', label='stomach')\n        ctx.legend(handles=[red_patch, green_patch, blue_patch], fontsize=figsize[0]/2)","metadata":{"execution":{"iopub.status.busy":"2022-06-23T06:06:21.130497Z","iopub.execute_input":"2022-06-23T06:06:21.131027Z","iopub.status.idle":"2022-06-23T06:06:21.142179Z","shell.execute_reply.started":"2022-06-23T06:06:21.13099Z","shell.execute_reply":"2022-06-23T06:06:21.141166Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dls.show_batch(nrows=2, ncols=4, figsize=(15, 7))","metadata":{"execution":{"iopub.status.busy":"2022-06-23T06:06:21.14344Z","iopub.execute_input":"2022-06-23T06:06:21.143917Z","iopub.status.idle":"2022-06-23T06:06:22.871062Z","shell.execute_reply.started":"2022-06-23T06:06:21.143878Z","shell.execute_reply":"2022-06-23T06:06:22.870318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### And now we have functional dataloader that can show batches easily, and has multi channels masks which can be used to make a decent baseline for this competition. ","metadata":{}},{"cell_type":"markdown","source":"<a id=\"data-three\"></a>\n\n## Fastai low level API\n\nWe have seen how the high level and mid level api works, and now it's time to get our hands dirty with the low level api, which will help us understand how the previous levels work, since they are both build upon it.\n\nThe low level api gives you the flexibility to pass a pipeline containing a series of transformations to a `Dataset` object, which will be used to create a dataloader.\n\nLet's imagine how we can build the previous version of the dataloader with the low level api. \n\n1. First we need a function to load the image and mask into PIL format\n2. Then we need to resize them to collate them together into batches\n3. Then we will convert them to tensors \n4. Then we will convert these tensors from int to float","metadata":{}},{"cell_type":"code","source":"# The list of transformations contain two lists\n# 1. The first is for the X part of the dataset (images)\n# 2. The second if for the Y part of the dataset (masks)\ntfms = [[load_image, PILImageBW.create], [label_func, PILMask.create]]\n\n# Then we just pass the fnames along the transformations\ndsets = Datasets(fnames, tfms)\n\n# With the DataBlock API, we didn't worry about passing item and batch transformations\n# As some are implicitly used like ToTensor and IntToFloatTensor\n# But in here we need to pass them explicitly\ndls = dsets.dataloaders(after_item=[Resize(224), ToTensor], after_batch=[IntToFloatTensor])\n\n\n# Now let's see if our visualization works\ndls.show_batch(nrows=2, ncols=4, figsize=(15, 7))","metadata":{"execution":{"iopub.status.busy":"2022-06-23T06:06:22.875991Z","iopub.execute_input":"2022-06-23T06:06:22.876813Z","iopub.status.idle":"2022-06-23T06:06:24.626132Z","shell.execute_reply.started":"2022-06-23T06:06:22.876775Z","shell.execute_reply":"2022-06-23T06:06:24.625357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now if you get this part, you probably now understand how the previous API levels functioned. They just abstracted some the parts we did here. For example, the data blocks are supposed to give us the ease of not worrying about how to load images or masks, while the high level API doesn't let you worry about anything unless how to load the masks.\n\nPersonally, I love the low level API as it gives greater flexibility as we'll see now when we \n1. Load the data using 2.5D approach\n2. Pad the images before loading\n3. Add custom augmentations using albumentations","metadata":{}},{"cell_type":"markdown","source":"<a id='data-four'></a>\n\n## 2.5D Data with Fastai low level API\n\nFirst I'd like to thank [Awsaf](https://www.kaggle.com/awsaf49) for introducing the use of this [approach](https://www.kaggle.com/code/awsaf49/uwmgi-2-5d-stride-2-data) into this competition\n\nThe idea behind 2.5D data is to load the sequnetial slices as image channels, so the problem we are solving is still 2D semantic segmentation, but the data carries along in it more than 2D information. So it's not completely 3D, and it's still not 2D.\n\nLet's start with using only 3 channels with 2 strides, just like the original notebook.\n\nNow we'll start using the dataframe as input for our dataset instead of using the `fnames`, and that is because it's easier to specify the slices which be use as supplementary channels for the main slice. And we'll create two new columns\n1. The first column will contain the slice `fname`\n2. The second will contain a list of slices, with the main slice always in the middle, that we'll call `fnames`","metadata":{}},{"cell_type":"code","source":"train_df['partial_fname'] = train_df.id\nfname_df = pd.DataFrame({'partial_fname': [f'{fname.parts[-3]}_slice_{fname.parts[-1][6:10]}' for fname in fnames],\n                         'fname': fnames})\n\ntrain_df = train_df.merge(fname_df, on='partial_fname').drop('partial_fname', axis=1)\n\ntrain_df['case_id'] = train_df.id.apply(lambda x: x.split('_')[0])\ntrain_df['day_num'] = train_df.id.apply(lambda x: x.split('_')[1])\n\ntrain_df['slice_w'] = train_df[\"fname\"].apply(lambda x: int(str(x)[:-4].rsplit(\"_\",4)[1]))\ntrain_df['slice_h'] = train_df[\"fname\"].apply(lambda x: int(str(x)[:-4].rsplit(\"_\",4)[2]))\n\n\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-23T06:06:24.627352Z","iopub.execute_input":"2022-06-23T06:06:24.62818Z","iopub.status.idle":"2022-06-23T06:06:24.973464Z","shell.execute_reply.started":"2022-06-23T06:06:24.628142Z","shell.execute_reply":"2022-06-23T06:06:24.972592Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"channels = 3\nstride = 2\nfor j, i in enumerate(range(-1*(channels-channels//2-1), channels//2+1)):\n    method = 'ffill'\n    if i <= 0: method = 'bfill'\n    train_df[f'fname_{j:02}'] = train_df.groupby(['case_id', 'day_num'])['fname'].shift(stride*-i).fillna(method=method)\n    \ntrain_df['fnames'] = train_df[[f'fname_{j:02d}' for j in range(channels)]].values.tolist()\n\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-23T06:06:24.974731Z","iopub.execute_input":"2022-06-23T06:06:24.975314Z","iopub.status.idle":"2022-06-23T06:06:25.075108Z","shell.execute_reply.started":"2022-06-23T06:06:24.975262Z","shell.execute_reply":"2022-06-23T06:06:25.07431Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### And now we just need a function to load this slices into one image, and make our transformations to create the dataloader. But we need to remember that now instead of passing fnames directly, we are passing rows. And that applies to the mask function too, so we'll need to change it.","metadata":{}},{"cell_type":"code","source":"def load_image(fname):\n    img = np.array(Image.open(fname))\n    img = np.interp(img, [np.min(img), np.max(img)], [0,255])\n    return img\n\ndef get_25D_image(row):\n    imgs = np.zeros((row['slice_h'], row['slice_w'], len(row['fnames'])))\n        \n    for i, fname in enumerate(row['fnames']):\n        img = load_image(fname)\n        imgs[..., i] += img\n    return imgs.astype(np.uint8)\n\n\ndef get_mask(row):\n    mask = np.zeros((row['slice_h'], row['slice_w'], 3))\n        \n    if isinstance(row['large_bowel'], str):\n        mask[..., 0] += rle_decode(row['large_bowel'], shape=(row['slice_h'], row['slice_w']), color=255)\n    if isinstance(row['small_bowel'], str):\n        mask[..., 1] += rle_decode(row['small_bowel'], shape=(row['slice_h'], row['slice_w']), color=255)\n    if isinstance(row['stomach'], str):\n        mask[..., 2] += rle_decode(row['stomach'], shape=(row['slice_h'], row['slice_w']), color=255)\n        \n    return mask.astype(np.uint8)","metadata":{"execution":{"iopub.status.busy":"2022-06-23T06:06:25.076235Z","iopub.execute_input":"2022-06-23T06:06:25.077053Z","iopub.status.idle":"2022-06-23T06:06:25.088117Z","shell.execute_reply.started":"2022-06-23T06:06:25.077012Z","shell.execute_reply":"2022-06-23T06:06:25.087087Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let's create out transormations another time and see how are dataloaders handles them.","metadata":{}},{"cell_type":"code","source":"tfms = [[get_25D_image, PILImage.create],\n        [get_mask, PILMask.create]]\n    \ndsets = Datasets(train_df, tfms)\n\ndls = dsets.dataloaders(bs=16, after_item=[Resize(224), ToTensor], after_batch=[IntToFloatTensor])\n\n# Now let's see if our visualization works\ndls.show_batch(nrows=4, ncols=4, max_n=16, figsize=(15, 15))","metadata":{"execution":{"iopub.status.busy":"2022-06-23T06:06:25.089621Z","iopub.execute_input":"2022-06-23T06:06:25.090016Z","iopub.status.idle":"2022-06-23T06:06:27.845516Z","shell.execute_reply.started":"2022-06-23T06:06:25.08998Z","shell.execute_reply":"2022-06-23T06:06:27.844696Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Aaaand voila we have created a 2.5D dataloader using fastai's low level api. ","metadata":{}},{"cell_type":"markdown","source":"<a id='data-five'></a>\n## Pre Padding 2.5D Data\n\nWhen I first started working on this competition, I used fastai's `Resize` with `method=squish` since it enabled easy resizing of masks without worrying about the different dimensions of slices. \n\nThe idea was as follows:\n1. Resize the images to (224, 224)\n2. Predict masks\n3. Upsize each mask to it's original size\n\n\nThis idea was pretty okay to get a pipeline working, but the slices with non-equal heights and widths experienced a slight decrease in their score after upsizing, which in turn decreased the credibility of the validation scores during training.\n\nThen I saw in the original [2.5D notebook](https://www.kaggle.com/code/awsaf49/uwmgi-2-5d-stride-2-data) the idea of pre-padding the slices so they are all the same dimension. I didn't know about it, and thought that it would produce the same results, but when I tested it, it made my validations scores more reliable, and it didn't mess up slices with different heights and widths.\n\nTo implemenet this idea, we need to provide a function which pads slices while loading them to a specific size, which should be either greater than all slices, or at-least the same the slices with the greatest size.\n\nThis function shall be inserted in `load_image` and `get_mask`","metadata":{}},{"cell_type":"code","source":"def pad_img(img, up_size=None):\n    if up_size is None:\n        return img\n    shape0 = np.array(img.shape[:2])\n    resize = np.array(up_size)\n    if np.any(shape0!=resize):\n        diff = resize - shape0\n        pad0 = diff[0]\n        pad1 = diff[1]\n        pady = [pad0//2, pad0//2 + pad0%2]\n        padx = [pad1//2, pad1//2 + pad1%2]\n        img = np.pad(img, [pady, padx])\n        img = img.reshape((*resize))\n    return img\n\n\ndef unpad_img(img, up_size, org_size):\n    shape0 = np.array(org_size)\n    resize = np.array(up_size)\n    if np.any(shape0!=resize):\n        diff = resize - shape0\n        pad0 = diff[0]\n        pad1 = diff[1]\n        pady = [pad0//2, pad0//2 + pad0%2]\n        padx = [pad1//2, pad1//2 + pad1%2]\n        img = img[pady[0]:-pady[1], padx[0]:-padx[1], :]\n        img = img.reshape((*shape0, 3))\n    return img\n\n    \ndef load_image(fname, up_size=None):\n    img = np.array(Image.open(fname))\n    img = np.interp(img, [np.min(img), np.max(img)], [0,255])\n    return pad_img(img, up_size)\n\ndef get_25D_image(row, up_size=None):\n    if up_size:\n        imgs = np.zeros((*up_size, len(row['fnames'])))\n    else:\n        imgs = np.zeros((row['slice_h'], row['slice_w'], len(row['fnames'])))\n        \n    for i, fname in enumerate(row['fnames']):\n        img = load_image(fname, up_size)\n        imgs[..., i] += img\n    return imgs.astype(np.uint8)\n                   \n\ndef get_mask(row, up_size=None):\n    if up_size:\n        mask = np.zeros((*up_size, 3))\n    else:\n        mask = np.zeros((row['slice_h'], row['slice_w'], 3))\n        \n    if isinstance(row['large_bowel'], str):\n        mask[..., 0] += pad_img(rle_decode(row['large_bowel'], shape=(row['slice_h'], row['slice_w']), color=255), up_size)\n    if isinstance(row['small_bowel'], str):\n        mask[..., 1] += pad_img(rle_decode(row['small_bowel'], shape=(row['slice_h'], row['slice_w']), color=255), up_size)\n    if isinstance(row['stomach'], str):\n        mask[..., 2] += pad_img(rle_decode(row['stomach'], shape=(row['slice_h'], row['slice_w']), color=255), up_size)\n        \n    return mask.astype(np.uint8)\n","metadata":{"execution":{"iopub.status.busy":"2022-06-23T06:57:13.008245Z","iopub.execute_input":"2022-06-23T06:57:13.008942Z","iopub.status.idle":"2022-06-23T06:57:13.056188Z","shell.execute_reply.started":"2022-06-23T06:57:13.008865Z","shell.execute_reply":"2022-06-23T06:57:13.055055Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Now what we only need to add the `up_size` argument with our preferred size, and let's see how that works out.\n\nI'll use `partial` which sets an argument to be passed as default when the function is called.","metadata":{}},{"cell_type":"code","source":"up_size = (320, 384)\n\ntfms = [[partial(get_25D_image, up_size=up_size), PILImage.create],\n        [partial(get_mask, up_size=up_size), PILMask.create]]\n    \ndsets = Datasets(train_df, tfms)\n\ndls = dsets.dataloaders(bs=16, after_item=[ToTensor], after_batch=[IntToFloatTensor])\n\n# Now let's see if our visualization works\ndls.show_batch(nrows=4, ncols=4, max_n=16, figsize=(15, 15))","metadata":{"execution":{"iopub.status.busy":"2022-06-23T06:06:27.89995Z","iopub.execute_input":"2022-06-23T06:06:27.900377Z","iopub.status.idle":"2022-06-23T06:06:30.083335Z","shell.execute_reply.started":"2022-06-23T06:06:27.900341Z","shell.execute_reply":"2022-06-23T06:06:30.082431Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### How about we use random resized crops (center crops for validations) with fastai augmentations?","metadata":{}},{"cell_type":"code","source":"up_size = (320, 384)\n\ntfms = [[partial(get_25D_image, up_size=up_size), PILImage.create],\n        [partial(get_mask, up_size=up_size), PILMask.create]]\n\nsplits = RandomSplitter()(train_df)\ndsets = Datasets(train_df, tfms, splits=splits)\n\ndls = dsets.dataloaders(bs=16, after_item=[RandomResizedCrop(224), ToTensor], after_batch=[IntToFloatTensor, *aug_transforms(mult=0.8)])\n\n# Now let's see if our visualization works\ndls.show_batch(nrows=4, ncols=4, max_n=16, figsize=(15, 15))","metadata":{"execution":{"iopub.status.busy":"2022-06-23T06:06:30.084647Z","iopub.execute_input":"2022-06-23T06:06:30.085158Z","iopub.status.idle":"2022-06-23T06:06:35.747172Z","shell.execute_reply.started":"2022-06-23T06:06:30.085125Z","shell.execute_reply":"2022-06-23T06:06:35.746413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check validation set\ndls.valid.show_batch(nrows=4, ncols=4, max_n=16, figsize=(15, 15))","metadata":{"execution":{"iopub.status.busy":"2022-06-23T06:06:35.748493Z","iopub.execute_input":"2022-06-23T06:06:35.749101Z","iopub.status.idle":"2022-06-23T06:06:37.927327Z","shell.execute_reply.started":"2022-06-23T06:06:35.749056Z","shell.execute_reply":"2022-06-23T06:06:37.926626Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='data-six'></a>\n## 2.5D Data with albumentations\n\nThe key to using Albumentations in fastai is to wrap the albumentation pipeline you want in a fastai `Transform`. I won't get into detail with this, but I'll provide a reciepe that you can customize however you like.\n\nFirst let's make two functions, one that gets the training augmentations, and another that get the validation augmentation.","metadata":{}},{"cell_type":"code","source":"import albumentations as A\n\ndef get_train_aug(img_size, crop=0.9, p=0.4):\n    crop_size = round(img_size[0]*crop)\n    return A.Compose([\n            A.RandomCrop(height=crop_size, width=crop_size, always_apply=True),\n            A.HorizontalFlip(p=p),\n            A.OneOf([\n            A.GridDistortion(num_steps=5, distort_limit=0.05, p=1.0),\n            A.ElasticTransform(alpha=1, sigma=50, alpha_affine=50, p=1.0)\n            ], p=p),\n            A.CoarseDropout(\n                max_holes=8, min_holes=8,\n                max_height=crop_size//10, max_width=crop_size//10,\n                min_height=4, min_width=4, mask_fill_value=0, p=0.2*p),\n            A.ShiftScaleRotate(\n                shift_limit=0.0625, scale_limit=0.2, rotate_limit=25,\n                interpolation=cv2.INTER_AREA, p=p),\n            A.HorizontalFlip(p=0.5*p),\n            A.OneOf([\n                A.MotionBlur(p=0.2*p),\n                A.MedianBlur(blur_limit=3, p=0.1*p),\n                A.Blur(blur_limit=3, p=0.1*p),\n            ], p=0.2*p),\n            A.GaussNoise(var_limit=0.001, p=0.2*p),\n            A.OneOf([\n                A.OpticalDistortion(p=0.3*p),\n                A.GridDistortion(p=0.1*p),\n                A.PiecewiseAffine(p=0.3*p),\n            ], p=0.2*p),\n            A.OneOf([\n                A.Sharpen(p=0.2*p),\n                A.Emboss(p=0.2*p),\n                A.RandomBrightnessContrast(p=0.2*p),\n            ]),\n        ])\n\n\ndef get_test_aug(img_size, crop=0.9):\n    crop_size = round(crop*img_size[0])\n    return  A.Compose([\n        A.CenterCrop(height=crop_size, width=crop_size),\n    ])","metadata":{"execution":{"iopub.status.busy":"2022-06-23T06:06:37.9286Z","iopub.execute_input":"2022-06-23T06:06:37.929084Z","iopub.status.idle":"2022-06-23T06:06:38.961603Z","shell.execute_reply.started":"2022-06-23T06:06:37.929049Z","shell.execute_reply":"2022-06-23T06:06:38.960823Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next we'll make `Transform` object that wraps our augmentation and makes it usable within the fastai low-level api pipeline.","metadata":{}},{"cell_type":"code","source":"import cv2\n\nclass AlbumentationsTransform(ItemTransform, RandTransform):\n    split_idx, order = None, 2\n    def __init__(self, train_aug, valid_aug): store_attr()\n    \n    def before_call(self, b, split_idx): self.idx = split_idx\n    \n    def encodes(self, x):\n        if len(x) > 1:\n            img, mask = x\n            if self.idx == 0:\n                aug = self.train_aug(image=np.array(img), mask=np.array(mask))    \n            else:\n                aug = self.valid_aug(image=np.array(img), mask=np.array(mask))\n            return PILImage.create(aug[\"image\"]), PILMask.create(aug[\"mask\"])\n        else:\n            img = x[0]\n            aug = self.valid_aug(image=np.array(img))\n            return PILImage.create(aug[\"image\"])","metadata":{"execution":{"iopub.status.busy":"2022-06-23T06:06:38.962866Z","iopub.execute_input":"2022-06-23T06:06:38.963289Z","iopub.status.idle":"2022-06-23T06:06:38.972734Z","shell.execute_reply.started":"2022-06-23T06:06:38.963252Z","shell.execute_reply":"2022-06-23T06:06:38.971842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"And now let's test it.","metadata":{}},{"cell_type":"code","source":"# Let's take a sample of the training dataframe since we aren't going to use it right now \n# in anything more than demonstration\n\ndev_df = train_df.sample(frac=0.2)\n\n\nup_size = (320, 384)\n\ntfms = [[partial(get_25D_image, up_size=up_size), PILImage.create],\n        [partial(get_mask, up_size=up_size), PILMask.create]]\n\nsplits = RandomSplitter()(dev_df)\ndsets = Datasets(train_df, tfms, splits=splits)\n\nalbu_aug = AlbumentationsTransform(get_train_aug(up_size), get_test_aug(up_size))\n\ndls = dsets.dataloaders(bs=16, after_item=[albu_aug, ToTensor], after_batch=[IntToFloatTensor(div_mask=255), \n                                                                             Normalize.from_stats(*imagenet_stats)])\n\n# Now let's see if our visualization works\ndls.show_batch(nrows=4, ncols=4, max_n=16, figsize=(15, 15))","metadata":{"execution":{"iopub.status.busy":"2022-06-23T06:06:38.974017Z","iopub.execute_input":"2022-06-23T06:06:38.974445Z","iopub.status.idle":"2022-06-23T06:06:41.589455Z","shell.execute_reply.started":"2022-06-23T06:06:38.974408Z","shell.execute_reply":"2022-06-23T06:06:41.58873Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='section-two'></a>\n# Training","metadata":{}},{"cell_type":"markdown","source":"<a id='train-one'></a>\n## Metrics and Loss Functions\n\nTo save ourselves some hassle, I'll here the metrics and loss functions.","metadata":{}},{"cell_type":"markdown","source":"### Metrics","metadata":{}},{"cell_type":"code","source":"from scipy.spatial.distance import directed_hausdorff\n\n\ndef dice_coeff_adj(inp, targ):\n    inp = np.where(sigmoid(inp).cpu().detach().numpy() > 0.5, 1, 0)\n    targ = targ.cpu().detach().numpy()\n    eps = 1e-5\n    dice_scores = []\n    for i in range(targ.shape[0]):\n        dice_i = []\n        for j in range(targ.shape[1]):\n            if inp[i, j].sum() == targ[i, j].sum() == 0:\n                continue\n            I = (targ[i, j] * inp[i, j]).sum()\n            U =  targ[i, j].sum() + inp[i, j].sum()\n            dice_i.append((2.*I)/(U+eps))\n        if dice_i:\n            dice_scores.append(np.mean(dice_i))\n    \n    if dice_scores:\n        return np.mean(dice_scores)\n    else:\n        return 0\n    \n    \ndef hd_dist_per_slice(inp, targ, seed):    \n    inp = np.argwhere(inp) / np.array(inp.shape)\n    targ = np.argwhere(targ) / np.array(targ.shape)\n    haussdorf_dist = 1 - directed_hausdorff(inp, targ, seed)[0]\n    return haussdorf_dist if haussdorf_dist > 0 else 0\n\ndef hd_dist_adj(inp, targ, seed=42):\n    inp = np.where(sigmoid(inp).cpu().detach().numpy() > 0.5, 1, 0)\n    targ = targ.cpu().detach().numpy()\n    hd_scores = []\n    for i in range(targ.shape[0]):\n        hd_i = []\n        for j in range(targ.shape[1]):\n            if inp[i, j].sum() == targ[i, j].sum() == 0:\n                continue\n            hd_i.append(hd_dist_per_slice(inp[i, j], targ[i, j], seed))\n        if hd_i:\n            hd_scores.append(np.mean(hd_i))\n    if hd_scores:\n        return np.mean(hd_scores)\n    else:\n        return 0\n\ndef custom_metric_adj(inp, targ, seed=42):\n    hd_score_per_batch = hd_dist_adj(inp, targ, seed)\n    dice_score_per_batch = dice_coeff_adj(inp, targ)\n    \n    return 0.4*dice_score_per_batch + 0.6*hd_score_per_batch","metadata":{"execution":{"iopub.status.busy":"2022-06-23T06:06:41.590746Z","iopub.execute_input":"2022-06-23T06:06:41.591219Z","iopub.status.idle":"2022-06-23T06:06:41.612477Z","shell.execute_reply.started":"2022-06-23T06:06:41.591184Z","shell.execute_reply":"2022-06-23T06:06:41.611581Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Loss Functions","metadata":{}},{"cell_type":"code","source":"# Loss functions\nclass DiceBCEModule(Module):\n    def __init__(self, eps:float=1e-5, from_logits=True):\n        store_attr()\n        \n    def forward(self, inp:Tensor, targ:Tensor) -> Tensor:\n        inp = inp.view(-1)\n        targ = targ.view(-1)\n        \n        if self.from_logits: \n            bce_loss = nn.BCEWithLogitsLoss()(inp, targ)\n            inp = torch.sigmoid(inp)\n            \n            \n        intersection = (inp * targ).sum()                            \n        dice = (2.*intersection + self.eps)/(inp.sum() + targ.sum() + self.eps)  \n        \n        return 0.5*(1 - dice) + 0.5*bce_loss\n\n\nclass DiceBCELoss(BaseLoss):\n    def __init__(self, *args, eps:float=1e-5, from_logits=True, thresh=0.5, **kwargs):\n        super().__init__(DiceBCEModule, *args, eps=eps, from_logits=from_logits, flatten=False, is_2d=True, floatify=True, **kwargs)\n        self.thresh = thresh\n    \n    def decodes(self, x:Tensor) -> Tensor:\n        \"Converts model output to target format\"\n        return (x>self.thresh).long()\n\n    def activation(self, x:Tensor) -> Tensor:\n        \"`nn.BCEWithLogitsLoss`'s fused activation function applied to model output\"\n        return torch.sigmoid(x)\n\n# Source: https://www.kaggle.com/code/thedrcat/focal-multilabel-loss-in-pytorch-explained/notebook\ndef focal_binary_cross_entropy(logits, targets, gamma=2, n=3):\n    p = torch.sigmoid(logits)\n    p = torch.where(targets >= 0.5, p, 1-p)\n    logp = - torch.log(torch.clamp(p, 1e-4, 1-1e-4))\n    loss = logp*((1-p)**gamma)\n    loss = n*loss.mean()\n    return loss\n\nclass DiceFocalModule(Module):\n    def __init__(self, eps:float=1e-5, from_logits=True, ws=[0.5, 0.5], gamma=2, n=3):\n        store_attr()\n        \n    def forward(self, inp:Tensor, targ:Tensor) -> Tensor:\n        inp = inp.view(-1)\n        targ = targ.view(-1)\n        \n        if self.from_logits: \n            focal_loss = focal_binary_cross_entropy(inp, targ, self.gamma, self.n)\n            inp = torch.sigmoid(inp)\n            \n            \n        intersection = (inp * targ).sum()                            \n        dice = (2.*intersection + self.eps)/(inp.sum() + targ.sum() + self.eps)  \n        \n        return self.ws[0]*(1 - dice) + self.ws[1]*focal_loss\n    \nclass DiceFocalLoss(BaseLoss):\n    def __init__(self, *args, eps:float=1e-5, from_logits=True, ws=[0.5, 0.5], gamma=2, n=3, thresh=0.5, **kwargs):\n        super().__init__(DiceFocalModule, *args, eps=eps, from_logits=from_logits, ws=ws, gamma=gamma, n=n, flatten=False, is_2d=True, floatify=True, **kwargs)\n        self.thresh = thresh\n    \n    def decodes(self, x:Tensor) -> Tensor:\n        \"Converts model output to target format\"\n        return (x>self.thresh).long()\n\n    def activation(self, x:Tensor) -> Tensor:\n        \"`nn.BCEWithLogitsLoss`'s fused activation function applied to model output\"\n        return torch.sigmoid(x)\n\nclass FocalTverskyLossModule(Module):\n    def __init__(self, eps:float=1e-5, from_logits=True, alpha=0.3, beta=0.7, gamma=3/4):\n        store_attr()\n        \n    def forward(self, inp:Tensor, targ:Tensor) -> Tensor:\n        inp = inp.view(-1)\n        targ = targ.view(-1)\n        \n        if self.from_logits: \n            inp = torch.sigmoid(inp)\n            \n        inp_0, inp_1 = inp, 1 - inp\n        targ_0, targ_1 = targ, 1 - targ\n            \n        num = (inp_0 * targ_0).sum() \n        denom = num + (self.alpha * (inp_0 * targ_1).sum()) + (self.beta * (inp_1 * targ_0).sum()) + self.eps\n        loss = 1 - (num / denom)\n        return loss**self.gamma \n    \nclass FocalTverskyLoss(BaseLoss):\n    def __init__(self, *args, eps:float=1e-5, from_logits=True, alpha=0.3, beta=0.7, gamma=3/4, thresh=0.5, **kwargs):\n        super().__init__(FocalTverskyLossModule, *args, eps=eps, from_logits=from_logits, alpha=alpha, beta=beta, gamma=gamma, flatten=False, is_2d=True, floatify=True, **kwargs)\n        self.thresh = thresh\n    \n    def decodes(self, x:Tensor) -> Tensor:\n        \"Converts model output to target format\"\n        return (x>self.thresh).long()\n\n    def activation(self, x:Tensor) -> Tensor:\n        \"`nn.BCEWithLogitsLoss`'s fused activation function applied to model output\"\n        return torch.sigmoid(x)\n\ndef focal_binary_cross_entropy(logits, targets, gamma=2, n=3):\n    p = torch.sigmoid(logits)\n    p = torch.where(targets >= 0.5, p, 1-p)\n    logp = - torch.log(torch.clamp(p, 1e-4, 1-1e-4))\n    loss = logp*((1-p)**gamma)\n    loss = n*loss.mean()\n    return loss\n\nclass ComboModule(Module):\n    def __init__(self, eps:float=1e-5, from_logits=True, ws=[2, 3, 1], gamma=2, n=3):\n        store_attr()\n        \n    def forward(self, inp:Tensor, targ:Tensor) -> Tensor:\n        inp = inp.view(-1)\n        targ = targ.view(-1)\n        \n        if self.from_logits: \n            focal_loss = focal_binary_cross_entropy(inp, targ, self.gamma, self.n)\n            bce_loss = nn.BCEWithLogitsLoss()(inp, targ)\n            inp = torch.sigmoid(inp)\n                \n        intersection = (inp * targ).sum()                            \n        dice = (2.*intersection + self.eps)/(inp.sum() + targ.sum() + self.eps)  \n        \n        return self.ws[0]*(1 - dice) + self.ws[1]*focal_loss + self.ws[2]*bce_loss\n    \nclass ComboLoss(BaseLoss):\n    def __init__(self, *args, eps:float=1e-5, from_logits=True, ws=[2, 3, 1], gamma=2, n=3, thresh=0.5, **kwargs):\n        super().__init__(ComboModule, *args, eps=eps, from_logits=from_logits, ws=ws, gamma=gamma, n=n, flatten=False, is_2d=True, floatify=True, **kwargs)\n        self.thresh = thresh\n    \n    def decodes(self, x:Tensor) -> Tensor:\n        \"Converts model output to target format\"\n        return (x>self.thresh).long()\n\n    def activation(self, x:Tensor) -> Tensor:\n        \"`nn.BCEWithLogitsLoss`'s fused activation function applied to model output\"\n        return torch.sigmoid(x)","metadata":{"execution":{"iopub.status.busy":"2022-06-23T06:06:41.613967Z","iopub.execute_input":"2022-06-23T06:06:41.61484Z","iopub.status.idle":"2022-06-23T06:06:41.654371Z","shell.execute_reply.started":"2022-06-23T06:06:41.614803Z","shell.execute_reply":"2022-06-23T06:06:41.653591Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='train-two'></a>\n## Using SMP models with Fastai\n\nNow that we have a decent data preparation pipeline, let's make use of smp's amazing library of models to build us a training pipeline.\n\nFirst let's make a function that returns a Unet with our preferred encoder.","metadata":{}},{"cell_type":"code","source":"!pip install segmentation_models_pytorch","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-06-23T06:06:41.657553Z","iopub.execute_input":"2022-06-23T06:06:41.657857Z","iopub.status.idle":"2022-06-23T06:06:58.396583Z","shell.execute_reply.started":"2022-06-23T06:06:41.657821Z","shell.execute_reply":"2022-06-23T06:06:58.395594Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import segmentation_models_pytorch as smp\n\ndef build_model(encoder_name, in_c=3, classes=3, weights=\"imagenet\"):\n    model = smp.Unet(\n        encoder_name=encoder_name,      \n        encoder_weights=weights,     \n        in_channels=in_c,                \n        classes=classes,        \n        activation=None\n    )\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-06-23T06:06:58.398169Z","iopub.execute_input":"2022-06-23T06:06:58.398577Z","iopub.status.idle":"2022-06-23T06:07:05.224873Z","shell.execute_reply.started":"2022-06-23T06:06:58.39852Z","shell.execute_reply":"2022-06-23T06:07:05.223994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now I'll provide a function that splits any of model parameters from smp into encoder and decoder, so that we can freeze and unfreeze encoder layers.","metadata":{}},{"cell_type":"code","source":"def smp_splitter(model):\n    model_layers = list(model.children())\n    encoder_params = params(model_layers[0])\n    decoder_params = params(model_layers[1]) + params(model_layers[2])\n    return L(encoder_params, decoder_params)","metadata":{"execution":{"iopub.status.busy":"2022-06-23T06:07:05.226423Z","iopub.execute_input":"2022-06-23T06:07:05.22682Z","iopub.status.idle":"2022-06-23T06:07:05.232386Z","shell.execute_reply.started":"2022-06-23T06:07:05.226783Z","shell.execute_reply":"2022-06-23T06:07:05.23167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"And now let's use the latest dataloader to create a training pipeline using `Learner`","metadata":{}},{"cell_type":"code","source":"model = build_model('efficientnet-b0')\nmetrics = [dice_coeff_adj, hd_dist_adj, custom_metric_adj]\nloss_func = ComboLoss()\nsplitter = smp_splitter","metadata":{"execution":{"iopub.status.busy":"2022-06-23T06:07:05.233731Z","iopub.execute_input":"2022-06-23T06:07:05.23423Z","iopub.status.idle":"2022-06-23T06:07:09.040321Z","shell.execute_reply.started":"2022-06-23T06:07:05.234192Z","shell.execute_reply":"2022-06-23T06:07:09.039565Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learn = Learner(dls, model, metrics=metrics, loss_func=loss_func, splitter=splitter).to_fp16()\nlearn.freeze()","metadata":{"execution":{"iopub.status.busy":"2022-06-23T06:07:09.041726Z","iopub.execute_input":"2022-06-23T06:07:09.042093Z","iopub.status.idle":"2022-06-23T06:07:09.075944Z","shell.execute_reply.started":"2022-06-23T06:07:09.042058Z","shell.execute_reply":"2022-06-23T06:07:09.075198Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Let's fine tune this model for 2 epochs, one frozen and another unfrozen. This isn't suppossed to produce world class results, but it's just a demonstration.","metadata":{}},{"cell_type":"code","source":"learn.fine_tune(1, 1e-2)","metadata":{"execution":{"iopub.status.busy":"2022-06-23T06:07:09.077082Z","iopub.execute_input":"2022-06-23T06:07:09.07744Z","iopub.status.idle":"2022-06-23T06:19:27.850293Z","shell.execute_reply.started":"2022-06-23T06:07:09.077404Z","shell.execute_reply":"2022-06-23T06:19:27.84937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learn.export('test_model.pkl')","metadata":{"execution":{"iopub.status.busy":"2022-06-23T06:19:27.852004Z","iopub.execute_input":"2022-06-23T06:19:27.852384Z","iopub.status.idle":"2022-06-23T06:19:28.021812Z","shell.execute_reply.started":"2022-06-23T06:19:27.852344Z","shell.execute_reply":"2022-06-23T06:19:28.02099Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"You can customize this basic recipe to your liking including:\n1. Image size and augmentations\n3. Loss function\n4. Model architecture and encoder\n5. Training scheme\n\nThe possibilities are endless. ","metadata":{}},{"cell_type":"markdown","source":"<a id=\"train-three\"></a>\n## Using Fastai's DynamicUnet with timm encoder?\n\nIn theory, you can pass any encoder to `DyanmicUnet` and it should work, but after some trials, I found out that unfortunatley, it doesn't work properly, so I made some adjustments to the original code to make it work.\n\nWhat's cool about fastai's `DynamicUnet` is that it has some configurations that aren't available in other UNet implementations. For example:\n1. You can add self attentions to the Unet Decoder blocks\n2. You can customize the activations function in the Unet Decoder blocks\n3. Unet Decoder blocks use PixelShuffle upsampling instead of regular upsampling","metadata":{}},{"cell_type":"code","source":"import timm\n\ndef timm_model_sizes(encoder, img_size):\n    sizes = []\n    for layer in encoder.feature_info:\n        sizes.append(torch.Size([1, layer['num_chs'], img_size[0]//layer['reduction'], img_size[1]//layer['reduction']]))\n    return sizes\n\n\ndef get_timm_output_layers(encoder):\n    outputs = []\n    for layer in encoder.feature_info:\n        # Converts 'blocks.0.0' to ['blocks', '0', '0']\n        attrs = layer['module'].split('.')\n        output_layer = getattr(encoder, attrs[0])[int(attrs[1])][int(attrs[2])]\n        outputs.append(output_layer)\n    return outputs\n\n\nclass DynamicTimmUnet(SequentialEx):\n    \"Create a U-Net from a given architecture in timm.\"\n    def __init__(self, encoder, n_out, img_size, blur=False, blur_final=True, self_attention=False,\n                 y_range=None, last_cross=True, bottle=False, act_cls=defaults.activation,\n                 init=nn.init.kaiming_normal_, norm_type=None, **kwargs):\n        imsize = img_size\n        sizes = timm_model_sizes(encoder, img_size)\n        sz_chg_idxs = list(reversed(range(len(sizes))))\n        outputs = list(reversed(get_timm_output_layers(encoder)))\n        self.sfs = hook_outputs(outputs, detach=False)\n        \n        # cut encoder\n        encoder = nn.Sequential(*list(encoder.children()))[:-5]\n        \n        x = dummy_eval(encoder, imsize).detach()\n\n        ni = sizes[-1][1]\n        middle_conv = nn.Sequential(ConvLayer(ni, ni*2, act_cls=act_cls, norm_type=norm_type, **kwargs),\n                                    ConvLayer(ni*2, ni, act_cls=act_cls, norm_type=norm_type, **kwargs)).eval()\n        x = middle_conv(x)\n        layers = [encoder, BatchNorm(ni), nn.ReLU(), middle_conv]\n\n        for i,idx in enumerate(sz_chg_idxs):\n            not_final = i!=len(sz_chg_idxs)-1\n            up_in_c, x_in_c = int(x.shape[1]), int(sizes[idx][1])\n            do_blur = blur and (not_final or blur_final)\n            sa = self_attention and (i==len(sz_chg_idxs)-3)\n            unet_block = UnetBlock(up_in_c, x_in_c, self.sfs[i], final_div=not_final, blur=do_blur, self_attention=sa,\n                                   act_cls=act_cls, init=init, norm_type=norm_type, **kwargs).eval()\n            layers.append(unet_block)\n            x = unet_block(x)\n\n        ni = x.shape[1]\n        if imsize != sizes[0][-2:]: layers.append(PixelShuffle_ICNR(ni, act_cls=act_cls, norm_type=norm_type))\n        layers.append(ResizeToOrig())\n        if last_cross:\n            layers.append(MergeLayer(dense=True))\n            ni += in_channels(encoder)\n            layers.append(ResBlock(1, ni, ni//2 if bottle else ni, act_cls=act_cls, norm_type=norm_type, **kwargs))\n        layers += [ConvLayer(ni, n_out, ks=1, act_cls=None, norm_type=norm_type, **kwargs)]\n        apply_init(nn.Sequential(layers[3], layers[-2]), init)\n        #apply_init(nn.Sequential(layers[2]), init)\n        if y_range is not None: layers.append(SigmoidRange(*y_range))\n        layers.append(ToTensorBase())\n        super().__init__(*layers)\n\n    def __del__(self):\n        if hasattr(self, \"sfs\"): self.sfs.remove()\n            \n            \ndef dynamic_unet_splitter(model):\n    return L(model[0], model[1:]).map(params)\n","metadata":{"execution":{"iopub.status.busy":"2022-06-23T06:19:28.023283Z","iopub.execute_input":"2022-06-23T06:19:28.023662Z","iopub.status.idle":"2022-06-23T06:19:28.044258Z","shell.execute_reply.started":"2022-06-23T06:19:28.023626Z","shell.execute_reply":"2022-06-23T06:19:28.043352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Now let's do this.","metadata":{}},{"cell_type":"code","source":"dls = dsets.dataloaders(bs=16, after_item=[albu_aug, ToTensor],\n                        after_batch=[IntToFloatTensor(div_mask=255), Normalize.from_stats(*imagenet_stats)])","metadata":{"execution":{"iopub.status.busy":"2022-06-23T06:19:28.045605Z","iopub.execute_input":"2022-06-23T06:19:28.04599Z","iopub.status.idle":"2022-06-23T06:19:28.103868Z","shell.execute_reply.started":"2022-06-23T06:19:28.045953Z","shell.execute_reply":"2022-06-23T06:19:28.102159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The training scheme I'll use right now is inspired by this [lesson](https://walkwithfastai.com/Segmentation) for walk with fastai","metadata":{}},{"cell_type":"code","source":"img_size = [round(0.9*320) for _ in range(2)]\n\nencoder = timm.create_model('efficientnet_b0', pretrained=True)\n\n# Let's use self attentions and Mish activation function \nmodel = DynamicTimmUnet(encoder, 3, img_size, self_attention=True, act_cls=Mish)\n\n# We'll also use ranger optimizer with is RAdam with Lookahead\nlearn = Learner(dls, model, metrics=metrics, loss_func=loss_func, splitter=dynamic_unet_splitter, opt_func=ranger).to_fp16()\nlearn.freeze()","metadata":{"execution":{"iopub.status.busy":"2022-06-23T06:19:28.105061Z","iopub.execute_input":"2022-06-23T06:19:28.105446Z","iopub.status.idle":"2022-06-23T06:19:31.962883Z","shell.execute_reply.started":"2022-06-23T06:19:28.10541Z","shell.execute_reply":"2022-06-23T06:19:31.961996Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learn.lr_find()","metadata":{"execution":{"iopub.status.busy":"2022-06-23T06:19:31.964139Z","iopub.execute_input":"2022-06-23T06:19:31.964522Z","iopub.status.idle":"2022-06-23T06:21:00.640439Z","shell.execute_reply.started":"2022-06-23T06:19:31.964486Z","shell.execute_reply":"2022-06-23T06:21:00.639532Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's also use flat cosine annealing lr shceduler\nlr = 1e-3\nlearn.fit_flat_cos(1, lr)","metadata":{"execution":{"iopub.status.busy":"2022-06-23T06:21:00.642138Z","iopub.execute_input":"2022-06-23T06:21:00.642693Z","iopub.status.idle":"2022-06-23T06:27:06.639866Z","shell.execute_reply.started":"2022-06-23T06:21:00.642648Z","shell.execute_reply":"2022-06-23T06:27:06.638933Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's unfreeze the encoder layers and train with discriminative learning rates.","metadata":{}},{"cell_type":"code","source":"learn.unfreeze()\nlearn.fit_flat_cos(2, slice(lr/400, lr/4))","metadata":{"execution":{"iopub.status.busy":"2022-06-23T06:27:06.641666Z","iopub.execute_input":"2022-06-23T06:27:06.642072Z","iopub.status.idle":"2022-06-23T06:39:38.381829Z","shell.execute_reply.started":"2022-06-23T06:27:06.642029Z","shell.execute_reply":"2022-06-23T06:39:38.380939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='infer'></a>\n# Inference\n\n### Okay let's say we have trained a model and we want to submit, what should we do?\n1. We should make a test dataframe that we can use similar to the train datafrrame\n1. We should predict using the full size of padded image or we can crop them if we want \n2. Then we should unpad the predicted masks to their specific sizes\n3. And we should convert the masks to rle format \n4. Finally we make our submission file\n\n\n*Note that in this notebook, the internet is enabled, while in real inference notebook the internet should be disabled, which could be a problem while importing `smp` and `timm`, but there is a work around where you can load these packages as datasets, then install them directly without internet.*","metadata":{}},{"cell_type":"markdown","source":"### Make a test dataframe that we can use similar to the train datafrrame","metadata":{}},{"cell_type":"code","source":"def create_df(df, fnames):\n    df = df.copy()\n    df = df.pivot(index='id', columns='class', values='segmentation').reset_index()\n    \n    df['partial_fname'] = df.id\n    fname_df = pd.DataFrame({'partial_fname': [f'{fname.parts[-3]}_slice_{fname.parts[-1][6:10]}' for fname in fnames],\n                             'fname': fnames})\n\n    df = df.merge(fname_df, on='partial_fname').drop('partial_fname', axis=1)\n\n    df['case_id'] = df.id.apply(lambda x: x.split('_')[0])\n    df['day_num'] = df.id.apply(lambda x: x.split('_')[1])\n\n    df['slice_w'] = df[\"fname\"].apply(lambda x: int(str(x)[:-4].rsplit(\"_\",4)[1]))\n    df['slice_h'] = df[\"fname\"].apply(lambda x: int(str(x)[:-4].rsplit(\"_\",4)[2]))\n    \n    channels = 3\n    stride = 2\n    for j, i in enumerate(range(-1*(channels-channels//2-1), channels//2+1)):\n        method = 'ffill'\n        if i <= 0: method = 'bfill'\n        df[f'fname_{j:02}'] = df.groupby(['case_id', 'day_num'])['fname'].shift(stride*-i).fillna(method=method)\n\n    df['fnames'] = df[[f'fname_{j:02d}' for j in range(channels)]].values.tolist()\n    \n    return df","metadata":{"execution":{"iopub.status.busy":"2022-06-23T06:48:54.896584Z","iopub.execute_input":"2022-06-23T06:48:54.897111Z","iopub.status.idle":"2022-06-23T06:48:54.918601Z","shell.execute_reply.started":"2022-06-23T06:48:54.89706Z","shell.execute_reply":"2022-06-23T06:48:54.917738Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_path = '../input/uw-madison-gi-tract-image-segmentation/'\n\ntrain_path = Path(data_path+'train')\ntest_path = Path(data_path+'test')\n\ntrain_fnames = get_image_files(train_path)\ntest_fnames = get_image_files(test_path)\n\nsample_submission = pd.read_csv(data_path+'sample_submission.csv')\n\nif sample_submission.shape[0] > 0: \n    test = sample_submission.copy()\nelse:\n    test_fnames = train_fnames\n    test_path = train_path\n    test = train.copy()\n    test = test.sample(frac=1.0, random_state=42)\n\ntest_df = create_df(test, test_fnames)","metadata":{"execution":{"iopub.status.busy":"2022-06-23T06:57:19.235466Z","iopub.execute_input":"2022-06-23T06:57:19.235949Z","iopub.status.idle":"2022-06-23T06:57:21.181673Z","shell.execute_reply.started":"2022-06-23T06:57:19.235907Z","shell.execute_reply":"2022-06-23T06:57:21.180805Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Prediction and conversion to rle masks","metadata":{}},{"cell_type":"markdown","source":"### Load learner","metadata":{}},{"cell_type":"code","source":"learn = load_learner('test_model.pkl')","metadata":{"execution":{"iopub.status.busy":"2022-06-23T06:57:21.18658Z","iopub.execute_input":"2022-06-23T06:57:21.188778Z","iopub.status.idle":"2022-06-23T06:57:42.366638Z","shell.execute_reply.started":"2022-06-23T06:57:21.188738Z","shell.execute_reply":"2022-06-23T06:57:42.365618Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Create test dataloader","metadata":{}},{"cell_type":"code","source":"# I'll sample the test set here to make a quick demonstartion\ntest_df = test_df.sample(frac=0.1)","metadata":{"execution":{"iopub.status.busy":"2022-06-23T06:57:42.371996Z","iopub.execute_input":"2022-06-23T06:57:42.374208Z","iopub.status.idle":"2022-06-23T06:57:42.417453Z","shell.execute_reply.started":"2022-06-23T06:57:42.374169Z","shell.execute_reply":"2022-06-23T06:57:42.416581Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bs = learn.dls.bs\ntest_dl = learn.dls.test_dl(test_df, bs=bs, shuffle=False).to('cuda')","metadata":{"execution":{"iopub.status.busy":"2022-06-23T06:57:42.421994Z","iopub.execute_input":"2022-06-23T06:57:42.424146Z","iopub.status.idle":"2022-06-23T06:57:42.434528Z","shell.execute_reply.started":"2022-06-23T06:57:42.424107Z","shell.execute_reply":"2022-06-23T06:57:42.433703Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Predict and convert to rle masks","metadata":{}},{"cell_type":"code","source":"def mask2rle(mask):\n    \"\"\"\n    img: numpy array, 1 - mask, 0 - background\n    Returns run length as string formated\n    \"\"\"\n    mask = np.array(mask)\n    pixels = mask.flatten()\n    pad = np.array([0])\n    pixels = np.concatenate([pad, pixels, pad])\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n    runs[1::2] -= runs[::2]\n\n    return \" \".join(str(x) for x in runs)\n\ndef resize_img_to_org_size(img, org_size):\n    shape0 = np.array(img.shape[:2])\n    diff = org_size - shape0\n    if np.any(diff < 0):\n        img = pad_img_nc(img, (320, 384))\n        resized = unpad_img_nc(img, org_size)\n    else:\n        resized = pad_img_nc(img, org_size)\n    return resized\n\ndef get_rle_masks(preds, df):\n    rle_masks = []\n    for pred, width, height in zip(preds, df['slice_w'], df['slice_h']):\n        upsized_mask = resize_img_to_org_size(pred, (height, width))\n        for i in range(3):\n            rle_mask = mask2rle(upsized_mask[:, :, i])\n            rle_masks.append(rle_mask)\n    return rle_masks\n\ndef unpad_img_nc(img, org_size):\n    shape0 = np.array(org_size)\n    resize = np.array(img.shape[:2])\n    if np.any(shape0!=resize):\n        diff = resize - shape0\n        pad0 = diff[0]\n        pad1 = diff[1]\n        pady = [pad0//2, pad0//2 + pad0%2]\n        padx = [pad1//2, pad1//2 + pad1%2]\n        \n        if pady[0] != 0:\n            img = img[pady[0]:-pady[1], :, :]\n            \n        if padx[0] != 0:\n            img = img[:, padx[0]:-padx[1], :]\n            \n        img = img.reshape((*shape0, img.shape[-1]))\n    return img\n\ndef pad_img_nc(img, up_size=None):\n    if up_size is None:\n        return img\n    shape0 = np.array(img.shape[:2])\n    resize = np.array(up_size)\n    if np.any(shape0!=resize):\n        diff = resize - shape0\n        pad0 = diff[0]\n        pad1 = diff[1]\n        pady = [pad0//2, pad0//2 + pad0%2]\n        padx = [pad1//2, pad1//2 + pad1%2]\n        padz = [0, 0]\n        img = np.pad(img, [pady, padx, padz])\n        img = img.reshape((*resize, img.shape[-1]))\n    return img","metadata":{"execution":{"iopub.status.busy":"2022-06-23T06:59:33.974368Z","iopub.execute_input":"2022-06-23T06:59:33.974942Z","iopub.status.idle":"2022-06-23T06:59:34.004817Z","shell.execute_reply.started":"2022-06-23T06:59:33.974895Z","shell.execute_reply":"2022-06-23T06:59:34.003613Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\nimport gc\n\nlearn.model = learn.model.cuda()\nlearn.model.eval()\nmasks = []\n\nwith torch.no_grad():\n    for i, b in enumerate(tqdm(test_dl)):\n        b.to('cuda')\n        b_preds = (sigmoid(learn.model(b)) > 0.5).permute(0, 2, 3, 1).cpu().detach().numpy().astype(np.uint8)\n\n        masks.extend(get_rle_masks(b_preds, test_df.iloc[i*bs:i*bs+bs]))\n\n        # test_preds[i*bs:i*bs+bs] = b_preds\n        del b_preds\n        torch.cuda.empty_cache()\n        gc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-06-23T06:59:43.524281Z","iopub.execute_input":"2022-06-23T06:59:43.52484Z","iopub.status.idle":"2022-06-23T07:04:02.069374Z","shell.execute_reply.started":"2022-06-23T06:59:43.524714Z","shell.execute_reply":"2022-06-23T07:04:02.068499Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Make submission file","metadata":{}},{"cell_type":"code","source":"def get_case_id(fname):\n    return fname.parts[5] + '_' + fname.parts[7][:10]","metadata":{"execution":{"iopub.status.busy":"2022-06-23T07:06:09.876645Z","iopub.execute_input":"2022-06-23T07:06:09.877198Z","iopub.status.idle":"2022-06-23T07:06:09.882979Z","shell.execute_reply.started":"2022-06-23T07:06:09.877154Z","shell.execute_reply":"2022-06-23T07:06:09.881974Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from itertools import chain\n\nsubmission = pd.DataFrame({\n        'id': chain.from_iterable([[get_case_id(fname)]*3 for fname in test_df['fname']]),\n        'class': chain.from_iterable([['large_bowel', 'small_bowel', 'stomach'] for _ in test_df['fname']]),\n        'predicted': masks,\n    })\n    \n\n# Merge with sample submission to preserve order to slices during scoring and avoid 0 scores\nif sample_submission.shape[0] > 0:\n    del sample_submission['segmentation']\n    submission = sample_submission.merge(submission, on=['id', 'class'])\n\nsubmission.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-06-23T07:06:10.365866Z","iopub.execute_input":"2022-06-23T07:06:10.36641Z","iopub.status.idle":"2022-06-23T07:06:10.550318Z","shell.execute_reply.started":"2022-06-23T07:06:10.366368Z","shell.execute_reply":"2022-06-23T07:06:10.549472Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### And that's how you do inference.","metadata":{}},{"cell_type":"markdown","source":"#### I highly recommend that you fork this notebook and start tinkering around with the code provided, as you'll understand more about than I could ever try to tell you in words. Morever take a deep dive into [fastai documentation](docs.fast.ai) tutorials section.\n\n#### Also if you are unware with fastai, I recommend that you check out there course, and their book which in my opinion is the best way around to get into deep learning due to it's application driven approach.","metadata":{}},{"cell_type":"markdown","source":"#### I'm not a great writer and I don't go into much detail, but if anyone wanted to clarify on any piece of code I'd be happy to walk them through how it works.\n\n#### Next up\n   - Using more than 3 channels in 2.5D Data","metadata":{}},{"cell_type":"markdown","source":"#### This is a work in progress. Don't forget to upvote if you liked this notebook so far.","metadata":{}}]}