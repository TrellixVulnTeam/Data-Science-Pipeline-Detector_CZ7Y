{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Exploratory Data Analysis ðŸ”Ž","metadata":{}},{"cell_type":"code","source":"!pip uninstall -y torchtext\n!mkdir -p frozen_packages/\n!cp ../input/demo-flash-semantic-segmentation/frozen_packages/* frozen_packages/\n!cp ../input/tract-segm-eda-3d-interactive-viewer/frozen_packages/* frozen_packages/\n# !pip install -q --upgrade torch torchvision\n!pip install -q \"lightning-flash[image]\" \"torchmetrics<0.8\" --pre --no-index --find-links frozen_packages/\n!pip install -q -U timm segmentation-models-pytorch --no-index --find-links frozen_packages/\n# !pip install -q \"https://github.com/PyTorchLightning/lightning-flash/archive/refs/heads/segm/multi-label.zip\"\n!pip install -q 'kaggle-image-segmentation' --no-index --find-links frozen_packages/\n\n! pip list | grep -e torch -e lightning\n! nvidia-smi -L","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-05-23T06:37:46.574312Z","iopub.execute_input":"2022-05-23T06:37:46.574644Z","iopub.status.idle":"2022-05-23T06:39:26.42322Z","shell.execute_reply.started":"2022-05-23T06:37:46.574558Z","shell.execute_reply":"2022-05-23T06:39:26.42236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os, glob\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nDATASET_FOLDER = \"/kaggle/input/uw-madison-gi-tract-image-segmentation\"\ndf_train = pd.read_csv(os.path.join(DATASET_FOLDER, \"train.csv\"))\ndisplay(df_train.head())\n\ndf_pred = pd.read_csv(os.path.join(DATASET_FOLDER, \"sample_submission.csv\"))\nWITH_SUBMISSION = not df_pred.empty","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-23T06:39:26.426804Z","iopub.execute_input":"2022-05-23T06:39:26.427006Z","iopub.status.idle":"2022-05-23T06:39:26.898098Z","shell.execute_reply.started":"2022-05-23T06:39:26.426983Z","shell.execute_reply":"2022-05-23T06:39:26.897448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_imgs = glob.glob(os.path.join(DATASET_FOLDER, \"train\", \"case*\", \"case*_day*\", \"scans\", \"*.png\"))\nall_imgs = [p.replace(DATASET_FOLDER, \"\") for p in all_imgs]\n\nprint(f\"images: {len(all_imgs)}\")\nprint(f\"annotated: {len(df_train['id'].unique())}\")","metadata":{"execution":{"iopub.status.busy":"2022-05-23T06:39:26.899159Z","iopub.execute_input":"2022-05-23T06:39:26.899386Z","iopub.status.idle":"2022-05-23T06:39:30.037641Z","shell.execute_reply.started":"2022-05-23T06:39:26.899353Z","shell.execute_reply":"2022-05-23T06:39:30.036747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pprint import pprint\nfrom kaggle_imsegm.data_io import extract_tract_details\n\npprint(extract_tract_details(df_train['id'].iloc[0], DATASET_FOLDER))\n\ndf_train[['Case','Day','Slice', 'image', 'image_path', 'height', 'width']] = df_train['id'].apply(\n    lambda x: pd.Series(extract_tract_details(x, DATASET_FOLDER))\n)\ndisplay(df_train.head())","metadata":{"execution":{"iopub.status.busy":"2022-05-23T06:39:30.039748Z","iopub.execute_input":"2022-05-23T06:39:30.040601Z","iopub.status.idle":"2022-05-23T06:41:35.53259Z","shell.execute_reply.started":"2022-05-23T06:39:30.040559Z","shell.execute_reply":"2022-05-23T06:41:35.531889Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prepare custom dataset ðŸ’½","metadata":{}},{"cell_type":"code","source":"import os.path\nfrom typing import Callable, Tuple, Sequence\n\nimport torch\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nfrom torch.utils.data import Dataset\nfrom tqdm.auto import tqdm\n\nfrom kaggle_imsegm.dataset import TractDataset2D\n\nds = TractDataset2D(df_train, DATASET_FOLDER)\nprint(len(ds))","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-23T06:41:35.534113Z","iopub.execute_input":"2022-05-23T06:41:35.534571Z","iopub.status.idle":"2022-05-23T06:42:24.536781Z","shell.execute_reply.started":"2022-05-23T06:41:35.534533Z","shell.execute_reply":"2022-05-23T06:42:24.536132Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"spl = ds[255]\nimg, seg = spl[\"input\"], spl[\"target\"]\nprint(img.shape)\nfig, axarr = plt.subplots(ncols=4, figsize=(12, 3))\naxarr[0].imshow(np.rollaxis(img.numpy(), 0, 3), cmap=\"gray\")\nprint(np.argmax(seg, axis=0).shape)\nfor i in range(seg.shape[0]):\n    axarr[i + 1].imshow(seg[i, ...])","metadata":{"execution":{"iopub.status.busy":"2022-05-23T06:42:24.538348Z","iopub.execute_input":"2022-05-23T06:42:24.538581Z","iopub.status.idle":"2022-05-23T06:42:25.059262Z","shell.execute_reply.started":"2022-05-23T06:42:24.538549Z","shell.execute_reply":"2022-05-23T06:42:25.058548Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from typing import Any, Callable, Dict, Tuple, Type, Union\nfrom pytorch_lightning import LightningDataModule\nfrom torch.utils.data import DataLoader, Dataset\nimport albumentations as alb\n\nfrom kaggle_imsegm.transform import FlashAlbumentationsAdapter\nfrom kaggle_imsegm.dataset import TractData\n\nCOLOR_MEAN: float = 0.349977\nCOLOR_STD: float = 0.215829\nDEFAULT_TRANSFORM = FlashAlbumentationsAdapter(\n    [alb.Resize(224, 224), alb.Normalize(mean=COLOR_MEAN, std=COLOR_STD, max_pixel_value=255)]\n)\n    \ndm = TractData(df_train, DATASET_FOLDER, dataloader_kwargs=dict(batch_size=12, num_workers=3))\ndm.setup()\nprint(len(dm.train_dataloader()))\nprint(len(dm.val_dataloader()))","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-23T06:42:25.060341Z","iopub.execute_input":"2022-05-23T06:42:25.060906Z","iopub.status.idle":"2022-05-23T06:43:10.368665Z","shell.execute_reply.started":"2022-05-23T06:42:25.060862Z","shell.execute_reply":"2022-05-23T06:43:10.36794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from kaggle_imsegm.visual import show_tract_datamodule_samples_2d\n\n_= show_tract_datamodule_samples_2d(dm.val_dataloader(), nb=3)","metadata":{"execution":{"iopub.status.busy":"2022-05-23T06:43:10.369638Z","iopub.execute_input":"2022-05-23T06:43:10.370646Z","iopub.status.idle":"2022-05-23T06:43:13.474623Z","shell.execute_reply.started":"2022-05-23T06:43:10.37061Z","shell.execute_reply":"2022-05-23T06:43:13.473889Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Lightningâš¡Flash & UNet++ & albumentations\n\nlets follow the Semantinc segmentation example: https://lightning-flash.readthedocs.io/en/stable/reference/semantic_segmentation.html","metadata":{}},{"cell_type":"code","source":"import torch\n\nimport flash\nfrom flash.core.data.utils import download_data\nfrom flash.image import SemanticSegmentation, SemanticSegmentationData\n\nprint(flash.__version__)","metadata":{"execution":{"iopub.status.busy":"2022-05-23T06:43:13.476018Z","iopub.execute_input":"2022-05-23T06:43:13.476476Z","iopub.status.idle":"2022-05-23T06:43:13.483256Z","shell.execute_reply.started":"2022-05-23T06:43:13.476436Z","shell.execute_reply":"2022-05-23T06:43:13.482122Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1. Create the DataModule","metadata":{}},{"cell_type":"code","source":"from dataclasses import dataclass\nfrom typing import Any, Callable, Dict, Mapping, Sequence, Tuple, Union\nimport albumentations as alb\nfrom flash.core.data.io.input_transform import InputTransform\nfrom flash.image.segmentation.input_transform import prepare_target, remove_extra_dimensions\n# from kaggle_imsegm.augment import FlashAlbumentationsAdapter\n\nIMAGE_SIZE = (320, 320)\nTRAIN_TRANSFORM = FlashAlbumentationsAdapter([\n    alb.Resize(*IMAGE_SIZE),\n    alb.VerticalFlip(p=0.5),\n    alb.HorizontalFlip(p=0.5),\n    alb.RandomRotate90(p=0.5),\n    alb.ShiftScaleRotate(shift_limit=0.2, scale_limit=0.08, rotate_limit=10, p=1.),\n    alb.GaussNoise(var_limit=(0.001, 0.02), mean=0, per_channel=False, p=1.0),\n    # alb.OneOf([\n    #     alb.GridDistortion(num_steps=5, distort_limit=0.05, p=1.0),\n    #     alb.ElasticTransform(alpha=1, sigma=50, alpha_affine=50, p=1.0),\n    # ], p=0.25),\n    alb.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.7),\n    alb.Normalize(mean=COLOR_MEAN, std=COLOR_STD, max_pixel_value=255),\n])\nVAL_TRANSFORM = FlashAlbumentationsAdapter([\n    alb.Resize(*IMAGE_SIZE), alb.Normalize(mean=COLOR_MEAN, std=COLOR_STD, max_pixel_value=255)\n])","metadata":{"execution":{"iopub.status.busy":"2022-05-23T06:43:13.487254Z","iopub.execute_input":"2022-05-23T06:43:13.487692Z","iopub.status.idle":"2022-05-23T06:43:13.497433Z","shell.execute_reply.started":"2022-05-23T06:43:13.487654Z","shell.execute_reply":"2022-05-23T06:43:13.496525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_imgs = glob.glob(os.path.join(DATASET_FOLDER, \"test\", \"**\", \"*.png\"), recursive=True)\nif not sample_imgs:\n    sample_imgs = glob.glob(os.path.join(DATASET_FOLDER, \"train\", \"case123\", \"**\", \"*.png\"), recursive=True)\nprint(f\"images: {len(sample_imgs)}\")\nsample_imgs = [p.replace(DATASET_FOLDER + os.path.sep, \"\") for p in sample_imgs[70:75]]\ntab_preds = pd.DataFrame({\"image_path\": sample_imgs})","metadata":{"execution":{"iopub.status.busy":"2022-05-23T06:43:13.498811Z","iopub.execute_input":"2022-05-23T06:43:13.499131Z","iopub.status.idle":"2022-05-23T06:43:13.523451Z","shell.execute_reply.started":"2022-05-23T06:43:13.499092Z","shell.execute_reply":"2022-05-23T06:43:13.522766Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"datamodule = TractData(\n    df_train,\n    dataset_dir=DATASET_FOLDER,\n    df_predict=tab_preds,\n    train_transform=TRAIN_TRANSFORM,\n    input_transform=VAL_TRANSFORM,\n    dataloader_kwargs=dict(batch_size=18, num_workers=3),\n    val_split=0.01 if WITH_SUBMISSION else 0.1, \n)\ndatamodule.setup()\nLABELS = datamodule.labels\nassert len(LABELS) == 3","metadata":{"execution":{"iopub.status.busy":"2022-05-23T06:43:13.524724Z","iopub.execute_input":"2022-05-23T06:43:13.524966Z","iopub.status.idle":"2022-05-23T06:43:58.351254Z","shell.execute_reply.started":"2022-05-23T06:43:13.524934Z","shell.execute_reply":"2022-05-23T06:43:58.350621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"_= show_tract_datamodule_samples_2d(datamodule.train_dataloader(), nb=5, skip_empty=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-23T06:43:58.352333Z","iopub.execute_input":"2022-05-23T06:43:58.352586Z","iopub.status.idle":"2022-05-23T06:44:02.8429Z","shell.execute_reply.started":"2022-05-23T06:43:58.35255Z","shell.execute_reply":"2022-05-23T06:44:02.842139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2. Build the task","metadata":{}},{"cell_type":"code","source":"# import segmentation_models_pytorch as smp\nfrom kaggle_imsegm.model import MixedLoss\nfrom kaggle_imsegm.transform import SemanticSegmentationOutputTransform\n\nmodel = SemanticSegmentation(\n    backbone=\"efficientnet-b3\",\n    head=\"unetplusplus\",\n    pretrained=False,\n    optimizer=\"AdamW\",\n    learning_rate=7e-3,\n    loss_fn=MixedLoss(\"dice\", smooth=0.01),\n    lr_scheduler=(\"cosineannealinglr\", {\"T_max\": 500, \"eta_min\": 1e-6}),\n    num_classes=3,\n    multi_label=True,\n    output_transform=SemanticSegmentationOutputTransform(),\n)","metadata":{"execution":{"iopub.status.busy":"2022-05-23T06:44:02.844168Z","iopub.execute_input":"2022-05-23T06:44:02.844415Z","iopub.status.idle":"2022-05-23T06:44:03.123421Z","shell.execute_reply.started":"2022-05-23T06:44:02.84438Z","shell.execute_reply":"2022-05-23T06:44:03.122606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3. Create the trainer and finetune the model","metadata":{}},{"cell_type":"code","source":"import pytorch_lightning as pl\n\nGPUs = torch.cuda.device_count()\n\ntrainer = flash.Trainer(\n    max_epochs=15 if WITH_SUBMISSION else 5,\n    logger=pl.loggers.CSVLogger(save_dir='logs/'),\n    gpus=GPUs,\n    precision=16 if GPUs else 32,\n    accumulate_grad_batches=24,\n    gradient_clip_val=0.01,\n    limit_train_batches=1.0 if WITH_SUBMISSION else 0.1,\n    limit_val_batches=1.0 if WITH_SUBMISSION else 0.2,\n)","metadata":{"execution":{"iopub.status.busy":"2022-05-23T06:44:03.124881Z","iopub.execute_input":"2022-05-23T06:44:03.125145Z","iopub.status.idle":"2022-05-23T06:44:03.714843Z","shell.execute_reply.started":"2022-05-23T06:44:03.125107Z","shell.execute_reply":"2022-05-23T06:44:03.71411Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n\n# Train the model\ntrainer.finetune(model, datamodule=datamodule, strategy=\"no_freeze\")\n\n# Save the model!\ntrainer.save_checkpoint(\"semantic_segmentation_model.pt\")","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-05-23T06:44:03.716222Z","iopub.execute_input":"2022-05-23T06:44:03.716456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sn\n\nmetrics = pd.read_csv(f'{trainer.logger.log_dir}/metrics.csv')\ndel metrics[\"step\"]\nmetrics.set_index(\"epoch\", inplace=True)\n# display(metrics.dropna(axis=1, how=\"all\").head())\ng = sn.relplot(data=metrics, kind=\"line\")\nplt.gcf().set_size_inches(12, 4)\nplt.grid()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4. Segment a few images!","metadata":{}},{"cell_type":"code","source":"from itertools import chain\n\npreds = trainer.predict(model, datamodule=datamodule)  #, output=\"preds\"\npreds = list(chain(*preds))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axarr = plt.subplots(ncols=4, nrows=len(sample_imgs), figsize=(12, 3 * len(sample_imgs)))\nfor i, pred in enumerate(preds):\n    print(pred.keys())\n    img = pred['input']\n    print(img.shape, img.min(), img.max())\n    axarr[i, 0].imshow(img)\n    for j, seg in enumerate(pred['preds']):\n        print(seg.shape, seg.min(), seg.max())\n        im = axarr[i, j + 1].imshow(seg, vmin=-10, vmax=10)\n        plt.colorbar(im, ax=axarr[i, j + 1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inference ðŸ”¥","metadata":{}},{"cell_type":"code","source":"model = SemanticSegmentation.load_from_checkpoint(\n    \"semantic_segmentation_model.pt\"\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sfolder = \"test\" if WITH_SUBMISSION else \"train\"\nls_images = glob.glob(os.path.join(DATASET_FOLDER, sfolder, \"**\", \"*.png\"), recursive=True)\nls_images = [p.replace(DATASET_FOLDER + os.path.sep, \"\") for p in ls_images]\ncase_day = [os.path.dirname(p).split(os.path.sep)[-2] for p in ls_images]\ndf_pred = pd.DataFrame({'Case_Day': case_day, 'image_path': ls_images})\n\nif not WITH_SUBMISSION:\n    df_pred = df_pred[df_pred[\"Case_Day\"].str.startswith(\"case123_day\")]\ndisplay(df_pred.head())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Predictions for test scans","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom itertools import chain\nfrom scipy.ndimage import binary_opening\nfrom skimage.morphology import disk\nfrom kaggle_imsegm.mask import rle_encode\n\npreds = []\nfor case_day, tab_preds in tqdm(df_pred.groupby(\"Case_Day\")):\n    dm = TractData(\n        df_train[df_train[\"id\"].str.startswith(\"case123_day\")],  # FAKE\n        dataset_dir=DATASET_FOLDER,\n        df_predict=tab_preds,\n        train_transform=TRAIN_TRANSFORM,\n        input_transform=VAL_TRANSFORM,\n        dataloader_kwargs=dict(batch_size=10, num_workers=3),\n    )\n    # dm.setup()\n    results = trainer.predict(model, datamodule=dm)\n    results = list(chain(*results))\n    assert len(tab_preds[\"image_path\"]) == len(results)\n    for img_path, spl in zip(tab_preds[\"image_path\"], results):\n        name, _ = os.path.splitext(os.path.basename(img_path))\n        id_ = f\"{case_day}_\" + \"_\".join(name.split(\"_\")[:2])\n        # print(spl.keys())\n        for i, mask in enumerate(spl[\"preds\"]):\n            mask = (mask >= 0).astype(np.uint8)\n            mask = binary_opening(mask, structure=disk(4)).astype(np.uint8)\n            # print(seg.shape)\n            rle = rle_encode(mask)[1] if np.sum(mask) > 1 else \"\"\n            preds.append({\"id\": id_, \"class\": LABELS[i], \"predicted\": rle})\n\nassert len(preds) == 3 * len(df_pred)\ndf_pred = pd.DataFrame(preds)","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(df_pred[df_pred[\"predicted\"] != \"\"].head())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Finalize submissions","metadata":{}},{"cell_type":"code","source":"df_ssub = pd.read_csv(os.path.join(DATASET_FOLDER, \"sample_submission.csv\"))\ndel df_ssub['predicted']\nif WITH_SUBMISSION:\n    assert len(df_ssub) == len(df_pred)\ndf_pred = df_ssub.merge(df_pred, on=['id','class'])\n\ndf_pred[['id', 'class', 'predicted']].to_csv(\"submission.csv\", index=False)\n\n!head submission.csv","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}