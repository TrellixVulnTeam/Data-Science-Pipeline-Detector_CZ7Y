{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"## Import dependencies \nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt \n%matplotlib inline\n\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nimport plotly.figure_factory as ff\nimport plotly.offline as offline\n\nimport os\nimport sys\nimport pathlib\nimport glob\nimport gc\ngc.enable()\nimport sys\nimport re\nimport math \nimport random\nimport time \nimport datetime as dt\nfrom tqdm import tqdm \nfrom pprint import pprint\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nimport torchvision\nfrom torchvision import transforms\nimport torchvision.transforms.functional as TF\nfrom torchvision.models import resnet18\n#!pip install torchinfo -q --user\n#from torchinfo import summary\n\nfrom sklearn.model_selection import KFold\n\nfrom PIL import Image\n\nprint('import done!')","metadata":{"execution":{"iopub.status.busy":"2022-06-05T04:13:49.83289Z","iopub.execute_input":"2022-06-05T04:13:49.833358Z","iopub.status.idle":"2022-06-05T04:13:54.460606Z","shell.execute_reply.started":"2022-06-05T04:13:49.833323Z","shell.execute_reply":"2022-06-05T04:13:54.459738Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!mkdir -p /tmp/pip/cache/\n!cp ../input/segmentationmodelspytorch-202208/torch-1.11.0-cp37-cp37m-manylinux1_x86_64.whl /kaggle/working/\n#!pip install --no-index --find-links /tmp/pip/cache/ torch-1.11.0-cp37-cp37m-manylinux1_x86_64.whl\n\n!cp ../input/segmentationmodelspytorch-202208/torchvision-0.12.0-cp37-cp37m-manylinux1_x86_64.whl /kaggle/working/\n#!pip install --no-index --find-links /tmp/pip/cache/ torchvision-0.12.0-cp37-cp37m-manylinux1_x86_64.whl\n\n#!cp ../input/segmentationmodelspytorch-202208/Pillow-9.1.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl /kaggle/working/\n#!cp ../input/segmentationmodelspytorch-202208/certifi-2022.5.18.1-py3-none-any.whl /kaggle/working/\n#!cp ../input/segmentationmodelspytorch-202208/charset_normalizer-2.0.12-py3-none-any.whl /kaggle/working/\n##!cp ../input/segmentationmodelspytorch-202208/idna-3.3-py3-none-any.whl /kaggle/working/\n#!cp ../input/segmentationmodelspytorch-202208/munch-2.5.0-py2.py3-none-any.whl /kaggle/working/\n#!cp ../input/segmentationmodelspytorch-202208/numpy-1.21.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl /kaggle/working/\n#!cp ../input/segmentationmodelspytorch-202208/requests-2.27.1-py2.py3-none-any.whl /kaggle/working/\n!cp ../input/segmentationmodelspytorch-202208/segmentation_models_pytorch-0.2.1-py3-none-any.whl /kaggle/working/\n#!cp ../input/segmentationmodelspytorch-202208/six-1.16.0-py2.py3-none-any.whl /kaggle/working/\n#!cp ../input/segmentationmodelspytorch-202208/tqdm-4.64.0-py2.py3-none-any.whl /kaggle/working/\n#!cp ../input/segmentationmodelspytorch-202208/typing_extensions-4.2.0-py3-none-any.whl /kaggle/working/\n#!cp ../input/segmentationmodelspytorch-202208/urllib3-1.26.9-py2.py3-none-any.whl /kaggle/working/\n!cp -rp ../input/segmentationmodelspytorch-202208/efficientnet_pytorch-0.6.3/efficientnet_pytorch-0.6.3 /kaggle/working/efficientnet_pytorch-0.6.3.tar.gz\n!cp -rp ../input/segmentationmodelspytorch-202208/pretrainedmodels-0.7.4/pretrainedmodels-0.7.4 /kaggle/working/pretrainedmodels-0.7.4.tar.gz\n!cp ../input/segmentationmodelspytorch-202208/timm-0.4.12-py3-none-any.whl /kaggle/working/","metadata":{"execution":{"iopub.status.busy":"2022-06-05T04:13:54.462493Z","iopub.execute_input":"2022-06-05T04:13:54.463396Z","iopub.status.idle":"2022-06-05T04:14:07.008646Z","shell.execute_reply.started":"2022-06-05T04:13:54.463357Z","shell.execute_reply":"2022-06-05T04:14:07.00751Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!cp -rp ../input/segmentationmodelspytorch-202208/efficientnet_pytorch-0.6.3/efficientnet_pytorch-0.6.3 /kaggle/working/efficientnet_pytorch-0.6.3.tar.gz\n!cp -rp ../input/segmentationmodelspytorch-202208/pretrainedmodels-0.7.4/pretrainedmodels-0.7.4 /kaggle/working/pretrainedmodels-0.7.4.tar.gz","metadata":{"execution":{"iopub.status.busy":"2022-06-05T04:14:07.010496Z","iopub.execute_input":"2022-06-05T04:14:07.011194Z","iopub.status.idle":"2022-06-05T04:14:08.363526Z","shell.execute_reply.started":"2022-06-05T04:14:07.011147Z","shell.execute_reply":"2022-06-05T04:14:08.36243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#tar.gzの中にもう一つディレクトリがあって、そのディレクトリにsetup.pyがあると動く\n#efficientnet_pytorch-0.6.3.tar.gz/efficientnet_pytorch-0.6.3/setup.py\n!pip install --no-index --find-links /tmp/pip/cache/ efficientnet_pytorch-0.6.3.tar.gz/efficientnet_pytorch-0.6.3\n!pip install --no-index --find-links /tmp/pip/cache/ pretrainedmodels-0.7.4.tar.gz/pretrainedmodels-0.7.4\n!pip install --no-index --find-links /tmp/pip/cache/ timm-0.4.12-py3-none-any.whl\n!pip install --no-index --find-links /tmp/pip/cache/ segmentation_models_pytorch-0.2.1-py3-none-any.whl","metadata":{"execution":{"iopub.status.busy":"2022-06-05T04:14:08.366666Z","iopub.execute_input":"2022-06-05T04:14:08.367063Z","iopub.status.idle":"2022-06-05T04:14:49.984246Z","shell.execute_reply.started":"2022-06-05T04:14:08.367021Z","shell.execute_reply":"2022-06-05T04:14:49.983252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#resnetをオフラインで利用するための読み取り元のディレクトリを作成\n!mkdir -p /root/.cache/torch/hub/checkpoints/\n!cp ../input/segmentationmodelspytorch-202208/resnet34-333f7ec4.pth /root/.cache/torch/hub/checkpoints/","metadata":{"execution":{"iopub.status.busy":"2022-06-05T04:14:49.987647Z","iopub.execute_input":"2022-06-05T04:14:49.98797Z","iopub.status.idle":"2022-06-05T04:14:52.67052Z","shell.execute_reply.started":"2022-06-05T04:14:49.987939Z","shell.execute_reply":"2022-06-05T04:14:52.669441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import segmentation_models_pytorch as smp","metadata":{"execution":{"iopub.status.busy":"2022-06-05T04:14:52.672583Z","iopub.execute_input":"2022-06-05T04:14:52.673064Z","iopub.status.idle":"2022-06-05T04:14:58.862489Z","shell.execute_reply.started":"2022-06-05T04:14:52.673014Z","shell.execute_reply":"2022-06-05T04:14:58.86149Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = smp.DeepLabV3(encoder_name='resnet34',\n                      encoder_depth=5,\n                      encoder_weights='imagenet',\n                      in_channels=1,\n                      classes=4,\n                      aux_params=None)\nmodel.cuda()\n#最適化手法\nLEARNING_RATE = 1e-4\noptimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n\n## For the model training loop.\nif torch.cuda.is_available():\n    DEVICE = 'cuda'\nelse: DEVICE = 'cpu'","metadata":{"execution":{"iopub.status.busy":"2022-06-05T04:14:58.864125Z","iopub.execute_input":"2022-06-05T04:14:58.864499Z","iopub.status.idle":"2022-06-05T04:15:04.593453Z","shell.execute_reply.started":"2022-06-05T04:14:58.864458Z","shell.execute_reply":"2022-06-05T04:15:04.592564Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df = pd.read_csv('../input/uw-madison-gi-tract-image-segmentation/sample_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2022-06-05T04:15:04.597735Z","iopub.execute_input":"2022-06-05T04:15:04.6014Z","iopub.status.idle":"2022-06-05T04:15:04.618022Z","shell.execute_reply.started":"2022-06-05T04:15:04.60136Z","shell.execute_reply":"2022-06-05T04:15:04.617097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class UWMadison2022Dataset(torch.utils.data.Dataset):\n    def __init__(self, files, dataframe=None, input_shape=256,predicted = False):\n        self.files = files\n        self.df = dataframe\n        self.input_shape = input_shape\n        self.transforms = transforms.Compose([\n            transforms.CenterCrop(self.input_shape),\n            transforms.Normalize(mean=[(0.485+0.456+0.406)/3], std=[(0.229+0.224+0.225)/3]),\n        ])\n        self.predicted = predicted\n        \n    def __len__(self):\n        return len(self.files)\n    \n    def __getitem__(self, idx):\n        p_file = self.files[idx]\n        #img = torchvision.io.read_image(p_file)\n        img = np.array(Image.open(p_file))\n        img_shape = torch.tensor(img.shape)\n        img = transforms.functional.to_tensor(img) / 255.\n        img = self.transforms(img)\n        #img = torch.cat([img, img, img], dim=0)\n        \n        if self.df is not None:\n            f_name = str(p_file).split('/')\n            case_day_id = f_name[5]\n            slice_id = f_name[7][:10]\n            f_id = '_'.join([case_day_id, slice_id])\n            labels_df = self.df.query('id == @f_id')\n            #display(labels_df.head())\n            \n            label = torch.zeros([img_shape[0]*img_shape[1]])\n            if self.predicted == False:\n                for i, organ in enumerate(['large_bowel', 'small_bowel', 'stomach']):\n                    seg_type = organ + \"_segmentation\"\n                    segmentation = labels_df[seg_type]#.item()\n                    if type(segmentation) is str:\n                        segmentation = segmentation.split(' ')\n                        for j in range(len(segmentation)//2):\n                            start_idx = int(segmentation[j*2])\n                            span = int(segmentation[j*2 + 1])\n                            label[start_idx:(start_idx+span)] = (i+1)\n            else:\n                segmentation = labels_df[\"predicted\"]#.item()\n                if type(segmentation) is str:\n                    segmentation = segmentation.split(' ')\n                    for j in range(len(segmentation)//2):\n                        start_idx = int(segmentation[j*2])\n                        span = int(segmentation[j*2 + 1])\n                        label[start_idx:(start_idx+span)] = (i+1)\n            \n            label = torch.reshape(label, (img_shape[0], img_shape[1]))#256, 256pixelに固定\n            label = transforms.CenterCrop(self.input_shape)(label)\n            label = torch.nn.functional.one_hot(label.to(torch.int64), num_classes=4)\n            label = label.permute(2, 0, 1)\n            return img, label, img_shape\n        \n        else: return img, img_shape","metadata":{"execution":{"iopub.status.busy":"2022-06-05T04:15:04.619361Z","iopub.execute_input":"2022-06-05T04:15:04.619859Z","iopub.status.idle":"2022-06-05T04:15:04.642384Z","shell.execute_reply.started":"2022-06-05T04:15:04.61982Z","shell.execute_reply":"2022-06-05T04:15:04.641615Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Prediction","metadata":{}},{"cell_type":"code","source":"test_images = glob.glob(os.path.join(\"../input/uw-madison-gi-tract-image-segmentation/test\", \"**\", \"*.png\"), recursive=True)\n#test_images\n\n#if len(test_images) == 0:\n    #submission_df = pd.read_csv(\"../input/uw-madison-gi-tract-image-segmentation/train.csv\")[[\"id\", \"class\"]].iloc[:100 * 3]\n    #submission_df[\"predicted\"] = \"\"\n    #test_images = glob.glob(os.path.join(\"../input/uw-madison-gi-tract-image-segmentation/train\",  \"**\", \"*.png\"), recursive = True)\n\n    \nid2img = {_.rsplit(\"/\", 4)[2] + \"_\" + \"_\".join(_.rsplit(\"/\", 4)[4].split(\"_\")[:2]): _ for _ in test_images}\nsubmission_df[\"file_name\"] = submission_df.id.map(id2img)\nsubmission_df[\"days\"] = submission_df.id.apply(lambda x: \"_\".join(x.split(\"_\")[:2]))\nfname2index = {f + c: i for f, c, i in zip(submission_df.file_name, submission_df[\"class\"], submission_df.index)}\n#submission_df\n","metadata":{"execution":{"iopub.status.busy":"2022-06-05T04:15:04.645316Z","iopub.execute_input":"2022-06-05T04:15:04.64654Z","iopub.status.idle":"2022-06-05T04:15:08.719903Z","shell.execute_reply.started":"2022-06-05T04:15:04.646504Z","shell.execute_reply":"2022-06-05T04:15:08.719143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_ids = np.unique(submission_df.id.values.tolist())\nsubmission_df_unique_ids = submission_df[submission_df.index %3 == 0]\n#submission_df_unique_ids.head()\n#display(submission_df.head())\nsubmission_df.shape","metadata":{"execution":{"iopub.status.busy":"2022-06-05T04:36:37.387631Z","iopub.execute_input":"2022-06-05T04:36:37.388472Z","iopub.status.idle":"2022-06-05T04:36:37.468194Z","shell.execute_reply.started":"2022-06-05T04:36:37.388385Z","shell.execute_reply":"2022-06-05T04:36:37.467014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()\ncheckpoint = torch.load(\"../input/uwmgi-leraning-model/checkpoint.pth\")\nmodel.load_state_dict(checkpoint[\"model\"])\noptimizer.load_state_dict(checkpoint[\"optimizer\"])\n        \nmodel.eval()\npredictions = []\n#predict_files = submission_df.file_name.values.tolist()\npredict_files = submission_df_unique_ids.file_name.values.tolist()#1idにつき１filenameのリスト\n\npredict_ds = UWMadison2022Dataset(predict_files, submission_df.reset_index(drop=True), input_shape=256, predicted = True)\nBATCH_SIZE = 32\n\nprint('------ predict_dl ------')\npredict_dl = torch.utils.data.DataLoader(predict_ds, batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\ntmp = predict_dl.__iter__()\nx, y, shape = tmp.next()\nprint(f\"x : {x.shape}\")\nprint(f\"labels: {y.shape}\")\nprint(f\"img_shapes: {shape.shape}\")\nprint(f\"n_samples: {len(predict_ds)}\")\nprint(f\"n_batches: {len(tmp)}\")\nprint()\ndel predict_ds\ngc.collect()\n\n\nwith torch.no_grad():\n    for batch in tqdm(predict_dl):\n        x = batch[0].to(DEVICE)\n        predict_pred = model(x)\n        predict_pred = torch.argmax(predict_pred, dim=1)\n        predict_pred = torch.nn.functional.one_hot(predict_pred, num_classes=4)\n        predict_pred = torch.permute(predict_pred, dims=[0, 3, 1, 2])\n        predict_pred = predict_pred[:, 1:, ...] ## We don't need background predictions.\n        predict_pred = predict_pred.detach().cpu().numpy()\n        predictions.append(predict_pred)\n    \npredictions = np.concatenate(predictions, axis=0)\npredictions = predictions.reshape([-1, 256, 256])\nprint(predictions.shape)","metadata":{"execution":{"iopub.status.busy":"2022-06-05T04:15:08.731188Z","iopub.execute_input":"2022-06-05T04:15:08.731617Z","iopub.status.idle":"2022-06-05T04:15:19.568218Z","shell.execute_reply.started":"2022-06-05T04:15:08.731533Z","shell.execute_reply":"2022-06-05T04:15:19.567213Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def rle_encode(img):\n    pixels = img.flatten()\n    pixels = np.concatenate([[0], pixels, [0]])\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n    runs[1::2] -= runs[::2]\n    encodes = ' '.join(str(x) for x in runs)\n    if encodes == '':\n        encodes = np.nan\n    return encodes\n\npredictions_rle = []\n\nfor pred in predictions:\n    pred_rle = rle_encode(pred)\n    predictions_rle.append(pred_rle)\n    \npredictions_rle = np.concatenate([predictions_rle], axis=0)\nprint(\"length : predictions_rle : \", len(predictions_rle))\nsubmission_df['predicted'] = predictions_rle\n\n#submission_df.head()\n#submission_df[100:150]","metadata":{"execution":{"iopub.status.busy":"2022-06-05T04:15:19.569588Z","iopub.execute_input":"2022-06-05T04:15:19.570031Z","iopub.status.idle":"2022-06-05T04:15:19.626102Z","shell.execute_reply.started":"2022-06-05T04:15:19.569993Z","shell.execute_reply":"2022-06-05T04:15:19.625214Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#submission_df.to_csv(\"submission_full_columns.csv\")\nsubmission_df = submission_df[[\"id\", \"class\", \"predicted\"]]\n\nsubmission_df.to_csv(\"submission.csv\", index = False)","metadata":{"execution":{"iopub.status.busy":"2022-06-05T04:15:19.62742Z","iopub.execute_input":"2022-06-05T04:15:19.627846Z","iopub.status.idle":"2022-06-05T04:15:19.637874Z","shell.execute_reply.started":"2022-06-05T04:15:19.62781Z","shell.execute_reply":"2022-06-05T04:15:19.637034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#submission_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-05T04:16:26.68142Z","iopub.execute_input":"2022-06-05T04:16:26.681779Z","iopub.status.idle":"2022-06-05T04:16:26.702721Z","shell.execute_reply.started":"2022-06-05T04:16:26.681749Z","shell.execute_reply":"2022-06-05T04:16:26.70199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}