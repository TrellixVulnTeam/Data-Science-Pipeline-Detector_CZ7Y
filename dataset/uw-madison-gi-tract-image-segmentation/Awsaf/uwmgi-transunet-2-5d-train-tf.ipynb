{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# [UW-Madison GI Tract Image Segmentation](https://www.kaggle.com/competitions/uw-madison-gi-tract-image-segmentation/)\n> Track healthy organs in medical scans to improve cancer treatment\n\n<img src=\"https://storage.googleapis.com/kaggle-competitions/kaggle/27923/logos/header.png?t=2021-06-02-20-30-25\">","metadata":{}},{"cell_type":"markdown","source":"# A. Methodlogy  🎯\n* In this notebook I'll demonstrate **2.5D** image Training for **Segmentation** with `tf.data`, `tfrecord` using `Tensorflow`. \n* In a nutshell, **2.5D Image Training** is training of **3D** image like **2D** Image.  More about **2.5D** training is discussed later. 2.5D images can take leverage of the extra depth information like our typical RGB image. In this notebook I'll be using 3 channels with 2 strides for 2.5D images\n* In this notebook, I'll be also re-implementing **[TransUNet: Transformers Make Strong Encoders for Medical Image Segmentation](https://arxiv.org/pdf/2102.04306.pdf)** Model using Tensorflow.\n* TFRecord dataset for **Segmentation** is created using [UWMGI: 2.5D TFRecord Data](https://www.kaggle.com/code/awsaf49/uwmgi-2-5d-tfrecord-data) notebook.\n* TFRecord files are created using **StratifiedGroupFold** to avoid data leakage due to `case` and to stratify `empty` and `non-empty` mask cases.\n* This notebook is compatible for both **GPU** and **TPU**. Device is automatically selected so you won't have to do anything to allocate device.\n* As there are overlaps between **Stomach**, **Large Bowel** & **Small Bowel** classes, this is a **MultiLabel Segmentation** task, so final activaion should be `sigmoid` instead of `softmax`.\n* You can play with different models and losses.","metadata":{}},{"cell_type":"markdown","source":"# B. Notebooks 📒\n📌 **2.5D-TransUNet**:\n* Train: [UWMGI: TransUnet 2.5D [Train] [TF]](https://www.kaggle.com/awsaf49/uwmgi-transunet-2-5d-train-tf/)\n<!-- * Infer:  UWMGI: TransUnet 2.5D [Infer] [TF]-->\n\n📌 **Data/Dataset**:\n* Mask-Data: [UWMGI: Mask Data](https://www.kaggle.com/code/awsaf49/uwmgi-mask-data)\n* Data: [UWMGI: 2.5D TFRecord Data](https://www.kaggle.com/code/awsaf49/uwmgi-2-5d-tfrecord-data)\n* Dataset: [UWMGI: 2.5D TFRecord Dataset](https://www.kaggle.com/datasets/awsaf49/uwmgi-25d-tfrecord-dataset)","metadata":{}},{"cell_type":"markdown","source":"# 1. Install Libraries 🛠\nSource code for TransUNet model is [here](https://github.com/awsaf49/TransUNet-tf)","metadata":{}},{"cell_type":"code","source":"!pip install -q transunet\n!pip install -q segmentation_models\n!pip install -qU wandb","metadata":{"execution":{"iopub.status.busy":"2022-05-19T11:11:17.905697Z","iopub.execute_input":"2022-05-19T11:11:17.906271Z","iopub.status.idle":"2022-05-19T11:11:47.785126Z","shell.execute_reply.started":"2022-05-19T11:11:17.906178Z","shell.execute_reply":"2022-05-19T11:11:47.783823Z"},"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Import Libraries 📚\nLet's imoport necessary libraries.","metadata":{}},{"cell_type":"code","source":"import pandas as pd, numpy as np, random,os, shutil\nimport tensorflow as tf, re, math\nimport tensorflow_addons as tfa\nimport tensorflow.keras.backend as K\nimport efficientnet.tfkeras as efn\nimport sklearn\nimport cv2\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Rectangle\nimport tensorflow_addons as tfa\nimport yaml\nfrom IPython import display as ipd\nimport json\nfrom datetime import datetime\n\nfrom glob import glob\nfrom tqdm.notebook import tqdm\nfrom kaggle_datasets import KaggleDatasets\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import roc_auc_score\nfrom IPython import display as ipd\n\nimport scipy\nimport warnings\n\ntfk = tf.keras\ntfkl = tfk.layers\ntfm = tf.math\n\n# Show less log messages\ntf.get_logger().setLevel('ERROR')\ntf.autograph.set_verbosity(0)\n\n# Set tf.keras as backend\nos.environ['SM_FRAMEWORK'] = 'tf.keras'\nimport segmentation_models as sm\n\n# Set true to show less logging messages\nos.environ[\"WANDB_SILENT\"] = \"false\"\nimport wandb","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-19T11:11:47.788531Z","iopub.execute_input":"2022-05-19T11:11:47.788861Z","iopub.status.idle":"2022-05-19T11:11:55.628919Z","shell.execute_reply.started":"2022-05-19T11:11:47.788817Z","shell.execute_reply":"2022-05-19T11:11:55.627993Z"},"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Configuration ⚙️","metadata":{}},{"cell_type":"code","source":"class CFG:\n    wandb = True\n    competition = \"uwmgi-tf\"\n    _wandb_kernel = \"awsaf49\"\n    debug = False\n    exp_name = \"v4\"\n    comment = \"TransUNet-ResNet50V2-128x128-noaug-2.5D\"\n\n    # Use verbose=0 for silent, 1 for interactive\n    verbose = 0\n    display_plot = True\n\n    # Device for training\n    device = None  # device is automatically selected\n\n    # Model & Backbone\n    model_name = \"TransUNet\"\n    backbone = \"ResNet50V2\"\n\n    # Seeding for reproducibility\n    seed = 101\n\n    # Number of folds\n    folds = 5\n\n    # Which Folds to train\n    selected_folds = [0, 1, 2, 3, 4]\n\n    # Image Size\n    img_size = [128, 128]\n\n    # Batch Size & Epochs\n    batch_size = 32\n    drop_remainder = False\n    epochs = 15\n    steps_per_execution = None\n\n    # Loss & Optimizer & LR Scheduler\n    loss = \"dice_loss\"\n    optimizer = \"Adam\"\n    lr = 5e-4\n    lr_schedule = \"CosineDecay\"\n    patience = 5\n\n    # Augmentation\n    augment = False\n    transform = False\n\n    # Transformation\n    fill_mode = \"constant\"\n    rot = 5.0  # proprtional\n    shr = 5.0  # proprtional\n    hzoom = 100.0  # inv proportional\n    wzoom = 100.0  # inv proportional\n    hshift = 10.0  # proportional\n    wshift = 10.0  # proportional\n\n    # Horizontal & Vertical Flip\n    hflip = 0.5\n    vflip = 0.5\n\n    # Clip values to [0, 1]\n    clip = False\n\n    # CutOut\n    drop_prob = 0.5\n    drop_cnt = 10\n    drop_size = 0.05\n\n    # Jitter\n    sat = [0.7, 1.3]  # saturation\n    cont = [0.8, 1.2]  # contrast\n    bri = 0.15  # brightness\n    hue = 0.0  # hue\n","metadata":{"execution":{"iopub.status.busy":"2022-05-19T11:11:55.630326Z","iopub.execute_input":"2022-05-19T11:11:55.630603Z","iopub.status.idle":"2022-05-19T11:11:55.642887Z","shell.execute_reply.started":"2022-05-19T11:11:55.630572Z","shell.execute_reply":"2022-05-19T11:11:55.640549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Reproducibility ♻️\nSets value for random seed to produce similar result in each run.","metadata":{}},{"cell_type":"code","source":"def seeding(SEED):\n    \"\"\"\n    Sets all random seeds for the program (Python, NumPy, and TensorFlow).\n    \"\"\"\n    np.random.seed(SEED)\n    random.seed(SEED)\n    os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n    os.environ[\"TF_CUDNN_DETERMINISTIC\"] = str(SEED)\n    tf.random.set_seed(SEED)\n    print(\"seeding done!!!\")\n\n\nseeding(CFG.seed)","metadata":{"execution":{"iopub.status.busy":"2022-05-19T11:11:55.646198Z","iopub.execute_input":"2022-05-19T11:11:55.646536Z","iopub.status.idle":"2022-05-19T11:11:55.686046Z","shell.execute_reply.started":"2022-05-19T11:11:55.646495Z","shell.execute_reply":"2022-05-19T11:11:55.68517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5. Set Up Device 📱\nFollowing codes automatically detects hardware(tpu or gpu or cpu). ","metadata":{}},{"cell_type":"code","source":"def configure_device():\n    try:\n        tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()  # connect to tpu cluster\n        strategy = tf.distribute.TPUStrategy(tpu) # get strategy for tpu\n        print('> Running on TPU ', tpu.master(), end=' | ')\n        print('Num of TPUs: ', strategy.num_replicas_in_sync)\n        device='TPU'\n    except: # otherwise detect GPUs\n        tpu = None\n        gpus = tf.config.list_logical_devices('GPU') # get logical gpus\n        ngpu = len(gpus)\n        if ngpu: # if number of GPUs are 0 then CPU\n            strategy = tf.distribute.MirroredStrategy(gpus) # single-GPU or multi-GPU\n            print(\"> Running on GPU\", end=' | ')\n            print(\"Num of GPUs: \", ngpu)\n            device='GPU'\n        else:\n            print(\"> Running on CPU\")\n            strategy = tf.distribute.get_strategy() # connect to single gpu or cpu\n            device='CPU'\n    return strategy, device, tpu","metadata":{"execution":{"iopub.status.busy":"2022-05-19T11:11:55.687898Z","iopub.execute_input":"2022-05-19T11:11:55.688347Z","iopub.status.idle":"2022-05-19T11:11:55.698062Z","shell.execute_reply.started":"2022-05-19T11:11:55.688236Z","shell.execute_reply":"2022-05-19T11:11:55.697018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"strategy, CFG.device, tpu = configure_device()\nAUTO = tf.data.experimental.AUTOTUNE\nREPLICAS = strategy.num_replicas_in_sync\nprint(f'REPLICAS: {REPLICAS}')","metadata":{"execution":{"iopub.status.busy":"2022-05-19T11:11:55.699782Z","iopub.execute_input":"2022-05-19T11:11:55.700534Z","iopub.status.idle":"2022-05-19T11:12:01.922012Z","shell.execute_reply.started":"2022-05-19T11:11:55.700491Z","shell.execute_reply":"2022-05-19T11:12:01.92108Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 6. Meta Data 📖\n* Files\n    * `train.csv` - IDs and masks for all training objects.\n    * `sample_submission.csv` - a sample submission file in the correct format\n    * `train/` - a folder of case/day folders, each containing slice images for a particular case on a given day.\n\n> **Note** that the image filenames include 4 numbers (ex. `276_276_1.63_1.63.png`). These four numbers are slice height / width (integers in pixels) and heigh/width pixel spacing (floating points in mm). The first two defines the resolution of the slide. The last two record the physical size of each pixel.\n\n* Columns\n    * `id` - unique identifier for object\n    * `class` - the predicted class for the object\n    * `EncodedPixels` - RLE-encoded pixels for the identified object","metadata":{}},{"cell_type":"code","source":"import re\ndef count_data_items(filenames):\n    n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) for filename in filenames]\n    return np.sum(n)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-19T11:12:01.925287Z","iopub.execute_input":"2022-05-19T11:12:01.925926Z","iopub.status.idle":"2022-05-19T11:12:01.930848Z","shell.execute_reply.started":"2022-05-19T11:12:01.925891Z","shell.execute_reply":"2022-05-19T11:12:01.929889Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To run code on **TPU** we need our data to be stored on **Google Cloud Storage**. Hence, we'll be needing **GCS_PATH** of our stored data. Worried about how we will get our data stored on **GCS**? \"Kaggle to the Rescue\" Kaggle provides a **GCS_PATH** for public datasets. Hence we can use it for training our model on **TPU**. Simply we have to use `KaggleDatasets()` to get `GCS_PATH` of our dataset.","metadata":{}},{"cell_type":"code","source":"BASE_PATH = '/kaggle/input/uw-madison-gi-tract-image-segmentation'\nGCS_PATH = KaggleDatasets().get_gcs_path('uwmgi-25d-tfrecord-dataset')\nALL_FILENAMES = tf.io.gfile.glob(GCS_PATH + '/uwmgi/*.tfrec')\nprint('NUM TFRECORD FILES: {:,}'.format(len(ALL_FILENAMES)))\nprint('NUM TRAINING IMAGES: {:,}'.format(count_data_items(ALL_FILENAMES)))","metadata":{"execution":{"iopub.status.busy":"2022-05-19T11:12:01.932287Z","iopub.execute_input":"2022-05-19T11:12:01.932638Z","iopub.status.idle":"2022-05-19T11:12:02.412412Z","shell.execute_reply.started":"2022-05-19T11:12:01.932596Z","shell.execute_reply":"2022-05-19T11:12:02.411459Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 7. Data Augmentation 🌈\n> **Caution:** Unlike classification problem, we have to augment both **image** & **mask** otherwise it'll create faulty data as **mask** won't match its corresponding **image**.\n\n<img src=\"https://raw.githubusercontent.com/dmlc/web-data/master/mxnet/doc/tutorials/data_aug/outputs/with_mask/aug_and_mask.png\" width=800>","metadata":{}},{"cell_type":"markdown","source":"## Used Augmentations\nSome Augmentations that were used here are,\n\n* RandomFlip (Left-Right)\n<img src=\"https://dataaspirant.com/wp-content/uploads/2020/08/5-horizontal-flip-technique.png\" width=400>\n\n* Random Rotation\n<img src=\"https://dataaspirant.com/wp-content/uploads/2020/08/4-rotation-technique.png\" width=500>\n\n\n* RandomBrightness\n<img src=\"https://affine.ai/wp-content/uploads/2022/02/2.jpg\" width=400>\n\n* RndomContrast\n<img src=\"https://affine.ai/wp-content/uploads/2022/02/3.jpg\" width=400>\n\n* Zoom\n<img src=\"https://affine.ai/wp-content/uploads/2022/02/16.jpg\" width=400>\n\n* Cutout\n<img src=\"https://i.ibb.co/3MKjW0t/cutout.png\" width=400>\n\n* Shear\n<img src=\"https://imgaug.readthedocs.io/en/latest/_images/shearx.jpg\" width=500>","metadata":{}},{"cell_type":"markdown","source":"## Utility","metadata":{}},{"cell_type":"code","source":"def random_int(shape=[], minval=0, maxval=1):\n    return tf.random.uniform(shape=shape, minval=minval, maxval=maxval, dtype=tf.int32)\n\n\ndef random_float(shape=[], minval=0.0, maxval=1.0):\n    rnd = tf.random.uniform(shape=shape, minval=minval, maxval=maxval, dtype=tf.float32)\n    return rnd\n\n\ndef get_mat(shear, height_zoom, width_zoom, height_shift, width_shift):\n    \"\"\"\n    ref: https://www.kaggle.com/code/cdeotte/triple-stratified-kfold-with-tfrecords/\n    \"\"\"\n    # returns 3x3 transformmatrix which transforms indicies\n\n    # CONVERT DEGREES TO RADIANS\n    # rotation = math.pi * rotation / 180.\n    shear = math.pi * shear / 180.0\n\n    def get_3x3_mat(lst):\n        return tf.reshape(tf.concat([lst], axis=0), [3, 3])\n\n    one = tf.constant([1], dtype=\"float32\")\n    zero = tf.constant([0], dtype=\"float32\")\n\n    # SHEAR MATRIX\n    c2 = tf.math.cos(shear)\n    s2 = tf.math.sin(shear)\n\n    shear_matrix = get_3x3_mat([one, s2, zero, zero, c2, zero, zero, zero, one])\n    # ZOOM MATRIX\n    zoom_matrix = get_3x3_mat(\n        [one / height_zoom, zero, zero, zero, one / width_zoom, zero, zero, zero, one]\n    )\n    # SHIFT MATRIX\n    shift_matrix = get_3x3_mat(\n        [one, zero, height_shift, zero, one, width_shift, zero, zero, one]\n    )\n\n    return K.dot(\n        shear_matrix, K.dot(zoom_matrix, shift_matrix)\n    )  # K.dot(K.dot(rotation_matrix, shear_matrix), K.dot(zoom_matrix, shift_matrix))\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-19T11:12:02.413923Z","iopub.execute_input":"2022-05-19T11:12:02.414224Z","iopub.status.idle":"2022-05-19T11:12:02.426778Z","shell.execute_reply.started":"2022-05-19T11:12:02.414186Z","shell.execute_reply":"2022-05-19T11:12:02.425867Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Augment Fn","metadata":{}},{"cell_type":"code","source":"def ShiftScaleRotate(image, mask=None, DIM=CFG.img_size, p=1.0):\n    \"\"\"\n    ref: https://www.kaggle.com/code/cdeotte/triple-stratified-kfold-with-tfrecords/\n    \"\"\"\n    if random_float() > p:\n        return image, mask\n    if DIM[0] > DIM[1]:\n        diff = DIM[0] - DIM[1]\n        pad = [diff // 2, diff // 2 + (1 if diff % 2 else 0)]\n        image = tf.pad(image, [[0, 0], [pad[0], pad[1]], [0, 0]])\n        NEW_DIM = DIM[0]\n        if mask is not None:\n            mask = tf.pad(mask, [[0, 0], [pad[0], pad[1]], [0, 0]])\n    elif DIM[0] < DIM[1]:\n        diff = DIM[1] - DIM[0]\n        pad = [diff // 2, diff // 2 + (1 if diff % 2 else 0)]\n        image = tf.pad(image, [[pad[0], pad[1]], [0, 0], [0, 0]])\n        NEW_DIM = DIM[1]\n        if mask is not None:\n            mask = tf.pad(mask, [[pad[0], pad[1]], [0, 0], [0, 0]])\n\n    rot = CFG.rot * tf.random.normal([1], dtype=\"float32\")\n    shr = CFG.shr * tf.random.normal([1], dtype=\"float32\")\n    h_zoom = 1.0 + tf.random.normal([1], dtype=\"float32\") / CFG.hzoom\n    w_zoom = 1.0 + tf.random.normal([1], dtype=\"float32\") / CFG.wzoom\n    h_shift = CFG.hshift * tf.random.normal([1], dtype=\"float32\")\n    w_shift = CFG.wshift * tf.random.normal([1], dtype=\"float32\")\n\n    transformation_matrix = tf.linalg.inv(\n        get_mat(shr, h_zoom, w_zoom, h_shift, w_shift)\n    )\n    flat_tensor = tfa.image.transform_ops.matrices_to_flat_transforms(\n        transformation_matrix\n    )\n    rotation = math.pi * rot / 180.0\n\n    image = tfa.image.transform(image, flat_tensor, fill_mode=CFG.fill_mode)\n    image = tfa.image.rotate(image, -rotation, fill_mode=CFG.fill_mode)\n    if mask is not None:\n        mask = tfa.image.transform(mask, flat_tensor, fill_mode=CFG.fill_mode)\n        mask = tfa.image.rotate(mask, -rotation, fill_mode=CFG.fill_mode)\n\n    if DIM[0] > DIM[1]:\n        image = tf.reshape(image, [NEW_DIM, NEW_DIM, 3])\n        image = image[:, pad[0] : -pad[1], :]\n        if mask is not None:\n            mask = tf.reshape(mask, [NEW_DIM, NEW_DIM, 3])\n            mask = mask[:, pad[0] : -pad[1], :]\n    elif DIM[1] > DIM[0]:\n        image = tf.reshape(image, [NEW_DIM, NEW_DIM, 3])\n        image = image[pad[0] : -pad[1], :, :]\n        if mask is not None:\n            mask = tf.reshape(mask, [NEW_DIM, NEW_DIM, 3])\n            mask = mask[pad[0] : -pad[1], :, :]\n\n    image = tf.reshape(image, [*DIM, 3])\n    if mask is not None:\n        mask = tf.reshape(mask, [*DIM, 3])\n    return image, mask\n\n\ndef CutOut(image, mask=None, DIM=CFG.img_size, PROBABILITY=0.6, CT=5, SZ=0.1):\n    \"\"\"\n    ref: https://www.kaggle.com/code/cdeotte/tfrecord-experiments-upsample-and-coarse-dropout\n    \"\"\"\n    # Input Image - is with shape [dim,dim,3] not of [None,dim,dim,3]\n    # Probability\n    P = tf.cast(random_float() < PROBABILITY, tf.int32)\n    if (P == 0) | (CT == 0) | (SZ == 0):\n        return image, mask\n    # Iterate Through Each Sample of Batch\n    for k in range(CT):\n        # Choose Random Location\n        x = tf.cast(tf.random.uniform([], 0, DIM[1]), tf.int32)\n        y = tf.cast(tf.random.uniform([], 0, DIM[0]), tf.int32)\n        # Compute Square for CutOut\n        WIDTH = tf.cast(SZ * min(DIM), tf.int32) * P\n        ya = tf.math.maximum(0, y - WIDTH // 2)\n        yb = tf.math.minimum(DIM[0], y + WIDTH // 2)\n        xa = tf.math.maximum(0, x - WIDTH // 2)\n        xb = tf.math.minimum(DIM[1], x + WIDTH // 2)\n        # CutOut Image\n        one = image[ya:yb, 0:xa, :]\n        two = tf.zeros([yb - ya, xb - xa, 3], dtype=image.dtype)\n        three = image[ya:yb, xb : DIM[1], :]\n        middle = tf.concat([one, two, three], axis=1)\n        image = tf.concat([image[0:ya, :, :], middle, image[yb : DIM[0], :, :]], axis=0)\n        image = tf.reshape(image, [*DIM, 3])\n        # CutOut Mask\n        if mask is not None:\n            one = mask[ya:yb, 0:xa, :]\n            two = tf.zeros([yb - ya, xb - xa, 3], dtype=mask.dtype)  # ch=3\n            three = mask[ya:yb, xb : DIM[1], :]\n            middle = tf.concat([one, two, three], axis=1)\n            mask = tf.concat(\n                [mask[0:ya, :, :], middle, mask[yb : DIM[0], :, :]], axis=0\n            )\n            mask = tf.reshape(mask, [*DIM, 3])  # ch=3\n    return image, mask\n\n\ndef RandomJitter(img, hue, sat, cont, bri, p=1.0):\n    if random_float() > p:\n        return img\n    img = tf.image.random_hue(img, hue)\n    img = tf.image.random_saturation(img, sat[0], sat[1])\n    img = tf.image.random_contrast(img, cont[0], cont[1])\n    img = tf.image.random_brightness(img, bri)\n    return img\n\n\ndef RandomFlip(img, msk=None, hflip_p=0.5, vflip_p=0.5):\n    if random_float() < hflip_p:\n        img = tf.image.flip_left_right(img)\n        if msk is not None:\n            msk = tf.image.flip_left_right(msk)\n    if random_float() < vflip_p:\n        img = tf.image.flip_up_down(img)\n        if msk is not None:\n            msk = tf.image.flip_up_down(msk)\n    return img, msk","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-19T11:12:02.431144Z","iopub.execute_input":"2022-05-19T11:12:02.4315Z","iopub.status.idle":"2022-05-19T11:12:02.470482Z","shell.execute_reply.started":"2022-05-19T11:12:02.431386Z","shell.execute_reply":"2022-05-19T11:12:02.469575Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 8. Data Pipeline 🍚","metadata":{}},{"cell_type":"markdown","source":"## 2.5D Training\n**What is 2.5D Training?**\n\nEven though we can do easy straight-forward 2D training, we can utilize ct slices for extra depth information. For example, we can stack consecutive slices of the scans to get a 3D volume. But one of the reasons why I'm inferring them as 2.5D is that we'll be training 3D images like 2D images. Those who haven't come across this method may get confused at first but let me explain. When we train 2D images like RGB images we actually pass a 3D tensor ex:`[None, channel, height, width]` to a model. For PyTorch, the last two dimensions are spacial(height & width) and the first one is the **channel** dimension. Now for the ct image, we don't have any channel information so we can use that dimension to **stack multiple ct scans as channels and train them as 2d images**. \n\nThis method has some cool advantages over 3D training for instance,\n* Low GPU/memory cost.\n* Simple pipeline.\n* Easier augmentation.\n* Quick inference.\n* Many open-source models.\n\nIn my notebook, I've stacked 3 slices with stride=2, you can check the demo image above for example. It kinda looks like **3d movie scene in the theatre**. \n\n<div align=center><img src=\"https://i.ibb.co/sgsPf4v/Capture.png\" width=800></div>\n<div align=center><img src=\"https://i.ibb.co/KKtZ7Gn/Picture1-3d.png\" width=500></div>","metadata":{}},{"cell_type":"markdown","source":"## Reading TFRecord Data\n**What is TFRecord & Why use it for Segmentation?**\n\n* The `.tfrecord`/`.tfrec` format is TensorFlow's custom data format which is used for storing a sequence of binary records.\n* For **Segmentation** unlike any other data formatk in `.tfrecord` we don't have to the read file twice (one for image and one for mask). In `tfrecord` we just have to read file once and we can access both image and mask.\n* TFRecord consumes **less storage on disk**, and has **faster read and write time from the disk**, which makes it suitable for **segmentation** task.\n* Apart from that there are a number of advantages to using TFRecords: \n    * Efficient usage of storage.\n    * Better I/O Speed.\n    * TPUs require that you pass data to them in TFRecord format\n    \n**How TFRecord is created for Segmentation?**\n\n* Mask is stored in `tfrecord` exactly the same way as a image that is as a byte-string. \n* So, you can easily access the both image and mask from example_proto using `exmple[\"image\"]` & `example[\"mask\"]`. \n* Then, to decode it to `tf.Tensor` simply we can use `tf.io.decode_raw()` function.\n* For more information, checout [UWMGI: 2.5D TFRecord Data](https://www.kaggle.com/code/awsaf49/uwmgi-2-5d-tfrecord-data) notebook.","metadata":{}},{"cell_type":"code","source":"# Decode image from bytestring to tensor\ndef decode_image(data, height, width, target_size=CFG.img_size):\n    img = tf.io.decode_raw(data, out_type=tf.uint16)\n    img = tf.reshape(img, [height, width, 3])  # explicit size needed for TPU\n    img = tf.cast(img, tf.float32)\n    img = tf.math.divide_no_nan(img, tf.math.reduce_max(img))  # scale image to [0, 1]\n    img = tf.image.resize_with_pad(\n        img, target_size[0], target_size[1], method=\"nearest\"\n    )  # resize with pad to avoid distortion\n    img = tf.reshape(img, [*target_size, 3])  # reshape after resize\n    return img\n\n\n# Decode mask from bytestring to tensor\ndef decode_mask(data, height, width, target_size=CFG.img_size):\n    msk = tf.io.decode_raw(data, out_type=tf.uint8)\n    msk = tf.reshape(msk, [height, width, 3])  # explicit size needed for TPU\n    msk = tf.cast(msk, tf.float32)\n    msk = msk / 255.0  # scale mask data to[0, 1]\n    msk = tf.image.resize_with_pad(\n        msk, target_size[0], target_size[1], method=\"nearest\"\n    )\n    msk = tf.reshape(msk, [*target_size, 3])  # reshape after resize\n    return msk\n\n\n# Read tfrecord data & parse it & do augmentation\ndef read_tfrecord(example, augment=True, return_id=False, dim=CFG.img_size):\n    tfrec_format = {\n        \"id\": tf.io.FixedLenFeature([], tf.string),\n        \"image\": tf.io.FixedLenFeature([], tf.string),  # tf.string means bytestring\n        \"height\": tf.io.FixedLenFeature([], tf.int64),\n        \"width\": tf.io.FixedLenFeature([], tf.int64),\n        \"mask\": tf.io.FixedLenFeature([], tf.string),\n    }\n    example = tf.io.parse_single_example(\n        example, tfrec_format\n    )  # parses a single example proto.\n    image_id = example[\"id\"]\n    height = example[\"height\"]\n    width = example[\"width\"]\n    img = decode_image(example[\"image\"], height, width, dim)  # access image\n    msk = decode_mask(example[\"mask\"], height, width, dim)  # access mask\n    if augment:  # do augmentation\n        img, msk = ShiftScaleRotate(img, msk, DIM=dim, p=0.75)\n        img, msk = RandomFlip(img, msk, hflip_p=CFG.hflip, vflip_p=CFG.vflip)\n        img = RandomJitter(img, CFG.hue, CFG.sat, CFG.cont, CFG.bri, p=0.8)\n        img, msk = CutOut(\n            img,\n            msk,\n            DIM=dim,\n            PROBABILITY=CFG.drop_prob,\n            CT=CFG.drop_cnt,\n            SZ=CFG.drop_size,\n        )\n    img = tf.clip_by_value(img, 0, 1) if CFG.clip else img\n    img = tf.reshape(img, [*dim, 3])\n    msk = tf.reshape(msk, [*dim, 3])\n    return (img, msk) if not return_id else (img, image_id, msk)\n","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2022-05-19T11:12:02.47169Z","iopub.execute_input":"2022-05-19T11:12:02.472271Z","iopub.status.idle":"2022-05-19T11:12:02.490212Z","shell.execute_reply.started":"2022-05-19T11:12:02.47224Z","shell.execute_reply":"2022-05-19T11:12:02.489466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Pipeline with **tf.data**\n<div align=center> <img src=\"https://storage.googleapis.com/jalammar-ml/tf.data/images/tf.data.png\" width=700></div>\n\nTo build data pipeline using `tfrecrod/tfrec`, we need to use `tf.data` API.\n\n* We can build complex input pipelines from simple, reusable pieces using`tf.data` API . For example, the pipeline for an image model might aggregate data from files in a distributed file system, apply random transformation/augmentation to each image, and merge randomly selected images into a batch for training.\n* Moreover `tf.data` API provides a `tf.data.Dataset` feature that represents a sequence of components where each component comprises one or more pieces. For instances, in an image pipeline, an component might be a single training example, with a pair of tensor pieces representing the image and its label.\n\nCheckout this [doc](https://www.tensorflow.org/guide/data) if you want to learn more about `tf.data`.\n\n## Pipeline\n* Read **TFRecord** files.\n* `cache` data to speed up the training.\n* `repeat` the data stream (for training only & test-time augmentation).\n* `shuffle` the data (for training only).\n* Unparse **tfrecord** data & convert it to Image data from ByteString.\n* Process Image & Mask.\n* Apply Augmentations.\n* Batch Data.","metadata":{}},{"cell_type":"code","source":"def get_dataset(\n    filenames,\n    shuffle=True,\n    repeat=True,\n    augment=True,\n    cache=True,\n    return_id=False,\n    batch_size=CFG.batch_size * REPLICAS,\n    target_size=CFG.img_size,\n    drop_remainder=False,\n    seed=CFG.seed,\n):\n    dataset = tf.data.TFRecordDataset(\n        filenames, num_parallel_reads=AUTO\n    )  # read tfrecord files\n    if cache:\n        dataset = dataset.cache()  # cache data for speedup\n    if repeat:\n        dataset = dataset.repeat()  # repeat the data (for training only)\n    if shuffle:\n        dataset = dataset.shuffle(\n            1024, seed=seed\n        )  # shuffle the data (for training only)\n        options = tf.data.Options()\n        options.experimental_deterministic = (\n            False  # order won't be maintained when we shuffle\n        )\n        dataset = dataset.with_options(options)\n    dataset = dataset.map(\n        lambda x: read_tfrecord(\n            x,\n            augment=augment,  # unparse tfrecord data with masks\n            return_id=return_id,\n            dim=target_size,\n        ),\n        num_parallel_calls=AUTO,\n    )\n    dataset = dataset.batch(batch_size, drop_remainder=drop_remainder)  # batch the data\n    dataset = dataset.prefetch(AUTO)  # prefatch data for speedup\n    return dataset","metadata":{"execution":{"iopub.status.busy":"2022-05-19T11:12:02.492911Z","iopub.execute_input":"2022-05-19T11:12:02.493664Z","iopub.status.idle":"2022-05-19T11:12:02.505453Z","shell.execute_reply.started":"2022-05-19T11:12:02.493619Z","shell.execute_reply":"2022-05-19T11:12:02.504793Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 9. Visualization 🔭\nTo ensure our pipeline is generating **image** and **mask** correctly, we'll check some samples from a batch.","metadata":{}},{"cell_type":"code","source":"clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\ndef display_batch(batch, row=2, col=5):\n    imgs, msks = batch\n    plt.figure(figsize=(2.5*col, 2.5*row))\n    for idx in range(row*col):\n        ax = plt.subplot(row, col, idx+1)\n        img = imgs[idx].numpy()*255.0\n        img = img.astype('uint8')\n        for i in range(3):\n            img[...,i] = clahe.apply(img[...,i])\n        ax.imshow(img, cmap='bone')\n        msk = msks[idx].numpy()\n        ax.imshow(msk,alpha=0.4)\n        ax.set_xticks([])\n        ax.set_yticks([])\n    plt.tight_layout();\n    plt.show();","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-19T11:12:02.506991Z","iopub.execute_input":"2022-05-19T11:12:02.507584Z","iopub.status.idle":"2022-05-19T11:12:02.520763Z","shell.execute_reply.started":"2022-05-19T11:12:02.507544Z","shell.execute_reply":"2022-05-19T11:12:02.520012Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ds = get_dataset(ALL_FILENAMES[:2], augment=False, cache=False, repeat=False).take(1)\nbatch = next(iter(ds.unbatch().batch(20)))\nimg, msk = batch\nprint(f'image_shape: {img.shape} mask_shape:{msk.shape}')\nprint(f'image_dtype: {img.dtype} mask_dtype: {msk.dtype}')\ndisplay_batch(batch)","metadata":{"execution":{"iopub.status.busy":"2022-05-19T11:12:02.522423Z","iopub.execute_input":"2022-05-19T11:12:02.522854Z","iopub.status.idle":"2022-05-19T11:12:09.542523Z","shell.execute_reply.started":"2022-05-19T11:12:02.522819Z","shell.execute_reply":"2022-05-19T11:12:09.541908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 10. Loss Fn 📉\nSome implemented loss_functions are,\n* Dice Loss\n$$ \nDice = \\frac{2\\cdot{TP}}{2\\cdot{TP} + FP + FN}\n$$\n* Tversky Loss (Modified IoU Loss)\n$$ \nIoU = \\frac{TP}{TP + FP + FN}\n$$\n$$\nTversky = \\frac{TP}{TP + \\alpha\\cdot{FP} + \\beta\\cdot{FN}}\n$$\n* Focal Tversky Loss (Focal Loss + Tversky Loss)\n$$ \nFocalTversky = (1 - Tversky)^\\gamma\n$$","metadata":{}},{"cell_type":"code","source":"from segmentation_models.base import functional as F\nimport tensorflow.keras.backend as K\n\nkwargs = {}\nkwargs[\"backend\"] = K  # set tensorflow.keras as backend\n\n\ndef dice_coef(y_true, y_pred):\n    \"\"\"Dice coefficient\"\"\"\n    dice = F.f_score(\n        y_true,\n        y_pred,\n        beta=1,\n        smooth=1e-5,\n        per_image=False,\n        threshold=0.5,\n        **kwargs,\n    )\n    return dice\n\n\ndef tversky(y_true, y_pred, axis=(0, 1, 2), alpha=0.3, beta=0.7, smooth=0.0001):\n    \"Tversky metric\"\n    y_true = tf.cast(y_true, tf.float32)\n    y_pred = tf.cast(y_pred, tf.float32)\n    tp = tf.math.reduce_sum(y_true * y_pred, axis=axis) # calculate True Positive\n    fn = tf.math.reduce_sum(y_true * (1 - y_pred), axis=axis) # calculate False Negative\n    fp = tf.math.reduce_sum((1 - y_true) * y_pred, axis=axis) # calculate False Positive\n    tv = (tp + smooth) / (tp + alpha * fn + beta * fp + smooth) # calculate tversky\n    tv = tf.math.reduce_mean(tv)\n    return tv\n\n\ndef tversky_loss(y_true, y_pred):\n    \"Tversky Loss\"\n    return 1 - tversky(y_true, y_pred)\n\n\ndef focal_tversky_loss(y_true, y_pred, gamma=0.75):\n    \"Focal Tversky Loss: Focal Loss + Tversky Loss\"\n    tv = tversky(y_true, y_pred)\n    return K.pow((1 - tv), gamma)\n\n\n# Register custom objects\ncustom_objs = {\n    \"dice_loss\": sm.losses.dice_loss,\n    \"dice_coef\": dice_coef,\n    \"bce_dice_loss\": sm.losses.bce_dice_loss,\n    \"bce_jaccard_loss\": sm.losses.bce_jaccard_loss,\n    \"tversky_loss\": tversky_loss,\n    \"focal_tversky_loss\": focal_tversky_loss,\n    \"jaccard_loss\": sm.losses.jaccard_loss,\n    \"precision\": sm.metrics.precision,\n    \"recall\": sm.metrics.recall,\n}\ntf.keras.utils.get_custom_objects().update(custom_objs)","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2022-05-19T11:12:09.543636Z","iopub.execute_input":"2022-05-19T11:12:09.544177Z","iopub.status.idle":"2022-05-19T11:12:09.557672Z","shell.execute_reply.started":"2022-05-19T11:12:09.544142Z","shell.execute_reply":"2022-05-19T11:12:09.557036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 11. LR Schedule ⚓\n* Learning Rate scheduler for transfer learning. \n* The learning rate starts from `initial_learning_rate`, then decreases to a`minimum_learning_rate` using different methods namely,\n    * **ReduceLROnPlateau**: Reduce lr when score isn't improving.\n    * **CosineDecay**: Follow Cosine graph to reduce lr.\n    * **ExponentialDecay**: Reduce lr exponentially.","metadata":{}},{"cell_type":"code","source":"def get_lr_callback():\n    if CFG.lr_schedule == \"ReduceLROnPlateau\":\n        lr_schedule = tf.keras.callbacks.ReduceLROnPlateau(\n            monitor=\"val_loss\",\n            factor=0.1,\n            patience=int(CFG.patience / 2),\n            min_lr=CFG.lr / 1e2,\n        )\n    elif CFG.lr_schedule == \"CosineDecay\":\n        lr_schedule = tf.keras.experimental.CosineDecay(\n            initial_learning_rate=CFG.lr, decay_steps=CFG.epochs + 2, alpha=CFG.lr / 1e2\n        )\n        lr_schedule = tf.keras.callbacks.LearningRateScheduler(lr_schedule, verbose=0)\n    elif CFG.lr_schedule == \"ExponentialDecay\":\n        lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n            initial_learning_rate=CFG.lr,\n            decay_steps=CFG.epochs + 2,\n            decay_rate=0.05,\n            staircase=False,\n        )\n        lr_schedule = tf.keras.callbacks.LearningRateScheduler(lr_schedule, verbose=0)\n    return lr_schedule","metadata":{"execution":{"iopub.status.busy":"2022-05-19T11:12:09.558828Z","iopub.execute_input":"2022-05-19T11:12:09.559191Z","iopub.status.idle":"2022-05-19T11:12:09.576513Z","shell.execute_reply.started":"2022-05-19T11:12:09.559148Z","shell.execute_reply":"2022-05-19T11:12:09.57588Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 12. TransUnet 🤖\n> [TransUNet: Transformers Make Strong Encoders for Medical Image Segmentation](https://arxiv.org/pdf/2102.04306.pdf)\n\nIn a nutshell, this work shows how to convert a **UNet** into a so-called **TransUNet** by using a visual transformer (ViT) network in the encoder. Details of the architecture are in Figure below. As opposed to other methods which use a pure **transformer-based encoder** to convert the input image into a latent vector. A series of convolutions (much like in the original UNet) is used to convert the input image into a set of lower-resolution feature maps which are then encode with a ViT.\nSo, main components fo **TransUNet** are,\n\n1. Encoder (Pure or Hybrid)\n    * CNN\n    * Transformer\n2. Decoder\n    * CNN\n3. Skip Connection in Hybrid\n    * Connection betwween CNN Encoder & CNN Decoder\n\n> **Codes below are adapted from [here](https://github.com/kenza-bouzid/TransUnet)**\n\n<img src=\"https://production-media.paperswithcode.com/social-images/hfPJrzzvUuaeIMvb.png\" width=800>","metadata":{}},{"cell_type":"markdown","source":"## Utils\n* Utility code to load `imagenet` weights","metadata":{}},{"cell_type":"code","source":"def apply_embedding_weights(target_layer, source_weights):\n    \"\"\"Apply embedding weights to a target layer.\n\n    Args:\n        target_layer: The target layer to which weights will\n            be applied.\n        source_weights: The source weights, which will be\n            resized as necessary.\n    \"\"\"\n    expected_shape = target_layer.weights[0].shape\n    if expected_shape == source_weights.shape:\n        grid = source_weights\n    elif expected_shape[1] == source_weights.shape[1] - 1:\n        grid = source_weights[:, 1:]\n    else:\n        _, grid = source_weights[0, :1], source_weights[0, 1:]\n        sin = int(np.sqrt(grid.shape[0]))\n        sout = int(np.sqrt(expected_shape[1]))\n        warnings.warn(\n            \"Resizing position embeddings from \" f\"{sin} to {sout}\",\n            UserWarning,\n        )\n        zoom = (sout / sin, sout / sin, 1)\n        grid = scipy.ndimage.zoom(grid.reshape(sin, sin, -1), zoom, order=1).reshape(\n            1, sout * sout, -1\n        )\n    target_layer.set_weights([grid])\n\n\ndef load_weights_numpy(model, params_path):\n    \"\"\"Load weights saved using Flax as a numpy array.\n\n    Args:\n        model: A Keras model to load the weights into.\n        params_path: Filepath to a numpy archive.\n    \"\"\"\n    params_dict = np.load(\n        params_path, allow_pickle=False\n    )  # pylint: disable=unexpected-keyword-arg\n    source_keys = list(params_dict.keys())\n\n    source_keys_used = []\n    n_transformers = len(\n        set(\n            \"/\".join(k.split(\"/\")[:2])\n            for k in source_keys\n            if k.startswith(\"Transformer/encoderblock_\")\n        )\n    )\n    n_transformers_out = sum(\n        l.name.startswith(\"Transformer/encoderblock_\") for l in model.layers\n    )\n    assert n_transformers == n_transformers_out, (\n        f\"Wrong number of transformers (\"\n        f\"{n_transformers_out} in model vs. {n_transformers} in weights).\"\n    )\n\n    matches = []\n    for tidx in range(n_transformers):\n        encoder = model.get_layer(f\"Transformer/encoderblock_{tidx}\")\n        source_prefix = f\"Transformer/encoderblock_{tidx}\"\n        matches.extend(\n            [\n                {\n                    \"layer\": layer,\n                    \"keys\": [\n                        f\"{source_prefix}/{norm}/{name}\" for name in [\"scale\", \"bias\"]\n                    ],\n                }\n                for norm, layer in [\n                    (\"LayerNorm_0\", encoder.layernorm1),\n                    (\"LayerNorm_2\", encoder.layernorm2),\n                ]\n            ]\n            + [\n                {\n                    \"layer\": encoder.mlpblock.get_layer(\n                        f\"{source_prefix}/Dense_{mlpdense}\"\n                    ),\n                    \"keys\": [\n                        f\"{source_prefix}/MlpBlock_3/Dense_{mlpdense}/{name}\"\n                        for name in [\"kernel\", \"bias\"]\n                    ],\n                }\n                for mlpdense in [0, 1]\n            ]\n            + [\n                {\n                    \"layer\": layer,\n                    \"keys\": [\n                        f\"{source_prefix}/MultiHeadDotProductAttention_1/{attvar}/{name}\"\n                        for name in [\"kernel\", \"bias\"]\n                    ],\n                    \"reshape\": True,\n                }\n                for attvar, layer in [\n                    (\"query\", encoder.att.query_dense),\n                    (\"key\", encoder.att.key_dense),\n                    (\"value\", encoder.att.value_dense),\n                    (\"out\", encoder.att.combine_heads),\n                ]\n            ]\n        )\n\n    # Embedding kernel and bias\n    matches.append(\n        {\n            \"layer\": model.get_layer(\"embedding\"),\n            \"keys\": [f\"embedding/{name}\" for name in [\"kernel\", \"bias\"]],\n        }\n    )\n\n    matches.append(\n        {\n            \"layer\": model.get_layer(\"Transformer/encoder_norm\"),\n            \"keys\": [f\"Transformer/encoder_norm/{name}\" for name in [\"scale\", \"bias\"]],\n        }\n    )\n    apply_embedding_weights(\n        target_layer=model.get_layer(\"Transformer/posembed_input\"),\n        source_weights=params_dict[\"Transformer/posembed_input/pos_embedding\"],\n    )\n    source_keys_used.append(\"Transformer/posembed_input/pos_embedding\")\n    for match in matches:\n        source_keys_used.extend(match[\"keys\"])\n        source_weights = [params_dict[k] for k in match[\"keys\"]]\n        if match.get(\"reshape\", False):\n            source_weights = [\n                source.reshape(expected.shape)\n                for source, expected in zip(\n                    source_weights, match[\"layer\"].get_weights()\n                )\n            ]\n        match[\"layer\"].set_weights(source_weights)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-19T11:12:09.577947Z","iopub.execute_input":"2022-05-19T11:12:09.578416Z","iopub.status.idle":"2022-05-19T11:12:09.602285Z","shell.execute_reply.started":"2022-05-19T11:12:09.578375Z","shell.execute_reply":"2022-05-19T11:12:09.601653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Encoder\nKey components of Encoder are,\n* AddPositionEmbs\n* MultiHeadSelfAttention\n* TransformerBlock\n* ResNet_Embeddings","metadata":{}},{"cell_type":"code","source":"class AddPositionEmbs(tfkl.Layer):\n    \"\"\"Adds (optionally learned) positional embeddings to the inputs.\"\"\"\n\n    def __init__(self, trainable=True, **kwargs):\n        super().__init__(trainable=trainable, **kwargs)\n        self.trainable = trainable\n\n    def build(self, input_shape):\n        assert (\n            len(input_shape) == 3\n        ), f\"Number of dimensions should be 3, got {len(input_shape)}\"\n        self.pe = tf.Variable(\n            name=\"pos_embedding\",\n            initial_value=tf.random_normal_initializer(stddev=0.06)(\n                shape=(1, input_shape[1], input_shape[2])\n            ),\n            dtype=\"float32\",\n            trainable=self.trainable,\n        )\n\n    def call(self, inputs):\n        return inputs + tf.cast(self.pe, dtype=inputs.dtype)\n\n\nclass MultiHeadSelfAttention(tfkl.Layer):\n    def __init__(self, *args, trainable=True, n_heads, **kwargs):\n        super().__init__(trainable=trainable, *args, **kwargs)\n        self.n_heads = n_heads\n\n    def build(self, input_shape):\n        hidden_size = input_shape[-1]\n        n_heads = self.n_heads\n        if hidden_size % n_heads != 0:\n            raise ValueError(\n                f\"embedding dimension = {hidden_size} should be divisible by number of heads = {n_heads}\"\n            )\n        self.hidden_size = hidden_size\n        self.projection_dim = hidden_size // n_heads\n        self.query_dense = tfkl.Dense(hidden_size, name=\"query\")\n        self.key_dense = tfkl.Dense(hidden_size, name=\"key\")\n        self.value_dense = tfkl.Dense(hidden_size, name=\"value\")\n        self.combine_heads = tfkl.Dense(hidden_size, name=\"out\")\n\n    # pylint: disable=no-self-use\n    def attention(self, query, key, value):\n        score = tf.matmul(query, key, transpose_b=True)\n        dim_key = tf.cast(tf.shape(key)[-1], score.dtype)\n        scaled_score = score / tf.math.sqrt(dim_key)\n        weights = tf.nn.softmax(scaled_score, axis=-1)\n        output = tf.matmul(weights, value)\n        return output, weights\n\n    def separate_heads(self, x, batch_size):\n        x = tf.reshape(x, (batch_size, -1, self.n_heads, self.projection_dim))\n        return tf.transpose(x, perm=[0, 2, 1, 3])\n\n    def call(self, inputs):\n        batch_size = tf.shape(inputs)[0]\n        query = self.query_dense(inputs)\n        key = self.key_dense(inputs)\n        value = self.value_dense(inputs)\n        query = self.separate_heads(query, batch_size)\n        key = self.separate_heads(key, batch_size)\n        value = self.separate_heads(value, batch_size)\n\n        attention, weights = self.attention(query, key, value)\n        attention = tf.transpose(attention, perm=[0, 2, 1, 3])\n        concat_attention = tf.reshape(attention, (batch_size, -1, self.hidden_size))\n        output = self.combine_heads(concat_attention)\n        return output, weights\n\n\nclass TransformerBlock(tfkl.Layer):\n    \"\"\"Implements a Transformer block.\"\"\"\n\n    def __init__(self, *args, n_heads, mlp_dim, dropout, trainable=True, **kwargs):\n        super().__init__(*args, trainable=trainable, **kwargs)\n        self.n_heads = n_heads\n        self.mlp_dim = mlp_dim\n        self.dropout = dropout\n\n    def build(self, input_shape):\n        self.att = MultiHeadSelfAttention(\n            n_heads=self.n_heads,\n            name=\"MultiHeadDotProductAttention_1\",\n        )\n        self.mlpblock = tfk.Sequential(\n            [\n                tfkl.Dense(\n                    self.mlp_dim, activation=\"linear\", name=f\"{self.name}/Dense_0\"\n                ),\n                tfkl.Lambda(lambda x: tfk.activations.gelu(x, approximate=False))\n                if hasattr(tfk.activations, \"gelu\")\n                else tfkl.Lambda(lambda x: tfa.activations.gelu(x, approximate=False)),\n                tfkl.Dropout(self.dropout),\n                tfkl.Dense(input_shape[-1], name=f\"{self.name}/Dense_1\"),\n                tfkl.Dropout(self.dropout),\n            ],\n            name=\"MlpBlock_3\",\n        )\n        self.layernorm1 = tfkl.LayerNormalization(epsilon=1e-6, name=\"LayerNorm_0\")\n        self.layernorm2 = tfkl.LayerNormalization(epsilon=1e-6, name=\"LayerNorm_2\")\n        self.dropout = tfkl.Dropout(self.dropout)\n\n    def call(self, inputs, training):\n        x = self.layernorm1(inputs)\n        x, weights = self.att(x)\n        x = self.dropout(x, training=training)\n        x = x + inputs\n        y = self.layernorm2(x)\n        y = self.mlpblock(y)\n        return x + y, weights\n\n\ndef resnet_embeddings(x, image_size=224, n_skip=3):\n    \"\"\"Get resnet embeddings for Decoder\"\"\"\n    resnet50v2 = tfk.applications.ResNet50V2(\n        weights=\"imagenet\", include_top=False, input_shape=(image_size, image_size, 3)\n    )\n    _ = resnet50v2(x)\n    layers = [\"conv3_block4_preact_relu\", \"conv2_block3_preact_relu\", \"conv1_conv\"]\n    features = []\n    if n_skip > 0:\n        for l in layers:\n            features.append(resnet50v2.get_layer(l).output)\n    return resnet50v2, features","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-19T11:12:09.603632Z","iopub.execute_input":"2022-05-19T11:12:09.604102Z","iopub.status.idle":"2022-05-19T11:12:09.637321Z","shell.execute_reply.started":"2022-05-19T11:12:09.60406Z","shell.execute_reply":"2022-05-19T11:12:09.636538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Decoder\nKey components of Decoders are,\n* SegmentationHead\n* Conv2DReLu\n* DecoderBlock\n* DecoderCup","metadata":{}},{"cell_type":"code","source":"L2_WEIGHT_DECAY = 1e-4\n\n\nclass SegmentationHead(tfkl.Layer):\n    def __init__(\n        self,\n        name=\"seg_head\",\n        num_classes=9,\n        kernel_size=1,\n        final_act=\"sigmoid\",\n        **kwargs\n    ):\n        super(SegmentationHead, self).__init__(name=name, **kwargs)\n        self.num_classes = num_classes\n        self.kernel_size = kernel_size\n        self.final_act = final_act\n\n    def build(self, input_shape):\n        self.conv = tfkl.Conv2D(\n            filters=self.num_classes,\n            kernel_size=self.kernel_size,\n            padding=\"same\",\n            kernel_regularizer=tfk.regularizers.L2(L2_WEIGHT_DECAY),\n            kernel_initializer=tfk.initializers.LecunNormal(),\n        )\n        self.act = tfkl.Activation(self.final_act)\n\n    def call(self, inputs):\n        x = self.conv(inputs)\n        x = self.act(x)\n        return x\n\n\nclass Conv2DReLu(tfkl.Layer):\n    def __init__(self, filters, kernel_size, padding=\"same\", strides=1, **kwargs):\n        super().__init__(**kwargs)\n        self.filters = filters\n        self.kernel_size = kernel_size\n        self.padding = padding\n        self.strides = strides\n\n    def build(self, input_shape):\n        self.conv = tfkl.Conv2D(\n            filters=self.filters,\n            kernel_size=self.kernel_size,\n            strides=self.strides,\n            padding=self.padding,\n            use_bias=False,\n            kernel_regularizer=tfk.regularizers.L2(L2_WEIGHT_DECAY),\n            kernel_initializer=\"lecun_normal\",\n        )\n\n        self.bn = tfkl.BatchNormalization(momentum=0.9, epsilon=1e-5)\n\n    def call(self, inputs):\n        x = self.conv(inputs)\n        x = self.bn(x)\n        x = tf.nn.relu(x)\n        return x\n\n\nclass DecoderBlock(tfkl.Layer):\n    def __init__(self, filters, **kwargs):\n        super().__init__(**kwargs)\n        self.filters = filters\n\n    def build(self, input_shape):\n        self.conv1 = Conv2DReLu(filters=self.filters, kernel_size=3)\n        self.conv2 = Conv2DReLu(filters=self.filters, kernel_size=3)\n        self.upsampling = tfkl.UpSampling2D(size=2, interpolation=\"bilinear\")\n\n    def call(self, inputs, skip=None):\n        x = self.upsampling(inputs)\n        if skip is not None:\n            x = tf.concat([x, skip], axis=-1)\n        x = self.conv1(x)\n        x = self.conv2(x)\n        return x\n\n\nclass DecoderCup(tfkl.Layer):\n    def __init__(self, decoder_channels, n_skip=3, **kwargs):\n        super().__init__(**kwargs)\n        self.decoder_channels = decoder_channels\n        self.n_skip = n_skip\n\n    def build(self, input_shape):\n        self.conv_more = Conv2DReLu(filters=512, kernel_size=3)\n        self.blocks = [DecoderBlock(filters=out_ch) for out_ch in self.decoder_channels]\n\n    def call(self, hidden_states, features):\n        x = self.conv_more(hidden_states)\n        for i, decoder_block in enumerate(self.blocks):\n            if features is not None:\n                skip = features[i] if (i < self.n_skip) else None\n            else:\n                skip = None\n            x = decoder_block(x, skip=skip)\n        return x\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-19T11:12:09.638667Z","iopub.execute_input":"2022-05-19T11:12:09.638887Z","iopub.status.idle":"2022-05-19T11:12:09.660184Z","shell.execute_reply.started":"2022-05-19T11:12:09.638862Z","shell.execute_reply":"2022-05-19T11:12:09.659403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model\n* Merge Encoder & Decoder part of TransUNet Model.\n* You can also ditch so many lines of code above for **TransUNet** model and load model directly using simple two lines of code,\n```py\nfrom transunet import TransUNet\nmodel = TransUNet(image_size=224, pretrain=True)\n```","metadata":{}},{"cell_type":"code","source":"MODELS_URL = \"https://storage.googleapis.com/vit_models/imagenet21k/\"\n\n\ndef load_pretrained(model, fname=\"R50+ViT-B_16.npz\"):\n    \"\"\"Load model weights for a known configuration.\"\"\"\n    origin = MODELS_URL + fname\n    local_filepath = tf.keras.utils.get_file(fname, origin, cache_subdir=\"weights\")\n    load_weights_numpy(model, local_filepath)\n\n\ndef TransUNet(\n    image_size=224,\n    patch_size=16,\n    hybrid=True,\n    grid=(14, 14),\n    resnet_n_layers=(3, 4, 9),\n    hidden_size=768,\n    n_layers=12,\n    n_heads=12,\n    mlp_dim=3072,\n    dropout=0.1,\n    decoder_channels=[256, 128, 64, 16],\n    n_skip=3,\n    num_classes=3,\n    final_act=\"sigmoid\",\n    pretrain=True,\n    freeze_enc_cnn=True,\n    name=\"TransUNet\",\n):\n    # Tranformer Encoder\n    assert image_size % patch_size == 0, \"image_size must be a multiple of patch_size\"\n    x = tf.keras.layers.Input(shape=(image_size, image_size, 3))\n\n    #  CNN + Transformer\n    if hybrid:\n        grid_size = grid\n        patch_size = image_size // 16 // grid_size[0]\n        if patch_size == 0:\n            patch_size = 1\n        resnet50v2, features = resnet_embeddings(\n            x, image_size=image_size, n_skip=n_skip\n        )\n        if freeze_enc_cnn:\n            resnet50v2.trainable = False\n        y = resnet50v2.get_layer(\"conv4_block6_preact_relu\").output\n        x = resnet50v2.input\n    else:\n        y = x\n        features = None\n\n    y = tfkl.Conv2D(\n        filters=hidden_size,\n        kernel_size=patch_size,\n        strides=patch_size,\n        padding=\"valid\",\n        name=\"embedding\",\n        trainable=not freeze_enc_cnn,\n    )(y)\n    y = tfkl.Reshape((y.shape[1] * y.shape[2], hidden_size))(y)\n    y = AddPositionEmbs(name=\"Transformer/posembed_input\", trainable=True)(y)\n\n    y = tfkl.Dropout(0.1)(y)\n\n    # Transformer/Encoder\n    for n in range(n_layers):\n        y, _ = TransformerBlock(\n            n_heads=n_heads,\n            mlp_dim=mlp_dim,\n            dropout=dropout,\n            name=f\"Transformer/encoderblock_{n}\",\n            trainable=True,\n        )(y)\n    y = tfkl.LayerNormalization(epsilon=1e-6, name=\"Transformer/encoder_norm\")(y)\n\n    n_patch_sqrt = int(math.sqrt(y.shape[1]))\n\n    y = tfkl.Reshape(target_shape=[n_patch_sqrt, n_patch_sqrt, hidden_size])(y)\n\n    # Decoder\n    if len(decoder_channels):\n        y = DecoderCup(decoder_channels=decoder_channels, n_skip=n_skip)(y, features)\n\n    # Segmentation Head\n    y = SegmentationHead(num_classes=num_classes, final_act=final_act)(y)\n\n    # Build Model\n    model = tfk.models.Model(inputs=x, outputs=y, name=name)\n\n    # Load Pretrain Weights\n    if pretrain:\n        load_pretrained(model)\n\n    return model","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-19T11:12:09.661704Z","iopub.execute_input":"2022-05-19T11:12:09.662027Z","iopub.status.idle":"2022-05-19T11:12:09.68215Z","shell.execute_reply.started":"2022-05-19T11:12:09.661989Z","shell.execute_reply":"2022-05-19T11:12:09.681556Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Build Model\n* Build complete model.\n* Select Loss, LR_Scheduling, Metrics and so on.\n* Compile model for training.","metadata":{}},{"cell_type":"code","source":"def get_model(name=CFG.model_name, loss=CFG.loss, backbone=CFG.backbone):\n    model = TransUNet(image_size=CFG.img_size[0], freeze_enc_cnn=False, pretrain=True)\n\n    lr = CFG.lr\n    if CFG.optimizer == \"Adam\":\n        opt = tf.keras.optimizers.Adam(learning_rate=lr)\n    elif CFG.optimizer == \"AdamW\":\n        opt = tfa.optimizers.AdamW(learning_rate=lr, weight_decay=lr)\n    elif CFG.optimizer == \"RectifiedAdam\":\n        opt = tfa.optimizers.RectifiedAdam(learning_rate=lr)\n    else:\n        raise ValueError(\"Wrong Optimzer Name\")\n\n    model.compile(\n        optimizer=opt,\n        loss=loss,\n        steps_per_execution=CFG.steps_per_execution, # to reduce idle time\n        metrics=[\n            dice_coef,\n            \"precision\",\n            \"recall\",\n        ],\n    )\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-05-19T11:12:09.683119Z","iopub.execute_input":"2022-05-19T11:12:09.683426Z","iopub.status.idle":"2022-05-19T11:12:09.696917Z","shell.execute_reply.started":"2022-05-19T11:12:09.683401Z","shell.execute_reply":"2022-05-19T11:12:09.696221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = get_model()\nmodel.summary()","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-05-19T11:12:09.697782Z","iopub.execute_input":"2022-05-19T11:12:09.698284Z","iopub.status.idle":"2022-05-19T11:12:23.799033Z","shell.execute_reply.started":"2022-05-19T11:12:09.698244Z","shell.execute_reply":"2022-05-19T11:12:23.798291Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 13. WandB 🪄\n<img src=\"https://camo.githubusercontent.com/dd842f7b0be57140e68b2ab9cb007992acd131c48284eaf6b1aca758bfea358b/68747470733a2f2f692e696d6775722e636f6d2f52557469567a482e706e67\" width=\"400\" alt=\"Weights & Biases\" />\n\nTo track model's training I'll be using **Weights & Biases** tool. Weights & Biases (W&B) is MLOps platform for tracking our experiemnts. We can use it to Build better models faster with experiment tracking, dataset versioning, and model management.","metadata":{}},{"cell_type":"code","source":"if CFG.wandb:\n    \"login in wandb otherwise run anonymously\"\n    try:\n        # Addo-ons > Secrets > WANDB\n        from kaggle_secrets import UserSecretsClient\n        user_secrets = UserSecretsClient()\n        api_key = user_secrets.get_secret(\"WANDB\")\n        wandb.login(key=api_key)\n        anonymous = None\n    except:\n        anonymous = \"must\"\n\n\ndef wandb_init(fold):\n    \"initialize project on wandb\"\n    id_ = wandb.util.generate_id() # generate random id\n    config = {k: v for k, v in dict(vars(CFG)).items() if \"__\" not in k} # convert class to dict\n    config[\"id\"] = id_\n    config[\"fold\"] = int(fold) # np to python\n    run = wandb.init(\n        id=id_,\n        project=\"uwmgi-tf\",\n        name=f\"fold-{fold}|dim-{CFG.img_size[0]}x{CFG.img_size[1]}|model-{CFG.model_name}|backbone-{CFG.backbone}\",\n        config=config,\n        anonymous=anonymous,\n        group=CFG.comment,\n        reinit=True,\n        resume=\"allow\",\n    )\n    return run","metadata":{"execution":{"iopub.status.busy":"2022-05-19T11:12:23.800478Z","iopub.execute_input":"2022-05-19T11:12:23.800799Z","iopub.status.idle":"2022-05-19T11:12:25.1732Z","shell.execute_reply.started":"2022-05-19T11:12:23.80076Z","shell.execute_reply":"2022-05-19T11:12:25.172449Z"},"_kg_hide-input":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 14. Training 🚅\nOur model will be trained for the number of `FOLDS` and `EPOCHS` you chose in the configuration above. Each fold the model with hightest validation `Dice Score` will be saved and used to predict OOF and test. ","metadata":{}},{"cell_type":"code","source":"M = {}\n# Which Metrics to store\nmetrics = [\n    \"loss\",\n    \"dice_coef\",\n    \"precision\",\n    \"recall\",\n]\n# Intialize Metrics\nfor fm in metrics:\n    M[\"val_\" + fm] = []\n\nALL_FILENAMES = sorted(ALL_FILENAMES)\n\n# Split tfrecord using KFold\nkf = KFold(n_splits=CFG.folds, shuffle=True, random_state=CFG.seed) # kfold between trrecord files\nfor fold, (train_idx, valid_idx) in enumerate(kf.split(ALL_FILENAMES)):\n    # If fold is not in selected folds then avoid that fold\n    if fold not in CFG.selected_folds:\n        continue\n        \n    # Initialize wandb Run\n    if CFG.wandb:\n        run = wandb_init(fold)\n        WandbCallback = wandb.keras.WandbCallback(save_model=False)\n\n    # Train and validation files\n    TRAIN_FILENAMES = [ALL_FILENAMES[i] for i in train_idx]\n    VALID_FILENAMES = [ALL_FILENAMES[i] for i in valid_idx]\n    \n    # Take Only 10 Files if run in Debug Mode\n    if CFG.debug:\n        TRAIN_FILENAMES = TRAIN_FILENAMES[:10]\n        VALID_FILENAMES = VALID_FILENAMES[:10]\n\n    # Shuffle train files\n    random.shuffle(TRAIN_FILENAMES)\n\n    # Count train and valid samples\n    NUM_TRAIN = count_data_items(TRAIN_FILENAMES)\n    NUM_VALID = count_data_items(VALID_FILENAMES)\n\n    # Compute batch size & steps_per_epoch\n    BATCH_SIZE = CFG.batch_size * REPLICAS\n    STEPS_PER_EPOCH = NUM_TRAIN // BATCH_SIZE\n\n    print(\"#\" * 65)\n    print(\"#### FOLD:\", fold)\n    print(\n        \"#### IMAGE_SIZE: (%i, %i) | BATCH_SIZE: %i | EPOCHS: %i\"\n        % (CFG.img_size[0], CFG.img_size[1], BATCH_SIZE, CFG.epochs)\n    )\n    print(\n        \"#### MODEL: %s | BACKBONE: %s | LOSS: %s\"\n        % (CFG.model_name, CFG.backbone, CFG.loss)\n    )\n    print(\"#### NUM_TRAIN: {:,} | NUM_VALID: {:,}\".format(NUM_TRAIN, NUM_VALID))\n    print(\"#\" * 65)\n\n    # Log in w&B before training\n    if CFG.wandb:\n        wandb.log(\n            {\n                \"num_train\": NUM_TRAIN,\n                \"num_valid\": NUM_VALID,\n            }\n        )\n\n    # Build model in device\n    K.clear_session()\n    with strategy.scope():\n        model = get_model(name=CFG.model_name, backbone=CFG.backbone, loss=CFG.loss)\n\n    # Callbacks\n    checkpoint = tf.keras.callbacks.ModelCheckpoint(\n        \"/kaggle/working/fold-%i.h5\" % fold,\n        verbose=CFG.verbose,\n        monitor=\"val_dice_coef\",\n        mode=\"max\",\n        save_best_only=True,\n        save_weights_only=True,\n    )\n    callbacks = [checkpoint, get_lr_callback()]\n\n    if CFG.wandb:\n        # Include w&b callback if WANDB is True\n        callbacks.append(WandbCallback)\n\n    # Create train & valid dataset\n    train_ds = get_dataset(\n        TRAIN_FILENAMES,\n        augment=CFG.augment,\n        batch_size=BATCH_SIZE,\n        cache=False,\n        drop_remainder=False,\n    )\n    valid_ds = get_dataset(\n        VALID_FILENAMES,\n        shuffle=False,\n        augment=False,\n        repeat=False,\n        batch_size=BATCH_SIZE,\n        cache=False,\n        drop_remainder=False,\n    )\n\n    # Train model\n    history = model.fit(\n        train_ds,\n        epochs=CFG.epochs if not CFG.debug else 2,\n        steps_per_epoch=STEPS_PER_EPOCH,\n        callbacks=callbacks,\n        validation_data=valid_ds,\n        #         validation_steps = NUM_VALID/BATCH_SIZE,\n        verbose=CFG.verbose,\n    )\n\n    # Convert dict history to df history\n    history = pd.DataFrame(history.history)\n\n    # Load best weights\n    model.load_weights(\"/kaggle/working/fold-%i.h5\" % fold)\n\n    # Compute & save best valid result\n    print(\"\\nValid Result:\")\n    m = model.evaluate(\n        get_dataset(\n            VALID_FILENAMES,\n            batch_size=BATCH_SIZE,\n            augment=False,\n            shuffle=False,\n            repeat=False,\n            cache=False,\n        ),\n        return_dict=True,\n#        steps=NUM_VALID/BATCH_SIZE,\n        verbose=1,\n    )\n    print()\n    \n    # Store valid results\n    for fm in metrics:\n        M[\"val_\" + fm].append(m[fm])\n        \n    # Log in wandb\n    if CFG.wandb:\n        m[\"epoch\"] = np.argmax(history[\"val_dice_coef\"]) + 1\n        wandb.log({\"best\": m})\n        wandb.run.finish()\n\n    # Plot Training History\n    if CFG.display_plot:\n        plt.figure(figsize=(15, 5))\n        plt.plot(\n            np.arange(len(history[\"dice_coef\"])),\n            history[\"dice_coef\"],\n            \"-o\",\n            label=\"Train Dice\",\n            color=\"#ff7f0e\",\n        )\n        plt.plot(\n            np.arange(len(history[\"dice_coef\"])),\n            history[\"val_dice_coef\"],\n            \"-o\",\n            label=\"Val Dice\",\n            color=\"#1f77b4\",\n        )\n        x = np.argmax(history[\"val_dice_coef\"])\n        y = np.max(history[\"val_dice_coef\"])\n        xdist = plt.xlim()[1] - plt.xlim()[0]\n        ydist = plt.ylim()[1] - plt.ylim()[0]\n        plt.scatter(x, y, s=200, color=\"#1f77b4\")\n        plt.text(x - 0.03 * xdist, y - 0.13 * ydist, \"max dice\\n%.2f\" % y, size=14)\n        plt.ylabel(\"dice_coef\", size=14)\n        plt.xlabel(\"Epoch\", size=14)\n        plt.legend(loc=2)\n        plt2 = plt.gca().twinx()\n        plt2.plot(\n            np.arange(len(history[\"dice_coef\"])),\n            history[\"loss\"],\n            \"-o\",\n            label=\"Train Loss\",\n            color=\"#2ca02c\",\n        )\n        plt2.plot(\n            np.arange(len(history[\"dice_coef\"])),\n            history[\"val_loss\"],\n            \"-o\",\n            label=\"Val Loss\",\n            color=\"#d62728\",\n        )\n        x = np.argmin(history[\"val_loss\"])\n        y = np.min(history[\"val_loss\"])\n        ydist = plt.ylim()[1] - plt.ylim()[0]\n        plt.scatter(x, y, s=200, color=\"#d62728\")\n        plt.text(x - 0.03 * xdist, y + 0.05 * ydist, \"min loss\", size=14)\n        plt.ylabel(\"Loss\", size=14)\n        plt.title(\"FOLD %i\" % (fold), size=18)\n        plt.legend(loc=3)\n        plt.savefig(f\"fig-{fold}.png\")\n        plt.show()","metadata":{"_kg_hide-input":true,"_kg_hide-output":false,"execution":{"iopub.status.busy":"2022-05-19T11:12:25.174634Z","iopub.execute_input":"2022-05-19T11:12:25.174971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training Log\n### [Click Here ➡️](https://wandb.ai/awsaf49/uwmgi-tf) to check training log in **WandB** dashboard.\n\n<img src=\"https://i.ibb.co/V3XGd4r/wandb-dashboard.png\">","metadata":{}},{"cell_type":"markdown","source":"# 15. Calculate OOF 👀\nLet's check our average score across all folds. This will help us compare our model's performance.","metadata":{}},{"cell_type":"code","source":"# Save Metrics\nM['datetime'] = str(datetime.now())\nfor fm in metrics:\n    M['oof_'+fm] = np.mean(M['val_'+fm])\n    print('OOF '+ fm + ': '+ str(M['oof_'+fm]))\nwith open('metrics.json', 'w') as outfile:\n    json.dump(M, outfile)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 16. Reference 💡\n* [Triple Stratified KFold with TFRecords](https://www.kaggle.com/cdeotte/triple-stratified-kfold-with-tfrecords) by [Chris Deotte](https://www.kaggle.com/cdeotte)\n* [TransUNet](https://github.com/Beckschen/TransUNet)(Official)\n* [TransUnet](https://github.com/kenza-bouzid/TransUnet)(Keras)","metadata":{}},{"cell_type":"markdown","source":"# 17. Remove Files ✂️","metadata":{}},{"cell_type":"code","source":"import shutil\ntry:\n    !rm -r ./wandb\nexcept:\n    pass","metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]}]}