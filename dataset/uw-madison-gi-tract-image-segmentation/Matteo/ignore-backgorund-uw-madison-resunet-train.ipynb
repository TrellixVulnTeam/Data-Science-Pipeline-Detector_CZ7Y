{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n'''import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))'''\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-06-12T16:31:11.107913Z","iopub.execute_input":"2022-06-12T16:31:11.108319Z","iopub.status.idle":"2022-06-12T16:31:11.136242Z","shell.execute_reply.started":"2022-06-12T16:31:11.108255Z","shell.execute_reply":"2022-06-12T16:31:11.135572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install -q timm\n!pip install -q tqdm\n!pip install -q pytorch-lightning\n!pip install -q segmentation-models-pytorch","metadata":{"execution":{"iopub.status.busy":"2022-06-12T16:31:11.16169Z","iopub.execute_input":"2022-06-12T16:31:11.162045Z","iopub.status.idle":"2022-06-12T16:31:58.179022Z","shell.execute_reply.started":"2022-06-12T16:31:11.162018Z","shell.execute_reply":"2022-06-12T16:31:58.177933Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \n\nimport os\n\nimport timm\nfrom tqdm.notebook import tqdm\nimport pytorch_lightning as pl\nfrom pytorch_lightning.callbacks import ModelCheckpoint\nfrom pytorch_lightning import loggers as pl_loggers\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau, StepLR\nfrom pytorch_lightning.callbacks.early_stopping import EarlyStopping\nfrom pytorch_lightning import Trainer\nfrom pytorch_lightning.loggers import TensorBoardLogger\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data import random_split\n\nimport torchvision\nfrom torchvision.utils import make_grid\nfrom torchvision import transforms\n\nimport cv2\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Rectangle\n\n\nfrom sklearn.model_selection import StratifiedKFold, KFold, StratifiedGroupKFold\n\nfrom PIL import Image\nimport segmentation_models_pytorch as smp","metadata":{"execution":{"iopub.status.busy":"2022-06-12T16:31:58.180903Z","iopub.execute_input":"2022-06-12T16:31:58.181258Z","iopub.status.idle":"2022-06-12T16:32:08.241071Z","shell.execute_reply.started":"2022-06-12T16:31:58.181217Z","shell.execute_reply":"2022-06-12T16:32:08.240092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BASE_PATH  = '/home/matteo/Documents/uw-madison-gi-tract-image-segmentation/dataset'\ndf = pd.read_csv('../input/create-mask-dataset-for-uw-madison/train_processed.csv')\ndf","metadata":{"execution":{"iopub.status.busy":"2022-06-12T16:32:08.261226Z","iopub.execute_input":"2022-06-12T16:32:08.261893Z","iopub.status.idle":"2022-06-12T16:32:09.225946Z","shell.execute_reply.started":"2022-06-12T16:32:08.261858Z","shell.execute_reply":"2022-06-12T16:32:09.225077Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#df[\"file_path\"] = df[\"file_path\"].apply(lambda x: \"../input/create-mask-dataset-for-uw-madison\" + x[47:] )\ndf[\"mask_path\"] = df[\"mask_path\"].apply(lambda x: \"../input/create-mask-dataset-for-uw-madison\" + x[1:] )\ndf","metadata":{"execution":{"iopub.status.busy":"2022-06-12T16:32:09.229135Z","iopub.execute_input":"2022-06-12T16:32:09.229802Z","iopub.status.idle":"2022-06-12T16:32:09.307277Z","shell.execute_reply.started":"2022-06-12T16:32:09.229764Z","shell.execute_reply":"2022-06-12T16:32:09.306419Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.iloc[0][\"mask_path\"]","metadata":{"execution":{"iopub.status.busy":"2022-06-12T16:32:09.308771Z","iopub.execute_input":"2022-06-12T16:32:09.309126Z","iopub.status.idle":"2022-06-12T16:32:09.31669Z","shell.execute_reply.started":"2022-06-12T16:32:09.309092Z","shell.execute_reply":"2022-06-12T16:32:09.315774Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CFG:\n    seed          = 101\n    debug         = False # set debug=False for Full Training\n    exp_name      = 'Baselinev2'\n    comment       = 'unet-efficientnet_b1-224x224-aug2-split2'\n    model_name    = 'Unet'\n    backbone      = 'efficientnet-b1'\n    train_bs      = 128\n    valid_bs      = 128\n    img_size      = [224, 224]\n    epochs        = 100\n    lr            = 2e-3\n    scheduler     = 'ReduceLROnPlateau'\n    min_lr        = 1e-6\n    T_max         = int(30000/train_bs*epochs)+50\n    T_0           = 25\n    warmup_epochs = 0\n    wd            = 1e-6\n    n_accumulate  = max(1, 32//train_bs)\n    n_fold        = 5\n    num_classes   = 4\n    device        = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2022-06-12T16:32:09.318736Z","iopub.execute_input":"2022-06-12T16:32:09.31923Z","iopub.status.idle":"2022-06-12T16:32:09.375613Z","shell.execute_reply.started":"2022-06-12T16:32:09.319173Z","shell.execute_reply":"2022-06-12T16:32:09.374651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Utils","metadata":{}},{"cell_type":"code","source":"def id2mask(id_):\n    idf = df[df['id']==id_]\n    wh = idf[['height','width']].iloc[0]\n    shape = (wh.height, wh.width, 3)\n    mask = np.zeros(shape, dtype=np.uint8)\n    for i, class_ in enumerate(['large_bowel', 'small_bowel', 'stomach']):\n        cdf = idf[idf['class']==class_]\n        rle = cdf.segmentation.squeeze()\n        if len(cdf) and not pd.isna(rle):\n            mask[..., i] = rle_decode(rle, shape[:2])\n    return mask\n\ndef rgb2gray(mask):\n    pad_mask = np.pad(mask, pad_width=[(0,0),(0,0),(1,0)])\n    gray_mask = pad_mask.argmax(-1)\n    return gray_mask\n\ndef gray2rgb(mask):\n    rgb_mask = tf.keras.utils.to_categorical(mask, num_classes=4)\n    return rgb_mask[..., 1:].astype(mask.dtype)\n\n# ref: https://www.kaggle.com/code/awsaf49/uwmgi-unet-train-pytorch#%F0%9F%93%92-Notebooks\n\ndef load_img(path):\n    im = Image.open(path)\n    im = np.array(im).astype(np.float32)\n    return im\n\ndef load_msk(path):\n    msk = np.load(path)\n    msk = msk.astype('float32')\n    msk*=255.0\n    return msk\n    \n\ndef show_img(img, mask=None):\n    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n    plt.imshow(img)#, cmap='bone')\n    \n    if mask is not None:\n        plt.imshow(mask*255, alpha=0.5)\n        handles = [Rectangle((0,0),1,1, color=_c) for _c in [(0.667,0.0,0.0), (0.0,0.667,0.0), (0.0,0.0,0.667)]]\n        labels = [\"Large Bowel\", \"Small Bowel\", \"Stomach\"]\n        plt.legend(handles,labels)\n    plt.axis('off')\n    \n# ref: https://www.kaggle.com/paulorzp/run-length-encode-and-decode\ndef rle_decode(mask_rle, shape):\n    '''\n    mask_rle: run-length as string formated (start length)\n    shape: (height,width) of array to return \n    Returns numpy array, 1 - mask, 0 - background\n\n    '''\n    s = mask_rle.split()\n    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n    starts -= 1\n    ends = starts + lengths\n    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n    for lo, hi in zip(starts, ends):\n        img[lo:hi] = 1\n    return img.reshape(shape)  # Needed to align to RLE direction\n\n\n# ref.: https://www.kaggle.com/stainsby/fast-tested-rle\ndef rle_encode(img):\n    '''\n    img: numpy array, 1 - mask, 0 - background\n    Returns run length as string formated\n    '''\n    pixels = img.flatten()\n    pixels = np.concatenate([[0], pixels, [0]])\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n    runs[1::2] -= runs[::2]\n    return ' '.join(str(x) for x in runs)","metadata":{"execution":{"iopub.status.busy":"2022-06-12T16:32:09.377739Z","iopub.execute_input":"2022-06-12T16:32:09.378562Z","iopub.status.idle":"2022-06-12T16:32:09.404838Z","shell.execute_reply.started":"2022-06-12T16:32:09.378528Z","shell.execute_reply":"2022-06-12T16:32:09.404103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = df.query(\"empty == False\")\nfile_path,mask_path =  test.iloc[10][\"file_path\"], test.iloc[10][\"mask_path\"]\n\nfile_path = df.iloc[0][\"file_path\"]\nmask_path = df.iloc[0][\"mask_path\"]\n\nim = Image.open(file_path)\nim = np.array(im).astype(np.float32)\n#im = load_img(file_path)\n#mask = load_msk(mask_path)\nmask = np.load(mask_path)\nshow_img(im,mask)","metadata":{"execution":{"iopub.status.busy":"2022-06-12T16:32:09.406448Z","iopub.execute_input":"2022-06-12T16:32:09.406787Z","iopub.status.idle":"2022-06-12T16:32:09.62881Z","shell.execute_reply.started":"2022-06-12T16:32:09.406755Z","shell.execute_reply":"2022-06-12T16:32:09.628095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset","metadata":{}},{"cell_type":"code","source":"class BuildDataset(torch.utils.data.Dataset):\n    def __init__(self, df, label=True, transforms=None, target_transforms=None):\n        self.df         = df\n        self.label      = label\n        self.img_paths  = df['file_path'].tolist()\n        self.msk_paths  = df['mask_path'].tolist()\n        self.transforms = transforms\n        self.target_transforms = target_transforms\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, index):\n        img_path  = self.img_paths[index]\n        img = load_img(img_path)\n        \n        if self.label:\n            msk_path = self.msk_paths[index]\n            msk = load_msk(msk_path)\n            if self.transforms:\n                img = self.transforms(img)\n            if self.target_transforms:\n                msk = self.target_transforms(msk)\n            return img, msk\n        else:\n            if self.transforms:\n                img = self.transforms(img)\n            return img","metadata":{"execution":{"iopub.status.busy":"2022-06-12T16:32:09.630012Z","iopub.execute_input":"2022-06-12T16:32:09.630588Z","iopub.status.idle":"2022-06-12T16:32:09.640953Z","shell.execute_reply.started":"2022-06-12T16:32:09.630553Z","shell.execute_reply":"2022-06-12T16:32:09.640028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataloaders","metadata":{}},{"cell_type":"code","source":"classes = {\n    \"Large_Bowel\" : [255,0,0] ,\n    \"Small_Bowel\" : [0,255,0] , \n    \"Stomach\" : [0,0,255]\n}\n\n\ntrain_transforms = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Resize(CFG.img_size),\n])\n\nval_transforms = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Resize(CFG.img_size),\n])\n\ndata_transforms = {\n    \"train\": train_transforms,\n    \"valid\": val_transforms\n}\n\n\ndef prepare_loaders(fold=0, label = True, debug=False):\n    train_df = df.query(\"fold!=@fold\").reset_index(drop=True)\n    valid_df = df.query(\"fold==@fold\").reset_index(drop=True)\n    if debug:\n        train_df = train_df.head(32*5).query(\"empty==0\")\n        valid_df = valid_df.head(32*3).query(\"empty==0\")\n    train_dataset = BuildDataset(train_df,label=label, transforms=data_transforms['train'], target_transforms=data_transforms['train'])\n    valid_dataset = BuildDataset(valid_df,label=label, transforms=data_transforms['valid'], target_transforms=data_transforms['valid'])\n\n    train_loader = DataLoader(train_dataset, batch_size=CFG.train_bs if not debug else 20, \n                              num_workers=4, shuffle=True, pin_memory=True, drop_last=False)\n    valid_loader = DataLoader(valid_dataset, batch_size=CFG.valid_bs if not debug else 20, \n                              num_workers=4, shuffle=False, pin_memory=True)\n    \n    return train_loader, valid_loader","metadata":{"execution":{"iopub.status.busy":"2022-06-12T16:32:09.642355Z","iopub.execute_input":"2022-06-12T16:32:09.64295Z","iopub.status.idle":"2022-06-12T16:32:09.657264Z","shell.execute_reply.started":"2022-06-12T16:32:09.642913Z","shell.execute_reply":"2022-06-12T16:32:09.65648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Metrics","metadata":{}},{"cell_type":"code","source":"\"\"\"Common image segmentation metrics.\n\"\"\"\n\nimport torch\n\nEPS = 1e-10\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\ndef nanmean(x):\n    \"\"\"Computes the arithmetic mean ignoring any NaNs.\"\"\"\n    return torch.mean(x[x == x])\n\n\ndef fast_hist(true, pred, num_classes, device = device):\n    mask = (true >= 0) & (true < num_classes)\n    hist = torch.bincount(\n        num_classes * true[mask] + pred[mask],\n        minlength=num_classes ** 2,\n    ).reshape(num_classes, num_classes).to(device=device, dtype=torch.long)\n    #.float()\n    return hist\n\n\ndef overall_pixel_accuracy(hist):\n    \"\"\"Computes the total pixel accuracy.\n    The overall pixel accuracy provides an intuitive\n    approximation for the qualitative perception of the\n    label when it is viewed in its overall shape but not\n    its details.\n    Args:\n        hist: confusion matrix.\n    Returns:\n        overall_acc: the overall pixel accuracy.\n    \"\"\"\n    correct = torch.diag(hist).sum()\n    total = hist.sum()\n    overall_acc = correct / (total + EPS)\n    return overall_acc\n\n\ndef per_class_pixel_accuracy(hist, verbose = False):\n    \"\"\"Computes the average per-class pixel accuracy.\n    The per-class pixel accuracy is a more fine-grained\n    version of the overall pixel accuracy. A model could\n    score a relatively high overall pixel accuracy by\n    correctly predicting the dominant labels or areas\n    in the image whilst incorrectly predicting the\n    possibly more important/rare labels. Such a model\n    will score a low per-class pixel accuracy.\n    Args:\n        hist: confusion matrix.\n    Returns:\n        avg_per_class_acc: the average per-class pixel accuracy.\n    \"\"\"\n    correct_per_class = torch.diag(hist)\n    total_per_class = hist.sum(dim=1)\n    per_class_acc = correct_per_class / (total_per_class + EPS)\n    avg_per_class_acc = nanmean(per_class_acc)\n    if verbose:\n        return per_class_acc\n    else:\n        return avg_per_class_acc\n\n\n\ndef jaccard_index(hist, verbose = False):\n    \"\"\"Computes the Jaccard index, a.k.a the Intersection over Union (IoU).\n    Args:\n        hist: confusion matrix.\n    Returns:\n        avg_jacc: the average per-class jaccard index.\n    \"\"\"\n    A_inter_B = torch.diag(hist)\n    A = hist.sum(dim=1)\n    B = hist.sum(dim=0)\n    jaccard = A_inter_B / (A + B - A_inter_B + EPS)\n    avg_jacc = nanmean(jaccard)\n    if verbose:\n        return jaccard\n    else:\n        return avg_jacc\n\n\ndef dice_coefficient(hist, verbose = False):\n    \"\"\"Computes the Sørensen–Dice coefficient, a.k.a the F1 score.\n    Args:\n        hist: confusion matrix.\n    Returns:\n        avg_dice: the average per-class dice coefficient.\n    \"\"\"\n    A_inter_B = torch.diag(hist)\n    A = hist.sum(dim=1)\n    B = hist.sum(dim=0)\n    dice = (2 * A_inter_B) / (A + B + EPS)\n    avg_dice = nanmean(dice)\n    if verbose:\n        return dice\n    else:\n        return avg_dice\n\ndef ret_hist(true, pred, num_classes):\n    hist = torch.zeros((num_classes, num_classes)).to(device=device, dtype=torch.long)\n    for t, p in zip(true, pred):\n        hist += fast_hist(t.flatten(), p.flatten(), num_classes)\n    return hist\n\ndef eval_metrics(hist, verbose = False):\n    \"\"\"Computes various segmentation metrics on 2D feature maps.\n    Args:\n        true: a tensor of shape [B, H, W] or [B, 1, H, W].\n        pred: a tensor of shape [B, H, W] or [B, 1, H, W].\n        num_classes: the number of classes to segment. This number\n            should be less than the ID of the ignored class.\n    Returns:\n        overall_acc: the overall pixel accuracy.\n        avg_per_class_acc: the average per-class pixel accuracy.\n        avg_jacc: the jaccard index.\n        avg_dice: the dice coefficient.\n    \"\"\"\n    overall_acc = overall_pixel_accuracy(hist)\n    avg_per_class_acc = per_class_pixel_accuracy(hist, verbose)\n    avg_jacc = jaccard_index(hist, verbose)\n    avg_dice = dice_coefficient(hist, verbose)\n    if verbose:\n        return overall_acc.item(), list(avg_per_class_acc.cpu().numpy()), list(avg_jacc.cpu().numpy()), list(avg_dice.cpu().numpy())\n    else:\n        return overall_acc.item(), avg_per_class_acc.item(), avg_jacc.item(), avg_dice.item()","metadata":{"execution":{"iopub.status.busy":"2022-06-12T16:32:09.658668Z","iopub.execute_input":"2022-06-12T16:32:09.659403Z","iopub.status.idle":"2022-06-12T16:32:09.686523Z","shell.execute_reply.started":"2022-06-12T16:32:09.659365Z","shell.execute_reply":"2022-06-12T16:32:09.685638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"classes = {\n    \"Large_Bowel\" : [255,0,0] ,  # 0\n    \"Small_Bowel\" : [0,255,0] ,  # 1\n    \"Stomach\" : [0,0,255],       # 2\n    \"Background\": [0,0,0]        # 4\n}\n\n\ndef rgb2idx(msks):\n    \n    assert len(msks.shape) == 4 or len(msks.shape) == 3, \"inconsistent input shape\"\n    \n    Large_Bowel = torch.tensor([255,0,0]).float()\n    Small_Bowel = torch.tensor([0,255,0]).float()\n    Stomach = torch.tensor([0,0,255]).float()\n    \n    if len(msks.shape) == 4:\n        \n        H = msks.size(2)\n        W = msks.size(3)\n        res = torch.full((msks.size(0),H,W),4)\n        for idx, msk in enumerate(msks):\n            for i in range(H):\n                for j in range(W):\n                    if torch.equal(msk[:,i,j], Large_Bowel):\n                        res[idx,i,j] = 0\n                    elif torch.equal(msk[:,i,j] , Small_Bowel):\n                        res[idx,i,j] = 1\n                    elif torch.equal(msk[:,i,j] , Stomach):\n                        res[idx,i,j] = 2\n        return res\n    \n    elif len(msks.shape) == 3:\n        H = msks.size(1)\n        W = msks.size(2)\n        res = torch.full((H,W),4)\n        for i in range(H):\n            for j in range(W):\n                if torch.equal(msks[:,i,j], Large_Bowel):\n                    res[i,j] = 0\n                elif torch.equal(msks[:,i,j] , Small_Bowel):\n                    res[i,j] = 1\n                elif torch.equal(msks[:,i,j] , Stomach):\n                    res[i,j] = 2\n        return res\n        \n\nrgb2idx_jit = torch.jit.script(rgb2idx)\n\ndef idx2rgb(msks):\n    \n    assert len(msks.shape) == 2 or len(msks.shape) == 3, \"inconsistent input shape\"\n    \n    Large_Bowel = torch.tensor([255,0,0]).float()\n    Small_Bowel = torch.tensor([0,255,0]).float()\n    Stomach = torch.tensor([0,0,255]).float()\n    \n    Large_Bowel_index = torch.tensor([0])\n    Small_Bowel_index = torch.tensor([1])\n    Stomach_index = torch.tensor([2])\n    \n    if len(msks.shape) == 3:\n        H = msks.size(1)\n        W = msks.size(2)\n        res = torch.zeros((msks.size(0),3,H,W))\n        for idx, msk in enumerate(msks):\n            for i in range(H):\n                for j in range(W):\n                    if torch.equal(msk[i,j], Large_Bowel_index):\n                        res[idx,:,i,j] = Large_Bowel\n                    elif torch.equal(msk[i,j] , Small_Bowel_index):\n                        res[idx,:,i,j] = Small_Bowel\n                    elif torch.equal(msk[i,j] , Stomach_index):\n                        res[idx,:,i,j] = Stomach\n        return res\n    \n    elif len(msks.shape) == 2:\n        H = msks.size(0)\n        W = msks.size(1)\n        res = torch.zeros((3,H,W))\n        for i in range(H):\n            for j in range(W):\n                if torch.equal(msks[i,j], Large_Bowel_index):\n                    res[:,i,j] = Large_Bowel\n                elif torch.equal(msks[i,j] , Small_Bowel_index):\n                    res[:,i,j] = Small_Bowel\n                elif torch.equal(msks[i,j] , Stomach_index):\n                        res[:,i,j] = Stomach\n        return res\n    \n    \n\nidx2rgb_jit = torch.jit.script(idx2rgb)","metadata":{"execution":{"iopub.status.busy":"2022-06-12T16:32:09.688124Z","iopub.execute_input":"2022-06-12T16:32:09.688705Z","iopub.status.idle":"2022-06-12T16:32:09.736845Z","shell.execute_reply.started":"2022-06-12T16:32:09.688671Z","shell.execute_reply":"2022-06-12T16:32:09.736039Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# LightningModule","metadata":{}},{"cell_type":"code","source":"class UW_model(pl.LightningModule):\n\n    def __init__(self,df,fold):\n        super().__init__()\n        \n        self.model = smp.Unet(\n            encoder_name=\"resnet18\",        \n            encoder_weights=\"imagenet\",     \n            in_channels=1,                  \n            classes=4,                      \n        )\n        self.criterion = nn.CrossEntropyLoss(ignore_index = 3)\n        self.fold = fold\n        self.df = df\n        #self.automatic_optimization = False\n        self.metrics = {}\n        \n        self.freeze_encoder_upper_layers()\n    \n    def print_freezed_layers(self):\n        for name, param in self.model.encoder.layer1.named_parameters():\n            if(param.requires_grad == False):\n                print(f\"[{name}] layer freezed\")\n        \n    def freeze_encoder_upper_layers(self):\n        for name, param in self.model.encoder.layer1.named_parameters():\n            param.requires_grad = False\n        #for name, param in self.model.encoder.layer2.named_parameters():\n        #    param.requires_grad = False\n        #print(\"[INFO] encoder layer 1 and layer 2 freezed\")\n        print(\"[INFO] encoder layer 1 freezed\")\n            \n    def unfreeze_encoder_upper_layers(self):\n        for name, param in self.model.encoder.layer1.named_parameters():\n            param.requires_grad = True\n        #for name, param in self.model.encoder.layer2.named_parameters():\n        #    param.requires_grad = True\n        #print(\"[INFO] encoder layer 1 and layer 2 unfreezed\")\n        print(\"[INFO] encoder layer 1 unfreezed\")\n            \n    def forward(self, image):\n        mask = self.model(image)\n        return mask\n\n    def shared_step(self, batch, stage):\n        image, mask = batch\n        \n        predicted_mask = self.forward(image)\n        \n        pad = torch.full((mask.size(0),1,mask.size(2),mask.size(3)),0.5)\n        msks_padded = torch.cat((mask,pad.to(self.device)),1)\n        \n        msks_prob, pred_prob = torch.softmax(msks_padded,dim=1), torch.softmax(predicted_mask,dim=1)\n        msks_index, pred_index = torch.argmax(msks_prob,dim=1), torch.argmax(pred_prob,dim=1)\n        \n        msks_index_loss = msks_index.long()\n        pred_index_loss = pred_index.float().requires_grad_()\n        \n        loss = self.criterion(predicted_mask, msks_index_loss.to(self.device))\n        \n        hist = fast_hist(msks_index.flatten().type(torch.LongTensor),pred_index.flatten().type(torch.LongTensor),4)\n        metrics = eval_metrics(hist, verbose = True)\n        OA , acc, iou, f1 = metrics\n\n        return {\n            \"loss\": loss,\n            \"OA\":OA ,\n            \"acc\": acc,\n            \"iou\": iou,\n            \"f1\":f1\n        }\n\n    def shared_epoch_end(self, outputs, stage):\n        \n        m_loss  = torch.stack([x['loss'] for x in outputs]).mean()\n\n        m_OA    = np.vstack([x['OA'] for x in outputs]).mean()\n        m_acc   = np.hstack([x['acc'] for x in outputs]).mean( axis=0)\n        m_iou   = np.hstack([x['iou'] for x in outputs]).mean( axis=0)\n        m_f1    = np.hstack([x['f1'] for x in outputs]).mean( axis=0)\n\n        acc   = np.vstack([x['acc'] for x in outputs]).mean( axis=0)\n        iou   = np.vstack([x['iou'] for x in outputs]).mean( axis=0)\n        f1    = np.vstack([x['f1'] for x in outputs]).mean( axis=0)\n        \n\n        metrics = {\n            f\"loss/{stage}\": m_loss,\n            f\"m_OA/{stage}\": m_OA,\n            f\"m_acc/{stage}\": m_acc,\n            f\"m_iou/{stage}\": m_iou,\n            f\"m_f1/{stage}\": m_f1,\n            f\"m_acc_Large_Bowel/{stage}\": acc[0],\n            f\"m_acc_Small_Bowel/{stage}\": acc[1],\n            f\"m_acc_Stomach/{stage}\": acc[2],\n            f\"m_acc_Backgorund/{stage}\": acc[3],\n            f\"m_iou_Large_Bowel/{stage}\": iou[0],\n            f\"m_iou_Small_Bowel/{stage}\": iou[1],\n            f\"m_iou_Stomach/{stage}\": iou[2],\n            f\"m_iou_Backgorund/{stage}\": iou[3],\n            f\"m_f1_Large_Bowel/{stage}\": f1[0],\n            f\"m_f1_Small_Bowel/{stage}\": f1[1],\n            f\"m_f1_Stomach/{stage}\": f1[2],\n            f\"m_f1_Backgorund/{stage}\": f1[3],\n        }\n        \n        self.metrics[stage] = metrics\n        \n        self.log_dict(metrics,prog_bar=True)\n\n    def training_step(self, batch, batch_idx):        \n        return self.shared_step(batch, \"Train\")     \n\n    def training_epoch_end(self, outputs):\n        res = self.shared_epoch_end(outputs, \"Train\")\n        '''if self.metrics and \"Train\" in self.metrics.keys():\n            if self.metrics[\"Train\"][\"m_OA/Train\"] >= 0.5:\n                self.unfreeze_encoder_upper_layers()'''\n                \n        if self.current_epoch == 15:\n            self.unfreeze_encoder_upper_layers()\n        return res\n\n    def validation_step(self, batch, batch_idx):\n        return self.shared_step(batch, \"Val\")\n\n    def validation_epoch_end(self, outputs):\n        \n        res = self.shared_epoch_end(outputs, \"Val\")\n            \n        '''sch = self.lr_schedulers()\n        if isinstance(sch, torch.optim.lr_scheduler.ReduceLROnPlateau):\n            sch.step(self.trainer.callback_metrics[\"loss/Val\"])'''\n            \n        opt = self.optimizers()\n        for param_group in opt.param_groups:\n            self.log(\"learning_rate\",(param_group['lr']))\n            \n        return res\n\n    '''def test_step(self, batch, batch_idx):\n        return self.shared_step(batch, \"Test\")  \n\n    def test_epoch_end(self, outputs):\n        return self.shared_epoch_end(outputs, \"Test\")'''\n\n    def configure_optimizers(self):\n        optimizer = torch.optim.Adam(self.parameters(), lr=CFG.lr)\n        '''scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n                                                               mode='min',\n                                                               factor=0.2,\n                                                               patience=5,\n                                                               min_lr=CFG.min_lr,\n                                                               verbose=True)'''\n        scheduler = StepLR(optimizer, step_size=15, gamma=0.5)\n        return [optimizer], [scheduler]\n    \n    def train_dataloader(self):\n        fold = self.fold\n        train_df = self.df.query(\"fold!=@fold\").reset_index(drop=True)\n        train_dataset = BuildDataset(train_df,\n                                     label=True, \n                                     transforms=data_transforms['train'], \n                                     target_transforms=data_transforms['train'])\n        train_loader = DataLoader(train_dataset, \n                                  batch_size=CFG.train_bs, \n                                  num_workers=4, \n                                  shuffle=True, \n                                  pin_memory=True, \n                                  drop_last=True)\n        \n        return train_loader\n\n    def val_dataloader(self):\n        fold = self.fold\n        valid_df = self.df.query(\"fold==@fold\").reset_index(drop=True)\n        \n        \n        valid_dataset = BuildDataset(valid_df,\n                                     label=True, \n                                     transforms=data_transforms['valid'], \n                                     target_transforms=data_transforms['valid'])\n\n        \n        valid_loader = DataLoader(valid_dataset, \n                                  batch_size=CFG.valid_bs, \n                                  num_workers=4, \n                                  shuffle=False, \n                                  pin_memory=True,\n                                  drop_last=True)\n\n        return valid_loader","metadata":{"execution":{"iopub.status.busy":"2022-06-12T16:32:09.739465Z","iopub.execute_input":"2022-06-12T16:32:09.740109Z","iopub.status.idle":"2022-06-12T16:32:09.776166Z","shell.execute_reply.started":"2022-06-12T16:32:09.740073Z","shell.execute_reply":"2022-06-12T16:32:09.775357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''tmp = df.query(\"empty==False\")\ndf_reduced = tmp.sample(frac=0.10, replace=True, random_state=42).reset_index(drop=True)\n\nskf = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=CFG.seed)\n\nfor fold, (train_idx, val_idx) in enumerate(skf.split(df_reduced, df_reduced['empty'], groups = df_reduced[\"case\"])):\n    df_reduced.loc[val_idx, 'fold'] = fold\ndisplay(df_reduced.groupby(['fold','empty'])['id'].count())'''","metadata":{"execution":{"iopub.status.busy":"2022-06-12T16:32:09.77738Z","iopub.execute_input":"2022-06-12T16:32:09.777908Z","iopub.status.idle":"2022-06-12T16:32:09.790034Z","shell.execute_reply.started":"2022-06-12T16:32:09.777875Z","shell.execute_reply":"2022-06-12T16:32:09.788881Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"skf = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=CFG.seed)\n\nfor fold, (train_idx, val_idx) in enumerate(skf.split(df, df['empty'], groups = df[\"case\"])):\n    df.loc[val_idx, 'fold'] = fold\ndisplay(df.groupby(['fold','empty'])['id'].count())","metadata":{"execution":{"iopub.status.busy":"2022-06-12T16:32:09.791455Z","iopub.execute_input":"2022-06-12T16:32:09.79208Z","iopub.status.idle":"2022-06-12T16:32:10.163941Z","shell.execute_reply.started":"2022-06-12T16:32:09.792045Z","shell.execute_reply":"2022-06-12T16:32:10.163026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fold = 0\nlen(df.query(\"fold != @fold\")), len(df.query(\"fold == @fold\"))","metadata":{"execution":{"iopub.status.busy":"2022-06-12T16:32:10.165381Z","iopub.execute_input":"2022-06-12T16:32:10.166092Z","iopub.status.idle":"2022-06-12T16:32:10.199553Z","shell.execute_reply.started":"2022-06-12T16:32:10.166056Z","shell.execute_reply":"2022-06-12T16:32:10.198707Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.query(\"fold == 0\")['empty'].value_counts().plot.bar()","metadata":{"execution":{"iopub.status.busy":"2022-06-12T16:32:10.201002Z","iopub.execute_input":"2022-06-12T16:32:10.201421Z","iopub.status.idle":"2022-06-12T16:32:10.402904Z","shell.execute_reply.started":"2022-06-12T16:32:10.201385Z","shell.execute_reply":"2022-06-12T16:32:10.400086Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2022-06-12T16:32:34.571471Z","iopub.execute_input":"2022-06-12T16:32:34.572295Z","iopub.status.idle":"2022-06-12T16:32:34.576009Z","shell.execute_reply.started":"2022-06-12T16:32:34.572254Z","shell.execute_reply":"2022-06-12T16:32:34.575317Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epochs = 5\n\nmodel = UW_model(df = df, fold = 0)\n\ndrive_logs_folder = os.path.join(\"./experiments/logs\")\nos.makedirs(drive_logs_folder, exist_ok=True)\n\nmodel_name = \"ResUNet18\"\ntb_logger = pl_loggers.TensorBoardLogger(save_dir=drive_logs_folder, name=\"{}_logs\".format(model_name))\n\ncheckpoint_callback = ModelCheckpoint(\n    #dirpath=drive_logs_folder,\n    monitor=\"loss/Val\",\n    save_top_k=2,\n    mode=\"min\",\n    save_last = True\n)\n\neaerly_stopping_cb = EarlyStopping(monitor=\"loss/Train\", mode=\"min\",patience = 30)\n\ntrainer = pl.Trainer(\n    max_epochs=CFG.epochs,\n    gpus=1,\n    logger = tb_logger,\n    default_root_dir=drive_logs_folder,\n    callbacks=[checkpoint_callback,eaerly_stopping_cb]\n    )","metadata":{"execution":{"iopub.status.busy":"2022-06-12T16:34:52.584389Z","iopub.execute_input":"2022-06-12T16:34:52.584743Z","iopub.status.idle":"2022-06-12T16:34:52.950465Z","shell.execute_reply.started":"2022-06-12T16:34:52.584715Z","shell.execute_reply":"2022-06-12T16:34:52.949556Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.fit(model)","metadata":{"execution":{"iopub.status.busy":"2022-06-12T16:34:55.726758Z","iopub.execute_input":"2022-06-12T16:34:55.728156Z","iopub.status.idle":"2022-06-12T16:35:01.279643Z","shell.execute_reply.started":"2022-06-12T16:34:55.72812Z","shell.execute_reply":"2022-06-12T16:35:01.275476Z"},"trusted":true},"execution_count":null,"outputs":[]}]}