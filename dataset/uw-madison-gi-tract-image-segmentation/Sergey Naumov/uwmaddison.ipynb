{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nimport torchvision\nimport matplotlib.pyplot as plt\nimport PIL.Image as Image\n\nimport os","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-06-19T13:09:14.580055Z","iopub.execute_input":"2022-06-19T13:09:14.580396Z","iopub.status.idle":"2022-06-19T13:09:14.584623Z","shell.execute_reply.started":"2022-06-19T13:09:14.580367Z","shell.execute_reply":"2022-06-19T13:09:14.583919Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv('../input/uw-madison-gi-tract-image-segmentation/train.csv')","metadata":{"execution":{"iopub.status.busy":"2022-06-19T13:09:15.152455Z","iopub.execute_input":"2022-06-19T13:09:15.153216Z","iopub.status.idle":"2022-06-19T13:09:15.427129Z","shell.execute_reply.started":"2022-06-19T13:09:15.153178Z","shell.execute_reply":"2022-06-19T13:09:15.426326Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_path = '../input/uw-madison-gi-tract-image-segmentation/train/'","metadata":{"execution":{"iopub.status.busy":"2022-06-19T13:09:15.937609Z","iopub.execute_input":"2022-06-19T13:09:15.938366Z","iopub.status.idle":"2022-06-19T13:09:15.941957Z","shell.execute_reply.started":"2022-06-19T13:09:15.938321Z","shell.execute_reply":"2022-06-19T13:09:15.941197Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_pics_pathes = []","metadata":{"execution":{"iopub.status.busy":"2022-06-19T13:09:16.726448Z","iopub.execute_input":"2022-06-19T13:09:16.72705Z","iopub.status.idle":"2022-06-19T13:09:16.732262Z","shell.execute_reply.started":"2022-06-19T13:09:16.72699Z","shell.execute_reply":"2022-06-19T13:09:16.731356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for cases in os.listdir(train_path):\n    for days in os.listdir(os.path.join(train_path, cases)):\n        for slices in os.listdir(os.path.join(train_path, cases, days, 'scans')):\n            train_pics_pathes.append(os.path.join(cases, days, 'scans', slices))","metadata":{"execution":{"iopub.status.busy":"2022-06-19T13:09:17.661985Z","iopub.execute_input":"2022-06-19T13:09:17.662634Z","iopub.status.idle":"2022-06-19T13:09:22.153418Z","shell.execute_reply.started":"2022-06-19T13:09:17.662596Z","shell.execute_reply":"2022-06-19T13:09:22.152614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"id_pics_matching = dict()\nfor pics_path in train_pics_pathes:\n    pics_path_splitted = pics_path.split('/')\n    name_splitted = pics_path_splitted[3].split('_')\n    pics_id = pics_path_splitted[0] + '_' + pics_path_splitted[1].split('_')[1] + '_' + name_splitted[0] + '_' + name_splitted[1]\n    id_pics_matching[pics_id] = os.path.join(train_path, pics_path)","metadata":{"execution":{"iopub.status.busy":"2022-06-19T13:09:22.155825Z","iopub.execute_input":"2022-06-19T13:09:22.156351Z","iopub.status.idle":"2022-06-19T13:09:22.297784Z","shell.execute_reply.started":"2022-06-19T13:09:22.156313Z","shell.execute_reply":"2022-06-19T13:09:22.296922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = data.dropna().reset_index()","metadata":{"execution":{"iopub.status.busy":"2022-06-19T13:09:22.298947Z","iopub.execute_input":"2022-06-19T13:09:22.299321Z","iopub.status.idle":"2022-06-19T13:09:22.348827Z","shell.execute_reply.started":"2022-06-19T13:09:22.299286Z","shell.execute_reply":"2022-06-19T13:09:22.348016Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img = Image.open('../input/uw-madison-gi-tract-image-segmentation/train/case30/case30_day0/scans/slice_0135_266_266_1.50_1.50.png')\nimg = torchvision.transforms.ToTensor()(img)","metadata":{"execution":{"iopub.status.busy":"2022-06-19T13:09:22.35094Z","iopub.execute_input":"2022-06-19T13:09:22.351319Z","iopub.status.idle":"2022-06-19T13:09:22.370427Z","shell.execute_reply.started":"2022-06-19T13:09:22.351283Z","shell.execute_reply":"2022-06-19T13:09:22.36978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def look(img):\n    plt.imshow(img.detach().cpu().permute(1, 2, 0))\n    plt.title(str(img.shape))","metadata":{"execution":{"iopub.status.busy":"2022-06-19T13:09:22.371479Z","iopub.execute_input":"2022-06-19T13:09:22.37183Z","iopub.status.idle":"2022-06-19T13:09:22.376341Z","shell.execute_reply.started":"2022-06-19T13:09:22.371797Z","shell.execute_reply":"2022-06-19T13:09:22.37559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def decode_rle(img, seq):\n    img = img.clone()\n    seq = seq.split()\n    for start in range(0, len(seq), 2):\n        start_x = int(seq[start]) % img.shape[2]\n        start_y = int(seq[start]) // img.shape[1]\n        for pix in range(start_x, start_x+int(seq[start+1])):\n            img[0][start_y][pix] = 65536\n    return img","metadata":{"execution":{"iopub.status.busy":"2022-06-19T13:09:22.605429Z","iopub.execute_input":"2022-06-19T13:09:22.606115Z","iopub.status.idle":"2022-06-19T13:09:22.612193Z","shell.execute_reply.started":"2022-06-19T13:09:22.606075Z","shell.execute_reply":"2022-06-19T13:09:22.611409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h4>Testing encoder","metadata":{"execution":{"iopub.status.busy":"2022-06-09T12:03:19.113906Z","iopub.execute_input":"2022-06-09T12:03:19.114261Z","iopub.status.idle":"2022-06-09T12:03:19.119983Z","shell.execute_reply.started":"2022-06-09T12:03:19.114231Z","shell.execute_reply":"2022-06-09T12:03:19.11866Z"}}},{"cell_type":"code","source":"look(img)","metadata":{"execution":{"iopub.status.busy":"2022-06-19T13:09:34.1741Z","iopub.execute_input":"2022-06-19T13:09:34.174475Z","iopub.status.idle":"2022-06-19T13:09:34.423821Z","shell.execute_reply.started":"2022-06-19T13:09:34.174443Z","shell.execute_reply":"2022-06-19T13:09:34.423029Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"decoded_img = decode_rle(img, data['segmentation'][33909])","metadata":{"execution":{"iopub.status.busy":"2022-06-19T13:09:35.739016Z","iopub.execute_input":"2022-06-19T13:09:35.739598Z","iopub.status.idle":"2022-06-19T13:09:35.782073Z","shell.execute_reply.started":"2022-06-19T13:09:35.739561Z","shell.execute_reply":"2022-06-19T13:09:35.781292Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"look(decoded_img)","metadata":{"execution":{"iopub.status.busy":"2022-06-18T11:21:46.312857Z","iopub.execute_input":"2022-06-18T11:21:46.313194Z","iopub.status.idle":"2022-06-18T11:21:46.491517Z","shell.execute_reply.started":"2022-06-18T11:21:46.313166Z","shell.execute_reply":"2022-06-18T11:21:46.49077Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def encode_line(img, i):\n    pix = 0\n    rle = []\n    while pix < img.shape[2]:\n        if img[0][i][pix] == 65536:\n            start = pix\n            while pix < img.shape[2] and img[0][i][pix] == 65536:\n                pix += 1\n            rle.append(str(i*img.shape[1] + start))\n            rle.append(str(pix-start))\n        pix += 1\n    return rle\n\ndef encode_rle(img):\n    rle = []\n    for i in range(img.shape[1]):\n        rle += encode_line(img, i)\n    return ' '.join(rle)","metadata":{"execution":{"iopub.status.busy":"2022-06-19T13:10:03.68323Z","iopub.execute_input":"2022-06-19T13:10:03.683603Z","iopub.status.idle":"2022-06-19T13:10:03.692998Z","shell.execute_reply.started":"2022-06-19T13:10:03.683573Z","shell.execute_reply":"2022-06-19T13:10:03.691998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h4>Testing decoder","metadata":{}},{"cell_type":"code","source":"seq_rle = encode_rle(decoded_img)","metadata":{"execution":{"iopub.status.busy":"2022-06-19T13:10:05.067958Z","iopub.execute_input":"2022-06-19T13:10:05.068356Z","iopub.status.idle":"2022-06-19T13:10:05.88077Z","shell.execute_reply.started":"2022-06-19T13:10:05.068325Z","shell.execute_reply":"2022-06-19T13:10:05.879939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"look(decode_rle(img, seq_rle))","metadata":{"execution":{"iopub.status.busy":"2022-06-19T13:10:12.727222Z","iopub.execute_input":"2022-06-19T13:10:12.727574Z","iopub.status.idle":"2022-06-19T13:10:12.940081Z","shell.execute_reply.started":"2022-06-19T13:10:12.727544Z","shell.execute_reply":"2022-06-19T13:10:12.93872Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h4>Plot segmentation","metadata":{}},{"cell_type":"code","source":"def look_seg(img, segmentations):\n    \"\"\"\n    segmentation is 3d array with represents 3 types of organs segmentation\n    \"\"\"\n    img_seg = torch.zeros([3, img.shape[1], img.shape[2]])\n\n    img_seg[0], img_seg[1], img_seg[2] = img[0], img[0], img[0]\n    \n    if img_seg.max() > 1:\n        img_seg /= (2**16)\n    \n    colors = [[1, 0, 0], [0, 1, 0], [0, 0, 1]]\n    \n    for seg_channel in range(3):\n        seg = segmentations[seg_channel].split()\n        for start in range(0, len(seg), 2):\n            start_x = int(seg[start]) % img.shape[2]\n            start_y = int(seg[start]) // img.shape[1]\n            for pix in range(start_x, start_x+int(seg[start+1])):\n                for channel in range(3):\n                    img_seg[channel][start_y][pix] = colors[seg_channel][channel]\n    plt.imshow(img_seg.permute(1, 2, 0))","metadata":{"execution":{"iopub.status.busy":"2022-06-19T13:09:57.74975Z","iopub.execute_input":"2022-06-19T13:09:57.750105Z","iopub.status.idle":"2022-06-19T13:09:57.759366Z","shell.execute_reply.started":"2022-06-19T13:09:57.750077Z","shell.execute_reply":"2022-06-19T13:09:57.758668Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img_test = torchvision.transforms.ToTensor()(Image.open('../input/uw-madison-gi-tract-image-segmentation/train/case30/case30_day0/scans/slice_0137_266_266_1.50_1.50.png'))","metadata":{"execution":{"iopub.status.busy":"2022-06-19T13:11:06.888903Z","iopub.execute_input":"2022-06-19T13:11:06.889268Z","iopub.status.idle":"2022-06-19T13:11:06.903518Z","shell.execute_reply.started":"2022-06-19T13:11:06.889238Z","shell.execute_reply":"2022-06-19T13:11:06.902733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img_seg = torchvision.transforms.ToTensor()(Image.open('../input/uw-madison-gi-tract-image-segmentation/train/case30/case30_day0/scans/slice_0137_266_266_1.50_1.50.png')) \n\nlook_seg(img_test, [data['segmentation'][33911], '', data['segmentation'][33912]])","metadata":{"execution":{"iopub.status.busy":"2022-06-19T13:11:07.531707Z","iopub.execute_input":"2022-06-19T13:11:07.532194Z","iopub.status.idle":"2022-06-19T13:11:07.85732Z","shell.execute_reply.started":"2022-06-19T13:11:07.53216Z","shell.execute_reply":"2022-06-19T13:11:07.856516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"look_seg(img, [data['segmentation'][33909], '', ''])","metadata":{"execution":{"iopub.status.busy":"2022-06-19T13:11:08.197154Z","iopub.execute_input":"2022-06-19T13:11:08.197702Z","iopub.status.idle":"2022-06-19T13:11:08.617236Z","shell.execute_reply.started":"2022-06-19T13:11:08.197642Z","shell.execute_reply":"2022-06-19T13:11:08.616489Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h4>Preparing data","metadata":{}},{"cell_type":"code","source":"data_full = dict()\n\nfor ids in data['id']:\n    data_full[ids] = ['', '', '']\n\nfor i in range(len(data)):\n    class_seg = data['class'][i]\n    if class_seg == 'stomach':\n        data_full[data['id'][i]][0] += data['segmentation'][i]\n    if class_seg == 'small_bowel':\n        data_full[data['id'][i]][1] += data['segmentation'][i]\n    if class_seg == 'large_bowel':\n        data_full[data['id'][i]][2] += data['segmentation'][i]","metadata":{"execution":{"iopub.status.busy":"2022-06-19T13:11:09.163491Z","iopub.execute_input":"2022-06-19T13:11:09.163866Z","iopub.status.idle":"2022-06-19T13:11:10.049554Z","shell.execute_reply.started":"2022-06-19T13:11:09.163837Z","shell.execute_reply":"2022-06-19T13:11:10.048618Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"'id' -> [stomach_seg:str, small_b:str, large_b:str]","metadata":{}},{"cell_type":"markdown","source":"<h4>Working with device","metadata":{}},{"cell_type":"code","source":"def move_to(data, device):\n    \"\"\"\n    moving data to device\n    :param data: data to move\n    :param device: device\n    :return: moved data\n    \"\"\"\n    if isinstance(data, (list, tuple)):\n        return [move_to(x, device) for x in data]\n    return data.to(device, non_blocking=True)","metadata":{"execution":{"iopub.status.busy":"2022-06-19T13:11:11.238304Z","iopub.execute_input":"2022-06-19T13:11:11.238685Z","iopub.status.idle":"2022-06-19T13:11:11.244396Z","shell.execute_reply.started":"2022-06-19T13:11:11.238647Z","shell.execute_reply":"2022-06-19T13:11:11.243317Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')","metadata":{"execution":{"iopub.status.busy":"2022-06-19T13:11:13.161099Z","iopub.execute_input":"2022-06-19T13:11:13.161941Z","iopub.status.idle":"2022-06-19T13:11:13.219773Z","shell.execute_reply.started":"2022-06-19T13:11:13.161904Z","shell.execute_reply":"2022-06-19T13:11:13.218506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h4>Define Model","metadata":{}},{"cell_type":"code","source":"class Downsampler(torch.nn.Module):\n    def __init__(self, in_channels, out_channels, pooling = True):\n        super().__init__()\n        self.pooling = pooling\n        \n        self.conv1 = torch.nn.Conv2d(in_channels, out_channels, 3)\n        self.conv2 = torch.nn.Conv2d(out_channels, out_channels, 3)\n        self.conv3 = torch.nn.Conv2d(out_channels, out_channels, 3)\n        self.act = torch.nn.ReLU()\n        self.pool = torch.nn.MaxPool2d(2)\n        \n    def forward(self, X):\n        X = self.act(self.conv1(X))\n        X = self.act(self.conv2(X))\n        X = self.act(self.conv3(X))\n        if self.pooling:\n            X = self.pool(X)\n        return X","metadata":{"execution":{"iopub.status.busy":"2022-06-19T13:11:15.000158Z","iopub.execute_input":"2022-06-19T13:11:15.000782Z","iopub.status.idle":"2022-06-19T13:11:15.009284Z","shell.execute_reply.started":"2022-06-19T13:11:15.000748Z","shell.execute_reply":"2022-06-19T13:11:15.008276Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Upsampler(torch.nn.Module):\n    def __init__(self, in_channels):\n        super().__init__()\n        self.deconv = torch.nn.ConvTranspose2d(in_channels, in_channels//2, 2, 2)\n        self.conv1 = torch.nn.Conv2d(in_channels, in_channels//2, 3)\n        self.conv2 = torch.nn.Conv2d(in_channels//2, in_channels//2, 3)\n        self.act = torch.nn.ReLU()\n        \n    def forward(self, X, X_cat):\n        X = self.act(self.deconv(X))\n        X = torch.cat([X, X_cat], axis=1)\n        X = self.act(self.conv1(X))\n        X = self.act(self.conv2(X))\n        return X","metadata":{"execution":{"iopub.status.busy":"2022-06-19T13:11:16.731463Z","iopub.execute_input":"2022-06-19T13:11:16.731832Z","iopub.status.idle":"2022-06-19T13:11:16.739744Z","shell.execute_reply.started":"2022-06-19T13:11:16.7318Z","shell.execute_reply":"2022-06-19T13:11:16.738732Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class UNet(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.downsampler1 = Downsampler(1, 64)\n        self.downsampler2 = Downsampler(64, 128)\n        self.downsampler3 = Downsampler(128, 256)\n        self.downsampler4 = Downsampler(256, 512)\n        self.downsampler5 = Downsampler(512, 1024, pooling = False)\n        \n        self.upsampler1 = Upsampler(1024)\n        self.upsampler2 = Upsampler(512)\n        self.upsampler3 = Upsampler(256)\n        self.upsampler4 = Upsampler(128)\n        \n        self.final_conv = torch.nn.Conv2d(64, 3, 3)  # 3 channels for seg. maps of stomach, large and small bowel\n        self.final_act = torch.nn.Sigmoid()\n        \n    def copy_crop(self, X, shape):\n        top = (X.shape[2]-shape)//2  # as same as left\n        return torchvision.transforms.functional.crop(X, top, top, shape, shape).clone()\n        \n    \n    def forward(self, X):\n        X = self.downsampler1(X)\n        X_1 = X\n        X = self.downsampler2(X)\n        X_2 = X\n        X = self.downsampler3(X)\n        X_3 = X\n        X = self.downsampler4(X)\n        X_4 = X\n        X = self.downsampler5(X)\n        \n        X = self.upsampler1(X, self.copy_crop(X_4, 48))\n        X = self.upsampler2(X, self.copy_crop(X_3, 88))\n        X = self.upsampler3(X, self.copy_crop(X_2, 168))\n        X = self.upsampler4(X, self.copy_crop(X_1, 328))\n        \n        X = self.final_conv(X)\n        return self.final_act(X)","metadata":{"execution":{"iopub.status.busy":"2022-06-19T13:11:18.218861Z","iopub.execute_input":"2022-06-19T13:11:18.219649Z","iopub.status.idle":"2022-06-19T13:11:18.231385Z","shell.execute_reply.started":"2022-06-19T13:11:18.219611Z","shell.execute_reply":"2022-06-19T13:11:18.230231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def batch_loader(data, batch_size, id_pics_matching):\n    ind = 0\n    data_keys = list(data_full.keys())\n    while ind + batch_size < len(data_keys):\n        X = torch.zeros([batch_size, 1, 572, 572])\n        y = torch.zeros([batch_size, 3, 572, 572])\n        \n        for i in range(batch_size):\n            X_new = torchvision.transforms.ToTensor()(Image.open(id_pics_matching[data_keys[ind+i]])).type(torch.float32)/(2**16)\n            X[i] = torchvision.transforms.Resize([572, 572])(X_new)\n            \n            for j in range(3):            \n                y_new = decode_rle(X_new, data_full[data_keys[ind+i]][j]).type(torch.float32)/(2**16)\n                y[i][j] = torchvision.transforms.Resize([572, 572])(y_new)\n        ind += batch_size\n        \n        yield X, y","metadata":{"execution":{"iopub.status.busy":"2022-06-19T14:05:30.827408Z","iopub.execute_input":"2022-06-19T14:05:30.827784Z","iopub.status.idle":"2022-06-19T14:05:30.836978Z","shell.execute_reply.started":"2022-06-19T14:05:30.827753Z","shell.execute_reply":"2022-06-19T14:05:30.835914Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def batch_weights(y, n=1):\n    res = torch.log(n*torch.sqrt(y)+1)+0.2\n    return res","metadata":{"execution":{"iopub.status.busy":"2022-06-19T14:00:31.804628Z","iopub.execute_input":"2022-06-19T14:00:31.80531Z","iopub.status.idle":"2022-06-19T14:00:31.810073Z","shell.execute_reply.started":"2022-06-19T14:00:31.805275Z","shell.execute_reply":"2022-06-19T14:00:31.808554Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2022-06-19T13:13:45.476276Z","iopub.execute_input":"2022-06-19T13:13:45.476626Z","iopub.status.idle":"2022-06-19T13:13:45.480912Z","shell.execute_reply.started":"2022-06-19T13:13:45.476597Z","shell.execute_reply":"2022-06-19T13:13:45.479745Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size = 2\nepochs = 0\nlr = 2e-4\n\nmodel = move_to(UNet(), device)\nopt = torch.optim.Adam(model.parameters(), lr=lr)","metadata":{"execution":{"iopub.status.busy":"2022-06-19T17:40:04.342435Z","iopub.execute_input":"2022-06-19T17:40:04.343248Z","iopub.status.idle":"2022-06-19T17:40:04.690897Z","shell.execute_reply.started":"2022-06-19T17:40:04.343212Z","shell.execute_reply":"2022-06-19T17:40:04.690051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"opt.lr = 0.5e-4","metadata":{"execution":{"iopub.status.busy":"2022-06-19T14:58:04.030555Z","iopub.execute_input":"2022-06-19T14:58:04.030924Z","iopub.status.idle":"2022-06-19T14:58:04.035014Z","shell.execute_reply.started":"2022-06-19T14:58:04.030893Z","shell.execute_reply":"2022-06-19T14:58:04.034283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"backups = []\nbackup_losses = []\nbackup_rate = 500","metadata":{"execution":{"iopub.status.busy":"2022-06-19T15:47:13.372301Z","iopub.execute_input":"2022-06-19T15:47:13.372722Z","iopub.status.idle":"2022-06-19T15:47:13.376791Z","shell.execute_reply.started":"2022-06-19T15:47:13.372659Z","shell.execute_reply":"2022-06-19T15:47:13.375839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"av_loss = 0\nn_iters_loss = 100\ncnt = 0\n\n\nfor epoch in range(epochs*10):\n    for X, y in batch_loader(data, batch_size, id_pics_matching):\n        X, y = move_to(X, device), move_to(y, device)\n        out = torchvision.transforms.Resize([572, 572])(model(X))\n        \n        loss = torch.nn.BCELoss(weight = move_to(batch_weights(y, 3), device))(out, y)\n        opt.zero_grad()\n        \n        loss.backward()\n    \n        opt.step()\n        av_loss += loss.detach().cpu().numpy()\n        if cnt % n_iters_loss == 0 and cnt > 0:\n            print(cnt, av_loss/n_iters_loss)\n            av_loss = 0\n            \n        if cnt % backup_rate == 0 and cnt > 0:\n            backups.append(model.state_dict())\n            backup_losses.append(loss.detach().cpu().numpy())\n            print(\">Here d made backup\", len(backups)-1)\n        cnt += 1\n        torch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2022-06-19T17:40:09.035273Z","iopub.execute_input":"2022-06-19T17:40:09.035626Z","iopub.status.idle":"2022-06-19T17:40:14.863581Z","shell.execute_reply.started":"2022-06-19T17:40:09.035596Z","shell.execute_reply":"2022-06-19T17:40:14.862535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(data_full.keys())","metadata":{"execution":{"iopub.status.busy":"2022-06-18T14:57:31.098605Z","iopub.execute_input":"2022-06-18T14:57:31.098962Z","iopub.status.idle":"2022-06-18T14:57:31.105687Z","shell.execute_reply.started":"2022-06-18T14:57:31.098932Z","shell.execute_reply":"2022-06-18T14:57:31.104873Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#look(model(torchvision.transforms.Resize([572, 572])(move_to(img, device)).view(1, 1, 572, 572).type(torch.float32)/(2**16))[0])","metadata":{"execution":{"iopub.status.busy":"2022-06-19T17:40:20.468652Z","iopub.execute_input":"2022-06-19T17:40:20.469346Z","iopub.status.idle":"2022-06-19T17:40:20.688354Z","shell.execute_reply.started":"2022-06-19T17:40:20.469311Z","shell.execute_reply":"2022-06-19T17:40:20.687552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#look(out[0][0:1])","metadata":{"execution":{"iopub.status.busy":"2022-06-19T17:40:27.583865Z","iopub.execute_input":"2022-06-19T17:40:27.584416Z","iopub.status.idle":"2022-06-19T17:40:27.781244Z","shell.execute_reply.started":"2022-06-19T17:40:27.58438Z","shell.execute_reply":"2022-06-19T17:40:27.780507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Downsampler_XL(torch.nn.Module):\n    def __init__(self, in_channels, out_channels, pooling = True):\n        super().__init__()\n        self.pooling = pooling\n        \n        self.conv1 = torch.nn.Conv2d(in_channels, out_channels, 3)\n        self.conv2 = torch.nn.Conv2d(out_channels, out_channels, 3)\n        self.conv3 = torch.nn.Conv2d(out_channels, out_channels, 3)\n        self.conv4 = torch.nn.Conv2d(out_channels, out_channels, 3)\n        self.act = torch.nn.ReLU()\n        self.pool = torch.nn.MaxPool2d(2)\n        \n    def forward(self, X):\n        X = self.act(self.conv1(X))\n        X = self.act(self.conv2(X))\n        X = self.act(self.conv3(X))\n        if self.pooling:\n            X = self.pool(X)\n        return X","metadata":{"execution":{"iopub.status.busy":"2022-06-19T15:33:07.7039Z","iopub.execute_input":"2022-06-19T15:33:07.704249Z","iopub.status.idle":"2022-06-19T15:33:07.712481Z","shell.execute_reply.started":"2022-06-19T15:33:07.704219Z","shell.execute_reply":"2022-06-19T15:33:07.711697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Upsampler_XL(torch.nn.Module):\n    def __init__(self, in_channels):\n        super().__init__()\n        self.deconv = torch.nn.ConvTranspose2d(in_channels, in_channels//2, 2, 2)\n        self.conv1 = torch.nn.Conv2d(in_channels, in_channels//2, 3)\n        self.conv2 = torch.nn.Conv2d(in_channels//2, in_channels//2, 3)\n        self.conv2 = torch.nn.Conv2d(in_channels//2, in_channels//2, 3)\n        self.conv3 = torch.nn.Conv2d(in_channels//2, in_channels//2, 3)\n        self.conv4 = torch.nn.Conv2d(in_channels//2, in_channels//2, 3)\n        self.act = torch.nn.ReLU()\n        \n    def forward(self, X, X_cat):\n        X = self.act(self.deconv(X))\n        X = torch.cat([X, X_cat], axis=1)\n        X = self.act(self.conv1(X))\n        X = self.act(self.conv2(X))\n        return X","metadata":{"execution":{"iopub.status.busy":"2022-06-19T15:33:08.81842Z","iopub.execute_input":"2022-06-19T15:33:08.819135Z","iopub.status.idle":"2022-06-19T15:33:08.828128Z","shell.execute_reply.started":"2022-06-19T15:33:08.819095Z","shell.execute_reply":"2022-06-19T15:33:08.827393Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2022-06-19T15:35:06.118495Z","iopub.execute_input":"2022-06-19T15:35:06.119061Z","iopub.status.idle":"2022-06-19T15:35:06.129884Z","shell.execute_reply.started":"2022-06-19T15:35:06.119018Z","shell.execute_reply":"2022-06-19T15:35:06.129028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"UNET XL<br>\nБольше каналов у сверток: основание 100 вместо 64<br>\nНа один больше Downsamler и Sampler<br>\nДобавляем skip connection (для сверток через одну)","metadata":{}},{"cell_type":"code","source":"class UNet_XL(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.downsampler1 = Downsampler_XL(1, 64)\n        self.downsampler2 = Downsampler_XL(64, 128)\n        self.downsampler3 = Downsampler_XL(128, 256)\n        self.downsampler4 = Downsampler_XL(256, 512)\n        self.downsampler5 = Downsampler_XL(512, 1024)\n        self.downsampler6 = Downsampler_XL(1024, 1024, pooling = False)\n        \n        self.upsampler1 = Upsampler_XL(1024)\n        self.upsampler2 = Upsampler_XL(1024)\n        self.upsampler3 = Upsampler_XL(512)\n        self.upsampler4 = Upsampler_XL(256)\n        self.upsampler5 = Upsampler_XL(128)\n        \n        self.final_conv = torch.nn.Conv2d(64, 3, 3)  # 3 channels for seg. maps of stomach, large and small bowel\n        self.final_act = torch.nn.Sigmoid()\n        \n    def copy_crop(self, X, shape):\n        top = (X.shape[2]-shape)//2  # as same as left\n        return torchvision.transforms.functional.crop(X, top, top, shape, shape).clone()\n        \n    \n    def forward(self, X):\n        X = self.downsampler1(X)\n        X_1 = X\n        X = self.downsampler2(X)\n        X_2 = X\n        X = self.downsampler3(X)\n        X_3 = X\n        X = self.downsampler4(X)\n        X_4 = X\n        X = self.downsampler5(X)\n        #X_5 = X\n        #X = self.downsampler5(X)\n        \n        #X = self.upsampler1(X, self.copy_crop(X_5, 48))\n        X = self.upsampler1(X, self.copy_crop(X_4, 48))\n        X = self.upsampler2(X, self.copy_crop(X_3, 88))\n        X = self.upsampler3(X, self.copy_crop(X_2, 168))\n        X = self.upsampler4(X, self.copy_crop(X_1, 328))\n        \n        X = self.final_conv(X)\n        return self.final_act(X)","metadata":{"execution":{"iopub.status.busy":"2022-06-19T16:54:09.409583Z","iopub.execute_input":"2022-06-19T16:54:09.410259Z","iopub.status.idle":"2022-06-19T16:54:09.422275Z","shell.execute_reply.started":"2022-06-19T16:54:09.410219Z","shell.execute_reply":"2022-06-19T16:54:09.421539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def n_params(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)","metadata":{"execution":{"iopub.status.busy":"2022-06-19T16:53:16.725608Z","iopub.execute_input":"2022-06-19T16:53:16.726264Z","iopub.status.idle":"2022-06-19T16:53:16.731134Z","shell.execute_reply.started":"2022-06-19T16:53:16.726229Z","shell.execute_reply":"2022-06-19T16:53:16.730155Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_params(model)","metadata":{"execution":{"iopub.status.busy":"2022-06-19T16:53:17.602303Z","iopub.execute_input":"2022-06-19T16:53:17.602646Z","iopub.status.idle":"2022-06-19T16:53:17.609089Z","shell.execute_reply.started":"2022-06-19T16:53:17.602617Z","shell.execute_reply":"2022-06-19T16:53:17.608345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_params(UNet_XL())","metadata":{"execution":{"iopub.status.busy":"2022-06-19T16:54:12.775937Z","iopub.execute_input":"2022-06-19T16:54:12.776766Z","iopub.status.idle":"2022-06-19T16:54:13.683271Z","shell.execute_reply.started":"2022-06-19T16:54:12.776723Z","shell.execute_reply":"2022-06-19T16:54:13.682513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ","metadata":{}}]}