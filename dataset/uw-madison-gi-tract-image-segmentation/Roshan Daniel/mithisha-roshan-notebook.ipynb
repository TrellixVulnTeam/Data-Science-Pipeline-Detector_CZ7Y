{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from fastai.vision.all import *\n\ntrain = pd.read_csv('../input/uw-madison-gi-tract-image-segmentation/train.csv', low_memory=False)\ntrain.head()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-06-30T04:31:00.283695Z","iopub.execute_input":"2022-06-30T04:31:00.284146Z","iopub.status.idle":"2022-06-30T04:31:04.914773Z","shell.execute_reply.started":"2022-06-30T04:31:00.284049Z","shell.execute_reply":"2022-06-30T04:31:04.913649Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = train.pivot(index='id', columns='class', values='segmentation').reset_index()\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-30T04:31:04.920224Z","iopub.execute_input":"2022-06-30T04:31:04.923609Z","iopub.status.idle":"2022-06-30T04:31:05.117705Z","shell.execute_reply.started":"2022-06-30T04:31:04.923562Z","shell.execute_reply":"2022-06-30T04:31:05.115691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path = Path('../input/uw-madison-gi-tract-image-segmentation/train')\n\n# To get the filenames we'll use a nice helper function provided by fastai called `get_image_files` \n# that collects the filenames of all images within a Path\nfnames = get_image_files(path)\n\n# Let's inspect these so far\npath, fnames","metadata":{"execution":{"iopub.status.busy":"2022-06-30T04:31:05.122677Z","iopub.execute_input":"2022-06-30T04:31:05.124105Z","iopub.status.idle":"2022-06-30T04:31:09.810872Z","shell.execute_reply.started":"2022-06-30T04:31:05.124063Z","shell.execute_reply":"2022-06-30T04:31:09.809863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fnames[0].parts","metadata":{"execution":{"iopub.status.busy":"2022-06-30T04:31:09.814483Z","iopub.execute_input":"2022-06-30T04:31:09.815214Z","iopub.status.idle":"2022-06-30T04:31:09.82653Z","shell.execute_reply.started":"2022-06-30T04:31:09.815184Z","shell.execute_reply":"2022-06-30T04:31:09.825459Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.id[0]","metadata":{"execution":{"iopub.status.busy":"2022-06-30T04:31:09.830432Z","iopub.execute_input":"2022-06-30T04:31:09.831224Z","iopub.status.idle":"2022-06-30T04:31:09.83839Z","shell.execute_reply.started":"2022-06-30T04:31:09.831185Z","shell.execute_reply":"2022-06-30T04:31:09.83717Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fnames[0].parts[5] + '_' + fnames[0].parts[7][:10]","metadata":{"execution":{"iopub.status.busy":"2022-06-30T04:31:09.84003Z","iopub.execute_input":"2022-06-30T04:31:09.840491Z","iopub.status.idle":"2022-06-30T04:31:09.851211Z","shell.execute_reply.started":"2022-06-30T04:31:09.840455Z","shell.execute_reply":"2022-06-30T04:31:09.84998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_slice_id(fname):\n    return fname.parts[5] + '_' + fname.parts[7][:10]","metadata":{"execution":{"iopub.status.busy":"2022-06-30T04:31:09.854109Z","iopub.execute_input":"2022-06-30T04:31:09.854955Z","iopub.status.idle":"2022-06-30T04:31:09.86211Z","shell.execute_reply.started":"2022-06-30T04:31:09.854915Z","shell.execute_reply":"2022-06-30T04:31:09.861055Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def rle_decode(mask_rle, shape, color=1):\n    \"\"\" TBD\n    \n    Args:\n        mask_rle (str): run-length as string formated (start length)\n        shape (tuple of ints): (height,width) of array to return \n    \n    Returns: \n        Mask (np.array)\n            - 1 indicating mask\n            - 0 indicating background\n\n    \"\"\"\n    # Split the string by space, then convert it into a integer array\n    s = np.array(mask_rle.split(), dtype=int)\n\n    # Every even value is the start, every odd value is the \"run\" length\n    starts = s[0::2] - 1\n    lengths = s[1::2]\n    ends = starts + lengths\n\n    # The image image is actually flattened since RLE is a 1D \"run\"\n    if len(shape)==3:\n        h, w, d = shape\n        img = np.zeros((h * w, d), dtype=np.float32)\n    else:\n        h, w = shape\n        img = np.zeros((h * w,), dtype=np.float32)\n\n    # The color here is actually just any integer you want!\n    for lo, hi in zip(starts, ends):\n        img[lo : hi] = color\n        \n    # Don't forget to change the image back to the original shape\n    return img.reshape(shape)","metadata":{"execution":{"iopub.status.busy":"2022-06-30T04:31:09.863819Z","iopub.execute_input":"2022-06-30T04:31:09.864294Z","iopub.status.idle":"2022-06-30T04:31:09.875411Z","shell.execute_reply.started":"2022-06-30T04:31:09.864257Z","shell.execute_reply":"2022-06-30T04:31:09.874072Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's start with on slice fname\nfname = fnames[0]\n\n# First we need to get the slice row\nslice_id = get_slice_id(fname)\nslice_row = train_df.query('id == @slice_id')\n\n# Then we need to extract the slice height and width which are provided in the fname last part\nh, w = map(lambda x: int(x), fname.parts[-1].split('_')[2:4])\nh, w","metadata":{"execution":{"iopub.status.busy":"2022-06-30T04:31:09.877209Z","iopub.execute_input":"2022-06-30T04:31:09.877734Z","iopub.status.idle":"2022-06-30T04:31:09.901628Z","shell.execute_reply.started":"2022-06-30T04:31:09.877697Z","shell.execute_reply":"2022-06-30T04:31:09.900614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Now we create an empty numpy array with the dimensions of the slice and decode the rle mask into it\nmask = np.zeros((h, w, 1))\n\n# If the segmentation mask is not null then we shall populate the mask\nif not np.isnan(slice_row['large_bowel'].item()):\n    mask[:, :, 1] = rle_decode(slice_row['large_bowel'].item(), shape=(h, w), color=255)","metadata":{"execution":{"iopub.status.busy":"2022-06-30T04:31:09.906436Z","iopub.execute_input":"2022-06-30T04:31:09.907432Z","iopub.status.idle":"2022-06-30T04:31:09.914564Z","shell.execute_reply.started":"2022-06-30T04:31:09.907389Z","shell.execute_reply":"2022-06-30T04:31:09.913147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def label_func(fname):\n    # First we need to get the slice row\n    slice_id = get_slice_id(fname)\n    slice_row = train_df.query('id == @slice_id')\n    \n    # Then we need to extract the slice width and height which are provided in the fname last part\n    # Typically the height is the first part of a dimension, but for some reason the slices have\n    # widths provided first\n    w, h = map(lambda x: int(x), fname.parts[-1].split('_')[2:4])\n    \n    # Create mask array (It needs to have 3 channels but fastai will only keep the first one anyways)\n    mask = np.zeros((h, w, 3), dtype=np.uint8)\n    \n    # If the segmentation mask is str\n    if isinstance(slice_row['large_bowel'].item(), str):\n        # Right now the pixel having the mask will have a code of one\n        mask[:, :, 0] = rle_decode(slice_row['large_bowel'].item(), shape=(h, w), color=1)\n            \n    return mask","metadata":{"execution":{"iopub.status.busy":"2022-06-30T04:31:09.916307Z","iopub.execute_input":"2022-06-30T04:31:09.917227Z","iopub.status.idle":"2022-06-30T04:31:09.927506Z","shell.execute_reply.started":"2022-06-30T04:31:09.917188Z","shell.execute_reply":"2022-06-30T04:31:09.925999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# I have added Resize transformation in order to be able to collate batch of slices with different dimensions together\n# It isn't the most optimal way to do it like this and we'll see why as we go\ndls = SegmentationDataLoaders.from_label_func(path, fnames, label_func, item_tfms=Resize(224))","metadata":{"execution":{"iopub.status.busy":"2022-06-30T04:31:09.929481Z","iopub.execute_input":"2022-06-30T04:31:09.930771Z","iopub.status.idle":"2022-06-30T04:31:13.750412Z","shell.execute_reply.started":"2022-06-30T04:31:09.930722Z","shell.execute_reply":"2022-06-30T04:31:13.748972Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dls.show_batch()","metadata":{"execution":{"iopub.status.busy":"2022-06-30T04:31:13.752087Z","iopub.execute_input":"2022-06-30T04:31:13.752472Z","iopub.status.idle":"2022-06-30T04:31:15.659714Z","shell.execute_reply.started":"2022-06-30T04:31:13.752435Z","shell.execute_reply":"2022-06-30T04:31:15.658658Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def label_func(fname):\n    # First we need to get the slice row\n    slice_id = get_slice_id(fname)\n    slice_row = train_df.query('id == @slice_id')\n    \n    # Then we need to extract the slice width and height which are provided in the fname last part\n    # Typically the height is the first part of a dimension, but for some reason the slices have\n    # widths provided first\n    w, h = map(lambda x: int(x), fname.parts[-1].split('_')[2:4])\n    \n    # Create mask array (It needs to have 3 channels but fastai will only keep the first one anyways)\n    mask = np.zeros((h, w, 3), dtype=np.uint8)\n    \n    # If the segmentation mask is str\n    # Each mask should have it's own code (color) where fastai will use them for identification\n    if isinstance(slice_row['large_bowel'].item(), str):\n        mask[:, :, 0] = rle_decode(slice_row['large_bowel'].item(), shape=(h, w), color=1)\n        \n    if isinstance(slice_row['small_bowel'].item(), str):\n        mask[:, :, 0] = rle_decode(slice_row['small_bowel'].item(), shape=(h, w), color=2)\n        \n    # In case of overlap of first two masks\n    mask = np.where(mask == 3, 1, mask)\n        \n    if isinstance(slice_row['stomach'].item(), str):\n        mask[:, :, 0] = rle_decode(slice_row['stomach'].item(), shape=(h, w), color=3)\n        \n    # In case of overlap with one of the two masks\n    mask = np.where(mask == 4, 1, mask)\n    mask = np.where(mask == 5, 2, mask)\n        \n    return mask","metadata":{"execution":{"iopub.status.busy":"2022-06-30T04:31:15.660943Z","iopub.execute_input":"2022-06-30T04:31:15.662243Z","iopub.status.idle":"2022-06-30T04:31:15.676734Z","shell.execute_reply.started":"2022-06-30T04:31:15.662201Z","shell.execute_reply":"2022-06-30T04:31:15.675355Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"codes = {1: 'large_bowel', 2: 'small_bowel', 3: 'stomach'}\ndls = SegmentationDataLoaders.from_label_func(path, fnames, label_func, codes=codes, item_tfms=Resize(224))","metadata":{"execution":{"iopub.status.busy":"2022-06-30T04:31:15.678434Z","iopub.execute_input":"2022-06-30T04:31:15.678987Z","iopub.status.idle":"2022-06-30T04:31:15.990898Z","shell.execute_reply.started":"2022-06-30T04:31:15.678944Z","shell.execute_reply":"2022-06-30T04:31:15.989856Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dls.show_batch()","metadata":{"execution":{"iopub.status.busy":"2022-06-30T04:31:15.992342Z","iopub.execute_input":"2022-06-30T04:31:15.992747Z","iopub.status.idle":"2022-06-30T04:31:17.73132Z","shell.execute_reply.started":"2022-06-30T04:31:15.992708Z","shell.execute_reply":"2022-06-30T04:31:17.730274Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x, y = dls.one_batch()","metadata":{"execution":{"iopub.status.busy":"2022-06-30T04:31:17.732824Z","iopub.execute_input":"2022-06-30T04:31:17.73383Z","iopub.status.idle":"2022-06-30T04:31:19.217223Z","shell.execute_reply.started":"2022-06-30T04:31:17.73379Z","shell.execute_reply":"2022-06-30T04:31:19.21603Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.bincount(y.flatten())","metadata":{"execution":{"iopub.status.busy":"2022-06-30T04:31:19.219125Z","iopub.execute_input":"2022-06-30T04:31:19.219988Z","iopub.status.idle":"2022-06-30T04:31:19.264275Z","shell.execute_reply.started":"2022-06-30T04:31:19.219941Z","shell.execute_reply":"2022-06-30T04:31:19.263266Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def label_func(fname):\n    # First we need to get the slice row\n    slice_id = get_slice_id(fname)\n    slice_row = train_df.query('id == @slice_id')\n    \n    # Then we need to extract the slice width and height which are provided in the fname last part\n    # Typically the height is the first part of a dimension, but for some reason the slices have\n    # widths provided first\n    w, h = map(lambda x: int(x), fname.parts[-1].split('_')[2:4])\n    \n    # Create mask array (It needs to have 3 channels but fastai will only keep the first one anyways)\n    mask = np.zeros((h, w, 3), dtype=np.uint8)\n    \n    # If the segmentation mask is str\n    # Each mask should have it's own code (color) where fastai will use them for identification\n    if isinstance(slice_row['large_bowel'].item(), str):\n        mask[:, :, 0] = rle_decode(slice_row['large_bowel'].item(), shape=(h, w), color=255)\n        \n    if isinstance(slice_row['small_bowel'].item(), str):\n        mask[:, :, 1] = rle_decode(slice_row['small_bowel'].item(), shape=(h, w), color=255)\n        \n    if isinstance(slice_row['stomach'].item(), str):\n        mask[:, :, 2] = rle_decode(slice_row['stomach'].item(), shape=(h, w), color=255)\n        \n    return mask","metadata":{"execution":{"iopub.status.busy":"2022-06-30T04:31:19.267789Z","iopub.execute_input":"2022-06-30T04:31:19.268105Z","iopub.status.idle":"2022-06-30T04:31:19.279126Z","shell.execute_reply.started":"2022-06-30T04:31:19.268077Z","shell.execute_reply":"2022-06-30T04:31:19.277916Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dblock = DataBlock(blocks=(ImageBlock(), MaskBlock()),\n                           splitter=RandomSplitter(0.2),\n                           get_y=label_func,\n                           item_tfms=Resize(224))\n\ndls = dblock.dataloaders(fnames)\n\ndls.show_batch()","metadata":{"execution":{"iopub.status.busy":"2022-06-30T04:31:19.280707Z","iopub.execute_input":"2022-06-30T04:31:19.281801Z","iopub.status.idle":"2022-06-30T04:31:21.317711Z","shell.execute_reply.started":"2022-06-30T04:31:19.281757Z","shell.execute_reply":"2022-06-30T04:31:21.31638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MaskBlock??","metadata":{"execution":{"iopub.status.busy":"2022-06-30T04:31:21.31919Z","iopub.execute_input":"2022-06-30T04:31:21.319845Z","iopub.status.idle":"2022-06-30T04:31:21.392187Z","shell.execute_reply.started":"2022-06-30T04:31:21.319807Z","shell.execute_reply":"2022-06-30T04:31:21.391163Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This was the code available in fastai\n@ToTensor\ndef encodes(self, o:PILMask): return o._tensor_cls(image2tensor(o)[0])\n\n# And this is how we customize it to suit our needs\n@ToTensor\ndef encodes(self, o:PILMask): return o._tensor_cls(image2tensor(o))","metadata":{"execution":{"iopub.status.busy":"2022-06-30T04:31:21.393787Z","iopub.execute_input":"2022-06-30T04:31:21.394295Z","iopub.status.idle":"2022-06-30T04:31:21.400963Z","shell.execute_reply.started":"2022-06-30T04:31:21.39424Z","shell.execute_reply":"2022-06-30T04:31:21.399743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dblock = DataBlock(blocks=(ImageBlock, MaskBlock),\n                           splitter=RandomSplitter(0.2),\n                           get_y=label_func,\n                           item_tfms=Resize(224))\n\ndls = dblock.dataloaders(fnames)\n\ndls.show_batch()","metadata":{"execution":{"iopub.status.busy":"2022-06-30T04:31:21.402957Z","iopub.execute_input":"2022-06-30T04:31:21.403414Z","iopub.status.idle":"2022-06-30T04:31:23.566894Z","shell.execute_reply.started":"2022-06-30T04:31:21.403374Z","shell.execute_reply":"2022-06-30T04:31:23.564379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_image(fname):\n    img = np.array(Image.open(fname))\n    img = np.interp(img, [np.min(img), np.max(img)], [0,255])\n    return img","metadata":{"execution":{"iopub.status.busy":"2022-06-30T04:31:23.568614Z","iopub.execute_input":"2022-06-30T04:31:23.570137Z","iopub.status.idle":"2022-06-30T04:31:23.57871Z","shell.execute_reply.started":"2022-06-30T04:31:23.570089Z","shell.execute_reply":"2022-06-30T04:31:23.576701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dblock = DataBlock(blocks=(ImageBlock, MaskBlock),\n                           splitter=RandomSplitter(0.2),\n                           get_x=load_image,\n                           get_y=label_func,\n                           item_tfms=Resize(224))\n\ndls = dblock.dataloaders(fnames)\n\ndls.show_batch()","metadata":{"execution":{"iopub.status.busy":"2022-06-30T04:31:23.581017Z","iopub.execute_input":"2022-06-30T04:31:23.582338Z","iopub.status.idle":"2022-06-30T04:31:26.394449Z","shell.execute_reply.started":"2022-06-30T04:31:23.582232Z","shell.execute_reply":"2022-06-30T04:31:26.393215Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.patches as mpatches\n\n@typedispatch\ndef show_batch(x:TensorImage, y:TensorMask, samples, ctxs=None, max_n=6, nrows=None, ncols=2, figsize=None, **kwargs):\n    if figsize is None: figsize = (ncols*3, max_n//ncols * 3)\n    if ctxs is None: ctxs = get_grid(max_n, nrows=nrows, ncols=ncols, figsize=figsize)\n    for i,ctx in enumerate(ctxs): \n        x_i = x[i] / x[i].max()\n        show_image(x_i, ctx=ctx, cmap='gray', **kwargs)\n        show_image(y[i], ctx=ctx, cmap='Spectral_r', alpha=0.35, **kwargs)\n        red_patch = mpatches.Patch(color='red', label='large_bowel')\n        green_patch = mpatches.Patch(color='green', label='small_bowel')\n        blue_patch = mpatches.Patch(color='blue', label='stomach')\n        ctx.legend(handles=[red_patch, green_patch, blue_patch], fontsize=figsize[0]/2)","metadata":{"execution":{"iopub.status.busy":"2022-06-30T04:31:26.396244Z","iopub.execute_input":"2022-06-30T04:31:26.396883Z","iopub.status.idle":"2022-06-30T04:31:26.409976Z","shell.execute_reply.started":"2022-06-30T04:31:26.396842Z","shell.execute_reply":"2022-06-30T04:31:26.408779Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dls.show_batch(nrows=2, ncols=4, figsize=(15, 7))","metadata":{"execution":{"iopub.status.busy":"2022-06-30T04:31:26.411776Z","iopub.execute_input":"2022-06-30T04:31:26.412879Z","iopub.status.idle":"2022-06-30T04:31:28.452323Z","shell.execute_reply.started":"2022-06-30T04:31:26.412834Z","shell.execute_reply":"2022-06-30T04:31:28.451339Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The list of transformations contain two lists\n# 1. The first is for the X part of the dataset (images)\n# 2. The second if for the Y part of the dataset (masks)\ntfms = [[load_image, PILImageBW.create], [label_func, PILMask.create]]\n\n# Then we just pass the fnames along the transformations\ndsets = Datasets(fnames, tfms)\n\n# With the DataBlock API, we didn't worry about passing item and batch transformations\n# As some are implicitly used like ToTensor and IntToFloatTensor\n# But in here we need to pass them explicitly\ndls = dsets.dataloaders(after_item=[Resize(224), ToTensor], after_batch=[IntToFloatTensor])\n\n\n# Now let's see if our visualization works\ndls.show_batch(nrows=2, ncols=4, figsize=(15, 7))","metadata":{"execution":{"iopub.status.busy":"2022-06-30T04:31:28.460015Z","iopub.execute_input":"2022-06-30T04:31:28.460519Z","iopub.status.idle":"2022-06-30T04:31:30.494992Z","shell.execute_reply.started":"2022-06-30T04:31:28.460482Z","shell.execute_reply":"2022-06-30T04:31:30.493892Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df['partial_fname'] = train_df.id\nfname_df = pd.DataFrame({'partial_fname': [f'{fname.parts[-3]}_slice_{fname.parts[-1][6:10]}' for fname in fnames],\n                         'fname': fnames})\n\ntrain_df = train_df.merge(fname_df, on='partial_fname').drop('partial_fname', axis=1)\n\ntrain_df['case_id'] = train_df.id.apply(lambda x: x.split('_')[0])\ntrain_df['day_num'] = train_df.id.apply(lambda x: x.split('_')[1])\n\ntrain_df['slice_w'] = train_df[\"fname\"].apply(lambda x: int(str(x)[:-4].rsplit(\"_\",4)[1]))\ntrain_df['slice_h'] = train_df[\"fname\"].apply(lambda x: int(str(x)[:-4].rsplit(\"_\",4)[2]))\n\n\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-30T04:31:30.496462Z","iopub.execute_input":"2022-06-30T04:31:30.49744Z","iopub.status.idle":"2022-06-30T04:31:30.897902Z","shell.execute_reply.started":"2022-06-30T04:31:30.497399Z","shell.execute_reply":"2022-06-30T04:31:30.896509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"channels = 3\nstride = 2\nfor j, i in enumerate(range(-1*(channels-channels//2-1), channels//2+1)):\n    method = 'ffill'\n    if i <= 0: method = 'bfill'\n    train_df[f'fname_{j:02}'] = train_df.groupby(['case_id', 'day_num'])['fname'].shift(stride*-i).fillna(method=method)\n    \ntrain_df['fnames'] = train_df[[f'fname_{j:02d}' for j in range(channels)]].values.tolist()\n\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-30T04:31:30.900064Z","iopub.execute_input":"2022-06-30T04:31:30.90053Z","iopub.status.idle":"2022-06-30T04:31:31.019775Z","shell.execute_reply.started":"2022-06-30T04:31:30.900486Z","shell.execute_reply":"2022-06-30T04:31:31.018717Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_image(fname):\n    img = np.array(Image.open(fname))\n    img = np.interp(img, [np.min(img), np.max(img)], [0,255])\n    return img\n\ndef get_25D_image(row):\n    imgs = np.zeros((row['slice_h'], row['slice_w'], len(row['fnames'])))\n        \n    for i, fname in enumerate(row['fnames']):\n        img = load_image(fname)\n        imgs[..., i] += img\n    return imgs.astype(np.uint8)\n\n\ndef get_mask(row):\n    mask = np.zeros((row['slice_h'], row['slice_w'], 3))\n        \n    if isinstance(row['large_bowel'], str):\n        mask[..., 0] += rle_decode(row['large_bowel'], shape=(row['slice_h'], row['slice_w']), color=255)\n    if isinstance(row['small_bowel'], str):\n        mask[..., 1] += rle_decode(row['small_bowel'], shape=(row['slice_h'], row['slice_w']), color=255)\n    if isinstance(row['stomach'], str):\n        mask[..., 2] += rle_decode(row['stomach'], shape=(row['slice_h'], row['slice_w']), color=255)\n        \n    return mask.astype(np.uint8)","metadata":{"execution":{"iopub.status.busy":"2022-06-30T04:31:31.021588Z","iopub.execute_input":"2022-06-30T04:31:31.022628Z","iopub.status.idle":"2022-06-30T04:31:31.036166Z","shell.execute_reply.started":"2022-06-30T04:31:31.022585Z","shell.execute_reply":"2022-06-30T04:31:31.035012Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tfms = [[get_25D_image, PILImage.create],\n        [get_mask, PILMask.create]]\n    \ndsets = Datasets(train_df, tfms)\n\ndls = dsets.dataloaders(bs=16, after_item=[Resize(224), ToTensor], after_batch=[IntToFloatTensor])\n\n# Now let's see if our visualization works\ndls.show_batch(nrows=4, ncols=4, max_n=16, figsize=(15, 15))","metadata":{"execution":{"iopub.status.busy":"2022-06-30T04:31:31.038124Z","iopub.execute_input":"2022-06-30T04:31:31.038591Z","iopub.status.idle":"2022-06-30T04:31:33.554631Z","shell.execute_reply.started":"2022-06-30T04:31:31.038549Z","shell.execute_reply":"2022-06-30T04:31:33.553387Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def pad_img(img, up_size=None):\n    if up_size is None:\n        return img\n    shape0 = np.array(img.shape[:2])\n    resize = np.array(up_size)\n    if np.any(shape0!=resize):\n        diff = resize - shape0\n        pad0 = diff[0]\n        pad1 = diff[1]\n        pady = [pad0//2, pad0//2 + pad0%2]\n        padx = [pad1//2, pad1//2 + pad1%2]\n        img = np.pad(img, [pady, padx])\n        img = img.reshape((*resize))\n    return img\n\n\ndef unpad_img(img, up_size, org_size):\n    shape0 = np.array(org_size)\n    resize = np.array(up_size)\n    if np.any(shape0!=resize):\n        diff = resize - shape0\n        pad0 = diff[0]\n        pad1 = diff[1]\n        pady = [pad0//2, pad0//2 + pad0%2]\n        padx = [pad1//2, pad1//2 + pad1%2]\n        img = img[pady[0]:-pady[1], padx[0]:-padx[1], :]\n        img = img.reshape((*shape0, 3))\n    return img\n\n    \ndef load_image(fname, up_size=None):\n    img = np.array(Image.open(fname))\n    img = np.interp(img, [np.min(img), np.max(img)], [0,255])\n    return pad_img(img, up_size)\n\ndef get_25D_image(row, up_size=None):\n    if up_size:\n        imgs = np.zeros((*up_size, len(row['fnames'])))\n    else:\n        imgs = np.zeros((row['slice_h'], row['slice_w'], len(row['fnames'])))\n        \n    for i, fname in enumerate(row['fnames']):\n        img = load_image(fname, up_size)\n        imgs[..., i] += img\n    return imgs.astype(np.uint8)\n                   \n\ndef get_mask(row, up_size=None):\n    if up_size:\n        mask = np.zeros((*up_size, 3))\n    else:\n        mask = np.zeros((row['slice_h'], row['slice_w'], 3))\n        \n    if isinstance(row['large_bowel'], str):\n        mask[..., 0] += pad_img(rle_decode(row['large_bowel'], shape=(row['slice_h'], row['slice_w']), color=255), up_size)\n    if isinstance(row['small_bowel'], str):\n        mask[..., 1] += pad_img(rle_decode(row['small_bowel'], shape=(row['slice_h'], row['slice_w']), color=255), up_size)\n    if isinstance(row['stomach'], str):\n        mask[..., 2] += pad_img(rle_decode(row['stomach'], shape=(row['slice_h'], row['slice_w']), color=255), up_size)\n        \n    return mask.astype(np.uint8)","metadata":{"execution":{"iopub.status.busy":"2022-06-30T04:31:33.556255Z","iopub.execute_input":"2022-06-30T04:31:33.556658Z","iopub.status.idle":"2022-06-30T04:31:33.583517Z","shell.execute_reply.started":"2022-06-30T04:31:33.55662Z","shell.execute_reply":"2022-06-30T04:31:33.582371Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"up_size = (320, 384)\n\ntfms = [[partial(get_25D_image, up_size=up_size), PILImage.create],\n        [partial(get_mask, up_size=up_size), PILMask.create]]\n    \ndsets = Datasets(train_df, tfms)\n\ndls = dsets.dataloaders(bs=16, after_item=[ToTensor], after_batch=[IntToFloatTensor])\n\n# Now let's see if our visualization works\ndls.show_batch(nrows=4, ncols=4, max_n=16, figsize=(15, 15))","metadata":{"execution":{"iopub.status.busy":"2022-06-30T04:31:33.585019Z","iopub.execute_input":"2022-06-30T04:31:33.585953Z","iopub.status.idle":"2022-06-30T04:31:36.329205Z","shell.execute_reply.started":"2022-06-30T04:31:33.585922Z","shell.execute_reply":"2022-06-30T04:31:36.328256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"up_size = (320, 384)\n\ntfms = [[partial(get_25D_image, up_size=up_size), PILImage.create],\n        [partial(get_mask, up_size=up_size), PILMask.create]]\n\nsplits = RandomSplitter()(train_df)\ndsets = Datasets(train_df, tfms, splits=splits)\n\ndls = dsets.dataloaders(bs=16, after_item=[RandomResizedCrop(224), ToTensor], after_batch=[IntToFloatTensor, *aug_transforms(mult=0.8)])\n\n# Now let's see if our visualization works\ndls.show_batch(nrows=4, ncols=4, max_n=16, figsize=(15, 15))","metadata":{"execution":{"iopub.status.busy":"2022-06-30T04:31:36.331079Z","iopub.execute_input":"2022-06-30T04:31:36.331731Z","iopub.status.idle":"2022-06-30T04:31:43.206005Z","shell.execute_reply.started":"2022-06-30T04:31:36.331693Z","shell.execute_reply":"2022-06-30T04:31:43.204604Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check validation set\ndls.valid.show_batch(nrows=4, ncols=4, max_n=16, figsize=(15, 15))","metadata":{"execution":{"iopub.status.busy":"2022-06-30T04:31:43.207608Z","iopub.execute_input":"2022-06-30T04:31:43.208255Z","iopub.status.idle":"2022-06-30T04:31:45.428492Z","shell.execute_reply.started":"2022-06-30T04:31:43.208215Z","shell.execute_reply":"2022-06-30T04:31:45.427373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import albumentations as A\n\ndef get_train_aug(img_size, crop=0.9, p=0.4):\n    crop_size = round(img_size[0]*crop)\n    return A.Compose([\n            A.RandomCrop(height=crop_size, width=crop_size, always_apply=True),\n            A.HorizontalFlip(p=p),\n            A.OneOf([\n            A.GridDistortion(num_steps=5, distort_limit=0.05, p=1.0),\n            A.ElasticTransform(alpha=1, sigma=50, alpha_affine=50, p=1.0)\n            ], p=p),\n            A.CoarseDropout(\n                max_holes=8, min_holes=8,\n                max_height=crop_size//10, max_width=crop_size//10,\n                min_height=4, min_width=4, mask_fill_value=0, p=0.2*p),\n            A.ShiftScaleRotate(\n                shift_limit=0.0625, scale_limit=0.2, rotate_limit=25,\n                interpolation=cv2.INTER_AREA, p=p),\n            A.HorizontalFlip(p=0.5*p),\n            A.OneOf([\n                A.MotionBlur(p=0.2*p),\n                A.MedianBlur(blur_limit=3, p=0.1*p),\n                A.Blur(blur_limit=3, p=0.1*p),\n            ], p=0.2*p),\n            A.GaussNoise(var_limit=0.001, p=0.2*p),\n            A.OneOf([\n                A.OpticalDistortion(p=0.3*p),\n                A.GridDistortion(p=0.1*p),\n                A.PiecewiseAffine(p=0.3*p),\n            ], p=0.2*p),\n            A.OneOf([\n                A.Sharpen(p=0.2*p),\n                A.Emboss(p=0.2*p),\n                A.RandomBrightnessContrast(p=0.2*p),\n            ]),\n        ])\n\n\ndef get_test_aug(img_size, crop=0.9):\n    crop_size = round(crop*img_size[0])\n    return  A.Compose([\n        A.CenterCrop(height=crop_size, width=crop_size),\n    ])","metadata":{"execution":{"iopub.status.busy":"2022-06-30T04:31:45.430579Z","iopub.execute_input":"2022-06-30T04:31:45.431276Z","iopub.status.idle":"2022-06-30T04:31:46.849681Z","shell.execute_reply.started":"2022-06-30T04:31:45.431236Z","shell.execute_reply":"2022-06-30T04:31:46.848547Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import cv2\n\nclass AlbumentationsTransform(ItemTransform, RandTransform):\n    split_idx, order = None, 2\n    def __init__(self, train_aug, valid_aug): store_attr()\n    \n    def before_call(self, b, split_idx): self.idx = split_idx\n    \n    def encodes(self, x):\n        if len(x) > 1:\n            img, mask = x\n            if self.idx == 0:\n                aug = self.train_aug(image=np.array(img), mask=np.array(mask))    \n            else:\n                aug = self.valid_aug(image=np.array(img), mask=np.array(mask))\n            return PILImage.create(aug[\"image\"]), PILMask.create(aug[\"mask\"])\n        else:\n            img = x[0]\n            aug = self.valid_aug(image=np.array(img))\n            return PILImage.create(aug[\"image\"])","metadata":{"execution":{"iopub.status.busy":"2022-06-30T04:31:46.85145Z","iopub.execute_input":"2022-06-30T04:31:46.851856Z","iopub.status.idle":"2022-06-30T04:31:46.862128Z","shell.execute_reply.started":"2022-06-30T04:31:46.851815Z","shell.execute_reply":"2022-06-30T04:31:46.860868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's take a sample of the training dataframe since we aren't going to use it right now \n# in anything more than demonstration\n\ndev_df = train_df.sample(frac=0.2)\n\n\nup_size = (320, 384)\n\ntfms = [[partial(get_25D_image, up_size=up_size), PILImage.create],\n        [partial(get_mask, up_size=up_size), PILMask.create]]\n\nsplits = RandomSplitter()(dev_df)\ndsets = Datasets(train_df, tfms, splits=splits)\n\nalbu_aug = AlbumentationsTransform(get_train_aug(up_size), get_test_aug(up_size))\n\ndls = dsets.dataloaders(bs=16, after_item=[albu_aug, ToTensor], after_batch=[IntToFloatTensor(div_mask=255), \n                                                                             Normalize.from_stats(*imagenet_stats)])\n\n# Now let's see if our visualization works\ndls.show_batch(nrows=4, ncols=4, max_n=16, figsize=(15, 15))","metadata":{"execution":{"iopub.status.busy":"2022-06-30T04:31:46.863607Z","iopub.execute_input":"2022-06-30T04:31:46.865865Z","iopub.status.idle":"2022-06-30T04:31:50.112705Z","shell.execute_reply.started":"2022-06-30T04:31:46.865822Z","shell.execute_reply":"2022-06-30T04:31:50.111429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from scipy.spatial.distance import directed_hausdorff\n\n\ndef dice_coeff_adj(inp, targ):\n    inp = np.where(sigmoid(inp).cpu().detach().numpy() > 0.5, 1, 0)\n    targ = targ.cpu().detach().numpy()\n    eps = 1e-5\n    dice_scores = []\n    for i in range(targ.shape[0]):\n        dice_i = []\n        for j in range(targ.shape[1]):\n            if inp[i, j].sum() == targ[i, j].sum() == 0:\n                continue\n            I = (targ[i, j] * inp[i, j]).sum()\n            U =  targ[i, j].sum() + inp[i, j].sum()\n            dice_i.append((2.*I)/(U+eps))\n        if dice_i:\n            dice_scores.append(np.mean(dice_i))\n    \n    if dice_scores:\n        return np.mean(dice_scores)\n    else:\n        return 0\n    \n    \ndef hd_dist_per_slice(inp, targ, seed):    \n    inp = np.argwhere(inp) / np.array(inp.shape)\n    targ = np.argwhere(targ) / np.array(targ.shape)\n    haussdorf_dist = 1 - directed_hausdorff(inp, targ, seed)[0]\n    return haussdorf_dist if haussdorf_dist > 0 else 0\n\ndef hd_dist_adj(inp, targ, seed=42):\n    inp = np.where(sigmoid(inp).cpu().detach().numpy() > 0.5, 1, 0)\n    targ = targ.cpu().detach().numpy()\n    hd_scores = []\n    for i in range(targ.shape[0]):\n        hd_i = []\n        for j in range(targ.shape[1]):\n            if inp[i, j].sum() == targ[i, j].sum() == 0:\n                continue\n            hd_i.append(hd_dist_per_slice(inp[i, j], targ[i, j], seed))\n        if hd_i:\n            hd_scores.append(np.mean(hd_i))\n    if hd_scores:\n        return np.mean(hd_scores)\n    else:\n        return 0\n\ndef custom_metric_adj(inp, targ, seed=42):\n    hd_score_per_batch = hd_dist_adj(inp, targ, seed)\n    dice_score_per_batch = dice_coeff_adj(inp, targ)\n    \n    return 0.4*dice_score_per_batch + 0.6*hd_score_per_batch","metadata":{"execution":{"iopub.status.busy":"2022-06-30T04:31:50.114832Z","iopub.execute_input":"2022-06-30T04:31:50.115665Z","iopub.status.idle":"2022-06-30T04:31:50.154207Z","shell.execute_reply.started":"2022-06-30T04:31:50.115602Z","shell.execute_reply":"2022-06-30T04:31:50.152454Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class DiceBCEModule(Module):\n    def __init__(self, eps:float=1e-5, from_logits=True):\n        store_attr()\n        \n    def forward(self, inp:Tensor, targ:Tensor) -> Tensor:\n        inp = inp.view(-1)\n        targ = targ.view(-1)\n        \n        if self.from_logits: \n            bce_loss = nn.BCEWithLogitsLoss()(inp, targ)\n            inp = torch.sigmoid(inp)\n            \n            \n        intersection = (inp * targ).sum()                            \n        dice = (2.*intersection + self.eps)/(inp.sum() + targ.sum() + self.eps)  \n        \n        return 0.5*(1 - dice) + 0.5*bce_loss\n\n\nclass DiceBCELoss(BaseLoss):\n    def __init__(self, *args, eps:float=1e-5, from_logits=True, thresh=0.5, **kwargs):\n        super().__init__(DiceBCEModule, *args, eps=eps, from_logits=from_logits, flatten=False, is_2d=True, floatify=True, **kwargs)\n        self.thresh = thresh\n    \n    def decodes(self, x:Tensor) -> Tensor:\n        \"Converts model output to target format\"\n        return (x>self.thresh).long()\n\n    def activation(self, x:Tensor) -> Tensor:\n        \"`nn.BCEWithLogitsLoss`'s fused activation function applied to model output\"\n        return torch.sigmoid(x)\n\n# Source: https://www.kaggle.com/code/thedrcat/focal-multilabel-loss-in-pytorch-explained/notebook\ndef focal_binary_cross_entropy(logits, targets, gamma=2, n=3):\n    p = torch.sigmoid(logits)\n    p = torch.where(targets >= 0.5, p, 1-p)\n    logp = - torch.log(torch.clamp(p, 1e-4, 1-1e-4))\n    loss = logp*((1-p)**gamma)\n    loss = n*loss.mean()\n    return loss\n\nclass DiceFocalModule(Module):\n    def __init__(self, eps:float=1e-5, from_logits=True, ws=[0.5, 0.5], gamma=2, n=3):\n        store_attr()\n        \n    def forward(self, inp:Tensor, targ:Tensor) -> Tensor:\n        inp = inp.view(-1)\n        targ = targ.view(-1)\n        \n        if self.from_logits: \n            focal_loss = focal_binary_cross_entropy(inp, targ, self.gamma, self.n)\n            inp = torch.sigmoid(inp)\n            \n            \n        intersection = (inp * targ).sum()                            \n        dice = (2.*intersection + self.eps)/(inp.sum() + targ.sum() + self.eps)  \n        \n        return self.ws[0]*(1 - dice) + self.ws[1]*focal_loss\n    \nclass DiceFocalLoss(BaseLoss):\n    def __init__(self, *args, eps:float=1e-5, from_logits=True, ws=[0.5, 0.5], gamma=2, n=3, thresh=0.5, **kwargs):\n        super().__init__(DiceFocalModule, *args, eps=eps, from_logits=from_logits, ws=ws, gamma=gamma, n=n, flatten=False, is_2d=True, floatify=True, **kwargs)\n        self.thresh = thresh\n    \n    def decodes(self, x:Tensor) -> Tensor:\n        \"Converts model output to target format\"\n        return (x>self.thresh).long()\n\n    def activation(self, x:Tensor) -> Tensor:\n        \"`nn.BCEWithLogitsLoss`'s fused activation function applied to model output\"\n        return torch.sigmoid(x)\n\nclass FocalTverskyLossModule(Module):\n    def __init__(self, eps:float=1e-5, from_logits=True, alpha=0.3, beta=0.7, gamma=3/4):\n        store_attr()\n        \n    def forward(self, inp:Tensor, targ:Tensor) -> Tensor:\n        inp = inp.view(-1)\n        targ = targ.view(-1)\n        \n        if self.from_logits: \n            inp = torch.sigmoid(inp)\n            \n        inp_0, inp_1 = inp, 1 - inp\n        targ_0, targ_1 = targ, 1 - targ\n            \n        num = (inp_0 * targ_0).sum() \n        denom = num + (self.alpha * (inp_0 * targ_1).sum()) + (self.beta * (inp_1 * targ_0).sum()) + self.eps\n        loss = 1 - (num / denom)\n        return loss**self.gamma \n    \nclass FocalTverskyLoss(BaseLoss):\n    def __init__(self, *args, eps:float=1e-5, from_logits=True, alpha=0.3, beta=0.7, gamma=3/4, thresh=0.5, **kwargs):\n        super().__init__(FocalTverskyLossModule, *args, eps=eps, from_logits=from_logits, alpha=alpha, beta=beta, gamma=gamma, flatten=False, is_2d=True, floatify=True, **kwargs)\n        self.thresh = thresh\n    \n    def decodes(self, x:Tensor) -> Tensor:\n        \"Converts model output to target format\"\n        return (x>self.thresh).long()\n\n    def activation(self, x:Tensor) -> Tensor:\n        \"`nn.BCEWithLogitsLoss`'s fused activation function applied to model output\"\n        return torch.sigmoid(x)\n\ndef focal_binary_cross_entropy(logits, targets, gamma=2, n=3):\n    p = torch.sigmoid(logits)\n    p = torch.where(targets >= 0.5, p, 1-p)\n    logp = - torch.log(torch.clamp(p, 1e-4, 1-1e-4))\n    loss = logp*((1-p)**gamma)\n    loss = n*loss.mean()\n    return loss\n\nclass ComboModule(Module):\n    def __init__(self, eps:float=1e-5, from_logits=True, ws=[2, 3, 1], gamma=2, n=3):\n        store_attr()\n        \n    def forward(self, inp:Tensor, targ:Tensor) -> Tensor:\n        inp = inp.view(-1)\n        targ = targ.view(-1)\n        \n        if self.from_logits: \n            focal_loss = focal_binary_cross_entropy(inp, targ, self.gamma, self.n)\n            bce_loss = nn.BCEWithLogitsLoss()(inp, targ)\n            inp = torch.sigmoid(inp)\n                \n        intersection = (inp * targ).sum()                            \n        dice = (2.*intersection + self.eps)/(inp.sum() + targ.sum() + self.eps)  \n        \n        return self.ws[0]*(1 - dice) + self.ws[1]*focal_loss + self.ws[2]*bce_loss\n    \nclass ComboLoss(BaseLoss):\n    def __init__(self, *args, eps:float=1e-5, from_logits=True, ws=[2, 3, 1], gamma=2, n=3, thresh=0.5, **kwargs):\n        super().__init__(ComboModule, *args, eps=eps, from_logits=from_logits, ws=ws, gamma=gamma, n=n, flatten=False, is_2d=True, floatify=True, **kwargs)\n        self.thresh = thresh\n    \n    def decodes(self, x:Tensor) -> Tensor:\n        \"Converts model output to target format\"\n        return (x>self.thresh).long()\n\n    def activation(self, x:Tensor) -> Tensor:\n        \"`nn.BCEWithLogitsLoss`'s fused activation function applied to model output\"\n        return torch.sigmoid(x)","metadata":{"execution":{"iopub.status.busy":"2022-06-30T04:31:50.156587Z","iopub.execute_input":"2022-06-30T04:31:50.157519Z","iopub.status.idle":"2022-06-30T04:31:50.202009Z","shell.execute_reply.started":"2022-06-30T04:31:50.15747Z","shell.execute_reply":"2022-06-30T04:31:50.200791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install segmentation_models_pytorch","metadata":{"execution":{"iopub.status.busy":"2022-06-30T04:31:50.206025Z","iopub.execute_input":"2022-06-30T04:31:50.206735Z","iopub.status.idle":"2022-06-30T04:32:07.801078Z","shell.execute_reply.started":"2022-06-30T04:31:50.206699Z","shell.execute_reply":"2022-06-30T04:32:07.799402Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import segmentation_models_pytorch as smp\n\ndef build_model(encoder_name, in_c=3, classes=3, weights=\"imagenet\"):\n    model = smp.Unet(\n        encoder_name=encoder_name,      \n        encoder_weights=weights,     \n        in_channels=in_c,                \n        classes=classes,        \n        activation=None\n    )\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-06-30T04:32:07.803397Z","iopub.execute_input":"2022-06-30T04:32:07.804082Z","iopub.status.idle":"2022-06-30T04:32:16.355981Z","shell.execute_reply.started":"2022-06-30T04:32:07.804028Z","shell.execute_reply":"2022-06-30T04:32:16.354811Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def smp_splitter(model):\n    model_layers = list(model.children())\n    encoder_params = params(model_layers[0])\n    decoder_params = params(model_layers[1]) + params(model_layers[2])\n    return L(encoder_params, decoder_params)\n","metadata":{"execution":{"iopub.status.busy":"2022-06-30T04:32:16.357601Z","iopub.execute_input":"2022-06-30T04:32:16.358032Z","iopub.status.idle":"2022-06-30T04:32:16.368023Z","shell.execute_reply.started":"2022-06-30T04:32:16.357992Z","shell.execute_reply":"2022-06-30T04:32:16.366903Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = build_model('efficientnet-b0')\nmetrics = [dice_coeff_adj, hd_dist_adj, custom_metric_adj]\nloss_func = ComboLoss()\nsplitter = smp_splitter","metadata":{"execution":{"iopub.status.busy":"2022-06-30T04:32:16.369747Z","iopub.execute_input":"2022-06-30T04:32:16.370318Z","iopub.status.idle":"2022-06-30T04:32:17.401721Z","shell.execute_reply.started":"2022-06-30T04:32:16.370257Z","shell.execute_reply":"2022-06-30T04:32:17.400482Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learn = Learner(dls, model, metrics=metrics, loss_func=loss_func, splitter=splitter).to_fp16()\nlearn.freeze()","metadata":{"execution":{"iopub.status.busy":"2022-06-30T04:32:17.403295Z","iopub.execute_input":"2022-06-30T04:32:17.403937Z","iopub.status.idle":"2022-06-30T04:32:17.444131Z","shell.execute_reply.started":"2022-06-30T04:32:17.403886Z","shell.execute_reply":"2022-06-30T04:32:17.442878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learn.fine_tune(1, 1e-2)","metadata":{"execution":{"iopub.status.busy":"2022-06-30T04:32:17.44608Z","iopub.execute_input":"2022-06-30T04:32:17.446589Z","iopub.status.idle":"2022-06-30T04:46:30.368459Z","shell.execute_reply.started":"2022-06-30T04:32:17.446544Z","shell.execute_reply":"2022-06-30T04:46:30.367272Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learn.export('test_model.pkl')","metadata":{"execution":{"iopub.status.busy":"2022-06-30T04:46:30.370421Z","iopub.execute_input":"2022-06-30T04:46:30.37121Z","iopub.status.idle":"2022-06-30T04:46:30.569787Z","shell.execute_reply.started":"2022-06-30T04:46:30.37116Z","shell.execute_reply":"2022-06-30T04:46:30.568655Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import timm\n\ndef timm_model_sizes(encoder, img_size):\n    sizes = []\n    for layer in encoder.feature_info:\n        sizes.append(torch.Size([1, layer['num_chs'], img_size[0]//layer['reduction'], img_size[1]//layer['reduction']]))\n    return sizes\n\n\ndef get_timm_output_layers(encoder):\n    outputs = []\n    for layer in encoder.feature_info:\n        # Converts 'blocks.0.0' to ['blocks', '0', '0']\n        attrs = layer['module'].split('.')\n        output_layer = getattr(encoder, attrs[0])[int(attrs[1])][int(attrs[2])]\n        outputs.append(output_layer)\n    return outputs\n\n\nclass DynamicTimmUnet(SequentialEx):\n    \"Create a U-Net from a given architecture in timm.\"\n    def __init__(self, encoder, n_out, img_size, blur=False, blur_final=True, self_attention=False,\n                 y_range=None, last_cross=True, bottle=False, act_cls=defaults.activation,\n                 init=nn.init.kaiming_normal_, norm_type=None, **kwargs):\n        imsize = img_size\n        sizes = timm_model_sizes(encoder, img_size)\n        sz_chg_idxs = list(reversed(range(len(sizes))))\n        outputs = list(reversed(get_timm_output_layers(encoder)))\n        self.sfs = hook_outputs(outputs, detach=False)\n        \n        # cut encoder\n        encoder = nn.Sequential(*list(encoder.children()))[:-5]\n        \n        x = dummy_eval(encoder, imsize).detach()\n\n        ni = sizes[-1][1]\n        middle_conv = nn.Sequential(ConvLayer(ni, ni*2, act_cls=act_cls, norm_type=norm_type, **kwargs),\n                                    ConvLayer(ni*2, ni, act_cls=act_cls, norm_type=norm_type, **kwargs)).eval()\n        x = middle_conv(x)\n        layers = [encoder, BatchNorm(ni), nn.ReLU(), middle_conv]\n\n        for i,idx in enumerate(sz_chg_idxs):\n            not_final = i!=len(sz_chg_idxs)-1\n            up_in_c, x_in_c = int(x.shape[1]), int(sizes[idx][1])\n            do_blur = blur and (not_final or blur_final)\n            sa = self_attention and (i==len(sz_chg_idxs)-3)\n            unet_block = UnetBlock(up_in_c, x_in_c, self.sfs[i], final_div=not_final, blur=do_blur, self_attention=sa,\n                                   act_cls=act_cls, init=init, norm_type=norm_type, **kwargs).eval()\n            layers.append(unet_block)\n            x = unet_block(x)\n\n        ni = x.shape[1]\n        if imsize != sizes[0][-2:]: layers.append(PixelShuffle_ICNR(ni, act_cls=act_cls, norm_type=norm_type))\n        layers.append(ResizeToOrig())\n        if last_cross:\n            layers.append(MergeLayer(dense=True))\n            ni += in_channels(encoder)\n            layers.append(ResBlock(1, ni, ni//2 if bottle else ni, act_cls=act_cls, norm_type=norm_type, **kwargs))\n        layers += [ConvLayer(ni, n_out, ks=1, act_cls=None, norm_type=norm_type, **kwargs)]\n        apply_init(nn.Sequential(layers[3], layers[-2]), init)\n        #apply_init(nn.Sequential(layers[2]), init)\n        if y_range is not None: layers.append(SigmoidRange(*y_range))\n        layers.append(ToTensorBase())\n        super().__init__(*layers)\n\n    def __del__(self):\n        if hasattr(self, \"sfs\"): self.sfs.remove()\n            \n            \ndef dynamic_unet_splitter(model):\n    return L(model[0], model[1:]).map(params)","metadata":{"execution":{"iopub.status.busy":"2022-06-30T04:46:30.571626Z","iopub.execute_input":"2022-06-30T04:46:30.572023Z","iopub.status.idle":"2022-06-30T04:46:30.5986Z","shell.execute_reply.started":"2022-06-30T04:46:30.571982Z","shell.execute_reply":"2022-06-30T04:46:30.597547Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dls = dsets.dataloaders(bs=16, after_item=[albu_aug, ToTensor],\n                        after_batch=[IntToFloatTensor(div_mask=255), Normalize.from_stats(*imagenet_stats)])","metadata":{"execution":{"iopub.status.busy":"2022-06-30T04:46:30.602299Z","iopub.execute_input":"2022-06-30T04:46:30.603353Z","iopub.status.idle":"2022-06-30T04:46:30.672415Z","shell.execute_reply.started":"2022-06-30T04:46:30.603291Z","shell.execute_reply":"2022-06-30T04:46:30.67113Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img_size = [round(0.9*320) for _ in range(2)]\n\nencoder = timm.create_model('efficientnet_b0', pretrained=True)\n\n# Let's use self attentions and Mish activation function \nmodel = DynamicTimmUnet(encoder, 3, img_size, self_attention=True, act_cls=Mish)\n\n# We'll also use ranger optimizer with is RAdam with Lookahead\nlearn = Learner(dls, model, metrics=metrics, loss_func=loss_func, splitter=dynamic_unet_splitter, opt_func=ranger).to_fp16()\nlearn.freeze()","metadata":{"execution":{"iopub.status.busy":"2022-06-30T04:46:30.674332Z","iopub.execute_input":"2022-06-30T04:46:30.675396Z","iopub.status.idle":"2022-06-30T04:46:32.564919Z","shell.execute_reply.started":"2022-06-30T04:46:30.675349Z","shell.execute_reply":"2022-06-30T04:46:32.563478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learn.lr_find()","metadata":{"execution":{"iopub.status.busy":"2022-06-30T04:46:32.566673Z","iopub.execute_input":"2022-06-30T04:46:32.567434Z","iopub.status.idle":"2022-06-30T04:48:15.359958Z","shell.execute_reply.started":"2022-06-30T04:46:32.567391Z","shell.execute_reply":"2022-06-30T04:48:15.358598Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's also use flat cosine annealing lr shceduler\nlr = 1e-3\nlearn.fit_flat_cos(1, lr)","metadata":{"execution":{"iopub.status.busy":"2022-06-30T04:48:15.362064Z","iopub.execute_input":"2022-06-30T04:48:15.362562Z","iopub.status.idle":"2022-06-30T04:55:32.348718Z","shell.execute_reply.started":"2022-06-30T04:48:15.362516Z","shell.execute_reply":"2022-06-30T04:55:32.34739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learn.unfreeze()\nlearn.fit_flat_cos(2, slice(lr/400, lr/4))","metadata":{"execution":{"iopub.status.busy":"2022-06-30T04:55:32.352597Z","iopub.execute_input":"2022-06-30T04:55:32.352948Z","iopub.status.idle":"2022-06-30T05:10:01.963325Z","shell.execute_reply.started":"2022-06-30T04:55:32.352915Z","shell.execute_reply":"2022-06-30T05:10:01.962148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_df(df, fnames):\n    df = df.copy()\n    df = df.pivot(index='id', columns='class', values='segmentation').reset_index()\n    \n    df['partial_fname'] = df.id\n    fname_df = pd.DataFrame({'partial_fname': [f'{fname.parts[-3]}_slice_{fname.parts[-1][6:10]}' for fname in fnames],\n                             'fname': fnames})\n\n    df = df.merge(fname_df, on='partial_fname').drop('partial_fname', axis=1)\n\n    df['case_id'] = df.id.apply(lambda x: x.split('_')[0])\n    df['day_num'] = df.id.apply(lambda x: x.split('_')[1])\n\n    df['slice_w'] = df[\"fname\"].apply(lambda x: int(str(x)[:-4].rsplit(\"_\",4)[1]))\n    df['slice_h'] = df[\"fname\"].apply(lambda x: int(str(x)[:-4].rsplit(\"_\",4)[2]))\n    \n    channels = 3\n    stride = 2\n    for j, i in enumerate(range(-1*(channels-channels//2-1), channels//2+1)):\n        method = 'ffill'\n        if i <= 0: method = 'bfill'\n        df[f'fname_{j:02}'] = df.groupby(['case_id', 'day_num'])['fname'].shift(stride*-i).fillna(method=method)\n\n    df['fnames'] = df[[f'fname_{j:02d}' for j in range(channels)]].values.tolist()\n    \n    return df","metadata":{"execution":{"iopub.status.busy":"2022-06-30T05:10:01.965143Z","iopub.execute_input":"2022-06-30T05:10:01.965971Z","iopub.status.idle":"2022-06-30T05:10:01.981813Z","shell.execute_reply.started":"2022-06-30T05:10:01.965913Z","shell.execute_reply":"2022-06-30T05:10:01.980566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_path = '../input/uw-madison-gi-tract-image-segmentation/'\n\ntrain_path = Path(data_path+'train')\ntest_path = Path(data_path+'test')\n\ntrain_fnames = get_image_files(train_path)\ntest_fnames = get_image_files(test_path)\n\nsample_submission = pd.read_csv(data_path+'sample_submission.csv')\n\nif sample_submission.shape[0] > 0: \n    test = sample_submission.copy()\nelse:\n    test_fnames = train_fnames\n    test_path = train_path\n    test = train.copy()\n    test = test.sample(frac=1.0, random_state=42)\n\ntest_df = create_df(test, test_fnames)","metadata":{"execution":{"iopub.status.busy":"2022-06-30T05:10:01.983649Z","iopub.execute_input":"2022-06-30T05:10:01.984279Z","iopub.status.idle":"2022-06-30T05:10:03.737273Z","shell.execute_reply.started":"2022-06-30T05:10:01.984236Z","shell.execute_reply":"2022-06-30T05:10:03.736199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learn = load_learner('test_model.pkl')","metadata":{"execution":{"iopub.status.busy":"2022-06-30T05:10:03.738995Z","iopub.execute_input":"2022-06-30T05:10:03.739371Z","iopub.status.idle":"2022-06-30T05:10:03.819289Z","shell.execute_reply.started":"2022-06-30T05:10:03.739333Z","shell.execute_reply":"2022-06-30T05:10:03.81807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# I'll sample the test set here to make a quick demonstartion\ntest_df = test_df.sample(frac=0.1)\n","metadata":{"execution":{"iopub.status.busy":"2022-06-30T05:10:03.820944Z","iopub.execute_input":"2022-06-30T05:10:03.821377Z","iopub.status.idle":"2022-06-30T05:10:03.851064Z","shell.execute_reply.started":"2022-06-30T05:10:03.821319Z","shell.execute_reply":"2022-06-30T05:10:03.850062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bs = learn.dls.bs\ntest_dl = learn.dls.test_dl(test_df, bs=bs, shuffle=False).to('cuda')","metadata":{"execution":{"iopub.status.busy":"2022-06-30T05:10:03.852484Z","iopub.execute_input":"2022-06-30T05:10:03.853544Z","iopub.status.idle":"2022-06-30T05:10:03.863063Z","shell.execute_reply.started":"2022-06-30T05:10:03.853489Z","shell.execute_reply":"2022-06-30T05:10:03.8621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def mask2rle(mask):\n    \"\"\"\n    img: numpy array, 1 - mask, 0 - background\n    Returns run length as string formated\n    \"\"\"\n    mask = np.array(mask)\n    pixels = mask.flatten()\n    pad = np.array([0])\n    pixels = np.concatenate([pad, pixels, pad])\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n    runs[1::2] -= runs[::2]\n\n    return \" \".join(str(x) for x in runs)\n\ndef resize_img_to_org_size(img, org_size):\n    shape0 = np.array(img.shape[:2])\n    diff = org_size - shape0\n    if np.any(diff < 0):\n        img = pad_img_nc(img, (320, 384))\n        resized = unpad_img_nc(img, org_size)\n    else:\n        resized = pad_img_nc(img, org_size)\n    return resized\n\ndef get_rle_masks(preds, df):\n    rle_masks = []\n    for pred, width, height in zip(preds, df['slice_w'], df['slice_h']):\n        upsized_mask = resize_img_to_org_size(pred, (height, width))\n        for i in range(3):\n            rle_mask = mask2rle(upsized_mask[:, :, i])\n            rle_masks.append(rle_mask)\n    return rle_masks\n\ndef unpad_img_nc(img, org_size):\n    shape0 = np.array(org_size)\n    resize = np.array(img.shape[:2])\n    if np.any(shape0!=resize):\n        diff = resize - shape0\n        pad0 = diff[0]\n        pad1 = diff[1]\n        pady = [pad0//2, pad0//2 + pad0%2]\n        padx = [pad1//2, pad1//2 + pad1%2]\n        \n        if pady[0] != 0:\n            img = img[pady[0]:-pady[1], :, :]\n            \n        if padx[0] != 0:\n            img = img[:, padx[0]:-padx[1], :]\n            \n        img = img.reshape((*shape0, img.shape[-1]))\n    return img\n\ndef pad_img_nc(img, up_size=None):\n    if up_size is None:\n        return img\n    shape0 = np.array(img.shape[:2])\n    resize = np.array(up_size)\n    if np.any(shape0!=resize):\n        diff = resize - shape0\n        pad0 = diff[0]\n        pad1 = diff[1]\n        pady = [pad0//2, pad0//2 + pad0%2]\n        padx = [pad1//2, pad1//2 + pad1%2]\n        padz = [0, 0]\n        img = np.pad(img, [pady, padx, padz])\n        img = img.reshape((*resize, img.shape[-1]))\n    return img","metadata":{"execution":{"iopub.status.busy":"2022-06-30T05:10:03.864722Z","iopub.execute_input":"2022-06-30T05:10:03.865113Z","iopub.status.idle":"2022-06-30T05:10:03.885289Z","shell.execute_reply.started":"2022-06-30T05:10:03.865064Z","shell.execute_reply":"2022-06-30T05:10:03.884283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\nimport gc\n\nlearn.model = learn.model.cuda()\nlearn.model.eval()\nmasks = []\n\nwith torch.no_grad():\n    for i, b in enumerate(tqdm(test_dl)):\n        b.to('cuda')\n        b_preds = (sigmoid(learn.model(b)) > 0.5).permute(0, 2, 3, 1).cpu().detach().numpy().astype(np.uint8)\n\n        masks.extend(get_rle_masks(b_preds, test_df.iloc[i*bs:i*bs+bs]))\n\n        # test_preds[i*bs:i*bs+bs] = b_preds\n        del b_preds\n        torch.cuda.empty_cache()\n        gc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-06-30T05:10:03.886998Z","iopub.execute_input":"2022-06-30T05:10:03.887692Z","iopub.status.idle":"2022-06-30T05:12:10.822129Z","shell.execute_reply.started":"2022-06-30T05:10:03.887641Z","shell.execute_reply":"2022-06-30T05:12:10.820102Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_case_id(fname):\n    return fname.parts[5] + '_' + fname.parts[7][:10]","metadata":{"execution":{"iopub.status.busy":"2022-06-30T05:12:10.823678Z","iopub.execute_input":"2022-06-30T05:12:10.824084Z","iopub.status.idle":"2022-06-30T05:12:10.830955Z","shell.execute_reply.started":"2022-06-30T05:12:10.824042Z","shell.execute_reply":"2022-06-30T05:12:10.829371Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from itertools import chain\n\nsubmission = pd.DataFrame({\n        'id': chain.from_iterable([[get_case_id(fname)]*3 for fname in test_df['fname']]),\n        'class': chain.from_iterable([['large_bowel', 'small_bowel', 'stomach'] for _ in test_df['fname']]),\n        'predicted': masks,\n    })\n    \n\n# Merge with sample submission to preserve order to slices during scoring and avoid 0 scores\nif sample_submission.shape[0] > 0:\n    del sample_submission['segmentation']\n    submission = sample_submission.merge(submission, on=['id', 'class'])\n\nsubmission.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-06-30T05:12:10.832764Z","iopub.execute_input":"2022-06-30T05:12:10.833419Z","iopub.status.idle":"2022-06-30T05:12:10.991489Z","shell.execute_reply.started":"2022-06-30T05:12:10.833375Z","shell.execute_reply":"2022-06-30T05:12:10.990366Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}