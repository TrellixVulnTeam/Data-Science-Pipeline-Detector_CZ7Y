{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"---\n# [UW-Madison GI Tract Image Segmentation][1]\n\n- The goal of this competition is to create a model to automatically segment stomach and intestines on MRI scans.\n\n---\n#### **The aim of this notebook is to**\n- **1. Conduct Exploratory Data Analysis (EDA).**\n- **2. Build the PSPNet with ResNet18 (pretrained on ImageNet) as encoder, and other modules from scratch.**\n- **3. Build the U-Net model with ResNet18 as encoder, and decoder from scratch.**\n- **4. Build the Attention U-Net model with ResNet18 as encoder, and decoder from scratch.**\n- **5. Build the DeepLabv3 model with using [segmentation_models_pytorch][2] library.**\n- **6. Build the Swin-UNET model with using [keras-unet-collection][3] library.**\n- **7. Train the model with focal loss function for the unbalanced multi label semantic segmentation problem.**\n\n---\n#### **Please note**\n- **We need Internet access to run this code. Thus, this notebook doesn't fulfill the conditions for submitting (Internet access disabled).**\n\n---\n#### **References:** \nThanks to previous great blogs, codes and notebooks.\n- [YutaroOgawa/pytorch_advanced: 3_semantic_segmentation][4]\n- [Review: DeepLabv3 â€” Atrous Convolution (Semantic Segmentation)][5]\n- [Review: Swin Transformer][6]\n\n---\n#### **If you find this notebook useful, please do give me an upvote. It helps me keep up my motivation.**\n#### **Also, I would appreciate it if you find any mistakes and help me correct them.**\n\n---\n\n[1]: https://www.kaggle.com/competitions/uw-madison-gi-tract-image-segmentation/overview\n[2]: https://github.com/qubvel/segmentation_models.pytorch\n[3]: https://github.com/yingkaisha/keras-unet-collection\n[4]: https://github.com/YutaroOgawa/pytorch_advanced/tree/master/3_semantic_segmentation\n[5]: https://towardsdatascience.com/review-deeplabv3-atrous-convolution-semantic-segmentation-6d818bfd1d74\n[6]: https://sh-tsang.medium.com/review-swin-transformer-3438ea335585\n","metadata":{}},{"cell_type":"markdown","source":"<h1 style=\"background:#a1a8f0; border:0; border-radius: 10px; color:black\"><center>TABLE OF CONTENTS</center></h1>\n\n<ul class=\"list-group\" style=\"list-style-type:none;\">\n    <li><a href=\"#0\" class=\"list-group-item list-group-item-action\">0. Settings</a></li>\n    <li><a href=\"#1\" class=\"list-group-item list-group-item-action\">1. Data Loading</a>\n        <ul class=\"list-group\" style=\"list-style-type:none;\">\n            <li><a href=\"#1.1\" class=\"list-group-item list-group-item-action\">1.1 Feature Engineering </a></li>\n        </ul>\n    </li>\n    <li><a href=\"#2\" class=\"list-group-item list-group-item-action\">2. Exploratory Data Analysis</a></li>\n    <li><a href=\"#3\" class=\"list-group-item list-group-item-action\">3. Dataset & DataLoader</a></li>\n    <li><a href=\"#4\" class=\"list-group-item list-group-item-action\">4. Model Building</a>\n        <ul class=\"list-group\" style=\"list-style-type:none;\">\n            <li><a href=\"#4.1\" class=\"list-group-item list-group-item-action\">4.1 PSPNet </a></li>\n            <li><a href=\"#4.2\" class=\"list-group-item list-group-item-action\">4.2 U-Net (decoder from scratch) </a></li>\n            <li><a href=\"#4.3\" class=\"list-group-item list-group-item-action\">4.3 Attention U-Net (decoder from scratch) </a></li>\n            <li><a href=\"#4.4\" class=\"list-group-item list-group-item-action\">4.4 DeepLabv3 </a></li>\n            <li><a href=\"#4.5\" class=\"list-group-item list-group-item-action\">4.5 Swin-UNET </a></li>\n        </ul>\n    </li>\n    <li><a href=\"#5\" class=\"list-group-item list-group-item-action\">5. Training</a></li>\n    <li><a href=\"#6\" class=\"list-group-item list-group-item-action\">6. Prediction</a></li>\n</ul>\n","metadata":{}},{"cell_type":"markdown","source":"<a id =\"0\"></a><h1 style=\"background:#a1a8f0; border:0; border-radius: 10px; color:black\"><center>0. Settings</center></h1>","metadata":{}},{"cell_type":"code","source":"## Import dependencies \nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt \n%matplotlib inline\n\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nimport plotly.figure_factory as ff\nimport plotly.offline as offline\n\nimport os\nimport pathlib\nimport gc\nimport sys\nimport re\nimport math \nimport random\nimport time \nimport datetime as dt\nfrom tqdm import tqdm \nfrom pprint import pprint\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nimport torchvision\nfrom torchvision import transforms\nimport torchvision.transforms.functional as TF\nfrom torchvision.models import resnet18\n!pip install torchinfo -q --user\nfrom torchinfo import summary\n\nfrom PIL import Image\n\nprint('import done!')","metadata":{"execution":{"iopub.status.busy":"2022-05-21T01:17:23.609697Z","iopub.execute_input":"2022-05-21T01:17:23.610054Z","iopub.status.idle":"2022-05-21T01:17:39.029963Z","shell.execute_reply.started":"2022-05-21T01:17:23.609965Z","shell.execute_reply":"2022-05-21T01:17:39.029124Z"},"_kg_hide-output":true,"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## For reproducible results    \ndef seed_all(s):\n    random.seed(s)\n    np.random.seed(s)\n    torch.manual_seed(s)\n    torch.cuda.manual_seed(s)\n    os.environ['PYTHONHASHSEED'] = str(s) \n    print('Seeds setted!')\n    \nglobal_seed = 42\nseed_all(global_seed)","metadata":{"execution":{"iopub.status.busy":"2022-05-21T01:17:39.032134Z","iopub.execute_input":"2022-05-21T01:17:39.0325Z","iopub.status.idle":"2022-05-21T01:17:39.042078Z","shell.execute_reply.started":"2022-05-21T01:17:39.032453Z","shell.execute_reply":"2022-05-21T01:17:39.041247Z"},"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id =\"1\"></a><h1 style=\"background:#a1a8f0; border:0; border-radius: 10px; color:black\"><center>1. Data Loading</center></h1>","metadata":{}},{"cell_type":"markdown","source":"---\n### [Files Descriptions](https://www.kaggle.com/competitions/uw-madison-gi-tract-image-segmentation/data)\n\n- **train.csv** - IDs and masks for all training objects.\n\n- **train** - A folder of case/day folders, each containing slice images for a particular case on a given day.\n\n- **test** - The test set is entirely unseen. It is roughly 50 cases, with a varying number of days and slices, as seen in the training set.\n\n- **sample_submission.csv** - A sample submission file in the correct format.\n\n---\n### [Field Descriptions](https://www.kaggle.com/competitions/uw-madison-gi-tract-image-segmentation/data)\n\n- **train.csv**\n - `id` - unique identifier for object\n - `class` - the predicted class for the object\n - `EncodedPixels` - RLE-encoded pixels for the identified object\n \n--- \n\n### [Submission File](https://www.kaggle.com/competitions/uw-madison-gi-tract-image-segmentation/overview/evaluation)\nIn order to reduce the submission file size, our metric uses run-length encoding on the pixel values.  Instead of submitting an exhaustive list of indices for your segmentation, you will submit pairs of values that contain a start position and a run length. E.g. '1 3' implies starting at pixel 1 and running a total of 3 pixels (1,2,3).\n\nNote that, at the time of encoding, the mask should be binary, meaning the masks for all objects in an image are joined into a single large mask. A value of 0 should indicate pixels that are not masked, and a value of 1 will indicate pixels that are masked.\n\nThe competition format requires a space delimited list of pairs. For example, '1 3 10 5' implies pixels 1,2,3,10,11,12,13,14 are to be included in the mask. The metric checks that the pairs are sorted, positive, and the decoded pixel values are not duplicated. The pixels are numbered from top to bottom, then left to right: 1 is pixel (1,1), 2 is pixel (2,1), etc.","metadata":{}},{"cell_type":"code","source":"## Data Loading\ndata_config = {'train_csv_path': '../input/uw-madison-gi-tract-image-segmentation/train.csv',\n               'train_folder_path': '../input/uw-madison-gi-tract-image-segmentation/train',\n               'test_folder_path': '../input/uw-madison-gi-tract-image-segmentation/test',\n               'sample_submission_path': '../input/uw-madison-gi-tract-image-segmentation/sample_submission.csv',\n              }\n\ntrain_df = pd.read_csv(data_config['train_csv_path'])\nsubmission_df = pd.read_csv(data_config['sample_submission_path'])\n\nprint(f'train_length: {len(train_df)}')\nprint(f'submission_length: {len(submission_df)}')","metadata":{"execution":{"iopub.status.busy":"2022-05-21T01:17:39.044256Z","iopub.execute_input":"2022-05-21T01:17:39.045711Z","iopub.status.idle":"2022-05-21T01:17:39.532364Z","shell.execute_reply.started":"2022-05-21T01:17:39.045683Z","shell.execute_reply":"2022-05-21T01:17:39.531616Z"},"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Null Value Check\nprint('train_df.info()'); print(train_df.info(), '\\n')\n\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-21T01:17:39.534203Z","iopub.execute_input":"2022-05-21T01:17:39.534906Z","iopub.status.idle":"2022-05-21T01:17:39.589578Z","shell.execute_reply.started":"2022-05-21T01:17:39.53487Z","shell.execute_reply":"2022-05-21T01:17:39.588876Z"},"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id =\"1.1\"></a><h2 style=\"background:#d9afed; border:0; border-radius: 8px; color:black\"><center>1.1 Feature Engineering</center></h2>","metadata":{}},{"cell_type":"code","source":"## Separate 'id' columns' texts, and create new id columns.\n## This code takes about 2 minutets to execute.\n\ndef create_id_list(text, p_train = pathlib.Path(data_config['train_folder_path'])):\n    t = text.split('_')\n    \n    case_id = t[0][4:]\n    day_id = t[1][3:]\n    slice_id = t[3]\n    \n    case_folder = t[0]\n    day_folder = ('_').join([t[0], t[1]])\n    slice_file = ('_').join([t[2], t[3]])\n    \n    p_folder = p_train / case_folder / day_folder / 'scans'\n    file_name = [p.name for p in p_folder.iterdir() if p.name[6:10] == slice_id]\n    id_list = [case_id, day_id, slice_id, case_folder, day_folder, slice_file]\n    id_list.extend(file_name)    \n    return id_list\n\ndef create_new_ids(dataframe, new_ids = ['case_id', 'day_id', 'slice_id', 'case_folder', 'day_folder', 'slice_file', 'file_name']):\n    dataframe['id_list'] = dataframe['id'].map(create_id_list)   \n    for i, item in enumerate(new_ids):\n        dataframe[item] = dataframe['id_list'].map(lambda x: x[i])\n    dataframe = dataframe.drop(['id_list'], axis=1)\n    return dataframe\n\ntrain_df = create_new_ids(train_df)\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-21T01:17:39.609946Z","iopub.execute_input":"2022-05-21T01:17:39.610211Z","iopub.status.idle":"2022-05-21T01:19:12.558868Z","shell.execute_reply.started":"2022-05-21T01:17:39.610163Z","shell.execute_reply":"2022-05-21T01:19:12.558224Z"},"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Create detection column (1: non NaN segmentation, 0: NaN segmentation).\ntrain_df['detection'] = train_df['segmentation'].notna() * 1\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-21T01:19:12.560147Z","iopub.execute_input":"2022-05-21T01:19:12.560555Z","iopub.status.idle":"2022-05-21T01:19:12.584544Z","shell.execute_reply.started":"2022-05-21T01:19:12.560518Z","shell.execute_reply":"2022-05-21T01:19:12.583736Z"},"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"total_img_n = int(len(train_df) / 3)\nprint('The number of imgs: ', total_img_n)","metadata":{"execution":{"iopub.status.busy":"2022-05-21T01:19:12.585939Z","iopub.execute_input":"2022-05-21T01:19:12.586201Z","iopub.status.idle":"2022-05-21T01:19:12.590914Z","shell.execute_reply.started":"2022-05-21T01:19:12.586154Z","shell.execute_reply":"2022-05-21T01:19:12.590266Z"},"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Calculate segmentation areas and img size.\ndef cal_pos_area(segmentation):\n    pos_area = 0\n    if type(segmentation) is str:\n        seg_list = segmentation.split(' ')\n        for i in range(len(seg_list)//2):\n            pos_area += int(seg_list[i*2 + 1])\n    return pos_area\n\ndef cal_total_area(file_name):\n    img_h = int(file_name[11:14])\n    img_w = int(file_name[15:18])\n    total_area = img_h * img_w\n    return total_area\n\ntrain_df['pos_area'] = train_df['segmentation'].map(cal_pos_area)\ntrain_df['total_area'] = train_df['file_name'].map(cal_total_area)\ntrain_df['pos_area_percentage'] = train_df['pos_area'] / train_df['total_area'] * 100\n\n## Check\ntrain_df[1920:1930]","metadata":{"execution":{"iopub.status.busy":"2022-05-21T01:19:12.592277Z","iopub.execute_input":"2022-05-21T01:19:12.592719Z","iopub.status.idle":"2022-05-21T01:19:13.626706Z","shell.execute_reply.started":"2022-05-21T01:19:12.592683Z","shell.execute_reply":"2022-05-21T01:19:13.626022Z"},"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Split the samples based on the 'class'.\ntrain_lb_df = train_df[train_df['class']=='large_bowel'].reset_index(drop=True)\ntrain_sb_df = train_df[train_df['class']=='small_bowel'].reset_index(drop=True)\ntrain_st_df = train_df[train_df['class']=='stomach'].reset_index(drop=True)\n\n## Calculate each segmentation pixels' ratio to the total img pixels.\nlb_area_ratio = train_lb_df['pos_area'].sum() / train_lb_df['total_area'].sum()\nsb_area_ratio = train_sb_df['pos_area'].sum() / train_sb_df['total_area'].sum()\nst_area_ratio = train_st_df['pos_area'].sum() / train_st_df['total_area'].sum()\nbg_area_ratio = 1 - (lb_area_ratio + sb_area_ratio + st_area_ratio)\n\nprint(lb_area_ratio, sb_area_ratio, st_area_ratio, bg_area_ratio)","metadata":{"execution":{"iopub.status.busy":"2022-05-21T01:19:13.629642Z","iopub.execute_input":"2022-05-21T01:19:13.630283Z","iopub.status.idle":"2022-05-21T01:19:13.762971Z","shell.execute_reply.started":"2022-05-21T01:19:13.63024Z","shell.execute_reply":"2022-05-21T01:19:13.76208Z"},"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Split the samples which have non-null values in 'segmentation' as positive ones.\ntrain_positive_df = train_df.dropna(subset=['segmentation']).reset_index(drop=True)\ntrain_negative_df = train_df[train_df['segmentation'].isna()].reset_index(drop=True)\n\npos_lb_df = train_positive_df[train_positive_df['class']=='large_bowel'].reset_index(drop=True)\npos_sb_df = train_positive_df[train_positive_df['class']=='small_bowel'].reset_index(drop=True)\npos_st_df = train_positive_df[train_positive_df['class']=='stomach'].reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-21T01:19:13.764317Z","iopub.execute_input":"2022-05-21T01:19:13.764582Z","iopub.status.idle":"2022-05-21T01:19:13.88984Z","shell.execute_reply.started":"2022-05-21T01:19:13.764548Z","shell.execute_reply":"2022-05-21T01:19:13.889093Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id =\"2\"></a><h1 style=\"background:#a1a8f0; border:0; border-radius: 10px; color:black\"><center>2. Exploratory Data Analysis</center></h1>","metadata":{}},{"cell_type":"code","source":"## Plot the bar graph of the detection percentages (per total number of images) of each classes.\nclass_group = train_df.groupby(['class'])['detection'].mean() * 100\n\nfig = px.bar(class_group)\nfig.update_layout(title = \"<span style='font-size:36px;>Detection Percentages (per total number of images) of Each Classes</span>\", \n                  yaxis_title = 'detection percentage')  ","metadata":{"execution":{"iopub.status.busy":"2022-05-21T01:19:13.891353Z","iopub.execute_input":"2022-05-21T01:19:13.891608Z","iopub.status.idle":"2022-05-21T01:19:14.659292Z","shell.execute_reply.started":"2022-05-21T01:19:13.891575Z","shell.execute_reply":"2022-05-21T01:19:14.658605Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Plot the histogram of the detection percentage of large_bowel class in each case_ids\nlb_detection_mean = train_lb_df.groupby(['case_id'])['detection'].mean() * 100\nfig = px.histogram(lb_detection_mean, nbins=25, marginal='box')\nfig.update_layout(title = \"<span style='font-size:36px;>Detection Percentage of 'large_bowel' in Each Case_ids</span>\",\n                  xaxis_title = 'detection percentage')  ","metadata":{"execution":{"iopub.status.busy":"2022-05-21T01:19:14.660694Z","iopub.execute_input":"2022-05-21T01:19:14.660966Z","iopub.status.idle":"2022-05-21T01:19:14.792874Z","shell.execute_reply.started":"2022-05-21T01:19:14.660931Z","shell.execute_reply":"2022-05-21T01:19:14.792085Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Plot the histogram of the detection percentage of small_bowel class in each case_ids\nsb_detection_mean = train_sb_df.groupby(['case_id'])['detection'].mean() * 100\nfig = px.histogram(sb_detection_mean, nbins=25, marginal='box')\nfig.update_layout(title = \"<span style='font-size:36px;>Detection Percentage of 'small_bowel' in Each Case_ids</span>\", \n                  xaxis_title = 'detection percentage')  ","metadata":{"execution":{"iopub.status.busy":"2022-05-21T01:19:14.794399Z","iopub.execute_input":"2022-05-21T01:19:14.794651Z","iopub.status.idle":"2022-05-21T01:19:14.88235Z","shell.execute_reply.started":"2022-05-21T01:19:14.794616Z","shell.execute_reply":"2022-05-21T01:19:14.881625Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Plot the histogram of the detection percentage of stomach class in each case_ids\nst_detection_mean = train_st_df.groupby(['case_id'])['detection'].mean() * 100\nfig = px.histogram(st_detection_mean, nbins=25, marginal='box')\nfig.update_layout(title = \"<span style='font-size:36px;>Histogram of Detection Percentage of 'stomach' in Each Case_ids</span>\", \n                  xaxis_title = 'detection percentage')  ","metadata":{"execution":{"iopub.status.busy":"2022-05-21T01:19:14.883497Z","iopub.execute_input":"2022-05-21T01:19:14.884308Z","iopub.status.idle":"2022-05-21T01:19:14.974569Z","shell.execute_reply.started":"2022-05-21T01:19:14.884271Z","shell.execute_reply":"2022-05-21T01:19:14.973906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Compare the above three histograms in one figure.\nfig = go.Figure()\nlb_detection_mean = train_lb_df.groupby(['case_id'])['detection'].mean() * 100\nfig.add_trace(go.Histogram(x=lb_detection_mean.values, nbinsx=25, \n                           opacity=0.5, name='large_bowel',\n                           histnorm='probability'))\n\nsb_detection_mean = train_sb_df.groupby(['case_id'])['detection'].mean() * 100\nfig.add_trace(go.Histogram(x=sb_detection_mean.values, nbinsx=25, \n                           opacity=0.5, name='small_bowel',\n                           histnorm='probability'))\n\nst_detection_mean = train_st_df.groupby(['case_id'])['detection'].mean() * 100\nfig.add_trace(go.Histogram(x=st_detection_mean.values, nbinsx=25, \n                           opacity=0.5, name='stomach',\n                           histnorm='probability'))\nfig.update_layout(barmode='overlay', \n                  title = \"<span style='font-size:36px;>Comparison of Detection Percentages in Each Case_ids</span>\",\n                  xaxis_title = 'detection percentage',\n                  yaxis_title = 'n_cases / n_total_cases')","metadata":{"execution":{"iopub.status.busy":"2022-05-21T01:19:14.975684Z","iopub.execute_input":"2022-05-21T01:19:14.976483Z","iopub.status.idle":"2022-05-21T01:19:15.019801Z","shell.execute_reply.started":"2022-05-21T01:19:14.976445Z","shell.execute_reply":"2022-05-21T01:19:15.019047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Plot the bar graph of the detection area percentages of three classes.\nclass_group = train_positive_df.groupby(['class'])['pos_area_percentage']\n\nfig = px.bar(class_group.mean(), error_y=class_group.std())\nfig.update_layout(title = \"<span style='font-size:36px;>Mean Area Percentages (for the area of an image) of Eacg Classes</span>\", \n                  yaxis_title = 'detection area percentage') ","metadata":{"execution":{"iopub.status.busy":"2022-05-21T01:19:15.020997Z","iopub.execute_input":"2022-05-21T01:19:15.021913Z","iopub.status.idle":"2022-05-21T01:19:15.0925Z","shell.execute_reply.started":"2022-05-21T01:19:15.021869Z","shell.execute_reply":"2022-05-21T01:19:15.091828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Plot the histogram of the detection area percentages of three classes.\nfig = go.Figure()\nfig.add_trace(go.Histogram(x=pos_lb_df['pos_area_percentage'], nbinsx=100, \n                           opacity=0.5, name='large_bowel',\n                           histnorm='probability'))\nfig.add_trace(go.Histogram(x=pos_sb_df['pos_area_percentage'], nbinsx=100, \n                           opacity=0.5, name='small_bowel',\n                           histnorm='probability'))\nfig.add_trace(go.Histogram(x=pos_st_df['pos_area_percentage'], nbinsx=100, \n                           opacity=0.5, name='stomach',\n                           histnorm='probability'))\nfig.update_layout(barmode='overlay', \n                  title = \"<span style='font-size:36px;>Comparison of Detection Area Percentage </span>\",\n                  xaxis_title = 'detection area percentage',\n                  yaxis_title = 'n_images / n_total_detected_imgs')","metadata":{"execution":{"iopub.status.busy":"2022-05-21T01:19:15.093638Z","iopub.execute_input":"2022-05-21T01:19:15.094336Z","iopub.status.idle":"2022-05-21T01:19:15.12393Z","shell.execute_reply.started":"2022-05-21T01:19:15.094297Z","shell.execute_reply":"2022-05-21T01:19:15.123238Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id =\"3\"></a><h1 style=\"background:#a1a8f0; border:0; border-radius: 10px; color:black\"><center>3. Dataset & DataLoader</center></h1>","metadata":{}},{"cell_type":"code","source":"## Train - Valid - Test split\n## I split the train, valid, test data based on the case_id (imgs that have the same case_id are assigned in the same set).\n\ntrain_ratio = 0.85\nvalid_ratio = 0.10\ntest_ratio = 0.05\n\ncase_ids = train_df['case_id'].unique()\nidxs = np.random.permutation(range(len(case_ids)))\ncut_1 = int(train_ratio * len(idxs))\ncut_2 = int((train_ratio + valid_ratio) * len(idxs))\n\ntrain_case_ids = case_ids[idxs[:cut_1]]\nvalid_case_ids = case_ids[idxs[cut_1:cut_2]]\ntest_case_ids = case_ids[idxs[cut_2:]]\n\ntrain = train_df.query('case_id in @train_case_ids')\nvalid = train_df.query('case_id in @valid_case_ids')\ntest = train_df.query('case_id in @test_case_ids')\n\nprint(len(train), len(valid), len(test), len(train_df))","metadata":{"execution":{"iopub.status.busy":"2022-05-21T01:19:15.124831Z","iopub.execute_input":"2022-05-21T01:19:15.125037Z","iopub.status.idle":"2022-05-21T01:19:15.296056Z","shell.execute_reply.started":"2022-05-21T01:19:15.125009Z","shell.execute_reply":"2022-05-21T01:19:15.292054Z"},"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_case_folders = train['case_folder'].unique()\ntrain_files = []\nfor case_folder in train_case_folders:\n    p_train = pathlib.Path(data_config['train_folder_path'])\n    p_folder = p_train / case_folder\n    tmp_files = list(p_folder.glob('**/scans/*.png'))\n    train_files.extend(tmp_files)\n    \nvalid_case_folders = valid['case_folder'].unique()\nvalid_files = []\nfor case_folder in valid_case_folders:\n    p_train = pathlib.Path(data_config['train_folder_path'])\n    p_folder = p_train / case_folder\n    tmp_files = list(p_folder.glob('**/scans/*.png'))\n    valid_files.extend(tmp_files)\n    \ntest_case_folders = test['case_folder'].unique()\ntest_files = []\nfor case_folder in test_case_folders:\n    p_train = pathlib.Path(data_config['train_folder_path'])\n    p_folder = p_train / case_folder\n    tmp_files = list(p_folder.glob('**/scans/*.png'))\n    test_files.extend(tmp_files)\n    \nprint(len(train_files), len(valid_files), len(test_files))","metadata":{"execution":{"iopub.status.busy":"2022-05-21T01:19:15.301802Z","iopub.execute_input":"2022-05-21T01:19:15.302208Z","iopub.status.idle":"2022-05-21T01:19:17.099822Z","shell.execute_reply.started":"2022-05-21T01:19:15.302143Z","shell.execute_reply":"2022-05-21T01:19:17.098902Z"},"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Building Dataset and DataLoader\nclass UWMadison2022Dataset(torch.utils.data.Dataset):\n    def __init__(self, files, dataframe=None, input_shape=256,):\n        self.files = files\n        self.df = dataframe\n        self.input_shape = input_shape\n        self.transforms = transforms.Compose([\n            transforms.CenterCrop(self.input_shape),\n            transforms.Normalize(mean=[(0.485+0.456+0.406)/3], std=[(0.229+0.224+0.225)/3]),\n        ])\n        \n    def __len__(self):\n        return len(self.files)\n    \n    def __getitem__(self, idx):\n        p_file = self.files[idx]\n        #img = torchvision.io.read_image(p_file)\n        img = np.array(Image.open(p_file))\n        img_shape = torch.tensor(img.shape)\n        img = transforms.functional.to_tensor(img) / 255.\n        img = self.transforms(img)\n        #img = torch.cat([img, img, img], dim=0)\n        \n        if self.df is not None:\n            f_name = str(p_file).split('/')\n            case_day_id = f_name[5]\n            slice_id = f_name[7][:10]\n            f_id = '_'.join([case_day_id, slice_id])\n            labels_df = self.df.query('id == @f_id')\n            \n            label = torch.zeros([img_shape[0]*img_shape[1]])\n            for i, organ in enumerate(['large_bowel', 'small_bowel', 'stomach']):\n                segmentation = labels_df[labels_df['class'] == organ]['segmentation'].item()\n                if type(segmentation) is str:\n                    segmentation = segmentation.split(' ')\n                    for j in range(len(segmentation)//2):\n                        start_idx = int(segmentation[j*2])\n                        span = int(segmentation[j*2 + 1])\n                        label[start_idx:(start_idx+span)] = (i+1)\n            label = torch.reshape(label, (img_shape[0], img_shape[1]))\n            label = transforms.CenterCrop(self.input_shape)(label)\n            label = torch.nn.functional.one_hot(label.to(torch.int64), num_classes=4)\n            label = label.permute(2, 0, 1)\n            return img, label, img_shape\n        \n        else: return img, img_shape\n        \ntrain_ds = UWMadison2022Dataset(train_files, train, input_shape=256)\nvalid_ds = UWMadison2022Dataset(valid_files, valid, input_shape=256)\ntest_ds = UWMadison2022Dataset(test_files, test, input_shape=256)\n\nBATCH_SIZE = 32\n\n## Checking dataset and dataloder  \nprint('------ train_dl ------')\ntrain_dl = torch.utils.data.DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\ntmp = train_dl.__iter__()\nx, y, shape = tmp.next()\nprint(f\"x : {x.shape}\")\nprint(f\"labels: {y.shape}\")\nprint(f\"img_shapes: {shape.shape}\")\nprint(f\"n_samples: {len(train_ds)}\")\nprint(f\"n_batches: {len(tmp)}\")\nprint()\n\nprint('------ valid_dl ------')\nvalid_dl = torch.utils.data.DataLoader(valid_ds, batch_size=BATCH_SIZE, shuffle=False, drop_last=True)\ntmp = valid_dl.__iter__()\nx, y, shape = tmp.next()\nprint(f\"x : {x.shape}\")\nprint(f\"labels: {y.shape}\")\nprint(f\"img_shapes: {shape.shape}\")\nprint(f\"n_samples: {len(valid_ds)}\")\nprint(f\"n_batches: {len(tmp)}\")\nprint()\n\nprint('------ test_dl ------')\ntest_dl = torch.utils.data.DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\ntmp = test_dl.__iter__()\nx, y, shape = tmp.next()\nprint(f\"x : {x.shape}\")\nprint(f\"labels: {y.shape}\")\nprint(f\"img_shapes: {shape.shape}\")\nprint(f\"n_samples: {len(test_ds)}\")\nprint(f\"n_batches: {len(tmp)}\")\nprint()","metadata":{"execution":{"iopub.status.busy":"2022-05-21T01:19:17.101187Z","iopub.execute_input":"2022-05-21T01:19:17.101483Z","iopub.status.idle":"2022-05-21T01:19:18.712637Z","shell.execute_reply.started":"2022-05-21T01:19:17.101443Z","shell.execute_reply":"2022-05-21T01:19:18.711238Z"},"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id =\"4\"></a><h1 style=\"background:#a1a8f0; border:0; border-radius: 10px; color:black\"><center>4. Model Building</center></h1>","metadata":{}},{"cell_type":"markdown","source":"<a id =\"4.1\"></a><h2 style=\"background:#d9afed; border:0; border-radius: 8px; color:black\"><center>4.1 PSPNet</center></h2>","metadata":{}},{"cell_type":"markdown","source":"- **[PSPNet](https://arxiv.org/abs/1612.01105), or Pyramid Scene Parsing Network, is a semantic segmentation model that utilises a pyramid pooling module that exploits global context information by different-region based context aggregation.**\n- **We use ResNet18 (pretrained on Imagenet) for the encoder, and build pyramid pooling module and decoder from scratch.**\n\n<img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F482094%2F176d3ce5-8e3a-cd40-efd4-526f1702f343.png?ixlib=rb-4.0.0&auto=format&gif-q=60&q=75&w=1400&fit=max&s=0fa30aa09b4880ee27a38839bd007bc4\" width=\"750\"/>\n\n- **The feature map size is 1/8 of the input image.**\n- **We use the 4-level pyramid pooling module to gather various context information.**\n- **Then, it is followed by a convolution and upsampling layer to generate the final prediction.**\n- **For the auxiliary of training, auxiliary (Aux) loss is added to the final loss of the network.**\n- **The idea of Aux loss comes from [GoogLeNet](https://arxiv.org/abs/1409.4842) paper. Auxiliary network's task is to predict pixel's labels using the intermediate output of encoder module. When the encoder generates big loss, that helps to improve the encoder directly.**","metadata":{}},{"cell_type":"code","source":"## Structure of Resnet18\nresnet = resnet18(pretrained=True)\nbatch_size = 16\n\nsummary(\n    resnet,\n    input_size=(batch_size, 3, 256, 256),\n    col_names=[\"output_size\", \"num_params\"],\n)","metadata":{"execution":{"iopub.status.busy":"2022-05-21T01:19:18.713945Z","iopub.execute_input":"2022-05-21T01:19:18.714325Z","iopub.status.idle":"2022-05-21T01:19:28.198804Z","shell.execute_reply.started":"2022-05-21T01:19:18.714278Z","shell.execute_reply":"2022-05-21T01:19:28.198095Z"},"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will use the feature outputs of ResNet18 at **Sequential: 1-5 [batch_size, 64, 64, 64] ('conv2_x' in the figure below)** for the inputs of Auxiliary network, and **Sequential: 1-6 [batch_size, 128, 32, 32] ('conv3_x' in the figure below)** for the PyramidPooling module.\n\n\n- **Reference: Comparison of various ResNets' architecture**\n\n<img src=\"https://pystyle.info/wp/wp-content/uploads/2021/11/pytorch-resnet_06.jpg\" width=\"750\"/>","metadata":{}},{"cell_type":"code","source":"class conv2DBatchNorm(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size,\n                 stride, padding, dilation, bias, activation=False):\n        super().__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels,\n                              kernel_size, stride, \n                              padding, dilation, bias=bias)\n        self.batchnorm = nn.BatchNorm2d(out_channels)\n        self.activation = activation\n        if self.activation:\n            self.relu = nn.ReLU(inplace=True)\n        \n    def forward(self, x):\n        x = self.conv(x)\n        x = self.batchnorm(x)\n        if self.activation:\n            outputs = self.relu(x)\n        else:\n            outputs = x\n            \n        return outputs","metadata":{"execution":{"iopub.status.busy":"2022-05-21T01:19:28.200165Z","iopub.execute_input":"2022-05-21T01:19:28.200566Z","iopub.status.idle":"2022-05-21T01:19:28.208523Z","shell.execute_reply.started":"2022-05-21T01:19:28.200528Z","shell.execute_reply":"2022-05-21T01:19:28.20765Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class FeatureExtractor(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.resnet = resnet18(pretrained=True)\n        # Change first conv layer to accept single-channel (grayscale) input\n        self.resnet.conv1.weight = torch.nn.Parameter(self.resnet.conv1.weight.sum(dim=1).unsqueeze(1)) \n        \n    def forward(self, x):\n        for i in range(6):\n            x = list(self.resnet.children())[i](x)\n            if i == 4:\n                aux_inputs = x\n        encoder_outputs = x\n        \n        return encoder_outputs, aux_inputs","metadata":{"execution":{"iopub.status.busy":"2022-05-21T01:19:28.210245Z","iopub.execute_input":"2022-05-21T01:19:28.210801Z","iopub.status.idle":"2022-05-21T01:19:28.237143Z","shell.execute_reply.started":"2022-05-21T01:19:28.21075Z","shell.execute_reply":"2022-05-21T01:19:28.23624Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class PyramidPooling(nn.Module):\n    def __init__(self, in_channels, pool_sizes, height, width):\n        super().__init__()\n        \n        self.height = height\n        self.width = width\n        \n        out_channels = int(in_channels / len(pool_sizes))\n        \n        ## pool__sizes: [6, 3, 2, 1]\n        self.avpool_1 = nn.AdaptiveAvgPool2d(output_size=pool_sizes[0])\n        self.cbr_1 = conv2DBatchNorm(\n            in_channels, out_channels, kernel_size=1, stride=1,\n            padding=0, dilation=1, bias=False, activation=True)\n        \n        self.avpool_2 = nn.AdaptiveAvgPool2d(output_size=pool_sizes[1])\n        self.cbr_2 = conv2DBatchNorm(\n            in_channels, out_channels, kernel_size=1, stride=1,\n            padding=0, dilation=1, bias=False, activation=True)\n        \n        self.avpool_3 = nn.AdaptiveAvgPool2d(output_size=pool_sizes[2])\n        self.cbr_3 = conv2DBatchNorm(\n            in_channels, out_channels, kernel_size=1, stride=1,\n            padding=0, dilation=1, bias=False, activation=True)\n        \n        self.avpool_4 = nn.AdaptiveAvgPool2d(output_size=pool_sizes[3])\n        self.cbr_4 = conv2DBatchNorm(\n            in_channels, out_channels, kernel_size=1, stride=1,\n            padding=0, dilation=1, bias=False, activation=True)\n        \n    def forward(self, x):\n        out1 = self.cbr_1(self.avpool_1(x))\n        out1 = F.interpolate(out1, size=(\n            self.height, self.width), mode='bilinear', align_corners=True)\n        \n        out2 = self.cbr_2(self.avpool_2(x))\n        out2 = F.interpolate(out2, size=(\n            self.height, self.width), mode='bilinear', align_corners=True)\n        \n        out3 = self.cbr_3(self.avpool_3(x))\n        out3 = F.interpolate(out3, size=(\n            self.height, self.width), mode='bilinear', align_corners=True)\n        \n        out4 = self.cbr_4(self.avpool_4(x))\n        out4 = F.interpolate(out4, size=(\n            self.height, self.width), mode='bilinear', align_corners=True)\n        \n        output = torch.cat([x, out1, out2, out3, out4], dim=1)\n        \n        return output","metadata":{"execution":{"iopub.status.busy":"2022-05-21T01:19:28.239257Z","iopub.execute_input":"2022-05-21T01:19:28.239569Z","iopub.status.idle":"2022-05-21T01:19:28.256903Z","shell.execute_reply.started":"2022-05-21T01:19:28.239527Z","shell.execute_reply":"2022-05-21T01:19:28.256068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class DecodePSPFeature(nn.Module):\n    def __init__(self, height, width, n_classes):\n        super().__init__()\n        \n        self.height = height\n        self.width = width\n        \n        self.cbr = conv2DBatchNorm(\n            in_channels=256, out_channels=64, kernel_size=3, stride=1, \n            padding=1, dilation=1,  bias=False, activation=True)\n        self.dropout = nn.Dropout2d(p=0.1)\n        self.classification = nn.Conv2d(\n            in_channels=64, out_channels=n_classes, kernel_size=1,\n            stride=1, padding=0)\n        \n    def forward(self, x):\n        x = self.cbr(x)\n        x = self.dropout(x)\n        x = self.classification(x)\n        output = F.interpolate(\n            x, size=(self.height, self.width), \n            mode='bilinear', align_corners=True)\n        \n        return output","metadata":{"execution":{"iopub.status.busy":"2022-05-21T01:19:28.258392Z","iopub.execute_input":"2022-05-21T01:19:28.258781Z","iopub.status.idle":"2022-05-21T01:19:28.27041Z","shell.execute_reply.started":"2022-05-21T01:19:28.25874Z","shell.execute_reply":"2022-05-21T01:19:28.269385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class AuxiliaryPSPlayers(nn.Module):\n    def __init__(self, in_channels, height, width, n_classes):\n        super().__init__()\n        \n        self.height = height\n        self.width = width\n        \n        self.cbr = conv2DBatchNorm(\n            in_channels=in_channels, out_channels=64,\n            kernel_size=3, stride=1, padding=1, \n            dilation=1, bias=False, activation=True)\n        self.dropout = nn.Dropout2d(p=0.1)\n        self.classification = nn.Conv2d(\n            in_channels=64, out_channels=n_classes, \n            kernel_size=1, stride=1, padding=0)\n        \n    def forward(self, x):\n        x = self.cbr(x)\n        x = self.dropout(x)\n        x = self.classification(x)\n        output = F.interpolate(\n            x, size=(self.height, self.width), \n            mode='bilinear', align_corners=True)\n        \n        return output","metadata":{"execution":{"iopub.status.busy":"2022-05-21T01:19:28.271973Z","iopub.execute_input":"2022-05-21T01:19:28.272841Z","iopub.status.idle":"2022-05-21T01:19:28.283674Z","shell.execute_reply.started":"2022-05-21T01:19:28.272791Z","shell.execute_reply":"2022-05-21T01:19:28.282902Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class PSPNet(nn.Module):\n    def __init__(self, n_classes):\n        super().__init__()\n        \n        full_img_size = 256\n        feature_map_size = 32\n        \n        self.feature_extractor = FeatureExtractor()\n        self.pyramid_pooling = PyramidPooling(\n            in_channels=128, pool_sizes=[6, 3, 2, 1],\n            height=feature_map_size, width=feature_map_size)\n        self.decode_feature = DecodePSPFeature(\n            height=full_img_size, width=full_img_size,\n            n_classes=n_classes)\n        self.aux = AuxiliaryPSPlayers(\n            in_channels=64, n_classes=n_classes,\n            height=full_img_size, width=full_img_size)\n        \n    def forward(self, x):\n        encoder_outputs, aux_inputs = self.feature_extractor(x)\n        pyramid_outputs = self.pyramid_pooling(encoder_outputs)\n        docoder_outputs = self.decode_feature(pyramid_outputs)\n        aux_outputs = self.aux(aux_inputs)\n        \n        return (docoder_outputs, aux_outputs)","metadata":{"execution":{"iopub.status.busy":"2022-05-21T01:19:28.289013Z","iopub.execute_input":"2022-05-21T01:19:28.289665Z","iopub.status.idle":"2022-05-21T01:19:28.298069Z","shell.execute_reply.started":"2022-05-21T01:19:28.289624Z","shell.execute_reply":"2022-05-21T01:19:28.297334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = PSPNet(n_classes=4)\n\nsummary(\n    model,\n    input_size=(batch_size, 1, 256, 256),\n    col_names=[\"output_size\", \"num_params\"],\n)","metadata":{"execution":{"iopub.status.busy":"2022-05-21T01:19:28.299654Z","iopub.execute_input":"2022-05-21T01:19:28.300588Z","iopub.status.idle":"2022-05-21T01:19:28.608825Z","shell.execute_reply.started":"2022-05-21T01:19:28.300482Z","shell.execute_reply":"2022-05-21T01:19:28.608135Z"},"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## We calculate the final loss and Aux loss each other, and mix them up.\n\nclass PSPLoss(nn.Module):\n    def __init__(self, aux_weight=0.4):\n        super().__init__()\n        self.aux_weight = aux_weight\n        \n    def forward(self, outputs, targets):\n        loss = F.cross_entropy(outputs[0], targets, reduction='mean')\n        loss_aux = F.cross_entropy(outputs[1], targets, reduction='mean')\n        \n        return loss + self.aux_weight * loss_aux\n    \ncriterion = PSPLoss(aux_weight=0.4)","metadata":{"execution":{"iopub.status.busy":"2022-05-21T01:19:28.610109Z","iopub.execute_input":"2022-05-21T01:19:28.610524Z","iopub.status.idle":"2022-05-21T01:19:28.617477Z","shell.execute_reply.started":"2022-05-21T01:19:28.610486Z","shell.execute_reply":"2022-05-21T01:19:28.616833Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## In the inference phase, we don't use Aux loss.\n\n#model.eval()\n#outputs = model(x)\n#pred = outputs[0]","metadata":{"execution":{"iopub.status.busy":"2022-05-21T01:19:28.618969Z","iopub.execute_input":"2022-05-21T01:19:28.619244Z","iopub.status.idle":"2022-05-21T01:19:28.625537Z","shell.execute_reply.started":"2022-05-21T01:19:28.619208Z","shell.execute_reply":"2022-05-21T01:19:28.624812Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id =\"4.2\"></a><h2 style=\"background:#d9afed; border:0; border-radius: 8px; color:black\"><center>4.2 U-Net (decoder from scratch)</center></h2>","metadata":{}},{"cell_type":"markdown","source":"- **The U-Net consists of encoder - decoder network architecture.**\n- **We use ResNet18 (pretrained on Imagenet) for the encoder, and build decoder from scratch.**\n- **We have to make skip connections from encoder to decoder.**\n\n<img src=\"https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/u-net-architecture.png\" width=\"500\"/>","metadata":{}},{"cell_type":"markdown","source":"We will use the feature outputs of ResNet18 at \n- **ReLU: 1-3 [batch_size, 64, 128, 128]**\n- **Sequential: 1-5 [batch_size, 64, 64, 64]**\n- **Sequential: 1-6 [batch_size, 128, 32, 32]**\n- **Sequential: 1-7 [batch_size, 256, 16, 16]**\n\n for the skip connections. And, outputs at \n\n- **Sequential: 1-8 [16, 512, 8, 8]**\n\n is for the bottleneck features (first inputs for the decoder).","metadata":{}},{"cell_type":"code","source":"## The Extractor of intermediate features of ResNet (encoder) for the skip connections to the decoder.\nclass FeatureExtractor(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.resnet = resnet18(pretrained=True)\n        # Change first conv layer to accept single-channel (grayscale) input\n        self.resnet.conv1.weight = torch.nn.Parameter(self.resnet.conv1.weight.sum(dim=1).unsqueeze(1)) \n        \n    def forward(self, x):\n        skip_connections = []\n        for i in range(8):\n            x = list(self.resnet.children())[i](x)\n            if i in [2, 4, 5, 6, 7]:\n                skip_connections.append(x)\n        encoder_outputs = skip_connections.pop(-1)\n        skip_connections = skip_connections[::-1]\n        \n        return encoder_outputs, skip_connections","metadata":{"execution":{"iopub.status.busy":"2022-05-21T01:19:28.627048Z","iopub.execute_input":"2022-05-21T01:19:28.627323Z","iopub.status.idle":"2022-05-21T01:19:28.635779Z","shell.execute_reply.started":"2022-05-21T01:19:28.627289Z","shell.execute_reply":"2022-05-21T01:19:28.635099Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## The modules used for building the decoder architecture.\nclass DoubleConv(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.dconv = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, 3, 1, 1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n        )\n        \n    def forward(self, x):\n        return  self.dconv(x)\n    \n    \nclass UnetUpSample(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.convt = nn.ConvTranspose2d(\n            in_channels, out_channels, kernel_size=2, stride=2)\n        self.norm1 = nn.BatchNorm2d(out_channels)\n        self.act1 = nn.ReLU(inplace=True)\n        self.dconv = DoubleConv(out_channels*2, out_channels)\n         \n    def forward(self, layer_input, skip_input):\n        u = self.convt(layer_input)\n        u = self.norm1(u)\n        u = self.act1(u)\n        u = torch.cat((u, skip_input), dim=1)\n        u = self.dconv(u)\n        return u","metadata":{"execution":{"iopub.status.busy":"2022-05-21T01:19:28.637121Z","iopub.execute_input":"2022-05-21T01:19:28.637759Z","iopub.status.idle":"2022-05-21T01:19:28.649809Z","shell.execute_reply.started":"2022-05-21T01:19:28.637722Z","shell.execute_reply":"2022-05-21T01:19:28.649005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## U-Net architechture.\nclass UW2022Unet(nn.Module):\n    def __init__(self, out_channels):\n        super().__init__()\n        self.encoder = FeatureExtractor()\n        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n        \n        self.upsample1 = UnetUpSample(512, 256)\n        self.upsample2 = UnetUpSample(256, 128)\n        self.upsample3 = UnetUpSample(128, 64)\n        self.upsample4 = UnetUpSample(64, 64)\n        \n        self.final_convt = nn.ConvTranspose2d(\n            64, 64, kernel_size=2, stride=2)\n        self.final_conv = nn.Conv2d(64, out_channels, kernel_size=1)\n        \n    def forward(self, x):\n        x1, skip_connections = self.encoder(x)\n        x2 = self.upsample1(x1, skip_connections[0])\n        x3 = self.upsample2(x2, skip_connections[1])\n        x4 = self.upsample3(x3, skip_connections[2])\n        x5 = self.upsample4(x4, skip_connections[3])\n        x6 = self.final_convt(x5)\n        \n        return self.final_conv(x6)\n    \nmodel = UW2022Unet(out_channels=4)\n\nsummary(\n    model,\n    input_size=(batch_size, 1, 256, 256),\n    col_names=[\"output_size\", \"num_params\"],\n)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-05-21T01:19:28.651126Z","iopub.execute_input":"2022-05-21T01:19:28.651768Z","iopub.status.idle":"2022-05-21T01:19:29.042029Z","shell.execute_reply.started":"2022-05-21T01:19:28.65173Z","shell.execute_reply":"2022-05-21T01:19:29.041075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id =\"4.3\"></a><h2 style=\"background:#d9afed; border:0; border-radius: 8px; color:black\"><center>4.3 Attention U-Net (decoder from scratch)</center></h2>","metadata":{}},{"cell_type":"markdown","source":"- **The [Attention U-Net](https://arxiv.org/abs/1804.03999) has attention architecture (the figure below) in the skip connections between encoder and decoder.**\n\n<img src=\"https://cdn-ak.f.st-hatena.com/images/fotolife/y/y_kurashina/20190429/20190429231337.jpg\" width=\"500\"/>\n<img src=\"https://cdn-ak.f.st-hatena.com/images/fotolife/y/y_kurashina/20190429/20190429233922.jpg\" width=\"500\"/>","metadata":{}},{"cell_type":"code","source":"## The Modules used for building Attention architecture.\nclass GateSignal(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.gate_conv = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True)\n        ) \n        \n    def forward(self, x):\n        return self.gate_conv(x)\n        \nclass AttentionBlock(nn.Module):\n    def __init__(self, gate_channels, x_channels, inter_channels):\n        super().__init__()\n        self.theta = nn.Conv2d(x_channels, inter_channels, kernel_size=2, stride=2)\n        self.phi = nn.Conv2d(gate_channels, inter_channels, kernel_size=1, stride=1)\n        self.act1 = nn.ReLU(inplace=True)\n        self.psi = nn.Conv2d(inter_channels, 1, kernel_size=1, stride=1)\n        self.act2 = nn.Sigmoid()\n        self.upsample = nn.ConvTranspose2d(1, 1, kernel_size=2, stride=2)\n        self.final_conv = nn.Conv2d(inter_channels, x_channels, kernel_size=1, stride=1, bias=False)\n        self.norm = nn.BatchNorm2d(x_channels)\n        \n    def forward(self, x, g):\n        theta_x = self.theta(x)\n        phi_g = self.phi(g)\n        xg = torch.add(theta_x, phi_g)\n        xg = self.act1(xg)\n        score = self.psi(xg)\n        score = self.act2(score)\n        score = self.upsample(score)\n        score = score.expand(x.shape)        \n        att_x = torch.mul(score, x)\n        att_x = self.final_conv(att_x) \n        att_x = self.norm(att_x)\n        return att_x","metadata":{"execution":{"iopub.status.busy":"2022-05-21T01:19:29.043116Z","iopub.execute_input":"2022-05-21T01:19:29.043485Z","iopub.status.idle":"2022-05-21T01:19:29.058406Z","shell.execute_reply.started":"2022-05-21T01:19:29.043449Z","shell.execute_reply":"2022-05-21T01:19:29.057637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Attention U-Net architechture.\nclass UW2022AttentionUnet(nn.Module):\n    def __init__(self, out_channels):\n        super().__init__()\n        self.encoder = FeatureExtractor()\n        \n        self.upsample1 = UnetUpSample(512, 256)\n        self.upsample2 = UnetUpSample(256, 128)\n        self.upsample3 = UnetUpSample(128, 64)\n        self.upsample4 = UnetUpSample(64, 64)\n        \n        self.gate_signal1 = GateSignal(512, 256)\n        self.gate_signal2 = GateSignal(256, 128)\n        self.gate_signal3 = GateSignal(128, 64)\n        self.gate_signal4 = GateSignal(64, 64)\n        \n        self.attention1 = AttentionBlock(256, 256, 256)\n        self.attention2 = AttentionBlock(128, 128, 128)\n        self.attention3 = AttentionBlock(64, 64, 64)\n        self.attention4 = AttentionBlock(64, 64, 64)\n        \n        self.final_convt = nn.ConvTranspose2d(64, 64, kernel_size=2, stride=2)\n        self.final_conv = nn.Conv2d(64, out_channels, kernel_size=1)\n        \n    def forward(self, x):\n        x1, skip_connections = self.encoder(x)\n        \n        g1 = self.gate_signal1(x1)\n        attention_skip1 = self.attention1(skip_connections[0], g1)\n        x2 = self.upsample1(x1, attention_skip1)\n        \n        g2 = self.gate_signal2(x2)\n        attention_skip2 = self.attention2(skip_connections[1], g2)\n        x3 = self.upsample2(x2, attention_skip2)\n        \n        g3 = self.gate_signal3(x3)\n        attention_skip3 = self.attention3(skip_connections[2], g3)\n        x4 = self.upsample3(x3, attention_skip3)\n        \n        g4 = self.gate_signal4(x4)\n        attention_skip4 = self.attention4(skip_connections[3], g4)\n        x5 = self.upsample4(x4, attention_skip4)\n\n        x6 = self.final_convt(x5)       \n        return self.final_conv(x6)\n\nmodel = UW2022AttentionUnet(out_channels=4)\n\nsummary(\n    model,\n    input_size=(batch_size, 1, 256, 256),\n    col_names=[\"output_size\", \"num_params\"],\n)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-05-21T01:19:29.059848Z","iopub.execute_input":"2022-05-21T01:19:29.060309Z","iopub.status.idle":"2022-05-21T01:19:29.562823Z","shell.execute_reply.started":"2022-05-21T01:19:29.060273Z","shell.execute_reply":"2022-05-21T01:19:29.562103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id =\"4.4\"></a><h2 style=\"background:#d9afed; border:0; border-radius: 8px; color:black\"><center>4.4 DeepLabv3</center></h2>","metadata":{}},{"cell_type":"markdown","source":"- **In [DeepLabv3](https://pytorch.org/hub/pytorch_vision_deeplabv3_resnet101/), Atrous Convolution (or Dilated Convolution) block is used to enlarge the field of view of filters to incorporate larger context.**\n\n<img src=\"https://miro.medium.com/max/1130/1*-r7CL0AkeO72MIDpjRxfog.png\" width=\"500\"/>\n<img src=\"https://miro.medium.com/max/1400/1*nFJ_GqK1D3zKCRgtnRfrcw.png\" width=\"750\"/>\n\n---\n- **In Atrous Spatial Pyramid Pooling (ASPP), parallel atrous convolution with different rate applied in the input feature map, and fuse together. As objects of the same class can have different scales in the image, ASPP helps to account for different object scales which can improve the accuracy.**\n\n<img src=\"https://miro.medium.com/max/1400/1*8Lg66z7e7ijuLmSkOzhYvA.png\" width=\"750\"/>","metadata":{}},{"cell_type":"code","source":"!pip install -q -U segmentation_models_pytorch\nimport segmentation_models_pytorch as smp\n\nmodel = smp.DeepLabV3(encoder_name='resnet34',\n                      encoder_depth=5,\n                      encoder_weights='imagenet',\n                      in_channels=1,\n                      classes=4,\n                      aux_params=None)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-05-21T01:19:29.564246Z","iopub.execute_input":"2022-05-21T01:19:29.564703Z","iopub.status.idle":"2022-05-21T01:19:45.890475Z","shell.execute_reply.started":"2022-05-21T01:19:29.564669Z","shell.execute_reply":"2022-05-21T01:19:45.889584Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"summary(\n    model,\n    input_size=(batch_size, 1, 256, 256),\n    col_names=[\"output_size\", \"num_params\"],\n)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-05-21T01:19:45.89209Z","iopub.execute_input":"2022-05-21T01:19:45.892643Z","iopub.status.idle":"2022-05-21T01:19:46.01395Z","shell.execute_reply.started":"2022-05-21T01:19:45.892603Z","shell.execute_reply":"2022-05-21T01:19:46.013248Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id =\"4.5\"></a><h2 style=\"background:#d9afed; border:0; border-radius: 8px; color:black\"><center>4.5 Swin-UNET</center></h2>","metadata":{}},{"cell_type":"markdown","source":"- **[Swin Transformer](https://github.com/microsoft/Swin-Transformer), is an applications of Transformers in vision problems.**\n\n<img src=\"https://miro.medium.com/max/1400/1*x50n-zyiU6TMHamEPM2HeA.png\" width=\"750\"/>\n\n---\n- **One of the biggest difference from ViT (Vision Transformer) is Swin Transformer block, which consists of a shifted window based Multi-Headed Self-Attention (MSA) module, and its efficient computation by shifted configuration (cyclic shift) method.**\n\n<img src=\"https://miro.medium.com/max/924/1*wc1e3QQu2AIhRWHvfId8pw.png\" width=\"500\"/>\n<img src=\"https://miro.medium.com/max/930/1*Z8MJcU0p2G3NayYmNV_dnQ.png\" width=\"500\"/>\n\n---\n- **The Architecture of Swin-Unet.**\n\n<img src=\"https://aisholar.s3.ap-northeast-1.amazonaws.com/media/April2022/2022_04_08_1f7df21e7573181e3115g-05.jpeg\" width=\"750\"/>","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\n\n## Limit GPU Memory in TensorFlow\n## Because TensorFlow, by default, allocates the full amount of available GPU memory when it is launched. \nphysical_devices = tf.config.list_physical_devices('GPU')\nif len(physical_devices) > 0:\n    for device in physical_devices:\n        tf.config.experimental.set_memory_growth(device, True)\n        print('{} memory growth: {}'.format(device, tf.config.experimental.get_memory_growth(device)))\nelse:\n    print(\"Not enough GPU hardware devices available\")","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-05-21T01:19:46.01785Z","iopub.execute_input":"2022-05-21T01:19:46.019782Z","iopub.status.idle":"2022-05-21T01:19:50.491438Z","shell.execute_reply.started":"2022-05-21T01:19:46.01974Z","shell.execute_reply":"2022-05-21T01:19:50.489081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install keras-unet-collection -q -U\nfrom keras_unet_collection import models, losses\n\ntf_model = models.swin_unet_2d((256, 256, 1), filter_num_begin=64,\n                               n_labels=4, depth=4, stack_num_down=2, stack_num_up=2,\n                               patch_size=(4, 4), num_heads=[4, 8, 8, 8],\n                               window_size=[4, 2, 2, 2], num_mlp=512, \n                               output_activation='Softmax', shift_window=True,\n                               name='swin_unet')","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-05-21T01:19:50.492637Z","iopub.execute_input":"2022-05-21T01:19:50.493365Z","iopub.status.idle":"2022-05-21T01:20:06.374208Z","shell.execute_reply.started":"2022-05-21T01:19:50.493325Z","shell.execute_reply":"2022-05-21T01:20:06.373466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf_model.compile(loss='binary_crossentropy',\n              optimizer=keras.optimizers.Adam(lr=1e-3),\n              metrics=['accuracy', losses.dice_coef])\ntf_model.summary()\n## To train this tf_model, we have to create TensorFlow Datasets.","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-05-21T01:20:06.375638Z","iopub.execute_input":"2022-05-21T01:20:06.375883Z","iopub.status.idle":"2022-05-21T01:20:06.419092Z","shell.execute_reply.started":"2022-05-21T01:20:06.375848Z","shell.execute_reply":"2022-05-21T01:20:06.418342Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del tf_model\nkeras.backend.clear_session()","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-05-21T01:20:06.420425Z","iopub.execute_input":"2022-05-21T01:20:06.420663Z","iopub.status.idle":"2022-05-21T01:20:06.428838Z","shell.execute_reply.started":"2022-05-21T01:20:06.420628Z","shell.execute_reply":"2022-05-21T01:20:06.427964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id =\"5\"></a><h1 style=\"background:#a1a8f0; border:0; border-radius: 10px; color:black\"><center>5. Training</center></h1>","metadata":{}},{"cell_type":"markdown","source":"Because this competition's label data (mask) is unbalanced (positive areas are smaller than negative areas), I used **\"Focal Loss\"** instead of normal cross entropy loss for the training loss function. It is said that focal loss function is good for multiclass classification where some classes are difficult to classify, and others are easy. **\"gamma\"** is a parameter of focal loss function that decides how emphasise the minor labels (the bigger gamma is, the more emphasis on minors we put).","metadata":{}},{"cell_type":"code","source":"## Focal Loss Function\nclass SegmentationFocalLoss(nn.Module):\n    def __init__(self, gamma=2, weight=None):\n        super().__init__()\n        self.gamma = gamma\n        if torch.cuda.is_available():\n            self.loss = torch.nn.CrossEntropyLoss(weight=weight).cuda()\n        else:\n            self.loss = nn.CrossEntropyLoss(weight=weight)\n\n    def forward(self, pred, target):\n        ce_loss = self.loss(pred, target)\n        #ce_loss = torch.nn.functional.cross_entropy(pred, target, reduce=False)\n        pt = torch.exp(-ce_loss)\n        focal_loss = (1. - pt) ** self.gamma * ce_loss\n        return torch.mean(focal_loss)\n\n##Setting the weight parameter of CrossEntropyLoss.\nlb_weight = 1 / lb_area_ratio\nsb_weight = 1 / sb_area_ratio\nst_weight = 1 / st_area_ratio\nbg_weight = 1 / bg_area_ratio\ntotal_weight = lb_weight + sb_weight + st_weight + bg_weight\n\nlb_weight = lb_weight / total_weight * 5\nsb_weight = sb_weight / total_weight * 5 \nst_weight = st_weight / total_weight * 5\nbg_weight = bg_weight / total_weight * 5\nweight = torch.tensor([bg_weight, lb_weight, sb_weight, st_weight], dtype=torch.float)\nprint(f'bg:{bg_weight}, lb:{lb_weight}, sb:{sb_weight}, st{st_weight}')\n\nloss_fn = SegmentationFocalLoss(gamma=3, weight=weight)","metadata":{"execution":{"iopub.status.busy":"2022-05-21T01:20:06.430835Z","iopub.execute_input":"2022-05-21T01:20:06.431032Z","iopub.status.idle":"2022-05-21T01:20:06.442096Z","shell.execute_reply.started":"2022-05-21T01:20:06.431007Z","shell.execute_reply":"2022-05-21T01:20:06.441306Z"},"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"LEARNING_RATE = 1e-4\noptimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)","metadata":{"execution":{"iopub.status.busy":"2022-05-21T01:20:06.443605Z","iopub.execute_input":"2022-05-21T01:20:06.444321Z","iopub.status.idle":"2022-05-21T01:20:06.453296Z","shell.execute_reply.started":"2022-05-21T01:20:06.444279Z","shell.execute_reply":"2022-05-21T01:20:06.452445Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## For the model training loop.\nif torch.cuda.is_available():\n    DEVICE = 'cuda'\nelse: DEVICE = 'cpu'\n\ndef train_fn(loader, model, optimizer, loss_fn, device=DEVICE):\n    model.train()\n    train_loss = 0.\n    loop = tqdm(loader)\n    \n    for batch_idx, (data, targets, img_size) in enumerate(loop):\n        data = data.to(device=device)\n        targets = targets.to(device=device)\n        \n        predictions = model(data)\n        targets = torch.argmax(targets, dim=1)\n        loss = loss_fn(predictions, targets)\n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        loop.set_postfix(loss=loss.item())\n        train_loss += loss.detach().cpu().numpy() * BATCH_SIZE\n        \n    train_loss = train_loss / (BATCH_SIZE * len(train_dl))\n    return train_loss\n\n## For the model validation loop.\ndef valid_fn(loader, model, loss_fn, device=DEVICE):\n    model.eval()\n    valid_loss = 0.\n    loop = tqdm(loader)\n    \n    with torch.no_grad():\n        for batch_idx, (data, targets, img_size) in enumerate(loop):\n            data = data.to(device=device)\n            targets = targets.to(device=device)\n            \n            predictions = model(data)\n            targets = torch.argmax(targets, dim=1)\n            loss = loss_fn(predictions, targets)\n            valid_loss += loss * BATCH_SIZE\n            \n            loop.set_postfix(loss=loss.item())\n            \n        valid_loss = valid_loss / (BATCH_SIZE * len(valid_dl))\n    return valid_loss","metadata":{"execution":{"iopub.status.busy":"2022-05-21T01:20:06.456181Z","iopub.execute_input":"2022-05-21T01:20:06.456416Z","iopub.status.idle":"2022-05-21T01:20:06.471242Z","shell.execute_reply.started":"2022-05-21T01:20:06.456385Z","shell.execute_reply":"2022-05-21T01:20:06.470452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## For the train & validation loop.\nNUM_EPOCHS = 1\n\n## DeepLabv3 model\nmodel.to(device=DEVICE)\n\nbest_loss = 100\nfor epoch in range(NUM_EPOCHS):\n    print('-------------')\n    print('Epoch {}/{}'.format(epoch+1, NUM_EPOCHS))\n    print('-------------')\n    \n    train_loss = train_fn(train_dl, model, optimizer, loss_fn, DEVICE)\n    valid_loss = valid_fn(valid_dl, model, loss_fn, DEVICE)\n    \n    if valid_loss < best_loss:\n        checkpoint = {\n            'model': model.state_dict(),\n            'optimizer': optimizer.state_dict(),\n        }\n        torch.save(checkpoint, \"./checkpoint.pth\")\n        print('best model saved!')\n        best_loss = valid_loss\n    \n    print(f'Train Loss: {train_loss},  Valid Loss: {valid_loss}')","metadata":{"execution":{"iopub.status.busy":"2022-05-21T01:20:06.472737Z","iopub.execute_input":"2022-05-21T01:20:06.473142Z","iopub.status.idle":"2022-05-21T01:47:27.472603Z","shell.execute_reply.started":"2022-05-21T01:20:06.473105Z","shell.execute_reply":"2022-05-21T01:47:27.47168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id =\"6\"></a><h1 style=\"background:#a1a8f0; border:0; border-radius: 10px; color:black\"><center>6. Prediction</center></h1>","metadata":{}},{"cell_type":"code","source":"checkpoint = torch.load(\"./checkpoint.pth\")\nmodel.load_state_dict(checkpoint[\"model\"])\noptimizer.load_state_dict(checkpoint[\"optimizer\"])\n        \nmodel.eval()\npredictions = []\n\nwith torch.no_grad():\n    for batch in tqdm(test_dl):\n        x = batch[0].to(DEVICE)\n        test_pred = model(x)\n        test_pred = torch.argmax(test_pred, dim=1)\n        test_pred = torch.nn.functional.one_hot(test_pred, num_classes=4)\n        test_pred = torch.permute(test_pred, dims=[0, 3, 1, 2])\n        test_pred = test_pred[:, 1:, ...] ## We don't need background predictions.\n        test_pred = test_pred.detach().cpu().numpy()\n        predictions.append(test_pred)\n    \npredictions = np.concatenate(predictions, axis=0)\npredictions = predictions.reshape([-1, 256, 256])\nprint(predictions.shape)","metadata":{"execution":{"iopub.status.busy":"2022-05-21T01:47:27.474006Z","iopub.execute_input":"2022-05-21T01:47:27.474445Z","iopub.status.idle":"2022-05-21T01:48:09.049033Z","shell.execute_reply.started":"2022-05-21T01:47:27.474408Z","shell.execute_reply":"2022-05-21T01:48:09.048265Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def rle_encode(img):\n    pixels = img.flatten()\n    pixels = np.concatenate([[0], pixels, [0]])\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n    runs[1::2] -= runs[::2]\n    encodes = ' '.join(str(x) for x in runs)\n    if encodes == '':\n        encodes = np.nan\n    return encodes\n\npredictions_rle = []\n\nfor pred in predictions:\n    pred_rle = rle_encode(pred)\n    predictions_rle.append(pred_rle)\n    \npredictions_rle = np.concatenate([predictions_rle], axis=0)\ntest['prediction'] = predictions_rle\n\ntest.head(20)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-05-21T01:48:09.050424Z","iopub.execute_input":"2022-05-21T01:48:09.050663Z","iopub.status.idle":"2022-05-21T01:48:10.125718Z","shell.execute_reply.started":"2022-05-21T01:48:09.050633Z","shell.execute_reply":"2022-05-21T01:48:10.12505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}