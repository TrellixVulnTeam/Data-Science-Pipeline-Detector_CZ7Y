{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# UW-Madison GI Tract Image Segmentation\n<br><center><img src=\"https://storage.googleapis.com/kaggle-competitions/kaggle/27923/logos/header.png?t=2021-06-02-20-30-25\" width=100%></center>\n  Students: David Vaisbrud‏, Shahar Cohen\n  \n  In 2019, an estimated 5 million people were diagnosed with a cancer of the gastro-intestinal tract worldwide. Of these patients, about half are eligible for radiation therapy, usually delivered over 10-15 minutes a day for 1-6 weeks.<br>\n    Radiation oncologists try to deliver high doses of radiation using X-ray beams pointed to tumors while avoiding the stomach and intestines. <br>\n    With newer technology such as integrated magnetic resonance imaging and linear accelerator systems, also known as MR-Linacs, oncologists are able to visualize the daily position of the tumor and intestines, which can vary day to day. <br>\n    In these scans, radiation oncologists must manually outline the position of the stomach and intestines in order to adjust the direction of the x-ray beams to increase the dose delivery to the tumor and avoid the stomach and intestines. <br>\n    This is a time-consuming and labor intensive process that can prolong treatments from 15 minutes a day to an hour a day, which can be difficult for patients to tolerate—unless deep learning could help automate the segmentation process.<br>\n    <br>\nThis notebook will run the following subjects :<br>\n1. **Problem description**<br> \n    create a model to automatically segment the stomach and intestines on MRI scans.<br> \n    The MRI scans are from actual cancer patients who had 1-5 MRI scans on separate days during their radiation treatment. <br> You'll base your algorithm on a dataset of these scans to come up with creative deep learning solutions that will help cancer patients get better care.<br><br>\n2. **Data gathering**<br> \n    The UW-Madison Carbone Cancer Center is a pioneer in MR-Linac based radiotherapy, and has treated patients with MRI guided radiotherapy based on their daily anatomy since 2015.<br> UW-Madison has generously agreed to support this project which provides anonymized MRIs of patients treated at the UW-Madison Carbone Cancer Center. The University of Wisconsin-Madison is a public land-grant research university in Madison, Wisconsin.<br>  The Wisconsin Idea is the university's pledge to the state, the nation, and the world that their endeavors will benefit all citizens.<br><br>\n2. **EDA  - investigate the data set and visualizing the data.**\n    * Exploring tha dataset\n    * Investigate the segmentation - look at labeling distrebution\n    * Investigate Pixel Spacing\n    * Investigate mask size/areas\n    * Distribution of Images per Case IDS\n    * Case id sequence data\n    * Mask dataset creation, class overlap.\n    * Pixel values\n    * Heuristucs or rules regarfing segmantation\n    * 3D gif with case mask\n\n\n3. **Model selection and Evaluation**<br>\n    **Model selection**\n    \n    * Unet\n    * DeepLab3 - TBD\n   \n   **Evaluation:** <br>\n   This competition is evaluated on the mean <a href=\"https://en.wikipedia.org/wiki/S%C3%B8rensen%E2%80%93Dice_coefficient\"><b>Dice coefficient</b></a> and <a href=\"https://github.com/scipy/scipy/blob/master/scipy/spatial/_hausdorff.pyx\"><b>3D Hausdorff distance</b></a>. The Dice coefficient can be used to compare the pixel-wise agreement between a predicted segmentation and its corresponding ground truth. The formula is given by:\n\n    $$\n    \\frac{2 * |X \\cap Y|}{|X| + |Y|}\n    $$\n\n    where $X$ is the predicted set of pixels and $Y$ is the ground truth. \n    * The Dice coefficient is defined to be $1$ when both $X$ and $Y$ are empty. \n    * The leaderboard score is the <b>mean of the Dice coefficients for each image in the test set.</b>\n\n    Hausdorff distance is a method for calculating the distance between segmentation objects A and B, by calculating the furthest point on object A from the nearest point on object B. For 3D Hausdorff, we construct 3D volumes by combining each 2D segmentation with slice depth as the Z coordinate and then find the Hausdorff distance between them. **(In this competition, the slice depth for all scans is set to 1.)** <a href=\"https://github.com/scipy/scipy/blob/master/scipy/spatial/_hausdorff.pyx\"><b>The scipy code for Hausdorff is linked</b></a>. The expected / predicted pixel locations are normalized by image size to create a bounded 0-1 score.\n\n    <br>\n\n    ---\n\n    <b>NOTE: The two metrics are combined during evaluation!</b>\n\n    * <b>Weight of 0.4 for the Dice metric</b>\n    * <b>Weight of 0.6 for the Hausdorff distance.</b>\n\n    ---\n\n\n ## To do before submission.\n### Improve the existing analyses:\n\n* Improve Unet model visualization and analyses\n* Investigate which categories and images the model fails\n\n### Add more analyses:\n* EDA: check the image size (currently we use [256,256] px ), and try to find a heuristic based on the pixel spacing.\n* Try Unet++ (it should work better, made for this kind of assignment)\n* Try DeepLabv3\n* Check different optimizers.","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"markdown","source":"# Data gathering \nIn this section we will import the relative libraries and obtain the dataset.\n<br><br>\nIn this competition we are segmenting organs cells in images. The training annotations are provided as RLE-encoded masks, and the images are in 16-bit grayscale PNG format.\n<br><br>\nEach case in this competition is represented by multiple sets of scan slices (each set is identified by the day the scan took place). Some cases are split by time (early days are in train, later days are in test) while some cases are split by case - the entirety of the case is in train or test. The goal of this competition is to be able to generalize to both partially and wholly unseen cases.\n<br><br>\nNote that, in this case, the test set is entirely unseen. It is roughly 50 cases, with a varying number of days and slices, as seen in the training set.\n\n## Segmentation RLE\nRLE is run-length encoding. It is used to encode the location of foreground objects in segmentation. Instead of outputting a mask image, you give a list of start pixels and how many pixels after each of those starts is included in the mask.\n<br>\nThe encoding rule is pretty simple: Where the mask is. Index of the mask, and how many pixels follow\n\n\n<br>\n\n## DataSet overview\n\n### General information\n\n<b>In this competition we are segmenting organs cells in images</b>. \n\nThe training **annotations are provided as RLE-encoded masks**, and the images are in **16-bit**, **grayscale**, **PNG format**.\n\nEach case in this competition is represented by multiple sets of scan slices\n* Each set is identified by the day the scan took place\n* Some cases are split by time\n    * early days are in train\n    * later days are in test\n* Some cases are split by case\n    * the entirety of the case is in train or test\n\n<b>The goal of this competition is to be able to generalize to both partially and wholly unseen cases.</b>\n\nNote that, in this case, the test set is entirely unseen.\n* It is roughly 50 cases\n* It contains a varying number of days and slices, (similar to the training set)\n\n### Files imformation\n\n**`train.csv`** \n- IDs and masks for all training objects.\n- **Columns**\n    * **`id`**\n        * unique identifier for object\n    * **`class`**\n        * the predicted class for the object\n    * **`EncodedPixels`**\n        * RLE-encoded pixels for the identified object\n\n<br>\n\n**`sample_submission.csv`**\n- A sample submission file in the correct format\n\n<br>\n\n**`train/`**\n- a folder of case/day folders, each containing slice images for a particular case on a given day.\n","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"# install libraries\n!pip install -q segmentation_models_pytorch","metadata":{"execution":{"iopub.status.busy":"2022-06-11T18:37:06.508539Z","iopub.execute_input":"2022-06-11T18:37:06.508895Z","iopub.status.idle":"2022-06-11T18:37:20.848599Z","shell.execute_reply.started":"2022-06-11T18:37:06.50882Z","shell.execute_reply":"2022-06-11T18:37:20.847749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import libraries\n# Operating system libraries\nfrom glob import glob\nimport os\nimport time\nimport copy\nimport gc\nfrom collections import defaultdict\n\n# linear algebra and data processing\nimport numpy as np\nimport pandas as pd\n\n\n# visualization\nimport cv2\nimport plotly.express as px\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Rectangle\nfrom matplotlib import animation, rc\nrc('animation', html='jshtml')\nimport seaborn as sns\n\n# Progress bars to know cell progress in pandas apply\nfrom tqdm import tqdm\nfrom tqdm.notebook import tqdm_notebook\ntqdm_notebook.pandas()\n\n# PyTorch deep learning semantic segmentation\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.cuda import amp\nimport segmentation_models_pytorch as smp\n# Sklearn ML algorithems\nfrom sklearn.model_selection import GroupKFold\n\n# Albumentations for image augmentations\nimport albumentations as A","metadata":{"pycharm":{"name":"#%%\n"},"execution":{"iopub.status.busy":"2022-06-11T18:37:20.850765Z","iopub.execute_input":"2022-06-11T18:37:20.851019Z","iopub.status.idle":"2022-06-11T18:37:28.260591Z","shell.execute_reply.started":"2022-06-11T18:37:20.850984Z","shell.execute_reply":"2022-06-11T18:37:28.259691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Basic data definitions","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"markdown","source":"### EDA Constants","metadata":{}},{"cell_type":"code","source":"# Open the training dataframe and display the initial dataframe\nDATA_DIR = \"/kaggle/input/uw-madison-gi-tract-image-segmentation\"\nTRAIN_DIR = os.path.join(DATA_DIR, \"train\")\nTRAIN_CSV = os.path.join(DATA_DIR, \"train.csv\")\n\n# Submission constants\nTEST_DIR = os.path.join(DATA_DIR, \"test\")\nSS_CSV = os.path.join(DATA_DIR, \"sample_submission.csv\")\n\n\n# Dictionary for classes\nSF2LF = {\"lb\":\"Large Bowel\",\"sb\":\"Small Bowel\",\"st\":\"Stomach\"}\nLF2SF = {v:k for k,v in SF2LF.items()}\n\n# Directory for working\nNPY_DIR = \"/kaggle/working/npy_files\"","metadata":{"execution":{"iopub.status.busy":"2022-06-11T18:37:28.262453Z","iopub.execute_input":"2022-06-11T18:37:28.262721Z","iopub.status.idle":"2022-06-11T18:37:28.268933Z","shell.execute_reply.started":"2022-06-11T18:37:28.262685Z","shell.execute_reply":"2022-06-11T18:37:28.268307Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data reading","metadata":{}},{"cell_type":"code","source":"# Get dataframes\ntrain_df = pd.read_csv(TRAIN_CSV)\nss_df = pd.read_csv(SS_CSV)\n\n# Get all training images\nall_train_images = glob(os.path.join(TRAIN_DIR, \"**\", \"*.png\"), recursive=True)\n\nprint(\"\\n... ORIGINAL TRAINING DATAFRAME... \\n\")\ndisplay(train_df)\n\n# Get all testing images if there are any\nall_test_images = glob(os.path.join(TEST_DIR, \"**\", \"*.png\"), recursive=True)\n    \n\nprint(\"\\n\\n\\n... ORIGINAL SUBMISSION DATAFRAME... \\n\")    \ndisplay(ss_df)\n\n\nprint(\"\\n... BASIC DATA SETUP FINISHED ...\\n\\n\")","metadata":{"pycharm":{"name":"#%%\n"},"execution":{"iopub.status.busy":"2022-06-11T18:37:28.271111Z","iopub.execute_input":"2022-06-11T18:37:28.271612Z","iopub.status.idle":"2022-06-11T18:37:31.82147Z","shell.execute_reply.started":"2022-06-11T18:37:28.271575Z","shell.execute_reply":"2022-06-11T18:37:31.820581Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Update dataframe with external information\nBefore we start the EDA, we will preprocess the data to create a informative dataframe with the data seperated to freatures.<br>\nThe feature we choose to creat from the train df are:\n* case_id\n* file path\n* number of segmentation masks\n* specify the segmantation mask\n* Slice dimentions of the file(hight and width)\n* class\n* <br>\n\n\nThe label will be changed as the following<br>\n* **large_bowel** --> **lb**\n* **small_bowel** --> **sb**\n* **stomach** --> **st**","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"def get_filepath_from_partial_identifier(_ident, file_list):\n    return [x for x in file_list if _ident in x][0]\n\n\ndef df_preprocessing(df, globbed_file_list, is_test=False):\n    \"\"\" The preprocessing steps applied to get column information \"\"\"\n    # 1. Get Case-ID as a column (str and int)\n    df[\"case_id_str\"] = df[\"id\"].apply(lambda x: x.split(\"_\", 2)[0])\n    df[\"case_id\"] = df[\"id\"].apply(lambda x: int(x.split(\"_\", 2)[0].replace(\"case\", \"\")))\n\n    # 2. Get Day as a column\n    df[\"day_num_str\"] = df[\"id\"].apply(lambda x: x.split(\"_\", 2)[1])\n    df[\"day_num\"] = df[\"id\"].apply(lambda x: int(x.split(\"_\", 2)[1].replace(\"day\", \"\")))\n\n    # 3. Get Slice Identifier as a column\n    df[\"slice_id\"] = df[\"id\"].apply(lambda x: x.split(\"_\", 2)[2])\n\n    # 4. Get full file paths for the representative scans\n    df[\"_partial_ident\"] = (globbed_file_list[0].rsplit(\"/\", 4)[0] + \"/\" +  # /kaggle/input/uw-madison-gi-tract-image-segmentation/train/\n                            df[\"case_id_str\"] + \"/\" +  # .../case###/\n                            df[\"case_id_str\"] + \"_\" + df[\"day_num_str\"] +  # .../case###_day##/\n                            \"/scans/\" + df[\"slice_id\"])  # .../slice_#### \n    _tmp_merge_df = pd.DataFrame({\"_partial_ident\": [x.rsplit(\"_\", 4)[0] for x in globbed_file_list],\n                                  \"f_path\": globbed_file_list})\n    df = df.merge(_tmp_merge_df, on=\"_partial_ident\").drop(columns=[\"_partial_ident\"])\n    del _tmp_merge_df\n    # 5. Get slice dimensions from filepath (int in pixels)\n    df[\"slice_h\"] = df[\"f_path\"].apply(lambda x: int(x[:-4].rsplit(\"_\", 4)[1]))\n    df[\"slice_w\"] = df[\"f_path\"].apply(lambda x: int(x[:-4].rsplit(\"_\", 4)[2]))\n\n    # 6. Pixel spacing from filepath (float in mm)\n    df[\"px_spacing_h\"] = df[\"f_path\"].apply(lambda x: float(x[:-4].rsplit(\"_\", 4)[3]))\n    df[\"px_spacing_w\"] = df[\"f_path\"].apply(lambda x: float(x[:-4].rsplit(\"_\", 4)[4]))\n\n    if not is_test:\n        # 7. Merge 3 Rows Into A Single Row (As This/Segmentation-RLE Is The Only Unique Information Across Those Rows)\n        l_bowel_df = df[df[\"class\"] == \"large_bowel\"][[\"id\", \"segmentation\"]].rename(\n            columns={\"segmentation\": \"lb_seg_rle\"})\n        s_bowel_df = df[df[\"class\"] == \"small_bowel\"][[\"id\", \"segmentation\"]].rename(\n            columns={\"segmentation\": \"sb_seg_rle\"})\n        stomach_df = df[df[\"class\"] == \"stomach\"][[\"id\", \"segmentation\"]].rename(columns={\"segmentation\": \"st_seg_rle\"})\n        df = df.merge(l_bowel_df, on=\"id\", how=\"left\")\n        df = df.merge(s_bowel_df, on=\"id\", how=\"left\")\n        df = df.merge(stomach_df, on=\"id\", how=\"left\")\n        df = df.drop_duplicates(subset=[\"id\", ]).reset_index(drop=True)\n        df[\"lb_seg_flag\"] = df[\"lb_seg_rle\"].apply(lambda x: not pd.isna(x))\n        df[\"sb_seg_flag\"] = df[\"sb_seg_rle\"].apply(lambda x: not pd.isna(x))\n        df[\"st_seg_flag\"] = df[\"st_seg_rle\"].apply(lambda x: not pd.isna(x))\n        df[\"n_segs\"] = df[\"lb_seg_flag\"].astype(int) + df[\"sb_seg_flag\"].astype(int) + df[\"st_seg_flag\"].astype(int)\n\n    # 8. Reorder columns to the a new ordering (drops class and segmentation as no longer necessary)\n    new_col_order = [\"id\", \"f_path\", \"n_segs\",\n                     \"lb_seg_rle\", \"lb_seg_flag\",\n                     \"sb_seg_rle\", \"sb_seg_flag\",\n                     \"st_seg_rle\", \"st_seg_flag\",\n                     \"slice_h\", \"slice_w\", \"px_spacing_h\",\n                     \"px_spacing_w\", \"case_id_str\", \"case_id\",\n                     \"day_num_str\", \"day_num\", \"slice_id\", ]\n    if is_test: new_col_order.insert(1, \"class\")\n    new_col_order = [_c for _c in new_col_order if _c in df.columns]\n    df = df[new_col_order]\n\n    return df","metadata":{"pycharm":{"name":"#%%\n"},"execution":{"iopub.status.busy":"2022-06-11T18:37:31.823126Z","iopub.execute_input":"2022-06-11T18:37:31.823509Z","iopub.status.idle":"2022-06-11T18:37:31.846847Z","shell.execute_reply.started":"2022-06-11T18:37:31.823471Z","shell.execute_reply":"2022-06-11T18:37:31.84615Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Old Df columns:\\n{train_df.columns}\")\ntrain_df =  df_preprocessing(train_df, all_train_images)\nprint(f\"New Df columns:\\n{train_df.columns}\")\n\nprint(\"\\n... UPDATED TRAINING DATAFRAME... \\n\")\ndisplay(train_df)\n\nprint(\"\\n... UPDATING DATAFRAMES WITH ACCESSIBLE INFORMATION FINISHED ...\\n\\n\")","metadata":{"pycharm":{"name":"#%%\n"},"execution":{"iopub.status.busy":"2022-06-11T18:37:31.849867Z","iopub.execute_input":"2022-06-11T18:37:31.850051Z","iopub.status.idle":"2022-06-11T18:37:33.401523Z","shell.execute_reply.started":"2022-06-11T18:37:31.850027Z","shell.execute_reply":"2022-06-11T18:37:33.400782Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EDA","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"markdown","source":"## Helper functions\nBefore we start the data observation, we will need some functions to decode and encode the segmentation masks","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"# ref: https://www.kaggle.com/paulorzp/run-length-encode-and-decode\n# modified from: https://www.kaggle.com/inversion/run-length-decoding-quick-start\ndef rle_decode(mask_rle, shape, color=1):\n    \"\"\"\n    Args:\n        mask_rle (str): run-length as string formated (start length)\n        shape (tuple of ints): (height,width) of array to return \n\n    Returns: \n        Mask (np.array)\n            - 1 indicating mask\n            - 0 indicating background\n\n    \"\"\"\n    # Split the string by space, then convert it into a integer array\n    s = np.array(mask_rle.split(), dtype=int)\n\n    # Every even value is the start, every odd value is the \"run\" length\n    starts = s[0::2] - 1\n    lengths = s[1::2]\n    ends = starts + lengths\n\n    # The image is actually flattened since RLE is a 1D \"run\"\n    if len(shape) == 3:\n        h, w, d = shape\n        img = np.zeros((h * w, d), dtype=np.float32)\n    else:\n        h, w = shape\n        img = np.zeros((h * w,), dtype=np.float32)\n\n    # The color here is actually just any integer you want!\n    for lo, hi in zip(starts, ends):\n        img[lo: hi] = color\n\n    # Don't forget to change the image back to the original shape\n    return img.reshape(shape)\n\n\n# https://www.kaggle.com/namgalielei/which-reshape-is-used-in-rle\ndef rle_decode_top_to_bot_first(mask_rle, shape):\n    \"\"\" \n    Args:\n        mask_rle (str): run-length as string formated (start length)\n        shape (tuple of ints): (height,width) of array to return \n\n    Returns:\n        Mask (np.array)\n            - 1 indicating mask\n            - 0 indicating background\n\n    \"\"\"\n    s = mask_rle.split()\n    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n    starts -= 1\n    ends = starts + lengths\n    img = np.zeros(shape[0] * shape[1], dtype=np.uint8)\n    for lo, hi in zip(starts, ends):\n        img[lo:hi] = 1\n    return img.reshape((shape[1], shape[0]), order='F').T  # Reshape from top -> bottom first\n\n\n# ref.: https://www.kaggle.com/stainsby/fast-tested-rle\ndef rle_encode(img):\n    \"\"\"\n    Args:\n        img (np.array): \n            - 1 indicating mask\n            - 0 indicating background\n\n    Returns: \n        run length as string formated\n    \"\"\"\n\n    pixels = img.flatten()\n    pixels = np.concatenate([[0], pixels, [0]])\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n    runs[1::2] -= runs[::2]\n    return ' '.join(str(x) for x in runs)\n\n\ndef flatten_l_o_l(nested_list):\n    \"\"\" Flatten a list of lists \"\"\"\n    return [item for sublist in nested_list for item in sublist]\n\n\ndef load_json_to_dict(json_path):\n    \"\"\" tbd \"\"\"\n    with open(json_path) as json_file:\n        data = json.load(json_file)\n    return data\n\n\ndef tf_load_png(img_path):\n    return tf.image.decode_png(tf.io.read_file(img_path), channels=3)\n\n\ndef open_gray16(_path, normalize=True, to_rgb=False):\n    \"\"\" Helper to open files \"\"\"\n    if normalize:\n        if to_rgb:\n            return np.tile(np.expand_dims(cv2.imread(_path, cv2.IMREAD_ANYDEPTH) / 65535., axis=-1), 3)\n        else:\n            return cv2.imread(_path, cv2.IMREAD_ANYDEPTH) / 65535.\n    else:\n        if to_rgb:\n            return np.tile(np.expand_dims(cv2.imread(_path, cv2.IMREAD_ANYDEPTH), axis=-1), 3)\n        else:\n            return cv2.imread(_path, cv2.IMREAD_ANYDEPTH)\n\n\ndef load_img(path):\n    img = cv2.imread(path, cv2.IMREAD_UNCHANGED)\n    img = np.tile(img[..., None], [1, 1, 3])  # gray to rgb\n    img = img.astype('float32')  # original is uint16\n    mx = np.max(img)\n    if mx:\n        img /= mx  # scale image to [0, 1]\n    return img\n\n\ndef load_msk(path):\n    msk = np.load(path)\n    msk = msk.astype('float32')\n    return msk","metadata":{"pycharm":{"name":"#%%\n"},"execution":{"iopub.status.busy":"2022-06-11T18:37:33.40307Z","iopub.execute_input":"2022-06-11T18:37:33.403545Z","iopub.status.idle":"2022-06-11T18:37:33.423055Z","shell.execute_reply.started":"2022-06-11T18:37:33.403508Z","shell.execute_reply":"2022-06-11T18:37:33.42229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Exploring tha dataset\nAfter preprocessing the multiple datasets and adding feature to investigate, now we can start exploring the data.\n\nFirst we will define some visualization function to help us plot the images with the segmantation masks.","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"def get_overlay(img_path, rle_strs, img_shape, _alpha=0.999, _beta=0.35, _gamma=0):\n    \"\"\"\n        A simple function to return the images with the segmentation masks.\n        \"\"\"\n    _img = open_gray16(img_path, to_rgb=True).astype(np.float32)\n    _img = ((_img - _img.min()) / (_img.max() - _img.min())).astype(np.float32)\n    _seg_rgb = np.stack([rle_decode(rle_str, shape=img_shape, color=1)\n                         if (rle_str is not None and not pd.isna(rle_str))\n                         else np.zeros(img_shape, dtype=np.float32)\n                         for rle_str in rle_strs], axis=-1).astype(np.float32)\n\n    seg_overlay = cv2.addWeighted(src1=_img, alpha=_alpha,\n                                  src2=_seg_rgb, beta=_beta, gamma=_gamma)\n    return seg_overlay\n\n\ndef examine_id(ex_id, df=train_df, plot_overlay=True, print_meta=False, plot_grayscale=False,\n               plot_binary_segmentation=False):\n    \"\"\" Wrapper function to allow for easy visual exploration of an example \"\"\"\n    print(f\"\\n... ID ({ex_id}) EXPLORATION STARTED ...\\n\\n\")\n    demo_ex = df[df.id == ex_id].squeeze()\n\n    if print_meta:\n        print(f\"\\n... WITH DEMO_ID=`{DEMO_ID}` WE HAVE THE FOLLOWING DEMO EXAMPLE TO WORK FROM... \\n\\n\")\n        display(demo_ex.to_frame())\n\n    if plot_grayscale:\n        print(f\"\\n\\n... GRAYSCALE IMAGE PLOT ...\\n\")\n        plt.figure(figsize=(12, 12))\n        plt.imshow(open_gray16(demo_ex.f_path), cmap=\"gray\")\n        plt.title(f\"Original Grayscale Image For ID: {demo_ex.id}\", fontweight=\"bold\")\n        plt.axis(False)\n        plt.show()\n\n    if plot_binary_segmentation:\n        print(f\"\\n\\n... BINARY SEGMENTATION MASKS ...\\n\")\n        plt.figure(figsize=(20, 10))\n        for i, _seg_type in enumerate([\"lb\", \"sb\", \"st\"]):\n            if pd.isna(demo_ex[f\"{_seg_type}_seg_rle\"]): continue\n            plt.subplot(1, 3, i + 1)\n            plt.imshow(rle_decode(demo_ex[f\"{_seg_type}_seg_rle\"], shape=(demo_ex.slice_w, demo_ex.slice_h), color=1))\n            plt.title(f\"RLE Encoding For {SF2LF[_seg_type]} Segmentation\", fontweight=\"bold\")\n            plt.axis(False)\n        plt.tight_layout()\n        plt.show()\n\n    if plot_overlay:\n        print(f\"\\n\\n... IMAGE WITH RGB SEGMENTATION MASK OVERLAY ...\\n\")\n        _rle_strs = [demo_ex[f\"{_seg_type}_seg_rle\"] if not pd.isna(demo_ex[f\"{_seg_type}_seg_rle\"]) else None for\n                     _seg_type in [\"lb\", \"sb\", \"st\"]]\n        seg_overlay = get_overlay(demo_ex.f_path, _rle_strs, img_shape=(demo_ex.slice_w, demo_ex.slice_h))\n\n        plt.figure(figsize=(12, 12))\n        plt.imshow(seg_overlay)\n        plt.title(f\"Segmentation Overlay For ID: {demo_ex.id}\", fontweight=\"bold\")\n        handles = [Rectangle((0, 0), 1, 1, color=_c) for _c in\n                   [(0.667, 0.0, 0.0), (0.0, 0.667, 0.0), (0.0, 0.0, 0.667)]]\n        labels = [\"Large Bowel Segmentation Map\", \"Small Bowel Segmentation Map\", \"Stomach Segmentation Map\"]\n        plt.legend(handles, labels)\n        plt.axis(False)\n        plt.show()\n\n    print(\"\\n\\n... SINGLE ID EXPLORATION FINISHED ...\\n\\n\")\n","metadata":{"pycharm":{"name":"#%%\n"},"execution":{"iopub.status.busy":"2022-06-11T18:37:33.426169Z","iopub.execute_input":"2022-06-11T18:37:33.42644Z","iopub.status.idle":"2022-06-11T18:37:33.444519Z","shell.execute_reply.started":"2022-06-11T18:37:33.426383Z","shell.execute_reply":"2022-06-11T18:37:33.443765Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"\\n... SINGLE ID EXPLORATION STARTED ...\\n\\n\")\n\n# Pick a id case to visulize\nDEMO_ID = \"case123_day20_slice_0082\"\ndemo_ex = train_df[train_df.id == DEMO_ID].squeeze()  # change dimentions from 1,18 to 18\n\nprint(f\"\\n... WITH DEMO_ID=`{DEMO_ID}` WE HAVE THE FOLLOWING DEMO EXAMPLE TO WORK FROM... \\n\\n\")\ndisplay(demo_ex.to_frame())  # Convert Series to DataFrame.\n\nprint(f\"\\n\\n... LET'S PLOT THE IMAGE FIRST ...\\n\")\nplt.figure(figsize=(12, 12))\nplt.imshow(open_gray16(demo_ex.f_path), cmap=\"gray\")\nplt.title(f\"Original Grayscale Image For ID: {demo_ex.id}\", fontweight=\"bold\")\nplt.axis(False)\nplt.show()\n\nprint(f\"\\n\\n... LET'S PLOT THE 3 SEGMENTATION MASKS ...\\n\")\n\nplt.figure(figsize=(20, 10))\nfor i, _seg_type in enumerate([\"lb\", \"sb\", \"st\"]):\n    if pd.isna(demo_ex[f\"{_seg_type}_seg_rle\"]): continue\n    plt.subplot(1, 3, i + 1)\n    plt.imshow(rle_decode(demo_ex[f\"{_seg_type}_seg_rle\"], shape=(demo_ex.slice_w, demo_ex.slice_h), color=1))\n    plt.title(f\"RLE Encoding For {SF2LF[_seg_type]} Segmentation\", fontweight=\"bold\")\n    plt.axis(False)\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\n\\n... LET'S PLOT THE IMAGE WITH AN RGB SEGMENTATION MASK OVERLAY ...\\n\")\n\n# We need to normalize the loaded image values to be between 0 and 1 or else our plot will look weird\n_img = open_gray16(demo_ex.f_path, to_rgb=True)\n_img = ((_img - _img.min()) / (_img.max() - _img.min())).astype(np.float32)\n_seg_rgb = np.stack([rle_decode(demo_ex[f\"{_seg_type}_seg_rle\"], shape=(demo_ex.slice_w, demo_ex.slice_h), color=1) if\n                     not pd.isna(demo_ex[f\"{_seg_type}_seg_rle\"]) else\n                     np.zeros((demo_ex.slice_w, demo_ex.slice_h)) for\n                     _seg_type in [\"lb\", \"sb\", \"st\"]], axis=-1).astype(np.float32)\nseg_overlay = cv2.addWeighted(src1=_img, alpha=0.99,\n                              src2=_seg_rgb, beta=0.33, gamma=0.0)\n\nplt.figure(figsize=(12, 12))\nplt.imshow(seg_overlay)\nplt.title(f\"Segmentation Overlay For ID: {demo_ex.id}\", fontweight=\"bold\")\nhandles = [Rectangle((0, 0), 1, 1, color=_c) for _c in [(0.667, 0.0, 0.0), (0.0, 0.667, 0.0), (0.0, 0.0, 0.667)]]\nlabels = [\"Large Bowel Segmentation Map\", \"Small Bowel Segmentation Map\", \"Stomach Segmentation Map\"]\nplt.legend(handles, labels)\nplt.axis(False)\nplt.show()\n\nprint(f\"\\n\\n... LET'S PRINT THE RELEVANT INFORMATION ...\\n\")\nprint(f\"\\t--> IMAGE CASE ID              : {demo_ex.case_id}\")\nprint(f\"\\t--> IMAGE DAY NUMBER           : {demo_ex.day_num}\")\nprint(f\"\\t--> IMAGE SLICE WIDTH          : {demo_ex.slice_w}\")\nprint(f\"\\t--> IMAGE SLICE HEIGHT         : {demo_ex.slice_h}\")\nprint(f\"\\t--> IMAGE PIXEL SPACING WIDTH  : {demo_ex.px_spacing_w}\")\nprint(f\"\\t--> IMAGE PIXEL SPACING HEIGHT : {demo_ex.px_spacing_h}\")\n\nprint(\"\\n\\n... SINGLE ID EXPLORATION FINISHED ...\\n\\n\")\n\n# cleanup\ndel _img\ndel _seg_rgb,\ndel seg_overlay\n","metadata":{"pycharm":{"name":"#%%\n"},"execution":{"iopub.status.busy":"2022-06-11T18:37:33.446055Z","iopub.execute_input":"2022-06-11T18:37:33.446613Z","iopub.status.idle":"2022-06-11T18:37:34.726004Z","shell.execute_reply.started":"2022-06-11T18:37:33.446577Z","shell.execute_reply":"2022-06-11T18:37:34.725279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot 3 random-ids where all tumor locales are present (max one id per case)\nN_TO_PLOT = 3\nfor _id in train_df[train_df.n_segs == 3].groupby(\"case_id\")[\"id\"].first().sample(N_TO_PLOT):\n    examine_id(_id)","metadata":{"pycharm":{"name":"#%%\n"},"execution":{"iopub.status.busy":"2022-06-11T18:37:34.729555Z","iopub.execute_input":"2022-06-11T18:37:34.736385Z","iopub.status.idle":"2022-06-11T18:37:35.798089Z","shell.execute_reply.started":"2022-06-11T18:37:34.736345Z","shell.execute_reply":"2022-06-11T18:37:35.79738Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Investigate the segmentation \nThe counter plot can show us that most of the images have larg bowel segmentation mask, and less cases of stomach and small bowel cases.<br>\nEach case contaion 3 masks of segmantataion, lets check how many cases have no segmentation masks","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"def get_seg_combo_str(row):\n    seg_str_list = []\n    if row[\"lb_seg_flag\"]: seg_str_list.append(\"Large Bowel\")\n    if row[\"sb_seg_flag\"]: seg_str_list.append(\"Small Bowel\")\n    if row[\"st_seg_flag\"]: seg_str_list.append(\"Stomach\")\n    if len(seg_str_list) > 0:\n        return \", \".join(seg_str_list)\n    else:\n        return \"No Mask\"\n\n\ntrain_df[\"seg_combo_str\"] = train_df.progress_apply(get_seg_combo_str, axis=1)\n\nfig = px.histogram(train_df, train_df[\"n_segs\"].astype(str), color=\"seg_combo_str\",\n                   title=\"<b>Number of Segmentation Masks Per Image</b>\",\n                   labels={\"x\": \"Number of Segmentation Masks Per Image\",\n                           \"seg_combo_str\": \"<b>Segmentation Masks Present</b>\"})\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-11T18:37:35.799514Z","iopub.execute_input":"2022-06-11T18:37:35.799956Z","iopub.status.idle":"2022-06-11T18:37:37.832214Z","shell.execute_reply.started":"2022-06-11T18:37:35.799921Z","shell.execute_reply":"2022-06-11T18:37:37.831584Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The distrebution shows that most of the images does not have segmantation mask.\n\nNow lets print the summeriezd data and check the number.","metadata":{}},{"cell_type":"code","source":"print(\"Total number of images : \", train_df.shape[0])\nno_ann_df = train_df[~train_df['lb_seg_flag'] & ~train_df['sb_seg_flag'] & ~train_df['st_seg_flag']]\nprint(\"Number of images without any annotation : \", no_ann_df.shape[0], \"percentege : \",\n      round(no_ann_df.shape[0] / train_df.shape[0], 2))\nprint(\"Number of images with 1 annotation or more : \",\n      (train_df.shape[0] - no_ann_df.shape[0]), \"percentege : \",\n      round((train_df.shape[0] - no_ann_df.shape[0]) / train_df.shape[0], 2))\nprint(\"Number of Stomach annotation: \", train_df[train_df['st_seg_flag']].shape[0])\nprint(\"Number of Small bowel annotation: \", train_df[train_df['sb_seg_flag']].shape[0])\nprint(\"Number of Large bowel annotation: \", train_df[train_df['lb_seg_flag']].shape[0])","metadata":{"pycharm":{"name":"#%%\n"},"execution":{"iopub.status.busy":"2022-06-11T18:37:37.833515Z","iopub.execute_input":"2022-06-11T18:37:37.834368Z","iopub.status.idle":"2022-06-11T18:37:37.881376Z","shell.execute_reply.started":"2022-06-11T18:37:37.834327Z","shell.execute_reply":"2022-06-11T18:37:37.880641Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the observation we can lean that,<br>\nMost of the cases that have only 1 segmentation mask is the **Stomach**.<br>\nImages that has Small bowel segmentation mask usualy has large bowel segmentation mask.<br>\nAs we saw from the previous cell, most of the masks don't have any segmentation mask. <br>\nLet's plot without the images without mask to see the distrebution of the annotation.\n","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"def add_seg_str(x):\n    list_seg = []\n    if x[\"lb_seg_flag\"]: list_seg.append(\"Large Bowel\")\n    if x[\"sb_seg_flag\"]: list_seg.append(\"Small Bowel\")\n    if x[\"st_seg_flag\"]: list_seg.append(\"Stomach\")\n    if len(list_seg) > 0:\n        return \", \".join(list_seg)\n    else:\n        return \"No Mask\"\n\n\ntrain_df[\"seg_str\"] = train_df.apply(add_seg_str, axis=1)\nfig = sns.catplot(x=\"n_segs\", kind=\"count\",hue = 'seg_str', data=train_df[train_df['n_segs']>0]).set(title = 'Segmentation masks per image')\nfig.set_xlabels(\"number of segmentation masks\")\nfig.set_ylabels(\"number of images\")","metadata":{"pycharm":{"name":"#%%\n"},"execution":{"iopub.status.busy":"2022-06-11T18:37:37.882717Z","iopub.execute_input":"2022-06-11T18:37:37.883129Z","iopub.status.idle":"2022-06-11T18:37:39.133117Z","shell.execute_reply.started":"2022-06-11T18:37:37.883091Z","shell.execute_reply":"2022-06-11T18:37:39.132423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It is clear now that most of the images with 2 segmentation masks are Large Bowel and Small Bowel.\nLets investigate the images with only 1 annotation","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"n_sb = train_df[(train_df['seg_str'] == 'Small Bowel') & ( train_df['n_segs'] == 1)].shape[0]\nn_st = train_df[(train_df['seg_str'] == 'Stomach') & ( train_df['n_segs'] == 1)].shape[0]\nn_lb = train_df[(train_df['seg_str'] == 'Large Bowel') & ( train_df['n_segs'] == 1)].shape[0]\nbar_df = pd.DataFrame([['Small Bowel',n_sb],['Stomach',n_st],['Large Bowel',n_lb]],columns=['Type','Number of images'])\nsns.barplot(x=\"Type\", y=\"Number of images\", data=bar_df).set_title(\"Only 1 mask of segmentation\")\n\n# Clean up\ndel n_sb\ndel n_st\ndel n_lb","metadata":{"pycharm":{"name":"#%%\n"},"execution":{"iopub.status.busy":"2022-06-11T18:37:39.134407Z","iopub.execute_input":"2022-06-11T18:37:39.13473Z","iopub.status.idle":"2022-06-11T18:37:39.31698Z","shell.execute_reply.started":"2022-06-11T18:37:39.134692Z","shell.execute_reply":"2022-06-11T18:37:39.316238Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we can see clearly that the vast majority of images that has 1 segmentation mask are stomach","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"markdown","source":"## Investigate Image Sizes \nWhile observing the images we identified different image size. We can layout the types and thier distribution and plot it.","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"# Count the examples\ntotal_obs = train_df['id'].count()\nprint(\"Total Observations:\", total_obs)\n\ntrain_df_slices_subset = train_df.drop_duplicates(subset=[\"slice_w\", \"slice_h\"])\ncolor = \"(\" + train_df_slices_subset[\"slice_w\"].astype(str) + \"x\" + train_df_slices_subset[\"slice_h\"].astype(str) + \")\"\n\nfig = px.scatter(train_df_slices_subset, x=\"slice_w\", y=\"slice_h\",\n                 size=train_df.groupby([\"slice_w\", \"slice_h\"])[\"id\"].transform(\"count\").iloc[train_df_slices_subset.index],\n                 color=color,\n                 title=\"<b>The Various Image Sizes</b>\",\n                 labels={\"color\": \"<b>Size Legend</b>\",\n                         \"size\": \"<b>Total Observations</b>\",\n                         \"slice_h\": \"<b>Image Slice Height in pixels</b>\",\n                         \"slice_w\": \"<b>Image Slice Width in pixels</b>\"},\n                 size_max=128)\nfig.show()","metadata":{"pycharm":{"name":"#%%\n"},"execution":{"iopub.status.busy":"2022-06-11T18:37:39.318263Z","iopub.execute_input":"2022-06-11T18:37:39.318636Z","iopub.status.idle":"2022-06-11T18:37:39.416195Z","shell.execute_reply.started":"2022-06-11T18:37:39.318602Z","shell.execute_reply":"2022-06-11T18:37:39.41551Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<b>After observing the image sized we can see that:</b>\n* There are **38,496** total examples.\n* There are **4** unique sizes:\n    * $234 \\times 234$\n        * Least frequent image size\n        * Smallest image size\n        * Only 44 of the 38,496 occurences are this size (0.37%)\n    * $266 \\times 266$\n        * Most frequent image size\n        * Second smallest image size\n        * 25,920 of the 38,496 occurences are this size (67.33%)\n    * $276 \\times 276$\n        * Second least frequent image size\n        * Second largest image size\n        * 1,200 of the 38,496 occurences are this size (3.12%)\n    * $310 \\times 360$\n        * Second most frequent image size\n        * Largest image size\n        * 11,232 of the 38,496 occurences are this size (29.17%)","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"markdown","source":"## Investigate Pixel Spacing \nWhile observing the images we identified different image pixel spacing. We can layout the types and thier distribution and plot it.\nPixel Spacing defined the physical distance in the patient between the center of each pixel, specified by a numeric pair - adjacent row spacing (delimiter) adjacent column spacing in mm. See Section 10.7.1.3 for further explanation.\n","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"train_df_spacing_subset = train_df.drop_duplicates(subset=[\"px_spacing_w\", \"px_spacing_h\"])\ncolor = \"(\" + train_df_slices_subset[\"px_spacing_w\"].astype(str) + \"x\" + train_df_slices_subset[\"px_spacing_h\"].astype(str) + \")\"\nfig = px.scatter(train_df_slices_subset, x=\"px_spacing_w\", y=\"px_spacing_h\",\n                 size=train_df.groupby([\"px_spacing_w\", \"px_spacing_h\"])[\"id\"].transform(\"count\").iloc[train_df_slices_subset.index],\n                 color=color,\n                 title=\"<b>The Various Pixel Spacings</b>\",\n                 labels={\"color\": \"<b>Pixel Spacing Legend</b>\",\n                         \"size\": \"<b>Number Of Observations</b>\",\n                         \"px_spacing_h\": \"<b>Pixel Spacing Height in mm</b>\",\n                         \"px_spacing_w\": \"<b>Pixel Spacing Width in mm</b>\"},\n                 size_max=128)\nfig.show()","metadata":{"pycharm":{"name":"#%%\n"},"execution":{"iopub.status.busy":"2022-06-11T18:37:39.417311Z","iopub.execute_input":"2022-06-11T18:37:39.417824Z","iopub.status.idle":"2022-06-11T18:37:39.489566Z","shell.execute_reply.started":"2022-06-11T18:37:39.417785Z","shell.execute_reply":"2022-06-11T18:37:39.48889Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<b>Pixel spacing observation tells us that:</b>\n* There are 38,496 total examples.\n* There are only 2 unique sets of pixel spacings:\n    * $1.50mm \\times 1.50mm$\n        * Most frequent pixel spacing\n        * Smallest pixel spacing (barely)\n        * 37,296 of the 38,496 occurences are this size (96.88%)\n    * $1.63mm \\times 1.63mm$\n        * Least frequent image size\n        * Largest pixel spacing (barely)\n        * 1,200 of the 38,496 occurences are this size (3.12%)\n\nTBD: to check predictions according to the pixel spacing.","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"markdown","source":"## Investigate mask sizes/areas\nlet's see of the masks overlap, that may cause an issue in segmentation detection or make better preference if we see that there is a pattern.","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"def get_mask_area(rle):\n    \"Returns the sum of area of the rle mask\"\n    if pd.isna(rle):\n        return None\n    # sum every other number in an RLE\n    return sum([int(x) for x in rle.split()[1::2]])\n\n\ntrain_df[\"lb_seg_area\"] = train_df.lb_seg_rle.apply(get_mask_area)\ntrain_df[\"sb_seg_area\"] = train_df.sb_seg_rle.apply(get_mask_area)\ntrain_df[\"st_seg_area\"] = train_df.st_seg_rle.apply(get_mask_area)\n\nfig = px.histogram(train_df, [\"lb_seg_area\", \"sb_seg_area\", \"st_seg_area\"],\n                   title=\"<b>Mask Areas Overlaid</b>\",\n                   barmode=\"overlay\",\n                   labels={\"value\": \"<b>Mask Area</b>\"})\nfig.show()","metadata":{"pycharm":{"name":"#%%\n"},"execution":{"iopub.status.busy":"2022-06-11T18:37:39.4909Z","iopub.execute_input":"2022-06-11T18:37:39.49113Z","iopub.status.idle":"2022-06-11T18:37:40.81953Z","shell.execute_reply.started":"2022-06-11T18:37:39.491099Z","shell.execute_reply":"2022-06-11T18:37:40.818899Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The distributions of mask area is mostly normal although it skews slightly to the smaller side...\nAll the distributions are similar although the Stomach distribution has an odd gap between 400-750 pixels.\nIt's interesting to note that, while not common, we do have some VERY large masks (>7500 pixels)\nAlso, it's kind of funny that the biggest masks are for small bowel","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"print(\"\\n\\n\\n... EXAMPLE WITH A LARGE AMOUNT OF SEGMENTATION MASK ...\\n\")\nexamine_id(\"case134_day22_slice_0102\")","metadata":{"pycharm":{"name":"#%%\n"},"execution":{"iopub.status.busy":"2022-06-11T18:37:40.820509Z","iopub.execute_input":"2022-06-11T18:37:40.820836Z","iopub.status.idle":"2022-06-11T18:37:41.1376Z","shell.execute_reply.started":"2022-06-11T18:37:40.820805Z","shell.execute_reply":"2022-06-11T18:37:41.136899Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Case ID's\n\nThe host described the case id as:\n<br>\n\"Each case in this competition is represented by multiple sets of scan slices (each set is identified by the day the scan took place). Some cases are split by time (early days are in train, later days are in test) while some cases are split by case - the entirety of the case is in train or test. The goal of this competition is to be able to generalize to both partially and wholly unseen cases.\"\n\nIn this following section we will observe the distribution of images per case id.<br>","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"fig = px.histogram(train_df, train_df.case_id.astype(str),\n                   color=\"day_num_str\",\n                   title=\"<b>Distribution Of Images Per Case ID</b>\",\n                   labels={\"x\":\"<b>Case ID</b>\", \"day_num_str\": \"<b>The Day The Scan Took Place</b>\"},\n                   text_auto=True,\n                   width=2000)\nfig.show()","metadata":{"pycharm":{"name":"#%%\n"},"execution":{"iopub.status.busy":"2022-06-11T18:37:41.139009Z","iopub.execute_input":"2022-06-11T18:37:41.139472Z","iopub.status.idle":"2022-06-11T18:37:41.612662Z","shell.execute_reply.started":"2022-06-11T18:37:41.139436Z","shell.execute_reply":"2022-06-11T18:37:41.610921Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"When we colour by day, we can see that all cases are made up (mostly) of groups of **144**, or less frequently, **80**, images from different days.\n\nNow lets see the distrebution according to the segmentation masks.","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"seg_masks = ['lb_seg_flag', 'sb_seg_flag', 'st_seg_flag']\n\nfor seg_mask in seg_masks:\n    sub_df = train_df[train_df[seg_mask]]\n    fig = px.histogram(sub_df, sub_df.case_id.astype(str), color=\"day_num_str\",\n                       title=f\"<b>Distribution Of Images Per Case ID with {seg_mask}</b>\",\n                       labels={\"x\": \"<b>Case ID</b>\", \"day_num_str\": \"<b>The Day The Scan Took Place</b>\"},\n                       text_auto=True, width=2000)\n    fig.show()","metadata":{"pycharm":{"name":"#%%\n"},"execution":{"iopub.status.busy":"2022-06-11T18:37:41.614086Z","iopub.execute_input":"2022-06-11T18:37:41.614547Z","iopub.status.idle":"2022-06-11T18:37:42.593566Z","shell.execute_reply.started":"2022-06-11T18:37:41.614507Z","shell.execute_reply":"2022-06-11T18:37:42.592727Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"When we look at the distrebution per segmentation mask we can see that there isn't any segmentation mask with the same count.\n<p>\n    Now, lets look at the distrebution of the case id with segmentation masks.\n</p>","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"fig = px.histogram(train_df, train_df.case_id.astype(str), color=\"seg_str\",\n                   title=f\"<b>Distribution Of Images Per Case ID and Segmentation masks/b>\",\n                   labels={\"x\": \"<b>Case ID</b>\", \"day_num_str\": \"<b>The Day The Scan Took Place</b>\"}, text_auto=True,\n                   width=2000)\nfig.show()","metadata":{"pycharm":{"name":"#%%\n"},"execution":{"iopub.status.busy":"2022-06-11T18:37:42.594913Z","iopub.execute_input":"2022-06-11T18:37:42.595224Z","iopub.status.idle":"2022-06-11T18:37:42.96382Z","shell.execute_reply.started":"2022-06-11T18:37:42.59519Z","shell.execute_reply":"2022-06-11T18:37:42.96316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"No mask appear to be the most dominant segmentation mask as we can estimate from the previuse section.<br>\nMost fo the cases contain segmentation masks of Large bowel and small bowel.<br>\nAnd the majoraty of the cases has about 700 images and in those 700 almost 400 has no segmentation mask.<br>\nWe can expect the model to handel well when both orgens are in the picture and identify both of there segmentation masks.\n\n### Plot case id sequence data\nLets plot the sequence data to see movement of orgens in the pictures to see if there is some pattern.","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"def plot_case(case_id,day = None,df = train_df,_figsize = (20,30),num_cols =10,max_rows = 10):\n    \"\"\"\n    Function to plot all images of a single case including segmentation masks\n    \"\"\"\n    print(f\"----Plotting images for case {case_id}----\")\n    \n    # initialize\n    case_df = df[df['case_id']==case_id]\n    # Handle specific day\n    if day is not None:\n        _case_df = case_df[case_df.day_num == day]\n        # check if there is no pictures in that day\n        if len(_case_df<=0):\n            print(f\"there are no samples in the day specified\")\n        else:\n            shrink_ratio = len(_case_df)/len(case_df) # save shrink ratio\n            case_df = _case_df\n            # change fig size to match the new ratio (original row,original col*shrink ratio)\n            _figsize = (_figsize[0],int(1.25*_figsize[1]* shrink_ratio))\n        del _case_df # clean up\n    ex_num = len(case_df)\n    \n    # Get relevant data\n    case_img_path = case_df.f_path.tolist()\n    case_rle = [_rles for _rles in case_df[[\"lb_seg_rle\", \"sb_seg_rle\", \"st_seg_rle\"]].values.tolist()]\n    case_img_shapes = [(_w,_h) for _w,_h in case_df[[\"slice_w\",\"slice_h\"]].values.tolist()]\n    all_overlays = [get_overlay(img_path, rle_strs, img_shape) for img_path, rle_strs, img_shape in zip(case_img_path, case_rle, case_img_shapes)]\n    \n    # Plot the images of the case\n    num_rows = int(np.ceil(ex_num/num_cols))\n    \n    if num_rows > max_rows:\n        num_rows = max_rows\n    \n    # Define the grid cells\n    fig, axs = plt.subplots(ncols=num_cols, nrows = num_rows,figsize = _figsize,sharey = True)\n    fig.suptitle(f'Case images - Case {case_id}')\n\n    img = all_overlays.pop()\n    row = 0\n    col = 0\n    while len(all_overlays) and row < num_rows:\n        while len(all_overlays) and col < num_cols:\n            axs[row,col].imshow(img,cmap = \"gray\",aspect = 'auto')\n            axs[row,col].axis(False)\n            axs[row,col].set_xticklabels([])\n            axs[row,col].set_yticklabels([])\n            axs[row,col].axis(\"off\")\n            img = all_overlays.pop()\n            col += 1\n        row +=1\n        col = 0\n    # adjust the height and width space and show figure\n    plt.subplots_adjust(wspace=.001, hspace=.001)\n    plt.show()\n    \n\n# Plot some cases\nplot_case(7)\n","metadata":{"pycharm":{"name":"#%%\n"},"execution":{"iopub.status.busy":"2022-06-11T18:37:42.965221Z","iopub.execute_input":"2022-06-11T18:37:42.965688Z","iopub.status.idle":"2022-06-11T18:37:53.671648Z","shell.execute_reply.started":"2022-06-11T18:37:42.965651Z","shell.execute_reply":"2022-06-11T18:37:53.669321Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As shown in the cases the first and lasts picturs dosn't contaion any segmentation.\n<br>Lets check the maximum and minimum images per case to see outliners","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"case_count = train_df.groupby(by=['case_id']).count().reset_index()[['case_id', \"id\"]]\nmincase_id, min_img_per_case = case_count.iloc[case_count['id'].idxmin()]\nmaxcase_id, max_img_per_case = case_count.iloc[case_count['id'].idxmax()]\nprint(f\"The MAX images per case is {max_img_per_case} in the case {mincase_id}\")\nprint(f\"The MIN images per case is {min_img_per_case} in the case {maxcase_id}\")\n\n# Clean up\ndel case_count","metadata":{"pycharm":{"name":"#%%\n"},"execution":{"iopub.status.busy":"2022-06-11T18:37:53.672827Z","iopub.execute_input":"2022-06-11T18:37:53.673226Z","iopub.status.idle":"2022-06-11T18:37:53.746728Z","shell.execute_reply.started":"2022-06-11T18:37:53.673193Z","shell.execute_reply":"2022-06-11T18:37:53.745894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lets plot the segmentation destribution of there segmentation maps","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"case_df = train_df[train_df['case_id'] == mincase_id]\nfig = px.histogram(case_df, case_df.seg_str.astype(str), color='seg_str',\n                   title=f\"<b>Distribution Of Images for min Case ID and Segmentation masks</b>\",\n                   labels={\"x\": \"<b>Case ID</b>\", \"day_num_str\": \"<b>The Day The Scan Took Place</b>\"}, text_auto=True,\n                   width=2000)\nfig.show()\ncase_df = train_df[train_df['case_id'] == maxcase_id]\nfig = px.histogram(case_df, case_df.seg_str.astype(str), color='seg_str',\n                   title=f\"<b>Distribution Of Images for max Case ID and Segmentation masks</b>\",\n                   labels={\"x\": \"<b>Case ID</b>\", \"day_num_str\": \"<b>The Day The Scan Took Place</b>\"}, text_auto=True,\n                   width=2000)\nfig.show()\n# Clean up\ndel case_df","metadata":{"pycharm":{"name":"#%%\n"},"execution":{"iopub.status.busy":"2022-06-11T18:37:53.74826Z","iopub.execute_input":"2022-06-11T18:37:53.74853Z","iopub.status.idle":"2022-06-11T18:37:53.898Z","shell.execute_reply.started":"2022-06-11T18:37:53.748495Z","shell.execute_reply":"2022-06-11T18:37:53.897205Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The main diffrece between them is the Stomach annotation.<br>\nManly the distribution correlated with the previuse analysis we did.","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"markdown","source":"### Mask dataset creation, class overlap.\n\nIt's important to determine if the the masks overlap one another (**multilabel**) or not (**multiclass**). To do this, we will quickly create a dataset of **`npy`** files. During this creation process we will check for overlap.\n\nTo save the masks and images masks we will use npy type files for quickly reading and saving \n<br><center><img src=\"https://miro.medium.com/max/492/1*xwpjjSdZwiOMnPJtdp9L2w.png\" width=50%></center><br><center>","metadata":{}},{"cell_type":"code","source":"def get_row_masks(row):\n    _slice_shape = (row.slice_w, row.slice_h)\n\n    if not pd.isna(row.lb_seg_rle):\n        lb_mask = rle_decode(row.lb_seg_rle, _slice_shape, )\n    else:\n        lb_mask = np.zeros(_slice_shape)\n    if not pd.isna(row.sb_seg_rle):\n        sb_mask = rle_decode(row.sb_seg_rle, _slice_shape)\n    else:\n        sb_mask = np.zeros(_slice_shape)\n    if not pd.isna(row.st_seg_rle):\n        st_mask = rle_decode(row.st_seg_rle, _slice_shape)\n    else:\n        st_mask = np.zeros(_slice_shape)\n    return lb_mask, sb_mask, st_mask\n\n\ndef is_overlap(_arr):\n    return _arr.sum(axis=-1).max()>1\n\n\ndef get_mask_overlap(row, check_overlap=False):\n    lb_mask, sb_mask, st_mask = get_row_masks(row)\n\n    mask_arr = np.stack([lb_mask, sb_mask, st_mask], axis=-1).astype(np.uint8)\n    np.save(f\"./npy_files/{row.id}_mask\", mask_arr)\n\n    if check_overlap:\n        if is_overlap(mask_arr):\n            return np.where(mask_arr.sum(axis=-1) > 1, 1, 0).sum()\n        else:\n            return 0\n    \nif not os.path.isdir(NPY_DIR): os.makedirs(NPY_DIR, exist_ok=True)\ntrain_df[\"seg_overlap_area\"] = train_df.progress_apply(lambda x: get_mask_overlap(x, check_overlap=True), axis=1)\n\nprint(\"\\n... LET'S EXAMINE THE IMAGE WITH THE HIGHEST AMOUNT OF OVERLAP ...\\n\")\n\nexamine_id(train_df[train_df.seg_overlap_area==train_df.seg_overlap_area.max()].id.values[0])\n\nfig = px.histogram(train_df[train_df.seg_overlap_area>0], \"seg_overlap_area\", color=\"seg_combo_str\", nbins=50,\n                   log_y=True, title=\"<b>Distribution of Non-Zero Segmentation Overlaps <sub>(Count Is Logarithmic)</sub></b>\",  \n                   labels={\"seg_overlap_area\":\"<b>Area of Mask Overlap</b>\", \n                           \"seg_combo_str\":\"<b>Segmentation Masks In Image</b>\"})\nfig.update_layout(legend=dict(\n    yanchor=\"top\",\n    y=0.99,\n    xanchor=\"right\",\n    x=0.995\n))\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-11T18:37:53.89932Z","iopub.execute_input":"2022-06-11T18:37:53.899698Z","iopub.status.idle":"2022-06-11T18:40:04.945372Z","shell.execute_reply.started":"2022-06-11T18:37:53.899663Z","shell.execute_reply":"2022-06-11T18:40:04.944659Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Main observation:\n\n* There is overlap, and while it is not that common, some images exhibit a high degree of overlap.\n* This means that we cannot frame the problem as simple categorical semantic segmentation.\n* We must instead frame the problem as multi-label semantic segmentation\n* This means our mask will take the form --> $W \\times H \\times 3$\n    * Where the channel dimensions are binary masks for each respective segmentation type\n    * This will allow for the masks to overlap\n\n\n**Note On The Plotted Image Ubove:**\n* In the examined image below we can see a section of the small bowel is completely inside of a larger section of larger bowel.\n* This shows why treating this as multi-label semantic segmentation is so important!\n","metadata":{}},{"cell_type":"markdown","source":"### Pixel Values In Our Dataset\n\nIt's important to analyse the dataset because we will need to normalize the data to convert it into a format that is more expected for machine learning (uint8 (0-255) or float32 (0-1)). Without knowing the limits of the images, we may diminish the resolution of the data by accident when normalizing.\n","metadata":{}},{"cell_type":"code","source":"def get_image_vals(row):\n    _img = cv2.imread(row.f_path, -1)\n    _nonzero_px_count = np.count_nonzero(_img)\n    \n    row[\"nonzero_num_pxs\"] = _nonzero_px_count\n    row[\"max_px_value\"] = _img.max()\n    row[\"min_px_value\"] = _img.min()\n    row[\"mean_px_value\"] = _img.mean()\n    row[\"nonzero_mean_px_value\"] = _img.sum()/_nonzero_px_count\n    del _img\n    return row\n\ntrain_df = train_df.progress_apply(get_image_vals, axis=1)\n\nprint(f\"\\n\\n\\n... UPDATED TRAIN DATAFRAME ...\\n\")\ndisplay(train_df.head())\nprint(\"\\n\\n\")\n\nfor _c in [\"nonzero_num_pxs\", \"max_px_value\", \"min_px_value\", \"mean_px_value\", \"nonzero_mean_px_value\"]:\n    print(f\"\\n... STATS FOR COLUMN --> `{_c}`...\")\n    print(f\"\\t--> MIN  VAL: {train_df[_c].min():.1f}\")\n    print(f\"\\t--> MEAN VAL: {train_df[_c].mean():.1f}\")\n    print(f\"\\t--> MAX  VAL: {train_df[_c].max():.1f}\")","metadata":{"pycharm":{"name":"#%%\n"},"execution":{"iopub.status.busy":"2022-06-11T18:40:04.946955Z","iopub.execute_input":"2022-06-11T18:40:04.947521Z","iopub.status.idle":"2022-06-11T18:47:05.730387Z","shell.execute_reply.started":"2022-06-11T18:40:04.947478Z","shell.execute_reply":"2022-06-11T18:47:05.729308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Interestingly the maximum value in the dataset is equiavlent to less than half of an int16 or a quarter of a uint16.\n* Max Value for UINT16\n    * **65535**\n* Max Value for INT16\n    * **32767**\n* Half of Max Value for INT16\n    * **16384**\n* Actual Max Value in the dataset\n    * **15865**","metadata":{}},{"cell_type":"markdown","source":"### Identify Any Heuristics Or Rules Regarding Segmentation\n\nLets try to fined pattern or ruls for the given dataset.","metadata":{}},{"cell_type":"code","source":"train_df[\"slice_count\"] = train_df.id.apply(lambda x: int(x.rsplit(\"_\", 1)[-1]))\n\nprint(\"\\n... CASE-ID/DAY-NUM SLICE INFORMATION ...\\n\")\ntrain_df.groupby([\"case_id\", \"day_num\"])[\"slice_count\"].max().value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-06-11T18:47:05.731606Z","iopub.execute_input":"2022-06-11T18:47:05.732112Z","iopub.status.idle":"2022-06-11T18:47:05.784497Z","shell.execute_reply.started":"2022-06-11T18:47:05.732074Z","shell.execute_reply":"2022-06-11T18:47:05.783795Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"slice_to_occurence_df = train_df.groupby(\"slice_count\")[[\"lb_seg_flag\", \"sb_seg_flag\", \"st_seg_flag\"]].sum().reset_index()\nfig = px.bar(slice_to_occurence_df,\n             x=\"slice_count\", y=[\"lb_seg_flag\", \"sb_seg_flag\", \"st_seg_flag\"],\n             orientation=\"v\",\n             labels={\n                \"slice_count\": \"<b>Slice Number</b>\",\n                \"value\": \"<b>Number Of Examples</b>\",\n             },\n             title=\"<b>Number of Examples Per Example For Our 3 Organs</b>\")\n\nfig.update_layout(legend_title=\"<b>Organ Type Legend</b>\")\nfig.show()\n\nprint(\"\\n... WHICH SLICES ARE ALWAYS BLANK (NO SEG) BY LABEL ...\\n\")\nkeep_slice_blank_map = {\n    _sh_lbl: slice_to_occurence_df[slice_to_occurence_df[f\"{_sh_lbl}_seg_flag\"] == 0].slice_count.to_list() for _sh_lbl in [\"lb\", \"sb\", \"st\"]\n}\nkeep_slice_blank_map","metadata":{"execution":{"iopub.status.busy":"2022-06-11T18:47:05.788661Z","iopub.execute_input":"2022-06-11T18:47:05.788977Z","iopub.status.idle":"2022-06-11T18:47:05.911677Z","shell.execute_reply.started":"2022-06-11T18:47:05.788949Z","shell.execute_reply":"2022-06-11T18:47:05.910977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For a given **case-id** and **day number** there are two different amounts of scans present\n* 144 slices --> 259 instances\n* 80 slices ---> 15 instances\n\nSome other observations about our training dataset\n* There are no examples for slices number **1, 138, 139, 140, 141, 142, 143 or 144** that have any segmentation masks\n* If we break it down by organ we get the following no-value slices for each respective organ\n    * Large Bowel – **1, 138, 139, 140, 141, 142, 143, 144**\n    * Small Bowel – **1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 138, 139, 140, 141, 142, 143, 144**\n    * Stomach – **1, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144**","metadata":{}},{"cell_type":"markdown","source":"## Create a 3D GIF for case with mask\n\nBecause the images is a 3D model of a patiant orgens, a good way to look at the sequence data is visualize it by using 3D gif.","metadata":{}},{"cell_type":"code","source":"def create_animation(case_id, day_num, df=train_df):\n    \n    sub_df = df[(df.case_id==case_id) & (df.day_num==day_num)]\n    \n    f_paths  = sub_df.f_path.tolist()\n    lb_rles  = sub_df.lb_seg_rle.tolist()\n    sb_rles  = sub_df.sb_seg_rle.tolist()\n    st_rles  = sub_df.st_seg_rle.tolist()\n    slice_ws = sub_df.slice_w.tolist()\n    slice_hs = sub_df.slice_h.tolist()\n    \n    animation_arr = np.stack([\n        get_overlay(img_path=_f, rle_strs=(_lb, _sb, _st), img_shape=(_w, _h)) \\\n        for _f, _lb, _sb, _st, _w, _h in \\\n        zip(f_paths, lb_rles, sb_rles, st_rles, slice_ws, slice_hs)\n    ], axis=0)\n    \n    fig = plt.figure(figsize=(8,8))\n    \n    plt.axis('off')\n    im = plt.imshow(animation_arr[0])\n    plt.title(f\"3D Animation for Case {case_id} on Day {day_num}\", fontweight=\"bold\")\n    \n    def animate_func(i):\n        im.set_array(animation_arr[i])\n        return [im]\n    plt.close()\n    \n    return animation.FuncAnimation(fig, animate_func, frames = animation_arr.shape[0], interval = 1000//12)\n\n\ncreate_animation(case_id=115, day_num=0)","metadata":{"execution":{"iopub.status.busy":"2022-06-11T18:47:05.913192Z","iopub.execute_input":"2022-06-11T18:47:05.913644Z","iopub.status.idle":"2022-06-11T18:47:20.734359Z","shell.execute_reply.started":"2022-06-11T18:47:05.913606Z","shell.execute_reply":"2022-06-11T18:47:20.730334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Scan with errors\nWhile observing those cases and looking in the form, we noted they have wrong segmentation masks\n\nhttps://www.kaggle.com/competitions/uw-madison-gi-tract-image-segmentation/discussion/319963\n\nCase 7 – Day 0\nCase 81 – Day 30","metadata":{}},{"cell_type":"code","source":"problem_case_1 = 7\nproblem_day_1 = 0\nproblem_case_2 = 81\nproblem_day_2 = 30\n\nprint(\"\\n... PROBLEM CASE NUMBER 1 ...\\n\")\ncreate_animation(case_id=problem_case_1, day_num=problem_day_1)\nprint(\"\\n... PROBLEM CASE NUMBER 2 ...\\n\")\ncreate_animation(case_id=problem_case_2, day_num=problem_day_2)","metadata":{"execution":{"iopub.status.busy":"2022-06-11T18:47:20.735773Z","iopub.execute_input":"2022-06-11T18:47:20.736206Z","iopub.status.idle":"2022-06-11T18:47:36.685552Z","shell.execute_reply.started":"2022-06-11T18:47:20.736166Z","shell.execute_reply":"2022-06-11T18:47:36.683357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Remove broken scans","metadata":{}},{"cell_type":"code","source":"remove_ids = [\"case7_day0\", \"case81_day30\"]\nfor _id in remove_ids:\n    train_df = train_df[~train_df.id.str.contains(_id)].reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2022-06-11T18:47:36.686958Z","iopub.execute_input":"2022-06-11T18:47:36.687401Z","iopub.status.idle":"2022-06-11T18:47:36.8062Z","shell.execute_reply.started":"2022-06-11T18:47:36.687349Z","shell.execute_reply":"2022-06-11T18:47:36.80552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"APPROXIMATE CLASS WEIGHTING (CATEGORICAL ASSUMPTION)\".lower().title()","metadata":{"execution":{"iopub.status.busy":"2022-06-11T18:47:36.807301Z","iopub.execute_input":"2022-06-11T18:47:36.808016Z","iopub.status.idle":"2022-06-11T18:47:36.813851Z","shell.execute_reply.started":"2022-06-11T18:47:36.807976Z","shell.execute_reply":"2022-06-11T18:47:36.81317Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Approximate Class Weighting (Categorical Assumption)\n\nLet us calculate a naive approximation of the weights of classes based on the frequency of occurence of various classes. For the purpose of this investigation we will treat the background as it's own class (the most common class probably).\n","metadata":{}},{"cell_type":"code","source":"# # Get total image area\ntrain_df[\"img_px_area\"] = train_df[\"slice_w\"] * train_df[\"slice_h\"]\ntrain_df[[\"lb_seg_area\", \"sb_seg_area\", \"st_seg_area\"]].fillna(0, inplace=True)\ntrain_df[\"bg_area\"] = (train_df[\"img_px_area\"] - train_df[[\"lb_seg_area\", \"sb_seg_area\", \"st_seg_area\"]].sum(axis=1)).astype(int)\n\nprint(f\"\\nALL TRAINING DATA PIXEL COUNT         : {train_df.img_px_area.sum()}\")\nprint(f\"BACKGROUND TRAINING DATA PIXEL COUNT  : {train_df.bg_area.sum()}\")\nprint(f\"LARGE BOWEL TRAINING DATA PIXEL COUNT : {train_df.lb_seg_area.sum()}\")\nprint(f\"SMALL BOWEL TRAINING DATA PIXEL COUNT : {train_df.sb_seg_area.sum()}\")\nprint(f\"STOMACH TRAINING DATA PIXEL COUNT     : {train_df.st_seg_area.sum()}\\n\")\n\nprint(f\"\\nALL TRAINING DATA PIXEL COUNT (%)         : %{100:.4f}\")\nprint(f\"BACKGROUND TRAINING DATA PIXEL COUNT (%)  : %{100 * train_df.bg_area.sum() / train_df.img_px_area.sum():.4f}\")\nprint(f\"LARGE BOWEL TRAINING DATA PIXEL COUNT (%) : %{100 * train_df.lb_seg_area.sum() / train_df.img_px_area.sum():.4f}\")\nprint(f\"SMALL BOWEL TRAINING DATA PIXEL COUNT (%) : %{100 * train_df.sb_seg_area.sum() / train_df.img_px_area.sum():.4f}\")\nprint(f\"STOMACH TRAINING DATA PIXEL COUNT (%)     : %{100 * train_df.st_seg_area.sum() / train_df.img_px_area.sum():.4f}\\n\")","metadata":{"execution":{"iopub.status.busy":"2022-06-11T18:47:36.815269Z","iopub.execute_input":"2022-06-11T18:47:36.815832Z","iopub.status.idle":"2022-06-11T18:47:36.848668Z","shell.execute_reply.started":"2022-06-11T18:47:36.815798Z","shell.execute_reply":"2022-06-11T18:47:36.847921Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the resaults we can varify for each label:\n* Class **0** - **Background**\n  * Total Pixel Count In Training Dataset = **3113614754** \n* Class **1** - **Large Bowel**\n  * Total Pixel Count In Training Dataset = **21827402**\n* Class **2** - **Small Bowel**\n  * Total Pixel Count In Training Dataset = **19898898**\n* Class **3** - **Stomach**\n  * Total Pixel Count In Training Dataset = **11064002**\n","metadata":{}},{"cell_type":"code","source":"# Remove unnecessary files\nfor dirname, _, filenames in os.walk(NPY_DIR):\n    for filename in filenames:\n        os.remove(os.path.join(dirname, filename))","metadata":{"execution":{"iopub.status.busy":"2022-06-11T18:56:05.529802Z","iopub.execute_input":"2022-06-11T18:56:05.530211Z","iopub.status.idle":"2022-06-11T18:56:07.757805Z","shell.execute_reply.started":"2022-06-11T18:56:05.530171Z","shell.execute_reply":"2022-06-11T18:56:07.757006Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modeling \n\nBefore we start running ML algorithems to preform semantic segmentation, lets modify the data in a model format.\nThat means we need to:\n1. Creat dataset folds - validation and training\n2. Creat mask dataset","metadata":{}},{"cell_type":"markdown","source":"### Models constants","metadata":{}},{"cell_type":"code","source":"# Debuge mode \nDEBUGE = False\nDEB_EXAMPLES = 20 # Debuge training amount\n# specify number of folds for cross validation \nN_FOLDS = 4\nIMAGE_SHAPE = SEG_SHAPE = (256,256)\n# define batch sizes, only use them if were not in debuge mode\nTR_BATCH = 32  # 64 and 128 are to big for memory in kaggle\nVALID_BATCH = TR_BATCH * 2\n# Data loader constant\nNUM_WORKERS = 2\n\n# Model configuration\nENCODER = 'efficientnet-b7'\nNUM_CLASSES = 3  # Number of orgens.\nDEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")  # Use GPU is avaliable\n# set learning rates\nLR = 2e-3\nMIN_LR = 1e-6\n# Epochs and weight decay\nEPOCHS = 10\nWD = 1e-6\n# Maximum iterations\nT_MAX = int(30000/TR_BATCH*EPOCHS)+50\n# Number of iterations for the first restart\nT_0 = 25","metadata":{"execution":{"iopub.status.busy":"2022-06-11T18:54:32.299707Z","iopub.execute_input":"2022-06-11T18:54:32.29998Z","iopub.status.idle":"2022-06-11T18:54:32.306455Z","shell.execute_reply.started":"2022-06-11T18:54:32.29995Z","shell.execute_reply":"2022-06-11T18:54:32.305474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Cross validation\nCross-Validation has two main steps: splitting the data into subsets (called folds) and rotating the training and validation among them. The splitting technique commonly has the following properties:\n\n* Each fold has approximately the same size.\n* Data can be randomly selected in each fold or stratified.\n* All folds are used to train the model except one, which is used for validation. That validation fold should be rotated until all folds have become a validation fold once and only once.\n* Each example is recommended to be contained in one and only one fold.\n\nK-fold and CV are two terms that are used interchangeably. K-fold is just describing how many folds you want to split your dataset into. Many libraries use k=10 as a default value representing 90% going to training and 10% going to the validation set. The next figure describes the process of iterating over the picked ten folds of the dataset.","metadata":{}},{"cell_type":"code","source":"# User sklearn cross validation groups\ngkf = GroupKFold(n_splits=N_FOLDS) \n\n### train only images with segmentation masks ###\n# train_df = train_df[train_df.n_segs>0].reset_index(drop=True)\n### train only images with segmentation masks end###\n\ntrain_df[\"which_segs\"] = train_df.lb_seg_flag.astype(int).astype(str)+\\\n                         train_df.sb_seg_flag.astype(int).astype(str)+\\\n                         train_df.st_seg_flag.astype(int).astype(str)\n\nfolds = []# list of indexes for validation and training folds.\nfor train_idxs, val_idxs in gkf.split(train_df[\"id\"], train_df[\"which_segs\"], train_df[\"case_id\"]):\n    folds.append([train_idxs,val_idxs])\n\nprint(f\"Shape of folds: {np.shape(folds)}\")\n\n# lets print one test and validation set to see the data\nprint(\"\\nFOLD 1: TRAIN DF\\n\\n\")\ntrain_df.iloc[folds[0][0]].head()\n\nprint(\"\\n\\n\\n\\nFOLD 1: VAL DF\\n\\n\")\ntrain_df.iloc[folds[0][1]].head()\n","metadata":{"execution":{"iopub.status.busy":"2022-06-11T18:54:35.293422Z","iopub.execute_input":"2022-06-11T18:54:35.294124Z","iopub.status.idle":"2022-06-11T18:54:35.454385Z","shell.execute_reply.started":"2022-06-11T18:54:35.294085Z","shell.execute_reply":"2022-06-11T18:54:35.453396Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Mask dataset\n\nWe will create our masks to have a shape of $W×H×3$ where each channel is binary mask for a particular segmentation class in the order\n\n* Channel 0 --> \"Large Bowel\"\n* Channel 1 --> \"Small Bowel\"\n* Channel 2 --> \"Stomach\"\n\nWe will first frame this problem as simple categorical segmentation and simply overlap values from 2-->0\ni.e. if we have overlapping Stomach, and Small Bowel... the Small Bowel mask will overwrite the Stomach mask\ni.e. if we have overlapping Large Bowel and Small Bowel... the Large Bowel mask will overwrite the Small Bowel mask.\n\nWe will save both version of this dataset so we can try experimenting later\n\nAt this point we have to determine the size of our dataset... as most images are fairly small, let's target a size of  $256×256$","metadata":{}},{"cell_type":"code","source":"def save_get_mask_path(row, output_dir, resize_to):\n    \"\"\"\n    The function saves the masks in the output dir.\n    1. creat segmentation mask\n    2. Determine if the problem is multi class or multi label\n    3. Save the mask with the correct shape.\n    \"\"\"\n    lb_mask, sb_mask, st_mask = get_row_masks(row)\n\n    _output_style = \"multiclass\" if \"multiclass\" in output_dir else \"multilabel\"\n    if _output_style == \"multiclass\":\n        mask_arr = st_mask * 3  # stomach = 3\n        mask_arr = np.where(sb_mask == 1, 2, mask_arr)  # small bowel = 2\n        mask_arr = np.where(lb_mask == 1, 1, mask_arr)  # large bowel = 1\n    else:\n        mask_arr = np.stack([lb_mask, sb_mask, st_mask], axis=-1)\n\n    # resize to image shape\n    mask_arr = cv2.resize(mask_arr, resize_to, interpolation=cv2.INTER_NEAREST).astype(np.uint8)\n    mask_path = os.path.join(output_dir, f\"{row.id}_mask\")\n    np.save(mask_path, mask_arr)\n    return mask_path + \".npy\"\n\n# Create both multi label and multi class masks\n# styles = ['multilabel','multiclass']\nstyles = ['multilabel']\nfor style in styles:\n    _output_dir = f\"/kaggle/working/{style}/npy_files\"\n    if not os.path.isdir(_output_dir): os.makedirs(_output_dir, exist_ok=True)\n    train_df[f\"{style}_mask_path\"] = train_df.progress_apply(lambda _row: save_get_mask_path(_row, _output_dir, resize_to=SEG_SHAPE), axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-06-11T18:57:13.227404Z","iopub.execute_input":"2022-06-11T18:57:13.227857Z","iopub.status.idle":"2022-06-11T18:58:18.110056Z","shell.execute_reply.started":"2022-06-11T18:57:13.227809Z","shell.execute_reply":"2022-06-11T18:58:18.109381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load dataset before training\nIn this step we will load the data into 2 subsets for training.\n\nWe will build pytorch model dataset to match the model format.\n\nSelected parameters:\n* batch size = 32, we tried 64 and 128 but kaggle machine don't have enoght ram memory to upload that much information.\n* validation batch = batch size*2, to test validation we don't realy need to devide to batches but to make sure that the memory will hold with this size we will take twice as much data in the validation batch.\n","metadata":{}},{"cell_type":"code","source":"def prepare_loaders(fold, debug=False):\n    \"\"\"\n    Get pytorch train and validation data loaders(image paths with masks).\n    \"\"\"\n    tr_idxs, val_idxs = folds[fold]  # get trian and validation indexes\n    if debug:\n        tr_idxs = tr_idxs[:DEB_EXAMPLES]\n        val_idxs = val_idxs[:DEB_EXAMPLES]\n    tr_df = train_df.iloc[tr_idxs].reset_index(drop=True)\n    val_df = train_df.iloc[val_idxs].reset_index(drop=True)\n    # Build folds datasets\n    train_dataset = BuildDataset(tr_df, transforms=data_transforms['train'])\n    valid_dataset = BuildDataset(val_df, transforms=data_transforms['valid'])\n\n    train_loader = DataLoader(train_dataset, batch_size=TR_BATCH if not debug else DEB_EXAMPLES,\n                              num_workers=NUM_WORKERS, shuffle=True, pin_memory=True, drop_last=False)\n    valid_loader = DataLoader(valid_dataset, batch_size=VALID_BATCH if not debug else DEB_EXAMPLES,\n                              num_workers=NUM_WORKERS, shuffle=False, pin_memory=True)\n\n    return train_loader, valid_loader","metadata":{"execution":{"iopub.status.busy":"2022-06-11T18:58:18.111941Z","iopub.execute_input":"2022-06-11T18:58:18.112206Z","iopub.status.idle":"2022-06-11T18:58:18.123718Z","shell.execute_reply.started":"2022-06-11T18:58:18.11217Z","shell.execute_reply":"2022-06-11T18:58:18.122815Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Image augmantation\nDeep neural networks require a lot of training data to obtain good results and prevent overfitting. However, it often very difficult to get enough training samples. Multiple reasons could make it very hard or even impossible to gather enough data:\n\nTo make a training dataset, you need to obtain images and then label them. For example, you need to assign correct class labels if you have an image classification task. For an object detection task, you need to draw bounding boxes around objects. For a **semantic segmentation** task, you need to assign a correct class to each input image pixel. This process requires manual labor, and sometimes it could be very costly to label the training data. For example, to correctly label medical images, you need expensive domain experts.\n\nSometimes even collecting training images could be hard. There are many legal restrictions for working with healthcare data, and obtaining it requires a lot of effort. Sometimes getting the training images is more feasible, but it will cost a lot of money. For example, to get satellite images, you need to pay a satellite operator to take those photos. To get images for road scene recognition, you need an operator that will drive a car and collect the required data.\n\nBasic augmentations techniques were used almost in all papers that describe the state-of-the-art models for image recognition.\n\nAlexNet was the first model that demonstrated exceptional capabilities of using deep neural networks for image recognition. For training, the authors used a set of basic image augmentation techniques. They resized original images to the fixed size of 256 by 256 pixels, and then they cropped patches of size 224 by 224 pixels as well as their horizontal reflections from those resized images. Also, they altered the intensities of the RGB channels in images.\n\nSuccessive state-of-the-art models such as Inception, ResNet, and EfficientNet also used image augmentation techniques for training.\n\nTo do this task we will use the open source packege **albumentations**.\n\n#### Basic image augmantation configuration:\n* Resize to correct image shape.\n* Make horizantal flip with randomality of 0.5\n* Shif scale and rotate image.\n* Select one of transforms to apply between \n    *  GridDistortion - Blur the input image using a Generalized Normal filter with a randomly selected parameters. This transform also adds multiplicative noise to generated kernel before convolution.\n    *  ElasticTransform - Elastic deformation of images as described in [Simard2003]_ (with modifications). Based on https://gist.github.com/ernestum/601cdf56d2b424757de5\n* CoarseDropout of the rectangular regions in the image.\n\nThe validaion data only get resize to fit the model, thats because we need the test data to be as close as we can to reality.\n","metadata":{}},{"cell_type":"code","source":"data_transforms = {\n    \"train\": A.Compose([\n        A.Resize(*IMAGE_SHAPE, interpolation=cv2.INTER_NEAREST),\n        A.HorizontalFlip(p=0.5),\n#         A.VerticalFlip(p=0.5),\n        A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.05, rotate_limit=10, p=0.5),\n        A.OneOf([\n            A.GridDistortion(num_steps=5, distort_limit=0.05, p=1.0),\n# #             A.OpticalDistortion(distort_limit=0.05, shift_limit=0.05, p=1.0),\n            A.ElasticTransform(alpha=1, sigma=50, alpha_affine=50, p=1.0)\n        ], p=0.25),\n        A.CoarseDropout(max_holes=8, max_height=IMAGE_SHAPE[0]//20, max_width=IMAGE_SHAPE[1]//20,\n                         min_holes=5, fill_value=0, mask_fill_value=0, p=0.5),\n        ], p=1.0),\n    \n    \"valid\": A.Compose([\n        A.Resize(*IMAGE_SHAPE, interpolation=cv2.INTER_NEAREST),\n        ], p=1.0)\n}\n","metadata":{"execution":{"iopub.status.busy":"2022-06-11T18:58:18.124972Z","iopub.execute_input":"2022-06-11T18:58:18.125586Z","iopub.status.idle":"2022-06-11T18:58:18.135946Z","shell.execute_reply.started":"2022-06-11T18:58:18.125548Z","shell.execute_reply":"2022-06-11T18:58:18.135082Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## First model training - Unet\nBefore preforming augmantation we will train the Unet base model and see the results\n<br><center><img src=\"https://production-media.paperswithcode.com/methods/Screen_Shot_2020-07-07_at_9.08.00_PM_rpNArED.png\" width=75%></center>\nU-Net is an architecture for semantic segmentation. It consists of a contracting path and an expansive path. The contracting path follows the typical architecture of a convolutional network. It consists of the repeated application of two 3x3 convolutions (unpadded convolutions), each followed by a rectified linear unit (ReLU) and a 2x2 max pooling operation with stride 2 for downsampling. At each downsampling step we double the number of feature channels. Every step in the expansive path consists of an upsampling of the feature map followed by a 2x2 convolution (“up-convolution”) that halves the number of feature channels, a concatenation with the correspondingly cropped feature map from the contracting path, and two 3x3 convolutions, each followed by a ReLU. The cropping is necessary due to the loss of border pixels in every convolution. At the final layer a 1x1 convolution is used to map each 64-component feature vector to the desired number of classes. In total the network has 23 convolutional layers.\n\n#### Pros:\n- Performs well even with smaller data\n- Can be used with imagenet pretrain models\n#### Cons:\n- Struggles with edge cases\n- Semantic Difference in Skip Connection","metadata":{}},{"cell_type":"markdown","source":"### Build model database\nUsing pytorch api we will create a dataset to train.","metadata":{}},{"cell_type":"code","source":"class BuildDataset(torch.utils.data.Dataset):\n    def __init__(self, df, label=True, transforms=None,_style = 'multilabel'):\n        self.df         = df\n        self.label      = label\n        self.img_paths  = df['f_path'].tolist()\n        self.msk_paths  = df[f\"{_style}_mask_path\"].tolist()\n        self.transforms = transforms\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, index):\n        img_path  = self.img_paths[index]\n        img = []\n        img = load_img(img_path)\n        \n        if self.label:\n            msk_path = self.msk_paths[index]\n            msk = load_msk(msk_path)\n            if self.transforms:\n                data = self.transforms(image=img, mask=msk)\n                img  = data['image']\n                msk  = data['mask']\n            img = np.transpose(img, (2, 0, 1))\n            msk = np.transpose(msk, (2, 0, 1))\n            return torch.tensor(img), torch.tensor(msk)\n        else:\n            if self.transforms:\n                data = self.transforms(image=img)\n                img  = data['image']\n            img = np.transpose(img, (2, 0, 1))\n            return torch.tensor(img)","metadata":{"execution":{"iopub.status.busy":"2022-06-11T18:58:18.137997Z","iopub.execute_input":"2022-06-11T18:58:18.138273Z","iopub.status.idle":"2022-06-11T18:58:18.148967Z","shell.execute_reply.started":"2022-06-11T18:58:18.13822Z","shell.execute_reply":"2022-06-11T18:58:18.148186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model configuration\n\nFor the model configuration we need to decide on:\n1. Encoder  - we selected efficientnet-b7 to get a fast and efficient encoder.\n2. Number of classes - as we know there are 3 classes as the number of orgens we need to detect.\n3. Device - if a GPU is avaliable then we will use it.\n4. Encoder weights - to use **transfer learning** we will initialize the network weights to the imagenet.","metadata":{}},{"cell_type":"code","source":"def build_model():\n    unet_model = smp.Unet(\n        encoder_name=ENCODER,      # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n        encoder_weights=\"imagenet\",     # use `imagenet` pre-trained weights for encoder initialization\n        in_channels=3,                  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n        classes=NUM_CLASSES,        # model output channels (number of classes in your dataset)\n        activation=None,\n    )\n    unet_model.to(DEVICE) \n    return unet_model\n\ndef load_model(path):\n    model = build_model()\n    model.load_state_dict(torch.load(path))\n    model.eval()\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-06-11T18:58:18.150188Z","iopub.execute_input":"2022-06-11T18:58:18.150679Z","iopub.status.idle":"2022-06-11T18:58:18.159538Z","shell.execute_reply.started":"2022-06-11T18:58:18.150587Z","shell.execute_reply":"2022-06-11T18:58:18.15877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Loss functions\nFor the loss function we aill use the multilabel mode.\n\nFor the first try we will use a combination of TverskyLoss and BCELoss \n\n#### TverskyLoss\nThis loss was introduced in \"Tversky loss function for image segmentationusing 3D fully convolutional deep networks\", retrievable here: https://arxiv.org/abs/1706.05721. It was designed to optimise segmentation on imbalanced medical datasets by utilising constants that can adjust how harshly different types of error are penalised in the loss function. From the paper:\n\n... in the case of α=β=0.5 the Tversky index simplifies to be the same as the Dice coefficient, which is also equal to the F1 score. With α=β=1, Equation 2 produces Tanimoto coefficient, and setting α+β=1 produces the set of Fβ scores. Larger βs weigh recall higher than precision (by placing more emphasis on false negatives).\n\n#### BCELoss\n\nBinary cross-entropy is the loss function for binary classification with a single output unit, and categorical cross-entropy is the loss function for multiclass classification. In the PyTorch, the categorical cross-entropy loss takes in ground truth labels as integers, for example, y=2, out of three classes, 0, 1, and 2.\n\n#### JaccardLoss - IoU\n<br><center><img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/c/c7/Intersection_over_Union_-_visual_equation.png/300px-Intersection_over_Union_-_visual_equation.png\" width=25%></center>\n\n\nThe metric that is required in the compatitaion is the Dice  metrice So we evaluate our preformence according to it.\n","metadata":{}},{"cell_type":"code","source":"# Optional loss functions:\nJaccardLoss = smp.losses.JaccardLoss(mode='multilabel')\nDiceLoss = smp.losses.DiceLoss(mode='multilabel')\nBCELoss = smp.losses.SoftBCEWithLogitsLoss()\nLovaszLoss = smp.losses.LovaszLoss(mode='multilabel', per_image=False)\nTverskyLoss = smp.losses.TverskyLoss(mode='multilabel', log_loss=False)\n\n\ndef dice_coef(y_true, y_pred, thr=0.5, dim=(2, 3), epsilon=0.001):\n    y_true = y_true.to(torch.float32)\n    y_pred = (y_pred > thr).to(torch.float32)\n    inter = (y_true * y_pred).sum(dim=dim)\n    den = y_true.sum(dim=dim) + y_pred.sum(dim=dim)\n    dice = ((2 * inter + epsilon) / (den + epsilon)).mean(dim=(1, 0))\n    return dice\n\n\ndef iou_coef(y_true, y_pred, thr=0.5, dim=(2, 3), epsilon=0.001):\n    y_true = y_true.to(torch.float32)\n    y_pred = (y_pred > thr).to(torch.float32)\n    inter = (y_true * y_pred).sum(dim=dim)\n    union = (y_true + y_pred - y_true * y_pred).sum(dim=dim)\n    iou = ((inter + epsilon) / (union + epsilon)).mean(dim=(1, 0))\n    return iou\n\n\ndef criterion(y_pred, y_true):\n    return 0.5 * BCELoss(y_pred, y_true) + 0.5 * TverskyLoss(y_pred, y_true)","metadata":{"execution":{"iopub.status.busy":"2022-06-11T18:58:18.160606Z","iopub.execute_input":"2022-06-11T18:58:18.16099Z","iopub.status.idle":"2022-06-11T18:58:18.172617Z","shell.execute_reply.started":"2022-06-11T18:58:18.160953Z","shell.execute_reply":"2022-06-11T18:58:18.171975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Train function\nFor training we define a train one epoch that trains the model, save the gradient and update the network values. ","metadata":{}},{"cell_type":"code","source":"# define one epoch train\ndef train_one_epoch(model, optimizer, scheduler, dataloader, device, epoch):\n    model.train()\n    scaler = amp.GradScaler()\n    \n    dataset_size = 0\n    running_loss = 0.0\n    \n    pbar = tqdm(enumerate(dataloader), total=len(dataloader), desc='Train ')\n    for step, (images, masks) in pbar:         \n        images = images.to(device, dtype=torch.float)\n        masks  = masks.to(device, dtype=torch.float)\n        \n        batch_size = images.size(0)\n        \n        with amp.autocast(enabled=True):\n            y_pred = model(images)\n            loss   = criterion(y_pred, masks)\n            \n        scaler.scale(loss).backward()\n    \n        if (step + 1) % 1 == 0:\n            scaler.step(optimizer)\n            scaler.update()\n\n            # zero the parameter gradients\n            optimizer.zero_grad()\n\n            if scheduler is not None:\n                scheduler.step()\n                \n        running_loss += (loss.item() * batch_size)\n        dataset_size += batch_size\n        \n        epoch_loss = running_loss / dataset_size\n        \n        mem = torch.cuda.memory_reserved() / 1E9 if torch.cuda.is_available() else 0\n        current_lr = optimizer.param_groups[0]['lr']\n        pbar.set_postfix(train_loss=f'{epoch_loss:0.4f}',\n                        lr=f'{current_lr:0.5f}',\n                        gpu_mem=f'{mem:0.2f} GB')\n    torch.cuda.empty_cache()\n    gc.collect()\n    \n    return epoch_loss","metadata":{"execution":{"iopub.status.busy":"2022-06-11T18:58:18.174039Z","iopub.execute_input":"2022-06-11T18:58:18.174564Z","iopub.status.idle":"2022-06-11T18:58:18.186358Z","shell.execute_reply.started":"2022-06-11T18:58:18.174524Z","shell.execute_reply":"2022-06-11T18:58:18.185695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Validation function \nThe validation function doesnt comput the gradient and only makes predictions with the test data.","metadata":{}},{"cell_type":"code","source":"@torch.no_grad() # dont compute gradient when testing validation set\ndef valid_one_epoch(model, dataloader, device, epoch):\n    model.eval()\n    \n    dataset_size = 0\n    running_loss = 0.0\n    \n    val_scores = []\n    \n    pbar = tqdm(enumerate(dataloader), total=len(dataloader), desc='Valid ')\n    for step, (images, masks) in pbar:        \n        images  = images.to(device, dtype=torch.float)\n        masks   = masks.to(device, dtype=torch.float)\n        \n        batch_size = images.size(0)\n        \n        y_pred  = model(images)\n        loss    = criterion(y_pred, masks)\n        \n        running_loss += (loss.item() * batch_size)\n        dataset_size += batch_size\n        \n        epoch_loss = running_loss / dataset_size\n        \n        y_pred = nn.Sigmoid()(y_pred)\n        val_dice = dice_coef(masks, y_pred).cpu().detach().numpy()\n        val_jaccard = iou_coef(masks, y_pred).cpu().detach().numpy()\n        val_scores.append([val_dice, val_jaccard])\n        \n        mem = torch.cuda.memory_reserved() / 1E9 if torch.cuda.is_available() else 0\n        current_lr = optimizer.param_groups[0]['lr']\n        pbar.set_postfix(valid_loss=f'{epoch_loss:0.4f}',\n                        lr=f'{current_lr:0.5f}',\n                        gpu_memory=f'{mem:0.2f} GB')\n    val_scores  = np.mean(val_scores, axis=0)\n    torch.cuda.empty_cache()\n    gc.collect()\n    \n    return epoch_loss, val_scores","metadata":{"execution":{"iopub.status.busy":"2022-06-11T18:58:18.187747Z","iopub.execute_input":"2022-06-11T18:58:18.188094Z","iopub.status.idle":"2022-06-11T18:58:18.201349Z","shell.execute_reply.started":"2022-06-11T18:58:18.18806Z","shell.execute_reply":"2022-06-11T18:58:18.200623Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Run training\nThe following function runs the training for number of epoches and with the arguments that she recived.","metadata":{}},{"cell_type":"code","source":"def run_training(model, optimizer, scheduler, device, num_epochs):\n    if torch.cuda.is_available():\n        print(\"cuda: {}\\n\".format(torch.cuda.get_device_name()))\n    \n    start = time.time()\n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_dice      = -np.inf\n    best_epoch     = -1\n    history = defaultdict(list)\n    \n    for epoch in range(1, num_epochs + 1): \n        gc.collect()\n        print(f'Epoch {epoch}/{num_epochs}', end='')\n        train_loss = train_one_epoch(model, optimizer, scheduler, \n                                           dataloader=train_loader, \n                                           device=device, epoch=epoch)\n        \n        val_loss, val_scores = valid_one_epoch(model, valid_loader, \n                                                 device=DEVICE, \n                                                 epoch=epoch)\n        val_dice, val_jaccard = val_scores\n    \n        history['Train Loss'].append(train_loss)\n        history['Valid Loss'].append(val_loss)\n        history['Valid Dice'].append(val_dice)\n        history['Valid Jaccard'].append(val_jaccard)\n    \n        \n        print(f'Valid Dice: {val_dice:0.4f} | Valid Jaccard: {val_jaccard:0.4f}')\n        \n        # deep copy the model\n        if val_dice >= best_dice:\n            print(f\"Valid Score Improved ({best_dice:0.4f} ---> {val_dice:0.4f})\")\n            best_dice    = val_dice\n            best_jaccard = val_jaccard\n            best_epoch   = epoch\n            run[\"Best Dice\"]    = best_dice\n            run[\"Best Jaccard\"] = best_jaccard\n            run[\"Best Epoch\"]   = best_epoch\n            best_model_wts = copy.deepcopy(model.state_dict())\n            path = f\"best_epoch-{fold:02d}.bin\"\n            torch.save(model.state_dict(), path)\n            print(f\"Model Saved\")\n            \n        last_model_wts = copy.deepcopy(model.state_dict())\n        path = f\"last_epoch-{fold:02d}.bin\"\n        torch.save(model.state_dict(), path)\n            \n        print(); print()\n    \n    end = time.time()\n    time_elapsed = end - start\n    print('Training complete in {:.0f}h {:.0f}m {:.0f}s'.format(\n        time_elapsed // 3600, (time_elapsed % 3600) // 60, (time_elapsed % 3600) % 60))\n    print(\"Best Score: {:.4f}\".format(best_jaccard))\n    \n    # load best model weights\n    model.load_state_dict(best_model_wts)\n    \n    return model, history","metadata":{"execution":{"iopub.status.busy":"2022-06-11T18:58:18.202937Z","iopub.execute_input":"2022-06-11T18:58:18.20319Z","iopub.status.idle":"2022-06-11T18:58:18.218016Z","shell.execute_reply.started":"2022-06-11T18:58:18.203157Z","shell.execute_reply":"2022-06-11T18:58:18.216732Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Optimizer\nAfter choosing the optimizer we will add a scheduler to reduce learning rate.\nSchesulers that we choose to expiramate are:\n1. CosineAnnealingLR - https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.CosineAnnealingLR.html\n2. CosineAnnealingWarmRestarts - https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.CosineAnnealingWarmRestarts.html\n3. ReduceLROnPlateau - https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.ReduceLROnPlateau.html\n4. ExponentialLR - https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.ExponentialLR.html","metadata":{}},{"cell_type":"code","source":"def fetch_scheduler(optimizer,scheduler_name):\n    if scheduler_name == 'CosineAnnealingLR':\n        scheduler = lr_scheduler.CosineAnnealingLR(optimizer,T_max=T_MAX, \n                                                   eta_min=MIN_LR)\n    elif scheduler_name == 'CosineAnnealingWarmRestarts':\n        scheduler = lr_scheduler.CosineAnnealingWarmRestarts(optimizer,T_0=T_0, \n                                                             eta_min=MIN_LR)\n    elif scheduler_name == 'ReduceLROnPlateau':\n        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer,\n                                                   mode='min',\n                                                   factor=0.1,\n                                                   patience=7,\n                                                   threshold=0.0001,\n                                                   min_lr=MIN_LR)\n    elif scheduler_name == 'ExponentialLR':\n        scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=0.85)\n    elif scheduler_name == None:\n        return None\n        \n    return scheduler","metadata":{"execution":{"iopub.status.busy":"2022-06-11T18:58:18.220754Z","iopub.execute_input":"2022-06-11T18:58:18.221503Z","iopub.status.idle":"2022-06-11T18:58:18.229086Z","shell.execute_reply.started":"2022-06-11T18:58:18.221464Z","shell.execute_reply":"2022-06-11T18:58:18.228237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Download the model with initial weghits\nIn this section we will download the unet model using the build_model function that we have created.\n\nAfter that we will set the optimizer and check the scheduler function.","metadata":{}},{"cell_type":"code","source":"# Check if model build sucseefully\nunet_model = build_model()\noptimizer = optim.Adam(unet_model.parameters(), lr=LR, weight_decay=WD)\nscheduler = fetch_scheduler(optimizer, 'CosineAnnealingLR')","metadata":{"execution":{"iopub.status.busy":"2022-06-11T18:58:18.230179Z","iopub.execute_input":"2022-06-11T18:58:18.231011Z","iopub.status.idle":"2022-06-11T18:58:19.640018Z","shell.execute_reply.started":"2022-06-11T18:58:18.230973Z","shell.execute_reply":"2022-06-11T18:58:19.639301Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Train the model!","metadata":{}},{"cell_type":"code","source":"scheduler_name = 'CosineAnnealingLR'\n# Clean GPU\ngc.collect()\ntorch.cuda.empty_cache()\nrun = {}\nruning_folds = 2 #N_FOLDS takes to much time for kaggle machine with limit of 12 hours.\nif DEBUGE: runing_folds = 1\nfor fold in range(runing_folds):\n    print(f'#'*15)\n    print(f'### Fold: {fold}')\n    print(f'#'*15)\n   \n    train_loader, valid_loader = prepare_loaders(fold=fold, debug=DEBUGE)\n    unet_model = build_model()\n    optimizer = optim.Adam(unet_model.parameters(), lr=LR, weight_decay=WD)\n    scheduler = fetch_scheduler(optimizer,scheduler_name)\n    unet_model, history = run_training(\n        unet_model, optimizer, scheduler,\n        device=DEVICE,\n        num_epochs=EPOCHS\n    )\n# Show model metrics\nfor key, value in run.items():\n    print(key, ' : ', value)","metadata":{"execution":{"iopub.status.busy":"2022-06-11T18:58:19.641203Z","iopub.execute_input":"2022-06-11T18:58:19.641492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Visualize the training and validation loss and Dice","metadata":{}},{"cell_type":"code","source":"def plot_history(history, title, labels, subplot):\n    plt.subplot(*subplot)\n    plt.title(title)\n    plt.xlabel(\"Epoch #\")\n    for label in labels:\n        plt.plot(history[label], label=label)\n    plt.legend()\n    \ndef plot_fit_result(history):\n    plt.figure(figsize=(20, 8))\n    plot_history(history, \"Training Loss on Dataset\", ['Train Loss','Valid Loss'], (1, 2, 1))\n    plot_history(history, \"Dice and IoU\", ['Valid Dice','Valid Jaccard'], (1, 2, 2))\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_fit_result(history)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Make predictions\nAfter training the model lets make prediction on a fold validation set and see the results.","metadata":{}},{"cell_type":"code","source":"tr_idxs, val_idxs = folds[0]  # get trian and validation indexes\nval_df = train_df.iloc[val_idxs].reset_index(drop=True)\n\ntest_dataset = BuildDataset(val_df, label=False, \n                            transforms=data_transforms['valid'])\ntest_loader  = DataLoader(test_dataset, batch_size=5, \n                          num_workers=4, shuffle=False, pin_memory=True)\nimgs = next(iter(test_loader))\nimgs = imgs.to(DEVICE, dtype=torch.float)\n\npreds = []\nfor fold in range(1):\n    model = load_model(f\"best_epoch-{fold:02d}.bin\")\n    with torch.no_grad():\n        pred = model(imgs)\n        pred = (nn.Sigmoid()(pred)>0.5).double()\n    preds.append(pred)\n    \nimgs  = imgs.cpu().detach()\npreds = torch.mean(torch.stack(preds, dim=0), dim=0).cpu().detach()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Visualize prediction","metadata":{}},{"cell_type":"code","source":"def show_img(img, mask=None):\n    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n#     img = clahe.apply(img)\n#     plt.figure(figsize=(10,10))\n    plt.imshow(img, cmap='bone')\n    \ndef plot_batch(imgs, msks, size=3):\n    plt.figure(figsize=(5*5, 5))\n    for idx in range(size):\n        plt.subplot(1, 5, idx+1)\n        img = imgs[idx,].permute((1, 2, 0)).numpy()*255.0\n        img = img.astype('uint8')\n        msk = msks[idx,].permute((1, 2, 0)).numpy()*255.0\n        show_img(img, msk)\n    plt.tight_layout()\n    plt.show()  \n\nplot_batch(imgs, preds, size=5)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}