{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":" # <h1 style='background:#F7B2B0; border:0; color:black'><center>UNETR: Transformers for 3D Medical Image Segmentation</center></h1> ","metadata":{}},{"cell_type":"markdown","source":"This notebook is the first ever implementation of the paper in TensorFlow\n\nUNETR Paper : https://arxiv.org/pdf/2103.10504.pdf\n\nFully Convolutional neural networks have been used predominantly in medical image segmentation since the past decade. Despite their success, locality of convolutional layers limits their learning capacity of long- range spatial dependencies. Long-range spatial is the rate of decay of statistical dependence of two points with increasing time intervals or spatial distance between the points.UNETR uses a transformer as the encoder to learn sequence representations of the input volume and capture the global multi-scale information, following the “U-shaped” network design for the encoder and decoder. The transformer encoder is directly connected to a decoder via skip connections at different resolutions to compute the final semantic segmentation output.\n\nThe performance of UNETR has been evaluated on the Multi Atlas Labelling Beyond The Cranial Vault (BTCV) dataset for multiorgan segmentation and the Medical Segmentation Decathlon (MSD) dataset for brain tumour and spleen segmentation tasks. The benchmarks demonstrate new state-of-the-art performance on the BTCV leaderboard. \n\n### Introduction\n\n<div class=\"alert alert-block alert-info\">  \n\nFCNN and “U-shaped” encoder- decoder architecture have achieved state-of-the-art results in medical semantic segmentation tasks. However, their performance in learning long-range dependencies is limited to their localised receptive fields. This leads to sub-optimal segmentation of structures with variable shapes and scales (e.g. brain lesions with different sizes).\n\nFrom the success of transformers in NLP, we know that due to their flexible formulation, they can effectively learn long-range information. \nThe 3D image segmentation task has been reformulated as  a 1D sequence-to-sequence prediction problem and uses a transformer as the encoder to learn contextual information from the embedded input patches. The extracted representations from the transformer encoder are merged with the CNN-based decoder via skip connections at multiple resolutions to predict the segmentation outputs. Instead of using transformers in the decoder, UNETR uses a CNN-based decoder. \n\nThe effectiveness of UNETR  on 3D CT and MRI segmentation tasks has been validated using Beyond the Cranial Vault (BTCV) and Medical Segmentation Decathlon (MSD) datasets.\n\n</div>\n\n![](https://i.imgur.com/Pqmf8tN.png)\n\n### Related work\n\n**CNN-based Segmentation Networks :** \n CNN has achieved state-of-the-art results on various 2D and 3D medical image segmentation tasks. For volume-wise segmentation, tri-planar architectures are sometimes used to combine three-view slices for each voxel, also known for 2.5D methods. In contrast, 3D approaches directly utilize the full volumetric image represented by a sequence of 2D slices or modalities.  \n\nThese methods provide pioneer studies of 3D medical image segmentation at multiple levels, which reduces problems in spatial context and low-resolution condition. A limitation of these networks is their poor performance in learning global context and long-range spatial dependencies, which can severely impact the segmentation performance\n\n**Vision Transformers :**\n\nIn computer vision, they demonstrated state-of-the-art performance on image classification datasets by large-scale pre-training and fine-tuning of a pure transformer. Recently, hierarchical vision transformers with varying resolutions and spatial embeddings have been proposed. These methodologies gradually decrease the resolution of features in the transformer layers and utilise sub-sampled attention modules. \n","metadata":{}},{"cell_type":"code","source":"\nimport pandas as pd, numpy as np, random,os, shutil\nimport tensorflow as tf\nimport re\nimport math\nimport cv2\nimport matplotlib.pyplot as plt\nfrom glob import glob\nfrom kaggle_datasets import KaggleDatasets\n\nfrom PIL import Image\nfrom tqdm import tqdm\nimport wandb\nimport glob\n\nimport math\nimport tensorflow as tf\nimport tensorflow.keras.backend as k\n\nimport os\nfrom typing import List, Tuple\nfrom pathlib import Path\n\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom skimage import io\nfrom skimage.color import gray2rgb\nfrom skimage.transform import resize\nfrom rich.jupyter import print","metadata":{"id":"oY8_qJ3KdH4Z","executionInfo":{"status":"ok","timestamp":1655598847076,"user_tz":-330,"elapsed":2352,"user":{"displayName":"Tensor Girl","userId":"02972928350054045429"}},"execution":{"iopub.status.busy":"2022-06-21T12:22:45.083497Z","iopub.execute_input":"2022-06-21T12:22:45.084563Z","iopub.status.idle":"2022-06-21T12:22:46.934333Z","shell.execute_reply.started":"2022-06-21T12:22:45.084508Z","shell.execute_reply":"2022-06-21T12:22:46.933311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nwandb_key = user_secrets.get_secret(\"api_key\")\nwandb.login(key = wandb_key)","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **<span style=\"color:#F7B2B0;\">Data Preprocessing</span>**","metadata":{}},{"cell_type":"code","source":"# Code copied from https://www.kaggle.com/code/ayuraj/quick-data-eda-segmentation-viz-using-w-b\n\nROOT_DIR = '../input/uw-madison-gi-tract-image-segmentation/'\ndf = pd.read_csv(ROOT_DIR+'train.csv')\n# Remove rows with NaN Segmentation masks\ndf = df[df.segmentation.notna()].reset_index(drop=False)\ndef get_case_str(row):\n    case_num = row.id.split('_')[0]\n    return case_num\n\ndef get_case_id(row):\n    case_num = row.id.split('_')[0]\n    return int(case_num[4:])\n\ndf['case_str'] = df.apply(lambda row: get_case_str(row), axis=1)\ndf['case_id'] = df.apply(lambda row: get_case_id(row), axis=1)\n\ndef get_day_str(row):\n    return row.id.split('_')[1]\n\ndef get_day_id(row):\n    return int(row.id.split('_')[1][3:])\n\ndf['day_str'] = df.apply(lambda row: get_day_str(row), axis=1)\ndf['day_id'] = df.apply(lambda row: get_day_id(row), axis=1)\n\ndef get_slice_str(row):\n    slice_id = row.id.split('_')[-1]\n    return f'slice_{slice_id}'\n\ndf['slice_str'] = df.apply(lambda row: get_slice_str(row), axis=1)\nfilepaths = glob.glob(ROOT_DIR+'train/*/*/*/*')\n\n\nfile_df = pd.DataFrame(columns=['case_str', 'day_str', 'slice_str', 'filename', 'filepath'])\nfor idx, filepath in tqdm(enumerate(filepaths)):\n    case_day_str = filepath.split('/')[5]\n    case_str, day_str = case_day_str.split('_')\n\n    filename = filepath.split('/')[-1]\n    slice_id = filename.split('_')[1]\n    slice_str = f'slice_{slice_id}'\n    \n    file_df.loc[idx] = [case_str, day_str, slice_str, filename, filepath]\n\ndf = pd.merge(df, file_df, on=['case_str', 'day_str', 'slice_str'])\n\ndef get_image_height(row):\n    return int(row.filename[:-4].split('_')[2])\n    \ndef get_image_width(row):\n    return int(row.filename[:-4].split('_')[3])\n\ndef get_pixel_height(row):\n    return float(row.filename[:-4].split('_')[4])\n\ndef get_pixel_width(row):\n    return float(row.filename[:-4].split('_')[5])\n\ndf['img_height'] = df.apply(lambda row: get_image_height(row), axis=1)\ndf['img_width'] = df.apply(lambda row: get_image_width(row), axis=1)\ndf['pixel_height (mm)'] = df.apply(lambda row: get_pixel_height(row), axis=1)\ndf['pixel_width (mm)'] = df.apply(lambda row: get_pixel_width(row), axis=1)\n\ndf.drop('index', axis=1, inplace=True)\n\nby_case = df.groupby('case_str')\ncase_df = by_case.get_group('case123')\n\nby_day = case_df.groupby('day_str')\nday_df = by_day.get_group('day0')\n\nby_slice = day_df.groupby('slice_str')\nslice_df = by_slice.get_group('slice_0075')\n\n# saving the dataframe\ndf.to_csv('df.csv')\n# saving the dataframe\nslice_df.to_csv('slice_df.csv')","metadata":{"execution":{"iopub.status.busy":"2022-06-21T11:16:28.798695Z","iopub.execute_input":"2022-06-21T11:16:28.799051Z","iopub.status.idle":"2022-06-21T11:18:44.740879Z","shell.execute_reply.started":"2022-06-21T11:16:28.799015Z","shell.execute_reply":"2022-06-21T11:18:44.740012Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **<span style=\"color:#F7B2B0;\">W & B Artifacts</span>**\n\nAn artifact as a versioned folder of data.Entire datasets can be directly stored as artifacts .\n\nW&B Artifacts are used for dataset versioning, model versioning . They are also used for tracking dependencies and results across machine learning pipelines.Artifact references can be used to point to data in other systems like S3, GCP, or your own system.\n\nYou can learn more about W&B artifacts [here](https://docs.wandb.ai/guides/artifacts)\n\n![](https://drive.google.com/uc?id=1JYSaIMXuEVBheP15xxuaex-32yzxgglV)","metadata":{}},{"cell_type":"code","source":"# Save training data to W&B Artifacts\nrun = wandb.init(project='UW-Madison-Viz', name='processed_data') \nartifact = wandb.Artifact(name='processed_data',type='dataset')\nartifact.add_file(\"./df.csv\")\nartifact.add_file(\"./slice_df.csv\")\nwandb.log_artifact(artifact)\nwandb.finish()","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"![](https://i.imgur.com/cYkCWRg.png)","metadata":{}},{"cell_type":"markdown","source":"# **<span style=\"color:#F7B2B0;\">Visualizing Random Samples</span>**","metadata":{}},{"cell_type":"code","source":"# Code copied from https://www.kaggle.com/code/bsridatta/eda-for-a-healthy-gi-tract\n\n# load flattened metadata\ntrain_df = pd.read_csv(\"/kaggle/input/uwmadison-flattened-metadata/train_flat.csv\")\ntrain_path = Path(\"/kaggle/input/uw-madison-gi-tract-image-segmentation/train/\")\n\ndef get_image_array(file_paths: List[str], \n                    image_folder:str=\"/kaggle/input/uw-madison-gi-tract-image-segmentation/train/\", \n                    shape: Tuple=(300,300)\n                   ) -> np.array:\n    \"\"\"read all images as a single array for plotly.\"\"\"\n    read_img = lambda file: io.imread(f\"{image_folder}/{file}\")\n    images = []\n    empty = 0\n    for x in file_paths:\n        img = read_img(x)\n        images.append(resize(img, shape))\n        if not img.any():\n            empty += 1\n    # print(f\"{empty} images are blank\")\n    return np.asarray(images)\n\ndef parse_paths(ids: List[str], image_files: List[str]) -> List[str]:\n    paths = []\n    for id, file in zip(ids, image_files):\n        splits = id.split(\"_\")\n        paths.append(f\"{splits[0]}/{splits[0]}_{splits[1]}/scans/{file}\")\n    return paths\n\nsave_path = Path(\"/kaggle/working/\")\n\nsamples = train_df[['id', 'image_file']].sample(5, random_state=400)\nsample_paths = parse_paths(samples.id.values, samples.image_file.values)\nsample_images = get_image_array(sample_paths)\n\n# plot image grid\nfig = px.imshow(sample_images, facet_col=0, binary_string=True, facet_row_spacing=0.0, facet_col_spacing=0)\nfig.update_xaxes(showticklabels=False).update_yaxes(showticklabels=False)\n\n# add species name and individual_id\nfor i, a in enumerate(fig.layout.annotations):\n    a.text = samples.iloc[i].id\nfig.update_layout(title=f\"Random Samples\", autosize=True, hovermode=False, height=400, width=1200, margin=dict(l=40, r=0, t=140, b=80))\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-21T12:23:17.160351Z","iopub.execute_input":"2022-06-21T12:23:17.160742Z","iopub.status.idle":"2022-06-21T12:23:17.900143Z","shell.execute_reply.started":"2022-06-21T12:23:17.160711Z","shell.execute_reply":"2022-06-21T12:23:17.89914Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **<span style=\"color:#F7B2B0;\">Visualize Segmentation Masks using W&B</span>**\n","metadata":{}},{"cell_type":"code","source":"filepath = slice_df.filepath.values[0]\nimage = cv2.imread(filepath, cv2.IMREAD_UNCHANGED)\n\ndef rle2mask(rles, class_names, height, width, class_dict):\n    img = np.zeros(height*width, dtype=np.uint16)\n    for rle, class_name in zip(rles, class_names):\n        s = rle.split(' ')\n        starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n        starts -= 1\n        ends = starts + lengths\n        for lo, hi in zip(starts, ends):\n            img[lo:hi] = class_dict[class_name]\n        \n    mask = img.reshape((width, height))\n    return mask\n\nclass2id = {class_name: idx+1 for idx, class_name in enumerate(df['class'].unique())} # 0 is reserved for background\nid2class = {v:k for k, v in class2id.items()}\n\nmask = rle2mask(slice_df.segmentation.values,\n                slice_df['class'].values,\n                slice_df.img_height.values[0],\n                slice_df.img_width.values[0],\n                class2id)\n\n","metadata":{"execution":{"iopub.status.busy":"2022-06-21T11:18:55.558601Z","iopub.execute_input":"2022-06-21T11:18:55.558956Z","iopub.status.idle":"2022-06-21T11:18:55.599818Z","shell.execute_reply.started":"2022-06-21T11:18:55.558918Z","shell.execute_reply":"2022-06-21T11:18:55.599128Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 1. Generate a dict of mask data to log\nwandb_mask = {\n    'gt_mask':{\n        'mask_data': mask,\n        'class_labels': id2class\n    }\n}\n\nrun = wandb.init(project='UW-Madison-Viz')\nwandb.log({'Ground Truth Segmentation': wandb.Image(image, masks=wandb_mask)})\nwandb.finish()\nrun","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"![](https://i.imgur.com/IocKRfx.png)","metadata":{}},{"cell_type":"markdown","source":"# **<span style=\"color:#F7B2B0;\">Visualize a case day-wise using W&B Tables</span>**\n","metadata":{}},{"cell_type":"code","source":"# A dict with key id and name for logging segmentation mask as W&B Tables.\nwandb_class_set = wandb.Classes([{\n                     'id': id,\n                     'name': name\n                  } for id, name in id2class.items()])\n\nfor day, day_df in by_day:\n    print('The day the scan was taken: ', day)\n    \n    # 1. Initialize a W&B Run\n    run = wandb.init(project='UW-Madison-Viz', group='case123-viz')\n\n    # 2. Initialize a W&B Table\n    data_at = wandb.Table(columns=['slice', 'image'])\n    \n    # Group by slice\n    by_slice = day_df.groupby('slice_str')\n    \n    # Iterate through each slice, open the image, and get mask\n    for slice_num, slice_df in tqdm(by_slice):\n        # Open the image\n        filepath = slice_df.filepath.values[0]\n        image = cv2.imread(filepath, cv2.IMREAD_UNCHANGED)\n        image = tf.convert_to_tensor(image, dtype=tf.uint16)\n        image = tf.image.convert_image_dtype(image, dtype=tf.float16)\n        \n        # Get mask\n        mask = rle2mask(slice_df.segmentation.values,\n                slice_df['class'].values,\n                slice_df.img_height.values[0],\n                slice_df.img_width.values[0],\n                class2id)\n        \n        # 3. Generate a dict of mask data to log\n        wandb_mask = {\n            'gt_mask':{\n                'mask_data': mask,\n                'class_labels': id2class\n            }\n        }\n        \n        # 4. Add the data as a new row\n        data_at.add_data(\n            slice_num,\n            wandb.Image(image, masks=wandb_mask, classes=wandb_class_set)\n        )\n        \n    # 5. Log the table onto W&B dashboard\n    wandb.log({f'Segmentation Viz {day}': data_at})\n\n    # 6. Close the W&B run\n    wandb.finish()","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"![](https://i.imgur.com/GVsxPGK.png)","metadata":{}},{"cell_type":"code","source":"class CFG:\n\n    debug = False   \n   \n    verbose = 0\n    display_plot = True\n\n    # Device for training\n    device = None  # device is automatically selected\n\n    # Seeding for reproducibility\n    seed = 101\n\n    # Image Size\n    img_size = [96, 96]\n\n    # Batch Size & Epochs\n    batch_size = 2\n    drop_remainder = False\n    epochs = 15\n    steps_per_execution = None\n\n    # Loss & Optimizer & LR Scheduler\n    loss = \"dice_loss\"\n    optimizer = \"Adam\"\n    lr = 5e-4\n    lr_schedule = \"CosineDecay\"\n    patience = 5\n   \n    # Clip values to [0, 1]\n    clip = False\n    \nAUTO = tf.data.experimental.AUTOTUNE\n\nimport re\ndef count_data_items(filenames):\n    n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) for filename in filenames]\n    return np.sum(n)\n","metadata":{"id":"KnjDZfbpGGBe","executionInfo":{"status":"ok","timestamp":1655598847076,"user_tz":-330,"elapsed":3,"user":{"displayName":"Tensor Girl","userId":"02972928350054045429"}},"execution":{"iopub.status.busy":"2022-06-21T11:19:58.995179Z","iopub.execute_input":"2022-06-21T11:19:58.995555Z","iopub.status.idle":"2022-06-21T11:19:59.003294Z","shell.execute_reply.started":"2022-06-21T11:19:58.995515Z","shell.execute_reply":"2022-06-21T11:19:59.002437Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BASE_PATH = '/kaggle/input/uw-madison-gi-tract-image-segmentation'\nGCS_PATH = KaggleDatasets().get_gcs_path('uwmgi-25d-tfrecord-dataset')\nALL_FILENAMES = tf.io.gfile.glob(GCS_PATH + '/uwmgi/*.tfrec')\nprint('NUM TFRECORD FILES: {:,}'.format(len(ALL_FILENAMES)))\nprint('NUM TRAINING IMAGES: {:,}'.format(count_data_items(ALL_FILENAMES)))","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **<span style=\"color:#F7B2B0;\">Input Data Pipeline</span>**","metadata":{}},{"cell_type":"code","source":"# Decode image from bytestring to tensor\ndef decode_image(data, height, width, target_size=CFG.img_size):\n    img = tf.io.decode_raw(data, out_type=tf.uint16)\n    img = tf.reshape(img, [height, width, 3])  # explicit size needed for TPU\n    img = tf.cast(img, tf.float32)\n    img = tf.math.divide_no_nan(img, tf.math.reduce_max(img))  # scale image to [0, 1]\n    img = tf.image.resize_with_pad(\n        img, target_size[0], target_size[1], method=\"nearest\"\n    )  # resize with pad to avoid distortion\n    img = tf.reshape(img, [*target_size, 3])  # reshape after resize\n    return img\n\n\n# Decode mask from bytestring to tensor\ndef decode_mask(data, height, width, target_size=CFG.img_size):\n    msk = tf.io.decode_raw(data, out_type=tf.uint8)\n    msk = tf.reshape(msk, [height, width, 3])  # explicit size needed for TPU\n    msk = tf.cast(msk, tf.float32)\n    msk = msk / 255.0  # scale mask data to[0, 1]\n    msk = tf.image.resize_with_pad(\n        msk, target_size[0], target_size[1], method=\"nearest\"\n    )\n    msk = tf.reshape(msk, [*target_size, 3])  # reshape after resize\n    return msk\n\n\n# Read tfrecord data & parse it & do augmentation\ndef read_tfrecord(example, augment=True, return_id=False, dim=CFG.img_size):\n    tfrec_format = {\n        \"id\": tf.io.FixedLenFeature([], tf.string),\n        \"image\": tf.io.FixedLenFeature([], tf.string),  # tf.string means bytestring\n        \"height\": tf.io.FixedLenFeature([], tf.int64),\n        \"width\": tf.io.FixedLenFeature([], tf.int64),\n        \"mask\": tf.io.FixedLenFeature([], tf.string),\n    }\n    example = tf.io.parse_single_example(\n        example, tfrec_format\n    )  # parses a single example proto.\n    image_id = example[\"id\"]\n    height = example[\"height\"]\n    width = example[\"width\"]\n    img = decode_image(example[\"image\"], height, width, dim)  # access image\n    msk = decode_mask(example[\"mask\"], height, width, dim)  # access mask\n    img = tf.reshape(img, [*dim, 3])\n    msk = tf.reshape(msk, [*dim, 3])\n    img = tf.repeat(img[:, :, np.newaxis,:], 96, axis=2)\n    msk = tf.repeat(msk[:, :, np.newaxis,:], 96, axis=2)\n    return (img, msk) if not return_id else (img, image_id, msk)\n\ndef get_dataset(\n    filenames,\n    shuffle=False,\n    repeat=False,\n    augment=False,\n    cache=False,\n    return_id=False,\n    batch_size=CFG.batch_size ,\n    target_size=CFG.img_size,\n    drop_remainder=False,\n    seed=CFG.seed,\n):\n    dataset = tf.data.TFRecordDataset(filenames)  # read tfrecord files\n    dataset = dataset.map(\n        lambda x: read_tfrecord(\n            x,\n            augment=augment,  # unparse tfrecord data with masks\n            return_id=return_id,\n            dim=target_size,\n        ))\n    dataset = dataset.batch(batch_size, drop_remainder=drop_remainder)  # batch the data\n    dataset = dataset.prefetch(AUTO)  # prefetch data for speedup\n    return dataset\n\nds = get_dataset(ALL_FILENAMES, augment=False, cache=False, repeat=False)","metadata":{"id":"pZ9HFk1fH0z9","executionInfo":{"status":"ok","timestamp":1655598847747,"user_tz":-330,"elapsed":2,"user":{"displayName":"Tensor Girl","userId":"02972928350054045429"}},"execution":{"iopub.status.busy":"2022-06-21T11:20:00.222431Z","iopub.execute_input":"2022-06-21T11:20:00.222729Z","iopub.status.idle":"2022-06-21T11:20:00.241258Z","shell.execute_reply.started":"2022-06-21T11:20:00.222702Z","shell.execute_reply":"2022-06-21T11:20:00.240223Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **<span style=\"color:#F7B2B0;\">Model Architecture</span>**","metadata":{}},{"cell_type":"markdown","source":"\n![](https://i.imgur.com/eUyPwD0.png)\n\n<div class=\"alert alert-block alert-info\">  \n\nUNETR uses a contracting-expanding pattern consisting of a stack of transformers as the encoder which is connected to a decoder via skip connections. 1D sequence of a 3D input volume x ∈ R^(H×W×D×C) is created with resolution (H,W,D) and C input channels and divide it into flattened uniform non-overlapping patches xv ∈R^(N×(P^3 .C)) where (P, P, P) denotes the resolution of each patch and N = (H×W ×D)/P^3 is the length of the sequence.Then the patches are projected into a K dimensional embedding space using a linear layer and add 1D positional embedding to it. After embeddings a stack of transformer blocks consisting of multi-head self-attention (MSA) and multilayer perceptron (MLP) sublayers are used.\n    \n</div>\n\n# **<span style=\"color:#F7B2B0;\">CNN Blocks</span>**\n","metadata":{}},{"cell_type":"code","source":"class SingleDeconv3DBlock(tf.keras.layers.Layer):\n\n    def __init__(self,filters):\n        super(SingleDeconv3DBlock, self).__init__()\n        self.block = tf.keras.layers.Conv3DTranspose(filters= filters, \n                                                     kernel_size=2, strides=2, \n                                                     padding=\"valid\", \n                                                     output_padding=None)\n                                                     \n\n    def call(self, inputs):        \n        return self.block(inputs)\n\n\n\nclass SingleConv3DBlock(tf.keras.layers.Layer):\n\n    def __init__(self, filters,kernel_size):\n        super(SingleConv3DBlock, self).__init__()\n        self.kernel=kernel_size\n        self.res = tuple(map(lambda i: (i - 1)//2, self.kernel))\n        self.block = tf.keras.layers.Conv3D(filters= filters, \n                                            kernel_size=kernel_size, \n                                            strides=1, \n                                            padding='same')\n\n    def call(self, inputs):\n        return self.block(inputs)\n    \nclass Conv3DBlock(tf.keras.layers.Layer):\n\n    def __init__(self, filters,kernel_size=(3,3,3)):\n        super(Conv3DBlock, self).__init__()\n        self.a= tf.keras.Sequential([\n                                     SingleConv3DBlock(filters,kernel_size=kernel_size),\n                                     tf.keras.layers.BatchNormalization(),\n                                     tf.keras.layers.Activation('relu')\n        ])\n        \n\n    def call(self, inputs):\n        return self.a(inputs)\n    \nclass Deconv3DBlock(tf.keras.layers.Layer):\n\n    def __init__(self, filters,kernel_size=(3,3,3)):\n        super(Deconv3DBlock, self).__init__()\n        self.a= tf.keras.Sequential([\n                                     SingleDeconv3DBlock(filters=filters),\n                                     SingleConv3DBlock(filters=filters,kernel_size=kernel_size),\n                                     tf.keras.layers.BatchNormalization(),\n                                     tf.keras.layers.Activation('relu')\n        ])\n  \n    def call(self, inputs):\n        return self.a(inputs)\n    \n","metadata":{"id":"gLuldKPYCNJp","executionInfo":{"status":"ok","timestamp":1655598852877,"user_tz":-330,"elapsed":4,"user":{"displayName":"Tensor Girl","userId":"02972928350054045429"}},"execution":{"iopub.status.busy":"2022-06-21T11:20:00.823091Z","iopub.execute_input":"2022-06-21T11:20:00.823555Z","iopub.status.idle":"2022-06-21T11:20:01.715988Z","shell.execute_reply.started":"2022-06-21T11:20:00.823519Z","shell.execute_reply":"2022-06-21T11:20:01.715151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\">  \n\nMLP comprises two linear layers with GELU activation functions, i is the intermediate block identifier, and L is the number of transformer layers.A MSA sublayer comprises parallel self-attention (SA) heads. The SA block is a parameterized function that learns the mapping between a query (q) and the corresponding key (k) and value (v) representations in a sequence. The attention weights (A) are computed by measuring the similarity between two elements in z and their key-value pairs using softmax function. \n    \n</div>\n\n\n# **<span style=\"color:#F7B2B0;\">Self Attention and MLP</span>**","metadata":{}},{"cell_type":"code","source":"class SelfAttention(tf.keras.layers.Layer):\n\n    def __init__(self, num_heads,embed_dim,dropout):\n        super(SelfAttention, self).__init__()\n\n        self.num_attention_heads = num_heads\n        self.attention_head_size = int(embed_dim / num_heads)\n        self.all_head_size = self.num_attention_heads * self.attention_head_size\n\n        self.query=tf.keras.layers.Dense(self.all_head_size)\n        self.key = tf.keras.layers.Dense(self.all_head_size)\n        self.value = tf.keras.layers.Dense(self.all_head_size)                \n\n        self.out=tf.keras.layers.Dense(embed_dim)\n        self.attn_dropout=tf.keras.layers.Dropout(dropout)\n        self.proj_dropout=tf.keras.layers.Dropout(dropout)\n\n        self.softmax=tf.keras.layers.Softmax()\n\n        self.vis=False\n\n    def transpose_for_scores(self,x):\n        new_x_shape=list(x.shape[:-1] + (self.num_attention_heads, self.attention_head_size))\n        new_x_shape[0] = -1\n        y = tf.reshape(x, new_x_shape)\n        return tf.transpose(y,perm=[0,2,1,3])\n\n    def call(self, hidden_states):\n        mixed_query_layer = self.query(hidden_states)\n        mixed_key_layer = self.key(hidden_states)\n        mixed_value_layer = self.value(hidden_states)\n\n        query_layer = self.transpose_for_scores(mixed_query_layer)\n        key_layer = self.transpose_for_scores(mixed_key_layer)\n        value_layer = self.transpose_for_scores(mixed_value_layer)  \n        attention_scores= query_layer @ tf.transpose(key_layer,perm=[0,1,3,2])\n        attention_scores= attention_scores/math.sqrt(self.attention_head_size)\n        attention_probs=self.softmax(attention_scores)\n        weights = attention_probs if self.vis else None\n        attention_probs = self.attn_dropout(attention_probs)\n\n        context_layer= attention_probs @ value_layer\n        context_layer=tf.transpose( context_layer, perm=[0,2,1,3])\n        new_context_layer_shape = list(context_layer.shape[:-2] + (self.all_head_size,))\n        new_context_layer_shape[0]= -1\n        context_layer = tf.reshape(context_layer,new_context_layer_shape)\n        attention_output = self.out(context_layer)\n        attention_output = self.proj_dropout(attention_output)\n        return attention_output, weights        ","metadata":{"id":"R33LarglnuAZ","executionInfo":{"status":"ok","timestamp":1655598852877,"user_tz":-330,"elapsed":4,"user":{"displayName":"Tensor Girl","userId":"02972928350054045429"}},"execution":{"iopub.status.busy":"2022-06-21T11:20:01.717573Z","iopub.execute_input":"2022-06-21T11:20:01.717933Z","iopub.status.idle":"2022-06-21T11:20:01.733436Z","shell.execute_reply.started":"2022-06-21T11:20:01.717898Z","shell.execute_reply":"2022-06-21T11:20:01.731069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Mlp(tf.keras.layers.Layer):\n\n    def __init__(self, output_features, drop=0.):\n        super(Mlp, self).__init__()\n        self.a=tf.keras.layers.Dense(units=output_features,activation=tf.nn.gelu)\n        self.b=tf.keras.layers.Dropout(drop)\n\n    def call(self, inputs):\n        x=self.a(inputs)\n        return self.b(x)\n\nclass PositionwiseFeedForward(tf.keras.layers.Layer):\n\n    def __init__(self, d_model=768,d_ff=2048, dropout=0.1):\n        super(PositionwiseFeedForward, self).__init__()\n        self.a=tf.keras.layers.Dense(units=d_ff)\n        self.b=tf.keras.layers.Dense(units=d_model)\n        self.c=tf.keras.layers.Dropout(dropout)\n\n    def call(self, inputs):\n        return self.b(self.c(tf.nn.relu(self.a(inputs))))\n\n##embeddings, projection_dim=embed_dim\nclass PatchEmbedding(tf.keras.layers.Layer): \n  def __init__(self ,  cube_size, patch_size , embed_dim):\n        super(PatchEmbedding, self).__init__()\n        self.num_of_patches=int((cube_size[0]*cube_size[1]*cube_size[2])/(patch_size*patch_size*patch_size))\n        self.patch_size=patch_size\n        self.size = patch_size\n        self.embed_dim = embed_dim\n\n        self.projection = tf.keras.layers.Dense(embed_dim)\n\n        self.clsToken = tf.Variable(tf.keras.initializers.GlorotNormal()(shape=(1 , 512 , embed_dim)) , trainable=True)\n\n        self.positionalEmbedding = tf.keras.layers.Embedding(self.num_of_patches , embed_dim)\n        self.patches=None\n        self.lyer = tf.keras.layers.Conv3D(filters= self.embed_dim,kernel_size=self.patch_size, strides=self.patch_size,padding='valid')\n        #embedding - basically is adding numerical embedding to the layer along with an extra dim  \n      \n  def call(self , inputs):\n        patches =self.lyer(inputs)\n        patches = tf.reshape(patches , (tf.shape(inputs)[0] , -1 , self.size * self.size * 3))\n        patches = self.projection(patches)\n        positions = tf.range(0 , self.num_of_patches , 1)[tf.newaxis , ...]\n        positionalEmbedding = self.positionalEmbedding(positions)\n        patches = patches + positionalEmbedding\n\n        return patches, positionalEmbedding","metadata":{"id":"orvzy6sXuxA3","executionInfo":{"status":"ok","timestamp":1655598852878,"user_tz":-330,"elapsed":4,"user":{"displayName":"Tensor Girl","userId":"02972928350054045429"}},"execution":{"iopub.status.busy":"2022-06-21T11:20:01.735891Z","iopub.execute_input":"2022-06-21T11:20:01.736242Z","iopub.status.idle":"2022-06-21T11:20:01.762515Z","shell.execute_reply.started":"2022-06-21T11:20:01.736207Z","shell.execute_reply":"2022-06-21T11:20:01.761467Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\">  \n\nA sequence representation zi (i ∈ {3,6,9,12}) is extracted with size H×W×D /P^3 ×K, from the transformer and reshaped into a (H/P) × (W/P) × (D/P) ×K tensor. At the bottleneck of the encoder (i.e. output of the transformer's last layer), a deconvolutional layer is applied to the transformed feature map to increase its resolution by a factor of 2. The resized feature map is then concatenated with the feature map of the previous transformer output (e.g. z9), and fed into consecutive 3 × 3 × 3 convolutional layers and the output is upsampled using a deconvolutional layer. This process is repeated for all the other subsequent layers up to the original input resolution where the final output is fed into a 1×1×1 convolutional layer with a softmax activation function to generate voxel-wise semantic predictions.\n    \n</div>\n\n# **<span style=\"color:#F7B2B0;\">Transformer Block</span>**\n","metadata":{"id":"mznQvGsEOJK5"}},{"cell_type":"code","source":"##transformerblock\nclass TransformerLayer(tf.keras.layers.Layer):\n    def __init__(self ,  embed_dim, num_heads ,dropout, cube_size, patch_size):\n      super(TransformerLayer,self).__init__()\n\n      self.attention_norm = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n\n      self.mlp_norm = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n\n#embed_dim/no-of_heads\n      self.mlp_dim = int((cube_size[0] * cube_size[1] * cube_size[2]) / (patch_size * patch_size * patch_size))\n      \n      self.mlp = PositionwiseFeedForward(embed_dim,2048)\n      self.attn = SelfAttention(num_heads, embed_dim, dropout)\n    \n    def call(self ,x  , training=True):\n      h=x\n      x=self.attention_norm(x)\n      x,weights= self.attn(x)\n      x=x+h\n      h=x\n\n      x = self.mlp_norm(x)\n      x = self.mlp(x)\n\n      x = x + h\n\n      return x, weights\n\n\nclass TransformerEncoder(tf.keras.layers.Layer):\n  def __init__(self ,embed_dim , num_heads,cube_size, patch_size , num_layers=12 , dropout=0.1,extract_layers=[3,6,9,12]):\n    super(TransformerEncoder,self).__init__()\n#  embed_dim, num_heads ,dropout, cube_size, patch_size\n    self.embeddings = PatchEmbedding(cube_size,patch_size, embed_dim)\n    self.extract_layers =extract_layers\n    self.encoders = [TransformerLayer(embed_dim, num_heads,dropout, cube_size, patch_size) for _ in range(num_layers)]\n  \n  def call(self , inputs , training=True):\n    extract_layers = []\n    x = inputs\n    x,_=self.embeddings(x)\n    \n    for depth,layer in enumerate(self.encoders):\n      x,_= layer(x , training=training)\n      if depth + 1 in self.extract_layers:\n                extract_layers.append(x)\n    \n    return extract_layers","metadata":{"id":"Wyv90IvrTM7r","executionInfo":{"status":"ok","timestamp":1655598853319,"user_tz":-330,"elapsed":3,"user":{"displayName":"Tensor Girl","userId":"02972928350054045429"}},"execution":{"iopub.status.busy":"2022-06-21T11:20:01.763972Z","iopub.execute_input":"2022-06-21T11:20:01.764899Z","iopub.status.idle":"2022-06-21T11:20:01.7826Z","shell.execute_reply.started":"2022-06-21T11:20:01.764856Z","shell.execute_reply":"2022-06-21T11:20:01.781162Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class UNETR(tf.keras.Model):\n    def __init__(self, img_shape=(96,96, 96), input_dim=3, output_dim=3, embed_dim=768, patch_size=16, num_heads=12, dropout=0.1):\n        super(UNETR,self).__init__()\n        self.input_dim = input_dim\n        self.output_dim = output_dim\n        self.embed_dim = embed_dim\n        self.img_shape = img_shape\n        self.patch_size = patch_size\n        self.num_heads = num_heads\n        self.dropout = dropout\n        self.num_layers = 12\n        self.ext_layers = [3, 6, 9, 12]\n        \n        self.patch_dim = [int(x / patch_size) for x in img_shape]\n        self.transformer = \\\n            TransformerEncoder(\n                self.embed_dim,\n                self.num_heads,\n                self.img_shape,\n                self.patch_size,\n                self.num_layers,\n                self.dropout,\n                self.ext_layers\n            )\n        \n        # U-Net Decoder\n        self.decoder0 = \\\n            tf.keras.Sequential([\n                Conv3DBlock(32, (3,3,3)),\n                Conv3DBlock(64, (3,3,3))]\n            )\n      \n        self.decoder3 = \\\n            tf.keras.Sequential([\n                Deconv3DBlock(512),\n                Deconv3DBlock(256),\n                Deconv3DBlock(128)]\n            )\n   \n        self.decoder6 = \\\n            tf.keras.Sequential([\n                Deconv3DBlock(512),\n                Deconv3DBlock(256)]\n            )\n    \n        self.decoder9 = \\\n            Deconv3DBlock(512)\n\n        self.decoder12_upsampler = \\\n            SingleDeconv3DBlock(512)\n\n        self.decoder9_upsampler = \\\n            tf.keras.Sequential([\n                Conv3DBlock(512),\n                Conv3DBlock(512),\n                Conv3DBlock(512),\n                SingleDeconv3DBlock(256)]\n            )\n\n        self.decoder6_upsampler = \\\n            tf.keras.Sequential([\n                Conv3DBlock(256),\n                Conv3DBlock(256),\n                SingleDeconv3DBlock(128)]\n            )\n\n        self.decoder3_upsampler = \\\n            tf.keras.Sequential(\n                [Conv3DBlock(128),\n                Conv3DBlock(128),\n                SingleDeconv3DBlock(64)]\n            )\n\n        self.decoder0_header = \\\n            tf.keras.Sequential(\n                [Conv3DBlock(64),\n                Conv3DBlock(64),\n                SingleConv3DBlock(output_dim, (1,1,1))]\n            ) \n\n \n    def call(self, x):\n        z = self.transformer(x)\n        z0, z3, z6, z9, z12 = x, z[0],z[1],z[2],z[3]\n        z3 = tf.reshape(tf.transpose(z3,perm=[0,2,1]),[-1,  *self.patch_dim,self.embed_dim])\n        z6 = tf.reshape(tf.transpose(z6,perm=[0,2,1]),[-1,  *self.patch_dim,self.embed_dim])\n        z9 = tf.reshape(tf.transpose(z9,perm=[0,2,1]),[-1,  *self.patch_dim,self.embed_dim])\n        z12 = tf.reshape(tf.transpose(z12,perm=[0,2,1]),[-1,  *self.patch_dim,self.embed_dim])\n        z12 = self.decoder12_upsampler(z12)\n        z9 = self.decoder9(z9)\n        z9 = self.decoder9_upsampler(tf.concat([z9, z12], 4))\n        z6 = self.decoder6(z6)\n        z6 = self.decoder6_upsampler(tf.concat([z6, z9], 4))\n        z3 = self.decoder3(z3)\n        z3 = self.decoder3_upsampler(tf.concat([z3, z6], 4))\n        z0 = self.decoder0(z0)\n        output = self.decoder0_header(tf.concat([z0, z3], 4))\n        return output\n        ","metadata":{"id":"mvULdDmYdy_I","executionInfo":{"status":"ok","timestamp":1655598853319,"user_tz":-330,"elapsed":3,"user":{"displayName":"Tensor Girl","userId":"02972928350054045429"}},"execution":{"iopub.status.busy":"2022-06-21T11:20:01.784778Z","iopub.execute_input":"2022-06-21T11:20:01.785319Z","iopub.status.idle":"2022-06-21T11:20:01.81279Z","shell.execute_reply.started":"2022-06-21T11:20:01.785276Z","shell.execute_reply":"2022-06-21T11:20:01.811845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **<span style=\"color:#F7B2B0;\">Loss Functions</span>**","metadata":{}},{"cell_type":"code","source":"def tversky(y_true, y_pred, axis=(0, 1, 2), alpha=0.3, beta=0.7, smooth=0.0001):\n    \"Tversky metric\"\n    y_true = tf.cast(y_true, tf.float32)\n    y_pred = tf.cast(y_pred, tf.float32)\n    tp = tf.math.reduce_sum(y_true * y_pred, axis=axis) # calculate True Positive\n    fn = tf.math.reduce_sum(y_true * (1 - y_pred), axis=axis) # calculate False Negative\n    fp = tf.math.reduce_sum((1 - y_true) * y_pred, axis=axis) # calculate False Positive\n    tv = (tp + smooth) / (tp + alpha * fn + beta * fp + smooth) # calculate tversky\n    tv = tf.math.reduce_mean(tv)\n    return tv\ndef focal_tversky_loss(y_true, y_pred, gamma=0.75):\n    \"Focal Tversky Loss: Focal Loss + Tversky Loss\"\n    tv = tversky(y_true, y_pred)\n    return k.pow((1 - tv), gamma)\n","metadata":{"id":"yV9fyOHfAd7O","executionInfo":{"status":"ok","timestamp":1655599434924,"user_tz":-330,"elapsed":531215,"user":{"displayName":"Tensor Girl","userId":"02972928350054045429"}},"outputId":"57045962-2900-4208-a432-513296dda524","execution":{"iopub.status.busy":"2022-06-21T11:20:01.814395Z","iopub.execute_input":"2022-06-21T11:20:01.815367Z","iopub.status.idle":"2022-06-21T11:20:01.82765Z","shell.execute_reply.started":"2022-06-21T11:20:01.815206Z","shell.execute_reply":"2022-06-21T11:20:01.82676Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"custom_objs = {\n    \"tversky\": tversky,\n    \"focal_tversky_loss\": focal_tversky_loss,\n}\ntf.keras.utils.get_custom_objects().update(custom_objs)","metadata":{"execution":{"iopub.status.busy":"2022-06-21T11:20:01.829925Z","iopub.execute_input":"2022-06-21T11:20:01.830823Z","iopub.status.idle":"2022-06-21T11:20:01.841547Z","shell.execute_reply.started":"2022-06-21T11:20:01.830781Z","shell.execute_reply":"2022-06-21T11:20:01.840759Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **<span style=\"color:#F7B2B0;\">Model Summary</span>**","metadata":{}},{"cell_type":"code","source":"model = tf.keras.models.load_model('../input/unetr-model/unetr')\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2022-06-21T11:20:01.842837Z","iopub.execute_input":"2022-06-21T11:20:01.844585Z","iopub.status.idle":"2022-06-21T11:20:37.023406Z","shell.execute_reply.started":"2022-06-21T11:20:01.844551Z","shell.execute_reply":"2022-06-21T11:20:37.022572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **<span style=\"color:#F7B2B0;\">Making Predictions</span>**","metadata":{}},{"cell_type":"code","source":"pred = model.predict(ds.skip(200).take(1))\n","metadata":{"id":"dRwY76d6k3aK","executionInfo":{"status":"ok","timestamp":1655600158095,"user_tz":-330,"elapsed":15744,"user":{"displayName":"Tensor Girl","userId":"02972928350054045429"}},"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **<span style=\"color:#F7B2B0;\">Saving the trained model using W&B artifacts</span>**\n","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# Save the trained model to W&B Artifacts\nmodel.save(\"./wandb_model\")\nrun = wandb.init(project='UW-Madison-Viz', name='UNETR_model') \ntrainedmodel_artifact = wandb.Artifact(name='UNETR_model',type='model')\ntrainedmodel_artifact.add_dir(\"./wandb_model\")\nwandb.log_artifact(trainedmodel_artifact)\nwandb.finish()","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"![](https://i.imgur.com/WjhawzU.png)","metadata":{}},{"cell_type":"markdown","source":"UNETR achieves new state-of-the-art performance in both Standard and Free Competitions on the BTCV leaderboard for the multi-organ segmentation and outperforms competing approaches for brain tumour and spleen segmentation on the MSD dataset. This model can be the foundation for a new class of transformer-based segmentation models in medical images analysis. \n\n## **<span style=\"color:#F7B2B0;\">Acknowledgements</span>**\n\nGoogle supported this work by providing Google Cloud Credits\n\n## **<span style=\"color:#F7B2B0;\">References</span>**\n\n\nhttps://arxiv.org/pdf/2103.10504.pdf\n\nhttps://github.com/tamasino52/UNETR (Pytorch)\n\nhttps://github.com/Project-MONAI/tutorials/blob/main/3d_segmentation/unetr_btcv_segmentation_3d_lightning.ipynb (Pytorch)\n\nhttps://www.kaggle.com/code/ayuraj/quick-data-eda-segmentation-viz-using-w-b\n\nhttps://www.kaggle.com/code/awsaf49/uwmgi-transunet-2-5d-train-tf\n\nhttps://www.kaggle.com/datasets/awsaf49/uwmgi-25d-tfrecord-dataset\n\nhttps://www.kaggle.com/code/bsridatta/eda-for-a-healthy-gi-tract\n\nhttps://www.kaggle.com/datasets/bsridatta/uwmadison-flattened-metadata\n","metadata":{}}]}