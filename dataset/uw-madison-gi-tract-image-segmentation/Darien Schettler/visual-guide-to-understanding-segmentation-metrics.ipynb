{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<br>\n\n## Visual Exploration of Semantic Segmentation Metrics\n\n---\n\n#### <b><font color=\"red\">This is a work in progress.<br><br>I plan to finish it by the weekend.<br><br>I'm making it public while I work on it though.<br><br>Please bear with me while I finish it out. Thanks!</font></b>","metadata":{}},{"cell_type":"markdown","source":"<br>\n\n#### **IMPORTS**\n\n---","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport cv2\nimport os","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-04-22T12:36:06.472098Z","iopub.execute_input":"2022-04-22T12:36:06.473133Z","iopub.status.idle":"2022-04-22T12:36:06.714484Z","shell.execute_reply.started":"2022-04-22T12:36:06.472917Z","shell.execute_reply":"2022-04-22T12:36:06.713459Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>\n\n#### **DEFINE OUR INITIAL GROUND TRUTH MASK AND PREDICTION MASK**\n\n---","metadata":{}},{"cell_type":"code","source":"DEMO_SHAPE = (10,10)\n\nground_truth_mask = np.zeros(DEMO_SHAPE, dtype=np.float32)\nfor i in range(10): ground_truth_mask[i:i+2, i:i+2] = 1.0\nfor i in range(4,7): ground_truth_mask[i, i:i+1] = 0.0\nground_truth_rgb_mask = np.stack([ground_truth_mask, ground_truth_mask[::-1], np.roll(ground_truth_mask, shift=3)], axis=-1)\n\nrandom_pred_mask = np.random.randint(low=0, high=2, size=DEMO_SHAPE).astype(np.float32)\nrandom_pred_rgb_mask = np.random.randint(low=0, high=2, size=(*DEMO_SHAPE, 3)).astype(np.float32)","metadata":{"execution":{"iopub.status.busy":"2022-04-22T13:15:24.880282Z","iopub.execute_input":"2022-04-22T13:15:24.880569Z","iopub.status.idle":"2022-04-22T13:15:24.88782Z","shell.execute_reply.started":"2022-04-22T13:15:24.880538Z","shell.execute_reply":"2022-04-22T13:15:24.886885Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>\n\n#### **HELPER FUNCTIONS FOR PLOTTING**\n\n---\n\nWe then use these helpers to plot our ground truth mask and randomly generated prediction mask","metadata":{}},{"cell_type":"code","source":"def grid_imshow(mask):\n    ax = plt.gca()\n    plt.imshow(mask, cmap=\"gray\" if len(mask.shape)==2 else None, interpolation=\"none\", vmin=0, vmax=1, aspect=\"equal\")\n\n    # Major ticks\n    ax.set_xticks(np.arange(0, DEMO_SHAPE[1], 1))\n    ax.set_yticks(np.arange(0, DEMO_SHAPE[0], 1))\n\n    # Labels for major ticks\n    ax.set_xticklabels(np.arange(1, DEMO_SHAPE[1]+1, 1))\n    ax.set_yticklabels(np.arange(1, DEMO_SHAPE[0]+1, 1))\n\n    # Minor ticks\n    ax.set_xticks(np.arange(-.5, DEMO_SHAPE[1], 1), minor=True)\n    ax.set_yticks(np.arange(-.5, DEMO_SHAPE[0], 1), minor=True)\n\n    # Gridlines based on minor ticks\n    ax.grid(which='minor', color='gray', linestyle='-', linewidth=2)\n    \ndef compare_masks(gt_mask, pred_mask, gt_title=\"Ground Truth Mask\", pred_title=\"Prediction Mask\", _figshape=(20,10)):\n    plt.figure(figsize=_figshape)\n\n    plt.subplot(1,2,1)    \n    grid_imshow(gt_mask)\n    plt.title(gt_title, fontweight=\"bold\")\n    \n    plt.subplot(1,2,2)\n    grid_imshow(pred_mask)\n    plt.title(pred_title, fontweight=\"bold\")\n    \n    plt.tight_layout()\n    plt.show()\n    \ncompare_masks(ground_truth_mask, random_pred_mask)\ncompare_masks(ground_truth_rgb_mask, random_pred_rgb_mask, gt_title=\"Ground Truth RGB Mask\", pred_title=\"Prediction RGB Mask\")","metadata":{"execution":{"iopub.status.busy":"2022-04-22T13:15:28.59286Z","iopub.execute_input":"2022-04-22T13:15:28.593134Z","iopub.status.idle":"2022-04-22T13:15:29.789486Z","shell.execute_reply.started":"2022-04-22T13:15:28.593106Z","shell.execute_reply":"2022-04-22T13:15:29.787942Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>\n\n#### **WHAT IS SEMANTIC SEGMENTATION AND WHAT DOES IT HAVE TO DO WITH THESE GRIDS**\n\n---\n\nIn simplest terms, **semantic segmentation is simply per-pixel classification.** This pex-pixel classification can take similar forms to what you've seen/heard about in regular machine learning classification problems\n* Binary Semantic Segmentation ***(Binary Classification)***\n* Multi-Class Semantic Segmentation ***(Multiclass Classification)***\n* Multi-Label Semantic Segmentation ***(Multilabel Classification)***\n\n---\n\nKnowing this, we will simply take this per pixel definition to the visual first principles, and investigate a case of **binary semantic segmentation** where we are trying to predict whether a grid-tile (read pixel) is white or black.\n* This is a much simpler version of an analagous task like predicting which pixels contain cat in a given image\n\n---\n\nWe will follow that investigation up with a second example where we will perform **multilabel** semantic segmentation on our 10x10 grid. We will predict the RGB binary values in an attempt to match a ground truth RGB image.\n* This is simply a much simpler version of the analagous task found in this UWM GI Tract Image Segmentation Competition\n","metadata":{}},{"cell_type":"markdown","source":"<br>\n\n#### **LET'S LOOK AT ACCURACY! – BINARY SEGMENTATION VERSION**\n\n---\n\nWell... before we dive into the literature. Let's think about things. If we completely disregard the idea of segmentation as a whole and look at what we are really doing – **per-pixel binary classification** – it's not absurd to think that we could simply calculate the **per-pixel binary accuracy**.\n* This is kind of like treating each pixel as a single example and then just averaging over the shape of the image\n\n---\n\n<br>\n\n**I will illustrate this in the cell below in an inefficient, but verbose way to demonstrate what is happening**\n* Note that because the prediction mask is random you will have some variability in accuracy.\n* However, the usual is between 45-60%.","metadata":{}},{"cell_type":"code","source":"compare_masks(ground_truth_mask, random_pred_mask)\n\ncorrect_pred_indices = []\nincorrect_pred_indices = []\nfor i in range(DEMO_SHAPE[0]): # loop row by row\n    for j in range(DEMO_SHAPE[1]): # walk the row one cell at a time\n        gt_pixel_val = ground_truth_mask[i, j]\n        pred_pixel_val = random_pred_mask[i, j]\n        if gt_pixel_val==pred_pixel_val:\n            correct_pred_indices.append((i,j))\n        else:\n            incorrect_pred_indices.append((i,j))\n\nbinary_pixel_classification_accuracy = len(correct_pred_indices)/(DEMO_SHAPE[0]*DEMO_SHAPE[1])\n\nviz_error_mask = np.zeros((*DEMO_SHAPE, 3), dtype=np.float32)\nviz_error_mask[np.array(correct_pred_indices)[:, 0], np.array(correct_pred_indices)[:, 1], 1] = 1.0\nviz_error_mask[np.array(incorrect_pred_indices)[:, 0], np.array(incorrect_pred_indices)[:, 1], 0] = 1.0\n\nprint(f\"\\n\\n\\n... BINARY ACCURACY = {100*binary_pixel_classification_accuracy:.2f} ...\\n\")\ncompare_masks(ground_truth_mask, viz_error_mask, pred_title=\"Error Visualization\\nGreen=Agreement\\nRed=Disagreement\")","metadata":{"execution":{"iopub.status.busy":"2022-04-22T12:36:09.391975Z","iopub.execute_input":"2022-04-22T12:36:09.392178Z","iopub.status.idle":"2022-04-22T12:36:10.583582Z","shell.execute_reply.started":"2022-04-22T12:36:09.392152Z","shell.execute_reply":"2022-04-22T12:36:10.582826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>\n\n#### **WHAT'S WRONG WITH WHAT WE JUST DID?**\n\n---\n\nThere's nothing inherently wrong with what we just did... however, let's imagine a different ground truth case. One where only a few pixels are white. Now let's see how we do. ","metadata":{}},{"cell_type":"code","source":"ground_truth_mask_2 = np.zeros(DEMO_SHAPE, dtype=np.float32)\nground_truth_mask_2[0,0] = 1.0\nground_truth_mask_2[0,DEMO_SHAPE[1]-1] = 1.0\nground_truth_mask_2[DEMO_SHAPE[0]-1,0] = 1.0\nground_truth_mask_2[DEMO_SHAPE[0]-1,DEMO_SHAPE[1]-1] = 1.0\n\ncompare_masks(ground_truth_mask_2, random_pred_mask)\n\ncorrect_pred_indices = []\nincorrect_pred_indices = []\nfor i in range(DEMO_SHAPE[0]): # loop row by row\n    for j in range(DEMO_SHAPE[1]): # walk the row one cell at a time\n        gt_pixel_val = ground_truth_mask_2[i, j]\n        pred_pixel_val = random_pred_mask[i, j]\n        if gt_pixel_val==pred_pixel_val:\n            correct_pred_indices.append((i,j))\n        else:\n            incorrect_pred_indices.append((i,j))\n\nbinary_pixel_classification_accuracy = len(correct_pred_indices)/(DEMO_SHAPE[0]*DEMO_SHAPE[1])\n\nviz_error_mask = np.zeros((*DEMO_SHAPE, 3), dtype=np.float32)\nviz_error_mask[np.array(correct_pred_indices)[:, 0], np.array(correct_pred_indices)[:, 1], 1] = 1.0\nviz_error_mask[np.array(incorrect_pred_indices)[:, 0], np.array(incorrect_pred_indices)[:, 1], 0] = 1.0\n\nprint(f\"\\n\\n\\n... BINARY ACCURACY = {100*binary_pixel_classification_accuracy:.2f} ...\\n\")\ncompare_masks(ground_truth_mask_2, viz_error_mask, pred_title=\"Error Visualization\\nGreen=Agreement\\nRed=Disagreement\")","metadata":{"execution":{"iopub.status.busy":"2022-04-22T12:36:10.585204Z","iopub.execute_input":"2022-04-22T12:36:10.58541Z","iopub.status.idle":"2022-04-22T12:36:11.788067Z","shell.execute_reply.started":"2022-04-22T12:36:10.585385Z","shell.execute_reply":"2022-04-22T12:36:11.787222Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>\n\n#### **OK... WE GOT AROUND THE SAME ACCURACY?**\n\n---\n\nSee now I look silly. Now it looks like it didn't matter. However, let's further consider... that instead of predicting a random mask, we simply predict all zeros!\n* This type of scenario is analagous to medical imagery where often, much of the image IS NOT the area of interest.\n* This results in a large amount of \"background\" (i.e. class=0) and a small (sometimes very small) amount of \"foreground\". \n* This can make models prone to simply put out all zeros as a prediction (and it certainly would if we framed the problem as binary classification without any classweighting).","metadata":{}},{"cell_type":"code","source":"all_black_pred = np.zeros_like(ground_truth_mask_2)\ncompare_masks(ground_truth_mask_2, all_black_pred)\n\ncorrect_pred_indices = []\nincorrect_pred_indices = []\nfor i in range(DEMO_SHAPE[0]): # loop row by row\n    for j in range(DEMO_SHAPE[1]): # walk the row one cell at a time\n        gt_pixel_val = ground_truth_mask_2[i, j]\n        pred_pixel_val = all_black_pred[i, j]\n        if gt_pixel_val==pred_pixel_val:\n            correct_pred_indices.append((i,j))\n        else:\n            incorrect_pred_indices.append((i,j))\n\nbinary_pixel_classification_accuracy = len(correct_pred_indices)/(DEMO_SHAPE[0]*DEMO_SHAPE[1])\n\nviz_error_mask = np.zeros((*DEMO_SHAPE, 3), dtype=np.float32)\nviz_error_mask[np.array(correct_pred_indices)[:, 0], np.array(correct_pred_indices)[:, 1], 1] = 1.0\nviz_error_mask[np.array(incorrect_pred_indices)[:, 0], np.array(incorrect_pred_indices)[:, 1], 0] = 1.0\n\nprint(f\"\\n\\n\\n... BINARY ACCURACY = {100*binary_pixel_classification_accuracy:.2f} ...\\n\")\ncompare_masks(ground_truth_mask_2, viz_error_mask, pred_title=\"Error Visualization\\nGreen=Agreement\\nRed=Disagreement\")","metadata":{"execution":{"iopub.status.busy":"2022-04-22T12:36:17.59309Z","iopub.execute_input":"2022-04-22T12:36:17.593635Z","iopub.status.idle":"2022-04-22T12:36:18.930622Z","shell.execute_reply.started":"2022-04-22T12:36:17.593594Z","shell.execute_reply":"2022-04-22T12:36:18.928287Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>\n\n#### **OHHH. I SEE!**\n\n---\n\n**BINARY ACCURACY DOESN'T TAKE INTO CONSIDERATION LARGE CLASS IMBALANCES!** This is the main reason why something as simple as binary accuracy doesn't make sense in cases where a background class is so dominant... Like is usually the case for semantic segmentation.\n\nWhat now....\n\n<br>\n\n<center><img src=\"https://i.ibb.co/LhfQzxf/final-6261f0f883d46a00a328c606-592769.png\"></center>\n\n<br>\n\n","metadata":{}},{"cell_type":"markdown","source":"<br>\n\n#### **LET'S LOOK AT PRECISION, RECALL, F1-SCORE, & AUC! BINARY SEGMENTATION VERSION**\n\n---\n\nBefore I throw up (pun intended) all of the images and formulas to explain these metrics, we first have to understand these conceptual metrics:\n* **True-Positives**\n* **False-Positives**\n* **True-Negatives**\n* **False-Negatives**\n\nTo explain these term's visually let's use our first ground truth mask and our first random prediction and generate examples of each. When we plot these, we will also offer a straightforward definition to help you understand.\n\n---\n\n","metadata":{}},{"cell_type":"markdown","source":"<br>\n\n#### **TRUE-POSITIVES**\n\n---\n\nWe will use image below in every section to illustrate (graphically) what we are trying to capture.\n\n<center><img src=\"https://miro.medium.com/max/1336/1*uzJKEMrjHEv9DBAGNke3EQ.png\" w=25%></center>\n\n<br>\n\n**FROM THE ABOVE IMAGE WE CAN GIVE A BASIC DEFINITION**\n\nA **True-Positive**, is **an outcome where the model CORRECTLY predicts the POSITIVE class**\n* What the model predicts as the positive class is shown as the circle \n* What the actual ground truth positive classes are is shown as the entire left side.\n* The points that are on the left side of the image inside the circle are **True-Positives** as they represent the \"overlap\" of positive class model predictions that match the ground truth values.\n\n**LET'S SEE WHAT THIS LOOKS LIKE FOR OUR PREDICTION**\n* For simplicity we will make **True-Positives** appear <b><font color=\"lightgreen\">Light Green</font></b>\n* For visualization purposes, all other cells will be displayed in **black**.\n* As we understand more and more of the terminology, our visualization will get more colourful!","metadata":{}},{"cell_type":"code","source":"compare_masks(ground_truth_mask, random_pred_mask)\n\ntrue_positive_indices = []\nfor i in range(DEMO_SHAPE[0]): # loop row by row\n    for j in range(DEMO_SHAPE[1]): # walk the row one cell at a time\n        gt_pixel_val = ground_truth_mask[i, j]\n        pred_pixel_val = random_pred_mask[i, j]\n        if (gt_pixel_val==pred_pixel_val) and (gt_pixel_val==1.0):\n            true_positive_indices.append((i,j))\n\nviz_mask = np.zeros((*DEMO_SHAPE, 3), dtype=np.float32)\nviz_mask[np.array(true_positive_indices)[:, 0], np.array(true_positive_indices)[:, 1], 1] = 1.0\n\ncompare_masks(ground_truth_mask, viz_mask, pred_title=\"True Positive Visualization\\nGreen=TP\")","metadata":{"execution":{"iopub.status.busy":"2022-04-22T12:36:28.677442Z","iopub.execute_input":"2022-04-22T12:36:28.677719Z","iopub.status.idle":"2022-04-22T12:36:29.70659Z","shell.execute_reply.started":"2022-04-22T12:36:28.677686Z","shell.execute_reply":"2022-04-22T12:36:29.705131Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>\n\n#### **TRUE-NEGATIVES**\n\n---\n\nWe will use image below in every section to illustrate (graphically) what we are trying to capture.\n\n<center><img src=\"https://miro.medium.com/max/1336/1*uzJKEMrjHEv9DBAGNke3EQ.png\" w=25%></center>\n\n<br>\n\n**FROM THE ABOVE IMAGE WE CAN GIVE A BASIC DEFINITION**\n\nA **True-Negative**, is **an outcome where the model CORRECTLY predicts the NEGATIVE class**\n* What the model predicts as the negative class is shown as everything outside the circle \n* What the actual ground truth negative classes are is shown as the entire right side\n* The points that are on the right side of the image outside the circle are **True-Negatives** as they represent the \"overlap\" of negative class model predictions that match the ground truth values.\n\n**LET'S SEE WHAT THIS LOOKS LIKE FOR OUR PREDICTION**\n* Remember that our **True-Positives** appear <b><font color=\"lightgreen\">Light Green</font></b>\n* For simplicity we will make **True-Negatives** appear <b><font color=\"darkgreen\">Dark Green</font></b>\n* For visualization purposes, all other cells will be displayed in **black**.\n* As we understand more and more of the terminology, our visualization will get more colourful!","metadata":{}},{"cell_type":"code","source":"compare_masks(ground_truth_mask, random_pred_mask)\n\ntrue_positive_indices = []\ntrue_negative_indices = []\nfor i in range(DEMO_SHAPE[0]): # loop row by row\n    for j in range(DEMO_SHAPE[1]): # walk the row one cell at a time\n        gt_pixel_val = ground_truth_mask[i, j]\n        pred_pixel_val = random_pred_mask[i, j]\n        if gt_pixel_val==pred_pixel_val:\n            if gt_pixel_val==1.0:\n                true_positive_indices.append((i,j))\n            else:\n                true_negative_indices.append((i,j))\n\nviz_mask = np.zeros((*DEMO_SHAPE, 3), dtype=np.float32)\nviz_mask[np.array(true_positive_indices)[:, 0], np.array(true_positive_indices)[:, 1], 1] = 1.0\nviz_mask[np.array(true_negative_indices)[:, 0], np.array(true_negative_indices)[:, 1], 1] = 0.333\n\ncompare_masks(ground_truth_mask, viz_mask, pred_title=\"True Positive/Negative Visualization\\nLight-Green=TP\\nDark-Green=TN\")","metadata":{"execution":{"iopub.status.busy":"2022-04-22T12:36:34.642113Z","iopub.execute_input":"2022-04-22T12:36:34.6424Z","iopub.status.idle":"2022-04-22T12:36:35.636621Z","shell.execute_reply.started":"2022-04-22T12:36:34.642371Z","shell.execute_reply":"2022-04-22T12:36:35.635582Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>\n\n#### **FALSE-POSITIVES**\n\n---\n\nWe will use image below in every section to illustrate (graphically) what we are trying to capture.\n\n<center><img src=\"https://miro.medium.com/max/1336/1*uzJKEMrjHEv9DBAGNke3EQ.png\" w=25%></center>\n\n<br>\n\n**FROM THE ABOVE IMAGE WE CAN GIVE A BASIC DEFINITION**\n\nA **False-Positive**, is **an outcome where the model INCORRECTLY predicts the POSITIVE class**\n* What the model predicts as the positive class is shown as everything inside the circle \n* What the actual ground truth positive classes are is shown as the entire left side\n* The points that are on the right side of the image inside the circle are **False-Positives** as they are the positive class model predictions that DO NOT match the ground truth values.\n\n**LET'S SEE WHAT THIS LOOKS LIKE FOR OUR PREDICTION**\n* Remember that our **True-Positives** appear <b><font color=\"lightgreen\">Light Green</font></b>\n* Remember that our **True-Negatives** appear <b><font color=\"darkgreen\">Dark Green</font></b>\n* For simplicity we will make **False-Positives** appear <b><font color=\"red\">Light Red</font></b>\n* For visualization purposes, all other cells will be displayed in **black**.\n* As we understand more and more of the terminology, our visualization will get more colourful!","metadata":{}},{"cell_type":"code","source":"compare_masks(ground_truth_mask, random_pred_mask)\n\ntrue_positive_indices = []\ntrue_negative_indices = []\nfalse_positive_indices = []\nfor i in range(DEMO_SHAPE[0]): # loop row by row\n    for j in range(DEMO_SHAPE[1]): # walk the row one cell at a time\n        gt_pixel_val = ground_truth_mask[i, j]\n        pred_pixel_val = random_pred_mask[i, j]\n        if gt_pixel_val==pred_pixel_val:\n            if gt_pixel_val==1.0:\n                true_positive_indices.append((i,j))\n            else:\n                true_negative_indices.append((i,j))\n        else:\n            if gt_pixel_val==1.0:\n                false_positive_indices.append((i,j))\n\nviz_mask = np.zeros((*DEMO_SHAPE, 3), dtype=np.float32)\nviz_mask[np.array(true_positive_indices)[:, 0], np.array(true_positive_indices)[:, 1], 1] = 1.0\nviz_mask[np.array(true_negative_indices)[:, 0], np.array(true_negative_indices)[:, 1], 1] = 0.333\nviz_mask[np.array(false_positive_indices)[:, 0], np.array(false_positive_indices)[:, 1], 0] = 1.0\n\ncompare_masks(ground_truth_mask, viz_mask, pred_title=\"TP, FP, & TN Visualization\\nLight-Green=TP\\nDark-Green=TN\\nLight-Red=FP\")","metadata":{"execution":{"iopub.status.busy":"2022-04-22T12:36:37.878008Z","iopub.execute_input":"2022-04-22T12:36:37.878333Z","iopub.status.idle":"2022-04-22T12:36:39.012352Z","shell.execute_reply.started":"2022-04-22T12:36:37.878291Z","shell.execute_reply":"2022-04-22T12:36:39.010988Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>\n\n#### **FALSE-NEGATIVES**\n\n---\n\nWe will use image below in every section to illustrate (graphically) what we are trying to capture.\n\n<center><img src=\"https://miro.medium.com/max/1336/1*uzJKEMrjHEv9DBAGNke3EQ.png\" w=25%></center>\n\n<br>\n\n**FROM THE ABOVE IMAGE WE CAN GIVE A BASIC DEFINITION**\n\nA **False-Negatives**, are **an outcome where the model INCORRECTLY predicts the NEGATIVE class**\n* What the model predicts as the negative class is shown as everything outside the circle \n* What the actual ground truth negative classes are is shown as the entire right side\n* The points that are on the left side of the image outside the circle are **False-Negatives** as they are the negative class model predictions that DO NOT match the ground truth values.\n\n**LET'S SEE WHAT THIS LOOKS LIKE FOR OUR PREDICTION**\n* Remember that our **True-Positives** appear <b><font color=\"lightgreen\">Light Green</font></b>\n* Remember that our **True-Negatives** appear <b><font color=\"darkgreen\">Dark Green</font></b>\n* Remember that our **False-Positives** appear <b><font color=\"red\">Light Red</font></b>\n* For simplicity we will make **False-Negatives** appear <b><font color=\"darkred\">Dark Red</font></b>\n* We are now at maximum COLORFULNESS and no cells are displayed as black","metadata":{}},{"cell_type":"code","source":"compare_masks(ground_truth_mask, random_pred_mask)\n\ntrue_positive_indices = []\ntrue_negative_indices = []\nfalse_positive_indices = []\nfalse_negative_indices = []\nfor i in range(DEMO_SHAPE[0]): # loop row by row\n    for j in range(DEMO_SHAPE[1]): # walk the row one cell at a time\n        gt_pixel_val = ground_truth_mask[i, j]\n        pred_pixel_val = random_pred_mask[i, j]\n        if gt_pixel_val==pred_pixel_val:\n            if gt_pixel_val==1.0:\n                true_positive_indices.append((i,j))\n            else:\n                true_negative_indices.append((i,j))\n        else:\n            if gt_pixel_val==1.0:\n                false_positive_indices.append((i,j))\n            else:\n                false_negative_indices.append((i,j))\nviz_mask = np.zeros((*DEMO_SHAPE, 3), dtype=np.float32)\nviz_mask[np.array(true_positive_indices)[:, 0], np.array(true_positive_indices)[:, 1], 1] = 1.0\nviz_mask[np.array(true_negative_indices)[:, 0], np.array(true_negative_indices)[:, 1], 1] = 0.333\nviz_mask[np.array(false_positive_indices)[:, 0], np.array(false_positive_indices)[:, 1], 0] = 1.0\nviz_mask[np.array(false_negative_indices)[:, 0], np.array(false_negative_indices)[:, 1], 0] = 0.333\n\ncompare_masks(ground_truth_mask, viz_mask, pred_title=\"TP, FP, TN, & FN Visualization\\nLight-Green=TP\\nDark-Green=TN\\nLight-Red=FP\\Dark-Red=FN\")","metadata":{"execution":{"iopub.status.busy":"2022-04-22T12:36:42.324543Z","iopub.execute_input":"2022-04-22T12:36:42.32553Z","iopub.status.idle":"2022-04-22T12:36:43.530266Z","shell.execute_reply.started":"2022-04-22T12:36:42.325477Z","shell.execute_reply":"2022-04-22T12:36:43.529686Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>\n\n#### **LET'S REVIEW FOR A MINUTE**\n\n---\n\nWe can now see in the above image a colourful representation of not only how well we did (Green v. Red), but we can also see how well we did w.r.t. both the foreground and the background classes. \n\n<br>\n\n**This gives us some more options!**\n\n<br>\n\n**RECALL** (Also known as *sensitivity* or *true-positive rate*)\n* We could identify the accuracy w.r.t. the ground truth foreground! i.e. What percentage of all ground truth positives did we manage to guess correctly.\n    * This would be represented as the formula: \n$$\n\\dfrac{TP}{TP+FN}\n$$\n    \n<br>\n\n**PRECISION**\n* We could identify the accuracy w.r.t. the predicted foreground! i.e. What percentage of all of the predicted positives did we manage to guess correctly.\n    * This would be represented as the formula:\n$$\n\\dfrac{TP}{TP+FP}\n$$\n    \n<br>\n\n**There are other metrics, but for the most part this should cover the basics of what we need**\n\n<br>\n\n---\n\n<br>\n\nWe could then take things even further by combining some of these metrics to get a better understanding of how well we predicted everything (foreground, background, etc.). This leads us to a real strong candidate for a satisfactory metric... the F1 Score!\n\n<br>\n\n**F1 SCORE**\n* One simple approach would be to take the ***harmonic mean*** of precision and recall. \n    * NOTE: We use the ***harmonic mean*** because it penalizes extreme values (all the bg wrong, etc.)\n    * This would be represented as the formula: <br><br>\n$$\n2 \\times \\dfrac{PRECISION \\times RECALL}{PRECISION + RECALL}\n$$\n    \n---\n\n**BRIEF ASIDE ON DIFFERENT TYPES OF MEANS...**\n\n<center><img src=\"https://miro.medium.com/max/1086/1*WUYsiOqd1UtBoMf1UcSsMg.gif\"></center>\n\n---\n","metadata":{}},{"cell_type":"markdown","source":"<br>\n\n#### **LET'S MAKE SURE WE CAN'T BE TRICKED LIKE BEFORE!**\n\n---\n\nLet's calculate the F1-Score for the previous example and see how it compares.\n* Let's first validate that our calculated F1-Score yields the same as an off the shelf tool\n    * We will see below that the sklearn.metrics.f1_score works just as well as our tool to calculate the f1-score... as a result we will use that when we simply need to calculate F1-Score","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import f1_score\n\ntrue_positive_indices = []\ntrue_negative_indices = []\nfalse_positive_indices = []\nfalse_negative_indices = []\n\ncompare_masks(ground_truth_mask, random_pred_mask)\n\nfor i in range(DEMO_SHAPE[0]): # loop row by row\n    for j in range(DEMO_SHAPE[1]): # walk the row one cell at a time\n        gt_pixel_val = ground_truth_mask[i, j]\n        pred_pixel_val = random_pred_mask[i, j]\n        if gt_pixel_val==pred_pixel_val:\n            if gt_pixel_val==1.0:\n                true_positive_indices.append((i,j))\n            else:\n                true_negative_indices.append((i,j))\n        else:\n            if gt_pixel_val==1.0:\n                false_positive_indices.append((i,j))\n            else:\n                false_negative_indices.append((i,j))\n\nviz_mask = np.zeros((*DEMO_SHAPE, 3), dtype=np.float32)\nviz_mask[np.array(true_positive_indices)[:, 0], np.array(true_positive_indices)[:, 1], 1] = 1.0\nviz_mask[np.array(true_negative_indices)[:, 0], np.array(true_negative_indices)[:, 1], 1] = 0.333\nviz_mask[np.array(false_positive_indices)[:, 0], np.array(false_positive_indices)[:, 1], 0] = 1.0\nviz_mask[np.array(false_negative_indices)[:, 0], np.array(false_negative_indices)[:, 1], 0] = 0.333\n\nprecision = len(true_positive_indices)/(len(true_positive_indices)+len(false_positive_indices))\nrecall    = len(true_positive_indices)/(len(true_positive_indices)+len(false_negative_indices))\n\nf1_score_ours = (2*precision*recall)/(precision+recall)\nsk_f1_score   = f1_score(ground_truth_mask.flatten(), random_pred_mask.flatten())\n\ncompare_masks(ground_truth_mask, viz_mask, pred_title=f\"F1 SCORE VISUALIZATION PLOT\\n        OUR F1 SCORE: {f1_score_ours}\\nSKLEARN F1 SCORE: {sk_f1_score}\")\n\nprint(\"\\n\\n\\n... WE CAN SEE THAT OUR F1 SCORE AND THE SKLEARN F1 SCORE ARE THE SAME ...\\n\\n\")\nprint(\"\\n... LET'S NOW USE THAT TOOL TO COMPUTE THE F1-SCORES FOR OUR RANDOM PRED, A BLACK PRED, AND A WHITE PRED\")\nprint(f\"\\t... RANDOM PREDICTION F1-SCORE: {f1_score(ground_truth_mask.flatten(), random_pred_mask.flatten()):.4f}\")\nprint(f\"\\t... ALL ZEROS (BLACK) F1-SCORE: {f1_score(ground_truth_mask.flatten(), np.zeros(DEMO_SHAPE).flatten()):.4f}\")\nprint(f\"\\t... ALL ONES  (WHITE) F1-SCORE: {f1_score(ground_truth_mask.flatten(), np.ones(DEMO_SHAPE).flatten()):.4f}\")\n\nprint(\"\\n\\n\\n... NOW LET'S CHECK THE SAME FOR THE OUR MOSTLY BLACK IMAGE ...\\n\\n\")\n\ntrue_positive_indices = []\ntrue_negative_indices = []\nfalse_positive_indices = []\nfalse_negative_indices = []\n\ncompare_masks(ground_truth_mask_2, random_pred_mask)\n\nfor i in range(DEMO_SHAPE[0]): # loop row by row\n    for j in range(DEMO_SHAPE[1]): # walk the row one cell at a time\n        gt_pixel_val = ground_truth_mask_2[i, j]\n        pred_pixel_val = random_pred_mask[i, j]\n        if gt_pixel_val==pred_pixel_val:\n            if gt_pixel_val==1.0:\n                true_positive_indices.append((i,j))\n            else:\n                true_negative_indices.append((i,j))\n        else:\n            if gt_pixel_val==1.0:\n                false_positive_indices.append((i,j))\n            else:\n                false_negative_indices.append((i,j))\n\nviz_mask = np.zeros((*DEMO_SHAPE, 3), dtype=np.float32)\nviz_mask[np.array(true_positive_indices)[:, 0], np.array(true_positive_indices)[:, 1], 1] = 1.0\nviz_mask[np.array(true_negative_indices)[:, 0], np.array(true_negative_indices)[:, 1], 1] = 0.333\nviz_mask[np.array(false_positive_indices)[:, 0], np.array(false_positive_indices)[:, 1], 0] = 1.0\nviz_mask[np.array(false_negative_indices)[:, 0], np.array(false_negative_indices)[:, 1], 0] = 0.333\n\ncompare_masks(ground_truth_mask_2, viz_mask, pred_title=f\"F1 SCORE VISUALIZATION PLOT\\n\")\n\nprint(\"\\n... LET'S NOW USE THAT TOOL TO COMPUTE THE F1-SCORES FOR OUR RANDOM PRED, A BLACK PRED, AND A WHITE PRED\")\nprint(f\"\\t... RANDOM PREDICTION F1-SCORE: {f1_score(ground_truth_mask_2.flatten(), random_pred_mask.flatten()):.4f}\")\nprint(f\"\\t... ALL ZEROS (BLACK) F1-SCORE: {f1_score(ground_truth_mask_2.flatten(), np.zeros(DEMO_SHAPE).flatten()):.4f}\")\nprint(f\"\\t... ALL ONES  (WHITE) F1-SCORE: {f1_score(ground_truth_mask_2.flatten(), np.ones(DEMO_SHAPE).flatten()):.4f}\")\n","metadata":{"execution":{"iopub.status.busy":"2022-04-22T13:00:35.291983Z","iopub.execute_input":"2022-04-22T13:00:35.292429Z","iopub.status.idle":"2022-04-22T13:00:37.298206Z","shell.execute_reply.started":"2022-04-22T13:00:35.292384Z","shell.execute_reply":"2022-04-22T13:00:37.297198Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>\n\n#### **WHILE THAT'S BETTER! OUR RANDOM SCORE SHOULD BE QUITE SIMILAR TO OUR TRICK SCORES.**\n\n---\n\nThis seems good enough right!? For now we will leave AUC alone. If I have time to come back to it I will update this notebook accordingly.\n\nLet's try the same experiment as previously but with a **multi-label** prediction","metadata":{}},{"cell_type":"markdown","source":"<br>\n\n#### **HOW DOES THIS ALL WORK FOR MULTILABEL CLASSIFICATION?**\n\n---\n\nHonestly, this is pretty straightforward. We will simply treat the problem the same as we treated the binary classification problem, for each respective channel. Then we will take the average across the channels!\n","metadata":{}},{"cell_type":"code","source":"compare_masks(ground_truth_rgb_mask, random_pred_rgb_mask, \n              gt_title=\"Ground Truth RGB Mask\", pred_title=\"Prediction RGB Mask\")\n\nprint(\"\\n\\n... THIS RGB IMAGE COMPARISON BECOMES THREE INDIVIDUAL COMPARISONS ...\\n\\n\")\n\nfor i in range(3):\n    compare_masks(ground_truth_rgb_mask[..., i], random_pred_rgb_mask[..., i], \n                  gt_title=f\"Ground Truth {'RGB'[i]} Mask\", pred_title=f\"Prediction {'RGB'[i]} Mask\")","metadata":{"execution":{"iopub.status.busy":"2022-04-22T13:15:57.537407Z","iopub.execute_input":"2022-04-22T13:15:57.537984Z","iopub.status.idle":"2022-04-22T13:15:59.956986Z","shell.execute_reply.started":"2022-04-22T13:15:57.537947Z","shell.execute_reply":"2022-04-22T13:15:59.956094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>\n\n#### **LET'S CALCULATE THE INDIVIDUAL F1-SCORES FOR OUR MULTILABEL TASK**\n\n---\n\nBut wait... does this make sense? \n* Imagine a channel where we only have one or two positive examples? \n* What about a channel where it occurs very frequently?\n\nThis is why we have to take into consideration whether or not directly averaging across the classes makes sense. We have two options for how to deal with this:\n\n1. **MACRO AVERAGING**\n* Calculate metrics for each label, and find their unweighted mean. \n* This does not take label imbalance into account.\n\n2. **MICRO AVERAGING**\n* Calculate metrics **globally** by counting the total true positives, false negatives and false positives.\n\n---\n\nThe **DICE COEFFICIENT** is simply the 1-**MICRO F1SCORE**. The dice coefficient is ","metadata":{}},{"cell_type":"code","source":"from scipy.spatial.distance import dice\n\nprint(f\"MACRO SCORE  : {f1_score(ground_truth_rgb_mask.reshape(-1, 3), random_pred_rgb_mask.reshape(-1, 3), average='macro')}\")\nprint(f\"MICRO SCORE  : {f1_score(ground_truth_rgb_mask.reshape(-1, 3), random_pred_rgb_mask.reshape(-1, 3), average='micro')}\")\n\nprint(f\"\\nINVERSE DICE SCORE : {1-dice(ground_truth_rgb_mask.flatten(), random_pred_rgb_mask.flatten())}\")\nprint(f\"DICE SCORE         : {dice(ground_truth_rgb_mask.flatten(), random_pred_rgb_mask.flatten())}\")","metadata":{"execution":{"iopub.status.busy":"2022-04-22T13:38:06.659499Z","iopub.execute_input":"2022-04-22T13:38:06.659771Z","iopub.status.idle":"2022-04-22T13:38:06.672369Z","shell.execute_reply.started":"2022-04-22T13:38:06.65974Z","shell.execute_reply":"2022-04-22T13:38:06.671729Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>\n\n#### **WE FINALLY MADE IT TO THE DICE SCORE! ONE OF THE COMPETITION METRICS**\n\n---\n\n<br>\n    \n**Simply put, the Dice Coefficient is 2 * the Area of Overlap divided by the total number of pixels in both images.**\n\n<br>\n\n**Essentially... the F1-SCORE**\n\n<br>\n\n<center><img src=\"https://miro.medium.com/max/858/1*yUd5ckecHjWZf6hGrdlwzA.png\"></center>\n\n<br>\n\n---\n\n\n","metadata":{}},{"cell_type":"markdown","source":"<br>\n\n#### **LET'S TAKE AN ASIDE AND DISCUSS AN ALTERNATIVE... THE JACCARD INDEX (IOU)**\n\n---\n\n<br>\n\n**Simply put, the Jaccard Index (IoU) is the area of overlap between the predicted segmentation and the ground truth divided by the area of union between the predicted segmentation and the ground truth.**<br><br>\n\n<center><img src=\"https://miro.medium.com/max/600/0*kraYHnYpoJOhaMzq.png\"></center>\n\n<br>\n\nSo, the numerator stays the same – $2 \\times PRECISION \\times RECALL$ – as before, however, we change our denominator from being ALL PIXELS to AREA-OF-UNION. The difference here is that ALL PIXELS would count overlap TWICE wheras AREA-OF-UNION would only count the intersection pixels once.\n\n<br>","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import jaccard_score\n\nprint(f\"INVERSE DICE SCORE : {1-dice(ground_truth_rgb_mask.flatten(), random_pred_rgb_mask.flatten())}\")\nprint(f\"JACCARD SCORE      : {jaccard_score(ground_truth_rgb_mask.flatten(), random_pred_rgb_mask.flatten())}\")","metadata":{"execution":{"iopub.status.busy":"2022-04-22T13:49:13.851034Z","iopub.execute_input":"2022-04-22T13:49:13.85131Z","iopub.status.idle":"2022-04-22T13:49:13.860041Z","shell.execute_reply.started":"2022-04-22T13:49:13.85128Z","shell.execute_reply":"2022-04-22T13:49:13.859169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>\n\n#### **FINALLY LET'S TAKE A LOOK AT HAUSDORFF DISTANCE**\n\n---\n\n<br>\n","metadata":{}},{"cell_type":"code","source":"from skimage.metrics import hausdorff_distance\nfrom scipy.ndimage import distance_transform_edt\n!pip install -q monai\nimport monai\n\nground_truth_3d_rgb_mask = np.stack([ground_truth_rgb_mask, ground_truth_rgb_mask, ground_truth_rgb_mask, ground_truth_rgb_mask], axis=0)\nrandom_pred_3d_mask = np.stack([random_pred_rgb_mask, random_pred_rgb_mask, random_pred_rgb_mask, random_pred_rgb_mask], axis=0)\nmonai.metrics.compute_percent_hausdorff_distance(ground_truth_3d_rgb_mask.astype(np.uint8), random_pred_3d_mask.astype(np.uint8))","metadata":{"execution":{"iopub.status.busy":"2022-04-22T13:54:35.524366Z","iopub.execute_input":"2022-04-22T13:54:35.524666Z","iopub.status.idle":"2022-04-22T13:54:44.455367Z","shell.execute_reply.started":"2022-04-22T13:54:35.524627Z","shell.execute_reply":"2022-04-22T13:54:44.454414Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_surface_distance(seg_pred, seg_gt, distance_metric=\"euclidean\"):\n    \"\"\"\n    This function is used to compute the surface distances from `seg_pred` to `seg_gt`.\n\n    Args:\n        seg_pred: the edge of the predictions.\n        seg_gt: the edge of the ground truth.\n        distance_metric: : [``\"euclidean\"``, ``\"chessboard\"``, ``\"taxicab\"``]\n            the metric used to compute surface distance. Defaults to ``\"euclidean\"``.\n\n            - ``\"euclidean\"``, uses Exact Euclidean distance transform.\n            - ``\"chessboard\"``, uses `chessboard` metric in chamfer type of transform.\n            - ``\"taxicab\"``, uses `taxicab` metric in chamfer type of transform.\n\n    Note:\n        If seg_pred or seg_gt is all 0, may result in nan/inf distance.\n\n    \"\"\"\n\n    # Check if mask is empty... if so set dis to array of infinite\n    if not np.any(seg_gt):\n        dis = np.inf * np.ones_like(seg_gt)\n        \n    \n    else:\n        if not np.any(seg_pred):\n            dis = np.inf * np.ones_like(seg_gt)\n            return np.asarray(dis[seg_gt])\n        if distance_metric == \"euclidean\":\n            dis = distance_transform_edt(~seg_gt)\n        elif distance_metric in {\"chessboard\", \"taxicab\"}:\n            dis = distance_transform_cdt(~seg_gt, metric=distance_metric)\n        else:\n            raise ValueError(f\"distance_metric {distance_metric} is not implemented.\")\n\n    return np.asarray(dis[seg_pred])","metadata":{"execution":{"iopub.status.busy":"2022-04-22T14:01:31.435514Z","iopub.execute_input":"2022-04-22T14:01:31.436008Z","iopub.status.idle":"2022-04-22T14:01:31.443472Z","shell.execute_reply.started":"2022-04-22T14:01:31.435961Z","shell.execute_reply":"2022-04-22T14:01:31.442584Z"},"trusted":true},"execution_count":null,"outputs":[]}]}