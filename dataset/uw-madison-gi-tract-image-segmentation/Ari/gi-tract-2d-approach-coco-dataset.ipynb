{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## **Quick Note**","metadata":{}},{"cell_type":"markdown","source":"This is a 3D segmentation competition. However, I thought it might be useful to have a 2D Coco Dataset so here it is.","metadata":{}},{"cell_type":"markdown","source":"# **Train Notebook**","metadata":{}},{"cell_type":"markdown","source":"https://www.kaggle.com/code/vexxingbanana/gi-tract-2d-approach-mmdetection-training","metadata":{}},{"cell_type":"markdown","source":"# **References**","metadata":{}},{"cell_type":"markdown","source":"https://www.kaggle.com/code/vexxingbanana/sartorius-coco-dataset-notebook\n\nhttps://www.kaggle.com/code/coldfir3/efficient-coco-dataset-generator","metadata":{}},{"cell_type":"markdown","source":"# **Install Libraries**","metadata":{}},{"cell_type":"code","source":"!pip install pycocotools","metadata":{"execution":{"iopub.status.busy":"2022-04-15T21:26:54.457161Z","iopub.execute_input":"2022-04-15T21:26:54.457668Z","iopub.status.idle":"2022-04-15T21:27:33.137679Z","shell.execute_reply.started":"2022-04-15T21:26:54.457575Z","shell.execute_reply":"2022-04-15T21:27:33.136639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sklearn\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\nimport pandas as pd\nimport os\nimport matplotlib.pyplot as plt\nimport json\nimport tqdm\nfrom tqdm import tqdm\nimport glob\nimport pycocotools\nfrom pycocotools import mask\nimport random\nimport cv2\nimport re","metadata":{"execution":{"iopub.status.busy":"2022-04-15T21:29:33.728859Z","iopub.execute_input":"2022-04-15T21:29:33.729158Z","iopub.status.idle":"2022-04-15T21:29:33.734512Z","shell.execute_reply.started":"2022-04-15T21:29:33.729119Z","shell.execute_reply":"2022-04-15T21:29:33.733751Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Helper Functions**","metadata":{}},{"cell_type":"code","source":"def rle_decode(mask_rle, shape):\n    '''\n    mask_rle: run-length as string formated (start length)\n    shape: (height,width) of array to return \n    Returns numpy array, 1 - mask, 0 - background\n\n    '''\n    s = mask_rle.split()\n    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n    starts -= 1\n    ends = starts + lengths\n    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n    for lo, hi in zip(starts, ends):\n        img[lo:hi] = 1\n    return img.reshape(shape)\n\ndef rle_encode(img):\n    '''\n    img: numpy array, 1 - mask, 0 - background\n    Returns run length as string formated\n    '''\n    pixels = img.flatten()\n    pixels = np.concatenate([[0], pixels, [0]])\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n    runs[1::2] -= runs[::2]\n    return ' '.join(str(x) for x in runs)\n\n\ndef flatten_l_o_l(nested_list):\n    \"\"\" Flatten a list of lists \"\"\"\n    return [item for sublist in nested_list for item in sublist]\n\n\ndef load_json_to_dict(json_path):\n    \"\"\" tbd \"\"\"\n    with open(json_path) as json_file:\n        data = json.load(json_file)\n    return data\n\ndef get_img_and_mask(img_path, annotation, width, height):\n    \"\"\" Capture the relevant image array as well as the image mask \"\"\"\n    img_mask = np.zeros((height, width), dtype=np.uint8)\n    for i, annot in enumerate(annotation): \n        img_mask = np.where(rle_decode(annot, (height, width))!=0, i, img_mask)\n    img = cv2.imread(img_path)[..., ::-1]\n    return img[..., 0], img_mask\n\ndef plot_img_and_mask(img, mask, invert_img=True, boost_contrast=True):\n    \"\"\" Function to take an image and the corresponding mask and plot\n    \n    Args:\n        img (np.arr): 1 channel np arr representing the image of cellular structures\n        mask (np.arr): 1 channel np arr representing the instance masks (incrementing by one)\n        invert_img (bool, optional): Whether or not to invert the base image\n        boost_contrast (bool, optional): Whether or not to boost contrast of the base image\n        \n    Returns:\n        None; Plots the two arrays and overlays them to create a merged image\n    \"\"\"\n    plt.figure(figsize=(20,10))\n    \n    plt.subplot(1,3,1)\n    _img = np.tile(np.expand_dims(img, axis=-1), 3)\n    \n    # Flip black-->white ... white-->black\n    if invert_img:\n        _img = _img.max()-_img\n        \n    if boost_contrast:\n        _img = np.asarray(ImageEnhance.Contrast(Image.fromarray(_img)).enhance(16))\n        \n    plt.imshow(_img)\n    plt.axis(False)\n    plt.title(\"Cell Image\", fontweight=\"bold\")\n    \n    plt.subplot(1,3,2)\n    _mask = np.zeros_like(_img)\n    _mask[..., 0] = mask\n    plt.imshow(mask, cmap='rainbow')\n    plt.axis(False)\n    plt.title(\"Instance Segmentation Mask\", fontweight=\"bold\")\n    \n    merged = cv2.addWeighted(_img, 0.75, np.clip(_mask, 0, 1)*255, 0.25, 0.0,)\n    plt.subplot(1,3,3)\n    plt.imshow(merged)\n    plt.axis(False)\n    plt.title(\"Cell Image w/ Instance Segmentation Mask Overlay\", fontweight=\"bold\")\n    \n    plt.tight_layout()\n    plt.show()\n    \ndef polygonFromMask(maskedArr, idx):\n  # adapted from https://github.com/hazirbas/coco-json-converter/blob/master/generate_coco_json.py\n    contours, _ = cv2.findContours(maskedArr, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n    segmentation = []\n    valid_poly = 0\n    for contour in contours:\n  # Valid polygons have >= 6 coordinates (3 points)\n        if contour.size >= 6:\n            segmentation.append(contour.astype(float).flatten().tolist())\n            valid_poly += 1\n    if valid_poly == 0:\n        raise ValueError(idx)\n    return [segmentation]\n\ndef np_encoder(object):\n    if isinstance(object, np.generic):\n        return object.item()","metadata":{"execution":{"iopub.status.busy":"2022-04-15T21:28:27.415251Z","iopub.execute_input":"2022-04-15T21:28:27.415553Z","iopub.status.idle":"2022-04-15T21:28:27.436306Z","shell.execute_reply.started":"2022-04-15T21:28:27.415518Z","shell.execute_reply":"2022-04-15T21:28:27.435253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Data Processing**","metadata":{}},{"cell_type":"code","source":"DATA_PATH = \"../input/uw-madison-gi-tract-image-segmentation/\"","metadata":{"execution":{"iopub.status.busy":"2022-04-15T21:28:55.669962Z","iopub.execute_input":"2022-04-15T21:28:55.670539Z","iopub.status.idle":"2022-04-15T21:28:55.675067Z","shell.execute_reply.started":"2022-04-15T21:28:55.670501Z","shell.execute_reply":"2022-04-15T21:28:55.674069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv(f\"{DATA_PATH}train.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-04-15T21:29:03.279175Z","iopub.execute_input":"2022-04-15T21:29:03.279618Z","iopub.status.idle":"2022-04-15T21:29:03.809135Z","shell.execute_reply.started":"2022-04-15T21:29:03.279584Z","shell.execute_reply":"2022-04-15T21:29:03.808511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img_paths = []\nimg_heights = []\nimg_widths = []\npixel_scaling_heights = []\npixel_scaling_widths = []\n\nfor i, row in tqdm(train_df.iterrows()):\n    ids = row.id.split(\"_\")\n    img_path = f\"{DATA_PATH}/train/{ids[0]}/{ids[0]}_{ids[1]}/scans/{ids[2]}_{ids[3]}*\"\n    img = glob.glob(img_path)[0]\n    \n    splits = img.split(f\"{ids[3]}_\")[1].split(\"_\")\n    img_height = int(splits[0])\n    img_width = int(splits[1])\n    pixel_scaling_height = float(splits[2])\n    pixel_scaling_width = float(splits[3].split(\".png\")[0])\n    \n    img_paths.append(img)\n    img_heights.append(img_height)\n    img_widths.append(img_width)\n    pixel_scaling_heights.append(pixel_scaling_height)\n    pixel_scaling_widths.append(pixel_scaling_width)","metadata":{"execution":{"iopub.status.busy":"2022-04-15T21:29:36.924764Z","iopub.execute_input":"2022-04-15T21:29:36.925234Z","iopub.status.idle":"2022-04-15T21:31:07.713156Z","shell.execute_reply.started":"2022-04-15T21:29:36.925183Z","shell.execute_reply":"2022-04-15T21:31:07.712524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df['img_path'] = img_paths\ntrain_df['img_height'] = img_heights\ntrain_df['img_width'] = img_widths\ntrain_df['pixel_scaling_height'] = pixel_scaling_heights\ntrain_df['pixel_scaling_width'] = pixel_scaling_widths","metadata":{"execution":{"iopub.status.busy":"2022-04-15T21:31:07.714288Z","iopub.execute_input":"2022-04-15T21:31:07.714625Z","iopub.status.idle":"2022-04-15T21:31:07.84487Z","shell.execute_reply.started":"2022-04-15T21:31:07.714594Z","shell.execute_reply":"2022-04-15T21:31:07.844001Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **5 Fold Coco Dataset**","metadata":{}},{"cell_type":"code","source":"FOLD = 1","metadata":{"execution":{"iopub.status.busy":"2022-04-15T22:01:12.230235Z","iopub.execute_input":"2022-04-15T22:01:12.230643Z","iopub.status.idle":"2022-04-15T22:01:12.234835Z","shell.execute_reply.started":"2022-04-15T22:01:12.230612Z","shell.execute_reply":"2022-04-15T22:01:12.233935Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_ids = train_df.id.unique()","metadata":{"execution":{"iopub.status.busy":"2022-04-15T22:01:17.783093Z","iopub.execute_input":"2022-04-15T22:01:17.783501Z","iopub.status.idle":"2022-04-15T22:01:17.799904Z","shell.execute_reply.started":"2022-04-15T22:01:17.783469Z","shell.execute_reply":"2022-04-15T22:01:17.799134Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_idxs = [i for i in range(len(train_ids)) if i%5!=FOLD]\nval_idxs = [i for i in range(len(train_ids)) if i%5==FOLD]","metadata":{"execution":{"iopub.status.busy":"2022-04-15T22:01:12.810374Z","iopub.execute_input":"2022-04-15T22:01:12.810654Z","iopub.status.idle":"2022-04-15T22:01:12.822028Z","shell.execute_reply.started":"2022-04-15T22:01:12.810621Z","shell.execute_reply":"2022-04-15T22:01:12.821147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_ids = train_ids[val_idxs]\ntrain_ids = train_ids[train_idxs]","metadata":{"execution":{"iopub.status.busy":"2022-04-15T22:01:32.897468Z","iopub.execute_input":"2022-04-15T22:01:32.897868Z","iopub.status.idle":"2022-04-15T22:01:32.904766Z","shell.execute_reply.started":"2022-04-15T22:01:32.897838Z","shell.execute_reply":"2022-04-15T22:01:32.903819Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"categories = {\"large_bowel\": 1, \"small_bowel\": 2, \"stomach\": 3}","metadata":{"execution":{"iopub.status.busy":"2022-04-15T21:44:14.09206Z","iopub.execute_input":"2022-04-15T21:44:14.092363Z","iopub.status.idle":"2022-04-15T21:44:14.096741Z","shell.execute_reply.started":"2022-04-15T21:44:14.09232Z","shell.execute_reply":"2022-04-15T21:44:14.095793Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output_json_dict = {\n    \"images\": [],\n    \"annotations\": [],\n    \"categories\": []\n}\n\ncategory_dict = {\"id\": 1, \"name\": \"large_bowel\", \"supercategory\": \"none\"}\noutput_json_dict[\"categories\"].append(category_dict)\ncategory_dict = {\"id\": 2, \"name\": \"small_bowel\", \"supercategory\": \"none\"}\noutput_json_dict[\"categories\"].append(category_dict)\ncategory_dict = {\"id\": 3, \"name\": \"stomach\", \"supercategory\": \"none\"}\noutput_json_dict[\"categories\"].append(category_dict)","metadata":{"execution":{"iopub.status.busy":"2022-04-15T22:07:46.392714Z","iopub.execute_input":"2022-04-15T22:07:46.392997Z","iopub.status.idle":"2022-04-15T22:07:46.429916Z","shell.execute_reply.started":"2022-04-15T22:07:46.392969Z","shell.execute_reply":"2022-04-15T22:07:46.428887Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_image_and_annot_info(df, fold, ids):\n    i = 1\n    g = 1\n    \n    for id_ in tqdm(ids):\n        id_df = df[df['id'] == id_]\n        \n        image_id = i\n        i += 1\n        width = id_df.iloc[0, 5]\n        height = id_df.iloc[0, 4]\n        file_path = id_df.iloc[0, 3]\n        \n        image_info = {\n            \"id\": image_id,\n            \"width\": width,\n            \"height\": height,\n            \"file_name\": file_path\n        }\n        output_json_dict['images'].append(image_info)\n        \n        for j, row in id_df.iterrows():\n            if pd.isna(row.segmentation) == False:\n                annot = rle_decode(row.segmentation, (height, width))\n                _, count = np.unique(annot, return_counts=True)\n                annot_mask = annot.astype(np.bool)\n                annot_mask = np.asfortranarray(annot_mask)\n                \n                Rs = mask.encode(annot_mask)\n                Rs['counts'] = Rs['counts'].decode('utf-8')\n                bbox = mask.toBbox(Rs)\n                bbox_list = []\n                for element in bbox:\n                    bbox_list.append(int(element))\n                    \n                category = categories[row['class']]\n                \n                annot_dict = {\n                    \"category_id\": category,\n                    \"segmentation\": Rs,\n                    \"area\": int(mask.area(Rs)),\n                    \"bbox\": bbox_list,\n                    \"id\": g,\n                    \"image_id\": i,\n                    \"iscrowd\": 0}\n                \n                g += 1\n                output_json_dict['annotations'].append(annot_dict)","metadata":{"execution":{"iopub.status.busy":"2022-04-15T22:01:42.307598Z","iopub.execute_input":"2022-04-15T22:01:42.307927Z","iopub.status.idle":"2022-04-15T22:01:42.318975Z","shell.execute_reply.started":"2022-04-15T22:01:42.307894Z","shell.execute_reply":"2022-04-15T22:01:42.318086Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"get_image_and_annot_info(train_df, FOLD, train_ids)\nwith open(f'train_dataset_fold_{FOLD}.json', 'w') as f:\n    output_json = json.dumps(output_json_dict, default=np_encoder)\n    f.write(output_json)","metadata":{"execution":{"iopub.status.busy":"2022-04-15T22:08:12.31367Z","iopub.execute_input":"2022-04-15T22:08:12.313978Z","iopub.status.idle":"2022-04-15T22:13:27.022686Z","shell.execute_reply.started":"2022-04-15T22:08:12.313943Z","shell.execute_reply":"2022-04-15T22:13:27.021615Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output_json_dict = {\n    \"images\": [],\n    \"annotations\": [],\n    \"categories\": []\n}\n\ncategory_dict = {\"id\": 1, \"name\": \"large_bowel\", \"supercategory\": \"none\"}\noutput_json_dict[\"categories\"].append(category_dict)\ncategory_dict = {\"id\": 2, \"name\": \"small_bowel\", \"supercategory\": \"none\"}\noutput_json_dict[\"categories\"].append(category_dict)\ncategory_dict = {\"id\": 3, \"name\": \"stomach\", \"supercategory\": \"none\"}\noutput_json_dict[\"categories\"].append(category_dict)","metadata":{"execution":{"iopub.status.busy":"2022-04-15T22:18:58.066162Z","iopub.execute_input":"2022-04-15T22:18:58.066433Z","iopub.status.idle":"2022-04-15T22:18:58.083891Z","shell.execute_reply.started":"2022-04-15T22:18:58.066401Z","shell.execute_reply":"2022-04-15T22:18:58.083138Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"get_image_and_annot_info(train_df, FOLD, val_ids)\nwith open(f'val_dataset_fold_{FOLD}.json', 'w') as f:\n    output_json = json.dumps(output_json_dict, default=np_encoder)\n    f.write(output_json)","metadata":{"execution":{"iopub.status.busy":"2022-04-15T22:18:59.068251Z","iopub.execute_input":"2022-04-15T22:18:59.068513Z","iopub.status.idle":"2022-04-15T22:20:24.368307Z","shell.execute_reply.started":"2022-04-15T22:18:59.068485Z","shell.execute_reply":"2022-04-15T22:20:24.367237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Verify Coco Json Files**","metadata":{}},{"cell_type":"code","source":"from pycocotools.coco import COCO\nimport matplotlib.pyplot as plt\nfrom pathlib import Path\nfrom PIL import Image","metadata":{"execution":{"iopub.status.busy":"2022-04-15T22:21:47.092886Z","iopub.execute_input":"2022-04-15T22:21:47.093158Z","iopub.status.idle":"2022-04-15T22:21:47.098459Z","shell.execute_reply.started":"2022-04-15T22:21:47.09313Z","shell.execute_reply":"2022-04-15T22:21:47.097476Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"annFile = Path(f'./train_dataset_fold_{FOLD}.json')\ncoco = COCO(annFile)\nimgIds = coco.getImgIds()","metadata":{"execution":{"iopub.status.busy":"2022-04-15T22:22:10.230697Z","iopub.execute_input":"2022-04-15T22:22:10.231594Z","iopub.status.idle":"2022-04-15T22:22:10.733638Z","shell.execute_reply.started":"2022-04-15T22:22:10.231552Z","shell.execute_reply":"2022-04-15T22:22:10.732702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"imgs = coco.loadImgs(imgIds[65:69])\n_,axs = plt.subplots(len(imgs),2,figsize=(40,15 * len(imgs)))\nfor img, ax in zip(imgs, axs):\n#     I = Image.open(img['file_name'])\n    I = Image.fromarray(np.array(Image.open(img['file_name'])).astype(\"uint16\"))\n    annIds = coco.getAnnIds(imgIds=[img['id']])\n    anns = coco.loadAnns(annIds)\n    ax[0].imshow(I)\n    ax[1].imshow(I)\n    plt.sca(ax[1])\n    coco.showAnns(anns, draw_bbox=True)","metadata":{"execution":{"iopub.status.busy":"2022-04-15T22:30:33.091367Z","iopub.execute_input":"2022-04-15T22:30:33.091928Z","iopub.status.idle":"2022-04-15T22:30:38.165578Z","shell.execute_reply.started":"2022-04-15T22:30:33.091891Z","shell.execute_reply":"2022-04-15T22:30:38.164249Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"An upvote would be appreciated! :)","metadata":{}}]}