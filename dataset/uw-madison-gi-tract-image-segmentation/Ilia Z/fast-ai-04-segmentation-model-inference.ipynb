{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Using the `fastai` library for Inference (aka \"Production\") \n\nThis notebook is a final step of a small segmentation tutorial started [here](https://www.kaggle.com/code/purplejester/fast-ai-01-basic-segmentation-model-training). We load the trained segmentation model and use it to forecast newly encountered images.\n\nAs medical images is a rather specific domain, we take a small shortcut here and instead of using brand-new images, just go with the same data that was used for training. Also, we do our \"production\" run on Kaggle which is quite different from doing it on a dedicated service running somewhere on cloud. (So the title of this notebook should be taken with a grain of salt...) However, the logic shouldn't be too different. It is all about loading pretrained weights and running it on images loaded from a persistent storage, posted via HTTP request, or anything similar.\n\n> **Note:** if you refer to the [fast.ai course's lectures](course.fast.ai), you'll find a more \"real\" example of model's deployment, like running it via Gradle, Streamlit, or a dedicated website.\n\n# Import","metadata":{}},{"cell_type":"code","source":"import logging\nfrom pathlib import Path\nfrom fastai.vision.all import *\n\nlogging.captureWarnings(True)","metadata":{"execution":{"iopub.status.busy":"2022-05-09T15:36:16.547838Z","iopub.execute_input":"2022-05-09T15:36:16.548588Z","iopub.status.idle":"2022-05-09T15:36:17.585502Z","shell.execute_reply.started":"2022-05-09T15:36:16.548486Z","shell.execute_reply":"2022-05-09T15:36:17.584668Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DATA_DIR = Path(\"/kaggle/input/uw-madison-gi-tract-image-segmentation/\")","metadata":{"execution":{"iopub.status.busy":"2022-05-09T15:36:17.587066Z","iopub.execute_input":"2022-05-09T15:36:17.587306Z","iopub.status.idle":"2022-05-09T15:36:17.594365Z","shell.execute_reply.started":"2022-05-09T15:36:17.587271Z","shell.execute_reply":"2022-05-09T15:36:17.593617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading a Pretrained Model\n\n## Training Code\nThe model loaded here was trained on a different machine and uploaded to Kaggle. The following snippet shows how the training code looks. (Nothing fancy!)\n\n```python\nimport logging\nfrom pathlib import Path\nfrom fastai.vision.all import *\nfrom nbs.fast_ai_utils import get_dataset_size, COMBINED_MASK_CODES, OUTPUT_DIR\n\nDATASET_SIZE = get_dataset_size(debug=False)\n\nlogging.captureWarnings(True)\n\n\ndef get_items(source_dir: Path):\n    return get_image_files(source_dir.joinpath(\"images\"))[:DATASET_SIZE]\n\n\ndef get_y(fn: Path):\n    return fn.parent.parent.joinpath(\"masks\").joinpath(f\"{fn.stem}.png\")\n\n\ndef train():\n    seg = DataBlock(blocks=(ImageBlock, MaskBlock(COMBINED_MASK_CODES)),\n                    get_items=get_items,\n                    get_y=get_y,\n                    splitter=RandomSplitter(),\n                    item_tfms=[Resize(192, method=\"squash\")])\n\n    dls = seg.dataloaders(OUTPUT_DIR, bs=50)\n    learn = unet_learner(dls, resnet18, metrics=DiceMulti)\n    learn.fine_tune(20)\n    learn.save(\"unet_resnet18_e20\")\n\n\nif __name__ == '__main__':\n    train()\n```\n\n## Restoring the Exported Model\n\nNote that the `fastai` library saves both the model and data transformations. Therefore, it is important to include into inference code the same classes and functions that were using during training/validation. Otherwise, reading of a pretrained model (deserialization) fails. \n\nAlso, I had to patch `PIL.Image` as the model complained about missing `PIL.Image.Resampling` attribute. I'm not sure why it happened, maybe some versions conflict. This is something to be aware of when you decide to deploy a model: different library versions can lead to crashes or deteriorated performance. I remember a case when I used different versions of the same vision models' library during training and inference, and got different results even though the model's checkpoints were same.","metadata":{}},{"cell_type":"code","source":"import PIL.Image\nfrom enum import IntEnum\n\nclass Resampling(IntEnum):\n    NEAREST = 0\n    BOX = 4\n    BILINEAR = 2\n    HAMMING = 5\n    BICUBIC = 3\n    LANCZOS = 1\n    \nPIL.Image.Resampling = Resampling\n\ndef get_items(source_dir: Path): return get_image_files(source_dir.joinpath(\"images\"))\n\ndef get_y(fn: Path): return fn.parent.parent.joinpath(\"masks\").joinpath(f\"{fn.stem}.png\")\n\nlearn = load_learner(\"/kaggle/input/uwm-models/unet_resnet18_e20.pth\")","metadata":{"execution":{"iopub.status.busy":"2022-05-09T15:36:18.186262Z","iopub.execute_input":"2022-05-09T15:36:18.186487Z","iopub.status.idle":"2022-05-09T15:36:18.294346Z","shell.execute_reply.started":"2022-05-09T15:36:18.186462Z","shell.execute_reply":"2022-05-09T15:36:18.293537Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inference\n\nOk, the model was successfully loaded, let's try to run it on some images.","metadata":{}},{"cell_type":"code","source":"png_files = get_image_files(DATA_DIR)\n\nsome_files = np.random.choice(png_files, size=20, replace=False)\n\nwith learn.no_bar():    \n    \n    predicted = []\n\n    for fn in some_files:\n        mask, *_ = learn.predict(fn)\n        predicted.append(mask.numpy())","metadata":{"execution":{"iopub.status.busy":"2022-05-09T15:37:34.438612Z","iopub.execute_input":"2022-05-09T15:37:34.438875Z","iopub.status.idle":"2022-05-09T15:37:45.194149Z","shell.execute_reply.started":"2022-05-09T15:37:34.438845Z","shell.execute_reply":"2022-05-09T15:37:45.193404Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The model [returns multiple tensors](https://docs.fast.ai/learner.html#Learner.predict) but we need only the first one which contains predicted segmentation mask. The following code renders images overlayed with the forecasted masks to see what was predicted.","metadata":{}},{"cell_type":"code","source":"from skimage import color\nfrom skimage import transform\nfrom skimage import exposure\n\ndef mask_overlay(image_file: str, mask_file: str, mask_alpha: float = 0.5) -> PIL.Image:\n    \"\"\"Overlay a mask on top of an image.\"\"\"\n\n    img, seg = [\n        np.asarray(PIL.Image.open(fn)) \n        for fn in (image_file, mask_file)\n    ]  # type: ignore\n\n    img = np.amax(img) - img\n\n    return color.label2rgb(seg, img, kind=\"overlay\", alpha=mask_alpha)\n\ndef equalize(data: np.array, adaptive: bool) -> np.ndarray:\n    \"\"\"Histogram equalization to normalize images before previewing.\"\"\"\n    \n    data = data - np.min(data)\n    data = data / np.max(data)\n    data = (data * 255).astype(np.uint8)\n    method = (\n        exposure.equalize_adapthist\n        if adaptive\n        else exposure.equalize_hist\n    )\n    return method(data)\n\nfigsize_mult = 4\n\nn_rows, n_cols = 5, 4\n\nf, axes = plt.subplots(\n    n_rows, n_cols,\n    figsize=(n_rows * figsize_mult, n_cols * figsize_mult)\n)\n\nfor fn, mask, ax in zip(some_files, predicted, axes.flat):\n    img = np.asarray(PIL.Image.open(fn))\n    img = equalize(img, True)\n    img = transform.resize(img, mask.shape)\n    img = np.amax(img) - img\n    overlay = color.label2rgb(mask, img, kind=\"overlay\", alpha=0.5)\n    overlay = transform.resize(overlay, (128, 128))\n    ax.imshow(overlay)\n    ax.set_axis_off()","metadata":{"execution":{"iopub.status.busy":"2022-05-09T15:54:15.345771Z","iopub.execute_input":"2022-05-09T15:54:15.346051Z","iopub.status.idle":"2022-05-09T15:54:17.092391Z","shell.execute_reply.started":"2022-05-09T15:54:15.346019Z","shell.execute_reply":"2022-05-09T15:54:17.090958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We see that many images don't have any predictions but the ones that have it look reasonable: they show segmentation masks within body scans and also show shapes that resemble the original masks. Of course, it is far away from the perfect model, but could be a good start!\n\n# Conclusion\n\nI hope this and previous notebooks helped you to get a short overview of the `fastai` library and its capabilities. I still have a quite basic knowledge about its functionality but this little series of experiments gave me a good impression about this tool. Compared to some other frameworks, it helps to quickly jump into modelling and get great out-of-the-box results.\n\nGood luck with your data projects, and see you on the [forums](forums.fast.ai)!","metadata":{}}]}