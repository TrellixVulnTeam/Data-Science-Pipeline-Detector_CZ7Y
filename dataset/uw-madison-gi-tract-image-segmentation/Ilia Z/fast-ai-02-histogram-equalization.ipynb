{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Preparing a Dataset to Build a Segmentation Model\n\nIn the previous notebook, we created a simple segmentation model using prepared dataset. However, there was no source code for dataset preparation, but only the result. In this notebook, I include the code I used to prepare the data with one major change: [histogram equalization](https://en.wikipedia.org/wiki/Histogram_equalization).\n\n[As the previous notebook shows](https://www.kaggle.com/code/purplejester/fast-ai-01-basic-segmentation-model-training), some of the training samples are rather strange. A white blob covering huge area of an image without any distinguishable parts of an X-ray scan. To solve this issue, we need to normalize samples to ensure that each of them has a similar distribution of pixel values. As in the \"raw\" data, some samples are bright, while others are dim, and so on.\n\nThe notebook creates a new dataset that (hopefully) works better for our model.\n\n# Imports","metadata":{}},{"cell_type":"code","source":"import os\nfrom dataclasses import dataclass, field, asdict\nfrom pprint import pprint as pp\nfrom pathlib import Path\nfrom typing import List\n\nimport altair as alt\nimport cv2 as cv\nimport pandas as pd\nimport PIL.Image\nfrom skimage import exposure\n\nfrom fastai.vision.all import *\nfrom fastcore.all import *\nfrom fastprogress import progress_bar\nfrom fast_ai_utils import rle_decode, get_dataset_size, COMBINED_MASK_CODES, DATA_ROOT, OUTPUT_DIR, TRAIN_CSV","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-04T20:06:06.414355Z","iopub.execute_input":"2022-05-04T20:06:06.414632Z","iopub.status.idle":"2022-05-04T20:06:10.263427Z","shell.execute_reply.started":"2022-05-04T20:06:06.4146Z","shell.execute_reply":"2022-05-04T20:06:10.26262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"get_dataset_size()  # hard-coded from the number of samples in the \"raw\" dataset","metadata":{"execution":{"iopub.status.busy":"2022-05-04T20:06:13.602311Z","iopub.execute_input":"2022-05-04T20:06:13.603192Z","iopub.status.idle":"2022-05-04T20:06:17.756153Z","shell.execute_reply.started":"2022-05-04T20:06:13.60315Z","shell.execute_reply":"2022-05-04T20:06:17.755459Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"png_files = get_image_files(DATA_ROOT)\npng_files[np.random.randint(0, get_dataset_size(), size=5)]","metadata":{"execution":{"iopub.status.busy":"2022-05-04T20:06:17.757606Z","iopub.execute_input":"2022-05-04T20:06:17.757845Z","iopub.status.idle":"2022-05-04T20:06:19.252478Z","shell.execute_reply.started":"2022-05-04T20:06:17.757811Z","shell.execute_reply":"2022-05-04T20:06:19.251776Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_labels = pd.read_csv(TRAIN_CSV)\ndf_labels.sample(5)","metadata":{"execution":{"iopub.status.busy":"2022-05-04T20:06:19.253784Z","iopub.execute_input":"2022-05-04T20:06:19.254185Z","iopub.status.idle":"2022-05-04T20:06:19.773923Z","shell.execute_reply.started":"2022-05-04T20:06:19.254147Z","shell.execute_reply":"2022-05-04T20:06:19.77319Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Before we start preparing the dataset, let's check a bit its properities.","metadata":{}},{"cell_type":"code","source":"(\n    alt.Chart(\n        data=(\n            df_labels.segmentation.isna().value_counts().reset_index()\n            .rename(columns={\"index\": \"Has Mask?\", \"segmentation\": \"Count\"})\n        )\n    )\n    .mark_bar()\n    .encode(y=\"Has Mask?:O\", x=\"Count\", color=\"Has Mask?\")\n)","metadata":{"execution":{"iopub.status.busy":"2022-05-04T20:06:34.898439Z","iopub.execute_input":"2022-05-04T20:06:34.898983Z","iopub.status.idle":"2022-05-04T20:06:34.960704Z","shell.execute_reply.started":"2022-05-04T20:06:34.898944Z","shell.execute_reply":"2022-05-04T20:06:34.960044Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"(\n    alt.Chart(\n        data=(\n            df_labels[\"class\"].value_counts(dropna=False).reset_index()\n            .rename(columns={\"index\": \"Class\", \"class\": \"Count\"})\n        )\n    )\n    .mark_bar()\n    .encode(y=\"Class:O\", x=\"Count\", color=\"Class\")\n)","metadata":{"execution":{"iopub.status.busy":"2022-05-04T20:06:35.899121Z","iopub.execute_input":"2022-05-04T20:06:35.899383Z","iopub.status.idle":"2022-05-04T20:06:35.963021Z","shell.execute_reply.started":"2022-05-04T20:06:35.899354Z","shell.execute_reply":"2022-05-04T20:06:35.962219Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"First of all, a significant part of the images don't have any segmentation mask. Second, the segmentation classes distributed equally among samples. Which sounds like a good thing as we don't need to care much about classes imbalance.","metadata":{}},{"cell_type":"code","source":"UNIQUE_CLASSES = df_labels[\"class\"].unique().tolist()\nUNIQUE_CLASSES","metadata":{"execution":{"iopub.status.busy":"2022-05-04T20:06:49.413657Z","iopub.execute_input":"2022-05-04T20:06:49.413919Z","iopub.status.idle":"2022-05-04T20:06:49.428195Z","shell.execute_reply.started":"2022-05-04T20:06:49.413891Z","shell.execute_reply":"2022-05-04T20:06:49.42729Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Also, we have only three segmentation classes. But could it be that some of the segmentation masks overlap? We'll check it soon. But first of all, we'll read image properties and prepare samples.\n\n# Reading image properties\n\nIn the dataset we use, each file path includes additional information about a sample it points to. We can use this information when building a model. For example, to come up with a reliable cross-validation scheme. Therefore, we parse this information from files and store into a convenice object called `ImageProperties`.","metadata":{}},{"cell_type":"code","source":"@dataclass\nclass ImageProperties:\n    slice_id: int\n    h: int\n    w: int\n    px_h: float\n    px_w: float\n    rel_path: str\n    case: int = field(init=False)\n    day: int = field(init=False)\n    sample_id: str = field(init=False)\n        \n    def __post_init__(self):\n        case_str, day_str, *_ = self.rel_path.split(\"/\")\n        self.case = int(case_str.replace(\"case\", \"\"))\n        self.day = int(day_str.split(\"_\")[-1].replace(\"day\", \"\"))\n        self.sample_id = f\"case{self.case}_day{self.day}_slice_{self.slice_id:04d}\"\n\ndef get_image_properties(path: Path) -> ImageProperties:\n    rel_path = path.relpath(DATA_ROOT/\"train\")\n    _, *parts = path.stem.split(\"_\")\n    slice_id, h, w = map(int, parts[:3])\n    px_h, px_w = map(float, parts[3:])\n    return ImageProperties(slice_id, h, w, px_h, px_w, str(rel_path))","metadata":{"execution":{"iopub.status.busy":"2022-05-04T20:06:50.976387Z","iopub.execute_input":"2022-05-04T20:06:50.977176Z","iopub.status.idle":"2022-05-04T20:06:50.987237Z","shell.execute_reply.started":"2022-05-04T20:06:50.977123Z","shell.execute_reply":"2022-05-04T20:06:50.986311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ncached_file = OUTPUT_DIR/\"image_props.csv\"\n\n# uncomment the following line to force re-reading properties (used for debugging)\n# cached_file.unlink()\n\nif not OUTPUT_DIR.exists():\n    OUTPUT_DIR.mkdir()\n    \nif cached_file.exists():\n    print(f\"Reading image properties from cache: {cached_file}\")\n    df_props = pd.read_csv(cached_file)\n    props = [\n        ImageProperties(**{\n            k: v for \n            k, v in t._asdict().items() \n            if k not in (\"case\", \"day\", \"sample_id\")\n        })\n        for t in df_props.itertuples(index=False)\n    ]\n    \nelse:\n    print(\"Extracting metadata from image names...\", end=\"\")\n    props = list(map(get_image_properties, png_files))\n    df_props = pd.DataFrame(props)\n    df_props.to_csv(cached_file, index=False)\n    print(\"done!\")","metadata":{"execution":{"iopub.status.busy":"2022-05-04T20:06:51.52688Z","iopub.execute_input":"2022-05-04T20:06:51.527132Z","iopub.status.idle":"2022-05-04T20:06:51.972969Z","shell.execute_reply.started":"2022-05-04T20:06:51.527104Z","shell.execute_reply":"2022-05-04T20:06:51.972227Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> **Note:** a trivial caching logic shown above isn't strictly required here, but it helped to debug the snippet. So it could be a good idea to include such conditional processing steps into a notebook to iterate quicker. Also, we store the retrieved properties in the working directory to include into the output of this notebook and use it later.","metadata":{}},{"cell_type":"code","source":"df_props = pd.read_csv(cached_file)\ndf_props.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-04T20:06:52.396447Z","iopub.execute_input":"2022-05-04T20:06:52.397003Z","iopub.status.idle":"2022-05-04T20:06:52.482113Z","shell.execute_reply.started":"2022-05-04T20:06:52.396967Z","shell.execute_reply":"2022-05-04T20:06:52.48143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MASK_CODES = dict(zip(UNIQUE_CLASSES, (0b001, 0b010, 0b100)))\nMASK_CODES","metadata":{"execution":{"iopub.status.busy":"2022-05-04T20:06:53.319481Z","iopub.execute_input":"2022-05-04T20:06:53.320032Z","iopub.status.idle":"2022-05-04T20:06:53.325067Z","shell.execute_reply.started":"2022-05-04T20:06:53.319994Z","shell.execute_reply":"2022-05-04T20:06:53.3244Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Equalizing samples and building segmentation masks\n\nAs we retrieved the metadata information, we're ready to start processing the images. For this purpose, we use `scikit-image` package that includes many useful utilities for this kind of work. In essense, our histogram equalization procedure should make samples more similar to each other, without extremely bright or dim outliers.","metadata":{}},{"cell_type":"code","source":"image_sizes = df_props.set_index(\"sample_id\")[[\"h\", \"w\"]].to_dict(\"index\")","metadata":{"execution":{"iopub.status.busy":"2022-05-04T20:06:54.779929Z","iopub.execute_input":"2022-05-04T20:06:54.78036Z","iopub.status.idle":"2022-05-04T20:06:54.912247Z","shell.execute_reply.started":"2022-05-04T20:06:54.780325Z","shell.execute_reply":"2022-05-04T20:06:54.911535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def equalize(data: np.array, adaptive: bool) -> np.ndarray:\n    data = data - np.min(data)\n    data = data / np.max(data)\n    data = (data * 255).astype(np.uint8)\n    method = (\n        exposure.equalize_adapthist\n        if adaptive\n        else exposure.equalize_hist\n    )\n    return method(data)","metadata":{"execution":{"iopub.status.busy":"2022-05-04T20:06:55.285533Z","iopub.execute_input":"2022-05-04T20:06:55.286101Z","iopub.status.idle":"2022-05-04T20:06:55.291322Z","shell.execute_reply.started":"2022-05-04T20:06:55.286065Z","shell.execute_reply":"2022-05-04T20:06:55.290633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time \n\nimages_dir = OUTPUT_DIR/\"images\"\n\nif not images_dir.exists():\n    images_dir.mkdir()\n\nfor prop in progress_bar(props):\n    src_path = DATA_ROOT.joinpath(\"train\").joinpath(prop.rel_path)\n    dst_path = images_dir/f\"{prop.sample_id}.png\"\n    arr = cv.imread(str(src_path), cv.CV_16UC1)\n    arr = equalize(arr, adaptive=True)\n    arr = (arr * 255).astype(np.uint8)\n    pil_image = PIL.Image.fromarray(arr)\n    pil_image.save(dst_path)","metadata":{"execution":{"iopub.status.busy":"2022-05-04T20:06:56.437307Z","iopub.execute_input":"2022-05-04T20:06:56.438139Z","iopub.status.idle":"2022-05-04T20:07:02.435092Z","shell.execute_reply.started":"2022-05-04T20:06:56.438093Z","shell.execute_reply":"2022-05-04T20:07:02.434382Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\nempty_masks = set()\n\nmasks_dir = OUTPUT_DIR/\"masks\"\n\nif not masks_dir.exists():\n    masks_dir.mkdir()\n\nfor sample_id, df_group in progress_bar(df_labels.groupby(\"id\")):\n    size = image_sizes[sample_id]\n    shape = size[\"h\"], size[\"w\"]\n    decoded_mask = np.zeros(shape, dtype=np.uint8)\n\n    for _, row in df_group.iterrows():\n        rle_mask = row.segmentation\n        if isinstance(rle_mask, str):\n            decoded_mask |= rle_decode(rle_mask, shape, value=MASK_CODES[row[\"class\"]])\n    \n    if not decoded_mask.any():\n        empty_masks.add(sample_id)\n    \n    mask_image = PIL.Image.fromarray(decoded_mask.T)\n    mask_path = masks_dir/f\"{row['id']}.png\"\n    mask_image.save(mask_path)","metadata":{"execution":{"iopub.status.busy":"2022-05-04T20:07:02.436773Z","iopub.execute_input":"2022-05-04T20:07:02.437232Z","iopub.status.idle":"2022-05-04T20:07:07.013152Z","shell.execute_reply.started":"2022-05-04T20:07:02.437195Z","shell.execute_reply":"2022-05-04T20:07:07.012365Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Sanity check\n\nThe dataset is ready, but it is always a good idea to check if everything looks as expected. For this purpose, we train a model on a random subset of the data and visualize samples, and predictions one more time. ","metadata":{}},{"cell_type":"code","source":"def get_items(source_dir: Path):\n    return get_image_files(source_dir.joinpath(\"images\"))[:get_dataset_size()]\n\ndef get_y(fn: Path):\n    return fn.parent.parent.joinpath(\"masks\").joinpath(f\"{fn.stem}.png\")\n\nseg = DataBlock(blocks=(ImageBlock, MaskBlock(COMBINED_MASK_CODES)),\n                get_items=get_items,\n                get_y=get_y,\n                splitter=RandomSplitter(),\n                item_tfms=[Resize(192, method=\"squash\")])\n\ndls = seg.dataloaders(OUTPUT_DIR, bs=32)\n\nlearn = unet_learner(dls, resnet18, metrics=DiceMulti)\n\nlearn.fine_tune(3)\n\ninterp = SegmentationInterpretation.from_learner(learn)\n\ninterp.plot_top_losses(4, largest=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-04T20:07:09.008901Z","iopub.execute_input":"2022-05-04T20:07:09.009178Z","iopub.status.idle":"2022-05-04T20:07:09.890064Z","shell.execute_reply.started":"2022-05-04T20:07:09.00915Z","shell.execute_reply":"2022-05-04T20:07:09.886812Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Much better this time! We see that each sample is a clearly visible X-ray scan, instead of a white blob we had before. \n\n# What's Next?\n\nNow we can use this dataset as our starting point in building of new models, without worrying about bad quality images. Let's see what can be achieved. Stay tuned!","metadata":{}}]}