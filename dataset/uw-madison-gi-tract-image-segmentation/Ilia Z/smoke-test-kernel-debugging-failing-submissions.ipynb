{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# What's Wrong with Submissions?\n\nSoon after the competition started, I created an inference kernel to submit predictions. However, all of my attempts failed. I know there are public inference kernels that work, but I don't want to simply copy-paste someone's pipeline and would prefer to use my own instead. That's why I created this \"smoke test\" kernel. I want to understand why submissions fail on private test set.\n\nIn this kernel, I'm trying to debug submission issues in order to understand what's wrong with my submission format. \n\nSo far, I collected the following findings, which I describe more details in the following section.\n\n1. ✅ Submission works if using all-zeros masks -> empty RLE strings\n2. ✅ Submission works if using a hard-coded RLE string like \"1 10 20 5\"\n3. ✅ Submission works if fill only the central part of the mask, like 3/4-th of it\n4. ❌ Submission FAILS in case if all-ones mask is submitted\n5. ❌ Submission FAILS if all-ones except thin margin mask is submitted\n\n## Details\nFrom the third case, we can guess that for some samples, all-ones masks leads to failure. The all-ones mask is easily encoded into RLE format: it is a string that has the following format.\n```python\nimg = read_image(filename)\nh, w = img.shape\nrle_mask = f\"1 {h * w}\"\n```\nAs we know the exact format of the encoding in this case, we don't need to use any encoding function for that. Therefore, we can assume that the encoded mask is valid. However, the submission fails. And if this kind of very simple string fails, it means that the scoring algorithm probably expects a mask of a different shape for some (many?) of the test samples. How could it happen? One possible explanation is that some file paths encode wrong information about image size in their names. However, the same failure happens in the case if I derive the mask's shape directly from the image shape, like the following snippet shows.\n```python\nh, w = PIL.Image.open(filename).size\n```\nThen it means that the scoring system erroneously treats (?) some images as being of a smaller size than they are in reality. Therefore, the predicted mask doesn't fit, as it is too big, and the number of one values represented as RLE string goes beyond anticipated image limits.\n\nTo solve this issue, I decided to try different submission formats. And one of them is created as all-zeros mask, and fill its _central area_ with ones. In this way, the RLE-encoded mask should be in expected the limits. And this worked! (Please see the code in the kernel.)\n\nTherefore, my hypothesis is that somehow, the submission checking code fails because of an obscure problem with expected shapes. Maybe I'm wrong, and it is my code faulty. However, I cannot understand why a dummy all-ones submission fails, while a partially-filled mask works.\n\nPlease let me know your thoughts! It is quite upsetting to spend so much time to figure out the submission format instead of doing some real modelling. Not that this is the first competition that has a bit of challenging submission format. But each and every time it happens again, so would be great to make it easier. Thank you!","metadata":{}},{"cell_type":"code","source":"import logging\nfrom dataclasses import asdict, dataclass, field\nfrom pathlib import Path\n\nimport pandas\nfrom fastai.vision.all import *  # get_image_files()\nfrom fast_ai_utils import rle_numba_encode\n\nlogging.captureWarnings(True)","metadata":{"execution":{"iopub.status.busy":"2022-05-12T09:32:18.262308Z","iopub.execute_input":"2022-05-12T09:32:18.263165Z","iopub.status.idle":"2022-05-12T09:32:18.268829Z","shell.execute_reply.started":"2022-05-12T09:32:18.263123Z","shell.execute_reply":"2022-05-12T09:32:18.267687Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@dataclass\nclass Metadata:\n    sample_id: str\n    full_path: str\n    h: int\n    w: int\n    \n    @classmethod\n    def extract(cls, path: Path) -> \"Metadata\":\n        case_and_day = path.parents[1].stem\n        _, slice_no, h, w, *_ = path.stem.split(\"_\")\n        sample_id = f\"{case_and_day}_slice_{int(slice_no):04d}\"\n        return Metadata(sample_id, str(path), int(h), int(w))","metadata":{"execution":{"iopub.status.busy":"2022-05-12T09:32:18.392007Z","iopub.execute_input":"2022-05-12T09:32:18.392366Z","iopub.status.idle":"2022-05-12T09:32:18.401976Z","shell.execute_reply.started":"2022-05-12T09:32:18.392333Z","shell.execute_reply":"2022-05-12T09:32:18.400797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DATA_DIR = Path(\"/kaggle/input/uw-madison-gi-tract-image-segmentation/\")\n\nDEBUG = !cat {DATA_DIR}/sample_submission.csv | test $(wc -l) -eq 1 && echo 1\n\nTEST_IDS = pd.read_csv(DATA_DIR/(\"train.csv\" if DEBUG else \"sample_submission.csv\"))[\"id\"].drop_duplicates().tolist()\n\nTEST_FILES = get_image_files(DATA_DIR/(\"train\" if DEBUG else \"test\"))\n\nMETADATA = {m.sample_id: m for m in TEST_FILES.map(Metadata.extract)}","metadata":{"execution":{"iopub.status.busy":"2022-05-12T09:32:18.648756Z","iopub.execute_input":"2022-05-12T09:32:18.64956Z","iopub.status.idle":"2022-05-12T09:32:20.336033Z","shell.execute_reply.started":"2022-05-12T09:32:18.649513Z","shell.execute_reply":"2022-05-12T09:32:20.334812Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_metadata = pd.DataFrame([asdict(m) for m in METADATA.values()])","metadata":{"execution":{"iopub.status.busy":"2022-05-12T09:32:20.338447Z","iopub.execute_input":"2022-05-12T09:32:20.339063Z","iopub.status.idle":"2022-05-12T09:32:21.022832Z","shell.execute_reply.started":"2022-05-12T09:32:20.339023Z","shell.execute_reply":"2022-05-12T09:32:21.021831Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.crosstab(df_metadata[\"h\"], df_metadata[\"w\"])","metadata":{"execution":{"iopub.status.busy":"2022-05-12T09:32:21.024199Z","iopub.execute_input":"2022-05-12T09:32:21.024489Z","iopub.status.idle":"2022-05-12T09:32:21.057824Z","shell.execute_reply.started":"2022-05-12T09:32:21.024437Z","shell.execute_reply":"2022-05-12T09:32:21.056872Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_metadata.apply(lambda row: row.h * row.w, axis=1).value_counts().plot.bar();","metadata":{"execution":{"iopub.status.busy":"2022-05-12T09:32:21.059861Z","iopub.execute_input":"2022-05-12T09:32:21.060217Z","iopub.status.idle":"2022-05-12T09:32:22.331912Z","shell.execute_reply.started":"2022-05-12T09:32:21.060182Z","shell.execute_reply":"2022-05-12T09:32:22.330797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.read_csv(DATA_DIR/(\"train.csv\" if DEBUG else \"sample_submission.csv\"))[\"id\"]","metadata":{"execution":{"iopub.status.busy":"2022-05-12T09:32:22.333428Z","iopub.execute_input":"2022-05-12T09:32:22.333766Z","iopub.status.idle":"2022-05-12T09:32:22.675174Z","shell.execute_reply.started":"2022-05-12T09:32:22.333722Z","shell.execute_reply":"2022-05-12T09:32:22.674448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from enum import IntEnum\n\nclass TestMethod(IntEnum):\n    ALL_ZEROS = 0\n    ALL_ONES = 1\n    FIXED = 2\n    ALL_ONES_FROM_IMAGE = 3\n    ALL_ONES_MARGIN_05 = 4\n    ALL_ONES_MARGIN_10 = 5\n    CENTER = 6\n    \nSELECTED_METHOD = TestMethod.CENTER","metadata":{"execution":{"iopub.status.busy":"2022-05-12T09:32:22.676395Z","iopub.execute_input":"2022-05-12T09:32:22.677007Z","iopub.status.idle":"2022-05-12T09:32:22.683971Z","shell.execute_reply.started":"2022-05-12T09:32:22.676957Z","shell.execute_reply":"2022-05-12T09:32:22.682563Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from fastprogress import progress_bar\n\npreds = []\n\nfor test_id in progress_bar(TEST_IDS):\n\n    case = METADATA[test_id]\n\n    if SELECTED_METHOD == TestMethod.ALL_ZEROS:\n        # ✅ works!\n        mask = np.zeros((case.h, case.w), dtype=np.uint8)\n        rle_string = rle_numba_encode(mask)\n\n    elif SELECTED_METHOD == TestMethod.ALL_ONES:\n        # ❌ failed\n        mask = np.ones((case.h, case.w), dtype=np.uint8)\n        rle_string = rle_numba_encode(mask)\n\n    elif SELECTED_METHOD == TestMethod.ALL_ONES_FROM_IMAGE:\n        # ❌ failed\n        img = PIL.Image.open(case.full_path)  \n        h, w = img.shape\n        rle_string = f\"1 {h * w}\"\n\n    elif SELECTED_METHOD == TestMethod.ALL_ONES_MARGIN_05:\n        # ❌ failed\n        mask = np.zeros((case.h, case.w), dtype=np.uint8)\n        mask[5:-5, 5:-5] = 1\n        rle_string = rle_numba_encode(mask)\n\n    elif SELECTED_METHOD == TestMethod.ALL_ONES_MARGIN_10:\n        # ❌ failed\n        mask = np.zeros((case.h, case.w), dtype=np.uint8)\n        mask[10:-10, 10:-10] = 1\n        rle_string = rle_numba_encode(mask)\n\n    elif SELECTED_METHOD == TestMethod.CENTER:\n        # ✅ works!\n        h, w = case.h, case.w\n        h_center, w_center = h // 2, w // 2\n        h_margin, w_margin = h // 4, w // 4\n        mask = np.zeros((h, w), dtype=np.uint8)\n        mask[h_center - h_margin:h_center + h_margin, w_center - w_margin:w_center + w_margin] = 1\n        rle_string = rle_numba_encode(mask)\n\n    else:\n        # ✅ works!\n        rle_string = \"1 10 20 5\"\n\n    for name in (\"large_bowel\", \"small_bowel\", \"stomach\"):\n        preds.append({\n            \"id\": test_id,\n            \"class\": name,\n            \"predicted\": rle_string\n        })","metadata":{"execution":{"iopub.status.busy":"2022-05-12T09:32:22.685443Z","iopub.execute_input":"2022-05-12T09:32:22.685762Z","iopub.status.idle":"2022-05-12T09:32:28.550761Z","shell.execute_reply.started":"2022-05-12T09:32:22.685726Z","shell.execute_reply":"2022-05-12T09:32:28.549767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_preds = pd.DataFrame(preds)\ndf_preds.head(10)","metadata":{"execution":{"iopub.status.busy":"2022-05-12T09:32:28.551957Z","iopub.execute_input":"2022-05-12T09:32:28.552196Z","iopub.status.idle":"2022-05-12T09:32:28.660668Z","shell.execute_reply.started":"2022-05-12T09:32:28.552166Z","shell.execute_reply":"2022-05-12T09:32:28.659672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_submit = pd.read_csv(DATA_DIR/\"sample_submission.csv\")\ndf_submit = df_submit.drop(columns=\"predicted\").merge(df_preds, on=[\"id\", \"class\"], how=\"left\")\ndf_submit.to_csv(\"submission.csv\", index=False)\npd.read_csv(\"submission.csv\").head(10)","metadata":{"execution":{"iopub.status.busy":"2022-05-12T09:32:31.251684Z","iopub.execute_input":"2022-05-12T09:32:31.252001Z","iopub.status.idle":"2022-05-12T09:32:31.308852Z","shell.execute_reply.started":"2022-05-12T09:32:31.251969Z","shell.execute_reply":"2022-05-12T09:32:31.307948Z"},"trusted":true},"execution_count":null,"outputs":[]}]}