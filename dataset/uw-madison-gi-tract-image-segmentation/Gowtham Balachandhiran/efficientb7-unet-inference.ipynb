{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-06-28T04:20:37.401078Z","iopub.execute_input":"2022-06-28T04:20:37.401766Z","iopub.status.idle":"2022-06-28T04:20:41.506001Z","shell.execute_reply.started":"2022-06-28T04:20:37.401661Z","shell.execute_reply":"2022-06-28T04:20:41.505055Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from PIL import Image\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nfrom tqdm.notebook import tqdm\nfrom datetime import datetime\nimport json,itertools\nfrom typing import Optional\nfrom glob import glob\n\nfrom sklearn.model_selection import StratifiedKFold, KFold, StratifiedGroupKFold\n\nfrom tensorflow import keras\nimport tensorflow as tf\nimport keras\nfrom keras import backend as K\nfrom keras.models import Model\nfrom keras.layers import Input\nfrom keras.layers.convolutional import Conv2D, Conv2DTranspose\nfrom keras.layers.pooling import MaxPooling2D\nfrom keras.layers.merge import concatenate\nfrom keras.losses import binary_crossentropy\nfrom keras.callbacks import Callback, ModelCheckpoint, EarlyStopping\nfrom keras.models import load_model, save_model\n\nimport matplotlib.gridspec as gridspec\nimport matplotlib.patches as mpatches\nimport matplotlib as mpl","metadata":{"execution":{"iopub.status.busy":"2022-06-28T04:20:41.507732Z","iopub.execute_input":"2022-06-28T04:20:41.508313Z","iopub.status.idle":"2022-06-28T04:20:48.382587Z","shell.execute_reply.started":"2022-06-28T04:20:41.508241Z","shell.execute_reply":"2022-06-28T04:20:48.381668Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train set\ntrain_df = pd.read_csv('../input/uw-madison-gi-tract-image-segmentation/train.csv')\nprint(train_df.shape)\ntrain_df.head()\n","metadata":{"execution":{"iopub.status.busy":"2022-06-28T04:20:48.383721Z","iopub.execute_input":"2022-06-28T04:20:48.384424Z","iopub.status.idle":"2022-06-28T04:20:49.056876Z","shell.execute_reply.started":"2022-06-28T04:20:48.384389Z","shell.execute_reply":"2022-06-28T04:20:49.055613Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Test set\ntest_df = pd.read_csv('../input/uw-madison-gi-tract-image-segmentation/sample_submission.csv')\n\nif len(test_df)==0:\n    DEBUG=True\n    test_df = train_df.sample(frac =.00001)\n    print(test_df.shape)\n    test_df[\"segmentation\"]=''\n    test_df=test_df.rename(columns={\"segmentation\":\"prediction\"})\nelse:\n    DEBUG=False\n\nsubmission=test_df.copy()\ntest_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-28T04:20:49.059532Z","iopub.execute_input":"2022-06-28T04:20:49.059994Z","iopub.status.idle":"2022-06-28T04:20:49.096435Z","shell.execute_reply.started":"2022-06-28T04:20:49.059948Z","shell.execute_reply":"2022-06-28T04:20:49.095297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Preprocessing","metadata":{}},{"cell_type":"code","source":"# Metadata\ndef preprocessing(df, subset=\"train\"):\n    #--------------------------------------------------------------------------\n    df[\"case\"] = df[\"id\"].apply(lambda x: int(x.split(\"_\")[0].replace(\"case\", \"\")))\n    df[\"day\"] = df[\"id\"].apply(lambda x: int(x.split(\"_\")[1].replace(\"day\", \"\")))\n    df[\"slice\"] = df[\"id\"].apply(lambda x: x.split(\"_\")[3])\n    #--------------------------------------------------------------------------\n    if (subset==\"train\") or (DEBUG):\n        DIR=\"../input/uw-madison-gi-tract-image-segmentation/train\"\n    else:\n        DIR=\"../input/uw-madison-gi-tract-image-segmentation/test\"\n    \n    all_images = glob(os.path.join(DIR, \"**\", \"*.png\"), recursive=True)\n    \n    x = all_images[0].rsplit(\"/\", 4)[0] ## ../input/uw-madison-gi-tract-image-segmentation/train\n\n    path_partial_list = []\n    for i in range(0, df.shape[0]):\n        path_partial_list.append(os.path.join(x,\n                              \"case\"+str(df[\"case\"].values[i]),\n                              \"case\"+str(df[\"case\"].values[i])+\"_\"+ \"day\"+str(df[\"day\"].values[i]),\n                              \"scans\",\n                              \"slice_\"+str(df[\"slice\"].values[i])))\n    df[\"path_partial\"] = path_partial_list\n    #--------------------------------------------------------------------------\n    path_partial_list = []\n    for i in range(0, len(all_images)):\n        path_partial_list.append(str(all_images[i].rsplit(\"_\",4)[0]))\n\n    tmp_df = pd.DataFrame()\n    tmp_df['path_partial'] = path_partial_list\n    tmp_df['path'] = all_images\n\n    #--------------------------------------------------------------------------\n    df = df.merge(tmp_df, on=\"path_partial\").drop(columns=[\"path_partial\"])\n    print(df.columns)\n    #--------------------------------------------------------------------------\n    df[\"width\"] = df[\"path\"].apply(lambda x: int(x[:-4].rsplit(\"_\",4)[1]))\n    df[\"height\"] = df[\"path\"].apply(lambda x: int(x[:-4].rsplit(\"_\",4)[2]))\n    #--------------------------------------------------------------------------\n    del x, path_partial_list, tmp_df\n    #--------------------------------------------------------------------------\n    \n    return df","metadata":{"execution":{"iopub.status.busy":"2022-06-28T04:20:49.098265Z","iopub.execute_input":"2022-06-28T04:20:49.099388Z","iopub.status.idle":"2022-06-28T04:20:49.117344Z","shell.execute_reply.started":"2022-06-28T04:20:49.099342Z","shell.execute_reply":"2022-06-28T04:20:49.116177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = preprocessing(train_df, subset=\"train\")\n","metadata":{"execution":{"iopub.status.busy":"2022-06-28T04:20:49.118553Z","iopub.execute_input":"2022-06-28T04:20:49.119358Z","iopub.status.idle":"2022-06-28T04:20:54.015698Z","shell.execute_reply.started":"2022-06-28T04:20:49.119326Z","shell.execute_reply":"2022-06-28T04:20:54.014453Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.shape","metadata":{"execution":{"iopub.status.busy":"2022-06-28T04:20:54.017272Z","iopub.execute_input":"2022-06-28T04:20:54.017787Z","iopub.status.idle":"2022-06-28T04:20:54.025613Z","shell.execute_reply.started":"2022-06-28T04:20:54.01774Z","shell.execute_reply":"2022-06-28T04:20:54.024455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df=preprocessing(test_df, subset=\"test\")\n","metadata":{"execution":{"iopub.status.busy":"2022-06-28T04:20:54.027377Z","iopub.execute_input":"2022-06-28T04:20:54.027831Z","iopub.status.idle":"2022-06-28T04:20:54.933954Z","shell.execute_reply.started":"2022-06-28T04:20:54.027786Z","shell.execute_reply":"2022-06-28T04:20:54.932985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Restructure\ndef restructure(df, subset=\"train\"):\n    # RESTRUCTURE  DATAFRAME\n    df_out = pd.DataFrame({'id': df['id'][::3]})\n\n    if subset==\"train\":\n        df_out['large_bowel'] = df['segmentation'][::3].values\n        df_out['small_bowel'] = df['segmentation'][1::3].values\n        df_out['stomach'] = df['segmentation'][2::3].values\n\n    df_out['path'] = df['path'][::3].values\n    df_out['case'] = df['case'][::3].values\n    df_out['day'] = df['day'][::3].values\n    df_out['slice'] = df['slice'][::3].values\n    df_out['width'] = df['width'][::3].values\n    df_out['height'] = df['height'][::3].values\n\n    df_out=df_out.reset_index(drop=True)\n    df_out=df_out.fillna('')\n    if subset==\"train\":\n        df_out['count'] = np.sum(df_out.iloc[:,1:4]!='',axis=1).values\n    \n    return df_out","metadata":{"execution":{"iopub.status.busy":"2022-06-28T04:20:54.945605Z","iopub.execute_input":"2022-06-28T04:20:54.946015Z","iopub.status.idle":"2022-06-28T04:20:54.958857Z","shell.execute_reply.started":"2022-06-28T04:20:54.945983Z","shell.execute_reply":"2022-06-28T04:20:54.957875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df=restructure(train_df, subset=\"train\")\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-28T04:20:54.960409Z","iopub.execute_input":"2022-06-28T04:20:54.96157Z","iopub.status.idle":"2022-06-28T04:20:55.060213Z","shell.execute_reply.started":"2022-06-28T04:20:54.961524Z","shell.execute_reply":"2022-06-28T04:20:55.05934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Helper function","metadata":{}},{"cell_type":"code","source":"# Run-length encoding\ndef rle_encode(img):\n    '''\n    img: numpy array, 1 - mask, 0 - background\n    Returns run length as string formated\n    '''\n    pixels = img.flatten()\n    pixels = np.concatenate([[0], pixels, [0]])\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n    runs[1::2] -= runs[::2]\n    return ' '.join(str(x) for x in runs)\n\ndef rle_decode(mask_rle, shape, color=1):\n    '''\n    mask_rle: run-length as string formated (start length)\n    shape: (height,width) of array to return \n    Returns numpy array, 1 - mask, 0 - background\n\n    '''\n    s = mask_rle.split()\n    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n    starts -= 1\n    ends = starts + lengths\n    img = np.zeros((shape[0] * shape[1], shape[2]), dtype=np.float32)\n    for lo, hi in zip(starts, ends):\n        img[lo : hi] = color\n    return img.reshape(shape)\n","metadata":{"execution":{"iopub.status.busy":"2022-06-28T04:20:55.061342Z","iopub.execute_input":"2022-06-28T04:20:55.062308Z","iopub.status.idle":"2022-06-28T04:20:55.074478Z","shell.execute_reply.started":"2022-06-28T04:20:55.062242Z","shell.execute_reply":"2022-06-28T04:20:55.073284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BATCH_SIZE = 16\nEPOCHS=10","metadata":{"execution":{"iopub.status.busy":"2022-06-28T04:20:55.075774Z","iopub.execute_input":"2022-06-28T04:20:55.076303Z","iopub.status.idle":"2022-06-28T04:20:55.090133Z","shell.execute_reply.started":"2022-06-28T04:20:55.076248Z","shell.execute_reply":"2022-06-28T04:20:55.089345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nfrom sklearn.model_selection import StratifiedKFold, KFold, StratifiedGroupKFold","metadata":{"execution":{"iopub.status.busy":"2022-06-28T04:20:55.09134Z","iopub.execute_input":"2022-06-28T04:20:55.092412Z","iopub.status.idle":"2022-06-28T04:20:55.101833Z","shell.execute_reply.started":"2022-06-28T04:20:55.092337Z","shell.execute_reply":"2022-06-28T04:20:55.100721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Do a folded split","metadata":{}},{"cell_type":"code","source":"n_splits = 5\n\n\nfold_selected=2\n\n\nskf = StratifiedGroupKFold(n_splits=n_splits, shuffle=True, random_state=42)\nfor fold, (_, val_idx) in enumerate(skf.split(X=train_df, y=train_df['count'], groups=train_df['case']), 1):\n    train_df.loc[val_idx, 'fold'] = fold\n    \ntrain_df['fold'] = train_df['fold'].astype(np.uint8)\n\ntrain_ids = train_df[train_df[\"fold\"]!=fold_selected].index\nvalid_ids = train_df[train_df[\"fold\"]==fold_selected].index\n\nX_train = train_df[train_df.index.isin(train_ids)]\nX_valid = train_df[train_df.index.isin(valid_ids)]\n\ntrain_df.groupby('fold').size()","metadata":{"execution":{"iopub.status.busy":"2022-06-28T04:20:55.103398Z","iopub.execute_input":"2022-06-28T04:20:55.103912Z","iopub.status.idle":"2022-06-28T04:20:55.313286Z","shell.execute_reply.started":"2022-06-28T04:20:55.103861Z","shell.execute_reply":"2022-06-28T04:20:55.312335Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check submission format works\nexperimental=False\nif experimental:\n    X_train=X_train[X_train.case.isin(X_train.case.unique()[:5])]       # take first few cases\n    X_valid=X_valid[X_valid.case.isin(X_valid.case.unique()[:2])]       # take first few cases\n    \nprint('X_train shape:', X_train.shape)\nprint('X_valid shape:', X_valid.shape)","metadata":{"execution":{"iopub.status.busy":"2022-06-28T04:20:55.314394Z","iopub.execute_input":"2022-06-28T04:20:55.314691Z","iopub.status.idle":"2022-06-28T04:20:55.321245Z","shell.execute_reply.started":"2022-06-28T04:20:55.314665Z","shell.execute_reply":"2022-06-28T04:20:55.320141Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load trained model from Json","metadata":{}},{"cell_type":"code","source":"from keras.models import model_from_json","metadata":{"execution":{"iopub.status.busy":"2022-06-28T04:20:55.322557Z","iopub.execute_input":"2022-06-28T04:20:55.322847Z","iopub.status.idle":"2022-06-28T04:20:55.333324Z","shell.execute_reply.started":"2022-06-28T04:20:55.322814Z","shell.execute_reply":"2022-06-28T04:20:55.332483Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras import backend, layers\nclass FixedDropout(layers.Dropout):\n    def _get_noise_shape(self, inputs):\n        if self.noise_shape is None:\n            return self.noise_shape\n\n        symbolic_shape = backend.shape(inputs)\n        noise_shape = [symbolic_shape[axis] if shape is None else shape\n                           for axis, shape in enumerate(self.noise_shape)]\n        return tuple(noise_shape)","metadata":{"execution":{"iopub.status.busy":"2022-06-28T04:20:55.33459Z","iopub.execute_input":"2022-06-28T04:20:55.334857Z","iopub.status.idle":"2022-06-28T04:20:55.765656Z","shell.execute_reply.started":"2022-06-28T04:20:55.334832Z","shell.execute_reply":"2022-06-28T04:20:55.764678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#load json and create model\njson_file = open('../input/efficientb7-unet-model-file/model.json', 'r')\nloaded_model_json = json_file.read()\njson_file.close()\nunet_model= model_from_json(loaded_model_json,custom_objects={'FixedDropout':FixedDropout(rate=0.1)})\n# load weights into new model\nunet_model.load_weights(\"../input/efficientb7-unet-model-file/model.h5\")\nprint(\"Loaded model from disk\")","metadata":{"execution":{"iopub.status.busy":"2022-06-28T04:20:55.766938Z","iopub.execute_input":"2022-06-28T04:20:55.767995Z","iopub.status.idle":"2022-06-28T04:21:09.1211Z","shell.execute_reply.started":"2022-06-28T04:20:55.76795Z","shell.execute_reply":"2022-06-28T04:21:09.120042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from glob import glob\nsub_df = pd.read_csv('../input/uw-madison-gi-tract-image-segmentation/sample_submission.csv')\nif not len(sub_df):\n    debug = True\n    sub_df = pd.read_csv('../input/uw-madison-gi-tract-image-segmentation/train.csv')\n    test_df = preprocessing(sub_df,  subset = 'train')\n    test_df = test_df[:100*3]\nelse : \n    debug = False\n    test_df = preprocessing(sub_df , subset = 'test')\n    \ntest_df.head(5)","metadata":{"execution":{"iopub.status.busy":"2022-06-28T04:21:09.129157Z","iopub.execute_input":"2022-06-28T04:21:09.12961Z","iopub.status.idle":"2022-06-28T04:21:14.295153Z","shell.execute_reply.started":"2022-06-28T04:21:09.129581Z","shell.execute_reply":"2022-06-28T04:21:14.294329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CFG:\n    BATCH_SIZE    = 64\n    img_size      = (128, 128, 3)\n    n_fold        = 5\n    fold_selected = 1\n    epochs        = 100\n    seed          = 42\n    steps_per_epoch_train = None\n    steps_per_epoch_val = None","metadata":{"execution":{"iopub.status.busy":"2022-06-28T04:21:14.296519Z","iopub.execute_input":"2022-06-28T04:21:14.296816Z","iopub.status.idle":"2022-06-28T04:21:14.302217Z","shell.execute_reply.started":"2022-06-28T04:21:14.296789Z","shell.execute_reply":"2022-06-28T04:21:14.301019Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class DataGenerator(tf.keras.utils.Sequence):\n    def __init__(self, df, batch_size = CFG.BATCH_SIZE, subset=\"train\", shuffle=False , img_shape = CFG.img_size):\n        super().__init__()\n        self.df = df\n        self.shuffle = shuffle\n        self.subset = subset\n        self.batch_size = batch_size\n        self.img_shape = img_shape\n        self.indexes = np.arange(len(df))\n        self.on_epoch_end()\n\n    def __len__(self):\n        return int(np.floor(len(self.df) / self.batch_size))\n    \n    def on_epoch_end(self):\n        if self.shuffle == True:\n            np.random.shuffle(self.indexes)\n    \n    def __getitem__(self, index):\n        X = np.empty((self.batch_size,self.img_shape[0],self.img_shape[1],self.img_shape[2]))\n        y = np.empty((self.batch_size,self.img_shape[0],self.img_shape[1],self.img_shape[2]))\n        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n        id_, heights, widths, classes = [] , [] ,[], [] \n        \n        for i, img_path in enumerate(self.df['path'].iloc[indexes]):\n            if self.subset != 'train':\n                id_.append(self.df['id'].iloc[indexes[i]])\n                heights.append(self.df['height'].iloc[indexes[i]])\n                widths.append(self.df['width'].iloc[indexes[i]])\n                classes.append(self.df['class'].iloc[indexes[i]])\n            \n            w=self.df['width'].iloc[indexes[i]]\n            h=self.df['height'].iloc[indexes[i]]\n            \n            img = self.__load_grayscale(img_path)  \n            X[i,] = img   \n            \n            if self.subset == 'train':\n                for k,j in enumerate([\"large_bowel\",\"small_bowel\",\"stomach\"]):\n                    rles = self.df[j].iloc[indexes[i]]\n                    mask = rle_decode(rles, shape=(h, w, 1))\n                    mask = cv2.resize(mask, self.img_shape[0:2] )\n                    y[i,:,:,k] = mask\n                    \n        if self.subset == 'train':\n            return X, y\n        else: \n            return X , id_ , widths , heights , classes\n\n    def __load_grayscale(self, img_path):\n        img = cv2.imread(img_path, cv2.IMREAD_ANYDEPTH)\n#         img = cv2.imread(img_path,cv2.IMREAD_UNCHANGED)\n        dsize = self.img_shape[0:2]\n        img = cv2.resize(img, dsize)\n#         img = img.astype(np.int8) / 255.\n        img = img.astype('float32') # original is uint16\n        img = (img - img.min())/(img.max() - img.min())*255.0 # scale image to [0, 255]\n        img = img.astype('uint8')/255\n        img = np.expand_dims(img, axis=-1)\n        return img\n","metadata":{"execution":{"iopub.status.busy":"2022-06-28T04:21:14.303651Z","iopub.execute_input":"2022-06-28T04:21:14.30411Z","iopub.status.idle":"2022-06-28T04:21:14.327562Z","shell.execute_reply.started":"2022-06-28T04:21:14.304073Z","shell.execute_reply":"2022-06-28T04:21:14.32661Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def infer(DF , model , batch_size = CFG.BATCH_SIZE) : \n    pred_rle = []; pred_ids = []; pred_classes = [];\n    \n    DF_batch = DataGenerator(DF, batch_size =batch_size, subset=\"test\", shuffle=False)\n    for idx , (img , id_, widths , heights , classes) in enumerate(tqdm(DF_batch)):\n#         msk = np.empty((batch_size,CFG.img_size[0],CFG.img_size[1],CFG.img_size[2]))\n                                       \n        preds = model.predict(img,verbose=0)\n        \n        # Rle encode \n        for j in range(batch_size):\n            k = 0 if classes[j]=='large_bowel' else 1 if classes[j]=='small_bowel' else 2\n\n            pred_img = cv2.resize(preds[j,:,:,k], ( widths[j] , heights[j]),\n                                  interpolation=cv2.INTER_NEAREST) # resize probabilities to original shape\n            pred_img = (pred_img>0.5).astype(dtype='uint8')    # classify\n\n            pred_ids.append(id_[j])\n            pred_classes.append(classes[j])\n            pred_rle.append(rle_encode(pred_img))\n    \n    return pred_rle, pred_ids , pred_classes","metadata":{"execution":{"iopub.status.busy":"2022-06-28T04:21:14.328787Z","iopub.execute_input":"2022-06-28T04:21:14.329647Z","iopub.status.idle":"2022-06-28T04:21:14.344551Z","shell.execute_reply.started":"2022-06-28T04:21:14.329615Z","shell.execute_reply":"2022-06-28T04:21:14.343443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import cv2\nCFG.BATCH_SIZE = 3   \npred_rle, pred_ids , pred_classes = infer(test_df, unet_model, batch_size = CFG.BATCH_SIZE)","metadata":{"execution":{"iopub.status.busy":"2022-06-28T04:21:14.345948Z","iopub.execute_input":"2022-06-28T04:21:14.346524Z","iopub.status.idle":"2022-06-28T04:22:16.490496Z","shell.execute_reply.started":"2022-06-28T04:21:14.346491Z","shell.execute_reply":"2022-06-28T04:22:16.48935Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"repertory ='/kaggle/input/'\n\nDIR = repertory + 'uw-madison-gi-tract-image-segmentation/' \nTRAIN_DIR = DIR + 'train'\nTEST_DIR = DIR + 'test'\ntrain_csv = DIR +'train.csv' \nsample_sub = DIR + 'sample_submission.csv'\n\ndf_train = pd.read_csv(train_csv)\n","metadata":{"execution":{"iopub.status.busy":"2022-06-28T04:22:16.491797Z","iopub.execute_input":"2022-06-28T04:22:16.492146Z","iopub.status.idle":"2022-06-28T04:22:16.801624Z","shell.execute_reply.started":"2022-06-28T04:22:16.492114Z","shell.execute_reply":"2022-06-28T04:22:16.800624Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Creating Submission file","metadata":{}},{"cell_type":"code","source":"submission = pd.DataFrame({\n    \"id\":pred_ids,\n    \"class\":pred_classes,\n    \"predicted\":pred_rle\n})\n\nif debug :\n    sub_df = pd.read_csv(train_csv)\n    del sub_df['segmentation']\nelse:\n    sub_df = pd.read_csv(sample_sub)\n    del sub_df['predicted']\n\nsub_df = sub_df.merge(submission, on=['id','class'])\nsub_df.to_csv('submission.csv',index=False)\n\nsubmission.sample(10)","metadata":{"execution":{"iopub.status.busy":"2022-06-28T04:22:16.802683Z","iopub.execute_input":"2022-06-28T04:22:16.802996Z","iopub.status.idle":"2022-06-28T04:22:17.171788Z","shell.execute_reply.started":"2022-06-28T04:22:16.802968Z","shell.execute_reply":"2022-06-28T04:22:17.170622Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv(\"submission.csv\", index = False)\nsubmission","metadata":{"execution":{"iopub.status.busy":"2022-06-28T04:22:17.176366Z","iopub.execute_input":"2022-06-28T04:22:17.177353Z","iopub.status.idle":"2022-06-28T04:22:17.19689Z","shell.execute_reply.started":"2022-06-28T04:22:17.177308Z","shell.execute_reply":"2022-06-28T04:22:17.195912Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}