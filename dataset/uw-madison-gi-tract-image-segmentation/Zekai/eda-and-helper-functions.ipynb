{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## **0. Why this notebook?**\nI wanted to share my EDA and helper functions for this competition. Becase:\n1. I am building my public portfolio.\n2. I do not work on competition to win. I do it to learn and gain experience.\n3. If my stuff helped you to get a good place in the final round, it would prove my ideas. I would like to see comments about my ideas.\n\n**Update in Section 4** June 17 2022: Added function to split the dataset by cases. The function makes sure that each fold has **similar amount of data** interms of number of images. This is important because in the hiddent test set, there are cases that do not show in given training set. Overall splitting by cases will help to mimic the hiddent test set.","metadata":{}},{"cell_type":"markdown","source":"## **1. Drop Images (by ordering)**\nWhy should we drop some images if we can?\nFor neural nets, it takes **weights** to learn that something is **important** (**input - target** relation), and it also takes **weights** to learn that something is **not important** (e.g. **large black background**). If we can **systematically** tell that some images always (or most likely) do not have targets (large bowel, small bowel and stomache), we may drop them from the train set, which will help the neural nets to focus on learning the input - target relation. This whole thing will work like a filter, on both train set and test set.\n\nBy quickly checking through the image set, we can see that some **very dark image** (almost all black) always do not have labels, and most of time, the **first and the last couple of images** in each day do not have labels. \n\nThe following part is to find out how many image in the beginning and end of the day we can drop.\n\n**Summary**: If you do not want to miss any images that has targets in it, systematically we can filtered out 7845 images out of 38496 images, which is about 20%. If you are willing to risk it, you may be able to filter out 9000 images with only lossing less than 50 images with targets.\n\nThe **toBeDropped_order.csv** file will be **saved** to output, so you can just **download** it for your project.\n\n**Criteria** to filter images without missing targets: for the days that have **144** slices, drop the **first 23** images and **last 7** images. For the days that have **80** slices, drop the **first** image and **last 4** images. The image ids are in the dataframe **toBeDropped**. Feel free to check.","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)","metadata":{"execution":{"iopub.status.busy":"2022-06-17T21:51:56.874636Z","iopub.execute_input":"2022-06-17T21:51:56.87517Z","iopub.status.idle":"2022-06-17T21:51:56.905077Z","shell.execute_reply.started":"2022-06-17T21:51:56.875075Z","shell.execute_reply":"2022-06-17T21:51:56.904314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# read the label file.\nmask_csv = pd.read_csv(r'../input/uw-madison-gi-tract-image-segmentation/train.csv')\n# create a new column to show if one id (image) has any labels\nmask_csv['hasSeg'] = mask_csv['segmentation'].notnull()\nid_seg = mask_csv.groupby('id')['hasSeg'].sum().to_frame(name = 'hasSeg').reset_index()\nid_seg['hasSeg'] = id_seg['hasSeg']>0\nid_seg.head(5)","metadata":{"execution":{"iopub.status.busy":"2022-06-17T21:51:58.379358Z","iopub.execute_input":"2022-06-17T21:51:58.37977Z","iopub.status.idle":"2022-06-17T21:51:59.063638Z","shell.execute_reply.started":"2022-06-17T21:51:58.379736Z","shell.execute_reply":"2022-06-17T21:51:59.062634Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Here we see that 21906 images do not have labels, and 16590 images have labels. 43% of the train set has labels.\nid_seg.hasSeg.value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-06-16T22:53:08.860814Z","iopub.execute_input":"2022-06-16T22:53:08.861335Z","iopub.status.idle":"2022-06-16T22:53:08.871839Z","shell.execute_reply.started":"2022-06-16T22:53:08.861289Z","shell.execute_reply":"2022-06-16T22:53:08.87064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In the following cell, we get the day information and the slice number from ID. Some days have **144** images, and some days have **80** images. I guess when the doctor is sure about the location of the target, they use less images, so in the following I treated them as two groups. The **maxSlice** is to show how many slices are in that day.","metadata":{}},{"cell_type":"code","source":"id_seg['day'] = id_seg.apply(lambda row: row['id'].split('slice')[0], axis = 1)\nid_seg['slice'] = id_seg.apply(lambda row: int(row['id'].split('_')[-1]), axis = 1)\n\ntemp_slice = id_seg.groupby('day')['slice'].max().to_frame(name = 'maxSlice').reset_index()\nmax_slice = pd.merge(id_seg, temp_slice, on='day')\nmax_slice.head(5)","metadata":{"execution":{"iopub.status.busy":"2022-06-16T22:53:08.873606Z","iopub.execute_input":"2022-06-16T22:53:08.874564Z","iopub.status.idle":"2022-06-16T22:53:10.147562Z","shell.execute_reply.started":"2022-06-16T22:53:08.874514Z","shell.execute_reply":"2022-06-16T22:53:10.146607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Only **15** ( = 1200 / 80 ) days have 80 slices. **259** ( = 37296 / 144 ) days have 144 slices.","metadata":{}},{"cell_type":"code","source":"max_slice.maxSlice.value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-06-11T21:54:37.650943Z","iopub.execute_input":"2022-06-11T21:54:37.651451Z","iopub.status.idle":"2022-06-11T21:54:37.6635Z","shell.execute_reply.started":"2022-06-11T21:54:37.651405Z","shell.execute_reply":"2022-06-11T21:54:37.662306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Seperate them\nmax_slice_80 = max_slice[max_slice.maxSlice == 80].copy()\nmax_slice_144 = max_slice[max_slice.maxSlice == 144].copy()","metadata":{"execution":{"iopub.status.busy":"2022-06-11T21:54:38.170623Z","iopub.execute_input":"2022-06-11T21:54:38.171303Z","iopub.status.idle":"2022-06-11T21:54:38.184731Z","shell.execute_reply.started":"2022-06-11T21:54:38.171253Z","shell.execute_reply":"2022-06-11T21:54:38.183929Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In the following cell, I use for loop to check if the first x images of each day has targets (labels), and then save the ratio = (the number of images that has lables) / (total number of images in this distance).\n\nFor example, if distance is set to 3, then for each day, I get all the first 2 ( = 3 - 2) slices, and check how many of them have labels. In this case 1 out of 30 images have labels, so the ratio is 0.03333333333333333.\n\nAnother example, if distance is set to 5, then for each day, I get all the first 4 ( = 5 - 1) slices, and check how many of them have labels. In this case 5 out of 60 images have labels, so the ratio is 0.08333333333333333.\n\nWhy is this useful? This means that if the train set is **representitive**, I can **safely** drop **all the first image** from each day, since they do not have any labels.","metadata":{}},{"cell_type":"code","source":"ratio_80_beginning = []\nfor distance in range(2,10):\n    max_slice_80['isCloseToStart'] = max_slice_80.apply(lambda x: (x['slice'] < distance), axis=1)\n    ratio_80_beginning.append(max_slice_80[max_slice_80['isCloseToStart'] == True]['hasSeg'].sum() / (15*(distance-1)))\n    #This 15 is from that 15 days have nb of slices = 80\nratio_80_beginning","metadata":{"execution":{"iopub.status.busy":"2022-06-08T19:26:00.16041Z","iopub.execute_input":"2022-06-08T19:26:00.1613Z","iopub.status.idle":"2022-06-08T19:26:00.330028Z","shell.execute_reply.started":"2022-06-08T19:26:00.161234Z","shell.execute_reply":"2022-06-08T19:26:00.329115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"temp = max_slice[(max_slice['maxSlice'] == 80) & (max_slice['slice'] < 3)]['hasSeg']\nprint('nb of images with lables with distance(3): ' + str(temp.sum()))\nprint('nb of images within distance(3): '  + str(temp.shape[0]))\nprint('ratio of images with labels:' + str(temp.sum() / temp.shape[0]))","metadata":{"execution":{"iopub.status.busy":"2022-06-08T19:26:00.331212Z","iopub.execute_input":"2022-06-08T19:26:00.331666Z","iopub.status.idle":"2022-06-08T19:26:00.340524Z","shell.execute_reply.started":"2022-06-08T19:26:00.331637Z","shell.execute_reply":"2022-06-08T19:26:00.339518Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"temp = max_slice[(max_slice['maxSlice'] == 80) & (max_slice['slice'] < 5)]['hasSeg']\nprint('nb of images with lables with distance(5): ' + str(temp.sum()))\nprint('nb of images within distance(5): '  + str(temp.shape[0]))\nprint('ratio of images with labels:' + str(temp.sum() / temp.shape[0]))","metadata":{"execution":{"iopub.status.busy":"2022-06-08T19:26:00.34166Z","iopub.execute_input":"2022-06-08T19:26:00.342351Z","iopub.status.idle":"2022-06-08T19:26:00.356204Z","shell.execute_reply.started":"2022-06-08T19:26:00.342315Z","shell.execute_reply":"2022-06-08T19:26:00.35535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The following code shows the ratio for the end side of days that 80 slices, the start and end side of days that has 144 slices.","metadata":{}},{"cell_type":"code","source":"ratio_80_end = []\nfor distance in range(2,10):\n    max_slice_80['isCloseToEnd'] = max_slice_80.apply(lambda x: (x['maxSlice'] - x['slice'])<distance-1, axis=1)\n    ratio_80_end.append(max_slice_80[max_slice_80['isCloseToEnd'] == True]['hasSeg'].sum() / (15*distance-1))\nprint(ratio_80_end)\n\nratio_144_beg = []\nfor distance in range(2,30):\n    max_slice_144['isCloseToBeg'] = max_slice_144.apply(lambda x: (x['slice'] < distance), axis=1)\n    ratio_144_beg.append(max_slice_144[max_slice_144['isCloseToBeg'] == True]['hasSeg'].sum() / (259*(distance-1)))\nprint(ratio_144_beg)\n\nratio_144_end = []\nfor distance in range(2,20):\n    max_slice_144['isCloseToEnd'] = max_slice_144.apply(lambda x: (x['maxSlice'] - x['slice'])<distance-1, axis=1)\n    ratio_144_end.append(max_slice_144[max_slice_144['isCloseToEnd'] == True]['hasSeg'].sum() / (259*(distance-1)))\nprint(ratio_144_end)","metadata":{"execution":{"iopub.status.busy":"2022-06-08T19:27:29.817275Z","iopub.execute_input":"2022-06-08T19:27:29.817713Z","iopub.status.idle":"2022-06-08T19:27:56.19027Z","shell.execute_reply.started":"2022-06-08T19:27:29.817681Z","shell.execute_reply":"2022-06-08T19:27:56.189196Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"toBeDropped = max_slice[((max_slice['maxSlice'] == 144) & ( max_slice['slice'] < 24)) |\n         ((max_slice['maxSlice'] == 144) & ((144 - max_slice['slice']) < 7)|\n         ((max_slice['maxSlice'] == 80) & (max_slice['slice'] < 2)|\n         (max_slice['maxSlice'] == 80) & ((80 - max_slice['slice']) < 4)))]\nprint('total number of dropped images with labels:' + str(toBeDropped.hasSeg.sum()))\nprint('total number of dropped images:' + str(toBeDropped.shape[0]))","metadata":{"execution":{"iopub.status.busy":"2022-06-08T19:26:27.482087Z","iopub.execute_input":"2022-06-08T19:26:27.482871Z","iopub.status.idle":"2022-06-08T19:26:27.498123Z","shell.execute_reply.started":"2022-06-08T19:26:27.482815Z","shell.execute_reply":"2022-06-08T19:26:27.497025Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"toBeDropped.head(5)","metadata":{"execution":{"iopub.status.busy":"2022-06-08T19:33:00.759005Z","iopub.execute_input":"2022-06-08T19:33:00.759543Z","iopub.status.idle":"2022-06-08T19:33:00.773905Z","shell.execute_reply.started":"2022-06-08T19:33:00.759499Z","shell.execute_reply":"2022-06-08T19:33:00.773Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"s1 = pd.merge(mask_csv, toBeDropped, how='inner', on=['id'])\nprint('double check, should be 0 if all these images do not have target: '+ str(s1.hasSeg_x.sum()))\nprint('nb of image filtered out: ' + str(s1.shape[0]/3))","metadata":{"execution":{"iopub.status.busy":"2022-06-08T19:26:27.54242Z","iopub.execute_input":"2022-06-08T19:26:27.543038Z","iopub.status.idle":"2022-06-08T19:26:27.585666Z","shell.execute_reply.started":"2022-06-08T19:26:27.543002Z","shell.execute_reply":"2022-06-08T19:26:27.584867Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# save the csv file.\ntoBeDropped.to_csv('toBeDropped_order.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2022-06-08T20:47:55.056096Z","iopub.execute_input":"2022-06-08T20:47:55.056536Z","iopub.status.idle":"2022-06-08T20:47:55.096557Z","shell.execute_reply.started":"2022-06-08T20:47:55.056503Z","shell.execute_reply":"2022-06-08T20:47:55.095781Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **2. Drop Images (by intensity)**\nSimilarly with the previous section, we are going to check if we can drop some images depending on the **intensity** of the image. Some images have **lower average intensity** and other features, so we may be able to create filters to drop those images that pretty much would not have any defects.\n\nFor example, amongest the images that have targets in them, the **least average image intensity** is **29.66**. This means that, if a given image has average intensity less then 29.66, we could just treat it as non-informative image and drop it from the train set. This process will be done similar during testing.\n\nHowever, we need to set a **safty margin** here, since the given train set is mostly like not be 100% representitive. Here I set the safty margin = 0.8, which means that if an image has average intensity less than **29.66 * 0.8 = 23.72**, I will drop it from the train set, or just give all zero prediction if it is in test set.\n\nThe safty margin works similarly on other features.\n\nI also made more features:\n1. ratio_nonZero: the ratio of number of non zero pixels to total number of pixels, for each image.\n2. ratio_larger10: the ratio of number of pixels that has value larger than 10, to total number of pixels, for each image.\n3. ratio_larger20, ratio_larger50, ratio_larger100 are defined similarly.\n\n**Summary**: With **safty margin** = 0.8, this method alone can drop **2888** images. If you **combine** this method with the previou method, you can drop **8334** images in total, which means that most of dark images are taken in the beginning or the end of the days.\n\nThe **toBeDropped_intensity.csv** file will be **saved** to output, so you can just **download** it for your project.","metadata":{}},{"cell_type":"code","source":"import os\nimport cv2","metadata":{"execution":{"iopub.status.busy":"2022-06-08T19:50:15.459306Z","iopub.execute_input":"2022-06-08T19:50:15.460011Z","iopub.status.idle":"2022-06-08T19:50:15.728966Z","shell.execute_reply.started":"2022-06-08T19:50:15.459949Z","shell.execute_reply":"2022-06-08T19:50:15.727093Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The major part of the following decode function is from:\nhttps://www.kaggle.com/code/awsaf49/uwmgi-unet-train-pytorch\n\n**Note**: Output is RGB, in which **Red: Large bowel**, **Green: Small bowel**, **Blue: Stomach**","metadata":{}},{"cell_type":"code","source":"# Helper function to decode mask.\n# Inputs: masks_csv: the csv file that contain all the label information. Straight from competition.\n# id: the image id. It can only decode one id at a time.\n# shape: the shape of the corresponding image.\ndef decode(masks_csv, id, shape):\n    temp_masks = masks_csv[masks_csv['id'] == id]\n    temp_mask_list = []\n    for name, i in [('large_bowel',1), ('small_bowel',2), ('stomach',3)]:\n        temp_mask_rle = temp_masks[temp_masks['class']==name]['segmentation'].values[0]\n        h, w = shape\n        img = np.zeros((h * w,), dtype=np.float32)\n        if type(temp_mask_rle) == str:\n            s = temp_mask_rle.split()\n            s = list(map(int, s))\n            starts = np.array(s[0::2]) - 1\n            lengths = s[1::2]\n            ends = starts + lengths\n            for lo, hi in zip(starts, ends):\n                img[lo : hi] = 1\n        img = img.reshape(shape)\n        temp_mask_list.append(img)\n\n    mask = np.stack(temp_mask_list, -1).astype(np.uint8)\n    return mask","metadata":{"execution":{"iopub.status.busy":"2022-06-16T22:53:10.149692Z","iopub.execute_input":"2022-06-16T22:53:10.150997Z","iopub.status.idle":"2022-06-16T22:53:10.16156Z","shell.execute_reply.started":"2022-06-16T22:53:10.150942Z","shell.execute_reply":"2022-06-16T22:53:10.160853Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mask_stats = pd.DataFrame(columns=['id', 'hasMask', 'avgIntensity', \\\n                'ratio_nonZero', 'ratio_larger10', 'ratio_larger20', 'ratio_larger50', 'ratio_larger100'])\n\nsrc = r'../input/uw-madison-gi-tract-image-segmentation/train'\n\n# counter = 0\nfor path, subdirs, files in os.walk(src):\n    if len(files)>0:\n        for name in files: \n            # counter += 1\n            # if counter <10:\n            case_temp = path.split('/')[-3]\n            day_temp = (path.split('/')[-2]).split('_')[-1]\n            filename_original = os.path.join(path, name)\n            image = cv2.imread(filename_original, cv2.IMREAD_UNCHANGED)\n            filename_dest = case_temp+'_' + day_temp + '_' + name\n            splits_temp = filename_dest.split('_')\n\n            shape_temp = (int(splits_temp[5]),int(splits_temp[4]))\n            id_temp = splits_temp[0] +'_' + splits_temp[1] +'_' + splits_temp[2] +'_' +splits_temp[3]\n            mask = decode(mask_csv, id_temp, shape_temp)\n\n            nb_pixels = image.shape[0]*image.shape[1]\n            \n            temp_row = pd.DataFrame({'id':id_temp, \n            'hasMask':(mask.max()>0) + 0, \n            'avgIntensity':np.mean(image),\n            'ratio_nonZero':(image>0).sum() / nb_pixels, \n            'ratio_larger10': (image>10).sum() / nb_pixels, \n            'ratio_larger20': (image>20).sum() / nb_pixels, \n            'ratio_larger50': (image>50).sum() / nb_pixels, \n            'ratio_larger100': (image>100).sum() / nb_pixels},  index=[0])\n            mask_stats = pd.concat([mask_stats, temp_row],ignore_index= True)","metadata":{"execution":{"iopub.status.busy":"2022-06-08T19:50:58.519092Z","iopub.execute_input":"2022-06-08T19:50:58.51956Z","iopub.status.idle":"2022-06-08T20:12:37.418463Z","shell.execute_reply.started":"2022-06-08T19:50:58.519524Z","shell.execute_reply":"2022-06-08T20:12:37.41709Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mask_stats.head(2)","metadata":{"execution":{"iopub.status.busy":"2022-06-08T20:12:43.749033Z","iopub.execute_input":"2022-06-08T20:12:43.750181Z","iopub.status.idle":"2022-06-08T20:12:43.766719Z","shell.execute_reply.started":"2022-06-08T20:12:43.750125Z","shell.execute_reply":"2022-06-08T20:12:43.765515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mask_stats[mask_stats.hasMask == 1].avgIntensity.min()","metadata":{"execution":{"iopub.status.busy":"2022-06-08T20:17:43.373043Z","iopub.execute_input":"2022-06-08T20:17:43.373502Z","iopub.status.idle":"2022-06-08T20:17:43.389725Z","shell.execute_reply.started":"2022-06-08T20:17:43.373469Z","shell.execute_reply":"2022-06-08T20:17:43.388959Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Visulization: you can clearly see that in some cases, all the images within that bin do not contain targets.","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nmask = mask_stats[mask_stats.hasMask == 1]\nnoMask = mask_stats[mask_stats.hasMask == 0]\nplt.figure(figsize=[18,9])\ncolumn_names = ['avgIntensity', 'ratio_nonZero', 'ratio_larger10', 'ratio_larger20', 'ratio_larger50', 'ratio_larger100']\nfor i in range(len(column_names)):\n    cur_subplot = 231 + i\n    cur_feature = column_names[i]\n    plt.subplot(cur_subplot)\n    plt.hist(x = [mask[cur_feature], noMask[cur_feature]], stacked=False,label = ['mask','noMask'], color = ['skyblue','orange'])\n    plt.xlabel(cur_feature)\n    plt.ylabel('# of Passengers')\n    plt.legend()","metadata":{"execution":{"iopub.status.busy":"2022-06-08T20:12:45.921565Z","iopub.execute_input":"2022-06-08T20:12:45.922572Z","iopub.status.idle":"2022-06-08T20:12:47.158924Z","shell.execute_reply.started":"2022-06-08T20:12:45.922522Z","shell.execute_reply":"2022-06-08T20:12:47.157754Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"safty_margin = 0.8\n\ntoBeDropped_intensity = pd.DataFrame()\nfor i in column_names:\n    temp_min = mask_stats[mask_stats.hasMask == 1][i].min()\n    temp_toBeDropped = mask_stats[(mask_stats.hasMask == 0) & (mask_stats[i] < temp_min*safty_margin)]\n    toBeDropped_intensity = pd.concat([toBeDropped_intensity, temp_toBeDropped])\ntoBeDropped_intensity = toBeDropped_intensity.drop_duplicates()","metadata":{"execution":{"iopub.status.busy":"2022-06-08T20:31:38.430908Z","iopub.execute_input":"2022-06-08T20:31:38.431358Z","iopub.status.idle":"2022-06-08T20:31:38.547208Z","shell.execute_reply.started":"2022-06-08T20:31:38.431324Z","shell.execute_reply":"2022-06-08T20:31:38.545736Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"toBeDropped_intensity.head(5)","metadata":{"execution":{"iopub.status.busy":"2022-06-08T20:31:48.814272Z","iopub.execute_input":"2022-06-08T20:31:48.815669Z","iopub.status.idle":"2022-06-08T20:31:48.839271Z","shell.execute_reply.started":"2022-06-08T20:31:48.815609Z","shell.execute_reply":"2022-06-08T20:31:48.837929Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"toBeDropped_intensity.to_csv('toBeDropped_intensity.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2022-06-08T20:31:23.791535Z","iopub.execute_input":"2022-06-08T20:31:23.791973Z","iopub.status.idle":"2022-06-08T20:31:23.842759Z","shell.execute_reply.started":"2022-06-08T20:31:23.791936Z","shell.execute_reply":"2022-06-08T20:31:23.841922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(set(toBeDropped_intensity.id.tolist() + toBeDropped.id.tolist()))","metadata":{"execution":{"iopub.status.busy":"2022-06-08T20:34:24.733333Z","iopub.execute_input":"2022-06-08T20:34:24.733902Z","iopub.status.idle":"2022-06-08T20:34:24.743617Z","shell.execute_reply.started":"2022-06-08T20:34:24.733861Z","shell.execute_reply":"2022-06-08T20:34:24.742694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **3. False Annotation**\nThanks to https://www.kaggle.com/competitions/uw-madison-gi-tract-image-segmentation/discussion/319963 and https://www.kaggle.com/competitions/uw-madison-gi-tract-image-segmentation/discussion/321979. \n\n**Masks for case7_day0 and case81_day30 are incorrect. Some masks are missing for case138_day0.**\n\n**Summary**: These two notebooks should draw more attention. Flase annotation is a typical and serious problem in deep learning. Especially with a medium size dataset like this competition, these false annotation could be critical that can affect final result. I personally **removed all slices** from these 3 days. ","metadata":{}},{"cell_type":"markdown","source":"## **4. Train-Test Split**\nData description: \"Each case in this competition is represented by multiple sets of scan slices (each set is identified by the day the scan took place). **Some cases are split by time (early days are in train, later days are in test) while some cases are split by case - the entirety of the case is in train or test.** The goal of this competition is to be able to generalize to both partially and wholly unseen cases.\"\nFrom https://www.kaggle.com/competitions/uw-madison-gi-tract-image-segmentation/data.\n\nWhen we do the train-test split, we should try our best to **mimic the hidden test set**. When doing the **N-fold cross validation / ensemble**, we should split the data by **days** or even **cases**. I personaly am splitting them by days, since this is easier to make sure all folds have similar amount of images. Later I may try to randomly split it by both cases and days.\n\n**Do NOT just copy all images together in one folder and split them..**\n\n**Update**: added function to split training data into folds that have similart amount of data regarding the number of images. Some cases have more days and some have less, so when splitting training data by cases, it is important to make sure the split is even. The first two helper function are from: https://stackoverflow.com/questions/3420937/algorithm-to-find-which-number-in-a-list-sum-up-to-a-certain-number. I use dynamic programming to make sure that the number of images in each fold is similar.","metadata":{}},{"cell_type":"code","source":"def f(v, i, S, memo):\n    if i >= len(v): return 1 if S == 0 else 0\n    if (i, S) not in memo:  # <-- Check if value has not been calculated.\n        count = f(v, i + 1, S, memo)\n        count += f(v, i + 1, S - v[i], memo)\n        memo[(i, S)] = count  # <-- Memoize calculated result.\n    return memo[(i, S)]     # <-- Return memoized value.\ndef g(v, S, memo):\n    subset = []\n    for i, x in enumerate(v):\n    # Check if there is still a solution if we include v[i]\n        if f(v, i + 1, S - x, memo) > 0:\n            subset.append(x)\n            S -= x\n    return subset","metadata":{"execution":{"iopub.status.busy":"2022-06-17T22:09:54.557628Z","iopub.execute_input":"2022-06-17T22:09:54.557971Z","iopub.status.idle":"2022-06-17T22:09:54.566495Z","shell.execute_reply.started":"2022-06-17T22:09:54.557943Z","shell.execute_reply":"2022-06-17T22:09:54.565844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# helper function, get the size of a folder\ndef get_dir_size(path='.'):\n    total = 0\n    with os.scandir(path) as it:\n        for entry in it:\n            if entry.is_file():\n                total += entry.stat().st_size\n            elif entry.is_dir():\n                total += get_dir_size(entry.path)\n    return total","metadata":{"execution":{"iopub.status.busy":"2022-06-17T22:09:54.835804Z","iopub.execute_input":"2022-06-17T22:09:54.83611Z","iopub.status.idle":"2022-06-17T22:09:54.840658Z","shell.execute_reply.started":"2022-06-17T22:09:54.836088Z","shell.execute_reply":"2022-06-17T22:09:54.839946Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport numpy as np\nfull_directory = r'../input/uw-madison-gi-tract-image-segmentation/train'\n# create a dict that maps the case name with its folder size, in MB\nsize_dirc = {}\nfor temp_dirc in os.listdir(full_directory):\n    temp_size = np.around(get_dir_size(full_directory + '//' + temp_dirc)/(1024*1024)).astype(np.uint8)\n    size_dirc[temp_dirc] = temp_size","metadata":{"execution":{"iopub.status.busy":"2022-06-17T22:10:03.577516Z","iopub.execute_input":"2022-06-17T22:10:03.577864Z","iopub.status.idle":"2022-06-17T22:10:06.681029Z","shell.execute_reply.started":"2022-06-17T22:10:03.577837Z","shell.execute_reply":"2022-06-17T22:10:06.680513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# input: dictionary that has the case name as key, and the size of the folder as value\n# output: two dictionaries that have similar amout of data\ndef split_cases_twoGroups(size_directory):\n    size_list = np.array(list(size_directory.values()))\n#     since the previous 0-1 Knapsack Problem function depends on the order of the options, \n#     here I shuffle the order first to introduce randomness\n    np.random.seed(42)\n    np.random.shuffle(size_list)\n#     the target here is the amount of data each fold should have.\n#     you may change it, if you want to split it to 5 fold or something else\n    target = int(size_list.sum()/2)\n    memo = dict()\n    if f(size_list, 0, target, memo) == 0: print(\"There are no valid subsets.\")\n    else: half_sizes = np.array((g(size_list, target, memo)))\n\n    size_dirc_half_0 = size_directory.copy()\n    size_dirc_half_1 = {}\n    used_key = []\n    for size in half_sizes:\n#         find the key by searching value. Since there could be more than one key that have the same value\n#         need to pop the key after it is found\n#         the left over dict is the second half\n        key_temp = list(size_dirc_half_0.keys())[list(size_dirc_half_0.values()).index(size)]\n        size_dirc_half_1[key_temp] = size\n        size_dirc_half_0.pop(key_temp)\n    return size_dirc_half_0, size_dirc_half_1","metadata":{"execution":{"iopub.status.busy":"2022-06-17T22:10:06.682093Z","iopub.execute_input":"2022-06-17T22:10:06.682398Z","iopub.status.idle":"2022-06-17T22:10:06.688125Z","shell.execute_reply.started":"2022-06-17T22:10:06.682376Z","shell.execute_reply":"2022-06-17T22:10:06.687728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"size_dirc_half_0, size_dirc_half_1 = split_cases_twoGroups(size_directory = size_dirc)\nsize_dirc_quarter_0, size_dirc_quarter_1 = split_cases_twoGroups(size_directory = size_dirc_half_0)\nsize_dirc_quarter_2, size_dirc_quarter_3 = split_cases_twoGroups(size_directory = size_dirc_half_1)","metadata":{"execution":{"iopub.status.busy":"2022-06-17T22:10:06.689031Z","iopub.execute_input":"2022-06-17T22:10:06.689447Z","iopub.status.idle":"2022-06-17T22:10:07.090251Z","shell.execute_reply.started":"2022-06-17T22:10:06.689408Z","shell.execute_reply":"2022-06-17T22:10:07.089505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"size_dirc_quarter_0","metadata":{"execution":{"iopub.status.busy":"2022-06-17T22:10:07.091795Z","iopub.execute_input":"2022-06-17T22:10:07.092068Z","iopub.status.idle":"2022-06-17T22:10:07.099842Z","shell.execute_reply.started":"2022-06-17T22:10:07.092042Z","shell.execute_reply":"2022-06-17T22:10:07.099345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"size_dirc_quarter_1","metadata":{"execution":{"iopub.status.busy":"2022-06-17T22:02:40.028536Z","iopub.execute_input":"2022-06-17T22:02:40.028927Z","iopub.status.idle":"2022-06-17T22:02:40.035566Z","shell.execute_reply.started":"2022-06-17T22:02:40.028895Z","shell.execute_reply":"2022-06-17T22:02:40.03481Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('size of the 1 quarter (MB): ' + str(np.array(list(size_dirc_quarter_0.values())).sum()))\nprint('size of the 2 quarter (MB): ' + str(np.array(list(size_dirc_quarter_1.values())).sum()))\nprint('size of the 3 quarter (MB): ' + str(np.array(list(size_dirc_quarter_2.values())).sum()))\nprint('size of the 4 quarter (MB): ' + str(np.array(list(size_dirc_quarter_3.values())).sum()))","metadata":{"execution":{"iopub.status.busy":"2022-06-17T22:16:22.295967Z","iopub.execute_input":"2022-06-17T22:16:22.296287Z","iopub.status.idle":"2022-06-17T22:16:22.303059Z","shell.execute_reply.started":"2022-06-17T22:16:22.29626Z","shell.execute_reply":"2022-06-17T22:16:22.302496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **5. Create 2.5D Images / Files**\nCredits to the idea here: https://www.kaggle.com/competitions/uw-madison-gi-tract-image-segmentation/discussion/322549. 2.5 images give the model  **temporal** information, on the top of **spatial** imformation from single images. The following function is to generate 2.5D image, given a center image. \n\n**Note**: \n1. In the result of the function, the **target** image (the \"center\" image) are places as the **first slice** in the pile. This is for convenience reasons. Such **ordering** will not hurt the model perfomance, since neural network will **automatically** decide which layer is more important. As long as the ordering is **consistant**, the ordering itself does not matter.\n2. For this 2.5D function, there could be **many slices (images)** but it will have only **one mask** that corresponds to the \"center\" image. This is because 2.5 D image is used to give more information to CNN, and thus it is **N to 1** segmentation. If **more than one mask** are used, it will become **N to N** segmentation and will be unnecessarily complicated.\n3. This helper function supports **augmentation** (only **Albumentations**, because it has nice support for 16 bit images). If augmentation is passes, all slices and the mask will have the **exactly same** augmentation, which is done by adding **addition targets** in the augmentator.\n\n**Parameters**:\n1. **masks_csv**: the label mask csv. This is needed to get decoded mask.\n2. **image_path**: the path to the \"center\" image. Again, this image will be at index 0 in the output.\n3. **nb_layers_oneSide**: the number of layers for one side. E.g. if this is set to 3, the output will have 3*2+1 = 7 layers(images). 3 images from left, and 3 images from right.\n4. **stride**: the stride to pick image. E.g. if the slice number of the center image is 10, nb_layers_oneSide = 3 and stride = 2, the output will have slices: 4, 6, 8, 10, 12, 14, 16.\n5. **augmentation**: optional. If yes, a transform from Albumantation is needed.\n\nAt the end, if needed, I recommend to save the ouput as numpy file, since it can contain more than 3 layers.","metadata":{}},{"cell_type":"code","source":"import cv2\nimport os\nfrom PIL import Image","metadata":{"execution":{"iopub.status.busy":"2022-06-17T22:08:51.756085Z","iopub.execute_input":"2022-06-17T22:08:51.756518Z","iopub.status.idle":"2022-06-17T22:08:51.797545Z","shell.execute_reply.started":"2022-06-17T22:08:51.756482Z","shell.execute_reply":"2022-06-17T22:08:51.796533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def twoPointFiveD(masks_csv, image_path, nb_layers_oneSide, stride, augmentation = None):\n    # sparse the file path, and get case, day, shape and ID\n    case_temp = image_path.split('/')[-4]\n    day_temp = (image_path.split('/')[-3]).split('_')[-1]\n    filename = case_temp + '_' + day_temp + '_' +image_path.split('/')[-1]\n    splits_temp = filename.split('_')\n    shape_temp = (int(splits_temp[5]),int(splits_temp[4]))\n    id_temp = splits_temp[0] +'_' + splits_temp[1] +'_' + splits_temp[2] +'_' +splits_temp[3]\n\n    image = cv2.imread(image_path, cv2.IMREAD_UNCHANGED)\n    mask = decode(masks_csv, id_temp, shape_temp)\n    \n\n    # get slice offsets\n    slice_number = int(image_path.split('slice_')[1].split('_')[0])\n    offsets = np.arange(-nb_layers_oneSide, nb_layers_oneSide+1)*stride\n\n    # load other slices and stack them together\n    # NOTE: The target image will be the first image with index 0, instead of being in the middle like sandwich\n    layers = [image]\n    for offset in offsets:\n        temp_zeros = (4-len(str(slice_number+offset)))*'0'\n        temp_path = image_path.split('slice_')[0] + 'slice_' + temp_zeros +str(slice_number+offset) + image_path.split('slice_')[1][4:]\n        temp_layer = None\n        # print(temp_path)\n        if offset ==0:\n            continue\n        elif not os.path.isfile(temp_path):\n            layers.append(np.zeros(image.shape))\n        else:\n            layers.append(cv2.imread(temp_path, cv2.IMREAD_UNCHANGED))\n    result = np.moveaxis(np.stack(layers), 0, -1)\n\n    # if augmentation, all images and one single mask will have exactly the same augmentation\n    # NOTE: The augmented target image will be the first image with index 0, instead of being in the middle like sandwich\n    if augmentation:\n        additional_targets = {}\n        for i in range(1,nb_layers_oneSide*2+1):\n            additional_targets['image' + str(i)] = 'image'\n        augmentation.add_targets(additional_targets)\n\n        args = {'image':result[:,:,0], 'mask':mask}\n        for i in range(1,nb_layers_oneSide*2+1):\n            args['image' + str(i)] = result[:,:,i]\n        transformed = augmentation(**args)\n\n        result_layers = [transformed['image']]\n        for key in additional_targets.keys():\n            result_layers.append(transformed[key])\n            mask = transformed['mask']\n        result = np.moveaxis(np.stack(result_layers), 0, -1)\n    return result, mask","metadata":{"execution":{"iopub.status.busy":"2022-06-17T22:08:52.045019Z","iopub.execute_input":"2022-06-17T22:08:52.045885Z","iopub.status.idle":"2022-06-17T22:08:52.064762Z","shell.execute_reply.started":"2022-06-17T22:08:52.045842Z","shell.execute_reply":"2022-06-17T22:08:52.063624Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_path = '../input/uw-madison-gi-tract-image-segmentation/train/case145/case145_day19/scans/slice_0102_360_310_1.50_1.50.png'","metadata":{"execution":{"iopub.status.busy":"2022-06-17T22:08:52.512744Z","iopub.execute_input":"2022-06-17T22:08:52.513277Z","iopub.status.idle":"2022-06-17T22:08:52.517371Z","shell.execute_reply.started":"2022-06-17T22:08:52.513241Z","shell.execute_reply":"2022-06-17T22:08:52.516421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nb_layers_oneSide = 1\nstride = 2\nimages_stacked, mask = twoPointFiveD(masks_csv = mask_csv, image_path= image_path, \n            nb_layers_oneSide = nb_layers_oneSide, stride = stride, augmentation = None)","metadata":{"execution":{"iopub.status.busy":"2022-06-17T22:08:52.851626Z","iopub.execute_input":"2022-06-17T22:08:52.852563Z","iopub.status.idle":"2022-06-17T22:08:52.904675Z","shell.execute_reply.started":"2022-06-17T22:08:52.852524Z","shell.execute_reply":"2022-06-17T22:08:52.903573Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"colors = {0:(255,0,0), 1:(0,255,0),2:(0,0,255),}\nimage_original = images_stacked[:,:,0]\nmask_original = mask\nimage_toShow = ((image_original/image_original.max())*255).astype(np.uint8)\nimage_contour = cv2.cvtColor(image_toShow.copy(),cv2.COLOR_GRAY2BGR)\n\nfor i in range(3):\n    output_temp_class = np.zeros(image_original.shape).astype(np.uint8)\n    output_temp_class = mask_original[:,:,i]\n    contours, hierarchy = cv2.findContours(output_temp_class, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n    _ = cv2.drawContours(image_contour, contours, contourIdx = -1, color = colors[i], thickness = 1)\nImage.fromarray(image_contour)","metadata":{"execution":{"iopub.status.busy":"2022-06-11T22:15:50.345994Z","iopub.execute_input":"2022-06-11T22:15:50.346399Z","iopub.status.idle":"2022-06-11T22:15:50.406838Z","shell.execute_reply.started":"2022-06-11T22:15:50.346364Z","shell.execute_reply":"2022-06-11T22:15:50.405975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import albumentations as A\ntransform = A.Compose([\n    A.ToFloat(max_value = 65535.0),\n    A.Affine(scale=(0.9,1.1), rotate = (-15,15), shear = (-7,7), p = 0.5),\n    A.RandomBrightnessContrast(brightness_limit=0.001, contrast_limit=0.001, p = 0.5),\n    A.GaussNoise(var_limit=0.0000002, p = 0.5),\n    A.RandomCrop(width=224, height=224),\n    A.FromFloat(max_value = 65535.0),\n])\n\nnb_layers_oneSide = 2\nstride = 2\n\nimage_path = '../input/uw-madison-gi-tract-image-segmentation/train/case123/case123_day20/scans/slice_0082_266_266_1.50_1.50.png'\nimages_stacked, mask = twoPointFiveD(masks_csv = mask_csv, image_path= image_path, \n            nb_layers_oneSide = nb_layers_oneSide, stride = stride, augmentation = transform)\nprint(images_stacked.shape)","metadata":{"execution":{"iopub.status.busy":"2022-06-16T19:04:12.884314Z","iopub.execute_input":"2022-06-16T19:04:12.884739Z","iopub.status.idle":"2022-06-16T19:04:14.935051Z","shell.execute_reply.started":"2022-06-16T19:04:12.884703Z","shell.execute_reply":"2022-06-16T19:04:14.93342Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"colors = {0:(255,0,0), 1:(0,255,0),2:(0,0,255),}\nimage_original = images_stacked[:,:,0]\nmask_original = mask\nimage_toShow = ((image_original/image_original.max())*255).astype(np.uint8)\nimage_contour = cv2.cvtColor(image_toShow.copy(),cv2.COLOR_GRAY2RGB)\n\nfor i in range(3):\n    output_temp_class = np.zeros(image_original.shape).astype(np.uint8)\n    output_temp_class = mask_original[:,:,i]\n    contours, hierarchy = cv2.findContours(output_temp_class, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n    _ = cv2.drawContours(image_contour, contours, contourIdx = -1, color = colors[i], thickness = 1)\nImage.fromarray(image_contour)","metadata":{"execution":{"iopub.status.busy":"2022-06-11T22:32:25.760534Z","iopub.execute_input":"2022-06-11T22:32:25.76094Z","iopub.status.idle":"2022-06-11T22:32:25.790122Z","shell.execute_reply.started":"2022-06-11T22:32:25.760906Z","shell.execute_reply":"2022-06-11T22:32:25.789384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **6. Augmentation**\nAugmentation is very porject specific. A good augmentation should improve the **robustness** of the dataset, which can help in production to deal with subtle **randomness** (camera angle change, dust, ambient light change, etc.).\n\nHere I would like to share my augmentation. Since the original image is in **16 bit**, everything about augmenting brightness and contrast is **treakier** than 8 bit images. You will see later that I set some parameters **extremly small**, because otherwise the pixel value will just go crazy. I am not sure if this is a bug of albumentation. Anyhow, the reasult of this augmentation seems reasonable to me.\n\nAlso, instead of using randome crop with a image size, I iterate through each image with 4 corners with size 224(changable, depending on your neural nets). For example, for an image with size (310 , 360), the position of each corner image is:\n1. Top left:  [0:224, 0:224]\n2. Top right: [0:224, 136:360]\n3. Bottom left: [86:310, 0:224]\n4. Bottom right: [86:310, 136:360]\n\nIn this case, the **center part** of the image will be most likely **repeated** in each corner image. This is actually going to be helpful for training, because in most of the images, the center part contains more information and more targets.\n\n**NOTE**: I apply augmentation on **FULL** image **BEFORE** such corner cropping, because when agumentating (translating, resizing down..), it may generate garbage pixels (zeros), Applying augmentation on full image and then cropping can help to reduce such garbage pixels.","metadata":{}},{"cell_type":"code","source":"import albumentations as A\nimport cv2\n\ntransform_aug = A.Compose([\n    A.ToFloat(max_value = 65535.0),\n    A.Affine(translate_percent = (0,0.1), scale=(0.9,1.1), rotate = (-15,15), shear = (-7,7), p = 0.75),\n    A.RandomBrightnessContrast(brightness_limit=0.0005, contrast_limit=0.0005, p = 0.25),\n    A.GaussNoise(var_limit=0.0000001, p = 0.25),\n    A.FromFloat(max_value = 65535.0),\n])","metadata":{"execution":{"iopub.status.busy":"2022-06-16T22:53:19.449106Z","iopub.execute_input":"2022-06-16T22:53:19.449642Z","iopub.status.idle":"2022-06-16T22:53:21.352624Z","shell.execute_reply.started":"2022-06-16T22:53:19.449599Z","shell.execute_reply":"2022-06-16T22:53:21.35146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#  The first part is same as creating 2.5D image. Just to get the id and decoded mask.\nimage_path = '../input/uw-madison-gi-tract-image-segmentation/train/case145/case145_day19/scans/slice_0102_360_310_1.50_1.50.png'\ncase_temp = image_path.split('/')[-4]\nday_temp = (image_path.split('/')[-3]).split('_')[-1]\nfilename = case_temp + '_' + day_temp + '_' +image_path.split('/')[-1]\nsplits_temp = filename.split('_')\nshape_temp = (int(splits_temp[5]),int(splits_temp[4]))\nid_temp = splits_temp[0] +'_' + splits_temp[1] +'_' + splits_temp[2] +'_' +splits_temp[3]\n\nimage = cv2.imread(image_path, cv2.IMREAD_UNCHANGED)\nmask = decode(mask_csv, id_temp, shape_temp)\n\n# Calculating offsets.\noffset_height = image.shape[0] - 224\noffset_width = image.shape[1] - 224\noffsets = [(0,0), (offset_height,0), (0, offset_width), (offset_height, offset_width)]\nimage_patches = []\nmask_patches = []\n\n# for each corner, augment the full image and then crop.\nfor offset_index in range(4):\n    offset_height_temp, offset_width_temp = offsets[offset_index]\n    transformed = transform_aug(image=image, mask = mask)\n    transformed_image = transformed['image']\n    transformed_mask = transformed['mask']\n    image_aug_temp = transformed_image[offset_height_temp:offset_height_temp+224, offset_width_temp:offset_width_temp+224]\n    mask_aug_temp = transformed_mask[offset_height_temp:offset_height_temp+224, offset_width_temp:offset_width_temp+224,:]\n    image_patches.append(image_aug_temp)\n    mask_patches.append(mask_aug_temp)\n","metadata":{"execution":{"iopub.status.busy":"2022-06-16T22:53:21.354867Z","iopub.execute_input":"2022-06-16T22:53:21.35536Z","iopub.status.idle":"2022-06-16T22:53:21.45847Z","shell.execute_reply.started":"2022-06-16T22:53:21.355315Z","shell.execute_reply":"2022-06-16T22:53:21.457412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from PIL import Image\n# a helper function to visulize the mask and the image.\ndef visulize_contour_mask(image, mask):\n    colors = {0:(255,0,0), 1:(0,255,0),2:(0,0,255),}\n    image_original = image\n    mask_original = mask\n    image_toShow = ((image_original/image_original.max())*255).astype(np.uint8)\n    image_contour = cv2.cvtColor(image_toShow.copy(),cv2.COLOR_GRAY2BGR)\n\n    for i in range(3):\n        output_temp_class = np.zeros(image_original.shape).astype(np.uint8)\n        output_temp_class = mask_original[:,:,i]\n        contours, hierarchy = cv2.findContours(output_temp_class, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n        _ = cv2.drawContours(image_contour, contours, contourIdx = -1, color = colors[i], thickness = 1)\n    return Image.fromarray(image_contour)","metadata":{"execution":{"iopub.status.busy":"2022-06-16T23:11:59.464409Z","iopub.execute_input":"2022-06-16T23:11:59.465963Z","iopub.status.idle":"2022-06-16T23:11:59.475776Z","shell.execute_reply.started":"2022-06-16T23:11:59.465885Z","shell.execute_reply":"2022-06-16T23:11:59.475027Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"visulize_contour_mask(image_patches[0], mask_patches[0])","metadata":{"execution":{"iopub.status.busy":"2022-06-16T23:12:00.995719Z","iopub.execute_input":"2022-06-16T23:12:00.996209Z","iopub.status.idle":"2022-06-16T23:12:01.020458Z","shell.execute_reply.started":"2022-06-16T23:12:00.996128Z","shell.execute_reply":"2022-06-16T23:12:01.019169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"visulize_contour_mask(image_patches[1], mask_patches[1])","metadata":{"execution":{"iopub.status.busy":"2022-06-16T23:12:01.328289Z","iopub.execute_input":"2022-06-16T23:12:01.328713Z","iopub.status.idle":"2022-06-16T23:12:01.358053Z","shell.execute_reply.started":"2022-06-16T23:12:01.328678Z","shell.execute_reply":"2022-06-16T23:12:01.35677Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}