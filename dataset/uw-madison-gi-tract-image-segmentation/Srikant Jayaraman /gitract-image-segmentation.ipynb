{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# TODO\n1. Weights and Biases to log models\n2. Build Validation Set\n3. Loops for Training, Validation and RunTraining\n4. Memory Management Clarity","metadata":{"id":"L22pN6VcHAav"}},{"cell_type":"markdown","source":"Inspired heavily from the great work in https://www.kaggle.com/code/awsaf49/uwmgi-unet-train-pytorch#ðŸ“’-Notebooks","metadata":{}},{"cell_type":"markdown","source":"# Config","metadata":{"id":"L6N_rrC1KoqI"}},{"cell_type":"code","source":"\n# Params\ntrain_csv_location = '../input/uw-madison-gi-tract-image-segmentation/train.csv'\ntraining_images_directory = '../input/uw-madison-gi-tract-image-segmentation/train/'\n\ntraining_prop = 0.8\nimage_size = 224\nbatch_size = 64\n\nrandom_seed = 100\n","metadata":{"id":"AzLJduR2Kq4H","execution":{"iopub.status.busy":"2022-06-15T18:05:49.160906Z","iopub.execute_input":"2022-06-15T18:05:49.161417Z","iopub.status.idle":"2022-06-15T18:05:49.189513Z","shell.execute_reply.started":"2022-06-15T18:05:49.16128Z","shell.execute_reply":"2022-06-15T18:05:49.188821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load Packages","metadata":{"id":"rMSLI6BwyuH-"}},{"cell_type":"markdown","source":"## Install Required Packages","metadata":{"id":"lNGjVc7VHSEW"}},{"cell_type":"markdown","source":"## Load","metadata":{"id":"4Lwq6PS5NRxP"}},{"cell_type":"code","source":"\n# Python Basics\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\npd.options.plotting.backend = \"plotly\"\nimport os, glob, cv2, PIL\nimport matplotlib.pyplot as plt\n\n# Torch\nimport torch\n\n## Torch Dataset and Loaders\nfrom torch.utils.data import Dataset, DataLoader\n\n## TorchVision\nfrom torchvision.transforms import ToTensor\nfrom torchvision import utils\nfrom torchvision.io import read_image\n\n## Torch Models\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# From Reference Notebook\n\nimport random\nfrom glob import glob\nimport os, shutil\nfrom tqdm import tqdm\ntqdm.pandas()\nimport time\nimport copy\nimport joblib\nfrom collections import defaultdict\nimport gc\nfrom IPython import display as ipd\n\n# visualization\nimport cv2\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Rectangle\n\n# Sklearn\nfrom sklearn.model_selection import StratifiedKFold, KFold, StratifiedGroupKFold\n\n# PyTorch \nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.cuda import amp\n\n#import timm\n\n# Albumentations for augmentations\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\n\n\n# Albumentations for augmentations\n#import albumentations as A\n#from albumentations.pytorch import ToTensorV2\n\n#import rasterio\n#from joblib import Parallel, delayed\n\n# For colored terminal text\n#from colorama import Fore, Back, Style\n#c_  = Fore.GREEN\n#sr_ = Style.RESET_ALL\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# For descriptive error messages\nos.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"","metadata":{"_uuid":"29968e4e-4de9-47ee-980e-f1ef5c6eae61","_cell_guid":"8bf8e92a-d19c-4ba1-968a-00537c2b13ae","jupyter":{"outputs_hidden":false},"scrolled":true,"id":"1iSsIJ5AyuIC","collapsed":false,"execution":{"iopub.status.busy":"2022-06-15T18:05:49.191106Z","iopub.execute_input":"2022-06-15T18:05:49.191466Z","iopub.status.idle":"2022-06-15T18:05:52.738935Z","shell.execute_reply.started":"2022-06-15T18:05:49.191432Z","shell.execute_reply":"2022-06-15T18:05:52.738088Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prepare Environment","metadata":{"id":"lXQqAMyKHiKi"}},{"cell_type":"markdown","source":"## Reproducibility","metadata":{"id":"jDh-7kJnLYrE"}},{"cell_type":"code","source":"import numpy as np\ndef set_seed(seed = 42):\n    '''Sets the seed of the entire notebook so results are the same every time we run.\n    This is for REPRODUCIBILITY.'''\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    # When running on the CuDNN backend, two further options must be set\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    # Set a fixed value for the hash seed\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    print('> SEEDING DONE')\n    \nset_seed(random_seed)","metadata":{"id":"xPAe_JZILaNB","outputId":"c3afe107-9336-4d69-9d20-f82023823ad6","execution":{"iopub.status.busy":"2022-06-15T18:05:52.740546Z","iopub.execute_input":"2022-06-15T18:05:52.74124Z","iopub.status.idle":"2022-06-15T18:05:52.751476Z","shell.execute_reply.started":"2022-06-15T18:05:52.741205Z","shell.execute_reply":"2022-06-15T18:05:52.75061Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%load_ext autoreload\n%autoreload 2","metadata":{"id":"Squ3oWcxJzn5","execution":{"iopub.status.busy":"2022-06-15T18:05:52.753542Z","iopub.execute_input":"2022-06-15T18:05:52.754068Z","iopub.status.idle":"2022-06-15T18:05:52.805733Z","shell.execute_reply.started":"2022-06-15T18:05:52.754028Z","shell.execute_reply":"2022-06-15T18:05:52.805052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Define Functions and Classes\n","metadata":{"id":"nFCAgq_bINUj"}},{"cell_type":"markdown","source":"## Utils","metadata":{"id":"9amPRaFvIC2w"}},{"cell_type":"code","source":"# Functions\ndef rle2mask(mask_rle, shape, label=1):\n    \"\"\"\n    mask_rle: run-length as string formatted (start length)\n    shape: (height,width) of array to return\n    Returns numpy array, 1 - mask, 0 - background\n\n    \"\"\"\n    \n    img = np.zeros(shape[0] * shape[1], dtype=np.uint8)\n    \n    if mask_rle != 'nan':\n        s = mask_rle.split()\n        starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n        starts -= 1\n        ends = starts + lengths\n\n        for lo, hi in zip(starts, ends):\n            img[lo:hi] = label\n            \n    return img.reshape(shape)  # Needed to align to RLE direction\n\ndef show_img(img, mask=None):\n    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n#     img = clahe.apply(img)\n#     plt.figure(figsize=(10,10))\n    plt.imshow(img, cmap='bone')\n    \n    if mask is not None:\n        # plt.imshow(np.ma.masked_where(mask!=1, mask), alpha=0.5, cmap='autumn')\n        plt.imshow(mask, alpha=0.5)\n        handles = [Rectangle((0,0),1,1, color=_c) for _c in [(0.667,0.0,0.0), (0.0,0.667,0.0), (0.0,0.0,0.667)]]\n        labels = [\"Large Bowel\", \"Small Bowel\", \"Stomach\"]\n        plt.legend(handles,labels)\n    plt.axis('off')\n    \n    \ndef plot_batch(imgs, msks, size=3):\n    plt.figure(figsize=(5*5, 5))\n    for idx in range(size):\n        plt.subplot(1, 5, idx+1)\n        img = imgs[idx,].permute((1, 2, 0)).numpy()*255.0\n        img = img.astype('uint8')\n        msk = msks[idx,].permute((1, 2, 0)).numpy()*255.0\n        show_img(img, msk)\n    plt.tight_layout()\n    plt.show()\n    \n    \ndef load_img(path):\n    img = cv2.imread(path, cv2.IMREAD_UNCHANGED)\n    img = np.tile(img[...,None], [1, 1, 3]) # gray to rgb\n    img = img.astype('float32') # original is uint16\n    mx = np.max(img)\n    if mx:\n        img/=mx # scale image to [0, 1]\n    return img","metadata":{"id":"CGWRYER8IeGf","execution":{"iopub.status.busy":"2022-06-15T18:05:52.807393Z","iopub.execute_input":"2022-06-15T18:05:52.807663Z","iopub.status.idle":"2022-06-15T18:05:52.861381Z","shell.execute_reply.started":"2022-06-15T18:05:52.807623Z","shell.execute_reply":"2022-06-15T18:05:52.860483Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Prepare Training Data","metadata":{"id":"Uy6wz_wIIEfr"}},{"cell_type":"code","source":"def parse_case_day_slice_info(row):\n    # Row-wise function to parse [Case, Day, Slice] numbers from Train.csv\n    id_strings = row['id'].split('_')\n    case_number = id_strings[0]\n    day_number = id_strings[1]\n    slice_number = id_strings[3]\n    return case_number, day_number, slice_number\n\ndef parse_image_filename(row):\n    # Row-wise function to parse [Case, Day, Slice, height, width, h_pixels, w_pixels] numbers from FilaName\n    # To be joined with Annotations, also used for scaling\n    \n    #if file_name is None:\n    #    file_name_string = row['file_name']\n    #    print('Parsing String {}'.format(filename))\n        \n    relevant_string = file_name_string.split('slice')[1].split('_')\n    \n    slice_number = relevant_string[1]\n    height = relevant_string[2]\n    width = relevant_string[3]\n    h_pixels = relevant_string[4]\n    w_pixels = relevant_string[5].rstrip('.png')\n    \n    case_number = file_name_string.split('train/')[1].split('/')[0]\n    day_number = file_name_string.split('train/')[1].split('/')[1].split('_')[1]\n    day_id = int(day_number.strip('day'))\n    \n    return case_number, day_number, day_id, slice_number, height, width, h_pixels, w_pixels\n\ndef parse_image_info(row):\n    # Row-wise function to parse [Case, Day, Slice, height, width, h_pixels, w_pixels] numbers from FilaName\n    # To be joined with Annotations, also used for scaling\n    \n    relevant_string = row['file_name'].split('slice')[1].split('_')\n    slice_number = relevant_string[1]\n    height = relevant_string[2]\n    width = relevant_string[3]\n    h_pixels = relevant_string[4]\n    w_pixels = relevant_string[5].rstrip('.png')\n    \n    case_number = row['file_name'].split('train/')[1].split('/')[0]\n    day_number = row['file_name'].split('train/')[1].split('/')[1].split('_')[1]\n    day_id = int(day_number.strip('day'))\n    \n    return case_number, day_number, day_id, slice_number, height, width, h_pixels, w_pixels\n\n\ndef get_training_masks(train_csv_location = train_csv_location):\n    # Returns MaskRLE for each case_day_slice combination.\n    # Each row referenced by case_day_slice_id\n    # This portion has been referenced from another Kaggle notebook. Insert reference.\n    # Get CSV of annotated mask information\n    print('Getting All Available annotations for the training')\n    train_csv = pd.read_csv(train_csv_location)\n\n    # Change layout to have each slice Info on one row\n    train_csv = pd.pivot_table(\n        train_csv, \n        values='segmentation', \n        index='id',\n        columns='class',\n        aggfunc=np.max\n    ).astype(str).fillna('')\n    \n    train_csv = train_csv.reset_index()\n    \n    # Transform train_csv to have clear information - parsed columns\n    train_csv['id_info'] = train_csv.apply(parse_case_day_slice_info, axis = 1)\n    train_csv[['case_number','day_number', 'slice_number']] = pd.DataFrame(train_csv.id_info.tolist(), index= train_csv.index)\n\n    return train_csv\n\n\ndef get_training_image_names(location = training_images_directory):\n    \n    # Find all files in mentioned location, Make a DF for later use\n    print('Getting Filenames of all available training images')\n    print('From location {}'.format(location))\n    location = training_images_directory\n    print(location)\n    x = glob(location + '*/*/*/*')\n    print(len(x))\n    image_info = pd.DataFrame(x, columns = ['file_name'])\n    \n    image_info['slice_info'] = image_info.apply(parse_image_info, axis = 1)\n    image_info[['case_number','day_number', 'day_id', 'slice_number','height', 'width', 'h_pixels', 'w_pixels']] = pd.DataFrame(\n        image_info.slice_info.tolist(), \n        index= image_info.index\n    )\n\n    return image_info\n\ndef get_training_dataset_info():\n    # Fetches Image F\n    # Output: DF with [case_day_slice_id, file_name_location, target_mask_1, target_mask_2, target_mask_3]\n    train_csv = get_training_masks()\n    # ow that we know all the masks, get all the case day slice image file name and locations for easy retrieval\n    train_image_names = get_training_image_names()\n    \n    joined_data = train_csv.merge(train_image_names, on = ['case_number', 'day_number', 'slice_number'], how = 'inner')\n    print('Joined Data has shape {}'.format(joined_data.shape))\n    \n    return joined_data","metadata":{"id":"0LI7MhkzInBE","execution":{"iopub.status.busy":"2022-06-15T18:05:52.862711Z","iopub.execute_input":"2022-06-15T18:05:52.863193Z","iopub.status.idle":"2022-06-15T18:05:52.919935Z","shell.execute_reply.started":"2022-06-15T18:05:52.863156Z","shell.execute_reply":"2022-06-15T18:05:52.918843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Custom DataSet Generator","metadata":{"id":"s32qdGAMIsrz"}},{"cell_type":"code","source":"\n\nclass CustomImageDataset(Dataset):\n# Takes in location, outputs an image - \n# Each image is a [case_number, day_number, slice_number]\n\n    def __init__(self, image_mask_associations, img_dir = training_images_directory, transform=None, labels = True):\n\n        #print('Initilaising Data at location {}'.format(img_dir))\n        self.image_mask_associations = image_mask_associations\n        self.img_dir = img_dir\n        self.transforms = transform\n        self.labels = labels\n        \n\n    def __len__(self):\n        return len(self.image_mask_associations)\n\n    def get_masks_for_image(self, idx):\n#        print('Getting masks for img at location {}'.format(img_path))\n        # Returns the three channels masks for the given Image File Name\n        rle_stomach = self.image_mask_associations.loc[idx]['stomach']\n        rle_lb = self.image_mask_associations.loc[idx]['large_bowel']\n        rle_sb = self.image_mask_associations.loc[idx]['small_bowel']\n\n        img_size_h = int(self.image_mask_associations.loc[idx]['height'])\n        img_size_w = int(self.image_mask_associations.loc[idx]['width'])\n\n        mask_stomach = rle2mask(rle_stomach, shape=[img_size_w,img_size_h])\n        mask_lb = rle2mask(rle_lb, shape=[img_size_w,img_size_h])\n        mask_sb = rle2mask(rle_sb, shape=[img_size_w,img_size_h])\n        \n        masks = np.stack((mask_stomach, mask_lb, mask_sb), axis = 2).astype('uint8')\n        #masks/=255.0\n        return masks\n\n        \n    def __getitem__(self, idx):\n        # Get location of Image based on idx, from image_mask_associations\n        img_path = self.image_mask_associations['file_name'].iloc[idx]\n        img = []\n        img = load_img(img_path)\n        \n        # In case labels are required, fetch them. Else, transform Images and return\n        if self.labels:\n            msks = self.get_masks_for_image(idx)\n            \n            if self.transforms:\n                data = self.transforms(image = img, mask = msks)\n                img = data['image']\n                msks = data['mask']\n            \n            img = np.transpose(img, (2, 0, 1))\n            msks = np.transpose(msks, (2, 0, 1))\n            \n            return torch.tensor(img), torch.tensor(msks)\n        else:\n            if self.transforms:\n                data = self.transforms(image = img)\n                img = data['image']\n                \n            img = np.transpose(img, (2, 0, 1))\n            \n            return torch.tensor(img)","metadata":{"id":"r9LLos89Iuy-","execution":{"iopub.status.busy":"2022-06-15T18:05:52.921127Z","iopub.execute_input":"2022-06-15T18:05:52.921466Z","iopub.status.idle":"2022-06-15T18:05:52.972014Z","shell.execute_reply.started":"2022-06-15T18:05:52.921431Z","shell.execute_reply":"2022-06-15T18:05:52.971256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#data_transforms = A.Compose([\n#  \t A.Resize(*[224,224])\n# ]\n#)\nimg_size = [224,224]\ndata_transforms = {\n    \"train\": A.Compose([\n        A.Resize(*img_size, interpolation=cv2.INTER_NEAREST),\n        A.HorizontalFlip(p=0.5),\n#         A.VerticalFlip(p=0.5),\n        A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.05, rotate_limit=10, p=0.5),\n        A.OneOf([\n            A.GridDistortion(num_steps=5, distort_limit=0.05, p=1.0),\n# #             A.OpticalDistortion(distort_limit=0.05, shift_limit=0.05, p=1.0),\n            A.ElasticTransform(alpha=1, sigma=50, alpha_affine=50, p=1.0)\n        ], p=0.25),\n        A.CoarseDropout(max_holes=8, max_height=img_size[0]//20, max_width=img_size[1]//20,\n                         min_holes=5, fill_value=0, mask_fill_value=0, p=0.5),\n        ], p=1.0),\n    \n    \"valid\": A.Compose([\n        A.Resize(*img_size, interpolation=cv2.INTER_NEAREST),\n        ], p=1.0)\n}","metadata":{"execution":{"iopub.status.busy":"2022-06-15T18:05:52.973118Z","iopub.execute_input":"2022-06-15T18:05:52.973468Z","iopub.status.idle":"2022-06-15T18:05:53.017659Z","shell.execute_reply.started":"2022-06-15T18:05:52.973435Z","shell.execute_reply":"2022-06-15T18:05:53.016971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train / Validation Split","metadata":{"id":"hG3JrAMKK3hO"}},{"cell_type":"code","source":"# Get Image-Mask Associations\nimage_mask_associations = get_training_dataset_info()","metadata":{"execution":{"iopub.status.busy":"2022-06-15T18:05:53.018577Z","iopub.execute_input":"2022-06-15T18:05:53.019452Z","iopub.status.idle":"2022-06-15T18:06:09.51921Z","shell.execute_reply.started":"2022-06-15T18:05:53.019417Z","shell.execute_reply":"2022-06-15T18:06:09.518336Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cases = image_mask_associations['case_number'].unique()\ntraining_set_membership = int(training_prop*len(cases))\ntraining_set_cases = list(set(random.sample(list(cases), int(0.8*len(cases)))))\nvalidation_set_cases = [x for x in cases if x not in training_set_cases]\n\ndef validation_set_split(row):\n    if row['case_number'] in training_set_cases: \n        return 'training' \n    elif row['case_number'] in validation_set_cases:\n        return 'validation'\n    else: \n        return 'leave_for_now'\n    \n\nimage_mask_associations['t_v_set'] = image_mask_associations.apply(validation_set_split, axis = 1)\nimage_mask_associations.groupby('t_v_set')['case_number'].count()","metadata":{"id":"pXKboH5IK5oq","outputId":"54948019-ae21-4c92-d741-22a631768684","execution":{"iopub.status.busy":"2022-06-15T18:06:09.522267Z","iopub.execute_input":"2022-06-15T18:06:09.523122Z","iopub.status.idle":"2022-06-15T18:06:09.812222Z","shell.execute_reply.started":"2022-06-15T18:06:09.523087Z","shell.execute_reply":"2022-06-15T18:06:09.811498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Transforms and Prepare Loaders","metadata":{"id":"F3TvTEJagZ0X"}},{"cell_type":"code","source":"image_mask_associations.groupby('case_number').count()","metadata":{"execution":{"iopub.status.busy":"2022-06-15T18:06:09.813519Z","iopub.execute_input":"2022-06-15T18:06:09.813904Z","iopub.status.idle":"2022-06-15T18:06:09.910369Z","shell.execute_reply.started":"2022-06-15T18:06:09.813867Z","shell.execute_reply":"2022-06-15T18:06:09.90949Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_mask_associations.groupby('t_v_set').count()","metadata":{"execution":{"iopub.status.busy":"2022-06-15T18:06:09.911823Z","iopub.execute_input":"2022-06-15T18:06:09.912179Z","iopub.status.idle":"2022-06-15T18:06:09.998214Z","shell.execute_reply.started":"2022-06-15T18:06:09.912143Z","shell.execute_reply":"2022-06-15T18:06:09.997467Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_mask_associations.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-15T18:06:09.999517Z","iopub.execute_input":"2022-06-15T18:06:09.999906Z","iopub.status.idle":"2022-06-15T18:06:10.062593Z","shell.execute_reply.started":"2022-06-15T18:06:09.999869Z","shell.execute_reply":"2022-06-15T18:06:10.0617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prepare_loaders(batch_size, transforms = data_transforms):\n\n    train_df = image_mask_associations.query(\"t_v_set == 'training'\").reset_index(drop=True)\n    valid_df = image_mask_associations.query(\"t_v_set == 'validation'\").reset_index(drop=True)\n    #if debug:\n    #    train_df = train_df.head(32*5).query(\"empty==0\")\n    #    valid_df = valid_df.head(32*3).query(\"empty==0\")\n    train_dataset = CustomImageDataset(train_df, transform=transforms['train'])\n    valid_dataset = CustomImageDataset(valid_df, transform=transforms['valid'])\n\n    train_loader = DataLoader(train_dataset, \n                              batch_size = batch_size, \n                              num_workers=4, \n                              shuffle=True, \n                              pin_memory=True, \n                             drop_last=False\n                              )\n    \n    valid_loader = DataLoader(valid_dataset,\n                              batch_size = batch_size, \n                              num_workers=4, \n                              shuffle=False, \n                              pin_memory=True)\n    \n    return train_loader, valid_loader","metadata":{"id":"hHUPw8b3gZCD","execution":{"iopub.status.busy":"2022-06-15T18:06:10.064059Z","iopub.execute_input":"2022-06-15T18:06:10.064656Z","iopub.status.idle":"2022-06-15T18:06:10.115999Z","shell.execute_reply.started":"2022-06-15T18:06:10.064542Z","shell.execute_reply":"2022-06-15T18:06:10.115047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## UNet Model Definitions","metadata":{"id":"xJP7LfVMIGnI"}},{"cell_type":"code","source":"class UNET(nn.Module):\n    \n    def __init__(self, in_channels=3, classes=1):\n        super(UNET, self).__init__()\n        self.layers = [in_channels, 64, 128, 256, 512, 1024, 2048]\n        \n        self.double_conv_downs = nn.ModuleList(\n            [self.__double_conv(layer, layer_n) for layer, layer_n in zip(self.layers[:-1], self.layers[1:])])\n        \n        self.up_trans = nn.ModuleList(\n            [nn.ConvTranspose2d(layer, layer_n, kernel_size=2, stride=2)\n             for layer, layer_n in zip(self.layers[::-1][:-2], self.layers[::-1][1:-1])])\n            \n        self.double_conv_ups = nn.ModuleList(\n        [self.__double_conv(layer, layer//2) for layer in self.layers[::-1][:-2]])\n        \n        self.max_pool_2x2 = nn.MaxPool2d(kernel_size=2, stride=2)\n        \n        self.final_conv = nn.Conv2d(64, classes, kernel_size=1)\n\n        \n    def __double_conv(self, in_channels, out_channels):\n        conv = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True)\n        )\n        return conv\n    \n    def forward(self, x):\n        # down layers\n        concat_layers = []\n        \n        for down in self.double_conv_downs:\n            x = down(x)\n            if down != self.double_conv_downs[-1]:\n                concat_layers.append(x)\n                x = self.max_pool_2x2(x)\n        \n        concat_layers = concat_layers[::-1]\n        \n        # up layers\n        for up_trans, double_conv_up, concat_layer  in zip(self.up_trans, self.double_conv_ups, concat_layers):\n            x = up_trans(x)\n            if x.shape != concat_layer.shape:\n                x = TF.resize(x, concat_layer.shape[2:])\n            \n            concatenated = torch.cat((concat_layer, x), dim=1)\n            x = double_conv_up(concatenated)\n            \n        x = self.final_conv(x)\n        #x = nn.ReLU(x)\n        \n        return x ","metadata":{"id":"Ia5zqDzTI2SI","execution":{"iopub.status.busy":"2022-06-15T18:06:10.11734Z","iopub.execute_input":"2022-06-15T18:06:10.11779Z","iopub.status.idle":"2022-06-15T18:06:10.170871Z","shell.execute_reply.started":"2022-06-15T18:06:10.117753Z","shell.execute_reply":"2022-06-15T18:06:10.170033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Block(nn.Module):\n    def __init__(self, in_ch, out_ch):\n        super().__init__()\n        self.conv1 = nn.Conv2d(in_ch, out_ch, 3)\n        self.relu  = nn.ReLU()\n        self.conv2 = nn.Conv2d(out_ch, out_ch, 3)\n    \n    def forward(self, x):\n        return self.conv2(self.relu(self.conv1(x)))\n\n\nclass Encoder(nn.Module):\n    def __init__(self, chs=(3,64,128,256,512,1024)):\n        super().__init__()\n        self.enc_blocks = nn.ModuleList([Block(chs[i], chs[i+1]) for i in range(len(chs)-1)])\n        self.pool       = nn.MaxPool2d(2)\n    \n    def forward(self, x):\n        ftrs = []\n        for block in self.enc_blocks:\n            x = block(x)\n            ftrs.append(x)\n            x = self.pool(x)\n        return ftrs\n\n\nclass Decoder(nn.Module):\n    def __init__(self, chs=(1024, 512, 256, 128, 64)):\n        super().__init__()\n        self.chs         = chs\n        self.upconvs    = nn.ModuleList([nn.ConvTranspose2d(chs[i], chs[i+1], 2, 2) for i in range(len(chs)-1)])\n        self.dec_blocks = nn.ModuleList([Block(chs[i], chs[i+1]) for i in range(len(chs)-1)]) \n        \n    def forward(self, x, encoder_features):\n        for i in range(len(self.chs)-1):\n            x        = self.upconvs[i](x)\n            enc_ftrs = self.crop(encoder_features[i], x)\n            x        = torch.cat([x, enc_ftrs], dim=1)\n            x        = self.dec_blocks[i](x)\n        return x\n    \n    def crop(self, enc_ftrs, x):\n        _, _, H, W = x.shape\n        enc_ftrs   = torchvision.transforms.CenterCrop([H, W])(enc_ftrs)\n        return enc_ftrs\n\n\nclass UNet(nn.Module):\n    def __init__(self, enc_chs=(3,64,128,256,512,1024), dec_chs=(1024, 512, 256, 128, 64), num_class=1, retain_dim=False, out_sz=(572,572)):\n        super().__init__()\n        self.encoder     = Encoder(enc_chs)\n        self.decoder     = Decoder(dec_chs)\n        self.head        = nn.Conv2d(dec_chs[-1], num_class, 1)\n        self.retain_dim  = retain_dim\n\n    def forward(self, x):\n        enc_ftrs = self.encoder(x)\n        out      = self.decoder(enc_ftrs[::-1][0], enc_ftrs[::-1][1:])\n        out      = self.head(out)\n        if self.retain_dim:\n            out = F.interpolate(out, out_sz)\n        return out\n    \n    ","metadata":{"execution":{"iopub.status.busy":"2022-06-15T18:06:10.173889Z","iopub.execute_input":"2022-06-15T18:06:10.17416Z","iopub.status.idle":"2022-06-15T18:06:10.228437Z","shell.execute_reply.started":"2022-06-15T18:06:10.174136Z","shell.execute_reply":"2022-06-15T18:06:10.227496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_model():\n    model = smp.Unet(\n        encoder_name='efficientnet-b2',      # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n        #encoder_weights=\"imagenet\",     # use `imagenet` pre-trained weights for encoder initialization\n        in_channels=3,                  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n        classes=3,        # model output channels (number of classes in your dataset)\n        activation=None,\n    )\n    model.to(device)\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-06-15T18:06:10.229606Z","iopub.execute_input":"2022-06-15T18:06:10.230231Z","iopub.status.idle":"2022-06-15T18:06:10.271512Z","shell.execute_reply.started":"2022-06-15T18:06:10.230189Z","shell.execute_reply":"2022-06-15T18:06:10.270549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-06-15T18:06:10.272939Z","iopub.execute_input":"2022-06-15T18:06:10.27329Z","iopub.status.idle":"2022-06-15T18:06:10.43211Z","shell.execute_reply.started":"2022-06-15T18:06:10.273243Z","shell.execute_reply":"2022-06-15T18:06:10.430565Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Losses","metadata":{"id":"t7UQjujDgiS1"}},{"cell_type":"code","source":"!pip install segmentation_models_pytorch\nimport segmentation_models_pytorch as smp\n\nJaccardLoss = smp.losses.JaccardLoss(mode='multilabel')\nDiceLoss    = smp.losses.DiceLoss(mode='multilabel')\nBCELoss     = smp.losses.SoftBCEWithLogitsLoss()\nLovaszLoss  = smp.losses.LovaszLoss(mode='multilabel', per_image=False)\nTverskyLoss = smp.losses.TverskyLoss(mode='multilabel', log_loss=False)\n\ndef dice_coef(y_true, y_pred, thr=0.5, dim=(2,3), epsilon=0.001):\n    y_true = y_true.to(torch.float32)\n    y_pred = (y_pred>thr).to(torch.float32)\n    inter = (y_true*y_pred).sum(dim=dim)\n    den = y_true.sum(dim=dim) + y_pred.sum(dim=dim)\n    dice = ((2*inter+epsilon)/(den+epsilon)).mean(dim=(1,0))\n    return dice\n\ndef iou_coef(y_true, y_pred, thr=0.5, dim=(2,3), epsilon=0.001):\n    y_true = y_true.to(torch.float32)\n    y_pred = (y_pred>thr).to(torch.float32)\n    inter = (y_true*y_pred).sum(dim=dim)\n    union = (y_true + y_pred - y_true*y_pred).sum(dim=dim)\n    iou = ((inter+epsilon)/(union+epsilon)).mean(dim=(1,0))\n    return iou\n\ndef criterion(y_pred, y_true):\n    return DiceLoss(y_pred, y_true)#0.5*BCELoss(y_pred, y_true) + 0.5*TverskyLoss(y_pred, y_true)","metadata":{"id":"WFTf66KWgjs7","execution":{"iopub.status.busy":"2022-06-15T18:06:10.433564Z","iopub.execute_input":"2022-06-15T18:06:10.434103Z","iopub.status.idle":"2022-06-15T18:06:33.763148Z","shell.execute_reply.started":"2022-06-15T18:06:10.434066Z","shell.execute_reply":"2022-06-15T18:06:33.762277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Begin Modelling Process","metadata":{"id":"HYjK5G7_KwOj"}},{"cell_type":"markdown","source":"## Train One Epoch","metadata":{"id":"qZQv3_3ChWI-"}},{"cell_type":"code","source":"def train_one_epoch(model, optimizer, scheduler, dataloader, device, epoch):\n    model.train()\n    scaler = amp.GradScaler()\n    \n    dataset_size = 0\n    running_loss = 0.0\n    \n    pbar = tqdm(enumerate(dataloader), total=len(dataloader), desc='Train ')\n    for step, (images, masks) in pbar:         \n        images = images.to(device, dtype=torch.float)\n        masks  = masks.to(device, dtype=torch.float)\n        \n        batch_size = images.size(0)\n        \n        with amp.autocast(enabled=True):\n            y_pred = model(images)\n            loss   = dice_coef(y_pred, masks)\n            loss   = loss / n_accumulate\n            \n        scaler.scale(loss).backward()\n    \n        if (step + 1) % n_accumulate == 0:\n            scaler.step(optimizer)\n            scaler.update()\n\n            # zero the parameter gradients\n            optimizer.zero_grad()\n\n            if scheduler is not None:\n                scheduler.step(loss)\n                \n        running_loss += (loss.item() * batch_size)\n        dataset_size += batch_size\n        \n        epoch_loss = running_loss / dataset_size\n        \n        mem = torch.cuda.memory_reserved() / 1E9 if torch.cuda.is_available() else 0\n        current_lr = optimizer.param_groups[0]['lr']\n        pbar.set_postfix(train_loss=f'{epoch_loss:0.4f}',\n                        lr=f'{current_lr:0.5f}',\n                        gpu_mem=f'{mem:0.2f} GB')\n    torch.cuda.empty_cache()\n    gc.collect()\n    \n    return epoch_loss","metadata":{"id":"HP924Bj2hNhh","execution":{"iopub.status.busy":"2022-06-15T18:06:33.764441Z","iopub.execute_input":"2022-06-15T18:06:33.764831Z","iopub.status.idle":"2022-06-15T18:06:33.836037Z","shell.execute_reply.started":"2022-06-15T18:06:33.764794Z","shell.execute_reply":"2022-06-15T18:06:33.83512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Validate One Epoch","metadata":{"id":"bCxhvValhbOT"}},{"cell_type":"code","source":"@torch.no_grad()\ndef valid_one_epoch(model, dataloader, device, epoch):\n    model.eval()\n    \n    dataset_size = 0\n    running_loss = 0.0\n    \n    val_scores = []\n    \n    pbar = tqdm(enumerate(dataloader), total=len(dataloader), desc='Valid ')\n    for step, (images, masks) in pbar:        \n        images  = images.to(device, dtype=torch.float)\n        masks   = masks.to(device, dtype=torch.float)\n        \n        batch_size = images.size(0)\n        \n        y_pred  = model(images)\n        loss    = criterion(y_pred, masks)\n        \n        running_loss += (loss.item() * batch_size)\n        dataset_size += batch_size\n        \n        epoch_loss = running_loss / dataset_size\n        \n        y_pred = nn.Sigmoid()(y_pred)\n        val_dice = dice_coef(masks, y_pred).cpu().detach().numpy()\n        val_jaccard = iou_coef(masks, y_pred).cpu().detach().numpy()\n        val_scores.append([val_dice, val_jaccard])\n        \n        mem = torch.cuda.memory_reserved() / 1E9 if torch.cuda.is_available() else 0\n        current_lr = optimizer.param_groups[0]['lr']\n        pbar.set_postfix(valid_loss=f'{epoch_loss:0.4f}',\n                        lr=f'{current_lr:0.7f}',\n                        gpu_memory=f'{mem:0.2f} GB')\n    val_scores  = np.mean(val_scores, axis=0)\n    torch.cuda.empty_cache()\n    gc.collect()\n    \n    return epoch_loss, val_scores","metadata":{"id":"mjXCd8mhhaav","execution":{"iopub.status.busy":"2022-06-15T18:06:33.837464Z","iopub.execute_input":"2022-06-15T18:06:33.837878Z","iopub.status.idle":"2022-06-15T18:06:33.907481Z","shell.execute_reply.started":"2022-06-15T18:06:33.837843Z","shell.execute_reply":"2022-06-15T18:06:33.906641Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Run Training","metadata":{"id":"xIhdGOBohdhJ"}},{"cell_type":"code","source":"\ndef run_training(model, optimizer, scheduler, device, num_epochs):\n    # To automatically log gradients\n    #wandb.watch(model, log_freq=100)\n    \n    if torch.cuda.is_available():\n        print(\"cuda: {}\\n\".format(torch.cuda.get_device_name()))\n    \n    start = time.time()\n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_dice      = -np.inf\n    best_epoch     = -1\n    history = defaultdict(list)\n    \n    for epoch in range(1, num_epochs + 1): \n        gc.collect()\n        print(f'Epoch {epoch}/{num_epochs}', end='')\n        train_loss = train_one_epoch(model, optimizer, scheduler, \n                                           dataloader=train_loader, \n                                           device=device, epoch=epoch)\n        \n        val_loss, val_scores = valid_one_epoch(model, valid_loader, \n                                                 device=device, \n                                                 epoch=epoch)\n        val_dice, val_jaccard = val_scores\n    \n        history['Train Loss'].append(train_loss)\n        history['Valid Loss'].append(val_loss)\n        history['Valid Dice'].append(val_dice)\n        history['Valid Jaccard'].append(val_jaccard)\n        \n        # Log the metrics\n        #wandb.log({\"Train Loss\": train_loss, \n        #           \"Valid Loss\": val_loss,\n        #           \"Valid Dice\": val_dice,\n        #           \"Valid Jaccard\": val_jaccard,\n        #           \"LR\":scheduler.get_last_lr()[0]})\n        \n        print(f'Valid Dice: {val_dice:0.4f} | Valid Jaccard: {val_jaccard:0.4f}')\n        \n        # deep copy the model\n        if val_dice >= best_dice:\n            print(f\"Valid Score Improved ({best_dice:0.4f} ---> {val_dice:0.4f})\")\n            best_dice    = val_dice\n            best_jaccard = val_jaccard\n            best_epoch   = epoch\n            #run.summary[\"Best Dice\"]    = best_dice\n            #run.summary[\"Best Jaccard\"] = best_jaccard\n            #run.summary[\"Best Epoch\"]   = best_epoch\n            best_model_wts = copy.deepcopy(model.state_dict())\n            PATH = f\"best_epoch.bin\"\n            torch.save(model.state_dict(), PATH)\n            # Save a model file from the current directory\n            #wandb.save(PATH)\n            print(f\"Model Saved\")\n            \n        last_model_wts = copy.deepcopy(model.state_dict())\n        PATH = f\"last_epoch_{epoch}.bin\"\n        torch.save(model.state_dict(), PATH)\n            \n        print(); print()\n    \n    end = time.time()\n    time_elapsed = end - start\n    print('Training complete in {:.0f}h {:.0f}m {:.0f}s'.format(\n        time_elapsed // 3600, (time_elapsed % 3600) // 60, (time_elapsed % 3600) % 60))\n    print(\"Best Score: {:.4f}\".format(best_jaccard))\n    \n    # load best model weights\n    model.load_state_dict(best_model_wts)\n    \n    return model, history","metadata":{"id":"ZbU9anORhjIj","execution":{"iopub.status.busy":"2022-06-15T18:06:33.909143Z","iopub.execute_input":"2022-06-15T18:06:33.909598Z","iopub.status.idle":"2022-06-15T18:06:33.982096Z","shell.execute_reply.started":"2022-06-15T18:06:33.909548Z","shell.execute_reply":"2022-06-15T18:06:33.98134Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Optimizer","metadata":{"id":"Yh2QIfwghorR"}},{"cell_type":"code","source":"\n\"\"\"\ndef fetch_scheduler(optimizer):\n    if CFG.scheduler == 'CosineAnnealingLR':\n        scheduler = lr_scheduler.CosineAnnealingLR(optimizer,T_max=CFG.T_max, \n                                                   eta_min=CFG.min_lr)\n    elif CFG.scheduler == 'CosineAnnealingWarmRestarts':\n        scheduler = lr_scheduler.CosineAnnealingWarmRestarts(optimizer,T_0=CFG.T_0, \n                                                             eta_min=CFG.min_lr)\n    elif CFG.scheduler == 'ReduceLROnPlateau':\n        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer,\n                                                   mode='min',\n                                                   factor=0.1,\n                                                   patience=7,\n                                                   threshold=0.0001,\n                                                   min_lr=CFG.min_lr,)\n    elif CFG.scheduer == 'ExponentialLR':\n        scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=0.85)\n    elif CFG.scheduler == None:\n        return None\n        \n    return scheduler\n\n\"\"\"","metadata":{"id":"bBwzSCLvhp9D","outputId":"ea41ccdf-9323-4477-f859-b85c07e759bb","execution":{"iopub.status.busy":"2022-06-15T18:06:33.983407Z","iopub.execute_input":"2022-06-15T18:06:33.984177Z","iopub.status.idle":"2022-06-15T18:06:34.049269Z","shell.execute_reply.started":"2022-06-15T18:06:33.984149Z","shell.execute_reply":"2022-06-15T18:06:34.048514Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gc\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-06-15T18:06:34.050339Z","iopub.execute_input":"2022-06-15T18:06:34.051236Z","iopub.status.idle":"2022-06-15T18:06:34.311767Z","shell.execute_reply.started":"2022-06-15T18:06:34.051198Z","shell.execute_reply":"2022-06-15T18:06:34.31088Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train Model","metadata":{"id":"GUySyhlYKzq3"}},{"cell_type":"code","source":"train_loader, valid_loader = prepare_loaders(batch_size = 128)","metadata":{"id":"YFVSv_vte68g","execution":{"iopub.status.busy":"2022-06-15T18:06:34.313331Z","iopub.execute_input":"2022-06-15T18:06:34.314013Z","iopub.status.idle":"2022-06-15T18:06:34.407602Z","shell.execute_reply.started":"2022-06-15T18:06:34.313976Z","shell.execute_reply":"2022-06-15T18:06:34.406861Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_accumulate  = max(1, 32//batch_size)\nepochs        = 15\nlr            = 2e-3\n\nmin_lr        = 1e-6\nT_max         = int(30000/batch_size*epochs)+50\nT_0           = 25\nwarmup_epochs = 0\nwd            = 1e-6\n\nmodel     = build_model()\n","metadata":{"execution":{"iopub.status.busy":"2022-06-15T18:06:34.408886Z","iopub.execute_input":"2022-06-15T18:06:34.409305Z","iopub.status.idle":"2022-06-15T18:06:42.932207Z","shell.execute_reply.started":"2022-06-15T18:06:34.409264Z","shell.execute_reply":"2022-06-15T18:06:42.931363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=wd)\n\nscheduler = lr_scheduler.ReduceLROnPlateau(optimizer,\n                                                   mode='min',\n                                                   factor=0.005,\n                                                   patience=50,\n                                                   threshold=0.0001,\n                                                   min_lr=1e-6,\n                                           verbose = True)","metadata":{"execution":{"iopub.status.busy":"2022-06-15T18:06:42.933489Z","iopub.execute_input":"2022-06-15T18:06:42.933867Z","iopub.status.idle":"2022-06-15T18:06:43.001869Z","shell.execute_reply.started":"2022-06-15T18:06:42.933829Z","shell.execute_reply":"2022-06-15T18:06:43.000877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#unet = UNET(in_channels=1, classes=3).to(device)\n#optimizer = optim.SGD(unet.parameters(), lr=0.7, momentum=0.9)\n#scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=0.85)\n#scheduler = lr_scheduler.ReduceLROnPlateau(optimizer,\n#                                                   mode='min',\n#                                                   factor=0.5,\n#                                                   patience=10,\n#                                                   threshold=0.001,\n#                                                   min_lr = 1e-6, verbose = True)\n\nmodel, history = run_training(model, \n                              optimizer, \n                              scheduler,\n                              device= device,\n                              num_epochs= 25)\n#run.finish()\n#display(ipd.IFrame(run.url, width=1000, height=720))\n","metadata":{"id":"cgwTiAKuyuII","outputId":"848232f0-ea63-4721-f34a-57be4170ff43","execution":{"iopub.status.busy":"2022-06-15T18:06:43.005846Z","iopub.execute_input":"2022-06-15T18:06:43.006262Z","iopub.status.idle":"2022-06-15T18:07:39.506023Z","shell.execute_reply.started":"2022-06-15T18:06:43.006234Z","shell.execute_reply":"2022-06-15T18:07:39.504503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nimgs = a\nmsks = b\nidx = 10\nimg = imgs[idx,].permute((1, 2, 0)).numpy()*255.0\nimg = img.astype('uint8')\nmsk = msks[idx,].permute((1, 2, 0)).numpy()*255.0\noutput = pred[idx,].cpu().detach().permute((1, 2, 0)).numpy()\n\nplt.subplot(1, 5, 1)\nplt.imshow(img)\nplt.subplot(1, 5, 2)\nplt.imshow(msk)\nplt.subplot(1, 5, 3)\nplt.imshow(output)\"\"\"","metadata":{"execution":{"iopub.status.busy":"2022-06-15T18:07:39.507471Z","iopub.status.idle":"2022-06-15T18:07:39.508171Z","shell.execute_reply.started":"2022-06-15T18:07:39.507936Z","shell.execute_reply":"2022-06-15T18:07:39.507962Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\"\"\"# Display image and label.\ntrain_features, train_labels = next(iter(train_dataloader))\nprint(f\"Feature batch shape: {train_features.size()}\")\nprint(f\"Labels batch shape: {train_labels.size()}\")\nimg = train_features[0].squeeze()\nlabel = train_labels[0]\nplt.imshow(img, cmap=\"gray\")\nplt.imshow(np.moveaxis(np.asarray(label), 0, 2), cmap=\"gray\", alpha = 0.2)\nplt.show()\n#print(f\"Label: {label}\")\"\"\"","metadata":{"id":"AtWTCE34yuII","execution":{"iopub.status.busy":"2022-06-15T18:07:39.50946Z","iopub.status.idle":"2022-06-15T18:07:39.510139Z","shell.execute_reply.started":"2022-06-15T18:07:39.509906Z","shell.execute_reply":"2022-06-15T18:07:39.509929Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\n\nprint(f\"Using {device} device\")\n\n\nfor epoch in range(100):  # loop over the dataset multiple times\n\n  running_loss = 0.0\n  for i, data in enumerate(train_dataloader, 0):\n      # get the inputs; data is a list of [inputs, labels]\n      #print(i)\n      inputs, labels = data\n      inputs, labels = inputs.cuda(), labels.cuda() # add this line\n      # zero the parameter gradients\n      optimizer.zero_grad()\n\n      # forward + backward + optimize\n      outputs = unet(inputs)\n      loss = criterion(outputs, labels)\n      loss.backward()\n      optimizer.step()\n      #plt.imshow(outputs)\n      # print statistics\n      running_loss += loss.item()\n      \n      if i % 20 == 19:    # print every 2000 mini-batches\n          \n          print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n          print(running_loss)\n          running_loss = 0.0\n  scheduler.step(loss)\n    \nprint('Finished Training')\n\"\"\"","metadata":{"id":"U5U3dWaeyuIJ","execution":{"iopub.status.busy":"2022-06-15T18:07:39.511339Z","iopub.status.idle":"2022-06-15T18:07:39.512025Z","shell.execute_reply.started":"2022-06-15T18:07:39.511792Z","shell.execute_reply":"2022-06-15T18:07:39.511817Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Inspect Model","metadata":{}},{"cell_type":"markdown","source":"# Prediction","metadata":{}},{"cell_type":"code","source":"# TODO : Build Test from Images that do not have masks.\n#test_df = image_data_associations.query()\n#test_dataset = BuildDataset(df.query(\"fold==0 & empty==0\").sample(frac=1.0), label=False, \n                            #transforms=data_transforms['valid'])\n#test_loader  = DataLoader(test_dataset, batch_size=5, \n                        #  num_workers=4, shuffle=False, pin_memory=True)\ndef load_model(path):\n    model = build_model() #UNET(in_channels=1, classes=3).to(device)\n    model.load_state_dict(torch.load(path))\n    model.eval()\n    return model\n\nimgs, msks = next(iter(train_loader))\nimgs = imgs.to(device, dtype=torch.float)\nmsks = msks.to(device, dtype=torch.float)\n\npreds = []\n\nmodel2 = load_model(f\"best_epoch.bin\")\nwith torch.no_grad():\n    pred = model2(imgs)\n    pred = (nn.Sigmoid()(pred)>0.5).double()\n#preds.append(pred)\n    \nimgs  = imgs.cpu().detach()\npreds = pred#torch.mean(pred, dim=0).cpu().detach()\nprint(pred.min(), pred.max())\nplot_batch(imgs.cpu(), preds.cpu(), size=5)","metadata":{"id":"aUXYDQS3yuIN","execution":{"iopub.status.busy":"2022-06-15T18:07:39.513268Z","iopub.status.idle":"2022-06-15T18:07:39.513956Z","shell.execute_reply.started":"2022-06-15T18:07:39.513731Z","shell.execute_reply":"2022-06-15T18:07:39.513755Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_batch(imgs.cpu(), msks.cpu(), size=5)","metadata":{"execution":{"iopub.status.busy":"2022-06-15T18:07:39.515134Z","iopub.status.idle":"2022-06-15T18:07:39.515815Z","shell.execute_reply.started":"2022-06-15T18:07:39.515559Z","shell.execute_reply":"2022-06-15T18:07:39.515583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#pred = (nn.Sigmoid()(pred)>0.5).double()\npred.min(), pred.max()","metadata":{"execution":{"iopub.status.busy":"2022-06-15T18:07:39.517015Z","iopub.status.idle":"2022-06-15T18:07:39.517703Z","shell.execute_reply.started":"2022-06-15T18:07:39.517447Z","shell.execute_reply":"2022-06-15T18:07:39.517472Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}