{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import train_test_split\n%matplotlib inline\nimport albumentations as A\nimport os\nimport numpy as np\nimport pandas as pd\n\nimport albumentations as A\nimport cv2\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision\nimport torch.optim as optim\n\nfrom tqdm.notebook import tqdm\nfrom torch.utils.data import Dataset, DataLoader\nfrom albumentations.pytorch import ToTensorV2\n\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import KFold, StratifiedKFold\n\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\nimport warnings  \nwarnings.filterwarnings('ignore')\n\n__print__ = print\n\ndef print(string):\n    os.system(f'echo \\\"{string}\\\"')\n    __print__(string)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"EPOCHS=20\nSAMPLE_LEN=100\nIMAGE_PATH=\"../input/plant-pathology-2021-fgvc8/train_images\"\nTRAIN_PATH=\"../input/plant-pathology-2021-fgvc8/train.csv\"\nSUB_PATH=\"../input/plant-pathology-2021-fgvc8/sample_submission.csv\"\nDIR_INPUT = '../input/plant-pathology-2021-fgvc8'\n\nSEED = 42\nN_FOLDS = 5\nN_EPOCHS = 10\nBATCH_SIZE = 64\nSIZE = 224\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub=pd.read_csv(SUB_PATH)\ntrain_data=pd.read_csv(TRAIN_PATH)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encoded=dict(zip(range(12),list(train_data['labels'].value_counts().keys())))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encoded","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class PlantTestDataset(Dataset):\n    def __init__(self,df,transforms=None):\n        self.image_id=df.image.values\n        self.transforms=transforms\n        \n    def __len__(self):\n        return len(self.image_id)\n    def __getitem__(self,idx):\n        path=DIR_INPUT+'/test_images'+'/'+self.image_id[idx]\n        image=cv2.imread(path,cv2.IMREAD_COLOR)\n        image=cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n        if self.transforms:\n            image=self.transforms(image=image)['image']\n            \n        return image    \n        ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Plantmodel(nn.Module):\n    \n    def __init__(self,num_classes=12):\n        super().__init__()\n        self.model=torchvision.models.resnet18(pretrained=False)\n        in_features=self.model.fc.in_features\n        self.model.fc=nn.Linear(in_features,num_classes)\n    def forward(self,x):\n        batch_size, C, H, W = x.shape\n        x=self.model(x)\n       # x=self.backbone.maxpool(x)\n        \n       # x=self.backbone.layer1(x)\n       # x=self.backbone.layer2(x)\n       # x=self.backbone.layer3(x)\n       # x=self.backbone.layer4(x)\n        \n       # x=F.adaptive_avg_pool2d(x,1).reshape(batch_size,-1)\n       # x=F.dropout(x,0.25,self.training)\n       # x=self.classifier(x)\n        return x\n        ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transforms_train = A.Compose([\n    A.RandomResizedCrop(height=SIZE, width=SIZE, p=1.0),\n    A.Flip(),\n    A.ShiftScaleRotate(rotate_limit=1.0, p=0.8),\n\n    # Pixels\n    A.OneOf([\n        A.IAAEmboss(p=1.0),\n        A.IAASharpen(p=1.0),\n        A.Blur(p=1.0),\n    ], p=0.5),\n\n    # Affine\n    A.OneOf([\n        A.ElasticTransform(p=1.0),\n        A.IAAPiecewiseAffine(p=1.0)\n    ], p=0.5),\n\n    A.Normalize(p=1.0),\n    ToTensorV2(p=1.0),\n])\n\ntransforms_valid = A.Compose([\n    A.Resize(height=SIZE, width=SIZE, p=1.0),\n    ToTensorV2(p=1.0),\n])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"PATH=\"../input/plant-resnet/Plant_2021_epoch10.pth\"\ndevice = torch.device(\"cuda\")\nmodel=Plantmodel()\nmodel.load_state_dict(torch.load(PATH))\nmodel.to(device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dataset=PlantTestDataset(sub,transforms=transforms_valid)\ntest_loader=DataLoader(test_dataset,batch_size=64,num_workers=0,shuffle=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prediction_all=[]\ncount=0\nfor data in test_loader:\n    image=data\n    image=image.to(device,dtype=torch.float)\n    output=model(image)\n    prediction=torch.argmax(output,dim=1)\n    prediction_all.append(prediction)\n    \n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prediction_all=torch.cat(prediction_all,dim=0)\nprediction_all=prediction_all.to('cpu').numpy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_label=[]\nfor i in prediction_all:\n    pred_label.append(encoded[i])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub['labels']=pred_label","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub.to_csv('submission.csv',index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}