{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom PIL import Image\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport cv2\nfrom sklearn.model_selection import train_test_split, StratifiedShuffleSplit\nfrom sklearn.preprocessing import MultiLabelBinarizer\nimport tensorflow_datasets as tfds\nimport tensorflow as tf\nfrom tensorflow.python.framework import ops\nimport math\nimport glob\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.models import Sequential","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_image_path = '../input/plant-pathology-2021-fgvc8/train_images'\ntest_image_path = '../input/plant-pathology-2021-fgvc8/test_images'\ntrain_df_path = '../input/plant-pathology-2021-fgvc8/train.csv'\ntest_df_path = '../input/plant-pathology-2021-fgvc8/sample_submission.csv'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = pd.read_csv(train_df_path)\ndf_train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_image_urls = [train_image_path+\"/\"+f for f in df_train['image']]\ntrain_label_l = list(df_train.labels)\ntrain_label_l = [s.split(\" \") for s in train_label_l]\ntrain_label =[]\nfor i in train_label_l:\n    train_label.append(tuple(i))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mlb = MultiLabelBinarizer() \nmlb.fit(train_label) \nprint(mlb.classes_)\ntrain_label_e = mlb.transform(train_label) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"AUTOTUNE = tf.data.experimental.AUTOTUNE\nIMG_SIZE = 256\ndef decode_img(img):\n  # 압축된 문자열을 3D uint8 텐서로 변환합니다\n  img = tf.image.decode_jpeg(img, channels=3)\n  # `convert_image_dtype`은0~1 사이의 float 값으로 변환해줍니다.\n  img = tf.image.convert_image_dtype(img, tf.float32)\n  # 이미지를 원하는 크기로 조정합니다.\n  return tf.image.resize(img, [IMG_SIZE, IMG_SIZE])\n\ndef process_path(file_path):\n    img = tf.io.read_file(file_path)\n    img = decode_img(img)\n    return img\n\nimage_ds = tf.data.Dataset.list_files(train_image_urls)\nimage_ds = image_ds.map(process_path, num_parallel_calls=AUTOTUNE)\n\nlabel_ds = tf.data.Dataset.from_tensor_slices(train_label_e)\n\ntrain_ds = ds = tf.data.Dataset.zip((image_ds, label_ds))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DATASET_SIZE = 18632\ntrain_size = int(0.9 * DATASET_SIZE)\nval_size = int(0.1 * DATASET_SIZE)\n\ntrain_ds = train_ds.take(train_size)\nval_ds = train_ds.skip(train_size)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size = 32\ntrain_ds = train_ds.batch(batch_size)\nval_ds = val_ds.batch(batch_size)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\nval_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nIMG_SHAPE = (IMG_SIZE, IMG_SIZE, 3)\n\n\nbase_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,\n                                               include_top=False,\n                                               weights='imagenet')\nbase_model.trainable = False","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#with tf.device('/gpu:0'):\nmodel = tf.keras.Sequential([\n      base_model,\n      layers.GlobalAveragePooling2D(),\n      layers.Dense(6, activation='sigmoid')\n    ])\n\nmodel.compile(loss='binary_crossentropy', optimizer='adam')\n\nmodel.summary()\n\n\nhistory = model.fit(train_ds, epochs=10, validation_data = val_ds)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}