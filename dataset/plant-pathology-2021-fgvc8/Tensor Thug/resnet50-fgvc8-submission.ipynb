{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch.nn as nn\nimport cv2\nfrom tqdm.notebook import tqdm\nfrom collections import OrderedDict\nimport os\nimport torch \nimport pytorch_lightning as pl\nfrom torch.utils.data import Dataset,DataLoader\nimport glob\nfrom torchvision.models import resnet50,resnet18\nfrom albumentations import Compose,Resize,Normalize,CenterCrop\nfrom albumentations.pytorch import ToTensorV2","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class configs:\n    img_res = 512\n    num_classes = 5\n    device = [\"cuda\" if torch.cuda.is_available() else \"cpu\"]\n    paths = \"../input/pytorch-fgvc8-weigthts/Resnet18_512_Checkpoint-ValLossval_loss0.1270-F1val_f10.8357.ckpt\"\n    classes = np.array(['complex','frog_eye_leaf_spot','powdery_mildew','rust','scab'])\n\n\nclass_map = np.load(\"../input/pytorch-fgvc8-weigthts/Classes.npy\",allow_pickle=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def average_model(paths):\n    weights = np.ones((len(paths),))\n    weights = weights/weights.sum()\n    for i, p in enumerate(paths):\n        m = torch.load(p)['state_dict']\n        if i == 0:\n            averaged_w = OrderedDict()\n            for k in m.keys():\n                if 'pos' in k: continue\n                # remove pl prefix in state dict\n                knew = k.replace('model.', '')\n                averaged_w[knew] = weights[i]*m[k]\n        else:\n            for k in m.keys():\n                if 'pos' in k: continue\n                knew = k.replace('model.', '')\n                averaged_w[knew] = averaged_w[knew] + weights[i]*m[k]\n    return averaged_w","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"samp_data = pd.read_csv(\"../input/plant-pathology-2021-fgvc8/sample_submission.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"samp_data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"samp_data.image.values","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class FGVCNet(pl.LightningModule):\n    def __init__(self):\n        super(FGVCNet,self).__init__()\n        self.model = resnet18(pretrained=False)\n        num_ftrs = self.model.fc.in_features\n        \n        self.model.fc = nn.Sequential(\n            nn.Linear(num_ftrs,num_ftrs),\n            nn.ReLU(inplace=True),\n            nn.Dropout(0.5),\n            nn.Linear(num_ftrs,5)\n        )\n\n    def forward(self,x):\n        return self.model(x)\n    \n    def configure_optimizers(self):\n        return torch.optim.Adam(self.parameters(),lr=0.01)\n\n    def training_step(self,batch,batch_idx):\n        X,y = batch\n        y_hat = self.model(X)\n        loss_tr = F.cross_entropy(y_hat,y)\n        f1_tr = pl.metrics.functional.f1(y_hat,y,12) \n        self.log(\"TrainLoss\",loss_tr,prog_bar=True)\n        self.log(\"TrainF1\",f1_tr,prog_bar=True)\n        return loss_tr\n    \n    def validation_step(self,batch,batch_idx):\n        X,y = batch\n        y_hat = self.model(X)\n        loss_val = F.cross_entropy(y_hat,y)\n        f1_val = pl.metrics.functional.f1(y_hat,y,12)\n        self.log(\"val_loss\",loss_val,prog_bar=True)\n        self.log(\"val_f1\",f1_val,prog_bar=True)\n        return loss_val","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def Test_Augs():\n    return Compose([\n        Resize(configs.img_res,configs.img_res),\n        Normalize(),\n        ToTensorV2()\n    ])\n\n\nclass FGVCTest(Dataset):\n    def __init__(self,df,root_dir,transforms=None):\n        self.df = df\n        self.root = root_dir\n        self.transforms = transforms\n    def __len__(self):\n        return len(self.df)\n    def __getitem__(self,idx):\n        img_name = os.path.join(self.root,self.df.loc[idx,'image'])\n        img = cv2.imread(img_name)\n        if self.transforms is not None:\n            img = self.transforms(image=img)['image']\n        return img","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = FGVCTest(df=samp_data,root_dir=\"../input/plant-pathology-2021-fgvc8/test_images\",transforms=Test_Augs())\ndataset = DataLoader(dataset,batch_size=1,num_workers=4)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def Ensemble():\n    preds = []\n    for p in range(len(configs.paths)):\n        model = FGVCNet()\n        new_model = model.load_from_checkpoint(configs.paths[p])\n        new_model.eval()\n        pbar = tqdm(enumerate(dataset),total=len(dataset))\n        for i,img in pbar:\n            new_model = new_model.to(configs.device[0])\n            out = new_model(img.to(configs.device[0]))\n            out = nn.functional.softmax(out)\n            _,pred = torch.max(out,dim=-1)\n            sub_df.loc[i,f\"Model-{p}\"] = pred.detach().cpu().numpy()[0]\n    total = sub_df[f\"Model-0\"].values+sub_df[f\"Model-1\"].values\n    total = total // len(configs.paths)\n    samp_data['labels'] = class_map[total.astype(np.int64)]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def standalone():\n    preds = [] \n    c = []\n    model = FGVCNet()\n    new_model = model.load_from_checkpoint(configs.paths)\n    new_model.eval()\n    pbar = tqdm(enumerate(dataset),total=len(dataset))\n    for i,img in pbar:\n        new_model = new_model.to(configs.device[0])\n        img = img.to(configs.device[0])\n        out = new_model(img)\n        out = nn.functional.sigmoid(out)\n        _,pred = torch.max(out,dim=-1)\n        preds.append(configs.classes[pred.item()])\n    return preds  \n    #print(preds)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"p = standalone()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"samp_data['labels'] = p","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"samp_data.to_csv(\"submission.csv\",index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}