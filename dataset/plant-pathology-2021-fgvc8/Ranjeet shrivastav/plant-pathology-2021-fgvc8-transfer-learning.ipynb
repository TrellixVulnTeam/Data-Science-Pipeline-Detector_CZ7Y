{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Plant Pathology 2021 - FGVC8\n#### Identify the category of foliar diseases in apple trees","metadata":{}},{"cell_type":"markdown","source":"Apples are one of the most important temperate fruit crops in the world. Foliar (leaf) diseases pose a major threat to the overall productivity and quality of apple orchards. The current process for disease diagnosis in apple orchards is based on manual scouting by humans, which is time-consuming and expensive.\n\nAlthough computer vision-based models have shown promise for plant disease identification, there are some limitations that need to be addressed. Large variations in visual symptoms of a single disease across different apple cultivars, or new varieties that originated under cultivation, are major challenges for computer vision-based disease identification. These variations arise from differences in natural and image capturing environments, for example, leaf color and leaf morphology, the age of infected tissues, non-uniform image background, and different light illumination during imaging etc.","metadata":{}},{"cell_type":"markdown","source":"#### Importing libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nfrom pathlib import Path\nimport os.path\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport os\nimport cv2\n\nfrom keras.callbacks import EarlyStopping","metadata":{"execution":{"iopub.status.busy":"2021-10-01T19:08:03.513905Z","iopub.execute_input":"2021-10-01T19:08:03.514402Z","iopub.status.idle":"2021-10-01T19:08:08.443636Z","shell.execute_reply.started":"2021-10-01T19:08:03.514281Z","shell.execute_reply":"2021-10-01T19:08:08.442745Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a list with the filepaths for training and testing\ntrain_img_Path = '../input/plant-pathology-2021-fgvc8/train_images'\n\ntest_img_Path = '../input/plant-pathology-2021-fgvc8/test_images'\n\nimg_Path = '../input/resized-plant2021/img_sz_256'\n\ntrain = pd.read_csv(r'../input/plant-pathology-2021-fgvc8/train.csv')\n\nsample_submission = pd.read_csv(r'../input/plant-pathology-2021-fgvc8/sample_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2021-10-01T19:08:09.633394Z","iopub.execute_input":"2021-10-01T19:08:09.633721Z","iopub.status.idle":"2021-10-01T19:08:09.680325Z","shell.execute_reply.started":"2021-10-01T19:08:09.633694Z","shell.execute_reply":"2021-10-01T19:08:09.679387Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-01T19:08:10.755202Z","iopub.execute_input":"2021-10-01T19:08:10.755555Z","iopub.status.idle":"2021-10-01T19:08:10.777694Z","shell.execute_reply.started":"2021-10-01T19:08:10.755525Z","shell.execute_reply":"2021-10-01T19:08:10.77648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'Number of pictures in the training dataset: {train.shape[0]}\\n')\nprint(f'Number of different labels: {len(train.labels.unique())}\\n')\nprint(f'Labels: {train.labels.unique()}')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-10-01T19:08:11.202978Z","iopub.execute_input":"2021-10-01T19:08:11.20328Z","iopub.status.idle":"2021-10-01T19:08:11.218605Z","shell.execute_reply.started":"2021-10-01T19:08:11.203251Z","shell.execute_reply":"2021-10-01T19:08:11.215366Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['labels'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-10-01T19:08:12.233126Z","iopub.execute_input":"2021-10-01T19:08:12.233488Z","iopub.status.idle":"2021-10-01T19:08:12.24699Z","shell.execute_reply.started":"2021-10-01T19:08:12.233457Z","shell.execute_reply":"2021-10-01T19:08:12.245939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(14,7))\nb = sns.countplot(x='labels', data=train, order=sorted(train['labels'].unique()))\nfor item in b.get_xticklabels():\n    item.set_rotation(90)\nplt.title('Label Distribution', weight='bold')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-01T17:33:44.579312Z","iopub.execute_input":"2021-10-01T17:33:44.579563Z","iopub.status.idle":"2021-10-01T17:33:44.82919Z","shell.execute_reply.started":"2021-10-01T17:33:44.57954Z","shell.execute_reply":"2021-10-01T17:33:44.828221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(20,40))\ni=1\nfor idx,s in train.head(9).iterrows():\n    img_path = os.path.join(img_Path,s['image'])\n    img=cv2.imread(img_path)\n    img=cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n    fig=plt.subplot(9,3,i)\n    fig.imshow(img)\n    fig.set_title(s['labels'])\n    i+=1","metadata":{"execution":{"iopub.status.busy":"2021-10-01T17:33:44.942501Z","iopub.execute_input":"2021-10-01T17:33:44.942839Z","iopub.status.idle":"2021-10-01T17:33:46.612332Z","shell.execute_reply.started":"2021-10-01T17:33:44.94281Z","shell.execute_reply":"2021-10-01T17:33:46.611362Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CLASSES = train['labels'].unique().tolist()","metadata":{"execution":{"iopub.status.busy":"2021-10-01T19:08:17.649763Z","iopub.execute_input":"2021-10-01T19:08:17.650089Z","iopub.status.idle":"2021-10-01T19:08:17.656428Z","shell.execute_reply.started":"2021-10-01T19:08:17.650058Z","shell.execute_reply":"2021-10-01T19:08:17.655118Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\n\n# Preprocessing the Training set\ntrain_datagen = ImageDataGenerator(rescale=1./255,\n                                   shear_range = 0.1,\n                                   zoom_range = 0.1,\n                                   horizontal_flip = True,\n                                   validation_split=0.25)\n\ntrain_data = train_datagen.flow_from_dataframe(train,\n                                              directory=img_Path,\n                                              classes=CLASSES,\n                                              x_col=\"image\",\n                                              y_col=\"labels\",\n                                              target_size=(150, 150),\n                                              subset='training')\n\nval_data = train_datagen.flow_from_dataframe(train,\n                                            directory=img_Path,\n                                            classes=CLASSES,\n                                            x_col=\"image\",\n                                            y_col=\"labels\",\n                                            target_size=(150, 150),\n                                            subset='validation')","metadata":{"execution":{"iopub.status.busy":"2021-10-01T19:08:19.469652Z","iopub.execute_input":"2021-10-01T19:08:19.469981Z","iopub.status.idle":"2021-10-01T19:09:11.010522Z","shell.execute_reply.started":"2021-10-01T19:08:19.469952Z","shell.execute_reply":"2021-10-01T19:09:11.009152Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dict_classes = train_data.class_indices\ndict_classes","metadata":{"execution":{"iopub.status.busy":"2021-10-01T19:09:11.012003Z","iopub.execute_input":"2021-10-01T19:09:11.012369Z","iopub.status.idle":"2021-10-01T19:09:11.018615Z","shell.execute_reply.started":"2021-10-01T19:09:11.012329Z","shell.execute_reply":"2021-10-01T19:09:11.017546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom keras.preprocessing.image import load_img\nfrom keras.utils import to_categorical\nfrom keras import Sequential\nfrom keras.applications import InceptionResNetV2, DenseNet169, ResNet152V2\nfrom tensorflow.keras.layers import Dense","metadata":{"execution":{"iopub.status.busy":"2021-10-01T19:09:11.021098Z","iopub.execute_input":"2021-10-01T19:09:11.02176Z","iopub.status.idle":"2021-10-01T19:09:11.030982Z","shell.execute_reply.started":"2021-10-01T19:09:11.021722Z","shell.execute_reply":"2021-10-01T19:09:11.030152Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* I am using the ResNet152V2, InceptionResNetV2, DenseNet169.","metadata":{}},{"cell_type":"markdown","source":"#### Defining the ResNet152V2 Convolutional Neural Net:","metadata":{}},{"cell_type":"code","source":"base_Net = ResNet152V2(include_top = False, \n                         weights = '../input/keras-pretrained-models/ResNet152V2_NoTop_ImageNet.h5', \n                         input_shape = train_data.image_shape, \n                         pooling='avg',\n                         classes = CLASSES)","metadata":{"execution":{"iopub.status.busy":"2021-10-01T19:09:11.032466Z","iopub.execute_input":"2021-10-01T19:09:11.032836Z","iopub.status.idle":"2021-10-01T19:09:20.917446Z","shell.execute_reply.started":"2021-10-01T19:09:11.032784Z","shell.execute_reply":"2021-10-01T19:09:20.916496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Callbacks\nEarlyStop_callback = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\nmy_callback=[EarlyStop_callback]","metadata":{"execution":{"iopub.status.busy":"2021-10-01T19:09:20.919327Z","iopub.execute_input":"2021-10-01T19:09:20.919697Z","iopub.status.idle":"2021-10-01T19:09:20.926241Z","shell.execute_reply.started":"2021-10-01T19:09:20.91966Z","shell.execute_reply":"2021-10-01T19:09:20.925407Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Adding the final layers to the above base models where the actual classification is done in the dense layers\nmodel_Net = Sequential()\nmodel_Net.add(base_Net)\nmodel_Net.add(Dense(12, activation=('softmax')))\n\nmodel_Net.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['AUC'])\nmodel_Net.summary()\n\n# Training the CNN on the Train data and evaluating it on the val data\nb = model_Net.fit(train_data, validation_data = val_data, epochs = 10,callbacks=my_callback, batch_size=128)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-10-01T19:09:20.927655Z","iopub.execute_input":"2021-10-01T19:09:20.928074Z","iopub.status.idle":"2021-10-01T19:36:12.827651Z","shell.execute_reply.started":"2021-10-01T19:09:20.928034Z","shell.execute_reply":"2021-10-01T19:36:12.826787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Defining the InceptionResNetV2 Convolutional Neural Net:","metadata":{}},{"cell_type":"code","source":"base_InceptionResNetV2 = InceptionResNetV2(include_top = False, \n                         weights = '../input/keras-pretrained-models/InceptionResNetV2_NoTop_ImageNet.h5', \n                         input_shape = train_data.image_shape, \n                         pooling='avg',\n                         classes = CLASSES)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Adding the final layers to the above base models where the actual classification is done in the dense layers\nmodel_IResNet2 = Sequential()\nmodel_IResNet2.add(base_InceptionResNetV2)\nmodel_IResNet2.add(Dense(12, activation=('softmax')))\n\nmodel_IResNet2.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\nmodel_IResNet2.summary()\n\n# Training the CNN on the Train data and evaluating it on the val data\nc = model_IResNet2.fit(train_data, validation_data = val_data, epochs = 10,callbacks=my_callback, batch_size=128)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Defining the DenseNet169 Convolutional Neural Net:","metadata":{}},{"cell_type":"code","source":"base_DenseNet169 = DenseNet169(include_top = False, \n                         weights = '../input/keras-pretrained-models/DenseNet169_NoTop_ImageNet.h5', \n                         input_shape = train_data.image_shape, \n                         pooling='avg',\n                         classes = CLASSES)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Adding the final layers to the above base models where the actual classification is done in the dense layers\nmodel_dense = Sequential()\nmodel_dense.add(base_DenseNet169)\nmodel_dense.add(Dense(12, activation=('softmax')))\n\nmodel_dense.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\nmodel_dense.summary()\n\n# Training the CNN on the Train data and evaluating it on the val data\nd = model_dense.fit(train_data, validation_data = val_data, epochs = 10,callbacks=my_callback, batch_size=128)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dir = '/kaggle/input/plant-pathology-2021-fgvc8/test_images/'\ntest_df = pd.DataFrame()\ntest_df['image'] = os.listdir(test_dir)\n\ntest_data = train_datagen.flow_from_dataframe(dataframe=test_df,\n                                    directory=test_dir,\n                                    x_col=\"image\",\n                                    y_col=None,\n                                    batch_size=32,\n                                    seed=42,\n                                    shuffle=False,\n                                    class_mode=None,\n                                    target_size=(150, 150))","metadata":{"execution":{"iopub.status.busy":"2021-10-01T19:42:57.015454Z","iopub.execute_input":"2021-10-01T19:42:57.015807Z","iopub.status.idle":"2021-10-01T19:42:57.038761Z","shell.execute_reply.started":"2021-10-01T19:42:57.015775Z","shell.execute_reply":"2021-10-01T19:42:57.037929Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Making predictions on test data:","metadata":{}},{"cell_type":"code","source":"pred_net = model_Net.predict(test_data)\npred_iresnet2 = model_IResNet2.predict(test_data)\npred_dense = model_dense.predict(test_data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred = ((pred_net+pred_iresnet2+pred_dense)/3).tolist()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(len(pred)):\n    pred[i] = np.argmax(pred[i])\n\n    \ndef get_key(val):\n    for key, value in dict_classes.items():\n        if val == value:\n            return key\n        \n\nfor i in range(len(pred)):\n    pred[i] = get_key(pred[i])","metadata":{"execution":{"iopub.status.busy":"2021-10-01T19:43:45.51973Z","iopub.execute_input":"2021-10-01T19:43:45.520144Z","iopub.status.idle":"2021-10-01T19:43:45.531075Z","shell.execute_reply.started":"2021-10-01T19:43:45.520105Z","shell.execute_reply":"2021-10-01T19:43:45.530033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df['labels'] = pred\ntest_df.to_csv('submission.csv',index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### If you liked the Notebook Please Upvote It !\n#### Thank You","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}