{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Loading packages"},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport cv2\nimport pandas as pd\nimport albumentations as A\nimport matplotlib.pyplot as plt\n\nfrom tqdm import tqdm","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Defining environment"},{"metadata":{"trusted":true},"cell_type":"code","source":"DATA_ROOT = os.path.join('..', 'input')\nDATA_COMPT = os.path.join(DATA_ROOT, 'plant-pathology-2021-fgvc8')\nDATA_TRAIN_IMAGES = os.path.join(DATA_COMPT, 'train_images')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"DATA_OUTPUT = './'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"DATA_TRAIN_IMAGES_2672x4000 = os.path.join(DATA_OUTPUT, 'train_images_2672x4000')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"DATA_TRAIN_IMAGES_224 = os.path.join(DATA_OUTPUT, 'train_images_224')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"DATA_TRAIN_IMAGES_224x336 = os.path.join(DATA_OUTPUT, 'train_images_224x336')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"DATA_TRAIN_IMAGES_448 = os.path.join(DATA_OUTPUT, 'train_images_448')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"DATA_TRAIN_IMAGES_448x670 = os.path.join(DATA_OUTPUT, 'train_images_448x670')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Loading metadata"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = pd.read_csv(os.path.join(DATA_COMPT, 'train.csv'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Images"},{"metadata":{"trusted":true},"cell_type":"code","source":"def show_img(image):\n    plt.figure(figsize=(10, 10))\n    plt.imshow(image)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def resize_images(path_in:str, path_out:str, filenames:list, resize_to:dict):\n    if not os.path.isdir(path_out): os.mkdir(path_out)\n    \n    trfTranspose = A.Transpose(p=1)\n    trfResize = A.Resize (height=resize_to['height'], width=resize_to['width'], p=1)\n    for filename in tqdm(filenames):\n        image = cv2.imread( os.path.join(path_in, filename) )\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        \n        height, width, _ = image.shape\n        if height/width > 1: image = trfTranspose(image=image)['image']\n        \n        image_resized = trfResize(image=image)['image']\n        \n        save_to = os.path.join(path_out, filename)\n        image_resized = cv2.cvtColor(image_resized, cv2.COLOR_RGB2BGR)\n        if not cv2.imwrite(filename=save_to, img=image_resized): print(f'Failed to save image: {filename} to dir:{path_out}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"filenames = df_train['image'].tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"resize_to_2672x4000={'height':2672, 'width':4000, 'dir':DATA_TRAIN_IMAGES_2672x4000}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#resize_images(path_in=DATA_TRAIN_IMAGES, path_out=resize_to_2672x4000['dir'], filenames=filenames, resize_to=resize_to_2672x4000)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Information on PyTorch native pre-trained models on imagenet data with resolution 224x224 can be found [here](https://pytorch.org/vision/stable/models.html)"},{"metadata":{"trusted":true},"cell_type":"code","source":"resize_to_224={'height':224, 'width':224, 'dir':DATA_TRAIN_IMAGES_224}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#resize_images(path_in=DATA_TRAIN_IMAGES, path_out=resize_to_224['dir'], filenames=filenames, resize_to=resize_to_224)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"resize_to_224x336 = {'height':224, 'width':336, 'dir':DATA_TRAIN_IMAGES_224x336}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nresize_images(path_in=DATA_TRAIN_IMAGES, path_out=resize_to_224x336['dir'], filenames=filenames, resize_to=resize_to_224x336)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"resize_to_448={'height':448, 'width':448, 'dir':DATA_TRAIN_IMAGES_448}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#resize_images(path_in=DATA_TRAIN_IMAGES, path_out=resize_to_448['dir'], filenames=filenames, resize_to=resize_to_448)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"resize_to_448x670 = {'height':448, 'width':670, 'dir':DATA_TRAIN_IMAGES_448x670}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nresize_images(path_in=DATA_TRAIN_IMAGES, path_out=resize_to_448x670['dir'], filenames=filenames, resize_to=resize_to_448x670)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}