{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Introduction**","metadata":{"id":"e1pKGE2btd2k"}},{"cell_type":"markdown","source":"By Yvtsan Levy ","metadata":{"id":"4wNm97NGtlEI"}},{"cell_type":"markdown","source":"Misdiagnosis of the many diseases impacting agricultural crops can lead to misuse of chemicals leading to the emergence of resistant pathogen strains, increased input costs, and more outbreaks with significant economic loss and environmental impacts. Current disease diagnosis based on human scouting is time-consuming and expensive, and although computer-vision based models have the promise to increase efficiency, the great variance in symptoms due to age of infected tissues, genetic variations, and light conditions within trees decreases the accuracy of detection.\n\nThe purpose of the project is to diagnose apple tree diseases solely based on leaf images. The data is a set of images that is sapareted to those categories: \"healthy\", \"scab\", \"rust\", and \"multiple diseases\". Solving this problem is important because diagnosing plant diseases early can save tonnes of agricultural produce every year. This will benefit not only the general population by reducing hunger, but also the farmers by ensuring they get the harvest they deserve.\n\nThe project is based on the data set \"[Plant Pathology 2020 - FGVC7](https://www.kaggle.com/c/plant-pathology-2020-fgvc7/overview/description)\" Contains 3642 images of apple leaves divided into training data and test data.","metadata":{"id":"g3yLTNeus5RQ"}},{"cell_type":"markdown","source":"# **Domain knowledge**","metadata":{"id":"iLG7NpcTVQKY"}},{"cell_type":"markdown","source":"Global human population growth amounts to around 83 million annually, and as the population  grows, so does the global demand for food is rising. In order to meet the demand, agricultural productivity must increase. One of the factors for decrease in crop yield are plants diseases, and identifing them is one of the bottle necks in the process of treating them. Current disease diagnosis based on human scouting is time-consuming and expensive. Diagnosing plants diseases using computer-vision based models can improve the efficeincy of those processes and increace crop yield.\n\nApple tree are one of the most common fruit trees, and worldwide production of apples in 2018 was 86 million tonnes. 2 of the most common diseases of apple trees are apple scab and rust. Apple scab is a common disease of plants in the rose family (Rosaceae) that is caused by the ascomycete fungus Venturia inaequalis. Although apple scab rarely kills its host, infection typically leads to fruit deformation and premature leaf and fruit drop. The reduction of fruit quality and yield may result in crop losses of up to 70%. Rusts are plant diseases caused by pathogenic fungi of the order Pucciniales. Rusts are considered among the most harmful pathogens to agriculture, horticulture and forestry. Rust fungi are major concerns and limiting factors for successful cultivation of agricultural and forest crops.\n\nadd image of rust and scabs***\n","metadata":{"id":"a0pVo_WdT8cr"}},{"cell_type":"markdown","source":"# **Project's Target**","metadata":{"id":"OePBPP9eyv5O"}},{"cell_type":"markdown","source":"\nOur goal is to produce a model that classify correcly the health of a leaf and can say in great confidence what disease does it have. We will use images of apple leaves that are divided into 4 classes: healty leaves,leaves with scab,leaves with rust, and leaves with scab and rust. Our data-base contains 3642 images of apple leaves, 1821 classified images and 1821 images with no classification. We will try to train a model that can classify correcly images of apple leaves to those classes.","metadata":{"id":"lzv78dpB69Ea"}},{"cell_type":"markdown","source":"# **Install and Import Necessary Libraries**","metadata":{"id":"k1RVIoYM69Ec"}},{"cell_type":"markdown","source":"Importing the libraries that we will use","metadata":{"id":"hfsu46GUTCSZ"}},{"cell_type":"code","source":"","metadata":{"id":"1EQ_3Kb769Eh","outputId":"de597814-1a81-4cdb-9ffd-3fa3bfd55f6c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install -q efficientnet\nimport os\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport cv2\nimport matplotlib.pyplot as plt\nimport matplotlib.pyplot\n%matplotlib inline\nimport sklearn\nimport sklearn.cluster\nimport plotly.graph_objects as go\nimport keras\nfrom sklearn.preprocessing import MultiLabelBinarizer\nimport tensorflow as tf\nimport tensorflow.keras.layers as L\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras.preprocessing import image\nimport imblearn.over_sampling\nimport warnings\nfrom tqdm.notebook import tqdm\ntqdm.pandas()\n\n\nfrom glob import glob\nimport itertools\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"id":"GRyJncMO69Ek","outputId":"88399709-5868-4e0f-eba3-68187804cac0","execution":{"iopub.status.busy":"2021-12-15T18:30:54.782537Z","iopub.execute_input":"2021-12-15T18:30:54.783024Z","iopub.status.idle":"2021-12-15T18:31:12.765493Z","shell.execute_reply.started":"2021-12-15T18:30:54.782935Z","shell.execute_reply":"2021-12-15T18:31:12.764192Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#**Data Loading and Constant Defining** ","metadata":{"id":"ONWGIztv69Er"}},{"cell_type":"markdown","source":"Firstly we will load our data and define constants","metadata":{"id":"BUvm9c5y4m-o"}},{"cell_type":"code","source":"IMAGE_PATH = '../input/plant-pathology-2021-fgvc8/train_images/'\nTRAIN_PATH = '../input/plant-pathology-2021-fgvc8/train.csv'\nvalid_path = '../input/plant-pathology-2021-fgvc8/test_images'\nseed=1994\n\ntrain_data = pd.read_csv(TRAIN_PATH)","metadata":{"id":"cqQbG6Pe69Er","execution":{"iopub.status.busy":"2021-12-15T18:31:12.767604Z","iopub.execute_input":"2021-12-15T18:31:12.768096Z","iopub.status.idle":"2021-12-15T18:31:12.812163Z","shell.execute_reply.started":"2021-12-15T18:31:12.768048Z","shell.execute_reply":"2021-12-15T18:31:12.811118Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# useful for getting number of files\nimage_files = glob(IMAGE_PATH + '/*.jp*g')\nvalid_image_files = glob(valid_path + '/*/*.jp*g')","metadata":{"execution":{"iopub.status.busy":"2021-12-15T18:31:12.814493Z","iopub.execute_input":"2021-12-15T18:31:12.814842Z","iopub.status.idle":"2021-12-15T18:31:13.390208Z","shell.execute_reply.started":"2021-12-15T18:31:12.814806Z","shell.execute_reply":"2021-12-15T18:31:13.388965Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# useful for getting number of classes\nfolders = glob(TRAIN_PATH + '/*')","metadata":{"execution":{"iopub.status.busy":"2021-12-15T18:31:13.392711Z","iopub.execute_input":"2021-12-15T18:31:13.393216Z","iopub.status.idle":"2021-12-15T18:31:13.400595Z","shell.execute_reply.started":"2021-12-15T18:31:13.393169Z","shell.execute_reply":"2021-12-15T18:31:13.399053Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# look at an image for fun\nplt.imshow(image.load_img(np.random.choice(image_files)));","metadata":{"execution":{"iopub.status.busy":"2021-12-15T18:31:13.402865Z","iopub.execute_input":"2021-12-15T18:31:13.403518Z","iopub.status.idle":"2021-12-15T18:31:14.981292Z","shell.execute_reply.started":"2021-12-15T18:31:13.403469Z","shell.execute_reply":"2021-12-15T18:31:14.979933Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#lets count the instances of each class we have :\n\nfig,ax=plt.subplots(figsize=(16,8))\nsns.countplot(train_data['labels'])\n#rotate labels\nplt.setp(ax.get_xticklabels(),rotation=45)\n\nplt.title('Label counts')","metadata":{"execution":{"iopub.status.busy":"2021-12-15T18:31:14.982952Z","iopub.execute_input":"2021-12-15T18:31:14.983418Z","iopub.status.idle":"2021-12-15T18:31:15.305265Z","shell.execute_reply.started":"2021-12-15T18:31:14.983358Z","shell.execute_reply":"2021-12-15T18:31:15.303998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#converting the labels as multiple labels:\ntrain_data['labels']=train_data['labels'].str.split(' ')\n\nmlb = MultiLabelBinarizer()\n\n# one hot encode labels\nlab=mlb.fit_transform(train_data['labels'])\nlab[:10]","metadata":{"execution":{"iopub.status.busy":"2021-12-15T18:31:15.307081Z","iopub.execute_input":"2021-12-15T18:31:15.307949Z","iopub.status.idle":"2021-12-15T18:31:15.360069Z","shell.execute_reply.started":"2021-12-15T18:31:15.307898Z","shell.execute_reply":"2021-12-15T18:31:15.358858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data[:10]","metadata":{"execution":{"iopub.status.busy":"2021-12-15T18:31:15.364266Z","iopub.execute_input":"2021-12-15T18:31:15.364699Z","iopub.status.idle":"2021-12-15T18:31:15.385559Z","shell.execute_reply.started":"2021-12-15T18:31:15.364663Z","shell.execute_reply":"2021-12-15T18:31:15.384246Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#classes for OHE encoded var.\nclasses=mlb.classes_\nclasses","metadata":{"execution":{"iopub.status.busy":"2021-12-15T18:31:15.388462Z","iopub.execute_input":"2021-12-15T18:31:15.388936Z","iopub.status.idle":"2021-12-15T18:31:15.396171Z","shell.execute_reply.started":"2021-12-15T18:31:15.388891Z","shell.execute_reply":"2021-12-15T18:31:15.394737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data['image']","metadata":{"execution":{"iopub.status.busy":"2021-12-15T18:31:15.398501Z","iopub.execute_input":"2021-12-15T18:31:15.399458Z","iopub.status.idle":"2021-12-15T18:31:15.411143Z","shell.execute_reply.started":"2021-12-15T18:31:15.399402Z","shell.execute_reply":"2021-12-15T18:31:15.409836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DF = pd.DataFrame(\n    data=lab,\n    columns=classes\n)\n\nprint(DF.head(),DF.shape)","metadata":{"execution":{"iopub.status.busy":"2021-12-15T18:31:15.412923Z","iopub.execute_input":"2021-12-15T18:31:15.413428Z","iopub.status.idle":"2021-12-15T18:31:15.423649Z","shell.execute_reply.started":"2021-12-15T18:31:15.413376Z","shell.execute_reply":"2021-12-15T18:31:15.422368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DF['image'] =  train_data['image']\nprint(DF.head(),DF.shape)","metadata":{"execution":{"iopub.status.busy":"2021-12-15T18:31:15.425348Z","iopub.execute_input":"2021-12-15T18:31:15.426241Z","iopub.status.idle":"2021-12-15T18:31:15.44111Z","shell.execute_reply.started":"2021-12-15T18:31:15.426187Z","shell.execute_reply":"2021-12-15T18:31:15.439792Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = DF","metadata":{"execution":{"iopub.status.busy":"2021-12-15T18:31:15.442897Z","iopub.execute_input":"2021-12-15T18:31:15.443362Z","iopub.status.idle":"2021-12-15T18:31:15.448761Z","shell.execute_reply.started":"2021-12-15T18:31:15.443309Z","shell.execute_reply":"2021-12-15T18:31:15.447491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#**Functions**","metadata":{"id":"3TSGQc-m69Ey"}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will use utility functions several times in our project, so we will define them here. Most of them are used to process images and reshape them or the data-sets that contins them","metadata":{"id":"kjZrYQeK5dsP"}},{"cell_type":"code","source":"#prints a image\n# gets an image to prints\n# returns nothing\ndef show_image(img):\n  fig = plt.imshow(img)\n\n#loads an image and returns it \n# gets the image id of the image we wants to load from the raw images and the shape of the output image\n# returns an image with the shape of image_size\ndef load_image(image_id,image_size=(100,100)):\n    file_path = image_id\n    image = cv2.imread(IMAGE_PATH + file_path,1)\n    image = cv2.resize(image, image_size)\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    return image\n\n#loads an image from returns it \n# gets the image id of the image we wants to load, the path to the dictunary and the shape of the output image\n# returns an image with the shape of image_size\ndef load_dif_image(image_id,path,image_size=(100,100)):\n    file_path = image_id \n    image = cv2.imread(path + file_path,1)\n    image = cv2.resize(image, image_size)\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    return image\n\n#creat from image objects list a reshaped 4-dimentional array(image_number,row,column,color_channel)\n# get a list of image objects\n# returns a 4-dimentional np array of the same images\ndef images_4d_array(files_list):\n  images_list = [img[np.newaxis, :, :, :3] for img in files_list]\n  images_array = np.vstack(images_list)\n  return (images_array)\n\n# retrive a list of images that fullfil the cond and show up to 9 pics of them. cond is a health situation name name. \n# gets health situation name:'scab','rust','multiple_diseases','healthy'\n# returns a list of images that fullfil the condition, and prints up to 9 images that fullfil them\ndef show_cond(cond):\n  cond_list=train_images[train_data[cond]==1]\n  cols, rows = 3,3  \n  fig, ax = plt.subplots(nrows=rows, ncols=cols, figsize=(15, rows*10/3))\n  for i in range(cols*rows):\n    if(i>=cond_list.size):\n      break\n    ax[int(i/cols), int(i%rows)].imshow(cond_list.iloc[i])\n  plt.show()\n  return (cond_list)\n\n# retrive a list of images that fullfil the cond. cond is a health situation name name. \n# gets health situation name:'scab','rust','multiple_diseases','healthy'\n# returns a list of images that fullfil the condition\ndef get_cond(cond):\n  cond_list=train_images[train_data[cond]==1] \n  return (cond_list)\n\n# calculate the avarge per color channel\n# gets a 4-dimensional  array and an int  \n# returns the avarage value of the cells by the 4th dimension channel position \ndef get_average_channel(x, channel):\n    return x[:,:,:,channel].mean(axis = (1, 2)).reshape(-1, 1)\n\n# claculate the avarge per color channel for x\n# gets image 4-dimentional array \n# returns array of \ndef get_channels(x):\n    return np.hstack([get_average_channel(x, i) for i in range(3)])\n\n# reshapes the 4-dimentional array into 2-dimentional array\n# gets a 4-dimentional array\n# returns a 2-dimentional array\ndef get_all_pixels(x):\n    return x.reshape(-1, np.prod(x.shape[1:]))\n\n# merges images to one image\n# gets an 4-dimentional array and the numbers of images to print as number per rows and number of images per row\n# returns an image that is a marge of the other ones\ndef merge_images(image_batch, size = [20, 20]):\n    h,w = image_batch.shape[1], image_batch.shape[2]\n    c = image_batch.shape[3]\n    img = np.zeros((int(h*size[0]), w*size[1], c))\n    for idx, im in enumerate(image_batch):\n        i = idx % size[1]\n        j = idx // size[1]\n        img[j*h:j*h+h, i*w:i*w+w,:] = im/255\n    return img\n\n# get the data and creates a numerated classes array\n# gets the images data\n# returns an array of numerated classes\ndef get_target_array(data):\n  targets=[]\n  for index, row in data.drop('image',axis=1).iterrows():\n    if (row[0]==1):\n      targets.append(0)\n    elif (row[1]==1):\n      targets.append(1)\n    elif (row[2]==1):\n      targets.append(2)\n    elif (row[3]==1):\n      targets.append(3)\n    elif (row[4]==1):\n      targets.append(4)\n    elif (row[5]==1):\n      targets.append(5)\n  return (np.array(targets))\n\n\n# get the data and creates a numerated classes array\n# gets the images data\n# returns an array of numerated classes\ndef get_target_array_no_im_id(data):\n  targets=[]\n  for index, row in data.iterrows():\n    if (row[0]==1):\n      targets.append(0)\n    elif (row[1]==1):\n      targets.append(1)\n    elif (row[2]==1):\n      targets.append(2)\n    elif (row[3]==1):\n      targets.append(3)\n    elif (row[4]==1):\n      targets.append(4)\n    elif (row[5]==1):\n      targets.append(5)\n  return (np.array(targets))","metadata":{"id":"SA5QGPcN69Ey","execution":{"iopub.status.busy":"2021-12-15T18:31:15.4509Z","iopub.execute_input":"2021-12-15T18:31:15.451971Z","iopub.status.idle":"2021-12-15T18:31:15.47958Z","shell.execute_reply.started":"2021-12-15T18:31:15.451906Z","shell.execute_reply":"2021-12-15T18:31:15.478281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Load Images**","metadata":{"id":"E8Ia42eWbegZ"}},{"cell_type":"markdown","source":"Thus far, we talk about how our images fit in our classes, but we haven't has a look at them, so we shall do it now.","metadata":{"id":"Qu-Qrih3begZ"}},{"cell_type":"code","source":"n_sample = 1500","metadata":{"execution":{"iopub.status.busy":"2021-12-15T18:33:15.064595Z","iopub.execute_input":"2021-12-15T18:33:15.065012Z","iopub.status.idle":"2021-12-15T18:33:15.069435Z","shell.execute_reply.started":"2021-12-15T18:33:15.064977Z","shell.execute_reply":"2021-12-15T18:33:15.068276Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#loads all of the images\n# train_images = train_data[\"image\"].progress_apply(load_image, args=((100,100),))\n\n#load a random sample\ntrain_images = train_data[\"image\"].sample(n = n_sample, random_state = seed).progress_apply(load_image)\n\n#Build the dataset of current images\ncurrent_train_data=train_data.loc[train_images.index]\ntargets=get_target_array(current_train_data)","metadata":{"id":"MpLhGQ5DbegZ","outputId":"c39e833a-2caf-419f-f7b7-890f5bb1776a","execution":{"iopub.status.busy":"2021-12-15T18:33:34.110805Z","iopub.execute_input":"2021-12-15T18:33:34.111257Z","iopub.status.idle":"2021-12-15T18:37:05.353425Z","shell.execute_reply.started":"2021-12-15T18:33:34.111224Z","shell.execute_reply":"2021-12-15T18:37:05.352157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"current_train_data=train_data.loc[train_images.index]\ntargets=lab","metadata":{"execution":{"iopub.status.busy":"2021-12-15T18:37:17.381108Z","iopub.execute_input":"2021-12-15T18:37:17.38148Z","iopub.status.idle":"2021-12-15T18:37:17.387745Z","shell.execute_reply.started":"2021-12-15T18:37:17.381451Z","shell.execute_reply":"2021-12-15T18:37:17.386545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Data Exploration and Analysis**","metadata":{"id":"qOWZneep-xNo"}},{"cell_type":"markdown","source":"As a first step with our data, should explore our data and check what classes and data we have, the number of classes and the amount of images can tell us what dificulties can occur further in the project","metadata":{"id":"2ZYKFE2z_Uom"}},{"cell_type":"markdown","source":"Lets print our data of the images","metadata":{"id":"UEXgPpjrQKam"}},{"cell_type":"code","source":"print(train_data.head(),train_data.shape)","metadata":{"id":"gGSz36b169Ev","outputId":"c74d5405-06e7-4fc0-cb61-f3ef27d536d9","execution":{"iopub.status.busy":"2021-12-15T18:37:17.389749Z","iopub.execute_input":"2021-12-15T18:37:17.390451Z","iopub.status.idle":"2021-12-15T18:37:17.412098Z","shell.execute_reply.started":"2021-12-15T18:37:17.390366Z","shell.execute_reply":"2021-12-15T18:37:17.410877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As can be seen, we have a list of images and their class out of 4, healthy, multiple_diseases, rust and scab. Every entry is an image and have the value 1 for bieng in the class and 0 for not.\n","metadata":{"id":"yeEQo95UABEO"}},{"cell_type":"markdown","source":"We have 18632 images, lets find the distribution of the data in the classes.","metadata":{"id":"JQ44iChhQPic"}},{"cell_type":"code","source":"","metadata":{"id":"XJECIotUBbEB","outputId":"25595839-ae9b-40da-d416-cd350396cbcb","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As can be seen, 3 of the 4 classes have similar representation in our sata set. The multiple_diseases class is highly underrepresented, we have less then 100 images of this class, out of 18632 images. Our data is imbalanced, and we have to remember it as we draw conclusions","metadata":{"id":"4n5Rb2RmCwKm"}},{"cell_type":"markdown","source":"## **First Look at Our Images**","metadata":{"id":"1ot1NWOh69E0"}},{"cell_type":"markdown","source":"Thus far, we talk about how our images fit in our classes, but we haven't has a look at them, so we shall do it now.","metadata":{"id":"WsJTJSoY69E1"}},{"cell_type":"markdown","source":"First, lest see some leaves with scab","metadata":{"id":"78-FkI2aUZ2i"}},{"cell_type":"code","source":"scab_list=show_cond('scab') ","metadata":{"id":"5DkLVk3E69E4","execution":{"iopub.status.busy":"2021-12-15T18:37:17.414923Z","iopub.execute_input":"2021-12-15T18:37:17.415676Z","iopub.status.idle":"2021-12-15T18:37:18.589036Z","shell.execute_reply.started":"2021-12-15T18:37:17.415639Z","shell.execute_reply":"2021-12-15T18:37:18.587784Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As can be seen from the images, scabs manifest itself as dark brown spots on the leaves. one can easily think that those are just dirty leaves, but a professional will know that they have scab.","metadata":{"id":"mDhU8TLIUoRe"}},{"cell_type":"markdown","source":"Lest see some leaves with rust","metadata":{"id":"15P_uWnuUmtx"}},{"cell_type":"code","source":"rust_list=show_cond('rust')","metadata":{"id":"pM96HfCb69E6","outputId":"bd1311be-e5b8-411f-a531-948fcc952200","execution":{"iopub.status.busy":"2021-12-15T18:37:18.591239Z","iopub.execute_input":"2021-12-15T18:37:18.59191Z","iopub.status.idle":"2021-12-15T18:37:19.727163Z","shell.execute_reply.started":"2021-12-15T18:37:18.591855Z","shell.execute_reply":"2021-12-15T18:37:19.726051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As can be seen from the images, rust manifest itself as yellow-brownish spots on the leaves. The spots are much more identifiable and noticeable.","metadata":{"id":"D4KAm_4-V1kD"}},{"cell_type":"markdown","source":"Lest see some leaves with complex","metadata":{"id":"w6a3q7XxWpyf"}},{"cell_type":"code","source":"multi_list=show_cond('complex')","metadata":{"id":"Ezf-ewY_69E-","outputId":"4358b52c-efc8-4061-b43b-67c04123021e","execution":{"iopub.status.busy":"2021-12-15T18:37:19.728565Z","iopub.execute_input":"2021-12-15T18:37:19.729132Z","iopub.status.idle":"2021-12-15T18:37:21.063522Z","shell.execute_reply.started":"2021-12-15T18:37:19.72909Z","shell.execute_reply":"2021-12-15T18:37:21.06261Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As can be seen from the images, those leaves have features of both diseases.","metadata":{"id":"hrD9uTv3WzD1"}},{"cell_type":"markdown","source":"Lets see some healthy leaves","metadata":{"id":"cWaEhGGlZHMn"}},{"cell_type":"code","source":"healthy_list=show_cond('healthy')","metadata":{"id":"-LCV2wLQ69FC","outputId":"71c1aa81-b604-4c27-83ca-de60f85a98d7","execution":{"iopub.status.busy":"2021-12-15T18:37:21.065132Z","iopub.execute_input":"2021-12-15T18:37:21.065793Z","iopub.status.idle":"2021-12-15T18:37:22.220319Z","shell.execute_reply.started":"2021-12-15T18:37:21.065754Z","shell.execute_reply":"2021-12-15T18:37:22.219307Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"AS we expected the leaves has no features of the diseases","metadata":{"id":"RaV9moOqZUF8"}},{"cell_type":"markdown","source":"Lets see some frog_eye_leaf_spot leaves","metadata":{}},{"cell_type":"code","source":"frog_eye_list=show_cond('frog_eye_leaf_spot')","metadata":{"execution":{"iopub.status.busy":"2021-12-15T18:37:22.221631Z","iopub.execute_input":"2021-12-15T18:37:22.22209Z","iopub.status.idle":"2021-12-15T18:37:23.339137Z","shell.execute_reply.started":"2021-12-15T18:37:22.222054Z","shell.execute_reply":"2021-12-15T18:37:23.33791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lets see some powdery_mildew leaves","metadata":{}},{"cell_type":"code","source":"powdery_mildew_list=show_cond('powdery_mildew')","metadata":{"execution":{"iopub.status.busy":"2021-12-15T18:37:23.340717Z","iopub.execute_input":"2021-12-15T18:37:23.341429Z","iopub.status.idle":"2021-12-15T18:37:24.464565Z","shell.execute_reply.started":"2021-12-15T18:37:23.341363Z","shell.execute_reply":"2021-12-15T18:37:24.463664Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Color Analysis**","metadata":{"id":"FIDjsf6h69FE"}},{"cell_type":"markdown","source":"Colors in pictures are the basic features, thus analyising them is our first step. We already saw that the dieases has characteristics that are connected to color, so it is naturally a good direction.","metadata":{"id":"Ao_xZOdOZoFQ"}},{"cell_type":"markdown","source":"Changing the images array to 4 dimentional (image number, row, column, color channel) array so we could analyze them ","metadata":{"id":"_nnKo-z969FF"}},{"cell_type":"code","source":"healthy_array=images_4d_array(healthy_list)\nscab_array=images_4d_array(scab_list)\nmulti_array=images_4d_array(multi_list)\nrust_array=images_4d_array(rust_list)\nfrog_eye_array=images_4d_array(frog_eye_list)\npowdery_mildew_array=images_4d_array(powdery_mildew_list)\n\nprint(healthy_array.shape,scab_array.shape,multi_array.shape,rust_array.shape,frog_eye_array.shape,powdery_mildew_array.shape)","metadata":{"id":"QyfSirIG69FF","outputId":"58a151df-0403-4672-81e4-bd63825f16e6","execution":{"iopub.status.busy":"2021-12-15T18:37:24.468688Z","iopub.execute_input":"2021-12-15T18:37:24.469448Z","iopub.status.idle":"2021-12-15T18:37:24.509907Z","shell.execute_reply.started":"2021-12-15T18:37:24.4694Z","shell.execute_reply":"2021-12-15T18:37:24.508812Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" Lets plot the color histograms for each class","metadata":{"id":"etWdC--S69FI"}},{"cell_type":"code","source":"#print histograms of colors\ndef plot_hist_normed(images, channel, col):\n    vals = images[:,:,:,channel].flatten()\n    matplotlib.pyplot.ylabel(col)\n    matplotlib.pyplot.hist(vals)\n    matplotlib.pyplot.yticks([])\n\n\n#red  \nmatplotlib.pyplot.figure(figsize =(30, 15))\nmatplotlib.pyplot.subplot(3, 6, 1)\nplot_hist_normed(healthy_array, 0, 'red')\nmatplotlib.pyplot.title('healthy leaves')\nmatplotlib.pyplot.subplot(3, 6, 2)\nplot_hist_normed(scab_array, 0, 'red')\nmatplotlib.pyplot.title('scab leaves')\nmatplotlib.pyplot.subplot(3, 6, 3)\nplot_hist_normed(rust_array, 0, 'red')\nmatplotlib.pyplot.title('rust leaves')\nmatplotlib.pyplot.subplot(3, 6, 4)\nplot_hist_normed(multi_array, 0, 'red')\nmatplotlib.pyplot.title('multiple diseases leaves')\nmatplotlib.pyplot.subplot(3, 6, 5)\nplot_hist_normed(powdery_mildew_array, 0, 'red')\nmatplotlib.pyplot.title('powdery_mildew leaves')\nmatplotlib.pyplot.subplot(3, 6, 6)\nplot_hist_normed(frog_eye_array, 0, 'red')\nmatplotlib.pyplot.title('frog_eye_array leaves')\n\n#green\nmatplotlib.pyplot.subplot(3, 6, 7)\nplot_hist_normed(healthy_array, 1, 'green')\nmatplotlib.pyplot.title('healthy leaves')\nmatplotlib.pyplot.subplot(3, 6, 8)\nplot_hist_normed(scab_array, 1, 'green')\nmatplotlib.pyplot.title('scab leaves')\nmatplotlib.pyplot.subplot(3, 6, 9)\nplot_hist_normed(rust_array, 1, 'green')\nmatplotlib.pyplot.title('rust leaves')\nmatplotlib.pyplot.subplot(3, 6, 10)\nplot_hist_normed(multi_array, 1, 'green')\nmatplotlib.pyplot.title('multiple diseases leaves')\nmatplotlib.pyplot.subplot(3, 6, 11)\nplot_hist_normed(powdery_mildew_array, 1, 'green')\nmatplotlib.pyplot.title('powdery_mildew_array leaves')\nmatplotlib.pyplot.subplot(3, 6, 12)\nplot_hist_normed(frog_eye_array, 1, 'green')\nmatplotlib.pyplot.title('frog_eye_ leaves')\n\n#blue\nmatplotlib.pyplot.subplot(3, 6, 13)\nplot_hist_normed(healthy_array, 2, 'blue')\nmatplotlib.pyplot.title('healthy leaves')\nmatplotlib.pyplot.subplot(3, 6, 14)\nplot_hist_normed(scab_array, 2, 'blue')\nmatplotlib.pyplot.title('scab leaves')\nmatplotlib.pyplot.subplot(3, 6, 15)\nplot_hist_normed(rust_array, 2, 'blue')\nmatplotlib.pyplot.title('rust leaves')\nmatplotlib.pyplot.subplot(3, 6, 16)\nplot_hist_normed(multi_array, 2, 'blue')\nmatplotlib.pyplot.title('multiple diseases leaves')\nmatplotlib.pyplot.subplot(3, 6, 17)\nplot_hist_normed(powdery_mildew_array, 2, 'blue')\nmatplotlib.pyplot.title('powdery_mildew_array leaves')\nmatplotlib.pyplot.subplot(3, 6, 18)\nplot_hist_normed(frog_eye_array, 2, 'blue')\nmatplotlib.pyplot.title('frog_eye leaves')\n\n\nmatplotlib.pyplot.show()","metadata":{"id":"r8xjRhRi69FI","outputId":"110fd0eb-5880-4dee-c5de-60f5f32c54d3","execution":{"iopub.status.busy":"2021-12-15T18:37:24.512843Z","iopub.execute_input":"2021-12-15T18:37:24.513364Z","iopub.status.idle":"2021-12-15T18:37:27.673701Z","shell.execute_reply.started":"2021-12-15T18:37:24.513316Z","shell.execute_reply":"2021-12-15T18:37:27.672656Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It is not very informative, it is hard to notice the differences as the images are mostly similar. maybe we sould check the median and the mean of each channel","metadata":{"id":"7mnSa0S8gtqV"}},{"cell_type":"markdown","source":"Lest plot the mean and median values for the classes","metadata":{"id":"SzKW1Hpe69FK"}},{"cell_type":"code","source":"def summary(images, channel, col):\n    vals = images[:,:,:,channel].flatten()\n    chan_mean = np.mean(vals)\n    chan_median = np.median(vals)\n    print('{} mean: {}, median: {}'.format(col, str(chan_mean), str(chan_median)))\n\nprint('healthy leaves:')\nsummary(healthy_array, 0, 'red')\nsummary(healthy_array, 1, 'green')\nsummary(healthy_array, 2, 'blue')\nprint('scab leaves:')\nsummary(scab_array, 0, 'red')\nsummary(scab_array, 1, 'green')\nsummary(scab_array, 2, 'blue')\nprint('rust leaves:')\nsummary(rust_array, 0, 'red')\nsummary(rust_array, 1, 'green')\nsummary(rust_array, 2, 'blue')\nprint('multiple diseases leaves:')\nsummary(multi_array, 0, 'red')\nsummary(multi_array, 1, 'green')\nsummary(multi_array, 2, 'blue')\nprint('powdery_mildew leaves:')\nsummary(powdery_mildew_array, 0, 'red')\nsummary(powdery_mildew_array, 1, 'green')\nsummary(powdery_mildew_array, 2, 'blue')\nprint('frog_eye leaves:')\nsummary(frog_eye_array, 0, 'red')\nsummary(frog_eye_array, 1, 'green')\nsummary(frog_eye_array, 2, 'blue')\n","metadata":{"id":"rJmLsYDM69FL","outputId":"4e8b7f57-7fdf-4d6f-ee22-5ce92669c2c8","execution":{"iopub.status.busy":"2021-12-15T18:37:27.675271Z","iopub.execute_input":"2021-12-15T18:37:27.675714Z","iopub.status.idle":"2021-12-15T18:37:28.057893Z","shell.execute_reply.started":"2021-12-15T18:37:27.675676Z","shell.execute_reply":"2021-12-15T18:37:28.056637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As can be seen the blue channels mean and median of the diseased leaves are lower, and the red channels mean and median are higher then the healthy ones. We should explore the difference further by comparing the healty and diseased leaves pictures","metadata":{"id":"47PTATpA69FN"}},{"cell_type":"code","source":"diseased_leaves_array=np.concatenate((rust_array,multi_array,scab_array,frog_eye_array,powdery_mildew_array))\ndiseased_leaves_array.shape","metadata":{"id":"j2ljA_bK69FN","outputId":"a9afe2ee-a00d-4848-da5c-b4d4494d226e","execution":{"iopub.status.busy":"2021-12-15T18:37:28.059511Z","iopub.execute_input":"2021-12-15T18:37:28.059926Z","iopub.status.idle":"2021-12-15T18:37:28.08237Z","shell.execute_reply.started":"2021-12-15T18:37:28.059898Z","shell.execute_reply":"2021-12-15T18:37:28.081154Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#red  \nmatplotlib.pyplot.figure(figsize =(30, 15))\nmatplotlib.pyplot.subplot(3, 2, 1)\nplot_hist_normed(healthy_array, 0, 'red')\nmatplotlib.pyplot.title('healthy leaves')\nmatplotlib.pyplot.subplot(3, 2, 2)\nplot_hist_normed(diseased_leaves_array, 0, 'red')\nmatplotlib.pyplot.title('diseased leaves')\n\n#green\nmatplotlib.pyplot.subplot(3, 2, 3)\nplot_hist_normed(healthy_array, 1, 'green')\nmatplotlib.pyplot.title('healthy leaves')\nmatplotlib.pyplot.subplot(3, 2, 4)\nplot_hist_normed(diseased_leaves_array, 1, 'green')\nmatplotlib.pyplot.title('diseased leaves')\n\n#blue\nmatplotlib.pyplot.subplot(3, 2, 5)\nplot_hist_normed(healthy_array, 2, 'blue')\nmatplotlib.pyplot.title('healthy leaves')\nmatplotlib.pyplot.subplot(3, 2, 6)\nplot_hist_normed(diseased_leaves_array, 2, 'blue')\nmatplotlib.pyplot.title('diseased leaves')\n\nmatplotlib.pyplot.show()\n\n\nprint('healthy leaves:')\nsummary(healthy_array, 0, 'red')\nsummary(healthy_array, 1, 'green')\nsummary(healthy_array, 2, 'blue')\nprint('diseased leaves:')\nsummary(diseased_leaves_array, 0, 'red')\nsummary(diseased_leaves_array, 1, 'green')\nsummary(diseased_leaves_array, 2, 'blue')","metadata":{"id":"nIxW2C6j69FQ","outputId":"32477521-308a-44de-b7d9-057d327881a7","execution":{"iopub.status.busy":"2021-12-15T18:37:28.084048Z","iopub.execute_input":"2021-12-15T18:37:28.084667Z","iopub.status.idle":"2021-12-15T18:37:29.908494Z","shell.execute_reply.started":"2021-12-15T18:37:28.084596Z","shell.execute_reply":"2021-12-15T18:37:29.906404Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we expected the blue values are lower and the red values are higher. We can see that the diseased leaves have brown or yellow spots on them. That might be the reason for the discrepancy as in those colors the blue channel has a lower value and the red color has a higher value, and if that is the reason, those differences might help us to identify the healthy leaves. This hypothesis is bold and in order to try and see if it has any truth in it more exploration is needed\n","metadata":{"id":"XG-KeL1l69FS"}},{"cell_type":"markdown","source":"We want to further explore the main difference between the classes, and to try and check our hypothesis. Principal component analysis (PCA) is an effective way to do so. PCA is a method to reduce the dimension of our features, and find the most dominant ones ","metadata":{"id":"M-D0JbI169FT"}},{"cell_type":"markdown","source":"# **Modeling**","metadata":{"id":"14nId2_mtj0v"}},{"cell_type":"markdown","source":"In this section we will try to fit models to our data so we could predict on new data, which of the 4 classes they are part of.","metadata":{"id":"qjlS_ke3u18x"}},{"cell_type":"markdown","source":"## **CNN-Convolution Neural Network**","metadata":{"id":"1J9t9fXSv17Z"}},{"cell_type":"markdown","source":"Convolution neural network is the fittest and most robust model that we know of to this task. It will serve as our main tool in this classification task, thus we will prepare the ground before applying it. A convolutional neural network consists of an input and an output layer, as well as multiple hidden layers. The hidden layers of a CNN typically consist of a series of convolutional layers that convolve with a multiplication or other dot product. The activation function is commonly a ReLU layer, and is subsequently followed by additional convolutions such as pooling layers, fully connected layers and normalization layers, referred to as hidden layers because their inputs and outputs are masked by the activation function and final convolution.","metadata":{"id":"DxwCYnuAs_Wj"}},{"cell_type":"markdown","source":"### **Set Early Stopping Parameters**","metadata":{"id":"hGylXNCFPLVj"}},{"cell_type":"markdown","source":"We have limited resources, and we want to make the most out of them. In order to do so, we set stopping parameters which determands that as the model reaching a plateau, the run will stop","metadata":{"id":"iLzGSUt1uuxb"}},{"cell_type":"code","source":"LR_reduce=keras.callbacks.ReduceLROnPlateau(monitor='val_accuracy',\n                            factor=.5,\n                            patience=10,\n                            min_lr=.000001,\n                            verbose=0)\n\nES_monitor=keras.callbacks.EarlyStopping(monitor='val_loss',\n                          patience=20)\n\n\nreg = .0005","metadata":{"id":"yhp6yAY46fg1","execution":{"iopub.status.busy":"2021-12-15T18:37:29.91029Z","iopub.execute_input":"2021-12-15T18:37:29.910749Z","iopub.status.idle":"2021-12-15T18:37:29.917779Z","shell.execute_reply.started":"2021-12-15T18:37:29.910708Z","shell.execute_reply":"2021-12-15T18:37:29.916316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Plot History Function**","metadata":{"id":"JhFyHZLUT7PV"}},{"cell_type":"markdown","source":"We will use this function to plot our model progression","metadata":{"id":"UceHTdLyyoQU"}},{"cell_type":"code","source":"def plot_history(history):\n\n  h = history.history\n\n  offset = 5\n  epochs = range(offset, len(h['loss']))\n\n  matplotlib.pyplot.figure(1, figsize=(20, 6))\n\n  matplotlib.pyplot.subplot(121)\n  matplotlib.pyplot.xlabel('epochs')\n  matplotlib.pyplot.ylabel('loss')\n  matplotlib.pyplot.plot(epochs, h['loss'][offset:], label='train')\n  matplotlib.pyplot.plot(epochs, h['val_loss'][offset:], label='val')\n  matplotlib.pyplot.legend()\n\n  matplotlib.pyplot.subplot(122)\n  matplotlib.pyplot.xlabel('epochs')\n  matplotlib.pyplot.ylabel('accuracy')\n  matplotlib.pyplot.plot(h[f'accuracy'], label='train')\n  matplotlib.pyplot.plot(h[f'val_accuracy'], label='val')\n  matplotlib.pyplot.legend()\n\n  matplotlib.pyplot.show()\n\n  pred_test = model.predict(x_val)\n  roc_sum = 0\n  classes=['healthy',\t'complex',\t'rust',\t'scab' , 'frog_eye_leaf_spot' , 'powdery_mildew']\n  for i in range(6):\n      score = sklearn.metrics.roc_auc_score(y_val.iloc[:,i].values.astype('int32'), pred_test[:,i])\n      roc_sum += score\n      print(f'AUC-ROC {classes[i]}  {score:.3f}')\n\n  roc_sum /= 6\n  print(f'totally roc score:{roc_sum:.3f}')","metadata":{"id":"99_kJiBAUC9-","execution":{"iopub.status.busy":"2021-12-15T18:37:29.919616Z","iopub.execute_input":"2021-12-15T18:37:29.920272Z","iopub.status.idle":"2021-12-15T18:37:29.933675Z","shell.execute_reply.started":"2021-12-15T18:37:29.920215Z","shell.execute_reply":"2021-12-15T18:37:29.932432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **CNN Model**","metadata":{"id":"LbjpiKvwQfLA"}},{"cell_type":"markdown","source":"Now we will create a CNN model to our use, it is based on several standards CNN we found and combined","metadata":{"id":"hW2qYbNmzHP5"}},{"cell_type":"code","source":"def creat_model(img_size):\n  model = keras.models.Sequential()\n\n  model.add(keras.layers.Conv2D(32, kernel_size=(5,5),activation='relu', input_shape=(img_size, img_size, 3), kernel_regularizer=keras.regularizers.l2(reg)))\n  model.add(keras.layers.BatchNormalization(axis=-1,center=True,scale=False))\n  model.add(keras.layers.Conv2D(128, kernel_size=(5,5),activation='relu', kernel_regularizer=keras.regularizers.l2(reg)))\n  model.add(keras.layers.BatchNormalization(axis=-1,center=True,scale=False))\n  model.add(keras.layers.MaxPooling2D(pool_size=(2,2), padding='SAME'))\n  model.add(keras.layers.Dropout(.25))\n\n  model.add(keras.layers.Conv2D(32, kernel_size=(3,3),activation='relu', kernel_regularizer=keras.regularizers.l2(reg)))\n  model.add(keras.layers.BatchNormalization(axis=-1,center=True,scale=False))\n  model.add(keras.layers.Conv2D(128, kernel_size=(3,3),activation='relu',kernel_regularizer=keras.regularizers.l2(reg)))\n  model.add(keras.layers.BatchNormalization(axis=-1,center=True,scale=False))\n  model.add(keras.layers.MaxPooling2D(pool_size=(2,2), padding='SAME'))\n  model.add(keras.layers.Dropout(.25))\n\n\n  model.add(keras.layers.Conv2D(128, kernel_size=(5,5),activation='relu', kernel_regularizer=keras.regularizers.l2(reg)))\n  model.add(keras.layers.BatchNormalization(axis=-1,center=True,scale=False))\n  model.add(keras.layers.Conv2D(512, kernel_size=(5,5),activation='relu',kernel_regularizer=keras.regularizers.l2(reg)))\n  model.add(keras.layers.BatchNormalization(axis=-1,center=True,scale=False))\n  model.add(keras.layers.MaxPooling2D(pool_size=(2,2), padding='SAME'))\n  model.add(keras.layers.Dropout(.25))\n\n  model.add(keras.layers.Conv2D(128, kernel_size=(3,3),activation='relu',kernel_regularizer=keras.regularizers.l2(reg)))\n  model.add(keras.layers.BatchNormalization(axis=-1,center=True,scale=False))\n  model.add(keras.layers.Conv2D(512, kernel_size=(3,3),activation='relu',kernel_regularizer=keras.regularizers.l2(reg)))\n  model.add(keras.layers.BatchNormalization(axis=-1,center=True,scale=False))\n  model.add(keras.layers.MaxPooling2D(pool_size=(2,2), padding='SAME'))\n  model.add(keras.layers.Dropout(.25))\n\n  model.add(keras.layers.Flatten())\n  model.add(keras.layers.Dense(300,activation='relu'))\n  model.add(keras.layers.BatchNormalization(axis=-1,center=True,scale=False))\n  model.add(keras.layers.Dropout(.25))\n  model.add(keras.layers.Dense(200,activation='relu'))\n  model.add(keras.layers.BatchNormalization(axis=-1,center=True,scale=False))\n  model.add(keras.layers.Dropout(.25))\n  model.add(keras.layers.Dense(100,activation='relu'))\n  model.add(keras.layers.BatchNormalization(axis=-1,center=True,scale=False))\n  model.add(keras.layers.Dropout(.25))\n  model.add(keras.layers.Dense(6,activation='softmax'))\n\n  model.summary()\n \n  \n  return model","metadata":{"id":"OUZOQjFN5_Nw","execution":{"iopub.status.busy":"2021-12-15T18:37:29.935131Z","iopub.execute_input":"2021-12-15T18:37:29.935615Z","iopub.status.idle":"2021-12-15T18:37:29.960435Z","shell.execute_reply.started":"2021-12-15T18:37:29.935572Z","shell.execute_reply":"2021-12-15T18:37:29.959053Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **First Attempt- 100x100 Image Size**\n","metadata":{"id":"ctkxbSpXOkhN"}},{"cell_type":"markdown","source":"First we will try our model on images that have 100 pixels as their heigth and 100 pixels are their lenght","metadata":{"id":"KebThCP52IFo"}},{"cell_type":"markdown","source":"Lets load the images","metadata":{"id":"Q95boSdY4MLU"}},{"cell_type":"code","source":"# y_val","metadata":{"execution":{"iopub.status.busy":"2021-12-15T18:37:29.962095Z","iopub.execute_input":"2021-12-15T18:37:29.962657Z","iopub.status.idle":"2021-12-15T18:37:29.978767Z","shell.execute_reply.started":"2021-12-15T18:37:29.962564Z","shell.execute_reply":"2021-12-15T18:37:29.977651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_100X100_images = train_data[\"image\"].progress_apply(load_image, args=((100,100),))\n","metadata":{"id":"uWQtv7UK0dI8","outputId":"d1255e3c-907d-418a-fd7c-9b792dfdc8c8","execution":{"iopub.status.busy":"2021-12-15T18:37:29.980575Z","iopub.execute_input":"2021-12-15T18:37:29.981206Z","iopub.status.idle":"2021-12-15T18:37:29.991266Z","shell.execute_reply.started":"2021-12-15T18:37:29.981153Z","shell.execute_reply":"2021-12-15T18:37:29.990166Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we will splits the data into train and test data","metadata":{"id":"6bgdj2--suGJ"}},{"cell_type":"code","source":"# targets=train_data.drop('image',axis=1)","metadata":{"id":"ZMghZA7iW9Ez","execution":{"iopub.status.busy":"2021-12-15T18:37:29.994851Z","iopub.execute_input":"2021-12-15T18:37:29.995209Z","iopub.status.idle":"2021-12-15T18:37:30.004328Z","shell.execute_reply.started":"2021-12-15T18:37:29.99517Z","shell.execute_reply":"2021-12-15T18:37:30.003099Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_data","metadata":{"execution":{"iopub.status.busy":"2021-12-15T18:37:30.005746Z","iopub.execute_input":"2021-12-15T18:37:30.00646Z","iopub.status.idle":"2021-12-15T18:37:30.016622Z","shell.execute_reply.started":"2021-12-15T18:37:30.006378Z","shell.execute_reply":"2021-12-15T18:37:30.015562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# current_train_data=train_data.loc[train_100X100_images.index]\n# images_array=images_4d_array(train_100X100_images)\n# targets=current_train_data.drop('image',axis=1)\n# x_train, x_val, y_train, y_val = sklearn.model_selection.train_test_split(images_array, targets, test_size=0.2, random_state=seed)\n# x_train = x_train.astype('float32')\n# x_val = x_val.astype('float32')\n# x_train /= 255\n# x_val /= 255\n# x_train.shape, x_val.shape, y_train.shape, y_val.shape","metadata":{"id":"9Z4CLxeMfN1P","outputId":"d52f4711-343b-42f2-e892-517514bd29a2","execution":{"iopub.status.busy":"2021-12-15T18:37:30.018402Z","iopub.execute_input":"2021-12-15T18:37:30.018861Z","iopub.status.idle":"2021-12-15T18:37:30.027306Z","shell.execute_reply.started":"2021-12-15T18:37:30.01882Z","shell.execute_reply":"2021-12-15T18:37:30.026064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Running the model on 100x100 images","metadata":{"id":"IXVRsDu-RGDe"}},{"cell_type":"code","source":"# model = creat_model(100)\n# model.compile(optimizer='rmsprop',\n#               loss='categorical_crossentropy',\n#               metrics=['accuracy']\n#               )","metadata":{"id":"jj5qgXH-6lAt","outputId":"5957de52-9c6b-4b0b-8e2f-82a43c6b407f","execution":{"iopub.status.busy":"2021-12-15T18:37:30.029147Z","iopub.execute_input":"2021-12-15T18:37:30.029593Z","iopub.status.idle":"2021-12-15T18:37:30.041533Z","shell.execute_reply.started":"2021-12-15T18:37:30.029538Z","shell.execute_reply":"2021-12-15T18:37:30.040359Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# history = model.fit(x_train,\n#                     y_train,\n#                     batch_size=24,\n#                     epochs=100,\n#                     steps_per_epoch=x_train.shape[0] // 24,\n#                     verbose=0,\n#                     callbacks=[ES_monitor,LR_reduce],\n#                     validation_data=(x_val, y_val),\n#                     validation_steps=x_val.shape[0]//24\n#                     )","metadata":{"id":"IkztNL9c68NO","execution":{"iopub.status.busy":"2021-12-15T18:37:30.043609Z","iopub.execute_input":"2021-12-15T18:37:30.044104Z","iopub.status.idle":"2021-12-15T18:37:30.051391Z","shell.execute_reply.started":"2021-12-15T18:37:30.043989Z","shell.execute_reply":"2021-12-15T18:37:30.050297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lets print our results","metadata":{"id":"_J11wQ1id026"}},{"cell_type":"code","source":"# score = model.evaluate(x_val, y_val, verbose=0)\n\n# print('Test loss:', score[0])\n# print('Test accuracy:', score[1])\n# plot_history(history)","metadata":{"id":"a5P9LHV4-roq","outputId":"063ca2e2-093e-499e-8944-45e9cc727b1c","execution":{"iopub.status.busy":"2021-12-15T18:37:30.053257Z","iopub.execute_input":"2021-12-15T18:37:30.053711Z","iopub.status.idle":"2021-12-15T18:37:30.061185Z","shell.execute_reply.started":"2021-12-15T18:37:30.05367Z","shell.execute_reply":"2021-12-15T18:37:30.060179Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Our accuracy has improved drastically to 84%, but there are still some problems, we can see at the end that there is a gap between the validation and the train matrices, which can indicate that there is some kind of overfitting. We can also see that the model is having hard time classifing the \"multiple_diseases\" class, due to it lower AUC-ROC score.\n\n\n\n\n","metadata":{"id":"8XcuWIU89kvt"}},{"cell_type":"markdown","source":"Lets take a look at the confusin matrix for more insight","metadata":{"id":"P5jnehuQxW9S"}},{"cell_type":"code","source":"# y_val_pred = model.predict_classes(x_val)\n# mat = sklearn.metrics.confusion_matrix(get_target_array_no_im_id(y_val),y_val_pred)\n# sns.heatmap(mat.T, square=True, annot=True, cbar=False)\n# matplotlib.pyplot.xlabel('true label')\n# matplotlib.pyplot.ylabel('predict label');","metadata":{"id":"R-XR0ZlfqJgL","outputId":"7cadff78-7ce4-494a-84c2-00e0633da6a3","execution":{"iopub.status.busy":"2021-12-15T18:37:30.062946Z","iopub.execute_input":"2021-12-15T18:37:30.063418Z","iopub.status.idle":"2021-12-15T18:37:30.07098Z","shell.execute_reply.started":"2021-12-15T18:37:30.063374Z","shell.execute_reply":"2021-12-15T18:37:30.070014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The classes are ordered in this manner: \"healthy\"  \"multiple_diseases\"  \"rust\"  \"scab\". We can see that our model is pretty good at identifing the \"rust\" class and that it is over all can identify the classes we a decent accuracy. We can see that the main issue of our model is it poor prediction of the \"multiple_diseases\" class, it could be that the under representation of that class is a big factor of that. We can that our model predicts some leaves with \"scab\" as \"healthy\", we think that it can mistake the rust to be the leaves veins. We think that maybe using better resolution of the images can help that problems.","metadata":{"id":"R7UCT0cj42YD"}},{"cell_type":"code","source":"# model.save(\"..output/kaggle/working/model_100X100.h5\")","metadata":{"id":"EairoTO_9G_W","execution":{"iopub.status.busy":"2021-12-15T18:37:30.072792Z","iopub.execute_input":"2021-12-15T18:37:30.0734Z","iopub.status.idle":"2021-12-15T18:37:30.080186Z","shell.execute_reply.started":"2021-12-15T18:37:30.073359Z","shell.execute_reply":"2021-12-15T18:37:30.078723Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Second Attempt- 224x224 Image Size**\n","metadata":{"id":"BemH-w6_SWTK"}},{"cell_type":"markdown","source":"We will try to use better resolution so the model could maybe classify some of the classes (\"scab\") better ","metadata":{"id":"f_dIjP7p3tlk"}},{"cell_type":"code","source":"#In order to save memory in kaggle I resized it to 100 by 100\n# img_size = 224\nimg_size = 100","metadata":{"id":"xl6dk4AI-LTL","execution":{"iopub.status.busy":"2021-12-15T18:37:30.088125Z","iopub.execute_input":"2021-12-15T18:37:30.088392Z","iopub.status.idle":"2021-12-15T18:37:30.092876Z","shell.execute_reply.started":"2021-12-15T18:37:30.088366Z","shell.execute_reply":"2021-12-15T18:37:30.091645Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_224X224_images = train_data[\"image\"].progress_apply(load_image, args=((img_size,img_size),))","metadata":{"id":"VcXKVr362pCv","outputId":"1c822f27-c3d5-4a7d-abfc-793dee63a81a","execution":{"iopub.status.busy":"2021-12-15T18:37:30.099033Z","iopub.execute_input":"2021-12-15T18:37:30.099701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we will splits the data into train and test data","metadata":{"id":"cVTRP3xLdre4"}},{"cell_type":"code","source":"current_train_data=train_data.loc[train_224X224_images.index]\nimages_array=images_4d_array(train_224X224_images)\ntargets=current_train_data.drop('image',axis=1)\nx_train2, x_val, y_train2, y_val = sklearn.model_selection.train_test_split(images_array, targets, test_size=0.2, random_state=seed)\nx_train2 = x_train2.astype('float32')\nx_val = x_val.astype('float32')\nx_train2 /= 255\nx_val /= 255\nx_train2.shape, x_val.shape, y_train2.shape, y_val.shape","metadata":{"id":"zgSmngwb2oQT","outputId":"598dcdae-b254-46bc-8aa0-6ff1c295aaa7","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Running the model on 224x224 images","metadata":{"id":"gaP1Rk-Nzc9U"}},{"cell_type":"code","source":"model = creat_model(img_size)\nmodel.compile(optimizer='rmsprop',\n              loss='categorical_crossentropy',\n              metrics=['accuracy']\n              )","metadata":{"id":"wxTqXVOK-G10","outputId":"341b0b1b-0a9e-42a5-d8ce-18955ab05807","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train2.shape","metadata":{"id":"dH8xwWCr1Bsr","outputId":"47d1afd0-e6a1-4156-ed82-43ebba8f26f8","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit(x_train2,\n                    y_train2,\n                    batch_size=24,\n                    epochs=100,\n                    steps_per_epoch=x_train2.shape[0] // 24,\n                    verbose=0,\n                    callbacks=[ES_monitor,LR_reduce],\n                    validation_data=(x_val, y_val),\n                    validation_steps=x_val.shape[0]//24\n                    )","metadata":{"id":"TJUhMLkW-aGZ","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lets print our results","metadata":{"id":"2mZ2pB12d67i"}},{"cell_type":"code","source":"score = model.evaluate(x_val, y_val, verbose=0)\n\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])","metadata":{"id":"_G_etafUOolK","outputId":"6bc64b5a-ded0-45c1-b9f3-d6f68c956978","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_history(history)","metadata":{"id":"oclTTF30UYoP","outputId":"f15677f0-f679-42ed-cf0a-241e212b425d","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Our accuracy has improved to 87%, but there are still some problems, we can see at the end that there is still a gap between the validation and the train matrices, which can indicate that there is some kind of overfitting. We can also see that the model is still having hard time classifing the multi-diseased class, due to it lower AUC-ROC score.\nSo where did the improvment came from?\n\n\n\n","metadata":{"id":"tZttLfQF0sXM"}},{"cell_type":"markdown","source":"Lets take a look at the confusin matrix for more insight","metadata":{"id":"S8uU5zWq0sXN"}},{"cell_type":"code","source":"y_val_pred = model.predict_classes(x_val)\nmat = sklearn.metrics.confusion_matrix(get_target_array_no_im_id(y_val),y_val_pred)\nsns.heatmap(mat.T, square=True, annot=True, cbar=False)\nmatplotlib.pyplot.xlabel('true label')\nmatplotlib.pyplot.ylabel('predict label');","metadata":{"id":"UTZqXwoK0UqO","outputId":"eee9f506-098e-49d7-d89c-754738e960f7","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The classes are ordered in this manner: \"healthy\"  \"multiple_diseases\"  \"rust\"  \"scab\". We can see that our  new model is pretty good at identifing the \"rust\" class and that it is over all can identify the classes we a decent accuracy as did the last one, but here we can see that using a better rsoluton made the model to identify the \"scab\" leaves better, as it can easily differ the leaf veins from the disease symptoms. We can see that the main issue of our new model has retied it poor prediction of the \"multiple_diseases\" class, it classifies those images as having one of the diseases and not both. It could be that the underrepresentation of that class is a big factor of that. ","metadata":{"id":"P2WKJ_VD2TUJ"}},{"cell_type":"code","source":"model.save(\"..output/kaggle/working/model_224X224.h5\")","metadata":{"id":"g0wsNPKiQ8Le","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Data Transformations and Augmentation**","metadata":{"id":"YFLkLO66UfTz"}},{"cell_type":"markdown","source":"Image transformations and augmentation is an efficient way to diversify data and to generalize a model, we will try to use it to improve our model predictions and to maybe aid it to classify the \"multiple_diseases\" class better, as generating more images could give it a greater number of relevant images ","metadata":{"id":"EKqn5_bf5DYD"}},{"cell_type":"markdown","source":"We will use a built in function to do so, and we will change images rotation, orientation, brightness and scale\n\n ","metadata":{"id":"HCc2JDphWOoW"}},{"cell_type":"code","source":"datagen = keras.preprocessing.image.ImageDataGenerator(rotation_range=45,\n                             shear_range=.25,\n                              zoom_range=.25,\n                              width_shift_range=.25,\n                              height_shift_range=.25,\n                              rescale=1/255,\n                              brightness_range=[.5,1.5],\n                              horizontal_flip=True,\n                              vertical_flip=True,\n                              fill_mode='nearest'\n#                              featurewise_center=True,\n#                              samplewise_center=True,\n#                              featurewise_std_normalization=True,\n#                              samplewise_std_normalization=True,\n#                              zca_whitening=True\n                              )","metadata":{"id":"5U0bMaR2U1rV","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Running the model on 224x224 images with the transformations and augmentation","metadata":{"id":"fBZmdQ0TWWG_"}},{"cell_type":"code","source":"model = creat_model(img_size)\nmodel.compile(optimizer='rmsprop',\n              loss='categorical_crossentropy',\n              metrics=['accuracy']\n              )","metadata":{"id":"Z_IDAnYXU9gF","outputId":"753d2c55-cfa3-40be-ef5d-301b244581c5","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit_generator(datagen.flow(x_train2, y_train2, batch_size=24),\n                              epochs=100,\n                              steps_per_epoch=x_train2.shape[0] // 24,\n                              verbose=0,\n                              callbacks=[ES_monitor,LR_reduce],\n                              validation_data=datagen.flow(x_val, y_val,batch_size=24),\n                              validation_steps=x_val.shape[0]//24\n                              )","metadata":{"id":"i5Nn5W9oVO-w","outputId":"7e121db2-a31f-498a-adca-8bfb5e02bd6b","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lets print our results","metadata":{"id":"QMTkNsyId_Cn"}},{"cell_type":"code","source":"score = model.evaluate(x_val, y_val, verbose=0)\n\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])","metadata":{"id":"TzbUbeCtU-2j","outputId":"bfe59be3-674a-4a52-e657-5d8d316f17bb","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_history(history)","metadata":{"id":"RImEOWUTVIIr","outputId":"0c29e7d6-82b7-4f2f-8a93-9cad76108ba4","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Our accuracy has improved to 92%, a great accomplishment. we can see that the vlidation and test graph have no gap which can show that the model didn't overpitted to the test. We can see that the model is having a better time classifing the \"multiple_diseases\" class from the better AUC-ROC score. ","metadata":{"id":"Yx2s4V4P74t-"}},{"cell_type":"markdown","source":"Lets take a look at the confusin matrix for more insight","metadata":{"id":"qXdex2RiIk4A"}},{"cell_type":"code","source":"y_val_pred = model.predict_classes(x_val)\nmat = sklearn.metrics.confusion_matrix(get_target_array_no_im_id(y_val),y_val_pred)\nsns.heatmap(mat.T, square=True, annot=True, cbar=False)\nmatplotlib.pyplot.xlabel('true label')\nmatplotlib.pyplot.ylabel('predict label');","metadata":{"id":"V10MoqZN6EbW","outputId":"245a3838-1bb9-40de-b8a1-7752590666a1","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The classes are ordered in this manner: \"healthy\" \"multiple_diseases\" \"rust\" \"scab\". We can see that our new model is better at idintifing the \"healthy\" \"rust\" \"scab\" classes, as it classified corretly almost all of the images of those classes, but still even though is classified more images of that class correctly, the classification seems random.  Again it could be that the underrepresentation of that class is a big factor of that.","metadata":{"id":"u4-u8MkrKseL"}},{"cell_type":"code","source":"model.save(\"..output/kaggle/working/model_224X224_image_generate.h5\")","metadata":{"id":"m83xcesC9Yu6","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Handling Imbalanced in Our Dataset -SMOTE**\n","metadata":{"id":"b26IjCgJcM8g"}},{"cell_type":"markdown","source":"We'll be applying Synthetic Minority Oversampling Technique (SMOTE). SMOTE works by selecting examples that are close in the feature space, drawing a line between the examples in the feature space and drawing a new sample at a point along that line, and by that balance a minority class. [SMOTE](https://machinelearningmastery.com/smote-oversampling-for-imbalanced-classification/)\n","metadata":{"id":"SB7LS85Rc1Fj"}},{"cell_type":"markdown","source":" Lets check its affect on our dataset. The size of our dataset before running the function","metadata":{"id":"N3k6WMWkdKkE"}},{"cell_type":"code","source":"print(x_train2.shape,y_train2.shape)","metadata":{"id":"lGMU74rZxS5V","outputId":"a917be7c-173b-4f7e-fed1-fb6e3bff3d9f","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Applying SMOTE on our dataset","metadata":{"id":"eD6c6tmcEL71"}},{"cell_type":"code","source":" sm = imblearn.over_sampling.SMOTE(random_state = 115) \n \nx_train2, y_train2 = sm.fit_resample(get_all_pixels(x_train2),y_train2.to_numpy())\nx_train2 = x_train2.reshape((-1, img_size, img_size, 3))\nx_train2.shape, y_train2.sum(axis=0)","metadata":{"id":"ryG6MKNOvP3l","outputId":"1b7fc9ed-28a7-4b60-befd-19b347cae1c8","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We see that we have added about 500 examples,most of them to the minority class, and our data is now balanced","metadata":{"id":"uzLB1eEDx9dN"}},{"cell_type":"markdown","source":"Let's try to fit the CNN to the new dataset","metadata":{"id":"H1XCryy2FjP4"}},{"cell_type":"code","source":"model = creat_model(img_size)\nmodel.compile(optimizer='rmsprop',\n              loss='categorical_crossentropy',\n              metrics=['accuracy']\n              )","metadata":{"id":"XDqEBJvOwfdA","outputId":"151e7e2d-634a-4f54-e70c-229743a0aca4","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit_generator(datagen.flow(x_train2, y_train2, batch_size=24),\n                              epochs=250,\n                              steps_per_epoch=x_train2.shape[0] // 24,\n                              verbose=0,\n                              callbacks=[ES_monitor,LR_reduce],\n                              validation_data=datagen.flow(x_val, y_val,batch_size=24),\n                              validation_steps=x_val.shape[0]//24\n                              )","metadata":{"id":"0joLyHtJwkc0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lets print our results","metadata":{"id":"nYeIVYr_eLA6"}},{"cell_type":"code","source":"score = model.evaluate(x_val, y_val, verbose=0)\n\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])","metadata":{"id":"J67RP9KswoLb","outputId":"d01f0c47-91a3-42ec-e816-2e13d4ac9da1","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_history(history)","metadata":{"id":"BGKGtXCgvWNB","outputId":"a0dbc66f-8cea-4b7c-bbe9-9bc23d2ed4b2","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Our accuracy has decreased to 74%, Worst then our first CNN. It hard to understand what went wrong; so lets take a look at the confusin matrix for more insight","metadata":{"id":"HxZ6U6yeF3nA"}},{"cell_type":"code","source":"y_val_pred = model.predict_classes(x_val)\nmat = sklearn.metrics.confusion_matrix(get_target_array_no_im_id(y_val),y_val_pred)\nsns.heatmap(mat.T, square=True, annot=True, cbar=False)\nmatplotlib.pyplot.xlabel('true label')\nmatplotlib.pyplot.ylabel('predict label');","metadata":{"id":"0n9qbfmS_KyL","outputId":"bc1d46c6-0a2c-415c-aa55-447f918db45a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The classes are ordered in this manner: \"healthy\" \"multiple_diseases\" \"rust\" \"scab\". We can see that our new model classified most of the \"multiple_diseases\" class, but by doing it, it also classified leaves that are in the \"rust\" class as \"multiple_diseases\", and that has made it worse. It seems that the fact that the \"multiple_diseases\" has similar features of the other classes made the \"synthetic\" samlpes have similar atribute to this classes which made the model to get worse. ","metadata":{"id":"CEVx27PpUYhY"}},{"cell_type":"code","source":"model.save(\"..output/kaggle/working/model_224X224_smoth.h5\")","metadata":{"id":"GXWoiqwk9j40","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Transfer Learning**","metadata":{"id":"7vUVJErXZm7t"}},{"cell_type":"markdown","source":"The general idea of transfer learning is to use knowledge learned from tasks for which a lot of labelled data is available in settings where only little labelled data is available. We will use the pretrained model DenseNet121 inorder to classify","metadata":{"id":"aaO_j-UBZ6p3"}},{"cell_type":"markdown","source":"### **Densenet Model**","metadata":{"id":"qkpYXpFAlN1M"}},{"cell_type":"markdown","source":"We chose DenseNet because it have some advantages over CNN when they are deep. This is because the path for information from the input layer until the output layer (and for the gradient in the opposite direction) becomes so big, that they can get vanished before reaching the other side.\nDenseNets simplify the connectivity pattern between layers introduced in other architectures:\nHighway Networks [2]\nResidual Networks [3]\nFractal Networks [4]\nThis solve the problem ensuring maximum information (and gradient) flow. To do it, the model is simply connected from every layer directly to the other ones. Instead of drawing representational power from extremely deep or wide architectures, DenseNets exploit the potential of the network through feature reuse. Counter-intuitively, by connecting this way DenseNets require fewer parameters than an equivalent traditional CNN, as there is no need to learn redundant feature maps","metadata":{"id":"G0be_-M0oZmJ"}},{"cell_type":"markdown","source":"We will start by setting a TPU","metadata":{"id":"mpnUtBSedNqS"}},{"cell_type":"code","source":"AUTO = tf.data.experimental.AUTOTUNE\ntpu = tf.distribute.cluster_resolver.TPUClusterResolver()\ntf.config.experimental_connect_to_cluster(tpu)\ntf.tpu.experimental.initialize_tpu_system(tpu)\nstrategy = tf.distribute.experimental.TPUStrategy(tpu)\nwarnings.filterwarnings(\"ignore\")","metadata":{"id":"3TLl23G4CwAD","outputId":"0ef3d876-3954-4c78-ad58-1aea07e15f5f","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we will load the images and split them to train and validation","metadata":{"id":"qPW-NaH9dYJu"}},{"cell_type":"code","source":"img_size = 224","metadata":{"id":"g7q61qYWNMKk","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_224X224_images = train_data[\"image\"].progress_apply(load_image, args=((img_size,img_size),))","metadata":{"id":"2yynxPAsIWHl","outputId":"153923d4-83ee-4953-9e19-6ea30df66bb2","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"current_train_data=train_data.loc[train_224X224_images.index]\nimages_array=images_4d_array(train_224X224_images)\ntargets=current_train_data.drop('image',axis=1)\nx_train2, x_val, y_train2, y_val = sklearn.model_selection.train_test_split(images_array, targets, test_size=0.2, random_state=seed)\nx_train2 = x_train2.astype('float32')\nx_val = x_val.astype('float32')\nx_train2 /= 255\nx_val /= 255\nx_train2.shape, x_val.shape, y_train2.shape, y_val.shape","metadata":{"id":"NJS6BqsHH8IO","outputId":"d2637eb3-5e7a-453b-90d1-eb213131ff20","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we will define the model and try to fit it","metadata":{"id":"3r9iRKv_eYg5"}},{"cell_type":"code","source":"with strategy.scope():\n    model = tf.keras.Sequential([tf.keras.applications.DenseNet121(input_shape=(224, 224, 3),\n                                             weights='imagenet',\n                                             include_top=False),\n                                 L.GlobalAveragePooling2D(),\n                                 L.Dense(y_train2.shape[1],\n                                         activation='softmax')])\n        \n    model.compile(optimizer='rmsprop',\n              loss='categorical_crossentropy',\n              metrics=['accuracy']\n              )\n    model.summary()","metadata":{"id":"7Hrh6aT319TT","outputId":"68f5b3b2-9b84-4561-f91e-bbcf78c0d9d0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit(x_train2,y_train2,\n                    epochs=250,\n                    callbacks=[ES_monitor,LR_reduce],\n                    steps_per_epoch=15,\n                    verbose=0,\n                    validation_data=(x_val, y_val))","metadata":{"id":"GOXKQzAzNenp","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lets print our results","metadata":{"id":"h2VxAQxEi7a_"}},{"cell_type":"code","source":"score = model.evaluate(x_val, y_val, verbose=0)\n\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])","metadata":{"id":"Wfd75UW5riNG","outputId":"ac4ca416-065f-45de-df40-36eec3ec8157","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_history(history)","metadata":{"id":"geDgZmfm_epj","outputId":"cbd23e13-72d6-4932-cd54-8142fb886985","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Our accuracy has stayed 92%, and the fitting was much faster, but there are still some problems, we can see at the end that there i a gap between the validation and the train matrices, which can indicate that there is some kind of overfittin, the accuracy of the train data is almos perferct, which can show again over fitting, but we can see that this is the case from almot the beggining which indicate that it could have been because of the pre trained model and it's proficiency  ","metadata":{"id":"9_pcU_W0nV8g"}},{"cell_type":"markdown","source":"Lets take a look at the confusin matrix for more insight","metadata":{"id":"JfMCWA_XpK7a"}},{"cell_type":"code","source":"y_val_pred = model.predict_classes(x_val)\nmat = sklearn.metrics.confusion_matrix(get_target_array_no_im_id(y_val),y_val_pred)\nsns.heatmap(mat.T, square=True, annot=True, cbar=False)\nmatplotlib.pyplot.xlabel('true label')\nmatplotlib.pyplot.ylabel('predict label');","metadata":{"id":"HQ6pPm351Z6S","outputId":"c55e6000-2c27-486c-eb9a-b32013e0a76e","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The classes are ordered in this manner: \"healthy\" \"multiple_diseases\" \"rust\" \"scab\". We can see that our model is pretty accurate at idintifing the \"healthy\" \"rust\" \"scab\" classes, as it classified corretly almost all of the images of those classes. Again the \"multiple_diseases\" class images' classification is not better then random. It could be that the underrepresentation of that class is a big factor of that and we haven't found a model that whould help us with that.","metadata":{"id":"ZIo97uNFpiVE"}},{"cell_type":"code","source":"model.save(\"..output/kaggle/working/model_densnet121.h5\")\n","metadata":{"id":"JeFehfilCKq8","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **DenseNet With SMOTE**","metadata":{"id":"ssH6oFZI-TB7"}},{"cell_type":"markdown","source":"Our last attempt with SMOTE left us with mixed feeling about it, but it did had an impact on the \"multiple_diseases\" class, was the wannted one. So we will try it once more but with DenseNet this time ","metadata":{"id":"-hDgcAvhrljq"}},{"cell_type":"markdown","source":"Applying SMOTE on our dataset","metadata":{"id":"VbSUXSQIuFC-"}},{"cell_type":"code","source":"sm = imblearn.over_sampling.SMOTE(random_state = 115) \n \nx_train2, y_train2 = sm.fit_resample(get_all_pixels(x_train2),y_train2.to_numpy())\nx_train2 = x_train2.reshape((-1, img_size, img_size, 3))\nx_train2.shape, y_train2.sum(axis=0)","metadata":{"id":"TH285jk5-R14","outputId":"7bba1af7-4cef-40fa-dde7-2ec9d7d45466","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we will define the model and try to fit it","metadata":{"id":"XRWmpHt6uZr2"}},{"cell_type":"code","source":"with strategy.scope():\n    model = tf.keras.Sequential([tf.keras.applications.DenseNet121(input_shape=(224, 224, 3),\n                                             weights='imagenet',\n                                             include_top=False),\n                                 L.GlobalAveragePooling2D(),\n                                 L.Dense(y_train2.shape[1],\n                                         activation='softmax')])\n        \n    model.compile(optimizer='rmsprop',\n              loss='categorical_crossentropy',\n              metrics=['accuracy']\n              )\n    model.summary()","metadata":{"id":"7txeetgR-8jq","outputId":"58143125-2962-4ee4-e4c8-4d95df829dd9","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit(x_train2,y_train2,\n                    epochs=250,\n                    callbacks=[ES_monitor,LR_reduce],\n                    steps_per_epoch=15,\n                    verbose=0,\n                    validation_data=(x_val, y_val))","metadata":{"id":"GQjE9E-z-8jr","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lets print our results","metadata":{"id":"hnsvZAaJudZY"}},{"cell_type":"code","source":"score = model.evaluate(x_val, y_val, verbose=0)\n\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])","metadata":{"id":"-AkzwsG--8jr","outputId":"a957576f-e36c-4d61-a474-073969ab8ad4","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_history(history)","metadata":{"id":"eU4SX-lQ-8jr","outputId":"023ae301-9eee-4e0b-d7ab-7a67b96ba094","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Our accuracy has stayed 92%, but there are the same problems, we can see at the end that there is a gap between the validation and the train matrices, which can indicate that there is some kind of overfittin, the accuracy of the train data is almos perferct, which can show again over fitting, but we can see that this is the case from almot the beggining which indicate that it could have been because of the pre trained model and it's proficiency","metadata":{"id":"pQdQCorBup5Z"}},{"cell_type":"markdown","source":"Lets take a look at the confusin matrix for more insight","metadata":{"id":"Svqf6A_lvFdG"}},{"cell_type":"code","source":"y_val_pred = model.predict_classes(x_val)\nmat = sklearn.metrics.confusion_matrix(get_target_array_no_im_id(y_val),y_val_pred)\nsns.heatmap(mat.T, square=True, annot=True, cbar=False)\nmatplotlib.pyplot.xlabel('true label')\nmatplotlib.pyplot.ylabel('predict label');","metadata":{"id":"p4hmWV-x-8jr","outputId":"ab3a2f34-d32c-4314-c412-c26d0fb704d3","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The classes are ordered in this manner: \"healthy\" \"multiple_diseases\" \"rust\" \"scab\". We can see that there were minimal change to the classification and that SMOTE did't help us classify the \"multiple_diseases\" better","metadata":{"id":"MMu43XoSvG04"}},{"cell_type":"code","source":"model.save(\"..output/kaggle/working/model_densnet121_SMOTE.h5\")","metadata":{"id":"vBuJ8_b9-8jr","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **DenseNet With Data Transformations and Augmentation**","metadata":{"id":"9P0xFybvyfcx"}},{"cell_type":"markdown","source":"As using DenseNet with SMOTE didn't produced a better model, we will use Data Transformations and Augmentation as well. Maybe that could produce a better model. We Didn't use it in at first place because using TPU and ImageDataGenerator is no possible, and using a GPU take considerebly longer.","metadata":{"id":"mHgmK_okzPXj"}},{"cell_type":"markdown","source":"Loading the images and spliting them to train and test","metadata":{"id":"fptobmiG0MDR"}},{"cell_type":"code","source":"img_size = 224","metadata":{"id":"IYYBUMjWzPro","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_224X224_images = train_data[\"image_id\"].progress_apply(load_image, args=((img_size,img_size),))","metadata":{"id":"Blnl9Eo1zPro","outputId":"db3ccbe7-4232-4916-8b89-b9d9470b3e6b","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"current_train_data=train_data.loc[train_224X224_images.index]\nimages_array=images_4d_array(train_224X224_images)\ntargets=current_train_data.drop('image_id',axis=1)\nx_train2, x_val, y_train2, y_val = sklearn.model_selection.train_test_split(images_array, targets, test_size=0.2, random_state=seed)\nx_train2 = x_train2.astype('float32')\nx_val = x_val.astype('float32')\nx_train2 /= 255\nx_val /= 255\nx_train2.shape, x_val.shape, y_train2.shape, y_val.shape","metadata":{"id":"QUn0fyBazPrp","outputId":"9f5cfdfa-0a40-407e-9da0-4d4648734ab9","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we will define the model and try to fit it","metadata":{"id":"hng6IfVKzPrq"}},{"cell_type":"code","source":"\nmodel = tf.keras.Sequential([tf.keras.applications.DenseNet121(input_shape=(224, 224, 3),\n                                          weights='imagenet',\n                                          include_top=False),\n                              L.GlobalAveragePooling2D(),\n                              L.Dense(y_train2.shape[1],\n                                      activation='softmax')])\n    \nmodel.compile(optimizer='rmsprop',\n          loss='categorical_crossentropy',\n          metrics=['accuracy']\n          )\nmodel.summary()","metadata":{"id":"MMnp0ix1zPrq","outputId":"59cb6b3c-b18f-43eb-bb27-df1e5bb5c57d","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit(datagen.flow(x_train2,y_train2,batch_size=24),\n                    epochs=250,\n                    callbacks=[ES_monitor,LR_reduce],\n                    steps_per_epoch=15,\n                    verbose=0,\n                    validation_data=(x_val, y_val))","metadata":{"id":"6qKdNefOzPrq","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lets print our results","metadata":{"id":"n2YxQvmu0Ygm"}},{"cell_type":"code","source":"score = model.evaluate(x_val, y_val, verbose=0)\n\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])","metadata":{"id":"AxJKfj-YzPrq","outputId":"ca44df2b-e2cb-4624-a089-5bb37837a753","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_history(history)","metadata":{"id":"6dKbl1i_zPrq","outputId":"4b838126-e051-496e-fbc2-b458570ef1b6","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Our accuracy has improved to 94%, the best accuracy thus far we can see that the vlidation and test graph have no gap which can show that the model didn't overpitted to the test. We can see that the model is having a better time classifing the \"multiple_diseases\" class then ever before from the better AUC-ROC score.","metadata":{"id":"xv-1_MVQ0vwc"}},{"cell_type":"markdown","source":"Lets take a look at the confusin matrix for more insight","metadata":{"id":"GmoV3E0T1j-9"}},{"cell_type":"code","source":"y_val_pred = model.predict_classes(x_val)\nmat = sklearn.metrics.confusion_matrix(get_target_array_no_im_id(y_val),y_val_pred)\nsns.heatmap(mat.T, square=True, annot=True, cbar=False)\nmatplotlib.pyplot.xlabel('true label')\nmatplotlib.pyplot.ylabel('predict label');","metadata":{"id":"NZl91ns-zPrq","outputId":"8ed202c2-caae-4e55-b15b-f77829da27f2","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The classes are ordered in this manner: \"healthy\" \"multiple_diseases\" \"rust\" \"scab\". We can see that our new model is better at idintifing the \"multiple_diseases\" class while classifing the other classes, then any other model we produced. It main problem is still this class but it does much better job at classifing it.","metadata":{"id":"JBDgKDnp2YWg"}},{"cell_type":"code","source":"model.save(\"..output/kaggle/working/model_densnet121_DG.h5\")\n","metadata":{"id":"fr7JFThKzPrr","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **DenseNet With Data Transformations and Augmentation and With  SMOTE**","metadata":{"id":"_dMd6Aq93Gpn"}},{"cell_type":"markdown","source":"Our last model did great, but still can have difficulties in classifing the \"multiple_diseases\". Even though SMOTE has faild us one, and had no effect the other time, e will try to use SMOTE to better our model, we don't have great hopes for it to work but maybe it wiil prove us wrong ","metadata":{"id":"q31pkMwS33Ai"}},{"cell_type":"markdown","source":"Again we will use the same procider as before but we will use SMOTE as well","metadata":{"id":"rUBRXCeK5O1-"}},{"cell_type":"markdown","source":"Applying SMOTE on our dataset","metadata":{"id":"Z3OMFYeh5eE_"}},{"cell_type":"code","source":"sm = imblearn.over_sampling.SMOTE(random_state = 115) \n \nx_train2, y_train2 = sm.fit_resample(get_all_pixels(x_train2),y_train2.to_numpy())\nx_train2 = x_train2.reshape((-1, img_size, img_size, 3))\nx_train2.shape, y_train2.sum(axis=0)","metadata":{"id":"W_-bbOWH3Gpo","outputId":"df67216e-1d15-486e-d414-ab95054cbc5d","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we will define the model and try to fit it","metadata":{"id":"O1BXmV0K5mF5"}},{"cell_type":"code","source":"\nmodel = tf.keras.Sequential([tf.keras.applications.DenseNet121(input_shape=(224, 224, 3),\n                                          weights='imagenet',\n                                          include_top=False),\n                              L.GlobalAveragePooling2D(),\n                              L.Dense(y_train2.shape[1],\n                                      activation='softmax')])\n    \nmodel.compile(optimizer='rmsprop',\n          loss='categorical_crossentropy',\n          metrics=['accuracy']\n          )\nmodel.summary()","metadata":{"id":"cdWgfxHw3Gpp","outputId":"07d64474-ebce-4327-882c-4ba1c9fc4f99","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit(datagen.flow(x_train2,y_train2,batch_size=24),\n                    epochs=250,\n                    callbacks=[ES_monitor,LR_reduce],\n                    steps_per_epoch=15,\n                    verbose=0,\n                    validation_data=(x_val, y_val))","metadata":{"id":"oldG8R0Q3Gpp","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lets print our results","metadata":{"id":"aRyBmIc55XLZ"}},{"cell_type":"code","source":"score = model.evaluate(x_val, y_val, verbose=0)\n\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])","metadata":{"id":"fIP7s1Ji3Gpp","outputId":"5d6a1226-ac24-41a0-e81b-31e2501c921d","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_history(history)","metadata":{"id":"tySfs7IV3Gpq","outputId":"3c2aa91b-8f31-4106-88e4-60b837aa1f8b","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"SMOTE didn't fail us and didn't help us. our accuracy has decreased to 88%, not the worst but a downgarde from our privious model. lets take a look at the confusin matrix for more insight about what went wrong","metadata":{"id":"s9IISxF25xQS"}},{"cell_type":"code","source":"y_val_pred = model.predict_classes(x_val)\nmat = sklearn.metrics.confusion_matrix(get_target_array_no_im_id(y_val),y_val_pred)\nsns.heatmap(mat.T, square=True, annot=True, cbar=False)\nmatplotlib.pyplot.xlabel('true label')\nmatplotlib.pyplot.ylabel('predict label');","metadata":{"id":"DPEMM-Ey3Gpq","outputId":"6470c374-4994-4420-ddf5-2abae728f9db","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The classes are ordered in this manner: \"healthy\" \"multiple_diseases\" \"rust\" \"scab\". We can see that our new model is worse then the one before at any classification. We can see that the SMOTE didnt had the effect that we wanted and it had all the negative effects from the first time we used it","metadata":{"id":"A95w83MO6qcW"}},{"cell_type":"code","source":"model.save(\"..output/kaggle/working/model_densnet121_SMOTE_DG.h5\")\n","metadata":{"id":"Zm2QjDxg3Gpq","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Discussion**","metadata":{"id":"fQOn6EM0wj1l"}},{"cell_type":"markdown","source":"Our goal in this project was to produce a model that can classify leaves images to 4 classes of health condition. The data set we had wes imbalance, one of the classes had drastically less images. Throughout the project that fact had great influence on our models and success. At first we explored our data and noticed that there is color difference between classes, and thought that it could heip us, but as the PCA showed us, the color of the image is greatly infuenced by the background and it has a greater effect then the symptoms of the diseases on the leaves. We tried to negate this  effect with 2 methods, cropping the leaves out ot the image and by blacking out the background, non of those methods were successful. Per image we can use the methods to great extent, but automatin those methods prove too sofisticated for us, because of the color of the background compare to the leaves, because of the curved shape of the leave which made their edges lose focus in some instances, which meant that their veins were more prominent as an edge.\n\nAs of next we tried some classic prediction models, which didn't go good at all, and gave prediction that are good as guessing.\n\nNext we tried the the most common method for classifing images, neural network. At first we tried a basic model that didn't performd well at all, next we continued to convolutional neural network, a great method for image classifing, that wes the first model that gave us good results. We tried at first to use it on images with the size of 100X100 pixels, it had good results but it misclassified some diseased infected leave as healthy, but more noticeably it didn't classified the \"multiple_diseases\" class well at all, due to it being underrepresented and having features of other classes. Using bigger images, 224X244 pixels to be specific, we managed to reduce it's misclassification of the diseased leaves as healthy, but didn't change the \"multiple_diseases\" problem, we saw as well some signs of overfiting to the train set. \n\nAs we saw the problems in our previous model, we use data augmentatinons, we changed images rotation, orientation, brightness and scale, and fed them to the CNN. This method improved our model and msde his be more presice when classifing diseased leaves, but once again we colud not classify the \"multiple_diseases\" class well.\n\nWe tried next a method that could maybe help us with this problem, we used SMOTE, it's a method that balances our data by generating samples to the minor class, but as it turn out this method just made the model worse. The problem was that our \"multiple_diseases\" class has similar featurs as the other two diseases calsses, and by generating the samples it made those features ones that assosiate greatly with the \"multiple_diseases\" class, and not the other diseased classes.\n\nThus far our best attempt ,data augmentation, was pretty good but had some difficulties and could not tackle the main issue of the data set, it's imbalance. \n\nNext we tried to incorporate transfer learning to our attempts. This methos is using an allready traind model on general case as our base model, and in addition we used a different neural network, denstenet which has different kind of architecture the our CNN. We tried using it with or without SMOTE and with or without data augmentation. The best model was produced as a result of the the one with data augmentation and without SMOTE. It can tackle the \"multiple_diseases\" class much better but still has a bit of dificult with this class, at the rest it does great work.\n\n\nOur data main challenges were it's imbalance and the fact that the leaves were filmed on a dinamic background. Further more, another challenge is the resemblance of some of its classes features. All of the above made our work interesting and challenging. We could not find an efficient way to reduce the impact that the back ground has, and could not find a way that could heip us with the imbalances without reducing model accurecy. The first problem could be solved by creating a model that can by himself black out the background, but that problem is another big one that would required a project by itself. The secound problem could be tackled by gathering more samples of this class, a mission for plant disease expert that can identify the correct disease. \n\nOverall we produced a competant model that can in great success predict the disease an apple leaf has.\n\n","metadata":{"id":"rO18hcJyxbvn"}},{"cell_type":"markdown","source":"# **Conclusion**","metadata":{"id":"s2H_Fh9No9og"}},{"cell_type":"markdown","source":"This project was a great challenge but a great fun to work on. We've learned a lot and were able to dive into the world of data science. We learned that some task that look trivial like identifing a leaf and differentiating it from the background can be convoluted if the background has certain features. We also learnd that in the case that we have two classes with shared features using sample genaration would probebly wont work. We saw first-handed how promenent of a  solution neural networks for classifing images, especially over classic methods. We found out that managing resources is a big part of a data science project and can have big impect on our time and effort.","metadata":{"id":"Q_YMnbXaphqA"}}]}