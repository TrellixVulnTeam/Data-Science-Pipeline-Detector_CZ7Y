{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport numpy as np\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('../input/plant-pathology-2021-fgvc8/train.csv')\n#apply label encoding to labels\ndf.labels = pd.factorize(df.labels)[0]\nfilepath = df['image'].values\nlabels = df['labels'].values\ndata_len = len(labels)\nprint(data_len)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#split data to train and test\ntrain_size = int(0.8*data_len)\nprint(\"train size: \",train_size )\nprint(\"test size: \",val_size )\ntrain_label, test_label = labels[:train_size], labels[train_size:]\ntrain_filepath, test_filepath = filepath[:train_size], filepath[train_size:]\nprint(train_label.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#load train images\n\ntrain_image = []\ndir1 = '../input/plant-pathology-2021-fgvc8/train_images/'\ni = 0\nfor file in train_filepath:\n    i+=1\n    if i % 500 == 0:\n        print(\"Train image#\",i)\n    img_clr = tf.io.read_file(dir1+file)\n    img1 = tf.image.decode_image(img_clr,channels=1,dtype=tf.float32)\n    # Resize images to a common size\n    img = tf.image.resize(img1, size=(256,256))\n    img = np.asarray(img).reshape(-1)\n    #img = np.asarray(img).reshape([-1, 256*256])\n    # Normalize\n    img = img /255.\n    #print(img)\n    train_image.append(img)\n    \nprint(\"shape of train_image\",np.asarray(train_image).shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Training parameters.\nlearning_rate = 0.01\ntraining_steps = 5000\nbatch_size = 512\ndisplay_step = 50","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Use tf.data API to shuffle and batch TRAIN data.\ntrain_data = tf.data.Dataset.from_tensor_slices((train_image,train_label))\ntrain_data = train_data.repeat().shuffle(5000).batch(batch_size).prefetch(1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Training parameters.\nlearning_rate = 0.0001\ntraining_steps = 1000\n\n# dataset parameters.\nnum_classes = 12 # 0 to 3 classes\nnum_features = 256*256 # 28*28\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# apply LOGISTIC REGRESSION\n\n# Weight of shape [784, 10], the 28*28 image features, and a total number of classes.\nW = tf.Variable(tf.ones([num_features, num_classes]), name=\"weight\")\n\n# Bias of shape [10], the total number of classes.\nb = tf.Variable(tf.zeros([num_classes]), name=\"bias\")\n\n# Logistic regression (Wx + b).\ndef logistic_regression(x):\n\n    # Apply softmax to normalize the logits to a probability distribution.\n    return tf.nn.softmax(tf.matmul(x, W) + b)\n\n# Cross-Entropy loss function.\ndef cross_entropy(y_pred, y_true):\n\n    # Encode label to a one hot vector.\n    y_true = tf.one_hot(y_true, depth=num_classes)\n\n    # Clip prediction values to avoid log(0) error.\n    y_pred = tf.clip_by_value(y_pred, 1e-9, 1.)\n\n    # Compute cross-entropy.\n    return tf.reduce_mean(-tf.reduce_sum(y_true * tf.math.log(y_pred)))\n\n# Accuracy metric.\n\ndef accuracy(y_pred, y_true):\n\n    # Predicted class is the index of the highest score in prediction vector (i.e. argmax).\n    correct_prediction = tf.equal(tf.argmax(y_pred, 1), tf.cast(y_true, tf.int64))\n    return tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n\n# Stochastic gradient descent optimizer.\noptimizer = tf.optimizers.SGD(learning_rate)\n\n# Optimization process. \ndef run_optimization(x, y):\n\n# Wrap computation inside a GradientTape for automatic differentiation.\n    with tf.GradientTape() as g:\n        pred = logistic_regression(x)\n        loss = cross_entropy(pred, y)\n    # Compute gradients.\n    gradients = g.gradient(loss, [W, b])\n    # Update W and b following gradients.\n    optimizer.apply_gradients(zip(gradients, [W, b]))\n    \n# Run training for the given number of steps.\nfor epoch,(batch_x, batch_y) in enumerate(train_data.take(training_steps), 1):\n    # Run the optimization to update W and b values.\n    run_optimization(batch_x, batch_y)\n    if epoch % display_step == 0:\n        pred = logistic_regression(batch_x)\n        loss = cross_entropy(pred, batch_y)\n        acc = accuracy(pred, batch_y)\n        print(\"epoch: %i, loss: %f, accuracy: %f\" % (epoch, loss, acc))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#load test images\n\ntest_image=[]\ndir1 = '../input/plant-pathology-2021-fgvc8/train_images/'\ni = 0\nfor file in test_filepath:\n    i+=1\n    if i % 500 == 0:\n        print(\"Test image#\",i)\n    img_clr = tf.io.read_file(dir1+file)\n    img1 = tf.image.decode_image(img_clr,channels=1,dtype=tf.float32)\n    # Resize images to a common size\n    img = tf.image.resize(img1, size=(256,256))\n    img = np.asarray(img).reshape(-1)\n    #img = np.asarray(img).reshape([-1, 256*256])\n    # Normalize\n    img = img /255.\n    #print(img)\n    test_image.append(img)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Test model on validation set.\n\npred = logistic_regression(test_image)\nprint(\"Test Accuracy: %f\" % accuracy(pred, test_label))","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}