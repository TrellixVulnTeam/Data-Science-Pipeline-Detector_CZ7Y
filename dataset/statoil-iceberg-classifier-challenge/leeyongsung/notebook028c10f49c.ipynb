{"cells":[{"metadata":{"_cell_guid":"ea3f4874-a9aa-42f1-9605-b1784a6f48ba","_uuid":"58c82d3b3c4b4305b388a6ac4eeca49d600f9105","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom os.path import join as opj\nfrom matplotlib import pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nimport pylab\nplt.rcParams['figure.figsize'] = 10, 10\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install py7zr\nimport py7zr\nimport os\n\nif not os.path.exists('/kaggle/train/') :\n    os.makedirs('/kaggle/train/')\n\n\nif not os.path.exists('/kaggle/test/') :\n    os.makedirs('/kaggle/test/')\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with py7zr.SevenZipFile(\"/kaggle/input/statoil-iceberg-classifier-challenge/train.json.7z\", 'r') as archive:\n    archive.extractall(path=\"/kaggle/train\")\n\nwith py7zr.SevenZipFile(\"/kaggle/input/statoil-iceberg-classifier-challenge/test.json.7z\", 'r') as archive:\n    archive.extractall(path=\"/kaggle/test\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for dirname, _, filenames in os.walk('/kaggle'): \n    for filename in filenames: \n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = pd.read_json('/kaggle/train/data/processed/train.json')\ndf_test = pd.read_json('/kaggle/test/data/processed/test.json')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom keras.models import Sequential, Model\nfrom keras.layers import Dense, Dropout, Flatten, Input\nfrom keras.layers import Conv2D, MaxPooling2D\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.optimizers import Adam\nimport cv2\nimport keras\n\nnp.random.seed(1234)\n\ndef get_scaled_imgs(df):\n    imgs = []\n\n    for i, row in df.iterrows():\n        #make 75x75 image\n        band_1 = np.array(row['band_1']).reshape(75, 75)\n        band_2 = np.array(row['band_2']).reshape(75, 75)\n        band_3 = band_1 + band_2 # plus since log(x*y) = log(x) + log(y)\n\n        # Rescale\n        a = (band_1 - band_1.mean()) / (band_1.max() - band_1.min())\n        b = (band_2 - band_2.mean()) / (band_2.max() - band_2.min())\n        c = (band_3 - band_3.mean()) / (band_3.max() - band_3.min())\n\n        imgs.append(np.dstack((a, b, c)))\n\n    return np.array(imgs)\ndef get_more_images(imgs):\n    \n    more_images = []\n    vert_flip_imgs = []\n    hori_flip_imgs = []\n      \n    for i in range(0,imgs.shape[0]):\n        a=imgs[i,:,:,0]\n        b=imgs[i,:,:,1]\n        c=imgs[i,:,:,2]\n        \n        av=cv2.flip(a,1)\n        ah=cv2.flip(a,0)\n        bv=cv2.flip(b,1)\n        bh=cv2.flip(b,0)\n        cv=cv2.flip(c,1)\n        ch=cv2.flip(c,0)\n        \n        vert_flip_imgs.append(np.dstack((av, bv, cv)))\n        hori_flip_imgs.append(np.dstack((ah, bh, ch)))\n      \n    v = np.array(vert_flip_imgs)\n    h = np.array(hori_flip_imgs)\n       \n    more_images = np.concatenate((imgs,v,h))\n    \n    return more_images","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Xtrain = get_scaled_imgs(df_train)\nYtrain = np.array(df_train['is_iceberg'])\n\ndf_train.inc_angle = df_train.inc_angle.replace('na',0)\nidx_tr = np.where(df_train.inc_angle>0)\n\nYtrain = Ytrain[idx_tr[0]]\nXtrain = Xtrain[idx_tr[0],...]\nXinc = df_train.inc_angle[idx_tr[0]]\n\nXtrain = get_more_images(Xtrain)\nXinc = np.concatenate((Xinc,Xinc,Xinc))\nYtrain = np.concatenate((Ytrain,Ytrain,Ytrain))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import sys\nnp.set_printoptions(threshold=sys.maxsize)\nprint(Xinc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.inc_angle = df_test.inc_angle.replace('na',0)\nXtest = (get_scaled_imgs(df_test))\nXinc = df_test.inc_angle\npred_test = model.predict([Xtest,Xinc])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame({'id': df_test[\"id\"], 'is_iceberg': np.round(pred_test).astype('int16')[0,:, 0]})\nprint(submission.head(10))\n\nsubmission.to_csv('cnn1.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_test = np.array(pred_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_test.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Model, Input\nfrom keras.layers import Conv2D, MaxPooling2D, Dropout, GlobalAveragePooling2D, Dense, AveragePooling2D, Flatten, Concatenate\nfrom keras.optimizers import SGD\nfrom keras.callbacks import Callback\n\nfrom keras.utils import to_categorical\nfrom keras.datasets import cifar10\nimport numpy as np\n\nfrom keras.layers import Layer\nfrom keras import backend as K\nif K.backend() == 'theano':\n    import theano.tensor as T\nelif K.backend() == 'tensorflow':\n    import tensorflow as tf\nelse:\n    raise NotImplementedError\nclass LearningRateSchedule(Callback):\n    def on_epoch_end(self, epoch, logs=None):\n        if (epoch+1)%8 == 0:\n            lr = K.get_value(self.model.optimizer.lr)\n            K.set_value(self.model.optimizer.lr, lr*0.96)\n\nclass LocalResponseNormalization(Layer):\n    def __init__(self, n=5, alpha=1e-4, beta=0.75, k=2, **kwargs):\n        self.n = n\n        self.alpha = alpha\n        self.beta = beta\n        self.k = k\n        super(LocalResponseNormalization, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        self.shape = input_shape\n        super(LocalResponseNormalization, self).build(input_shape)\n\n    def call(self, x):\n        _, r, c, f = self.shape \n        squared = K.square(x)\n        pooled = K.pool2d(squared, (self.n, self.n), strides=(1,1), padding=\"same\", pool_mode='avg')\n        summed = K.sum(pooled, axis=3, keepdims=True)\n        averaged = self.alpha * K.repeat_elements(summed, f, axis=3)\n            \n        denom = K.pow(self.k + averaged, self.beta)\n        \n        return x / denom \n    \n    def get_output_shape_for(self, input_shape):\n        return input_shape\n\ndef Upscaling_Data(data_list, reshape_dim):\n    ...\n\ndef inception(input_tensor, filter_channels):\n    filter_1x1, filter_3x3_R, filter_3x3, filter_5x5_R, filter_5x5, pool_proj = filter_channels\n    \n    branch_1 = Conv2D(filter_1x1, (1, 1), strides=1, padding='same', activation='relu', kernel_initializer='he_normal')(input_tensor)\n    \n    branch_2 = Conv2D(filter_3x3_R, (1, 1), strides=1, padding='same', activation='relu', kernel_initializer='he_normal')(input_tensor)\n    branch_2 = Conv2D(filter_3x3, (3, 3), strides=1, padding='same', activation='relu', kernel_initializer='he_normal')(branch_2)\n\n    branch_3 = Conv2D(filter_5x5_R, (1, 1), strides=1, padding='same', activation='relu', kernel_initializer='he_normal')(input_tensor)\n    branch_3 = Conv2D(filter_5x5, (5, 5), strides=1, padding='same', activation='relu', kernel_initializer='he_normal')(branch_3)\n    \n    branch_4 = MaxPooling2D((3, 3), strides=1, padding='same')(input_tensor)\n    branch_4 = Conv2D(pool_proj, (1, 1), strides=1, padding='same', activation='relu', kernel_initializer='he_normal')(branch_4)\n    \n    DepthConcat = Concatenate()([branch_1, branch_2, branch_3, branch_4])\n    \n    return DepthConcat\n    \ndef GoogLeNet(model_input, classes=1):\n    conv_1 = Conv2D(64, (7, 7), strides=2, padding='same', activation='relu')(model_input) # (112, 112, 64)\n    pool_1 = MaxPooling2D((3, 3), strides=2, padding='same')(conv_1) # (56, 56, 64)\n    LRN_1 = LocalResponseNormalization()(pool_1) # (56, 56, 64)\n    \n    conv_2 = Conv2D(64, (1, 1), strides=1, padding='valid', activation='relu')(LRN_1) # (56, 56, 64)\n    conv_3 = Conv2D(192, (3, 3), strides=1, padding='same', activation='relu')(conv_2) # (56, 56, 192)\n    LRN_2 = LocalResponseNormalization()(conv_3) # (56, 56, 192)\n    pool_2 = MaxPooling2D((3, 3), strides=2, padding='same')(LRN_2) # (28, 28, 192)\n    \n    inception_3a = inception(pool_2, [64, 96, 128, 16, 32, 32]) # (28, 28, 256)\n    inception_3b = inception(inception_3a, [128, 128, 192, 32, 96, 64]) # (28, 28, 480)\n    \n    pool_3 = MaxPooling2D((3, 3), strides=2, padding='same')(inception_3b) # (14, 14, 480)\n    \n    inception_4a = inception(pool_3, [192, 96, 208, 16, 48, 64]) # (14, 14, 512)\n    inception_4b = inception(inception_4a, [160, 112, 224, 24, 64, 64]) # (14, 14, 512)\n    inception_4c = inception(inception_4b, [128, 128, 256, 24, 64, 64]) # (14, 14, 512)\n    inception_4d = inception(inception_4c, [112, 144, 288, 32, 64, 64]) # (14, 14, 528)\n    inception_4e = inception(inception_4d, [256, 160, 320, 32, 128, 128]) # (14, 14, 832)\n    \n    pool_4 = MaxPooling2D((3, 3), strides=2, padding='same')(inception_4e) # (7, 7, 832)\n    \n    inception_5a = inception(pool_4, [256, 160, 320, 32, 128, 128]) # (7, 7, 832)\n    inception_5b = inception(inception_5a, [384, 192, 384, 48, 128, 128]) # (7, 7, 1024)\n    \n    avg_pool = GlobalAveragePooling2D()(inception_5b)\n    dropout = Dropout(0.4)(avg_pool)\n    \n    linear = Dense(1, activation='relu')(dropout)\n    \n    model_output = Dense(classes, activation='sigmoid', name='main_classifier')(linear) # 'softmax'\n    \n    # Auxiliary Classifier\n    auxiliary_4a = AveragePooling2D((5, 5), strides=3, padding='valid')(inception_4a)\n    auxiliary_4a = Conv2D(128, (1, 1), strides=1, padding='same', activation='relu')(auxiliary_4a)\n    auxiliary_4a = Flatten()(auxiliary_4a)\n    auxiliary_4a = Dense(1024, activation='relu')(auxiliary_4a)\n    auxiliary_4a = Dropout(0.7)(auxiliary_4a)\n    auxiliary_4a = Dense(classes, activation='sigmoid', name='auxiliary_4a')(auxiliary_4a)\n    \n    auxiliary_4d = AveragePooling2D((5, 5), strides=3, padding='valid')(inception_4d)\n    auxiliary_4d = Conv2D(128, (1, 1), strides=1, padding='same', activation='relu')(auxiliary_4d)\n    auxiliary_4d = Flatten()(auxiliary_4d)\n    auxiliary_4d = Dense(1024, activation='relu')(auxiliary_4d)\n    auxiliary_4d = Dropout(0.7)(auxiliary_4d)\n    auxiliary_4d = Dense(classes, activation='sigmoid', name='auxiliary_4d')(auxiliary_4d)\n    \n    \n    model = Model(model_input, [model_output, auxiliary_4a, auxiliary_4d])\n    \n    return model\n\n\ninput_shape = (75, 75, 3)\n\nmodel_input = Input( shape=input_shape )\n\nmodel = GoogLeNet(model_input, 1)\n\noptimizer = SGD(momentum=0.9)\n\nmodel.compile(optimizer, \n        \tloss={'main_classifier' : 'binary_crossentropy',\n                    'auxiliary_4a' : 'binary_crossentropy',\n                        'auxiliary_4d' : 'binary_crossentropy'},\n                loss_weights={'main_classifier' : 1.0,\n                                'auxiliary_4a' : 0.3, \n                                'auxiliary_4d' : 0.3}, \n                metrics=['acc'])\n\nmodel.fit([Xtrain,Xinc],{'main_classifier' : Ytrain, \n                'auxiliary_4a' : Ytrain, \n                'auxiliary_4d' : Ytrain},  \n        epochs=30, batch_size=24,verbose=1, callbacks=LearningRateSchedule())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}