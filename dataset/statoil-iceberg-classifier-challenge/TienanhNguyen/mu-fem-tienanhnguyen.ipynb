{"metadata":{"language_info":{"nbconvert_exporter":"python","version":"3.6.3","mimetype":"text/x-python","file_extension":".py","codemirror_mode":{"version":3,"name":"ipython"},"pygments_lexer":"ipython3","name":"python"},"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"}},"nbformat":4,"cells":[{"metadata":{"_cell_guid":"f8206687-5637-4a63-bd90-199cfd4db707","_uuid":"0ce971937ef9a2dd65023f09eee9fc6635160f34"},"cell_type":"markdown","source":"# Statoil/C-CORE Iceberg Classifier Challenge\n\n## Summary\n\nThis kernel explain the best approach that the result in the highest accuracy I had made in this competition. It will also discuss myriad of more complicated approach which didn't work as well as this simple approach.\n\n## Reading in the data"},{"metadata":{"_cell_guid":"12d9219e-1183-4362-b32e-0b95467ad117","_uuid":"76bd850ca1ab07ca2d5906a6f2d3a2b1aeca517f"},"outputs":[],"execution_count":null,"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\ntrain_df = pd.read_json(\"../input/train.json\")\ntest_df = pd.read_json(\"../input/test.json\")"},{"metadata":{"_cell_guid":"ffb91227-c385-4c42-aa60-acd277b4fd36","_uuid":"b348235c2cd66a45b4d2c281bab8c46b30369016"},"cell_type":"markdown","source":"## Preprocessing the data\n\nHere I take in the data and separate them into 2 bands. This is a very simple and typical ways to precess the data. \n\nI have also try diffrent ways to preprocess it like fliping the image, separate them into 3 bands, ... But it only seem to make the accuracy decrease."},{"metadata":{"_cell_guid":"cef968a2-2830-4bbc-b48d-0730a832599a","_uuid":"7681b4cda702228fcdac2edc576e05f7df8a9887"},"outputs":[],"execution_count":null,"cell_type":"code","source":"# Train data\nx_band1 = np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in train_df[\"band_1\"]])\nx_band2 = np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in train_df[\"band_2\"]])\nX_train = np.concatenate([x_band1[:, :, :, np.newaxis], x_band2[:, :, :, np.newaxis]], axis=-1)\ny_train = np.array(train_df[\"is_iceberg\"])\nprint(\"Xtrain:\", X_train.shape)\n\n# Test data\nx_band1 = np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in test_df[\"band_1\"]])\nx_band2 = np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in test_df[\"band_2\"]])\nX_test = np.concatenate([x_band1[:, :, :, np.newaxis], x_band2[:, :, :, np.newaxis]], axis=-1)\nprint(\"Xtest:\", X_test.shape)"},{"metadata":{"_cell_guid":"6acbc73e-ffb9-4c7d-bf9d-55c7153f8c71","_uuid":"7a7e3ddfce76dab4b5f0a1d8e1baec0bc9d881f4"},"outputs":[],"execution_count":null,"cell_type":"code","source":"from matplotlib import pyplot\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Input, Flatten, Activation\nfrom keras.layers import GlobalMaxPooling2D\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.layers.merge import Concatenate\nfrom keras.models import Model\nfrom keras import initializers\nfrom keras.optimizers import Adam\nfrom keras.callbacks import ModelCheckpoint, Callback, EarlyStopping\nfrom sklearn.model_selection import train_test_split\nimport os"},{"metadata":{"_cell_guid":"ecba9e01-42d8-4fae-94a1-5d0cc00a8d53","_uuid":"046f0b08b2698b8dd56caa8701c4b0efa337bed3"},"cell_type":"markdown","source":"## Building the model\n\nHere is the CNN models which I used to train my data. \n\nI have of course try different parameter and layers for the model, but this seem to be working the best. Also this is the particular model which most other kernels used in terms of number of layers."},{"metadata":{"_cell_guid":"70c9c207-2d2a-4fa0-8b73-d55191957454","_uuid":"5151cbbf3d81a1cbcbbce6137d5c55d3a282989c","collapsed":true},"outputs":[],"execution_count":null,"cell_type":"code","source":"def getModel():\n    #Building the model\n    gmodel=Sequential()\n    #Conv Layer 1\n    gmodel.add(Conv2D(64, kernel_size=(3, 3),activation='relu', input_shape=(75, 75, 2)))\n    gmodel.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))\n    gmodel.add(Dropout(0.2))\n\n    #Conv Layer 2\n    gmodel.add(Conv2D(64, kernel_size=(3, 3), activation='relu' ))\n    gmodel.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n    gmodel.add(Dropout(0.2))\n\n    #Conv Layer 3\n    gmodel.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n    gmodel.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n    gmodel.add(Dropout(0.2))\n\n    #Conv Layer 4\n    gmodel.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n    gmodel.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n    gmodel.add(Dropout(0.2))\n\n    #Flatten the data for upcoming dense layers\n    gmodel.add(Flatten())\n\n    #Dense Layers\n    gmodel.add(Dense(512))\n    gmodel.add(Activation('relu'))\n    gmodel.add(Dropout(0.2))\n\n    #Dense Layer 2\n    gmodel.add(Dense(256))\n    gmodel.add(Activation('relu'))\n    gmodel.add(Dropout(0.2))\n\n    #Sigmoid Layer\n    gmodel.add(Dense(1))\n    gmodel.add(Activation('sigmoid'))\n\n    mypotim=Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n    gmodel.compile(loss='binary_crossentropy',\n                  optimizer=mypotim,\n                  metrics=['accuracy'])\n    gmodel.summary()\n    return gmodel\ndef get_callbacks(filepath, patience=2):\n    es = EarlyStopping('val_loss', patience=patience, mode=\"min\")\n    msave = ModelCheckpoint(filepath, save_best_only=True)\n    return [es, msave]\nfile_path = \".model_weights.hdf5\"\ncallbacks = get_callbacks(filepath=file_path, patience=5)"},{"metadata":{"_cell_guid":"593adccb-7a1b-4977-9175-ef25dd551a84","_uuid":"2d3cd653c5bb31ccb234293b5ec071f89287f59f"},"outputs":[],"execution_count":null,"cell_type":"code","source":"X_train_cv, X_valid, y_train_cv, y_valid = train_test_split(X_train, y_train, random_state=1, train_size=0.75)"},{"metadata":{"_cell_guid":"a8e46a34-ec4d-43ff-a76c-8998ba347394","_uuid":"3157510fc50cc31b8c048620f81987aaeaeeb891"},"outputs":[],"execution_count":null,"cell_type":"code","source":"model=getModel()\nmodel.fit(X_train_cv, y_train_cv,\n          batch_size=24,\n          epochs=50,\n          verbose=1,\n          validation_data=(X_valid, y_valid))"},{"metadata":{"_cell_guid":"63b6113a-ce7d-43b7-830c-ab1252f062e2","_kg_hide-output":false,"_uuid":"453ebf10533bb17fd39334f8afedb1cc2bbdae5c","collapsed":true,"scrolled":true},"outputs":[],"execution_count":null,"cell_type":"code","source":"model.load_weights(filepath=file_path)\npredicted_test=model.predict_proba(X_test)"},{"metadata":{"_cell_guid":"5cd81f77-930f-444e-967c-5d9681719dc8","_uuid":"2944092a73d076526b8b8e95a44a1e1b3021e213"},"cell_type":"markdown","source":"## Conclusion\n\nThrough this competition, I was able to learn many to preprocess an image input. Unfortunenately, the one turn outs to work out the best was the most simple one. I think what I could do more is try more different technique like using more than 1 model to predict and stacking. "},{"metadata":{"_cell_guid":"109e09a2-dd21-4e5d-be37-3651b546ef30","_uuid":"d66d86c1741afcbff9dac4fc2f0fbdbe3987d2f9","collapsed":true},"outputs":[],"execution_count":null,"cell_type":"code","source":"submit_df = pd.DataFrame({'id': test_df[\"id\"], 'is_iceberg': predicted_test.flatten()})\nsubmit_df.to_csv(\"./naive_submission.csv\", index=False)"}],"nbformat_minor":1}