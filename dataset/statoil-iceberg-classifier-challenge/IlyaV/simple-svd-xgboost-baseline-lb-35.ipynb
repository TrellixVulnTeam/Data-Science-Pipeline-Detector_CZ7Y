{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"version":3,"name":"ipython"},"version":"3.6.3","mimetype":"text/x-python","name":"python","file_extension":".py","pygments_lexer":"ipython3","nbconvert_exporter":"python"}},"nbformat":4,"nbformat_minor":1,"cells":[{"metadata":{},"source":"We aim to use svd to dimensionally reduce the images to just a few features (20 per image in this particular case), which works nicely as it turns out most of variance in the image is just noise. LB should be aroun ~0.35, but I've played a bit with xgb parameters and the crossvalidation improved, this suggest that perhaps you can get lower LB if you submit this notebook. ","cell_type":"markdown"},{"metadata":{"_uuid":"e6b43e7b7d786bc565deb7c52acd78d2885d9837","_cell_guid":"08e02f57-9c97-487a-ab0d-b2afad161d53","collapsed":true},"execution_count":null,"outputs":[],"source":"#load with pandas, manipulate with numpy, plot with matplotlib\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\n\n#ML - we will classify using a naive xgb with stratified cross validation\nimport xgboost as xgb\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import log_loss\n\n\n\n","cell_type":"code"},{"metadata":{"_uuid":"027ce3e753ae41fbe6600369f6dd38047e1fdd59","_cell_guid":"232aecf4-a711-415c-8b23-d277824788c3","collapsed":true},"execution_count":null,"outputs":[],"source":"#filenames\ninputFolder = \"../input/\"\ntrainSet = 'train.json'\ntestSet = 'test.json'\nsubName = 'iceberg-svd-xgb-3fold.csv'\n","cell_type":"code"},{"metadata":{"_uuid":"f52fbe147c7723767b3f03200c9746ba5abdaae5","_cell_guid":"2d055ac8-6c5d-4bd3-8f93-c981379fe57a","collapsed":true},"execution_count":null,"outputs":[],"source":"#load data\ntrainDF = pd.read_json(inputFolder+trainSet)\ntestDF = pd.read_json(inputFolder+testSet)","cell_type":"code"},{"metadata":{"_uuid":"7f08caa08f99b025488c086b4a610333e306d087","_cell_guid":"a2ff70e4-e630-419b-b6b3-9a8be46a7014","collapsed":true},"execution_count":null,"outputs":[],"source":"#get numpy arrays for train/test data, prob there is a more pythonic approach\nband1 = trainDF['band_1'].values\nim1 = np.zeros((len(band1),len(band1[0])))\nfor j in range(len(band1)):\n    im1[j,:]=np.asarray(band1[j])\n    \nband2 = trainDF['band_2'].values\nim2 = np.zeros((len(band2),len(band2[0])))\nfor j in range(len(band2)):\n    im2[j,:]=np.asarray(band2[j])\n    \n#get numpy array for test data\nband1test = testDF['band_1'].values\nim1test = np.zeros((len(band1test),len(band1test[0])))\nfor j in range(len(band1test)):\n    im1test[j,:]=np.asarray(band1test[j])\n    \nband2test = testDF['band_2'].values\nim2test = np.zeros((len(band2test),len(band2test[0])))\nfor j in range(len(band2test)):\n    im2test[j,:]=np.asarray(band2test[j])","cell_type":"code"},{"metadata":{"_uuid":"78e22d6cc95e32ddbcbbf4d07dc7138b7b81e978","_cell_guid":"eec13fca-3462-46e8-8f1e-c462f1c90d99","collapsed":true},"execution_count":null,"outputs":[],"source":"#svd of the two bands\nU1,s1,V1 = np.linalg.svd(im1,full_matrices = 0)\nU2,s2,V2 = np.linalg.svd(im2,full_matrices = 0)","cell_type":"code"},{"metadata":{"_uuid":"785d0669746961b99c6eb84a50d57b0965dbd328","_cell_guid":"36b32c28-2701-4840-96e7-0f3b0ed1f2e0"},"execution_count":null,"outputs":[],"source":"#fraction of variance explained in the first 100 modes of train data.\n#note band 2 is somehow much more dependent on the first svd mode than band 1\n\nplt.figure()\n\nfrac1 = np.cumsum(s1)/np.sum(s1)\nfrac2 = np.cumsum(s2)/np.sum(s2)\n\nplt.plot(frac1[:100])\nplt.plot(frac2[:100],'r')\n\n","cell_type":"code"},{"metadata":{"_uuid":"e8e87c37e83b9be919a0f98a8ccda59495466ee8","_cell_guid":"28f0bea0-ad1e-4dac-be86-f76096aeab2f"},"execution_count":null,"outputs":[],"source":"#original \n\nfig, ax = plt.subplots(2,3)\nplt.suptitle('original')\nax[0,0].imshow(np.reshape(im2[0,:],(75,75)))\n\nax[0,1].imshow(np.reshape(im2[1,:],(75,75)))\nax[0,2].imshow(np.reshape(im2[2,:],(75,75)))\nax[1,0].imshow(np.reshape(im2[3,:],(75,75)))\nax[1,1].imshow(np.reshape(im2[4,:],(75,75)))\nax[1,2].imshow(np.reshape(im2[5,:],(75,75)))\n\n#first 100 modes (only 1/16th total modes, ~35% of variance)\n\nnmodes = 100\n\nim1p=np.dot(np.dot(U1[:,:nmodes],np.diag(s1[:nmodes])),V1[:nmodes,])\nim2p=np.dot(np.dot(U2[:,:nmodes],np.diag(s2[:nmodes])),V2[:nmodes,])\n\nfig, ax = plt.subplots(2,3)\nplt.suptitle('first 100 modes')\nax[0,0].imshow(np.reshape(im2p[0,:],(75,75)))\n\nax[0,1].imshow(np.reshape(im2p[1,:],(75,75)))\nax[0,2].imshow(np.reshape(im2p[2,:],(75,75)))\nax[1,0].imshow(np.reshape(im2p[3,:],(75,75)))\nax[1,1].imshow(np.reshape(im2p[4,:],(75,75)))\nax[1,2].imshow(np.reshape(im2p[5,:],(75,75)))\n\n#first 20 modes (~27% of variance explained)\n\nnmodes = 20\n\nim1p=np.dot(np.dot(U1[:,:nmodes],np.diag(s1[:nmodes])),V1[:nmodes,])\nim2p=np.dot(np.dot(U2[:,:nmodes],np.diag(s2[:nmodes])),V2[:nmodes,])\n\nfig, ax = plt.subplots(2,3)\n\nplt.suptitle('first 20 modes')\nax[0,0].imshow(np.reshape(im2p[0,:],(75,75)))\n\nax[0,1].imshow(np.reshape(im2p[1,:],(75,75)))\nax[0,2].imshow(np.reshape(im2p[2,:],(75,75)))\nax[1,0].imshow(np.reshape(im2p[3,:],(75,75)))\nax[1,1].imshow(np.reshape(im2p[4,:],(75,75)))\nax[1,2].imshow(np.reshape(im2p[5,:],(75,75)))","cell_type":"code"},{"metadata":{"_uuid":"8f769429220e7b3b4cb252da26dc75d308bf1404","_cell_guid":"8907b35b-6d4a-4b52-9c64-7ef47f604809","collapsed":true},"execution_count":null,"outputs":[],"source":"# OK, so first 20 modes (20 numbers per image) have most of useful information, \n# as most of variance is just noise. Let's run a simple xgboost classifier\n\n#transofrm test data\nU1test=np.dot(np.dot(im1test,V1.T),np.diag(1/s1))\nU2test=np.dot(np.dot(im2test,V2.T),np.diag(1/s2))\n\nnmodes = 20\n\nX = np.hstack((U1[:,:nmodes],U2[:,:nmodes]))\nX_test = np.hstack((U1test[:,:nmodes],U2test[:,:nmodes]))\ny = trainDF['is_iceberg'].values","cell_type":"code"},{"metadata":{"_uuid":"581325179817744f640cbad47b040c4dc282552a","_cell_guid":"a1569666-7286-43af-b31e-842c9dbb8795","collapsed":true},"execution_count":null,"outputs":[],"source":"#is there a native xgb way of doing it?\ndef logloss_xgb(preds, dtrain):\n    labels = dtrain.get_label()\n    score = log_loss(labels, preds)\n    return 'logloss', score","cell_type":"code"},{"metadata":{"_uuid":"b1f4522cbe45692088e1fe1d817f1db387cddaee","_cell_guid":"db22c09b-3f05-4cab-b65c-98b96ef2a2c5"},"execution_count":null,"outputs":[],"source":"nfolds = 3;\nxgb_mdl=[None]*nfolds\n\n\nxgb_params = {\n        'objective': 'binary:logistic',\n        'n_estimators':1000,\n        'max_depth': 8,\n        'subsample': 0.9,\n        'colsample_bytree': 0.9 ,\n     #   'max_delta_step': 1,\n     #   'min_child_weight': 10,\n        'eta': 0.01,\n      #  'gamma': 0.5\n        }\n\n\nfolds = list(StratifiedKFold(n_splits=nfolds, shuffle=True, random_state=2016).split(X, y))\n\nd_test = xgb.DMatrix(X_test)\n\npreds = np.zeros((X_test.shape[0],nfolds))\n\nfor j, (train_idx, valid_idx) in enumerate(folds):\n    X_train = X[train_idx]\n    y_train = y[train_idx]\n    \n    X_valid = X[valid_idx]\n    y_valid = y[valid_idx]\n    \n    d_train =  xgb.DMatrix(X_train,label=y_train)\n    d_valid =  xgb.DMatrix(X_valid,label=y_valid)\n    \n    watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n    \n    xgb_mdl[j]=xgb.train(\n            xgb_params, \n            d_train, \n            1600, watchlist, \n            early_stopping_rounds=70, \n            feval=logloss_xgb, \n            maximize=False, \n            verbose_eval=100)\n    preds[:,j] = xgb_mdl[j].predict(d_test)","cell_type":"code"},{"metadata":{"_uuid":"ee960d6bb03549e9b7956715ebd3a670af600a8e","_cell_guid":"1fe79978-e5fa-4e5d-9192-9b309ae12e45","collapsed":true},"execution_count":null,"outputs":[],"source":"y_pred = np.mean(preds,axis=1)\nsub = pd.DataFrame()\nsub['id'] = testDF['id']\nsub['is_iceberg'] = y_pred\nsub.to_csv(subName, index=False)\n","cell_type":"code"}]}