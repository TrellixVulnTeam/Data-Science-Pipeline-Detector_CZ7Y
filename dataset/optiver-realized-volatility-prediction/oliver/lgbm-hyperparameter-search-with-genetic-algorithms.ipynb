{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install galearn","metadata":{"execution":{"iopub.status.busy":"2021-09-26T15:14:35.34937Z","iopub.execute_input":"2021-09-26T15:14:35.349967Z","iopub.status.idle":"2021-09-26T15:14:46.039706Z","shell.execute_reply.started":"2021-09-26T15:14:35.349828Z","shell.execute_reply":"2021-09-26T15:14:46.038651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"raw","source":"This is a hyperparameter optimization package I created as part of homework last semester.\nIt's sklearn-API compatible and thus also works with lightgbm.\n\nThe training data is preprocessed minimally for show. \nexchange with your own.\n\nI'd be glad to hear your feedback and thoughts. \nLet me know how it works for you and where I can improve it.\n\nDocumentation is just in the form of docstrings at the moment but I'm working on creating a read the docs (sorry!!!).\n\nIt's fairly straightforward and fast.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport glob\nimport os\nimport gc\nfrom galearn import *\nfrom joblib import Parallel, delayed\nfrom sklearn.model_selection import KFold\n\nfrom sklearn.metrics import make_scorer\nfrom sklearn.preprocessing import MinMaxScaler,StandardScaler,LabelEncoder\nfrom sklearn.preprocessing import QuantileTransformer\nfrom sklearn.metrics import r2_score\nimport lightgbm as lgb\nimport matplotlib.pyplot as plt \n\n\npath_submissions = '/'\n\ntarget_name = 'target'\nscores_folds = {}","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-09-26T15:14:46.041261Z","iopub.execute_input":"2021-09-26T15:14:46.041547Z","iopub.status.idle":"2021-09-26T15:14:47.92854Z","shell.execute_reply.started":"2021-09-26T15:14:46.041516Z","shell.execute_reply":"2021-09-26T15:14:47.9276Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Note the score is negative RMSPE because cross-validate expects a utility function, don't get rid of the minus sign below!","metadata":{}},{"cell_type":"code","source":"# Function to calculate the root mean squared percentage error\ndef rmspe(y_true, y_pred):\n    return -np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))","metadata":{"execution":{"iopub.status.busy":"2021-09-26T15:14:47.930277Z","iopub.execute_input":"2021-09-26T15:14:47.930583Z","iopub.status.idle":"2021-09-26T15:14:47.935083Z","shell.execute_reply.started":"2021-09-26T15:14:47.930555Z","shell.execute_reply":"2021-09-26T15:14:47.934204Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = pd.read_parquet(\"../input/train-and-test-data/test.pq\")\ntrain = pd.read_parquet(\"../input/train-and-test-data/train.pq\")","metadata":{"execution":{"iopub.status.busy":"2021-09-26T15:16:10.831317Z","iopub.execute_input":"2021-09-26T15:16:10.831682Z","iopub.status.idle":"2021-09-26T15:16:12.331014Z","shell.execute_reply.started":"2021-09-26T15:16:10.831651Z","shell.execute_reply":"2021-09-26T15:16:12.329724Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Parameter Grid\nInput the parameter grid like in GridSearch as an iterable (lists, array). I will soon make it work with scipy distributions as well.","metadata":{}},{"cell_type":"code","source":"params = dict()\nparams['boosting_type'] = ['gbdt', 'dart']\nparams['n_jobs'] = [-1]\nparams['num_leaves'] = np.arange(50,300)\nparams['learning_rate'] = np.linspace(0.0001, 0.5, 1000)\nparams['colsample_bytree'] = np.linspace(0.5, 1, 1000)\nparams['subsample'] = np.linspace(0.25, 0.9, 1000)\nparams['n_estimators'] = np.arange(70, 300)\nparams['min_child_weight'] = np.linspace(0.00001, 50, 1000)\nparams['min_child_samples'] = np.arange(3, 50)\nparams['reg_alpha'] = np.linspace(0.001, 1, 1000)\nparams['reg_lambda'] = np.linspace(0.0001, 0.5, 1000)\nparams['random_state'] = [42]","metadata":{"execution":{"iopub.status.busy":"2021-09-26T15:16:15.261664Z","iopub.execute_input":"2021-09-26T15:16:15.262247Z","iopub.status.idle":"2021-09-26T15:16:15.272011Z","shell.execute_reply.started":"2021-09-26T15:16:15.262194Z","shell.execute_reply":"2021-09-26T15:16:15.27109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scorer = make_scorer(rmspe)","metadata":{"execution":{"iopub.status.busy":"2021-09-26T15:16:15.500126Z","iopub.execute_input":"2021-09-26T15:16:15.500647Z","iopub.status.idle":"2021-09-26T15:16:15.504515Z","shell.execute_reply.started":"2021-09-26T15:16:15.500616Z","shell.execute_reply":"2021-09-26T15:16:15.503494Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features = [col for col in train.columns if col not in {\"time_id\", \"target\", \"row_id\"}]\ny = train['target']\nx = train[features]","metadata":{"execution":{"iopub.status.busy":"2021-09-26T15:16:19.899181Z","iopub.execute_input":"2021-09-26T15:16:19.899749Z","iopub.status.idle":"2021-09-26T15:16:19.950073Z","shell.execute_reply.started":"2021-09-26T15:16:19.899714Z","shell.execute_reply":"2021-09-26T15:16:19.949038Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Parameters:\n\nscorer: sklearn scorer function, greater is better than\n\niterations: the number of iterations to run the\n\nmodel: an sklearn API compatible model\n\ncv: number of cross validation folds, also accepts input kfold, groupkfold objects\n\nselection: selection algorithm to be used, see more -> galearn.selection\n\nIf you have trouble it's worth trying to tweak the probabilities\n\np_cross: probability of crossover (mixing params between best scoring models)\n\np_mutate: probability of mutation (generating new params of best scoring models)\n\nsim_ann: Simulated Annealing, decay p_mutate and p_cross with time by rate decay\n\nrestrict_gene_pool: restrict gene_pool size by rate = decay (narrow search of parameters)\n\ndecay: used as exponential decay for probabilities and gene_pool_window\n\npop_size: size of population, i.e. the number of models to consider per iteration\n\nelitism: the fixed number of individuals to make it into each iteration (keep the best models)","metadata":{}},{"cell_type":"code","source":"best = simulate(params, scorer, 10,\n                        lgb.LGBMRegressor,\n                        x, y,\n                        selection = 'tournament',\n                        p_cross = 0.8,\n                        p_mutate = 0.5,\n                        sim_ann = True)","metadata":{"execution":{"iopub.status.busy":"2021-09-26T15:16:24.664961Z","iopub.execute_input":"2021-09-26T15:16:24.667002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(best)","metadata":{"execution":{"iopub.status.busy":"2021-09-26T15:14:48.137083Z","iopub.status.idle":"2021-09-26T15:14:48.13771Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Have fun, may the markets be with you!","metadata":{}}]}