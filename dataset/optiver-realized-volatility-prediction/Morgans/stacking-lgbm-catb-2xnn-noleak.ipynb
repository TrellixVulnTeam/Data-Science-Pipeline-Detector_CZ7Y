{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport gc\npd.set_option('max_rows', 400)\npd.set_option('max_columns', 400)\n\nimport os\nimport glob\n\nbook_inc=100\ntrade_inc=100\nbook_intervals=np.arange(book_inc, 600, book_inc).tolist()\ntrade_intervals=np.arange(trade_inc, 600, trade_inc).tolist()\n\n# Kaggle dir:\ndata_dir = '../input/optiver-realized-volatility-prediction/'\n\n# Local dir:\n# data_dir = './input/optiver-realized-volatility-prediction/'\n\n############################################################################\n# Feature Engineering Part I ###############################################\n############################################################################\n\n# Function to calculate first WAP\ndef calc_wap1(df):\n    wap = (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']) / (df['bid_size1'] + df['ask_size1'])\n    return wap\n\n# Function to calculate second WAP\ndef calc_wap2(df):\n    wap = (df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2']) / (df['bid_size2'] + df['ask_size2'])\n    return wap\n\ndef calc_wap3(df):\n    wap = (df['bid_price1'] * df['bid_size1'] + df['ask_price1'] * df['ask_size1']) / (df['bid_size1'] + df['ask_size1'])\n    return wap\n\ndef calc_wap4(df):\n    wap = (df['bid_price2'] * df['bid_size2'] + df['ask_price2'] * df['ask_size2']) / (df['bid_size2'] + df['ask_size2'])\n    return wap\n\ndef log_return(list_stock_prices):\n    return np.log(list_stock_prices).diff()\n\ndef realized_volatility(series):\n    return np.sqrt(np.sum(series**2))\n\ndef count_unique(series):\n    return len(np.unique(series))\n\n\n# Create book features\n\ndef preprocessor_book(file_path):\n    df = pd.read_parquet(file_path)\n    #calculate return etc\n    df['wap1'] = calc_wap1(df)\n    df['wap2'] = calc_wap2(df)\n    df['wap3'] = calc_wap3(df)\n    df['wap4'] = calc_wap4(df)\n\n    df['log_return1'] = df.groupby(['time_id'])['wap1'].apply(log_return)\n    df['log_return2'] = df.groupby(['time_id'])['wap2'].apply(log_return)\n    df['log_return3'] = df.groupby(['time_id'])['wap3'].apply(log_return)\n    df['log_return4'] = df.groupby(['time_id'])['wap4'].apply(log_return)\n\n    df['wap_balance'] = abs(df['wap1'] - df['wap2'])\n\n    df['price_spread'] = (df['ask_price1'] - df['bid_price1']) / ((df['ask_price1'] + df['bid_price1']) / 2)\n    df['price_spread2'] = (df['ask_price2'] - df['bid_price2']) / ((df['ask_price2'] + df['bid_price2']) / 2)\n\n    df['bid_spread'] = df['bid_price1'] - df['bid_price2']\n    df['ask_spread'] = df['ask_price1'] - df['ask_price2']\n    df[\"bid_ask_spread\"] = abs(df['bid_spread'] - df['ask_spread'])\n\n    df['total_volume'] = (df['ask_size1'] + df['ask_size2']) + (df['bid_size1'] + df['bid_size2'])\n    df['volume_imbalance'] = abs((df['ask_size1'] + df['ask_size2']) - (df['bid_size1'] + df['bid_size2']))\n\n    #dict for aggregate\n\n    create_feature_dict = {\n        'wap1': [np.sum, np.std, np.mean],\n        'wap2': [np.sum, np.std, np.mean],\n        'wap3': [np.sum, np.std, np.mean],\n        'wap4': [np.sum, np.std, np.mean],\n        'log_return1': [realized_volatility],\n        'log_return2': [realized_volatility],\n        'log_return3': [realized_volatility],\n        'log_return4': [realized_volatility],\n        'wap_balance': [np.sum, np.max, np.mean],\n        'price_spread':[np.sum, np.max, np.mean],\n        'price_spread2':[np.sum, np.max, np.mean],\n        'bid_spread':[np.sum, np.max, np.mean],\n        'ask_spread':[np.sum, np.max, np.mean],\n        'bid_ask_spread':[np.sum,  np.max, np.mean],\n        'total_volume':[np.sum, np.max, np.mean],\n        'volume_imbalance':[np.sum, np.max, np.mean],\n    }\n\n    create_feature_dict_time = {\n        'log_return1': [realized_volatility],\n        'log_return2': [realized_volatility],\n        'log_return3': [realized_volatility],\n        'log_return4': [realized_volatility],\n    }\n\n    #####groupby / all seconds\n    df_feature = pd.DataFrame(df.groupby(['time_id']).agg(create_feature_dict)).reset_index()\n\n    df_feature.columns = ['_'.join(col) for col in df_feature.columns] #time_id is changed to time_id_\n\n    ######groupby / s seconds\n    for i in range(0,len(book_intervals)):\n        s_min=book_intervals[i]\n\n        df_feature_sec = pd.DataFrame(df.query(f'seconds_in_bucket >= {s_min}').groupby(['time_id']).agg(create_feature_dict_time)).reset_index()\n\n        df_feature_sec.columns = ['_'.join(col) for col in df_feature_sec.columns] #time_id is changed to time_id_\n\n        df_feature_sec = df_feature_sec.add_suffix('_' + str(s_min))\n\n        df_feature = pd.merge(df_feature,df_feature_sec,how='left',left_on='time_id_',right_on=f'time_id__{s_min}')\n        df_feature = df_feature.drop([f'time_id__{s_min}'],axis=1)\n\n    #create row_id\n    stock_id = file_path.split('=')[1]\n    df_feature['row_id'] = df_feature['time_id_'].apply(lambda x:f'{stock_id}-{x}')\n    df_feature = df_feature.drop(['time_id_'],axis=1)\n\n    return df_feature\n\n# Example\n# file_path = data_dir + \"book_train.parquet/stock_id=0\"\n# preprocessor_book(file_path)\n\n# Create Trade features\n\ndef preprocessor_trade(file_path):\n    df = pd.read_parquet(file_path)\n    df['log_return'] = df.groupby('time_id')['price'].apply(log_return)\n    df['amount']=df['price']*df['size']\n\n\n    create_feature_dict = {\n        'log_return':[realized_volatility],\n        'seconds_in_bucket':[count_unique],\n        'size':[np.sum, np.max, np.min],\n        'order_count':[np.sum,np.max,np.mean],\n        'amount':[np.sum,np.max,np.min],\n    }\n\n    create_feature_dict_time = {\n        'log_return':[realized_volatility],\n        'seconds_in_bucket':[count_unique],\n        'size':[np.sum],\n        'order_count':[np.sum],\n    }\n\n\n    df_feature = df.groupby('time_id').agg(create_feature_dict)\n\n    df_feature = df_feature.reset_index()\n    df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n\n\n    ######groupby / seconds\n    for i in range(0,len(trade_intervals)):\n        s_min=trade_intervals[i]\n\n        df_feature_sec = df.query(f'seconds_in_bucket >= {s_min}').groupby('time_id').agg(create_feature_dict_time)\n        df_feature_sec = df_feature_sec.reset_index()\n\n        df_feature_sec.columns = ['_'.join(col) for col in df_feature_sec.columns]\n        df_feature_sec = df_feature_sec.add_suffix('_' + str(s_min))\n\n        df_feature = pd.merge(df_feature,df_feature_sec,how='left',left_on='time_id_',right_on=f'time_id__{s_min}')\n        df_feature = df_feature.drop([f'time_id__{s_min}'],axis=1)\n\n    def tendency(price, vol):\n        df_diff = np.diff(price)\n        val = (df_diff/price[1:])*100\n        power = np.sum(val*vol[1:])\n        return(power)\n\n    lis = []\n    for n_time_id in df['time_id'].unique():\n        df_id = df[df['time_id'] == n_time_id]\n        tendencyV = tendency(df_id['price'].values, df_id['size'].values)\n        f_max = np.sum(df_id['price'].values > np.mean(df_id['price'].values))\n        f_min = np.sum(df_id['price'].values < np.mean(df_id['price'].values))\n        df_max =  np.sum(np.diff(df_id['price'].values) > 0)\n        df_min =  np.sum(np.diff(df_id['price'].values) < 0)\n        # new\n        abs_diff = np.median(np.abs( df_id['price'].values - np.mean(df_id['price'].values)))\n        energy = np.mean(df_id['price'].values**2)\n        iqr_p = np.percentile(df_id['price'].values,75) - np.percentile(df_id['price'].values,25)\n\n        # vol vars\n\n        abs_diff_v = np.median(np.abs( df_id['size'].values - np.mean(df_id['size'].values)))\n        energy_v = np.sum(df_id['size'].values**2)\n        iqr_p_v = np.percentile(df_id['size'].values,75) - np.percentile(df_id['size'].values,25)\n\n        lis.append({'time_id':n_time_id,'tendency':tendencyV,'f_max':f_max,'f_min':f_min,'df_max':df_max,'df_min':df_min,\n                   'abs_diff':abs_diff,'energy':energy,'iqr_p':iqr_p,'abs_diff_v':abs_diff_v,'energy_v':energy_v,'iqr_p_v':iqr_p_v})\n\n    df_lr = pd.DataFrame(lis)\n\n    df_feature = df_feature.merge(df_lr, how = 'left', left_on = 'time_id_', right_on = 'time_id')\n\n\n    df_feature = df_feature.add_prefix('trade_')\n    stock_id = file_path.split('=')[1]\n    df_feature['row_id'] = df_feature['trade_time_id_'].apply(lambda x:f'{stock_id}-{x}')\n    df_feature = df_feature.drop(['trade_time_id_'],axis=1)\n\n    return df_feature\n\n# Example\n# file_path = data_dir + \"trade_train.parquet/stock_id=0\"\n# preprocessor_trade(file_path)\n\ndef preprocessor(list_stock_ids, is_train = True):\n    from joblib import Parallel, delayed # parallel computing to save time\n    df = pd.DataFrame()\n\n    def for_joblib(stock_id):\n        if is_train:\n            file_path_book = data_dir + \"book_train.parquet/stock_id=\" + str(stock_id)\n            file_path_trade = data_dir + \"trade_train.parquet/stock_id=\" + str(stock_id)\n        else:\n            file_path_book = data_dir + \"book_test.parquet/stock_id=\" + str(stock_id)\n            file_path_trade = data_dir + \"trade_test.parquet/stock_id=\" + str(stock_id)\n\n        df_tmp = pd.merge(preprocessor_book(file_path_book),preprocessor_trade(file_path_trade),on='row_id',how='left')\n\n        return pd.concat([df,df_tmp])\n\n    df = Parallel(n_jobs=-1, verbose=1)(\n        delayed(for_joblib)(stock_id) for stock_id in list_stock_ids\n        )\n\n    df =  pd.concat(df,ignore_index = True)\n    return df\n\n# Train\n\n# Debug example:\n# train = pd.read_csv(data_dir + 'train.csv')\n# list_s = [0,1]\n# df_train = preprocessor(list_s, is_train = True)\n\n# Real:\ntrain = pd.read_csv(data_dir + 'train.csv')\ntrain_ids = train.stock_id.unique()\ndf_train = preprocessor(list_stock_ids= train_ids, is_train = True)\n\n# Test\ntest = pd.read_csv(data_dir + 'test.csv')\ntest_ids = test.stock_id.unique()\ndf_test = preprocessor(list_stock_ids = test_ids, is_train = False)\n\n\n############################################################################\n# Feature Engineering Part II ##############################################\n############################################################################\n\n# TRAIN\n####################################################################################\ntrain['row_id'] = train['stock_id'].astype(str) + '-' + train['time_id'].astype(str)\ntrain = train[['row_id','time_id','target']]\ndf_train = train.merge(df_train, on = ['row_id'], how = 'left')\ndf_train['stock_id'] = df_train['row_id'].apply(lambda x:x.split('-')[0])\n\n#stock_id target encoding\nstock_id_target_mean = df_train.groupby('stock_id')['target'].mean()\ndf_train['stock_id_target_enc'] = df_train['stock_id'].map(stock_id_target_mean) # train_set\n\n# Stock_id as integer\ndf_train['stock_id'] = df_train['stock_id'].astype(int)\n\n# tau\ndf_train['size_tau'] = np.sqrt( 1/ df_train['trade_seconds_in_bucket_count_unique'] )\ndf_train['size_tau_400'] = np.sqrt( 1/ df_train['trade_seconds_in_bucket_count_unique_400'] )\ndf_train['size_tau_300'] = np.sqrt( 1/ df_train['trade_seconds_in_bucket_count_unique_300'] )\ndf_train['size_tau_200'] = np.sqrt( 1/ df_train['trade_seconds_in_bucket_count_unique_200'] )\n\ndf_train['size_tau2'] = np.sqrt( 1/ df_train['trade_order_count_sum'] )\ndf_train['size_tau2_400'] = np.sqrt( 0.33/ df_train['trade_order_count_sum'] )\ndf_train['size_tau2_300'] = np.sqrt( 0.5/ df_train['trade_order_count_sum'] )\ndf_train['size_tau2_200'] = np.sqrt( 0.66/ df_train['trade_order_count_sum'] )\n\ndf_train['size_tau2_d'] = df_train['size_tau2_400'] - df_train['size_tau2']\n\n# target to the end\ncols=list(df_train)\ncols.insert(len(cols), cols.pop(cols.index('target')))\ndf_train = df_train.reindex(columns= cols)\n\n\n# TEST\n####################################################################################\ndf_test = test.merge(df_test, on = ['row_id'], how = 'left')\ndf_test['stock_id'] = df_test['row_id'].apply(lambda x:x.split('-')[0])\n\n# stock_id to the end (preserve same ordention than train)\ncols=list(df_test)\ncols.insert(len(cols), cols.pop(cols.index('stock_id')))\ndf_test = df_test.reindex(columns= cols)\n\n#stock_id target encoding\ndf_test['stock_id_target_enc'] = df_test['stock_id'].map(stock_id_target_mean) # test_set\n\n# Stock_id as integer\ndf_test['stock_id'] = df_test['stock_id'].astype(int)\n\n# tau\ndf_test['size_tau'] = np.sqrt( 1/ df_test['trade_seconds_in_bucket_count_unique'] )\ndf_test['size_tau_400'] = np.sqrt( 1/ df_test['trade_seconds_in_bucket_count_unique_400'] )\ndf_test['size_tau_300'] = np.sqrt( 1/ df_test['trade_seconds_in_bucket_count_unique_300'] )\ndf_test['size_tau_200'] = np.sqrt( 1/ df_test['trade_seconds_in_bucket_count_unique_200'] )\n\ndf_test['size_tau2'] = np.sqrt( 1/ df_test['trade_order_count_sum'] )\ndf_test['size_tau2_400'] = np.sqrt( 0.33/ df_test['trade_order_count_sum'] )\ndf_test['size_tau2_300'] = np.sqrt( 0.5/ df_test['trade_order_count_sum'] )\ndf_test['size_tau2_200'] = np.sqrt( 0.66/ df_test['trade_order_count_sum'] )\n\ndf_test['size_tau2_d'] = df_test['size_tau2_400'] - df_test['size_tau2']\n\n# row_id to the begining\ncols=list(df_test)\ncols.insert(0, cols.pop(cols.index('row_id')))\n\ndf_test = df_test.reindex(columns= cols)\n\n# Check\nprint(list(df_train)[0:-1]==list(df_test))\n\n\ndef get_time_stock(df):\n    vol_cols = ['log_return1_realized_volatility', 'log_return2_realized_volatility', 'log_return1_realized_volatility_500', 'log_return2_realized_volatility_500', 'log_return1_realized_volatility_400', 'log_return2_realized_volatility_400',\n                'log_return1_realized_volatility_300', 'log_return2_realized_volatility_300', 'log_return1_realized_volatility_200', 'log_return2_realized_volatility_200', 'log_return1_realized_volatility_100', 'log_return2_realized_volatility_100',\n                'trade_log_return_realized_volatility', 'trade_log_return_realized_volatility_500','trade_log_return_realized_volatility_400', 'trade_log_return_realized_volatility_300', 'trade_log_return_realized_volatility_200','trade_log_return_realized_volatility_100']\n\n    # Group by the stock id\n    df_stock_id = df.groupby(['stock_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n    # Rename columns joining suffix\n    df_stock_id.columns = ['_'.join(col) for col in df_stock_id.columns]\n    df_stock_id = df_stock_id.add_suffix('_' + 'stock')\n\n    # Group by the stock id\n    df_time_id = df.groupby(['time_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n    # Rename columns joining suffix\n    df_time_id.columns = ['_'.join(col) for col in df_time_id.columns]\n    df_time_id = df_time_id.add_suffix('_' + 'time')\n\n    # Merge with original dataframe\n    df = df.merge(df_stock_id, how = 'left', left_on = ['stock_id'], right_on = ['stock_id__stock'])\n    df = df.merge(df_time_id, how = 'left', left_on = ['time_id'], right_on = ['time_id__time'])\n    df.drop(['stock_id__stock', 'time_id__time'], axis = 1, inplace = True)\n    return df\n\n\ndf_train = get_time_stock(df_train)\ndf_test = get_time_stock(df_test)\n\n# target to the end\ncols=list(df_train)\ncols.insert(len(cols), cols.pop(cols.index('target')))\ndf_train = df_train.reindex(columns= cols)\n\n# Check\nprint(list(df_train)[0:-1]==list(df_test))\n\n############################################################################\n# Feature Engineering Part III #############################################\n############################################################################\n\nfrom sklearn.cluster import KMeans\n# making agg features\n\ntrain_p = pd.read_csv(data_dir + 'train.csv')\ntrain_p = train_p.pivot(index='time_id', columns='stock_id', values='target')\n\ncorr = train_p.corr()\n\nids = corr.index\n\nkmeans = KMeans(n_clusters=7, random_state=0).fit(corr.values)\nprint(kmeans.labels_)\n\nl = []\nfor n in range(7):\n    l.append ( [ (x-1) for x in ( (ids+1)*(kmeans.labels_ == n)) if x > 0] )\n\n\nmat = []\nmatTest = []\n\nnnn = ['time_id',\n     'log_return1_realized_volatility',\n     'log_return2_realized_volatility',\n     'total_volume_sum',\n     'trade_size_sum',\n     'trade_order_count_sum',\n     'price_spread_sum',\n     'bid_spread_sum',\n     'ask_spread_sum',\n     'volume_imbalance_sum',\n     'bid_ask_spread_sum',\n     'size_tau',\n     'size_tau2',\n     'stock_id']\n\nn = 0\nfor ind in l:\n    if len(ind)>=5:\n\n        print(ind)\n        newDf = df_train.loc[df_train['stock_id'].isin(ind),nnn]\n        newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n        newDf.loc[:,'stock_id'] = 'c'+str(n)\n        mat.append ( newDf )\n\n        newDf = df_test.loc[df_test['stock_id'].isin(ind),nnn]\n        newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n        newDf.loc[:,'stock_id'] = 'c'+str(n)\n        matTest.append ( newDf )\n\n    n+=1\n\nmat1 = pd.concat(mat).reset_index()\nmat2 = pd.concat(matTest).reset_index()\nmat2 = pd.concat([mat2,mat1.loc[mat1.time_id==5]])\n\n\nmat1 = mat1.pivot(index='time_id', columns='stock_id')\nmat1.columns = [\"_\".join(x) for x in mat1.columns.ravel()]\nmat1.reset_index(inplace=True)\n\nmat2 = mat2.pivot(index='time_id', columns='stock_id')\nmat2.columns = [\"_\".join(x) for x in mat2.columns.ravel()]\nmat2.reset_index(inplace=True)\n\ndf_train = pd.merge(df_train,mat1,how='left',on='time_id')\ndf_train['cluster_id']=df_train['stock_id'].apply(lambda s_id:[i for i, l in enumerate(l) if s_id in l][0])\n\n# target to the end\ncols=list(df_train)\ncols.insert(len(cols), cols.pop(cols.index('target')))\ndf_train = df_train.reindex(columns= cols)\n\ndf_test = pd.merge(df_test,mat2,how='left',on='time_id')\ndf_test['cluster_id']=df_test['stock_id'].apply(lambda s_id:[i for i, l in enumerate(l) if s_id in l][0])\n\n# Check\nprint(list(df_train)[0:-1]==list(df_test))\n\ndel mat1,mat2\ngc.collect()\n\n############################################################################\n# GBM Models ###############################################################\n############################################################################\n\npred_features=list(df_train)[2:-1]\nX_train=df_train[pred_features].reset_index(drop=True)\nY_train=df_train.target.reset_index(drop=True)\nX_test=df_test[pred_features].reset_index(drop=True)\n\ndef rmspe(y_true, y_pred):\n    return  (np.sqrt(np.mean(np.square((y_true - y_pred) / y_true))))\n\ndef feval_RMSPE(preds, lgbm_train):\n    labels = lgbm_train.get_label()\n    return 'RMSPE', round(rmspe(y_true = labels, y_pred = preds),5), False\n\n\n# LightGBM Model\n############################################################################\nfrom sklearn.model_selection import KFold\nimport lightgbm as lgbm\n\n\ndef Model_lgbm_cv(params, k, X_train, X_test, Y_train, RS, cat, esr=100, makepred=True):\n    # Create the k folds\n    kf=KFold(n_splits=k, shuffle=True, random_state=RS)\n\n    # first level train and test\n    Level_1_train = pd.DataFrame(np.zeros((X_train.shape[0],1)), columns=['train_yhat'])\n    if makepred==True:\n        Level_1_test = pd.DataFrame()\n\n    # Main loop for each fold. Initialize counter\n    count=0\n    for train_index, test_index in kf.split(X_train, Y_train):\n        count+=1\n        # Define train and test depending in which fold are we\n        fold_train= X_train.loc[train_index.tolist(), :]\n        fold_test=X_train.loc[test_index.tolist(), :]\n        fold_ytrain=Y_train[train_index.tolist()]\n        fold_ytest=Y_train[test_index.tolist()]\n\n\n        weights = 1/np.square(fold_ytrain)\n        lgbm_train = lgbm.Dataset(fold_train,fold_ytrain, weight = weights, categorical_feature=cat)\n\n        weights = 1/np.square(fold_ytest)\n        lgbm_valid = lgbm.Dataset(fold_test,fold_ytest,reference = lgbm_train, weight = weights, categorical_feature=cat)\n\n        # (k-1)-folds model adjusting\n        MODEL = lgbm.train(params=params,\n                          train_set=lgbm_train,\n                          valid_sets=[lgbm_train, lgbm_valid],\n                          num_boost_round=15000,\n                          feval=feval_RMSPE,\n                          verbose_eval=100,\n                          early_stopping_rounds=esr,\n                          categorical_feature=cat)\n\n        # Predict on the free fold to evaluate metric\n        # and on train to have an overfitting-free prediction for the next level\n        p_fold=MODEL.predict(fold_test)\n        p_fold_train=MODEL.predict(fold_train)\n\n        # Save in Level_1_train the \"free\" predictions concatenated\n        Level_1_train.loc[test_index.tolist(),'train_yhat'] = p_fold\n\n        # Predict in test to make the k model mean\n        # Define name of the prediction (p_\"iteration number\")\n        score=rmspe(fold_ytest,p_fold)\n        print('\\n',k, '- cv, Fold', count, 'RMSPE:', round(score,5),'\\n')\n        if makepred==True:\n            name = 'p_' + str(count)\n            # Predictin to real test\n            real_pred = MODEL.predict(X_test)\n            # Name\n            real_pred = pd.DataFrame({name:real_pred}, columns=[name])\n            # Add to Level_1_test\n            Level_1_test=pd.concat((Level_1_test,real_pred),axis=1)\n\n    # Compute the metric of the total concatenated prediction (and free of overfitting) in train\n    score_total=rmspe(Y_train, Level_1_train['train_yhat'])\n    print('\\n',k, '- cv, TOTAL RMSPE:', round(score_total,5),'\\n')\n\n    # mean of the k predictions in test\n    if makepred==True:\n        Level_1_test['model']=Level_1_test.mean(axis=1)\n\n    # Return train and test sets with predictions and the performance\n    if makepred==True:\n        return Level_1_train, pd.DataFrame({'test_yhat':Level_1_test['model']}), score_total\n    else:\n        return score_total\n\nparams = {\n      \"objective\": \"rmse\",\n      \"metric\": \"rmse\",\n      \"boosting_type\": \"gbdt\",\n      'learning_rate': 0.05,\n      'max_depth': -1,\n      'max_bin':100,\n      'min_data_in_leaf':500,\n      'subsample': 0.72,\n      'subsample_freq': 4,\n      'feature_fraction': 0.5,\n      'lambda_l1': 0.5,\n      'lambda_l2': 1.0\n  }\n\ncategorical_feature=['stock_id','cluster_id']\n\n\nlgbm_train, lgbm_test, slgbm = Model_lgbm_cv(params,5,X_train, X_test, Y_train, RS=2305, cat=categorical_feature, esr=100, makepred=True)\n\n\n# Catboost Model\n############################################################################\nfrom sklearn.model_selection import KFold\nimport catboost as catb\n\n\ndef Model_catb_cv(params, k, X_train, X_test, Y_train, RS, cat, esr=100, makepred=True):\n    # Cat features position\n    Pos=list()\n    for col in cat:\n        Pos.append(X_train.columns.get_loc(col))\n\n    # Create the k folds\n    kf=KFold(n_splits=k, shuffle=True, random_state=RS)\n\n    # first level train and test\n    Level_1_train = pd.DataFrame(np.zeros((X_train.shape[0],1)), columns=['train_yhat'])\n    if makepred==True:\n        Level_1_test = pd.DataFrame()\n\n    # Main loop for each fold. Initialize counter\n    count=0\n    for train_index, test_index in kf.split(X_train, Y_train):\n        count+=1\n        # Define train and test depending in which fold are we\n        fold_train= X_train.loc[train_index.tolist(), :]\n        fold_test=X_train.loc[test_index.tolist(), :]\n        fold_ytrain=Y_train[train_index.tolist()]\n        fold_ytest=Y_train[test_index.tolist()]\n\n\n        weights = 1/np.square(fold_ytrain)\n        lgbm_train = catb.Pool(fold_train,fold_ytrain, weight = weights, cat_features=cat)\n\n        weights = 1/np.square(fold_ytest)\n        lgbm_valid = catb.Pool(fold_test,fold_ytest, weight = weights, cat_features=cat)\n\n        # (k-1)-folds model adjusting\n        MODEL = catb.train(params=params,\n                          pool=lgbm_train,\n                          eval_set=lgbm_valid,\n                          num_boost_round=15000,\n                          verbose_eval=100,\n                          early_stopping_rounds=esr)\n\n        # Predict on the free fold to evaluate metric\n        # and on train to have an overfitting-free prediction for the next level\n        p_fold=MODEL.predict(fold_test)\n        p_fold_train=MODEL.predict(fold_train)\n\n        # Save in Level_1_train the \"free\" predictions concatenated\n        Level_1_train.loc[test_index.tolist(),'train_yhat'] = p_fold\n\n        # Predict in test to make the k model mean\n        # Define name of the prediction (p_\"iteration number\")\n        score=rmspe(fold_ytest,p_fold)\n        print('\\n',k, '- cv, Fold', count, 'RMSPE:', round(score,5),'\\n')\n        if makepred==True:\n            name = 'p_' + str(count)\n            # Predictin to real test\n            real_pred = MODEL.predict(X_test)\n            # Name\n            real_pred = pd.DataFrame({name:real_pred}, columns=[name])\n            # Add to Level_1_test\n            Level_1_test=pd.concat((Level_1_test,real_pred),axis=1)\n\n    # Compute the metric of the total concatenated prediction (and free of overfitting) in train\n    score_total=rmspe(Y_train, Level_1_train['train_yhat'])\n    print('\\n',k, '- cv, TOTAL RMSPE:', round(score_total,5),'\\n')\n\n    # mean of the k predictions in test\n    if makepred==True:\n        Level_1_test['model']=Level_1_test.mean(axis=1)\n\n    # Return train and test sets with predictions and the performance\n    if makepred==True:\n        return Level_1_train, pd.DataFrame({'test_yhat':Level_1_test['model']}), score_total\n    else:\n        return score_total\n\n\nparams = {'objective': 'RMSE',\n            'learning_rate': 0.05,\n            'depth': 4,\n            'min_data_in_leaf': 700,\n            'rsm': 0.8,\n            'subsample': 0.8,\n            'bootstrap_type': 'Bernoulli'}\n\ncategorical_feature=['stock_id','cluster_id']\n\ncatb_train, catb_test, scatb = Model_catb_cv(params,5,X_train, X_test, Y_train, RS=2305, cat=categorical_feature, esr=100, makepred=True)\n\n\ndel X_train, Y_train, X_test\ngc.collect()\n\n############################################################################\n# NN Models ################################################################\n############################################################################\n\n# 1) Imports and definitions\n############################################################################\nfrom numpy.random import seed\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import backend as K\n\ndef rmspe(y_true, y_pred):\n    return  (np.sqrt(np.mean(np.square((y_true - y_pred) / y_true))))\n\ndef root_mean_squared_per_error(y_true, y_pred):\n         return K.sqrt(K.mean(K.square( (y_true - y_pred)/ y_true )))\n\nes = tf.keras.callbacks.EarlyStopping(\n    monitor='val_loss', patience=20, verbose=0,\n    mode='min',restore_best_weights=True)\n\nplateau = tf.keras.callbacks.ReduceLROnPlateau(\n    monitor='val_loss', factor=0.2, patience=7, verbose=0,\n    mode='min')\n\nfrom tensorflow.keras.backend import sigmoid\nfrom tensorflow.keras.utils import get_custom_objects\nfrom tensorflow.keras.layers import Activation\n\ndef swish(x, beta = 1):\n    return (x * sigmoid(beta * x))\n\nget_custom_objects().update({'swish': Activation(swish)})\n\n# 2) Basic Model Structure\n############################################################################\n\ndef base_model(hidden_units,stock_embedding_size,max_cat_data):\n    # Each instance will consist of two inputs:\n    stock_id_input = keras.Input(shape=(1,), name='stock_id')\n    num_input = keras.Input(shape=(317,), name='num_data')\n\n    #embedding, flatenning and concatenating\n    stock_embedded = keras.layers.Embedding(max_cat_data+1, stock_embedding_size,\n                                           input_length=1, name='stock_embedding')(stock_id_input)\n    stock_flattened = keras.layers.Flatten()(stock_embedded)\n    out = keras.layers.Concatenate()([stock_flattened, num_input])\n\n    # Add one or more hidden layers\n    for n_hidden in hidden_units:\n        out = keras.layers.Dense(n_hidden, activation='swish')(out)\n\n    # A single output: our predicted vol\n    out = keras.layers.Dense(1, activation='linear', name='prediction')(out)\n\n    model = keras.Model(\n    inputs = [stock_id_input, num_input],\n    outputs = out,\n    )\n\n    return model\n\n# 3) Transform Dataframes\n############################################################################\nfrom sklearn.preprocessing import QuantileTransformer\n\ncolNames = [col for col in list(df_train.columns)\n            if col not in {'row_id','time_id','stock_id','stock_id_target_enc','cluster_id','target'}]\ndf_train.replace([np.inf, -np.inf], np.nan,inplace=True)\ndf_test.replace([np.inf, -np.inf], np.nan,inplace=True)\n\ntrain_nn=df_train[colNames].copy()\ntest_nn=df_test[colNames].copy()\n\n# Quantile transformation\nfor col in colNames:\n    qt = QuantileTransformer(random_state=21,n_quantiles=2000, output_distribution='normal')\n    train_nn[col] = qt.fit_transform(train_nn[[col]])\n    test_nn[col] = qt.transform(test_nn[[col]])\n\ntrain_nn[['time_id','stock_id','stock_id_target_enc','target']] = df_train[['time_id','stock_id','stock_id_target_enc','target']]\ntest_nn[['time_id','stock_id','stock_id_target_enc']] = df_test[['time_id','stock_id','stock_id_target_enc']]\n\n# Missing imputation\nfeatures_to_consider = list(train_nn)\nfeatures_to_consider.remove('time_id')\nfeatures_to_consider.remove('target')\n\ntrain_nn[features_to_consider] = train_nn[features_to_consider].fillna(train_nn[features_to_consider].mean())\ntest_nn[features_to_consider] = test_nn[features_to_consider].fillna(train_nn[features_to_consider].mean())\n\ndel df_train,df_test\ngc.collect()\n\n\n# 4) Kfold Function\n############################################################################\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import MinMaxScaler\n\ndef Model_NN_cv(params, k, NN_train, NN_test, RS, epochs=300, makepred=True):\n    # Create the k folds\n    kf=KFold(n_splits=k, shuffle=True, random_state=RS)\n\n    # first level train and test\n    Level_1_train = pd.DataFrame(np.zeros((NN_train.shape[0],1)), columns=['train_yhat'])\n    if makepred==True:\n        Level_1_test = np.zeros(NN_test.shape[0])\n\n    # Main loop for each fold\n    count = 1\n    for train_index, test_index in kf.split(NN_train):\n        print('CV {}/{}'.format(count, k))\n\n        # Define train and test depending in which fold are we\n        fold_train = NN_train.loc[train_index.tolist(), features_to_consider]\n        fold_ytrain = NN_train.loc[train_index.tolist(),'target']\n        fold_test = NN_train.loc[test_index.tolist(), features_to_consider]\n        fold_ytest = NN_train.loc[test_index.tolist(), 'target']\n\n        model = base_model(**params)\n        model.compile(\n            keras.optimizers.Adam(learning_rate=0.006),\n            loss=root_mean_squared_per_error\n        )\n\n        try:\n            features_to_consider.remove('stock_id')\n        except:\n            pass\n\n        scaler = MinMaxScaler(feature_range=(-1,1))\n\n        num_data = fold_train[features_to_consider]\n        num_data = scaler.fit_transform(num_data.values)\n        cat_data = fold_train['stock_id']\n\n\n        num_data_test = fold_test[features_to_consider]\n        num_data_test = scaler.transform(num_data_test.values)\n        cat_data_test = fold_test['stock_id']\n\n        model.fit([cat_data, num_data],\n                  fold_ytrain,\n                  batch_size=2048,\n                  epochs=epochs,\n                  validation_data=([cat_data_test, num_data_test], fold_ytest),\n                  callbacks=[es, plateau],\n                  validation_batch_size=len(fold_ytest),\n                  shuffle=True,\n                  verbose = 1)\n\n        preds = model.predict([cat_data_test, num_data_test]).reshape(1,-1)[0]\n\n        score = round(rmspe(y_true = fold_ytest, y_pred = preds),5)\n        print('Fold {} {}: {}'.format(count, 'Neural Network', score))\n\n        # Train prediction\n        Level_1_train.loc[test_index.tolist(),'train_yhat'] = preds\n\n        # Test prediction\n        if makepred==True:\n            tt = scaler.transform(NN_test[features_to_consider].values)\n            Level_1_test += model.predict([NN_test['stock_id'], tt]).reshape(1,-1)[0].clip(0,1e10)/k\n\n        features_to_consider.append('stock_id')\n        count += 1\n\n    score_total=rmspe(NN_train['target'], Level_1_train['train_yhat'])\n    print('\\n',k, '- cv, TOTAL RMSPE:', round(score_total,5),'\\n')\n\n    if makepred==True:\n        Level_1_test=pd.DataFrame(Level_1_test)\n        Level_1_test.columns=['test_yhat']\n        return Level_1_train, Level_1_test, score_total\n    else:\n        return score_total\n\n\n# 5.1) Train NN 1\n############################################################################\nseed(55)\ntf.random.set_seed(55)\n\nparams = {'hidden_units': (128,64,32,16,8),\n          'stock_embedding_size': 36,\n          'max_cat_data': max(train_nn['stock_id'])\n          }\n\nneuraln_train1, neuraln_test1, s1 = Model_NN_cv(params, 5, NN_train=train_nn, NN_test=test_nn, RS=2305, epochs=300, makepred=True)\n\n# 5.2) Train NN 2\n############################################################################\nseed(33)\ntf.random.set_seed(33)\n\nparams = {'hidden_units': (128,64,32,16),\n          'stock_embedding_size': 36,\n          'max_cat_data': max(train_nn['stock_id'])\n          }\n\nneuraln_train2, neuraln_test2, s2 = Model_NN_cv(params, 5, NN_train=train_nn, NN_test=test_nn, RS=2305, epochs=300, makepred=True)\n\n\n############################################################################\n# Stacking Level 2 #########################################################\n############################################################################\n\nX1_train=pd.DataFrame({\n                       'stock_id': train_nn['stock_id'],\n                       'lgbm':lgbm_train['train_yhat'],\n                       'catb':catb_train['train_yhat'],\n                       'neuraln1':neuraln_train1['train_yhat'],\n                       'neuraln2':neuraln_train2['train_yhat']\n                      })\n\nX1_test=pd.DataFrame({\n                       'stock_id': test_nn['stock_id'],\n                       'lgbm':lgbm_test['test_yhat'],\n                       'catb':catb_test['test_yhat'],\n                       'neuraln1':neuraln_test1['test_yhat'],\n                       'neuraln2':neuraln_test2['test_yhat']\n                      })\n\nparams = {\n     'objective': 'rmse',\n     'metric': 'rmse',\n     'boosting_type': 'gbdt',\n     'learning_rate': 0.05,\n     'num_leaves': 5,\n     'min_data_in_leaf': 500,\n     'colsample_bytree': 0.8,\n  }\n\n# Model LightGBM\ncategorical_feature=['stock_id']\nlgbm_train2, lgbm_test2, s = Model_lgbm_cv(params,5,X1_train, X1_test, Y_train=train['target'], RS=2305, cat=categorical_feature, esr=50, makepred=True)\n\n# Output\ny_pred = pd.DataFrame({'row_id': test['row_id'],'target': lgbm_test2['test_yhat']})\n\n# Kaggle output:\ny_pred.to_csv('submission.csv',index = False)\n\n# Local output:\n# y_pred.to_csv('./output/submission.csv',index = False)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]}]}