{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from IPython.core.display import display, HTML\n\nimport pandas as pd\nimport numpy as np \nimport glob\nimport os\nimport gc\n\nfrom joblib import Parallel, delayed\nfrom sklearn.neighbors import NearestNeighbors\n\nfrom sklearn import preprocessing, model_selection\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import QuantileTransformer\nfrom sklearn.metrics import r2_score\n\nimport matplotlib.pyplot as plt \nimport seaborn as sns\nimport numpy.matlib\nfrom sklearn.cluster import KMeans\n\nfrom fastai.tabular.all import *\nimport pickle\nfrom sklearn.model_selection import KFold, GroupKFold\nimport lightgbm as lgb\nfrom catboost import CatBoostRegressor, Pool\nfrom scipy import special\nfrom scipy.stats import skew\n\nfrom numpy.random import seed\n\nimport tensorflow as tf\n\nfrom tensorflow import keras\nfrom keras import backend as K\nfrom keras.backend import sigmoid\nfrom keras.utils.generic_utils import get_custom_objects\nfrom keras.layers import Activation\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nseed(42)\ntf.random.set_seed(42)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-09-27T11:35:19.131079Z","iopub.execute_input":"2021-09-27T11:35:19.131573Z","iopub.status.idle":"2021-09-27T11:35:27.99683Z","shell.execute_reply.started":"2021-09-27T11:35:19.131466Z","shell.execute_reply":"2021-09-27T11:35:27.995979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip -q install ../input/pytorchtabnet/pytorch_tabnet-3.1.1-py3-none-any.whl","metadata":{"execution":{"iopub.status.busy":"2021-09-27T11:35:27.99821Z","iopub.execute_input":"2021-09-27T11:35:27.998561Z","iopub.status.idle":"2021-09-27T11:35:54.977871Z","shell.execute_reply.started":"2021-09-27T11:35:27.998528Z","shell.execute_reply":"2021-09-27T11:35:54.976905Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from scipy import stats\nfrom scipy.stats import norm\nfrom pytorch_tabnet.metrics import Metric\nfrom pytorch_tabnet.tab_model import TabNetRegressor\n\nimport torch\nfrom torch.optim import Adam, SGD\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingWarmRestarts\nfrom sklearn.preprocessing import MinMaxScaler, LabelEncoder, StandardScaler","metadata":{"execution":{"iopub.status.busy":"2021-09-27T11:35:54.979892Z","iopub.execute_input":"2021-09-27T11:35:54.980199Z","iopub.status.idle":"2021-09-27T11:35:54.99972Z","shell.execute_reply.started":"2021-09-27T11:35:54.98017Z","shell.execute_reply":"2021-09-27T11:35:54.998964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# data directory\ndata_dir = '../input/optiver-realized-volatility-prediction/'\n\n# Function to calculate first WAP\ndef calc_wap1(df):\n    wap = (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']) / (df['bid_size1'] + df['ask_size1'])\n    return wap\n\n# Function to calculate second WAP\ndef calc_wap2(df):\n    wap = (df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2']) / (df['bid_size2'] + df['ask_size2'])\n    return wap\n\ndef calc_wap3(df):\n    wap = (df['bid_price1'] * df['bid_size1'] + df['ask_price1'] * df['ask_size1']) / (df['bid_size1'] + df['ask_size1'])\n    return wap\n\ndef calc_wap4(df):\n    wap = (df['bid_price2'] * df['bid_size2'] + df['ask_price2'] * df['ask_size2']) / (df['bid_size2'] + df['ask_size2'])\n    return wap\n\n# Function to calculate the log of the return\n# Remember that logb(x / y) = logb(x) - logb(y)\ndef log_return(series):\n    return np.log(series).diff()\n\n# Calculate the realized volatility\ndef realized_volatility(series):\n    return np.sqrt(np.sum(series**2))\n\n# Calculate the absolute max log return\ndef max_log_return(series):\n    return np.max(np.abs(series))\n\n# Function to count unique elements of a series\ndef count_unique(series):\n    return len(np.unique(series))\n\n\n# Function to read our base train and test set\ndef read_train_test():\n    train = pd.read_csv('../input/optiver-realized-volatility-prediction/train.csv')\n    test = pd.read_csv('../input/optiver-realized-volatility-prediction/test.csv')\n    # Create a key to merge with book and trade data\n    train['row_id'] = train['stock_id'].astype(str) + '-' + train['time_id'].astype(str)\n    test['row_id'] = test['stock_id'].astype(str) + '-' + test['time_id'].astype(str)\n    print(f'Our training set has {train.shape[0]} rows')\n    return train, test\n\n\n# Function to preprocess book data (for each stock id)\ndef book_preprocessor(file_path):\n    df = pd.read_parquet(file_path)\n    # Calculate Wap\n    df['wap1'] = calc_wap1(df)\n    df['wap2'] = calc_wap2(df)\n    df['wap3'] = calc_wap3(df)\n    df['wap4'] = calc_wap4(df)\n    # Calculate log returns\n    df['log_return1'] = df.groupby(['time_id'])['wap1'].apply(log_return)\n    df['log_return2'] = df.groupby(['time_id'])['wap2'].apply(log_return)\n    df['log_return3'] = df.groupby(['time_id'])['wap3'].apply(log_return)\n    df['log_return4'] = df.groupby(['time_id'])['wap4'].apply(log_return)\n    # Calculate wap balance\n    df['wap_balance1'] = abs(df['wap1'] - df['wap2'])\n    df['wap_balance2'] = abs(df['wap3'] - df['wap4'])\n    # Calculate spread\n    df['price_spread'] = (df['ask_price1'] - df['bid_price1']) / ((df['ask_price1'] + df['bid_price1']) / 2)\n    df['price_spread2'] = (df['ask_price2'] - df['bid_price2']) / ((df['ask_price2'] + df['bid_price2']) / 2)\n    df['bid_spread'] = abs(df['bid_price1'] - df['bid_price2'])\n    df['ask_spread'] = abs(df['ask_price1'] - df['ask_price2'])\n    df[\"bid_ask_spread\"] = abs(df['bid_spread'] - df['ask_spread'])\n    df['total_volume'] = (df['ask_size1'] + df['ask_size2']) + (df['bid_size1'] + df['bid_size2'])\n    df['volume_imbalance'] = abs((df['ask_size1'] + df['ask_size2']) - (df['bid_size1'] + df['bid_size2']))\n    df['bid_ask_max_spread1'] = df['ask_price1'].max() - df['bid_price1'].min()\n    df['bid_ask_max_spread2'] = df['ask_price2'].max() - df['bid_price2'].min()\n    \n    # Dict for aggregations\n    create_feature_dict = {\n        'wap1': [np.mean, np.max, np.min],\n        'wap2': [np.mean, np.max, np.min],\n        'wap3': [np.mean, np.max, np.min],\n        'wap4': [np.mean, np.max, np.min],\n        'log_return1': [np.sum, max_log_return, realized_volatility],\n        'log_return2': [np.sum, max_log_return, realized_volatility],\n        'log_return3': [np.sum, max_log_return, realized_volatility],\n        'log_return4': [np.sum, max_log_return, realized_volatility],\n        'wap_balance1': [np.mean, np.max],\n        'wap_balance2': [np.mean, np.max],\n        'price_spread':[np.mean, np.max],\n        'price_spread2':[np.mean, np.max],\n        'bid_spread':[np.max],\n        'ask_spread':[np.max],\n        'total_volume':[np.sum, np.max],\n        'volume_imbalance':[np.sum, np.max],\n        \"bid_ask_spread\":[np.sum,  np.max],\n        \"bid_ask_max_spread1\":[np.mean],\n        \"bid_ask_max_spread2\":[np.mean]\n    }\n    create_feature_dict_time = {\n        'log_return1': [max_log_return, realized_volatility, np.sum],\n        'log_return2': [max_log_return, realized_volatility, np.sum],\n        'log_return3': [max_log_return, realized_volatility, np.sum],\n        'log_return4': [max_log_return, realized_volatility, np.sum],\n    }\n    \n    # Function to get group stats for different windows (seconds in bucket)\n    def get_stats_window(fe_dict,seconds_in_bucket, add_suffix = False):\n        # Group by the window\n        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(fe_dict).reset_index()\n        # Rename columns joining suffix\n        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n        # Add a suffix to differentiate windows\n        if add_suffix:\n            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n        return df_feature\n    \n    # Get the stats for different windows\n    df_feature = get_stats_window(create_feature_dict,seconds_in_bucket = 0, add_suffix = False)\n    df_feature_500 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 500, add_suffix = True)\n    df_feature_400 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 400, add_suffix = True)\n    df_feature_300 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 300, add_suffix = True)\n    df_feature_200 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 200, add_suffix = True)\n    df_feature_100 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 100, add_suffix = True)\n\n    # Merge all\n    df_feature = df_feature.merge(df_feature_500, how = 'left', left_on = 'time_id_', right_on = 'time_id__500')\n    df_feature = df_feature.merge(df_feature_400, how = 'left', left_on = 'time_id_', right_on = 'time_id__400')\n    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n    df_feature = df_feature.merge(df_feature_200, how = 'left', left_on = 'time_id_', right_on = 'time_id__200')\n    df_feature = df_feature.merge(df_feature_100, how = 'left', left_on = 'time_id_', right_on = 'time_id__100')\n    # Drop unnecesary time_ids\n    df_feature.drop(['time_id__500','time_id__400', 'time_id__300', 'time_id__200','time_id__100'], axis = 1, inplace = True)\n    \n    \n    # Create row_id so we can merge\n    stock_id = file_path.split('=')[1]\n    df_feature['row_id'] = df_feature['time_id_'].apply(lambda x: f'{stock_id}-{x}')\n    df_feature.drop(['time_id_'], axis = 1, inplace = True)\n    return df_feature\n\n\n# Function to preprocess trade data (for each stock id)\ndef trade_preprocessor(file_path):\n    df = pd.read_parquet(file_path)\n    df['log_return'] = df.groupby('time_id')['price'].apply(log_return)\n    df['amount']=df['price']*df['size']\n    df['average_size'] = df['size']/df['order_count']\n    # Dict for aggregations\n    create_feature_dict = {\n        'log_return':[np.sum, realized_volatility, max_log_return],\n        'seconds_in_bucket':[count_unique],\n        'average_size': [np.mean, np.std, np.max],\n        'size':[np.sum, np.max],\n        'order_count':[np.sum,np.max],\n        'amount':[np.sum,np.max],\n    }\n    create_feature_dict_time = {\n        'log_return':[np.sum, realized_volatility, max_log_return],\n        'seconds_in_bucket':[count_unique],\n        'size':[np.sum, np.max],\n        'order_count':[np.sum, np.max],\n    }\n    # Function to get group stats for different windows (seconds in bucket)\n    def get_stats_window(fe_dict,seconds_in_bucket, add_suffix = False):\n        # Group by the window\n        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(fe_dict).reset_index()\n        # Rename columns joining suffix\n        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n        # Add a suffix to differentiate windows\n        if add_suffix:\n            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n        return df_feature\n    \n\n    # Get the stats for different windows\n    df_feature = get_stats_window(create_feature_dict,seconds_in_bucket = 0, add_suffix = False)\n    df_feature_500 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 500, add_suffix = True)\n    df_feature_400 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 400, add_suffix = True)\n    df_feature_300 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 300, add_suffix = True)\n    df_feature_200 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 200, add_suffix = True)\n    df_feature_100 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 100, add_suffix = True)\n    \n    def tendency(price, vol):    \n        df_diff = np.diff(price)\n        val = (df_diff/price[1:])*100\n        power = np.sum(val*vol[1:])\n        return(power)\n    \n    lis = []\n    for n_time_id in df['time_id'].unique():\n        df_id = df[df['time_id'] == n_time_id]        \n        tendencyV = tendency(df_id['price'].values, df_id['size'].values)      \n        f_max = np.sum(df_id['price'].values > np.mean(df_id['price'].values))\n        f_min = np.sum(df_id['price'].values < np.mean(df_id['price'].values))\n        df_max =  np.sum(np.diff(df_id['price'].values) > 0)\n        df_min =  np.sum(np.diff(df_id['price'].values) < 0)\n        # new\n        abs_diff = np.median(np.abs( df_id['price'].values - np.mean(df_id['price'].values)))        \n        energy = np.mean(df_id['price'].values**2)\n        iqr_p = np.percentile(df_id['price'].values,75) - np.percentile(df_id['price'].values,25)\n        \n        # vol vars\n        \n        abs_diff_v = np.median(np.abs( df_id['size'].values - np.mean(df_id['size'].values)))        \n        energy_v = np.sum(df_id['size'].values**2)\n        iqr_p_v = np.percentile(df_id['size'].values,75) - np.percentile(df_id['size'].values,25)\n        \n        lis.append({'time_id':n_time_id,'tendency':tendencyV,'f_max':f_max,'f_min':f_min,'df_max':df_max,'df_min':df_min,\n                   'abs_diff':abs_diff,'energy':energy,'iqr_p':iqr_p,'abs_diff_v':abs_diff_v,'energy_v':energy_v,'iqr_p_v':iqr_p_v})\n    \n    df_lr = pd.DataFrame(lis)\n        \n   \n    df_feature = df_feature.merge(df_lr, how = 'left', left_on = 'time_id_', right_on = 'time_id')\n    \n    # Merge all\n    df_feature = df_feature.merge(df_feature_500, how = 'left', left_on = 'time_id_', right_on = 'time_id__500')\n    df_feature = df_feature.merge(df_feature_400, how = 'left', left_on = 'time_id_', right_on = 'time_id__400')\n    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n    df_feature = df_feature.merge(df_feature_200, how = 'left', left_on = 'time_id_', right_on = 'time_id__200')\n    df_feature = df_feature.merge(df_feature_100, how = 'left', left_on = 'time_id_', right_on = 'time_id__100')\n    # Drop unnecesary time_ids\n    df_feature.drop(['time_id__500','time_id__400', 'time_id__300', 'time_id__200','time_id','time_id__100'], axis = 1, inplace = True)\n    \n    \n    df_feature = df_feature.add_prefix('trade_')\n    stock_id = file_path.split('=')[1]\n    df_feature['row_id'] = df_feature['trade_time_id_'].apply(lambda x:f'{stock_id}-{x}')\n    df_feature.drop(['trade_time_id_'], axis = 1, inplace = True)\n    return df_feature\n\n\n# Function to get group stats for the stock_id and time_id\ndef get_time_stock(df):\n    vol_cols = ['log_return1_realized_volatility', 'log_return2_realized_volatility', 'log_return1_realized_volatility_400', 'log_return2_realized_volatility_400', \n                'log_return1_realized_volatility_300', 'log_return2_realized_volatility_300', 'log_return1_realized_volatility_200', 'log_return2_realized_volatility_200', \n                'trade_log_return_realized_volatility', 'trade_log_return_realized_volatility_400', 'trade_log_return_realized_volatility_300', 'trade_log_return_realized_volatility_200', \n                'log_return1_max_log_return', 'log_return2_max_log_return', 'log_return1_max_log_return_400', 'log_return2_max_log_return_400', \n                'log_return1_max_log_return_300', 'log_return2_max_log_return_300', 'log_return1_max_log_return_200', 'log_return2_max_log_return_200', \n                'trade_log_return_max_log_return', 'trade_log_return_max_log_return_400', 'trade_log_return_max_log_return_300', 'trade_log_return_max_log_return_200']\n\n\n    # Group by the stock id\n    df_stock_id = df.groupby(['stock_id'])[vol_cols].agg(['mean', 'std', skew]).reset_index()\n    # Rename columns joining suffix\n    df_stock_id.columns = ['_'.join(col) for col in df_stock_id.columns]\n    df_stock_id = df_stock_id.add_suffix('_' + 'stock')\n\n    # Group by the stock id\n    df_time_id = df.groupby(['time_id'])[vol_cols].agg(['mean', 'std', skew]).reset_index()\n    # Rename columns joining suffix\n    df_time_id.columns = ['_'.join(col) for col in df_time_id.columns]\n    df_time_id = df_time_id.add_suffix('_' + 'time')\n    \n    # Merge with original dataframe\n    df = df.merge(df_stock_id, how = 'left', left_on = ['stock_id'], right_on = ['stock_id__stock'])\n    df = df.merge(df_time_id, how = 'left', left_on = ['time_id'], right_on = ['time_id__time'])\n    df.drop(['stock_id__stock', 'time_id__time'], axis = 1, inplace = True)\n    return df\n# Funtion to make preprocessing function in parallel (for each stock id)\ndef preprocessor(list_stock_ids, is_train = True):\n    \n    # Parrallel for loop\n    def for_joblib(stock_id):\n        # Train\n        if is_train:\n            file_path_book = data_dir + \"book_train.parquet/stock_id=\" + str(stock_id)\n            file_path_trade = data_dir + \"trade_train.parquet/stock_id=\" + str(stock_id)\n        # Test\n        else:\n            file_path_book = data_dir + \"book_test.parquet/stock_id=\" + str(stock_id)\n            file_path_trade = data_dir + \"trade_test.parquet/stock_id=\" + str(stock_id)\n    \n        # Preprocess book and trade data and merge them\n        df_tmp = pd.merge(book_preprocessor(file_path_book), trade_preprocessor(file_path_trade), on = 'row_id', how = 'left')\n        \n        # Return the merge dataframe\n        return df_tmp\n    \n    # Use parallel api to call paralle for loop\n    df = Parallel(n_jobs = -1, verbose = 1)(delayed(for_joblib)(stock_id) for stock_id in list_stock_ids)\n    # Concatenate all the dataframes that return from Parallel\n    df = pd.concat(df, ignore_index = True)\n    return df","metadata":{"execution":{"iopub.status.busy":"2021-09-27T11:35:55.00269Z","iopub.execute_input":"2021-09-27T11:35:55.002932Z","iopub.status.idle":"2021-09-27T11:35:55.291248Z","shell.execute_reply.started":"2021-09-27T11:35:55.002909Z","shell.execute_reply":"2021-09-27T11:35:55.29028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Read train and test\ntrain_df, test_df = read_train_test()\n\nif os.path.isfile('../input/optiver-final-dataset/train_df.pkl'):\n    train_df =  pd.read_pickle('../input/optiver-final-dataset/train_df.pkl')\nelse:\n    train_stock_ids = train_df['stock_id'].unique()\n    train_ = preprocessor(train_stock_ids, is_train = True)\n    train_df = train_df.merge(train_, on = ['row_id'], how = 'left')\n    train_df = get_time_stock(train_df)\n    \n    del train_stock_ids, train_\n    gc.collect()\n    \n    train_df.to_pickle(\"train_df.pkl\")\n\ntest_stock_ids = test_df['stock_id'].unique()\ntest_ = preprocessor(test_stock_ids, is_train = False)\ntest_df = test_df.merge(test_, on = ['row_id'], how = 'left')\ntest_df = get_time_stock(test_df)\n\ndel test_stock_ids, test_\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-09-27T11:35:55.292529Z","iopub.execute_input":"2021-09-27T11:35:55.292869Z","iopub.status.idle":"2021-09-27T11:36:06.15747Z","shell.execute_reply.started":"2021-09-27T11:35:55.292835Z","shell.execute_reply":"2021-09-27T11:36:06.156675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# replace by order sum (tau)\ntrain_df['size_tau'] = np.sqrt( 1/ train_df['trade_seconds_in_bucket_count_unique'] )\ntest_df['size_tau'] = np.sqrt( 1/ test_df['trade_seconds_in_bucket_count_unique'] )\ntrain_df['size_tau_400'] = np.sqrt( 1/ train_df['trade_seconds_in_bucket_count_unique_400'] )\ntest_df['size_tau_400'] = np.sqrt( 1/ test_df['trade_seconds_in_bucket_count_unique_400'] )\ntrain_df['size_tau_300'] = np.sqrt( 1/ train_df['trade_seconds_in_bucket_count_unique_300'] )\ntest_df['size_tau_300'] = np.sqrt( 1/ test_df['trade_seconds_in_bucket_count_unique_300'] )\ntrain_df['size_tau_200'] = np.sqrt( 1/ train_df['trade_seconds_in_bucket_count_unique_200'] )\ntest_df['size_tau_200'] = np.sqrt( 1/ test_df['trade_seconds_in_bucket_count_unique_200'] )\n\ntrain_df['size_tau2'] = np.sqrt( 1/ train_df['trade_order_count_sum'] )\ntest_df['size_tau2'] = np.sqrt( 1/ test_df['trade_order_count_sum'] )\ntrain_df['size_tau2_400'] = np.sqrt( 0.33/ train_df['trade_order_count_sum'] )\ntest_df['size_tau2_400'] = np.sqrt( 0.33/ test_df['trade_order_count_sum'] )\ntrain_df['size_tau2_300'] = np.sqrt( 0.5/ train_df['trade_order_count_sum'] )\ntest_df['size_tau2_300'] = np.sqrt( 0.5/ test_df['trade_order_count_sum'] )\ntrain_df['size_tau2_200'] = np.sqrt( 0.66/ train_df['trade_order_count_sum'] )\ntest_df['size_tau2_200'] = np.sqrt( 0.66/ test_df['trade_order_count_sum'] )\n\ntrain_df['size_tau2_d'] = train_df['size_tau2_400'] - train_df['size_tau2']\ntest_df['size_tau2_d'] = test_df['size_tau2_400'] - test_df['size_tau2']","metadata":{"execution":{"iopub.status.busy":"2021-09-27T11:36:06.158762Z","iopub.execute_input":"2021-09-27T11:36:06.159092Z","iopub.status.idle":"2021-09-27T11:36:06.210703Z","shell.execute_reply.started":"2021-09-27T11:36:06.159053Z","shell.execute_reply":"2021-09-27T11:36:06.20986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Clustered Features","metadata":{}},{"cell_type":"code","source":"def create_agg_features(train, test):\n\n    # Making agg features\n\n    l = [[1, 11, 22, 50, 55, 56, 62, 73, 76, 78, 84, 87, 96, 101, 112, 116, 122, 124, 126], \n         [0, 4, 5, 10, 15, 16, 17, 23, 26, 28, 29, 36, 42, 44, 48, 53, 66, 69, 72, 85, 94, 95, 100, 102, 109, 111, 113, 115, 118, 120], [3, 6, 9, 18, 61, 63, 86, 97], \n         [27, 31, 33, 37, 38, 40, 58, 59, 60, 74, 75, 77, 82, 83, 88, 89, 90, 98, 99, 110], \n         [2, 7, 13, 14, 19, 20, 21, 30, 32, 34, 35, 39, 41, 43, 46, 47, 51, 52, 64, 67, 68, 70, 93, 103, 104, 105, 107, 108, 114, 119, 123, 125], [81], [8, 80]]\n    \n    mat = []\n    matTest = []\n    n = 0\n    for ind in l:\n        newDf = train.loc[train['stock_id'].isin(ind) ]\n        newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n        newDf.loc[:,'stock_id'] = str(n)+'c1'\n        mat.append ( newDf )\n        newDf = test.loc[test['stock_id'].isin(ind) ]    \n        newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n        newDf.loc[:,'stock_id'] = str(n)+'c1'\n        matTest.append ( newDf )\n        n+=1\n\n    mat1 = pd.concat(mat).reset_index()\n    mat1.drop(columns=['target'],inplace=True)\n    mat2 = pd.concat(matTest).reset_index()\n    \n    mat2 = pd.concat([mat2,mat1.loc[mat1.time_id==5]])\n    \n    mat1 = mat1.pivot(index='time_id', columns='stock_id')\n    mat1.columns = [\"_\".join(x) for x in mat1.columns.ravel()]\n    mat1.reset_index(inplace=True)\n    \n    mat2 = mat2.pivot(index='time_id', columns='stock_id')\n    mat2.columns = [\"_\".join(x) for x in mat2.columns.ravel()]\n    mat2.reset_index(inplace=True)\n    \n    prefix = ['log_return1_realized_volatility', 'log_return2_realized_volatility', 'total_volume_sum', 'trade_size_sum', 'trade_order_count_sum',\n              'price_spread_mean', 'volume_imbalance_sum','size_tau2', 'trade_amount_sum', 'trade_log_return_realized_volatility', \n             'log_return1_max_log_return', 'log_return2_max_log_return', 'trade_log_max_log_return']\n    \n    selected_cols=mat1.filter(regex='|'.join(f'^{x}.(0|1|3|4|6)c1' for x in prefix)).columns.tolist()\n    selected_cols.append('time_id')\n    \n    train_m = pd.merge(train,mat1[selected_cols],how='left',on='time_id')\n    test_m = pd.merge(test,mat2[selected_cols],how='left',on='time_id')\n    \n    # filling missing values with train means\n\n#     features = [col for col in train_m.columns.tolist() if col not in ['time_id','target','row_id']]\n#     train_m[features] = train_m[features].fillna(train_m[features].mean())\n#     test_m[features] = test_m[features].fillna(train_m[features].mean())\n\n    return train_m, test_m","metadata":{"execution":{"iopub.status.busy":"2021-09-27T11:36:06.212043Z","iopub.execute_input":"2021-09-27T11:36:06.212403Z","iopub.status.idle":"2021-09-27T11:36:06.229654Z","shell.execute_reply.started":"2021-09-27T11:36:06.212358Z","shell.execute_reply":"2021-09-27T11:36:06.228702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Aggregating some features\ntrain_df, test_df = create_agg_features(train_df,test_df)","metadata":{"execution":{"iopub.status.busy":"2021-09-27T11:36:06.233063Z","iopub.execute_input":"2021-09-27T11:36:06.233725Z","iopub.status.idle":"2021-09-27T11:36:27.037155Z","shell.execute_reply.started":"2021-09-27T11:36:06.233686Z","shell.execute_reply":"2021-09-27T11:36:27.036088Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Nearest Neighbor based Features","metadata":{}},{"cell_type":"code","source":"vol_cols = ['log_return1_max_log_return', 'log_return1_realized_volatility', 'log_return2_max_log_return', \n'log_return2_realized_volatility', 'price_spread_mean', 'total_volume_sum', 'volume_imbalance_sum', \n'log_return1_max_log_return_500', 'log_return1_realized_volatility_500', 'log_return2_max_log_return_500', \n'log_return2_realized_volatility_500', 'log_return1_max_log_return_400', 'log_return1_realized_volatility_400',\n 'log_return2_max_log_return_400', 'log_return2_realized_volatility_400', 'log_return1_max_log_return_300', \n 'log_return1_realized_volatility_300', 'log_return2_max_log_return_300', 'log_return2_realized_volatility_300', \n 'log_return1_max_log_return_200', 'log_return1_realized_volatility_200', 'log_return2_max_log_return_200', \n 'log_return2_realized_volatility_200', 'log_return1_max_log_return_100', 'log_return1_realized_volatility_100', \n 'log_return2_max_log_return_100', 'log_return2_realized_volatility_100', 'trade_log_return_realized_volatility', \n 'trade_size_sum', 'trade_order_count_sum', 'trade_amount_sum', 'trade_log_return_realized_volatility_500', \n 'trade_size_sum_500', 'trade_order_count_sum_500', 'trade_log_return_realized_volatility_400', 'trade_size_sum_400',\n 'trade_order_count_sum_400', 'trade_log_return_realized_volatility_300', 'trade_size_sum_300', 'trade_order_count_sum_300',\n 'trade_log_return_realized_volatility_200', 'trade_size_sum_200', 'trade_order_count_sum_200',\n 'trade_log_return_realized_volatility_100', 'trade_size_sum_100', 'trade_order_count_sum_100']\n\n#Nearest Neighbor based Feature Generation\nprint('Before nearest neighbor:', train_df.shape)\n\nif os.path.isfile('../input/optiver-utils-files/neg_idx.pkl'):\n    with open('../input/optiver-utils-files/neg_idx.pkl', 'rb') as fin:\n        neg_idx = pickle.load(fin)\n    with open('../input/optiver-utils-files/pos_idx.pkl', 'rb') as fin:\n        pos_idx = pickle.load(fin)\nelse:\n    train_p = pd.read_csv('../input/optiver-realized-volatility-prediction/train.csv')\n    train_p = train_p.pivot(index='stock_id', columns='time_id', values='target')\n    train_p = train_p.fillna(0)\n\n    n=15\n    model = NearestNeighbors(n_neighbors = 112, metric='cosine')\n    model.fit(train_p)\n    distances, indices = model.kneighbors(train_p)\n\n    for i, col in enumerate(train_p.index):\n        indices[i, :] = (train_p.index[indices[i, :]].values)\n\n    pos_idx = {}\n    neg_idx = {}\n\n    for i, col in enumerate(train_p.index):\n        pos_idx[col] = list(indices[i][1:n])\n        neg_idx[col] = list(indices[i][-n:])\n\n    with open('pos_idx.pkl', 'wb') as handle:\n        pickle.dump(pos_idx, handle, protocol=pickle.HIGHEST_PROTOCOL)\n\n    with open('neg_idx.pkl', 'wb') as handle:\n        pickle.dump(neg_idx, handle, protocol=pickle.HIGHEST_PROTOCOL)\n\n    del train_p, indices, distances, model\n    gc.collect()\n\nmat = []\nmat_test = []\n\nfor stock in train_df['stock_id'].unique():\n    ind = pos_idx[stock]\n    newDf = train_df.loc[train_df['stock_id'].isin(ind)]\n    newDf = newDf[vol_cols+['time_id']].groupby(['time_id']).agg(np.nanmean)\n    newDf.columns = [x + \"_pos\" for x in newDf.columns]\n    newDf.loc[:, 'stock_id'] = stock\n    mat.append(newDf)\n    \n    newDf = test_df.loc[test_df['stock_id'].isin(ind)]\n    newDf = newDf[vol_cols+['time_id']].groupby(['time_id']).agg(np.nanmean)\n    newDf.columns = [x + \"_pos\" for x in newDf.columns]\n    newDf['stock_id'] = stock\n    mat_test.append(newDf)\n\n\nmat = pd.concat(mat).reset_index()\nmat_test = pd.concat(mat_test).reset_index()\n\ntrain_df = train_df.merge(mat, how='left', on=['time_id', \"stock_id\"])\ntest_df = test_df.merge(mat_test, how='left', on=['time_id', \"stock_id\"])\n\nmat = []\nmat_test = []\n\nfor stock in train_df['stock_id'].unique():\n    ind = neg_idx[stock]\n    newDf = train_df.loc[train_df['stock_id'].isin(ind)]\n    newDf = newDf[vol_cols+['time_id']].groupby(['time_id']).agg(np.nanmean)\n    newDf.columns = [x + \"_neg\" for x in newDf.columns]\n    newDf.loc[:, 'stock_id'] = stock\n    mat.append(newDf)\n    \n    newDf = test_df.loc[test_df['stock_id'].isin(ind)]\n    newDf = newDf[vol_cols+['time_id']].groupby(['time_id']).agg(np.nanmean)\n    newDf.columns = [x + \"_neg\" for x in newDf.columns]\n    newDf['stock_id'] = stock\n    mat_test.append(newDf)\n\nmat = pd.concat(mat).reset_index()\nmat_test = pd.concat(mat_test).reset_index()\n\ntrain_df = train_df.merge(mat, how='left', on=['time_id', \"stock_id\"])\ntest_df = test_df.merge(mat_test, how='left', on=['time_id', \"stock_id\"])\n\ndel mat, mat_test\ngc.collect()\n\nprint('After nearest neighbor:', train_df.shape)","metadata":{"execution":{"iopub.status.busy":"2021-09-27T11:36:27.03906Z","iopub.execute_input":"2021-09-27T11:36:27.0394Z","iopub.status.idle":"2021-09-27T11:37:32.471043Z","shell.execute_reply.started":"2021-09-27T11:36:27.039366Z","shell.execute_reply":"2021-09-27T11:37:32.470131Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Predicting LGBM and CatBoost","metadata":{}},{"cell_type":"code","source":"cat_cols = ['stock_id']\n\n# cont_cols = list(train_df)\n# cont_cols.remove('time_id')\n# cont_cols.remove('target')\n# cont_cols.remove('row_id')\n\nfeatures = pd.read_csv('../input/optiver-feature-importance-based-lgb/feature_importance.csv')\ncont_cols = features.loc[features['Value']>=100, 'Feature'].to_list()\n\ncont_cols = [f for f in cont_cols if f not in cat_cols]","metadata":{"execution":{"iopub.status.busy":"2021-09-27T11:37:32.472321Z","iopub.execute_input":"2021-09-27T11:37:32.472834Z","iopub.status.idle":"2021-09-27T11:37:32.494743Z","shell.execute_reply.started":"2021-09-27T11:37:32.472795Z","shell.execute_reply":"2021-09-27T11:37:32.493943Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to calculate the root mean squared percentage error\ndef rmspe(y_true, y_pred):\n    return np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\n\n# Function to early stop with root mean squared percentage error\ndef feval_rmspe(y_pred, lgb_train):\n    y_true = lgb_train.get_label()\n    return 'RMSPE', rmspe(y_true, y_pred), False","metadata":{"execution":{"iopub.status.busy":"2021-09-27T11:37:32.495932Z","iopub.execute_input":"2021-09-27T11:37:32.496256Z","iopub.status.idle":"2021-09-27T11:37:32.501793Z","shell.execute_reply.started":"2021-09-27T11:37:32.496223Z","shell.execute_reply":"2021-09-27T11:37:32.500582Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class feval_rmspe:\n    def is_max_optimal(self):\n        return False\n\n    def evaluate(self, approxes, target, weight):\n        assert len(approxes) == 1\n        assert len(target) == len(approxes[0])\n\n        approx = approxes[0]\n\n        y_pred = np.array(approx)\n        y_true = np.array(target)\n\n        output_weight = 1  # weight is not used\n\n        score = rmspe(y_true, y_pred)\n\n        return score, output_weight\n\n    def get_final_error(self, error, weight):\n        return error","metadata":{"execution":{"iopub.status.busy":"2021-09-27T11:37:32.503379Z","iopub.execute_input":"2021-09-27T11:37:32.503787Z","iopub.status.idle":"2021-09-27T11:37:32.511784Z","shell.execute_reply.started":"2021-09-27T11:37:32.503752Z","shell.execute_reply":"2021-09-27T11:37:32.510729Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pickle\n\nmodel_path_lgb = '../input/d/lhagiimn/optiver-fastai-and-keras-models'\nmodel_path_cat = '../input/optiver-catboost-models'\npred_lgb0 = np.zeros(test_df.shape[0])\npred_lgb1 = np.zeros(test_df.shape[0])\npred_cat0 = np.zeros(test_df.shape[0])\nNfolds = 8\n    \nfor fold in range(Nfolds):\n    with open(f'{model_path_lgb}/model_lgb_0_{fold}.pkl', 'rb') as fin:\n        model_lgb = pickle.load(fin)\n            \n    pred_lgb0 += model_lgb.predict(test_df[cat_cols+cont_cols])/Nfolds\n    \n    del model_lgb\n    gc.collect()\n    \n    with open(f'{model_path_lgb}/model_lgb_1_{fold}.pkl', 'rb') as fin:\n        model_lgb = pickle.load(fin)\n            \n    pred_lgb1 += model_lgb.predict(test_df[cat_cols+cont_cols])/Nfolds\n    \n    del model_lgb\n    gc.collect()\n    \n    with open(f'{model_path_cat}/model_cat_{fold}.pkl', 'rb') as cat:\n            model_cat = pickle.load(cat)\n    \n    pred_cat0 += model_cat.predict(test_df[cat_cols+cont_cols])/Nfolds\n    \n    del model_cat\n    gc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-09-27T11:37:32.513074Z","iopub.execute_input":"2021-09-27T11:37:32.513736Z","iopub.status.idle":"2021-09-27T11:37:41.091043Z","shell.execute_reply.started":"2021-09-27T11:37:32.513697Z","shell.execute_reply":"2021-09-27T11:37:41.089863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Predicting FastAI nn","metadata":{}},{"cell_type":"code","source":"from fastai.tabular.all import *\n\nNfolds=8\ndef pred_tabular_nn(train_df, test_df, model_path, dummy=False):\n    train_df = train_df.fillna(0)\n    train_df.stock_id = train_df.stock_id.astype('category')\n    if dummy==True:\n        train_df.dummy = train_df.dummy.astype('category')\n    cont_nn,cat_nn = cont_cat_split(train_df,  dep_var='target')\n    res = torch.zeros(len(test_df))\n    \n    dls = TabularPandas(train_df, [Categorify, Normalize], cat_nn, cont_nn, y_names='target').dataloaders(1024)\n    learn = tabular_learner(dls, y_range=(0,.1), layers=[1024, 512, 256], n_out=1, path = model_path)\n\n    for fold in range(Nfolds): \n        print(f'Fold-{fold}')\n        learn.load(f'nn_model_{fold}')\n        \n        test_dl = dls.test_dl(test_df.fillna(0))\n        preds, _ = learn.get_preds(dl=test_dl)\n        res += preds.squeeze() / Nfolds\n        \n        del test_dl, preds, _\n        gc.collect()\n    \n    del dls, learn\n    gc.collect()\n    \n    return res.numpy()","metadata":{"execution":{"iopub.status.busy":"2021-09-27T11:37:41.09278Z","iopub.execute_input":"2021-09-27T11:37:41.093132Z","iopub.status.idle":"2021-09-27T11:37:41.104234Z","shell.execute_reply.started":"2021-09-27T11:37:41.093098Z","shell.execute_reply":"2021-09-27T11:37:41.103391Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cont_cols = list(train_df)\ncont_cols.remove('time_id')\ncont_cols.remove('target')\ncont_cols.remove('row_id')\n\ncont_cols = [f for f in cont_cols if f not in cat_cols]\npred_nn0 = pred_tabular_nn(train_df[cat_cols + cont_cols +['target']], test_df[cat_cols + cont_cols], \n                                    model_path='../input/d/lhagiimn/optiver-fastai-and-tabnet-models/', dummy=False)","metadata":{"execution":{"iopub.status.busy":"2021-09-27T11:37:41.105959Z","iopub.execute_input":"2021-09-27T11:37:41.106436Z","iopub.status.idle":"2021-09-27T11:38:47.357842Z","shell.execute_reply.started":"2021-09-27T11:37:41.106402Z","shell.execute_reply":"2021-09-27T11:38:47.357011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-09-27T11:38:47.359306Z","iopub.execute_input":"2021-09-27T11:38:47.359665Z","iopub.status.idle":"2021-09-27T11:38:47.536757Z","shell.execute_reply.started":"2021-09-27T11:38:47.359627Z","shell.execute_reply":"2021-09-27T11:38:47.535778Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Tabnet Model Training","metadata":{}},{"cell_type":"code","source":"from pytorch_tabnet.metrics import Metric\nimport torch\nfrom torch.optim import Adam, SGD\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingWarmRestarts\n\ndef rmspe(y_true, y_pred):\n    # Function to calculate the root mean squared percentage error\n    return np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\n\nclass RMSPE(Metric):\n    def __init__(self):\n        self._name = \"rmspe\"\n        self._maximize = False\n\n    def __call__(self, y_true, y_score):\n        \n        return np.sqrt(np.mean(np.square((y_true - y_score) / y_true)))\n    \n\n\ndef RMSPELoss(y_pred, y_true):\n    return torch.sqrt(torch.mean( ((y_true - y_pred) / y_true) ** 2 )).clone()","metadata":{"execution":{"iopub.status.busy":"2021-09-27T11:38:47.53839Z","iopub.execute_input":"2021-09-27T11:38:47.539013Z","iopub.status.idle":"2021-09-27T11:38:47.548424Z","shell.execute_reply.started":"2021-09-27T11:38:47.538974Z","shell.execute_reply":"2021-09-27T11:38:47.54748Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def Tabnet_pred(train_df, test_df):\n    \n    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n    \n    train_df = train_df.fillna(0)\n    test_df = test_df.fillna(0)\n    \n    nunique = train_df[cont_cols+cat_cols].nunique()\n    types = train_df[cont_cols+cat_cols].dtypes\n\n    categorical_columns = []\n    categorical_dims =  {}\n\n    for col in cont_cols+cat_cols:\n        if  col == 'stock_id':\n            l_enc = LabelEncoder()\n            train_df[col] = l_enc.fit_transform(train_df[col].values)\n            test_df[col] = l_enc.transform(test_df[col].values)\n            categorical_columns.append(col)\n            categorical_dims[col] = len(l_enc.classes_)\n        else:\n            scaler = StandardScaler()\n            train_df[col] = scaler.fit_transform(train_df[col].values.reshape(-1, 1))\n            test_df[col] = scaler.transform(test_df[col].values.reshape(-1, 1))\n\n\n    cat_idxs = [ i for i, f in enumerate(train_df[cont_cols+cat_cols].columns.tolist()) if f in categorical_columns]\n    cat_dims = [ categorical_dims[f] for i, f in enumerate(train_df[cont_cols+cat_cols].columns.tolist()) if f in categorical_columns]\n    \n    tabnet_params = dict(\n                    cat_idxs=cat_idxs,\n                    cat_dims=cat_dims,\n                    cat_emb_dim=1,\n                    n_d = 16,\n                    n_a = 16,\n                    n_steps = 2,\n                    gamma = 2,\n                    n_independent = 2,\n                    n_shared = 2,\n                    lambda_sparse = 0,\n                    optimizer_fn = Adam,\n                    optimizer_params = dict(lr = (2e-2)),\n                    mask_type = \"entmax\",\n                    scheduler_params = dict(T_0=100, T_mult=1, eta_min=1e-4, last_epoch=-1, verbose=False),\n                    scheduler_fn = CosineAnnealingWarmRestarts,\n                    seed = 42,\n                    verbose = 5, \n                    device_name = DEVICE)\n    \n    kfold = KFold(n_splits = 5, random_state = 42, shuffle = False)\n    # Create out of folds array\n    oof_predictions = np.zeros((train_df.shape[0], 1))\n    test_predictions = np.zeros(test_df.shape[0])\n    \n    for fold, (trn_ind, val_ind) in enumerate(kfold.split(train_df.index)):\n        print(f'Training fold {fold + 1}')\n        X_train, X_val = train_df.loc[trn_ind, cont_cols+cat_cols].values, train_df.loc[val_ind, cont_cols+cat_cols].values\n        y_train, y_val = train_df.loc[trn_ind, 'target'].values.reshape(-1,1), train_df.loc[val_ind, 'target'].values.reshape(-1,1)\n\n\n        clf =  TabNetRegressor(**tabnet_params)\n        clf.fit(\n          X_train, y_train,\n          eval_set=[(X_val, y_val)],\n          max_epochs = 100,\n          patience = 20,\n          batch_size = 1024*5, \n          virtual_batch_size = 128*5,\n          num_workers = 4,\n          drop_last = False,\n          eval_metric=[RMSPE],\n          loss_fn=RMSPELoss\n          )\n\n        saving_path_name = f\"./fold{fold}\"\n        saved_filepath = clf.save_model(saving_path_name)\n        \n        oof_predictions[val_ind] = clf.predict(X_val)\n        test_predictions+=clf.predict(test_df[cont_cols+cat_cols].values).flatten()/5\n    \n    #print(f'OOF score across folds: {rmspe(train_df['target'].values, oof_predictions)}')\n    \n    return test_predictions","metadata":{"execution":{"iopub.status.busy":"2021-09-27T11:38:47.549822Z","iopub.execute_input":"2021-09-27T11:38:47.550269Z","iopub.status.idle":"2021-09-27T11:38:47.56878Z","shell.execute_reply.started":"2021-09-27T11:38:47.550232Z","shell.execute_reply":"2021-09-27T11:38:47.567887Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tab_pred = Tabnet_pred(train_df, test_df)\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-09-27T11:38:47.570257Z","iopub.execute_input":"2021-09-27T11:38:47.572006Z","iopub.status.idle":"2021-09-27T11:38:47.579709Z","shell.execute_reply.started":"2021-09-27T11:38:47.571973Z","shell.execute_reply":"2021-09-27T11:38:47.57898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Predicting Keras","metadata":{}},{"cell_type":"code","source":"from tensorflow import keras\nfrom keras import backend as K\n\ndef root_mean_squared_per_error(y_true, y_pred):\n         return K.sqrt(K.mean(K.square( (y_true - y_pred)/ y_true )))\n    \nes = tf.keras.callbacks.EarlyStopping(\n    monitor='val_loss', patience=50, verbose=0,\n    mode='min',restore_best_weights=True)\n\nplateau = tf.keras.callbacks.ReduceLROnPlateau(\n    monitor='val_loss', factor=0.2, patience=10, verbose=0,\n    mode='min')\n\n\nout_train = pd.read_csv('../input/optiver-realized-volatility-prediction/train.csv')\nout_train = out_train.pivot(index='time_id', columns='stock_id', values='target')\n\n#out_train[out_train.isna().any(axis=1)]\nout_train = out_train.fillna(out_train.mean())\nout_train.head()\n\n\n# data separation based on knn ++\nnfolds = 5\nindex = []\ntotDist = []\nvalues = []\n# generates a matriz with the values of \nmat = out_train.values\n\nscaler = MinMaxScaler(feature_range=(-1, 1))\nmat = scaler.fit_transform(mat)\n\nnind = int(mat.shape[0]/nfolds) # number of individuals\n\n# adds index in the last column\nmat = np.c_[mat,np.arange(mat.shape[0])]\n\n\nlineNumber = np.random.choice(np.array(mat.shape[0]), size=nfolds, replace=False)\n\nlineNumber = np.sort(lineNumber)[::-1]\n\nfor n in range(nfolds):\n    totDist.append(np.zeros(mat.shape[0]-nfolds))\n\n# saves index\nfor n in range(nfolds):\n    values.append([lineNumber[n]])    \n\n\ns=[]\nfor n in range(nfolds):\n    s.append(mat[lineNumber[n],:])\n    \n    mat = np.delete(mat, obj=lineNumber[n], axis=0)\n\nfor n in range(nind-1):    \n\n    luck = np.random.uniform(0,1,nfolds)\n    \n    for cycle in range(nfolds):\n         # saves the values of index           \n\n        s[cycle] = np.matlib.repmat(s[cycle], mat.shape[0], 1)\n\n        sumDist = np.sum( (mat[:,:-1] - s[cycle][:,:-1])**2 , axis=1)   \n        totDist[cycle] += sumDist        \n                \n        # probabilities\n        f = totDist[cycle]/np.sum(totDist[cycle]) # normalizing the totdist\n        j = 0\n        kn = 0\n        for val in f:\n            j += val        \n            if (j > luck[cycle]): # the column was selected\n                break\n            kn +=1\n        lineNumber[cycle] = kn\n        \n        # delete line of the value added    \n        for n_iter in range(nfolds):\n            \n            totDist[n_iter] = np.delete(totDist[n_iter],obj=lineNumber[cycle], axis=0)\n            j= 0\n        \n        s[cycle] = mat[lineNumber[cycle],:]\n        values[cycle].append(int(mat[lineNumber[cycle],-1]))\n        mat = np.delete(mat, obj=lineNumber[cycle], axis=0)\n\n\nfor n_mod in range(nfolds):\n    values[n_mod] = out_train.index[values[n_mod]]","metadata":{"execution":{"iopub.status.busy":"2021-09-27T11:38:47.581102Z","iopub.execute_input":"2021-09-27T11:38:47.581543Z","iopub.status.idle":"2021-09-27T11:38:55.920662Z","shell.execute_reply.started":"2021-09-27T11:38:47.581456Z","shell.execute_reply":"2021-09-27T11:38:55.919734Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open('../input/d/lhagiimn/optiver-fastai-and-keras-models/colNames.pkl', 'rb') as fin:\n    colNames = pickle.load(fin)\n    \ncolNames = [f for f in colNames if f in list(train_df)]","metadata":{"execution":{"iopub.status.busy":"2021-09-27T11:38:55.922008Z","iopub.execute_input":"2021-09-27T11:38:55.922337Z","iopub.status.idle":"2021-09-27T11:38:55.953041Z","shell.execute_reply.started":"2021-09-27T11:38:55.922303Z","shell.execute_reply":"2021-09-27T11:38:55.952256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#colNames.remove('row_id')\ntrain_df.replace([np.inf, -np.inf], np.nan,inplace=True)\ntest_df.replace([np.inf, -np.inf], np.nan,inplace=True)\n\ntrain_df=train_df[['stock_id','time_id', 'row_id', 'target'] + colNames]\ntest_df=test_df[['stock_id','time_id', 'row_id'] + colNames]\ngc.collect()\n\nfor col in colNames:\n    qt = QuantileTransformer(random_state=21,n_quantiles=2000, output_distribution='normal')\n    train_df[col] = qt.fit_transform(train_df[[col]])\n    test_df[col] = qt.transform(test_df[[col]])    \n\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-09-27T11:38:55.955711Z","iopub.execute_input":"2021-09-27T11:38:55.95596Z","iopub.status.idle":"2021-09-27T11:39:35.477931Z","shell.execute_reply.started":"2021-09-27T11:38:55.955936Z","shell.execute_reply":"2021-09-27T11:39:35.477148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Aggregating some features\ntrain_df,test_df = create_agg_features(train_df, test_df)","metadata":{"execution":{"iopub.status.busy":"2021-09-27T11:39:35.481222Z","iopub.execute_input":"2021-09-27T11:39:35.481481Z","iopub.status.idle":"2021-09-27T11:39:56.118517Z","shell.execute_reply.started":"2021-09-27T11:39:35.481455Z","shell.execute_reply":"2021-09-27T11:39:56.117581Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vol_cols = ['log_return1_max_log_return', 'log_return1_realized_volatility', 'log_return2_max_log_return', \n'log_return2_realized_volatility', 'price_spread_mean', 'total_volume_sum', 'volume_imbalance_sum', \n'log_return1_max_log_return_500', 'log_return1_realized_volatility_500', 'log_return2_max_log_return_500', \n'log_return2_realized_volatility_500', 'log_return1_max_log_return_400', 'log_return1_realized_volatility_400',\n 'log_return2_max_log_return_400', 'log_return2_realized_volatility_400', 'log_return1_max_log_return_300', \n 'log_return1_realized_volatility_300', 'log_return2_max_log_return_300', 'log_return2_realized_volatility_300', \n 'log_return1_max_log_return_200', 'log_return1_realized_volatility_200', 'log_return2_max_log_return_200', \n 'log_return2_realized_volatility_200', 'log_return1_max_log_return_100', 'log_return1_realized_volatility_100', \n 'log_return2_max_log_return_100', 'log_return2_realized_volatility_100', 'trade_log_return_realized_volatility', \n 'trade_size_sum', 'trade_order_count_sum', 'trade_amount_sum', 'trade_log_return_realized_volatility_500', \n 'trade_size_sum_500', 'trade_order_count_sum_500', 'trade_log_return_realized_volatility_400', 'trade_size_sum_400',\n 'trade_order_count_sum_400', 'trade_log_return_realized_volatility_300', 'trade_size_sum_300', 'trade_order_count_sum_300',\n 'trade_log_return_realized_volatility_200', 'trade_size_sum_200', 'trade_order_count_sum_200',\n 'trade_log_return_realized_volatility_100', 'trade_size_sum_100', 'trade_order_count_sum_100']\n\n#Nearest Neighbor based Feature Generation\nprint('Before nearest neighbor:', train_df.shape)\n\nif os.path.isfile('../input/optiver-utils-files/neg_idx.pkl'):\n    with open('../input/optiver-utils-files/neg_idx.pkl', 'rb') as fin:\n        neg_idx = pickle.load(fin)\n    with open('../input/optiver-utils-files/pos_idx.pkl', 'rb') as fin:\n        pos_idx = pickle.load(fin)\nelse:\n    train_p = pd.read_csv('../input/optiver-realized-volatility-prediction/train.csv')\n    train_p = train_p.pivot(index='stock_id', columns='time_id', values='target')\n    train_p = train_p.fillna(0)\n\n    n=15\n    model = NearestNeighbors(n_neighbors = 112, metric='cosine')\n    model.fit(train_p)\n    distances, indices = model.kneighbors(train_p)\n\n    for i, col in enumerate(train_p.index):\n        indices[i, :] = (train_p.index[indices[i, :]].values)\n\n    pos_idx = {}\n    neg_idx = {}\n\n    for i, col in enumerate(train_p.index):\n        pos_idx[col] = list(indices[i][1:n])\n        neg_idx[col] = list(indices[i][-n:])\n\n    with open('pos_idx.pkl', 'wb') as handle:\n        pickle.dump(pos_idx, handle, protocol=pickle.HIGHEST_PROTOCOL)\n\n    with open('neg_idx.pkl', 'wb') as handle:\n        pickle.dump(neg_idx, handle, protocol=pickle.HIGHEST_PROTOCOL)\n\n    del train_p, indices, distances, model\n    gc.collect()\n\nmat = []\nmat_test = []\n\nfor stock in train_df['stock_id'].unique():\n    ind = pos_idx[stock]\n    newDf = train_df.loc[train_df['stock_id'].isin(ind)]\n    newDf = newDf[vol_cols+['time_id']].groupby(['time_id']).agg(np.nanmean)\n    newDf.columns = [x + \"_pos\" for x in newDf.columns]\n    newDf.loc[:, 'stock_id'] = stock\n    mat.append(newDf)\n\n    newDf = test_df.loc[test_df['stock_id'].isin(ind)]\n    newDf = newDf[vol_cols+['time_id']].groupby(['time_id']).agg(np.nanmean)\n    newDf.columns = [x + \"_pos\" for x in newDf.columns]\n    newDf['stock_id'] = stock\n    mat_test.append(newDf)\n\n\nmat = pd.concat(mat).reset_index()\nmat_test = pd.concat(mat_test).reset_index()\n\ntrain_df = train_df.merge(mat, how='left', on=['time_id', \"stock_id\"])\ntest_df = test_df.merge(mat_test, how='left', on=['time_id', \"stock_id\"])\n\nmat = []\nmat_test = []\n\nfor stock in train_df['stock_id'].unique():\n    ind = neg_idx[stock]\n    newDf = train_df.loc[train_df['stock_id'].isin(ind)]\n    newDf = newDf[vol_cols+['time_id']].groupby(['time_id']).agg(np.nanmean)\n    newDf.columns = [x + \"_neg\" for x in newDf.columns]\n    newDf.loc[:, 'stock_id'] = stock\n    mat.append(newDf)\n\n    newDf = test_df.loc[test_df['stock_id'].isin(ind)]\n    newDf = newDf[vol_cols+['time_id']].groupby(['time_id']).agg(np.nanmean)\n    newDf.columns = [x + \"_neg\" for x in newDf.columns]\n    newDf['stock_id'] = stock\n    mat_test.append(newDf)\n\nmat = pd.concat(mat).reset_index()\nmat_test = pd.concat(mat_test).reset_index()\n\ntrain_df = train_df.merge(mat, how='left', on=['time_id', \"stock_id\"])\ntest_df = test_df.merge(mat_test, how='left', on=['time_id', \"stock_id\"])\n\ndel mat, mat_test\ngc.collect()\n\nprint('After nearest neighbor:', train_df.shape)","metadata":{"execution":{"iopub.status.busy":"2021-09-27T11:39:56.120224Z","iopub.execute_input":"2021-09-27T11:39:56.120568Z","iopub.status.idle":"2021-09-27T11:40:51.438707Z","shell.execute_reply.started":"2021-09-27T11:39:56.120531Z","shell.execute_reply":"2021-09-27T11:40:51.437761Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cont_cols = list(train_df)\ncont_cols.remove('time_id')\ncont_cols.remove('target')\ncont_cols.remove('row_id')\n\ncont_cols = [f for f in cont_cols if f not in cat_cols]\n\nlen(cont_cols)","metadata":{"execution":{"iopub.status.busy":"2021-09-27T11:40:51.440006Z","iopub.execute_input":"2021-09-27T11:40:51.440541Z","iopub.status.idle":"2021-09-27T11:40:51.449224Z","shell.execute_reply.started":"2021-09-27T11:40:51.440487Z","shell.execute_reply":"2021-09-27T11:40:51.448452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.backend import sigmoid\nfrom keras import backend as K\n\ndef swish(x, beta = 1):\n    return (x * sigmoid(beta * x))\n\ndef mish(x, beta = 1):\n    return (x * K.tanh(K.softplus(x)))\n\nfrom keras.utils.generic_utils import get_custom_objects\nfrom keras.layers import Activation\nget_custom_objects().update({'swish': Activation(swish)})\nget_custom_objects().update({'mish': Activation(mish)})\n\nhidden_units = (256, 128, 64)\nstock_embedding_size = 24\n\ncat_data = train_df['stock_id']\n\ndef base_model():\n    \n    # Each instance will consist of two inputs: a single user id, and a single movie id\n    stock_id_input = keras.Input(shape=(1,), name='stock_id')\n    num_input = keras.Input(shape=(len(cont_cols),), name='num_data')\n\n\n    #embedding, flatenning and concatenating\n    stock_embedded = keras.layers.Embedding(max(cat_data)+1, stock_embedding_size, \n                                           input_length=1, name='stock_embedding')(stock_id_input)\n    stock_flattened = keras.layers.Flatten()(stock_embedded)\n    out = keras.layers.Concatenate()([stock_flattened, num_input])\n    \n    # Add one or more hidden layers\n    for n_hidden in hidden_units:\n\n        out = keras.layers.Dense(n_hidden, activation='swish')(out)\n        \n\n    #out = keras.layers.Concatenate()([out, num_input])\n\n    # A single output: our predicted rating\n    out = keras.layers.Dense(1, activation='linear', name='prediction')(out)\n    \n    model = keras.Model(\n    inputs = [stock_id_input, num_input],\n    outputs = out,\n    )\n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2021-09-27T11:41:01.755512Z","iopub.execute_input":"2021-09-27T11:41:01.755852Z","iopub.status.idle":"2021-09-27T11:41:01.767692Z","shell.execute_reply.started":"2021-09-27T11:41:01.755822Z","shell.execute_reply":"2021-09-27T11:41:01.76656Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to calculate the root mean squared percentage error\ndef rmspe(y_true, y_pred):\n    return np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))","metadata":{"execution":{"iopub.status.busy":"2021-09-27T11:41:03.876934Z","iopub.execute_input":"2021-09-27T11:41:03.877254Z","iopub.status.idle":"2021-09-27T11:41:03.881772Z","shell.execute_reply.started":"2021-09-27T11:41:03.877226Z","shell.execute_reply":"2021-09-27T11:41:03.88037Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ntarget_name='target'\nscores_folds = {}\nmodel_name = 'NN'\npred_name = 'pred_{}'.format(model_name)\n\nn_folds = 5\nkf = model_selection.KFold(n_splits=n_folds, shuffle=True, random_state=2020)\nscores_folds[model_name] = []\ncounter = 1\n\nfeatures_to_consider = cont_cols + ['stock_id']\n\n\ntrain_df[features_to_consider] = train_df[features_to_consider].fillna(train_df[features_to_consider].mean()).fillna(0)\ntest_df[features_to_consider] = test_df[features_to_consider].fillna(train_df[features_to_consider].mean()).fillna(0)\n\ntrain_df[pred_name] = 0\ntest_df[target_name] = 0\ntest_predictions_nn = np.zeros(test_df.shape[0])\n\nfor n_count in range(n_folds):\n    print('CV {}/{}'.format(counter, n_folds))\n    \n    indexes = np.arange(nfolds).astype(int)    \n    indexes = np.delete(indexes,obj=n_count, axis=0) \n    \n    indexes = np.r_[values[indexes[0]],values[indexes[1]],values[indexes[2]],values[indexes[3]]]\n    \n    X_train = train_df.loc[train_df.time_id.isin(indexes), features_to_consider]\n    y_train = train_df.loc[train_df.time_id.isin(indexes), target_name]\n    X_test = train_df.loc[train_df.time_id.isin(values[n_count]), features_to_consider]\n    y_test = train_df.loc[train_df.time_id.isin(values[n_count]), target_name]\n    \n    #############################################################################################\n    # NN\n    #############################################################################################\n    \n    model = base_model()\n    \n    model.compile(\n        keras.optimizers.Adam(learning_rate=0.005),\n        loss=root_mean_squared_per_error\n    )\n    \n    try:\n        features_to_consider.remove('stock_id')\n    except:\n        pass\n    \n    num_data = X_train[features_to_consider]\n    \n    scaler = MinMaxScaler(feature_range=(-1, 1))         \n    num_data = scaler.fit_transform(num_data.values)    \n    \n    cat_data = X_train['stock_id']    \n    target =  y_train\n    \n    num_data_test = X_test[features_to_consider]\n    num_data_test = scaler.transform(num_data_test.values)\n    cat_data_test = X_test['stock_id']\n    \n    if os.path.isfile(f'../input/optiver-keras-models/model_nn_{counter}.h5'):\n        model =  keras.models.load_model(f'../input/optiver-keras-models/model_nn_{counter}.h5', \n                                         custom_objects={'swish': swish, 'Activation': Activation, \n                                                         'root_mean_squared_per_error':root_mean_squared_per_error})\n        \n    else:\n\n        model.fit([cat_data.values, num_data], \n                  target.values,               \n                  batch_size=2048,\n                  epochs=1000,\n                  validation_data=([cat_data_test.values, num_data_test], y_test.values),\n                  callbacks=[es, plateau],\n                  validation_batch_size=len(y_test),\n                  shuffle=True, verbose = 1)\n\n        model.save(f\"model_nn_{counter}.h5\")\n\n    preds = model.predict([cat_data_test.values, num_data_test]).reshape(1,-1)[0]\n    \n    score = round(rmspe(y_true = y_test, y_pred = preds),5)\n    print('Fold {} {}: {}'.format(counter, model_name, score))\n    scores_folds[model_name].append(score)\n    \n    tt =scaler.transform(test_df[features_to_consider].values)\n    test_predictions_nn += model.predict([test_df['stock_id'], tt]).reshape(1,-1)[0]/n_folds\n  \n    counter += 1\n    features_to_consider.append('stock_id')\n    ","metadata":{"execution":{"iopub.status.busy":"2021-09-27T11:44:41.575413Z","iopub.execute_input":"2021-09-27T11:44:41.575778Z","iopub.status.idle":"2021-09-27T11:46:35.328509Z","shell.execute_reply.started":"2021-09-27T11:44:41.575747Z","shell.execute_reply":"2021-09-27T11:46:35.326133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_predictions_nn","metadata":{"execution":{"iopub.status.busy":"2021-09-27T11:40:59.021405Z","iopub.status.idle":"2021-09-27T11:40:59.022124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"weights = [0.30, 0.05, 0.05, 0.10, 0.15, 0.35]\npreds = [pred_lgb0, pred_lgb1, pred_cat0, pred_nn0, tab_pred, test_predictions_nn]\n\npred = np.zeros(test_df.shape[0])\nfor w, p in zip(weights, preds):\n    pred +=w*p\n\ntest_df['target'] = pred\n\ngroup1 = [1, 2, 10, 13, 14, 15, 17, 20, 22, 23, 26, 29, 32, 34, 35, 36, 39, 41, 43, 44, 46, 47, 48, 50, 51, 52,\n          53, 56, 59, 61, 62, 63, 64, 67, 68, 69, 70, 73, 76, 77, 84, 87, 93, 95, 96, 100, 101, 104, 105, 107, 108,\n          109, 111, 113, 114, 119, 120, 122, 123, 124, 125]\n\ngroup2 = [3, 6, 7, 8, 11, 19, 21, 28, 38, 42, 55, 66, 72, 74, 78, 82, 85, 86, 94, 99, 102, 115, 116, 118, 126]\ngroup3 = [0, 4, 5, 9, 16, 30, 40, 58, 75, 83, 89, 90, 97, 98, 103, 112, 18, 27, 33, 37, 60, 80, 81, 88, 110]\ngroup4 = [31]\n\n# test_df.loc[test_df.stock_id.isin(group1), 'target'] = test_df.loc[test_df.stock_id.isin(group1), 'target'].values * 1.00\n# test_df.loc[test_df.stock_id.isin(group2), 'target'] = test_df.loc[test_df.stock_id.isin(group2), 'target'].values * 0.995\n# test_df.loc[test_df.stock_id.isin(group3), 'target'] = test_df.loc[test_df.stock_id.isin(group3), 'target'].values * 0.99\n# test_df.loc[test_df.stock_id.isin(group4), 'target'] = test_df.loc[test_df.stock_id.isin(group4), 'target'].values * 0.775\n\ntest_df[['row_id', 'target']].to_csv('submission.csv',index = False)","metadata":{"execution":{"iopub.status.busy":"2021-09-27T11:40:59.02326Z","iopub.status.idle":"2021-09-27T11:40:59.023965Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df[['row_id', 'target']].head()","metadata":{"execution":{"iopub.status.busy":"2021-09-27T11:40:59.02502Z","iopub.status.idle":"2021-09-27T11:40:59.025732Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}