{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport torch\nfrom torch import nn\nfrom torch.nn.functional import mse_loss\nfrom torch.optim.adam import Adam\nfrom tqdm import tqdm\nimport pandas as pd\nimport numpy as np\n\n\nclass EarlyStop:\n    STOP = 0\n    CONTINUE = 1\n\n    def __init__(self, patience=5):\n        self.max_patience = patience\n        self.current_patience = 0\n        self.min_loss = 2000000000\n\n    def count(self, loss):\n        if self.min_loss > loss:\n            self.min_loss = loss\n            self.current_patience = 0\n            return EarlyStop.CONTINUE\n        else:\n            self.current_patience += 1\n            if self.current_patience > self.max_patience:\n                return EarlyStop.STOP\n            else:\n                return EarlyStop.CONTINUE\n\n    def reset(self):\n        self.current_patience = 0\n        self.min_loss = 2000000000\n\n\nclass LSTM(nn.Module):\n    def __init__(self, input_size=4, hidden_layer_size=128, output_size=4):\n        super().__init__()\n        self.hidden_layer_size = hidden_layer_size\n\n        self.lstm = nn.LSTM(input_size, hidden_layer_size)\n\n        self.linear_0 = nn.Linear(hidden_layer_size, hidden_layer_size // 2)\n        self.relu = nn.LeakyReLU()\n        self.linear_1 = nn.Linear(hidden_layer_size // 2, hidden_layer_size // 4)\n        self.relu2 = nn.LeakyReLU()\n        self.linear_2 = nn.Linear(hidden_layer_size // 4, output_size)\n        self.sigmoid = nn.Sigmoid()\n        self.hidden_cell = (torch.zeros(1, 1, self.hidden_layer_size),\n                            torch.zeros(1, 1, self.hidden_layer_size))\n\n    def forward(self, input_seq):\n        lstm_out, self.hidden_cell = self.lstm(input_seq.view(len(input_seq), 1, -1), self.hidden_cell)\n        predictions = self.linear_0(lstm_out.view(len(input_seq), -1))\n        predictions = self.relu(predictions)\n        predictions = self.linear_1(predictions)\n        predictions = self.relu2(predictions)\n        predictions = self.linear_2(predictions)\n        predictions = self.sigmoid(predictions)\n        return predictions[-1]\n\n\ndef make_dataset(stock_train_file, sequence_length, step=16):\n    stock_book = pd.read_parquet(stock_train_file)\n\n    time_ids = stock_book.groupby('time_id')\n    sequence_data = []\n    labels = []\n    for item in tqdm(time_ids):\n        time_id = item[0]\n        time_id_data = item[1]\n        num_trade = time_id_data.shape[0]\n        for i in range(0, num_trade - sequence_length, step):\n            sequence = time_id_data[['bid_price1', 'ask_price1', 'bid_size1', 'ask_size1']][\n                       i:i + sequence_length].astype(float).to_numpy()\n            label = time_id_data[['bid_price1', 'ask_price1', 'bid_size1', 'ask_size1']][\n                    i + sequence_length:i + sequence_length + 1].astype(float).to_numpy()\n            sequence_data.append(sequence)\n            labels.append(label)\n    return np.array(sequence_data), np.array(labels)\n\n\ndef get_stock_file(root_data, stock):\n    dir = os.path.join(root_data, \"stock_id=\" + str(stock))\n    file_path = os.listdir(dir)[0]\n    return os.path.join(dir, file_path)\n\n\ndef validate(model, val_data, device):\n    max_price = max(val_data[0][:, :, 0:2].max(), val_data[1][:, :, 0:2].max())\n    min_price = min(val_data[0][:, :, 0:2].min(), val_data[1][:, :, 0:2].min())\n    max_size = max(val_data[0][:, :, 2:4].max(), val_data[1][:, :, 2:4].max())\n    min_size = min(val_data[0][:, :, 2:4].min(), val_data[1][:, :, 2:4].min())\n\n    val_data[0][:, :, 0:2] = (val_data[0][:, :, 0:2] - min_price) / (max_price - min_price)\n    val_data[1][:, :, 0:2] = (val_data[1][:, :, 0:2] - min_price) / (max_price - min_price)\n    val_data[0][:, :, 2:4] = (val_data[0][:, :, 2:4] - min_size) / (max_size - min_size)\n    val_data[1][:, :, 2:4] = (val_data[1][:, :, 2:4] - min_size) / (max_size - min_size)\n\n    # val step\n    model.eval()\n    data_len = val_data[0].shape[0]\n    val_loss = 0\n    for i in tqdm(range(data_len)):\n        seq = val_data[0][i]\n        labels = val_data[1][i]\n        seq = torch.from_numpy(seq).type(torch.FloatTensor).to(device)\n        labels = torch.from_numpy(labels).type(torch.FloatTensor).to(device)\n        model.hidden_cell = (torch.zeros(1, 1, model.hidden_layer_size).to(device),\n                             torch.zeros(1, 1, model.hidden_layer_size).to(device))\n\n        y_pred = model(seq)\n        single_loss = loss_function(y_pred, labels.view(4))\n        val_loss += single_loss.item()\n    return val_loss\n\n\ndef get_stocks(train):\n    return np.unique(train['stock_id'])\n\n\nif __name__ == \"__main__\":\n    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n    print(f\"Working on {device}\")\n    root_data = \"../input/optiver-realized-volatility-prediction/book_train.parquet\"\n    train = pd.read_csv('../input/optiver-realized-volatility-prediction/train.csv')\n\n    epochs = 150  # for each stock\n    sequence_length = 100\n    train_ratio = 0.75\n    lstm_hidden_size = 128\n    input_size = 4\n    output_size = 4\n    learning_rate = 0.0001\n    max_patience = 7\n    data_step = 200\n    early_stop = EarlyStop(max_patience)\n\n    stocks = get_stocks(train)\n    sequence_data = np.ones((0, sequence_length, input_size))\n    labels = np.ones((0, 1, input_size))\n    for stock in stocks:\n        stock_book_file = get_stock_file(root_data, stock)\n        a_sequence_data, a_label = make_dataset(stock_book_file, sequence_length, step=data_step)\n        sequence_data = np.vstack((sequence_data, a_sequence_data))\n        labels = np.vstack((labels, a_label))\n        \n    rand_indices = np.arange(0, sequence_data.__len__())\n    np.random.shuffle(rand_indices)\n\n    sequence_data = sequence_data[rand_indices]\n    labels = labels[rand_indices]\n\n    train_data = sequence_data[:int(train_ratio * sequence_data.__len__())], labels[:int(\n        train_ratio * sequence_data.__len__())]\n    val_data = sequence_data[int(train_ratio * sequence_data.__len__()):], labels[int(\n        train_ratio * sequence_data.__len__()):]\n\n    model = LSTM(input_size, lstm_hidden_size, output_size).to(device)\n    optimizer = Adam(model.parameters(), lr=learning_rate)\n    loss_function = mse_loss\n\n    # scale the train data only\n    max_price = max(train_data[0][:, :, 0:2].max(), train_data[1][:, :, 0:2].max())\n    min_price = min(train_data[0][:, :, 0:2].min(), train_data[1][:, :, 0:2].min())\n    max_size = max(train_data[0][:, :, 2:4].max(), train_data[1][:, :, 2:4].max())\n    min_size = min(train_data[0][:, :, 2:4].min(), train_data[1][:, :, 2:4].min())\n\n    train_data[0][:, :, 0:2] = (train_data[0][:, :, 0:2] - min_price) / (max_price - min_price)\n    train_data[1][:, :, 0:2] = (train_data[1][:, :, 0:2] - min_price) / (max_price - min_price)\n    train_data[0][:, :, 2:4] = (train_data[0][:, :, 2:4] - min_size) / (max_size - min_size)\n    train_data[1][:, :, 2:4] = (train_data[1][:, :, 2:4] - min_size) / (max_size - min_size)\n\n    best_val_loss = 1999999999\n    early_stop.reset()\n    for e in range(epochs):\n        # train step\n        model.train()\n        train_loss = 0\n        data_len = train_data[0].shape[0]\n        for i in tqdm(range(data_len)):\n            seq = train_data[0][i]\n            labels = train_data[1][i]\n            seq = torch.from_numpy(seq).type(torch.FloatTensor).to(device)\n            labels = torch.from_numpy(labels).type(torch.FloatTensor).to(device)\n            optimizer.zero_grad()\n            model.hidden_cell = (torch.zeros(1, 1, model.hidden_layer_size).to(device),\n                                 torch.zeros(1, 1, model.hidden_layer_size).to(device))\n\n            y_pred = model(seq)\n\n            single_loss = loss_function(y_pred, labels.view(4))\n            train_loss += single_loss.item()\n            single_loss.backward()\n            optimizer.step()\n        print(f'epoch: {e:3} train loss: {train_loss:10.8f}')\n        # val step\n        val_loss = validate(model, val_data, device)\n        action = early_stop.count(val_loss)\n        if action == EarlyStop.STOP:\n            break  # stop training\n        print(f'epoch: {e:3} val loss: {val_loss:10.8f}')\n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            torch.save(model.state_dict(), f\"model_best_all.pth\")\n    debug = 1\n","metadata":{"execution":{"iopub.status.busy":"2021-07-15T12:14:45.678458Z","iopub.execute_input":"2021-07-15T12:14:45.678797Z","iopub.status.idle":"2021-07-15T16:14:28.106164Z","shell.execute_reply.started":"2021-07-15T12:14:45.678767Z","shell.execute_reply":"2021-07-15T16:14:28.104458Z"},"trusted":true},"execution_count":null,"outputs":[]}]}