{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Understanding Optiver Realized Volatility Prediction ","metadata":{}},{"cell_type":"markdown","source":"**Welcome fellow Kaggler, I know you might have got bored of looking at everyone's notebook trying to explain what's going on. And I know I am too late to this party, but I am one of those who take a lot more time to understand anything. So if you still have some doubts left and want to understand this competition and its data, I have tried my best to explain. Optiver was kind enough to provide us with a basic notebook structure it was really good to develop understanding, but I felt the language was not very easy, so I removed the useless part of it, made it simple, and also explained some of the terms and features in simple words. This is my first public notebook, so please be supportive \"but\" I am ready to take questions (to answer them and learn from them)**.","metadata":{}},{"cell_type":"markdown","source":"## Note: \n**Please read the commented lines also to understand the code line by line, I have tried to be as thorough as possible.**","metadata":{}},{"cell_type":"markdown","source":"**Required Libraries**","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os\nimport glob\nfrom IPython.display import Image\nimport plotly.express as px\nfrom sklearn.metrics import r2_score","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Important Terms**","metadata":{}},{"cell_type":"markdown","source":"**Try to understand , but as Lord Ng says**","metadata":{}},{"cell_type":"code","source":"Image(filename=\"../input/optiver2/andrewng.jpeg\")","metadata":{"execution":{"iopub.status.busy":"2021-07-05T00:52:03.283548Z","iopub.execute_input":"2021-07-05T00:52:03.28398Z","iopub.status.idle":"2021-07-05T00:52:03.295293Z","shell.execute_reply.started":"2021-07-05T00:52:03.283947Z","shell.execute_reply":"2021-07-05T00:52:03.294699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1. Bid/Ask spread\n\nAs different stocks trade on different level on the market we take the ratio of best offer price and best bid price to calculate the bid-ask spread. Best prices corresponds to Level 1 data (the least ask price, the highest bid price in order book), scroll down & look for order_book image to understand more.\n\n$$BidAskSpread = BestOffer / BestBid -1$$\n\n## 2. Weighted averaged price\n\nThe order book is also one of the primary source for stock valuation. A fair book-based valuation must take two factors into account: the level and the size(volume) of orders. In this competition we used weighted averaged price, or WAP, to calculate the instantaneous stock valuation(price) and calculate realized volatility as our target. \n\nThe formula of WAP can be written as below, which takes the top level price and volume information into account:\n\n$$ WAP = \\frac{BidPrice_{1}*AskSize_{1} + AskPrice_{1}*BidSize_{1}}{BidSize_{1} + AskSize_{1}} $$\n\nAs you can see, if two books have both bid and ask offers on the same price level respectively, the one with more sell offers in place will generate a lower stock valuation, as there are more intended seller in the book, and more seller implies a fact of more supply on the market resulting in a lower stock valuation.\n\n*Point to note, if a trade occurs its entry is not stored in Order Book(It will be available in trade book).\n\n## 3. Log returns\n\n**How can we compare the price of a stock between yesterday and today?**\n\n**stock return** - Example, the return for stock A will be given by $\\frac{\\$102 - \\$100 }{\\$100} = 2\\%$, Where,  102 is  price at  t  and  100  is price at  t-1.\n\n**log returns** - These are preferred whenever some mathematical modelling is required. Calling $S_t$ the price of the stock $S$ at time $t$, we can define the log return between $t_1$ and $t_2$ as:\n$$\nr_{t_1, t_2} = \\log \\left( \\frac{S_{t_2}}{S_{t_1}} \\right)\n$$\nUsually, we look at log returns over fixed time intervals, so with 10-minute log return we mean $r_t = r_{t - 10 min, t}$.\n\nLog returns present several advantages, for example:\n- they are additive across time $r_{t_1, t_2} + r_{t_2, t_3} = r_{t_1, t_3}$\n- regular returns cannot go below -100%, while log returns are not bounded\n\n## 4. Realized volatility\n\nWe will compute the log returns over all consecutive book updates and we define the **realized volatility, $\\sigma$,** as the squared root of the sum of squared log returns.\n\n$$\n\\sigma = \\sqrt{\\sum_{t}r_{t-1, t}^2}\n$$\n\nWhere we use **WAP** as price of the stock to compute log returns. Scroll to **Example section** to learn more about WAP.\n\n**Note** : The data is not annualizing the volatility (which is how it is actually calculated when you need Realized Volatility) and it is assumed that log returns have 0 mean => So basically, do not go for calculating Standard Deviation and all. The above formula is what you need here.","metadata":{}},{"cell_type":"markdown","source":"# OKAY LET'S START WITH WHAT IS GIVEN","metadata":{}},{"cell_type":"markdown","source":"**To read Trade and Order book for stock_id = 0**","metadata":{}},{"cell_type":"code","source":"book_example = pd.read_parquet('../input/optiver-realized-volatility-prediction/book_train.parquet/stock_id=0')\ntrade_example =  pd.read_parquet('../input/optiver-realized-volatility-prediction/trade_train.parquet/stock_id=0')\nstock_id = '0'\nbook_example = book_example[book_example['time_id']==5]\nbook_example.loc[:,'stock_id'] = stock_id\ntrade_example = trade_example[trade_example['time_id']==5]\ntrade_example.loc[:,'stock_id'] = stock_id","metadata":{"execution":{"iopub.status.busy":"2021-07-05T00:52:03.303378Z","iopub.execute_input":"2021-07-05T00:52:03.303811Z","iopub.status.idle":"2021-07-05T00:52:03.596227Z","shell.execute_reply.started":"2021-07-05T00:52:03.303769Z","shell.execute_reply":"2021-07-05T00:52:03.595215Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Order Book Data","metadata":{}},{"cell_type":"markdown","source":"This is the snapshot of a moment of running market , where each second thousands or maybe millions of prices and volumes are quoted by buyers and sellers. At a given moment it shows current availability of buy/sell opportunity. If it is filled neck to neck , the order book is called liquid ,but if **Bid/Ask spread** is large -> it is less liquid, which means it is a bad order book as not many sellers/buyers available. So if you are a buyer, you will need to go at much higher price and vice-versa if you are seller(as the gap is large) to make a trade.","metadata":{}},{"cell_type":"code","source":"book_example.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-05T00:52:03.597663Z","iopub.execute_input":"2021-07-05T00:52:03.597999Z","iopub.status.idle":"2021-07-05T00:52:03.614286Z","shell.execute_reply.started":"2021-07-05T00:52:03.597963Z","shell.execute_reply":"2021-07-05T00:52:03.613321Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**What are these features?** \n\n**1.   [bid_price1, ask_price1, bid_price2, ask_price2, bid_size1, ask_size1, bid_size2, ask_size2]**","metadata":{}},{"cell_type":"code","source":"Image(filename=\"../input/optiver/OrderBook3 - Copy.png\")","metadata":{"execution":{"iopub.status.busy":"2021-07-05T00:52:03.615618Z","iopub.execute_input":"2021-07-05T00:52:03.616005Z","iopub.status.idle":"2021-07-05T00:52:03.631618Z","shell.execute_reply.started":"2021-07-05T00:52:03.615966Z","shell.execute_reply":"2021-07-05T00:52:03.630833Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**The immediate rows adjacent to boundary between Bidder(Bid) and Seller(ask) are level1(which gives best offer price and best bid price,and further used to calculate bid/ask spread - as mentioned in tutorial notebook) and the next adjacents are level 2,This is what Level 1 and level 2 depth means here. But as you can see, there can be many such levels if the order book is liquid and has lot of buyer/sellers. But here only two immediates are given.**","metadata":{}},{"cell_type":"markdown","source":"**2.  [time_id,  seconds_in_bucket]**\n\n* **time_id** : It is basically a group of 10 min(which refers to the orderbook snapshot during those 10 min, which are further divided into seconds represented by seconds_in_bucket), which is identified with the integers they are assigned like- 0,1,2..so on. So what that means , group 0 is the first 10 min group and then other groups in ascending order.\n\n* **seconds_in_bucket(sib)** : refers to the seconds in those 10 min of a time_id. So it shows order book's snapshot of **i-1**th second. ","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('../input/optiver-realized-volatility-prediction/train.csv')\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-05T00:52:03.632891Z","iopub.execute_input":"2021-07-05T00:52:03.633376Z","iopub.status.idle":"2021-07-05T00:52:03.860452Z","shell.execute_reply.started":"2021-07-05T00:52:03.633339Z","shell.execute_reply":"2021-07-05T00:52:03.859623Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Taking the first row of data, it implies that the realized vol of the **target bucket** for time_id 5, stock_id 0 is 0.004136.","metadata":{}},{"cell_type":"markdown","source":"# **Trade data**","metadata":{}},{"cell_type":"markdown","source":"This data contains information about trade that actually occured between the above order book data. So you might think that - Ok, so trade data is basically like subset of order_book -> Yes and No.\n\nIt can be seen as subset as it occured within the time_id(10 min blocks) range. But the point to note is - if at a particular moment a trade occured , it is not saved in order book(as it is a book to maintain available buyers/sellers and what they are offering in the market, like no. of stocks, at what price etc.), so basically you will never see trade data in order book, you can just see the volumes decreasing/increasing as someone buy/sell the stocks or options.","metadata":{}},{"cell_type":"code","source":"trade_example.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-05T00:52:03.861523Z","iopub.execute_input":"2021-07-05T00:52:03.861803Z","iopub.status.idle":"2021-07-05T00:52:03.872589Z","shell.execute_reply.started":"2021-07-05T00:52:03.861776Z","shell.execute_reply":"2021-07-05T00:52:03.871653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Let's discuss what are these features ->**\n\n* **price** : It is the aggregated mean of the prices of all the trades that occured during last second, ex- from 20-21 sec will be shown at 21'th second.\n\n* **size** : It is the aggregated sum of the volumes traded during last second.\n\n* **order_count** : This corresponds to number transactions or trades occured during last second.\n","metadata":{}},{"cell_type":"markdown","source":"# Functions used in this notebook","metadata":{}},{"cell_type":"code","source":"#Function returns log_return value of the given feature\ndef log_return(list_stock_prices):\n    return np.log(list_stock_prices).diff() \n\n#Calculate Realized Volatility (outputs a aggregated value) for given feature\ndef realized_volatility(series_log_return):\n    return np.sqrt(np.sum(series_log_return**2))\n\n#This function uses above function(as agg function) to calculate Volatility per time_id for a given stock_id\ndef realized_volatility_per_time_id(file_path, prediction_column_name):\n    df_book_data = pd.read_parquet(file_path)\n    \n    # As I said you can play with WAP (be little sensible while doing so)\n    #wap1\n    df_book_data['wap1'] = calculate_wap1(df_book_data)\n    df_book_data['log_return1'] = df_book_data.groupby(['time_id'])['wap1'].apply(log_return)\n    df_book_data = df_book_data[~df_book_data['log_return1'].isnull()]\n    #wap2\n    df_book_data['wap2'] = calculate_wap2(df_book_data)\n    df_book_data['log_return2'] = df_book_data.groupby(['time_id'])['wap2'].apply(log_return)\n    df_book_data = df_book_data[~df_book_data['log_return2'].isnull()]\n    # final log_return_price from log_return1 & log_return2\n    df_book_data['final_log_return'] = 0.6*df_book_data['log_return1'] + 0.4*df_book_data['log_return2']\n    # Using final_returns to calculate predicted volatility for each time_id by agg. it using \"realized_volatility\" function\n    df_realized_vol_per_stock =  pd.DataFrame(df_book_data.groupby(['time_id'])['final_log_return'].agg(realized_volatility)).reset_index()\n    df_realized_vol_per_stock = df_realized_vol_per_stock.rename(columns = {'final_log_return':prediction_column_name})\n    #For a given stock_id\n    stock_id = file_path.split('=')[1]\n    # Stock_id_time_id -> submission format (AS THEY SAY IN MANDALORIAN , THIS IS THE WAY)\n    df_realized_vol_per_stock['row_id'] = df_realized_vol_per_stock['time_id'].apply(lambda x:f'{stock_id}-{x}')\n    return df_realized_vol_per_stock[['row_id',prediction_column_name]]\n\n#Loop through stock_id's to calculate  Volatility per stock\ndef past_realized_volatility_per_stock(list_file,prediction_column_name):\n    df_past_realized = pd.DataFrame()\n    for file in list_file:\n        df_past_realized = pd.concat([df_past_realized, realized_volatility_per_time_id(file, prediction_column_name)])\n    return df_past_realized\n\n#RMSPE\ndef rmspe(y_true, y_pred):\n    return  (np.sqrt(np.mean(np.square((y_true - y_pred) / y_true))))\n\n#WAP1\ndef calculate_wap1(df):\n    a1 = df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']\n    b1 = df['bid_size1'] + df['ask_size1']\n    a2 = df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2']\n    b2 = df['bid_size2'] + df['ask_size2']\n    \n    x = (a1/b1 + a2/b2)/ 2\n    \n    return x\n\n#WAP2\ndef calculate_wap2(df):\n        \n    a1 = df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']\n    a2 = df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2']\n    b = df['bid_size1'] + df['ask_size1'] + df['bid_size2']+ df['ask_size2']\n    \n    x = (a1 + a2)/ b\n    return x","metadata":{"execution":{"iopub.status.busy":"2021-07-05T00:52:03.875158Z","iopub.execute_input":"2021-07-05T00:52:03.875406Z","iopub.status.idle":"2021-07-05T00:52:03.891507Z","shell.execute_reply.started":"2021-07-05T00:52:03.875383Z","shell.execute_reply":"2021-07-05T00:52:03.890899Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Example Section ","metadata":{}},{"cell_type":"markdown","source":"**Realized volatility calculation and more about WAP**","metadata":{}},{"cell_type":"markdown","source":"In this competition, our target is to predict short-term realized volatility.\nAs realized volatility is a statistical measure of price changes on a given stock, to calculate the price change we first need to have a stock price at each second( as the data is given for seconds). We will use weighted averaged price, or **WAP**, of the given order book data. You can play with the level1 , level 2 features as the aim of WAP is to show affect of randomness or agrression or noise, due to quoted price and volume that got listed in market at a given point of time.","metadata":{}},{"cell_type":"code","source":"#Given formula for WAP\nbook_example['wap'] = (book_example['bid_price1'] * book_example['ask_size1'] +\n                                book_example['ask_price1'] * book_example['bid_size1']) / (\n                                       book_example['bid_size1']+ book_example['ask_size1'])","metadata":{"execution":{"iopub.status.busy":"2021-07-05T00:52:03.892634Z","iopub.execute_input":"2021-07-05T00:52:03.893006Z","iopub.status.idle":"2021-07-05T00:52:03.91Z","shell.execute_reply.started":"2021-07-05T00:52:03.892981Z","shell.execute_reply":"2021-07-05T00:52:03.908919Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**The WAP of the stock is plotted below**","metadata":{}},{"cell_type":"code","source":"fig = px.line(book_example, x=\"seconds_in_bucket\", y=\"wap\", title='WAP of stock_id_0, time_id_5')\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-05T00:52:03.911204Z","iopub.execute_input":"2021-07-05T00:52:03.911678Z","iopub.status.idle":"2021-07-05T00:52:04.001538Z","shell.execute_reply.started":"2021-07-05T00:52:03.911642Z","shell.execute_reply":"2021-07-05T00:52:04.000636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To compute the log return, we can simply take **the logarithm of the ratio** between two consecutive(rows) **WAP**. The first row will have an empty return as the previous book update is unknown, therefore the empty return data point will be dropped.\n\n**Note** : We already have function for log_return in **Functions** section.","metadata":{}},{"cell_type":"code","source":"book_example.loc[:,'log_return'] = log_return(book_example['wap'])\nbook_example = book_example[~book_example['log_return'].isnull()]","metadata":{"execution":{"iopub.status.busy":"2021-07-05T00:52:04.002878Z","iopub.execute_input":"2021-07-05T00:52:04.00322Z","iopub.status.idle":"2021-07-05T00:52:04.011277Z","shell.execute_reply.started":"2021-07-05T00:52:04.003191Z","shell.execute_reply":"2021-07-05T00:52:04.010194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Let's plot the tick-to-tick return of this instrument over this time bucket**","metadata":{"execution":{"iopub.status.busy":"2021-06-09T15:01:53.679074Z","iopub.execute_input":"2021-06-09T15:01:53.679605Z","iopub.status.idle":"2021-06-09T15:01:53.686279Z","shell.execute_reply.started":"2021-06-09T15:01:53.67957Z","shell.execute_reply":"2021-06-09T15:01:53.684738Z"}}},{"cell_type":"code","source":"fig = px.line(book_example, x=\"seconds_in_bucket\", y=\"log_return\", title='Log return of stock_id_0, time_id_5')\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-05T00:52:04.012416Z","iopub.execute_input":"2021-07-05T00:52:04.012667Z","iopub.status.idle":"2021-07-05T00:52:04.080452Z","shell.execute_reply.started":"2021-07-05T00:52:04.012643Z","shell.execute_reply":"2021-07-05T00:52:04.0795Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The realized vol of stock 0 in this feature bucket, will be:","metadata":{}},{"cell_type":"code","source":"#  Calculate Realized Volatility (outputs a aggregated value) for given feature\nrealized_vol = realized_volatility(book_example['log_return'])\nprint(f'Realized volatility for stock_id 0 on time_id 5 is {realized_vol}')","metadata":{"execution":{"iopub.status.busy":"2021-07-05T00:52:04.081453Z","iopub.execute_input":"2021-07-05T00:52:04.081692Z","iopub.status.idle":"2021-07-05T00:52:04.087449Z","shell.execute_reply.started":"2021-07-05T00:52:04.081669Z","shell.execute_reply":"2021-07-05T00:52:04.086513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Back to the problem now and let's do a \"Naive prediction\": using past realized volatility as target","metadata":{}},{"cell_type":"markdown","source":"A commonly known fact about volatility is that it tends to be autocorrelated (previous trends can dictate future trends to some extent). We can use this property to implement a naive model that just \"predicts\" realized volatility by using whatever the realized volatility was in the initial 10 minutes.\n\nLet's calculate the past realized volatility across the training set to see how predictive a single naive signal can be.","metadata":{}},{"cell_type":"code","source":"#Loading Train_Order_book parquet folder => and storing path for each stock's order book\nlist_order_book_file_train = glob.glob('/kaggle/input/optiver-realized-volatility-prediction/book_train.parquet/*')","metadata":{"execution":{"iopub.status.busy":"2021-07-05T00:52:04.088604Z","iopub.execute_input":"2021-07-05T00:52:04.088895Z","iopub.status.idle":"2021-07-05T00:52:04.100348Z","shell.execute_reply.started":"2021-07-05T00:52:04.088871Z","shell.execute_reply":"2021-07-05T00:52:04.099495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can get the past realized volatility as prediction for each individual stocks, \nusing **(\"past_realized_volatility_per_stock\" func for looping through each stock)** which internally calls **(\"realized_volatility_per_time_id\" for looping through each time_id within the stock(id))** function.","metadata":{}},{"cell_type":"code","source":"#Looping through each stock using \"past_realized_volatility_per_stock\" func\ndf_past_realized_train = past_realized_volatility_per_stock(list_file=list_order_book_file_train,\n                                                           prediction_column_name='pred')","metadata":{"execution":{"iopub.status.busy":"2021-07-05T00:52:04.101792Z","iopub.execute_input":"2021-07-05T00:52:04.102202Z","iopub.status.idle":"2021-07-05T01:00:21.701111Z","shell.execute_reply.started":"2021-07-05T00:52:04.102163Z","shell.execute_reply":"2021-07-05T01:00:21.700284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Let's join the output dataframe with train.csv to see the performance of the naive prediction on training set.**","metadata":{}},{"cell_type":"code","source":"train['row_id'] = train['stock_id'].astype(str) + '-' + train['time_id'].astype(str) # remember that Mandalorian's line , yes CTRL+F\ntrain = train[['row_id','target']]\ndf_joined = train.merge(df_past_realized_train[['row_id','pred']], on = ['row_id'], how = 'left')","metadata":{"execution":{"iopub.status.busy":"2021-07-05T01:00:21.702226Z","iopub.execute_input":"2021-07-05T01:00:21.702456Z","iopub.status.idle":"2021-07-05T01:00:23.243457Z","shell.execute_reply.started":"2021-07-05T01:00:21.702433Z","shell.execute_reply":"2021-07-05T01:00:23.242735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**We will evaluate the naive prediction result by two metrics: RMSPE and R squared.**","metadata":{}},{"cell_type":"code","source":"R2 = round(r2_score(y_true = df_joined['target'], y_pred = df_joined['pred']),3)\nRMSPE = round(rmspe(y_true = df_joined['target'], y_pred = df_joined['pred']),3)\nprint(f'Performance of the naive prediction: R2 score: {R2}, RMSPE: {RMSPE}')","metadata":{"execution":{"iopub.status.busy":"2021-07-05T01:00:23.244652Z","iopub.execute_input":"2021-07-05T01:00:23.244945Z","iopub.status.idle":"2021-07-05T01:00:23.259347Z","shell.execute_reply.started":"2021-07-05T01:00:23.244917Z","shell.execute_reply":"2021-07-05T01:00:23.258267Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**It is a reasonable benchmark to start with. Remember we have not used trade data yet.**","metadata":{}},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"markdown","source":"**I am using some of the codes from introductory notebook from Optiver , including the one below - for submission.\nAs there's only one time_id given in order book of test set i.e 4 , but submission is required for 3 time ids 4, 32, 34. So its possible you  try and submit prediction with that one row itself and then it will error at submission. So...yeah **USE THIS****","metadata":{"execution":{"iopub.status.busy":"2021-06-09T15:25:51.891717Z","iopub.execute_input":"2021-06-09T15:25:51.89209Z","iopub.status.idle":"2021-06-09T15:25:51.898582Z","shell.execute_reply.started":"2021-06-09T15:25:51.892059Z","shell.execute_reply":"2021-06-09T15:25:51.89729Z"}}},{"cell_type":"code","source":"list_order_book_file_test = glob.glob('/kaggle/input/optiver-realized-volatility-prediction/book_test.parquet/*')\ndf_naive_pred_test = past_realized_volatility_per_stock(list_file=list_order_book_file_test,\n                                                           prediction_column_name='target')\ndf_naive_pred_test.to_csv('submission.csv',index = False)","metadata":{"execution":{"iopub.status.busy":"2021-07-05T01:00:23.260617Z","iopub.execute_input":"2021-07-05T01:00:23.260991Z","iopub.status.idle":"2021-07-05T01:00:23.298328Z","shell.execute_reply.started":"2021-07-05T01:00:23.260955Z","shell.execute_reply":"2021-07-05T01:00:23.297625Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_naive_pred_test","metadata":{"execution":{"iopub.status.busy":"2021-07-05T01:00:23.299283Z","iopub.execute_input":"2021-07-05T01:00:23.299519Z","iopub.status.idle":"2021-07-05T01:00:23.307321Z","shell.execute_reply.started":"2021-07-05T01:00:23.299493Z","shell.execute_reply":"2021-07-05T01:00:23.306503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## If I have got anything wrong, please tell me in the comments, it will help me and everyone reading this notebook. \n\n# AND PLEASE UPVOTE IF IT WAS HELPFUL","metadata":{}}]}