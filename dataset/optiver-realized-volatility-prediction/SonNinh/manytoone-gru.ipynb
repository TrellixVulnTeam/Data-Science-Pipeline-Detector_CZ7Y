{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\nfrom torch.utils.data import Dataset\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n\n\n\nclass StockDataset(Dataset):\n    def __init__(self, df, valid_features, predict_len=300, row_id=None):\n        self.df = df\n        self.valid_features = valid_features\n        self.predict_len = predict_len\n        self.row_id = row_id\n        \n    def __len__(self):\n        return self.df.shape[0]//600\n    \n    def __getitem__(self, idx):\n        seq = self.df[idx*600: (idx+1)*600]\n        \n        if self.row_id is None:\n            x = seq[:, :-1]#.reshape(1, -1, len(self.valid_features))\n            y = seq[0 , -1:]#.reshape(1, -1, len(self.valid_features))\n        else:\n            x = seq # (600, n_feature)\n            y = self.row_id[idx] #(2, )\n\n        return x, y\n\n\n\ndef log_return(list_stock_prices):\n    return np.log(list_stock_prices).diff() \n\n\ndef ffill(data_df):\n    data_df=data_df.set_index(['time_id', 'seconds_in_bucket'])\n    data_df = data_df.reindex(pd.MultiIndex.from_product([data_df.index.levels[0], np.arange(0,600)], names = ['time_id', 'seconds_in_bucket']), method='ffill')\n    return data_df.reset_index()\n\n\ndef fix_offsets(data_df):\n    offsets = data_df.groupby(['time_id']).agg({'seconds_in_bucket':'min'})\n    offsets.columns = ['offset']\n    data_df = data_df.join(offsets, on='time_id')\n    data_df.seconds_in_bucket = data_df.seconds_in_bucket - data_df.offset\n    return data_df\n\n\ndef engineering(path):\n    book = pd.read_parquet(path)\n    \n    book = ffill(\n        fix_offsets(book)\n    )\n    assert book.shape[0] % 600 == 0\n    \n    for n in range(1, 3):\n        p1 = book[f\"bid_price{n}\"]\n        p2 = book[f\"ask_price{n}\"]\n        s1 = book[f\"bid_size{n}\"]\n        s2 = book[f\"ask_size{n}\"]\n        book[f\"wap{n}\"] = (p1*s2 + p2*s1) / (s1 + s2)\n        \n        book[f'log_return{n}'] = book.groupby(['time_id'])[f'wap{n}'].apply(log_return)\n        book[f'log_return{n}'] = book[f'log_return{n}'].fillna(0.)\n    \n    assert book.shape[0] % 600 == 0\n    \n    return book\n    \n\ndef load_data(parquet_path, stock_id, valid_features, normalize=True):\n    df = engineering(parquet_path)\n    \n    df[\"stock_id\"] = stock_id\n\n    if normalize:\n        scaler = StandardScaler()\n        df[valid_features] = scaler.fit_transform(df[valid_features])\n\n    return df\n\n\ndef get_dataset_test(data_path, stock_id, valid_features):\n    keep_columns = ['stock_id', 'time_id', 'seconds_in_bucket'] + valid_features\n    df = load_data(data_path, stock_id, valid_features, normalize=True)\n    df = df[keep_columns]\n    \n    features = df[valid_features].values\n    row_id = df[['stock_id', 'time_id']].values\n    row_id = row_id[::600]\n    \n    test_data = StockDataset(\n        features,\n        valid_features,\n        row_id = row_id\n    )\n    \n \n    return test_data\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-19T04:48:20.157749Z","iopub.execute_input":"2021-07-19T04:48:20.158098Z","iopub.status.idle":"2021-07-19T04:48:20.176028Z","shell.execute_reply.started":"2021-07-19T04:48:20.158064Z","shell.execute_reply":"2021-07-19T04:48:20.174852Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\nimport torch\nfrom torch import nn\n\n\nclass RNNEncoder(nn.Module):\n    def __init__(self, rnn_num_layers=1, input_feature_len=1, hidden_size=100, device='cpu', rnn_dropout=0.2):\n        super().__init__()\n        self.hidden_size = hidden_size\n        self.input_feature_len = input_feature_len\n        self.num_layers = rnn_num_layers\n        self.lstm = nn.LSTM(\n            num_layers=rnn_num_layers,\n            input_size=input_feature_len,\n            hidden_size=hidden_size,\n            batch_first=True,\n            bidirectional=False,\n            dropout=rnn_dropout\n        )\n        self.device = device\n\n    def forward(self, input_seq):\n        ht = torch.zeros(\n            self.num_layers,\n            input_seq.size(0), \n            self.hidden_size, \n            device=self.device\n        )\n        ct = torch.zeros(\n            self.num_layers,\n            input_seq.size(0), \n            self.hidden_size, \n            device=self.device\n        )\n        \n        \n            \n        gru_out, hidden = self.lstm(input_seq, (ht, ct))\n        # hidden = hidden.permute(1, 0, 2).reshape(input_seq.size(0), -1)\n        # gru_out (B, len, fea), hidden (B, fea*layer)\n            \n        return gru_out, hidden\n\n\nclass Many2One(nn.Module):\n    def __init__(self, encoder, hidden_size):\n        super().__init__()\n        self.encoder = encoder\n        self.out = nn.Linear(hidden_size, 1)\n        # self.relu = nn.ELU(inplace=True)\n\n    def forward(self, x):\n        encoder_out, hidden = self.encoder(x)\n        \n        output = self.out(encoder_out[:, -1])\n        # output = self.relu(output)\n        output = torch.exp(output)\n        return output","metadata":{"execution":{"iopub.status.busy":"2021-07-19T04:48:21.782903Z","iopub.execute_input":"2021-07-19T04:48:21.783253Z","iopub.status.idle":"2021-07-19T04:48:21.794256Z","shell.execute_reply.started":"2021-07-19T04:48:21.783221Z","shell.execute_reply":"2021-07-19T04:48:21.79334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.optim as optim\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\n\n\nimport os\n\n\ndef rmspe(y_true, y_pred):\n        return (torch.sqrt(torch.mean(torch.square((y_true - y_pred) / y_true))))\n\ndef load_model(model, model_path, optimizer=None, resume=False, \n               lr=None, lr_step=None):\n    start_epoch = 0\n    checkpoint = torch.load(model_path, map_location=lambda storage, loc: storage)\n    print('loaded {}, epoch {}'.format(model_path, checkpoint['epoch']))\n    state_dict_ = checkpoint['state_dict']\n    state_dict = {}\n  \n    # convert data_parallal to model\n    for k in state_dict_:\n        if k.startswith('module') and not k.startswith('module_list'):\n            state_dict[k[7:]] = state_dict_[k]\n        else:\n            state_dict[k] = state_dict_[k]\n\n    model_state_dict = model.state_dict()\n\n    # check loaded parameters and created model parameters\n    msg = 'If you see this, your model does not fully load the '+\\\n            'pre-trained weight. Please make sure '+\\\n            'you have correctly specified --arch xxx '+\\\n            'or set the correct --num_classes for your own dataset.'\n\n    for k in state_dict:\n        if k in model_state_dict:\n            if state_dict[k].shape != model_state_dict[k].shape:\n                print('Skip loading parameter {}, required shape{}, '\\\n                    'loaded shape{}. {}'.format(\n                k, model_state_dict[k].shape, state_dict[k].shape, msg))\n                state_dict[k] = model_state_dict[k]\n        else:\n            print('Drop parameter {}.'.format(k) + msg)\n    \n    for k in model_state_dict:\n        if not (k in state_dict):\n            print('No param {}.'.format(k) + msg)\n            state_dict[k] = model_state_dict[k]\n    model.load_state_dict(state_dict, strict=False)\n\n    # resume optimizer parameters\n    if optimizer is not None and resume:\n        if 'optimizer' in checkpoint:\n            optimizer.load_state_dict(checkpoint['optimizer'])\n            start_epoch = checkpoint['epoch']\n            start_lr = lr\n            for step in lr_step:\n                if start_epoch >= step:\n                    start_lr *= 0.1\n            for param_group in optimizer.param_groups:\n                param_group['lr'] = start_lr\n            print('Resumed optimizer with start lr', start_lr)\n        else:\n            print('No optimizer parameters in checkpoint.')\n\n    if optimizer is not None:\n        return model, optimizer, checkpoint['epoch'], checkpoint['loss']\n    else:\n        return model\n\n\ndef get_stocks(train):\n    return np.sort(np.unique(train['stock_id']))\n\n\ndef get_stock_file(root_data, stock):\n    dir = os.path.join(root_data, \"book_test.parquet\", \"stock_id=\" + str(stock))\n    file_path = os.listdir(dir)[0]\n    return os.path.join(dir, file_path)\n\n\ndef main(stock_book_file, stock_id, label_df, rv, row_ids):\n\n    print(stock_book_file, stock_id)\n    valid_columns = [\n        'bid_price1', 'ask_price1', 'bid_size1', 'ask_size1', 'wap1', 'log_return1',\n        'bid_price2', 'ask_price2', 'bid_size2', 'ask_size2', 'wap2', 'log_return2'\n    ]\n    test_data = get_dataset_test(\n        stock_book_file,\n        stock_id,\n        valid_columns\n    )\n    input_feature_len = len(valid_columns)\n\n    test_dataloader = DataLoader(test_data, batch_size=16, shuffle=False, num_workers=8)\n\n    device = torch.device('cuda:0')\n\n    rnn_num_layers=3\n    hidden_size=100\n    encoder = RNNEncoder(\n        rnn_num_layers=rnn_num_layers, \n        input_feature_len=input_feature_len, \n        hidden_size=hidden_size,\n        device=device, \n        rnn_dropout=0.2\n    )\n\n    model = Many2One(encoder, hidden_size)\n    model = model.to(device)\n\n    \n    model = load_model(model, f'../input/myweights/best_{stock_id}.pth')\n    model.eval()\n\n    bar = tqdm(test_dataloader, position=0, leave=True, bar_format='{l_bar}{bar:10}{r_bar}{bar:-10b}')\n    for x, y in bar:\n        x_cuda = x.float().to(device)\n        y = y.numpy()\n        # row_id = '{}-{}'.format(row_id[0], row_id[1])\n        \n        \n        pred = model(\n            x_cuda\n        )\n        \n        pred = pred.cpu().detach().numpy()\n        \n        rv.append(pred)\n        row_ids.append(y)\n    \n    return rv, row_ids\n    \n","metadata":{"execution":{"iopub.status.busy":"2021-07-19T04:48:22.669029Z","iopub.execute_input":"2021-07-19T04:48:22.66942Z","iopub.status.idle":"2021-07-19T04:48:22.805977Z","shell.execute_reply.started":"2021-07-19T04:48:22.669385Z","shell.execute_reply":"2021-07-19T04:48:22.804781Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"root_dir = '../input/optiver-realized-volatility-prediction'\n\n\nlabel_df = pd.read_csv('../input/optiver-realized-volatility-prediction/test.csv')\nstocks = get_stocks(label_df)\n\nrv, row_ids = [], []\nfor stock_id in stocks:\n    stock_book_file = get_stock_file(root_dir, stock_id)\n    rv, row_ids = main(stock_book_file, stock_id, label_df, rv, row_ids)\n\n\nrv = np.concatenate(rv)\nrow_ids = np.concatenate(row_ids)\n\nsubmit_data = pd.DataFrame(\n    np.concatenate((row_ids, rv), axis=-1),\n    columns=['stock_id', 'time_id', 'target']\n)\n\nsubmit_data['row_id'] = submit_data['stock_id'].astype(int).astype(str) + '-' + submit_data['time_id'].astype(int).astype(str)\nsubmit_data = submit_data[['row_id','target']]\nsubmit_data.to_csv('submission.csv',index = False)","metadata":{"execution":{"iopub.status.busy":"2021-07-19T04:48:24.808861Z","iopub.execute_input":"2021-07-19T04:48:24.809199Z","iopub.status.idle":"2021-07-19T04:48:30.811565Z","shell.execute_reply.started":"2021-07-19T04:48:24.809165Z","shell.execute_reply":"2021-07-19T04:48:30.810454Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# submit_data","metadata":{"execution":{"iopub.status.busy":"2021-07-19T04:48:54.341407Z","iopub.execute_input":"2021-07-19T04:48:54.341812Z","iopub.status.idle":"2021-07-19T04:48:54.359792Z","shell.execute_reply.started":"2021-07-19T04:48:54.34178Z","shell.execute_reply":"2021-07-19T04:48:54.35884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}