{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This notebook is intended for starters who are just trying to understand the competition and understand what needs to be done for a basic submission.\n<br>\n<b>I myself am a beginner and hope to learn from this exercise.</b>","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-03T09:49:28.258544Z","iopub.execute_input":"2021-07-03T09:49:28.259049Z","iopub.status.idle":"2021-07-03T09:49:28.266257Z","shell.execute_reply.started":"2021-07-03T09:49:28.258945Z","shell.execute_reply":"2021-07-03T09:49:28.264344Z"}}},{"cell_type":"markdown","source":" # **Importing the required libraries**","metadata":{}},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np","metadata":{"execution":{"iopub.status.busy":"2021-07-05T17:33:11.241462Z","iopub.execute_input":"2021-07-05T17:33:11.241966Z","iopub.status.idle":"2021-07-05T17:33:11.245039Z","shell.execute_reply.started":"2021-07-05T17:33:11.241914Z","shell.execute_reply":"2021-07-05T17:33:11.244351Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading data and understanding the data.","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('../input/optiver-realized-volatility-prediction/train.csv')","metadata":{"execution":{"iopub.status.busy":"2021-07-05T17:33:11.246104Z","iopub.execute_input":"2021-07-05T17:33:11.246481Z","iopub.status.idle":"2021-07-05T17:33:11.547155Z","shell.execute_reply.started":"2021-07-05T17:33:11.246455Z","shell.execute_reply":"2021-07-05T17:33:11.546188Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(train['stock_id'].unique())","metadata":{"execution":{"iopub.status.busy":"2021-07-05T17:33:11.548952Z","iopub.execute_input":"2021-07-05T17:33:11.549301Z","iopub.status.idle":"2021-07-05T17:33:11.570183Z","shell.execute_reply.started":"2021-07-05T17:33:11.54927Z","shell.execute_reply":"2021-07-05T17:33:11.569209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This csv contains the stock-id(total 112 in number) and the time-id along with the target, i.e. the volatility score at that point of time for that particular stock.","metadata":{}},{"cell_type":"markdown","source":"# Loading Parquete Files","metadata":{}},{"cell_type":"markdown","source":"As an example, loading just a single file for now to check the data and draw some insights and understand the format.","metadata":{"execution":{"iopub.status.busy":"2021-07-03T10:24:08.604456Z","iopub.execute_input":"2021-07-03T10:24:08.6049Z","iopub.status.idle":"2021-07-03T10:24:08.609313Z","shell.execute_reply.started":"2021-07-03T10:24:08.604864Z","shell.execute_reply":"2021-07-03T10:24:08.608416Z"}}},{"cell_type":"code","source":"book_train_stock_id = pd.read_parquet(f'../input/optiver-realized-volatility-prediction/book_train.parquet/stock_id=0/')\n\n# len(train['time_id'].unique())\nbook_train_stock_id.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-05T17:33:11.572065Z","iopub.execute_input":"2021-07-05T17:33:11.572519Z","iopub.status.idle":"2021-07-05T17:33:12.152907Z","shell.execute_reply.started":"2021-07-05T17:33:11.572477Z","shell.execute_reply":"2021-07-05T17:33:12.152138Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As different stocks trade on different level on the market we take the ratio of best offer price and best bid price to calculate the bid-ask spread.\n\nThe formula of bid/ask spread can be written in below form:\nBidAskSpread=BestOffer/BestBidâˆ’1\n\nSource: https://www.kaggle.com/jiashenliu/introduction-to-financial-concepts-and-data","metadata":{}},{"cell_type":"code","source":"book_train_stock_id['bid_ask_spread'] = book_train_stock_id[['ask_price1', 'ask_price2']].min(axis=1)/book_train_stock_id[['bid_price1', 'bid_price2']].max(axis=1) - 1\nbook_train_stock_id.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-05T17:33:12.154084Z","iopub.execute_input":"2021-07-05T17:33:12.154575Z","iopub.status.idle":"2021-07-05T17:33:12.233151Z","shell.execute_reply.started":"2021-07-05T17:33:12.154543Z","shell.execute_reply":"2021-07-05T17:33:12.23208Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The definition and formula for weighted average can be found from the link below.\nhttps://www.kaggle.com/jiashenliu/introduction-to-financial-concepts-and-data?scriptVersionId=67183666&cellId=18\n\nThe weighted average price is the metric that is used to calculate the actual value of the stock in consideration.\n\nThe WAP is calculated from the top level price and volume information.","metadata":{}},{"cell_type":"code","source":"book_train_stock_id['wap'] = (book_train_stock_id['bid_price1'] * book_train_stock_id['ask_size1'] +\n                            book_train_stock_id['ask_price1'] * book_train_stock_id['bid_size1']) / (\n                            book_train_stock_id['bid_size1']+ book_train_stock_id['ask_size1'])","metadata":{"execution":{"iopub.status.busy":"2021-07-05T17:33:12.234727Z","iopub.execute_input":"2021-07-05T17:33:12.235103Z","iopub.status.idle":"2021-07-05T17:33:12.2493Z","shell.execute_reply.started":"2021-07-05T17:33:12.23507Z","shell.execute_reply":"2021-07-05T17:33:12.248282Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"book_train_stock_id.groupby(by = ['time_id']).mean().head()","metadata":{"execution":{"iopub.status.busy":"2021-07-05T17:33:12.250796Z","iopub.execute_input":"2021-07-05T17:33:12.251178Z","iopub.status.idle":"2021-07-05T17:33:12.413956Z","shell.execute_reply.started":"2021-07-05T17:33:12.251144Z","shell.execute_reply":"2021-07-05T17:33:12.412685Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Utility Functions","metadata":{}},{"cell_type":"code","source":"# This is the function for calculating the log of return.\n# This metric is useful to get the difference of the stock value compared between 2 time steps.\n# diff function in pandas returns the difference between consecutive values in a pandas dataframe.\ndef log_return(list_stock_prices):\n    return np.log(list_stock_prices).diff() #The value present in each cell is \n                                            #the difference of current cell value with \n                                            #the previous row corresponding cell\n\n#The function is for computing the value of realised volatility given a series of log returns.\n# Volatility is a metric that depends on how much the stock price has changed in a short period of time.\n# So if the log return has changed a lot on consecutive time steps, it is more volatile and vise-versa.\ndef realized_volatility(series_log_return):\n    return np.sqrt(np.sum(series_log_return**2))","metadata":{"execution":{"iopub.status.busy":"2021-07-05T17:33:12.415187Z","iopub.execute_input":"2021-07-05T17:33:12.415533Z","iopub.status.idle":"2021-07-05T17:33:12.420483Z","shell.execute_reply.started":"2021-07-05T17:33:12.415502Z","shell.execute_reply":"2021-07-05T17:33:12.419521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<b>The code below is used to run the 'log_return' function on each group of time_id.</b>\n<br>\ni.e. All the list of 'wap' values for every time_id are taken and the difference of consecutive 'wap' is given as a list in return. \n<br>\nAlso filling the na values with 0 in the process.","metadata":{}},{"cell_type":"code","source":"book_train_stock_id['log_return'] = (book_train_stock_id.groupby(by = ['time_id'])['wap'].apply(log_return).\n                                       reset_index(drop = True).\n                                       fillna(0))","metadata":{"execution":{"iopub.status.busy":"2021-07-05T17:33:12.423509Z","iopub.execute_input":"2021-07-05T17:33:12.423825Z","iopub.status.idle":"2021-07-05T17:33:14.216025Z","shell.execute_reply.started":"2021-07-05T17:33:12.423799Z","shell.execute_reply":"2021-07-05T17:33:14.215196Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"book_train_stock_id.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-05T17:33:14.217646Z","iopub.execute_input":"2021-07-05T17:33:14.217979Z","iopub.status.idle":"2021-07-05T17:33:14.235688Z","shell.execute_reply.started":"2021-07-05T17:33:14.217934Z","shell.execute_reply":"2021-07-05T17:33:14.234793Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After getting the 'log_return' column for 'wap' of each 'time_id', we need to get the realized volatility using the formula provided in the link.\n<br>\nhttps://www.kaggle.com/jiashenliu/introduction-to-financial-concepts-and-data","metadata":{}},{"cell_type":"code","source":"df_realized_vol = book_train_stock_id.groupby(by = ['time_id'])['log_return'].agg(realized_volatility)","metadata":{"execution":{"iopub.status.busy":"2021-07-05T17:33:14.236964Z","iopub.execute_input":"2021-07-05T17:33:14.237276Z","iopub.status.idle":"2021-07-05T17:33:15.363323Z","shell.execute_reply.started":"2021-07-05T17:33:14.237232Z","shell.execute_reply":"2021-07-05T17:33:15.362414Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The below cell gets the mean of 'bid_ask_spread' for each group of 'time_id'","metadata":{}},{"cell_type":"code","source":"pd_ba_spread = book_train_stock_id.groupby(by = ['time_id'], as_index = False)['bid_ask_spread'].mean()","metadata":{"execution":{"iopub.status.busy":"2021-07-05T17:33:15.364561Z","iopub.execute_input":"2021-07-05T17:33:15.364839Z","iopub.status.idle":"2021-07-05T17:33:15.410744Z","shell.execute_reply.started":"2021-07-05T17:33:15.364813Z","shell.execute_reply":"2021-07-05T17:33:15.409692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Merging the above 2 dataframes for getting the feature columns","metadata":{}},{"cell_type":"code","source":"merged_dataframe = pd.merge(df_realized_vol, pd_ba_spread, on=['time_id'], how='left')","metadata":{"execution":{"iopub.status.busy":"2021-07-05T17:33:15.412028Z","iopub.execute_input":"2021-07-05T17:33:15.412346Z","iopub.status.idle":"2021-07-05T17:33:15.428414Z","shell.execute_reply.started":"2021-07-05T17:33:15.412317Z","shell.execute_reply":"2021-07-05T17:33:15.427366Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"merged_dataframe.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-05T17:33:15.429956Z","iopub.execute_input":"2021-07-05T17:33:15.430437Z","iopub.status.idle":"2021-07-05T17:33:15.444206Z","shell.execute_reply.started":"2021-07-05T17:33:15.430396Z","shell.execute_reply":"2021-07-05T17:33:15.442426Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Assigning the stock_id value to the new column. We need to repeat the above process for all the stock_ids and create the final dataframe.","metadata":{}},{"cell_type":"code","source":"merged_dataframe['stock_id'] = 0","metadata":{"execution":{"iopub.status.busy":"2021-07-05T17:33:15.445903Z","iopub.execute_input":"2021-07-05T17:33:15.446357Z","iopub.status.idle":"2021-07-05T17:33:15.452236Z","shell.execute_reply.started":"2021-07-05T17:33:15.446311Z","shell.execute_reply":"2021-07-05T17:33:15.451133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"merged_dataframe.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-05T17:33:15.453824Z","iopub.execute_input":"2021-07-05T17:33:15.454281Z","iopub.status.idle":"2021-07-05T17:33:15.482302Z","shell.execute_reply.started":"2021-07-05T17:33:15.454218Z","shell.execute_reply":"2021-07-05T17:33:15.480674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataSet = pd.merge(train, merged_dataframe, on = ['stock_id', 'time_id'], how = 'left')","metadata":{"execution":{"iopub.status.busy":"2021-07-05T17:33:15.483982Z","iopub.execute_input":"2021-07-05T17:33:15.484499Z","iopub.status.idle":"2021-07-05T17:33:15.560424Z","shell.execute_reply.started":"2021-07-05T17:33:15.484453Z","shell.execute_reply":"2021-07-05T17:33:15.559438Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataSet.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-05T17:33:15.561887Z","iopub.execute_input":"2021-07-05T17:33:15.562371Z","iopub.status.idle":"2021-07-05T17:33:15.575135Z","shell.execute_reply.started":"2021-07-05T17:33:15.562326Z","shell.execute_reply":"2021-07-05T17:33:15.574146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train = train_dataSet['target']\nX_train = train_dataSet.drop(['stock_id', 'time_id', 'target'], axis = 1)","metadata":{"execution":{"iopub.status.busy":"2021-07-05T17:33:15.576546Z","iopub.execute_input":"2021-07-05T17:33:15.576849Z","iopub.status.idle":"2021-07-05T17:33:15.589618Z","shell.execute_reply.started":"2021-07-05T17:33:15.576821Z","shell.execute_reply":"2021-07-05T17:33:15.58839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After getting the train features and targets, we need to train the model on the this data and run the prediction.\n\nTo do the training we need to put all the above steps in a function and loop over the available stock-ids and create the training dataset.","metadata":{}},{"cell_type":"code","source":"# Each step in this function is executed as a single cell for everyone to run and check the intermediate outputs.\n# This is helpful in looking at the dataframes and understanding exactly what is happening at each level of code.\n\ndef get_stock_data(stock_id, train='train'):\n    parquet_file_path = f'../input/optiver-realized-volatility-prediction/book_'+train+'.parquet/stock_id='+str(stock_id)+'/'\n    book_train_stock_id = pd.read_parquet(parquet_file_path)\n    \n    book_train_stock_id['bid_ask_spread'] = book_train_stock_id[['ask_price1', 'ask_price2']].min(axis=1)/book_train_stock_id[['bid_price1', 'bid_price2']].max(axis=1) - 1\n    book_train_stock_id['wap'] = (book_train_stock_id['bid_price1'] * book_train_stock_id['ask_size1'] +\n                            book_train_stock_id['ask_price1'] * book_train_stock_id['bid_size1']) / (\n                            book_train_stock_id['bid_size1']+ book_train_stock_id['ask_size1'])\n    book_train_stock_id['log_return'] = (book_train_stock_id.groupby(by = ['time_id'])['wap'].apply(log_return).\n                                       reset_index(drop = True).\n                                       fillna(0))\n    df_realized_vol = book_train_stock_id.groupby(by = ['time_id'])['log_return'].agg(realized_volatility)\n    pd_ba_spread = book_train_stock_id.groupby(by = ['time_id'], as_index = False)['bid_ask_spread'].mean()\n    \n    merged_dataframe = pd.merge(df_realized_vol, pd_ba_spread, on=['time_id'], how='left')\n    merged_dataframe['stock_id'] = stock_id\n    \n    return merged_dataframe","metadata":{"execution":{"iopub.status.busy":"2021-07-05T17:33:15.591303Z","iopub.execute_input":"2021-07-05T17:33:15.591707Z","iopub.status.idle":"2021-07-05T17:33:15.601754Z","shell.execute_reply.started":"2021-07-05T17:33:15.591661Z","shell.execute_reply":"2021-07-05T17:33:15.600999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The list of stock ids can be fetched from the train df.\n\nstock_ids = train['stock_id'].unique()\n\n# Calling the get_stock_data function for each stock_id to create a list of dataframes.\n# These dataframes are then concatenated to form a single dataframe.\ndf_list = [get_stock_data(i) for i in stock_ids]\nfinal_concat_list = pd.concat(df_list, ignore_index = True)","metadata":{"execution":{"iopub.status.busy":"2021-07-05T17:33:15.602763Z","iopub.execute_input":"2021-07-05T17:33:15.603175Z","iopub.status.idle":"2021-07-05T17:39:58.958001Z","shell.execute_reply.started":"2021-07-05T17:33:15.603135Z","shell.execute_reply":"2021-07-05T17:39:58.95707Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Joining the train df with the above df on stock_id and time_id\ntrain_dataSet = pd.merge(train, final_concat_list, on = ['stock_id', 'time_id'], how = 'left')","metadata":{"execution":{"iopub.status.busy":"2021-07-05T17:44:28.670181Z","iopub.execute_input":"2021-07-05T17:44:28.670588Z","iopub.status.idle":"2021-07-05T17:44:28.766861Z","shell.execute_reply.started":"2021-07-05T17:44:28.670552Z","shell.execute_reply":"2021-07-05T17:44:28.766013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train = train_dataSet['target']\nX_train = train_dataSet.drop(['stock_id', 'time_id', 'target'], axis = 1)","metadata":{"execution":{"iopub.status.busy":"2021-07-05T17:46:34.119212Z","iopub.execute_input":"2021-07-05T17:46:34.119645Z","iopub.status.idle":"2021-07-05T17:46:34.127225Z","shell.execute_reply.started":"2021-07-05T17:46:34.119598Z","shell.execute_reply":"2021-07-05T17:46:34.12642Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Importing the required packages for training the ML model\nimport xgboost as xgb\nfrom xgboost.sklearn import XGBRegressor\nfrom xgboost import cv\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedKFold","metadata":{"execution":{"iopub.status.busy":"2021-07-05T18:05:14.816979Z","iopub.execute_input":"2021-07-05T18:05:14.817404Z","iopub.status.idle":"2021-07-05T18:05:14.822391Z","shell.execute_reply.started":"2021-07-05T18:05:14.817373Z","shell.execute_reply":"2021-07-05T18:05:14.821309Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = XGBRegressor(random_state = 0\n                   #,n_estimators = 200\n                   #,learning_rate = 0.1\n                   #,subsample = 0.8\n                   #,colsample_bytree = 0.8\n                   ,n_jobs= - 1)\n\ncv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n# evaluate model\nscores = cross_val_score(model, X_train, y_train, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\nprint('Mean MAE: %.3f (%.3f)' % (scores.mean(), scores.std()))","metadata":{"execution":{"iopub.status.busy":"2021-07-05T18:23:52.325209Z","iopub.execute_input":"2021-07-05T18:23:52.325625Z","iopub.status.idle":"2021-07-05T18:41:34.061117Z","shell.execute_reply.started":"2021-07-05T18:23:52.325586Z","shell.execute_reply":"2021-07-05T18:41:34.060285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = pd.read_csv('../input/optiver-realized-volatility-prediction/test.csv')\n\n# test_stock_stat_df = get_dataSet(stock_ids = test['stock_id'].unique(), dataType = 'test')\ntest_stock_ids = test['stock_id'].unique()\ntest_df_list = [get_stock_data(i) for i in test_stock_ids]\ntest_final_concat_list = pd.concat(test_df_list, ignore_index = True)\n\ntest_dataSet = pd.merge(test, test_final_concat_list, on = ['stock_id', 'time_id'], how = 'left')\ntest_dataSet = test_dataSet.drop(['stock_id', 'time_id'], axis = 1)\n\ny_pred = test_dataSet[['row_id']]\nX_test = test_dataSet.drop(['row_id'], axis = 1).fillna(0)","metadata":{"execution":{"iopub.status.busy":"2021-07-05T18:45:06.875335Z","iopub.execute_input":"2021-07-05T18:45:06.875741Z","iopub.status.idle":"2021-07-05T18:45:09.899711Z","shell.execute_reply.started":"2021-07-05T18:45:06.875705Z","shell.execute_reply":"2021-07-05T18:45:09.898501Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2021-07-05T18:54:13.39535Z","iopub.execute_input":"2021-07-05T18:54:13.395744Z","iopub.status.idle":"2021-07-05T18:54:33.949518Z","shell.execute_reply.started":"2021-07-05T18:54:13.395711Z","shell.execute_reply":"2021-07-05T18:54:33.948186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = y_pred.assign(target = model.predict(X_test))\ny_pred.to_csv('submission.csv',index = False)","metadata":{"execution":{"iopub.status.busy":"2021-07-05T18:54:59.324237Z","iopub.execute_input":"2021-07-05T18:54:59.324623Z","iopub.status.idle":"2021-07-05T18:54:59.346752Z","shell.execute_reply.started":"2021-07-05T18:54:59.324594Z","shell.execute_reply":"2021-07-05T18:54:59.344821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-05T18:55:10.309177Z","iopub.execute_input":"2021-07-05T18:55:10.309601Z","iopub.status.idle":"2021-07-05T18:55:10.320366Z","shell.execute_reply.started":"2021-07-05T18:55:10.30957Z","shell.execute_reply":"2021-07-05T18:55:10.319237Z"},"trusted":true},"execution_count":null,"outputs":[]}]}