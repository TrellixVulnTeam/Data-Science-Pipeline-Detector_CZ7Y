{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# !pip list --format=freeze > requirements.txt","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Optiver Realized Volatility Prediction\n\n# Problématique\n\nLa société Optiver, une société de négoce pour compte propre et un broker/dealer pour divers instruments financiers, a lancée un concours sur Kaggle.  \nIl s'agit de prédire la volatilité de \"stocks\" financiers.\n\nPrédire avec précision la volatilité est essentiel pour la négociation d'options, dont le prix est directement lié à la volatilité du produit sous-jacent (ici le stock).  \nLes options ont souvent un rôle d'effet levier de l'action.  \nDans notre cas une volatilité importante de notre stock créera probablement une variation encore plus importe de l'option associée.\n\n## Terminologie\n\n**Stock** : action financière  \nEx: Apple\n<br>\n<img src=\"https://github.com/abugeia/P8_kaggle_competition/blob/master/img/appl_stock.PNG?raw=true\" width=\"800px\">  \n<br>\n\n**Option**  \nProduit dérivé qui établit un contrat entre un acheteur et un vendeur.  \nL'acheteur de l'option obtient le droit, et non pas l'obligation, d'acheter ou de vendre un actif sous-jacent à un prix fixé à l'avance, pendant un temps donné ou à une date fixée.  \n<br>\n\n**Order book** (Carnet d'ordres)  \nListe électronique d'ordres d'achat et de vente pour un titre ou un instrument financier spécifique organisé par niveau de prix.  \nLes ordres d'achat prévus sont sur le côté gauche affichés comme \"bid\" tandis que tous les ordres de vente prévus sont sur la droite côté du livre affiché comme \"ask\"  \n<br>\n<img src=\"https://github.com/abugeia/P8_kaggle_competition/blob/master/img/OrderBook3.png?raw=true\" width=\"200px\">  \n<br>\n\n**Trade book** (carnets de transactions effectuées)  \nUn carnet d'ordres est une représentation de l'intention de négociation sur le marché, mais le marché a besoin d'un acheteur et d'un vendeur au même prix pour que la transaction se produise.  \nLe trade book trace l'ensemble des transactions qui ont eu lieu\n\n**bid/ask spread**  \n<br>\n<img src=\"https://github.com/abugeia/P8_kaggle_competition/blob/master/img/spread.PNG?raw=true\" width=\"350px\">\n<br><br>\n**WAP** (Weighted averaged price)  \n<br>\n<img src=\"https://github.com/abugeia/P8_kaggle_competition/blob/master/img/wap.PNG?raw=true\" width=\"450px\">  \n<br><br>\n\nExample :\n<br>\n<img src=\"https://github.com/abugeia/P8_kaggle_competition/blob/master/img/wap_bidask.PNG?raw=true\" width=\"450px\">  \n<br>\nDans cette compétition nous n'avons accès qu'aux rangs 1 & 2 des ordres.\n\n**Log return**\nPermet de comparer le cours d'une action entre deux moments.  \nEn appelant St le prix de l'action S à l'instant t , nous pouvons définir le retour de log entre t1 et t2 comme\n<br>\n<img src=\"https://github.com/abugeia/P8_kaggle_competition/blob/master/img/logrtn.PNG?raw=true\" width=\"200px\">  \n<br>\n\n**Volatilité**\nGrâce aux calculs des log return sur toutes les données consécutives du book nous pouvons définir la volatilité réalisée.  \nIl s'agit de la racine carrée de la somme des log return au carré.\n<br>\n<img src=\"https://github.com/abugeia/P8_kaggle_competition/blob/master/img/volatility.PNG?raw=true\" width=\"200px\">  \n<br>\n\n# Description du dataset\n\nLe dataset est constitué de données financières et plus particulièrement de carnets d'ordres et de carnets de transactions effectuées.  \nCes deux \"book\" sont deux ensembles de fichiers séparés.\n\nChaque book est classé par stock qui représente un indice financier.  \nPour chaque stock nous avons plusieurs time_id.  \nCelles ci font référence à une fenêtre de valeurs réelles de 20 min. Elles ne sont pas chronologiquement consécutives.  \nCertains de ces time_id sont publiques et font partie de l'échantillon train, d'autres sont caché et constituent l'échantillon de test.\n\nDans chacune de ces fenêtres de 20 min nous avons accès aux premières 10 min de données et nous devons prédire la volatilité des 10 min suivantes.  \nLa volatilité de ces dernières 10 min nous est fournie (pour l'échantillon train) et sera notre target.\n\n<img src=\"https://github.com/abugeia/P8_kaggle_competition/blob/master/img/DataBucketing.webp?raw=true\" width=\"300px\">  \n<br>\n<br>\nExample et explication :\n<br>\n<img src=\"https://github.com/abugeia/P8_kaggle_competition/blob/master/img/Data_chart.PNG?raw=true\" width=\"800px\"><br>\n<img src=\"https://github.com/abugeia/P8_kaggle_competition/blob/master/img/Data_explainations.PNG?raw=true\" width=\"600px\">\n\n# Process machine learning\n\nIl s'agit d'un problème de régression supervisé.\n\nDans un premier temps nous effectuerons un nettoyage éventuel et explorerons nos données.  \nPuis nous Ferons du feature engineering sur nos dataset de book et trade.\n\n## Evaluation\n\nL'évaluation des performances de nos prédictions par rapport aux données de l'échantillon de test se fera avec une métrique imposée : le RMSPE (Root mean square percentage error).  \nC'est ainsi une erreur quadratique moyenne normalisée puisqu'elle s'exprime en pourcentage.\n\n<br>\n<img src=\"https://github.com/abugeia/P8_kaggle_competition/blob/master/img/RMSPE.PNG?raw=true\" width=\"300px\"><br>\n\nIl faut agréger nos données sur un seul dataframe avec une ligne par time_id avant d'entraîner et appliquer un modèle.  \nEn effet voici la structure que devra avoir notre prédiction :\n<br>\n<img src=\"https://github.com/abugeia/P8_kaggle_competition/blob/master/img/sub_form.PNG?raw=true\" width=\"250px\"><br>\n\nIci la colonne row_id est composé de {stock}-{time_id} et target est la prédiction de notre volatilité.\n\n## Références\n\nCes Kernels m'ont particulièrement aidé dans ma participation :\n\n[https://www.kaggle.com/alexioslyon/lgbm-baseline](https://www.kaggle.com/alexioslyon/lgbm-baseline)  \n[https://www.kaggle.com/munumbutt/feature-engineering-tuned-xgboost-lgbm](https://www.kaggle.com/munumbutt/feature-engineering-tuned-xgboost-lgbm)\n","metadata":{}},{"cell_type":"markdown","source":"# Librairies\n","metadata":{}},{"cell_type":"code","source":"%matplotlib inline\n# generic libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nimport pandas as pd\nfrom joblib import Parallel, delayed\nimport pickle\nimport time\nimport plotly.graph_objects as go\n\n# machine learning\nfrom lightgbm import LGBMRegressor\nfrom sklearn.linear_model import Ridge, Lasso\nfrom sklearn.model_selection import train_test_split, KFold, cross_val_score\nfrom sklearn.metrics import r2_score, make_scorer\nfrom sklearn.ensemble import StackingRegressor, RandomForestRegressor\nfrom sklearn.pipeline import make_pipeline\nimport optuna\nfrom xgboost import XGBRegressor\nfrom catboost import CatBoostRegressor\n\n# path and files treatment\nimport glob\nimport os","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Variables\n","metadata":{}},{"cell_type":"code","source":"# env could be 'local' or 'kaggle'\nenv = 'kaggle'\n\nif env == 'local':\n    data_folder = './data'\n    output = './output/'\n    save_path = './img/'\n    if not(os.path.exists(output)):\n        os.makedirs(output)\n    if not(os.path.exists(save_path)):\n        os.makedirs(save_path)    \n\nelif env == 'kaggle':\n    data_folder = '../input/optiver-realized-volatility-prediction'\n    output = './output/'\n    save_path = './img/'\n    os.makedirs(save_path)\n    os.makedirs(output)\nelse:\n    print('env variable must be defined')\n\nbk_train_fol = '/book_train.parquet/'\ntd_train_fol = '/trade_train.parquet/'\nbk_test_fol = '/book_test.parquet/'\ntd_test_fol = '/trade_test.parquet/'\n\nmodel_final = 'finalized_model.sav'\n\nRANDOM_SEED = 42\n\n# Remove non efficient (and slow) cells to be faster\nfast = False\n\n# if None take all the dataset\nnumber_of_stocks = 5\n\npd.set_option('max_rows', 300)\npd.set_option('max_columns', 300)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Exploration\n","metadata":{}},{"cell_type":"markdown","source":"## Functions\n","metadata":{}},{"cell_type":"code","source":"def load_df(df_folder, nb_stock_to_load=0, data_folder=data_folder):\n    '''load a parquet \n    \n    arguments\n    ---------------\n    data_folder (str)\n    df_folder (str)\n    nb_stock_to_load (int)\n        number of subfolders to load\n    '''\n    stock_list = os.listdir(data_folder + df_folder)\n\n    if nb_stock_to_load == 0:\n        nb_stock_to_load = len(stock_list)\n    nb_stock_to_load = min(nb_stock_to_load, len(stock_list))\n    \n    if nb_stock_to_load == 1:\n        df = pd.read_parquet(data_folder + df_folder + '/stock_id=0')\n        df['stock_id'] = 0\n    else:\n        ## depreciated\n        # subset_paths = []\n        # for stock in stock_list[:nb_stock_to_load]:\n        #     subset_path = glob.glob(data_folder + df_folder + stock + '/*')\n        #     subset_paths.append(subset_path[0])\n\n        subset_paths = [glob.glob(data_folder + df_folder + stock + '/*')[0] for stock in stock_list[:nb_stock_to_load]]\n        ## doesn't work\n        # subset_paths = glob.glob(data_folder + df_folder + '/*')[:nb_stock_to_load]\n        \n        df = pd.read_parquet(subset_paths)\n        df['stock_id'] = df['stock_id'].astype(int)\n    return df","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"###############################\n# Functions to add features\n###############################\n\ndef add_wap(df, number=1, column_prefix='wap', standard=True):\n    '''adding one wap\n\n    number (int): the position of the price to take it could be 1 or 2\n    standard (bool): use standard method to calculate wap or use a custom method\n    '''\n    if standard:\n        df[column_prefix + str(number)] = (\n            df['bid_price'+ str(number)] * df['ask_size'+ str(number)] + df['ask_price'+ str(number)] * df['bid_size'+ str(number)]) / (\n                df['ask_size'+ str(number)]+ df['bid_size'+ str(number)])\n    else:\n        df[column_prefix + str(number) + '_ns'] = (\n            df['bid_price'+ str(number)] * df['bid_size'+ str(number)] + df['ask_price'+ str(number)] * df['ask_size'+ str(number)]) / (\n                df['ask_size'+ str(number)]+ df['bid_size'+ str(number)])\n\ndef log_return(list_stock_prices):\n    return np.log(list_stock_prices).diff()\n\ndef realized_volatility(series_log_return):\n    return np.sqrt(np.sum(series_log_return**2))\n\ndef add_waps(df):\n        '''add many waps'''\n        add_wap(df, 1, column_prefix='wap')\n        add_wap(df, 2, column_prefix='wap')\n        add_wap(df, 1, column_prefix='wap', standard=False)\n        add_wap(df, 2, column_prefix='wap', standard=False)\n        df['wap_p'] = ((\n                df['wap1'] * (df['ask_size1'] + df['bid_size1']) +\n                df['wap2'] * (df['ask_size2'] + df['bid_size2'])) /\n                (df['ask_size1'] + df['bid_size1'] + df['ask_size2'] + df['bid_size2']))\n        df['wap_balance'] = abs(df['wap1'] - df['wap2'])\n\ndef add_log_return(df, price_col, log_col_name, group='time_id'):\n        df[log_col_name] = df.groupby([group])[price_col].apply(log_return)\n\ndef add_spreads(df):\n        # # tests with ponderates features\n        # df['bid_spread_p'] = (df['bid_price1'] * df['bid_size1'] - df['bid_price2'] * df['bid_size1'])/(df['bid_size1'] + df['bid_size2'])\n        # df['ask_spread_p'] = (df['ask_price1'] * df['ask_size1'] - df['ask_price2'] * df['ask_size1'])/(df['ask_size1'] + df['ask_size2'])\n        # df[\"bid_ask_spread_p\"] = abs(df['bid_spread_p'] - df['ask_spread_p'])\n        df['bid_spread'] = df['bid_price1'] - df['bid_price2']\n        df['ask_spread'] = df['ask_price1'] - df['ask_price2']\n        df['price_spread'] = (df['ask_price1'] - df['bid_price1']) / ((df['ask_price1'] + df['bid_price1'])/2)\n        df[\"bid_ask_spread1\"] = (df['ask_price1'] - df['bid_price1'])/df['bid_price1']\n        df[\"bid_ask_spread2\"] = (df['ask_price2'] - df['bid_price2'])/df['bid_price2']\n        df[\"bid_ask_spread_p\"] = ((df['ask_price1'] + df['ask_price2']) - (df['bid_price1'] + df['bid_price2']))/(df['bid_price1'] + df['bid_price2'])\n\ndef add_volumes(df):\n        df['total_volume'] = (df['ask_size1'] + df['ask_size2']) + (df['bid_size1'] + df['bid_size2'])\n        df['volume_imbalance'] = abs((df['ask_size1'] + df['ask_size2']) - (df['bid_size1'] + df['bid_size2']))\n\ndef add_EMA(df, wap_col, nb_period):\n        df[wap_col + '_' + str(nb_period) + 'sec_EWM'] = df[wap_col].ewm(span=nb_period, adjust=False).mean()\n\n###############################\n# Evaluation\n###############################\ndef rmspe(y_true, y_pred):\n    return  (np.sqrt(np.mean(np.square((y_true - y_pred) / y_true))))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Book train\n","metadata":{}},{"cell_type":"code","source":"book_train = load_df(bk_train_fol, nb_stock_to_load=1)\nbook_train.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"book_train.info()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Sample\n","metadata":{}},{"cell_type":"code","source":"# Sample\nbook_train_sample = book_train[(book_train['stock_id'] == 0) & (book_train['time_id'] < 35)].copy()\nadd_wap(book_train_sample)\nfig = px.line(book_train_sample, x=\"seconds_in_bucket\", y=\"wap1\", title='WAP of stock_id_0, time_id <35', color='time_id')\nif env == 'local':\n    fig.write_image(save_path + 'wap_sample.png')\nfig.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"book_train_sample['log_return'] = book_train_sample.groupby(['time_id'])['wap1'].apply(log_return)\nbook_train_sample = book_train_sample[~book_train_sample['log_return'].isnull()] # removing each Nan of firsts time_id, ~ : invers the mask","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = px.line(book_train_sample, x=\"seconds_in_bucket\", y=\"log_return\", title='Log return of stock_id_0, time_id <35', color='time_id')\nif env == 'local':\n    fig.write_image(save_path + 'logreturn_sample.png')\nfig.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Realized volatility on our sample\nrealized_vol = book_train_sample.groupby(['time_id'])['log_return'].agg(realized_volatility)\nprint('Realized volatility for stock_id 0 :')\nfor i in realized_vol.index:\n    print(f'- time_id {i} is {round(realized_vol.loc[i], 7)}')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Trade train\n","metadata":{}},{"cell_type":"code","source":"# Test tp know if the book and trade data are on same stocks\nos.listdir(data_folder + td_train_fol) == os.listdir(data_folder + bk_train_fol)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trade_train = load_df(td_train_fol, nb_stock_to_load=2)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trade_train.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Sample\n","metadata":{}},{"cell_type":"code","source":"trade_train_sample = trade_train[(trade_train.stock_id == 0) & (trade_train.time_id < 35)]\n\nfig = px.line(trade_train_sample, x=\"seconds_in_bucket\", y=\"price\", title='Price of stock_id_0, time_id <35', color='time_id')\nif env == 'local':\n    fig.write_image(save_path + 'trade_prices_sample.png')\nfig.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12, 6))\nsns.scatterplot(data=trade_train_sample, x=\"price\", y=\"size\", hue=\"order_count\")\nif env == 'local' or env == 'kaggle':\n    plt.savefig(save_path + 'trade_sample.png')\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Book/Trade test\n\nThese file are here just to show the shape and firsts value of the hidden 10 min window.\n","metadata":{}},{"cell_type":"code","source":"book_test = load_df(bk_test_fol)\nbook_test.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trade_test = load_df(td_test_fol)\ntrade_test.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Targets / realized volatility\n","metadata":{}},{"cell_type":"code","source":"# this dataset is just a sample, it will be replaced by the real one at each submission.\nvol_test = pd.read_csv(data_folder +'/test.csv')\nvol_test","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vol_train = pd.read_csv(data_folder +'/train.csv')\nvol_train.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vol_train.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vol_stock0 = vol_train[vol_train['stock_id'] == 0]\n\nsns.set_theme(style=\"ticks\")\nfig = plt.figure(figsize=(16, 6))\n# fig.suptitle('Images after equalization preprocessing', fontsize=16)\n# fig.tight_layout()\n\nplt.subplot(1, 2, 1)\nplt.title(\"Train realized volatility\")\nplt.hist(vol_stock0['target'], bins=50)\n\nplt.subplot(1, 2, 2)\nplt.title(\"Train realized volatility - log\")\nplt.hist(np.log(vol_stock0['target']), bins=50)\n\nif env == 'local' or env == 'kaggle':\n    plt.savefig(save_path + 'realized_volatility.png')\n\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessing & baseline\n\n---\n","metadata":{}},{"cell_type":"markdown","source":"## Functions\n","metadata":{}},{"cell_type":"code","source":"###############################\n# Lists of dataset paths\n###############################\n\n# Create a list of stocks paths books from the dataset\nif number_of_stocks is None:\n    list_order_book_file_train = glob.glob(data_folder + bk_train_fol + '*')\n    list_order_trade_file_train = glob.glob(data_folder + td_train_fol + '*')\n    stock_id_max = max([int(path.split('=')[1]) for path in list_order_trade_file_train]) # files on kaggle are random sorted\nelse:\n    stock_id_max = number_of_stocks-1 # stocks start at 0\n    # take only stocks <= stock_id_max\n    list_order_book_file_train = [path for path in glob.glob(data_folder + bk_train_fol + '*') if int(path.split('=')[1]) <= stock_id_max]\n    list_order_trade_file_train = [path for path in glob.glob(data_folder + td_train_fol + '*') if int(path.split('=')[1]) <= stock_id_max]\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Naive RMSPE\n\nUn fait bien connu à propos de la volatilité est qu'elle a tendance à être autocorrélée. Nous pouvons utiliser cette propriété pour implémenter un modèle naïf qui \"prédit\" simplement la volatilité réalisée en utilisant la volatilité réalisée au cours des 10 premières minutes.\n\nCalculons la volatilité réalisée de la première partie de la fenêtre sur le jeu de donnée train.\n","metadata":{}},{"cell_type":"code","source":"# select all stocks books\nlist_order_book_file_train = glob.glob(data_folder + bk_train_fol + '/*')\nlist_order_book_file_train[:2] # sample","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# specific for naive model\ndef realized_volatility_per_time_id(file_path, prediction_column_name):\n    '''load datas of one stock_id then calculate WAP, log_return\n    set a new DF and put inside realized_volatility per time_id\n    add a column with competition form : {stock_id}-{time_id} called row_id\n\n    file_path : path of subfolders with stock_id\n        example : ./data/book_train/stock_id=0\n    prediction_column_name : name of the realized_volatility column\n    \n    return row_id, prediction_name columns'''\n    df_book_data = pd.read_parquet(file_path)\n    add_wap(df_book_data)\n\n    df_book_data['log_return'] = df_book_data.groupby(['time_id'])['wap1'].apply(log_return)\n    df_book_data = df_book_data[~df_book_data['log_return'].isnull()] # removing each Nan of firsts time_id, ~ : invers the mask\n\n    df_realized_vol_per_stock =  pd.DataFrame(df_book_data.groupby(['time_id'])['log_return'].agg(realized_volatility)).reset_index()\n    df_realized_vol_per_stock = df_realized_vol_per_stock.rename(columns = {'log_return':prediction_column_name})\n    \n    stock_id = file_path.split('=')[1]\n    df_realized_vol_per_stock['row_id'] = df_realized_vol_per_stock['time_id'].apply(lambda x:f'{stock_id}-{x}')\n    \n    return df_realized_vol_per_stock[['row_id',prediction_column_name]]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def past_realized_volatility_per_stock(list_file,prediction_column_name):\n    df_past_realized = pd.DataFrame()\n    for file in list_file:\n        df_past_realized = pd.concat([df_past_realized,\n                                     realized_volatility_per_time_id(file,prediction_column_name)])\n    return df_past_realized\n\n# test on all 126 stocks \n# long ! 230 sec\nif not(fast):\n    df_past_realized_train = past_realized_volatility_per_stock(list_file=list_order_book_file_train,\n                                                            prediction_column_name='pred')\n    df_past_realized_train.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if not(fast):\n    df_naive = vol_train.copy()\n    # Let's join the output dataframe with train.csv to see the performance of the naive prediction on training set.\n    # naive prediction = predict same volatility in the next 10min window (auto realisation)\n    df_naive['row_id'] = df_naive['stock_id'].astype(str) + '-' + df_naive['time_id'].astype(str)\n    df_naive = df_naive[['row_id','target']]\n    df_naive = df_naive.merge(df_past_realized_train[['row_id','pred']], on = ['row_id'], how = 'left')\n    df_naive.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if not(fast):\n    R2 = round(r2_score(y_true = df_naive['target'], y_pred = df_naive['pred']),3)\n    RMSPE = round(rmspe(y_true = df_naive['target'], y_pred = df_naive['pred']),3)\n    print(f'Performance of the naive prediction: R2 score: {R2}, RMSPE: {RMSPE}')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Order book train\n","metadata":{}},{"cell_type":"markdown","source":"**Process flow**  \n<br>\n<img src=\"https://github.com/abugeia/P8_kaggle_competition/blob/master/img/p8_process_orders.png?raw=true\" width=\"900px\"><br>\n","metadata":{}},{"cell_type":"code","source":"# list of waps for applying log return, EMA and EMA log return\n# this list is also used in the creation of aggregation dic\nwaps = ['wap1', 'wap2', 'wap1_ns', 'wap2_ns', 'wap_p']\n\ndef book_feature_eng_per_stock(file_path, waps=waps):\n    ''' Load datas of one stock_id then adding features.\n    Removing Nan rows of theses features\n    \n    file_path : path of subfolders with stock_id\n        example : ./data/book_train/stock_id=0\n\n    return the df '''\n    book_train = pd.read_parquet(file_path)\n\n    add_waps(book_train)\n    \n    for wap in waps:\n        add_log_return(book_train, price_col=wap, log_col_name=wap + '_log_return')\n        for period in  [20, 100]:\n            add_EMA(book_train, wap, period)\n            EMA_col_name = wap + '_' + str(period) + 'sec_EWM'\n            add_log_return(book_train, price_col=EMA_col_name, log_col_name=EMA_col_name + '_log_return')\n            book_train['diff_' + EMA_col_name] = abs(book_train[wap] - book_train[EMA_col_name])\n    \n    add_spreads(book_train)\n    add_volumes(book_train)\n\n    # book_train = book_train[~(book_train['wap1_log_return'].isnull() | book_train['log_return2'].isnull() | book_train['log_return_p'].isnull())] # at the end ?\n    book_train = book_train.fillna(book_train.median())\n    \n    return book_train","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sample with stock 0\ndf_sample = book_feature_eng_per_stock(list_order_book_file_train[0])\ndf_sample.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# list of spreads to apply the aggregate functions\nspreads = ['bid_ask_spread1', 'bid_ask_spread2', 'bid_ask_spread_p', 'bid_spread', 'ask_spread', 'price_spread']\n\n#########################################\n# Creation of order book aggregation dic\n#########################################\nbk_feature_dic = {}\nfor wap in waps:\n    bk_feature_dic[wap + '_log_return'] = [realized_volatility]\n    # bk_feature_dic[wap] = [np.std, pd.Series.mad]\n    for period in [20, 100]:\n        EMA_col_name = wap + '_' + str(period) + 'sec_EWM'\n        # bk_feature_dic[EMA_col_name + '_log_return'] = [realized_volatility]\n        bk_feature_dic['diff_' + EMA_col_name] = [np.sum, np.std]\nfor spread in spreads:\n    bk_feature_dic[spread] = [np.sum, np.std]\n\nbk_feature_dic['total_volume'] = [np.sum, np.mean]\nbk_feature_dic['volume_imbalance'] = [np.std]\nbk_feature_dic['wap_balance'] = [np.sum, np.mean]\n\nbk_feature_dic","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def book_agg_form_parallele(file):\n    ''' Create a new df that aggregate data by time_id and apply the feature dic\n    add :\n    - a stock_id columns\n    - a competition form column : {stock_id}-{time_id} called row_id\n    return the new df\n    '''\n    stock_id = file.split('=')[1]\n    df_agg_stock = book_feature_eng_per_stock(file)\n    df_agg_stock = pd.DataFrame(df_agg_stock.groupby(['time_id']).agg(bk_feature_dic).reset_index())\n\n    df_agg_stock.columns = ['_'.join(col).rstrip('_') for col in df_agg_stock.columns.values]\n    df_agg_stock['row_id'] = df_agg_stock['time_id'].apply(lambda x:f'{stock_id}-{x}')\n    # df_agg_stock['stock_id'] = stock_id\n    df_agg_stock.drop('time_id', axis=1, inplace=True)\n\n    return df_agg_stock","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def agg_df_and_concatenate_parallel(paths_list, func):\n    ''' Create an concateneted df of preprocessed stocks df by the func'''\n\n    df_agg = Parallel(n_jobs=-1)(\n        delayed(func)(file) \n        for file in paths_list)\n    \n    df_agg = pd.concat(df_agg, ignore_index = True)\n\n    return df_agg","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ndf_order_agg = agg_df_and_concatenate_parallel(list_order_book_file_train, book_agg_form_parallele)\ndf_order_agg.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n\n## Trades book train\n","metadata":{}},{"cell_type":"markdown","source":"**Process flow**  \n<br>\n<img src=\"https://github.com/abugeia/P8_kaggle_competition/blob/master/img/p8_process_trades.png?raw=true\" width=\"900px\"><br>\n","metadata":{}},{"cell_type":"code","source":"###############################\n# Functions to add features\n###############################\ndef add_amount(df):\n    df['amount'] = df['price'] * df['size']\ndef add_power(df):\n    df['power'] = (df['price'] - df['price'].shift(1))/df['price']*df['size']\n\n# Function to count unique elements of a series\ndef count_unique(series):\n    return len(np.unique(series))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def trade_feature_eng_per_stock(file_path):\n    ''' Load datas of one stock_id then adding features.\n    Removing Nan rows of theses features\n    \n    file_path : path of subfolders with stock_id\n        example : ./data/trade_train/stock_id=0\n\n    return the df '''\n    df = pd.read_parquet(file_path)\n        \n    add_log_return(df, price_col='price', log_col_name='td_log_return')\n\n    add_amount(df)\n    for period in  [20, 100]:\n        add_EMA(df, 'amount', period)\n        EMA_col_name = 'amount_' + str(period) + 'sec_EWM'\n        add_log_return(df, price_col=EMA_col_name, log_col_name=EMA_col_name + '_log_return')\n        \n    df['diff_td'] = df.seconds_in_bucket.diff() # same Nan as log_return\n    df['amount_p_order'] = df.amount / df.order_count\n    add_power(df)\n\n    df = df[~df['td_log_return'].isnull()]\n    return df","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sample with stock 0\ndf_sample = trade_feature_eng_per_stock(list_order_trade_file_train[0])\ndf_sample.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#########################################\n# Creation of order book aggregation dic\n#########################################\ntd_feature_dic = {}\nfor period in  [20, 100]:\n    EMA_col_name = 'amount_' + str(period) + 'sec_EWM_log_return'\n    td_feature_dic[EMA_col_name] = [realized_volatility]\ntd_feature_dic['td_log_return'] = [realized_volatility]\n# td_feature_dic['seconds_in_bucket'] = [count_unique] # removed after feature importance analysis\ntd_feature_dic['diff_td'] = [np.std]\ntd_feature_dic['amount_p_order'] = [np.mean]\ntd_feature_dic['price'] = [np.mean]\ntd_feature_dic['amount'] = [np.std, pd.Series.mad]\ntd_feature_dic['amount_p_order'] = [np.mean, np.sum]\n# td_feature_dic['size'] = [np.mean, np.sum] # removed after feature importance analysis\ntd_feature_dic['order_count'] = [np.mean, np.sum]\n\ntd_feature_dic","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def trade_agg_form_parallele(file):\n    ''' Create a new df that aggregate data by time_id\n    add :\n    - a stock_id columns\n    - a with competition form column : {stock_id}-{time_id} called row_id\n    return the new df\n    '''\n    stock_id = file.split('=')[1]\n    df_agg_stock = trade_feature_eng_per_stock(file)\n    df_agg_stock = pd.DataFrame(df_agg_stock.groupby(['time_id']).agg(td_feature_dic)).reset_index()\n    \n    df_agg_stock.columns = ['_'.join(col).rstrip('_') for col in df_agg_stock.columns.values]\n    df_agg_stock['row_id'] = df_agg_stock['time_id'].apply(lambda x:f'{stock_id}-{x}')\n    # df_agg_stock['stock_id'] = stock_id\n    df_agg_stock.drop('time_id', axis=1, inplace=True)\n\n    return df_agg_stock\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ndf_trade_agg = agg_df_and_concatenate_parallel(list_order_trade_file_train, trade_agg_form_parallele)\ndf_trade_agg.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n\n## Final DF train\n","metadata":{}},{"cell_type":"code","source":"def process_final_df(df_order_agg, df_trade_agg, df_target):\n    '''select the targets of the chosen stocks\n    merge target df to order and trades df\n    return new df'''\n\n    df = df_target[df_target.stock_id <= stock_id_max].copy()\n    #  adding the same index in our books df to merge\n    df['row_id'] = df['stock_id'].astype(str) + '-' + df['time_id'].astype(str)\n\n    df = df.merge(df_order_agg, on = ['row_id'], how = 'left')\n    df = df.merge(df_trade_agg, on = ['row_id'], how = 'left')\n\n    return df    ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = process_final_df(df_order_agg, df_trade_agg, vol_train)\ndf_train.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"###########################################\n# Ploting realized volatility per stock\n###########################################\n\n# #Création d'un sous échantillon par modalité\ngroupes = []\nfor s in df_train['stock_id'].unique():\n    groupes.append(df_train[df_train['stock_id'] == s]['wap1_log_return_realized_volatility'])\n \n# 'OO' méthode pour plot\nfig, ax = plt.subplots(figsize=(30,8))\n\n# Propriétés graphiques\nmedianprops = {'color':\"black\"}\nmeanprops = {'marker':'o', 'markeredgecolor':'black',\n            'markerfacecolor':'firebrick'}\n\nax.boxplot(groupes,\n           labels=df_train['stock_id'].unique(),\n           showfliers=False,\n           medianprops=medianprops, \n           vert=True,\n           patch_artist=True,\n           showmeans=True,\n           meanprops=meanprops)\n\nax.set(title='Distribution des wap1_log_return_realized_volatility par stock',\n      xlabel=\"Stock Id\",\n      ylabel='wap1_log_return_realized_volatility')\n\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"###############################\n# Saving preprocessed train ds\n###############################\n# df_train.to_pickle(output + 'dataset_train.bz2', compression='bz2')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Test dataset\n","metadata":{}},{"cell_type":"code","source":"# list of test books paths\nlist_order_book_file_test = glob.glob(data_folder + bk_test_fol + '*')\nlist_order_trade_file_test = glob.glob(data_folder + td_test_fol + '*')\n\n# preprocess test dataset\ndf_order_test_agg = agg_df_and_concatenate_parallel(list_order_book_file_test, book_agg_form_parallele)\ndf_trade_test_agg = agg_df_and_concatenate_parallel(list_order_trade_file_test, trade_agg_form_parallele)\n\n# Merging df\ndf_test = process_final_df(df_order_test_agg, df_trade_test_agg, vol_test)\ndf_test.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Machine learning\n\n---\n","metadata":{}},{"cell_type":"markdown","source":"### Functions and variables\n","metadata":{}},{"cell_type":"code","source":"# For optuna studies\nn_trials = 10\n\nkfolds = KFold(n_splits=5, shuffle=True, random_state=RANDOM_SEED)\n\nscorer_rmspe = make_scorer(rmspe,\n    # greater_is_better=False\n    )","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## New idea for the dic structure\n# dic_eval = dict.fromkeys(['names', 'models', 'rmspe_scores', 'r2_scores'])\n# dic_eval","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dic_eval = {}\ndef evaluate(name, model, dic, X_test, y_test):\n    y_pred = model.predict(X_test)\n    R2 = round(r2_score(y_test, y_pred), 6)\n    RMSPE = round(rmspe(y_test, y_pred), 6)\n    dic[name] = [model, RMSPE, R2]\n    print(f'Performance of the {name} prediction: R2 score: {R2}, RMSPE: {RMSPE}')\n\ndef evaluateCV(name, model, dic, X_train, y_train, save=True):\n    start_time = time.time()\n    RMSPE =  round(cross_val_score(\n        model, X_train, y_train, cv=kfolds, scoring=scorer_rmspe\n    ).mean(), 6)\n    # model.fit(X_train, y_train)\n    if save:\n        dic[name] = [model, RMSPE]\n    print(f'RMSPE of the {name} prediction: {RMSPE} in {round(time.time() - start_time, 3)} sec.')\n    if not(save):\n        return RMSPE","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Dataset\n","metadata":{}},{"cell_type":"code","source":"###############################\n# Loading preprocessed train ds\n###############################\n# df_train = pd.read_pickle(output + 'dataset_train.bz2')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.isnull().sum()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.fillna(df_train.median(), inplace=True)\ndf_train.isnull().sum().sum()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Scalers\n","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import QuantileTransformer\n\n\nqtn = QuantileTransformer(output_distribution='normal', random_state=42)\nqtu = QuantileTransformer(output_distribution='uniform', random_state=42)\nstd = StandardScaler()\nminmax = MinMaxScaler()\n\nscalers = [qtn, qtu, std, minmax]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def scaler_selection(scalers):\n    rmspe_min = 1\n    for scaler in scalers:\n        X = df_train.drop(['row_id', 'target'], axis = 1)\n        y = df_train['target']\n\n        X_train, X_test, y_train, y_test = train_test_split(X, y,\n        test_size=0.15, random_state=42, shuffle=True)\n\n        # model_xgb = make_pipeline(scaler,\n        #                     XGBRegressor(tree_method='hist', random_state=42, n_jobs= - 1))\n        if env == 'kaggle':\n            model_xgb = make_pipeline(scaler,\n            XGBRegressor(tree_method='gpu_hist', random_state=42, n_jobs= - 1))\n        else:\n            model_xgb = make_pipeline(scaler,\n                XGBRegressor(tree_method='hist', random_state=42, n_jobs= - 1))\n\n        # model_xgb.fit(X_train, y_train)\n        # evaluateCV('XGBOOST_'+ str(scaler), model_xgb, dic_eval, X_test, y_test, save=False)\n        rmspe_model = evaluateCV('XGBOOST_'+ str(scaler), model_xgb, dic_eval, X_train, y_train, save=False)\n\n        # rmspe_model = dic_eval['XGBOOST_'+ str(scaler)][1]\n        if rmspe_model < rmspe_min:\n            rmspe_min = rmspe_model\n            selected_scaler = scaler\n\n    return selected_scaler, rmspe_min\n\nselected_scaler, rmspe_scaler = scaler_selection(scalers)\nprint(f'the selected scaler is {selected_scaler}')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Train values\n","metadata":{}},{"cell_type":"code","source":"X_train = df_train.drop(['row_id', 'target'], axis = 1)\n# X_val = X.values\ny_train = df_train['target']\n# y_val = y.values\n\nX_train.shape, y_train.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Test values\n","metadata":{}},{"cell_type":"code","source":"X_test = df_test.drop(['row_id'], axis = 1)\ndf_pred = df_test[['row_id']]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## We don't split anymore as we use CV\n# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42, shuffle=False)\n# X_train.shape, X_test.shape, y_train.shape, y_test.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Ridge\n\n---\n","metadata":{}},{"cell_type":"code","source":"def tune(objective, n_trials=n_trials):\n    start_time = time.time()\n    study = optuna.create_study(direction=\"minimize\", pruner=optuna.pruners.MedianPruner(n_warmup_steps=5))\n    study.optimize(objective, n_trials=n_trials, gc_after_trial=True)\n\n    params = study.best_params\n    print(\"--- %s seconds ---\" % (time.time() - start_time))\n    return params","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def ridge_objectiveCV(trial):\n\n    _alpha = trial.suggest_float(\"alpha\", 1e-8, 20, log=True)\n\n    # normalize=True to add ?\n    # model_ridge = make_pipeline(selected_scaler,\n    #     Ridge(alpha=_alpha, random_state=RANDOM_SEED))    \n    model_ridge = Ridge(alpha=_alpha, random_state=RANDOM_SEED, normalize=True)\n\n    score = cross_val_score(\n        model_ridge, X_train, y_train, cv=kfolds, scoring=scorer_rmspe\n    ).mean()\n    return score","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if not(fast):\n    ridge_params = tune(ridge_objectiveCV)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if not(fast):\n    ridge_opt = Ridge(**ridge_params, random_state=RANDOM_SEED, normalize=True)\n    ## V17\n    # ridge_opt.fit(X, y)\n    # evaluate('Ridge', ridge_opt, dic_eval, X_test, y_test)\n\n    evaluateCV('Ridge', ridge_opt, dic_eval, X_train, y_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Lasso\n\n---\n","metadata":{}},{"cell_type":"code","source":"def lasso_objective(trial):\n    _alpha = trial.suggest_loguniform(\"alpha\", 0.0001, 10)\n    lasso = Lasso(alpha=_alpha, random_state=RANDOM_SEED)\n\n    score = cross_val_score(\n        lasso, X_train, y_train, cv=kfolds, scoring=scorer_rmspe\n    ).mean()\n    return score","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if not(fast):\n    lasso_params = tune(lasso_objective, n_trials=5) # long and results are bad\n    lasso_opt = Lasso(**lasso_params, random_state=RANDOM_SEED)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if not(fast):\n    # lasso_opt.fit(X_train, y_train)\n    # evaluate('Lasso', lasso_opt, dic_eval, X_test, y_test)\n\n    evaluateCV('Lasso', lasso_opt, dic_eval, X_train, y_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## RandomForrest\n","metadata":{}},{"cell_type":"code","source":"# def randomforest_objective(trial):\n#     _n_estimators = trial.suggest_int(\"n_estimators\", 50, 200)\n#     _max_depth = trial.suggest_int(\"max_depth\", 5, 20)\n#     _min_samp_split = trial.suggest_int(\"min_samples_split\", 2, 10)\n#     _min_samples_leaf = trial.suggest_int(\"min_samples_leaf\", 2, 10)\n\n#     rf = RandomForestRegressor(\n#         max_depth=_max_depth,\n#         min_samples_split=_min_samp_split,\n#         min_samples_leaf=_min_samples_leaf,\n#         n_estimators=_n_estimators,\n#         n_jobs=-1,\n#         random_state=RANDOM_SEED,\n#     )\n\n#     rf.fit(X_train, y_train)\n\n#     preds = rf.predict(X_test)\n#     return rmspe(y_test, preds)\n\ndef randomforest_objectiveCV(trial):\n    _n_estimators = trial.suggest_int(\"n_estimators\", 50, 200)\n    _max_depth = trial.suggest_int(\"max_depth\", 5, 20)\n    _min_samp_split = trial.suggest_int(\"min_samples_split\", 2, 10)\n    _min_samples_leaf = trial.suggest_int(\"min_samples_leaf\", 2, 10)\n\n    rf = RandomForestRegressor(\n        max_depth=_max_depth,\n        min_samples_split=_min_samp_split,\n        min_samples_leaf=_min_samples_leaf,\n        n_estimators=_n_estimators,\n        n_jobs=-1,\n        random_state=RANDOM_SEED,\n    )\n    score = cross_val_score(\n        rf, X_train, y_train, cv=kfolds, scoring=scorer_rmspe\n    ).mean()\n    return score","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if not(fast):\n    randomforest_params = tune(randomforest_objectiveCV, n_trials=5) # long, average results...\n    rf_opt = RandomForestRegressor(n_jobs=-1, random_state=RANDOM_SEED, **randomforest_params)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if not(fast):\n    ## V17\n    # rf_opt.fit(X_train, y_train)\n    # evaluate('RandomForrest', rf_opt, dic_eval, X_test, y_test)\n\n    evaluateCV('RandomForrest', rf_opt, dic_eval, X_train, y_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Basic XGB model\n\n---\n","metadata":{}},{"cell_type":"code","source":"## Old function\n# def plot_feature_importance(df_train, model):\n#     feature_importances_df = pd.DataFrame({\n#         'feature': df_train.columns,\n#         'importance_score': model.feature_importances_\n#     })\n#     fig = plt.figure(figsize=(20, 5))\n#     ax = sns.barplot(x = \"feature\", y = \"importance_score\", data = feature_importances_df)\n#     ax.set(xlabel=\"Features\", ylabel = \"Importance Score\")\n#     # plt.xticks(ha='left', rotation=45)\n#     fig.autofmt_xdate(bottom=0.2, rotation=30, ha='right')\n#     plt.show()\n#     # return feature_importances_df\n\ndef plot_feature_importance(df_train, model, name=None):\n\n    feature_imp = pd.DataFrame(sorted(zip(model.feature_importances_,df_train.columns)), columns=['Value','Feature'])\n\n    plt.figure(figsize=(20, 10))\n    sns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False))\n    plt.title('Features importance')\n    plt.tight_layout()\n    if (env == 'local' or env == 'kaggle') and name is not None:\n        plt.savefig(save_path + name +'_feat_imp.png')\n    plt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if env == 'kaggle':\n    xgb = XGBRegressor(tree_method='gpu_hist', random_state=42, n_jobs= - 1)\nelse:\n    xgb = XGBRegressor(tree_method='hist', random_state=42, n_jobs= - 1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# %%time\n# xgb.fit(X_train, y_train)\n# evaluate('XGBOOST', xgb, dic_eval, X_test, y_test)\n\nevaluateCV('XGBOOST', xgb, dic_eval, X_train, y_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if not(fast):\n    xgb.fit(X_train, y_train)\n    plot_feature_importance(X_train, xgb, 'xgb')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Basic LGBMRegressor model\n","metadata":{}},{"cell_type":"code","source":"if env == 'kaggle':\n    lgbm = LGBMRegressor(device='gpu', random_state=42)\nelse:\n    lgbm = LGBMRegressor(device='cpu', random_state=42)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# %%time\n# lgbm.fit(X_train, y_train)\n# evaluate('LIGHTGBM', lgbm, dic_eval, X_test, y_test)\n\nevaluateCV('LIGHTGBM', lgbm, dic_eval, X_train, y_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if not(fast):\n    lgbm.fit(X_train, y_train)\n    plot_feature_importance(X_train, lgbm, 'lgbm')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Removing useless features\n","metadata":{}},{"cell_type":"code","source":"if not(fast):\n    features_imp_lgbm_xgb = [x/sum(lgbm.feature_importances_) + y/sum(xgb.feature_importances_) for x, y in zip(lgbm.feature_importances_, xgb.feature_importances_)] \n    feature_imp = pd.DataFrame(sorted(zip(features_imp_lgbm_xgb,X_train.columns)), columns=['Value','Feature'])\n\n    plt.figure(figsize=(20, 10))\n    sns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False))\n    plt.title('LightGBM Features (avg over folds)')\n    plt.tight_layout()\n    plt.show()\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## CatBoost\n","metadata":{}},{"cell_type":"code","source":"cbr = CatBoostRegressor(iterations=500, random_seed=42)\n# Fit model\n# cbr.fit(X_train, y_train)\n# evaluate('catboost', cbr, dic_eval, X_test, y_test)\n\nevaluateCV('catboost', cbr, dic_eval, X_train, y_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Optuna Tuned XGBoost\n\nOptuna va nous permettre de trouver nos meilleurs hyperparamètres.  \nIl suffira ensuite d'entraîner notre modèle avec ces paramètres pour l'évaluer.\n","metadata":{}},{"cell_type":"code","source":"# def objective_xgb(trial):\n\n#     param = {'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n#             'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n#             'colsample_bytree': trial.suggest_categorical('colsample_bytree', [0.3,0.4,0.5,0.6,0.7,0.8,0.9, 1.0]),\n#             'subsample': trial.suggest_categorical('subsample', [0.4,0.5,0.6,0.7,0.8,1.0]),\n#             'learning_rate': trial.suggest_categorical('learning_rate', [0.012,0.014,0.016,0.018, 0.02, 0.025]),\n#             'n_estimators': trial.suggest_int('n_estimators', 500, 3000),\n#             'max_depth': trial.suggest_categorical('max_depth', [5,7,9,11,13,15,17,20]),\n#             'min_child_weight': trial.suggest_int('min_child_weight', 1, 300)}\n\n#     if env == 'kaggle':\n#         param['tree_method'] = 'gpu_hist'\n#     else:\n#         param['tree_method'] = 'hist'\n    \n#     # model = make_pipeline(selected_scaler, XGBRegressor(**param, random_state=42))\n#     model = XGBRegressor(**param, random_state=42)\n    \n#     # pruning_callback = optuna.integration.XGBoostPruningCallback(trial, \"validation-rmse\")\n#     model.fit(X_train , y_train, eval_set=[(X_test, y_test)], early_stopping_rounds=100, verbose=False)\n    \n#     preds = model.predict(X_test)\n    \n#     return rmspe(y_test, preds)\n\n\ndef objective_xgbCV(trial):\n\n    param = {\n            # 'lambda': trial.suggest_loguniform('lambda', 1e-3, 1),\n            'alpha': trial.suggest_loguniform('alpha', 1e-3, 1),\n            'colsample_bytree': trial.suggest_categorical('colsample_bytree', [0.3,0.4,0.5,0.6,0.7,0.8,0.9]),\n            'subsample': trial.suggest_categorical('subsample', [0.5,0.6,0.7,0.8,1.0]),\n            'learning_rate': trial.suggest_categorical('learning_rate', [0.008,0.009,0.01,0.012,0.014,0.016,0.018, 0.02]),\n            'n_estimators': trial.suggest_int('n_estimators', 500, 3000),\n            # 'max_depth': trial.suggest_categorical('max_depth', [5,7,9,11,13,15]),\n            # 'min_child_weight': trial.suggest_int('min_child_weight', 1, 300)\n            }\n\n    if env == 'kaggle':\n        param['tree_method'] = 'gpu_hist'\n    else:\n        param['tree_method'] = 'hist'\n    \n    # model = make_pipeline(selected_scaler, XGBRegressor(**param, random_state=42))\n    model = XGBRegressor(**param, random_state=42)\n\n    score = cross_val_score(\n        model, X_train, y_train, cv=kfolds, scoring=scorer_rmspe\n    ).mean()\n    return score","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nstudy_xgb = optuna.create_study(direction='minimize', pruner=optuna.pruners.MedianPruner(n_warmup_steps=5))\nstudy_xgb.optimize(objective_xgbCV, n_trials=n_trials, gc_after_trial=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Number of finished trials:', len(study_xgb.trials))\nprint('Best trial:', study_xgb.best_trial.params)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if not(fast):\n    optuna.visualization.plot_optimization_history(study_xgb)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if not(fast):\n    optuna.visualization.plot_param_importances(study_xgb)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_xgbparams = study_xgb.best_params\nbest_xgbparams","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# best_xgbparams = {'lambda': 0.050695864818244944,\n#  'alpha': 0.23319827340456734,\n#  'colsample_bytree': 0.5,\n#  'subsample': 0.8,\n#  'learning_rate': 0.02,\n#  'n_estimators': 1590,\n#  'max_depth': 9,\n#  'min_child_weight': 218}\n\nif env == 'Kaggle':\n    xgb_opt = XGBRegressor(**best_xgbparams, tree_method='gpu_hist')\nelse:\n    xgb_opt = XGBRegressor(**best_xgbparams, tree_method='hist', n_jobs= - 1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# %%time\n# xgb_opt.fit(X_train, y_train, eval_set=[(X_test, y_test)], early_stopping_rounds=100, verbose=False)\n\n# evaluate('XGB_opt', xgb_opt, dic_eval, X_test, y_test)\n\nevaluateCV('XGB_opt', xgb_opt, dic_eval, X_train, y_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Optuna Tuned LGBM\n","metadata":{}},{"cell_type":"code","source":"# def objective_lgbm(trial):\n#         param = {\"device\": \"gpu\",\n#                 \"metric\": \"rmse\",\n#                 \"verbosity\": -1,\n#                 'learning_rate':trial.suggest_loguniform('learning_rate', 0.005, 0.5),\n#                 \"max_depth\": trial.suggest_int(\"max_depth\", 2, 500),\n#                 \"lambda_l1\": trial.suggest_float(\"lambda_l1\", 1e-8, 10.0, log=True),\n#                 \"lambda_l2\": trial.suggest_float(\"lambda_l2\", 1e-8, 10.0, log=True),\n#                 \"num_leaves\": trial.suggest_int(\"num_leaves\", 2, 256),\n#                 \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 4000),\n#         #         \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 100000, 700000),\n#                 \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.4, 1.0),\n#                 \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.4, 1.0),\n#                 \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 1, 7),\n#                 \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 100)}\n\n#         if env == 'kaggle':\n#                 param[\"device\"] = \"gpu\"\n#         else:\n#                 param[\"device\"] = \"cpu\"\n\n#         pruning_callback = optuna.integration.LightGBMPruningCallback(trial, \"rmse\")\n#         model = LGBMRegressor(**param)\n\n#         model.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=False, callbacks=[pruning_callback], early_stopping_rounds=100)\n\n#         preds = model.predict(X_test)\n#         return rmspe(y_test, preds)\n\ndef objective_lgbmCV(trial):\n        param = {\n                \"metric\": \"rmse\",\n                \"verbosity\": -1,\n                'learning_rate':trial.suggest_loguniform('learning_rate', 0.005, 0.5),\n                \"max_depth\": trial.suggest_int(\"max_depth\", 2, 500),\n                # \"lambda_l1\": trial.suggest_float(\"lambda_l1\", 1e-8, 10.0, log=True),\n                # \"lambda_l2\": trial.suggest_float(\"lambda_l2\", 1e-8, 10.0, log=True),\n                \"num_leaves\": trial.suggest_int(\"num_leaves\", 2, 256),\n                \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 4000),\n        #         \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 100000, 700000),\n                # \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.4, 1.0),\n                \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.4, 1.0),\n                \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 1, 7),\n                \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 100)}\n\n        if env == 'kaggle':\n                param[\"device\"] = \"gpu\"\n        else:\n                param[\"device\"] = \"cpu\"\n\n        # pruning_callback = optuna.integration.LightGBMPruningCallback(trial, \"rmse\")\n        model = LGBMRegressor(**param, random_state=42)\n\n        score = cross_val_score(\n        # model, X_train, y_train, cv=kfolds, scoring=scorer_rmspe, fit_params={'callbacks': [pruning_callback]}\n        # ).mean()\n        model, X_train, y_train, cv=kfolds, scoring=scorer_rmspe).mean()\n        return score","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nstudy_lgbm = optuna.create_study(direction='minimize', pruner=optuna.pruners.MedianPruner(n_warmup_steps=5))\nstudy_lgbm.optimize(objective_lgbmCV, n_trials=n_trials, gc_after_trial=True) # n_jobs=-1 make the calcul longer !","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Number of finished trials:', len(study_lgbm.trials))\nprint('Best trial:', study_lgbm.best_trial.params)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if not(fast):\n    optuna.visualization.plot_optimization_history(study_lgbm)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if not(fast):\n    optuna.visualization.plot_param_importances(study_lgbm)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_lgbmparams = study_lgbm.best_params\nbest_lgbmparams","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# best_lgbmparams = {'learning_rate': 0.012206112226610026,\n#     'max_depth': 176,\n#     'lambda_l1': 0.0911256640760148,\n#     'lambda_l2': 7.619751773104654e-07,\n#     'num_leaves': 87,\n#     'n_estimators': 2713,\n#     'feature_fraction': 0.6744552501464487,\n#     'bagging_fraction': 0.7249343934370382,\n#     'bagging_freq': 7,\n#     'min_child_samples': 53}\n\nif env == 'Kaggle':\n    lgbm_opt = LGBMRegressor(**best_lgbmparams, device='gpu')\nelse:\n    lgbm_opt = LGBMRegressor(**best_lgbmparams, device='cpu')\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# %%time\n# lgbm_opt.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=False, early_stopping_rounds=100)\n\n# evaluate('LIGHTGBM_opt', lgbm_opt, dic_eval, X_test, y_test)\n\nevaluateCV('LIGHTGBM_opt', lgbm_opt, dic_eval, X_train, y_train)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Stacking Regressor\n\nStack of estimators with a final regressor.\n\nStacked generalization consists in stacking the output of individual estimator and use a regressor to compute the final prediction. Stacking allows to use the strength of each individual estimator by using their output as input of a final estimator.\n","metadata":{}},{"cell_type":"code","source":"if not(fast):\n    if env == 'kaggle':\n        tree_method='gpu_hist'\n        device='gpu'\n        n_jobs=None\n    else:\n        tree_method='hist'\n        device='cpu'\n        n_jobs=-1\n\n    xgb = XGBRegressor(tree_method=tree_method, random_state = RANDOM_SEED)\n    lgbm = LGBMRegressor(device=device, random_state=RANDOM_SEED)\n\n    estimators = [('lgbm_opt', lgbm_opt),\n                ('xgb_opt', xgb_opt),\n                ('lgbm', lgbm),\n                ('xgb', xgb)]\n\n    stack_reg = StackingRegressor(estimators=estimators, final_estimator=None, verbose=1, n_jobs=n_jobs)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if not(fast):\n    evaluateCV('Stack_reg', stack_reg, dic_eval, X_train, y_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Scaler on best model","metadata":{}},{"cell_type":"code","source":"def model_selection(dic):\n    rmspe_min = 1\n    for key in dic.keys():\n        rmspe_model = dic[key][1]\n        if rmspe_model < rmspe_min:\n            rmspe_min = rmspe_model\n            model = dic[key][0]\n            name = key\n    return model, name","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dic_eval['XGBOOST'][1]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if rmspe_scaler < dic_eval['XGBOOST'][1]: #if scaler's perf is better than no scaler\n    model_to_scale, model_name = model_selection(dic_eval)\n    model = make_pipeline(selected_scaler, model_to_scale)\n    evaluateCV(model_name + '_scaled', model, dic_eval, X_train, y_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Score visualization\n","metadata":{}},{"cell_type":"code","source":"models = [k for k in dic_eval.keys()]\nrmspe_scores = [val[1] for val in dic_eval.values()]\n\nrmspe_scores, models = (list(t) for t in zip(*sorted(zip(rmspe_scores, models))))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(16,6))\n\nsns.barplot(x=rmspe_scores, y=models)\nplt.title('Models comparaison')\nplt.tight_layout()\n\nif (env == 'local' or env == 'kaggle'):\n    plt.savefig(save_path + 'models_comparaison.png')\nplt.show()\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_final, model_name = model_selection(dic_eval)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_final.fit(X_train, y_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"###############################\n# Save the best model\n###############################\n\nfilename = 'model_' + model_name + '.sav'\npickle.dump(model_final, open(output + filename, 'wb'))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission\n","metadata":{}},{"cell_type":"code","source":"###############################\n# Load a model previously saved\n###############################\n\n# model_final = pickle.load(open(output + filename, 'rb'))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"###############################\n# adding prediction to df & export\n###############################\n\ndf_pred = df_pred.assign(target = model_final.predict(X_test))\ndf_pred.to_csv('submission.csv', index=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.read_csv('submission.csv')","metadata":{},"execution_count":null,"outputs":[]}]}