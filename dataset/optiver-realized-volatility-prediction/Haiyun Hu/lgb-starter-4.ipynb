{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## LGB starter\nthanks to : https://www.kaggle.com/manels/lgb-starter","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2021-07-05T17:50:17.732318Z","iopub.status.busy":"2021-07-05T17:50:17.731903Z","iopub.status.idle":"2021-07-05T17:50:19.647572Z","shell.execute_reply":"2021-07-05T17:50:19.646491Z","shell.execute_reply.started":"2021-07-05T17:50:17.732235Z"},"papermill":{"duration":0.008057,"end_time":"2021-07-11T02:25:24.626535","exception":false,"start_time":"2021-07-11T02:25:24.618478","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"https://www.kaggle.com/haiyunhu/lgb-starter/edit","metadata":{"papermill":{"duration":0.006688,"end_time":"2021-07-11T02:25:24.64024","exception":false,"start_time":"2021-07-11T02:25:24.633552","status":"completed"},"tags":[]}},{"cell_type":"code","source":"from IPython.core.display import display, HTML\n\nimport pandas as pd\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport glob\nimport os\nimport gc\n\nfrom joblib import Parallel, delayed\n\nfrom sklearn import preprocessing, model_selection\nfrom sklearn import linear_model\nimport lightgbm as lgb\n\nfrom sklearn.metrics import r2_score\n\nimport matplotlib.pyplot as plt \nimport seaborn as sns\n\npath_root = '../input/optiver-realized-volatility-prediction'\npath_data = '../input/optiver-realized-volatility-prediction'\npath_submissions = '/'\n\ntarget_name = 'target'\nscores_folds = {}","metadata":{"papermill":{"duration":3.231969,"end_time":"2021-07-11T02:25:27.878914","exception":false,"start_time":"2021-07-11T02:25:24.646945","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-07-12T10:16:29.114626Z","iopub.execute_input":"2021-07-12T10:16:29.115205Z","iopub.status.idle":"2021-07-12T10:16:29.126243Z","shell.execute_reply.started":"2021-07-12T10:16:29.115131Z","shell.execute_reply":"2021-07-12T10:16:29.124808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_agg_info(df):\n    agg_df = df.groupby(['stock_id', 'time_id']).agg(mean_sec_in_bucket = ('seconds_in_bucket', 'mean'), \n                                                     mean_price = ('price', 'mean'),\n                                                     mean_size = ('size', 'mean'),\n                                                     mean_order = ('order_count', 'mean'),\n                                                     max_sec_in_bucket = ('seconds_in_bucket', 'max'), \n                                                     max_price = ('price', 'max'),\n                                                     max_size = ('size', 'max'),\n                                                     max_order = ('order_count', 'max'),\n                                                     min_sec_in_bucket = ('seconds_in_bucket', 'min'), \n                                                     min_price = ('price', 'min'),\n                                                     min_size = ('size', 'min'),\n                                                     min_order = ('order_count', 'min'),\n                                                     median_sec_in_bucket = ('seconds_in_bucket', 'median'), \n                                                     median_price = ('price', 'median'),\n                                                     median_size = ('size', 'median'),\n                                                     median_order = ('order_count', 'median')\n                                                    ).reset_index()\n    \n    return agg_df","metadata":{"papermill":{"duration":0.026887,"end_time":"2021-07-11T02:25:27.919756","exception":false,"start_time":"2021-07-11T02:25:27.892869","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-07-12T10:16:29.12819Z","iopub.execute_input":"2021-07-12T10:16:29.128569Z","iopub.status.idle":"2021-07-12T10:16:29.142843Z","shell.execute_reply.started":"2021-07-12T10:16:29.12853Z","shell.execute_reply":"2021-07-12T10:16:29.141577Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def log_return(list_stock_prices):\n    return np.log(list_stock_prices).diff() \n\ndef realized_volatility(series_log_return):\n    return np.sqrt(np.sum(series_log_return**2))\n\ndef rmspe(y_true, y_pred):\n    return  (np.sqrt(np.mean(np.square((y_true - y_pred) / y_true))))\n\ndef calculate_wap1(df):\n    a1 = df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']\n    b1 = df['bid_size1'] + df['ask_size1']\n    a2 = df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2']\n    b2 = df['bid_size2'] + df['ask_size2']\n    x = (a1/b1 + a2/b2)/ 2\n    return x\n\ndef calculate_wap2(df):\n        \n    a1 = df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']\n    a2 = df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2']\n    b = df['bid_size1'] + df['ask_size1'] + df['bid_size2']+ df['ask_size2']\n    \n    x = (a1 + a2)/ b\n    return x\n\ndef get_stock_stat(stock_id : int, dataType = 'train'):\n    key = ['stock_id', 'time_id', 'seconds_in_bucket']\n    \n    #Book features\n    df_book = pd.read_parquet(os.path.join(path_data, 'book_{}.parquet/stock_id={}/'.format(dataType, stock_id)))\n    df_book['stock_id'] = stock_id\n    cols = key + [col for col in df_book.columns if col not in key]\n    df_book = df_book[cols]\n    \n    df_book['bas'] = (df_book[['ask_price1', 'ask_price2']].min(axis = 1)\n                    / df_book[['bid_price1', 'bid_price2']].max(axis = 1) - 1)\n    \n    df_book['wap1'] = calculate_wap1(df_book)\n    df_book['wap2'] = calculate_wap2(df_book)\n    \n    df_book['log_return1'] = df_book.groupby(by = ['time_id'])['wap1'].apply(log_return).fillna(0)\n    df_book['log_return2'] = df_book.groupby(by = ['time_id'])['wap2'].apply(log_return).fillna(0)\n    \n    features_to_apply_realized_volatility = ['log_return'+str(i+1) for i in range(2)]\n    \n    stock_stat = df_book.groupby(by = ['stock_id', 'time_id'])[features_to_apply_realized_volatility]\\\n                        .agg(realized_volatility).reset_index()\n\n    #Trade features\n    trade_stat =  pd.read_parquet(os.path.join(path_data,'trade_{}.parquet/stock_id={}'.format(dataType, stock_id)))\n    trade_stat = trade_stat.sort_values(by=['time_id', 'seconds_in_bucket']).reset_index(drop=True)\n    trade_stat['stock_id'] = stock_id\n    cols = key + [col for col in trade_stat.columns if col not in key]\n    trade_stat = trade_stat[cols]\n    trade_stat['trade_log_return1'] = trade_stat.groupby(by = ['time_id'])['price'].apply(log_return).fillna(0)\n    trade_stat = trade_stat.groupby(by = ['stock_id', 'time_id'])[['trade_log_return1']]\\\n                           .agg(realized_volatility).reset_index()\n    #Joining book and trade features\n    stock_stat = stock_stat.merge(trade_stat, on=['stock_id', 'time_id'], how='left').fillna(-999)\n    \n    stock_stat = stock_stat.merge(\n        df_book.groupby(by = ['time_id'], as_index = False)['bas'].mean(),\n        on = ['time_id'],\n        how = 'left'\n    )\n    \n    trade_stat2 =  pd.read_parquet(os.path.join(path_data,'trade_{}.parquet/stock_id={}'.format(dataType, stock_id)))\n    trade_stat2 = trade_stat2.sort_values(by=['time_id', 'seconds_in_bucket']).reset_index(drop=True)\n    trade_stat2['stock_id'] = stock_id\n    stock_stat = pd.merge(stock_stat,\n        trade_stat2.groupby(by = ['time_id']).agg(\n            mean_sec_in_bucket = ('seconds_in_bucket', 'mean'), \n            mean_price = ('price', 'mean'),\n            mean_size = ('size', 'mean'),\n            mean_order = ('order_count', 'mean'),\n            max_sec_in_bucket = ('seconds_in_bucket', 'max'),\n            max_price = ('price', 'max'),\n            max_size = ('size', 'max'),\n            max_order = ('order_count', 'max'),\n            min_sec_in_bucket = ('seconds_in_bucket', 'min'),\n            min_price = ('price', 'min'),\n            min_size = ('size', 'min'),\n            min_order = ('order_count', 'min'),\n            median_sec_in_bucket = ('seconds_in_bucket', 'median'),\n            median_price = ('price', 'median'),\n            median_size = ('size', 'median'),\n            median_order = ('order_count', 'median')).reset_index(),\n        on=['time_id'], \n        how = 'left'\n    )\n    \n    return stock_stat\n\ndef get_dataSet(stock_ids : list, dataType = 'train'):\n\n    stock_stat = Parallel(n_jobs=-1)(\n        delayed(get_stock_stat)(stock_id, dataType) \n        for stock_id in stock_ids\n    )\n    \n    stock_stat_df = pd.concat(stock_stat, ignore_index = True)\n\n    return stock_stat_df\n\ndef feval_RMSPE(preds, train_data):\n    labels = train_data.get_label()\n    return 'RMSPE', round(rmspe(y_true = labels, y_pred = preds),5), False\n\nparams_lgbm = {\n        'task': 'train',\n        'boosting_type': 'gbdt',\n        'learning_rate': 0.01,\n        'objective': 'regression',\n        'metric': 'None',\n        'max_depth': -1,\n        'n_jobs': -1,\n        'feature_fraction': 0.7,\n        'bagging_fraction': 0.7,\n        'lambda_l2': 0.7,\n        'verbose': -1,\n        #'bagging_freq': 5\n}","metadata":{"papermill":{"duration":0.052614,"end_time":"2021-07-11T02:25:27.985459","exception":false,"start_time":"2021-07-11T02:25:27.932845","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-07-12T10:16:29.144777Z","iopub.execute_input":"2021-07-12T10:16:29.1451Z","iopub.status.idle":"2021-07-12T10:16:29.176979Z","shell.execute_reply.started":"2021-07-12T10:16:29.145071Z","shell.execute_reply":"2021-07-12T10:16:29.176134Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train and test datasets","metadata":{"papermill":{"duration":0.007553,"end_time":"2021-07-11T02:25:28.003462","exception":false,"start_time":"2021-07-11T02:25:27.995909","status":"completed"},"tags":[]}},{"cell_type":"code","source":"#train = pd.read_csv(os.path.join(path_data, 'train.csv'))\n#train_stock_stat_df = get_dataSet(stock_ids = train['stock_id'].unique(), dataType = 'train')\n#train = pd.merge(train, train_stock_stat_df, on = ['stock_id', 'time_id'], how = 'left')\n#train.insert(2,'row_id',train['stock_id'].astype(str)+'-'+train['time_id'].astype(str))\n#train.to_csv(\"orv_preprocessed_train_ds_with_bas.csv\",index=False)\ntrain = pd.read_csv(\"../input/orv-datasets-with-bas-etc/orv_preprocessed_train_ds_with_bas.csv\")\nprint('Train shape: {}'.format(train.shape))\ndisplay(train.head(2))\n\ntest = pd.read_csv(os.path.join(path_data, 'test.csv'))\ntest_stock_stat_df = get_dataSet(stock_ids = test['stock_id'].unique(), dataType = 'test')\ntest = pd.merge(test, test_stock_stat_df, on = ['stock_id', 'time_id'], how = 'left').fillna(0)\nprint('Test shape: {}'.format(test.shape))\ndisplay(test.head(2))","metadata":{"papermill":{"duration":3.714618,"end_time":"2021-07-11T02:25:31.725348","exception":false,"start_time":"2021-07-11T02:25:28.01073","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-07-12T10:16:29.232941Z","iopub.execute_input":"2021-07-12T10:16:29.233422Z","iopub.status.idle":"2021-07-12T10:16:33.677885Z","shell.execute_reply.started":"2021-07-12T10:16:29.233383Z","shell.execute_reply":"2021-07-12T10:16:33.676609Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training model and making predictions","metadata":{"papermill":{"duration":0.009453,"end_time":"2021-07-11T02:25:31.744792","exception":false,"start_time":"2021-07-11T02:25:31.735339","status":"completed"},"tags":[]}},{"cell_type":"code","source":"cats = ['stock_id']\nmodel_name = 'lgb1'\npred_name = 'pred_{}'.format(model_name)\nfeatures_to_consider = ['stock_id', 'log_return1', 'log_return2', 'trade_log_return1','bas',\n                        'mean_sec_in_bucket', 'max_sec_in_bucket',\n                        'mean_size','max_size',\n                        'mean_price','max_price',\n                        'mean_order','min_order','max_order','median_order',\n                        'median_sec_in_bucket','median_price','median_size',\n                        'min_sec_in_bucket','min_price','min_size',\n                       ]\nprint('We consider {} features'.format(len(features_to_consider)))\n\ntrain[pred_name] = 0\ntest['target'] = 0\n\nn_folds = 15\nn_rounds = 5000\nkf = model_selection.KFold(n_splits=n_folds, shuffle=True, random_state=2016)\nscores_folds[model_name] = []\ncounter = 1\nfor dev_index, val_index in kf.split(range(len(train))):\n    print('CV {}/{}'.format(counter, n_folds))\n    X_train = train.loc[dev_index, features_to_consider]\n    y_train = train.loc[dev_index, target_name].values\n    X_val = train.loc[val_index, features_to_consider]\n    y_val = train.loc[val_index, target_name].values\n    \n    #############################################################################################\n    #LGB\n    #############################################################################################\n    train_data = lgb.Dataset(X_train, label=y_train, categorical_feature=cats, weight=1/np.power(y_train,2))\n    val_data = lgb.Dataset(X_val, label=y_val, categorical_feature=cats, weight=1/np.power(y_val,2))\n    \n    model = lgb.train(params_lgbm, \n                      train_data, \n                      n_rounds, \n                      valid_sets=val_data, \n                      feval=feval_RMSPE,\n                      verbose_eval= 250,\n                      early_stopping_rounds=500,\n                      categorical_feature = ['stock_id'] \n                     )\n    preds = model.predict(train.loc[val_index, features_to_consider],num_iteration=model.best_iteration)\n    train.loc[val_index, pred_name] = preds\n    score = round(rmspe(y_true = y_val, y_pred = preds),5)\n    print('Fold {} {}: {}'.format(counter, model_name, score))\n    scores_folds[model_name].append(score)\n    counter += 1\n    test[target_name] += model.predict(test[features_to_consider]).clip(0,1e10)\ndel train_data, val_data\ntest[target_name] = test[target_name]/n_folds\n\nscore = round(rmspe(y_true = train[target_name].values, y_pred = train[pred_name].values),5)\nprint('RMSPE {}: {} - Folds: {}'.format(model_name, score, scores_folds[model_name]))\n\ndisplay(test[['row_id', target_name]].head(2))\ntest[['row_id', target_name]].to_csv('submission.csv',index = False)\n\nimportances = pd.DataFrame({'Feature': model.feature_name(), \n                            'Importance': model.feature_importance(importance_type='gain')})\nimportances.sort_values(by = 'Importance', inplace=True)\nimportances2 = importances.nlargest(50,'Importance', keep='first').sort_values(by='Importance', ascending=True)\nimportances2[['Importance', 'Feature']].plot(kind = 'barh', x = 'Feature', figsize = (8,6), color = 'blue', fontsize=11);plt.ylabel('Feature', fontsize=12)","metadata":{"papermill":{"duration":1126.769967,"end_time":"2021-07-11T02:44:18.524409","exception":false,"start_time":"2021-07-11T02:25:31.754442","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-07-12T10:16:33.680263Z","iopub.execute_input":"2021-07-12T10:16:33.680771Z","iopub.status.idle":"2021-07-12T10:30:08.027719Z","shell.execute_reply.started":"2021-07-12T10:16:33.680708Z","shell.execute_reply":"2021-07-12T10:30:08.026535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.067277,"end_time":"2021-07-11T02:44:18.657461","exception":false,"start_time":"2021-07-11T02:44:18.590184","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]}]}