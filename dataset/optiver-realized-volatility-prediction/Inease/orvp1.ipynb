{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### 导入需要的库","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport glob # Read File Routes\nfrom tqdm import tqdm\nimport sys, os","metadata":{"execution":{"iopub.status.busy":"2021-09-14T13:04:32.462472Z","iopub.execute_input":"2021-09-14T13:04:32.46287Z","iopub.status.idle":"2021-09-14T13:04:32.487642Z","shell.execute_reply.started":"2021-09-14T13:04:32.462762Z","shell.execute_reply":"2021-09-14T13:04:32.486507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reorganize_data_by_time(\"test\",\"trade\",\"0\")","metadata":{"execution":{"iopub.status.busy":"2021-09-14T15:22:27.756291Z","iopub.execute_input":"2021-09-14T15:22:27.757464Z","iopub.status.idle":"2021-09-14T15:22:27.781788Z","shell.execute_reply.started":"2021-09-14T15:22:27.757418Z","shell.execute_reply":"2021-09-14T15:22:27.781145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 定义常量","metadata":{}},{"cell_type":"code","source":"DATA_ROOT = \"/kaggle/input/optiver-realized-volatility-prediction\"\nDATA_FEATURE_COUNT = 10\nBATCH_SIZE = 1\nFEATURES_COUNT = 14","metadata":{"execution":{"iopub.status.busy":"2021-09-14T13:49:55.597951Z","iopub.execute_input":"2021-09-14T13:49:55.598406Z","iopub.status.idle":"2021-09-14T13:49:55.602178Z","shell.execute_reply.started":"2021-09-14T13:49:55.598361Z","shell.execute_reply":"2021-09-14T13:49:55.601433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 导入数据并进行预处理","metadata":{}},{"cell_type":"code","source":"def log_return(list_stock_prices):\n    return np.log(list_stock_prices).diff() ","metadata":{"execution":{"iopub.status.busy":"2021-09-14T13:04:32.494777Z","iopub.execute_input":"2021-09-14T13:04:32.495198Z","iopub.status.idle":"2021-09-14T13:04:32.507022Z","shell.execute_reply.started":"2021-09-14T13:04:32.495154Z","shell.execute_reply":"2021-09-14T13:04:32.506289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def reorganize_data_by_time(mode,data_type,stock=\"*\",path=DATA_ROOT):\n    # data_type = trade / book\n    # mode = test / train\n    book_paths = glob.glob(f'{path}/{data_type}_{mode}.parquet/stock_id={stock}/*')\n    for path in tqdm(book_paths,desc = \"Reading From parquet:\"):\n        stock_id = int(path.split(\"=\")[1].split(\"/\")[0])\n        book_df = pd.read_parquet(path)\n        books_by_time = dict()\n\n        for time_id in book_df.time_id.unique():\n            books_by_time[time_id] = book_df[book_df[\"time_id\"] == time_id].reset_index(drop=True).drop(\"time_id\",axis=1)\n\n    return books_by_time\n    \n\ndef load_predict_data(mode, path=DATA_ROOT):\n    if mode != \"train\" and mode != \"test\":\n        raise OSError\n    file_name = f'{path}/{mode}.csv'\n    return pd.read_csv(file_name)","metadata":{"execution":{"iopub.status.busy":"2021-09-14T13:04:32.508484Z","iopub.execute_input":"2021-09-14T13:04:32.50946Z","iopub.status.idle":"2021-09-14T13:04:32.521498Z","shell.execute_reply.started":"2021-09-14T13:04:32.509409Z","shell.execute_reply":"2021-09-14T13:04:32.520403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nfor idx , time_id in tqdm(enumerate(trade_data),desc = \"Organizing Data by time\",total = len(trade_data)):\n    stock_data = pd.merge(trade_data[time_id],book_data[time_id],how = 'outer').sort_values(by=['seconds_in_bucket']).fillna(0)\n    stock_data['wap'] = (stock_data['bid_price1'] * stock_data['ask_size1'] +\n                            stock_data['ask_price1'] * stock_data['bid_size1']) / (\n                                   stock_data['bid_size1']+ stock_data['ask_size1'])\n    stock_data[\"log_return\"] = log_return(stock_data['wap'])\n    stock_data.fillna(0,inplace = True)\n    # 开始填充数据\n    pad_data = np.zeros(shape=(800, FEATURES_COUNT))\n    print(stock_data.shape)\n    for index,data in stock_data.iterrows():\n        pad_data[index] = np.array(data)\n    # print(pad_data.shape)\n    stock_all_time_data[idx] = pad_data","metadata":{"execution":{"iopub.status.busy":"2021-09-14T13:43:04.121872Z","iopub.execute_input":"2021-09-14T13:43:04.122155Z","iopub.status.idle":"2021-09-14T13:43:04.820168Z","shell.execute_reply.started":"2021-09-14T13:43:04.122128Z","shell.execute_reply":"2021-09-14T13:43:04.818988Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 组建DataLoader","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import Dataset\nfrom torch import tensor\n\nclass TrainDataset(Dataset):\n    def __init__(self,mode = \"train\"):\n        self.mode = mode\n        self.target = load_predict_data(self.mode)\n        self.length = self.target.max()[\"stock_id\"]\n        self.max_seq_len = 800\n        self.max_stock_len = 8000\n#         self.trade_data = reorganize_data_by_time(self.mode,'trade')\n#         self.book_data = reorganize_data_by_time(self.mode,'book')\n        # 这两个不能预先加载 因为太大了，到时候根据stockid加载\n    # TODO: Logger\n    \n    def __getitem__(self,stock_id):\n        trade_data = reorganize_data_by_time(self.mode,\"trade\",str(stock_id))\n        book_data = reorganize_data_by_time(self.mode,\"book\",str(stock_id))\n        stock_data_lengths = []\n        label = self.target.loc[stock_id, \"target\"]\n        \n        stock_all_time_data = np.zeros(shape=(self.max_stock_len,self.max_seq_len,FEATURES_COUNT))\n        for idx , time_id in tqdm(enumerate(trade_data),desc = \"Organizing Data by time\",total = len(trade_data)):\n            stock_data = pd.merge(trade_data[time_id],book_data[time_id],how = 'outer').sort_values(by=['seconds_in_bucket']).fillna(0)\n            stock_data['wap'] = (stock_data['bid_price1'] * stock_data['ask_size1'] +\n                                    stock_data['ask_price1'] * stock_data['bid_size1']) / (\n                                           stock_data['bid_size1']+ stock_data['ask_size1'])\n            stock_data[\"log_return\"] = log_return(stock_data['wap'])\n            stock_data.fillna(0,inplace = True)\n            stock_data_lengths.append(stock_data.shape[0])\n            # 开始填充数据\n            pad_data = np.zeros(shape=(self.max_seq_len, FEATURES_COUNT))\n            for index,data in stock_data.iterrows():\n                pad_data[index] = np.array(data)\n            # print(pad_data.shape)\n            stock_all_time_data[idx] = pad_data\n        return {\"data\":tensor(stock_all_time_data), \"label\":tensor(label) ,'seq_len': tensor(stock_data_lengths)} # data : (time,seconds,data)\n        # return [tensor(stock_all_time_data), tensor(label) ,len(trade_data)]# data : (time,seconds,data)\n     \n    def __len__(self):\n        return self.length.astype(np.int16)","metadata":{"execution":{"iopub.status.busy":"2021-09-14T15:24:16.848491Z","iopub.execute_input":"2021-09-14T15:24:16.848811Z","iopub.status.idle":"2021-09-14T15:24:16.863893Z","shell.execute_reply.started":"2021-09-14T15:24:16.848763Z","shell.execute_reply":"2021-09-14T15:24:16.861906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 组建DataLoader","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import DataLoader\ntrain_data = DataLoader(TrainDataset(\"train\"),batch_size = BATCH_SIZE ,shuffle = False)","metadata":{"execution":{"iopub.status.busy":"2021-09-14T15:24:21.84563Z","iopub.execute_input":"2021-09-14T15:24:21.846247Z","iopub.status.idle":"2021-09-14T15:24:22.035528Z","shell.execute_reply.started":"2021-09-14T15:24:21.846198Z","shell.execute_reply":"2021-09-14T15:24:22.034798Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"aa = DataLoader(TrainDataset(mode=\"test\"),batch_size = 1 ,shuffle = False)","metadata":{"execution":{"iopub.status.busy":"2021-09-14T15:27:38.055936Z","iopub.execute_input":"2021-09-14T15:27:38.056811Z","iopub.status.idle":"2021-09-14T15:27:38.071365Z","shell.execute_reply.started":"2021-09-14T15:27:38.056758Z","shell.execute_reply":"2021-09-14T15:27:38.070329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in aa:\n    print(i)\n    break","metadata":{"execution":{"iopub.status.busy":"2021-09-14T15:27:51.259911Z","iopub.execute_input":"2021-09-14T15:27:51.260755Z","iopub.status.idle":"2021-09-14T15:27:51.267807Z","shell.execute_reply.started":"2021-09-14T15:27:51.260714Z","shell.execute_reply":"2021-09-14T15:27:51.266779Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 组建PackedSequence","metadata":{}},{"cell_type":"code","source":"np.array([1, 4,2,13])[np.argsort(np.array([1,4, 2,13]))[::-1]]","metadata":{"execution":{"iopub.status.busy":"2021-09-14T14:02:26.502574Z","iopub.execute_input":"2021-09-14T14:02:26.502874Z","iopub.status.idle":"2021-09-14T14:02:26.51052Z","shell.execute_reply.started":"2021-09-14T14:02:26.502846Z","shell.execute_reply":"2021-09-14T14:02:26.509552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"i[\"seq_len\"][0][order_idx[0]]","metadata":{"execution":{"iopub.status.busy":"2021-09-14T14:05:00.472392Z","iopub.execute_input":"2021-09-14T14:05:00.472883Z","iopub.status.idle":"2021-09-14T14:05:00.480183Z","shell.execute_reply.started":"2021-09-14T14:05:00.47285Z","shell.execute_reply":"2021-09-14T14:05:00.479098Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"order_idx[0]","metadata":{"execution":{"iopub.status.busy":"2021-09-14T14:04:53.032138Z","iopub.execute_input":"2021-09-14T14:04:53.03301Z","iopub.status.idle":"2021-09-14T14:04:53.038132Z","shell.execute_reply.started":"2021-09-14T14:04:53.032951Z","shell.execute_reply":"2021-09-14T14:04:53.037607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.nn.utils.rnn import pack_padded_sequence\nfor batch in i['data']:\n    order_idx = np.array(np.argsort(i[\"seq_len\"]))[::-1]\n    print('order_idx:', str(order_idx))\n    order_x = batch[order_idx.tolist()]\n    order_seq = np.array(i[\"seq_len\"][0][order_idx[0]])\n    # Pack it\n    pack = pack_padded_sequence(order_x, order_seq, batch_first=True ,enforce_sorted=False)\n    i['data '] = pack","metadata":{"execution":{"iopub.status.busy":"2021-09-14T14:54:48.883183Z","iopub.execute_input":"2021-09-14T14:54:48.883508Z","iopub.status.idle":"2021-09-14T14:54:49.304667Z","shell.execute_reply.started":"2021-09-14T14:54:48.883478Z","shell.execute_reply":"2021-09-14T14:54:49.303509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 实例化RNN网络","metadata":{}},{"cell_type":"code","source":"from torch.nn import RNN\nrnn = RNN(input_size=14, hidden_size=1, num_layers=20)\ncriterion = nn.CrossEntropyLoss()\noptimzier = torch.optim.Adadelta(net.parameters(), 1e-1)\n\ndef get_acc(output, label):\n    total = output.shape[0]\n    _, pred_label = output.max(1)\n    num_correct = (pred_label == label).sum().data\n    # print(num_correct, total)\n    return num_correct\n\ndef train(net, train_data, valid_data, num_epochs, optimizer, criterion):\n    if torch.cuda.is_available():\n        net = net.cuda()\n    for i in range(num_epochs):\n        train_loss = 0\n        train_acc = 0\n        net = net.train()\n        for im, label in train_data:\n            if torch.cuda.is_available():\n                im = Variable(im.cuda())\n                label = Variable(label.cuda())\n            else:\n                im = Variable(im)\n                label = Variable(label)\n            # forward\n            output = net(im)\n            total = output.shape[0]\n            loss = criterion(output, label)\n            # backward\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            train_loss += loss.data.cpu().numpy()/float(total)\n            train_acc += get_acc(output, label).cpu().numpy()/float(total)\n        if valid_data is not None:\n            valid_loss = 0\n            valid_acc = 0\n            net = net.eval()\n            for im, label in valid_data:\n                if torch.cuda.is_available():\n                    im = Variable(im.cuda(), volatile=True)\n                    label = Variable(label.cuda(), volatile=True)\n                else:\n                    im = Variable(im, volatile=True)\n                    label = Variable(label, volatile=True)\n                output = net(im)\n                total = output.shape[0]\n                loss = criterion(output, label)\n                valid_loss += loss.data.cpu().numpy()/float(total)\n                valid_acc += get_acc(output, label).cpu().numpy()/float(total)\n            print(\"epoch: %d, train_loss: %f, train_acc: %f, valid_loss: %f, valid_acc:%f\"\n                  % (i, train_loss/len(train_data),  train_acc/len(train_data),\n                  valid_loss/len(valid_data),  valid_acc/len(valid_data)))\n\n        else:\n            print(\"epoch= \", i, \"train_loss= \", train_loss/len(train_data), \"train_acc= \", train_acc/len(train_data))\n# 开始训练\ntrain(net, train_data, test_data, 10, optimzier, criterion)","metadata":{},"execution_count":null,"outputs":[]}]}