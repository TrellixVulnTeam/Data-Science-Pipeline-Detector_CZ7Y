{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-15T09:46:02.834202Z","iopub.execute_input":"2021-07-15T09:46:02.834735Z","iopub.status.idle":"2021-07-15T09:46:03.856502Z","shell.execute_reply.started":"2021-07-15T09:46:02.834653Z","shell.execute_reply":"2021-07-15T09:46:03.855399Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from glob import glob\nimport plotly_express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nfrom tqdm import tqdm\nfrom sklearn.metrics import *","metadata":{"execution":{"iopub.status.busy":"2021-07-15T09:46:03.857863Z","iopub.execute_input":"2021-07-15T09:46:03.858137Z","iopub.status.idle":"2021-07-15T09:46:06.28371Z","shell.execute_reply.started":"2021-07-15T09:46:03.858109Z","shell.execute_reply":"2021-07-15T09:46:06.282695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('../input/optiver-realized-volatility-prediction/train.csv')\ntest  = pd.read_csv('../input/optiver-realized-volatility-prediction/test.csv')\nsample_sub = pd.read_csv('../input/optiver-realized-volatility-prediction/sample_submission.csv')\ndisplay(train)\ndisplay(test)\ndisplay(sample_sub)","metadata":{"execution":{"iopub.status.busy":"2021-07-15T09:46:06.285563Z","iopub.execute_input":"2021-07-15T09:46:06.28589Z","iopub.status.idle":"2021-07-15T09:46:06.583802Z","shell.execute_reply.started":"2021-07-15T09:46:06.285859Z","shell.execute_reply":"2021-07-15T09:46:06.582992Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def file_path_to_volatility(path,info=False):\n    part_data = []\n    stock_id = path.split('/')[4].split('=')[1]\n    sample_book = pd.read_parquet(path)\n    sample_book['wap'] = (sample_book['bid_price1'] * sample_book['ask_size1'] + sample_book['ask_price1'] * sample_book['bid_size1']) / (sample_book['bid_size1']+ sample_book['ask_size1'])\n    for gid0,gid in sample_book.groupby('time_id'):\n        gid['log_return'] = gid['wap'].apply(lambda x:np.log(x)).diff()\n        if info :\n            print(f'Realized Volatiliy for time id {gid.time_id.iloc[0]} is ' ,np.sqrt(np.sum(gid['log_return'].reset_index(drop=True).drop(index = 0).apply(lambda x:x**2))))\n        part_data.append([stock_id,gid.time_id.iloc[0],np.sqrt(np.sum(gid['log_return'].reset_index(drop=True).drop(index = 0).apply(lambda x:x**2)))])\n    return part_data\ndef read_all_files(path):\n    \"\"\" Reads All file in the sub Folder (path / *) and read all parquets (trade/book) and picks only the first occurence based on Stock + Time\n        Returns a list of all dataframs use concat to join them back .\"\"\"\n    demo_all = []\n    for i in tqdm(glob(os.path.join(path,'*'))):\n        demo_merged = path_to_data(i)\n        demo = demo_merged.groupby(['stock_id','time_id']).first().reset_index()\n        demo.stock_id = demo.stock_id.astype('int64')\n        demo_all.append(demo)\n    return demo_all\n\ndef path_to_data(path):\n    \"\"\" This return a merged dataframe of trades where the trades actually took place \"\"\"\n#     print(path)\n    stock_id = path.split('/')[-1].split('=')[1]\n    curr_book = pd.read_parquet(path)\n    curr_trade = pd.read_parquet(path.replace('book','trade'))\n    merged_data = pd.merge(curr_book,curr_trade,on=['time_id','seconds_in_bucket'])\n    merged_data['stock_id'] = stock_id\n#     print(curr_book.shape,curr_trade.shape,len(merged_data))\n    if len(merged_data) ==0 :\n        merged_data = curr_trade.merge(curr_book, how='cross',suffixes=['','_y'])\n        merged_data['diff'] = abs(merged_data.seconds_in_bucket-merged_data.seconds_in_bucket_y)\n        merged_data = pd.merge(merged_data.groupby(['time_id','seconds_in_bucket'])['diff'].min().reset_index(),merged_data,how=\"left\")\n        merged_data.drop(columns=['time_id_y','seconds_in_bucket_y','diff'],inplace=True)\n        merged_data['stock_id'] = stock_id\n    merged_data.dropna(inplace=True)\n    merged_data.reset_index(drop=True)\n    return merged_data\n\ndef files_to_numbers(demo_all,vol_calculated,csv_path = '../input/optiver-realized-volatility-prediction/train.csv'):\n    \"\"\" Takes in a List of DataFrame and Merges them with a CSV File and then with preprocessed data that we have where we calculate the Volatility\n        at end of 10 min or bucket mark \"\"\"\n    csv_file = pd.read_csv(csv_path)\n    demo = pd.concat(demo_all).reset_index(drop=True)\n    demo_vol = pd.merge(csv_file,demo,on=['stock_id','time_id'])\n    demo_vol_all_data = pd.merge(demo_vol,vol_calculated)\n    return demo_vol_all_data\n\ndef rmspe(y_true, y_pred):\n    return  (np.sqrt(np.mean(np.square((y_true - y_pred) / y_true))))","metadata":{"execution":{"iopub.status.busy":"2021-07-15T09:46:06.585486Z","iopub.execute_input":"2021-07-15T09:46:06.586022Z","iopub.status.idle":"2021-07-15T09:46:06.60585Z","shell.execute_reply.started":"2021-07-15T09:46:06.58598Z","shell.execute_reply":"2021-07-15T09:46:06.605114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import *\nfrom sklearn.model_selection import *\nfrom sklearn.preprocessing import *\nfrom sklearn.linear_model import LinearRegression\nbook_dir = glob('../input/optiver-realized-volatility-prediction/book_train.parquet/*/*')\n# train_data_vol = []\n# for i in tqdm(book_dir):\n#     train_data_vol.append(file_path_to_volatility(i))\n# past_data = pd.concat([pd.DataFrame(i,columns=['stock_id','time_id','vol']) for i in train_data_vol])\n# past_data.stock_id = past_data.stock_id.astype('int64')\npast_data = pd.read_csv('../input/starter/train_data.csv')\ndemo_all = read_all_files('../input/optiver-realized-volatility-prediction/book_train.parquet')\ndata = files_to_numbers(demo_all,past_data,'../input/optiver-realized-volatility-prediction/train.csv')\ndata_df = data.copy()\ndf_traval,df_test = train_test_split(data_df,stratify=data_df['stock_id'])\ndf_train,df_val = train_test_split(df_traval,stratify=df_traval['stock_id'])\ndf_train.drop(columns=['stock_id','time_id'],inplace=True)\ndf_val.drop(columns=['stock_id','time_id'],inplace=True)\ndf_test.drop(columns=['stock_id','time_id'],inplace=True)\nfeature = list(df_train.columns)\nfeature.remove('target')\ntrain_x,train_y = df_train[feature],df_train['target']\nval_x,val_y = df_val[feature],df_val['target']\ntest_x,test_y = df_test[feature],df_test['target']\nstd_scaler = StandardScaler()\ntrain_x_transform = std_scaler.fit_transform(train_x)\nval_x_transform = std_scaler.transform(val_x)\ntest_x_transform = std_scaler.transform(test_x)\n\n\nlr = LinearRegression()\nlr.fit(train_x_transform,train_y)\nprint(lr.score(train_x_transform,train_y),lr.score(val_x_transform,val_y),lr.score(test_x_transform,test_y))\nprint('Mean Squared Error \\t RMSPE METRIC(COMP)')\nprint(mean_squared_error(lr.predict(train_x_transform),train_y),rmspe(lr.predict(train_x_transform),train_y))\nprint(mean_squared_error(lr.predict(val_x_transform),val_y),rmspe(lr.predict(val_x_transform),val_y))\nprint(mean_squared_error(lr.predict(test_x_transform),test_y),rmspe(lr.predict(test_x_transform),test_y))\nprint(lr.predict(test_x_transform[:5]),test_y[:5].values)\n\ndata_df.drop(columns=['stock_id','time_id'],inplace=True)\ntrain_data,train_label = data_df[feature],data_df['target']\nstd_scaler = StandardScaler()\ntrain_data_transform = std_scaler.fit_transform(train_data)\ntrain_x_transform = std_scaler.transform(train_x)\nval_x_transform = std_scaler.transform(val_x)\ntest_x_transform = std_scaler.transform(test_x)\n\n\nlr = LinearRegression()\nlr.fit(train_data_transform,train_label)\nprint(lr.score(train_x_transform,train_y),lr.score(val_x_transform,val_y),lr.score(test_x_transform,test_y))\nprint('Mean Squared Error \\t RMSPE METRIC(COMP)')\nprint(mean_squared_error(lr.predict(train_data_transform),train_label),rmspe(lr.predict(train_data_transform),train_label))\nprint(mean_squared_error(lr.predict(train_x_transform),train_y),rmspe(lr.predict(train_x_transform),train_y))\nprint(mean_squared_error(lr.predict(val_x_transform),val_y),rmspe(lr.predict(val_x_transform),val_y))\nprint(mean_squared_error(lr.predict(test_x_transform),test_y),rmspe(lr.predict(test_x_transform),test_y))\nprint(lr.predict(test_x_transform[:5]),test_y[:5].values)\n","metadata":{"execution":{"iopub.status.busy":"2021-07-15T09:46:06.60703Z","iopub.execute_input":"2021-07-15T09:46:06.607367Z","iopub.status.idle":"2021-07-15T09:47:38.053798Z","shell.execute_reply.started":"2021-07-15T09:46:06.607327Z","shell.execute_reply":"2021-07-15T09:47:38.052646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# book_dir_test = glob('../input/optiver-realized-volatility-prediction/book_test.parquet/*/*')\n# test_data_vol = []\n# for i in tqdm(book_dir_test):\n#     test_data_vol.append(file_path_to_volatility(i))\n# test_data = pd.concat([pd.DataFrame(i,columns=['stock_id','time_id','vol']) for i in test_data_vol])\n# display(test_data)\n# test_data.stock_id = test_data.stock_id.astype('int64')\nbook_test_dir = glob('../input/optiver-realized-volatility-prediction/book_test.parquet/*/*')\ntrain_data_vol_pred = []\nfor i in tqdm(book_test_dir):\n    train_data_vol_pred.append(file_path_to_volatility(i))\npast_data_pred = pd.concat([pd.DataFrame(i,columns=['stock_id','time_id','vol']) for i in train_data_vol_pred])\npast_data_pred.stock_id = past_data_pred.stock_id.astype('int64')\n\n\n\n\n# display(test_data.dtypes)\n# display(test.dtypes)\ndemo_all_pred =  read_all_files('../input/optiver-realized-volatility-prediction/book_test.parquet')\n\n\n\ndata_pred = files_to_numbers(demo_all_pred,past_data_pred,'../input/optiver-realized-volatility-prediction/test.csv')\ndisplay(data_pred)\ndata_pred.dropna(inplace=True)\ndata_pred.reset_index(drop=True)\nrow_orderings = data_pred.row_id\ndata_pred.drop(columns='row_id',inplace=True)\nfeature = ['seconds_in_bucket','bid_price1','ask_price1','bid_price2','ask_price2','bid_size1','ask_size1','bid_size2','ask_size2','price','size','order_count','vol']\ndata_pred = data_pred[feature]\npred = lr.predict(std_scaler.transform(data_pred))\n\n# naive_pred = pd.merge(test,test_data,on=['stock_id','time_id'],how='left')[['row_id','vol']]\n# naive_pred.columns=sample_sub.columns\n# naive_pred.target.fillna(0.003048022,inplace=True)\n# naive_pred.to_csv('submission.csv',index = False)\n# display(naive_pred)\n\nif np.sum(data_pred.isnull().sum())==0:\n    sub = pd.merge(sample_sub,pd.DataFrame({'row_id':row_orderings,'target':pred}),on='row_id',how='left',suffixes=['_old',''])[sample_sub.columns]\n    sub.target.fillna(0.003048022,inplace=True)\n    display(sub)\n    sub.to_csv('submission.csv',index=False)\n\n# naive_pred = pd.merge(test,past_data_pred,on=['stock_id','time_id'],how='left')[['row_id','vol']]\n# naive_pred.columns=sample_sub.columns\n# naive_pred.target.fillna(0.003048022,inplace=True)\n# naive_pred.to_csv('submission.csv',index = False)\n# display(naive_pred)","metadata":{"execution":{"iopub.status.busy":"2021-07-15T09:47:38.05544Z","iopub.execute_input":"2021-07-15T09:47:38.056044Z","iopub.status.idle":"2021-07-15T09:47:38.218993Z","shell.execute_reply.started":"2021-07-15T09:47:38.055995Z","shell.execute_reply":"2021-07-15T09:47:38.217982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# book_dir_test = glob('../input/optiver-realized-volatility-prediction/book_test.parquet/*/*')\n# test_data_vol = []\n# for i in tqdm(book_dir_test):\n#     test_data_vol.append(file_path_to_volatility(i))\n# stock_id = '0'\n# test_data = pd.concat([pd.DataFrame(i,columns=['stock_id','time_id','vol']) for i in test_data_vol])\n# display(test_data)\n# test_data.stock_id = test_data.stock_id.astype('int64')\n# display(test_data.dtypes)\n# display(test.dtypes)\n# naive_pred = pd.merge(test,test_data,on=['stock_id','time_id'],how='left')[['row_id','vol']]\n# naive_pred.columns=sample_sub.columns\n# naive_pred.target.fillna(0.003048022,inplace=True)\n# naive_pred.to_csv('submission.csv',index = False)\n# display(naive_pred)","metadata":{"execution":{"iopub.status.busy":"2021-07-15T09:47:38.220068Z","iopub.execute_input":"2021-07-15T09:47:38.220376Z","iopub.status.idle":"2021-07-15T09:47:38.225673Z","shell.execute_reply.started":"2021-07-15T09:47:38.220347Z","shell.execute_reply":"2021-07-15T09:47:38.223361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}