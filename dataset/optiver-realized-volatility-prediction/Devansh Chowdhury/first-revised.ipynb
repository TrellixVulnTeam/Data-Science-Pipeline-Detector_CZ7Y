{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport os\nfrom glob import glob\nimport plotly_express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nfrom tqdm import tqdm\nfrom sklearn.metrics import *\nfrom sklearn.model_selection import *\nfrom sklearn.preprocessing import *\nfrom sklearn.linear_model import LinearRegression","metadata":{"execution":{"iopub.status.busy":"2021-07-14T11:03:21.677069Z","iopub.execute_input":"2021-07-14T11:03:21.677372Z","iopub.status.idle":"2021-07-14T11:03:24.390157Z","shell.execute_reply.started":"2021-07-14T11:03:21.677339Z","shell.execute_reply":"2021-07-14T11:03:24.389196Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('../input/optiver-realized-volatility-prediction/train.csv')\ntest  = pd.read_csv('../input/optiver-realized-volatility-prediction/test.csv')\nsample_sub = pd.read_csv('../input/optiver-realized-volatility-prediction/sample_submission.csv')\ndisplay(train)\ndisplay(test)\ndisplay(sample_sub)","metadata":{"execution":{"iopub.status.busy":"2021-07-14T11:03:24.391818Z","iopub.execute_input":"2021-07-14T11:03:24.39209Z","iopub.status.idle":"2021-07-14T11:03:24.699762Z","shell.execute_reply.started":"2021-07-14T11:03:24.392062Z","shell.execute_reply":"2021-07-14T11:03:24.698663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Stock ID Act as a kind of unique token for different stock.\n\nTime ID not exact in sequence but kind of specifying a token for time bucket .//A bit Vague","metadata":{}},{"cell_type":"code","source":"print('Different Unique Stocks IDS in training ',len(glob('../input/optiver-realized-volatility-prediction/book_train.parquet/*')),\n      'Different Unique Stocks IDS in test ',len(glob('../input/optiver-realized-volatility-prediction/book_test.parquet/*'))\n     )\nprint('Uniques in \\n',train.nunique())\ndisplay(train[train.time_id == 5])\n\npx.histogram(train.time_id)","metadata":{"execution":{"iopub.status.busy":"2021-07-14T11:03:24.70208Z","iopub.execute_input":"2021-07-14T11:03:24.702471Z","iopub.status.idle":"2021-07-14T11:03:27.644396Z","shell.execute_reply.started":"2021-07-14T11:03:24.702436Z","shell.execute_reply":"2021-07-14T11:03:27.643244Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Analysis on Train csv First Row \n\n0\t5\t0.004136","metadata":{}},{"cell_type":"code","source":"book = pd.read_parquet('../input/optiver-realized-volatility-prediction/book_train.parquet/stock_id=0/c439ef22282f412ba39e9137a3fdabac.parquet')\ntrade_example =  pd.read_parquet('../input/optiver-realized-volatility-prediction/trade_train.parquet/stock_id=0')\ntest_example =  pd.read_parquet('../input/optiver-realized-volatility-prediction/trade_test.parquet/stock_id=0/31c83a67d81349208e7d5eace9dbbac8.parquet')\nstock_id = '0'\nbook = book[book.time_id==5]\nbook.loc[:,'stock_id'] = stock_id\ntrade_example = trade_example[trade_example.time_id==5]\ntrade_example.loc[:,'stock_id'] = stock_id\ndisplay(book)\ndisplay(trade_example.head())","metadata":{"execution":{"iopub.status.busy":"2021-07-14T11:03:27.64626Z","iopub.execute_input":"2021-07-14T11:03:27.646702Z","iopub.status.idle":"2021-07-14T11:03:28.220244Z","shell.execute_reply.started":"2021-07-14T11:03:27.646642Z","shell.execute_reply":"2021-07-14T11:03:28.218737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(sum(book.groupby(['time_id','seconds_in_bucket']).nunique()['stock_id']>1))\nprint(sum(trade_example.groupby(['time_id','seconds_in_bucket']).nunique()['stock_id']>1))\nbook[(book.time_id==5)&(book.seconds_in_bucket==21)]","metadata":{"execution":{"iopub.status.busy":"2021-07-14T11:03:28.221627Z","iopub.execute_input":"2021-07-14T11:03:28.22194Z","iopub.status.idle":"2021-07-14T11:03:28.255206Z","shell.execute_reply.started":"2021-07-14T11:03:28.221909Z","shell.execute_reply":"2021-07-14T11:03:28.254276Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = make_subplots(rows=2, cols=2,subplot_titles=['Second in Bucket Distribution','Difference in Consequent Rows','wap','log_return'])\nfig.append_trace(go.Histogram(x = book.seconds_in_bucket.sort_values().values),1,1)\nfig.append_trace(go.Histogram(x = book.seconds_in_bucket.diff()),1,2)\nbook['wap'] = (book['bid_price1'] * book['ask_size1'] + book['ask_price1'] * book['bid_size1']) / (book['bid_size1']+ book['ask_size1'])\nbook['log_return'] = book['wap'].apply(lambda x:np.log(x)).diff()\nfig.append_trace(go.Line(x = book['seconds_in_bucket'],y = book['wap']),2,1)\nfig.append_trace(go.Line(x = book['seconds_in_bucket'],y = book['log_return']),2,2)\nfig.show()\nprint(book.seconds_in_bucket[book.seconds_in_bucket.diff().idxmax()-1:book.seconds_in_bucket.diff().idxmax()+1])","metadata":{"execution":{"iopub.status.busy":"2021-07-14T11:03:28.256418Z","iopub.execute_input":"2021-07-14T11:03:28.256747Z","iopub.status.idle":"2021-07-14T11:03:28.352619Z","shell.execute_reply.started":"2021-07-14T11:03:28.256714Z","shell.execute_reply":"2021-07-14T11:03:28.351397Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def file_path_to_volatility(path,info=False):\n    part_data = []\n    stock_id = path.split('/')[4].split('=')[1]\n    sample_book = pd.read_parquet(path)\n    sample_book['wap'] = (sample_book['bid_price1'] * sample_book['ask_size1'] + sample_book['ask_price1'] * sample_book['bid_size1']) / (sample_book['bid_size1']+ sample_book['ask_size1'])\n    sample_book.dropna(inplace=True)\n    for gid0,gid in sample_book.groupby('time_id'):\n        gid['log_return'] = gid['wap'].apply(lambda x:np.log(x)).diff()\n        if info :\n            print(f'Realized Volatiliy for time id {gid.time_id.iloc[0]} is ' ,np.sqrt(np.sum(gid['log_return'].reset_index(drop=True).drop(index = 0).apply(lambda x:x**2))))\n        part_data.append([stock_id,gid.time_id.iloc[0],np.sqrt(np.sum(gid['log_return'].reset_index(drop=True).drop(index = 0).apply(lambda x:x**2)))])\n    return part_data\nbook_dir = glob('../input/optiver-realized-volatility-prediction/book_train.parquet/*/*')\n# PRE LOADING THIS FILE (ALREADY RAN AND SAVED IN INPUTS)\n# train_data_vol = []\n# for i in tqdm(book_dir):\n#     train_data_vol.append(file_path_to_volatility(i))\n# past_data = pd.concat([pd.DataFrame(i,columns=['stock_id','time_id','vol']) for i in train_data_vol])\n# past_data.stock_id = past_data.stock_id.astype('int64')\npast_data = pd.read_csv('../input/starter/train_data.csv')\nprint(past_data.dtypes)\nprint(train.dtypes)\ndisplay(past_data)","metadata":{"execution":{"iopub.status.busy":"2021-07-14T11:03:33.702694Z","iopub.execute_input":"2021-07-14T11:03:33.703062Z","iopub.status.idle":"2021-07-14T11:03:34.334738Z","shell.execute_reply.started":"2021-07-14T11:03:33.703031Z","shell.execute_reply":"2021-07-14T11:03:34.33378Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"aa = file_path_to_volatility(book_dir[0])\n# np.sum(.isnull())\nnp.sum(pd.DataFrame(aa).isnull())","metadata":{"execution":{"iopub.status.busy":"2021-07-14T11:05:06.377313Z","iopub.execute_input":"2021-07-14T11:05:06.377659Z","iopub.status.idle":"2021-07-14T11:05:14.348242Z","shell.execute_reply.started":"2021-07-14T11:05:06.37763Z","shell.execute_reply":"2021-07-14T11:05:14.346741Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# METRICS\ndef rmspe(y_true, y_pred):\n    return  (np.sqrt(np.mean(np.square((y_true - y_pred) / y_true))))\n# print('root mean square percentage error'.upper(),rmspe(train_pred['target'],train_pred['vol']),\n#      'root mean square error'.upper(),mean_squared_error(train_pred['target'],train_pred['vol']))\nprint(train.shape,past_data.shape)\ndisplay(pd.merge(train,past_data,on=['stock_id','time_id']))","metadata":{"execution":{"iopub.status.busy":"2021-07-14T09:06:24.763669Z","iopub.execute_input":"2021-07-14T09:06:24.763996Z","iopub.status.idle":"2021-07-14T09:06:24.887161Z","shell.execute_reply.started":"2021-07-14T09:06:24.763966Z","shell.execute_reply":"2021-07-14T09:06:24.885912Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Try to combine the 2 trade list and make the data suitable for Model **\n\n../input/optiver-realized-volatility-prediction/book_train.parquet/stock_id=0\n\n../input/optiver-realized-volatility-prediction/trade_train.parquet/stock_id=0\n\nAfter Merging Trade Book and Trade Train , i will add the train.csv file will give some rows the orignal true volatility and some of them NANS.\nFirst Just considering the orginals one .","metadata":{}},{"cell_type":"code","source":"def path_to_data(path):\n    \"\"\" This return a merged dataframe of trades where the trades actually took place \"\"\"\n#     print(path)\n    stock_id = path.split('/')[-1].split('=')[1]\n    curr_book = pd.read_parquet(path)\n    curr_trade = pd.read_parquet(path.replace('book','trade'))\n    merged_data = pd.merge(curr_book,curr_trade,on=['time_id','seconds_in_bucket'])\n    merged_data['stock_id'] = stock_id\n#     print(curr_book.shape,curr_trade.shape,len(merged_data))\n    if len(merged_data) ==0 :\n        merged_data = curr_trade.merge(curr_book, how='cross',suffixes=['','_y'])\n        merged_data['diff'] = abs(merged_data.seconds_in_bucket-merged_data.seconds_in_bucket_y)\n        merged_data = pd.merge(merged_data.groupby(['time_id','seconds_in_bucket'])['diff'].min().reset_index(),merged_data,how=\"left\")\n        merged_data.drop(columns=['time_id_y','seconds_in_bucket_y','diff'],inplace=True)\n        merged_data['stock_id'] = stock_id\n    merged_data.dropna(inplace=True)\n    merged_data.reset_index(drop=True)\n    return merged_data\n\ndef read_all_files(path):\n    \"\"\" Reads All file in the sub Folder (path / *) and read all parquets (trade/book) and picks only the first occurence based on Stock + Time\n        Returns a list of all dataframs use concat to join them back .\"\"\"\n    demo_all = []\n    for i in tqdm(glob(os.path.join(path,'*'))):\n        demo_merged = path_to_data(i)\n        demo = demo_merged.groupby(['stock_id','time_id']).first().reset_index()\n        demo.stock_id = demo.stock_id.astype('int64')\n        demo_all.append(demo)\n    return demo_all\n\ndef files_to_numbers(demo_all,vol_calculated,csv_path = '../input/optiver-realized-volatility-prediction/train.csv'):\n    \"\"\" Takes in a List of DataFrame and Merges them with a CSV File and then with preprocessed data that we have where we calculate the Volatility\n        at end of 10 min or bucket mark \"\"\"\n    csv_file = pd.read_csv(csv_path)\n    demo = pd.concat(demo_all).reset_index(drop=True)\n    demo_vol = pd.merge(csv_file,demo,on=['stock_id','time_id'])\n    demo_vol_all_data = pd.merge(demo_vol,vol_calculated)\n    return demo_vol_all_data\n\ndef ffill(data_df):\n    data_df=data_df.set_index(['time_id', 'seconds_in_bucket'])\n    data_df = data_df.reindex(pd.MultiIndex.from_product([data_df.index.levels[0], np.arange(0,600)], names = ['time_id', 'seconds_in_bucket']), method='ffill')\n    return data_df.reset_index()","metadata":{"execution":{"iopub.status.busy":"2021-07-14T09:06:24.888894Z","iopub.execute_input":"2021-07-14T09:06:24.889622Z","iopub.status.idle":"2021-07-14T09:06:24.908449Z","shell.execute_reply.started":"2021-07-14T09:06:24.88957Z","shell.execute_reply":"2021-07-14T09:06:24.907133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"STEPS TO START TRAINING\n1. Read All Files , Pick First Occurence by Grouping on stockid time_id\n2. Merge With Training csv to give these labels/target and then with preprocessed data that we have where we calculate the Volatility at end of 10 min or bucket mark.\n3. This gives a size of 428913 and train csv is of size 428932 . 19 Points have been discarded by us .","metadata":{}},{"cell_type":"code","source":"demo_all = read_all_files('../input/optiver-realized-volatility-prediction/book_train.parquet')\ndata = files_to_numbers(demo_all,past_data,'../input/optiver-realized-volatility-prediction/train.csv')\ndisplay(data)\n# print(data.stock_id.value_counts())\n# print(data.time_id.nunique())","metadata":{"execution":{"iopub.status.busy":"2021-07-14T09:06:26.060263Z","iopub.execute_input":"2021-07-14T09:06:26.060693Z","iopub.status.idle":"2021-07-14T09:08:29.872917Z","shell.execute_reply.started":"2021-07-14T09:06:26.060657Z","shell.execute_reply":"2021-07-14T09:08:29.871686Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_df = data.copy()\n# data_df.drop(columns=['stock_id','time_id'],inplace=True)\ndf_traval,df_test = train_test_split(data_df,stratify=data_df['stock_id'])\ndf_train,df_val = train_test_split(df_traval,stratify=df_traval['stock_id'])\ndf_train.drop(columns=['stock_id','time_id'],inplace=True)\ndf_val.drop(columns=['stock_id','time_id'],inplace=True)\ndf_test.drop(columns=['stock_id','time_id'],inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-07-14T09:08:29.874963Z","iopub.execute_input":"2021-07-14T09:08:29.875299Z","iopub.status.idle":"2021-07-14T09:08:30.577041Z","shell.execute_reply.started":"2021-07-14T09:08:29.875267Z","shell.execute_reply":"2021-07-14T09:08:30.575795Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature = list(df_train.columns)\nfeature.remove('target')\ntrain_x,train_y = df_train[feature],df_train['target']\nval_x,val_y = df_val[feature],df_val['target']\ntest_x,test_y = df_test[feature],df_test['target']\nstd_scaler = StandardScaler()\ntrain_x_transform = std_scaler.fit_transform(train_x)\nval_x_transform = std_scaler.transform(val_x)\ntest_x_transform = std_scaler.transform(test_x)","metadata":{"execution":{"iopub.status.busy":"2021-07-14T09:08:30.579537Z","iopub.execute_input":"2021-07-14T09:08:30.579954Z","iopub.status.idle":"2021-07-14T09:08:30.708486Z","shell.execute_reply.started":"2021-07-14T09:08:30.579911Z","shell.execute_reply":"2021-07-14T09:08:30.707328Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"LINEAR REGRESSION SCORES .305 on Test data","metadata":{}},{"cell_type":"code","source":"lr = LinearRegression()\nlr.fit(train_x_transform,train_y)\nprint(lr.score(train_x_transform,train_y),lr.score(val_x_transform,val_y),lr.score(test_x_transform,test_y))\nprint('Mean Squared Error \\t RMSPE METRIC(COMP)')\nprint(mean_squared_error(lr.predict(train_x_transform),train_y),rmspe(lr.predict(train_x_transform),train_y))\nprint(mean_squared_error(lr.predict(val_x_transform),val_y),rmspe(lr.predict(val_x_transform),val_y))\nprint(mean_squared_error(lr.predict(test_x_transform),test_y),rmspe(lr.predict(test_x_transform),test_y))\nprint(lr.predict(test_x_transform[:5]),test_y[:5].values)","metadata":{"execution":{"iopub.status.busy":"2021-07-14T09:08:30.710018Z","iopub.execute_input":"2021-07-14T09:08:30.710363Z","iopub.status.idle":"2021-07-14T09:08:31.081039Z","shell.execute_reply.started":"2021-07-14T09:08:30.710328Z","shell.execute_reply":"2021-07-14T09:08:31.079394Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To make Prediction on any folder we need 3 things ,\n1. Read All files in it \n2. Merge with the Correct CSV File it is related to \n3. Merge with Volatily result ie past data","metadata":{}},{"cell_type":"code","source":"demo_all_pred = read_all_files('../input/optiver-realized-volatility-prediction/book_test.parquet')\n\nbook_test_dir = glob('../input/optiver-realized-volatility-prediction/book_test.parquet/*/*')\ntrain_data_vol_pred = []\nfor i in tqdm(book_test_dir):\n    train_data_vol_pred.append(file_path_to_volatility(i))\npast_data_pred = pd.concat([pd.DataFrame(i,columns=['stock_id','time_id','vol']) for i in train_data_vol_pred])\npast_data_pred.stock_id = past_data_pred.stock_id.astype('int64')\n\ndata_pred = files_to_numbers(demo_all_pred,past_data_pred,'../input/optiver-realized-volatility-prediction/test.csv')\ndata_pred.dropna(inplace=True)\ndata_pred.reset_index(drop=True)\nrow_orderings = data_pred.row_id\ndata_pred.drop(columns='row_id',inplace=True)\ndata_pred = data_pred[feature]\npred = lr.predict(std_scaler.transform(data_pred))\nsub = pd.merge(sample_sub,pd.DataFrame({'row_id':row_orderings,'target':pred}),on='row_id',how='left',suffixes=['_old',''])[sample_sub.columns]\nsub.target.fillna(0.003048022,inplace=True)\ndisplay(sub)\nsub.to_csv('submission.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2021-07-14T09:08:31.083166Z","iopub.execute_input":"2021-07-14T09:08:31.083776Z","iopub.status.idle":"2021-07-14T09:08:31.258797Z","shell.execute_reply.started":"2021-07-14T09:08:31.083718Z","shell.execute_reply":"2021-07-14T09:08:31.25774Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Load via External Method","metadata":{}},{"cell_type":"code","source":"from joblib import Parallel, delayed\ndef log_return(list_stock_prices):\n    return np.log(list_stock_prices).diff()\ndef realized_volatility(series_log_return):\n    return np.sqrt(np.sum(series_log_return**2))\ndef get_dataSet(stock_ids : list, dataType = 'train'):\n\n    stock_stat = Parallel(n_jobs=-1)(\n        delayed(get_stock_stat_df)(stock_id, dataType) \n        for stock_id in stock_ids\n    )\n    \n    stock_stat_df = pd.concat(stock_stat, ignore_index = True)\n\n    return stock_stat_df\ndef get_stock_stat_df(stock_id, dataset):\n    \n    book = pd.read_parquet(f'../input/optiver-realized-volatility-prediction/book_{dataset}.parquet/stock_id={stock_id}/')\n    book['stock_id'] = stock_id\n    \n    # Calculate WAP\n    book['wap1'] = (book.bid_price1 * book.ask_size1 + book.ask_price1 * book.bid_size1) / (book.bid_size1 + book.ask_size1)\n    book['wap2'] = (book.bid_price2 * book.ask_size2 + book.ask_price2 * book.bid_size2) / (book.bid_size2+ book.ask_size2)\n    \n    # Log return\n    book['log_return1'] = log_return(book['wap1'])\n    book['log_return2'] = log_return(book['wap2'])\n    \n    # Bid Ask Spread\n    book['bid_ask_spread1'] = abs(book['bid_price1'] - book['ask_price1'])\n    book['bid_ask_spread2'] = abs(book['bid_price2'] - book['ask_price2'])\n    \n    book = book[~book['log_return1'].isnull()]\n    book = book[~book['log_return2'].isnull()]\n    \n    stock_stat = book.groupby(['stock_id', 'time_id']).agg({'bid_ask_spread1':['mean'],\n                                                           'bid_ask_spread2':['mean'],\n                                                           'log_return1':[realized_volatility],\n                                                           'log_return2':[realized_volatility]}).reset_index()\n    \n    # Trade\n    trade =  pd.read_parquet(f'../input/optiver-realized-volatility-prediction/trade_{dataset}.parquet/stock_id={stock_id}/')\n    trade['stock_id'] = stock_id\n    trade['trade_log_return'] = trade.groupby(['time_id'])['price'].apply(log_return).fillna(0)\n    trade_stat = trade.groupby(['stock_id', 'time_id']).agg({'price':['mean'], \n                                                             'size':['sum'], \n                                                             'order_count':['sum'],\n                                                             'trade_log_return':[realized_volatility]}).reset_index()\n    \n    # Merge book and trade dataframe\n    stats = stock_stat.merge(trade_stat, on=['stock_id', 'time_id'], how='left')\n    \n    return stats\n\ntest_stock_ids = list(test['stock_id'].unique())\ntest_stock_stats_df = get_dataSet(test_stock_ids, 'test')\ntest = pd.merge(test, test_stock_stats_df, on = ['stock_id', 'time_id'], how = 'left')\ntest.fillna(0, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-07-14T10:22:36.470479Z","iopub.execute_input":"2021-07-14T10:22:36.471076Z","iopub.status.idle":"2021-07-14T10:22:37.361173Z","shell.execute_reply.started":"2021-07-14T10:22:36.471019Z","shell.execute_reply":"2021-07-14T10:22:37.358917Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test","metadata":{"execution":{"iopub.status.busy":"2021-07-14T10:22:42.186526Z","iopub.execute_input":"2021-07-14T10:22:42.186905Z","iopub.status.idle":"2021-07-14T10:22:42.209598Z","shell.execute_reply.started":"2021-07-14T10:22:42.186868Z","shell.execute_reply":"2021-07-14T10:22:42.208657Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# demo = demo_merged.groupby(['stock_id','time_id']).first().reset_index()\n# demo.stock_id = demo.stock_id.astype('int64')\n# demo_merged.stock_id = demo_merged.stock_id.astype('int64')\n# demo_vol = pd.merge(train,demo,on=['stock_id','time_id'])[['stock_id','time_id','seconds_in_bucket','target']]\n# # demo_vol_all =  pd.merge(demo_merged,demo_vol,on=['stock_id','time_id','seconds_in_bucket'],how='left')\n# demo_vol_all =  pd.merge(demo_merged,demo_vol,on=['stock_id','time_id','seconds_in_bucket'])\n# demo_vol_all_data = pd.merge(demo_vol_all,past_data)\n# display(demo_vol_all)","metadata":{"execution":{"iopub.status.busy":"2021-07-13T12:09:31.959264Z","iopub.execute_input":"2021-07-13T12:09:31.959654Z","iopub.status.idle":"2021-07-13T12:09:31.964394Z","shell.execute_reply.started":"2021-07-13T12:09:31.959606Z","shell.execute_reply":"2021-07-13T12:09:31.962924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2021-07-13T13:01:21.952599Z","iopub.execute_input":"2021-07-13T13:01:21.952969Z","iopub.status.idle":"2021-07-13T13:01:21.989318Z","shell.execute_reply.started":"2021-07-13T13:01:21.952939Z","shell.execute_reply":"2021-07-13T13:01:21.988153Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}