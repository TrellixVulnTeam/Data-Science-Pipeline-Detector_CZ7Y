{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport os\nfrom glob import glob\nimport plotly_express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nfrom tqdm import tqdm\nfrom sklearn.metrics import *\nfrom sklearn.model_selection import *\nfrom sklearn.preprocessing import *\nfrom sklearn.linear_model import LinearRegression","metadata":{"execution":{"iopub.status.busy":"2021-07-15T09:56:07.879173Z","iopub.execute_input":"2021-07-15T09:56:07.879597Z","iopub.status.idle":"2021-07-15T09:56:10.343446Z","shell.execute_reply.started":"2021-07-15T09:56:07.879513Z","shell.execute_reply":"2021-07-15T09:56:10.342536Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('../input/optiver-realized-volatility-prediction/train.csv')\ntest  = pd.read_csv('../input/optiver-realized-volatility-prediction/test.csv')\nsample_sub = pd.read_csv('../input/optiver-realized-volatility-prediction/sample_submission.csv')\ndisplay(train)\ndisplay(test)\ndisplay(sample_sub)","metadata":{"execution":{"iopub.status.busy":"2021-07-15T09:56:10.344853Z","iopub.execute_input":"2021-07-15T09:56:10.345143Z","iopub.status.idle":"2021-07-15T09:56:10.642293Z","shell.execute_reply.started":"2021-07-15T09:56:10.345112Z","shell.execute_reply":"2021-07-15T09:56:10.641451Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Stock ID Act as a kind of unique token for different stock.\n\nTime ID not exact in sequence but kind of specifying a token for time bucket .//A bit Vague","metadata":{}},{"cell_type":"code","source":"print('Different Unique Stocks IDS in training ',len(glob('../input/optiver-realized-volatility-prediction/book_train.parquet/*')),\n      'Different Unique Stocks IDS in test ',len(glob('../input/optiver-realized-volatility-prediction/book_test.parquet/*'))\n     )\nprint('Uniques in \\n',train.nunique())\npx.histogram(train.time_id)","metadata":{"execution":{"iopub.status.busy":"2021-07-15T10:39:33.685594Z","iopub.execute_input":"2021-07-15T10:39:33.685981Z","iopub.status.idle":"2021-07-15T10:39:35.989863Z","shell.execute_reply.started":"2021-07-15T10:39:33.685941Z","shell.execute_reply":"2021-07-15T10:39:35.988704Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Analysis on Train csv First Row \n\n0\t5\t0.004136","metadata":{}},{"cell_type":"code","source":"def file_path_to_volatility(path,info=False):\n    part_data = []\n    stock_id = path.split('/')[4].split('=')[1]\n    sample_book = pd.read_parquet(path)\n    sample_book['wap'] = (sample_book['bid_price1'] * sample_book['ask_size1'] + sample_book['ask_price1'] * sample_book['bid_size1']) / (sample_book['bid_size1']+ sample_book['ask_size1'])\n    sample_book.dropna(inplace=True)\n    for gid0,gid in sample_book.groupby('time_id'):\n        gid['log_return'] = gid['wap'].apply(lambda x:np.log(x)).diff()\n        if info :\n            print(f'Realized Volatiliy for time id {gid.time_id.iloc[0]} is ' ,np.sqrt(np.sum(gid['log_return'].reset_index(drop=True).drop(index = 0).apply(lambda x:x**2))))\n        part_data.append([stock_id,gid.time_id.iloc[0],np.sqrt(np.sum(gid['log_return'].reset_index(drop=True).drop(index = 0).apply(lambda x:x**2)))])\n    return part_data\nbook_dir = glob('../input/optiver-realized-volatility-prediction/book_train.parquet/*/*')\n# PRE LOADING THIS FILE (ALREADY RAN AND SAVED IN INPUTS)\n# train_data_vol = []\n# for i in tqdm(book_dir):\n#     train_data_vol.append(file_path_to_volatility(i))\n# past_data = pd.concat([pd.DataFrame(i,columns=['stock_id','time_id','vol']) for i in train_data_vol])\n# past_data.stock_id = past_data.stock_id.astype('int64')\npast_data = pd.read_csv('../input/starter/train_data.csv')\nprint(past_data.dtypes)\nprint(train.dtypes)\ndisplay(past_data)","metadata":{"execution":{"iopub.status.busy":"2021-07-15T09:56:14.55576Z","iopub.execute_input":"2021-07-15T09:56:14.55633Z","iopub.status.idle":"2021-07-15T09:56:15.133826Z","shell.execute_reply.started":"2021-07-15T09:56:14.556289Z","shell.execute_reply":"2021-07-15T09:56:15.13309Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# METRICS\ndef rmspe(y_true, y_pred):\n    return  (np.sqrt(np.mean(np.square((y_true - y_pred) / y_true))))\n# print('root mean square percentage error'.upper(),rmspe(train_pred['target'],train_pred['vol']),\n#      'root mean square error'.upper(),mean_squared_error(train_pred['target'],train_pred['vol']))\nprint(train.shape,past_data.shape)\ndisplay(pd.merge(train,past_data,on=['stock_id','time_id']))","metadata":{"execution":{"iopub.status.busy":"2021-07-15T09:56:15.135032Z","iopub.execute_input":"2021-07-15T09:56:15.135338Z","iopub.status.idle":"2021-07-15T09:56:15.251952Z","shell.execute_reply.started":"2021-07-15T09:56:15.135309Z","shell.execute_reply":"2021-07-15T09:56:15.250922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Try to combine the 2 trade list and make the data suitable for Model **\n\n../input/optiver-realized-volatility-prediction/book_train.parquet/stock_id=0\n\n../input/optiver-realized-volatility-prediction/trade_train.parquet/stock_id=0\n\nAfter Merging Trade Book and Trade Train , i will add the train.csv file will give some rows the orignal true volatility and some of them NANS.\nFirst Just considering the orginals one .","metadata":{}},{"cell_type":"code","source":"def path_to_data(path):\n    \"\"\" This return a merged dataframe of trades where the trades actually took place \"\"\"\n#     print(path)\n    stock_id = path.split('/')[-1].split('=')[1]\n    curr_book = pd.read_parquet(path)\n    curr_trade = pd.read_parquet(path.replace('book','trade'))\n    merged_data = pd.merge(curr_book,curr_trade,on=['time_id','seconds_in_bucket'])\n    merged_data['stock_id'] = stock_id\n#     print(curr_book.shape,curr_trade.shape,len(merged_data))\n    if len(merged_data) ==0 :\n        merged_data = curr_trade.merge(curr_book, how='cross',suffixes=['','_y'])\n        merged_data['diff'] = abs(merged_data.seconds_in_bucket-merged_data.seconds_in_bucket_y)\n        merged_data = pd.merge(merged_data.groupby(['time_id','seconds_in_bucket'])['diff'].min().reset_index(),merged_data,how=\"left\")\n        merged_data.drop(columns=['time_id_y','seconds_in_bucket_y','diff'],inplace=True)\n        merged_data['stock_id'] = stock_id\n    merged_data.dropna(inplace=True)\n    merged_data.reset_index(drop=True)\n    return merged_data\n\ndef read_all_files(path):\n    \"\"\" Reads All file in the sub Folder (path / *) and read all parquets (trade/book) and picks only the first occurence based on Stock + Time\n        Returns a list of all dataframs use concat to join them back .\"\"\"\n    demo_all = []\n    for i in tqdm(glob(os.path.join(path,'*'))):\n        demo_merged = path_to_data(i)\n        demo = demo_merged.groupby(['stock_id','time_id']).first().reset_index()\n        demo.stock_id = demo.stock_id.astype('int64')\n        demo_all.append(demo)\n    return demo_all\n\ndef files_to_numbers(demo_all,vol_calculated,csv_path = '../input/optiver-realized-volatility-prediction/train.csv'):\n    \"\"\" Takes in a List of DataFrame and Merges them with a CSV File and then with preprocessed data that we have where we calculate the Volatility\n        at end of 10 min or bucket mark \"\"\"\n    csv_file = pd.read_csv(csv_path)\n    demo = pd.concat(demo_all).reset_index(drop=True)\n    demo_vol = pd.merge(csv_file,demo,on=['stock_id','time_id'])\n    demo_vol_all_data = pd.merge(demo_vol,vol_calculated)\n    return demo_vol_all_data\n\ndef ffill(data_df):\n    data_df=data_df.set_index(['time_id', 'seconds_in_bucket'])\n    data_df = data_df.reindex(pd.MultiIndex.from_product([data_df.index.levels[0], np.arange(0,600)], names = ['time_id', 'seconds_in_bucket']), method='ffill')\n    return data_df.reset_index()","metadata":{"execution":{"iopub.status.busy":"2021-07-15T09:56:15.253201Z","iopub.execute_input":"2021-07-15T09:56:15.253543Z","iopub.status.idle":"2021-07-15T09:56:15.268925Z","shell.execute_reply.started":"2021-07-15T09:56:15.253514Z","shell.execute_reply":"2021-07-15T09:56:15.267572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"STEPS TO START TRAINING\n1. Read All Files , Pick First Occurence by Grouping on stockid time_id\n2. Merge With Training csv to give these labels/target and then with preprocessed data that we have where we calculate the Volatility at end of 10 min or bucket mark.\n3. This gives a size of 428913 and train csv is of size 428932 . 19 Points have been discarded by us .","metadata":{}},{"cell_type":"code","source":"demo_all = read_all_files('../input/optiver-realized-volatility-prediction/book_train.parquet')\ndata = files_to_numbers(demo_all,past_data,'../input/optiver-realized-volatility-prediction/train.csv')\ndisplay(data)\n# print(data.stock_id.value_counts())\n# print(data.time_id.nunique())","metadata":{"execution":{"iopub.status.busy":"2021-07-15T09:56:15.271865Z","iopub.execute_input":"2021-07-15T09:56:15.272301Z","iopub.status.idle":"2021-07-15T09:57:48.607109Z","shell.execute_reply.started":"2021-07-15T09:56:15.272258Z","shell.execute_reply":"2021-07-15T09:57:48.606072Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_df = data.copy()\nfeature = ['seconds_in_bucket','bid_price1','ask_price1','bid_price2','ask_price2','bid_size1','ask_size1','bid_size2','ask_size2','price','size','order_count','vol']\ndata_df.drop(columns=['stock_id','time_id'],inplace=True)\ntrain_data,train_label = data_df[feature],data_df['target']\nstd_scaler = StandardScaler()\ntrain_data_transform = std_scaler.fit_transform(train_data)\n\n\n\nlr = LinearRegression()\nlr.fit(train_data_transform,train_label)\nprint(lr.score(train_data_transform,train_label))\nprint('Mean Squared Error \\t RMSPE METRIC(COMP)')\nprint(mean_squared_error(lr.predict(train_data_transform),train_label),rmspe(lr.predict(train_data_transform),train_label))\nprint(lr.predict(train_data_transform[:5]),train_label[:5].values)","metadata":{"execution":{"iopub.status.busy":"2021-07-15T10:03:10.544166Z","iopub.execute_input":"2021-07-15T10:03:10.544534Z","iopub.status.idle":"2021-07-15T10:03:10.900542Z","shell.execute_reply.started":"2021-07-15T10:03:10.54449Z","shell.execute_reply":"2021-07-15T10:03:10.899271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"LINEAR REGRESSION SCORES .305 - .315 on Test data","metadata":{}},{"cell_type":"markdown","source":"To make Prediction on any folder we need 3 things ,\n1. Read All files in it \n2. Merge with the Correct CSV File it is related to \n3. Merge with Volatily result ie past data","metadata":{}},{"cell_type":"code","source":"demo_all_pred = read_all_files('../input/optiver-realized-volatility-prediction/book_test.parquet')\n\nbook_test_dir = glob('../input/optiver-realized-volatility-prediction/book_test.parquet/*/*')\ntrain_data_vol_pred = []\nfor i in tqdm(book_test_dir):\n    train_data_vol_pred.append(file_path_to_volatility(i))\npast_data_pred = pd.concat([pd.DataFrame(i,columns=['stock_id','time_id','vol']) for i in train_data_vol_pred])\npast_data_pred.stock_id = past_data_pred.stock_id.astype('int64')\n\ndata_pred = files_to_numbers(demo_all_pred,past_data_pred,'../input/optiver-realized-volatility-prediction/test.csv')\ndata_pred.dropna(inplace=True)\ndata_pred.reset_index(drop=True)\nrow_orderings = data_pred.row_id\ndata_pred.drop(columns='row_id',inplace=True)\ndata_pred = data_pred[feature]\npred = lr.predict(std_scaler.transform(data_pred))\nsub = pd.merge(sample_sub,pd.DataFrame({'row_id':row_orderings,'target':pred}),on='row_id',how='left',suffixes=['_old',''])[sample_sub.columns]\nsub.target.fillna(0.003048022,inplace=True)\ndisplay(sub)\nsub.to_csv('submission.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2021-07-15T10:03:11.96184Z","iopub.execute_input":"2021-07-15T10:03:11.962321Z","iopub.status.idle":"2021-07-15T10:03:12.07426Z","shell.execute_reply.started":"2021-07-15T10:03:11.96229Z","shell.execute_reply":"2021-07-15T10:03:12.073359Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}