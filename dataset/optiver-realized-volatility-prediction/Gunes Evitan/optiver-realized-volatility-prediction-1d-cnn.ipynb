{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport random\nfrom tqdm.notebook import tqdm\n\nimport numpy as np\nimport pandas as pd\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import LabelEncoder\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader, SequentialSampler, RandomSampler\nimport torch.optim as optim\nfrom fastai.layers import SigmoidRange","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-15T11:50:47.068636Z","iopub.execute_input":"2021-08-15T11:50:47.068962Z","iopub.status.idle":"2021-08-15T11:50:47.259553Z","shell.execute_reply.started":"2021-08-15T11:50:47.068929Z","shell.execute_reply":"2021-08-15T11:50:47.258697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class PreprocessingPipeline:\n    \n    def __init__(self, df_train, df_test, n_splits, shuffle, random_state):\n        \n        self.df_train = df_train.copy(deep=True)\n        self.df_test = df_test.copy(deep=True)\n        self.n_splits = n_splits\n        self.shuffle = shuffle\n        self.random_state = random_state\n        \n    def _label_encode(self):\n\n        # Encoding stock_id for embeddings\n        le = LabelEncoder()\n        self.df_train['stock_id_encoded'] = le.fit_transform(self.df_train['stock_id'].values)\n        self.df_test['stock_id_encoded'] = le.transform(self.df_test['stock_id'].values)\n    \n    def _get_folds(self):\n        \n        skf = StratifiedKFold(n_splits=self.n_splits, shuffle=self.shuffle, random_state=self.random_state)\n        for fold, (_, val_idx) in enumerate(skf.split(X=self.df_train, y=self.df_train['stock_id']), 1):\n            df_train.loc[val_idx, 'fold'] = fold\n        self.df_train['fold'] = df_train['fold'].astype(np.uint8)\n            \n    def transform(self):\n        \n        self._label_encode()\n        self._get_folds()\n        \n        return self.df_train, self.df_test\n","metadata":{"execution":{"iopub.status.busy":"2021-08-15T11:22:47.98095Z","iopub.execute_input":"2021-08-15T11:22:47.981218Z","iopub.status.idle":"2021-08-15T11:22:47.991806Z","shell.execute_reply.started":"2021-08-15T11:22:47.98119Z","shell.execute_reply":"2021-08-15T11:22:47.990998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_test_dtypes = {\n    'stock_id': np.uint8,\n    'time_id': np.uint16,\n    'target': np.float64\n}\n\ndf_train = pd.read_csv('../input/optiver-realized-volatility-prediction/train.csv', dtype=train_test_dtypes)\ndf_test = pd.read_csv('../input/optiver-realized-volatility-prediction/test.csv', usecols=['stock_id', 'time_id'], dtype=train_test_dtypes)\n\npreprocessing_parameters = {\n    'df_train': df_train,\n    'df_test': df_test,\n    'n_splits': 5,\n    'shuffle': True,\n    'random_state': 42\n}\n\npreprocessing_pipeline = PreprocessingPipeline(**preprocessing_parameters)\ndf_train, df_test = preprocessing_pipeline.transform()\n\nprint(f'Training Set Shape: {df_train.shape} - Memory Usage: {df_train.memory_usage().sum() / 1024 ** 2:.2f} MB')\nprint(f'Test Set Shape: {df_test.shape} - Memory Usage: {df_test.memory_usage().sum() / 1024 ** 2:.2f} MB')","metadata":{"execution":{"iopub.status.busy":"2021-08-15T11:22:47.99485Z","iopub.execute_input":"2021-08-15T11:22:47.995178Z","iopub.status.idle":"2021-08-15T11:22:48.380462Z","shell.execute_reply.started":"2021-08-15T11:22:47.995071Z","shell.execute_reply":"2021-08-15T11:22:48.379528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def visualize_learning_curve(training_losses, validation_losses, title, path=None):\n    \n    \"\"\"\n    Visualize learning curves of the models\n\n    Parameters\n    ----------\n    training_losses [array-like of shape (n_epochs)]: Array of training losses computed after every epoch\n    validation_losses [array-like of shape (n_epochs)]: Array of validation losses computed after every epoch\n    title (str): Title of the plot\n    path (str or None): Path of the output file (if path is None, plot is displayed with selected backend)\n    \"\"\"\n\n    fig, ax = plt.subplots(figsize=(32, 8), dpi=100)\n\n    sns.lineplot(\n        x=np.arange(1, len(training_losses) + 1),\n        y=training_losses,\n        ax=ax,\n        label='train_loss'\n    )\n    sns.lineplot(\n        x=np.arange(1, len(validation_losses) + 1),\n        y=validation_losses,\n        ax=ax,\n        label='val_loss'\n    )\n\n    ax.set_xlabel('Epochs/Steps', size=15, labelpad=12.5)\n    ax.set_ylabel('Loss', size=15, labelpad=12.5)\n    ax.tick_params(axis='x', labelsize=12.5, pad=10)\n    ax.tick_params(axis='y', labelsize=12.5, pad=10)\n    ax.legend(prop={'size': 18})\n    ax.set_title(title, size=20, pad=15)\n\n    if path is None:\n        plt.show()\n    else:\n        plt.savefig(path)\n","metadata":{"execution":{"iopub.status.busy":"2021-08-15T11:42:45.219771Z","iopub.execute_input":"2021-08-15T11:42:45.220119Z","iopub.status.idle":"2021-08-15T11:42:45.23104Z","shell.execute_reply.started":"2021-08-15T11:42:45.220089Z","shell.execute_reply":"2021-08-15T11:42:45.230135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def set_seed(seed, deterministic_cudnn=False):\n\n    \"\"\"\n    Set random seed for reproducible results\n    \n    Parameters\n    ----------\n    seed (int): Random seed\n    deterministic_cudnn (bool): Whether to set deterministic cuDNN or not\n    \"\"\"\n\n    if deterministic_cudnn:\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\n\ndef rmspe_metric(y_true, y_pred):\n\n    \"\"\"\n    Calculate root mean squared percentage error between ground-truth and predictions\n    \n    Parameters\n    ----------\n    y_true [array-like of shape (n_samples)]: Ground-truth\n    y_pred [array-like of shape (n_samples)]: Predictions\n    \n    Returns\n    -------\n    rmspe (float): Root mean squared percentage error\n    \"\"\"\n\n    rmspe = np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\n    return rmspe\n\n\ndef rmspe_loss(y_true, y_pred):\n\n    \"\"\"\n    Calculate root mean squared percentage error between ground-truth and predictions\n    \n    Parameters\n    ----------\n    y_true [torch.tensor of shape (n_samples)]: Ground-truth\n    y_pred [torch.tensor of shape (n_samples)]: Predictions\n    \n    Returns\n    -------\n    rmspe (torch.FloatTensor): Root mean squared percentage error\n    \"\"\"\n\n    rmspe = torch.sqrt(torch.mean(torch.square((y_true - y_pred) / y_true)))\n    return rmspe\n","metadata":{"execution":{"iopub.status.busy":"2021-08-15T11:42:46.088559Z","iopub.execute_input":"2021-08-15T11:42:46.088909Z","iopub.status.idle":"2021-08-15T11:42:46.09642Z","shell.execute_reply.started":"2021-08-15T11:42:46.08888Z","shell.execute_reply":"2021-08-15T11:42:46.095518Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Optiver2DDataset(Dataset):\n\n    def __init__(self, df, flip_probability=0.):\n\n        self.df = df\n        # Normalizing sequences with global means and stds across stocks\n        book_means = np.array([\n            0.99969482421875, 1.000321388244629, 0.9995064735412598, 1.0005191564559937,\n            769.990177708821, 766.7345672818379, 959.3416027831918, 928.2202512713748,\n            1.0000068043192514, 1.0000055320253616, 5.129816581143487e-08, 9.831598141593519e-08\n        ])\n        book_stds = np.array([\n            0.0036880988627672195, 0.003687119111418724, 0.0037009266670793295, 0.0036990800872445107,\n            5354.051690318169, 4954.947103063445, 6683.816183660414, 5735.299917793827,\n            0.003689893218043926, 0.00370745215558702, 6.618708642293018e-07, 1.2508970015188411e-06\n        ])\n        # Not normalizing trade price and trade price log returns because of the sparsity\n        trade_means = np.array([0, 352.9736760331942, 4.1732040971227145, 0])\n        trade_stds = np.array([1, 1041.9441951057488, 7.79955795393431, 1])\n\n        self.transforms = {\n            'flip': flip_probability,\n            'normalize': {\n                'book_means': book_means,\n                'book_stds': book_stds,\n                'trade_means': trade_means,\n                'trade_stds': trade_stds\n            }\n        }\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n\n        \"\"\"\n        Get the idxth element in the dataset\n\n        Parameters\n        ----------\n        idx (int): Index of the sample (0 <= idx < len(self.df))\n\n        Returns\n        -------\n        stock_id_encoded [torch.LongTensor of shape (1)]: Encoded stock_id for stock embeddings\n        sequences [torch.FloatTensor of shape (600, 16)]: Concatenated sequences from book and trade data\n        target [torch.Tensor of shape (1)]: Target\n        \"\"\"\n\n        sample = self.df.iloc[idx]\n        stock_id = int(sample['stock_id'])\n        time_id = int(sample['time_id'])\n\n        # Sequences from book data\n        book_sequences = np.load(f'../input/optiver-realized-volatility-npy-files/book_train/book_train/stock_{stock_id}/time_{time_id}.npy')\n        book_wap1 = (book_sequences[:, 0] * book_sequences[:, 5] + book_sequences[:, 1] * book_sequences[:, 4]) /\\\n                    (book_sequences[:, 4] + book_sequences[:, 5])\n        book_wap2 = (book_sequences[:, 2] * book_sequences[:, 7] + book_sequences[:, 3] * book_sequences[:, 6]) /\\\n                    (book_sequences[:, 6] + book_sequences[:, 7])\n        book_wap1_log = np.log(book_wap1)\n        book_wap1_log_returns = np.diff(book_wap1_log, prepend=[book_wap1_log[0]])\n        book_wap2_log = np.log(book_wap2)\n        book_wap2_log_returns = np.diff(book_wap2_log, prepend=[book_wap2_log[0]])\n        book_sequences = np.hstack([\n            book_sequences,\n            book_wap1.reshape(-1, 1),\n            book_wap2.reshape(-1, 1),\n            book_wap1_log_returns.reshape(-1, 1),\n            book_wap2_log_returns.reshape(-1, 1),\n        ])\n        book_sequences = (book_sequences - self.transforms['normalize']['book_means']) / self.transforms['normalize']['book_stds']\n\n        # Sequences from trade data\n        trade_sequences = np.load(f'../input/optiver-realized-volatility-npy-files/trade_train/trade_train/stock_{stock_id}/time_{time_id}.npy')\n        trade_price_log1p = np.log1p(trade_sequences[:, 0])\n        trade_price_log_returns = np.diff(trade_price_log1p, prepend=trade_price_log1p[0])\n        trade_sequences = np.hstack([trade_sequences, trade_price_log_returns.reshape(-1, 1)])\n\n        # Concatenate book and trade sequences\n        sequences = np.hstack([book_sequences, trade_sequences])\n        sequences = torch.as_tensor(sequences, dtype=torch.float)\n\n        # Flip sequences on zeroth dimension\n        if np.random.rand() < self.transforms['flip']:\n            sequences = torch.flip(sequences, dims=[0])\n\n        stock_id_encoded = torch.as_tensor(sample['stock_id_encoded'], dtype=torch.long)\n        target = sample['target']\n        target = torch.as_tensor(target, dtype=torch.float)\n        return stock_id_encoded, sequences, target\n","metadata":{"execution":{"iopub.status.busy":"2021-08-15T11:42:46.853272Z","iopub.execute_input":"2021-08-15T11:42:46.853631Z","iopub.status.idle":"2021-08-15T11:42:46.87127Z","shell.execute_reply.started":"2021-08-15T11:42:46.8536Z","shell.execute_reply":"2021-08-15T11:42:46.870352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Conv1dBlock(nn.Module):\n\n    def __init__(self, in_channels, out_channels, kernel_size=(5,), stride=(1,), padding=(2,), skip_connection=False):\n\n        super(Conv1dBlock, self).__init__()\n\n        self.skip_connection = skip_connection\n        self.conv_block = nn.Sequential(\n            nn.Conv1d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding, padding_mode='replicate', bias=True),\n            nn.BatchNorm1d(out_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv1d(out_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding, padding_mode='replicate', bias=True),\n            nn.BatchNorm1d(out_channels),\n        )\n        self.downsample = nn.Sequential(\n            nn.Conv1d(in_channels, out_channels, kernel_size=(1,), stride=(1,), bias=False),\n            nn.BatchNorm1d(out_channels)\n        )\n        self.relu = nn.ReLU(inplace=True)\n\n    def forward(self, x):\n\n        output = self.conv_block(x)\n        if self.skip_connection:\n            x = self.downsample(x)\n            output += x\n        output = self.relu(output)\n\n        return output\n\n\nclass CNN1DModel(nn.Module):\n\n    def __init__(self, in_channels):\n\n        super(CNN1DModel, self).__init__()\n\n        self.stock_embeddings = nn.Embedding(num_embeddings=113, embedding_dim=16)\n        self.conv_block1 = Conv1dBlock(in_channels=in_channels, out_channels=32, skip_connection=True)\n        self.conv_block2 = Conv1dBlock(in_channels=32, out_channels=64, skip_connection=True)\n        self.conv_block3 = Conv1dBlock(in_channels=64, out_channels=128, skip_connection=True)\n        self.conv_block4 = Conv1dBlock(in_channels=128, out_channels=64, skip_connection=True)\n        self.conv_block5 = Conv1dBlock(in_channels=64, out_channels=32, skip_connection=True)\n        self.conv_block6 = Conv1dBlock(in_channels=32, out_channels=16, skip_connection=True)\n        self.conv_block7 = Conv1dBlock(in_channels=16, out_channels=8, skip_connection=True)\n        self.conv_block8 = Conv1dBlock(in_channels=8, out_channels=1, skip_connection=True)\n        self.pooling = nn.AvgPool1d(kernel_size=(3,), stride=(1,), padding=(1,))\n        self.linear = nn.Linear(616, 256, bias=True)\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(0.25)\n        self.head = nn.Sequential(\n            nn.Linear(256, 1, bias=True),\n            SigmoidRange(0, 0.1)\n        )\n\n    def forward(self, stock_ids, sequences):\n\n        x = torch.transpose(sequences, 1, 2)\n        x = self.conv_block1(x)\n        x = self.pooling(x)\n        x = self.conv_block2(x)\n        x = self.pooling(x)\n        x = self.conv_block3(x)\n        x = self.pooling(x)\n        x = self.conv_block4(x)\n        x = self.pooling(x)\n        x = self.conv_block5(x)\n        x = self.pooling(x)\n        x = self.conv_block6(x)\n        x = self.pooling(x)\n        x = self.conv_block7(x)\n        x = self.pooling(x)\n        x = self.conv_block8(x)\n        x = self.pooling(x)\n        x = x.view(x.size(0), -1)\n        embedded_stock_ids = self.stock_embeddings(stock_ids)\n        x = torch.cat([x, self.dropout(embedded_stock_ids)], dim=1)\n        x = self.relu(self.linear(x))\n        output = self.head(x)\n        \n        return output.view(-1)\n","metadata":{"execution":{"iopub.status.busy":"2021-08-15T11:56:00.434666Z","iopub.execute_input":"2021-08-15T11:56:00.43503Z","iopub.status.idle":"2021-08-15T11:56:00.454482Z","shell.execute_reply.started":"2021-08-15T11:56:00.434997Z","shell.execute_reply":"2021-08-15T11:56:00.453278Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Trainer:\n\n    def __init__(self, model_name, model_path, model_parameters, training_parameters):\n\n        self.model_name = model_name\n        self.model_path = model_path\n        self.model_parameters = model_parameters\n        self.training_parameters = training_parameters\n\n    def get_model(self):\n\n        model = None\n\n        if self.model_name == 'cnn1d':\n            model = CNN1DModel(**self.model_parameters)\n\n        return model\n\n    def train_fn(self, train_loader, model, criterion, optimizer, device):\n\n        print('\\n')\n        model.train()\n        progress_bar = tqdm(train_loader)\n        losses = []\n\n        if self.training_parameters['amp']:\n            scaler = torch.cuda.amp.GradScaler()\n        else:\n            scaler = None\n\n        for stock_id_encoded, sequences, target in progress_bar:\n            \n            stock_id_encoded, sequences, target = stock_id_encoded.to(device), sequences.to(device), target.to(device)\n\n            if scaler is not None:\n                with torch.cuda.amp.autocast():\n                    optimizer.zero_grad()\n                    output = model(stock_id_encoded, sequences)\n                    loss = criterion(target, output)\n                scaler.scale(loss).backward()\n                scaler.step(optimizer)\n                scaler.update()\n            else:\n                optimizer.zero_grad()\n                output = model(stock_id_encoded, sequences)\n                loss = criterion(target, output)\n                loss.backward()\n                optimizer.step()\n\n            losses.append(loss.item())\n            average_loss = np.mean(losses)\n            progress_bar.set_description(f'train_rmspe: {average_loss:.6f}')\n\n        train_loss = np.mean(losses)\n        return train_loss\n\n    def val_fn(self, val_loader, model, criterion, device):\n\n        model.eval()\n        progress_bar = tqdm(val_loader)\n        losses = []\n\n        with torch.no_grad():\n            \n            for stock_id_encoded, sequences, target in progress_bar:\n                \n                stock_id_encoded, sequences, target = stock_id_encoded.to(device), sequences.to(device), target.to(device)\n                output = model(stock_id_encoded, sequences)\n                loss = criterion(target, output)\n                losses.append(loss.item())\n                average_loss = np.mean(losses)\n                progress_bar.set_description(f'val_rmspe: {average_loss:.6f}')\n\n        val_loss = np.mean(losses)\n        return val_loss\n\n    def train_and_validate(self, df_train):\n\n        print(f'\\n{\"-\" * 26}\\nRunning Model for Training\\n{\"-\" * 26}\\n')\n\n        for fold in sorted(df_train['fold'].unique()):\n\n            print(f'\\nFold {fold}\\n{\"-\" * 6}')\n\n            trn_idx, val_idx = df_train.loc[df_train['fold'] != fold].index, df_train.loc[df_train['fold'] == fold].index\n            train_dataset = Optiver2DDataset(df=df_train.loc[trn_idx, :], flip_probability=0.)\n            train_loader = DataLoader(\n                train_dataset,\n                batch_size=self.training_parameters['batch_size'],\n                sampler=RandomSampler(train_dataset),\n                pin_memory=True,\n                drop_last=False,\n                num_workers=self.training_parameters['num_workers'],\n            )\n            val_dataset = Optiver2DDataset(df=df_train.loc[val_idx, :], flip_probability=0.)\n            val_loader = DataLoader(\n                val_dataset,\n                batch_size=self.training_parameters['batch_size'],\n                sampler=SequentialSampler(val_dataset),\n                pin_memory=True,\n                drop_last=False,\n                num_workers=self.training_parameters['num_workers'],\n            )\n\n            set_seed(self.training_parameters['random_state'], deterministic_cudnn=self.training_parameters['deterministic_cudnn'])\n            device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n            model = self.get_model()\n            model = model.to(device)\n\n            criterion = rmspe_loss\n            optimizer = optim.Adam(\n                model.parameters(),\n                lr=self.training_parameters['learning_rate'],\n                betas=self.training_parameters['betas'],\n                weight_decay=self.training_parameters['weight_decay']\n            )\n            scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n                optimizer,\n                mode='min',\n                patience=self.training_parameters['reduce_lr_patience'],\n                factor=self.training_parameters['reduce_lr_factor'],\n                min_lr=self.training_parameters['reduce_lr_min'],\n                verbose=True\n            )\n\n            early_stopping = False\n            summary = {\n                'train_loss': [],\n                'val_loss': []\n            }\n\n            for epoch in range(1, self.training_parameters['epochs'] + 1):\n\n                if early_stopping:\n                    break\n\n                train_loss = self.train_fn(train_loader, model, criterion, optimizer, device)\n                val_loss = self.val_fn(val_loader, model, criterion, device)\n                print(f'Epoch {epoch} - Training Loss: {train_loss:.6f} - Validation Loss: {val_loss:.6f}')\n                scheduler.step(val_loss)\n\n                best_val_loss = np.min(summary['val_loss']) if len(summary['val_loss']) > 0 else np.inf\n                if val_loss < best_val_loss:\n                    model_path = f'{self.model_path}/{self.model_name}_fold{fold}.pt'\n                    torch.save(model.state_dict(), model_path)\n                    print(f'Saving model to {model_path} (validation loss decreased from {best_val_loss:.6f} to {val_loss:.6f})')\n\n                summary['train_loss'].append(train_loss)\n                summary['val_loss'].append(val_loss)\n\n                best_iteration = np.argmin(summary['val_loss']) + 1\n                if len(summary['val_loss']) - best_iteration >= self.training_parameters['early_stopping_patience']:\n                    print(f'Early stopping (validation loss didn\\'t increase for {self.training_parameters[\"early_stopping_patience\"]} epochs/steps)')\n                    print(f'Best validation loss is {np.min(summary[\"val_loss\"]):.6f}')\n                    draw_learning_curve(\n                        training_losses=summary['train_loss'],\n                        validation_losses=summary['val_loss'],\n                        title=f'{self.model_name} - Fold {fold} Learning Curve',\n                        path=f'{self.model_path}/{self.model_name}_fold{fold}_learning_curve.png'\n                    )\n                    early_stopping = True\n\n    def inference(self, df_train):\n\n        print(f'\\n{\"-\" * 27}\\nRunning Model for Inference\\n{\"-\" * 27}')\n        df_train[f'{self.model_name}_predictions'] = 0\n\n        for fold in sorted(df_train['fold'].unique()):\n\n            _, val_idx = df_train.loc[df_train['fold'] != fold].index, df_train.loc[df_train['fold'] == fold].index\n            val_dataset = Optiver2DDataset(df=df_train.loc[val_idx, :], flip_probability=0.)\n            val_loader = DataLoader(\n                val_dataset,\n                batch_size=self.training_parameters['batch_size'],\n                sampler=SequentialSampler(val_dataset),\n                pin_memory=True,\n                drop_last=False,\n                num_workers=self.training_parameters['num_workers'],\n            )\n\n            set_seed(self.training_parameters['random_state'], deterministic_cudnn=self.training_parameters['deterministic_cudnn'])\n            device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n            model = self.get_model()\n            model.load_state_dict(torch.load(f'{self.model_path}/{self.model_name}_fold{fold}.pt'))\n            model.to(device)\n            model.eval()\n\n            val_predictions = []\n            with torch.no_grad():\n                for stock_id, sequences, target in val_loader:\n                    stock_id, sequences, target = stock_id.to(device), sequences.to(device), target.to(device)\n                    output = model(stock_id, sequences)\n                    output = output.detach().cpu().numpy().squeeze().tolist()\n                    val_predictions += output\n\n            df_train.loc[val_idx, f'{self.model_name}_predictions'] = val_predictions\n            fold_score = rmspe_metric(df_train.loc[val_idx, 'target'], val_predictions)\n            print(f'Fold {fold} - RMSPE: {fold_score:.6}')\n\n            del _, val_idx, val_dataset, val_loader, val_predictions, model\n\n        print(f'{\"-\" * 30}')\n        for stock_id in df_train['stock_id'].unique():\n            df_stock = df_train.loc[df_train['stock_id'] == stock_id, :]\n            stock_oof_score = rmspe_metric(df_stock['target'], df_stock[f'{self.model_name}_predictions'])\n            print(f'Stock {stock_id} - RMSPE: {stock_oof_score:.6}')\n\n        oof_score = rmspe_metric(df_train['target'], df_train[f'{self.model_name}_predictions'])\n        print(f'{\"-\" * 30}\\nOOF RMSPE: {oof_score:.6}\\n{\"-\" * 30}')\n","metadata":{"execution":{"iopub.status.busy":"2021-08-15T12:26:11.355856Z","iopub.execute_input":"2021-08-15T12:26:11.356248Z","iopub.status.idle":"2021-08-15T12:26:11.400555Z","shell.execute_reply.started":"2021-08-15T12:26:11.356212Z","shell.execute_reply":"2021-08-15T12:26:11.399479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cnn1d_parameters = {\n    'model_name': 'cnn1d',\n    'model_path': '.',\n    'model_parameters': {\n        'in_channels': 16,\n    },\n    'training_parameters': {\n        'amp': False,\n        'learning_rate': 0.0005,\n        'betas': (0.9, 0.999),\n        'weight_decay': 0,\n        'epochs': 3,\n        'batch_size': 256,\n        'reduce_lr_patience': 5,\n        'reduce_lr_factor': 0.25,\n        'reduce_lr_min': 0.000001,\n        'early_stopping_patience': 20,\n        'num_workers': 8,\n        'random_state': 42,\n        'deterministic_cudnn': False,\n        'random_state': 42\n    }\n}\n\ntrainer = Trainer(**cnn1d_parameters)\ntrainer.train_and_validate(df_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.inference(df_train)","metadata":{"execution":{"iopub.status.busy":"2021-08-15T12:26:19.401858Z","iopub.execute_input":"2021-08-15T12:26:19.40219Z","iopub.status.idle":"2021-08-15T12:28:08.551911Z","shell.execute_reply.started":"2021-08-15T12:26:19.402158Z","shell.execute_reply":"2021-08-15T12:28:08.549449Z"},"trusted":true},"execution_count":null,"outputs":[]}]}