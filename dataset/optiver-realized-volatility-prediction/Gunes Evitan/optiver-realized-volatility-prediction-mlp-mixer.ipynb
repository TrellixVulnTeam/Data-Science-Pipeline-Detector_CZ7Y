{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from glob import glob\nfrom tqdm.notebook import tqdm\n\nimport numpy as np\nimport pandas as pd\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)\n\nfrom sklearn.preprocessing import LabelEncoder\n\nimport torch\nimport torch.nn as nn\nfrom fastai.layers import SigmoidRange","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-09-19T07:13:21.314093Z","iopub.execute_input":"2021-09-19T07:13:21.314654Z","iopub.status.idle":"2021-09-19T07:13:26.765569Z","shell.execute_reply.started":"2021-09-19T07:13:21.314563Z","shell.execute_reply":"2021-09-19T07:13:26.764762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def rmspe_metric(y_true, y_pred):\n\n    \"\"\"\n    Calculate root mean squared percentage error between ground-truth and predictions\n\n    Parameters\n    ----------\n    y_true [array-like of shape (n_samples)]: Ground-truth\n    y_pred [array-like of shape (n_samples)]: Predictions\n\n    Returns\n    -------\n    rmspe (float): Root mean squared percentage error\n    \"\"\"\n\n    rmspe = np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\n    return rmspe\n\n\ndef evaluate_predictions(predictions_column, evaluate_stock=False):\n    \n    \"\"\"\n    Evaluate predictions of the given model for every individual stock and entire training set\n\n    Parameters\n    ----------\n    df_train [pandas.DataFrame of shape (n_samples)]: DataFrame with stock_id and prediction columns\n    predictions_column (str): Predictions of the model\n    \"\"\"\n    \n    for fold in sorted(df_train['fold'].unique()):\n\n        _, val_idx = df_train.loc[df_train['fold'] != fold].index, df_train.loc[df_train['fold'] == fold].index\n        fold_score = rmspe_metric(df_train.loc[val_idx, 'target'], df_train.loc[val_idx, predictions_column])\n        print(f'Fold {fold} - RMSPE: {fold_score:.6}')\n\n    oof_score = rmspe_metric(df_train['target'], df_train[predictions_column])\n    print(f'{\"-\" * 30}\\nOOF RMSPE: {oof_score:.6}\\n{\"-\" * 30}')\n\n    if evaluate_stock:\n        for stock_id in df_train['stock_id'].unique():\n            df_stock = df_train.loc[df_train['stock_id'] == stock_id, :]\n            stock_oof_score = rmspe_metric(df_stock['target'], df_stock[predictions_column])\n            print(f'Stock {stock_id} - OOF RMSPE: {stock_oof_score:.6}')\n","metadata":{"execution":{"iopub.status.busy":"2021-09-19T07:13:26.767344Z","iopub.execute_input":"2021-09-19T07:13:26.767623Z","iopub.status.idle":"2021-09-19T07:13:26.777158Z","shell.execute_reply.started":"2021-09-19T07:13:26.767574Z","shell.execute_reply":"2021-09-19T07:13:26.776274Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class PreprocessingPipeline:\n\n    def __init__(self, df_train, df_test, split_type):\n\n        self.df_train = df_train.copy(deep=True)\n        self.df_test = df_test.copy(deep=True)\n        self.split_type = split_type\n\n    def _label_encode(self):\n\n        le = LabelEncoder()\n        self.df_train['stock_id_encoded'] = le.fit_transform(self.df_train['stock_id'].values)\n        self.df_test['stock_id_encoded'] = le.transform(self.df_test['stock_id'].values)\n\n    def _get_folds(self):\n\n        df_folds = pd.read_csv('../input/optiver-realized-volatility-dataset/folds.csv')\n        self.df_train['fold'] = df_folds[f'fold_{self.split_type}']\n        \n    def transform(self):\n\n        self._get_folds()\n        self._label_encode()\n\n        return self.df_train, self.df_test\n","metadata":{"execution":{"iopub.status.busy":"2021-09-19T07:13:26.779169Z","iopub.execute_input":"2021-09-19T07:13:26.779466Z","iopub.status.idle":"2021-09-19T07:13:26.790518Z","shell.execute_reply.started":"2021-09-19T07:13:26.779431Z","shell.execute_reply":"2021-09-19T07:13:26.789833Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_test_dtypes = {\n    'stock_id': np.uint8,\n    'time_id': np.uint16,\n    'target': np.float64\n}\n\ndf_train = pd.read_csv('../input/optiver-realized-volatility-prediction/train.csv', dtype=train_test_dtypes)\ndf_test = pd.read_csv('../input/optiver-realized-volatility-prediction/test.csv', usecols=['stock_id', 'time_id'], dtype=train_test_dtypes)\n\n# Using only first row of test set if it is the placeholder\n# Other rows break the pipeline since some of the time buckets don't exist in book data\nif df_test.shape[0] == 3:\n    df_test = df_test.head(1)\n\npreprocessing_pipeline = PreprocessingPipeline(df_train, df_test, 'group')\ndf_train, df_test = preprocessing_pipeline.transform()\n\nprint(f'Training Set Shape: {df_train.shape} - Memory Usage: {df_train.memory_usage().sum() / 1024 ** 2:.2f} MB')\nprint(f'Test Set Shape: {df_test.shape} - Memory Usage: {df_test.memory_usage().sum() / 1024 ** 2:.2f} MB')","metadata":{"execution":{"iopub.status.busy":"2021-09-19T07:13:26.792631Z","iopub.execute_input":"2021-09-19T07:13:26.792996Z","iopub.status.idle":"2021-09-19T07:13:27.171056Z","shell.execute_reply.started":"2021-09-19T07:13:26.792921Z","shell.execute_reply":"2021-09-19T07:13:27.170313Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MLPBlock(nn.Module):\n\n    def __init__(self, input_dim, hidden_dim, dropout_rate):\n\n        super(MLPBlock, self).__init__()\n\n        self.mlp = nn.Sequential(\n            nn.Linear(input_dim, hidden_dim, bias=True),\n            nn.GELU(),\n            nn.Dropout(dropout_rate),\n            nn.Linear(hidden_dim, input_dim, bias=True),\n            nn.Dropout(dropout_rate)\n        )\n\n    def forward(self, x):\n        return self.mlp(x)\n\n\nclass MixerBlock(nn.Module):\n\n    def __init__(self, num_patches, hidden_dim, token_mixer_dim, token_mixer_dropout_rate, channel_mixer_dim, channel_mixer_dropout_rate):\n\n        super(MixerBlock, self).__init__()\n\n        self.layer_norm_token = nn.LayerNorm(hidden_dim)\n        self.token_mixer = MLPBlock(num_patches, token_mixer_dim, token_mixer_dropout_rate)\n        self.layer_norm_channel = nn.LayerNorm(hidden_dim)\n        self.channel_mixer = MLPBlock(hidden_dim, channel_mixer_dim, channel_mixer_dropout_rate)\n\n    def forward(self, x):\n\n        out = self.layer_norm_token(x).transpose(1, 2)\n        x = x + self.token_mixer(out).transpose(1, 2)\n        out = self.layer_norm_channel(x)\n        x = x + self.channel_mixer(out)\n\n        return x\n\n\nclass MLPMixerModel(nn.Module):\n\n    def __init__(self, sequence_length, channels, patch_size, hidden_dim, num_blocks, token_mixer_dim, token_mixer_dropout_rate, channel_mixer_dim, channel_mixer_dropout_rate, use_stock_id, stock_embedding_dims):\n\n        super(MLPMixerModel, self).__init__()\n\n        # Stock embeddings\n        self.use_stock_id = use_stock_id\n        self.stock_embedding_dims = stock_embedding_dims\n        self.stock_embeddings = nn.Embedding(num_embeddings=113, embedding_dim=self.stock_embedding_dims)\n        self.dropout = nn.Dropout(0.25)\n\n        # Patch embeddings\n        num_patches = (sequence_length // patch_size[0]) * (channels // patch_size[1])\n        self.patch_embeddings = nn.Conv2d(1, hidden_dim, kernel_size=patch_size, stride=patch_size, bias=False)\n\n        # Mixer blocks\n        self.mixer = nn.Sequential(\n            *[\n                MixerBlock(\n                    num_patches=num_patches,\n                    hidden_dim=hidden_dim,\n                    token_mixer_dim=token_mixer_dim,\n                    token_mixer_dropout_rate=token_mixer_dropout_rate,\n                    channel_mixer_dim=channel_mixer_dim,\n                    channel_mixer_dropout_rate=channel_mixer_dropout_rate\n                ) for _ in range(num_blocks)\n            ]\n        )\n\n        self.layer_norm = nn.LayerNorm(hidden_dim)\n        self.head = nn.Sequential(\n            nn.Linear(hidden_dim + self.stock_embedding_dims, 1, bias=True),\n            SigmoidRange(0, 0.1)\n        )\n\n    def forward(self, stock_ids, sequences):\n\n        x = sequences.view(-1, 1, sequences.shape[1], sequences.shape[2])\n        x = self.patch_embeddings(x)\n        x = x.flatten(2).transpose(1, 2)\n        x = self.mixer(x)\n        x = self.layer_norm(x)\n        x = x.mean(dim=1)\n\n        if self.use_stock_id:\n            embedded_stock_ids = self.stock_embeddings(stock_ids)\n            x = torch.cat([x, self.dropout(embedded_stock_ids)], dim=1)\n\n        output = self.head(x)\n        return output.view(-1)\n","metadata":{"execution":{"iopub.status.busy":"2021-09-19T07:13:28.333618Z","iopub.execute_input":"2021-09-19T07:13:28.334077Z","iopub.status.idle":"2021-09-19T07:13:28.35438Z","shell.execute_reply.started":"2021-09-19T07:13:28.334046Z","shell.execute_reply":"2021-09-19T07:13:28.353625Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Normalizing sequences with global means and stds\nbook_means = np.array([\n    # Raw sequences\n    0.99969482421875, 1.000321388244629, 0.9995064735412598, 1.0005191564559937,\n    769.990177708821, 766.7345672818379, 959.3416027831918, 928.2202512713748,\n    # Absolute log returns of raw sequences\n    5.05890857311897e-05, 5.1026330766035244e-05, 5.74059049540665e-05, 5.8218309277435765e-05,\n    0.3967152245253066, 0.39100519899866804, 0.3239659116907835, 0.31638538484106116,\n    # Weighted average prices\n    1.0000068043192514, 1.0000055320253616, 1.000006872969592,\n    # Absolute log returns of weighted average prices\n    8.211420490291096e-05, 0.00011112522790786203, 8.236187150264073e-05\n])\nbook_stds = np.array([\n    # Raw sequences\n    0.0036880988627672195, 0.003687119111418724, 0.0037009266670793295, 0.0036990800872445107,\n    5354.051690318169, 4954.947103063445, 6683.816183660414, 5735.299917793827,\n    # Absolute log returns of raw sequences\n    0.00016576898633502424, 0.00016801751917228103, 0.0001837657910073176, 0.0001868011022452265,\n    0.9121719707304721, 0.8988021131995019, 0.8415323589617927, 0.8244750862945265,\n    # Weighted average prices\n    0.003689893218043926, 0.00370745215558702, 0.0036913980961173682,\n    # Absolute log returns of weighted average prices\n    0.00021108155612872302, 0.00029320157822289604, 0.00019975085953727163\n])\ntrade_means = np.array([0.999971866607666, 352.9736760331942, 4.1732040971227145])\ntrade_stds = np.array([0.004607073962688446, 1041.9441951057488, 7.79955795393431])\n\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n\nmlp_mixer_parameters = {\n    'sequence_length': 600,\n    'channels': 25,\n    'patch_size': (30, 1),\n    'hidden_dim': 128,\n    'num_blocks': 2,\n    'token_mixer_dim': 64,\n    'token_mixer_dropout_rate': 0,\n    'channel_mixer_dim': 64,\n    'channel_mixer_dropout_rate': 0,\n    'use_stock_id': True,\n    'stock_embedding_dims': 16\n}\n\nprint(f'MLP Mixer Models\\n{\"-\" * 16}')\nmlp_mixer_models = []\nfor model_path in sorted(glob(f'../input/optiver-realized-volatility-dataset/mlp_mixer/*.pt')):\n    print(f'Loading model {model_path} into memory')\n    model = MLPMixerModel(**mlp_mixer_parameters)\n    model.load_state_dict(torch.load(model_path))\n    model.to(device)\n    model.eval()\n    mlp_mixer_models.append(model)\n    ","metadata":{"execution":{"iopub.status.busy":"2021-09-19T07:13:49.769578Z","iopub.execute_input":"2021-09-19T07:13:49.769827Z","iopub.status.idle":"2021-09-19T07:13:49.841876Z","shell.execute_reply.started":"2021-09-19T07:13:49.769802Z","shell.execute_reply":"2021-09-19T07:13:49.84106Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Loading pre-computed train predictions and evaluate them\ndf_train['mlp_mixer_predictions'] = pd.read_csv('../input/optiver-realized-volatility-dataset/mlp_mixer/mlp_mixer_predictions.csv').values\n\nprint(f'MLP Mixer Scores\\n{\"-\" * 16}')\nevaluate_predictions('mlp_mixer_predictions', evaluate_stock=True)","metadata":{"execution":{"iopub.status.busy":"2021-09-19T07:27:54.537355Z","iopub.execute_input":"2021-09-19T07:27:54.538032Z","iopub.status.idle":"2021-09-19T07:27:54.951839Z","shell.execute_reply.started":"2021-09-19T07:27:54.537996Z","shell.execute_reply":"2021-09-19T07:27:54.951141Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"book_features = ['bid_price1', 'ask_price1', 'bid_price2', 'ask_price2','bid_size1', 'ask_size1', 'bid_size2', 'ask_size2']\ntrade_features = ['price', 'size', 'order_count']\nstock_id_mapping = df_train.set_index('stock_id')['stock_id_encoded'].to_dict()\n\nfor stock_id in tqdm(df_test['stock_id'].unique()):\n    \n    df_stock = df_test.loc[df_test['stock_id'] == stock_id]\n    df_book = pd.read_parquet(f'../input/optiver-realized-volatility-prediction/book_test.parquet/stock_id={stock_id}')\n    df_trade = pd.read_parquet(f'../input/optiver-realized-volatility-prediction/trade_test.parquet/stock_id={stock_id}')\n    stock_time_buckets = df_test.loc[df_test['stock_id'] == stock_id, 'time_id'].reset_index(drop=True)\n    missing_time_buckets = stock_time_buckets[~stock_time_buckets.isin(df_trade['time_id'])]\n    df_trade = df_trade.merge(missing_time_buckets, how='outer')\n    \n    # Iterating over time_ids\n    for time_id in df_stock['time_id'].unique():\n        \n        # Resample order book to 600 seconds, forward fill and back fill for edge cases\n        df_book_time_bucket = df_book.loc[df_book['time_id'] == time_id]\n        df_book_time_bucket = df_book_time_bucket.set_index(['seconds_in_bucket'])\n        df_book_time_bucket = df_book_time_bucket.reindex(np.arange(0, 600), method='ffill').fillna(method='bfill')\n        \n        # Sequences from book data\n        book_sequences = df_book_time_bucket.reset_index(drop=True)[book_features].values\n        \n        # Absolute log returns of raw sequences\n        book_bid_price1_log = np.log(book_sequences[:, 0])\n        book_bid_price1_absolute_log_returns = np.abs(np.diff(book_bid_price1_log, prepend=[book_bid_price1_log[0]]))\n        book_ask_price1_log = np.log(book_sequences[:, 1])\n        book_ask_price1_absolute_log_returns = np.abs(np.diff(book_ask_price1_log, prepend=[book_ask_price1_log[0]]))\n        book_bid_price2_log = np.log(book_sequences[:, 2])\n        book_bid_price2_absolute_log_returns = np.abs(np.diff(book_bid_price2_log, prepend=[book_bid_price2_log[0]]))\n        book_ask_price2_log = np.log(book_sequences[:, 3])\n        book_ask_price2_absolute_log_returns = np.abs(np.diff(book_ask_price2_log, prepend=[book_ask_price2_log[0]]))\n        book_bid_size1_log = np.log(book_sequences[:, 4])\n        book_bid_size1_absolute_log_returns = np.abs(np.diff(book_bid_size1_log, prepend=[book_bid_size1_log[0]]))\n        book_ask_size1_log = np.log(book_sequences[:, 5])\n        book_ask_size1_absolute_log_returns = np.abs(np.diff(book_ask_size1_log, prepend=[book_ask_size1_log[0]]))\n        book_bid_size2_log = np.log(book_sequences[:, 6])\n        book_bid_size2_absolute_log_returns = np.abs(np.diff(book_bid_size2_log, prepend=[book_bid_size2_log[0]]))\n        book_ask_size2_log = np.log(book_sequences[:, 7])\n        book_ask_size2_absolute_log_returns = np.abs(np.diff(book_ask_size2_log, prepend=[book_ask_size2_log[0]]))\n\n        # Weighted average prices\n        book_wap1 = (book_sequences[:, 0] * book_sequences[:, 5] + book_sequences[:, 1] * book_sequences[:, 4]) /\\\n                    (book_sequences[:, 4] + book_sequences[:, 5])\n        book_wap2 = (book_sequences[:, 2] * book_sequences[:, 7] + book_sequences[:, 3] * book_sequences[:, 6]) /\\\n                    (book_sequences[:, 6] + book_sequences[:, 7])\n        book_wap3 = ((book_sequences[:, 0] * book_sequences[:, 5] + book_sequences[:, 1] * book_sequences[:, 4]) +\n                     (book_sequences[:, 2] * book_sequences[:, 7] + book_sequences[:, 3] * book_sequences[:, 6])) /\\\n                    (book_sequences[:, 4] + book_sequences[:, 5] + book_sequences[:, 6] + book_sequences[:, 7])\n\n        # Absolute log returns of weighted average prices\n        book_wap1_log = np.log(book_wap1)\n        book_wap1_absolute_log_returns = np.abs(np.diff(book_wap1_log, prepend=[book_wap1_log[0]]))\n        book_wap2_log = np.log(book_wap2)\n        book_wap2_absolute_log_returns = np.abs(np.diff(book_wap2_log, prepend=[book_wap2_log[0]]))\n        book_wap3_log = np.log(book_wap3)\n        book_wap3_absolute_log_returns = np.abs(np.diff(book_wap3_log, prepend=[book_wap3_log[0]]))\n\n        book_sequences = np.hstack([\n            book_sequences,\n            book_bid_price1_absolute_log_returns.reshape(-1, 1),\n            book_ask_price1_absolute_log_returns.reshape(-1, 1),\n            book_bid_price2_absolute_log_returns.reshape(-1, 1),\n            book_ask_price2_absolute_log_returns.reshape(-1, 1),\n            book_bid_size1_absolute_log_returns.reshape(-1, 1),\n            book_ask_size1_absolute_log_returns.reshape(-1, 1),\n            book_bid_size2_absolute_log_returns.reshape(-1, 1),\n            book_ask_size2_absolute_log_returns.reshape(-1, 1),\n            book_wap1.reshape(-1, 1),\n            book_wap2.reshape(-1, 1),\n            book_wap3.reshape(-1, 1),\n            book_wap1_absolute_log_returns.reshape(-1, 1),\n            book_wap2_absolute_log_returns.reshape(-1, 1),\n            book_wap3_absolute_log_returns.reshape(-1, 1),\n        ])\n        book_sequences = (book_sequences - book_means) / book_stds\n        \n        # Resample trade data to 600 seconds and fill missing values with 0\n        df_trade_time_bucket = df_trade.loc[df_trade['time_id'] == time_id]\n        df_trade_time_bucket = df_trade_time_bucket.set_index(['seconds_in_bucket'])\n        df_trade_time_bucket = df_trade_time_bucket.reindex(np.arange(0, 600)).fillna(0)\n        \n        # Sequences from trade data\n        trade_sequences = df_trade_time_bucket.reset_index(drop=True)[trade_features].values\n        # Not normalizing zero values in trade data\n        trade_sequences[trade_sequences[:, 0] != 0, :] = (trade_sequences[trade_sequences[:, 0] != 0, :] - trade_means) / trade_stds\n        \n        # Concatenate book and trade sequences\n        sequences = np.hstack([book_sequences, trade_sequences])\n        sequences = torch.as_tensor(sequences.reshape(1, 600, 25), dtype=torch.float)\n        sequences = sequences.to(device)\n        \n        stock_id_encoded = torch.as_tensor([stock_id_mapping[stock_id]], dtype=torch.long)\n        stock_id_encoded = stock_id_encoded.to(device)\n        \n        mlp_mixer_prediction = 0\n        for model in mlp_mixer_models:\n            with torch.no_grad():\n                prediction = model(stock_id_encoded, sequences).detach().cpu().numpy()\n                mlp_mixer_prediction += (prediction[0] / 5)\n        df_test.loc[(df_test['stock_id'] == stock_id) & (df_test['time_id'] == time_id), 'mlp_mixer_predictions'] = mlp_mixer_prediction\n","metadata":{"execution":{"iopub.status.busy":"2021-09-19T07:18:52.229565Z","iopub.execute_input":"2021-09-19T07:18:52.230107Z","iopub.status.idle":"2021-09-19T07:18:52.639484Z","shell.execute_reply.started":"2021-09-19T07:18:52.230073Z","shell.execute_reply":"2021-09-19T07:18:52.638821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test['target'] = df_test['mlp_mixer_predictions'].values\ndf_test['row_id'] = df_test['stock_id'].astype(str) + '-' + df_test['time_id'].astype(str)\ndf_test[['row_id', 'target']].to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-09-19T07:22:53.091009Z","iopub.execute_input":"2021-09-19T07:22:53.091698Z","iopub.status.idle":"2021-09-19T07:22:53.103344Z","shell.execute_reply.started":"2021-09-19T07:22:53.091665Z","shell.execute_reply":"2021-09-19T07:22:53.102538Z"},"trusted":true},"execution_count":null,"outputs":[]}]}