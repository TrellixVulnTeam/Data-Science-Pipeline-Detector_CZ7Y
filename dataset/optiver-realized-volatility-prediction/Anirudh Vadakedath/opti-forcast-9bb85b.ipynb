{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install pytorch-tabnet","metadata":{"execution":{"iopub.status.busy":"2021-08-11T10:56:40.679086Z","iopub.execute_input":"2021-08-11T10:56:40.67957Z","iopub.status.idle":"2021-08-11T10:56:49.089797Z","shell.execute_reply.started":"2021-08-11T10:56:40.679524Z","shell.execute_reply":"2021-08-11T10:56:49.088603Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import lightgbm as lgb","metadata":{"execution":{"iopub.status.busy":"2021-08-11T10:56:49.091913Z","iopub.execute_input":"2021-08-11T10:56:49.092307Z","iopub.status.idle":"2021-08-11T10:56:49.098999Z","shell.execute_reply.started":"2021-08-11T10:56:49.092266Z","shell.execute_reply":"2021-08-11T10:56:49.098167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nfrom joblib import Parallel, delayed\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom sklearn.preprocessing import MinMaxScaler\nimport math\nimport os\nfrom sklearn.model_selection import train_test_split\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom scipy import stats\n%matplotlib inline\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom sklearn.preprocessing import LabelEncoder \nimport glob\nfrom sklearn.model_selection import train_test_split, KFold\nfrom tqdm import tqdm\nimport gc\nfrom pytorch_tabnet.tab_model import TabNetRegressor\nfrom pytorch_tabnet.pretraining import TabNetPretrainer","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-11T12:30:16.16065Z","iopub.execute_input":"2021-08-11T12:30:16.161023Z","iopub.status.idle":"2021-08-11T12:30:16.171348Z","shell.execute_reply.started":"2021-08-11T12:30:16.160989Z","shell.execute_reply":"2021-08-11T12:30:16.1704Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def book_preprocessor(file_path):\n    df = pd.read_parquet(file_path)\n    # Calculate Wap\n    df['wap1'] = calc_wap1(df)\n    df['wap2'] = calc_wap2(df)\n    # Calculate log returns\n    df['log_return1'] = df.groupby(['time_id'])['wap1'].apply(log_return)\n    df['log_return2'] = df.groupby(['time_id'])['wap2'].apply(log_return)\n    # Calculate wap balance\n    df['wap_balance'] = abs(df['wap1'] - df['wap2'])\n    # Calculate spread\n    df['price_spread'] = (df['ask_price1'] - df['bid_price1']) / ((df['ask_price1'] + df['bid_price1']) / 2)\n    df['bid_spread'] = df['bid_price1'] - df['bid_price2']\n    df['ask_spread'] = df['ask_price1'] - df['ask_price2']\n    df['total_volume'] = (df['ask_size1'] + df['ask_size2']) + (df['bid_size1'] + df['bid_size2'])\n    df['volume_imbalance'] = abs((df['ask_size1'] + df['ask_size2']) - (df['bid_size1'] + df['bid_size2']))\n    \n    # Dict for aggregations\n    create_feature_dict = {\n        'wap1': [np.sum, np.mean, np.std],\n        'wap2': [np.sum, np.mean, np.std],\n        'log_return1': [np.sum, realized_volatility, np.mean, np.std],\n        'log_return2': [np.sum, realized_volatility, np.mean, np.std],\n        'wap_balance': [np.sum, np.mean, np.std],\n        'price_spread':[np.sum, np.mean, np.std],\n        'bid_spread':[np.sum, np.mean, np.std],\n        'ask_spread':[np.sum, np.mean, np.std],\n        'total_volume':[np.sum, np.mean, np.std],\n        'volume_imbalance':[np.sum, np.mean, np.std]\n    }\n    \n    # Function to get group stats for different windows (seconds in bucket)\n    def get_stats_window(seconds_in_bucket, add_suffix = False):\n        # Group by the window\n        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(create_feature_dict).reset_index()\n        # Rename columns joining suffix\n        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n        # Add a suffix to differentiate windows\n        if add_suffix:\n            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n        return df_feature\n    \n    # Get the stats for different windows\n    df_feature = get_stats_window(seconds_in_bucket = 0, add_suffix = False)\n    df_feature_450 = get_stats_window(seconds_in_bucket = 450, add_suffix = True)\n    df_feature_300 = get_stats_window(seconds_in_bucket = 300, add_suffix = True)\n    df_feature_150 = get_stats_window(seconds_in_bucket = 150, add_suffix = True)\n    \n    # Merge all\n    df_feature = df_feature.merge(df_feature_450, how = 'left', left_on = 'time_id_', right_on = 'time_id__450')\n    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n    df_feature = df_feature.merge(df_feature_150, how = 'left', left_on = 'time_id_', right_on = 'time_id__150')\n    # Drop unnecesary time_ids\n    df_feature.drop(['time_id__450', 'time_id__300', 'time_id__150'], axis = 1, inplace = True)\n    \n    # Create row_id so we can merge\n    stock_id = file_path.split('=')[1]\n    df_feature['row_id'] = df_feature['time_id_'].apply(lambda x: f'{stock_id}-{x}')\n    df_feature.drop(['time_id_'], axis = 1, inplace = True)\n    return df_feature\n\n# Function to preprocess trade data (for each stock id)\ndef trade_preprocessor(file_path):\n    df = pd.read_parquet(file_path)\n    df['log_return'] = df.groupby('time_id')['price'].apply(log_return)\n    \n    # Dict for aggregations\n    create_feature_dict = {\n        'log_return':[realized_volatility],\n        'seconds_in_bucket':[count_unique],\n        'size':[np.sum],\n        'order_count':[np.mean],\n    }\n    \n    # Function to get group stats for different windows (seconds in bucket)\n    def get_stats_window(seconds_in_bucket, add_suffix = False):\n        # Group by the window\n        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(create_feature_dict).reset_index()\n        # Rename columns joining suffix\n        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n        # Add a suffix to differentiate windows\n        if add_suffix:\n            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n        return df_feature\n    \n    # Get the stats for different windows\n    df_feature = get_stats_window(seconds_in_bucket = 0, add_suffix = False)\n    df_feature_450 = get_stats_window(seconds_in_bucket = 450, add_suffix = True)\n    df_feature_300 = get_stats_window(seconds_in_bucket = 300, add_suffix = True)\n    df_feature_150 = get_stats_window(seconds_in_bucket = 150, add_suffix = True)\n\n    # Merge all\n    df_feature = df_feature.merge(df_feature_450, how = 'left', left_on = 'time_id_', right_on = 'time_id__450')\n    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n    df_feature = df_feature.merge(df_feature_150, how = 'left', left_on = 'time_id_', right_on = 'time_id__150')\n    # Drop unnecesary time_ids\n    df_feature.drop(['time_id__450', 'time_id__300', 'time_id__150'], axis = 1, inplace = True)\n    \n    df_feature = df_feature.add_prefix('trade_')\n    stock_id = file_path.split('=')[1]\n    df_feature['row_id'] = df_feature['trade_time_id_'].apply(lambda x:f'{stock_id}-{x}')\n    df_feature.drop(['trade_time_id_'], axis = 1, inplace = True)\n    return df_feature\n\n# Function to get group stats for the stock_id and time_id\ndef get_time_stock(df):\n    # Get realized volatility columns\n    vol_cols = ['log_return1_realized_volatility', 'log_return2_realized_volatility', 'log_return1_realized_volatility_450', 'log_return2_realized_volatility_450', \n                'log_return1_realized_volatility_300', 'log_return2_realized_volatility_300', 'log_return1_realized_volatility_150', 'log_return2_realized_volatility_150', \n                'trade_log_return_realized_volatility', 'trade_log_return_realized_volatility_450', 'trade_log_return_realized_volatility_300', 'trade_log_return_realized_volatility_150']\n\n    # Group by the stock id\n    df_stock_id = df.groupby(['stock_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n    # Rename columns joining suffix\n    df_stock_id.columns = ['_'.join(col) for col in df_stock_id.columns]\n    df_stock_id = df_stock_id.add_suffix('_' + 'stock')\n\n    # Group by the stock id\n    df_time_id = df.groupby(['time_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n    # Rename columns joining suffix\n    df_time_id.columns = ['_'.join(col) for col in df_time_id.columns]\n    df_time_id = df_time_id.add_suffix('_' + 'time')\n    \n    # Merge with original dataframe\n    df = df.merge(df_stock_id, how = 'left', left_on = ['stock_id'], right_on = ['stock_id__stock'])\n    df = df.merge(df_time_id, how = 'left', left_on = ['time_id'], right_on = ['time_id__time'])\n    df.drop(['stock_id__stock', 'time_id__time'], axis = 1, inplace = True)\n    return df","metadata":{"execution":{"iopub.status.busy":"2021-08-11T12:33:03.874041Z","iopub.execute_input":"2021-08-11T12:33:03.874388Z","iopub.status.idle":"2021-08-11T12:33:03.904921Z","shell.execute_reply.started":"2021-08-11T12:33:03.874344Z","shell.execute_reply":"2021-08-11T12:33:03.904032Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def calc_wap1(df):\n    wap = (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']) / (df['bid_size1'] + df['ask_size1'])\n    return wap\n\n# Function to calculate second WAP\ndef calc_wap2(df):\n    wap = (df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2']) / (df['bid_size2'] + df['ask_size2'])\n    return wap\n\n# Function to calculate the log of the return\n# Remember that logb(x / y) = logb(x) - logb(y)\ndef log_return(series):\n    return np.log(series).diff()\n\n# Calculate the realized volatility\ndef realized_volatility(series):\n    return np.sqrt(np.sum(series**2))\n\n# Function to count unique elements of a series\ndef count_unique(series):\n    return len(np.unique(series))\n","metadata":{"execution":{"iopub.status.busy":"2021-08-11T12:33:47.733197Z","iopub.execute_input":"2021-08-11T12:33:47.733547Z","iopub.status.idle":"2021-08-11T12:33:47.741315Z","shell.execute_reply.started":"2021-08-11T12:33:47.733513Z","shell.execute_reply":"2021-08-11T12:33:47.740468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocessor(list_stock_ids, is_train = True):\n    \n    # Parrallel for loop\n    def for_joblib(stock_id):\n        # Train\n        data_dir = '../input/optiver-realized-volatility-prediction/'\n        if is_train:\n            file_path_book = data_dir + \"book_train.parquet/stock_id=\" + str(stock_id)\n            file_path_trade = data_dir + \"trade_train.parquet/stock_id=\" + str(stock_id)\n        # Test\n        else:\n            file_path_book = data_dir + \"book_test.parquet/stock_id=\" + str(stock_id)\n            file_path_trade = data_dir + \"trade_test.parquet/stock_id=\" + str(stock_id)\n    \n        # Preprocess book and trade data and merge them\n        df_tmp = pd.merge(book_preprocessor(file_path_book), trade_preprocessor(file_path_trade), on = 'row_id', how = 'left')\n        \n        # Return the merge dataframe\n        return df_tmp\n    \n    # Use parallel api to call paralle for loop\n    df = Parallel(n_jobs = -1, verbose = 1)(delayed(for_joblib)(stock_id) for stock_id in list_stock_ids)\n    # Concatenate all the dataframes that return from Parallel\n    df = pd.concat(df, ignore_index = True)\n    return df","metadata":{"execution":{"iopub.status.busy":"2021-08-11T12:31:17.021447Z","iopub.execute_input":"2021-08-11T12:31:17.021773Z","iopub.status.idle":"2021-08-11T12:31:17.031275Z","shell.execute_reply.started":"2021-08-11T12:31:17.021744Z","shell.execute_reply":"2021-08-11T12:31:17.030333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def calc_wap(df):\n    wap = (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1'])/(df['bid_size1'] + df['ask_size1'])\n    return wap\n\ndef calc_wap2(df):\n    wap = (df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2'])/(df['bid_size2'] + df['ask_size2'])\n    return wap\n\ndef log_return(list_stock_prices):\n    return np.log(list_stock_prices).diff() \n\ndef realized_volatility(series):\n    return np.sqrt(np.sum(series**2))\n\ndef count_unique(series):\n    return len(np.unique(series))\n\n\ndef preprocessor_book(df):\n    #calculate return etc\n    df['wap'] = calc_wap(df)\n    df['log_return'] = df.groupby('time_id')['wap'].apply(log_return)\n    \n    df['wap2'] = calc_wap2(df)\n    df['log_return2'] = df.groupby('time_id')['wap2'].apply(log_return)\n    \n    df['wap_balance'] = abs(df['wap'] - df['wap2'])\n    \n    df['price_spread'] = (df['ask_price1'] - df['bid_price1']) / ((df['ask_price1'] + df['bid_price1'])/2)\n    df['bid_spread'] = df['bid_price1'] - df['bid_price2']\n    df['ask_spread'] = df['ask_price1'] - df['ask_price2']\n    df['total_volume'] = (df['ask_size1'] + df['ask_size2']) + (df['bid_size1'] + df['bid_size2'])\n    df['volume_imbalance'] = abs((df['ask_size1'] + df['ask_size2']) - (df['bid_size1'] + df['bid_size2']))\n\n    #dict for aggregate\n    create_feature_dict = {\n        'log_return':[realized_volatility],\n        'log_return2':[realized_volatility],\n        'wap_balance':[np.mean],\n        'price_spread':[np.mean],\n        'bid_spread':[np.mean],\n        'ask_spread':[np.mean],\n        'volume_imbalance':[np.mean],\n        'total_volume':[np.mean],\n        'wap':[np.mean],\n            }\n\n    #####groupby / all seconds\n    df_feature = pd.DataFrame(df.groupby(['time_id']).agg(create_feature_dict)).reset_index()\n    \n    df_feature.columns = ['_'.join(col) for col in df_feature.columns] #time_id is changed to time_id_\n        \n    ######groupby / last XX seconds\n    last_seconds = [300]\n    \n    for second in last_seconds:\n        second = 600 - second \n    \n        df_feature_sec = pd.DataFrame(df.query(f'seconds_in_bucket >= {second}').groupby(['time_id']).agg(create_feature_dict)).reset_index()\n\n        df_feature_sec.columns = ['_'.join(col) for col in df_feature_sec.columns] #time_id is changed to time_id_\n     \n        df_feature_sec = df_feature_sec.add_suffix('_' + str(second))\n\n        df_feature = pd.merge(df_feature,df_feature_sec,how='left',left_on='time_id_',right_on=f'time_id__{second}')\n        df_feature = df_feature.drop([f'time_id__{second}'],axis=1)\n    \n    #create row_id\n    stock_id = file_path.split('=')[1]\n    df_feature['row_id'] = df_feature['time_id_'].apply(lambda x:f'{stock_id}-{x}')\n    df_feature = df_feature.drop(['time_id_'],axis=1)\n    \n    return df_feature\n\ndef preprocessor_trade(df):\n    df['log_return'] = df.groupby('time_id')['price'].apply(log_return)\n    \n    \n    aggregate_dictionary = {\n        'log_return':[realized_volatility],\n        'seconds_in_bucket':[count_unique],\n        'size':[np.sum],\n        'order_count':[np.mean],\n    }\n    \n    df_feature = df.groupby('time_id').agg(aggregate_dictionary)\n    \n    df_feature = df_feature.reset_index()\n    df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n\n    \n    ######groupby / last XX seconds\n    last_seconds = [300]\n    \n    for second in last_seconds:\n        second = 600 - second\n    \n        df_feature_sec = df.query(f'seconds_in_bucket >= {second}').groupby('time_id').agg(aggregate_dictionary)\n        df_feature_sec = df_feature_sec.reset_index()\n        \n        df_feature_sec.columns = ['_'.join(col) for col in df_feature_sec.columns]\n        df_feature_sec = df_feature_sec.add_suffix('_' + str(second))\n        \n        df_feature = pd.merge(df_feature,df_feature_sec,how='left',left_on='time_id_',right_on=f'time_id__{second}')\n        df_feature = df_feature.drop([f'time_id__{second}'],axis=1)\n    \n    df_feature = df_feature.add_prefix('trade_')\n    stock_id = file_path.split('=')[1]\n    df_feature['row_id'] = df_feature['trade_time_id_'].apply(lambda x:f'{stock_id}-{x}')\n    df_feature = df_feature.drop(['trade_time_id_'],axis=1)\n    \n    return df_feature","metadata":{"execution":{"iopub.status.busy":"2021-08-11T10:57:23.066865Z","iopub.execute_input":"2021-08-11T10:57:23.067207Z","iopub.status.idle":"2021-08-11T10:57:23.090395Z","shell.execute_reply.started":"2021-08-11T10:57:23.06717Z","shell.execute_reply":"2021-08-11T10:57:23.08924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_pickle(\"../input/optiverlgbbase/train.pkl\")\ntrain","metadata":{"execution":{"iopub.status.busy":"2021-08-11T10:57:23.092458Z","iopub.execute_input":"2021-08-11T10:57:23.092815Z","iopub.status.idle":"2021-08-11T10:57:30.083224Z","shell.execute_reply.started":"2021-08-11T10:57:23.092773Z","shell.execute_reply":"2021-08-11T10:57:30.082389Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y = train['target'].to_numpy()\ntrain = train.drop(columns=['target', 'row_id'])","metadata":{"execution":{"iopub.status.busy":"2021-08-11T10:57:30.08476Z","iopub.execute_input":"2021-08-11T10:57:30.085088Z","iopub.status.idle":"2021-08-11T10:57:30.290505Z","shell.execute_reply.started":"2021-08-11T10:57:30.085052Z","shell.execute_reply":"2021-08-11T10:57:30.28962Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for col in train.columns.to_list()[4:]:\n    train[col] = train[col].fillna(train[col].mean())","metadata":{"execution":{"iopub.status.busy":"2021-08-11T10:57:30.29183Z","iopub.execute_input":"2021-08-11T10:57:30.292239Z","iopub.status.idle":"2021-08-11T10:57:30.785048Z","shell.execute_reply.started":"2021-08-11T10:57:30.292192Z","shell.execute_reply":"2021-08-11T10:57:30.78404Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nle=LabelEncoder()\nle.fit(y)\nY = le.transform(y).reshape(-1,1)","metadata":{"execution":{"iopub.status.busy":"2021-08-11T10:57:30.786574Z","iopub.execute_input":"2021-08-11T10:57:30.78698Z","iopub.status.idle":"2021-08-11T10:57:30.996327Z","shell.execute_reply.started":"2021-08-11T10:57:30.786938Z","shell.execute_reply":"2021-08-11T10:57:30.995397Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nscaler.fit(train)\nX = scaler.transform(train)","metadata":{"execution":{"iopub.status.busy":"2021-08-11T10:57:30.997967Z","iopub.execute_input":"2021-08-11T10:57:30.998411Z","iopub.status.idle":"2021-08-11T10:57:32.34975Z","shell.execute_reply.started":"2021-08-11T10:57:30.998355Z","shell.execute_reply":"2021-08-11T10:57:32.348744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train1, X_test1, y_train, y_test = train_test_split(X,y, test_size=0.25, random_state=42)\nX_train2, X_test2, Y_train, Y_test = train_test_split(X,Y, test_size=0.25, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2021-08-11T10:58:09.234562Z","iopub.execute_input":"2021-08-11T10:58:09.234889Z","iopub.status.idle":"2021-08-11T10:58:11.778593Z","shell.execute_reply.started":"2021-08-11T10:58:09.234857Z","shell.execute_reply":"2021-08-11T10:58:11.77769Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_epochs = 1000 if not os.getenv(\"CI\", False) else 2","metadata":{"execution":{"iopub.status.busy":"2021-08-11T10:58:13.619048Z","iopub.execute_input":"2021-08-11T10:58:13.619387Z","iopub.status.idle":"2021-08-11T10:58:13.625567Z","shell.execute_reply.started":"2021-08-11T10:58:13.619337Z","shell.execute_reply":"2021-08-11T10:58:13.624732Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stat=\"sdv\"","metadata":{"execution":{"iopub.status.busy":"2021-08-11T10:58:15.49523Z","iopub.execute_input":"2021-08-11T10:58:15.495698Z","iopub.status.idle":"2021-08-11T10:58:15.499793Z","shell.execute_reply.started":"2021-08-11T10:58:15.495662Z","shell.execute_reply":"2021-08-11T10:58:15.498725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def rmspe(y_true, y_pred):\n    '''\n    Compute Root Mean Square Percentage Error between two arrays.\n    '''\n    \n    if (y_true == 0).any():\n        raise ValueError(\"Root Mean Square Percentage Error cannot be used when \"\n                         \"targets contain zero values.\")\n        \n    loss = np.sqrt(np.mean(np.square(((y_true - y_pred) / y_true)), axis=0)).item()\n\n    return loss\n\n# def rmspe(y_true, y_pred):\n#     return np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\n\nfrom pytorch_tabnet.metrics import Metric\n\nclass RMSPE(Metric):\n    def __init__(self):\n        self._name = \"rmspe\"\n        self._maximize = False\n\n    def __call__(self, y_true, y_score):\n        return rmspe(y_true, y_score)","metadata":{"execution":{"iopub.status.busy":"2021-08-11T11:00:11.285304Z","iopub.execute_input":"2021-08-11T11:00:11.285743Z","iopub.status.idle":"2021-08-11T11:00:11.29382Z","shell.execute_reply.started":"2021-08-11T11:00:11.285706Z","shell.execute_reply":"2021-08-11T11:00:11.292643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if stat!='load':\n    regressor = TabNetRegressor(verbose=1,\n        n_d = 64,\n        n_a = 64,\n        n_steps = 5,\n        gamma = 1.3,\n        lambda_sparse = 0,\n        optimizer_fn = optim.Adam,\n        optimizer_params = dict(lr = 1e-2, weight_decay = 1e-5),\n        mask_type = \"entmax\",\n        scheduler_params = dict(\n            mode = \"min\", patience = 5, min_lr = 1e-7, factor = 0.2),\n        scheduler_fn = ReduceLROnPlateau,\n        seed = 42,\n        #verbose = 5,\n        cat_dims=[len(le.classes_)], cat_emb_dim=[10], cat_idxs=[-1]\n    )\n#     '''\n#     unsupervised_model = TabNetPretrainer(\n#         optimizer_fn=torch.optim.Adam,\n#         optimizer_params=dict(lr=2e-2),\n#         mask_type='entmax' # \"sparsemax\",\n#     )\n\n#     unsupervised_model.fit(\n#         X_train=X_train,\n#         eval_set=[X_test],\n#         pretraining_ratio=0.8,\n#         max_epochs=max_epochs\n\n#     )\n#     '''\n    regressor.fit(X_train=X, y_train=Y,\n                  eval_set=[(X_test1, Y_test)],\n                  patience=30, max_epochs=max_epochs,\n                  eval_metric=[RMSPE])#from_unsupervised=unsupervised_model)\n    saving_path_name = \"./model\"\n    saved_filepath = regressor.save_model(saving_path_name)\nelse:\n    from shutil import copyfile,make_archive\n    !mkdir ./model\n    copyfile('../input/modelfile/model/model_params.json', \"./model/model_params.json\")\n    copyfile('../input/modelfile/model/network.pt', \"./model/network.pt\")\n    make_archive('./model', 'zip', './model')\n    regressor = TabNetRegressor()\n    regressor.load_model(\"./model.zip\")","metadata":{"execution":{"iopub.status.busy":"2021-08-11T11:02:36.004123Z","iopub.execute_input":"2021-08-11T11:02:36.00448Z","iopub.status.idle":"2021-08-11T11:26:05.745684Z","shell.execute_reply.started":"2021-08-11T11:02:36.004447Z","shell.execute_reply":"2021-08-11T11:26:05.744726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def feval_rmspe(y_pred, lgb_train):\n    y_true = lgb_train.get_label()\n    return 'RMSPE', rmspe(y_true, y_pred), False","metadata":{"execution":{"iopub.status.busy":"2021-08-11T11:26:05.74718Z","iopub.execute_input":"2021-08-11T11:26:05.747533Z","iopub.status.idle":"2021-08-11T11:26:05.752679Z","shell.execute_reply.started":"2021-08-11T11:26:05.747496Z","shell.execute_reply":"2021-08-11T11:26:05.75157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_weights = 1 / np.square(y)\ntest_weights = 1 / np.square(y_test)\n\nparams = {\n      'objective': 'rmse',  \n      'boosting_type': 'gbdt',\n      'num_leaves': 100,\n      'n_jobs': -1,\n      'learning_rate': 0.1,\n      'feature_fraction': 0.8,\n      'bagging_fraction': 0.8,\n      'verbose': -1\n    }\n\ntrain_dataset = lgb.Dataset(X, y, weight = train_weights)#, categorical_feature = ['stock_id'])\ntest_dataset = lgb.Dataset(X_test1, y_test, weight = test_weights)#, categorical_feature = ['stock_id'])\nmodel = lgb.train(params = params, \n                  train_set = train_dataset, \n                  valid_sets = [train_dataset, test_dataset], \n                  num_boost_round = 10000, \n                  early_stopping_rounds = 50, \n                  verbose_eval = 50,\n                  feval = feval_rmspe)","metadata":{"execution":{"iopub.status.busy":"2021-08-11T11:26:05.754734Z","iopub.execute_input":"2021-08-11T11:26:05.755316Z","iopub.status.idle":"2021-08-11T12:21:43.945809Z","shell.execute_reply.started":"2021-08-11T11:26:05.755269Z","shell.execute_reply":"2021-08-11T12:21:43.944867Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.save_model(filename='lgb_model')","metadata":{"execution":{"iopub.status.busy":"2021-08-11T12:21:43.947675Z","iopub.execute_input":"2021-08-11T12:21:43.948027Z","iopub.status.idle":"2021-08-11T12:21:48.614953Z","shell.execute_reply.started":"2021-08-11T12:21:43.947988Z","shell.execute_reply":"2021-08-11T12:21:48.614156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"yy1 = regressor.predict(X_test1).astype(int)\nyy2 = model.predict(X_test2)","metadata":{"execution":{"iopub.status.busy":"2021-08-11T12:21:48.61621Z","iopub.execute_input":"2021-08-11T12:21:48.616586Z","iopub.status.idle":"2021-08-11T12:23:29.266465Z","shell.execute_reply.started":"2021-08-11T12:21:48.616545Z","shell.execute_reply":"2021-08-11T12:23:29.265562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(y_test.shape)\nprint(yy1.shape)\nprint(yy2.shape)","metadata":{"execution":{"iopub.status.busy":"2021-08-11T12:23:29.26777Z","iopub.execute_input":"2021-08-11T12:23:29.268315Z","iopub.status.idle":"2021-08-11T12:23:29.274491Z","shell.execute_reply.started":"2021-08-11T12:23:29.268273Z","shell.execute_reply":"2021-08-11T12:23:29.273488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"yy11 = le.inverse_transform(yy1)","metadata":{"execution":{"iopub.status.busy":"2021-08-11T12:24:24.013111Z","iopub.execute_input":"2021-08-11T12:24:24.013492Z","iopub.status.idle":"2021-08-11T12:24:24.039035Z","shell.execute_reply.started":"2021-08-11T12:24:24.013455Z","shell.execute_reply":"2021-08-11T12:24:24.038099Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rmspe(Y_test, yy1)","metadata":{"execution":{"iopub.status.busy":"2021-08-11T12:25:46.29936Z","iopub.execute_input":"2021-08-11T12:25:46.299733Z","iopub.status.idle":"2021-08-11T12:25:46.30681Z","shell.execute_reply.started":"2021-08-11T12:25:46.2997Z","shell.execute_reply":"2021-08-11T12:25:46.305938Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"yy2","metadata":{"execution":{"iopub.status.busy":"2021-08-11T12:37:24.157548Z","iopub.execute_input":"2021-08-11T12:37:24.157885Z","iopub.status.idle":"2021-08-11T12:37:24.167599Z","shell.execute_reply.started":"2021-08-11T12:37:24.157856Z","shell.execute_reply":"2021-08-11T12:37:24.166705Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rmspe(y_test, yy2)","metadata":{"execution":{"iopub.status.busy":"2021-08-11T12:23:29.275968Z","iopub.execute_input":"2021-08-11T12:23:29.276329Z","iopub.status.idle":"2021-08-11T12:23:29.287204Z","shell.execute_reply.started":"2021-08-11T12:23:29.276291Z","shell.execute_reply":"2021-08-11T12:23:29.286083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"test = test_final_df\ncol = test['row_id']\ntest.drop(columns=['row_id'],inplace=True)\ntest = test.to_numpy()\nypred = regressor.predict(test).astype(int)","metadata":{"execution":{"iopub.status.busy":"2021-08-11T06:59:19.892598Z","iopub.execute_input":"2021-08-11T06:59:19.892926Z","iopub.status.idle":"2021-08-11T06:59:19.924568Z","shell.execute_reply.started":"2021-08-11T06:59:19.892889Z","shell.execute_reply":"2021-08-11T06:59:19.922441Z"},"jupyter":{"outputs_hidden":true}}},{"cell_type":"markdown","source":"### Reading Test Data","metadata":{}},{"cell_type":"code","source":"test = pd.read_csv('../input/optiver-realized-volatility-prediction/test.csv')\ntest['row_id'] = test['stock_id'].astype(str) + '-' + test['time_id'].astype(str)\ntest_stock_ids = test['stock_id'].unique()\ntest_ = preprocessor(test_stock_ids, is_train = False)\ntest = test.merge(test_, on = ['row_id'], how = 'left')\ntest = get_time_stock(test)\n","metadata":{"execution":{"iopub.status.busy":"2021-08-11T12:33:52.692917Z","iopub.execute_input":"2021-08-11T12:33:52.693248Z","iopub.status.idle":"2021-08-11T12:33:53.735499Z","shell.execute_reply.started":"2021-08-11T12:33:52.693215Z","shell.execute_reply":"2021-08-11T12:33:53.73383Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.columns.to_list()[4:]","metadata":{"execution":{"iopub.status.busy":"2021-08-11T12:39:50.861744Z","iopub.execute_input":"2021-08-11T12:39:50.862076Z","iopub.status.idle":"2021-08-11T12:39:50.871419Z","shell.execute_reply.started":"2021-08-11T12:39:50.862045Z","shell.execute_reply":"2021-08-11T12:39:50.870524Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for col in train.columns.to_list():\n    test[col] = test[col].fillna(train[col].mean())","metadata":{"execution":{"iopub.status.busy":"2021-08-11T12:40:08.675794Z","iopub.execute_input":"2021-08-11T12:40:08.676162Z","iopub.status.idle":"2021-08-11T12:40:08.905568Z","shell.execute_reply.started":"2021-08-11T12:40:08.676131Z","shell.execute_reply":"2021-08-11T12:40:08.904578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-11T12:40:14.342206Z","iopub.execute_input":"2021-08-11T12:40:14.342575Z","iopub.status.idle":"2021-08-11T12:40:14.367492Z","shell.execute_reply.started":"2021-08-11T12:40:14.342544Z","shell.execute_reply":"2021-08-11T12:40:14.366331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.shape","metadata":{"execution":{"iopub.status.busy":"2021-08-11T12:41:20.233462Z","iopub.execute_input":"2021-08-11T12:41:20.233844Z","iopub.status.idle":"2021-08-11T12:41:20.239509Z","shell.execute_reply.started":"2021-08-11T12:41:20.233806Z","shell.execute_reply":"2021-08-11T12:41:20.238559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":", 'time_id',\"stock_id\"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_test = test.drop(['row_id'], axis = 1).values\nx_test = scaler.transform(x_test)\nX_testdf = pd.DataFrame(x_test)\n\n# X_testdf[\"stock_id\"]=test[\"stock_id\"]","metadata":{"execution":{"iopub.status.busy":"2021-08-11T12:46:11.218569Z","iopub.execute_input":"2021-08-11T12:46:11.218931Z","iopub.status.idle":"2021-08-11T12:46:11.227055Z","shell.execute_reply.started":"2021-08-11T12:46:11.218897Z","shell.execute_reply":"2021-08-11T12:46:11.225738Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_test = X_testdf.values","metadata":{"execution":{"iopub.status.busy":"2021-08-11T12:46:13.035497Z","iopub.execute_input":"2021-08-11T12:46:13.035845Z","iopub.status.idle":"2021-08-11T12:46:13.040231Z","shell.execute_reply.started":"2021-08-11T12:46:13.035814Z","shell.execute_reply":"2021-08-11T12:46:13.038945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_test.shape","metadata":{"execution":{"iopub.status.busy":"2021-08-11T12:46:19.597886Z","iopub.execute_input":"2021-08-11T12:46:19.598245Z","iopub.status.idle":"2021-08-11T12:46:19.605822Z","shell.execute_reply.started":"2021-08-11T12:46:19.598211Z","shell.execute_reply":"2021-08-11T12:46:19.604569Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Predicting","metadata":{}},{"cell_type":"code","source":"teest = pd.read_csv('../input/testdata/test.csv')","metadata":{"execution":{"iopub.status.busy":"2021-08-11T12:49:19.934856Z","iopub.execute_input":"2021-08-11T12:49:19.935203Z","iopub.status.idle":"2021-08-11T12:49:19.948702Z","shell.execute_reply.started":"2021-08-11T12:49:19.935173Z","shell.execute_reply":"2021-08-11T12:49:19.947853Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"teest.drop(columns=['Unnamed: 0', 'stock_id', 'time_id',\n       'mean_sec_in_bucket_book', 'mean_bid_price1', 'mean_ask_price1',\n       'mean_bid_price2', 'mean_ask_price2', 'mean_bid_size1',\n       'mean_ask_size1', 'mean_bid_size2', 'mean_ask_size2',\n       'max_sec_in_bucket_book', 'max_bid_price1', 'max_ask_price1',\n       'max_bid_price2', 'max_ask_price2', 'max_bid_size1', 'max_ask_size1',\n       'max_bid_size2', 'max_ask_size2', 'min_sec_in_bucket_book',\n       'min_bid_price1', 'min_ask_price1', 'min_bid_price2', 'min_ask_price2',\n       'min_bid_size1', 'min_ask_size1', 'min_bid_size2', 'min_ask_size2',\n       'median_sec_in_bucket_book', 'median_bid_price1', 'median_ask_price1',\n       'median_bid_price2', 'median_ask_price2', 'median_bid_size1',\n       'median_ask_size1', 'median_bid_size2', 'median_ask_size2',\n       'mean_sec_in_bucket_trade', 'mean_price', 'mean_size', 'mean_order',\n       'max_sec_in_bucket_trade', 'max_price', 'max_size', 'max_order',\n       'min_sec_in_bucket_trade', 'min_price', 'min_size', 'min_order',\n       'median_sec_in_bucket_trade', 'median_price', 'median_size',\n       'median_order'], inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-08-11T12:49:31.984551Z","iopub.execute_input":"2021-08-11T12:49:31.984871Z","iopub.status.idle":"2021-08-11T12:49:31.993319Z","shell.execute_reply.started":"2021-08-11T12:49:31.984843Z","shell.execute_reply":"2021-08-11T12:49:31.992492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"teest.columns","metadata":{"execution":{"iopub.status.busy":"2021-08-11T12:48:25.356741Z","iopub.execute_input":"2021-08-11T12:48:25.357062Z","iopub.status.idle":"2021-08-11T12:48:25.363401Z","shell.execute_reply.started":"2021-08-11T12:48:25.357033Z","shell.execute_reply":"2021-08-11T12:48:25.362456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ypred = model.predict(x_test)","metadata":{"execution":{"iopub.status.busy":"2021-08-11T12:48:30.855985Z","iopub.execute_input":"2021-08-11T12:48:30.856408Z","iopub.status.idle":"2021-08-11T12:48:30.876564Z","shell.execute_reply.started":"2021-08-11T12:48:30.856352Z","shell.execute_reply":"2021-08-11T12:48:30.87567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"teest['target'] = pd.DataFrame(ypred)","metadata":{"execution":{"iopub.status.busy":"2021-08-11T12:50:36.40723Z","iopub.execute_input":"2021-08-11T12:50:36.407599Z","iopub.status.idle":"2021-08-11T12:50:36.412457Z","shell.execute_reply.started":"2021-08-11T12:50:36.407566Z","shell.execute_reply":"2021-08-11T12:50:36.41161Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"teest.to_csv('sample_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2021-08-11T12:51:21.548705Z","iopub.execute_input":"2021-08-11T12:51:21.549043Z","iopub.status.idle":"2021-08-11T12:51:21.558108Z","shell.execute_reply.started":"2021-08-11T12:51:21.549012Z","shell.execute_reply":"2021-08-11T12:51:21.557211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### End x-xx-x","metadata":{}},{"cell_type":"code","source":"list(le.inverse_transform(ypred.flatten()))","metadata":{"execution":{"iopub.status.busy":"2021-08-11T04:04:10.655737Z","iopub.execute_input":"2021-08-11T04:04:10.65609Z","iopub.status.idle":"2021-08-11T04:04:10.681644Z","shell.execute_reply.started":"2021-08-11T04:04:10.656052Z","shell.execute_reply":"2021-08-11T04:04:10.680642Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub = pd.DataFrame({'row_id':col.tolist(),'target':list(le.inverse_transform(ypred.flatten()))})","metadata":{"execution":{"iopub.status.busy":"2021-08-11T04:04:19.182981Z","iopub.execute_input":"2021-08-11T04:04:19.183383Z","iopub.status.idle":"2021-08-11T04:04:19.200762Z","shell.execute_reply.started":"2021-08-11T04:04:19.183348Z","shell.execute_reply":"2021-08-11T04:04:19.19981Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub.to_csv('submission.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2021-08-11T04:04:20.479034Z","iopub.execute_input":"2021-08-11T04:04:20.479455Z","iopub.status.idle":"2021-08-11T04:04:20.487781Z","shell.execute_reply.started":"2021-08-11T04:04:20.479421Z","shell.execute_reply":"2021-08-11T04:04:20.486804Z"},"trusted":true},"execution_count":null,"outputs":[]}]}