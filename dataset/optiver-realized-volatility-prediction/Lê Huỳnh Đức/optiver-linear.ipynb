{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import glob\n\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nfrom scipy.stats import kurtosis, skew\nimport os\nfrom multiprocessing import Pool\nfrom sklearn.model_selection import KFold, train_test_split, StratifiedKFold\nfrom sklearn.preprocessing import PolynomialFeatures, MinMaxScaler,QuantileTransformer\nfrom tqdm import tqdm\nimport lightgbm as lgbm\nimport warnings\nfrom numpy.random import seed\nfrom numpy import matlib\nseed(42)\nimport tensorflow as tf\ntf.random.set_seed(42)\nfrom tensorflow import keras\nfrom keras import backend as K","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-19T06:20:00.068088Z","iopub.execute_input":"2021-08-19T06:20:00.068417Z","iopub.status.idle":"2021-08-19T06:20:06.574875Z","shell.execute_reply.started":"2021-08-19T06:20:00.068389Z","shell.execute_reply":"2021-08-19T06:20:06.573886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"N_JOBS = 4 # os.cpu_count()\nDATA_DIR = '/kaggle/input/optiver-realized-volatility-prediction'\nNUM_GROUPS = 14","metadata":{"execution":{"iopub.status.busy":"2021-08-19T06:20:06.577424Z","iopub.execute_input":"2021-08-19T06:20:06.577724Z","iopub.status.idle":"2021-08-19T06:20:06.58185Z","shell.execute_reply.started":"2021-08-19T06:20:06.577696Z","shell.execute_reply":"2021-08-19T06:20:06.580875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\n\n\ndef rmspe(y_true, y_pred):\n    return (np.sqrt(np.mean(np.square((y_true - y_pred) / y_true))))\n\n\ndef log_return(list_stock_prices):\n    return np.log(list_stock_prices).diff()\n\n\ndef realized_volatility(series_log_return):\n    return np.sqrt(np.sum(series_log_return ** 2))\n\n\ndef realized_volatility_log_return(list_stock_prices):\n    series_log_return = np.log(list_stock_prices).diff()\n    return np.sqrt(np.sum(series_log_return ** 2))\n\n\ndef chunker(seq, size):\n    return [seq[pos:pos + size] for pos in range(0, len(seq), size)]","metadata":{"execution":{"iopub.status.busy":"2021-08-19T06:20:06.583998Z","iopub.execute_input":"2021-08-19T06:20:06.584427Z","iopub.status.idle":"2021-08-19T06:20:06.593431Z","shell.execute_reply.started":"2021-08-19T06:20:06.584387Z","shell.execute_reply":"2021-08-19T06:20:06.592407Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def count_unique(series):\n    return len(np.unique(series))\n\n\nclass FeatureExtraction:\n    def __init__(self):\n        pass\n\n    def calc_wap(self, df):\n        wap = (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']) / (\n                    df['bid_size1'] + df['ask_size1'])\n        return wap\n\n    def calc_wap2(self, df):\n        wap = (df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2']) / (\n                    df['bid_size2'] + df['ask_size2'])\n        return wap\n\n    def calc_wap3(self, df):\n        wap = (df['bid_price2'] * df['bid_size2'] + df['ask_price2'] * df['ask_size2']) / (\n                    df['bid_size2'] + df['ask_size2'])\n        return wap\n\n    def target_encoding(self):\n        pass\n\n    def trade_stock_stat(self, path):\n        df = pd.read_parquet(path)\n        df['log_return'] = df.groupby('time_id')['price'].apply(log_return)\n        aggregate_dictionary = {\n            'log_return': [realized_volatility, \"std\"],\n            'seconds_in_bucket': [count_unique],\n            'size': [\"sum\"],\n            'order_count': [\"sum\", \"mean\"]\n            \n        }\n\n        df_feature = df.groupby('time_id').agg(aggregate_dictionary)\n\n        df_feature = df_feature.reset_index()\n        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n\n        ######groupby / last XX seconds\n        last_seconds = [150, 300, 450]\n\n        for second in last_seconds:\n            second = 600 - second\n\n            df_feature_sec = df.query(f'seconds_in_bucket >= {second}').groupby('time_id').agg(aggregate_dictionary)\n            df_feature_sec = df_feature_sec.reset_index()\n\n            df_feature_sec.columns = ['_'.join(col) for col in df_feature_sec.columns]\n            df_feature_sec = df_feature_sec.add_suffix('_' + str(second))\n\n            df_feature = pd.merge(df_feature, df_feature_sec, how='left', left_on='time_id_',\n                                  right_on=f'time_id__{second}')\n            df_feature = df_feature.drop([f'time_id__{second}'], axis=1)\n\n        df_feature = df_feature.add_prefix('trade_')\n        stock_id = int(path.split('=')[1])\n        df_feature['stock_id'] = int(stock_id)\n        # df_feature['row_id'] = df_feature['trade_time_id_'].apply(lambda x: f'{stock_id}-{x}')\n        # df_feature = df_feature.drop(['trade_time_id_'], axis=1)\n        df_feature = df_feature.rename(columns={'trade_time_id_': 'time_id'})\n#         df_feature['trade_price_open'] = df.groupby('time_id')['price'].head(1).values\n#         df_feature['trade_price_close'] = df.groupby('time_id')['price'].tail(1).values\n#         df_feature['trade_price_gap'] = df_feature['trade_price_close'] - df_feature['trade_price_open']\n\n        return df_feature\n\n    def get_stock_stat(self, path):\n        df = pd.read_parquet(path)\n        # calculate return etc\n        df['wap'] = self.calc_wap(df)\n        df['wap2'] = self.calc_wap2(df)\n        # df['wap3'] = self.calc_wap3(df)\n        # df['wap3'] = (df['wap'] + df['wap2'])/2\n\n        df['log_return'] = df.groupby('time_id')['wap'].apply(log_return)\n        df['log_return2'] = df.groupby('time_id')['wap2'].apply(log_return)\n        # df['log_return3'] = df.groupby('time_id')['wap3'].apply(log_return)\n\n        df['wap_balance'] = abs(df['wap'] - df['wap2'])\n\n        df['price_spread'] = (df['ask_price1'] - df['bid_price1']) / (df['ask_price1'] + df['bid_price1'])\n        df['bid_spread'] = df['bid_price1'] - df['bid_price2']\n        df['ask_spread'] = df['ask_price1'] - df['ask_price2']\n        df['total_volume'] = (df['ask_size1'] + df['ask_size2']) + (df['bid_size1'] + df['bid_size2'])\n        df['volume_imbalance'] = abs((df['ask_size1'] + df['ask_size2']) - (df['bid_size1'] + df['bid_size2']))\n\n        df['bid_ask_spread'] = df['ask_price1'] / df['bid_price1']\n#         df['demand'] = df['ask_size1'] * df['ask_price1']\n#         df['provide'] = df['bid_size1'] * df['bid_price1']\n#         df['a/b'] = df['ask_price1'] / df['bid_size1']\n#         df['b/a'] = df['bid_price1'] / df['ask_size1']\n#         df['demand/provide'] = df['demand'] / df['provide']\n        # dict for aggregate\n        create_feature_dict = {\n            'log_return': [realized_volatility, np.std],\n            'log_return2': [realized_volatility],\n#             'seconds_in_bucket': [count_unique],\n            # 'log_return3': [realized_volatility],\n            'wap_balance': [np.mean, np.std],\n            'price_spread': [np.mean, np.std],\n            'bid_spread': [np.mean, np.std],\n            'ask_spread': [np.mean, np.std],\n            'volume_imbalance': [np.mean, np.std],\n            'total_volume': [np.mean, np.std],\n            'wap': [np.mean, np.std],\n            'bid_ask_spread': [np.mean, np.std],\n#             'demand': [np.mean, np.std],\n#             'provide': [np.mean, np.std],\n#             'demand/provide': [np.mean, np.std]\n        }\n\n        #####groupby / all seconds\n        df_feature = pd.DataFrame(df.groupby(['time_id']).agg(create_feature_dict)).reset_index()\n\n        df_feature.columns = ['_'.join(col) for col in df_feature.columns]  # time_id is changed to time_id_\n\n        ######groupby / last XX seconds\n        last_seconds = [150, 300, 450]\n\n        for second in last_seconds:\n            second = 600 - second\n\n            df_feature_sec = pd.DataFrame(\n                df.query(f'seconds_in_bucket >= {second}').groupby(['time_id']).agg(create_feature_dict)).reset_index()\n\n            df_feature_sec.columns = ['_'.join(col) for col in df_feature_sec.columns]  # time_id is changed to time_id_\n\n            df_feature_sec = df_feature_sec.add_suffix('_' + str(second))\n\n            df_feature = pd.merge(df_feature, df_feature_sec, how='left', left_on='time_id_',\n                                  right_on=f'time_id__{second}')\n            df_feature = df_feature.drop([f'time_id__{second}'], axis=1)\n\n        # create row_id\n        stock_id = path.split('=')[1]\n        df_feature['stock_id'] = int(stock_id)\n        # df_feature['row_id'] = df_feature['time_id_'].apply(lambda x: f'{stock_id}-{x}')\n        df_feature = df_feature.rename(columns={'time_id_': 'time_id'})\n#         df_feature['bid_price1_open'] = df.groupby('time_id')['bid_price1'].head(1).values\n#         df_feature['bid_price1_close'] = df.groupby('time_id')['bid_price1'].tail(1).values\n#         df_feature['book_bid_price_gap'] = df_feature['bid_price1_close'] - df_feature['bid_price1_open']\n#         df_feature['ask_price1_open'] = df.groupby('time_id')['ask_price1'].head(1).values\n#         df_feature['ask_price1_close'] = df.groupby('time_id')['ask_price1'].tail(1).values\n#         df_feature['book_ask_price_gap'] = df_feature['ask_price1_open'] - df_feature['ask_price1_close']\n        return df_feature\n\n    def get_stock_groups(self, df, group_number=NUM_GROUPS):\n        stock_groups = df.groupby(\"stock_id\").mean()[\"target\"].reset_index().sort_values(by=\"target\")\n        stock_groups = chunker(list(stock_groups[\"stock_id\"]), group_number)\n        return stock_groups\n\n\nfeature_extraction = FeatureExtraction()\n\n\ndef get_overall(book):\n    # total_df = feature_extraction.get_stock_stat(book[0])\n    # print(total_df.columns)\n    # exit()\n    with Pool(N_JOBS) as pool:\n        total_df = pool.map(feature_extraction.get_stock_stat, book)\n    total_df = pd.concat(total_df, axis=0)\n#     ic(total_df.shape)\n    return total_df\n\n\ndef get_trade_overall(book):\n    # total_df = feature_extraction.trade_stock_stat(book[0])\n    # print(total_df.columns)\n    # exit()\n    with Pool(N_JOBS) as pool:\n        total_df = pool.map(feature_extraction.trade_stock_stat, book)\n    total_df = pd.concat(total_df, axis=0)\n#     ic(total_df.shape)\n    return total_df\n\n\ndef merge_frames(dataset='train'):\n    order_book = sorted(glob.glob(f'{DATA_DIR}/book_{dataset}.parquet/*'))\n    total_df = get_overall(order_book)\n    trade_df = sorted(glob.glob(f'{DATA_DIR}/trade_{dataset}.parquet/*'))\n    total_trade = get_trade_overall(trade_df)\n    total_df = total_df.merge(total_trade, on=[\"stock_id\", \"time_id\"], how=\"left\")\n#     total_df['trade/book'] = total_df['trade_seconds_in_bucket_count_unique']/ total_df['seconds_in_bucket_count_unique']\n#     ic(total_df.shape)\n    return total_df\n\n\ndef get_time_stock(df, mapping_group):\n    # Get realized volatility columns\n    vol_cols = [col for col in df.columns if \"return\" in col or 'provide' in col or 'seconds_in_bucket_count_unique' == col or '/' in col] \n    # vol_cols = ['log_return_realized_volatility', 'log_return2_realized_volatility',\n    #             'log_return_realized_volatility_300', 'log_return2_realized_volatility_300',\n    #             'trade_log_return_realized_volatility', 'trade_log_return_realized_volatility_300']\n    # # Group by the stock id\n    # df_stock_id = df.groupby(['stock_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n    # # Rename columns joining suffix\n    # df_stock_id.columns = ['_'.join(col) for col in df_stock_id.columns]\n    # df_stock_id = df_stock_id.add_suffix('_' + 'by_stock')\n    #\n    # Group by the time id\n    df_time_id = df.groupby(['time_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n    # Rename columns joining suffix\n    df_time_id.columns = ['_'.join(col) for col in df_time_id.columns]\n    df_time_id = df_time_id.add_suffix('_' + 'by_time')\n\n    # Merge with original dataframe\n    # df = df.merge(df_stock_id, how='left', left_on=['stock_id'], right_on=['stock_id__by_stock'])\n    df = df.merge(df_time_id, how='left', left_on=['time_id'], right_on=['time_id__by_time'])\n\n    df['group'] = df['stock_id'].map(mapping_group)\n    df['group_time'] = df[['group', 'time_id']].apply(lambda x: f\"{x[0]}_{x[1]}\", axis=1)\n    # group_feature_columns = ['log_return_realized_volatility',\n    #                          'log_return_std',\n    #                          'log_return_realized_volatility_300',\n    #                          'log_return_std_300',\n    #                          'price_spread_mean',\n    #                          'trade_log_return_realized_volatility',\n    #                          'trade_log_return_std',\n    #                          'trade_log_return_realized_volatility_300'\n    #                          ]\n    for col in vol_cols:\n        df[f'group_{col}_mean'] = df.groupby(['group_time'])[col].transform(np.mean)\n        df[f'group_{col}_std'] = df.groupby(['group_time'])[col].transform(np.std)\n        df[f'group_{col}_max'] = df.groupby(['group_time'])[col].transform(np.max)\n        df[f'group_{col}_min'] = df.groupby(['group_time'])[col].transform(np.min)\n    df.drop(columns=['group', 'group_time', 'time_id__by_time'], inplace=True)\n    return df\n\n\ndef combined(dataset, mapping_group=None):\n    print(f\"Loading & extract features for {dataset}\")\n    df = pd.read_csv(f'{DATA_DIR}/{dataset}.csv')\n#     ic(df.shape)\n    book_trade_df = merge_frames(dataset=dataset)\n    df = df.merge(book_trade_df, on=[\"stock_id\", \"time_id\"], how=\"left\")\n    if dataset == \"train\":\n        stock_groups = feature_extraction.get_stock_groups(df)\n        mapping_group = {g: i for i, gs in enumerate(stock_groups) for g in gs}\n\n    df = get_time_stock(df, mapping_group)\n    df.fillna(method='ffill', inplace=True)\n#     ic(df.head())\n\n\n    return df, mapping_group\n\n\ndata_train, mapping_group = combined(dataset=\"train\")\n# feature_list = list(data_train.drop(columns=[\"stock_id\", \"target\"]).columns)\ndata_test, _ = combined(dataset=\"test\", mapping_group=mapping_group)","metadata":{"execution":{"iopub.status.busy":"2021-08-19T06:20:06.595481Z","iopub.execute_input":"2021-08-19T06:20:06.595916Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stock_ids = data_train['stock_id'].unique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" data_train[data_train.stock_id ==1].reset_index(drop=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"models = {}\nfrom sklearn.linear_model import Ridge, LinearRegression\nkfold = KFold(n_splits=5, random_state=42, shuffle=True)\nfor stock_id in stock_ids:\n    print(f'stock_id: {stock_id}')\n    cv_models = []\n    tiny_data =  data_train[data_train.stock_id ==stock_id].reset_index(drop=True)\n    X = tiny_data.drop(columns=['stock_id','time_id','target'])\n    y = tiny_data['target'].values\n    for fold, (train_index, valid_index) in enumerate(kfold.split(X)):\n        print(f'fold : {fold+1}')\n        X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n        y_train, y_valid = y[train_index], y[valid_index]\n        train_weights = 1 / np.square(y_train)\n        valid_weights = 1 / np.square(y_valid)\n        model = Ridge()\n        model.fit(X_train, y_train, sample_weight=train_weights)\n        y_pred = model.predict(X_valid)\n        print('RMSPE: ', rmspe(y_valid, y_pred))\n        cv_models.append(model)\n    models[stock_id] = cv_models","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_stock_ids = data_test['stock_id'].unique()\ndata_test.fillna(0, inplace=True)\ndata_test['target'] = 0\ntarget = []\nfor stock_id in test_stock_ids:\n    tiny_data =  data_test[data_test.stock_id ==stock_id].copy()\n    X_test = tiny_data.drop(columns=['stock_id','time_id','row_id','target'])\n    y_preds = []\n    for model in models[stock_id]:\n        y_pred = model.predict(X_test)\n        y_preds.append(y_pred)\n    y_preds = np.mean(y_preds,0)\n    data_test.loc[data_test.stock_id==stock_id,'target'] = y_preds","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_test[['row_id', 'target']].to_csv(\"submission.csv\", index=False)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}