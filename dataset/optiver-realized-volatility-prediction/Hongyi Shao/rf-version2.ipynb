{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans\n\nimport joblib\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\nimport glob","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-09-26T23:44:32.272798Z","iopub.execute_input":"2021-09-26T23:44:32.273138Z","iopub.status.idle":"2021-09-26T23:44:33.386628Z","shell.execute_reply.started":"2021-09-26T23:44:32.273053Z","shell.execute_reply":"2021-09-26T23:44:33.385873Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.tree import export_graphviz\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV","metadata":{"execution":{"iopub.status.busy":"2021-09-26T23:44:33.388058Z","iopub.execute_input":"2021-09-26T23:44:33.388286Z","iopub.status.idle":"2021-09-26T23:44:33.392327Z","shell.execute_reply.started":"2021-09-26T23:44:33.388262Z","shell.execute_reply":"2021-09-26T23:44:33.39176Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def log_return(stock_price):\n    \"\"\"\n    stock price: a series of stock price from groupby function\n    \"\"\"\n    return np.log(stock_price).diff()\n\ndef realized_vol(stock_log_return):\n    \"\"\"\n    log_return: the return of function log_return\n    return: the vol of ten mins\n    \"\"\"\n    return np.sqrt(np.sum(stock_log_return[1:]**2))\n\ndef realized_vol_seconds(stock_log_return,seconds):\n    \"\"\"\n    seconds: parameter that controls the target mins\n    Designed for feature engineering, the volatility of the last several mins\n    \"\"\"\n    return np.sqrt(np.sum(stock_log_return[stock_log_return.seconds_in_bucket >= seconds]['log_return']**2))\n\ndef book_train_feature(book_train_0):\n    book_train_0['wap1'] = (book_train_0['bid_price1'] * book_train_0['ask_size1']+book_train_0['ask_price1']*book_train_0['bid_size1'])/(book_train_0['bid_size1']+book_train_0['ask_size1'])\n    book_train_0['wap2'] = (book_train_0['bid_price2'] * book_train_0['ask_size2']+book_train_0['ask_price2']*book_train_0['bid_size2'])/(book_train_0['bid_size2']+book_train_0['ask_size2'])\n    book_train_0['wap_spread'] = book_train_0['wap1'] - book_train_0['wap2']\n\n    book_train_0['bidask_spread'] = (book_train_0['ask_price1'] - book_train_0['bid_price1']) / book_train_0['bid_price1']\n    book_train_0['bidask_spread2'] = (book_train_0['ask_price2'] - book_train_0['bid_price2']) / book_train_0['bid_price2']\n    # wap在bid和ask之间的范围\n    book_train_0['wap_bid_ask_position'] = abs(abs((book_train_0['ask_price2']+book_train_0['ask_price1'])/2-book_train_0['wap1']) - abs((book_train_0['bid_price2']+book_train_0['bid_price1'])/2-book_train_0['wap1']))\n\n    book_train_0['total_ask_size'] = book_train_0['ask_size1'] + book_train_0['ask_size2']\n    book_train_0['total_bid_size'] = book_train_0['bid_size1'] + book_train_0['bid_size2']\n    book_train_0['total_ask_amount'] = book_train_0['ask_price1'] * book_train_0['ask_size1'] + book_train_0['ask_price2'] * book_train_0['ask_size2']\n    book_train_0['total_bid_amount'] = book_train_0['bid_price1'] * book_train_0['bid_size1'] + book_train_0['bid_price2'] * book_train_0['bid_size2']\n    book_train_0['size_imbalance'] = book_train_0['total_ask_size'] / book_train_0['total_bid_size'] - 1\n    book_train_0['amount_imbalance'] = book_train_0['total_ask_amount'] / book_train_0['total_bid_amount'] - 1\n    book_train_0['log_return'] = book_train_0.groupby('time_id')['wap1'].apply(log_return).fillna(0)\n    book_train_0['log_return_wap2'] = book_train_0.groupby('time_id')['wap2'].apply(log_return).fillna(0)\n\n    # volatility feature\n    list_vol_feature_book = []\n    book_train_0_groupby_timeid = book_train_0.groupby('time_id')\n    for time_id, book_train_time_id in book_train_0_groupby_timeid:\n        realized_vol = realized_vol_seconds(book_train_time_id,seconds=0)\n        realized_vol_seconds_300 = realized_vol_seconds(book_train_time_id,seconds=300)\n        realized_vol_seconds_480 = realized_vol_seconds(book_train_time_id,seconds=480) \n        realized_vol_seconds_540 = realized_vol_seconds(book_train_time_id,seconds=540)\n\n        book_train_time_id_480 = book_train_time_id[book_train_time_id.seconds_in_bucket>=480]\n        book_train_time_id_540 = book_train_time_id[book_train_time_id.seconds_in_bucket>=540]\n\n        bidask_spread_range = book_train_time_id['bidask_spread'].quantile(0.75)- book_train_time_id['bidask_spread'].quantile(0.25)\n        bidask_spread_cv = book_train_time_id['bidask_spread'].std()/book_train_time_id['bidask_spread'].mean()\n        bidask_spread_range2 = book_train_time_id['bidask_spread2'].quantile(0.75)- book_train_time_id['bidask_spread'].quantile(0.25)\n\n        size_imbalance_range = book_train_time_id['size_imbalance'].quantile(0.75) - book_train_time_id['size_imbalance'].quantile(0.25)\n        amount_imbalance_range = book_train_time_id['amount_imbalance'].quantile(0.75) - book_train_time_id['amount_imbalance'].quantile(0.25)\n        wap_range = book_train_time_id['wap1'].max()/book_train_time_id['wap1'].min()\n        wap_bid_ask_imbalance = book_train_time_id['wap_bid_ask_position'].sum()\n        wap_bid_ask_imbalance_last_480 =  book_train_time_id_480['wap_bid_ask_position'].sum()\n        wap_bid_ask_imbalance_last_540 =  book_train_time_id_540['wap_bid_ask_position'].sum()\n        wap_bid_ask_imbalance_range = book_train_time_id['wap_bid_ask_position'].quantile(0.75) -book_train_time_id['wap_bid_ask_position'].quantile(0.25) \n\n        list_vol_feature_book.append([time_id,realized_vol,realized_vol_seconds_300,realized_vol_seconds_480,realized_vol_seconds_540,bidask_spread_range,bidask_spread_range2,wap_bid_ask_imbalance_range,\n        bidask_spread_cv,size_imbalance_range,amount_imbalance_range,wap_range,wap_bid_ask_imbalance,wap_bid_ask_imbalance_last_480,wap_bid_ask_imbalance_last_540])\n\n    volatility_feature_book = pd.DataFrame(list_vol_feature_book,columns=['time_id','realized_vol','realized_vol_seconds_300','realized_vol_seconds_480','realized_vol_seconds_540','bidask_spread_range','bidask_spread_range2','wap_bid_ask_imbalance_range',\n        'bidask_spread_cv','size_imbalance_range','amount_imbalance_range','wap_range','wap_bid_ask_imbalance','wap_bid_ask_imbalance_last_480','wap_bid_ask_imbalance_last_540'])\n    return volatility_feature_book","metadata":{"execution":{"iopub.status.busy":"2021-09-26T23:44:33.435256Z","iopub.execute_input":"2021-09-26T23:44:33.435783Z","iopub.status.idle":"2021-09-26T23:44:33.460842Z","shell.execute_reply.started":"2021-09-26T23:44:33.435744Z","shell.execute_reply":"2021-09-26T23:44:33.459563Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def price_cv_seconds(stock_series,seconds):\n    return stock_series[stock_series.seconds_in_bucket >= seconds]\n\ndef trade_train_feature(trade_train_0):\n    trade_train_0['amount'] = trade_train_0['size'] * trade_train_0['price']\n    trade_train_0['avg_order_amount'] = trade_train_0['amount'] / trade_train_0['order_count']\n\n\n    trade_train_groupby_timeid = trade_train_0.groupby('time_id')\n    # feature extraction in trade_train\n    list_vol_feature_trade = []\n    for time_id,trade_train_time_id in trade_train_groupby_timeid:\n\n        trade_train_time_id_300 = price_cv_seconds(trade_train_time_id,300)\n        trade_train_time_id_480 = price_cv_seconds(trade_train_time_id,480)\n        trade_train_time_id_540 = price_cv_seconds(trade_train_time_id,540)\n        \n        total_trade_order = trade_train_time_id['order_count'].sum()\n        total_trade_order_300 = trade_train_time_id_300['order_count'].sum()\n        total_trade_order_480 = trade_train_time_id_480['order_count'].sum()\n        total_trade_order_540 = trade_train_time_id_540['order_count'].sum()\n\n        price_range = trade_train_time_id['price'].max()/trade_train_time_id['price'].min()\n        price_std = trade_train_time_id['price'].std()\n        price_std_300 = trade_train_time_id_300['price'].std()\n        price_std_480 = trade_train_time_id_480['price'].std()\n        price_std_540 = trade_train_time_id_540['price'].std()\n\n        price_cv = trade_train_time_id['price'].std()/trade_train_time_id['price'].mean()\n        price_cv_300 = trade_train_time_id_300['price'].std()/trade_train_time_id_300['price'].mean()\n        price_cv_480 = trade_train_time_id_480['price'].std()/trade_train_time_id_480['price'].mean()\n        price_cv_540 = trade_train_time_id_540['price'].std()/trade_train_time_id_540['price'].mean()\n\n        size_cv = trade_train_time_id['size'].std()/trade_train_time_id['size'].mean()\n        size_cv_300 = trade_train_time_id_300['size'].std()/trade_train_time_id_300['size'].mean()\n        size_cv_540 = trade_train_time_id_540['size'].std()/trade_train_time_id_540['size'].mean()\n\n        list_vol_feature_trade.append([time_id,total_trade_order, total_trade_order_300,total_trade_order_480,total_trade_order_540,price_range,price_std,price_std_480,price_std_540,price_std_300,\n        price_cv,price_cv_300,price_cv_480,price_cv_540,size_cv,size_cv_300,size_cv_540])\n\n    volatility_feature_trade = pd.DataFrame(list_vol_feature_trade,columns=['time_id','total_trade_order', 'total_trade_order_300','total_trade_order_480','total_trade_order_540','price_range','price_std','price_std_480','price_std_540','price_std_300',\n    'price_cv','price_cv_300','price_cv_480','price_cv_540','size_cv','size_cv_300','size_cv_54'])\n    return volatility_feature_trade","metadata":{"execution":{"iopub.status.busy":"2021-09-26T23:44:35.605296Z","iopub.execute_input":"2021-09-26T23:44:35.606138Z","iopub.status.idle":"2021-09-26T23:44:35.620107Z","shell.execute_reply.started":"2021-09-26T23:44:35.606098Z","shell.execute_reply":"2021-09-26T23:44:35.619219Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# data_dir = '../input/optiver-realized-volatility-prediction/book_train.parquet'\n# stock_list = sorted([int(_.split('=')[1]) for _ in os.listdir(data_dir)])\n# target_df = pd.read_csv(\"../input/optiver-realized-volatility-prediction/train.csv\")\n# train_df = pd.DataFrame()\n# for stock_id in stock_list:\n#     print(stock_id)\n#     # 对两个数据集生成特征，每一个id对应的target进行合成\n#     book_train_0 = pd.read_parquet('../input/optiver-realized-volatility-prediction/book_train.parquet/'+f'stock_id={stock_id}')\n#     trade_train_0 = pd.read_parquet('../input/optiver-realized-volatility-prediction/trade_train.parquet/'+f'stock_id={stock_id}')\n#     target_df_stock_0 = target_df[target_df.stock_id==stock_id][['time_id','target']]\n#     # 合并特征\n#     volatility_feature = trade_train_feature(trade_train_0).merge(book_train_feature(book_train_0),on='time_id')\n#     volatility_feature = volatility_feature.fillna(method = 'ffill')\n#     volatility_feature = volatility_feature.fillna(0)    \n# #     volatility_feature_1 = pd.DataFrame(StandardScaler().fit_transform(volatility_feature.iloc[:,1:]),columns=volatility_feature.iloc[:,1:].columns)\n# #     volatility_feature_1 = \n#     volatility_feature['stock_id'] = stock_id\n# #     volatility_feature_1['time_id'] = volatility_feature['time_id']\n#     train_feature_temp = volatility_feature.merge(target_df_stock_0,on='time_id')\n#     # 把所有id的特征合并\n#     train_df = pd.concat([train_df,train_feature_temp])\n# # train_df = pd.read_csv('../input/train-data/train_df.csv')\n# train_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-26T23:44:37.082326Z","iopub.execute_input":"2021-09-26T23:44:37.082596Z","iopub.status.idle":"2021-09-26T23:44:37.087247Z","shell.execute_reply.started":"2021-09-26T23:44:37.08255Z","shell.execute_reply":"2021-09-26T23:44:37.08641Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv('../input/traindata/train_df_without_normalize.csv')\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-26T23:44:37.942674Z","iopub.execute_input":"2021-09-26T23:44:37.943534Z","iopub.status.idle":"2021-09-26T23:44:45.781788Z","shell.execute_reply.started":"2021-09-26T23:44:37.943491Z","shell.execute_reply":"2021-09-26T23:44:45.780852Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# classification \nnorm_feature_groupby = train_df.groupby('stock_id')\ninformation_stock = []\nfor stock_id,norm_feature_temp in norm_feature_groupby:\n    avg_target_vol = norm_feature_temp['target'].mean()\n    iqr_target = norm_feature_temp['target'].quantile(0.75) - norm_feature_temp['target'].quantile(0.25)\n    range_target = norm_feature_temp['target'].max() - norm_feature_temp['target'].min()\n    information_stock.append([stock_id,avg_target_vol,iqr_target,range_target])\ninformation_stock_df = pd.DataFrame(information_stock,columns=['stock_id','avg_target_vol','iqr_target','range_target'])\nestimator = KMeans(n_clusters = 10)\nestimator.fit(information_stock_df.iloc[:,1:])    \ny_pred = estimator.predict(information_stock_df.iloc[:,1:])\ninformation_stock_df['classification'] = y_pred\ncheck_dict = dict(zip(information_stock_df['stock_id'],information_stock_df['classification']))","metadata":{"execution":{"iopub.status.busy":"2021-09-26T22:47:48.961946Z","iopub.execute_input":"2021-09-26T22:47:48.962745Z","iopub.status.idle":"2021-09-26T22:47:49.304179Z","shell.execute_reply.started":"2021-09-26T22:47:48.962711Z","shell.execute_reply":"2021-09-26T22:47:49.303327Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# every stock has its own characters, \n# this is the k-means result based on the target features in kaggle_train_feature.ipynb\n  \ntrain_df['classification'] = list(train_df['stock_id'].map(check_dict))\ntrain_df_train = train_df.copy().drop(['time_id','stock_id','target'],1)","metadata":{"execution":{"iopub.status.busy":"2021-09-26T22:47:49.305367Z","iopub.execute_input":"2021-09-26T22:47:49.30559Z","iopub.status.idle":"2021-09-26T22:47:49.647679Z","shell.execute_reply.started":"2021-09-26T22:47:49.305565Z","shell.execute_reply":"2021-09-26T22:47:49.646731Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df_groupby_class = train_df.groupby('classification')\nmodel_list = []\nfor class_,df_ in train_df_groupby_class:\n    print(len(df_))\n    test_y = np.array(df_['target'])\n    test_x = np.array(df_.copy().drop(['time_id','stock_id','target','classification'],1))\n#     rf = RandomForestRegressor(n_estimators=30,max_depth=30,min_samples_leaf=2,max_features = 30 ,max_samples = 0.2)\n    if len(df_)>10000:\n        rf = RandomForestRegressor(n_estimators=10,max_depth=30,min_samples_leaf=5,max_features = 20)\n    else:\n        rf = RandomForestRegressor(n_estimators=5,max_depth=30,min_samples_leaf=1,max_features = 15)\n    model = rf.fit(test_x,test_y)\n    model_list.append([class_,model])\nmodel_dict = dict(model_list)","metadata":{"execution":{"iopub.status.busy":"2021-09-26T23:46:32.353379Z","iopub.execute_input":"2021-09-26T23:46:32.353659Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df_groupby_class = train_df.groupby('classification')\nmodel_list = []\nresult_pred = pd.Series()\nresult_pred_real = pd.Series()\nfor class_,df_ in train_df_groupby_class:\n    test_y = np.array(df_['target'])\n    test_x = np.array(df_.copy().drop(['time_id','stock_id','target','classification'],1))\n    pred_train_y = model_dict[class_].predict(test_x)\n    df_['pred'] = pred_train_y\n    result_pred = pd.concat([result_pred,df_['pred']])\n    result_pred_real = pd.concat([result_pred_real,df_['target']])\n    \nfrom sklearn.metrics import r2_score\ndef rmspe(y_true, y_pred):\n    return  (np.sqrt(np.mean(np.square((y_true - y_pred) / y_true))))\n# pred_train_y = model.predict(np.array(train_df_train))\npred_train_y = np.array(result_pred)\nprint(pred_train_y)\nR2 = round(r2_score(y_true =result_pred_real, y_pred =pred_train_y),3)\nRMSPE = round(rmspe(y_true = result_pred_real, y_pred =pred_train_y),3)\nprint(f'Performance of the naive prediction: R2 score: {R2}, RMSPE: {RMSPE}')","metadata":{"execution":{"iopub.status.busy":"2021-09-26T22:50:22.16068Z","iopub.execute_input":"2021-09-26T22:50:22.160903Z","iopub.status.idle":"2021-09-26T22:50:22.995283Z","shell.execute_reply.started":"2021-09-26T22:50:22.160877Z","shell.execute_reply":"2021-09-26T22:50:22.994271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#random forest model, parameters are chosen from grid_serch & feature are chosen from RFECV\n# rf2=RandomForestRegressor(n_estimators=100,max_depth=30,min_samples_leaf=2,max_features = 30 ,max_samples = 0.6)\n# test_y = np.array(train_df['target'])\n# test_x = np.array(train_df_train)\n# model = rf2.fit(test_x,test_y)","metadata":{"execution":{"iopub.status.busy":"2021-09-25T07:19:58.04184Z","iopub.execute_input":"2021-09-25T07:19:58.04216Z","iopub.status.idle":"2021-09-25T07:45:00.450694Z","shell.execute_reply.started":"2021-09-25T07:19:58.04212Z","shell.execute_reply":"2021-09-25T07:45:00.449903Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from sklearn.metrics import r2_score\n# def rmspe(y_true, y_pred):\n#     return  (np.sqrt(np.mean(np.square((y_true - y_pred) / y_true))))\n# # pred_train_y = model.predict(np.array(train_df_train))\n\n# R2 = round(r2_score(y_true = train_df['target'], y_pred =pred_train_y),3)\n# RMSPE = round(rmspe(y_true = train_df['target'], y_pred =pred_train_y),3)\n# print(f'Performance of the naive prediction: R2 score: {R2}, RMSPE: {RMSPE}')","metadata":{"execution":{"iopub.status.busy":"2021-09-25T07:45:00.451887Z","iopub.execute_input":"2021-09-25T07:45:00.45213Z","iopub.status.idle":"2021-09-25T07:45:17.706625Z","shell.execute_reply.started":"2021-09-25T07:45:00.452102Z","shell.execute_reply":"2021-09-25T07:45:17.705746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def model_socre(max_sample,n_estimators,min_samples_leaf):\n    rf2=RandomForestRegressor(n_estimators=n_estimators,max_depth=30,min_samples_leaf=min_samples_leaf,max_features = 30 ,max_samples = max_sample)\n    test_y = np.array(train_df['target'])\n    test_x = np.array(train_df_train)\n    model = rf2.fit(test_x,test_y)\n    pred_train_y = model.predict(np.array(train_df_train))\n    R2 = round(r2_score(y_true = train_df['target'], y_pred =pred_train_y),3)\n    RMSPE = round(rmspe(y_true = train_df['target'], y_pred =pred_train_y),3)\n    print(f'Performance of the <{max_sample}{n_estimators}{min_samples_leaf}> prediction: R2 score: {R2}, RMSPE: {RMSPE}')\n    ","metadata":{"execution":{"iopub.status.busy":"2021-09-25T07:45:17.70966Z","iopub.execute_input":"2021-09-25T07:45:17.710106Z","iopub.status.idle":"2021-09-25T07:45:17.718853Z","shell.execute_reply.started":"2021-09-25T07:45:17.710062Z","shell.execute_reply":"2021-09-25T07:45:17.717769Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#测试集数据\ndata_dir_test = '../input/optiver-realized-volatility-prediction/book_test.parquet'\nstock_list_test = sorted([int(_.split('=')[1]) for _ in os.listdir(data_dir_test)])\ntrain_df_test = pd.DataFrame()\nfor stock_id in stock_list_test:\n    book_test_0 = pd.read_parquet('../input/optiver-realized-volatility-prediction/book_test.parquet/'+f'stock_id={stock_id}')\n    trade_test_0 = pd.read_parquet('../input/optiver-realized-volatility-prediction/trade_test.parquet/'+f'stock_id={stock_id}')\n    volatility_feature = trade_train_feature(trade_test_0).merge(book_train_feature(book_test_0),on='time_id')\n    volatility_feature = volatility_feature.fillna(method='ffill')\n    volatility_feature = volatility_feature.fillna(0)\n    \n#     volatility_feature_1 = pd.DataFrame(StandardScaler().fit_transform(volatility_feature.iloc[:,1:]),columns=volatility_feature.iloc[:,1:].columns)\n    volatility_feature['stock_id'] = stock_id\n    train_df_test = pd.concat([train_df_test,volatility_feature])\n\ntrain_df_test['classification'] = list(train_df_test['stock_id'].map(check_dict))\ntrain_df_test_1 = train_df_test.copy().drop(['time_id','stock_id'],1)","metadata":{"execution":{"iopub.status.busy":"2021-09-26T05:41:56.374337Z","iopub.execute_input":"2021-09-26T05:41:56.374781Z","iopub.status.idle":"2021-09-26T05:41:56.456072Z","shell.execute_reply.started":"2021-09-26T05:41:56.374738Z","shell.execute_reply":"2021-09-26T05:41:56.45514Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # 测试集预测\n# # print('finished successfully')\n# pred_test_y = model.predict(np.array(train_df_test_1))\n\n# train_df_test['target'] = pred_test_y\n# train_df_test['row_id'] = train_df_test['stock_id'].astype(str) + '-' + train_df_test['time_id'].astype(str)\n# result = train_df_test[['row_id','target']].copy()\n# result_dict = dict(zip(train_df_test['row_id'],train_df_test['target']))\n# # result\n# test_for_index = pd.read_csv('../input/optiver-realized-volatility-prediction/test.csv')\n# test_for_index['target'] = list(test_for_index['row_id'].map(result_dict))\n# test_for_index = test_for_index.drop(['stock_id','time_id'],1)\n# test_for_index = test_for_index.fillna(method = 'ffill')\n# test_for_index.to_csv('submission.csv',index = False)\n","metadata":{"execution":{"iopub.status.busy":"2021-09-26T05:39:20.223456Z","iopub.execute_input":"2021-09-26T05:39:20.223822Z","iopub.status.idle":"2021-09-26T05:39:20.228839Z","shell.execute_reply.started":"2021-09-26T05:39:20.223788Z","shell.execute_reply":"2021-09-26T05:39:20.228042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df_groupby_class = train_df_test.groupby('classification')\nresult_pred_test = pd.DataFrame()\nfor class_,df_ in train_df_groupby_class:\n    test_x = np.array(df_.copy().drop(['time_id','stock_id','classification'],1))\n    pred_test_y = model_dict[class_].predict(test_x)\n    df_['pred'] = pred_test_y\n    result_pred_test = pd.concat([result_pred_test,df_[['stock_id','time_id','pred']]])\n\nresult_pred_test['row_id'] = result_pred_test['stock_id'].astype(str) + '-' + result_pred_test['time_id'].astype(str)\nresult = result_pred_test[['row_id','pred']].copy()\nresult_dict = dict(zip(result['row_id'],result['pred']))\n# result\ntest_for_index = pd.read_csv('../input/optiver-realized-volatility-prediction/test.csv')\ntest_for_index['target'] = list(test_for_index['row_id'].map(result_dict))\ntest_for_index = test_for_index.drop(['stock_id','time_id'],1)\ntest_for_index = test_for_index.fillna(method = 'ffill')\ntest_for_index.to_csv('submission.csv',index = False)\n","metadata":{"execution":{"iopub.status.busy":"2021-09-26T05:43:10.12323Z","iopub.execute_input":"2021-09-26T05:43:10.123609Z","iopub.status.idle":"2021-09-26T05:43:10.161928Z","shell.execute_reply.started":"2021-09-26T05:43:10.123568Z","shell.execute_reply":"2021-09-26T05:43:10.161099Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_for_index","metadata":{"execution":{"iopub.status.busy":"2021-09-26T05:43:12.710189Z","iopub.execute_input":"2021-09-26T05:43:12.711303Z","iopub.status.idle":"2021-09-26T05:43:12.723164Z","shell.execute_reply.started":"2021-09-26T05:43:12.711237Z","shell.execute_reply":"2021-09-26T05:43:12.721983Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}