{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### References\n* https://www.kaggle.com/realtimshady/2lgbm-2nn\n* https://www.kaggle.com/munumbutt/feature-engineering-tuned-xgboost-lgbm","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\nimport glob\nfrom IPython.core.display import display, HTML\nimport gc\nimport plotly.graph_objects as go\nfrom joblib import Parallel, delayed\nfrom sklearn import preprocessing, model_selection\nimport seaborn as sns\nfrom tqdm import tqdm\nfrom scipy.stats import probplot\n\npd.set_option('max_rows', 400)\npd.set_option('max_columns', 400)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_dir = '../input/optiver-realized-volatility-prediction/'","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## EDA","metadata":{}},{"cell_type":"code","source":"sample = pd.read_csv(\"../input/optiver-realized-volatility-prediction/sample_submission.csv\")\nsample","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = pd.read_csv(\"../input/optiver-realized-volatility-prediction/test.csv\")\ntest","metadata":{"execution":{"iopub.status.busy":"2021-09-24T03:01:29.867204Z","iopub.execute_input":"2021-09-24T03:01:29.867647Z","iopub.status.idle":"2021-09-24T03:01:29.950429Z","shell.execute_reply.started":"2021-09-24T03:01:29.867542Z","shell.execute_reply":"2021-09-24T03:01:29.949247Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv(\"../input/optiver-realized-volatility-prediction/train.csv\")\ntrain","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['stock_id'].value_counts()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['stock_id'].unique()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"book_train = pd.read_parquet('../input/optiver-realized-volatility-prediction/book_train.parquet/stock_id=10')\nbook_train","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trade_example = pd.read_parquet(\"../input/optiver-realized-volatility-prediction/trade_train.parquet/stock_id=1\")\ntrade_example","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## FUNC","metadata":{}},{"cell_type":"code","source":"def convert_to_32bit(df):\n    for f in df.columns:\n        if df[f].dtype == 'int64':\n            df[f] = df[f].astype('int32')\n        if df[f].dtype == 'float64':\n            df[f] = df[f].astype('float32')\n    return df","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def wap_1(df):\n    wap = (df['bid_price1'] * df['bid_size1'] + df['ask_price1'] * df['ask_size1'])/(df['bid_size1'] + df['ask_size1'])\n    return wap\ndef wap_2(df):\n    wap = (df['bid_price2'] * df['bid_size2'] + df['ask_price2'] * df['ask_size2'])/(df['bid_size2'] + df['ask_size2'])\n    return wap\ndef wap_bid(df):\n    wap = (df['bid_price1'] * df['bid_size1'] + df['bid_price2'] * df['bid_size2'])/(df['bid_size1'] + df['bid_size2'])\n    return wap\ndef wap_ask(df):\n    wap = (df['ask_price1'] * df['ask_size1'] + df['ask_price2'] * df['ask_size2'])/(df['ask_size1'] + df['ask_size2'])\n    return wap","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def log_return(list_stock_prices):\n    return np.log(list_stock_prices).diff() \ndef realized_volatility(series):\n    return np.sqrt(np.sum(series**2))\ndef count_unique(series):\n    return len(np.unique(series))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to read our base train and test set\ndef read_train_test():\n    train = pd.read_csv('../input/optiver-realized-volatility-prediction/train.csv')\n    test = pd.read_csv('../input/optiver-realized-volatility-prediction/test.csv')\n    # Create a key to merge with book and trade data\n    train['row_id'] = train['stock_id'].astype(str) + '-' + train['time_id'].astype(str)\n    test['row_id'] = test['stock_id'].astype(str) + '-' + test['time_id'].astype(str)\n    print(f'Our training set has {train.shape[0]} rows')\n    return train, test","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def book_preprocessor(file_path):\n    df = pd.read_parquet(file_path)\n    df = convert_to_32bit(df)\n    \n    #calculate return etc\n    df['wap1'] = wap_1(df)\n    df['log_return1'] = df.groupby('time_id')['wap1'].apply(log_return)\n    \n    df['wap2'] = wap_2(df)\n    df['log_return2'] = df.groupby('time_id')['wap2'].apply(log_return)\n    \n    df['wap_bid'] = wap_bid(df)\n    df['wap_ask'] = wap_ask(df)\n    \n    df['log_return_bid'] = df.groupby('time_id')['wap_bid'].apply(log_return)\n    df['log_return_ask'] = df.groupby('time_id')['wap_ask'].apply(log_return)\n    \n    df['wap_balance'] = abs(df['wap1'] - df['wap2'])\n    \n    df['price_spread'] = (df['ask_price1'] - df['bid_price1']) / ((df['ask_price1'] + df['bid_price1'])/2)\n    df['bid_spread'] = df['bid_price1'] - df['bid_price2']\n    df['ask_spread'] = df['ask_price1'] - df['ask_price2']\n    df[\"bid_ask_spread\"] = abs(df['bid_spread'] - df['ask_spread'])\n    df['bid_volume'] = df['bid_size1'] + df['bid_size2']\n    df['ask_volume'] = df['ask_size1'] + df['ask_size2']\n    df['bid_ask_volume'] = abs(df['bid_volume'] - df['ask_volume'])\n    df['total_volume'] = (df['ask_size1'] + df['ask_size2']) + (df['bid_size1'] + df['bid_size2'])\n    df['volume_imbalance'] = abs((df['ask_size1'] + df['ask_size2']) - (df['bid_size1'] + df['bid_size2']))\n\n    #dict for aggregate\n    create_feature_dict = {\n        'wap1': [np.sum, np.mean, np.min, np.max, np.std],\n        'wap2': [np.sum, np.mean, np.min, np.max, np.std],\n        'log_return1': [np.sum, realized_volatility, np.mean, np.min, np.max, np.std],\n        'log_return2': [np.sum, realized_volatility, np.mean, np.min, np.max, np.std],\n        'wap_bid': [np.sum, np.mean, np.min, np.max, np.std],\n        'wap_ask': [np.sum, np.mean, np.min, np.max, np.std],\n        'log_return_bid': [np.sum, realized_volatility, np.mean, np.min, np.max, np.std],\n        'log_return_ask': [np.sum, realized_volatility, np.mean, np.min, np.max, np.std],\n        'wap_balance': [np.sum, np.mean, np.min, np.max, np.std],\n        'price_spread':[np.sum, np.mean, np.min, np.max, np.std],\n        'bid_spread':[np.sum, np.mean, np.min, np.max, np.std],\n        'ask_spread':[np.sum, np.mean, np.min, np.max, np.std],\n        'bid_ask_spread':[np.sum, np.mean, np.min, np.max, np.std],\n        'bid_volume':[np.sum, np.mean, np.min, np.max, np.std],\n        'ask_volume':[np.sum, np.mean, np.min, np.max, np.std],\n        'bid_ask_volume':[np.sum, np.mean, np.min, np.max, np.std],\n        'total_volume':[np.sum, np.mean, np.min, np.max, np.std],\n        'volume_imbalance':[np.sum, np.mean, np.min, np.max, np.std]\n        }\n\n    create_feature_dict_time = {\n        'log_return1': [realized_volatility],\n        'log_return2': [realized_volatility],\n        'log_return_bid': [realized_volatility],\n        'log_return_bid': [realized_volatility],\n    }\n    \n    # Function to get group stats for different windows (seconds in bucket)\n    def get_stats_window(fe_dict,seconds_in_bucket, add_suffix = False):\n        # Group by the window\n        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(fe_dict).reset_index()\n        # Rename columns joining suffix\n        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n        # Add a suffix to differentiate windows\n        if add_suffix:\n            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n        return df_feature\n    \n    # Get the stats for different windows\n    df_feature = get_stats_window(create_feature_dict,seconds_in_bucket = 0, add_suffix = False)\n    df_feature_500 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 500, add_suffix = True)\n    df_feature_400 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 400, add_suffix = True)\n    df_feature_300 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 300, add_suffix = True)\n    df_feature_200 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 200, add_suffix = True)\n    df_feature_100 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 100, add_suffix = True)\n\n    # Merge all\n    df_feature = df_feature.merge(df_feature_500, how = 'left', left_on = 'time_id_', right_on = 'time_id__500')\n    df_feature = df_feature.merge(df_feature_400, how = 'left', left_on = 'time_id_', right_on = 'time_id__400')\n    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n    df_feature = df_feature.merge(df_feature_200, how = 'left', left_on = 'time_id_', right_on = 'time_id__200')\n    df_feature = df_feature.merge(df_feature_100, how = 'left', left_on = 'time_id_', right_on = 'time_id__100')\n    # Drop unnecesary time_ids\n    df_feature.drop(['time_id__500','time_id__400', 'time_id__300', 'time_id__200','time_id__100'], axis = 1, inplace = True)\n    \n    \n    # Create row_id so we can merge\n    stock_id = file_path.split('=')[1]\n    df_feature['row_id'] = df_feature['time_id_'].apply(lambda x: f'{stock_id}-{x}')\n    df_feature.drop(['time_id_'], axis = 1, inplace = True)\n    return df_feature","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\n# Function to preprocess trade data (for each stock id)\ndef trade_preprocessor(file_path):\n    df = pd.read_parquet(file_path)\n    df = convert_to_32bit(df)\n    \n    df['log_return'] = df.groupby('time_id')['price'].apply(log_return)\n    df['amount']=df['price']*df['size']\n    # Dict for aggregations\n    create_feature_dict = {\n        'log_return':[realized_volatility],\n        'seconds_in_bucket':[count_unique],\n        'size':[np.sum, np.max, np.min],\n        'order_count':[np.sum,np.max],\n        'amount':[np.sum,np.max,np.min],\n    }\n    create_feature_dict_time = {\n        'log_return':[realized_volatility],\n        'seconds_in_bucket':[count_unique],\n        'size':[np.sum],\n        'order_count':[np.sum],\n    }\n    # Function to get group stats for different windows (seconds in bucket)\n    def get_stats_window(fe_dict,seconds_in_bucket, add_suffix = False):\n        # Group by the window\n        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(fe_dict).reset_index()\n        # Rename columns joining suffix\n        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n        # Add a suffix to differentiate windows\n        if add_suffix:\n            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n        return df_feature\n    \n\n    # Get the stats for different windows\n    df_feature = get_stats_window(create_feature_dict,seconds_in_bucket = 0, add_suffix = False)\n    df_feature_500 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 500, add_suffix = True)\n    df_feature_400 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 400, add_suffix = True)\n    df_feature_300 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 300, add_suffix = True)\n    df_feature_200 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 200, add_suffix = True)\n    df_feature_100 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 100, add_suffix = True)\n    \n    def tendency(price, vol):    \n        df_diff = np.diff(price)\n        val = (df_diff/price[1:])*100\n        power = np.sum(val*vol[1:])\n        return(power)\n    \n    lis = []\n    for n_time_id in df['time_id'].unique():\n        df_id = df[df['time_id'] == n_time_id]        \n        tendencyV = tendency(df_id['price'].values, df_id['size'].values)      \n        f_max = np.sum(df_id['price'].values > np.mean(df_id['price'].values))\n        f_min = np.sum(df_id['price'].values < np.mean(df_id['price'].values))\n        df_max =  np.sum(np.diff(df_id['price'].values) > 0)\n        df_min =  np.sum(np.diff(df_id['price'].values) < 0)\n        # new\n        abs_diff = np.median(np.abs( df_id['price'].values - np.mean(df_id['price'].values)))        \n        energy = np.mean(df_id['price'].values**2)\n        iqr_p = np.percentile(df_id['price'].values,75) - np.percentile(df_id['price'].values,25)\n        \n        # vol vars\n        \n        abs_diff_v = np.median(np.abs( df_id['size'].values - np.mean(df_id['size'].values)))        \n        energy_v = np.sum(df_id['size'].values**2)\n        iqr_p_v = np.percentile(df_id['size'].values,75) - np.percentile(df_id['size'].values,25)\n        \n        lis.append({'time_id':n_time_id,'tendency':tendencyV,'f_max':f_max,'f_min':f_min,'df_max':df_max,'df_min':df_min,\n                   'abs_diff':abs_diff,'energy':energy,'iqr_p':iqr_p,'abs_diff_v':abs_diff_v,'energy_v':energy_v,'iqr_p_v':iqr_p_v})\n    \n    df_lr = pd.DataFrame(lis)\n        \n   \n    df_feature = df_feature.merge(df_lr, how = 'left', left_on = 'time_id_', right_on = 'time_id')\n    \n    # Merge all\n    df_feature = df_feature.merge(df_feature_500, how = 'left', left_on = 'time_id_', right_on = 'time_id__500')\n    df_feature = df_feature.merge(df_feature_400, how = 'left', left_on = 'time_id_', right_on = 'time_id__400')\n    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n    df_feature = df_feature.merge(df_feature_200, how = 'left', left_on = 'time_id_', right_on = 'time_id__200')\n    df_feature = df_feature.merge(df_feature_100, how = 'left', left_on = 'time_id_', right_on = 'time_id__100')\n    # Drop unnecesary time_ids\n    df_feature.drop(['time_id__500','time_id__400', 'time_id__300', 'time_id__200','time_id','time_id__100'], axis = 1, inplace = True)\n    \n    \n    df_feature = df_feature.add_prefix('trade_')\n    stock_id = file_path.split('=')[1]\n    df_feature['row_id'] = df_feature['trade_time_id_'].apply(lambda x:f'{stock_id}-{x}')\n    df_feature.drop(['trade_time_id_'], axis = 1, inplace = True)\n    return df_feature","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\ndef get_time_stock(df):\n    vol_cols = ['log_return1_realized_volatility', 'log_return2_realized_volatility', 'log_return1_realized_volatility_400', 'log_return2_realized_volatility_400', \n                'log_return1_realized_volatility_300', 'log_return2_realized_volatility_300', 'log_return1_realized_volatility_200', 'log_return2_realized_volatility_200', \n                'trade_log_return_realized_volatility', 'trade_log_return_realized_volatility_400', 'trade_log_return_realized_volatility_300', 'trade_log_return_realized_volatility_200']\n\n\n    # Group by the stock id\n    df_stock_id = df.groupby(['stock_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n    # Rename columns joining suffix\n    df_stock_id.columns = ['_'.join(col) for col in df_stock_id.columns]\n    df_stock_id = df_stock_id.add_suffix('_' + 'stock')\n\n    # Group by the stock id\n    df_time_id = df.groupby(['time_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n    # Rename columns joining suffix\n    df_time_id.columns = ['_'.join(col) for col in df_time_id.columns]\n    df_time_id = df_time_id.add_suffix('_' + 'time')\n    \n    # Merge with original dataframe\n    df = df.merge(df_stock_id, how = 'left', left_on = ['stock_id'], right_on = ['stock_id__stock'])\n    df = df.merge(df_time_id, how = 'left', left_on = ['time_id'], right_on = ['time_id__time'])\n    df.drop(['stock_id__stock', 'time_id__time'], axis = 1, inplace = True)\n    return df","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Funtion to make preprocessing function in parallel (for each stock id)\ndef preprocessor(list_stock_ids, is_train = True):\n    \n    # Parrallel for loop\n    def for_joblib(stock_id):\n        # Train\n        if is_train:\n            file_path_book = data_dir + \"book_train.parquet/stock_id=\" + str(stock_id)\n            file_path_trade = data_dir + \"trade_train.parquet/stock_id=\" + str(stock_id)\n        # Test\n        else:\n            file_path_book = data_dir + \"book_test.parquet/stock_id=\" + str(stock_id)\n            file_path_trade = data_dir + \"trade_test.parquet/stock_id=\" + str(stock_id)\n    \n        # Preprocess book and trade data and merge them\n        df_tmp = pd.merge(book_preprocessor(file_path_book), trade_preprocessor(file_path_trade), on = 'row_id', how = 'left')\n        \n        # Return the merge dataframe\n        return df_tmp\n    \n    # Use parallel api to call paralle for loop\n    df = Parallel(n_jobs = -1, verbose = 1)(delayed(for_joblib)(stock_id) for stock_id in list_stock_ids)\n    # Concatenate all the dataframes that return from Parallel\n    df = pd.concat(df, ignore_index = True)\n    return df","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to calculate the root mean squared percentage error\ndef rmspe(y_true, y_pred):\n    return np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to early stop with root mean squared percentage error\ndef feval_rmspe(y_pred, lgb_train):\n    y_true = lgb_train.get_label()\n    return 'RMSPE', rmspe(y_true, y_pred), False\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train, test = read_train_test()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_stock_ids = train['stock_id'].unique()\n# Preprocess them using Parallel and our single stock id functions\ntrain_ = preprocessor(train_stock_ids, is_train = True)\ntrain = train.merge(train_, on = ['row_id'], how = 'left')\n\n# Get unique stock ids \ntest_stock_ids = test['stock_id'].unique()\n# Preprocess them using Parallel and our single stock id functions\ntest_ = preprocessor(test_stock_ids, is_train = False)\ntest = test.merge(test_, on = ['row_id'], how = 'left')\n\n# Get group stats of time_id and stock_id\ntrain = get_time_stock(train)\ntest = get_time_stock(test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# replace by order sum (tau)\ntrain['size_tau'] = np.sqrt( 1/ train['trade_seconds_in_bucket_count_unique'] )\ntest['size_tau'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique'] )\n#train['size_tau_450'] = np.sqrt( 1/ train['trade_seconds_in_bucket_count_unique_450'] )\n#test['size_tau_450'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique_450'] )\ntrain['size_tau_400'] = np.sqrt( 1/ train['trade_seconds_in_bucket_count_unique_400'] )\ntest['size_tau_400'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique_400'] )\ntrain['size_tau_300'] = np.sqrt( 1/ train['trade_seconds_in_bucket_count_unique_300'] )\ntest['size_tau_300'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique_300'] )\n#train['size_tau_150'] = np.sqrt( 1/ train['trade_seconds_in_bucket_count_unique_150'] )\n#test['size_tau_150'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique_150'] )\ntrain['size_tau_200'] = np.sqrt( 1/ train['trade_seconds_in_bucket_count_unique_200'] )\ntest['size_tau_200'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique_200'] )","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['size_tau2'] = np.sqrt( 1/ train['trade_order_count_sum'] )\ntest['size_tau2'] = np.sqrt( 1/ test['trade_order_count_sum'] )\n#train['size_tau2_450'] = np.sqrt( 0.25/ train['trade_order_count_sum'] )\n#test['size_tau2_450'] = np.sqrt( 0.25/ test['trade_order_count_sum'] )\ntrain['size_tau2_400'] = np.sqrt( 0.33/ train['trade_order_count_sum'] )\ntest['size_tau2_400'] = np.sqrt( 0.33/ test['trade_order_count_sum'] )\ntrain['size_tau2_300'] = np.sqrt( 0.5/ train['trade_order_count_sum'] )\ntest['size_tau2_300'] = np.sqrt( 0.5/ test['trade_order_count_sum'] )\n#train['size_tau2_150'] = np.sqrt( 0.75/ train['trade_order_count_sum'] )\n#test['size_tau2_150'] = np.sqrt( 0.75/ test['trade_order_count_sum'] )\ntrain['size_tau2_200'] = np.sqrt( 0.66/ train['trade_order_count_sum'] )\ntest['size_tau2_200'] = np.sqrt( 0.66/ test['trade_order_count_sum'] )\n\n# delta tau\ntrain['size_tau2_d'] = train['size_tau2_400'] - train['size_tau2']\ntest['size_tau2_d'] = test['size_tau2_400'] - test['size_tau2']","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = train\ntest_data_set = test","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data_set['stock_id'] = test_data_set['stock_id'].astype(int)\ntest_data_set.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = df.drop(['row_id', 'target', 'time_id'], axis = 1)\ny = df['target']\nX.shape, y.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"thresh = int(len(df) * 0.9 / df['stock_id'].nunique())\nprint (thresh)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mask  = df.groupby('stock_id')['stock_id'].cumcount() < thresh","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = df[mask]\ntest = df[~mask]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = train.drop(['row_id', 'target', 'time_id'], axis = 1)\ny_train = train['target']\nX_train.shape, y_train.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_valid = test.drop(['row_id', 'target', 'time_id'], axis = 1)\ny_valid = test['target']\nX_valid.shape, y_valid.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train['stock_id'] = X_train['stock_id'].astype(int)\nX_valid['stock_id'] = X_valid['stock_id'].astype(int)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## XGBOOST","metadata":{}},{"cell_type":"code","source":"import optuna\nimport xgboost as xgb\nfrom optuna.samplers import TPESampler\nfrom sklearn.preprocessing import RobustScaler, StandardScaler, MinMaxScaler\nfrom sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, KFold\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params_xgb = {\n        'lambda': 0.0014832052084105417, \n        'alpha': 2.6885464964958112, \n        'max_depth': 17, \n        'learning_rate': 0.02, \n        'random_state': 24, \n        'n_estimators': 1540, \n        'eta': 0.12558915915760901, \n        'subsample': 0.6000000000000001, \n        'colsample_bytree': 0.3, \n        'min_child_weight': 77, \n        'reg_lambda': 0.001217091110648466, \n        'reg_alpha': 0.0019723477880301235\n        }\n\nxgb_model = xgb.XGBRegressor(**params_xgb, tree_method='gpu_hist')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nxgb_model.fit(X_train ,y_train, eval_set=[(X_valid, y_valid)], early_stopping_rounds=150, verbose=False)\n\npreds = xgb_model.predict(X_valid)\nRMSPE = round(rmspe(y_true = y_valid, y_pred = preds), 5)\nprint(f'Performance of the Tuned XGB prediction: RMSPE: {RMSPE}')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## LGBM","metadata":{}},{"cell_type":"code","source":"from lightgbm import LGBMRegressor","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params_lgbm = {\n        \"metric\": \"rmse\",\n        \"verbosity\": -1,\n        'learning_rate': 0.04412162462604988, \n        'max_depth': 300, \n        'lambda_l1': 0.12309589568066824, \n        'lambda_l2': 3.1044658548129586e-06, \n        'num_leaves': 246, \n        'n_estimators': 2350, \n        'feature_fraction': 0.531654883966269, \n        'bagging_fraction': 0.8553165643797457, \n        'bagging_freq': 8, \n        'min_child_samples': 42\n        }","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lgbm_model = LGBMRegressor(**params_lgbm, device='gpu')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nlgbm_model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], verbose=False, early_stopping_rounds=150)\n\npreds = lgbm_model.predict(X_valid)\nRMSPE = round(rmspe(y_true = y_valid, y_pred = preds), 5)\nprint(f'Performance of the Tuned LIGHTGBM prediction: RMSPE: {RMSPE}')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## CatBoostRegressor","metadata":{}},{"cell_type":"code","source":"import catboost as cat\nfrom catboost import CatBoostRegressor","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params_cb = {\n        'colsample_bylevel': 0.029576065862676762,\n        'depth': 91,\n        'learning_rate': 0.022293479743970765,\n        'iterations': 7000,\n        'max_bin': 120,\n        'min_data_in_leaf': 66,\n        'l2_leaf_reg': 0.0009704826955054485,\n        'bagging_temperature': 0.7432417203968587,\n        'subsample': 0.7022796507235656,\n        'grow_policy': 'Lossguide', \n        'leaf_estimation_method': 'Newton',\n        'loss_function': 'RMSE',\n        'eval_metric': 'RMSE',\n        'cat_features': ['stock_id']\n        }","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cb_model = CatBoostRegressor(**params_cb)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ncb_model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], verbose=False, early_stopping_rounds=150)\n\npreds = cb_model.predict(X_valid)\nRMSPE = round(rmspe(y_true = y_valid, y_pred = preds), 5)\nprint(f'Performance of the Tuned CATBOOST prediction: RMSPE: {RMSPE}')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Stacking","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import StackingRegressor","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mod_xgb = xgb.XGBRegressor(tree_method='gpu_hist', n_jobs= - 1)\nmod_lgbm = LGBMRegressor(device='gpu')\nmod_cb = CatBoostRegressor()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"estimators = [('mod_xgb', mod_xgb),\n              ('mod_lgbm', mod_lgbm),\n              ('mod_cb', mod_cb)]\n\nclf = StackingRegressor(estimators=estimators, verbose=1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nclf.fit(X_train, y_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = clf.predict(X_valid)\nRMSPE = round(rmspe(y_true = y_valid, y_pred = preds), 5)\nprint(f'Performance of the STACK prediction: RMSPE: {RMSPE}')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Submission","metadata":{}},{"cell_type":"code","source":"y_pred = test_data_set[['row_id']]\nX_test = test_data_set.drop(['time_id', 'row_id'], axis = 1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target = np.zeros(len(X_test))\n\npred = clf.predict(X_test[X_train.columns])\ntarget = pred","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = y_pred.assign(target = target)\ny_pred","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred.to_csv('submission.csv',index = False)","metadata":{},"execution_count":null,"outputs":[]}]}