{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport xgboost as xgb\nfrom scipy import stats\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, make_scorer\nfrom sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\nfrom sklearn import clone\n\nimport time\nfrom functools import reduce\nfrom operator import mul\nimport os\nimport glob\nimport math\nimport logging\n# from utils.logging import log_and_warn\n\n\nimport argparse\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-09-24T16:25:02.819306Z","iopub.execute_input":"2021-09-24T16:25:02.81983Z","iopub.status.idle":"2021-09-24T16:25:03.874476Z","shell.execute_reply.started":"2021-09-24T16:25:02.819798Z","shell.execute_reply":"2021-09-24T16:25:03.873678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"book_example = pd.read_parquet('../input/optiver-realized-volatility-prediction/book_train.parquet/stock_id=0')\ntrade_example =  pd.read_parquet('../input/optiver-realized-volatility-prediction/trade_train.parquet/stock_id=0')\nstock_id = '0'\nbook_example = book_example[book_example['time_id']==5]\nbook_example.loc[:,'stock_id'] = stock_id\ntrade_example = trade_example[trade_example['time_id']==5]\ntrade_example.loc[:,'stock_id'] = stock_id","metadata":{"execution":{"iopub.status.busy":"2021-09-24T16:25:03.875782Z","iopub.execute_input":"2021-09-24T16:25:03.876174Z","iopub.status.idle":"2021-09-24T16:25:04.499831Z","shell.execute_reply.started":"2021-09-24T16:25:03.876145Z","shell.execute_reply":"2021-09-24T16:25:04.499039Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"book_example['wap'] = (book_example['bid_price1'] * book_example['ask_size1'] +\n                                book_example['ask_price1'] * book_example['bid_size1']) / (\n                                       book_example['bid_size1']+ book_example['ask_size1'])","metadata":{"execution":{"iopub.status.busy":"2021-09-24T16:25:04.501166Z","iopub.execute_input":"2021-09-24T16:25:04.501553Z","iopub.status.idle":"2021-09-24T16:25:04.50967Z","shell.execute_reply.started":"2021-09-24T16:25:04.501525Z","shell.execute_reply":"2021-09-24T16:25:04.508675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def log_return(list_stock_prices):\n    return np.log(list_stock_prices).diff() \n","metadata":{"execution":{"iopub.status.busy":"2021-09-24T16:25:04.84108Z","iopub.execute_input":"2021-09-24T16:25:04.841432Z","iopub.status.idle":"2021-09-24T16:25:04.845645Z","shell.execute_reply.started":"2021-09-24T16:25:04.841401Z","shell.execute_reply":"2021-09-24T16:25:04.844843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"book_example.loc[:,'log_return'] = log_return(book_example['wap'])\nbook_example = book_example[~book_example['log_return'].isnull()]\nbook_example.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-24T16:25:05.417659Z","iopub.execute_input":"2021-09-24T16:25:05.418103Z","iopub.status.idle":"2021-09-24T16:25:05.455259Z","shell.execute_reply.started":"2021-09-24T16:25:05.418048Z","shell.execute_reply":"2021-09-24T16:25:05.454162Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def realized_volatility(series_log_return):\n    return np.sqrt(np.sum(series_log_return**2))\n\nrealized_vol = realized_volatility(book_example['log_return'])\nprint(f'Realized volatility for stock_id 0 on time_id 5 is {realized_vol}')","metadata":{"execution":{"iopub.status.busy":"2021-09-24T16:25:08.634411Z","iopub.execute_input":"2021-09-24T16:25:08.634815Z","iopub.status.idle":"2021-09-24T16:25:08.642494Z","shell.execute_reply.started":"2021-09-24T16:25:08.634784Z","shell.execute_reply":"2021-09-24T16:25:08.641694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nlist_order_book_file_train = glob.glob('/kaggle/input/optiver-realized-volatility-prediction/book_train.parquet/*')","metadata":{"execution":{"iopub.status.busy":"2021-09-24T16:25:11.628125Z","iopub.execute_input":"2021-09-24T16:25:11.628634Z","iopub.status.idle":"2021-09-24T16:25:11.650133Z","shell.execute_reply.started":"2021-09-24T16:25:11.628603Z","shell.execute_reply":"2021-09-24T16:25:11.649021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def realized_volatility_per_time_id(file_path, prediction_column_name):\n    df_book_data = pd.read_parquet(file_path)\n    df_book_data['wap'] =(df_book_data['bid_price1'] * df_book_data['ask_size1']+df_book_data['ask_price1'] * df_book_data['bid_size1'])  / (\n                                      df_book_data['bid_size1']+ df_book_data[\n                                  'ask_size1'])\n    df_book_data['log_return'] = df_book_data.groupby(['time_id'])['wap'].apply(log_return)\n    df_book_data = df_book_data[~df_book_data['log_return'].isnull()]\n    df_realized_vol_per_stock =  pd.DataFrame(df_book_data.groupby(['time_id'])['log_return'].agg(realized_volatility)).reset_index()\n    df_realized_vol_per_stock = df_realized_vol_per_stock.rename(columns = {'log_return':prediction_column_name})\n    stock_id = file_path.split('=')[1]\n    df_realized_vol_per_stock['row_id'] = df_realized_vol_per_stock['time_id'].apply(lambda x:f'{stock_id}-{x}')\n    return df_realized_vol_per_stock[['row_id',prediction_column_name]]","metadata":{"execution":{"iopub.status.busy":"2021-09-24T16:25:12.888424Z","iopub.execute_input":"2021-09-24T16:25:12.888823Z","iopub.status.idle":"2021-09-24T16:25:12.898523Z","shell.execute_reply.started":"2021-09-24T16:25:12.888791Z","shell.execute_reply":"2021-09-24T16:25:12.897153Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def past_realized_volatility_per_stock(list_file,prediction_column_name):\n    df_past_realized = pd.DataFrame()\n    for file in list_file:\n        df_past_realized = pd.concat([df_past_realized,\n                                     realized_volatility_per_time_id(file,prediction_column_name)])\n    return df_past_realized\n\n# df_past_realized_train = past_realized_volatility_per_stock(list_file=list_order_book_file_train,\n#                                                            prediction_column_name='pred')\n# df_past_realized_train","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-09-24T16:25:13.889523Z","iopub.execute_input":"2021-09-24T16:25:13.889897Z","iopub.status.idle":"2021-09-24T16:25:13.894443Z","shell.execute_reply.started":"2021-09-24T16:25:13.889866Z","shell.execute_reply":"2021-09-24T16:25:13.893662Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('../input/optiver-realized-volatility-prediction/train.csv')\ntrain.sample(10)","metadata":{"execution":{"iopub.status.busy":"2021-09-24T16:25:19.992864Z","iopub.execute_input":"2021-09-24T16:25:19.993218Z","iopub.status.idle":"2021-09-24T16:25:20.157984Z","shell.execute_reply.started":"2021-09-24T16:25:19.993188Z","shell.execute_reply":"2021-09-24T16:25:20.157094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['row_id'] = train['stock_id'].astype(str) + '-' + train['time_id'].astype(str)\ntrain = train[['row_id','target']]\n# df_joined = train.merge(df_past_realized_train[['row_id','pred']], on = ['row_id'], how = 'left')\ntrain.sample(10)","metadata":{"execution":{"iopub.status.busy":"2021-09-24T16:25:23.43797Z","iopub.execute_input":"2021-09-24T16:25:23.43833Z","iopub.status.idle":"2021-09-24T16:25:24.648974Z","shell.execute_reply.started":"2021-09-24T16:25:23.4383Z","shell.execute_reply":"2021-09-24T16:25:24.647922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df_joined","metadata":{"execution":{"iopub.status.busy":"2021-07-09T14:19:11.178277Z","iopub.execute_input":"2021-07-09T14:19:11.17855Z","iopub.status.idle":"2021-07-09T14:19:11.182214Z","shell.execute_reply.started":"2021-07-09T14:19:11.178517Z","shell.execute_reply":"2021-07-09T14:19:11.181379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from sklearn.metrics import r2_score\n# def rmspe(y_true, y_pred):\n#     return  (np.sqrt(np.mean(np.square((y_true - y_pred) / y_true))))\n# R2 = round(r2_score(y_true = df_joined['target'], y_pred = df_joined['pred']),3)\n# RMSPE = round(rmspe(y_true = df_joined['target'], y_pred = df_joined['pred']),3)\n# print(f'Performance of the naive prediction: R2 score: {R2}, RMSPE: {RMSPE}')","metadata":{"execution":{"iopub.status.busy":"2021-07-09T14:19:11.183414Z","iopub.execute_input":"2021-07-09T14:19:11.183679Z","iopub.status.idle":"2021-07-09T14:19:11.194754Z","shell.execute_reply.started":"2021-07-09T14:19:11.183646Z","shell.execute_reply":"2021-07-09T14:19:11.194088Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def data_prep(list_order_book_file):\n    df_past_realized = pd.DataFrame()\n    for file in list_order_book_file:\n        df_book_data = pd.read_parquet(file)\n        df_book_data['wap'] =(df_book_data['bid_price1'] * df_book_data['ask_size1']+df_book_data['ask_price1'] * df_book_data['bid_size1'])  / (\n                                              df_book_data['bid_size1']+ df_book_data[\n                                          'ask_size1'])\n        df_book_data['log_return'] = df_book_data.groupby(['time_id'])['wap'].apply(log_return)\n        df_book_data = df_book_data[~df_book_data['log_return'].isnull()]\n#     return df_book_data\n        df_realized_vol_per_stock =  pd.DataFrame(df_book_data.groupby(['time_id'])['log_return'].agg(realized_volatility)).reset_index()\n        df_realized_vol_per_stock = df_realized_vol_per_stock\n        stock_id = file.split('=')[1]\n        df_realized_vol_per_stock['row_id'] = df_realized_vol_per_stock['time_id'].apply(lambda x:f'{stock_id}-{x}')\n        df_past_realized = pd.concat([df_past_realized, df_realized_vol_per_stock[['row_id','log_return']]])\n    return df_past_realized[['row_id','log_return']]","metadata":{"execution":{"iopub.status.busy":"2021-07-09T14:19:11.195942Z","iopub.execute_input":"2021-07-09T14:19:11.196528Z","iopub.status.idle":"2021-07-09T14:19:11.213351Z","shell.execute_reply.started":"2021-07-09T14:19:11.196477Z","shell.execute_reply":"2021-07-09T14:19:11.212348Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nStratificationSplitter class\n\"\"\"\n\nclass StratificationSplitter:\n    def __init__(self, data, stratification_column_names, max_categories=10,\n                 test_size=None, val_size=None, n_train_cv_splits=3, random_state=None):\n    \n        logging.info(\n            \"Initializing StratificationSplitter with test_size {}, val_size {} and {} train data cross-validation \"\n            \"splits. Stratification by {} columns with max_categories {}\".format(\n                test_size if test_size else 0, val_size if val_size else 0, n_train_cv_splits,\n                stratification_column_names, max_categories\n            )\n        )\n        assert data.shape[0] > n_train_cv_splits\n        self._data = data[~data.index.duplicated(keep=\"last\")]\n        self.stratification_column_names = stratification_column_names\n        assert max_categories > 2\n        self.max_categories = max_categories\n        self.test_size = test_size\n        self.val_size = val_size\n        if test_size is not None:\n            self.test_size = math.ceil(self._data.shape[0] * test_size) if test_size < 1 else int(test_size)\n        if val_size is not None:\n            self.val_size = math.ceil(self._data.shape[0] * val_size) if val_size < 1 else int(val_size)\n        assert self.test_size is None or 0 < self.test_size <= self._data.shape[0] - n_train_cv_splits\n        assert self.val_size is None or 0 < self.val_size <= self._data.shape[0] - n_train_cv_splits\n        if test_size is not None and val_size is not None:\n            if self.test_size + self.val_size > data.shape[0] - n_train_cv_splits:\n                raise ValueError(\n                    \"Sum of test and validation sizes must be less than dataset size minus `n_train_cv_splits`\"\n                )\n        assert n_train_cv_splits >= 2\n        self.n_train_cv_splits = n_train_cv_splits\n        self.random_state = random_state\n        self._stratification = pd.DataFrame([], index=self._data.index.values)\n        self._train_ids = None\n        self._test_ids = None\n        self._val_ids = None\n        self._cv_ids = None\n        for col in stratification_column_names:\n            if col not in self._data.columns:\n                raise ValueError(\"Column `{}` wasn't found in dataset\".format(col))\n            self._add_stratification_column(self._data[col])\n        np.random.seed(self.random_state)\n        if self._stratification.empty:\n            random_col = np.zeros_like(self._stratification.index.values, dtype=np.int64)\n            random_col[:len(random_col) // 2] = 1\n            np.random.shuffle(random_col)\n            self._stratification[\"no_stratification (random)\"] = random_col\n\n    def _add_stratification_column(self, col):\n        unique, unique_counts = np.unique(col, return_counts=True)\n        if len(unique) <= self.max_categories and np.all(unique_counts >= self.n_train_cv_splits):\n            self._stratification[col.name] = col\n        elif len(unique) > self.max_categories and col.dtype.kind in {\"f\", \"u\", \"i\"}:\n            self._stratification[col.name] = pd.qcut(col, self.max_categories, duplicates=\"drop\")\n            buckets = list(zip(*np.unique(self._stratification[col.name], return_counts=True)))\n            logging.warning(\n                \"Column `{}` is numeric with more than {} unique values, so it was quantile-discretized \"\n                \"into {} buckets: {}\".format(col.name, self.max_categories, len(buckets), buckets)\n            )\n        elif ~np.all(unique_counts >= self.n_train_cv_splits):\n            print(\n                \"Column `{}` was removed from stratification because it had categories with less than {} members. The \"\n                \"minimum number of members in any category cannot be less than number of cross-validation \"\n                \"splits\".format(col.name, self.n_train_cv_splits)\n            )\n        else:\n            print(\n                \"Column `{}` was removed from stratification because it had more than {} categories and couldn't be \"\n                \"discretized\".format(col.name, self.max_categories)\n            )\n\n    def _split(self):\n        # train/test/validation split\n        logging.info(\"Creating train/test/validation set splits...\")\n        train_ids = self._stratification.index.values\n        all_train_test_val_ids = [self._stratification.index.values]\n        all_train_test_val_ids_names = [\"full dataset\"]\n        if self.test_size is not None:\n            train_ids, test_ids = train_test_split(\n                train_ids, stratify=self._stratification.loc[train_ids],\n                test_size=self.test_size, random_state=self.random_state\n            )\n            self._test_ids = test_ids\n            all_train_test_val_ids.append(test_ids)\n            all_train_test_val_ids_names.append(\"test\")\n        if self.val_size is not None:\n            train_ids, val_ids = train_test_split(\n                train_ids, stratify=self._stratification.loc[train_ids],\n                test_size=self.val_size, random_state=self.random_state\n            )\n            self._val_ids = val_ids\n            all_train_test_val_ids.append(val_ids)\n            all_train_test_val_ids_names.append(\"validation\")\n        self._train_ids = train_ids\n        all_train_test_val_ids.append(train_ids)\n        all_train_test_val_ids_names.append(\"train\")\n        # log stratification percentages\n        for feature in self._stratification.columns:\n            messages = list()\n            messages.append(\"Stratification on feature: %s\" % feature)\n            for i, ids in enumerate(all_train_test_val_ids):\n                messages.append(\n                    \"%s sample:  Shape: %s, statistics: %s\" % (\n                        all_train_test_val_ids_names[i].upper(), len(ids),\n                        (self._stratification.loc[ids, feature].value_counts().sort_index() / len(ids)).to_dict()\n                    )\n                )\n            logging.info(\"\\n\".join(messages))\n\n        # train cross validation split\n        logging.info(\"Creating KFold cross-validation train data splits...\")\n        cv = []\n        np.random.seed(self.random_state)\n        groups = self._stratification.loc[train_ids].groupby(list(self._stratification.columns)).groups.items()\n        reminder = 0\n        for key, idx in sorted(groups, key=lambda x: x[0]):\n            group_arr = idx.values.copy()\n            min_examples = int(len(group_arr) / self.n_train_cv_splits)\n            if min_examples < 1:\n                print(\n                    \"Combined category `{}` of columns {} cannot be split into cross-validation folds equally because \"\n                    \"the minimum number of members in any category cannot be less than number of cross-validation \"\n                    \"splits: {} < {}\".format(\n                        key, list(self._stratification.columns), len(group_arr), self.n_train_cv_splits\n                    )\n                )\n            np.random.shuffle(group_arr)\n            split = np.array_split(group_arr, self.n_train_cv_splits)\n            cv.append(split[self.n_train_cv_splits - reminder:] + split[0:self.n_train_cv_splits - reminder])\n            reminder = (reminder + len(group_arr)) % self.n_train_cv_splits\n            # log stratification percentages\n            message = (\n                \"KFold cross-validation stratification on combined category `{}` of columns {} split \"\n                \"sizes: {}\".format(key, list(self._stratification.columns), list(map(len, split)))\n            )\n            logging.info(message)\n        cv = list(map(np.concatenate, zip(*cv)))\n        fold_sizes = list(map(len, cv))\n        assert sum(fold_sizes) == len(train_ids)\n        message = \"KFold cross-validation split sizes: {}\".format(fold_sizes)\n        logging.info(message)\n        self._cv_ids = []\n        for idx in range(len(cv)):\n            val = cv[idx]\n            train = np.concatenate(cv[0:idx] + cv[idx+1:])\n            assert len(set(train).intersection(set(val))) == 0, \"No such indices that are in both train and val sets\"\n            self._cv_ids.append((train, val))\n\n    @property\n    def train_ids(self):\n        if self._train_ids is None:\n            self._split()\n        return self._train_ids\n\n    @property\n    def test_ids(self):\n        if self._test_ids is None and self.test_size is not None:\n            self._split()\n        return self._test_ids\n\n    @property\n    def val_ids(self):\n        if self._val_ids is None and self.val_size is not None:\n            self._split()\n        return self._val_ids\n\n    @property\n    def cv_ids(self):\n        if self._cv_ids is None:\n            self._split()\n        return self._cv_ids\n","metadata":{"execution":{"iopub.status.busy":"2021-08-16T15:49:40.95695Z","iopub.execute_input":"2021-08-16T15:49:40.957391Z","iopub.status.idle":"2021-08-16T15:49:40.969142Z","shell.execute_reply.started":"2021-08-16T15:49:40.957286Z","shell.execute_reply":"2021-08-16T15:49:40.96802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def format_time(seconds):\n    \"\"\"\n    Format time in seconds to time string, including minutes and hours when appropriate\n    :param seconds:                     float, seconds\n    :return:                            formatted time string\n    \"\"\"\n    if seconds < 60:\n        return \"0:{:0>2}\".format(int(seconds))\n    elif seconds < 3600:\n        minutes = int(seconds / 60)\n        seconds = int(seconds % 60)\n        return \"{}:{:0>2}\".format(minutes, seconds)\n    else:\n        hours = int(seconds / 3600)\n        minutes = int((seconds % 3600) / 60)\n        seconds = int((seconds % 3600) % 60)\n        return \"{}:{:0>2}:{:0>2}\".format(hours, minutes, seconds)","metadata":{"execution":{"iopub.status.busy":"2021-07-09T14:19:11.246686Z","iopub.execute_input":"2021-07-09T14:19:11.247283Z","iopub.status.idle":"2021-07-09T14:19:11.264067Z","shell.execute_reply.started":"2021-07-09T14:19:11.247237Z","shell.execute_reply":"2021-07-09T14:19:11.26309Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nFunctions for hyper-parameter optimization\n\"\"\"\n\ndef tune_hyperparameters_cv(estimator, x, y, search_spaces, optimizers, n_iter, scoring=None,\n                            cv=3, n_jobs=1, \n                            verbose_output=0, \n                            random_state=None, sample_weight=None, **kwargs):\n    allowed_optimizers = {\"grid\", \"random\"}\n    invalid_optimizers = [optimizer for optimizer in optimizers if optimizer not in allowed_optimizers]\n    if len(invalid_optimizers) > 0:\n        raise ValueError(\n            \"Values in `optimizers` must be one of {}, found: {}\".format(allowed_optimizers, invalid_optimizers)\n        )\n    if len(search_spaces) != len(optimizers):\n        raise ValueError(\n            \"`search_spaces` and `optimizers` must have the same length, found: \"\n            \"{} != {}\".format(len(search_spaces), len(optimizers))\n        )\n    n_steps = len(search_spaces)\n    cv_splits = cv if isinstance(cv, int) else len(cv)\n    processed_best_params = {}\n    processed_best_score = -float(\"inf\")\n    for step in range(n_steps):\n        step_start_time = time.time()\n        print(\n            \"Step {} of {} of hyper-parameter optimization \"\n            \"(`{}` optimizer)\".format(step + 1, n_steps, optimizers[step])\n        )\n        if optimizers[step] == \"grid\":\n            n_fits = reduce(mul, [len(par_values) for par_values in search_spaces[step].values()]) * cv_splits\n            optimizer_kwargs = {}\n            optimizer = GridSearchCV\n        else:\n            n_fits = n_iter * cv_splits\n            optimizer_kwargs = {\"random_state\": random_state, \"n_iter\": n_iter}\n            optimizer = RandomizedSearchCV\n        print(\"Models fitting time start: {}\".format(time.strftime('%H:%M:%S', time.gmtime(time.time()))))\n        print(\"Optimizer is fitting {} models...\".format(n_fits))\n        estimator_clone = clone(estimator)\n        estimator_clone.set_params(**processed_best_params)\n        model = optimizer(\n            estimator_clone, search_spaces[step], scoring=scoring, cv=cv,\n            n_jobs=n_jobs, refit=False, verbose=0, **optimizer_kwargs\n        )\n        model.fit(x, y, sample_weight, **kwargs)\n        iter_params = model.cv_results_[\"params\"]\n        iter_scores = model.cv_results_[\"mean_test_score\"]\n        iter_score_stds = model.cv_results_[\"std_test_score\"]\n        for iter_idx in range(len(iter_params)):\n            print(\n                \"Parameters {}: mean-score={:.12f}, \"\n                \"std-score={:.12f}\".format(iter_params[iter_idx], iter_scores[iter_idx], iter_score_stds[iter_idx])\n            )\n        print(\n            \"Step {} optimization time: {}\".format(step + 1, format_time(time.time() - step_start_time))\n        )\n        print(\n            \"Step {} completed. Best parameter tuning score is {:.12f} with \"\n            \"parameters: {}\".format(step + 1, model.best_score_, model.best_params_)\n        )\n        if model.best_score_ >= processed_best_score:\n            processed_best_score = model.best_score_\n            processed_best_params.update(model.best_params_)\n        else:\n            print(\n                \"Step {} best tuning score {:.12f} is worse than the previous step score {:.12f}, so parameters \"\n                \"will be discarded and the correspondent default parameter values from the previous step will \"\n                \"be used instead\".format(step + 1, model.best_score_, processed_best_score)\n            )\n    print(\n        \"Best parameter tuning score overall is {:.12f} with \"\n        \"parameters: {}\".format(processed_best_score, processed_best_params)\n    )\n    print(\"Refitting model on the whole train dataset with best parameters..\")\n    estimator.set_params(**processed_best_params)\n    estimator.fit(x, y, sample_weight, **kwargs)\n    print(\"Hyper-parameter tuning completed\")\n    return estimator, processed_best_params\n","metadata":{"execution":{"iopub.status.busy":"2021-07-09T14:19:11.265573Z","iopub.execute_input":"2021-07-09T14:19:11.265987Z","iopub.status.idle":"2021-07-09T14:19:11.282015Z","shell.execute_reply.started":"2021-07-09T14:19:11.265943Z","shell.execute_reply":"2021-07-09T14:19:11.281278Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_data_by_ids(features, ids, target_col):\n    samples = features.loc[ids, :]\n    y = samples[target_col].values\n    x = samples.drop(columns=[target_col])\n    return x, y\n\ndef get_indices_by_ids(features, ids):\n    \"\"\"\n    Retrieves indices of features for specified ids\n    :param features:                    DataFrame of all features with id index\n    :param ids:                         list of ids to retrieve indices for\n    :return:                            array of indices for requested ids\n    \"\"\"\n    features[\"indexing_column\"] = np.arange(features.shape[0], dtype=np.int64)\n    samples = features.loc[ids, :]\n    indices = samples[\"indexing_column\"].values\n    features.drop(columns=[\"indexing_column\"], inplace=True)\n    return indices","metadata":{"execution":{"iopub.status.busy":"2021-07-09T14:19:11.283306Z","iopub.execute_input":"2021-07-09T14:19:11.283849Z","iopub.status.idle":"2021-07-09T14:19:11.301326Z","shell.execute_reply.started":"2021-07-09T14:19:11.283808Z","shell.execute_reply":"2021-07-09T14:19:11.300413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ap = argparse.ArgumentParser(description=\"Regression model training with cross-validation\")\n# ap.add_argument(\"-v\", \"--verbose\", action=\"store_true\", help=\"flag for verbose output\")\n# args = ap.parse_args()","metadata":{"execution":{"iopub.status.busy":"2021-07-09T14:19:11.302674Z","iopub.execute_input":"2021-07-09T14:19:11.303047Z","iopub.status.idle":"2021-07-09T14:19:11.317264Z","shell.execute_reply.started":"2021-07-09T14:19:11.30301Z","shell.execute_reply":"2021-07-09T14:19:11.316042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_book = data_prep(list_order_book_file=list_order_book_file_train)\n\ndf_train = train.merge(df_book[['row_id','log_return']], on = ['row_id'], how = 'left')\ndf_train = df_train.set_index('row_id')\ndf_train.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-09T14:19:11.318684Z","iopub.execute_input":"2021-07-09T14:19:11.319089Z","iopub.status.idle":"2021-07-09T14:25:01.277188Z","shell.execute_reply.started":"2021-07-09T14:19:11.319038Z","shell.execute_reply":"2021-07-09T14:25:01.276289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df_train = df_book_data#.loc[df_book_data['time_id']==5]\n# df_wap['row_id'] = df_wap['time_id'].apply(lambda x:f'{stock_id}-{x}')\n\n# df_train[\"target\"] = df_train[\"log_return\"].copy()\n# df_train.head()\n","metadata":{"execution":{"iopub.status.busy":"2021-07-09T14:25:01.278592Z","iopub.execute_input":"2021-07-09T14:25:01.278968Z","iopub.status.idle":"2021-07-09T14:25:01.283182Z","shell.execute_reply.started":"2021-07-09T14:25:01.278929Z","shell.execute_reply":"2021-07-09T14:25:01.282279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"splitter = StratificationSplitter(\n        df_train, [\"target\"], val_size=0.1,\n        test_size=0.2, n_train_cv_splits=10, max_categories=30,\n        # random_state=42\n    )","metadata":{"execution":{"iopub.status.busy":"2021-07-09T14:25:01.284439Z","iopub.execute_input":"2021-07-09T14:25:01.284716Z","iopub.status.idle":"2021-07-09T14:25:05.537693Z","shell.execute_reply.started":"2021-07-09T14:25:01.28468Z","shell.execute_reply":"2021-07-09T14:25:05.53678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"NON_FEATURE_COLUMNS = [] #\"time_id\", \"wap\", \"log_return\"\nfeature_columns = [col for col in df_train.columns if col not in NON_FEATURE_COLUMNS]\nnon_feature_data = df_train[NON_FEATURE_COLUMNS + [\"target\"]].rename(columns={\"target\": \"y_true\"})\ndf_train = df_train[feature_columns]","metadata":{"execution":{"iopub.status.busy":"2021-07-09T14:25:05.54027Z","iopub.execute_input":"2021-07-09T14:25:05.540558Z","iopub.status.idle":"2021-07-09T14:25:05.552233Z","shell.execute_reply.started":"2021-07-09T14:25:05.540521Z","shell.execute_reply":"2021-07-09T14:25:05.551169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train, y_train = get_data_by_ids(df_train, splitter.train_ids, \"target\")\nx_val, y_val = get_data_by_ids(df_train, splitter.val_ids, \"target\")\nx_test, y_test = get_data_by_ids(df_train, splitter.test_ids, \"target\")\ncv_splits = []\nfor train_ids, val_ids in splitter.cv_ids:\n    train_idxs = get_indices_by_ids(x_train, train_ids)\n    val_idxs = get_indices_by_ids(x_train, val_ids)\n    cv_splits.append((train_idxs, val_idxs))","metadata":{"execution":{"iopub.status.busy":"2021-07-09T14:25:05.553959Z","iopub.execute_input":"2021-07-09T14:25:05.554286Z","iopub.status.idle":"2021-07-09T14:25:19.909195Z","shell.execute_reply.started":"2021-07-09T14:25:05.554259Z","shell.execute_reply":"2021-07-09T14:25:19.908196Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"XGB_PARAMETERS = {\n            \"booster\": \"gbtree\", \"verbosity\": 1,\n            \"objective\": \"reg:squaredlogerror\", \"eval_metric\": [\"rmse\",\"rmsle\"],\n            \"importance_type\": \"gain\",\n            \"learning_rate\": 0.05, \"n_estimators\": 2000,\n            \"max_depth\": 5, \"min_child_weight\": 6, \"gamma\": 0,\n            \"subsample\": 0.8, \"colsample_bytree\": 0.2,\n            \"reg_lambda\": 10, \"reg_alpha\": 10\n        }\nHYPER_PARAMETER_TUNE_RANDOM_N_ITER = 20\nXGB_TUNABLE_PARAMETERS_STEP_1 = {\"max_depth\": [4], \"min_child_weight\": [6]}#list(range(1, 8))\nXGB_TUNABLE_PARAMETERS_STEP_2 = {\n        \"gamma\": stats.uniform(0, 10), \"colsample_bytree\": stats.uniform(0.01, 0.99), \"subsample\": stats.uniform(0.7, 0.3)\n    }\n# XGB_TUNABLE_PARAMETERS_STEP_3 = {\"reg_lambda\": LogUniform(1, 1000), \"reg_alpha\": LogUniform(1, 1000)}\nXGB_TUNABLE_PARAMETERS_STEP_4 = {\"learning_rate\": [0.4, 0.2, 0.1, 0.06, 0.03, 0.01, 0.005]}\nXGB_TUNABLE_PARAMETERS = [\n        XGB_TUNABLE_PARAMETERS_STEP_1,\n#         XGB_TUNABLE_PARAMETERS_STEP_2,\n#         XGB_TUNABLE_PARAMETERS_STEP_3,\n#         XGB_TUNABLE_PARAMETERS_STEP_4\n    ]\nXGB_TUNING_OPTIMIZERS = [\"grid\"]# ,\"random\", \"grid\"","metadata":{"execution":{"iopub.status.busy":"2021-07-09T14:25:19.910758Z","iopub.execute_input":"2021-07-09T14:25:19.91116Z","iopub.status.idle":"2021-07-09T14:25:19.921502Z","shell.execute_reply.started":"2021-07-09T14:25:19.911119Z","shell.execute_reply":"2021-07-09T14:25:19.920404Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"estimator = xgb.sklearn.XGBRegressor(**XGB_PARAMETERS)","metadata":{"execution":{"iopub.status.busy":"2021-07-09T14:25:19.922652Z","iopub.execute_input":"2021-07-09T14:25:19.92293Z","iopub.status.idle":"2021-07-09T14:25:19.935076Z","shell.execute_reply.started":"2021-07-09T14:25:19.922905Z","shell.execute_reply":"2021-07-09T14:25:19.93399Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model, best_params = tune_hyperparameters_cv(\n    \n    estimator, x_train, y_train, XGB_TUNABLE_PARAMETERS, XGB_TUNING_OPTIMIZERS,\n    HYPER_PARAMETER_TUNE_RANDOM_N_ITER, scoring=make_scorer(mean_squared_error, greater_is_better=False),\n    n_jobs=1, cv=cv_splits, #verbose_output=args.verbose,\n    # model key-word params:\n    eval_set=[(x_val, y_val)], early_stopping_rounds=50, verbose=False,\n    # random_state=42\n        )\nXGB_PARAMETERS.update(best_params)","metadata":{"execution":{"iopub.status.busy":"2021-07-09T14:25:19.936162Z","iopub.execute_input":"2021-07-09T14:25:19.936508Z","iopub.status.idle":"2021-07-09T14:27:40.949551Z","shell.execute_reply.started":"2021-07-09T14:25:19.936477Z","shell.execute_reply":"2021-07-09T14:27:40.948774Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train_pred = model.predict(x_train, ntree_limit=model.best_ntree_limit)\ny_val_pred = model.predict(x_val, ntree_limit=model.best_ntree_limit)\ny_test_pred = model.predict(x_test, ntree_limit=model.best_ntree_limit)","metadata":{"execution":{"iopub.status.busy":"2021-07-09T14:27:40.952807Z","iopub.execute_input":"2021-07-09T14:27:40.953299Z","iopub.status.idle":"2021-07-09T14:27:41.117059Z","shell.execute_reply.started":"2021-07-09T14:27:40.953265Z","shell.execute_reply":"2021-07-09T14:27:41.116257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# non_feature_data_test = non_feature_data.loc[splitter.test_ids].copy()\n# non_feature_data_test[\"y_pred\"] = y_test_pred","metadata":{"execution":{"iopub.status.busy":"2021-07-09T14:27:41.118302Z","iopub.execute_input":"2021-07-09T14:27:41.118834Z","iopub.status.idle":"2021-07-09T14:27:41.122619Z","shell.execute_reply.started":"2021-07-09T14:27:41.1188Z","shell.execute_reply":"2021-07-09T14:27:41.121734Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_mae = mean_absolute_error(y_train, y_train_pred)\ntrain_mse = mean_squared_error(y_train, y_train_pred)\ntrain_r_2 = r2_score(y_train, y_train_pred)\nval_mae = mean_absolute_error(y_val, y_val_pred)\nval_mse = mean_squared_error(y_val, y_val_pred)\nval_r_2 = r2_score(y_val, y_val_pred)\ntest_mae = mean_absolute_error(y_test, y_test_pred)\ntest_mse = mean_squared_error(y_test, y_test_pred)\ntest_r_2 = r2_score(y_test, y_test_pred)\nprint(\n        \"Regression model, model trees={}, train_mae={:.12f}, train_mse={:.12f}, train_rmse={:.12f}, \"\n        \"train_r_2={:.12f}, val_mae={:.12f}, val_mse={:.12f}, val_rmse={:.12f}, val_r_2={:.12f},\"\n        \"test_mae={:.12f}, test_mse={:.12f}, test_rmse={:.12f}, test_r_2={:.12f}\".format(\n            model.best_ntree_limit, train_mae, train_mse, np.sqrt(train_mse), train_r_2,\n            val_mae, val_mse, np.sqrt(val_mse), val_r_2,\n            test_mae, test_mse, np.sqrt(test_mse), test_r_2\n        )\n    )","metadata":{"execution":{"iopub.status.busy":"2021-07-09T14:27:41.123795Z","iopub.execute_input":"2021-07-09T14:27:41.124082Z","iopub.status.idle":"2021-07-09T14:27:41.15087Z","shell.execute_reply.started":"2021-07-09T14:27:41.124057Z","shell.execute_reply":"2021-07-09T14:27:41.14985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def rmspe(y_true, y_pred):\n    return  (np.sqrt(np.mean(np.square((y_true - y_pred) / y_true))))\n# (np.sqrt(np.mean(np.square((y_true - y_pred) / y_true))))\n\n\n\nR2 = round(r2_score(y_true = y_test, y_pred = y_test_pred),3)\nRMSPE = round(rmspe(y_true = y_test, y_pred = y_test_pred),3)\nprint(f'Performance: R2 score: {R2}, RMSPE: {RMSPE}')","metadata":{"execution":{"iopub.status.busy":"2021-07-09T14:27:41.152248Z","iopub.execute_input":"2021-07-09T14:27:41.152626Z","iopub.status.idle":"2021-07-09T14:27:41.161529Z","shell.execute_reply.started":"2021-07-09T14:27:41.152586Z","shell.execute_reply":"2021-07-09T14:27:41.160232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# submit data","metadata":{"execution":{"iopub.status.busy":"2021-07-04T11:57:54.727154Z","iopub.execute_input":"2021-07-04T11:57:54.727495Z","iopub.status.idle":"2021-07-04T11:57:54.732867Z","shell.execute_reply.started":"2021-07-04T11:57:54.727466Z","shell.execute_reply":"2021-07-04T11:57:54.731875Z"}}},{"cell_type":"code","source":"list_order_book_file_test = glob.glob('/kaggle/input/optiver-realized-volatility-prediction/book_test.parquet/*')\ntest = pd.read_csv('../input/optiver-realized-volatility-prediction/test.csv')\n\n# for file in list_order_book_file_test:\n#     df = pd.read_parquet(file)\n#     print(df.head())\ndf_book = data_prep(list_order_book_file=list_order_book_file_test)\ndf_test = test.merge(df_book[['row_id','log_return']], on = ['row_id'], how = 'left').fillna(0)\ndf_test = df_test.set_index('row_id')\n\nNON_FEATURE_COLUMNS = ['stock_id','time_id']\nfeature_columns = [col for col in df_test.columns if col not in NON_FEATURE_COLUMNS]\ndf_test = df_test[feature_columns]\n# df_test = df_test.set_index('row_id')\n\ny_test_pred = model.predict(df_test, ntree_limit=model.best_ntree_limit)\n\ndf_test = df_test.reset_index()\ndf_test['pred'] = pd.DataFrame(y_test_pred)\ndf_test[['row_id','pred']].to_csv('submission.csv',index = False)\ndf_test[['row_id','pred']]\n# df_test","metadata":{"execution":{"iopub.status.busy":"2021-07-09T14:43:48.539721Z","iopub.execute_input":"2021-07-09T14:43:48.540298Z","iopub.status.idle":"2021-07-09T14:43:48.619124Z","shell.execute_reply.started":"2021-07-09T14:43:48.540251Z","shell.execute_reply":"2021-07-09T14:43:48.618166Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}