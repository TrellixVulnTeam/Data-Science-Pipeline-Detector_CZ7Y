{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"For submission, I am going to use gradient boosting from sklearn. For study, please refer to this book:https://www.kaggle.com/pkang0831/multiple-model-tryouts","metadata":{}},{"cell_type":"code","source":"# import necessary libraries: this will get updated as I go along\nimport pandas as pd\nimport numpy as np\nimport plotly.express as px\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn import metrics, model_selection, preprocessing\nimport warnings \nwarnings.filterwarnings('ignore')\n%matplotlib inline","metadata":{"execution":{"iopub.status.busy":"2021-08-05T04:23:04.79361Z","iopub.execute_input":"2021-08-05T04:23:04.794245Z","iopub.status.idle":"2021-08-05T04:23:07.977111Z","shell.execute_reply.started":"2021-08-05T04:23:04.794137Z","shell.execute_reply":"2021-08-05T04:23:07.975921Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_csv_route = '../input/optiver-realized-volatility-prediction/train.csv'\ntest_csv_route = '../input/optiver-realized-volatility-prediction/test.csv'\nbook_train = pd.read_parquet('../input/optiver-realized-volatility-prediction/book_train.parquet')\nbook_test = pd.read_parquet('../input/optiver-realized-volatility-prediction/book_test.parquet')\ntrade_train = pd.read_parquet('../input/optiver-realized-volatility-prediction/trade_train.parquet')\ntrade_test = pd.read_parquet('../input/optiver-realized-volatility-prediction/trade_test.parquet')\n\ntrain = pd.read_csv(train_csv_route)\ntest = pd.read_csv(test_csv_route)\ndata = {\n    'book_train':book_train,\n    'book_test':book_test,\n    'trade_train':trade_train,\n    'trade_test':trade_test,\n    'train':train,\n    'test':test\n}","metadata":{"execution":{"iopub.status.busy":"2021-08-05T04:23:07.97882Z","iopub.execute_input":"2021-08-05T04:23:07.979194Z","iopub.status.idle":"2021-08-05T04:23:25.403266Z","shell.execute_reply.started":"2021-08-05T04:23:07.979162Z","shell.execute_reply":"2021-08-05T04:23:25.401969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# configuring class that groups the functions\nclass Optiver_feature_engineered:\n    \n    \"\"\"\n    it is a collection of the features... docstring work in progress. \n    \"\"\"\n    TODO: 'complete docstring for this'\n    \n    def __init__(self,df=None,df_name=None):\n        self.df = df\n        self.df_name = df_name\n        \n    def BAS(self,ask_price,bid_price):\n        return [ask_p/bid_p - 1 for ask_p,bid_p in zip(ask_price,bid_price)]\n\n    def WAP(self,df):\n        wap = (df[df.columns[0]] * df[df.columns[1]] + df[df.columns[2]]*df[df.columns[3]])/(df[df.columns[1]]+df[df.columns[3]])\n        return wap\n\n    def log_return(self,list_stock_prices):\n        return np.log(list_stock_prices).diff() \n\n    def realized_volatility(self,series_log_return):\n        return np.sqrt(np.sum(series_log_return**2))","metadata":{"execution":{"iopub.status.busy":"2021-08-05T04:23:25.405437Z","iopub.execute_input":"2021-08-05T04:23:25.405824Z","iopub.status.idle":"2021-08-05T04:23:25.417097Z","shell.execute_reply.started":"2021-08-05T04:23:25.405781Z","shell.execute_reply":"2021-08-05T04:23:25.415623Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# preprocessing for booking dataset:\n# BAS\n# WAP\n# Log return\n# Calculated volatility\n\nbook_train = book_train[book_train['stock_id']==0]\ntrade_train = trade_train[trade_train['stock_id']==0]\ntrain = train[train['stock_id']==0]\n\nfe = Optiver_feature_engineered()\ndef preprocessings_book(df):\n    \n    df['seconds_in_bucket'] = df['seconds_in_bucket'] + 1 # cuz 0 seconds will mess up the data internally\n    df['seconds_bids'] = df['seconds_in_bucket']*(df['bid_size1']+df['bid_size2'])\n    df['seconds_asks'] = df['seconds_in_bucket']*(df['ask_size1']+df['ask_size2'])\n    df['BAS1'] = fe.BAS(df['ask_price1'],df['bid_price1'])\n    df['BAS2'] = fe.BAS(df['ask_price2'],df['bid_price2'])\n    df['WAP1'] = fe.WAP(df[['bid_price1','ask_size1','ask_price1','bid_size1']])\n    df['WAP2'] = fe.WAP(df[['bid_price2','ask_size2','ask_price2','bid_size2']])\n    df['logr1'] = df.groupby(['time_id'])['WAP1'].apply(fe.log_return)\n    df['logr2'] = df.groupby(['time_id'])['WAP2'].apply(fe.log_return)\n    apply_functions = {\"seconds_in_bucket\":\"mean\",\n                       \"bid_price1\":\"mean\",\n                       \"bid_price2\":\"mean\",\n                       \"ask_price1\":\"mean\",\n                       \"ask_price2\":\"mean\",\n                       \"BAS1\":\"mean\",\n                       \"BAS2\":\"mean\",\n                       \"WAP1\":\"mean\", # null values to be ignored when taking mean\n                       \"WAP2\":\"mean\", # null values to be ignored when taking mean\n                       \"logr1\":\"mean\",\n                       \"logr2\":\"mean\",\n                       \"seconds_bids\":\"sum\",\n                       \"seconds_asks\":\"sum\",\n                       'bid_size1':\"sum\",\n                       'bid_size2':\"sum\",\n                       'ask_size1':\"sum\",\n                       'ask_size2':\"sum\"\n                      }\n    df_feature = df.groupby(['time_id']).agg(apply_functions)\n    df_feature['vol_1'] = df.groupby(['time_id'])['logr1'].apply(fe.realized_volatility)\n    df_feature['vol_2'] = df.groupby(['time_id'])['logr2'].apply(fe.realized_volatility)\n    df_feature['seconds_bids'] = df_feature['seconds_bids']/(df_feature['bid_size1'] + df_feature['bid_size2'])\n    df_feature['seconds_asks'] = df_feature['seconds_asks']/(df_feature['ask_size1'] + df_feature['ask_size2'])\n    df_feature.reset_index(inplace=True)\n    df_feature.drop(columns='seconds_in_bucket',axis=1,inplace=True)\n    return df_feature\n\n# Preprocessing for trading dataset:\n\ndef preprocessings_trade(df):\n\n    df['seconds_in_bucket'] = df['seconds_in_bucket'] + 1\n    df['seconds_size'] = df['seconds_in_bucket']*df['size']\n    df['logr_p'] = df.groupby(['time_id'])['price'].apply(fe.log_return)\n    apply_func = {\n        'order_count':'sum',\n        'seconds_in_bucket':'mean',\n        'size':'sum',\n        'seconds_size':'sum',\n        'price':'mean',\n        'logr_p':'mean'\n    }\n\n    df_feature = df.groupby(['time_id']).agg(apply_func)\n    df_feature['spread'] = df.groupby(['time_id'])['price'].max() - df.groupby(['time_id'])['price'].min()\n    df_feature['vol_p'] = df.groupby(['time_id'])['logr_p'].apply(fe.realized_volatility)\n    df_feature['seconds_size'] = df_feature['seconds_size']/df_feature['size']\n    df_feature.reset_index(inplace=True)\n    df_feature.drop(columns='seconds_in_bucket',axis=1,inplace=True)\n    return df_feature\n\nbook_train_feature = preprocessings_book(book_train)\nbook_test_feature = preprocessings_book(book_test)\ntrade_train_feature = preprocessings_trade(trade_train)\ntrade_test_feature = preprocessings_trade(trade_test)\n\n\ndataset = pd.merge(book_train_feature,trade_train_feature,how='left',on=['time_id'])\ndf = pd.merge(dataset,train,how='right',on=['time_id'])\ndf.drop(columns='stock_id',inplace=True)\ndf.dropna(inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-08-05T04:23:25.419815Z","iopub.execute_input":"2021-08-05T04:23:25.420184Z","iopub.status.idle":"2021-08-05T04:23:36.70747Z","shell.execute_reply.started":"2021-08-05T04:23:25.420151Z","shell.execute_reply":"2021-08-05T04:23:36.706702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = pd.merge(book_train_feature,trade_train_feature,how='left',on=['time_id'])\ndf = pd.merge(dataset,train,how='right',on=['time_id'])\ndf.drop(columns='stock_id',inplace=True)\ndf.dropna(inplace=True)\ndataset_val = pd.merge(book_test_feature,trade_test_feature,how='left',on=['time_id'])\ndf_val = pd.merge(dataset_val,test,how='right',on=['time_id'])\ndf_val.drop(columns=['stock_id','row_id'],inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-08-05T04:23:36.70914Z","iopub.execute_input":"2021-08-05T04:23:36.709872Z","iopub.status.idle":"2021-08-05T04:23:36.751834Z","shell.execute_reply.started":"2021-08-05T04:23:36.709821Z","shell.execute_reply":"2021-08-05T04:23:36.750795Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn import ensemble\nalg = ensemble.GradientBoostingRegressor(\n    alpha=0.9,\n    criterion='friedman_mse',\n    learning_rate=0.05,\n    loss='huber',\n    max_depth=3,\n    max_features='sqrt',\n    min_samples_leaf=10,\n    min_samples_split=20,\n    n_estimators=150,\n    subsample=1.0,\n    validation_fraction=0.2\n)","metadata":{"execution":{"iopub.status.busy":"2021-08-05T04:23:36.753363Z","iopub.execute_input":"2021-08-05T04:23:36.753802Z","iopub.status.idle":"2021-08-05T04:23:36.966645Z","shell.execute_reply.started":"2021-08-05T04:23:36.753756Z","shell.execute_reply":"2021-08-05T04:23:36.965341Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split, cross_validate\nfrom sklearn import preprocessing\n\n# Split the data\n# X_train, X_test, Y_train, Y_test, = train_test_split(df.drop(columns='target',axis=1),df['target'], test_size = .33)\nX_train, X_test, Y_train, Y_test, = train_test_split(df.drop(columns='target',axis=1),df['target'], test_size = .2)\n\n\n# Normalizing the train,test predictor variables.\nscaler = preprocessing.StandardScaler()\n\n# Normalize the train predictors\nX_train[X_train.columns] = scaler.fit_transform(X_train[X_train.columns]) \n\n# Apply normalization traits to the test predictors\nX_test[X_test.columns] = scaler.transform(X_test[X_test.columns])\n\nprint(f'Train dataset shape: {X_train.shape}')\nprint(f'Test dataset shape: {X_test.shape}')\nprint(f'Train target dataset shape: {Y_train.shape}')\nprint(f'Test target dataset shape: {Y_test.shape}')\n\ndef rmspe(y_true, y_pred):\n    '''\n    Compute Root Mean Square Percentage Error between two arrays.\n    '''\n    loss = np.sqrt(np.mean(np.square(((y_true - y_pred) / y_true))))\n\n    return loss\n\nrmspe_loss = metrics.make_scorer(rmspe)","metadata":{"execution":{"iopub.status.busy":"2021-08-05T04:23:36.968514Z","iopub.execute_input":"2021-08-05T04:23:36.969064Z","iopub.status.idle":"2021-08-05T04:23:37.042538Z","shell.execute_reply.started":"2021-08-05T04:23:36.969007Z","shell.execute_reply":"2021-08-05T04:23:37.041344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scoring = {\n    'r2': 'r2',\n    'rmspe': rmspe_loss\n}\ncv_results = cross_validate(alg, X_train, Y_train, return_train_score=True, scoring=scoring)\n# fitting\nmodel = alg.fit(X_train,Y_train)\n# filling NaN with 0 so that model can handle it\ndf_val.fillna(0,inplace=True)\n# produce prediction\npredicted = model.predict(df_val)","metadata":{"execution":{"iopub.status.busy":"2021-08-05T04:23:37.044739Z","iopub.execute_input":"2021-08-05T04:23:37.045062Z","iopub.status.idle":"2021-08-05T04:23:43.46569Z","shell.execute_reply.started":"2021-08-05T04:23:37.045031Z","shell.execute_reply":"2021-08-05T04:23:43.464505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.concat([test.drop(['stock_id','time_id'],axis=1),pd.DataFrame(predicted,columns=['target'])],axis=1)\nsubmission","metadata":{"execution":{"iopub.status.busy":"2021-08-05T04:23:43.467577Z","iopub.execute_input":"2021-08-05T04:23:43.468072Z","iopub.status.idle":"2021-08-05T04:23:43.493013Z","shell.execute_reply.started":"2021-08-05T04:23:43.46802Z","shell.execute_reply":"2021-08-05T04:23:43.49178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv('submission.csv',index = False)","metadata":{"execution":{"iopub.status.busy":"2021-08-05T04:23:43.494614Z","iopub.execute_input":"2021-08-05T04:23:43.495106Z","iopub.status.idle":"2021-08-05T04:23:43.505398Z","shell.execute_reply.started":"2021-08-05T04:23:43.495056Z","shell.execute_reply":"2021-08-05T04:23:43.504169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}