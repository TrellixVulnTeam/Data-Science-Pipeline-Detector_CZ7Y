{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Go Deeper and Deeper! [Optimise RV]\n\nHow far can we go without using any machine learning (ML) models and just try the realised volatility (RV) function? The answer is it is much deeper than you think.\n\nIn this notebook, I show how to optimise the number of seconds in bucket we should consider for calculating the RV value for each stock. The insight is that old data may provide less importance to the future RV value that can be ignored.\n\nYou may consider this as a feature into your model.\n\nReferences:\n\n[We need to go deeper - and validate!][1]\n\n[1]: https://www.kaggle.com/konradb/we-need-to-go-deeper-and-validate","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport glob, gc\n\nfrom joblib import Parallel, delayed\nfrom tqdm.auto import tqdm\nimport os","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-04T19:37:06.321791Z","iopub.execute_input":"2021-08-04T19:37:06.322938Z","iopub.status.idle":"2021-08-04T19:37:06.590408Z","shell.execute_reply.started":"2021-08-04T19:37:06.322767Z","shell.execute_reply":"2021-08-04T19:37:06.589527Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_path = '../input/optiver-realized-volatility-prediction/'","metadata":{"execution":{"iopub.status.busy":"2021-08-04T19:37:06.591746Z","iopub.execute_input":"2021-08-04T19:37:06.592232Z","iopub.status.idle":"2021-08-04T19:37:06.596742Z","shell.execute_reply.started":"2021-08-04T19:37:06.592186Z","shell.execute_reply":"2021-08-04T19:37:06.595463Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('../input/optiver-realized-volatility-prediction/train.csv')\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-04T19:37:06.598879Z","iopub.execute_input":"2021-08-04T19:37:06.599221Z","iopub.status.idle":"2021-08-04T19:37:06.917918Z","shell.execute_reply.started":"2021-08-04T19:37:06.59919Z","shell.execute_reply":"2021-08-04T19:37:06.916961Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def calculate_wap(df):\n    a1 = df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']\n    b1 = df['bid_size1'] + df['ask_size1']\n    a2 = df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2']\n    b2 = df['bid_size2']+ df['ask_size2']\n    return (a1 + a2) / (b1 + b2), (a1 / b1 + a2 / b2) / 2\n\ndef log_return(list_stock_prices):\n    return np.log(list_stock_prices).diff()\n\ndef realized_volatility(series_log_return):\n    return np.sqrt(np.sum(series_log_return ** 2))","metadata":{"execution":{"iopub.status.busy":"2021-08-04T19:37:06.920071Z","iopub.execute_input":"2021-08-04T19:37:06.920425Z","iopub.status.idle":"2021-08-04T19:37:06.928745Z","shell.execute_reply.started":"2021-08-04T19:37:06.92039Z","shell.execute_reply":"2021-08-04T19:37:06.927187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def rmspe(predictions, targets):\n    return np.sqrt((((predictions - targets) / targets) ** 2).mean())","metadata":{"execution":{"iopub.status.busy":"2021-08-04T19:37:06.930796Z","iopub.execute_input":"2021-08-04T19:37:06.931274Z","iopub.status.idle":"2021-08-04T19:37:06.941536Z","shell.execute_reply.started":"2021-08-04T19:37:06.931226Z","shell.execute_reply":"2021-08-04T19:37:06.94063Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Optimise the Portion Number","metadata":{}},{"cell_type":"code","source":"def optimise_num(df : pd.DataFrame, stock_id : int, dataType = 'train'):\n    \n    book_train_subset = pd.read_parquet(data_path + f'book_{dataType}.parquet/stock_id={stock_id}/')\n    book_train_subset.sort_values(by = ['time_id', 'seconds_in_bucket'])\n    book_train_subset['wap1'], book_train_subset['wap2'] = calculate_wap(book_train_subset)\n\n    book_train_subset['log_return1'] = (book_train_subset.groupby(by = ['time_id'])['wap1'].\n                                       apply(log_return).\n                                       reset_index(drop = True).\n                                       fillna(0)\n                                      )\n    \n    book_train_subset['log_return2'] = (book_train_subset.groupby(by = ['time_id'])['wap2'].\n                                       apply(log_return).\n                                       reset_index(drop = True).\n                                       fillna(0)\n                                      )\n    \n    best_rmspe = np.inf\n    for sec in tqdm(np.arange(0, 600, 10), leave = False):\n        book_train = book_train_subset[book_train_subset['seconds_in_bucket'] >= sec]\n        stock_stat = pd.concat([\n            book_train.groupby(['time_id'])['log_return1'].agg(realized_volatility).rename('rv1_new'),\n            book_train.groupby(['time_id'])['log_return2'].agg(realized_volatility).rename('rv2_new'),\n            ], \n            axis = 1, \n        ).reset_index()\n        stock_stat['rv_new'] = (stock_stat['rv1_new'] + stock_stat['rv2_new']) / 2\n        stock_stat = stock_stat.merge(df.loc[df['stock_id'] == stock_id, ['time_id', 'target']], on = 'time_id', how = 'left')\n        rmspe_score = rmspe(stock_stat['rv_new'], stock_stat['target'])\n        if rmspe_score < best_rmspe:\n            best_rmspe = rmspe_score\n            best_sec = sec\n    print(stock_id, best_sec, best_rmspe)\n    return best_sec","metadata":{"execution":{"iopub.status.busy":"2021-08-04T19:37:06.942818Z","iopub.execute_input":"2021-08-04T19:37:06.943322Z","iopub.status.idle":"2021-08-04T19:37:06.957822Z","shell.execute_reply.started":"2021-08-04T19:37:06.943261Z","shell.execute_reply":"2021-08-04T19:37:06.956757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_stock_nums(df : pd.DataFrame, stock_ids : list, dataType = 'train', parallel = False):\n    if parallel:\n        nums = Parallel(n_jobs = -1)(\n            delayed(optimise_num)(df, stock_id, dataType) \n            for stock_id in tqdm(stock_ids, total = len(stock_ids))\n        )\n    else:\n        nums = []\n        for stock_id in tqdm(stock_ids, total = len(stock_ids)):\n            nums.append(optimise_num(df, stock_id, dataType))\n    stock_nums = dict(zip(stock_ids, nums))\n    return stock_nums","metadata":{"execution":{"iopub.status.busy":"2021-08-04T19:37:06.959228Z","iopub.execute_input":"2021-08-04T19:37:06.959753Z","iopub.status.idle":"2021-08-04T19:37:06.978374Z","shell.execute_reply.started":"2021-08-04T19:37:06.959705Z","shell.execute_reply":"2021-08-04T19:37:06.977325Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stock_secs = generate_stock_nums(df = train, stock_ids = train['stock_id'].unique(), dataType = 'train', parallel = True)","metadata":{"execution":{"iopub.status.busy":"2021-08-04T19:37:06.98079Z","iopub.execute_input":"2021-08-04T19:37:06.981327Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(stock_secs)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = gc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Validate on the Train Set","metadata":{}},{"cell_type":"code","source":"def get_stock_stat(stock_id : int, dataType = 'train'):\n    \n    book_train_subset = pd.read_parquet(data_path + f'book_{dataType}.parquet/stock_id={stock_id}/')\n    book_train_subset.sort_values(by = ['time_id', 'seconds_in_bucket'])\n    book_train_subset['wap1'], book_train_subset['wap2'] = calculate_wap(book_train_subset)\n\n    book_train_subset['log_return1'] = (book_train_subset.groupby(by = ['time_id'])['wap1'].\n                                       apply(log_return).\n                                       reset_index(drop = True).\n                                       fillna(0)\n                                      )\n    \n    book_train_subset['log_return2'] = (book_train_subset.groupby(by = ['time_id'])['wap2'].\n                                       apply(log_return).\n                                       reset_index(drop = True).\n                                       fillna(0)\n                                      )\n    \n    book_train = book_train_subset[book_train_subset['seconds_in_bucket'] >= stock_secs[stock_id]]\n    stock_stat = pd.concat([\n        book_train.groupby(['time_id'])['log_return1'].agg(realized_volatility).rename('rv1_new'),\n        book_train.groupby(['time_id'])['log_return2'].agg(realized_volatility).rename('rv2_new'),\n        ], \n        axis = 1, \n    ).reset_index()\n    stock_stat['rv_new'] = (stock_stat['rv1_new'] + stock_stat['rv2_new']) / 2\n    stock_stat['stock_id'] = stock_id\n    return stock_stat[['stock_id', 'time_id', 'rv_new']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_dataSet(stock_ids : list, dataType = 'train', parallel = False):\n    if parallel:\n        stock_stat = Parallel(n_jobs = -1)(\n            delayed(get_stock_stat)(stock_id, dataType) \n            for stock_id in tqdm(stock_ids, total = len(stock_ids))\n        )\n    else:\n        stock_stat = []\n        for stock_id in tqdm(stock_ids, total = len(stock_ids)):\n            stock_stat.append(get_stock_stat(stock_id, dataType))\n    stock_stat_df = pd.concat(stock_stat, ignore_index = True)\n    return stock_stat_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataSet = get_dataSet(stock_ids = train['stock_id'].unique(), dataType = 'train', parallel = True)\ntrain_dataSet = pd.merge(train, train_dataSet, on = ['stock_id', 'time_id'], how = 'left')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(rmspe(train_dataSet['rv_new'], train_dataSet['target']))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del train_dataSet\nx = gc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Predict on the Test Set","metadata":{}},{"cell_type":"code","source":"test = pd.read_csv('../input/optiver-realized-volatility-prediction/test.csv')\ntest.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dataSet = get_dataSet(stock_ids = test['stock_id'].unique(), dataType = 'test', parallel = True)\ntest_dataSet = pd.merge(test, test_dataSet, on = ['stock_id', 'time_id'], how = 'left')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = gc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submit","metadata":{}},{"cell_type":"code","source":"sub = pd.DataFrame()\nsub[['row_id', 'target']] = test_dataSet[['row_id', 'rv_new']]\nsub.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub.to_csv('submission.csv', index = False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}