{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# LSTM Baseline","metadata":{}},{"cell_type":"markdown","source":"This is my first LSTM model. I tried to describe what I have done and hope for some helpful feedback and an upvote if you like.\nPlease feel free to fork this notebook.","metadata":{}},{"cell_type":"code","source":"import glob\nimport numpy as np\nimport os\nimport pandas as pd\nimport tensorflow as tf\n\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.metrics import r2_score\nfrom sklearn.model_selection import GroupShuffleSplit\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\n\nfrom tensorflow import keras\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-18T21:11:46.935409Z","iopub.execute_input":"2021-07-18T21:11:46.93594Z","iopub.status.idle":"2021-07-18T21:11:53.68589Z","shell.execute_reply.started":"2021-07-18T21:11:46.935856Z","shell.execute_reply":"2021-07-18T21:11:53.684979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"list_order_book_file_train = glob.glob('/kaggle/input/optiver-realized-volatility-prediction/book_train.parquet/*')","metadata":{"execution":{"iopub.status.busy":"2021-07-05T18:46:14.357066Z","iopub.execute_input":"2021-07-05T18:46:14.357476Z","iopub.status.idle":"2021-07-05T18:46:14.371081Z","shell.execute_reply.started":"2021-07-05T18:46:14.357439Z","shell.execute_reply":"2021-07-05T18:46:14.369991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('../input/optiver-realized-volatility-prediction/train.csv')","metadata":{"execution":{"iopub.status.busy":"2021-07-05T18:46:14.372886Z","iopub.execute_input":"2021-07-05T18:46:14.373399Z","iopub.status.idle":"2021-07-05T18:46:14.622296Z","shell.execute_reply.started":"2021-07-05T18:46:14.373343Z","shell.execute_reply":"2021-07-05T18:46:14.620903Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Calculate additional features","metadata":{}},{"cell_type":"markdown","source":"Calculate the features from the Optiver examples","metadata":{}},{"cell_type":"code","source":"def log_return(list_stock_prices):\n    return np.log(list_stock_prices).diff() \n\ndef realized_volatility(series_log_return):\n    return np.sqrt(np.sum(series_log_return**2))\n\ndef realized_volatility_rolling(series_log_return):\n    return np.sqrt((series_log_return**2).expanding().mean())\n\ndef add_features_and_aggregate_data(df):\n    # spread between ask and bis price on first level in order book\n    df['price_spread_l1'] = df['ask_price1'] - df['bid_price1']\n    # added price spread as log difference to make it independent\n    df['price_spread_l1_log_diff'] = df.groupby('time_id')['price_spread_l1'].transform(log_return)\n    # I tried to aggregate the data in buckets of 50 seconds and called the bucket index timeslice\n    # I would like to reduce memory consumption and train time with this approach\n    # There is a maximum of 600(0-599) Seconds in every training bucket(stock_id, time_id)\n    # So there should be a maximum of 12 Buckets\n    df['timeslice'] = df['seconds_in_bucket'] // 50 \n    # calculated the weighted average price\n    df['wap'] = (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']) / (df['bid_size1']+ df['ask_size1'])\n    # calculated log return\n    df['log_return'] = df.groupby(['time_id'])['wap'].transform(log_return)\n    # drop rows with na. The na gets created by the diff function by calculating the log return\n    # use inplace to save memory\n    df.dropna(subset=['log_return', 'price_spread_l1_log_diff'], inplace=True)\n    #calculate realized voltality for every bucket\n    df['realized_vol'] = df.groupby(['time_id', 'timeslice'])['log_return'].transform(realized_volatility_rolling)\n    \n    return df.groupby(['time_id', 'timeslice']).agg(\n                stock_id=('stock_id', 'max'),\n                min_price_spread_l1_log_diff=('price_spread_l1_log_diff', 'min'),\n                max_price_spread_l1_log_diff=('price_spread_l1_log_diff', 'max'),\n                mean_price_spread_l1_log_diff=('price_spread_l1_log_diff', 'mean'),\n                min_realized_vol=('realized_vol', 'min'),\n                max_realized_vol=('realized_vol', 'max'),\n                mean_realized_vol=('realized_vol', 'mean'),\n    ).reset_index()","metadata":{"execution":{"iopub.status.busy":"2021-07-05T18:46:14.624088Z","iopub.execute_input":"2021-07-05T18:46:14.624398Z","iopub.status.idle":"2021-07-05T18:46:14.636489Z","shell.execute_reply.started":"2021-07-05T18:46:14.624367Z","shell.execute_reply":"2021-07-05T18:46:14.635597Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# LSTM","metadata":{}},{"cell_type":"markdown","source":"## Preprocess Input Data","metadata":{}},{"cell_type":"code","source":"feature_columns = ['stock_id', 'min_price_spread_l1_log_diff', 'max_price_spread_l1_log_diff', \n           'mean_price_spread_l1_log_diff', 'min_realized_vol', 'max_realized_vol', 'mean_realized_vol']","metadata":{"execution":{"iopub.status.busy":"2021-07-05T18:46:14.638209Z","iopub.execute_input":"2021-07-05T18:46:14.638668Z","iopub.status.idle":"2021-07-05T18:46:14.656735Z","shell.execute_reply.started":"2021-07-05T18:46:14.638623Z","shell.execute_reply":"2021-07-05T18:46:14.655619Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_input_data(list_file):\n    df_input = pd.DataFrame()\n    for file in list_file:\n        # read only needed columns to save memory\n        df_input_file = pd.read_parquet(file, \n                                        columns=['time_id', 'seconds_in_bucket', \n                                                 'bid_size1' ,'bid_price1', \n                                                 'ask_size1', 'ask_price1'])\n        # get stock id from filename\n        df_input_file['stock_id'] = int(file.split('=')[1])\n        # add features and aggregate data\n        df_input = pd.concat([df_input,\n                              add_features_and_aggregate_data(df_input_file)], \n                              ignore_index=True, copy=False)\n    return df_input","metadata":{"execution":{"iopub.status.busy":"2021-07-05T18:46:14.658113Z","iopub.execute_input":"2021-07-05T18:46:14.658795Z","iopub.status.idle":"2021-07-05T18:46:14.672286Z","shell.execute_reply.started":"2021-07-05T18:46:14.65875Z","shell.execute_reply":"2021-07-05T18:46:14.671216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_input = get_input_data(list_file=list_order_book_file_train)","metadata":{"execution":{"iopub.status.busy":"2021-07-05T18:46:14.673648Z","iopub.execute_input":"2021-07-05T18:46:14.674361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# add row id and targets to data\ntrain['row_id'] = train['stock_id'].astype(str) + '-' + train['time_id'].astype(str)\ndf_input = df_input.merge(train, on=['time_id', 'stock_id'], how = 'left')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# split train and validation set groupwise by row-id\ntrain_inds, val_inds = next(GroupShuffleSplit(test_size=.20, n_splits=2, random_state = 7)\n                            .split(df_input, groups=df_input['row_id'])\n                           )\n\ntrain = df_input.iloc[train_inds]\nvalidation = df_input.iloc[val_inds]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fit transformer on columns in train dataset\ncolumn_transformer = make_column_transformer(\n    (MinMaxScaler(), ['min_price_spread_l1_log_diff', 'max_price_spread_l1_log_diff', \n                        'mean_price_spread_l1_log_diff', 'min_realized_vol', \n                        'max_realized_vol', 'mean_realized_vol']),\n    remainder='passthrough')\ncolumn_transformer = column_transformer.fit(train[feature_columns])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# save memory\ndel(df_input)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# reshape and transform columns groupwise to get the needed shape for LSTM [batch, timesteps, feature]\n# Pad to length 12, the max of seconds_in_bucket/50, to get equal sized sequences.\ntrain_np = np.array([keras.preprocessing.sequence.pad_sequences(\n    column_transformer.transform(\n        x[feature_columns]\n    ).transpose(), \n    maxlen=12, \n    dtype='float32', \n    value=0.0).transpose() for _, x in train.groupby('row_id')])\nval_np = np.array([keras.preprocessing.sequence.pad_sequences(column_transformer.transform(x[feature_columns]).transpose(), \n                                                               maxlen=12, \n                                                               dtype='float32',\n                                                               value=0.0).transpose() for _, x in validation.groupby('row_id')])\n# scale targets\ntarget_scaler = StandardScaler()\ntarget_train = target_scaler.fit_transform(\n    train.groupby(['stock_id', 'time_id'])['target'].first().values.reshape(-1,1)\n).reshape(-1)\ntarget_val = target_scaler.transform(\n    validation.groupby(['stock_id', 'time_id'])['target'].first().values.reshape(-1,1)\n).reshape(-1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# save memory\ndel(train, validation)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# some simple LSTM\n# The architecture is mostly random. I don't know how to create a good architecture for this problem\nlearning_rate = 0.03\n\ninputs_lstm = keras.layers.Input(shape=(train_np.shape[1], train_np.shape[2]))\nmasking = keras.layers.Masking(mask_value=0.0, input_shape=(train_np.shape[1], train_np.shape[2]))(inputs_lstm)\nlstm_1_out = keras.layers.LSTM(128, return_sequences=True)(masking)\nlstm_2_out = keras.layers.LSTM(64, return_sequences=True)(lstm_1_out)\nlstm_3_out = keras.layers.LSTM(10, activation='relu')(lstm_2_out)\noutputs = keras.layers.Dense(1)(lstm_3_out)\n\nmodel = keras.Model(inputs=inputs_lstm, outputs=outputs)\n\nmodel.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate), loss=keras.metrics.mean_squared_error)\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# early stopping and fit function\ndef run_trainings_batch(dataset_train, target, val, epochs):\n    es_callback = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=5)\n\n    history = model.fit(\n        dataset_train,\n        target,\n        epochs=epochs,\n        batch_size=1000,\n        validation_data=val,\n        callbacks=[es_callback],\n    )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train model\nrun_trainings_batch(train_np, target_train, (val_np, target_val), 100)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# save memory\ndel(train_np, target_train, val_np, target_val)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"list_order_book_file_test = glob.glob('/kaggle/input/optiver-realized-volatility-prediction/book_test.parquet/*')\n# get prediction for every file\ndef get_predictions(list_file):\n    prediction = pd.DataFrame()\n    for file in list_file:\n        df_input_test = get_input_data(list_file=[file])\n        df_input_test['row_id'] = df_input_test['stock_id'].astype(str) + '-' + df_input_test['time_id'].astype(str)\n        df_pred_np = np.array([keras.preprocessing.sequence.pad_sequences(\n            column_transformer.transform(x[feature_columns]).transpose(), \n            maxlen=12, \n            dtype='float32', \n            value=0.0).transpose() for _, x in df_input_test.groupby('row_id')])\n        prediction_new = pd.DataFrame()\n        prediction_new['row_id'] = df_input_test['row_id'].unique()\n        prediction_new['target'] = model.predict(df_pred_np).reshape(-1)\n        prediction = pd.concat([prediction, prediction_new])\n    prediction['target'] = target_scaler.inverse_transform(prediction['target'])\n    return prediction\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# save submission\nget_predictions(list_file=list_order_book_file_test).to_csv('submission.csv',index = False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}