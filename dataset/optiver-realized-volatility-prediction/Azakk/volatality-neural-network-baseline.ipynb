{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"In this notebook, I will be showing you a baseline of what you could do with Neural Network with K Fold cross validation.  This one is demonstrated with training data of 3 numerical values (log_return_1, log_return_2, trade_log_return1) and 1 categorical feature (stock_id).","metadata":{}},{"cell_type":"code","source":"# Standard python libraries\nimport io, os, time, re\n\n# Installed libraries\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom keras.models import Sequential\nfrom keras.layers.core import Dense, Activation\nfrom sklearn.model_selection import KFold\nimport tensorflow as tf\nfrom joblib import Parallel, delayed\n\npath_data = '../input/optiver-realized-volatility-prediction'","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-09T22:14:09.458696Z","iopub.execute_input":"2021-07-09T22:14:09.459114Z","iopub.status.idle":"2021-07-09T22:14:11.351751Z","shell.execute_reply.started":"2021-07-09T22:14:09.459058Z","shell.execute_reply":"2021-07-09T22:14:11.350881Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create functions for common calculation use for this competition \ndef log_return(list_stock_prices):\n    return np.log(list_stock_prices).diff() \n\ndef realized_volatility(series_log_return):\n    return np.sqrt(np.sum(series_log_return**2))","metadata":{"execution":{"iopub.status.busy":"2021-07-09T22:14:11.357032Z","iopub.execute_input":"2021-07-09T22:14:11.357384Z","iopub.status.idle":"2021-07-09T22:14:11.362911Z","shell.execute_reply.started":"2021-07-09T22:14:11.357346Z","shell.execute_reply":"2021-07-09T22:14:11.361863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create functions for extracting the data.  \n# Thank you Manel for your insights on this from your original notebook https://www.kaggle.com/manels/lgb-starter\ndef get_dataSet(stock_ids : list, dataType = 'train'):\n    stock_stat = Parallel(n_jobs=-1)(\n        delayed(get_stock_stat)(stock_id, dataType) \n        for stock_id in stock_ids\n    )\n    stock_stat_df = pd.concat(stock_stat, ignore_index = True)\n    return stock_stat_df\n\ndef get_stock_stat(stock_id : int, dataType = 'train'):\n    key = ['stock_id', 'time_id', 'seconds_in_bucket']\n    \n    #Book features\n    df_book = pd.read_parquet(os.path.join(path_data, 'book_{}.parquet/stock_id={}/'.format(dataType, stock_id)))\n    df_book['stock_id'] = stock_id\n    cols = key + [col for col in df_book.columns if col not in key]\n    df_book = df_book[cols]\n\n    df_book['wap1'] = (df_book['bid_price1'] * df_book['ask_size1'] +\n                                    df_book['ask_price1'] * df_book['bid_size1']) / (df_book['bid_size1'] + df_book['ask_size1'])\n    df_book['wap2'] = (df_book['bid_price2'] * df_book['ask_size2'] +\n                                    df_book['ask_price2'] * df_book['bid_size2']) / (df_book['bid_size2'] + df_book['ask_size2'])\n    \n    df_book['log_return1'] = df_book.groupby(by = ['time_id'])['wap1'].apply(log_return)\n    df_book = df_book[~df_book['log_return1'].isnull()]\n    df_book['log_return2'] = df_book.groupby(by = ['time_id'])['wap2'].apply(log_return)\n    df_book = df_book[~df_book['log_return2'].isnull()]\n    \n    features_to_apply_realized_volatility = ['log_return'+str(i+1) for i in range(2)]\n    stock_stat = df_book.groupby(by = ['stock_id', 'time_id'])[features_to_apply_realized_volatility]\\\n                        .agg(realized_volatility).reset_index()\n\n    #Trade features\n    trade_stat =  pd.read_parquet(os.path.join(path_data,'trade_{}.parquet/stock_id={}'.format(dataType, stock_id)))\n    trade_stat = trade_stat.sort_values(by=['time_id', 'seconds_in_bucket']).reset_index(drop=True)\n    trade_stat['stock_id'] = stock_id\n    cols = key + [col for col in trade_stat.columns if col not in key]\n    trade_stat = trade_stat[cols]\n    \n    trade_stat['trade_log_return1'] = trade_stat.groupby(by = ['time_id'])['price'].apply(log_return)\n    trade_stat = trade_stat[~trade_stat['trade_log_return1'].isnull()]\n    \n    trade_stat = trade_stat.groupby(by = ['stock_id', 'time_id'])[['trade_log_return1']]\\\n                           .agg(realized_volatility).reset_index()\n    #Joining book and trade features\n    stock_stat = stock_stat.merge(trade_stat, on=['stock_id', 'time_id'], how='left').fillna(-999)\n    \n    return stock_stat","metadata":{"execution":{"iopub.status.busy":"2021-07-09T22:14:11.364362Z","iopub.execute_input":"2021-07-09T22:14:11.364729Z","iopub.status.idle":"2021-07-09T22:14:11.380579Z","shell.execute_reply.started":"2021-07-09T22:14:11.364692Z","shell.execute_reply":"2021-07-09T22:14:11.379453Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create the train dataframe\ntrain = pd.read_csv(os.path.join(path_data, 'train.csv'))\ntrain_stock_stat_df = get_dataSet(stock_ids = train['stock_id'].unique(), dataType = 'train')\ntrain = pd.merge(train, train_stock_stat_df, on = ['stock_id', 'time_id'], how = 'left')\nprint('Train shape: {}'.format(train.shape))\nprint(train)","metadata":{"execution":{"iopub.status.busy":"2021-07-09T22:14:12.815321Z","iopub.execute_input":"2021-07-09T22:14:12.815666Z","iopub.status.idle":"2021-07-09T22:26:46.149631Z","shell.execute_reply.started":"2021-07-09T22:14:12.815632Z","shell.execute_reply":"2021-07-09T22:26:46.148476Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create the test dataframe\ntest = pd.read_csv(os.path.join(path_data, 'test.csv'))\ntest_stock_stat_df = get_dataSet(stock_ids = test['stock_id'].unique(), dataType = 'test')\ntest = pd.merge(test, test_stock_stat_df, on = ['stock_id', 'time_id'], how = 'left').fillna(0)\nprint('Test shape: {}'.format(test.shape))\nprint(test)","metadata":{"execution":{"iopub.status.busy":"2021-07-09T22:26:46.151497Z","iopub.execute_input":"2021-07-09T22:26:46.152074Z","iopub.status.idle":"2021-07-09T22:26:46.226002Z","shell.execute_reply.started":"2021-07-09T22:26:46.152029Z","shell.execute_reply":"2021-07-09T22:26:46.22499Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create new dataframe called train_data with one-hot encoding for our categorical stock_id\ntrain_data = train.sort_index(axis=1) # Sort First\n\n# Make stock_id as one hot encoded categoral features\ntrain_data = pd.concat([train_data, pd.get_dummies(train_data['stock_id'], prefix=\"stock_id\")], axis=1)\ntrain_data = train_data.drop(['time_id', 'stock_id'], axis=1)\ntrain_data","metadata":{"execution":{"iopub.status.busy":"2021-07-09T22:26:46.228169Z","iopub.execute_input":"2021-07-09T22:26:46.22881Z","iopub.status.idle":"2021-07-09T22:26:46.50887Z","shell.execute_reply.started":"2021-07-09T22:26:46.228754Z","shell.execute_reply":"2021-07-09T22:26:46.507744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Separate out the train labels and targets and converting them to numpy array\nX = train_data.drop(['target'], axis=1).values\nY = train_data['target'].values","metadata":{"execution":{"iopub.status.busy":"2021-07-09T22:26:46.519573Z","iopub.execute_input":"2021-07-09T22:26:46.520111Z","iopub.status.idle":"2021-07-09T22:26:46.622404Z","shell.execute_reply.started":"2021-07-09T22:26:46.520074Z","shell.execute_reply":"2021-07-09T22:26:46.621516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function for constructing a NN model\ndef build_model():\n    model = Sequential()\n    model.add(Dense(84, activation='relu', input_shape=(X.shape[1],)))\n    model.add(Dense(48, activation='relu'))\n    model.add(Dense(12, activation='relu'))\n    model.add(Dense(1))\n    return model","metadata":{"execution":{"iopub.status.busy":"2021-07-09T22:26:46.51075Z","iopub.execute_input":"2021-07-09T22:26:46.511176Z","iopub.status.idle":"2021-07-09T22:26:46.518258Z","shell.execute_reply.started":"2021-07-09T22:26:46.511136Z","shell.execute_reply":"2021-07-09T22:26:46.517365Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a kfold cross validation training and monitor on the MAPE\nkfold = KFold(n_splits=5, shuffle=True, random_state=42)\ncvscores = []\nfold_no = 1\n\nfor train, val in kfold.split(X, Y):\n    model = build_model()\n    model.compile(optimizer='adam', loss='mean_absolute_percentage_error')\n    print(f'Training for Fold {fold_no} ...')\n    history = model.fit(X[train], Y[train],\n              batch_size=128,\n              epochs=100)\n    scores = model.evaluate(X[val], Y[val], verbose=0)\n    print(f'Fold {fold_no} CV Score: {scores}')\n    cvscores.append(scores)\n    fold_no = fold_no + 1\nprint(f'Overall Average of {len(cvscores)} folds: {sum(cvscores) / len(cvscores)}')","metadata":{"execution":{"iopub.status.busy":"2021-07-09T22:26:46.62475Z","iopub.execute_input":"2021-07-09T22:26:46.625305Z","iopub.status.idle":"2021-07-09T23:00:00.650021Z","shell.execute_reply.started":"2021-07-09T22:26:46.625264Z","shell.execute_reply":"2021-07-09T23:00:00.649101Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Process the test_data dataset just like the train_data so all columns are available & in same order\ntest_data = test.sort_index(axis=1) # Sort First\n\n# Make stock_id as one hot encoded categoral features\ntest_data = pd.concat([test_data, pd.get_dummies(test['stock_id'], prefix=\"stock_id\")], axis=1)\ntest_data = test_data.drop(['time_id', 'stock_id', 'row_id'], axis=1)\ntest_data = pd.DataFrame(data=test_data, columns = train_data.columns).fillna(0)\ntest_data = test_data.drop(['target'], axis=1)\ntest_data","metadata":{"execution":{"iopub.status.busy":"2021-07-09T23:00:49.807593Z","iopub.execute_input":"2021-07-09T23:00:49.807972Z","iopub.status.idle":"2021-07-09T23:00:49.853029Z","shell.execute_reply.started":"2021-07-09T23:00:49.807938Z","shell.execute_reply":"2021-07-09T23:00:49.851924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Use trained model to predict on the test_data in numpy array\npredictions = model.predict(test_data.values)\npredictions","metadata":{"execution":{"iopub.status.busy":"2021-07-09T23:11:25.045208Z","iopub.execute_input":"2021-07-09T23:11:25.045557Z","iopub.status.idle":"2021-07-09T23:11:25.144836Z","shell.execute_reply.started":"2021-07-09T23:11:25.045525Z","shell.execute_reply":"2021-07-09T23:11:25.144029Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a submission file with the predictions against the row_ids\ntest['target'] = predictions.reshape(len(predictions)).tolist()\nsubmission = test[['row_id', 'target']]\nsubmission.to_csv('submission.csv',index = False)\nsubmission","metadata":{"execution":{"iopub.status.busy":"2021-07-09T23:12:29.271499Z","iopub.execute_input":"2021-07-09T23:12:29.271852Z","iopub.status.idle":"2021-07-09T23:12:29.28655Z","shell.execute_reply.started":"2021-07-09T23:12:29.271818Z","shell.execute_reply":"2021-07-09T23:12:29.285136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I hope this leaves you with lots of run to make adjustments & create your own Neural Network to make predictions for the Realized Volatility in this Competition.  Best of luck !  If this helps you, please kindly let me know in comments and upvote :)","metadata":{}}]}