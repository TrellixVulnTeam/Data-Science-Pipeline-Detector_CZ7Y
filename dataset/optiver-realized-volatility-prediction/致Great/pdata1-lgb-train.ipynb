{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"1. pdata1-features1:feature engineering https://www.kaggle.com/quincyqiang/pdata1-features1\n2. pdata1-lgb-train：current notebook  https://www.kaggle.com/quincyqiang/pdata1-lgb-train\n2. pdata1-lgb-inference：inference https://www.kaggle.com/quincyqiang/pdata1-features1","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom tqdm import tqdm \nfrom glob import glob\nfrom imp import reload\nimport copy as cp\nimport sys\nsys.path.append('../input/features-util')\n# from utils import util\nimport util\nreload(util)\nimport joblib\n","metadata":{"execution":{"iopub.status.busy":"2021-08-14T04:58:02.970678Z","iopub.execute_input":"2021-08-14T04:58:02.971163Z","iopub.status.idle":"2021-08-14T04:58:02.984349Z","shell.execute_reply.started":"2021-08-14T04:58:02.971132Z","shell.execute_reply":"2021-08-14T04:58:02.983546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import copy as cp\nfrom glob import glob\nfrom imp import reload\nimport lightgbm as lgb\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nimport joblib\nfrom sklearn import model_selection\nimport os\n\n\ndef RMSPEMetric(XGBoost=False):\n    def RMSPE(yhat, dtrain, XGBoost=XGBoost):\n\n        y = dtrain.get_label()\n        elements = ((y - yhat) / y) ** 2\n        if XGBoost:\n            return 'RMSPE', float(np.sqrt(np.sum(elements) / len(y)))\n        else:\n            return 'RMSPE', float(np.sqrt(np.sum(elements) / len(y))), False\n\n    return RMSPE\n\n\nreload(util)\n\n\nif __name__ == '__main__':\n\n    path_lst = glob('../input/optiver-realized-volatility-prediction/book_train.parquet/*')\n    stock_lst = [os.path.basename(path).split('=')[-1] for path in path_lst]\n\n    print(len(stock_lst))\n    data_type = 'train'\n    fe_df = pd.read_pickle('../input/pdata1-features1/train_stock_df.pkl')\n    train = pd.read_csv('../input/optiver-realized-volatility-prediction/train.csv')\n\n    fe_df = fe_df.merge(\n        train\n        , how='left'\n        , on=['stock_id', 'time_id']\n    ).replace([np.inf, -np.inf], np.nan).fillna(method='ffill')\n\n    # for name in tqdm(name_lst):\n    #     ret = util.gen_data(name)\n    #     ret.to_csv('../data/20210731/{}.csv'.format())\n\n    # LightGBM parameters\n    params = {\n        'n_estimators': 10000,\n        'objective': 'rmse',\n        'boosting_type': 'gbdt',\n        'max_depth': -1,\n        'learning_rate': 0.01,\n        'subsample': 0.72,\n        'subsample_freq': 4,\n        'feature_fraction': 0.8,\n        'lambda_l1': 1,\n        'lambda_l2': 1,\n        'seed': 46,\n        'early_stopping_rounds': 300,\n        'verbose': -1\n    }\n\n    data = fe_df\n    label = fe_df['target']\n    features = fe_df.columns.difference(['time_id', 'target']).tolist()\n    data_ = fe_df[features]\n    cats = ['stock_id', ]\n    X_train = data_.reset_index(drop=True)\n    y_train = label\n    # y_train = pd.DataFrame(label_)\n    models = []\n    oof_df = fe_df[['time_id', 'stock_id']].copy()\n    oof_df['target'] = y_train\n    oof_df['pred'] = np.nan\n\n    cv = model_selection.KFold(n_splits=10,\n                               shuffle=True,\n                               random_state=666)\n\n    kf = cv.split(X_train, y_train)\n\n    fi_df = pd.DataFrame()\n    fi_df['features'] = features\n    fi_df['importance'] = 0\n\n    for fold_id, (train_index, valid_index) in tqdm(enumerate(kf)):\n        # split\n        X_tr = X_train.loc[train_index, features]\n        X_val = X_train.loc[valid_index, features]\n        y_tr = y_train.loc[train_index].values.reshape(-1)\n        y_val = y_train.loc[valid_index].values.reshape(-1)\n\n        # model (note inverse weighting)\n        train_set = lgb.Dataset(X_tr,\n                                y_tr,\n                                categorical_feature=cats,\n                                weight=1 / np.power(y_tr, 2))\n        val_set = lgb.Dataset(X_val,\n                              y_val,\n                              categorical_feature=cats,\n                              weight=1 / np.power(y_val, 2))\n        model = lgb.train(params,\n                          train_set,\n                          valid_sets=[train_set, val_set],\n                          feval=RMSPEMetric(),\n                          verbose_eval=250)\n\n        # feature importance\n        fi_df[f'importance_fold{fold_id}'] = model.feature_importance(\n            importance_type=\"gain\")\n        fi_df['importance'] += fi_df[f'importance_fold{fold_id}'].values\n\n        # save model\n        joblib.dump(model, f'model_fold{fold_id}.pkl')\n        print('model saved!')\n\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-14T04:58:03.000689Z","iopub.execute_input":"2021-08-14T04:58:03.001141Z"},"trusted":true},"execution_count":null,"outputs":[]}]}