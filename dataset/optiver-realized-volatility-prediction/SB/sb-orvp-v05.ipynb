{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"#### Thanks to https://www.kaggle.com/manels/lgb-starter for starting inspiration","metadata":{}},{"cell_type":"code","source":"import os\nimport glob\nimport pandas as pd\nimport numpy as np\nfrom IPython.core.display import display, HTML\nfrom timeit import default_timer as timer\nimport random\nimport gc\n\nGLOBAL_SEED_VALUE = 0\nos.environ['PYTHONHASHSEED']=str(GLOBAL_SEED_VALUE)\nrandom.seed(GLOBAL_SEED_VALUE)\nnp.random.seed(GLOBAL_SEED_VALUE)\nfrom tensorflow.random import set_seed\nset_seed(GLOBAL_SEED_VALUE)","metadata":{"execution":{"iopub.status.busy":"2021-09-26T23:21:15.938181Z","iopub.execute_input":"2021-09-26T23:21:15.938871Z","iopub.status.idle":"2021-09-26T23:21:20.242721Z","shell.execute_reply.started":"2021-09-26T23:21:15.938601Z","shell.execute_reply":"2021-09-26T23:21:20.241857Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DATA_DIR = '/kaggle/input/optiver-realized-volatility-prediction'\n\nBATCH_SIZE = 1024\nEPSILON = 1e20\nTRAIN_STOCK_IDS = []\n\nTIMEID_WINSIZE = 150\nTIMEID_SUBWINS = [i for i in range(int(600/TIMEID_WINSIZE))]\nprint('TIMEID_SUBWINS:', TIMEID_SUBWINS)","metadata":{"execution":{"iopub.status.busy":"2021-09-26T23:21:20.244031Z","iopub.execute_input":"2021-09-26T23:21:20.244354Z","iopub.status.idle":"2021-09-26T23:21:20.250935Z","shell.execute_reply.started":"2021-09-26T23:21:20.244322Z","shell.execute_reply":"2021-09-26T23:21:20.24951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import r2_score\n\ndef rmspe(y_true, y_pred):\n    return (np.sqrt(np.mean(np.square((y_pred - y_true)/y_true))))\n\ndef get_all_stock_ids(data_type):\n    paths = glob.glob(os.path.join(DATA_DIR, f'book_{data_type}.parquet/*'), recursive=True)\n    return [int(path.split('=')[1]) for path in paths]\n\ndef read_parquet_file(path):\n    stock_id = path.split('=')[1]\n    df = pd.read_parquet(path)\n    return stock_id, df\n\ndef read_parquet_file_for_stock(stock_id, data_type):\n    df_book  = pd.read_parquet(os.path.join(DATA_DIR, f'book_{data_type}.parquet', f'stock_id={stock_id}'))\n    df_trade = pd.read_parquet(os.path.join(DATA_DIR, f'trade_{data_type}.parquet', f'stock_id={stock_id}'))\n    return df_book, df_trade\n\ndef logr(series):\n    return np.log(series).diff()\n\ndef time_diff(seconds_in_bucket):\n    return seconds_in_bucket.diff()\n\ndef rv(returns):\n    return np.sqrt(np.sum(returns**2))\n\ndef fill_fb(series):\n    return series.fillna(method='ffill').fillna(method='bfill')\n\ndef cnz(series):\n    return np.count_nonzero(series)","metadata":{"execution":{"iopub.status.busy":"2021-09-26T23:21:20.252879Z","iopub.execute_input":"2021-09-26T23:21:20.253369Z","iopub.status.idle":"2021-09-26T23:21:20.74339Z","shell.execute_reply.started":"2021-09-26T23:21:20.253332Z","shell.execute_reply":"2021-09-26T23:21:20.742578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data model by pre-processing raw book & trade data","metadata":{}},{"cell_type":"code","source":"def preprocess_one_stock_id_data(stock_id:int, data_type, export=False, verbose=False):\n    # Model v100.5 --------------------------------------------------------------------------\n    df_book, df_trade = read_parquet_file_for_stock(stock_id, data_type)\n    df_trade.drop('order_count', axis=1, inplace=True)\n    logstr = f'Preprocessing log stock_id: {stock_id} raw shape: book: {df_book.shape}, trade: {df_trade.shape}'\n    \n    if verbose: print('\\nTime_check 1:', round(timer(), 0))\n    # ---------------------------------------------------------------------------------------\n    df_book['sib_id'] = (df_book['seconds_in_bucket']/TIMEID_WINSIZE).astype(int)\n    df_book['wap1']   = (df_book['bid_price1'] * df_book['ask_size1'] + df_book['ask_price1'] * df_book['bid_size1']) / (df_book['bid_size1'] + df_book['ask_size1'])\n    df_book['wap2']   = (df_book['bid_price2'] * df_book['ask_size2'] + df_book['ask_price2'] * df_book['bid_size2']) / (df_book['bid_size2'] + df_book['ask_size2'])\n    df_book['wap3']   = (df_book['bid_price1'] * df_book['bid_size1'] + df_book['ask_price1'] * df_book['ask_size1']) / (df_book['bid_size1'] + df_book['ask_size1'])\n    df_book['wap4']   = (df_book['bid_price2'] * df_book['bid_size2'] + df_book['ask_price2'] * df_book['ask_size2']) / (df_book['bid_size2'] + df_book['ask_size2'])\n    df_book['ret_w1'] = df_book.groupby(['time_id'])['wap1'].apply(logr).fillna(0)\n    df_book['ret_w2'] = df_book.groupby(['time_id'])['wap2'].apply(logr).fillna(0)\n    df_book['ret_w3'] = df_book.groupby(['time_id'])['wap3'].apply(logr).fillna(0)\n    df_book['ret_w4'] = df_book.groupby(['time_id'])['wap4'].apply(logr).fillna(0)\n    df_book['vimb1'] = (df_book['ask_size1'] - df_book['bid_size1']) / (df_book['ask_size1'] + df_book['bid_size1'])\n    df_book['chg_bs1'] = df_book.groupby(['time_id'])['bid_size1'].apply(logr).fillna(0)\n    df_book['chg_as1'] = df_book.groupby(['time_id'])['ask_size1'].apply(logr).fillna(0)\n    df_book['chg_bs2'] = df_book.groupby(['time_id'])['bid_size2'].apply(logr).fillna(0)\n    df_book['chg_as2'] = df_book.groupby(['time_id'])['ask_size2'].apply(logr).fillna(0)\n    df_book['sib_wt'] = df_book['seconds_in_bucket']/179700.0\n    df_book['wret_w1'] = df_book['sib_wt']*df_book['ret_w1']\n    df_book['wret_w2'] = df_book['sib_wt']*df_book['ret_w2']\n    df_book['wret_w3'] = df_book['sib_wt']*df_book['ret_w3']\n    df_book['wret_w4'] = df_book['sib_wt']*df_book['ret_w4']\n    # --- above ok ----\n    \n    # ---------------------------------------------------------------------------------------\n    df_trade['sib_id'] = (df_trade['seconds_in_bucket']/TIMEID_WINSIZE).astype(int)\n    df_trade['ret_p1'] = df_trade.groupby(['time_id'])['price'].apply(logr).fillna(0)\n    df_trade['sib_wt'] = df_trade['seconds_in_bucket']/179700.0\n    df_trade['wret_p1'] = df_trade['sib_wt']*df_trade['ret_p1']\n    # --- above ok ----\n    \n    if verbose: print('Time_check 2:', round(timer(), 0))\n    \n    # ---------------------------------------------------------------------------------------\n    df_merged = pd.merge(df_book, df_trade, how='left', on=['time_id', 'sib_id', 'seconds_in_bucket']) #.fillna(0)\n    df_merged['price'] = df_merged.groupby(['time_id'])['price'].apply(fill_fb) #compromise for now\n    df_merged.fillna(0, inplace=True)\n    df_merged['basprd1'] = (df_merged['bid_price1'] - df_merged['ask_price1']) #/(df_merged['price']+EPSILON) #better solution later\n    df_merged['basprd2'] = (df_merged['bid_price2'] - df_merged['ask_price2']) #/(df_merged['price']+EPSILON) #better solution later\n    # --- above ok ----\n    \n    # ---------------------------------------------------------------------------------------\n    aggregation_model = {\n        'ret_w1': [rv, cnz, np.std], 'ret_w2': [rv, cnz, np.std], 'ret_p1': [rv, cnz, np.std], 'ret_w3': [rv, cnz, np.std], 'ret_w4': [rv, cnz, np.std],\n        'wap1': [np.std], 'vimb1': [np.std], 'basprd1': [np.mean, np.std], 'basprd2': [np.mean, np.std],\n        'wret_w1': [rv, np.std], 'wret_w2': [rv, np.std], 'wret_w3': [rv, np.std], 'wret_w4': [rv, np.std], 'wret_p1': [rv, np.std],\n        'chg_bs1': [cnz], 'chg_as1': [cnz], 'chg_bs2': [cnz], 'chg_as2': [cnz]\n    }\n    syn_df = df_merged.groupby(['time_id', 'sib_id']).agg(aggregation_model).reset_index()\n    syn_df.columns = ['_'.join(col) for col in syn_df.columns]\n    syn_df = syn_df.rename(columns={'time_id_':'time_id', 'sib_id_':'sib_id'})\n    if verbose: print('Time_check 3:', round(timer(), 0))\n    \n    # ---------------------------------------------------------------------------------------\n    syn_df['stock_id'] = int(stock_id)\n    cols1 = ['stock_id', 'time_id', 'sib_id']\n    cols2 = [x for x in syn_df.columns if x not in cols1]\n    cols1.extend(cols2)\n    syn_df = syn_df[cols1]\n    if verbose: print('Time_check 4:', round(timer(), 0))\n    if export: df_merged.to_csv('df_merged.csv', index=False);syn_df.to_csv('syn_df.csv', index=False)\n    if verbose: print('Time_check 5:', round(timer(), 0))\n    \n    # Model v100.5 --------------------------------------------------------------------------\n    \n    logstr = logstr + f', pre-processed shape: {syn_df.shape}, {syn_df.isnull().T.any().T.sum()} NULLS: {syn_df.columns[syn_df.isnull().any()].tolist()}'\n    if verbose: print(logstr)\n    return stock_id, syn_df, logstr\n\nif False: #set to True to test\n    !rm /kaggle/working/*.csv\n    from timeit import default_timer as timer\n    STOCK_ID = 31\n    time_start = timer()\n    print(f'Running pre_processing test for stock_id {STOCK_ID} ... ', end=\"\")\n    _, df, logstr = preprocess_one_stock_id_data(STOCK_ID, 'train')\n    time_end = timer()\n    print(f'Time taken {round(time_end-time_start,1)} seconds')\n    print(f'\\n{logstr}\\n')\n    print('Processed data: columns:', list(df.columns))","metadata":{"execution":{"iopub.status.busy":"2021-09-26T23:21:20.746707Z","iopub.execute_input":"2021-09-26T23:21:20.746961Z","iopub.status.idle":"2021-09-26T23:21:20.791488Z","shell.execute_reply.started":"2021-09-26T23:21:20.746936Z","shell.execute_reply":"2021-09-26T23:21:20.790702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Parallelism","metadata":{}},{"cell_type":"code","source":"from joblib import Parallel, delayed\nfrom tqdm import tqdm\ndef prepare_feature_data_new(data_type, stock_ids_in_scope=[]):\n    stock_ids = get_all_stock_ids(data_type) if stock_ids_in_scope == [] else stock_ids_in_scope\n    print(f'Considering {len(stock_ids)} stock ids ... {stock_ids}\\n')\n    \n    job_inputs = tqdm(stock_ids)\n    job_result = Parallel(n_jobs=-1)(delayed(preprocess_one_stock_id_data)(i, data_type) for i in job_inputs)\n    print(f'extract_features completed: {len(job_result)} results returned.')\n    result_stock_ids, result_dfs, result_output_str = zip(*job_result)\n    assert sorted(set(result_stock_ids)) == sorted(set(stock_ids))\n    assert len(result_stock_ids) == len(result_dfs)\n    \n    print('\\nConsolidating the results ...')\n    consolidated_df  = pd.DataFrame()\n    for i in range(len(result_stock_ids)):\n        consolidated_df = consolidated_df.append(result_dfs[i], ignore_index=True)\n        print(' ', result_output_str[i])\n    print('Done. Prepared feature data: shape:', consolidated_df.shape, '\\n')\n    \n    return consolidated_df","metadata":{"execution":{"iopub.status.busy":"2021-09-26T23:21:20.794565Z","iopub.execute_input":"2021-09-26T23:21:20.794838Z","iopub.status.idle":"2021-09-26T23:21:20.805751Z","shell.execute_reply.started":"2021-09-26T23:21:20.794814Z","shell.execute_reply":"2021-09-26T23:21:20.804747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Training data","metadata":{}},{"cell_type":"code","source":"%%time\n!rm /kaggle/working/*.csv\npre_processed_train_df = prepare_feature_data_new(data_type='train', stock_ids_in_scope=TRAIN_STOCK_IDS)\n#pre_processed_train_df.to_csv('pre_processed_train_df.csv', index=False)\n\n#pre_processed_train_df = pd.read_csv('/kaggle/input/orvp1005s150/pre_processed_train_df.csv')\n\nprint('pre_processed_train_df.shape:', pre_processed_train_df.shape)\npre_processed_train_df.head(5)","metadata":{"execution":{"iopub.status.busy":"2021-09-26T23:21:20.809221Z","iopub.execute_input":"2021-09-26T23:21:20.809512Z","iopub.status.idle":"2021-09-26T23:21:39.480373Z","shell.execute_reply.started":"2021-09-26T23:21:20.809486Z","shell.execute_reply":"2021-09-26T23:21:39.479541Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"list(pre_processed_train_df.columns)\n#from IPython.display import FileLink\n#FileLink(r'pre_processed_train_df.csv')","metadata":{"execution":{"iopub.status.busy":"2021-09-26T23:21:39.481836Z","iopub.execute_input":"2021-09-26T23:21:39.482198Z","iopub.status.idle":"2021-09-26T23:21:39.490214Z","shell.execute_reply.started":"2021-09-26T23:21:39.48216Z","shell.execute_reply":"2021-09-26T23:21:39.489199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-09-26T23:21:39.493611Z","iopub.execute_input":"2021-09-26T23:21:39.493913Z","iopub.status.idle":"2021-09-26T23:21:39.665255Z","shell.execute_reply.started":"2021-09-26T23:21:39.49388Z","shell.execute_reply":"2021-09-26T23:21:39.664273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\n\ndef cluster_stock_ids(n_clusters):\n    print('Running KMeans to cluster stock_ids ...')\n    train_csv = pd.read_csv(os.path.join(DATA_DIR, 'train.csv'))\n    train_csv = train_csv.sample(frac=1, random_state=GLOBAL_SEED_VALUE).reset_index(drop=True)\n    feat = {'target': [np.mean, np.std]}\n    df = train_csv.groupby(['stock_id']).agg(feat).reset_index()\n    df.columns = ['_'.join(col) for col in df.columns]\n    df = df.rename(columns={'stock_id_': 'stock_id'})\n\n    kmeans_model = KMeans(n_clusters=n_clusters, random_state=GLOBAL_SEED_VALUE)\n    kmeans_model.fit(df[['target_mean', 'target_std']])\n    labels = kmeans_model.labels_\n    print('\\nIdentified stock_id cluster labels:')\n    display(labels)\n    df['cluster_label'] = kmeans_model.labels_\n\n    def centroid_values(stock_id):\n        lbl = df.loc[df['stock_id'] == stock_id, 'cluster_label'].values[0]\n        return kmeans_model.cluster_centers_[lbl, :]\n\n    df['centroid_mean'] = [centroid_values(stock_id)[0] for stock_id in df['stock_id'].to_list()]\n    df['centroid_std']  = [centroid_values(stock_id)[1] for stock_id in df['stock_id'].to_list()]\n    return df, kmeans_model.cluster_centers_\n\nstock_id_cluster_df, centroids = cluster_stock_ids(7)\nprint('\\nIdentified centroid values:')\ndisplay(centroids)\nprint('\\nResultant stock_id_cluster_df:')\ndisplay(stock_id_cluster_df.head(5))\n\nLABEL_COLOR_MAP = {0:'green', 1:'brown', 2:'blue', 3:'cyan', 4:'red', 5:'magenta', 6: 'black'}\nlabel_color = [LABEL_COLOR_MAP[l] for l in stock_id_cluster_df['cluster_label']]\n\nplt.rcParams[\"figure.figsize\"] = (12,8)\nax = stock_id_cluster_df.plot.scatter(x='target_mean', y='target_std', alpha=0.5, c=label_color)\n\nfor cc in range(centroids.shape[0]):\n    plt.scatter(centroids[cc,0], centroids[cc,1], s=100, marker='s', c='blue')\n\nfor i, txt in enumerate(stock_id_cluster_df['stock_id']):\n    ax.annotate(txt, (stock_id_cluster_df['target_mean'].iat[i], stock_id_cluster_df['target_std'].iat[i]))\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-26T23:21:39.66733Z","iopub.execute_input":"2021-09-26T23:21:39.667725Z","iopub.status.idle":"2021-09-26T23:21:40.984607Z","shell.execute_reply.started":"2021-09-26T23:21:39.667686Z","shell.execute_reply":"2021-09-26T23:21:40.983809Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Prepare Training & Test Data","metadata":{}},{"cell_type":"code","source":"import sklearn.preprocessing as preprocessing\n\ndef flatten(df):\n    print('Flatteing df ...')\n    gfcols = [col for col in df.columns if col.startswith('gf_')]\n    if len(gfcols) > 0:\n        sibcols = [col for col in df.columns if col not in gfcols]\n        gfcols.extend(['stock_id', 'time_id'])\n        print('  gfcols:', gfcols, 'sib_cols:', sibcols)\n        df_gf = df[gfcols]\n        df_gf = df_gf.drop_duplicates()\n        print('  df_gf.shape:', df_gf.shape, 'df_gf.columns:', df_gf.columns)\n    else:\n        sibcols = df.columns\n        df_gf = None\n    \n    flattened_df = pd.DataFrame()\n    for sib_id in sorted(TIMEID_SUBWINS):\n        df_temp = df[sibcols].loc[df['sib_id'] == sib_id].drop(['sib_id'], axis=1).reset_index(drop=True)\n        df_temp = df_temp.add_suffix(f'_sib_id_{sib_id}')\n        df_temp = df_temp.rename(columns={f'time_id_sib_id_{sib_id}' : 'time_id', f'stock_id_sib_id_{sib_id}' : 'stock_id'})\n        flattened_df = df_temp if flattened_df.shape[0] == 0 else flattened_df.merge(df_temp, how = 'left', on = ['stock_id', 'time_id'])\n        \n    print('  before merging df_gf, flattened_df.shape:', flattened_df.shape)\n    \n    if df_gf is not None:\n        flattened_df = pd.merge(flattened_df, df_gf, how='left', on = ['stock_id', 'time_id'])\n        print('  after merging df_gf, flattened_df.shape:', flattened_df.shape)\n    print('Flatteing df completed')\n    \n    flattened_df = pd.merge(flattened_df, stock_id_cluster_df[['stock_id', 'centroid_mean', 'centroid_std']], how='left', on='stock_id').reset_index(drop=True)\n    print('Kmeans cluster centroid mean/std added ... flattened_df.shape:', flattened_df.shape)\n    \n    return flattened_df\n\ndef scale_features(train_df, test_df, features):\n    df = pd.concat([train_df[features[1:]], test_df[features[1:]]])\n    scaler = preprocessing.StandardScaler()\n    df = scaler.fit_transform(df)\n    train_df[features[1:]] = df[0:train_df.shape[0]]\n    test_df[features[1:]]  = df[train_df.shape[0]:]\n    print(f'Scaled {len(features)-1} features: train_df.shape:', train_df.shape, 'test_df.shape:', test_df.shape)\n    return train_df, test_df\n\ndef prepare_training_and_test_data(submission_mode, verbose, test_df_fraction):\n    train_csv = pd.read_csv(os.path.join(DATA_DIR, 'train.csv'))\n    print('train_csv:', train_csv.shape, 'columns:', list(train_csv.columns))\n    test_csv = pd.read_csv(os.path.join(DATA_DIR, 'test.csv'))\n    print('test_csv :', test_csv.shape, 'columns:', list(test_csv.columns), '\\n')\n    \n    train_examples = flatten(pre_processed_train_df)\n    print('flattened train_examples.shape:', train_examples.shape, 'null columns:', train_examples.columns[train_examples.isnull().any()].tolist())\n    \n    train_df = pd.merge(train_csv, train_examples, how = 'left', on=['stock_id', 'time_id']).fillna(0)\n    train_df = train_df.sample(frac=1, random_state=GLOBAL_SEED_VALUE).reset_index(drop=True)\n    print('train_df.shape:', train_df.shape)\n    if verbose: display(train_df.head(2))\n\n    if submission_mode:\n        print('\\nSUBMISSION MODE ===>')\n        pre_processed_test_df = prepare_feature_data_new(data_type='test')\n        test_examples = flatten(pre_processed_test_df)\n        print('flattened test_examples.shape:', test_examples.shape, 'null columns:', test_examples.columns[test_examples.isnull().any()].tolist())\n        test_df = pd.merge(test_csv, test_examples, how = 'left', on=['stock_id', 'time_id']).fillna(0)\n    else:\n        print(f'\\nNON SUBMISSION MODE --> Taking {test_df_fraction}x sample from training data for test')\n        test_df  = train_df.groupby(\"stock_id\").sample(frac=test_df_fraction, random_state=GLOBAL_SEED_VALUE)\n        train_df = train_df.drop(test_df.index)\n        train_df = train_df.reset_index(drop=True)\n        test_df  = test_df.reset_index(drop=True)\n        test_df['row_id'] = test_df['stock_id'].astype(str) + '-' + test_df['time_id'].astype(str)\n        \n    print('test_df.shape:', test_df.shape)\n    if verbose: display(test_df.head(2))\n    print('\\nFinal train/test/feature data: ', 'train_df.shape:', train_df.shape, 'test_df.shape:', test_df.shape)\n    features = list(train_examples.columns)\n    features.remove('time_id')\n    print(f'\\n{len(features)} features selected::')\n    print(f'{features}\\n')\n    \n    train_df, test_df = scale_features(train_df, test_df, features)\n    \n    return train_df, test_df, features","metadata":{"execution":{"iopub.status.busy":"2021-09-26T23:21:40.985988Z","iopub.execute_input":"2021-09-26T23:21:40.986334Z","iopub.status.idle":"2021-09-26T23:21:41.009583Z","shell.execute_reply.started":"2021-09-26T23:21:40.986297Z","shell.execute_reply":"2021-09-26T23:21:41.008726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### LGBM model","metadata":{}},{"cell_type":"code","source":"import lightgbm as lgb\nfrom sklearn.model_selection import KFold\nimport matplotlib.pyplot as plt\n\ndef feval_rmspe(preds, train_data):\n    labels = train_data.get_label()\n    return 'feval_rmspe', round(rmspe(y_true = labels, y_pred = preds), 5), False\n\ndef lgb_model_1(model_name, X_train, Y_train, X_cval, Y_cval, features):\n    model_params = {\n        'task': 'train',\n        'categorical_column':[0],\n        'boosting_type': 'gbdt',\n        'max_depth': -1,\n        'max_bin':100,\n        'min_data_in_leaf':500,\n        'learning_rate': 0.05,\n        'subsample': 0.72,\n        'subsample_freq': 4,\n        'feature_fraction': 0.5,\n        'lambda_l1': 0.5,\n        'lambda_l2': 1.0,\n        'seed': GLOBAL_SEED_VALUE,\n        \"tree_learner\": 'voting',\n        'verbose': -1\n    }\n    \n    train_data = lgb.Dataset(X_train, label=Y_train, categorical_feature=['stock_id'], weight=1/np.power(Y_train,2))\n    cval_data  = lgb.Dataset(X_cval,  label=Y_cval, categorical_feature=['stock_id'], weight=1/np.power(Y_cval,2)) \n    model      = lgb.train(model_params, train_data, valid_sets=cval_data, feval=feval_rmspe, categorical_feature=['stock_id'],\n                           num_boost_round=5000, early_stopping_rounds=500, verbose_eval=False)\n    return model","metadata":{"execution":{"iopub.status.busy":"2021-09-26T23:21:41.011005Z","iopub.execute_input":"2021-09-26T23:21:41.011289Z","iopub.status.idle":"2021-09-26T23:21:42.993098Z","shell.execute_reply.started":"2021-09-26T23:21:41.011256Z","shell.execute_reply":"2021-09-26T23:21:42.992273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### NN","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, Callback\nfrom tensorflow.keras.layers import InputLayer, Dense, Activation, Dropout, Embedding, Flatten, Concatenate, LSTM, Reshape\nfrom tensorflow.keras import Sequential, regularizers, Input, Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import TensorBoard\nimport keras.backend as K\nfrom skopt.space import Real, Integer, Categorical\nfrom skopt.utils import use_named_args\nfrom skopt import gp_minimize\n\ndef my_rmspe(y_true, y_pred):\n    return (K.sqrt(K.mean(K.square((y_pred - y_true)/y_true))))","metadata":{"execution":{"iopub.status.busy":"2021-09-26T23:35:52.396905Z","iopub.execute_input":"2021-09-26T23:35:52.397261Z","iopub.status.idle":"2021-09-26T23:35:52.40526Z","shell.execute_reply.started":"2021-09-26T23:35:52.397219Z","shell.execute_reply":"2021-09-26T23:35:52.403851Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Bayes hyperparameter optimization","metadata":{}},{"cell_type":"code","source":"units             = Categorical(categories=[256, 512, 1024], name='units')\ndropout           = Categorical(categories=[0.1, 0.2, 0.3, 0.4, 0.5], name='dropout')\nl2_regularization = Real(low=0.001, high=0.5, prior='log-uniform', name='l2_regularization')\nlearning_rate     = Real(low=0.001, high=0.5, prior='log-uniform', name='learning_rate')\nhyperparam_range  = [units, dropout, l2_regularization, learning_rate]\ndefault_params    = [256, 0.3, 0.01, 0.01]\ndisplay(hyperparam_range)\n\nnn_bayes_inputs = None\ndef nn_initialize_bayes_inputs(model_name, train_df, test_df, features):\n    global nn_bayes_inputs\n    nn_bayes_inputs = {}\n    nn_bayes_inputs[\"iteration\"]  = 0\n    nn_bayes_inputs[\"best_rmspe\"] = 1000\n    \n    nn_bayes_inputs[\"model_name\"] = model_name\n    nn_bayes_inputs[\"train_df\"] = train_df\n    nn_bayes_inputs[\"test_df\"]  = test_df\n    nn_bayes_inputs[\"features\"] = features\n    \n    train_index, cval_index = next(KFold(n_splits=4, shuffle=True, random_state=GLOBAL_SEED_VALUE).split(nn_bayes_inputs[\"train_df\"]))\n    nn_bayes_inputs[\"train_index\"] = train_index\n    nn_bayes_inputs[\"cval_index\"]  = cval_index\n    \n    print('nn_initialize_bayes_inputs:', 'train_df.shape:', nn_bayes_inputs[\"train_df\"].shape, 'test_df.shape:', nn_bayes_inputs[\"test_df\"].shape)\n    return\n\n@use_named_args(dimensions=hyperparam_range)\ndef bayes_one_iteration(units, dropout, l2_regularization, learning_rate):\n    time_start = timer()\n    nn_bayes_inputs[\"iteration\"] += 1\n    \n    print(f'\\nRunning iteration {nn_bayes_inputs[\"iteration\"]}:')\n    print(f' Hyperparams: [{units} / {dropout} / {round(l2_regularization,4)} / {round(learning_rate,4)}]')\n    \n    hyperparams = {'layers':3, 'units':units, 'dropout':dropout, 'l2_regularization':l2_regularization,\n                   'learning_rate':learning_rate, 'activation':'relu', 'lstm_units':128, 'epochs':100}\n    model, eval_rmspe, history = nn_model_lstm(hyperparams,\n                                               nn_bayes_inputs[\"train_df\"], nn_bayes_inputs[\"train_index\"], nn_bayes_inputs[\"cval_index\"],\n                                               nn_bayes_inputs[\"features\"], verbose=False)\n    \n    test_preds = nn_make_predictions(model, test_df, features, model_name=nn_bayes_inputs[\"model_name\"], lstm_units=128, verbose=False).clip(0,1e10)\n    test_rmspe = rmspe(y_true = test_df['target'], y_pred = test_preds)\n    test_r2    = r2_score(y_true = test_df['target'], y_pred = test_preds)\n    print(f' Result: eval_rmspe: {round(eval_rmspe, 3)}, test_rmspe: {round(test_rmspe, 3)}, r2_score: {round(test_r2, 3)}, time taken: {int(timer()-time_start)} secs')\n    \n    if test_rmspe < nn_bayes_inputs[\"best_rmspe\"]:\n        nn_bayes_inputs[\"best_rmspe\"] = test_rmspe\n        print(f' Better hyperparameters found ({round(test_rmspe, 3)}):')\n        print(f'   hidden units:   {hyperparams[\"units\"]}')\n        print(f'   dropout:        {hyperparams[\"dropout\"]}')\n        print(f'   l2 reg rate:    {round(hyperparams[\"l2_regularization\"], 4)}')\n        print(f'   learning rate:  {round(hyperparams[\"learning_rate\"], 4)}')\n        print()\n\n    del model\n    K.clear_session()\n    return test_rmspe\n\ndef run_bayes_optimization(train_df, test_df, features, n_calls=11):\n    nn_initialize_bayes_inputs(\"LSTM\", train_df, test_df, features)\n    optimal_params = gp_minimize(func=bayes_one_iteration, dimensions=hyperparam_range,\n                                 x0=default_params, random_state=GLOBAL_SEED_VALUE,\n                                 n_calls=n_calls, acq_func='EI')\n    print('best_rmspe:', nn_bayes_inputs[\"best_rmspe\"], 'hyperparameters:', list(optimal_params.x))\n    return optimal_params","metadata":{"execution":{"iopub.status.busy":"2021-09-26T23:35:52.406996Z","iopub.execute_input":"2021-09-26T23:35:52.407359Z","iopub.status.idle":"2021-09-26T23:35:52.43332Z","shell.execute_reply.started":"2021-09-26T23:35:52.407323Z","shell.execute_reply":"2021-09-26T23:35:52.432506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### NN prediction function","metadata":{}},{"cell_type":"code","source":"def nn_make_predictions(model, X_df, features, model_name=None, verbose=False, lstm_units=None):\n    m = X_df.shape[0]\n    full_batches = int(m/BATCH_SIZE)\n    if verbose: print(f'  make_predictions: {m} examples, {full_batches} full batches, {m - full_batches*BATCH_SIZE} leftover')\n    if model_name == \"NN1\" or model_name == \"LSTM\":\n        categorical_features = ['stock_id']\n        numerical_features = features.copy()\n        numerical_features.remove('stock_id')\n\n    predictions = np.array([1.0])\n    for i in range(full_batches):\n        if verbose: print(f'   predicting {i}-th batch: {i*BATCH_SIZE} to {(i+1)*BATCH_SIZE}')\n        if model_name == \"NN1\":\n            Xc,  Xn = X_df[categorical_features], X_df[numerical_features]\n            X = [Xc[i*BATCH_SIZE:(i+1)*BATCH_SIZE], Xn[i*BATCH_SIZE:(i+1)*BATCH_SIZE]]\n        elif model_name == \"LSTM\":\n            Xc,  Xn = X_df[categorical_features], X_df[numerical_features]\n            a0 = np.zeros((BATCH_SIZE, lstm_units))\n            c0 = np.zeros((BATCH_SIZE, lstm_units))\n            X = [Xc[i*BATCH_SIZE:(i+1)*BATCH_SIZE], Xn[i*BATCH_SIZE:(i+1)*BATCH_SIZE], a0, c0]\n        else:\n            X = X_df[i*BATCH_SIZE:(i+1)*BATCH_SIZE]\n            X = X[features]\n        preds = model.predict(X)\n        predictions = preds if len(predictions) == 0 else np.append(predictions, preds)\n\n    if (m - full_batches*BATCH_SIZE) > 0:\n        if verbose: print(f'   predicting last batch: {full_batches*BATCH_SIZE} to {m}')\n        if model_name == \"NN1\":\n            Xc,  Xn = X_df[categorical_features], X_df[numerical_features]\n            X = [Xc[full_batches*BATCH_SIZE:], Xn[full_batches*BATCH_SIZE:]]\n        elif model_name == \"LSTM\":\n            Xc,  Xn = X_df[categorical_features], X_df[numerical_features]\n            a0 = np.zeros((m-full_batches*BATCH_SIZE, lstm_units))\n            c0 = np.zeros((m-full_batches*BATCH_SIZE, lstm_units))\n            X = [Xc[full_batches*BATCH_SIZE:], Xn[full_batches*BATCH_SIZE:], a0, c0]\n        else:\n            X = X_df[full_batches*BATCH_SIZE:]\n            X = X[features]\n        preds = model.predict(X)\n        predictions = preds if len(predictions) == 0 else np.append(predictions, preds)\n\n    predictions = np.delete(predictions, 0)\n    if verbose: print(f'  predictions.shape:', predictions.shape)\n    assert m == predictions.shape[0]\n    return predictions","metadata":{"execution":{"iopub.status.busy":"2021-09-26T23:35:52.436197Z","iopub.execute_input":"2021-09-26T23:35:52.43659Z","iopub.status.idle":"2021-09-26T23:35:52.45038Z","shell.execute_reply.started":"2021-09-26T23:35:52.436565Z","shell.execute_reply":"2021-09-26T23:35:52.449619Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### NN Model1","metadata":{}},{"cell_type":"code","source":"def nn_model_nn1(hyperparams, train_df, train_index, cval_index, features, verbose=True):\n    categorical_features = ['stock_id']\n    numerical_features = features.copy()\n    numerical_features.remove('stock_id')\n    \n    X_train_cat, X_train_num = train_df.loc[train_index, categorical_features], train_df.loc[train_index, numerical_features]\n    X_cval_cat,  X_cval_num  = train_df.loc[cval_index, categorical_features], train_df.loc[cval_index, numerical_features]\n    Y_train, Y_cval = train_df.loc[train_index, 'target'].values, train_df.loc[cval_index, 'target'].values\n    if verbose: print('X_train_cat.shape:', X_train_cat.shape, 'X_cval_cat.shape:', X_cval_cat.shape, 'Y_train.shape:', Y_train.shape)\n    if verbose: print('X_train_num.shape:', X_train_num.shape, 'X_cval_num.shape:', X_cval_num.shape, 'Y_cval.shape :', Y_cval.shape)\n    \n    cat_input = Input(shape=(1,), name='stock_id')\n    emb = Embedding(128, 24, input_length=1, name='embedding_inp')(cat_input)\n    emb = Flatten()(emb)\n    num_input = Input(shape=X_train_num.shape[1], name='numerical_input')\n    out = Concatenate()([emb, num_input])\n    for i in range(hyperparams['layers']):\n        out = Dense(units=hyperparams['units'], activation=hyperparams['activation'], kernel_regularizer=regularizers.l2(hyperparams['l2_regularization']))(out)\n        if hyperparams['dropout'] > 0:\n            out = Dropout(hyperparams['dropout'], seed=GLOBAL_SEED_VALUE)(out)\n    out = Dense(128, activation=hyperparams['activation'])(out)\n    out = Dense(1)(out)\n    model = Model(inputs=[cat_input, num_input], outputs=out)\n    model.compile(Adam(lr=hyperparams['learning_rate'], decay=5e-4), loss=my_rmspe)\n    escb = EarlyStopping(monitor='val_loss', mode='auto', restore_best_weights=True, patience=20, verbose=True)\n    class CustomCallback(Callback):\n        def __init__(self, verbose=False):\n            super(CustomCallback, self).__init__()\n            self.verbose = verbose\n        def on_epoch_end(self, epoch, logs=None):\n            if self.verbose and (epoch+1)%50 == 0: print(f'  --> epoch {epoch+1} completed')\n    \n    history = model.fit([X_train_cat, X_train_num], Y_train, validation_data=([X_cval_cat, X_cval_num], Y_cval),\n                        batch_size=BATCH_SIZE, shuffle=True, verbose=verbose, epochs=hyperparams['epochs'], callbacks=[escb, CustomCallback(not verbose)])\n    eval_rmspe = model.evaluate([X_cval_cat, X_cval_num], Y_cval, verbose=0, batch_size=BATCH_SIZE)\n    if verbose: print(' eval_rmspe:', round(eval_rmspe, 3))\n    return model, eval_rmspe, history","metadata":{"execution":{"iopub.status.busy":"2021-09-26T23:35:52.516241Z","iopub.execute_input":"2021-09-26T23:35:52.516645Z","iopub.status.idle":"2021-09-26T23:35:52.532918Z","shell.execute_reply.started":"2021-09-26T23:35:52.516608Z","shell.execute_reply":"2021-09-26T23:35:52.531989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### NN model LSTM","metadata":{}},{"cell_type":"code","source":"def create_lstm_model(hyperparams, features, verbose):\n    n_lstm_cells = len(TIMEID_SUBWINS)\n    n_features   = int((len(features) - 1)/n_lstm_cells)\n    \n    cat_input = Input(shape=(1,), name='I1')\n    emb = Embedding(128, 24, input_length=1, name='E1')(cat_input)\n    emb = Flatten(name='F1')(emb)\n    \n    X  = Input(shape=(len(features)-1), name='i_num') # for now\n    a0 = Input(shape=(hyperparams['lstm_units'],), name='a0')\n    c0 = Input(shape=(hyperparams['lstm_units'],), name='c0')\n    a  = a0\n    c  = c0\n    \n    last_out = None\n    for t in range(n_lstm_cells):\n        x = X[:, int(t*n_features):int((t+1)*n_features)]\n        x = Reshape((1, n_features), name=f'R{t}')(x)\n        a, _, c = LSTM(hyperparams['lstm_units'], return_state = True, name=f'LSTM{t}')(inputs=x, initial_state=[a, c])\n        last_out = a\n    \n    O = Concatenate(name='concat')([emb,last_out])\n    O = Dense(units=hyperparams['units'], activation=hyperparams['activation'], kernel_regularizer=regularizers.l2(hyperparams['l2_regularization']), name='dense1')(O)\n    if hyperparams['dropout'] > 0:\n        O = Dropout(hyperparams['dropout'], seed=GLOBAL_SEED_VALUE, name='dropout1')(O)\n    \n    O = Dense(units=hyperparams['units'], activation=hyperparams['activation'], kernel_regularizer=regularizers.l2(hyperparams['l2_regularization']), name='dense2')(O)\n    if hyperparams['dropout'] > 0:\n        O = Dropout(hyperparams['dropout'], seed=GLOBAL_SEED_VALUE, name='dropout2')(O)\n    \n    O = Dense(128, activation=hyperparams['activation'], name='dense3')(O)\n    O = Dense(1, activation='linear', name='output')(O)\n    \n    model = Model(inputs=[cat_input, X, a0, c0], outputs=O)\n    print(' Using LSTM model: lstm_units: {}, n_features: {} --> {} layers, {} params'.format(\n        hyperparams['lstm_units'], n_features, len(model.layers), model.count_params()))\n    if verbose: model.summary()\n    return model\n\ndef nn_model_lstm(hyperparams, train_df, train_index, cval_index, features, verbose=True):\n    lstm_units = hyperparams['lstm_units']\n    categorical_features = ['stock_id']\n    numerical_features = features.copy()\n    numerical_features.remove('stock_id')\n    \n    X_train_cat, X_train_num = train_df.loc[train_index, categorical_features], train_df.loc[train_index, numerical_features]\n    X_cval_cat,  X_cval_num  = train_df.loc[cval_index, categorical_features], train_df.loc[cval_index, numerical_features]\n    Y_train, Y_cval = train_df.loc[train_index, 'target'].values, train_df.loc[cval_index, 'target'].values\n    if verbose: print('X_train_cat.shape:', X_train_cat.shape, 'X_cval_cat.shape:', X_cval_cat.shape, 'Y_train.shape:', Y_train.shape)\n    if verbose: print('X_train_num.shape:', X_train_num.shape, 'X_cval_num.shape:', X_cval_num.shape, 'Y_cval.shape :', Y_cval.shape)\n        \n    model = create_lstm_model(hyperparams, features, verbose)\n    model.compile(Adam(lr=hyperparams['learning_rate'], decay=5e-4), loss=my_rmspe)\n    escb = EarlyStopping(monitor='val_loss', mode='auto', restore_best_weights=True, patience=20, verbose=True)\n    class CustomCallback(Callback):\n        def __init__(self, verbose=False):\n            super(CustomCallback, self).__init__()\n            self.verbose = verbose\n        def on_epoch_end(self, epoch, logs=None):\n            if self.verbose and (epoch+1)%50 == 0: print(f'  Epoch {epoch+1} completed')\n    \n    a0_train = np.zeros((X_train_cat.shape[0], lstm_units))\n    c0_train = np.zeros((X_train_cat.shape[0], lstm_units))\n    a0_cval  = np.zeros((X_cval_cat.shape[0],  lstm_units))\n    c0_cval  = np.zeros((X_cval_cat.shape[0],  lstm_units))\n    history = model.fit([X_train_cat, X_train_num, a0_train, c0_train], Y_train, validation_data=([X_cval_cat, X_cval_num, a0_cval, a0_cval], Y_cval),\n                        batch_size=BATCH_SIZE, shuffle=True, verbose=verbose, epochs=hyperparams['epochs'], callbacks=[escb, CustomCallback(not verbose)])\n    eval_rmspe = model.evaluate([X_cval_cat, X_cval_num, a0_cval, a0_cval], Y_cval, verbose=0, batch_size=BATCH_SIZE)\n    if verbose: print(' eval_rmspe:', round(eval_rmspe, 3))\n    return model, eval_rmspe, history","metadata":{"execution":{"iopub.status.busy":"2021-09-27T01:12:19.323332Z","iopub.execute_input":"2021-09-27T01:12:19.323686Z","iopub.status.idle":"2021-09-27T01:12:19.348205Z","shell.execute_reply.started":"2021-09-27T01:12:19.323637Z","shell.execute_reply":"2021-09-27T01:12:19.347009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Common","metadata":{}},{"cell_type":"code","source":"def create_train_model_and_predict(model_name, model_type, submission_mode, train_df, test_df, features, n_kfolds, verbose=False):\n    print(f'\\nRunning {model_type} model _{model_name}_ with {n_kfolds} folds ...')\n    print(f' Inputs: train_df.shape: {train_df.shape}, train_df.shape: {train_df.shape}, {len(features)} features')\n    print(f' Submission mode {submission_mode}')\n    \n    if not submission_mode:\n        saved_y_true = test_df['target'].values\n        syt = test_df[['row_id', 'stock_id', 'time_id', 'target']]\n        syt = syt.rename(columns={'target':'y_true'})\n    \n    kfold_scores = {}\n    kfold_scores[model_name] = []\n    model_results = {}\n    prediction_label = f'prediction_{model_name}'\n    train_df[prediction_label] = 0 # initialize\n    test_df['target'] = 0 # initialize\n    \n    kf = KFold(n_splits=n_kfolds, shuffle=True, random_state=GLOBAL_SEED_VALUE)\n    print(f'\\nKFold: {kf}\\n')\n    \n    kfold_iter = 0\n    for train_index, cval_index in kf.split(train_df):\n        time_start = timer()\n        kfold_iter += 1\n        print(f'KFold Iteration {kfold_iter}: train size {len(train_index)}, cval size {len(cval_index)} ...')\n        if model_type == \"NN\":\n            if model_name == \"NN1\":\n                hyperparams = {'layers':3, 'units':1024, 'dropout':0.4, 'l2_regularization':0.005, 'learning_rate':0.0005, 'activation':'relu', 'epochs':1000}\n                model, eval_rmspe, history = nn_model_nn1(hyperparams, train_df, train_index, cval_index, features, verbose=verbose)\n                cval_predictions = nn_make_predictions(model, train_df.loc[cval_index, features], features,\n                                                       model_name=model_name, verbose=False).clip(0,1e10)\n            elif model_name == \"LSTM\":\n                hyperparams = {'layers':3, 'units':1024, 'dropout':0.3, 'l2_regularization':0.005, 'learning_rate':0.0005, 'activation':'relu', 'epochs':1000, 'lstm_units':128}\n                model, eval_rmspe, history = nn_model_lstm(hyperparams, train_df, train_index, cval_index, features, verbose=verbose)\n                cval_predictions = nn_make_predictions(model, train_df.loc[cval_index, features], features, lstm_units=hyperparams['lstm_units'],\n                                                       model_name=model_name, verbose=False).clip(0,1e10)\n            model_results[kfold_iter] = history\n        elif model_type == \"LGB\":\n            X_train, X_cval = train_df.loc[train_index, features], train_df.loc[cval_index, features]\n            Y_train, Y_cval = train_df.loc[train_index, 'target'].values, train_df.loc[cval_index, 'target'].values\n            print(' X_train.shape:', X_train.shape, 'X_cval.shape:', X_cval.shape, 'Y_train.shape:', Y_train.shape, 'Y_cval.shape:', Y_cval.shape)\n            model = lgb_model_1(model_name, X_train, Y_train, X_cval, Y_cval, features)\n            cval_predictions = model.predict(X_cval)\n            model_results[kfold_iter] = pd.DataFrame({'Feature': model.feature_name(), 'Importance': model.feature_importance(importance_type='gain')})\n\n        train_df.loc[cval_index, prediction_label] = cval_predictions\n        cval_rmspe = round(rmspe(y_true=train_df.loc[cval_index, 'target'].values, y_pred=cval_predictions), 3)\n        print(f' KFold iteration {kfold_iter} score = {cval_rmspe}')\n        kfold_scores[model_name].append(cval_rmspe)\n        if model_type == \"NN\":\n            if model_name == \"NN1\":\n                test_df['target'] = test_df['target'] + nn_make_predictions(model, test_df, features, model_name=model_name,\n                                                                            verbose=False).clip(0,1e10)\n            elif model_name == \"LSTM\":\n                test_df['target'] = test_df['target'] + nn_make_predictions(model, test_df, features, model_name=model_name, lstm_units=hyperparams['lstm_units'],\n                                                                            verbose=False).clip(0,1e10)\n            del model\n            K.clear_session()\n        else:\n            test_df['target'] = test_df['target'] + model.predict(test_df[features]).clip(0,1e10)\n            del model\n        time_end = timer()\n        print(f'KFold iteration {kfold_iter} completed!!!, Took {round(time_end-time_start, 0)} seconds\\n')\n        \n    avg_cval_rmspe = round(rmspe(y_true = train_df['target'].values, y_pred = train_df[prediction_label].values), 3)\n    avg_eval_r2    = round(r2_score(y_true = train_df['target'].values, y_pred = train_df[prediction_label].values), 3)\n    print(f'After training, avg_cval_rmspe = {avg_cval_rmspe}, Fold Scores: {kfold_scores[model_name]}, R2 score = {avg_eval_r2}')\n    \n    test_df['target'] = test_df['target'] / n_kfolds\n    print('\\nFew test predictions ...')\n    display(test_df[['row_id', 'target']].head(3))\n    model_test_predictions = test_df[['row_id', 'target']]\n    if not submission_mode:\n        err_df = pd.merge(syt, model_test_predictions, how='left', on='row_id')\n        err_df = err_df.rename(columns={'target':'y_pred'})\n        err_df['key'] = 'k-' + err_df['row_id']\n        err_df = err_df[['key', 'row_id', 'stock_id', 'time_id', 'y_true', 'y_pred']]\n        err_df.to_csv(f'err_df-{model_name}.csv', index=False)\n    \n    if not submission_mode:\n        test_rmspe = round(rmspe(y_true = saved_y_true, y_pred = model_test_predictions['target'].values), 3)\n        test_r2    = round(r2_score(y_true = saved_y_true, y_pred = model_test_predictions['target'].values), 3)\n        print(f'On test data, RMSPE = {test_rmspe}, R2 score = {test_r2}')\n        \n    return model_results, model_test_predictions","metadata":{"execution":{"iopub.status.busy":"2021-09-27T01:12:19.349936Z","iopub.execute_input":"2021-09-27T01:12:19.350338Z","iopub.status.idle":"2021-09-27T01:12:19.376106Z","shell.execute_reply.started":"2021-09-27T01:12:19.350302Z","shell.execute_reply":"2021-09-27T01:12:19.375026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Run all -->","metadata":{}},{"cell_type":"code","source":"gc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-09-27T01:12:19.378291Z","iopub.execute_input":"2021-09-27T01:12:19.378579Z","iopub.status.idle":"2021-09-27T01:12:19.876418Z","shell.execute_reply.started":"2021-09-27T01:12:19.378542Z","shell.execute_reply":"2021-09-27T01:12:19.875537Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\nsubmission_mode = True\nverbose = False\nto_plot = True\ntest_df_fraction = 0.15\nn_kfolds = 4\nmodel_type = 'NN' # LGB / NN\nmodel_name = 'LSTM' # LGB1 / NN1 / LSTM\n\ntrain_df, test_df, features = prepare_training_and_test_data(submission_mode=submission_mode, verbose=verbose, test_df_fraction=test_df_fraction)\ngc.collect()\n#'''\nmodel_results, submission = create_train_model_and_predict(model_name=model_name, model_type=model_type, submission_mode=submission_mode, n_kfolds=n_kfolds,\n                                                           train_df=train_df, test_df=test_df, features=features, verbose=verbose)\nsubmission.to_csv('submission.csv', index=False)\n#'''\n\n#optimal_params = run_bayes_optimization(train_df, test_df, features, n_calls=50)","metadata":{"execution":{"iopub.status.busy":"2021-09-27T01:12:19.879688Z","iopub.execute_input":"2021-09-27T01:12:19.879949Z","iopub.status.idle":"2021-09-27T01:35:26.710003Z","shell.execute_reply.started":"2021-09-27T01:12:19.879924Z","shell.execute_reply":"2021-09-27T01:35:26.709195Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_lgb(axs, model_perf):\n    model_perf.sort_values(by='Importance', inplace=True)\n    model_perf = model_perf.nlargest(25,'Importance', keep='first').sort_values(by='Importance', ascending=True)\n    model_perf[['Importance', 'Feature']].plot(kind = 'barh', x = 'Feature', ax=axs, color = 'blue', fontsize=11)\n    return\n\ndef plot_nn(axs, history, fig_title):\n    hist = pd.DataFrame(history.history)\n    hist['epoch'] = history.epoch; hist = hist.iloc[1:]\n    axs.plot(hist['epoch'], hist['loss'], color='red')\n    axs.plot(hist['epoch'], hist['val_loss'], color='blue')\n    axs.set_title(fig_title); axs.grid(True)\n    return\n\nplt.rcParams[\"figure.figsize\"] = (20,6)\nplt.rcParams[\"font.size\"] = 9\nplt.rcParams[\"font.weight\"] = \"bold\"\nif model_type == 'LGB' and to_plot:\n    fig, axs = plt.subplots(nrows=1, ncols=2)\n    plot_lgb(axs[0], model_results[1]); plot_lgb(axs[1], model_results[2])\n    fig.tight_layout(); plt.show()\n    fig, axs = plt.subplots(nrows=1, ncols=2)\n    plot_lgb(axs[0], model_results[3]); plot_lgb(axs[1], model_results[4])\n    fig.tight_layout(); plt.show()\nelif model_type == 'NN' and to_plot:\n    fig, axs = plt.subplots(nrows=1, ncols=2)\n    plot_nn(axs[0], model_results[1], 'KFold Model 1'); plot_nn(axs[1], model_results[2], 'KFold Model 2')\n    fig.tight_layout(); plt.show()\n    fig, axs = plt.subplots(nrows=1, ncols=2)\n    plot_nn(axs[0], model_results[3], 'KFold Model 3'); plot_nn(axs[1], model_results[4], 'KFold Model 4')\n    fig.tight_layout(); plt.show()\nprint('Done')","metadata":{"execution":{"iopub.status.busy":"2021-09-27T01:35:26.71293Z","iopub.execute_input":"2021-09-27T01:35:26.713195Z","iopub.status.idle":"2021-09-27T01:35:27.367494Z","shell.execute_reply.started":"2021-09-27T01:35:26.713168Z","shell.execute_reply":"2021-09-27T01:35:27.366698Z"},"trusted":true},"execution_count":null,"outputs":[]}]}