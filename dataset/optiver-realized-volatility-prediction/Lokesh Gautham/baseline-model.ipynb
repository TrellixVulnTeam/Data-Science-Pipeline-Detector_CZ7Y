{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# !pip install torchsummary","metadata":{"execution":{"iopub.status.busy":"2021-07-22T05:45:33.793486Z","iopub.execute_input":"2021-07-22T05:45:33.793962Z","iopub.status.idle":"2021-07-22T05:46:06.700193Z","shell.execute_reply.started":"2021-07-22T05:45:33.793867Z","shell.execute_reply":"2021-07-22T05:46:06.698901Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport random\nimport plotly.express as px\nfrom multiprocessing import Pool\nimport matplotlib.pyplot as plt\nimport os\nfrom sklearn.metrics import r2_score\nimport glob\nfrom sklearn.metrics import r2_score\n\nimport torch\nimport wandb\nimport torch.nn as nn\nimport torch.optim as optim\n# from torchsummary import summary\nfrom torch.utils.data import DataLoader, Dataset, random_split\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, PowerTransformer","metadata":{"execution":{"iopub.status.busy":"2021-07-21T18:25:49.074805Z","iopub.execute_input":"2021-07-21T18:25:49.075096Z","iopub.status.idle":"2021-07-21T18:25:53.429138Z","shell.execute_reply.started":"2021-07-21T18:25:49.075065Z","shell.execute_reply":"2021-07-21T18:25:53.427883Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def log_return(list_stock_prices):\n    logs = np.log(list_stock_prices)\n    return np.diff(logs, prepend=logs[0])\n\ndef realized_volatility(series_log_return):\n    return np.sqrt(np.sum(series_log_return**2))","metadata":{"execution":{"iopub.status.busy":"2021-07-21T18:25:53.431977Z","iopub.execute_input":"2021-07-21T18:25:53.432446Z","iopub.status.idle":"2021-07-21T18:25:53.43935Z","shell.execute_reply.started":"2021-07-21T18:25:53.432395Z","shell.execute_reply":"2021-07-21T18:25:53.437767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_params(book_example):\n    buckets_index = pd.Index(np.arange(600))\n    book_example = book_example.set_index('seconds_in_bucket').reindex(buckets_index)\n    book_example = book_example.fillna(method='ffill')\n    \n    bid_prices1 = book_example['bid_price1'].to_numpy()\n    bid_prices2 = book_example['bid_price2'].to_numpy()\n    bid_sizes1 = book_example['bid_size1'].to_numpy()\n    bid_sizes2 = book_example['bid_size2'].to_numpy()\n    ask_prices1 = book_example['ask_price1'].to_numpy()\n    ask_prices2 = book_example['ask_price2'].to_numpy()\n    ask_sizes1 = book_example['ask_size1'].to_numpy()\n    ask_sizes2 = book_example['ask_size2'].to_numpy()\n    \n    trade_vols1 = (ask_sizes1 + bid_sizes1)[1:]\n    trade_vols2 = (ask_sizes2 + bid_sizes2)[1:]\n    trade_diffs1 = (ask_sizes1 - bid_sizes1)[1:]\n    trade_diffs2 = (ask_sizes2 - bid_sizes2)[1:]\n    \n    spreads1 = ((ask_prices1 / bid_prices1) - 1)[1:]\n    spreads2 = ((ask_prices2 / bid_prices2) - 1)[1:]\n    \n    waps1 = (bid_prices1 * ask_sizes1 + ask_prices1 * bid_sizes1) / (bid_sizes1 + ask_sizes1)\n    waps2 = (bid_prices2 * ask_sizes2 + ask_prices2 * bid_sizes2) / (bid_sizes2 + ask_sizes2)\n    \n    logs1 = log_return(waps1)[1:]\n    logs2 = log_return(waps2)[1:]\n    \n    waps1 = waps1[1:]\n    waps2 = waps2[1:]\n    \n    return [\n        waps1.mean(),\n        waps2.mean(),\n        waps1[450:500].mean(),\n        waps1[500:550].mean(),\n        waps1[550:600].mean(),\n        waps2[450:500].mean(),\n        waps2[500:550].mean(),\n        waps2[550:600].mean(),\n        waps1.std(),\n        waps2.std(),\n        waps1[550:600].std(),\n        waps2[550:600].std(),\n        logs1.mean(),\n        logs2.mean(),\n        logs1[450:500].mean(),\n        logs1[500:550].mean(),\n        logs1[550:600].mean(),\n        logs2[450:500].mean(),\n        logs2[500:550].mean(),\n        logs2[550:600].mean(),\n        trade_vols1.mean(),\n        trade_vols2.mean(),\n        trade_vols1[550:600].mean(),\n        trade_vols2[550:600].mean(),\n        trade_diffs1.mean(),\n        trade_diffs2.mean(),\n        trade_diffs1[550:600].mean(),\n        trade_diffs2[550:600].mean(),\n        realized_volatility(logs1),\n        realized_volatility(logs2),\n        int(book_example[\"time_id\"].mean())\n    ]","metadata":{"execution":{"iopub.status.busy":"2021-07-21T18:25:53.441272Z","iopub.execute_input":"2021-07-21T18:25:53.441593Z","iopub.status.idle":"2021-07-21T18:25:53.463921Z","shell.execute_reply.started":"2021-07-21T18:25:53.44156Z","shell.execute_reply":"2021-07-21T18:25:53.462564Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('../input/optiver-realized-volatility-prediction/train.csv')\ntrain['row_id'] = train['stock_id'].astype(str) + '-' + train['time_id'].astype(str)\ntrain = train[['row_id','target']].set_index(\"row_id\")\nlist_order_book_file_train = glob.glob('/kaggle/input/optiver-realized-volatility-prediction/book_train.parquet/*')\ncolumns = [\n    \"wap1\", \"wap2\", \"wap1_1\", \"wap1_2\", \"wap1_3\",\n    \"wap2_1\", \"wap2_2\", \"wap2_3\", \"wap1_std\",\n    \"wap2_std\", \"wap1l_std\", \"wap2l_std\", \"log1\",\n    \"log2\", \"log1_1\", \"log1_2\", \"log1_3\", \"log2_1\",\n    \"log2_2\", \"log2_3\", \"volume1\", \"volume2\", \"volume1l\",\n    \"volume2l\", \"diff1\", \"diff2\", \"diff1l\", \"diff2l\",\n    \"vol1\", \"vol2\", \"stock_id\", \"target\"\n]\ndef generate_feature_set(list_file):\n    data_list = []\n    for file in list_file:\n        book_example = pd.read_parquet(file)\n        groups = book_example.groupby(['time_id'])\n        pool = Pool(processes=None)\n        data_list += [\n            entry[:-1] + [int(file.split('=')[1]), train.loc[f\"{file.split('=')[1]}-{entry[-1]}\", \"target\"]]\n            for entry in pool.map(get_params, [groups.get_group(x) for x in list(groups.groups)])\n        ]\n        pool.close()\n        pool.join()\n    return pd.DataFrame(data_list, columns=columns)\ngenerate_feature_set(list_order_book_file_train).to_csv(\"feature_set.csv\", encoding='utf-8', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-07-21T18:25:53.465761Z","iopub.execute_input":"2021-07-21T18:25:53.466301Z","iopub.status.idle":"2021-07-21T18:43:28.945841Z","shell.execute_reply.started":"2021-07-21T18:25:53.46625Z","shell.execute_reply":"2021-07-21T18:43:28.943876Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def rmspe(y_true, y_pred):\n    return torch.sqrt(torch.mean(((y_true - y_pred) / y_true) ** 2))","metadata":{"execution":{"iopub.status.busy":"2021-07-21T18:43:28.948334Z","iopub.execute_input":"2021-07-21T18:43:28.948745Z","iopub.status.idle":"2021-07-21T18:43:28.955672Z","shell.execute_reply.started":"2021-07-21T18:43:28.948677Z","shell.execute_reply":"2021-07-21T18:43:28.954242Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epochs = 100\nbatch_size = 512\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nsplit_size = 0.15\nfeature_size = 31\nlr = 2e-3\nlr_gamma = 0.9\nbetas = (0.5, 0.5)","metadata":{"execution":{"iopub.status.busy":"2021-07-21T18:46:30.134593Z","iopub.execute_input":"2021-07-21T18:46:30.135043Z","iopub.status.idle":"2021-07-21T18:46:30.141004Z","shell.execute_reply.started":"2021-07-21T18:46:30.135006Z","shell.execute_reply":"2021-07-21T18:46:30.139733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class FeatureDataset(Dataset):\n    def __init__(self, file_path):\n        self.feature_standardizer = StandardScaler()\n        self.target_boxcox = PowerTransformer(method='box-cox', standardize=False)\n        self.target_scaler = MinMaxScaler()\n        \n        dataframe = pd.read_csv(file_path)\n        self.stocks_target_mean_val = dataframe.groupby(\"stock_id\")[\"target\"].mean()\n        dataframe.insert(loc=dataframe.shape[1] - 2, column=\"stock_target_mean\", value=dataframe['stock_id'].map(self.stocks_target_mean_val).values)\n        X = self.feature_standardizer.fit_transform(dataframe.iloc[:, :-2].to_numpy())\n        y = self.target_scaler.fit_transform(self.target_boxcox.fit_transform(dataframe.iloc[:, -1].to_numpy().reshape(-1, 1)))\n        # y = dataframe.iloc[:, -1].to_numpy()\n        \n        self.X_train = torch.tensor(X, dtype=torch.float32)\n        self.y_train = torch.tensor(y, dtype=torch.float32)\n        \n    def inverse_scale_transform(self, x):\n        if not torch.is_tensor(x):\n            x = torch.tensor(x, dtype=torch.float32)\n        \n        # Invert the 0-1 Scaler\n        x_mult = x * torch.tensor(self.target_scaler.data_max_ - self.target_scaler.data_min_, dtype=torch.float32, requires_grad=False)\n        x_scaled = x_mult + torch.tensor(self.target_scaler.data_min_, dtype=torch.float32, requires_grad=False)\n        \n        # Invert the Box-Cox Scaler\n        lda = torch.tensor(self.target_boxcox.lambdas_[0], dtype=torch.float32, requires_grad=False)\n        x_bcox_inv = torch.exp(torch.log(1 + lda * x_scaled) / lda)\n        return x_bcox_inv\n    \n    def __len__(self):\n        return len(self.y_train)\n    \n    def __getitem__(self, idx):\n        return self.X_train[idx], self.y_train[idx]\n\n# pre_path = \"../input/feature-set/\"\npre_path = \"\"\nfeature_set = FeatureDataset(pre_path + \"feature_set.csv\")\nval_size = int(split_size * len(feature_set))\ntrain_set, val_set = torch.utils.data.random_split(feature_set, [len(feature_set) - val_size, val_size])\n\nprint(f\"{len(train_set)} training samples\")\nprint(f\"{len(val_set)} validation samples\")\n\ntrain_loader = DataLoader(train_set, shuffle=True, batch_size=batch_size)\nval_loader = DataLoader(val_set, shuffle=True, batch_size=batch_size)","metadata":{"execution":{"iopub.status.busy":"2021-07-21T18:46:19.261687Z","iopub.execute_input":"2021-07-21T18:46:19.262094Z","iopub.status.idle":"2021-07-21T18:46:25.923574Z","shell.execute_reply.started":"2021-07-21T18:46:19.262053Z","shell.execute_reply":"2021-07-21T18:46:25.92254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.model = nn.Sequential(\n            nn.Linear(feature_size, 42),\n            nn.ReLU(),\n            nn.Linear(42, 1),\n            nn.Sigmoid()\n        )\n    \n    def forward(self, x):\n        return self.model(x)","metadata":{"execution":{"iopub.status.busy":"2021-07-21T18:46:32.884578Z","iopub.execute_input":"2021-07-21T18:46:32.884975Z","iopub.status.idle":"2021-07-21T18:46:32.890558Z","shell.execute_reply.started":"2021-07-21T18:46:32.884943Z","shell.execute_reply":"2021-07-21T18:46:32.889682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Model().to(device)\n# summary(model, (batch_size, feature_size))\n\ncriterion = nn.MSELoss()\ncriterion = rmspe\n# optimizer = optim.Adam(model.parameters(), lr=lr, betas=betas)\noptimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\nscheduler = optim.lr_scheduler.ExponentialLR(optimizer, lr_gamma)\n# scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, min_lr=3e-6, factor=0.1)","metadata":{"execution":{"iopub.status.busy":"2021-07-21T18:46:33.171324Z","iopub.execute_input":"2021-07-21T18:46:33.171917Z","iopub.status.idle":"2021-07-21T18:46:33.185478Z","shell.execute_reply.started":"2021-07-21T18:46:33.171879Z","shell.execute_reply":"2021-07-21T18:46:33.18354Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# wandb.init(project=\"Optiver-Kaggle-Contest\", entity=\"ag8011\")\n\nfor epoch in range(epochs):\n    model.train()\n    train_loss = 0.0\n    train_r2 = 0.0\n    for features, values in train_loader:\n        features = features.to(device)\n        values = values.to(device)\n        output = model(features)\n        \n        # Invert the scaling of the outputs\n        values_scaled = feature_set.inverse_scale_transform(values.cpu())\n        output_scaled = feature_set.inverse_scale_transform(output.cpu())\n        \n#         print(np.stack((values_scaled.cpu(), output_scaled.detach().cpu()), axis=-1))\n        \n        loss = criterion(values_scaled, output_scaled)\n        # print(np.stack((output.detach().cpu().reshape(-1), values.cpu()), axis=-1))\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        # train_loss += rmspe(values, output).item()\n        train_loss += loss.item()\n        train_r2 += r2_score(values_scaled.cpu(), output_scaled.detach().cpu())\n    \n    model.eval()\n    val_loss = 0.0\n    val_r2 = 0.0\n    for features, values in val_loader:\n        features = features.to(device)\n        values = values.to(device)\n        output = model(features)\n        \n        # Invert the scaling of the outputs\n        values_scaled = feature_set.inverse_scale_transform(values.cpu())\n        output_scaled = feature_set.inverse_scale_transform(output.cpu())\n        \n        loss = criterion(values_scaled, output_scaled)\n        # print(np.stack((values.cpu(), output.detach().cpu()), axis=-1))\n        val_loss += loss.item()\n        # val_loss += rmspe(values, output).item()\n        val_r2 += r2_score(values_scaled.cpu(), output_scaled.cpu().detach())\n    \n    # scheduler.step(train_loss / len(train_loader))\n    scheduler.step()\n#     wandb.log({\"Learning Rate\": optimizer.param_groups[0]['lr']})\n    \n    print(f\"Iteration {epoch}, Train RMSPE: {train_loss / len(train_loader)}, Val RMSPE: {val_loss / len(val_loader)}, Train R2: {train_r2 / len(train_loader)}, Val R2: {val_r2 / len(val_loader)}\")\n#     wandb.log({\n#         \"Train RMSPE\": train_loss / len(train_loader),\n#         \"Val RMSPE\": val_loss / len(val_loader),\n#         \"Train R2\": train_r2 / len(train_loader),\n#         \"Val R2\": val_r2 / len(val_loader)\n#     })","metadata":{"execution":{"iopub.status.busy":"2021-07-21T18:46:33.951226Z","iopub.execute_input":"2021-07-21T18:46:33.951915Z","iopub.status.idle":"2021-07-21T18:46:47.969112Z","shell.execute_reply.started":"2021-07-21T18:46:33.951874Z","shell.execute_reply":"2021-07-21T18:46:47.967946Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"list_order_book_file_test = glob.glob('/kaggle/input/optiver-realized-volatility-prediction/book_test.parquet/*')\noutputs = []\nwith torch.no_grad():\n    for file in list_order_book_file_test:\n        book_example = pd.read_parquet(file)\n        groups = book_example.groupby(['time_id'])\n        for time_id in list(groups.groups):\n            test_features = get_params(groups.get_group(time_id))\n            test_features.insert(-1, feature_set.stocks_target_mean_val[test_features[-1]])\n            X = np.array(test_features[:-1], dtype=np.float32).reshape(1, -1)\n            test_features_scaled = feature_set.feature_standardizer.transform(X)\n            model_out = model(torch.tensor(test_features[:-1], dtype=torch.float32))\n            model_scaled = feature_set.inverse_scale_transform(model_out).cpu().item()\n            outputs.append([f\"{int(file.split('=')[1])}-{int(time_id)}\", model_scaled])\npd.DataFrame(outputs, columns=[\"row_id\", \"target\"]).to_csv('submission.csv',index = False)","metadata":{"execution":{"iopub.status.busy":"2021-07-21T18:46:47.971102Z","iopub.execute_input":"2021-07-21T18:46:47.971689Z","iopub.status.idle":"2021-07-21T18:46:48.013714Z","shell.execute_reply.started":"2021-07-21T18:46:47.971631Z","shell.execute_reply":"2021-07-21T18:46:48.012449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}