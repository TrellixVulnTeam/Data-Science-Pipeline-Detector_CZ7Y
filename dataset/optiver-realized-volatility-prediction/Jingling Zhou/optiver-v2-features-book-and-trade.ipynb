{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip -q install ../input/pytorchtabnet/pytorch_tabnet-3.1.1-py3-none-any.whl","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-09-26T03:15:21.724438Z","iopub.execute_input":"2021-09-26T03:15:21.724971Z","iopub.status.idle":"2021-09-26T03:15:31.35509Z","shell.execute_reply.started":"2021-09-26T03:15:21.724887Z","shell.execute_reply":"2021-09-26T03:15:31.354075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy.matlib\n\nimport matplotlib.gridspec as gridspec\nfrom matplotlib.ticker import MaxNLocator\n\nfrom scipy import stats\nfrom scipy.stats import norm\nfrom joblib import Parallel, delayed\n\nimport shutil\nimport glob\nimport gc \nimport os\n\nfrom sklearn.preprocessing import MinMaxScaler, LabelEncoder, StandardScaler\nfrom sklearn.metrics import r2_score\nfrom sklearn.cluster import KMeans\nfrom sklearn.model_selection import KFold\n\nfrom pytorch_tabnet.metrics import Metric\nfrom pytorch_tabnet.tab_model import TabNetRegressor\n\nimport torch\nfrom torch.optim import Adam, SGD\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingWarmRestarts\n\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2021-09-26T03:15:45.514795Z","iopub.execute_input":"2021-09-26T03:15:45.515079Z","iopub.status.idle":"2021-09-26T03:15:45.526733Z","shell.execute_reply.started":"2021-09-26T03:15:45.515049Z","shell.execute_reply":"2021-09-26T03:15:45.525969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def read_train_test():\n    # Function to read our base train and test set\n    \n    train = pd.read_csv('../input/optiver-realized-volatility-prediction/train.csv')\n    test = pd.read_csv('../input/optiver-realized-volatility-prediction/test.csv')\n\n    # Create a key to merge with book and trade data\n    train['row_id'] = train['stock_id'].astype(str) + '-' + train['time_id'].astype(str)\n    test['row_id'] = test['stock_id'].astype(str) + '-' + test['time_id'].astype(str)\n    print(f'Our training set has {train.shape[0]} rows')\n    print(f'Our test set has {test.shape[0]} rows')\n    print(f'Our training set has {train.isna().sum().sum()} missing values')\n    print(f'Our test set has {test.isna().sum().sum()} missing values')\n    \n    return train, test","metadata":{"execution":{"iopub.status.busy":"2021-09-26T03:15:53.675586Z","iopub.execute_input":"2021-09-26T03:15:53.675892Z","iopub.status.idle":"2021-09-26T03:15:53.683782Z","shell.execute_reply.started":"2021-09-26T03:15:53.675862Z","shell.execute_reply":"2021-09-26T03:15:53.682799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train, test = read_train_test()","metadata":{"execution":{"iopub.status.busy":"2021-09-26T03:15:59.186546Z","iopub.execute_input":"2021-09-26T03:15:59.186821Z","iopub.status.idle":"2021-09-26T03:16:00.763685Z","shell.execute_reply.started":"2021-09-26T03:15:59.186794Z","shell.execute_reply":"2021-09-26T03:16:00.762692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def book_and_trade_preprocessor(file_path_book, file_path_trade):\n    df_book = pd.read_parquet(file_path_book)\n    df_trade = pd.read_parquet(file_path_trade)\n\n    df = df_book.merge(df_trade, on=[\"time_id\",\"seconds_in_bucket\"], how='outer').dropna(how='any', axis=0).reset_index(drop=True)\n    del df_book, df_trade\n    gc.collect()\n    df['bid_size_diff1'] = df[\"size\"] - df[\"bid_size1\"]\n    df['bid_size_diff2'] = df[\"size\"] - df[\"bid_size2\"]\n    df['ask_size_diff1'] = df[\"size\"] - df[\"ask_size1\"]\n    df['ask_size_diff2'] = df[\"size\"] - df[\"ask_size2\"]\n    df[\"ask_price_diff\"] = df[\"price\"] - (df[\"ask_price1\"] + df[\"ask_price2\"])/2.0\n    df[\"bid_price_diff\"] = df[\"price\"] - (df[\"bid_price1\"] + df[\"bid_price2\"])/2.0\n    # 计算交易价格 超过 卖一价的次数,表明买方提价促成交易\n    df['over_ask1'] = (df['price'] > df['ask_price1']).astype(int)\n    # 计算交易价格 低于 买一价的次数，表明卖方降价促成交易\n    df['below_bid1'] = (df['price'] < df['bid_price1']).astype(int)\n    # 计算交易量与出价量的比\n    df['ratio_bid'] = df['size'] / (df['bid_size1'] + df['bid_size2'])\n    # 计算交易量与要价量的比\n    df['ratio_ask'] = df['size'] / (df['ask_size1'] + df['ask_size2'])\n        \n    # Dict for aggregations : original feature\n    create_feature_dict = {\n        'bid_size_diff1':[np.sum],\n        'bid_size_diff2':[np.sum],\n        'ask_size_diff1':[np.sum],\n        'ask_size_diff2':[np.mean],\n        'ask_price_diff':[np.mean, realized_volatility],\n        'bid_price_diff':[np.mean, realized_volatility],\n        'over_ask1':[np.mean, np.sum],\n        'below_bid1':[np.mean, np.sum],\n        'ratio_bid':[np.mean, np.sum],\n        'ratio_ask':[np.mean, np.sum],\n    }\n    \n    # Function to get group stats for different windows (seconds in bucket)\n    def get_stats_window(seconds_in_bucket, add_suffix = False):\n        # Group by the window\n        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(create_feature_dict).reset_index()\n        # Rename columns joining suffix\n        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n        # Add a suffix to differentiate windows\n        if add_suffix:\n            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n        return df_feature\n    # Get the stats for different windows\n    df_feature = get_stats_window(seconds_in_bucket = 0, add_suffix = False)\n    df_feature_450 = get_stats_window(seconds_in_bucket = 450, add_suffix = True)\n    df_feature_300 = get_stats_window(seconds_in_bucket = 300, add_suffix = True)\n    df_feature_150 = get_stats_window(seconds_in_bucket = 150, add_suffix = True)\n    \n    # Merge all\n    df_feature = df_feature.merge(df_feature_450, how = 'left', left_on = 'time_id_', right_on = 'time_id__450')\n    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n    df_feature = df_feature.merge(df_feature_150, how = 'left', left_on = 'time_id_', right_on = 'time_id__150')\n\n    # Drop unnecesary time_ids\n    df_feature.drop(['time_id__450', 'time_id__300', 'time_id__150'], axis = 1, inplace = True)\n\n    df_feature = df_feature.add_prefix('book_and_trade_')\n    stock_id = file_path_book.split('=')[1]\n    df_feature['row_id'] = df_feature['book_and_trade_time_id_'].apply(lambda x:f'{stock_id}-{x}')\n    df_feature.drop(['book_and_trade_time_id_'], axis = 1, inplace = True)\n    return df_feature","metadata":{"execution":{"iopub.status.busy":"2021-09-26T03:19:28.816047Z","iopub.execute_input":"2021-09-26T03:19:28.816388Z","iopub.status.idle":"2021-09-26T03:19:28.836655Z","shell.execute_reply.started":"2021-09-26T03:19:28.816353Z","shell.execute_reply":"2021-09-26T03:19:28.836043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# data directory\ndata_dir = '../input/optiver-realized-volatility-prediction/'\n\ndef calc_wap1(df):\n    # Function to calculate first WAP\n    wap = (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']) / (df['bid_size1'] + df['ask_size1'])\n    return wap\n\ndef calc_wap2(df):\n    # Function to calculate second WAP\n    wap = (df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2']) / (df['bid_size2'] + df['ask_size2'])\n    return wap\n\ndef log_return(series):\n    # Function to calculate the log of the return\n    return np.log(series).diff()\n\ndef realized_volatility(series):\n    # Calculate the realized volatility\n    return np.sqrt(np.sum(series**2))\n\ndef count_unique(series):\n    # Function to count unique elements of a series\n    return len(np.unique(series))\n\ndef book_preprocessor(file_path):\n    # Function to preprocess book data (for each stock id)\n    \n    df = pd.read_parquet(file_path)\n    \n    # Calculate Wap\n    df['wap1'] = calc_wap1(df)\n    df['wap2'] = calc_wap2(df)\n    \n    # Calculate log returns\n    df['log_return1'] = df.groupby(['time_id'])['wap1'].apply(log_return)\n    df['log_return2'] = df.groupby(['time_id'])['wap2'].apply(log_return)\n    \n    # Calculate wap balance\n    df['wap_balance'] = abs(df['wap1'] - df['wap2'])\n    \n    # Calculate spread\n    df['price_spread'] = (df['ask_price1'] - df['bid_price1']) / ((df['ask_price1'] + df['bid_price1']) / 2)\n    df['price_spread2'] = (df['ask_price2'] - df['bid_price2']) / ((df['ask_price2'] + df['bid_price2']) / 2)\n    df['bid_spread'] = df['bid_price1'] - df['bid_price2']\n    df['ask_spread'] = df['ask_price1'] - df['ask_price2']\n    df[\"bid_ask_spread\"] = abs(df['bid_spread'] - df['ask_spread'])\n    df['total_volume'] = (df['ask_size1'] + df['ask_size2']) + (df['bid_size1'] + df['bid_size2'])\n    df['volume_imbalance'] = abs((df['ask_size1'] + df['ask_size2']) - (df['bid_size1'] + df['bid_size2']))\n    \n    # Dict for aggregations\n    create_feature_dict = {\n        'wap1': [np.sum, np.mean, np.std],\n        'wap2': [np.sum, np.mean, np.std],\n        'log_return1': [np.sum, realized_volatility, np.mean, np.std],\n        'log_return2': [np.sum, realized_volatility, np.mean, np.std],\n        'wap_balance': [np.sum, np.mean, np.std],\n        'price_spread':[np.sum, np.mean, np.std],\n        'price_spread2':[np.sum, np.mean, np.std],\n        'bid_spread':[np.sum, np.mean, np.std],\n        'ask_spread':[np.sum, np.mean, np.std],\n        'total_volume':[np.sum, np.mean, np.std],\n        'volume_imbalance':[np.sum, np.mean, np.std],\n        \"bid_ask_spread\":[np.sum, np.mean, np.std],\n    }\n    \n    def get_stats_window(seconds_in_bucket, add_suffix = False):\n        # Function to get group stats for different windows (seconds in bucket)\n        \n        # Group by the window\n        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(create_feature_dict).reset_index()\n        \n        # Rename columns joining suffix\n        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n        \n        # Add a suffix to differentiate windows\n        if add_suffix:\n            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n        return df_feature\n    \n    # Get the stats for different windows\n    df_feature = get_stats_window(seconds_in_bucket = 0, add_suffix = False)\n    df_feature_400 = get_stats_window(seconds_in_bucket = 400, add_suffix = True)\n    df_feature_300 = get_stats_window(seconds_in_bucket = 300, add_suffix = True)\n    df_feature_200 = get_stats_window(seconds_in_bucket = 200, add_suffix = True)\n    \n    # Merge all\n    df_feature = df_feature.merge(df_feature_400, how = 'left', left_on = 'time_id_', right_on = 'time_id__400')\n    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n    df_feature = df_feature.merge(df_feature_200, how = 'left', left_on = 'time_id_', right_on = 'time_id__200')\n\n    # Drop unnecesary time_ids\n    df_feature.drop(['time_id__400', 'time_id__300', 'time_id__200'], axis = 1, inplace = True)\n    \n    \n    # Create row_id so we can merge\n    stock_id = file_path.split('=')[1]\n    df_feature['row_id'] = df_feature['time_id_'].apply(lambda x: f'{stock_id}-{x}')\n    df_feature.drop(['time_id_'], axis = 1, inplace = True)\n    \n    return df_feature\n\n\ndef trade_preprocessor(file_path):\n    # Function to preprocess trade data (for each stock id)\n    \n    df = pd.read_parquet(file_path)\n    df['log_return'] = df.groupby('time_id')['price'].apply(log_return)\n    \n    # Dict for aggregations\n    create_feature_dict = {\n        'log_return':[realized_volatility],\n        'seconds_in_bucket':[count_unique],\n        'size':[np.sum, realized_volatility, np.mean, np.std, np.max, np.min],\n        'order_count':[np.mean,np.sum,np.max],\n    }\n    \n    def get_stats_window(seconds_in_bucket, add_suffix = False):\n        # Function to get group stats for different windows (seconds in bucket)\n        \n        # Group by the window\n        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(create_feature_dict).reset_index()\n        \n        # Rename columns joining suffix\n        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n        \n        # Add a suffix to differentiate windows\n        if add_suffix:\n            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n        return df_feature\n    \n    # Get the stats for different windows\n    df_feature = get_stats_window(seconds_in_bucket = 0, add_suffix = False)\n    df_feature_400 = get_stats_window(seconds_in_bucket = 400, add_suffix = True)\n    df_feature_300 = get_stats_window(seconds_in_bucket = 300, add_suffix = True)\n    df_feature_200 = get_stats_window(seconds_in_bucket = 200, add_suffix = True)\n    \n    def tendency(price, vol):    \n        df_diff = np.diff(price)\n        val = (df_diff/price[1:])*100\n        power = np.sum(val*vol[1:])\n        return(power)\n    \n    lis = []\n    for n_time_id in df['time_id'].unique():\n        df_id = df[df['time_id'] == n_time_id]        \n        tendencyV = tendency(df_id['price'].values, df_id['size'].values)      \n        f_max = np.sum(df_id['price'].values > np.mean(df_id['price'].values))\n        f_min = np.sum(df_id['price'].values < np.mean(df_id['price'].values))\n        df_max =  np.sum(np.diff(df_id['price'].values) > 0)\n        df_min =  np.sum(np.diff(df_id['price'].values) < 0)\n        abs_diff = np.median(np.abs( df_id['price'].values - np.mean(df_id['price'].values)))        \n        energy = np.mean(df_id['price'].values**2)\n        iqr_p = np.percentile(df_id['price'].values,75) - np.percentile(df_id['price'].values,25)\n        abs_diff_v = np.median(np.abs( df_id['size'].values - np.mean(df_id['size'].values)))        \n        energy_v = np.sum(df_id['size'].values**2)\n        iqr_p_v = np.percentile(df_id['size'].values,75) - np.percentile(df_id['size'].values,25)\n        \n        lis.append({'time_id':n_time_id,'tendency':tendencyV,'f_max':f_max,'f_min':f_min,'df_max':df_max,'df_min':df_min,\n                   'abs_diff':abs_diff,'energy':energy,'iqr_p':iqr_p,'abs_diff_v':abs_diff_v,'energy_v':energy_v,'iqr_p_v':iqr_p_v})\n    \n    df_lr = pd.DataFrame(lis)\n        \n   \n    df_feature = df_feature.merge(df_lr, how = 'left', left_on = 'time_id_', right_on = 'time_id')\n    \n    # Merge all\n    df_feature = df_feature.merge(df_feature_400, how = 'left', left_on = 'time_id_', right_on = 'time_id__400')\n    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n    df_feature = df_feature.merge(df_feature_200, how = 'left', left_on = 'time_id_', right_on = 'time_id__200')\n\n    # Drop unnecesary time_ids\n    df_feature.drop(['time_id__400', 'time_id__300', 'time_id__200','time_id'], axis = 1, inplace = True)\n    df_feature = df_feature.add_prefix('trade_')\n    stock_id = file_path.split('=')[1]\n    df_feature['row_id'] = df_feature['trade_time_id_'].apply(lambda x:f'{stock_id}-{x}')\n    df_feature.drop(['trade_time_id_'], axis = 1, inplace = True)\n    \n    def order_sum(df, sec:str):\n        new_col = 'size_tau' + sec\n        bucket_col = 'trade_seconds_in_bucket_count_unique' + sec\n        df[new_col] = np.sqrt(1/df[bucket_col])\n        \n        new_col2 = 'size_tau2' + sec\n        order_col = 'trade_order_count_sum' + sec\n        df[new_col2] = np.sqrt(1/df[order_col])\n        \n        if sec == '400_':\n            df['size_tau2_d'] = df['size_tau2_400'] - df['size_tau2']\n        \n\n    \n    for sec in ['','_200','_300','_400']:\n        order_sum(df_feature, sec)\n        \n    df_feature['size_tau2_d'] = df_feature['size_tau2_400'] - df_feature['size_tau2']\n    \n    return df_feature\n\n\ndef get_time_stock(df):\n    # Function to get group stats for the stock_id and time_id\n    \n    # Get realized volatility columns\n    vol_cols = ['log_return1_realized_volatility', 'log_return2_realized_volatility', 'log_return1_realized_volatility_400', 'log_return2_realized_volatility_400', \n                'log_return1_realized_volatility_300', 'log_return2_realized_volatility_300', 'log_return1_realized_volatility_200', 'log_return2_realized_volatility_200', \n                'trade_log_return_realized_volatility', 'trade_log_return_realized_volatility_400', 'trade_log_return_realized_volatility_300', 'trade_log_return_realized_volatility_200']\n\n    # Group by the stock id\n    df_stock_id = df.groupby(['stock_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n    \n    # Rename columns joining suffix\n    df_stock_id.columns = ['_'.join(col) for col in df_stock_id.columns]\n    df_stock_id = df_stock_id.add_suffix('_' + 'stock')\n\n    # Group by the stock id\n    df_time_id = df.groupby(['time_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n    \n    # Rename columns joining suffix\n    df_time_id.columns = ['_'.join(col) for col in df_time_id.columns]\n    df_time_id = df_time_id.add_suffix('_' + 'time')\n    \n    # Merge with original dataframe\n    df = df.merge(df_stock_id, how = 'left', left_on = ['stock_id'], right_on = ['stock_id__stock'])\n    df = df.merge(df_time_id, how = 'left', left_on = ['time_id'], right_on = ['time_id__time'])\n    df.drop(['stock_id__stock', 'time_id__time'], axis = 1, inplace = True)\n    \n    return df\n\ndef create_agg_features(train, test):\n\n    # Making agg features\n\n    train_p = pd.read_csv('../input/optiver-realized-volatility-prediction/train.csv')\n    train_p = train_p.pivot(index='time_id', columns='stock_id', values='target')\n    corr = train_p.corr()\n    ids = corr.index\n    kmeans = KMeans(n_clusters=7, random_state=0).fit(corr.values)\n    l = []\n    for n in range(7):\n        l.append ( [ (x-1) for x in ( (ids+1)*(kmeans.labels_ == n)) if x > 0] )\n\n    mat = []\n    matTest = []\n    n = 0\n    for ind in l:\n        newDf = train.loc[train['stock_id'].isin(ind) ]\n        newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n        newDf.loc[:,'stock_id'] = str(n)+'c1'\n        mat.append ( newDf )\n        newDf = test.loc[test['stock_id'].isin(ind) ]    \n        newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n        newDf.loc[:,'stock_id'] = str(n)+'c1'\n        matTest.append ( newDf )\n        n+=1\n\n    mat1 = pd.concat(mat).reset_index()\n    mat1.drop(columns=['target'],inplace=True)\n    mat2 = pd.concat(matTest).reset_index()\n    \n    mat2 = pd.concat([mat2,mat1.loc[mat1.time_id==5]])\n    \n    mat1 = mat1.pivot(index='time_id', columns='stock_id')\n    mat1.columns = [\"_\".join(x) for x in mat1.columns.ravel()]\n    mat1.reset_index(inplace=True)\n    \n    mat2 = mat2.pivot(index='time_id', columns='stock_id')\n    mat2.columns = [\"_\".join(x) for x in mat2.columns.ravel()]\n    mat2.reset_index(inplace=True)\n    \n    prefix = ['log_return1_realized_volatility', 'total_volume_mean', 'trade_size_mean', 'trade_order_count_mean','price_spread_mean','bid_spread_mean','ask_spread_mean',\n              'volume_imbalance_mean', 'bid_ask_spread_mean','size_tau2']\n    selected_cols=mat1.filter(regex='|'.join(f'^{x}.(0|1|3|4|6|2|5)c1' for x in prefix)).columns.tolist()\n    selected_cols.append('time_id')\n    \n    train_m = pd.merge(train,mat1[selected_cols],how='left',on='time_id')\n    test_m = pd.merge(test,mat2[selected_cols],how='left',on='time_id')\n    \n    # filling missing values with train means\n\n    features = [col for col in train_m.columns.tolist() if col not in ['time_id','target','row_id']]\n    train_m[features] = train_m[features].fillna(train_m[features].mean())\n    test_m[features] = test_m[features].fillna(train_m[features].mean())\n\n    return train_m, test_m\n    \n    \n# def preprocessor(list_stock_ids, is_train = True):\n#     # Funtion to make preprocessing function in parallel (for each stock id)\n    \n#     # Parrallel for loop\n#     def for_joblib(stock_id):\n#         # Train\n#         if is_train:\n#             file_path_book = data_dir + \"book_train.parquet/stock_id=\" + str(stock_id)\n#             file_path_trade = data_dir + \"trade_train.parquet/stock_id=\" + str(stock_id)\n#         # Test\n#         else:\n#             file_path_book = data_dir + \"book_test.parquet/stock_id=\" + str(stock_id)\n#             file_path_trade = data_dir + \"trade_test.parquet/stock_id=\" + str(stock_id)\n    \n#         # Preprocess book and trade data and merge them\n#         df_tmp = pd.merge(book_preprocessor(file_path_book), trade_preprocessor(file_path_trade), on = 'row_id', how = 'left')\n        \n#         # Return the merge dataframe\n#         return df_tmp\n    \n#     # Use parallel api to call paralle for loop\n#     df = Parallel(n_jobs = -1, verbose = 1)(delayed(for_joblib)(stock_id) for stock_id in list_stock_ids)\n    \n#     # Concatenate all the dataframes that return from Parallel\n#     df = pd.concat(df, ignore_index = True)\n    \n#     return df","metadata":{"execution":{"iopub.status.busy":"2021-09-26T03:19:29.629448Z","iopub.execute_input":"2021-09-26T03:19:29.629872Z","iopub.status.idle":"2021-09-26T03:19:29.696718Z","shell.execute_reply.started":"2021-09-26T03:19:29.629842Z","shell.execute_reply":"2021-09-26T03:19:29.696003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Funtion to make preprocessing function in parallel (for each stock id)\ndef preprocessor(list_stock_ids, is_train = True):\n    \n    # Parrallel for loop\n    def for_joblib(stock_id):\n        # Train\n        if is_train:\n            file_path_book = data_dir + \"book_train.parquet/stock_id=\" + str(stock_id)\n            file_path_trade = data_dir + \"trade_train.parquet/stock_id=\" + str(stock_id)\n        # Test\n        else:\n            file_path_book = data_dir + \"book_test.parquet/stock_id=\" + str(stock_id)\n            file_path_trade = data_dir + \"trade_test.parquet/stock_id=\" + str(stock_id)\n    \n        # Preprocess book and trade data and merge them\n        df_tmp = pd.merge(book_preprocessor(file_path_book), trade_preprocessor(file_path_trade), on = 'row_id', how = 'left')\n        book_trade_df = book_and_trade_preprocessor(file_path_book, file_path_trade)\n        df_tmp = pd.merge(df_tmp, book_trade_df, on='row_id', how='left')\n        # Return the merge dataframe\n        return df_tmp\n    \n    # Use parallel api to call paralle for loop\n    df = Parallel(n_jobs = -1, verbose = 1)(delayed(for_joblib)(stock_id) for stock_id in list_stock_ids)\n    # Concatenate all the dataframes that return from Parallel\n    df = pd.concat(df, ignore_index = True)\n    return df","metadata":{"execution":{"iopub.status.busy":"2021-09-26T03:19:30.246086Z","iopub.execute_input":"2021-09-26T03:19:30.247266Z","iopub.status.idle":"2021-09-26T03:19:30.255673Z","shell.execute_reply.started":"2021-09-26T03:19:30.247209Z","shell.execute_reply":"2021-09-26T03:19:30.254365Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pickle\nload_tabnet_data=False\nif load_tabnet_data:\n    dire = \"../input/tabnet-dataset/train_tabnet_dataset.pkl\"\n    train = pickle.load(open(dire,\"rb\"))\nelse:\n    train_stock_ids = train['stock_id'].unique()\n    train_ = preprocessor(train_stock_ids, is_train = True)\n    train = train.merge(train_, on = ['row_id'], how = 'left')\n    train = get_time_stock(train)\n    \ntrain_stock_ids = train['stock_id'].unique()\n\n# Preprocess them using Parallel and our single stock id functions\ntest_stock_ids = test['stock_id'].unique()\ntest_ = preprocessor(test_stock_ids, is_train = False)\ntest = test.merge(test_, on = ['row_id'], how = 'left')\ntest = get_time_stock(test)\n\n# # # Fill inf values\n# train.replace([np.inf, -np.inf], np.nan,inplace=True)\n# test.replace([np.inf, -np.inf], np.nan,inplace=True)\n\n# # # Aggregating some features\n# train, test = create_agg_features(train,test)","metadata":{"execution":{"iopub.status.busy":"2021-09-26T03:19:37.784941Z","iopub.execute_input":"2021-09-26T03:19:37.785257Z","iopub.status.idle":"2021-09-26T03:20:39.912405Z","shell.execute_reply.started":"2021-09-26T03:19:37.785217Z","shell.execute_reply":"2021-09-26T03:20:39.911189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # 保存数据\n# dire = \"train_book_tradev2.pkl\"\n# pickle.dump(train, open(dire, \"wb\"))","metadata":{"execution":{"iopub.status.busy":"2021-09-26T03:20:39.915261Z","iopub.execute_input":"2021-09-26T03:20:39.915585Z","iopub.status.idle":"2021-09-26T03:20:42.92732Z","shell.execute_reply.started":"2021-09-26T03:20:39.915552Z","shell.execute_reply":"2021-09-26T03:20:42.926616Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.to_csv(\"train_book_tradev2.csv\", index=False)","metadata":{},"execution_count":null,"outputs":[]}]}