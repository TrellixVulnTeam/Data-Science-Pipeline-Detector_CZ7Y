{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Exploration of the train data and stock / day clustering","metadata":{}},{"cell_type":"markdown","source":"The idea of this notebook was to perform a basic exploration of the train data. This base exploration of the target lead me to consider the squared error with the base prediciton and observe some correlation between stock and day, then to some basic hierarchical clustering. As our main data doesn't contain much information other than prices, learning and using those high level correlations might be at the core of the competition. Along the way I also find hindsight about the data and the target / error (and a weird outlier). If you find the notebook to be usefull / interesting feel free to upvote, this will keep me motivated to share more hindsights trough the competition.\n\n**Version 2 :** I updated the notebook to better reflect my understanding of the competition.\n\n**Version 3 :** I updated the notebook to add a codependance ยง as I feel like this is what this competition is all about.","metadata":{}},{"cell_type":"markdown","source":"- [Objective of the competition](#Objective)\n- [Stock and Time repartition](#Stock_time_repartition)\n- [Target Distribution](#Target_Distribution)\n- [Optiver Base Function](#Optiver_base_functions)\n- [Custom Objective](#RMSPE)\n- [Base Prediction Error](#Base_prediction_error)\n- [Codependance](#Codependance)\n- [Weird Outlier](#Weird_Outlier)\n- [Correlation Matrices - Clustering - Target](#Correlation_Matrices_target)\n- [Correlation Matrices - Clustering - Error](#Correlation_squared_error)","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport plotly.express as px\n\nimport matplotlib.pyplot as plt\n\ntrain = pd.read_csv('../input/optiver-realized-volatility-prediction/train.csv', dtype = {'stock_id': np.int32, 'time_id': np.int32, 'target': np.float64})\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-29T15:15:45.433209Z","iopub.execute_input":"2021-07-29T15:15:45.433695Z","iopub.status.idle":"2021-07-29T15:15:47.376528Z","shell.execute_reply.started":"2021-07-29T15:15:45.433587Z","shell.execute_reply":"2021-07-29T15:15:47.375465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"book_example = pd.read_parquet('../input/optiver-realized-volatility-prediction/book_train.parquet/stock_id=0')\ntrade_example =  pd.read_parquet('../input/optiver-realized-volatility-prediction/trade_train.parquet/stock_id=0')\nstock_id = '0'\nbook_example = book_example[book_example['time_id']==5]\nbook_example.loc[:,'stock_id'] = stock_id\ntrade_example = trade_example[trade_example['time_id']==5]\ntrade_example.loc[:,'stock_id'] = stock_id\n\n\nbook_example['wap'] = (book_example['bid_price1'] * book_example['ask_size1'] +\n                                book_example['ask_price1'] * book_example['bid_size1']) / (\n                                       book_example['bid_size1']+ book_example['ask_size1'])\n\n\ndef log_return(list_stock_prices):\n    return np.log(list_stock_prices).diff()\n\ndef realized_volatility(series_log_return):\n    return np.sqrt(np.sum(series_log_return**2))\n\nbook_example.loc[:,'log_return'] = log_return(book_example['wap'])\nbook_example = book_example[~book_example['log_return'].isnull()]","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-07-29T15:15:47.378121Z","iopub.execute_input":"2021-07-29T15:15:47.378446Z","iopub.status.idle":"2021-07-29T15:15:47.943278Z","shell.execute_reply.started":"2021-07-29T15:15:47.378414Z","shell.execute_reply":"2021-07-29T15:15:47.942552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='Objective'></a>\n# Objective of the competition :\n\nWe have a bunch of time series of 10 minutes length of differents stocks over different time periods. The goal of this competition is to predict the volatility of the stock over the next ten minutes, not the price. Let's have a look at one of the price time serie.","metadata":{}},{"cell_type":"code","source":"plt.plot(book_example['seconds_in_bucket'],book_example['wap'])","metadata":{"execution":{"iopub.status.busy":"2021-07-29T15:15:47.946892Z","iopub.execute_input":"2021-07-29T15:15:47.947296Z","iopub.status.idle":"2021-07-29T15:15:48.151674Z","shell.execute_reply.started":"2021-07-29T15:15:47.947262Z","shell.execute_reply":"2021-07-29T15:15:48.150338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next the volatility :","metadata":{}},{"cell_type":"code","source":"plt.plot(book_example['seconds_in_bucket'],book_example['log_return']**2)","metadata":{"execution":{"iopub.status.busy":"2021-07-29T15:15:48.153702Z","iopub.execute_input":"2021-07-29T15:15:48.154006Z","iopub.status.idle":"2021-07-29T15:15:48.30585Z","shell.execute_reply.started":"2021-07-29T15:15:48.153976Z","shell.execute_reply":"2021-07-29T15:15:48.304595Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The question is what happen next ?","metadata":{}},{"cell_type":"code","source":"x = np.append(book_example['seconds_in_bucket'].values,1200)\ny = np.append((book_example['log_return']**2).values,0)\nz = np.mean(book_example['log_return']**2)\n\nplt.hlines(z,xmin=0,xmax=600,color='red')\nplt.plot(x,y)\nplt.text(900, 0.6*1e-6,'???', ha='center', va='center', size = 40)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-07-29T15:15:48.307282Z","iopub.execute_input":"2021-07-29T15:15:48.307583Z","iopub.status.idle":"2021-07-29T15:15:48.615433Z","shell.execute_reply.started":"2021-07-29T15:15:48.307555Z","shell.execute_reply":"2021-07-29T15:15:48.613932Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='Stock_time_repartition'></a>\n# Repartition of Stock x Time","metadata":{}},{"cell_type":"markdown","source":"We start with a simple exploration of stock x time repartition to look for any exploitable pattern. ","metadata":{}},{"cell_type":"code","source":"print(f'Number of unique stocks is {train.stock_id.nunique()}')\nprint(f'Number of unique time_id is {train.time_id.nunique()}')","metadata":{"execution":{"iopub.status.busy":"2021-07-29T15:15:48.616896Z","iopub.execute_input":"2021-07-29T15:15:48.617215Z","iopub.status.idle":"2021-07-29T15:15:48.657405Z","shell.execute_reply.started":"2021-07-29T15:15:48.617183Z","shell.execute_reply":"2021-07-29T15:15:48.656033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\n\nn_stock = np.max(train.stock_id)\nn_time = np.max(train.time_id)\n\nmat_plot = np.zeros((n_stock, n_time))\n\ns_id = train.stock_id.values\nt_id = train.time_id.values\n\nfor k in range(train.shape[0]):\n    i = s_id[k]-1\n    j = t_id[k]-1\n    mat_plot[i,j] = 1\n  \nplt.figure(figsize=(12,8))\nplt.matshow(mat_plot, fignum=1, aspect='auto')","metadata":{"execution":{"iopub.status.busy":"2021-07-29T15:15:48.658868Z","iopub.execute_input":"2021-07-29T15:15:48.659187Z","iopub.status.idle":"2021-07-29T15:15:53.25009Z","shell.execute_reply.started":"2021-07-29T15:15:48.659156Z","shell.execute_reply":"2021-07-29T15:15:53.248787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There doesn't seems to be anything exploitable here. I even think times are shuffled so that it would be difficult to use anything based on those.","metadata":{}},{"cell_type":"markdown","source":"<a id='Target_Distribution'></a>\n# Target distribution","metadata":{}},{"cell_type":"markdown","source":"We observe a somewhat skewed distribution of the target. Given the square in the objective function dealing with those outliers mihgt be one of our main goals.","metadata":{}},{"cell_type":"code","source":"plt.hist(train.target, bins = 1000)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-29T15:15:53.253024Z","iopub.execute_input":"2021-07-29T15:15:53.253355Z","iopub.status.idle":"2021-07-29T15:15:55.259291Z","shell.execute_reply.started":"2021-07-29T15:15:53.253322Z","shell.execute_reply":"2021-07-29T15:15:55.257876Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hist = np.histogram(train.target, bins=1000)\nidx = np.argmax(hist[0])\n\nprint(f'Target mean is {np.mean(train.target)}')\nprint(f'Target median is {np.median(train.target)}')\nprint(f'Target mode is {hist[1][idx]}')","metadata":{"execution":{"iopub.status.busy":"2021-07-29T15:15:55.26191Z","iopub.execute_input":"2021-07-29T15:15:55.262268Z","iopub.status.idle":"2021-07-29T15:15:55.293061Z","shell.execute_reply.started":"2021-07-29T15:15:55.262234Z","shell.execute_reply":"2021-07-29T15:15:55.291794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Past seems is used as a benchmark. We might want to try other benchmarks suchs as the median or the mode, the more populous values, not the mean that can be influenced by outliers. (Or may be it might be better to get more influence from outliers, I don't know yet).","metadata":{}},{"cell_type":"code","source":"n_stock = np.max(train.stock_id)\nn_time = np.max(train.time_id)\n\nmat_plot = np.zeros((n_stock, n_time))\n\ns_id = train.stock_id.values\nt_id = train.time_id.values\nt = train.target.values\n\nfor k in range(train.shape[0]):\n    i = s_id[k]-1\n    j = t_id[k]-1\n    mat_plot[i,j] = t[k]\n  \nplt.figure(figsize=(12,8))\nplt.matshow(mat_plot, fignum=1, aspect='auto', vmax=0.0075)","metadata":{"execution":{"iopub.status.busy":"2021-07-29T15:15:55.294679Z","iopub.execute_input":"2021-07-29T15:15:55.294993Z","iopub.status.idle":"2021-07-29T15:16:00.105141Z","shell.execute_reply.started":"2021-07-29T15:15:55.294963Z","shell.execute_reply":"2021-07-29T15:16:00.104043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It's hard not to see time pattern in targets ... are we sure the time are shuffled ?","metadata":{}},{"cell_type":"markdown","source":"<a id='Optiver_base_functions'></a>\n# Optiver exemple - Base functions","metadata":{}},{"cell_type":"code","source":"book_example = pd.read_parquet('../input/optiver-realized-volatility-prediction/book_train.parquet/stock_id=0')\ntrade_example =  pd.read_parquet('../input/optiver-realized-volatility-prediction/trade_train.parquet/stock_id=0')\nstock_id = '0'\nbook_example = book_example[book_example['time_id']==5]\nbook_example.loc[:,'stock_id'] = stock_id\ntrade_example = trade_example[trade_example['time_id']==5]\ntrade_example.loc[:,'stock_id'] = stock_id\n\n\nbook_example['wap'] = (book_example['bid_price1'] * book_example['ask_size1'] +\n                                book_example['ask_price1'] * book_example['bid_size1']) / (\n                                       book_example['bid_size1']+ book_example['ask_size1'])\n\n\ndef log_return(list_stock_prices):\n    return np.log(list_stock_prices).diff()\n\nbook_example.loc[:,'log_return'] = log_return(book_example['wap'])\nbook_example = book_example[~book_example['log_return'].isnull()]\n\n\ndef realized_volatility(series_log_return):\n    return np.sqrt(np.sum(series_log_return**2))\nrealized_vol = realized_volatility(book_example['log_return'])\nprint(f'Realized volatility for stock_id 0 on time_id 5 is {realized_vol}')\n\nimport os\nfrom sklearn.metrics import r2_score\nimport glob\nlist_order_book_file_train = glob.glob('/kaggle/input/optiver-realized-volatility-prediction/book_train.parquet/*')\n\ndef realized_volatility_per_time_id(file_path, prediction_column_name):\n    df_book_data = pd.read_parquet(file_path)\n    df_book_data['wap'] =(df_book_data['bid_price1'] * df_book_data['ask_size1']+df_book_data['ask_price1'] * df_book_data['bid_size1'])  / (\n                                      df_book_data['bid_size1']+ df_book_data[\n                                  'ask_size1'])\n    df_book_data['log_return'] = df_book_data.groupby(['time_id'])['wap'].apply(log_return)\n    df_book_data = df_book_data[~df_book_data['log_return'].isnull()]\n    df_realized_vol_per_stock =  pd.DataFrame(df_book_data.groupby(['time_id'])['log_return'].agg(realized_volatility)).reset_index()\n    df_realized_vol_per_stock = df_realized_vol_per_stock.rename(columns = {'log_return':prediction_column_name})\n    stock_id = file_path.split('=')[1]\n    df_realized_vol_per_stock['row_id'] = df_realized_vol_per_stock['time_id'].apply(lambda x:f'{stock_id}-{x}')\n    return df_realized_vol_per_stock[['row_id',prediction_column_name]]\n\ndef past_realized_volatility_per_stock(list_file,prediction_column_name):\n    df_past_realized = pd.DataFrame()\n    for file in list_file:\n        df_past_realized = pd.concat([df_past_realized,\n                                     realized_volatility_per_time_id(file,prediction_column_name)])\n    return df_past_realized\ndf_past_realized_train = past_realized_volatility_per_stock(list_file=list_order_book_file_train,\n                                                           prediction_column_name='pred')\n\ntrain['row_id'] = train['stock_id'].astype(str) + '-' + train['time_id'].astype(str)\ntrain = train[['row_id','target']]\ndf_joined = train.merge(df_past_realized_train[['row_id','pred']], on = ['row_id'], how = 'left')\n\nfrom sklearn.metrics import r2_score\ndef rmspe(y_true, y_pred):\n    return  (np.sqrt(np.mean(np.square((y_true - y_pred) / y_true))))\n\nR2 = round(r2_score(y_true = df_joined['target'], y_pred = df_joined['pred']),3)\nRMSPE = round(rmspe(y_true = df_joined['target'], y_pred = df_joined['pred']),3)\nprint(f'Performance of the naive prediction: R2 score: {R2}, RMSPE: {RMSPE}')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-07-29T15:16:00.106642Z","iopub.execute_input":"2021-07-29T15:16:00.106968Z","iopub.status.idle":"2021-07-29T15:22:57.579848Z","shell.execute_reply.started":"2021-07-29T15:16:00.106936Z","shell.execute_reply":"2021-07-29T15:22:57.577977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I don't know why id are merged. More practical to demerge them :","metadata":{}},{"cell_type":"code","source":"df_joined['se'] = np.square((df_joined['target'] - df_joined['pred']) / df_joined['target'])\ndf_joined['stock_id'] = df_joined['row_id'].str.partition('-')[0].astype('int')\ndf_joined['time_id'] = df_joined['row_id'].str.partition('-')[2].astype('int')","metadata":{"execution":{"iopub.status.busy":"2021-07-29T15:22:57.581587Z","iopub.execute_input":"2021-07-29T15:22:57.581912Z","iopub.status.idle":"2021-07-29T15:22:59.494953Z","shell.execute_reply.started":"2021-07-29T15:22:57.581881Z","shell.execute_reply":"2021-07-29T15:22:59.49391Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='RMSPE'></a>\n# Custom objective - RMPSE\n\nIt's important to not we are not optimising the an usual regression loss. We need to optimise the RMPSE. That is the usual RMSE corrceted by the target to make it a relative error:\n\n$$ \\sqrt{\\frac{1}{n}\\sum^{n}_{i=1}((y_i-\\hat{y_i})/y_i)^2}$$\n\nAs it happen, for simple models this is equivalent to weighting the RMSE error by : \n\n$$ \\frac{1}{{y^2}_i} $$\n\nFor a complete discussion see here : \nhttps://www.kaggle.com/c/optiver-realized-volatility-prediction/discussion/250324\n\nFor a simple exemple (a linear regression) getting a good score using this basic idea see here :\nhttps://www.kaggle.com/lucasmorin/realised-vol-weighted-regression-baseline","metadata":{}},{"cell_type":"markdown","source":"<a id='Base_prediction_error'></a>\n# Base prediction - error\n\nHere we study the base prediction error to get some hindsigt on the data.","metadata":{}},{"cell_type":"code","source":"test_df = df_joined[df_joined.stock_id == 0]\n\nplt.plot(test_df.target,test_df.pred, 'o', label='realisations')\n\nm, b = np.polyfit(test_df.target,test_df.pred, 1)\n\nplt.plot(test_df.target, test_df.target, label='first secant')\nplt.plot(test_df.target, m*test_df.target+b, label='linear regression')\nplt.legend()","metadata":{"execution":{"iopub.status.busy":"2021-07-29T15:22:59.496346Z","iopub.execute_input":"2021-07-29T15:22:59.496688Z","iopub.status.idle":"2021-07-29T15:22:59.734138Z","shell.execute_reply.started":"2021-07-29T15:22:59.496654Z","shell.execute_reply":"2021-07-29T15:22:59.732691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A simple linear regression by stock might be a good first step into modelling. ","metadata":{}},{"cell_type":"markdown","source":"We then look at the repartition of errors :","metadata":{}},{"cell_type":"code","source":"plt.hist(df_joined['se'], bins = 1000)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-29T15:22:59.736215Z","iopub.execute_input":"2021-07-29T15:22:59.73676Z","iopub.status.idle":"2021-07-29T15:23:01.930704Z","shell.execute_reply.started":"2021-07-29T15:22:59.73671Z","shell.execute_reply":"2021-07-29T15:23:01.929457Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"My favorite graph so far. This is not a bug. You can see the data around 0. This mean we have high contributor to the error. Let's try with a log.","metadata":{}},{"cell_type":"code","source":"plt.hist(np.log(df_joined['se']), bins = 1000)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-29T15:23:01.932643Z","iopub.execute_input":"2021-07-29T15:23:01.933091Z","iopub.status.idle":"2021-07-29T15:23:04.238795Z","shell.execute_reply.started":"2021-07-29T15:23:01.933046Z","shell.execute_reply":"2021-07-29T15:23:04.237424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Seems better, right ?","metadata":{}},{"cell_type":"code","source":"##### n_stock = np.max(df_joined.stock_id)\nn_time = np.max(df_joined.time_id)\n\nmat_plot = np.zeros((n_stock, n_time))\n\ns_id = df_joined.stock_id.values\nt_id = df_joined.time_id.values\nt = df_joined['se'].values\n\nfor k in range(train.shape[0]):\n    i = s_id[k]-1\n    j = t_id[k]-1\n    mat_plot[i,j] = t[k]\n  \nplt.figure(figsize=(12,8))\nplt.matshow(mat_plot, fignum=1, aspect='auto',vmax=10*np.median(df_joined['se']))","metadata":{"execution":{"iopub.status.busy":"2021-07-29T15:23:04.240093Z","iopub.execute_input":"2021-07-29T15:23:04.240459Z","iopub.status.idle":"2021-07-29T15:23:05.835526Z","shell.execute_reply.started":"2021-07-29T15:23:04.240425Z","shell.execute_reply":"2021-07-29T15:23:05.834377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Errors seems correlated trough stocks. We might have periods with high variations of volatility trough the stock market. Again we can't say much about correlation trough time.","metadata":{}},{"cell_type":"markdown","source":"<a id='Codependance'></a>\n# Codependance","metadata":{}},{"cell_type":"markdown","source":"There is a lot of dependance going on. This might be what this competition is all about. Dependence between two stock targets: ","metadata":{}},{"cell_type":"code","source":"stock_0 = df_joined[df_joined.stock_id == 0]\nstock_1 = df_joined[df_joined.stock_id == 1]\n\nplt.scatter(stock_0.target,stock_1.target)","metadata":{"execution":{"iopub.status.busy":"2021-07-29T15:38:20.203705Z","iopub.execute_input":"2021-07-29T15:38:20.204157Z","iopub.status.idle":"2021-07-29T15:38:20.400761Z","shell.execute_reply.started":"2021-07-29T15:38:20.204119Z","shell.execute_reply":"2021-07-29T15:38:20.399473Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In a quantile-quantile approach:","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import quantile_transform\nqt_target_0 = quantile_transform(stock_0.target.values.reshape(-1, 1), n_quantiles=100)\nqt_target_1 = quantile_transform(stock_1.target.values.reshape(-1, 1), n_quantiles=100)\n\nplt.scatter(qt_target_0,qt_target_1)","metadata":{"execution":{"iopub.status.busy":"2021-07-29T15:42:26.920306Z","iopub.execute_input":"2021-07-29T15:42:26.920795Z","iopub.status.idle":"2021-07-29T15:42:27.101208Z","shell.execute_reply.started":"2021-07-29T15:42:26.920747Z","shell.execute_reply":"2021-07-29T15:42:27.099865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Quantile dependance between realized volatility and prediction :","metadata":{}},{"cell_type":"code","source":"stock_0 = df_joined[df_joined.stock_id == 0]\n\nfrom sklearn.preprocessing import quantile_transform\nqt_target_0 = quantile_transform(stock_0.target.values.reshape(-1, 1), n_quantiles=100)\nqt_pred_0 = quantile_transform(stock_0.pred.values.reshape(-1, 1), n_quantiles=100)\n\nplt.scatter(qt_target_0,qt_pred_0)","metadata":{"execution":{"iopub.status.busy":"2021-07-29T15:43:17.561842Z","iopub.execute_input":"2021-07-29T15:43:17.56223Z","iopub.status.idle":"2021-07-29T15:43:17.737283Z","shell.execute_reply.started":"2021-07-29T15:43:17.562198Z","shell.execute_reply":"2021-07-29T15:43:17.735853Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Copulas anyone ?","metadata":{}},{"cell_type":"markdown","source":"<a id='Weird_Outlier'></a>\n# Weird Outlier - Stock 31","metadata":{}},{"cell_type":"markdown","source":"Looking further into error we notice that a big contributor is stock 31. We might investigate outliers with higher contribution to the error.","metadata":{}},{"cell_type":"code","source":"df_joined['se'].nlargest(n=10)","metadata":{"execution":{"iopub.status.busy":"2021-07-29T15:23:05.836981Z","iopub.execute_input":"2021-07-29T15:23:05.83752Z","iopub.status.idle":"2021-07-29T15:23:05.858426Z","shell.execute_reply.started":"2021-07-29T15:23:05.83735Z","shell.execute_reply":"2021-07-29T15:23:05.857241Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_joined.iloc[df_joined['se'].nlargest(n=10).index,]","metadata":{"execution":{"iopub.status.busy":"2021-07-29T15:23:05.859764Z","iopub.execute_input":"2021-07-29T15:23:05.860123Z","iopub.status.idle":"2021-07-29T15:23:05.888296Z","shell.execute_reply.started":"2021-07-29T15:23:05.860093Z","shell.execute_reply":"2021-07-29T15:23:05.887279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df = df_joined[df_joined.stock_id == 31]\n\nplt.plot(test_df.target,test_df.pred, 'o', label='realisations')\n\nm, b = np.polyfit(test_df.target,test_df.pred, 1)\n\nplt.plot(test_df.target, test_df.target, label='first secant')\nplt.plot(test_df.target, m*test_df.target+b, label='linear regression')\nplt.legend()","metadata":{"execution":{"iopub.status.busy":"2021-07-29T15:23:05.889721Z","iopub.execute_input":"2021-07-29T15:23:05.89004Z","iopub.status.idle":"2021-07-29T15:23:06.099783Z","shell.execute_reply.started":"2021-07-29T15:23:05.890007Z","shell.execute_reply":"2021-07-29T15:23:06.09845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It's not evident what is the problem here as the linear correlation seems to hold. However this might be due to the definition of  the objective. The error being relative means that biggest contributions my lie near the origin.","metadata":{}},{"cell_type":"code","source":"test_df = df_joined[df_joined.stock_id == 31]\n\ntest_df2 = test_df.loc[test_df['se'].nlargest(n=50).index,]\n\nplt.plot(test_df2.target,test_df2.pred, 'o', label='realisations')\n\nm, b = np.polyfit(test_df.target,test_df.pred, 1)\n\nplt.plot(test_df.target, test_df.target, label='first secant')\nplt.plot(test_df.target, m*test_df.target+b, label='linear regression')\nplt.legend()","metadata":{"execution":{"iopub.status.busy":"2021-07-29T15:23:06.101247Z","iopub.execute_input":"2021-07-29T15:23:06.10158Z","iopub.status.idle":"2021-07-29T15:23:06.291263Z","shell.execute_reply.started":"2021-07-29T15:23:06.101524Z","shell.execute_reply":"2021-07-29T15:23:06.290511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='Correlation_Matrices_target'></a>\n# Correlation Matrices - Clustering - target\n\nIt's still unclear to me if stocks can be considered independtly or not. So I tried to look a bit into it, first with correlation, then with hierarchical clustering.","metadata":{}},{"cell_type":"markdown","source":"<a id='Stock_correlation_target'></a>\n## Stock correlation - Clustering","metadata":{}},{"cell_type":"code","source":"dfp = df_joined.pivot('time_id','stock_id')\nmat_corr = dfp.target.corr()\n\nplt.figure(figsize=(8,8))\nplt.matshow(mat_corr, fignum=1, aspect='auto')","metadata":{"execution":{"iopub.status.busy":"2021-07-29T15:23:06.292319Z","iopub.execute_input":"2021-07-29T15:23:06.292635Z","iopub.status.idle":"2021-07-29T15:23:06.946601Z","shell.execute_reply.started":"2021-07-29T15:23:06.292605Z","shell.execute_reply":"2021-07-29T15:23:06.945498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import scipy\nimport scipy.cluster.hierarchy as sch\n\nX = mat_corr\nd = sch.distance.pdist(X)   # vector of ('55' choose 2) pairwise distances\nL = sch.linkage(d, method='complete')\nind = sch.fcluster(L, 0.5*d.max(), 'distance')\ncolumns = [dfp.target.columns.tolist()[i] for i in list((np.argsort(ind)))]\n\nplt.figure(figsize=(8,8))\nplt.matshow(dfp.target[columns].corr(), fignum=1, aspect='auto')","metadata":{"execution":{"iopub.status.busy":"2021-07-29T15:23:06.948459Z","iopub.execute_input":"2021-07-29T15:23:06.949226Z","iopub.status.idle":"2021-07-29T15:23:07.664586Z","shell.execute_reply.started":"2021-07-29T15:23:06.949176Z","shell.execute_reply":"2021-07-29T15:23:07.662765Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from scipy.cluster.hierarchy import linkage, dendrogram, fcluster\nfrom scipy.spatial.distance import squareform\n\nplt.figure(figsize=(12,6))\ndissimilarity = 1 - abs(mat_corr)\nZ = linkage(squareform(dissimilarity), 'complete')\n\ndendrogram(Z, labels=dfp.target.columns, orientation='top', \n           leaf_rotation=90);","metadata":{"execution":{"iopub.status.busy":"2021-07-29T15:23:07.670381Z","iopub.execute_input":"2021-07-29T15:23:07.670966Z","iopub.status.idle":"2021-07-29T15:23:10.251392Z","shell.execute_reply.started":"2021-07-29T15:23:07.670916Z","shell.execute_reply":"2021-07-29T15:23:10.250373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There appears to be 2 outliers, other than that the conclusion seems to be that there is enough correlations so that it should be taken into account. ","metadata":{}},{"cell_type":"markdown","source":"<a id='Time_correlation_target'></a>\n## Time correlation - Clustering","metadata":{}},{"cell_type":"markdown","source":"Another approach of clustering might be trough time clustering. That is burst, of correlation might appears on a given date, due to a significant external event. ","metadata":{}},{"cell_type":"code","source":"dfp = df_joined.pivot('stock_id','time_id')\nmat_corr = dfp.target.corr()\n\nplt.figure(figsize=(8,8))\nplt.matshow(mat_corr, fignum=1, aspect='auto')","metadata":{"execution":{"iopub.status.busy":"2021-07-29T15:23:10.252935Z","iopub.execute_input":"2021-07-29T15:23:10.253236Z","iopub.status.idle":"2021-07-29T15:23:15.861304Z","shell.execute_reply.started":"2021-07-29T15:23:10.253206Z","shell.execute_reply":"2021-07-29T15:23:15.860389Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = mat_corr\nd = sch.distance.pdist(X)\nL = sch.linkage(d, method='complete')\nind = sch.fcluster(L, 0.5*d.max(), 'distance')\ncolumns = [dfp.se.columns.tolist()[i] for i in list((np.argsort(ind)))]\n\nplt.figure(figsize=(8,8))\nplt.matshow(dfp.se[columns].corr(), fignum=1, aspect='auto')","metadata":{"execution":{"iopub.status.busy":"2021-07-29T15:23:15.86258Z","iopub.execute_input":"2021-07-29T15:23:15.863025Z","iopub.status.idle":"2021-07-29T15:24:49.50621Z","shell.execute_reply.started":"2021-07-29T15:23:15.862984Z","shell.execute_reply":"2021-07-29T15:24:49.504795Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12,5))\ndissimilarity = 1 - abs(mat_corr)\nZ = linkage(squareform(dissimilarity), 'complete')\n\ndendrogram(Z, labels=dfp.target.columns, orientation='top', \n           leaf_rotation=90);","metadata":{"execution":{"iopub.status.busy":"2021-07-29T15:24:49.50784Z","iopub.execute_input":"2021-07-29T15:24:49.508399Z","iopub.status.idle":"2021-07-29T15:26:26.314885Z","shell.execute_reply.started":"2021-07-29T15:24:49.508355Z","shell.execute_reply":"2021-07-29T15:26:26.313777Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Correlation is less clear. Still, some lowly correlated clusters seems to appear.","metadata":{}},{"cell_type":"markdown","source":"<a id='Correlation_squared_error'></a>\n# Correlation - Clustering - squared error","metadata":{}},{"cell_type":"markdown","source":"The same can be done with base error prediction. This might help understand where the difficulty of the competition lie.","metadata":{}},{"cell_type":"markdown","source":"<a id='Stock_correlation_se'></a>\n## Stock correlation - Clustering","metadata":{}},{"cell_type":"code","source":"dfp = df_joined.pivot('time_id','stock_id')\nmat_corr = dfp.se.corr()\n\nplt.figure(figsize=(8,8))\nplt.matshow(mat_corr, fignum=1, aspect='auto')","metadata":{"execution":{"iopub.status.busy":"2021-07-29T15:26:26.316765Z","iopub.execute_input":"2021-07-29T15:26:26.317232Z","iopub.status.idle":"2021-07-29T15:26:27.097376Z","shell.execute_reply.started":"2021-07-29T15:26:26.317189Z","shell.execute_reply":"2021-07-29T15:26:27.095829Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = mat_corr\nd = sch.distance.pdist(X)   # vector of ('55' choose 2) pairwise distances\nL = sch.linkage(d, method='complete')\nind = sch.fcluster(L, 0.5*d.max(), 'distance')\ncolumns = [dfp.se.columns.tolist()[i] for i in list((np.argsort(ind)))]\n\nplt.figure(figsize=(8,8))\nplt.matshow(dfp.se[columns].corr(), fignum=1, aspect='auto')","metadata":{"execution":{"iopub.status.busy":"2021-07-29T15:26:27.099607Z","iopub.execute_input":"2021-07-29T15:26:27.100427Z","iopub.status.idle":"2021-07-29T15:26:27.877936Z","shell.execute_reply.started":"2021-07-29T15:26:27.10037Z","shell.execute_reply":"2021-07-29T15:26:27.876675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12,5))\ndissimilarity = 1 - abs(mat_corr)\nZ = linkage(squareform(dissimilarity), 'complete')\n\ndendrogram(Z, labels=dfp.target.columns, orientation='top', \n           leaf_rotation=90);","metadata":{"execution":{"iopub.status.busy":"2021-07-29T15:26:27.879836Z","iopub.execute_input":"2021-07-29T15:26:27.880508Z","iopub.status.idle":"2021-07-29T15:26:30.400311Z","shell.execute_reply.started":"2021-07-29T15:26:27.880458Z","shell.execute_reply":"2021-07-29T15:26:30.399557Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the correlation matrix there seems to be 2/3 clusters that appear on the dendogram. It is ot clear how to exploit that.","metadata":{}},{"cell_type":"markdown","source":"<a id='Time_correlation_se'></a>\n## Time correlation - Clustering","metadata":{}},{"cell_type":"code","source":"dfp = df_joined.pivot('stock_id','time_id')\nmat_corr = dfp.se.corr()\n\nplt.figure(figsize=(8,8))\nplt.matshow(mat_corr, fignum=1, aspect='auto')","metadata":{"execution":{"iopub.status.busy":"2021-07-29T15:26:30.401437Z","iopub.execute_input":"2021-07-29T15:26:30.401852Z","iopub.status.idle":"2021-07-29T15:26:35.965988Z","shell.execute_reply.started":"2021-07-29T15:26:30.401821Z","shell.execute_reply":"2021-07-29T15:26:35.965059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = mat_corr\nd = sch.distance.pdist(X)\nL = sch.linkage(d, method='complete')\nind = sch.fcluster(L, 0.5*d.max(), 'distance')\ncolumns = [dfp.se.columns.tolist()[i] for i in list((np.argsort(ind)))]\n\nplt.figure(figsize=(8,8))\nplt.matshow(dfp.se[columns].corr(), fignum=1, aspect='auto')","metadata":{"execution":{"iopub.status.busy":"2021-07-29T15:26:35.967218Z","iopub.execute_input":"2021-07-29T15:26:35.967675Z","iopub.status.idle":"2021-07-29T15:28:05.125628Z","shell.execute_reply.started":"2021-07-29T15:26:35.967642Z","shell.execute_reply":"2021-07-29T15:28:05.124444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12,5))\ndissimilarity = 1 - abs(mat_corr)\nZ = linkage(squareform(dissimilarity), 'complete')\n\ndendrogram(Z, labels=dfp.target.columns, orientation='top', \n           leaf_rotation=90);","metadata":{"execution":{"iopub.status.busy":"2021-07-29T15:28:05.127263Z","iopub.execute_input":"2021-07-29T15:28:05.127593Z","iopub.status.idle":"2021-07-29T15:29:41.916938Z","shell.execute_reply.started":"2021-07-29T15:28:05.127562Z","shell.execute_reply":"2021-07-29T15:29:41.915572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The time clustering appears more interesting here than the previous one as some clusters seems to appears more distinctively in the correlation matrix. However this might be due to sample that were chosen closely, not necessarily to something that can be exploited in the future.","metadata":{}}]}