{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Update \n* Rebase seconds_in_bucket because of the filler data \n* Forward fill of the `book_data` for train and test data","metadata":{}},{"cell_type":"markdown","source":"# Acknowledgment\n* [we-need-to-go-deeper-and-validate](https://www.kaggle.com/konradb/we-need-to-go-deeper-and-validate)\n* [accelerating-trading-on-gpu-via-rapids](https://www.kaggle.com/aerdem4/accelerating-trading-on-gpu-via-rapids)\n* [Forward filling book data](https://www.kaggle.com/c/optiver-realized-volatility-prediction/discussion/251277)\n* [Deep Learning approach with a CNN- inference](https://www.kaggle.com/slawekbiel/deep-learning-approach-with-a-cnn-inference)","metadata":{}},{"cell_type":"markdown","source":"# Library","metadata":{}},{"cell_type":"code","source":"import sys\nsys.path.append('../input/rapids-kaggle-utils')\n\nimport cupy as cp\nimport cudf\nimport cuml\nimport glob\nfrom tqdm import tqdm\nimport cu_utils.transform as cutran\n\nimport gc\nfrom joblib import Parallel, delayed","metadata":{"execution":{"iopub.status.busy":"2021-07-17T18:03:31.164592Z","iopub.execute_input":"2021-07-17T18:03:31.165365Z","iopub.status.idle":"2021-07-17T18:03:35.477493Z","shell.execute_reply.started":"2021-07-17T18:03:31.165257Z","shell.execute_reply":"2021-07-17T18:03:35.476598Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"PATH = \"../input/optiver-realized-volatility-prediction\"\norder_book_training = glob.glob(f'{PATH}/book_train.parquet/*/*')\nlen(order_book_training)","metadata":{"execution":{"iopub.status.busy":"2021-07-17T18:03:35.48253Z","iopub.execute_input":"2021-07-17T18:03:35.482829Z","iopub.status.idle":"2021-07-17T18:03:35.672845Z","shell.execute_reply.started":"2021-07-17T18:03:35.482795Z","shell.execute_reply":"2021-07-17T18:03:35.671886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Utils","metadata":{}},{"cell_type":"code","source":"def fix_offsets(data_df):\n    \n    offsets = data_df.groupby(['time_id']).agg({'seconds_in_bucket':'min'})\n    offsets.columns = ['offset']\n    data_df = data_df.join(offsets, on='time_id')\n    data_df.seconds_in_bucket = data_df.seconds_in_bucket - data_df.offset\n    \n    return data_df\n\ndef ffill(data_df):\n    # MultiIndex.from_product uses pandas in the background\n    # That's why we need to transform the data into pd dataframe\n    # Then the MultiIndex.from_product will return cudf dataframe \n    data_df=data_df.set_index(['time_id', 'seconds_in_bucket']).to_pandas()\n    data_df = data_df.reindex(pd.MultiIndex.from_product([data_df.index.levels[0], np.arange(0,600)], names = ['time_id', 'seconds_in_bucket']), method='ffill')\n    return data_df.reset_index()\n\n\ndef rel_vol_fe(df, null_val=-9999):\n    \n    # compute wap\n    for n in range(1, 3):\n        p1 = df[f\"bid_price{n}\"]\n        p2 = df[f\"ask_price{n}\"]\n        s1 = df[f\"bid_size{n}\"]\n        s2 = df[f\"ask_size{n}\"]\n        df[\"WAP\"] = (p1*s2 + p2*s1) / (s1 + s2)\n\n\n        df[\"log_wap\"] = df[\"WAP\"].log()\n        df[\"log_wap_shifted\"] = (df[[\"time_id\", \"log_wap\"]].groupby(\"time_id\", method='cudf')\n                                 .apply_grouped(cutran.get_cu_shift_transform(shift_by=1, null_val=null_val),\n                                                incols={\"log_wap\": 'x'},\n                                                outcols=dict(y_out=cp.float32),\n                                                tpb=32)[\"y_out\"])\n        df = df[df[\"log_wap_shifted\"] != null_val]\n\n        df[\"diff_log_wap\"] = df[\"log_wap\"] - df[\"log_wap_shifted\"]\n        df[f\"diff_log_wap{n}\"] = df[\"diff_log_wap\"]**2\n    \n\n    \n    # Summary statistics for different 'diff_log_wap'\n    sum_df = df.groupby(\"time_id\").agg({\"diff_log_wap1\": {\"sum\", \"mean\", \"std\", \"median\", \"max\", \"min\"}, \n                                        \"diff_log_wap2\": {\"sum\", \"mean\", \"std\", \"median\", \"max\", \"min\"}}\n                                      ).reset_index()\n    \n    # Create wanted features for training\n    def f(x):\n        if x[1] == \"\":\n            return x[0]\n        return x[0] + \"_\" + x[1]\n    \n    sum_df.columns = [f(x) for x in sum_df.columns]\n    sum_df[\"volatility1\"] = (sum_df[\"diff_log_wap1_sum\"])**0.5\n    sum_df[\"volatility2\"] = (sum_df[\"diff_log_wap2_sum\"])**0.5\n    sum_df[\"vol1_mean\"] = sum_df[\"diff_log_wap1_mean\"].fillna(0).values\n    sum_df[\"vol2_mean\"] = sum_df[\"diff_log_wap2_mean\"].fillna(0).values\n    sum_df[\"vol1_std\"] = sum_df[\"diff_log_wap1_std\"].fillna(0).values\n    sum_df[\"vol2_std\"] = sum_df[\"diff_log_wap2_std\"].fillna(0).values\n    sum_df[\"vol1_median\"] = sum_df[\"diff_log_wap1_median\"].fillna(0).values\n    sum_df[\"vol2_median\"] = sum_df[\"diff_log_wap2_median\"].fillna(0).values\n    sum_df[\"vol1_max\"] = sum_df[\"diff_log_wap1_max\"].fillna(0).values\n    sum_df[\"vol2_max\"] = sum_df[\"diff_log_wap2_max\"].fillna(0).values\n    sum_df[\"vol1_min\"] = sum_df[\"diff_log_wap1_min\"].fillna(0).values\n    sum_df[\"vol2_min\"] = sum_df[\"diff_log_wap2_min\"].fillna(0).values\n    sum_df[\"volatility_rate\"] = (sum_df[\"volatility1\"] / sum_df[\"volatility2\"]).fillna(0)\n    sum_df[\"mean_volatility_rate\"] = (sum_df[\"vol1_mean\"] / sum_df[\"vol2_mean\"]).fillna(0)\n    sum_df[\"std_volatility_rate\"] = (sum_df[\"vol1_std\"] / sum_df[\"vol2_std\"]).fillna(0)\n    sum_df[\"median_volatility_rate\"] = (sum_df[\"vol1_median\"] / sum_df[\"vol2_median\"]).fillna(0)\n    sum_df[\"max_volatility_rate\"] = (sum_df[\"vol1_max\"] / sum_df[\"vol2_max\"]).fillna(0)\n    sum_df[\"min_volatility_rate\"] = (sum_df[\"vol1_min\"] / sum_df[\"vol2_min\"]).fillna(0)\n    \n    return sum_df[[\"time_id\", \"volatility1\", \"volatility2\", \n                   \"volatility_rate\", \"vol1_std\", \"vol2_std\",\n                   \"vol1_mean\", \"vol2_mean\", \"vol1_median\", \"vol2_median\",\n                   \"vol1_max\", \"vol2_max\", \"vol1_min\", \"vol2_min\",\n                   \"mean_volatility_rate\", \"std_volatility_rate\",\n                   \"median_volatility_rate\", \"max_volatility_rate\",\n                   \"min_volatility_rate\"]]\n\ndef spread_fe(df):\n    \n    # Bid ask spread\n    df['bas'] = (df[['ask_price1', 'ask_price2']].min(axis = 1)\n                                / df[['bid_price1', 'bid_price2']].max(axis = 1) - 1)                               \n\n    # different spreads\n    df['h_spread_l1'] = df['ask_price1'] - df['bid_price1']\n    df['h_spread_l2'] = df['ask_price2'] - df['bid_price2']\n    df['v_spread_b'] = df['bid_price1'] - df['bid_price2']\n    df['v_spread_a'] = df['ask_price1'] - df['ask_price2']\n    \n    # Summary statistics for different spread\n    sum_df = df.groupby(\"time_id\").agg({\"h_spread_l1\": { \"mean\", \"std\", \"median\", \"max\", \"min\"}, \n                                        \"h_spread_l2\": { \"mean\", \"std\", \"median\", \"max\", \"min\"},\n                                        \"v_spread_b\": {\"mean\", \"std\", \"median\", \"max\", \"min\"},\n                                        \"v_spread_a\": {\"mean\", \"std\", \"median\", \"max\", \"min\"},\n                                        \"bas\": {\"mean\"}}\n                                      ).reset_index()\n    \n    \n    # Create wanted features for training\n    def f(x):\n        if x[1] == \"\":\n            return x[0]\n        return x[0] + \"_\" + x[1]\n    \n    sum_df.columns = [f(x) for x in sum_df.columns]\n\n    return sum_df\n    \n    \n\ndef get_stat(path):\n    \n    book = cudf.read_parquet(path)\n    stock_id = int(path.split(\"=\")[1].split(\"/\")[0])\n    book = fix_offsets(book)\n    #book = cudf.DataFrame(ffill(book))\n    rel_vol_data = rel_vol_fe(book)\n    spread_data = spread_fe(book)\n    transbook = cudf.merge(rel_vol_data,\n                           spread_data,\n                           on = ['time_id'], how = 'left')\n    transbook['stock_id'] = stock_id\n    \n    return transbook\n\n\ndef process_data(order_book_paths):\n    \n    df = Parallel(n_jobs=-1, verbose=1)(delayed(get_stat)(path)\n                             for path in tqdm(order_book_paths))\n    \n    stock_dfs = cudf.concat(df, ignore_index=True)\n    return stock_dfs","metadata":{"execution":{"iopub.status.busy":"2021-07-17T18:03:35.6773Z","iopub.execute_input":"2021-07-17T18:03:35.677641Z","iopub.status.idle":"2021-07-17T18:03:35.725322Z","shell.execute_reply.started":"2021-07-17T18:03:35.677607Z","shell.execute_reply":"2021-07-17T18:03:35.724262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature generation","metadata":{}},{"cell_type":"code","source":"%%time\ntrain_past_volatility = process_data(order_book_training)\nprint(train_past_volatility.shape)\ntrain_past_volatility.to_csv(\"./train_past_volatility.csv\")\ntrain_past_volatility.columns","metadata":{"execution":{"iopub.status.busy":"2021-07-17T18:03:35.730639Z","iopub.execute_input":"2021-07-17T18:03:35.733195Z","iopub.status.idle":"2021-07-17T18:05:39.185043Z","shell.execute_reply.started":"2021-07-17T18:03:35.73313Z","shell.execute_reply":"2021-07-17T18:05:39.183855Z"},"trusted":true},"execution_count":null,"outputs":[]}]}