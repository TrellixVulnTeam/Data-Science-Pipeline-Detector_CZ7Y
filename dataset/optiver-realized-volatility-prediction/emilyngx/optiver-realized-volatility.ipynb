{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Generate a series of short-term signals from the book and trade data of a fixed 10-minute window to predict the realized volatility of the next 10-minute window","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\nimport glob","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-12T22:34:25.98149Z","iopub.execute_input":"2021-08-12T22:34:25.981838Z","iopub.status.idle":"2021-08-12T22:34:25.988493Z","shell.execute_reply.started":"2021-08-12T22:34:25.981807Z","shell.execute_reply":"2021-08-12T22:34:25.986855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Data directory\ndata_dir = '../input/optiver-realized-volatility-prediction/'","metadata":{"execution":{"iopub.status.busy":"2021-08-12T22:34:27.417072Z","iopub.execute_input":"2021-08-12T22:34:27.417487Z","iopub.status.idle":"2021-08-12T22:34:27.42174Z","shell.execute_reply.started":"2021-08-12T22:34:27.417457Z","shell.execute_reply":"2021-08-12T22:34:27.42058Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Functions for preprocessing","metadata":{}},{"cell_type":"code","source":"# To calculate first WAP\ndef calc_wap(df):\n    wap = (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1'])/(df['bid_size1'] + df['ask_size1'])\n    return wap\n\n# To calculate second WAP\ndef calc_wap2(df):\n    wap = (df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2'])/(df['bid_size2'] + df['ask_size2'])\n    return wap\n\n# To calculate log return\ndef log_return(list_stock_prices):\n    return np.log(list_stock_prices).diff() \n\n# To calculate realized volatility \ndef realized_volatility(series):\n    return np.sqrt(np.sum(series**2))\n\n# To count unique elements of a series\ndef count_unique(series):\n    return len(np.unique(series))","metadata":{"execution":{"iopub.status.busy":"2021-08-12T22:34:28.935205Z","iopub.execute_input":"2021-08-12T22:34:28.935593Z","iopub.status.idle":"2021-08-12T22:34:28.945536Z","shell.execute_reply.started":"2021-08-12T22:34:28.935561Z","shell.execute_reply":"2021-08-12T22:34:28.94418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Book data","metadata":{}},{"cell_type":"code","source":"# Load parquet\nbook_train = pd.read_parquet(data_dir + \"book_train.parquet/stock_id=15\")\n\n# Book data snapshot\nbook_train.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-12T22:34:32.13352Z","iopub.execute_input":"2021-08-12T22:34:32.133904Z","iopub.status.idle":"2021-08-12T22:34:33.156397Z","shell.execute_reply.started":"2021-08-12T22:34:32.133869Z","shell.execute_reply":"2021-08-12T22:34:33.155189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Functions to preprocess book data","metadata":{}},{"cell_type":"code","source":"# To preprocess book data (for each stock_id)\ndef preprocessor_book(file_path):\n    df = pd.read_parquet(file_path)\n    \n    # Calculate WAP\n    df['wap'] = calc_wap(df)\n    df['wap2'] = calc_wap2(df)\n    \n    # Calculate log return\n    df['log_return'] = df.groupby('time_id')['wap'].apply(log_return)\n    df['log_return2'] = df.groupby('time_id')['wap2'].apply(log_return)\n    \n    # Calculate WAP balance\n    df['wap_balance'] = abs(df['wap'] - df['wap2'])\n    \n    # Calculate spread\n    df['price_spread'] = (df['ask_price1'] - df['bid_price1']) / ((df['ask_price1'] + df['bid_price1'])/2)\n    df['bid_spread'] = df['bid_price1'] - df['bid_price2']\n    df['ask_spread'] = df['ask_price1'] - df['ask_price2']\n    df['total_volume'] = (df['ask_size1'] + df['ask_size2']) + (df['bid_size1'] + df['bid_size2'])\n    df['volume_imbalance'] = abs((df['ask_size1'] + df['ask_size2']) - (df['bid_size1'] + df['bid_size2']))\n\n    # Dict for aggregations\n    create_feature_dict = {\n        'wap': [np.sum, np.mean, np.std],\n        'wap2': [np.sum, np.mean, np.std],\n        'log_return': [np.sum, realized_volatility, np.mean, np.std],\n        'log_return2': [np.sum, realized_volatility, np.mean, np.std],\n        'wap_balance': [np.sum, np.mean, np.std],\n        'price_spread': [np.sum, np.mean, np.std],\n        'bid_spread': [np.sum, np.mean, np.std],\n        'ask_spread': [np.sum, np.mean, np.std],\n        'total_volume': [np.sum, np.mean, np.std],\n        'volume_imbalance': [np.sum, np.mean, np.std]\n    }\n    \n    # Functions to get group stats for different time windows (0-199, 200-399, 400-599)\n    def get_stats_window(seconds_in_bucket, add_suffix = False):\n        # Group by time window\n        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(create_feature_dict).reset_index()\n        # Rename columns joining suffix\n        df_feature.columns = ['_'.join(col) for col in df_feature.columns] #time_id is changed to time_id_\n        # Add a suffix to differentiate time windows\n        if add_suffix:\n            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n        return df_feature\n    \n    # Get stats for 3 time windows\n    df_feature = get_stats_window(seconds_in_bucket = 0, add_suffix = False)\n    df_feature_200 = get_stats_window(seconds_in_bucket = 200, add_suffix = True)\n    df_feature_400 = get_stats_window(seconds_in_bucket = 400, add_suffix = True)\n    \n    # Merge them all\n    df_feature = df_feature.merge(df_feature_200, how = 'left', left_on = 'time_id_', right_on = 'time_id__200')\n    df_feature = df_feature.merge(df_feature_400, how = 'left', left_on = 'time_id_', right_on = 'time_id__400')\n    \n    # Drop unecessary time_id__200 and time_id__400\n    df_feature.drop(['time_id__200', 'time_id__400'], axis = 1, inplace = True)\n    \n    # Create row_id to merge book and trade data\n    stock_id = file_path.split('=')[1]\n    df_feature['row_id'] = df_feature['time_id_'].apply(lambda x: f'{stock_id}-{x}')\n    df_feature.drop(['time_id_'], axis = 1, inplace = True)\n    \n    return df_feature","metadata":{"execution":{"iopub.status.busy":"2021-08-12T22:34:35.323355Z","iopub.execute_input":"2021-08-12T22:34:35.323838Z","iopub.status.idle":"2021-08-12T22:34:35.343602Z","shell.execute_reply.started":"2021-08-12T22:34:35.323756Z","shell.execute_reply":"2021-08-12T22:34:35.342329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nfile_path = data_dir + \"book_train.parquet/stock_id=0\"\npreprocessor_book(file_path)","metadata":{"execution":{"iopub.status.busy":"2021-08-12T22:34:39.348816Z","iopub.execute_input":"2021-08-12T22:34:39.349255Z","iopub.status.idle":"2021-08-12T22:34:53.899795Z","shell.execute_reply.started":"2021-08-12T22:34:39.349208Z","shell.execute_reply":"2021-08-12T22:34:53.898328Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Trade data","metadata":{}},{"cell_type":"code","source":"# Load parquet\ntrade_train = pd.read_parquet(data_dir + \"trade_train.parquet/stock_id=0\")\n\n# Trade data snapshot\ntrade_train.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-12T22:34:53.903976Z","iopub.execute_input":"2021-08-12T22:34:53.904391Z","iopub.status.idle":"2021-08-12T22:34:53.972962Z","shell.execute_reply.started":"2021-08-12T22:34:53.904356Z","shell.execute_reply":"2021-08-12T22:34:53.971809Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Functions to preprocess trade data","metadata":{}},{"cell_type":"code","source":"# To preprocess trade data (for each stock_id)\ndef preprocessor_trade(file_path):\n    df = pd.read_parquet(file_path)\n    df['log_return'] = df.groupby('time_id')['price'].apply(log_return)\n    \n    # Dict for aggregations\n    create_feature_dict = {\n        'log_return':[realized_volatility],\n        'seconds_in_bucket':[count_unique],\n        'size':[np.sum],\n        'order_count':[np.mean]\n    }\n    \n    # Functions to get group stats for different time windows (0-199, 200-399, 400-599)\n    def get_stats_window(seconds_in_bucket, add_suffix = False):\n        # Group by time window\n        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(create_feature_dict).reset_index()\n        # Rename columns joining suffix\n        df_feature.columns = ['_'.join(col) for col in df_feature.columns] #time_id is changed to time_id_\n        # Add a suffix to differentiate time windows\n        if add_suffix:\n            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n        return df_feature\n    \n    # Get stats for 3 time windows\n    df_feature = get_stats_window(seconds_in_bucket = 0, add_suffix = False)\n    df_feature_200 = get_stats_window(seconds_in_bucket = 200, add_suffix = True)\n    df_feature_400 = get_stats_window(seconds_in_bucket = 400, add_suffix = True)\n    \n    # Merge them all\n    df_feature = df_feature.merge(df_feature_200, how = 'left', left_on = 'time_id_', right_on = 'time_id__200')\n    df_feature = df_feature.merge(df_feature_400, how = 'left', left_on = 'time_id_', right_on = 'time_id__400')\n    \n    # Drop unecessary time_id__200 and time_id__400\n    df_feature.drop(['time_id__200', 'time_id__400'], axis = 1, inplace = True)\n    \n    # Add prefix trade_\n    df_feature = df_feature.add_prefix('trade_')\n    \n    # Create row_id to merge with book and trade data\n    stock_id = file_path.split('=')[1]\n    df_feature['row_id'] = df_feature['trade_time_id_'].apply(lambda x:f'{stock_id}-{x}')\n    df_feature.drop(['trade_time_id_'], axis = 1, inplace = True)\n    \n    return df_feature","metadata":{"execution":{"iopub.status.busy":"2021-08-12T22:34:53.975307Z","iopub.execute_input":"2021-08-12T22:34:53.975983Z","iopub.status.idle":"2021-08-12T22:34:53.990604Z","shell.execute_reply.started":"2021-08-12T22:34:53.97591Z","shell.execute_reply":"2021-08-12T22:34:53.989218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nfile_path = data_dir + \"trade_train.parquet/stock_id=0\"\npreprocessor_trade(file_path)","metadata":{"execution":{"iopub.status.busy":"2021-08-12T22:34:53.99292Z","iopub.execute_input":"2021-08-12T22:34:53.993645Z","iopub.status.idle":"2021-08-12T22:35:00.564376Z","shell.execute_reply.started":"2021-08-12T22:34:53.993588Z","shell.execute_reply":"2021-08-12T22:35:00.56321Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Functions to get group stats for the stock_id and time_id","metadata":{}},{"cell_type":"code","source":"def get_time_stock(df):\n    # Get realized volatility columns\n    vol_cols = ['log_return_realized_volatility', 'log_return2_realized_volatility', \n                'log_return_realized_volatility_200', 'log_return2_realized_volatility_200', \n                'log_return_realized_volatility_400', 'log_return2_realized_volatility_400', \n                'trade_log_return_realized_volatility', \n                'trade_log_return_realized_volatility_200', \n                'trade_log_return_realized_volatility_400'\n               ]\n\n    # Group by the stock id\n    df_stock_id = df.groupby(['stock_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n    \n    # Rename columns joining suffix\n    df_stock_id.columns = ['_'.join(col) for col in df_stock_id.columns]\n    df_stock_id = df_stock_id.add_suffix('_' + 'stock')\n\n    # Group by the stock id\n    df_time_id = df.groupby(['time_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n    \n    # Rename columns joining suffix\n    df_time_id.columns = ['_'.join(col) for col in df_time_id.columns]\n    df_time_id = df_time_id.add_suffix('_' + 'time')\n    \n    # Merge with original dataframe\n    df = df.merge(df_stock_id, how = 'left', left_on = ['stock_id'], right_on = ['stock_id__stock'])\n    df = df.merge(df_time_id, how = 'left', left_on = ['time_id'], right_on = ['time_id__time'])\n    \n    # Drop unecessary columns\n    df.drop(['stock_id__stock', 'time_id__time'], axis = 1, inplace = True)\n    \n    return df","metadata":{"execution":{"iopub.status.busy":"2021-08-12T22:35:11.880786Z","iopub.execute_input":"2021-08-12T22:35:11.881244Z","iopub.status.idle":"2021-08-12T22:35:11.894053Z","shell.execute_reply.started":"2021-08-12T22:35:11.881211Z","shell.execute_reply":"2021-08-12T22:35:11.892837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Combined preprocessor function","metadata":{}},{"cell_type":"code","source":"# To make preprocessing function in parallel (for each stock_id)\ndef preprocessor(list_stock_ids, is_train = True):\n    from joblib import Parallel, delayed # parallel computing to save time\n    df = pd.DataFrame()\n    \n    # Parallel for loop\n    def for_joblib(stock_id):\n        # Train\n        if is_train:\n            file_path_book = data_dir + \"book_train.parquet/stock_id=\" + str(stock_id)\n            file_path_trade = data_dir + \"trade_train.parquet/stock_id=\" + str(stock_id)\n        # Test\n        else:\n            file_path_book = data_dir + \"book_test.parquet/stock_id=\" + str(stock_id)\n            file_path_trade = data_dir + \"trade_test.parquet/stock_id=\" + str(stock_id)\n        # Preprocess book & trade data and merge them    \n        df_tmp = pd.merge(preprocessor_book(file_path_book), preprocessor_trade(file_path_trade), on = 'row_id', how = 'left')\n        # Return the merged DataFrame\n        return pd.concat([df,df_tmp])\n    \n    # Use parallel API to call parallel for loop\n    df = Parallel(n_jobs = -1, verbose = 1)(delayed(for_joblib)(stock_id) for stock_id in list_stock_ids)\n    # Concatinate all the DataFrames that return from parallel\n    df = pd.concat(df, ignore_index = True)\n    \n    return df","metadata":{"execution":{"iopub.status.busy":"2021-08-12T22:35:14.850134Z","iopub.execute_input":"2021-08-12T22:35:14.850539Z","iopub.status.idle":"2021-08-12T22:35:14.860571Z","shell.execute_reply.started":"2021-08-12T22:35:14.850508Z","shell.execute_reply":"2021-08-12T22:35:14.858782Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"list_stock_ids = [0,1]\npreprocessor(list_stock_ids, is_train = True)","metadata":{"execution":{"iopub.status.busy":"2021-08-12T22:35:18.060192Z","iopub.execute_input":"2021-08-12T22:35:18.060611Z","iopub.status.idle":"2021-08-12T22:35:53.829953Z","shell.execute_reply.started":"2021-08-12T22:35:18.060578Z","shell.execute_reply":"2021-08-12T22:35:53.828584Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Change data type to reduce memory usage**","metadata":{}},{"cell_type":"code","source":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int8','int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df","metadata":{"execution":{"iopub.status.busy":"2021-08-12T22:35:53.835401Z","iopub.execute_input":"2021-08-12T22:35:53.83797Z","iopub.status.idle":"2021-08-12T22:35:53.860171Z","shell.execute_reply.started":"2021-08-12T22:35:53.837902Z","shell.execute_reply":"2021-08-12T22:35:53.858854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ntrain = reduce_mem_usage(pd.read_csv(data_dir + 'train.csv'))\n#test = reduce_mem_usage(pd.read_csv(data_dir + 'test.csv'))\nprint(\"Shape of train set: \",train.shape)\n#print(\"Shape of test set: \",test.shape)","metadata":{"execution":{"iopub.status.busy":"2021-08-12T22:35:53.865451Z","iopub.execute_input":"2021-08-12T22:35:53.867043Z","iopub.status.idle":"2021-08-12T22:35:54.152824Z","shell.execute_reply.started":"2021-08-12T22:35:53.866992Z","shell.execute_reply":"2021-08-12T22:35:54.151714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training set","metadata":{}},{"cell_type":"code","source":"train_ids = train.stock_id.unique()","metadata":{"execution":{"iopub.status.busy":"2021-08-12T22:35:54.155137Z","iopub.execute_input":"2021-08-12T22:35:54.155565Z","iopub.status.idle":"2021-08-12T22:35:54.165452Z","shell.execute_reply.started":"2021-08-12T22:35:54.155519Z","shell.execute_reply":"2021-08-12T22:35:54.164025Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ndf_train = preprocessor(list_stock_ids = train_ids, is_train = True)","metadata":{"execution":{"iopub.status.busy":"2021-08-12T22:35:54.167333Z","iopub.execute_input":"2021-08-12T22:35:54.167898Z","iopub.status.idle":"2021-08-12T23:07:58.018802Z","shell.execute_reply.started":"2021-08-12T22:35:54.167851Z","shell.execute_reply":"2021-08-12T23:07:58.01757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Combine stock_id and time_id\ntrain['row_id'] = train['stock_id'].astype(str) + '-' + train['time_id'].astype(str)\ntrain = train[['row_id','target']]\n\ndf_train = train.merge(df_train, on = ['row_id'], how = 'left')","metadata":{"execution":{"iopub.status.busy":"2021-08-12T23:07:58.020662Z","iopub.execute_input":"2021-08-12T23:07:58.021198Z","iopub.status.idle":"2021-08-12T23:08:03.750186Z","shell.execute_reply.started":"2021-08-12T23:07:58.021151Z","shell.execute_reply":"2021-08-12T23:08:03.749007Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-12T23:08:03.751699Z","iopub.execute_input":"2021-08-12T23:08:03.752134Z","iopub.status.idle":"2021-08-12T23:08:03.799097Z","shell.execute_reply.started":"2021-08-12T23:08:03.75209Z","shell.execute_reply":"2021-08-12T23:08:03.797728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Test set","metadata":{}},{"cell_type":"code","source":"test = pd.read_csv(data_dir + 'test.csv')","metadata":{"execution":{"iopub.status.busy":"2021-08-12T23:08:03.802857Z","iopub.execute_input":"2021-08-12T23:08:03.803296Z","iopub.status.idle":"2021-08-12T23:08:03.816742Z","shell.execute_reply.started":"2021-08-12T23:08:03.803263Z","shell.execute_reply":"2021-08-12T23:08:03.815292Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_ids = test.stock_id.unique()","metadata":{"execution":{"iopub.status.busy":"2021-08-12T23:08:03.819322Z","iopub.execute_input":"2021-08-12T23:08:03.819945Z","iopub.status.idle":"2021-08-12T23:08:03.826296Z","shell.execute_reply.started":"2021-08-12T23:08:03.819896Z","shell.execute_reply":"2021-08-12T23:08:03.824691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ndf_test = preprocessor(list_stock_ids = test_ids, is_train = False)","metadata":{"execution":{"iopub.status.busy":"2021-08-12T23:08:03.828092Z","iopub.execute_input":"2021-08-12T23:08:03.828749Z","iopub.status.idle":"2021-08-12T23:08:04.123414Z","shell.execute_reply.started":"2021-08-12T23:08:03.828686Z","shell.execute_reply":"2021-08-12T23:08:04.119187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test = test.merge(df_test, on = ['row_id'], how = 'left')","metadata":{"execution":{"iopub.status.busy":"2021-08-12T23:08:04.125902Z","iopub.execute_input":"2021-08-12T23:08:04.12671Z","iopub.status.idle":"2021-08-12T23:08:04.150299Z","shell.execute_reply.started":"2021-08-12T23:08:04.126647Z","shell.execute_reply":"2021-08-12T23:08:04.149133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Target encoding by stock_id","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import KFold\n#stock_id target encoding\ndf_train['stock_id'] = df_train['row_id'].apply(lambda x:x.split('-')[0])\ndf_test['stock_id'] = df_test['row_id'].apply(lambda x:x.split('-')[0])\n\nstock_id_target_mean = df_train.groupby('stock_id')['target'].mean() \ndf_test['stock_id_target_enc'] = df_test['stock_id'].map(stock_id_target_mean) # test_set\n\n#training\ntmp = np.repeat(np.nan, df_train.shape[0])\nkf = KFold(n_splits = 20, shuffle = True,random_state = 99)\nfor idx_1, idx_2 in kf.split(df_train):\n    target_mean = df_train.iloc[idx_1].groupby('stock_id')['target'].mean()\n\n    tmp[idx_2] = df_train['stock_id'].iloc[idx_2].map(target_mean)\ndf_train['stock_id_target_enc'] = tmp","metadata":{"execution":{"iopub.status.busy":"2021-08-12T23:08:04.153567Z","iopub.execute_input":"2021-08-12T23:08:04.153909Z","iopub.status.idle":"2021-08-12T23:08:08.787758Z","shell.execute_reply.started":"2021-08-12T23:08:04.15388Z","shell.execute_reply":"2021-08-12T23:08:08.786586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Building","metadata":{}},{"cell_type":"code","source":"df_train.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-12T23:08:08.789405Z","iopub.execute_input":"2021-08-12T23:08:08.789881Z","iopub.status.idle":"2021-08-12T23:08:08.8354Z","shell.execute_reply.started":"2021-08-12T23:08:08.789802Z","shell.execute_reply":"2021-08-12T23:08:08.834242Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-12T23:08:08.836959Z","iopub.execute_input":"2021-08-12T23:08:08.837473Z","iopub.status.idle":"2021-08-12T23:08:08.873123Z","shell.execute_reply.started":"2021-08-12T23:08:08.837422Z","shell.execute_reply":"2021-08-12T23:08:08.871745Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# LightGBM","metadata":{}},{"cell_type":"code","source":"import lightgbm as lgbm\nfrom bayes_opt import BayesianOptimization","metadata":{"execution":{"iopub.status.busy":"2021-08-12T23:08:08.874909Z","iopub.execute_input":"2021-08-12T23:08:08.875446Z","iopub.status.idle":"2021-08-12T23:08:12.006151Z","shell.execute_reply.started":"2021-08-12T23:08:08.875401Z","shell.execute_reply":"2021-08-12T23:08:12.004937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Transform stock_id to integer\ndf_train['stock_id'] = df_train['stock_id'].astype(int)\ndf_test['stock_id'] = df_test['stock_id'].astype(int)","metadata":{"execution":{"iopub.status.busy":"2021-08-12T23:08:12.007673Z","iopub.execute_input":"2021-08-12T23:08:12.008178Z","iopub.status.idle":"2021-08-12T23:08:12.080428Z","shell.execute_reply.started":"2021-08-12T23:08:12.008131Z","shell.execute_reply":"2021-08-12T23:08:12.079311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Cross Validation","metadata":{}},{"cell_type":"code","source":"# Split features and target\nX = df_train.drop(['row_id','target'], axis=1)\ny = df_train['target']","metadata":{"execution":{"iopub.status.busy":"2021-08-12T23:08:12.082102Z","iopub.execute_input":"2021-08-12T23:08:12.082561Z","iopub.status.idle":"2021-08-12T23:08:12.299807Z","shell.execute_reply.started":"2021-08-12T23:08:12.082514Z","shell.execute_reply":"2021-08-12T23:08:12.29855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Hyper-parameter tuning (Bayesian Optimization)\ndef bayes_parameter_opt_lgb(X, y, init_round = 15, opt_round = 25, n_folds = 3, random_seed = 6, n_estimators = 10000, output_process = False):\n    # prepare data\n    train_data = lgbm.Dataset(data = X, label = y, free_raw_data = False)\n    # parameters\n    def lgb_eval(learning_rate,num_leaves, feature_fraction, bagging_fraction, max_depth, max_bin, min_data_in_leaf,min_sum_hessian_in_leaf,subsample):\n        params = {'application':'binary', 'metric':'auc'}\n        params['learning_rate'] = max(min(learning_rate, 1), 0)\n        params[\"num_leaves\"] = int(round(num_leaves))\n        params['feature_fraction'] = max(min(feature_fraction, 1), 0)\n        params['bagging_fraction'] = max(min(bagging_fraction, 1), 0)\n        params['max_depth'] = int(round(max_depth))\n        params['max_bin'] = int(round(max_depth))\n        params['min_data_in_leaf'] = int(round(min_data_in_leaf))\n        params['min_sum_hessian_in_leaf'] = min_sum_hessian_in_leaf\n        params['subsample'] = max(min(subsample, 1), 0)\n        \n        cv_result = lgbm.cv(params, train_data, nfold = n_folds, seed=random_seed, stratified=True, verbose_eval =200, metrics=['auc'])\n        return max(cv_result['auc-mean'])\n     \n    lgbBO = BayesianOptimization(lgb_eval, {'learning_rate': (0.01, 1.0), # recommended to use smaller learning_rate with larger num_iterations\n                                            'num_leaves': (24, 30000), # num_leaves = 2^(max_depth)\n                                            'feature_fraction': (0.1, 0.9), # randomly select a subset of features on each iteration (tree) (%) -> speed + overfit\n                                            'bagging_fraction': (0.8, 1),\n                                            'max_depth': (5, 30),\n                                            'max_bin':(20,90),\n                                            'min_data_in_leaf': (20, 1000),\n                                            'min_sum_hessian_in_leaf':(0,100),\n                                           'subsample': (0.01, 1.0)}, random_state=200)\n\n    \n    #n_iter: How many steps of bayesian optimization you want to perform. The more steps the more likely to find a good maximum you are.\n    #init_points: How many steps of random exploration you want to perform. Random exploration can help by diversifying the exploration space.\n    \n    lgbBO.maximize(init_points = init_round, n_iter = opt_round)\n    \n    model_auc=[]\n    for model in range(len( lgbBO.res)):\n        model_auc.append(lgbBO.res[model]['target'])\n    \n    # return best parameters\n    return lgbBO.res[pd.Series(model_auc).idxmax()]['target'],lgbBO.res[pd.Series(model_auc).idxmax()]['params']\n\nopt_params = bayes_parameter_opt_lgb(X, y, init_round = 5, opt_round = 10, n_folds = 3, random_seed = 6,n_estimators = 10000)","metadata":{"execution":{"iopub.status.busy":"2021-08-12T23:08:12.301608Z","iopub.execute_input":"2021-08-12T23:08:12.302074Z","iopub.status.idle":"2021-08-12T23:08:17.822566Z","shell.execute_reply.started":"2021-08-12T23:08:12.302028Z","shell.execute_reply":"2021-08-12T23:08:17.81822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Optimized parameters\nopt_params[1][\"num_leaves\"] = int(round(opt_params[1][\"num_leaves\"]))\nopt_params[1]['max_depth'] = int(round(opt_params[1]['max_depth']))\nopt_params[1]['min_data_in_leaf'] = int(round(opt_params[1]['min_data_in_leaf']))\nopt_params[1]['max_bin'] = int(round(opt_params[1]['max_bin']))\nopt_params[1]['objective'] = 'binary'\nopt_params[1]['metric'] = 'auc'\nopt_params[1]['is_unbalance'] = True\nopt_params[1]['boost_from_average'] = False\nopt_params = opt_params[1]\nopt_params","metadata":{"execution":{"iopub.status.busy":"2021-08-12T21:56:44.022821Z","iopub.execute_input":"2021-08-12T21:56:44.023245Z","iopub.status.idle":"2021-08-12T21:56:44.05047Z","shell.execute_reply.started":"2021-08-12T21:56:44.02321Z","shell.execute_reply":"2021-08-12T21:56:44.048755Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# To calculate RMSPE\ndef rmspe(y_true, y_pred):\n    return (np.sqrt(np.mean(np.square((y_true - y_pred) / y_true))))\n\n# Early stop with RMSPE\ndef feval_RMSPE(preds, lgbm_train): #Customized evaluation function\n    labels = lgbm_train.get_label()\n    return 'RMSPE', round(rmspe(y_true = labels, y_pred = preds),5), False\n\nseed = 42\nparams = {\n      'objective': 'rmse', \n      'metric': 'rmse', \n      'boosting_type': 'gbdt',\n      'learning_rate': 0.01,        \n        'num_leaves': 27293,\n        'feature_fraction': 0.2812379369053784,\n        'min_sum_hessian_in_leaf': 35.74236812873617,\n        'bagging_fraction': 0.9895264513703341,\n        'max_bin': 50,\n        'min_data_in_leaf': 23,\n        'max_depth': 24,\n        'seed': seed,\n        'feature_fraction_seed': seed,\n        'bagging_seed': seed,\n        'drop_seed': seed,\n        'data_random_seed': seed,\n        'verbosity': -1,\n        'n_jobs': -1\n  }","metadata":{"execution":{"iopub.status.busy":"2021-08-12T23:14:13.834154Z","iopub.execute_input":"2021-08-12T23:14:13.83467Z","iopub.status.idle":"2021-08-12T23:14:13.844592Z","shell.execute_reply.started":"2021-08-12T23:14:13.834638Z","shell.execute_reply":"2021-08-12T23:14:13.843207Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import KFold\n# Create a KFold object\nkf = KFold(n_splits = 20, random_state = 19901028, shuffle = True)\noof = pd.DataFrame()                 # out-of-fold result\nmodels = []                          # models\nscores = 0.0                         # validation score","metadata":{"execution":{"iopub.status.busy":"2021-08-12T23:14:19.052167Z","iopub.execute_input":"2021-08-12T23:14:19.052572Z","iopub.status.idle":"2021-08-12T23:14:19.059424Z","shell.execute_reply.started":"2021-08-12T23:14:19.052538Z","shell.execute_reply":"2021-08-12T23:14:19.058104Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Iterate through each fold\n#%%time\nfor fold, (trn_idx, val_idx) in enumerate(kf.split(X, y)):\n\n    print(\"Fold\", fold + 1)\n    \n    # Create dataset\n    X_train, y_train = X.loc[trn_idx], y[trn_idx]\n    X_valid, y_valid = X.loc[val_idx], y[val_idx]\n    \n    # RMSPE weights\n    train_weights = 1/np.square(y_train)\n    lgbm_train = lgbm.Dataset(X_train, y_train, weight = train_weights)\n\n    valid_weights = 1/np.square(y_valid)\n    lgbm_valid = lgbm.Dataset(X_valid, y_valid, reference = lgbm_train, weight = valid_weights)\n    \n    # Model \n    model = lgbm.train(params = params,\n                      train_set = lgbm_train,\n                      valid_sets = [lgbm_train, lgbm_valid],\n                      num_boost_round = 10000,\n                      early_stopping_rounds = 100, # rule of thumb is to have it at 10% of your num_iterations.\n                      feval = feval_RMSPE,\n                      verbose_eval = 200, # the eval metric on the valid set is printed at every 100 boosting stage. \n                      categorical_feature = ['stock_id']                \n                     )\n    \n    # validation \n    y_pred = model.predict(X_valid, num_iteration = model.best_iteration)\n\n    RMSPE = round(rmspe(y_true = y_valid, y_pred = y_pred),3)\n    print(f'Performance of the　prediction: , RMSPE: {RMSPE}')\n\n    #keep scores and models\n    scores += RMSPE / 20\n    models.append(model)\n    print(\"*\" * 100)","metadata":{"execution":{"iopub.status.busy":"2021-08-12T23:14:23.348572Z","iopub.execute_input":"2021-08-12T23:14:23.349012Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scores","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Test set","metadata":{}},{"cell_type":"code","source":"df_test.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = df_test[['row_id']]\nX_test = df_test.drop(['time_id', 'row_id'], axis = 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target = np.zeros(len(X_test))\n\n# light gbm models\nfor model in models:\n    pred = model.predict(X_test[X_valid.columns], num_iteration=model.best_iteration)\n    target += pred / len(models)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = y_pred.assign(target = target)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred.to_csv('submission.csv',index = False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}