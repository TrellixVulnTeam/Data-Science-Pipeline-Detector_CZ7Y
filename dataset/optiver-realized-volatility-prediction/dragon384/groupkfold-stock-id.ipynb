{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## GroupKfoldを行なった後にstock_id系統のカラムを追加した場合の精度を調査","metadata":{}},{"cell_type":"code","source":"# 並列化\n# CPUを複数使ってくれるようになるらしい。これがあるのと無いのとでは処理の時間が全然違う\nfrom joblib import Parallel, delayed\n\nimport pandas as pd\nimport numpy as np\nimport scipy as sc\n\n# 交差検証Kfold\nfrom sklearn.model_selection import KFold\n# lgbm\nimport lightgbm as lgb\n# 警告文を出さないようにする\nimport warnings\nfrom sklearn.cluster import KMeans\nwarnings.filterwarnings('ignore')\n\npd.set_option('max_columns', 300)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-09-21T10:08:38.436291Z","iopub.execute_input":"2021-09-21T10:08:38.436871Z","iopub.status.idle":"2021-09-21T10:08:38.443929Z","shell.execute_reply.started":"2021-09-21T10:08:38.436814Z","shell.execute_reply":"2021-09-21T10:08:38.442987Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# WAPをbidとaskの最大値最小値を使って計算したものと2番目のものを使った場合の関数\ndef calc_wap1(df):\n    wap = (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']) / (df['bid_size1'] + df['ask_size1'])\n    return wap\n\ndef calc_wap2(df):\n    wap = (df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2']) / (df['bid_size2'] + df['ask_size2'])\n    return wap\n\n# Log returnを計算する関数\ndef log_return(series):\n    return np.log(series).diff()\n\n# 目的変数であるボラティリティを計算する関数\ndef realized_volatility(series):\n    return np.sqrt(np.sum(series**2))\n\n# seriesのユニークな値の個数をカウントする関数\ndef count_unique(series):\n    return len(np.unique(series))","metadata":{"execution":{"iopub.status.busy":"2021-09-21T10:08:38.445452Z","iopub.execute_input":"2021-09-21T10:08:38.445734Z","iopub.status.idle":"2021-09-21T10:08:38.458063Z","shell.execute_reply.started":"2021-09-21T10:08:38.445708Z","shell.execute_reply":"2021-09-21T10:08:38.457201Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pathを設定\ndata_dir = '../input/optiver-realized-volatility-prediction/'","metadata":{"execution":{"iopub.status.busy":"2021-09-21T10:08:38.459628Z","iopub.execute_input":"2021-09-21T10:08:38.459915Z","iopub.status.idle":"2021-09-21T10:08:38.476408Z","shell.execute_reply.started":"2021-09-21T10:08:38.459889Z","shell.execute_reply":"2021-09-21T10:08:38.475035Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# 訓練データとテストデータを読み込むための関数\ndef read_train_test():\n    train = pd.read_csv(data_dir + 'train.csv')\n    test = pd.read_csv(data_dir + 'test.csv')\n    # オーダーブックとトレードデータと結合するためにidとなるカラムを作成\n    train['row_id'] = train['stock_id'].astype(str) + '-' + train['time_id'].astype(str)\n    test['row_id'] = test['stock_id'].astype(str) + '-' + test['time_id'].astype(str)\n    # 関数が実行されたらtrainデータの行数をprintするように設定\n    print(f'Our training set has {train.shape[0]} rows')\n    return train, test\n\n# オーダーブックデータの前処理を行う関数\n# 今回行う前処り\ndef book_preprocessor(file_path):\n    df = pd.read_parquet(file_path)\n    # WAP1,2を計算した結果カラムとして追加\n    df['wap1'] = calc_wap1(df)\n    df['wap2'] = calc_wap2(df)\n    # time_idごとのlog return1,2を計算した結果カラムとして追加\n    df['log_return1'] = df.groupby(['time_id'])['wap1'].apply(log_return)\n    df['log_return2'] = df.groupby(['time_id'])['wap2'].apply(log_return)\n    # WAP1とWAP2の絶対値の差を取得してカラムとして追加\n    df['wap_balance'] = abs(df['wap1'] - df['wap2'])\n    # spreadを計算\n    # price_spreadは下記のように計算\n    # df['ask_price1'] - df['bid_price1']は最高売値 - 最低買値の差\n    # ((df['ask_price1'] + df['bid_price1']) / 2)は最高売値と最低買値の平均値\n    # 双方の値段が離れるほど大きくなる離れ度合い的な意味合い\n    df['price_spread'] = (df['ask_price1'] - df['bid_price1']) / ((df['ask_price1'] + df['bid_price1']) / 2)\n    df['price_spread2'] = (df['ask_price2'] - df['bid_price2']) / ((df['ask_price2'] + df['bid_price2']) / 2)\n    # 最大と二番目のそれぞれの差\n    # 値段が詰まっているほど人気？ → これが小さいほど良い市場と言えるのではないか\n    df['bid_spread'] = df['bid_price1'] - df['bid_price2']\n    df['ask_spread'] = df['ask_price1'] - df['ask_price2']\n    # spreadの分子。spreadと同じくこれが大きいと良い市場\n    df[\"bid_ask_spread\"] = abs(df['bid_spread'] - df['ask_spread'])\n    # askとbidの一番と二番目の予約の合計 → これが大きいということは売買が盛んということ（買いたい人と売りたい人が多い）\n    df['total_volume'] = (df['ask_size1'] + df['ask_size2']) + (df['bid_size1'] + df['bid_size2'])\n    # 買い手と売り手どっちの差の絶対値 → これが大きいとどちらかに需要が偏りがあるから良くない？\n    df['volume_imbalance'] = abs((df['ask_size1'] + df['ask_size2']) - (df['bid_size1'] + df['bid_size2']))\n    # 需要と供給の割合を追加\n    df['bid_ask_ratio'] = (df['ask_size1']+df['ask_size2'])/(df['bid_size1']+df['bid_size2'])\n    \n    # 辞書型でカラム名と操作を用意\n    create_feature_dict = {\n        'wap1': [np.sum, np.mean, np.std],\n        'wap2': [np.sum, np.mean, np.std],\n        'log_return1': [np.sum, realized_volatility, np.mean, np.std],\n        'log_return2': [np.sum, realized_volatility, np.mean, np.std],\n        'wap_balance': [np.sum, np.mean, np.std],\n        'price_spread':[np.sum, np.mean, np.std],\n        'price_spread2':[np.sum, np.mean, np.std],\n        'bid_spread':[np.sum, np.mean, np.std],\n        'ask_spread':[np.sum, np.mean, np.std],\n        'total_volume':[np.sum, np.mean, np.std],\n        'volume_imbalance':[np.sum, np.mean, np.std],\n        'bid_ask_ratio':[np.sum, np.mean, np.std],\n        \"bid_ask_spread\":[np.sum, np.mean, np.std],\n    }\n    \n    # データ数が多いのでtime_idごとに集計する関数を作成\n    def get_stats_window(seconds_in_bucket, add_suffix = False):\n        # 関数で引数に設定したsconds_in_bucketより後のデータだけをtime_idでgroup byした状態で取得\n        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(create_feature_dict).reset_index()\n        # ケツにカラム名を追加\n        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n        # ケツにそれぞれの「_seconds_in_bucket名」とカラムに追加\n        if add_suffix:\n            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n        return df_feature\n    \n    # 上の関数を用いてtime_idごとに集計したテーブルをさらにseconds_in_bucketで区切ったDataframeを作成\n    df_feature = get_stats_window(seconds_in_bucket = 0, add_suffix = False)\n    # add_suffix = Trueとするとカラム名の最後に「_450」って付く\n    df_feature_450 = get_stats_window(seconds_in_bucket = 450, add_suffix = True)\n    df_feature_300 = get_stats_window(seconds_in_bucket = 300, add_suffix = True)\n    df_feature_150 = get_stats_window(seconds_in_bucket = 150, add_suffix = True)\n    # 参考にしたnotebookには100秒ごとに同じ作業をした跡が残っていた。150秒ごとの方がいいと判断したのか\n    # df_feature_500 = get_stats_window(seconds_in_bucket = 500, add_suffix = True)\n    # df_feature_400 = get_stats_window(seconds_in_bucket = 400, add_suffix = True)\n    # df_feature_200 = get_stats_window(seconds_in_bucket = 200, add_suffix = True)\n\n    # 上で作成したDataFrameを一つに結合する\n    df_feature = df_feature.merge(df_feature_450, how = 'left', left_on = 'time_id_', right_on = 'time_id__450')\n    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n    df_feature = df_feature.merge(df_feature_150, how = 'left', left_on = 'time_id_', right_on = 'time_id__150')\n    # 結合で使用したid用のカラムを削除\n    df_feature.drop(['time_id__450', 'time_id__300', 'time_id__150'], axis = 1, inplace = True)\n    \n    # stock_idごとに別のフォルダにファイルが保存されているため、file_pathからstock_idを読み取った上で結合する\n    # file_pathが「./input/optiver-realized-volatility-prediction/book_train.parquet/stock_id=0/...」だったらstock_idを0と認識した上で結合\n    stock_id = file_path.split('=')[1]\n    df_feature['row_id'] = df_feature['time_id_'].apply(lambda x: f'{stock_id}-{x}')\n    # この時点で「time_id_0」などのvalueが入っている \n    df_feature.drop(['time_id_'], axis = 1, inplace = True)\n    return df_feature","metadata":{"execution":{"iopub.status.busy":"2021-09-21T10:08:38.478649Z","iopub.execute_input":"2021-09-21T10:08:38.479271Z","iopub.status.idle":"2021-09-21T10:08:38.49977Z","shell.execute_reply.started":"2021-09-21T10:08:38.479225Z","shell.execute_reply":"2021-09-21T10:08:38.498801Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# トレードデータをまとめて前処理\ndef trade_preprocessor(file_path):\n    df = pd.read_parquet(file_path)\n    # 実際に取引が行われた時の値段でlog_returnを計算した結果を「log_return」カラムに代入\n    df['log_return'] = df.groupby('time_id')['price'].apply(log_return)\n    \n    # 辞書型でカラム名と操作を用意\n    create_feature_dict = {\n        'log_return':[realized_volatility],\n        'seconds_in_bucket':[count_unique],\n        'size':[np.sum, realized_volatility, np.mean, np.std, np.max, np.min],\n        'order_count':[np.mean,np.sum,np.max],\n    }\n    \n    # データ数が多いのでtime_idごとに集計する関数を作成\n    def get_stats_window(seconds_in_bucket, add_suffix = False):\n        # 関数で引数に設定したsconds_in_bucketより後のデータだけをtime_idでgroup byした状態で取得\n        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(create_feature_dict).reset_index()\n        # ケツにカラム名を追加\n        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n        # ケツにそれぞれの「_seconds_in_bucket名」とカラムに追加\n        if add_suffix:\n            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n        return df_feature\n    \n\n    # 上の関数を用いてtime_idごとに集計したテーブルをさらにseconds_in_bucketで区切ったDataframeを作成\n    df_feature = get_stats_window(seconds_in_bucket = 0, add_suffix = False)\n    df_feature_450 = get_stats_window(seconds_in_bucket = 450, add_suffix = True)\n    df_feature_300 = get_stats_window(seconds_in_bucket = 300, add_suffix = True)\n    df_feature_150 = get_stats_window(seconds_in_bucket = 150, add_suffix = True)\n    # オーダーブック同様100秒ごとより150秒ごとの方が結果が良かったらしい\n    # df_feature_500 = get_stats_window(seconds_in_bucket = 500, add_suffix = True)\n    # df_feature_400 = get_stats_window(seconds_in_bucket = 400, add_suffix = True)\n    # df_feature_200 = get_stats_window(seconds_in_bucket = 200, add_suffix = True)\n    \n    # scoreに影響しているらしい\n    \n    # time_idごとにpriceがどれくらい変動してsizeがどれくらい行われたか\n    def tendency(price, vol):    \n        df_diff = np.diff(price)\n        val = (df_diff/price[1:])*100\n        power = np.sum(val*vol[1:])\n        return(power)\n    \n    # time_idごとに最大値や最小値を計算した配列を用意\n    # このやり方だとリークしている\n    lis = []\n    \n    for n_time_id in df['time_id'].unique():\n        df_id = df[df['time_id'] == n_time_id]\n        \n        # 上で作ったpowerを計算。time_idごとにpriceがどれくらい変動したか、取引がどれくらい行われたか\n        tendencyV = tendency(df_id['price'].values, df_id['size'].values)      \n        \n        # 平均以上と以下でpriceの合計値を取得\n        # 意図は分からないがとりあえず残す\n        f_max = np.sum(df_id['price'].values > np.mean(df_id['price'].values))\n        f_min = np.sum(df_id['price'].values < np.mean(df_id['price'].values))\n        \n        # 正か負かで分けた前後priceの差分の合計値\n        # こちらも意図は分からず\n        df_max =  np.sum(np.diff(df_id['price'].values) > 0)\n        df_min =  np.sum(np.diff(df_id['price'].values) < 0)\n        \n        ## ここからはpriceの話\n        # 偏差の中央値\n        abs_diff = np.median(np.abs(df_id['price'].values - np.mean(df_id['price'].values)))  \n        # 価格の二乗の平均値\n        energy = np.mean(df_id['price'].values**2)\n        # 第3四分位-第１四分位\n        iqr_p = np.percentile(df_id['price'].values,75) - np.percentile(df_id['price'].values,25)\n        \n        ## ここからはsizeの話\n        # それぞれ偏差の中央値、価格の二乗の平均値、第3四分位-第１四分位を計算\n        abs_diff_v = np.median(np.abs(df_id['size'].values - np.mean(df_id['size'].values)))        \n        energy_v = np.sum(df_id['size'].values**2)\n        iqr_p_v = np.percentile(df_id['size'].values,75) - np.percentile(df_id['size'].values,25)\n        \n        # まとめてlisにappend\n        lis.append({'time_id':n_time_id,'tendency':tendencyV,'f_max':f_max,'f_min':f_min,'df_max':df_max,'df_min':df_min,\n                   'abs_diff':abs_diff,'energy':energy,'iqr_p':iqr_p,'abs_diff_v':abs_diff_v,'energy_v':energy_v,'iqr_p_v':iqr_p_v})\n\n    # lisをDataFrameに変換  \n    df_lr = pd.DataFrame(lis)\n   \n    # dif_lrを結合\n    df_feature = df_feature.merge(df_lr, how = 'left', left_on = 'time_id_', right_on = 'time_id')\n    \n    # sconds_in_bucketで分けたDataFrameも全て結合\n    df_feature = df_feature.merge(df_feature_450, how = 'left', left_on = 'time_id_', right_on = 'time_id__450')\n    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n    df_feature = df_feature.merge(df_feature_150, how = 'left', left_on = 'time_id_', right_on = 'time_id__150')\n    # df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n    # df_feature = df_feature.merge(df_feature_100, how = 'left', left_on = 'time_id_', right_on = 'time_id__100')\n    \n    # 結合用に使ったカラムを削除\n    df_feature.drop(['time_id__450', 'time_id__300', 'time_id__150','time_id'], axis = 1, inplace = True)\n    \n    # オーダーブックと見分けをつけるためにカラムの先頭に「trade_」をつける\n    df_feature = df_feature.add_prefix('trade_')\n    stock_id = file_path.split('=')[1]\n    df_feature['row_id'] = df_feature['trade_time_id_'].apply(lambda x:f'{stock_id}-{x}')\n    df_feature.drop(['trade_time_id_'], axis = 1, inplace = True)\n    return df_feature","metadata":{"execution":{"iopub.status.busy":"2021-09-21T10:08:38.695227Z","iopub.execute_input":"2021-09-21T10:08:38.695571Z","iopub.status.idle":"2021-09-21T10:08:38.713731Z","shell.execute_reply.started":"2021-09-21T10:08:38.695541Z","shell.execute_reply":"2021-09-21T10:08:38.712945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# rmspeを計算するための関数\ndef rmspe(y_true, y_pred):\n    return np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\n\n# rmspeの値がぶれて来たら停止するための関数\ndef feval_rmspe(y_pred, lgb_train):\n    y_true = lgb_train.get_label()\n    return 'RMSPE', rmspe(y_true, y_pred), False","metadata":{"execution":{"iopub.status.busy":"2021-09-21T10:08:38.714993Z","iopub.execute_input":"2021-09-21T10:08:38.715408Z","iopub.status.idle":"2021-09-21T10:08:38.727554Z","shell.execute_reply.started":"2021-09-21T10:08:38.715372Z","shell.execute_reply":"2021-09-21T10:08:38.72673Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# stock_idとtime_idごとにデータをまとめるための関数\ndef get_time_stock(df):\n    # ボラティリティ周りにカラムを配列に用意\n    vol_cols = ['log_return1_realized_volatility', 'log_return2_realized_volatility', 'log_return1_realized_volatility_450', 'log_return2_realized_volatility_450', \n                'log_return1_realized_volatility_300', 'log_return2_realized_volatility_300', 'log_return1_realized_volatility_150', 'log_return2_realized_volatility_150', \n                'trade_log_return_realized_volatility', 'trade_log_return_realized_volatility_450', 'trade_log_return_realized_volatility_300', 'trade_log_return_realized_volatility_150']\n\n    # stock_idごとにまとめる\n    df_stock_id = df.groupby(['stock_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n    # stock_idごとにまとめた時に計算したmeanなどとわかるようにケツに「_stock」とつける\n    df_stock_id.columns = ['_'.join(col) for col in df_stock_id.columns]\n    df_stock_id = df_stock_id.add_suffix('_' + 'stock')\n\n    # time_idごとにまとめる（作業はstock_idの時と一緒）\n    df_time_id = df.groupby(['time_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n    df_time_id.columns = ['_'.join(col) for col in df_time_id.columns]\n    df_time_id = df_time_id.add_suffix('_' + 'time')\n    \n    # 元のDataFrameに今作った2つを結合\n    df = df.merge(df_stock_id, how = 'left', left_on = ['stock_id'], right_on = ['stock_id__stock'])\n    df = df.merge(df_time_id, how = 'left', left_on = ['time_id'], right_on = ['time_id__time'])\n    df.drop(['stock_id__stock', 'time_id__time'], axis = 1, inplace = True)\n    return df\n    \n# それぞれの前処理を並列して行えるようにするための関数\ndef preprocessor(list_stock_ids, is_train = True):\n    \n    # Parrallel for loop\n    def for_joblib(stock_id):\n        # is_train=Trueと指定された場合trainデータの方へのpathを作成\n        if is_train:\n            file_path_book = data_dir + \"book_train.parquet/stock_id=\" + str(stock_id)\n            file_path_trade = data_dir + \"trade_train.parquet/stock_id=\" + str(stock_id)\n        # is_train=Falseつまりtestデータならこちらへのpathを作成\n        else:\n            file_path_book = data_dir + \"book_test.parquet/stock_id=\" + str(stock_id)\n            file_path_trade = data_dir + \"trade_test.parquet/stock_id=\" + str(stock_id)\n    \n        # bookとtradeを前処理した上で結合する\n        # 些細な変化も全てレコードとして記録されているbookの方にrow_idを基準にtradeを外部結合する\n        df_tmp = pd.merge(book_preprocessor(file_path_book), trade_preprocessor(file_path_trade), on = 'row_id', how = 'left')\n        \n        # 結合したDataFrameを返す\n        return df_tmp\n    \n    # 並列APIを使用してfor_joblib関数を呼び出す\n    # n_jobs:最大同時実行ジョブ数。-1とすると全てのCPUが使用される\n    # verbose:ログの出力レベル（冗長性）。デフォルトでは何も出力されない。値を大きくすると出力レベルが上がる（冗長性が増す）。10より大きいとすべてのログが出力され、50以上だとstdout（標準出力）に出力される。\n    # delayed(<実行する関数>)(<関数への引数>) for 変数名 in イテラブル\n    # 実行する関数（bookとtradeを前処理して結合）をstock_idごとに行う\n    df = Parallel(n_jobs = -1, verbose = 1)(delayed(for_joblib)(stock_id) for stock_id in list_stock_ids)\n    # Parallelから返されるすべてのDataFrameを結合\n    # ignore_index=Trueでindexがconcat前のindexを無視して連番で振られる\n    df = pd.concat(df, ignore_index = True)\n    return df\n\n# RMSPE(平均平方二乗誤差率)\ndef rmspe(y_true, y_pred):\n    return np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\n\n# RMSPEで早期停止する関数\ndef feval_rmspe(y_pred, lgb_train):\n    y_true = lgb_train.get_label()\n    return 'RMSPE', rmspe(y_true, y_pred), False","metadata":{"execution":{"iopub.status.busy":"2021-09-21T10:08:38.729268Z","iopub.execute_input":"2021-09-21T10:08:38.729695Z","iopub.status.idle":"2021-09-21T10:08:38.752973Z","shell.execute_reply.started":"2021-09-21T10:08:38.729655Z","shell.execute_reply":"2021-09-21T10:08:38.75194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# trainデータとtestデータの読み込み\ntrain, test = read_train_test()\n\n# ユニークなstockのidを取得する\ntrain_stock_ids = train['stock_id'].unique()\n# 並列処理に使うために前処理を行う\n# 今回は並列化してbookとtradeをstock_idごとに処理をした上で結合する\ntrain_ = preprocessor(train_stock_ids, is_train = True)\n# row_idを基準にleft joinする\ntrain = train.merge(train_, on = ['row_id'], how = 'left')\n\n# テストデータに対しても同じ処理を行う\ntest_stock_ids = test['stock_id'].unique()\ntest_ = preprocessor(test_stock_ids, is_train = False)\ntest = test.merge(test_, on = ['row_id'], how = 'left')\n\n# time_idとstock_idが一緒になったデータを取得\n# get_time_stockはstock_idとtime_idごとの集計を行う関数\ntrain = get_time_stock(train)\ntest = get_time_stock(test)","metadata":{"execution":{"iopub.status.busy":"2021-09-21T10:08:38.754606Z","iopub.execute_input":"2021-09-21T10:08:38.755203Z","iopub.status.idle":"2021-09-21T10:34:56.871321Z","shell.execute_reply.started":"2021-09-21T10:08:38.755164Z","shell.execute_reply":"2021-09-21T10:34:56.870272Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(train.head())\ndisplay(train.shape)\ndisplay(test.head())\ndisplay(test.shape)","metadata":{"execution":{"iopub.status.busy":"2021-09-21T10:34:56.87426Z","iopub.execute_input":"2021-09-21T10:34:56.874552Z","iopub.status.idle":"2021-09-21T10:34:57.356148Z","shell.execute_reply.started":"2021-09-21T10:34:56.874524Z","shell.execute_reply":"2021-09-21T10:34:57.355182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# paramsを設定\nseed = 42\nparams = {\n    'learning_rate': 0.1,        \n    'lambda_l1': 2,\n    'lambda_l2': 7,\n    'num_leaves': 800,\n    'min_sum_hessian_in_leaf': 20,\n    'feature_fraction': 0.8,\n    'feature_fraction_bynode': 0.8,\n    'bagging_fraction': 0.9,\n    'bagging_freq': 42,\n    'min_data_in_leaf': 700,\n    'max_depth': 4,\n    'seed': seed,\n    'feature_fraction_seed': seed,\n    'bagging_seed': seed,\n    'drop_seed': seed,\n    'data_random_seed': seed,\n    'objective': 'rmse',\n    'boosting': 'gbdt',\n    'verbosity': -1,\n    'n_jobs': -1,\n} ","metadata":{"execution":{"iopub.status.busy":"2021-09-21T10:34:57.357579Z","iopub.execute_input":"2021-09-21T10:34:57.358025Z","iopub.status.idle":"2021-09-21T10:34:57.364309Z","shell.execute_reply.started":"2021-09-21T10:34:57.357981Z","shell.execute_reply":"2021-09-21T10:34:57.363336Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 重要度解析\n# 特徴量の重要度（feature_importance）の計算方法は以下の2つ\n# - 頻度: モデルでその特徴量が使用された回数（初期値）「importance_type = 'split'」\n# - ゲイン: その特徴量が使用する分岐からの目的関数の減少（こちらがおすすめらしい）「importance_type = 'gain'」\n# 今回はgainを選択。目的関数の減少に関わっている特徴量 → つまり重要な特徴量が降順でわかる\ndef calc_model_importance(model, feature_names=None, importance_type='gain'):\n    importance_df = pd.DataFrame(model.feature_importance(importance_type=importance_type),\n                                 index=feature_names,\n                                 columns=['importance']).sort_values('importance')\n    return importance_df\n\ndef calc_mean_importance(importance_df_list):\n    mean_importance = np.mean(\n        np.array([df['importance'].values for df in importance_df_list]), axis=0)\n    mean_df = importance_df_list[0].copy()\n    mean_df['importance'] = mean_importance\n    \n    return mean_df","metadata":{"execution":{"iopub.status.busy":"2021-09-21T10:34:57.365859Z","iopub.execute_input":"2021-09-21T10:34:57.366255Z","iopub.status.idle":"2021-09-21T10:34:57.388229Z","shell.execute_reply.started":"2021-09-21T10:34:57.366214Z","shell.execute_reply":"2021-09-21T10:34:57.387336Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## stock_idの統計量をKfoldの後に取得する","metadata":{}},{"cell_type":"code","source":"display(train.shape)\ndisplay(test.shape)\nprint('='*40 + 'stock_id削除後' + '='*40)\n# 「_stock」とは入っているカラムは全て削除\ntrain_drop_stock = train.drop(train.filter(like='_stock', axis=1).columns.to_list(), axis=1)\ndisplay(train_drop_stock.shape)","metadata":{"execution":{"iopub.status.busy":"2021-09-21T10:34:57.389514Z","iopub.execute_input":"2021-09-21T10:34:57.390058Z","iopub.status.idle":"2021-09-21T10:34:57.672657Z","shell.execute_reply.started":"2021-09-21T10:34:57.390013Z","shell.execute_reply":"2021-09-21T10:34:57.671576Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 特徴量とターゲットを分割\nx = train_drop_stock.drop(['row_id', 'target', 'time_id'], axis = 1)\ny = train_drop_stock['target']\nx_test = test.drop(['row_id', 'time_id'], axis = 1)\n\n# 結合用でobject型で入っていたstock_idを数値型に変換\nx['stock_id'] = x['stock_id'].astype(int)\nx_test['stock_id'] = x_test['stock_id'].astype(int)\n\n# xの行数分0が入った配列を用意\noof_predictions = np.zeros(x.shape[0])\ndisplay(oof_predictions)\ndisplay(oof_predictions.shape)\n# テストデータに対しても行う\ntest_predictions = np.zeros(x_test.shape[0])\ndisplay(test_predictions)\ndisplay(test_predictions.shape)","metadata":{"execution":{"iopub.status.busy":"2021-09-21T10:34:57.676378Z","iopub.execute_input":"2021-09-21T10:34:57.676682Z","iopub.status.idle":"2021-09-21T10:34:57.91166Z","shell.execute_reply.started":"2021-09-21T10:34:57.676653Z","shell.execute_reply":"2021-09-21T10:34:57.910709Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# stock_idごとにまとめる関数をもう一度作成\ndef get_time_stock_2(df):\n    # Get realized volatility columns\n    vol_cols = ['log_return1_realized_volatility', 'log_return2_realized_volatility', 'log_return1_realized_volatility_450', 'log_return2_realized_volatility_450', \n                'log_return1_realized_volatility_300', 'log_return2_realized_volatility_300', 'log_return1_realized_volatility_150', 'log_return2_realized_volatility_150', \n                'trade_log_return_realized_volatility', 'trade_log_return_realized_volatility_450', 'trade_log_return_realized_volatility_300', 'trade_log_return_realized_volatility_150']\n\n    # stock_idごとに集計\n    df_stock_id = df.groupby(['stock_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n    df_stock_id.columns = ['_'.join(col) for col in df_stock_id.columns]\n    df_stock_id = df_stock_id.add_suffix('_' + 'stock')\n    \n    # 元々のデータに今作成したDataFrameを作成\n    df = df.merge(df_stock_id, how = 'left', left_on = ['stock_id'], right_on = ['stock_id__stock'])\n    df.drop(['stock_id__stock'], axis = 1, inplace = True)\n    return df","metadata":{"execution":{"iopub.status.busy":"2021-09-21T10:34:57.913179Z","iopub.execute_input":"2021-09-21T10:34:57.913451Z","iopub.status.idle":"2021-09-21T10:34:57.921186Z","shell.execute_reply.started":"2021-09-21T10:34:57.913425Z","shell.execute_reply":"2021-09-21T10:34:57.920066Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 実行前の確認\nx.shape","metadata":{"execution":{"iopub.status.busy":"2021-09-21T10:34:57.922386Z","iopub.execute_input":"2021-09-21T10:34:57.922724Z","iopub.status.idle":"2021-09-21T10:34:57.936406Z","shell.execute_reply.started":"2021-09-21T10:34:57.922695Z","shell.execute_reply":"2021-09-21T10:34:57.934958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 重要度解析をgainで取得したverとsplitで取得したverに分けるためにそれぞれ配列を用意\ngain_importance_list = []\nsplit_importance_list = []\n\n# sklearnのGroupKFoldをimport\nfrom sklearn.model_selection import GroupKFold\n# ここはtime_id？\ngroup = train['stock_id']\n# trainデータを5つのfoldに分割\nkf = GroupKFold(n_splits=5)\n# 各foldで繰り返す\nfor fold, (trn_ind, val_ind) in enumerate(kf.split(x, groups=group)):\n    print(f'Training fold {fold + 1}')\n    x_train, x_val = x.iloc[trn_ind], x.iloc[val_ind]\n    y_train, y_val = y.iloc[trn_ind], y.iloc[val_ind]\n    \n    # 各foldの時のデータでstock_idごとに集計する関数を適用\n    x_train = get_time_stock_2(x_train)\n    x_val = get_time_stock_2(x_val)\n    \n    # RMSPE\n    train_weights = 1 / np.square(y_train)\n    val_weights = 1 / np.square(y_val)\n    train_dataset = lgb.Dataset(x_train, y_train, weight = train_weights)\n    val_dataset = lgb.Dataset(x_val, y_val, weight = val_weights)\n    model = lgb.train(params = params, \n                      train_set = train_dataset, \n                      categorical_feature = ['stock_id'],\n                      valid_sets = [train_dataset, val_dataset], \n                      num_boost_round = 5000, \n                      early_stopping_rounds = 30, \n                      verbose_eval = 100,\n                      feval = feval_rmspe)\n\n    # 各foldごとにlightgbmを実装して予測値を取得\n    # のちのrmspeの計算で必要\n    oof_predictions[val_ind] = model.predict(x_val)\n    \n    # 分けた5つのfoldごとの予測値の平均値を取得\n    test_predictions += model.predict(x_test) / 5\n\n    feature_names = x_train.columns.values.tolist()\n    gain_importance_df = calc_model_importance(\n        model, feature_names=feature_names, importance_type='gain')\n    gain_importance_list.append(gain_importance_df)\n\n    split_importance_df = calc_model_importance(\n        model, feature_names=feature_names, importance_type='split')\n    split_importance_list.append(split_importance_df)\n    \n    x_train.drop(train.filter(like='_stock', axis=1).columns.to_list(), axis=1, inplace=True)\n    x_val.drop(train.filter(like='_stock', axis=1).columns.to_list(), axis=1, inplace=True)\n\nrmspe_score = rmspe(y, oof_predictions)\nprint(f'Our out of folds RMSPE is {rmspe_score}')","metadata":{"execution":{"iopub.status.busy":"2021-09-21T10:34:57.937893Z","iopub.execute_input":"2021-09-21T10:34:57.938168Z","iopub.status.idle":"2021-09-21T10:48:27.105627Z","shell.execute_reply.started":"2021-09-21T10:34:57.938142Z","shell.execute_reply":"2021-09-21T10:48:27.104651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 重要度解析を出力\nmean_gain_df = calc_mean_importance(gain_importance_list)\nmean_gain_df = mean_gain_df.reset_index().rename(columns={'index': 'feature_names'})\n# csvにして全部見たい場合は下記\n# mean_gain_df.to_csv('gain_importance_mean groupkfold stock_id after Kflod.csv', index=False)\nmean_gain_df.head(20).sort_values(by='importance', ascending=False)","metadata":{"execution":{"iopub.status.busy":"2021-09-21T10:48:27.107285Z","iopub.execute_input":"2021-09-21T10:48:27.107897Z","iopub.status.idle":"2021-09-21T10:48:27.125283Z","shell.execute_reply.started":"2021-09-21T10:48:27.107851Z","shell.execute_reply":"2021-09-21T10:48:27.124518Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test['target'] = test_predictions\ntest[['row_id', 'target']].to_csv('submission.csv',index = False)","metadata":{"execution":{"iopub.status.busy":"2021-09-21T10:48:27.126305Z","iopub.execute_input":"2021-09-21T10:48:27.126691Z","iopub.status.idle":"2021-09-21T10:48:27.144258Z","shell.execute_reply.started":"2021-09-21T10:48:27.126662Z","shell.execute_reply":"2021-09-21T10:48:27.143406Z"},"trusted":true},"execution_count":null,"outputs":[]}]}