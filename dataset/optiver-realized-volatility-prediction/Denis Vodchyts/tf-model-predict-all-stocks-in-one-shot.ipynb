{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-24T14:59:32.390213Z","iopub.execute_input":"2021-08-24T14:59:32.39067Z","iopub.status.idle":"2021-08-24T14:59:32.84073Z","shell.execute_reply.started":"2021-08-24T14:59:32.390573Z","shell.execute_reply":"2021-08-24T14:59:32.839844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#import modules\nfrom sklearn.metrics import r2_score\nimport pandas as pd\nimport numpy as np\nimport plotly.express as px\n\nimport os\nimport glob\nimport shap\n\nfrom multiprocessing import Pool\n\nfrom joblib import Parallel, delayed\n\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\n\nimport tensorflow as tf\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.layers.experimental import preprocessing\nfrom tensorflow import feature_column\n\nimport time\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\npath_to_files = \"../input/optiver-realized-volatility-prediction\"\n\nbook_train_files =  path_to_files + '/book_train.parquet/stock_id={}'\ntrade_train_files =  path_to_files + '/trade_train.parquet/stock_id={}'\n\nbook_test_files =  path_to_files + '/book_test.parquet/stock_id={}'\ntrade_test_files =  path_to_files + '/trade_test.parquet/stock_id={}'\n\n#SMALL_F = 0.0000000000000001\nSMALL_F = 0.00000001\n\ntf.random.set_seed(111)\nnp.random.seed(111)","metadata":{"execution":{"iopub.status.busy":"2021-08-24T14:59:32.844042Z","iopub.execute_input":"2021-08-24T14:59:32.844378Z","iopub.status.idle":"2021-08-24T14:59:43.701303Z","shell.execute_reply.started":"2021-08-24T14:59:32.844347Z","shell.execute_reply":"2021-08-24T14:59:43.700232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Configuration\ncfg = dict(\n    isCollectDataOnly = True,\n    isStockIdUsed = False,\n    isTFModelUsed = False,\n    trainNotUsedCols = ['row_id', 'target', 'time_id', 'stock_id'],\n    predictNotUsedCols = ['row_id', 'time_id', 'stock_id'],\n    useHyperOpt = False,\n    useLabelTransformation = False,\n    volumeBarThreshold = 1000.0\n)\n\n\ncfg","metadata":{"execution":{"iopub.status.busy":"2021-08-24T14:59:43.702875Z","iopub.execute_input":"2021-08-24T14:59:43.703137Z","iopub.status.idle":"2021-08-24T14:59:43.711942Z","shell.execute_reply.started":"2021-08-24T14:59:43.703112Z","shell.execute_reply":"2021-08-24T14:59:43.710979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def log_return(series):\n    return np.log(series).diff()\n\ndef realized_volatility(series_log_return):\n    return np.sqrt(np.sum(series_log_return**2))\n\ndef rmspe(y_true, y_pred):\n    return  (np.sqrt(np.mean(np.square((y_true - y_pred) / y_true))))\n","metadata":{"execution":{"iopub.status.busy":"2021-08-24T14:59:43.713197Z","iopub.execute_input":"2021-08-24T14:59:43.713605Z","iopub.status.idle":"2021-08-24T14:59:43.733198Z","shell.execute_reply.started":"2021-08-24T14:59:43.713576Z","shell.execute_reply":"2021-08-24T14:59:43.732358Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def getDataFromBidAsk_numpy(df, ci):\n    a = 0\n    b = 0\n    spread  = {}\n    for k in [1,2]:\n        #k = i+1\n        bidp = 'bid_price{}'.format(k)\n        askp = 'ask_price{}'.format(k)\n        bids = 'bid_size{}'.format(k)\n        asks = 'ask_size{}'.format(k)\n        #calculate comulative wap\n        a += (df[:,ci[bidp]] * df[:,ci[asks]] + df[:,ci[askp]] * df[:,ci[bids]])\n        b += df[:,ci[bids]] + df[:,ci[asks]]\n\n        #wap 1 and 2\n        spread[f'fb_w_{k}'] = (df[:,ci[bidp]] * df[:,ci[asks]] + df[:,ci[askp]] * df[:,ci[bids]] ) / (df[:,ci[bids]] + df[:,ci[asks]] + SMALL_F)\n        spread[f'fb_mid_point_{k}'] = (df[:,ci[askp]]) + (df[:,ci[bidp]]) / 2\n    \n    # mean wap\n    spread['fb_w'] = (a/(b+SMALL_F))\n    # rates\n    spread['fb_w_rate'] = (spread['fb_w_1']) / (spread['fb_w_2']+SMALL_F) \n    #sum volume\n   \n    return spread\n\ndef Fx(group, stock_id=0, n=10):\n    new_df = pd.DataFrame()\n    name = int(group.time_id.unique()[0])\n    tmp = pd.DataFrame()\n\n    #calculate log return from the following features:\n    cols = [\n        'fb_w', \n        'fb_w_1', \n        'fb_w_2',\n        'fb_mid_point_1',\n        'fb_w_rate',\n    ]\n\n    new_cols = [s + '_lr' for s in cols]\n    group.loc[:,new_cols] = log_return(group[cols]).to_numpy()\n    group = group[~group['fb_w'].isnull()]\n\n    #calculate realized volatility\n    cols = new_cols\n    new_cols = [s + '_vola' for s in cols]\n    tmp = pd.concat([tmp, pd.DataFrame(realized_volatility(group.loc[:,cols]).to_numpy().reshape(1,-1), columns=new_cols)], axis=1)\n    \n    tmp.loc[:,'row_id'] = str(stock_id) + '-' + str(name)\n    tmp.loc[:,'time_id'] = int(name)\n    return tmp\n\ndef getFeaturesFromBookData(df, stock_id, n=10):\n    results = df.groupby(['time_id']).apply(Fx, stock_id=stock_id, n=n).reset_index(drop=True)\n    return results\n","metadata":{"execution":{"iopub.status.busy":"2021-08-24T14:59:43.734606Z","iopub.execute_input":"2021-08-24T14:59:43.73521Z","iopub.status.idle":"2021-08-24T14:59:43.75344Z","shell.execute_reply.started":"2021-08-24T14:59:43.735162Z","shell.execute_reply":"2021-08-24T14:59:43.75247Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def getDataFromTrade(df):\n    log_ret = log_return(df.price).dropna()\n    rz_vol = realized_volatility(log_ret)\n    \n    tmp = pd.DataFrame()\n    tmp.loc[:,'p_rz_vol'] = [rz_vol]\n\n    time_id = df.time_id.unique()[0]\n    tmp.loc[:,'time_id'] = time_id\n    return tmp\n\ndef getFeaturesFromTradeData(df):\n    return df.groupby(['time_id']).apply(getDataFromTrade).reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2021-08-24T14:59:43.754837Z","iopub.execute_input":"2021-08-24T14:59:43.755314Z","iopub.status.idle":"2021-08-24T14:59:43.770059Z","shell.execute_reply.started":"2021-08-24T14:59:43.75526Z","shell.execute_reply":"2021-08-24T14:59:43.76891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def constructPreprocessedDataFrame(file_path, isTrain):\n    stock_id = file_path.split('=')[1]\n    df_book_data = pd.read_parquet(file_path)\n    if True == isTrain:\n        df_trade_data =  pd.read_parquet(trade_train_files.format(stock_id))\n    else:\n        df_trade_data =  pd.read_parquet(trade_test_files.format(stock_id))\n\n    print('Processing stock id:', stock_id)\n    #display(df_book_data.time_id.unique())\n    #preprocess book\n    a = time.time()\n    spread = getDataFromBidAsk_numpy(df_book_data.to_numpy(),{k: v for v, k in enumerate(df_book_data.columns.values)})\n    df_book_data = pd.concat([df_book_data,pd.DataFrame(spread)], axis=1)\n    df_book_datar = getFeaturesFromBookData(df_book_data, stock_id, 10)\n    b = time.time()\n    #print(f'preprocess book: {b-a}')\n    \n    #preprocess trade\n    df_trade_datar = getFeaturesFromTradeData(df_trade_data)\n    df_book_datar = df_book_datar.merge(df_trade_datar, on = ['time_id'], how = 'left')\n    c = time.time()\n    #print(f'preprocess trade: {c-b}')\n\n    df_book_datar.loc[:,'stock_id'] = stock_id\n    df_book_datar = df_book_datar.fillna(0.0)\n    return df_book_datar\n\ndef constructBookDataDataFrame(list_file, isTrain=True):\n    df_book = pd.DataFrame()\n    for file in list_file:\n        df_book = pd.concat([df_book, constructPreprocessedDataFrame(file, isTrain=isTrain)])\n    return df_book\n\n\ndef preprocessor(list_file, isTrain = True):\n    # Use parallel api to call paralle for loop\n    df = Parallel(n_jobs = -1, verbose = 1)(delayed(constructPreprocessedDataFrame)(stock_file, isTrain) for stock_file in list_file)\n    # Concatenate all the dataframes that return from Parallel\n    df = pd.concat(df, ignore_index = True)\n    return df\n","metadata":{"execution":{"iopub.status.busy":"2021-08-24T14:59:43.771581Z","iopub.execute_input":"2021-08-24T14:59:43.772269Z","iopub.status.idle":"2021-08-24T14:59:43.786858Z","shell.execute_reply.started":"2021-08-24T14:59:43.772224Z","shell.execute_reply":"2021-08-24T14:59:43.786181Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"list_order_book_file_train = glob.glob(path_to_files + '/book_train.parquet/*')\nlist_order_book_file_train[0:1]","metadata":{"execution":{"iopub.status.busy":"2021-08-24T14:59:43.788804Z","iopub.execute_input":"2021-08-24T14:59:43.789214Z","iopub.status.idle":"2021-08-24T14:59:43.80403Z","shell.execute_reply.started":"2021-08-24T14:59:43.789171Z","shell.execute_reply":"2021-08-24T14:59:43.803136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nret_df = preprocessor(list_order_book_file_train)\ndisplay(ret_df.shape)\nret_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-24T14:59:43.805601Z","iopub.execute_input":"2021-08-24T14:59:43.805884Z","iopub.status.idle":"2021-08-24T15:34:22.952519Z","shell.execute_reply.started":"2021-08-24T14:59:43.805859Z","shell.execute_reply":"2021-08-24T15:34:22.951577Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ret_df.to_csv('111.csv')","metadata":{"execution":{"iopub.status.busy":"2021-08-24T15:34:22.954069Z","iopub.execute_input":"2021-08-24T15:34:22.954546Z","iopub.status.idle":"2021-08-24T15:34:30.027525Z","shell.execute_reply.started":"2021-08-24T15:34:22.954511Z","shell.execute_reply":"2021-08-24T15:34:30.026631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ret_df = pd.read_csv('111.csv', index_col=0)","metadata":{"execution":{"iopub.status.busy":"2021-08-24T15:34:30.028963Z","iopub.execute_input":"2021-08-24T15:34:30.029593Z","iopub.status.idle":"2021-08-24T15:34:31.039855Z","shell.execute_reply.started":"2021-08-24T15:34:30.029547Z","shell.execute_reply":"2021-08-24T15:34:31.03908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#important -> don't forget to sort by stock_id\nret_df = ret_df.sort_values(by='stock_id').reset_index(drop=True)\ncs = ['row_id', 'stock_id', 'time_id']\nused_cols = list(set(ret_df.columns.to_list()) - set(cs))\ny_col = 'target'\n\nALL_STOCKS = {k: v for v, k in enumerate(ret_df.stock_id.unique())}\nlen(ALL_STOCKS)","metadata":{"execution":{"iopub.status.busy":"2021-08-24T15:34:31.041176Z","iopub.execute_input":"2021-08-24T15:34:31.041765Z","iopub.status.idle":"2021-08-24T15:34:31.124911Z","shell.execute_reply.started":"2021-08-24T15:34:31.041721Z","shell.execute_reply":"2021-08-24T15:34:31.12421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"used_cols","metadata":{"execution":{"iopub.status.busy":"2021-08-24T15:34:31.125976Z","iopub.execute_input":"2021-08-24T15:34:31.12637Z","iopub.status.idle":"2021-08-24T15:34:31.131485Z","shell.execute_reply.started":"2021-08-24T15:34:31.12634Z","shell.execute_reply":"2021-08-24T15:34:31.130761Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scaler_target = MinMaxScaler()\ndef getTrainData(ret_df, seed = 42):\n    train = pd.read_csv(path_to_files + '/train.csv')\n    #convert stock_id to the same time as in train data\n    ret_df.stock_id = ret_df.stock_id.astype(int)\n    #merge\n    data_df = ret_df.merge(train, on = ['stock_id', 'time_id'], how = 'left')\n    data_df.loc[:,'target_orig'] = data_df.loc[:,'target'] \n\n    if True == cfg['useLabelTransformation']:\n        data_df.loc[:,'target'] = data_df.loc[:,'target'] * 100\n        scaler_target.fit(data_df.loc[:,'target'].to_numpy().reshape(-1,1))\n        data_df.loc[:,'target'] = scaler_target.transform(data_df.loc[:,'target'].to_numpy().reshape(-1,1)).flatten()\n\n    #get train test index \n    all_time_ids = data_df.time_id.unique()\n\n    train_ids, val_ids = train_test_split(all_time_ids, test_size=0.05, random_state=seed)\n    test_ids, val_ids = train_test_split(val_ids, test_size=0.5, random_state=seed)\n\n    f = data_df.time_id.isin(train_ids)\n    train_df = data_df.loc[f].reset_index(drop=True).copy()\n\n    f = data_df.time_id.isin(val_ids)\n    val_df = data_df.loc[f].reset_index(drop=True).copy()\n\n    f = data_df.time_id.isin(test_ids)\n    test_df = data_df.loc[f].reset_index(drop=True).copy()\n    \n    return train_df, val_df, test_df\n\ndef predictFromModel(model, df, used_cols=used_cols, prediction_column_name='target'):\n    predict = model.predict(df.loc[:, used_cols].values).flatten()\n    df_ret = pd.DataFrame()\n    df_ret[prediction_column_name] = predict\n    df_ret['row_id'] = df['row_id'].values\n    return df_ret[['row_id', prediction_column_name]].reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2021-08-24T15:34:31.132526Z","iopub.execute_input":"2021-08-24T15:34:31.132946Z","iopub.status.idle":"2021-08-24T15:34:31.147082Z","shell.execute_reply.started":"2021-08-24T15:34:31.132905Z","shell.execute_reply":"2021-08-24T15:34:31.146174Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import PowerTransformer\ndef firScalerAndNormalizer(train_df, used_cols=used_cols):\n    #scale data\n    scaler = MinMaxScaler()\n    #scaler = PowerTransformer()\n    scaler.fit(train_df.loc[:,used_cols].to_numpy())\n\n    #and normalize\n    normalizer = preprocessing.Normalization(axis=-1)\n    normalizer.adapt(np.array(train_df.loc[:,used_cols].to_numpy()))\n\n    return scaler, normalizer\n\n\n# Function to insert row in the dataframe\ndef Insert_row_(row_number, df, row_value):\n    # Slice the upper half of the dataframe\n    df1 = df[0:row_number]\n   \n    # Store the result of lower half of the dataframe\n    df2 = df[row_number:]\n   \n    # Inser the row in the upper half dataframe\n    df1 = df1.append(row_value, ignore_index=True)\n   \n    # Concat the two dataframes\n    df_result = pd.concat([df1, df2])\n   \n    # Return the updated dataframe\n    return df_result.reset_index(drop=True)\n\ndef get_one_input(X_df, time_id, used_cols, y_col=None):\n    f = X_df.time_id == time_id\n    y = None\n\n    if y_col is not None:\n        y = X_df.loc[f, y_col].to_numpy()\n    \n    X = X_df.loc[f].copy()\n    if(X.shape[0] < len(ALL_STOCKS.keys())):\n        #create new empty raw\n        new_row = np.zeros((X.shape[1]))\n        new_row = pd.DataFrame(np.zeros((1, X.shape[1])), columns = X.columns).astype(X.dtypes)\n        new_row.loc[:, 'stock_id'] = 255\n\n        missing_ids = list(set(ALL_STOCKS.keys()) - set(X.loc[:, 'stock_id'].unique()))\n        #print(missing_ids)\n        #print('Time_Id:', time_id)\n        for i in missing_ids:\n            X = Insert_row_(ALL_STOCKS[i], X, new_row)\n            if y_col is not None:\n                y = np.insert(y, ALL_STOCKS[i], .0)\n\n    return X.loc[:,used_cols].to_numpy(), y\n\ndef tf_data_generator(df, used_col, y_col=None):\n    while True:\n        tid = np.random.choice(df.time_id.unique())\n        #tf.print(tid)\n        X, y = get_one_input(df, tid, used_cols, y_col)\n        X = tf.convert_to_tensor(X, tf.float64, name='features')\n        y = tf.convert_to_tensor(y, tf.float64, name='labels')\n        yield X,y\n\n\ndef tf_data_generator2(df, used_col, y_col=None):\n    tids = np.array([])\n    while True:\n        if len(tids) == 0:\n            tids = df.time_id.unique()\n\n        tid =  np.random.choice(tids)\n        tids = np.setdiff1d(tids, [tid])\n\n        #tf.print(len(tids))\n        X, y = get_one_input(df, tid, used_cols, y_col)\n        \n        X = tf.convert_to_tensor(X, tf.float64, name='features')\n        y = tf.convert_to_tensor(y, tf.float64, name='labels')\n        \n        yield X,y\n\ndef getPredictionFromOneTimeId(model, X_df, time_id, used_cols):\n    X, _ = get_one_input(X_df, time_id, used_cols, None)\n    X = np.expand_dims(X, axis=0)\n    #print(X.shape)\n    predicted = model.predict(X).flatten()\n    \n    #get missing stock ids\n    f = time_id == X_df.time_id\n    if(X_df.loc[f].shape[0] < len(ALL_STOCKS.keys())):\n        missing_ids = list(set(ALL_STOCKS.keys()) - set(X_df.loc[f, 'stock_id'].unique()))\n        tl = [ALL_STOCKS[i] for i in missing_ids]\n        #print('tl:',tl)\n        #print('missing_ids:',missing_ids)\n        predicted = np.delete(predicted, tl)\n    return predicted\n\ndef getPredictionFromTheModel(model, X_df, used_cols):\n    df_res = pd.DataFrame()\n\n    for j in X_df.time_id.unique():    \n        predicted = getPredictionFromOneTimeId(model, X_df, j, used_cols)\n        f = j == X_df.time_id\n        targets = X_df.loc[f,'target_orig'].to_numpy()\n        row_ids = X_df.loc[f,'row_id'].to_numpy()\n        if len(predicted) != len(targets):\n            print(targets.shape)\n            print(predicted.shape)\n            print(row_ids.shape)\n        df_res = tmp = pd.concat([df_res, pd.DataFrame({'row_id':row_ids, 'predict':predicted, 'target':targets})])\n    \n    return df_res.reset_index(drop=True)    \n\ndef plotHistory(history):\n    # Plot history: MAE\n    plt.plot(history.history['loss'], label='Training loss')\n    plt.plot(history.history['val_loss'], label='Validation')\n    plt.title('History')\n    plt.ylabel('Loss value')\n    plt.xlabel('No. epoch')\n    plt.legend(loc=\"upper left\")\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-24T15:34:31.148461Z","iopub.execute_input":"2021-08-24T15:34:31.148934Z","iopub.status.idle":"2021-08-24T15:34:31.183635Z","shell.execute_reply.started":"2021-08-24T15:34:31.148895Z","shell.execute_reply":"2021-08-24T15:34:31.182384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#split data\ntrain_df, val_df, test_df = getTrainData(ret_df, seed = 42)\n#fit normalizer and scaler\nscaler, normalizer = firScalerAndNormalizer(train_df, used_cols=used_cols)\n\n#scale data\ntrain_df.loc[:,used_cols] = scaler.transform(train_df.loc[:,used_cols])\nval_df.loc[:,used_cols] = scaler.transform(val_df.loc[:,used_cols])\ntest_df.loc[:,used_cols] = scaler.transform(test_df.loc[:,used_cols])\n\ndisplay(train_df.loc[:,used_cols].head(1))\nprint('train_shape', train_df.loc[:,used_cols].shape)\nprint('train elements:',len(train_df.time_id.unique()))\nprint('test elements:',len(test_df.time_id.unique()))\nprint('val elements:',len(val_df.time_id.unique()))\n\n#create tf datasets\nn_feat = len(used_cols)\ntrain_ds = tf.data.Dataset.from_generator(\n        lambda: tf_data_generator2(train_df, used_cols, y_col),\n        output_signature=(\n            tf.TensorSpec(shape=(None,n_feat), dtype=tf.float64),\n            tf.TensorSpec(shape=(None,), dtype=tf.float64)))\n\nval_ds = tf.data.Dataset.from_generator(\n        lambda: tf_data_generator2(val_df, used_cols, y_col),\n        output_signature=(\n            tf.TensorSpec(shape=(None,n_feat), dtype=tf.float64),\n            tf.TensorSpec(shape=(None,), dtype=tf.float64)))\n\ntest_ds = tf.data.Dataset.from_generator(\n        lambda: tf_data_generator2(test_df, used_cols, y_col),\n        output_signature=(\n            tf.TensorSpec(shape=(None,n_feat), dtype=tf.float64),\n            tf.TensorSpec(shape=(None,), dtype=tf.float64)))\n\n#collect all data and save for faster training\ntf.data.experimental.save(train_ds.take(3638), \"./saved_train_ds\")\ntf.data.experimental.save(test_ds.take(96), \"./saved_test_ds\")\ntf.data.experimental.save(val_ds.take(96), \"./saved_val_ds\")\n","metadata":{"execution":{"iopub.status.busy":"2021-08-24T15:34:31.185826Z","iopub.execute_input":"2021-08-24T15:34:31.186297Z","iopub.status.idle":"2021-08-24T15:34:44.761216Z","shell.execute_reply.started":"2021-08-24T15:34:31.186251Z","shell.execute_reply":"2021-08-24T15:34:44.760239Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#load datasets\ntrain_ds = tf.data.experimental.load(\"./saved_train_ds\", element_spec=(\n            tf.TensorSpec(shape=(None,n_feat), dtype=tf.float64),\n            tf.TensorSpec(shape=(None,), dtype=tf.float64)))\ntest_ds = tf.data.experimental.load(\"./saved_test_ds\", element_spec=(\n            tf.TensorSpec(shape=(None,n_feat), dtype=tf.float64),\n            tf.TensorSpec(shape=(None,), dtype=tf.float64)))\nval_ds = tf.data.experimental.load(\"./saved_val_ds\", element_spec=(\n            tf.TensorSpec(shape=(None,n_feat), dtype=tf.float64),\n            tf.TensorSpec(shape=(None,), dtype=tf.float64)))","metadata":{"execution":{"iopub.status.busy":"2021-08-24T15:34:44.762635Z","iopub.execute_input":"2021-08-24T15:34:44.76293Z","iopub.status.idle":"2021-08-24T15:34:44.808188Z","shell.execute_reply.started":"2021-08-24T15:34:44.762903Z","shell.execute_reply":"2021-08-24T15:34:44.807106Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def printAndReturnModelDescrAndErrors(model, model_id, y_test, predict_y):\n    R2 = round(r2_score(y_true = y_test, y_pred = predict_y),5)\n    RMSPE = round(rmspe(y_true = y_test, y_pred = predict_y),5)\n    print(f'Model {model_id} Performance of the prediction: R2 score: {R2}, RMSPE: {RMSPE}')\n    \n    return  {'model':model, 'R2':R2, 'RMSPE':RMSPE}","metadata":{"execution":{"iopub.status.busy":"2021-08-24T15:34:44.809613Z","iopub.execute_input":"2021-08-24T15:34:44.809923Z","iopub.status.idle":"2021-08-24T15:34:44.815919Z","shell.execute_reply.started":"2021-08-24T15:34:44.809895Z","shell.execute_reply":"2021-08-24T15:34:44.814776Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_tf_model2(train_ds, val_ds, test_ds, model_id, epochs, steps, n_feat=8, y_size=112, batch=256, patience=20):\n    global normalizer, scaler_target\n\n\n    Y_Size = y_size\n    n_feat = n_feat\n\n    cbk_es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=patience, mode='min', restore_best_weights=True)\n\n    initial_learning_rate = 0.1\n    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n    initial_learning_rate,\n    decay_steps=200,\n    decay_rate=0.96,\n    staircase=True)\n\n    input_price = tf.keras.Input(shape=(Y_Size,n_feat),name='input_price')\n    output_shape = Y_Size\n\n    L = (input_price)\n    L = layers.Flatten()(L)\n    L = layers.Dense(200, activation='relu')(L)\n    L = layers.Dense(200, activation='relu')(L)\n    out = layers.Dense(output_shape)(L)\n\n    model = tf.keras.Model(inputs=input_price, outputs=out, name=str(model_id))\n\n    rmse = tf.keras.metrics.RootMeanSquaredError()\n    model.compile(loss=tf.keras.losses.MeanSquaredError(),# root_mean_squared_per_error,\n                  optimizer=tf.keras.optimizers.Adam(0.001),\n                  metrics=['mae'])\n\n\n    train_ds = train_ds.repeat().shuffle(3000).batch(batch).prefetch(tf.data.AUTOTUNE)\n    #train_ds = train_ds.batch(batch)\n    val_ds = val_ds.batch(1).take(96)\n    test_ds = test_ds.batch(1).take(96)\n\n    display(model.summary())\n    history = model.fit(\n        train_ds,\n        validation_data=val_ds,\n        verbose=2, \n        epochs=epochs,\n        steps_per_epoch = steps,\n        callbacks = [cbk_es]\n    )\n\n    df_predicted = getPredictionFromTheModel(model, test_df, used_cols)\n    if True == cfg['useLabelTransformation']:\n        df_predicted.loc[:,'predict'] = scaler_target.inverse_transform(df_predicted.loc[:,'predict'].to_numpy().reshape(-1,1)).flatten() / 100\n        print('Test')\n\n    model_descr = printAndReturnModelDescrAndErrors(model, \"Test Model\", df_predicted.loc[:,'target'].to_numpy(), df_predicted.loc[:,'predict'].to_numpy())\n    model_descr['history'] = history\n    return model_descr","metadata":{"execution":{"iopub.status.busy":"2021-08-24T15:34:44.817626Z","iopub.execute_input":"2021-08-24T15:34:44.81804Z","iopub.status.idle":"2021-08-24T15:34:44.834696Z","shell.execute_reply.started":"2021-08-24T15:34:44.817999Z","shell.execute_reply":"2021-08-24T15:34:44.833335Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size = 256\nsteps = (3638 // batch_size)+1\n#steps=114\nmodel_descr = train_tf_model2(train_ds, val_ds, test_ds, \"tf_model_2\", 1500, steps, n_feat=len(used_cols), batch=batch_size, y_size=len(ALL_STOCKS.keys()), patience=50)","metadata":{"execution":{"iopub.status.busy":"2021-08-24T15:34:44.836233Z","iopub.execute_input":"2021-08-24T15:34:44.836646Z","iopub.status.idle":"2021-08-24T15:36:08.245302Z","shell.execute_reply.started":"2021-08-24T15:34:44.836606Z","shell.execute_reply":"2021-08-24T15:36:08.244129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"list_order_book_file_test = glob.glob(path_to_files + '/book_test.parquet/*')\nlist_order_book_file_test[:1]","metadata":{"execution":{"iopub.status.busy":"2021-08-24T15:36:08.246952Z","iopub.execute_input":"2021-08-24T15:36:08.247399Z","iopub.status.idle":"2021-08-24T15:36:08.259569Z","shell.execute_reply.started":"2021-08-24T15:36:08.247354Z","shell.execute_reply":"2021-08-24T15:36:08.258654Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n#ret_df = constructBookDataDataFrame(list_order_book_file_test, isTrain=False)\nret_df = preprocessor(list_order_book_file_test, isTrain=False)\nret_df = ret_df.sort_values(by='stock_id').reset_index(drop=True)\nret_df.loc[:,'stock_id'] = ret_df.loc[:,'stock_id'].astype(np.int16)\nprint(ret_df.shape)\nret_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-24T15:36:08.261088Z","iopub.execute_input":"2021-08-24T15:36:08.261529Z","iopub.status.idle":"2021-08-24T15:36:08.354113Z","shell.execute_reply.started":"2021-08-24T15:36:08.261487Z","shell.execute_reply":"2021-08-24T15:36:08.352996Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def getTestPredictionFromTheModel(model, X_df, used_cols):\n    df_res = pd.DataFrame()\n\n    for j in X_df.time_id.unique():    \n        predicted = getPredictionFromOneTimeId(model, X_df, j, used_cols)\n        f = j == X_df.time_id\n        row_ids = X_df.loc[f,'row_id'].to_numpy()\n        df_res = pd.concat([df_res, pd.DataFrame({'row_id':row_ids, 'target':predicted})])\n    \n    return df_res.reset_index(drop=True)    \n","metadata":{"execution":{"iopub.status.busy":"2021-08-24T15:36:08.358454Z","iopub.execute_input":"2021-08-24T15:36:08.358839Z","iopub.status.idle":"2021-08-24T15:36:08.364884Z","shell.execute_reply.started":"2021-08-24T15:36:08.358803Z","shell.execute_reply":"2021-08-24T15:36:08.363745Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#local test prediction will not work if you didn't train you model with stockid 0\n#for proper submission model should be trained on all stocks\ndf_predicted = getTestPredictionFromTheModel(model_descr['model'], ret_df, used_cols)\ndf_predicted.to_csv('submisson.csv',index = False)\ndf_predicted.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-24T15:36:08.366301Z","iopub.execute_input":"2021-08-24T15:36:08.366583Z","iopub.status.idle":"2021-08-24T15:36:08.875956Z","shell.execute_reply.started":"2021-08-24T15:36:08.366557Z","shell.execute_reply":"2021-08-24T15:36:08.874937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}