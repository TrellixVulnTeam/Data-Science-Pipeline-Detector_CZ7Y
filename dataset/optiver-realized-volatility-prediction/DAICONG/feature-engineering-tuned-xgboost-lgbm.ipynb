{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"THIS WORK IS COPIED FROM https://www.kaggle.com/munumbutt/feature-engineering-tuned-xgboost-lgbm","metadata":{}},{"cell_type":"markdown","source":"Wouldnt be possible without these amazing notebooks\n\n* https://www.kaggle.com/abhishek1aa/feature-engineering-xgboost-lgbm-baseline/notebook\n* https://www.kaggle.com/yus002/realized-volatility-prediction-lgbm-train/data\n* https://www.kaggle.com/konradb/we-need-to-go-deeper","metadata":{"execution":{"iopub.status.busy":"2021-07-01T07:20:57.076615Z","iopub.execute_input":"2021-07-01T07:20:57.077556Z","iopub.status.idle":"2021-07-01T07:20:57.08989Z","shell.execute_reply.started":"2021-07-01T07:20:57.077462Z","shell.execute_reply":"2021-07-01T07:20:57.088128Z"}}},{"cell_type":"markdown","source":"### Imports","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import r2_score\nimport os\nimport glob\nfrom tqdm import tqdm\nfrom joblib import Parallel, delayed\nimport gc\n\nfrom sklearn.model_selection import train_test_split, KFold\n\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor","metadata":{"execution":{"iopub.status.busy":"2021-07-01T07:21:15.354971Z","iopub.execute_input":"2021-07-01T07:21:15.355374Z","iopub.status.idle":"2021-07-01T07:21:18.196779Z","shell.execute_reply.started":"2021-07-01T07:21:15.355342Z","shell.execute_reply":"2021-07-01T07:21:18.195947Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Config","metadata":{}},{"cell_type":"code","source":"class Config:\n    data_dir = '../input/optiver-realized-volatility-prediction/'\n    seed = 42","metadata":{"execution":{"iopub.status.busy":"2021-07-01T07:21:18.198144Z","iopub.execute_input":"2021-07-01T07:21:18.198524Z","iopub.status.idle":"2021-07-01T07:21:18.202803Z","shell.execute_reply.started":"2021-07-01T07:21:18.198482Z","shell.execute_reply":"2021-07-01T07:21:18.201955Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv(Config.data_dir + 'train.csv')\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-01T07:21:18.20536Z","iopub.execute_input":"2021-07-01T07:21:18.205826Z","iopub.status.idle":"2021-07-01T07:21:18.442666Z","shell.execute_reply.started":"2021-07-01T07:21:18.205744Z","shell.execute_reply":"2021-07-01T07:21:18.441857Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.stock_id.unique()","metadata":{"execution":{"iopub.status.busy":"2021-07-01T07:21:18.44422Z","iopub.execute_input":"2021-07-01T07:21:18.444566Z","iopub.status.idle":"2021-07-01T07:21:18.45587Z","shell.execute_reply.started":"2021-07-01T07:21:18.444529Z","shell.execute_reply":"2021-07-01T07:21:18.45467Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = pd.read_csv(Config.data_dir + 'test.csv')\ntest.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-01T07:21:18.457337Z","iopub.execute_input":"2021-07-01T07:21:18.457958Z","iopub.status.idle":"2021-07-01T07:21:18.471751Z","shell.execute_reply.started":"2021-07-01T07:21:18.457921Z","shell.execute_reply":"2021-07-01T07:21:18.470845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(train.groupby('stock_id').size())\n\nprint(\"\\nUnique size values\")\ndisplay(train.groupby('stock_id').size().unique())","metadata":{"execution":{"iopub.status.busy":"2021-07-01T07:21:18.473067Z","iopub.execute_input":"2021-07-01T07:21:18.473408Z","iopub.status.idle":"2021-07-01T07:21:18.50231Z","shell.execute_reply.started":"2021-07-01T07:21:18.473374Z","shell.execute_reply":"2021-07-01T07:21:18.501376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Helper Functions","metadata":{}},{"cell_type":"markdown","source":"#### File reading","metadata":{}},{"cell_type":"code","source":"def get_trade_and_book_by_stock_and_time_id(stock_id, time_id=None, dataType = 'train'):\n    book_example = pd.read_parquet(f'{Config.data_dir}book_{dataType}.parquet/stock_id={stock_id}')\n    trade_example =  pd.read_parquet(f'{Config.data_dir}trade_{dataType}.parquet/stock_id={stock_id}')\n    if time_id:\n        book_example = book_example[book_example['time_id']==time_id]\n        trade_example = trade_example[trade_example['time_id']==time_id]\n    book_example.loc[:,'stock_id'] = stock_id\n    trade_example.loc[:,'stock_id'] = stock_id\n    return book_example, trade_example","metadata":{"execution":{"iopub.status.busy":"2021-07-01T07:21:18.503673Z","iopub.execute_input":"2021-07-01T07:21:18.504016Z","iopub.status.idle":"2021-07-01T07:21:18.510351Z","shell.execute_reply.started":"2021-07-01T07:21:18.503981Z","shell.execute_reply":"2021-07-01T07:21:18.509367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Feature engineering","metadata":{}},{"cell_type":"code","source":"def log_return(list_stock_prices):\n    return np.log(list_stock_prices).diff() \n\ndef realized_volatility(series_log_return):\n    return np.sqrt(np.sum(series_log_return**2))\n\n\ndef rmspe(y_true, y_pred):\n    return  (np.sqrt(np.mean(np.square((y_true - y_pred) / y_true))))\n\ndef calculate_wap1(df):\n    a1 = df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']\n    b1 = df['bid_size1'] + df['ask_size1']\n    a2 = df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2']\n    b2 = df['bid_size2'] + df['ask_size2']\n    \n    x = (a1/b1 + a2/b2)/ 2\n    \n    return x\n\n\ndef calculate_wap2(df):\n        \n    a1 = df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']\n    a2 = df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2']\n    b = df['bid_size1'] + df['ask_size1'] + df['bid_size2']+ df['ask_size2']\n    \n    x = (a1 + a2)/ b\n    return x\n\ndef realized_volatility_per_time_id(file_path, prediction_column_name):\n\n    stock_id = file_path.split('=')[1]\n\n    df_book = pd.read_parquet(file_path)\n    df_book['wap1'] = calculate_wap1(df_book)\n    df_book['wap2'] = calculate_wap2(df_book)\n\n    df_book['log_return1'] = df_book.groupby(['time_id'])['wap1'].apply(log_return)\n    df_book['log_return2'] = df_book.groupby(['time_id'])['wap2'].apply(log_return)\n    df_book = df_book[~df_book['log_return1'].isnull()]\n\n    df_rvps =  pd.DataFrame(df_book.groupby(['time_id'])[['log_return1', 'log_return2']].agg(realized_volatility)).reset_index()\n    df_rvps[prediction_column_name] = 0.6 * df_rvps['log_return1'] + 0.4 * df_rvps['log_return2']\n\n    df_rvps['row_id'] = df_rvps['time_id'].apply(lambda x:f'{stock_id}-{x}')\n    \n    return df_rvps[['row_id',prediction_column_name]]","metadata":{"execution":{"iopub.status.busy":"2021-07-01T07:21:18.513481Z","iopub.execute_input":"2021-07-01T07:21:18.514072Z","iopub.status.idle":"2021-07-01T07:21:18.527116Z","shell.execute_reply.started":"2021-07-01T07:21:18.514036Z","shell.execute_reply":"2021-07-01T07:21:18.526059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_agg_info(df):\n    agg_df = df.groupby(['stock_id', 'time_id']).agg(mean_sec_in_bucket = ('seconds_in_bucket', 'mean'), \n                                                     mean_price = ('price', 'mean'),\n                                                     mean_size = ('size', 'mean'),\n                                                     mean_order = ('order_count', 'mean'),\n                                                     max_sec_in_bucket = ('seconds_in_bucket', 'max'), \n                                                     max_price = ('price', 'max'),\n                                                     max_size = ('size', 'max'),\n                                                     max_order = ('order_count', 'max'),\n                                                     min_sec_in_bucket = ('seconds_in_bucket', 'min'), \n                                                     min_price = ('price', 'min'),\n                                                     #min_size = ('size', 'min'),\n                                                     #min_order = ('order_count', 'min'),\n                                                     median_sec_in_bucket = ('seconds_in_bucket', 'median'), \n                                                     median_price = ('price', 'median'),\n                                                     median_size = ('size', 'median'),\n                                                     median_order = ('order_count', 'median')\n                                                    ).reset_index()\n    \n    return agg_df","metadata":{"execution":{"iopub.status.busy":"2021-07-01T07:21:18.529517Z","iopub.execute_input":"2021-07-01T07:21:18.529974Z","iopub.status.idle":"2021-07-01T07:21:18.538908Z","shell.execute_reply.started":"2021-07-01T07:21:18.529938Z","shell.execute_reply":"2021-07-01T07:21:18.538115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Most of the feature engineering code","metadata":{}},{"cell_type":"code","source":"def get_stock_stat(stock_id : int, dataType = 'train'):\n    \n    book_subset, trade_subset = get_trade_and_book_by_stock_and_time_id(stock_id, dataType=dataType)\n    book_subset.sort_values(by=['time_id', 'seconds_in_bucket'])\n\n    ## book data processing\n    \n    book_subset['bas'] = (book_subset[['ask_price1', 'ask_price2']].min(axis = 1)\n                                / book_subset[['bid_price1', 'bid_price2']].max(axis = 1)\n                                - 1)                               \n\n    \n    book_subset['wap1'] = calculate_wap1(book_subset)\n    book_subset['wap2'] = calculate_wap2(book_subset)\n    \n    book_subset['log_return_bid_price1'] = np.log(book_subset['bid_price1'].pct_change() + 1)\n    book_subset['log_return_ask_price1'] = np.log(book_subset['ask_price1'].pct_change() + 1)\n    # book_subset['log_return_bid_price2'] = np.log(book_subset['bid_price2'].pct_change() + 1)\n    # book_subset['log_return_ask_price2'] = np.log(book_subset['ask_price2'].pct_change() + 1)\n    book_subset['log_return_bid_size1'] = np.log(book_subset['bid_size1'].pct_change() + 1)\n    book_subset['log_return_ask_size1'] = np.log(book_subset['ask_size1'].pct_change() + 1)\n    # book_subset['log_return_bid_size2'] = np.log(book_subset['bid_size2'].pct_change() + 1)\n    # book_subset['log_return_ask_size2'] = np.log(book_subset['ask_size2'].pct_change() + 1)\n    book_subset['log_ask_1_div_bid_1'] = np.log(book_subset['ask_price1'] / book_subset['bid_price1'])\n    book_subset['log_ask_1_div_bid_1_size'] = np.log(book_subset['ask_size1'] / book_subset['bid_size1'])\n    \n\n    book_subset['log_return1'] = (book_subset.groupby(by = ['time_id'])['wap1'].\n                                  apply(log_return).\n                                  reset_index(drop = True).\n                                  fillna(0)\n                                 )\n    book_subset['log_return2'] = (book_subset.groupby(by = ['time_id'])['wap2'].\n                                  apply(log_return).\n                                  reset_index(drop = True).\n                                  fillna(0)\n                                 )\n    \n    stock_stat = pd.merge(\n        book_subset.groupby(by = ['time_id'])['log_return1'].agg(realized_volatility).reset_index(),\n        book_subset.groupby(by = ['time_id'], as_index = False)['bas'].mean(),\n        on = ['time_id'],\n        how = 'left'\n    )\n    \n    stock_stat = pd.merge(\n        stock_stat,\n        book_subset.groupby(by = ['time_id'])['log_return2'].agg(realized_volatility).reset_index(),\n        on = ['time_id'],\n        how = 'left'\n    )\n    \n    stock_stat = pd.merge(\n        stock_stat,\n        book_subset.groupby(by = ['time_id'])['log_return_bid_price1'].agg(realized_volatility).reset_index(),\n        on = ['time_id'],\n        how = 'left'\n    )\n    stock_stat = pd.merge(\n        stock_stat,\n        book_subset.groupby(by = ['time_id'])['log_return_ask_price1'].agg(realized_volatility).reset_index(),\n        on = ['time_id'],\n        how = 'left'\n    )\n    stock_stat = pd.merge(\n        stock_stat,\n        book_subset.groupby(by = ['time_id'])['log_return_bid_size1'].agg(realized_volatility).reset_index(),\n        on = ['time_id'],\n        how = 'left'\n    )\n    stock_stat = pd.merge(\n        stock_stat,\n        book_subset.groupby(by = ['time_id'])['log_return_ask_size1'].agg(realized_volatility).reset_index(),\n        on = ['time_id'],\n        how = 'left'\n    )\n    \n    stock_stat = pd.merge(\n        stock_stat,\n        book_subset.groupby(by = ['time_id'])['log_ask_1_div_bid_1'].agg(realized_volatility).reset_index(),\n        on = ['time_id'],\n        how = 'left'\n    )\n    stock_stat = pd.merge(\n        stock_stat,\n        book_subset.groupby(by = ['time_id'])['log_ask_1_div_bid_1_size'].agg(realized_volatility).reset_index(),\n        on = ['time_id'],\n        how = 'left'\n    )\n    \n    \n    stock_stat['stock_id'] = stock_id\n    \n    # Additional features that can be added. Referenced from https://www.kaggle.com/yus002/realized-volatility-prediction-lgbm-train/data\n    \n    # trade_subset_agg = get_agg_info(trade_subset)\n    \n    #     stock_stat = pd.merge(\n    #         stock_stat,\n    #         trade_subset_agg,\n    #         on = ['stock_id', 'time_id'],\n    #         how = 'left'\n    #     )\n    \n    ## trade data processing \n    \n    return stock_stat\n\ndef get_data_set(stock_ids : list, dataType = 'train'):\n\n    stock_stat = Parallel(n_jobs=-1)(\n        delayed(get_stock_stat)(stock_id, dataType) \n        for stock_id in stock_ids\n    )\n    \n    stock_stat_df = pd.concat(stock_stat, ignore_index = True)\n\n    return stock_stat_df","metadata":{"execution":{"iopub.status.busy":"2021-07-01T07:21:18.540272Z","iopub.execute_input":"2021-07-01T07:21:18.540664Z","iopub.status.idle":"2021-07-01T07:21:18.562095Z","shell.execute_reply.started":"2021-07-01T07:21:18.540629Z","shell.execute_reply":"2021-07-01T07:21:18.561258Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Metric","metadata":{}},{"cell_type":"code","source":"def rmspe(y_true, y_pred):\n    return  (np.sqrt(np.mean(np.square((y_true - y_pred) / y_true))))","metadata":{"execution":{"iopub.status.busy":"2021-07-01T07:21:18.563083Z","iopub.execute_input":"2021-07-01T07:21:18.563343Z","iopub.status.idle":"2021-07-01T07:21:18.573285Z","shell.execute_reply.started":"2021-07-01T07:21:18.563319Z","shell.execute_reply":"2021-07-01T07:21:18.572489Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Plotting","metadata":{}},{"cell_type":"code","source":"def plot_feature_importance(df, model):\n    feature_importances_df = pd.DataFrame({\n        'feature': df.columns,\n        'importance_score': model.feature_importances_\n    })\n    plt.rcParams[\"figure.figsize\"] = [10, 5]\n    ax = sns.barplot(x = \"feature\", y = \"importance_score\", data = feature_importances_df)\n    ax.set(xlabel=\"Features\", ylabel = \"Importance Score\")\n    plt.xticks(rotation=45)\n    plt.show()\n    return feature_importances_df","metadata":{"execution":{"iopub.status.busy":"2021-07-01T07:21:18.5745Z","iopub.execute_input":"2021-07-01T07:21:18.574905Z","iopub.status.idle":"2021-07-01T07:21:18.582727Z","shell.execute_reply.started":"2021-07-01T07:21:18.574869Z","shell.execute_reply":"2021-07-01T07:21:18.58184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Example of book and trade data","metadata":{}},{"cell_type":"code","source":"book_stock_1, trade_stock_1 = get_trade_and_book_by_stock_and_time_id(1, 5)\ndisplay(book_stock_1.shape)\ndisplay(trade_stock_1.shape)","metadata":{"execution":{"iopub.status.busy":"2021-07-01T07:21:18.583721Z","iopub.execute_input":"2021-07-01T07:21:18.583963Z","iopub.status.idle":"2021-07-01T07:21:19.593448Z","shell.execute_reply.started":"2021-07-01T07:21:18.58394Z","shell.execute_reply":"2021-07-01T07:21:19.5927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"book_stock_1.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-01T07:21:19.594724Z","iopub.execute_input":"2021-07-01T07:21:19.595059Z","iopub.status.idle":"2021-07-01T07:21:19.608073Z","shell.execute_reply.started":"2021-07-01T07:21:19.595018Z","shell.execute_reply":"2021-07-01T07:21:19.60669Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trade_stock_1.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-01T07:21:19.609859Z","iopub.execute_input":"2021-07-01T07:21:19.61029Z","iopub.status.idle":"2021-07-01T07:21:19.623455Z","shell.execute_reply.started":"2021-07-01T07:21:19.610251Z","shell.execute_reply":"2021-07-01T07:21:19.622382Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Preparing Train and Test set for training and prediction with the desired features\nThe following cell takes around 25 mins for execution. You can also use the pickled data from the notebook output and build on that","metadata":{}},{"cell_type":"code","source":"%%time\ntrain_stock_stat_df = get_data_set(train.stock_id.unique(), dataType = 'train')\ntrain_stock_stat_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-01T07:21:19.624927Z","iopub.execute_input":"2021-07-01T07:21:19.625353Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data_set = pd.merge(train, train_stock_stat_df, on = ['stock_id', 'time_id'], how = 'left')\ntrain_data_set.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data_set.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ntest_stock_stat_df = get_data_set(test['stock_id'].unique(), dataType = 'test')\ntest_stock_stat_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data_set = pd.merge(test, test_stock_stat_df, on = ['stock_id', 'time_id'], how = 'left')\ntest_data_set.fillna(-999, inplace=True)\ntest_data_set","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Storing for later usages. Processing time for features took 25 mins\nYou can directly use this from the notebook output and build on that","metadata":{}},{"cell_type":"code","source":"train_data_set.to_pickle('train_features_df.pickle')\ntest_data_set.to_pickle('test_features_df.pickle')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = gc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_display = train_data_set.drop(['stock_id', 'time_id', 'target'], axis = 1)\nX = X_display.values\ny = train_data_set['target'].values\n\nX.shape, y.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=Config.seed, shuffle=False)\nX_train.shape, X_test.shape, y_train.shape, y_test.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Optuna Tuned XGBoost","metadata":{}},{"cell_type":"code","source":"rs = Config.seed","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import optuna\nfrom optuna.samplers import TPESampler\n\ndef objective(trial, data=X, target=y):\n    \n    def rmspe(y_true, y_pred):\n        return  (np.sqrt(np.mean(np.square((y_true - y_pred) / y_true))))\n    \n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=rs, shuffle=False)\n    \n    param = {\n        'tree_method':'gpu_hist', \n        'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n        'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n        'colsample_bytree': trial.suggest_categorical('colsample_bytree', [0.3,0.4,0.5,0.6,0.7,0.8,0.9, 1.0]),\n        'subsample': trial.suggest_categorical('subsample', [0.4,0.5,0.6,0.7,0.8,1.0]),\n        'learning_rate': trial.suggest_categorical('learning_rate', [0.008,0.009,0.01,0.012,0.014,0.016,0.018, 0.02]),\n        'n_estimators': trial.suggest_int('n_estimators', 500, 3000),\n        'max_depth': trial.suggest_categorical('max_depth', [5,7,9,11,13,15,17,20]),\n        'random_state': trial.suggest_categorical('random_state', [24, 48,2020]),\n        'min_child_weight': trial.suggest_int('min_child_weight', 1, 300)}\n    \n    model = XGBRegressor(**param)\n    \n    pruning_callback = optuna.integration.XGBoostPruningCallback(trial, \"validation-rmse\")\n    model.fit(X_train ,y_train, eval_set=[(X_test, y_test)], early_stopping_rounds=100, verbose=False)\n    \n    preds = model.predict(X_test)\n    \n    rmspe = rmspe(y_test, preds)\n    \n    return rmspe","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"study = optuna.create_study(sampler=TPESampler(), direction='minimize', pruner=optuna.pruners.MedianPruner(n_warmup_steps=5))\nstudy.optimize(objective, n_trials=1000, gc_after_trial=True)","metadata":{"scrolled":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Number of finished trials:', len(study.trials))\nprint('Best trial:', study.best_trial.params)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optuna.visualization.plot_optimization_history(study)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optuna.visualization.plot_param_importances(study)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_xgbparams = study.best_params\nbest_xgbparams","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgb = XGBRegressor(**best_xgbparams, tree_method='gpu_hist')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nxgb.fit(X_train ,y_train, eval_set=[(X_test, y_test)], early_stopping_rounds=100, verbose=False)\n\npreds = xgb.predict(X_test)\nR2 = round(r2_score(y_true = y_test, y_pred = preds), 5)\nRMSPE = round(rmspe(y_true = y_test, y_pred = preds), 5)\nprint(f'Performance of the Tuned XGB prediction: R2 score: {R2}, RMSPE: {RMSPE}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Optuna Tuned LGBM","metadata":{}},{"cell_type":"code","source":"def objective(trial):\n    \n    def rmspe(y_true, y_pred):\n        return  (np.sqrt(np.mean(np.square((y_true - y_pred) / y_true))))\n    \n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=rs, shuffle=False)\n    valid = [(X_test, y_test)]\n    \n    param = {\n        \"device\": \"gpu\",\n        \"metric\": \"rmse\",\n        \"verbosity\": -1,\n        'learning_rate':trial.suggest_loguniform('learning_rate', 0.005, 0.5),\n        \"max_depth\": trial.suggest_int(\"max_depth\", 2, 500),\n        \"lambda_l1\": trial.suggest_float(\"lambda_l1\", 1e-8, 10.0, log=True),\n        \"lambda_l2\": trial.suggest_float(\"lambda_l2\", 1e-8, 10.0, log=True),\n        \"num_leaves\": trial.suggest_int(\"num_leaves\", 2, 256),\n        \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 4000),\n#         \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 100000, 700000),\n        \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.4, 1.0),\n        \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.4, 1.0),\n        \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 1, 7),\n        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 100)}\n\n    pruning_callback = optuna.integration.LightGBMPruningCallback(trial, \"rmse\")\n    model = LGBMRegressor(**param)\n    \n    model.fit(X_train, y_train, eval_set=valid, verbose=False, callbacks=[pruning_callback], early_stopping_rounds=100)\n\n    preds = model.predict(X_test)\n    \n    rmspe = rmspe(y_test, preds)\n    return rmspe","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"study = optuna.create_study(sampler=TPESampler(), direction='minimize', pruner=optuna.pruners.MedianPruner(n_warmup_steps=5))\nstudy.optimize(objective, n_trials=1000, gc_after_trial=True)","metadata":{"_kg_hide-output":true,"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Number of finished trials:', len(study.trials))\nprint('Best trial:', study.best_trial.params)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optuna.visualization.plot_optimization_history(study)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optuna.visualization.plot_param_importances(study)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_lgbmparams = study.best_params\nbest_lgbmparams","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lgbm = LGBMRegressor(**best_lgbmparams, device='gpu')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nlgbm.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=False, early_stopping_rounds=100)\n\npreds = xgb.predict(X_test)\nR2 = round(r2_score(y_true = y_test, y_pred = preds), 6)\nRMSPE = round(rmspe(y_true = y_test, y_pred = preds), 6)\nprint(f'Performance of the Tuned LIGHTGBM prediction: R2 score: {R2}, RMSPE: {RMSPE}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Stacking Regressor","metadata":{}},{"cell_type":"code","source":"def_xgb = XGBRegressor(tree_method='gpu_hist', random_state = rs, n_jobs= - 1)\n\ndef_lgbm = LGBMRegressor(device='gpu', random_state=rs)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import StackingRegressor\n\n\nestimators = [('def_xgb', def_xgb),\n              ('def_lgbm', def_lgbm),\n              ('tuned_xgb', xgb)]\n\nclf = StackingRegressor(estimators=estimators, final_estimator=lgbm, verbose=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nclf.fit(X_train, y_train)","metadata":{"_kg_hide-output":true,"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = clf.predict(X_test)\nR2 = round(r2_score(y_true = y_test, y_pred = preds),6)\nRMSPE = round(rmspe(y_true = y_test, y_pred = preds), 6)\nprint(f'Performance of the STACK prediction: R2 score: {R2}, RMSPE: {RMSPE}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"test_data_set_final = test_data_set.drop(['stock_id', 'time_id'], axis = 1)\n\ny_pred = test_data_set_final[['row_id']]\nX_test = test_data_set_final.drop(['row_id'], axis = 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = y_pred.assign(target = clf.predict(X_test))\ny_pred.to_csv('submission.csv',index = False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}