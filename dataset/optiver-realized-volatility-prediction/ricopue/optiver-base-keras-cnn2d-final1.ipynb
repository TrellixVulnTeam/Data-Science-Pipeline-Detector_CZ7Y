{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os\n\nfrom sklearn.model_selection import GroupKFold\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom keras import backend as K\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2021-10-14T19:30:01.222009Z","iopub.execute_input":"2021-10-14T19:30:01.222385Z","iopub.status.idle":"2021-10-14T19:30:06.496249Z","shell.execute_reply.started":"2021-10-14T19:30:01.22229Z","shell.execute_reply":"2021-10-14T19:30:06.495413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"local_work = False\nSEED=2021\nN_FOLDS=5\nbatch_size = 1024\n\ndef seed_everything(seed=2021):\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n\n\nseed_everything(SEED)","metadata":{"execution":{"iopub.status.busy":"2021-10-14T19:30:06.497645Z","iopub.execute_input":"2021-10-14T19:30:06.497969Z","iopub.status.idle":"2021-10-14T19:30:06.50705Z","shell.execute_reply.started":"2021-10-14T19:30:06.497935Z","shell.execute_reply":"2021-10-14T19:30:06.506181Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if local_work:\n    data_dir = '../'\nelse:\n    data_dir ='../input/optiver-realized-volatility-prediction/'\n\ntrain = pd.read_csv(data_dir +'train.csv')\ntest = pd.read_csv(data_dir +'test.csv')","metadata":{"execution":{"iopub.status.busy":"2021-10-14T19:30:06.510577Z","iopub.execute_input":"2021-10-14T19:30:06.510914Z","iopub.status.idle":"2021-10-14T19:30:06.843498Z","shell.execute_reply.started":"2021-10-14T19:30:06.510887Z","shell.execute_reply":"2021-10-14T19:30:06.838562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Remove worse time_id and convert target to float32.\ntime_id_remove=[25504,27174,24034,20439,3668,4851,6274,19260,27876,11579,28319,23792,23030,1544,2139,14447]\ntrain = train.drop(train[train['time_id'].isin(time_id_remove)].index)","metadata":{"execution":{"iopub.status.busy":"2021-10-14T19:30:06.845236Z","iopub.execute_input":"2021-10-14T19:30:06.845647Z","iopub.status.idle":"2021-10-14T19:30:06.906188Z","shell.execute_reply.started":"2021-10-14T19:30:06.84561Z","shell.execute_reply":"2021-10-14T19:30:06.904963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def calc_wap(df):\n    wap = (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1'])/(df['bid_size1'] + df['ask_size1'])\n    return wap\n\ndef calc_wap2(df):\n    wap = (df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2'])/(df['bid_size2'] + df['ask_size2'])\n    return wap\n\n\ndef log_return(list_stock_prices):\n    return np.log(list_stock_prices).diff() \n\ndef realized_volatility(series):\n    return np.sqrt(np.sum(series**2))\n\ndef count_unique(series):\n    return len(np.unique(series))\n\ndef rmspe(y_true, y_pred):\n    return  (np.sqrt(np.mean(np.square((y_true - y_pred) / y_true))))\n\ndef rmspe_exp(y_true, y_pred):\n    return  (np.sqrt(np.mean(np.square(((np.exp(y_true)-1) - (np.exp(y_pred)-1)) / (np.exp(y_true)-1)))))\n\ndef metric_rmspe(y_true, y_pred):\n         return K.sqrt(K.mean(K.square( (y_true - y_pred)/ y_true )))\n\ndef metric_rmspe_exp(y_true, y_pred):\n         return K.sqrt(K.mean(K.square( ((K.exp(y_true)-1) - (K.exp(y_pred)-1))/ (K.exp(y_true)-1) )))\n\n# Function to early stop with root mean squared percentage error\ndef feval_rmspe(y_pred, lgb_train):\n    y_true = lgb_train.get_label()\n    return 'RMSPE', rmspe(y_true, y_pred), False\n\n# Function to early stop with root mean squared percentage error\ndef feval_rmspe_exp(y_pred, lgb_train):\n    y_true = lgb_train.get_label()\n    return 'RMSPE', rmspe_exp(y_true, y_pred), False","metadata":{"execution":{"iopub.status.busy":"2021-10-14T19:30:06.907884Z","iopub.execute_input":"2021-10-14T19:30:06.908239Z","iopub.status.idle":"2021-10-14T19:30:06.948666Z","shell.execute_reply.started":"2021-10-14T19:30:06.908201Z","shell.execute_reply":"2021-10-14T19:30:06.944314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocessor_book(file_path):\n    df = pd.read_parquet(file_path)\n    #calculate return etc\n    df['wap1'] = calc_wap(df)\n    df['log_return1'] = df.groupby('time_id')['wap1'].apply(log_return)    \n    \n    df['wap2'] = calc_wap2(df)\n    df['log_return2'] = df.groupby('time_id')['wap2'].apply(log_return)\n    \n    df['total_volume'] = (df['ask_size1'] + df['ask_size2']) + (df['bid_size1'] + df['bid_size2'])\n    df['volume_imbalance'] = abs((df['ask_size1'] + df['ask_size2']) - (df['bid_size1'] + df['bid_size2']))\n\n\n    df['price_spread1'] = (df['ask_price1'] - df['bid_price1']) / ((df['ask_price1'] + df['bid_price1']) / 2)\n    df['price_spread2'] = (df['ask_price2'] - df['bid_price2']) / ((df['ask_price2'] + df['bid_price2']) / 2)\n    df['bid_spread'] = df['bid_price1'] - df['bid_price2']\n    df['ask_spread'] = df['ask_price1'] - df['ask_price2']\n    df[\"bid_ask_spread\"] = abs(df['bid_spread'] - df['ask_spread'])\n    \n    \n    #dict for aggregate\n    create_feature_dict = {\n        'log_return1':[realized_volatility],\n        'log_return2':[realized_volatility],\n        'total_volume':[np.mean],\n        'volume_imbalance':[np.mean],\n        'price_spread1':[np.max,np.sum],\n        'price_spread2':[np.max,np.sum],\n        'bid_ask_spread':[np.sum],}\n    \n    \n    #####groupby / all seconds\n    df_feature = pd.DataFrame(df.groupby(['time_id']).agg(create_feature_dict)).reset_index()\n    \n    df_feature.columns = ['_'.join(col) for col in df_feature.columns] #time_id is changed to time_id_\n    df_feature = df_feature.add_suffix('_T0')\n    \n    ######groupby / last XX seconds\n    last_seconds = [120,300,480]#120\n    last_seconds_name = ['T1','T2','T3']#T1\n    \n    for second,ls_name in zip(last_seconds,last_seconds_name):\n        second = 600 - second \n    \n        df_feature_sec = pd.DataFrame(df.query(f'seconds_in_bucket >= {second}').groupby(['time_id']).agg(create_feature_dict)).reset_index()\n\n        df_feature_sec.columns = ['_'.join(col) for col in df_feature_sec.columns] #time_id is changed to time_id_\n     \n        df_feature_sec = df_feature_sec.add_suffix('_' + ls_name)\n\n        df_feature = pd.merge(df_feature,df_feature_sec,how='left',left_on='time_id__T0',right_on=f'time_id__{ls_name}')\n        df_feature = df_feature.drop([f'time_id__{ls_name}'],axis=1)\n        \n    \n    #create row_id\n    stock_id = file_path.split('=')[1]\n    df_feature['row_id'] = df_feature['time_id__T0'].apply(lambda x:f'{stock_id}-{x}')\n    df_feature = df_feature.drop(['time_id__T0'],axis=1)\n    \n    return df_feature","metadata":{"execution":{"iopub.status.busy":"2021-10-14T19:30:06.953861Z","iopub.execute_input":"2021-10-14T19:30:06.954211Z","iopub.status.idle":"2021-10-14T19:30:06.98622Z","shell.execute_reply.started":"2021-10-14T19:30:06.954175Z","shell.execute_reply":"2021-10-14T19:30:06.981582Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocessor_trade(file_path):\n    df = pd.read_parquet(file_path)\n    df['log_return'] = df.groupby('time_id')['price'].apply(log_return)\n    df['amount']=df['price']*df['size']\n    \n    create_feature_dict = {\n        'log_return':[realized_volatility],\n        'seconds_in_bucket':[count_unique],\n        'size':[np.sum],\n       'order_count':[np.sum],\n       'amount':[np.max] \n    }\n    \n    df_feature = df.groupby('time_id').agg(create_feature_dict).reset_index()\n    df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n    df_feature = df_feature.add_suffix('_T0')\n    \n     ######groupby / last XX seconds\n    last_seconds = [120,300,480]#120\n    last_seconds_name = ['T1','T2','T3']#T1\n    \n    for second,ls_name in zip(last_seconds,last_seconds_name):\n        second = 600 - second \n    \n        df_feature_sec = pd.DataFrame(df.query(f'seconds_in_bucket >= {second}').groupby(['time_id']).agg(create_feature_dict)).reset_index()\n\n        df_feature_sec.columns = ['_'.join(col) for col in df_feature_sec.columns] #time_id is changed to time_id_\n     \n        df_feature_sec = df_feature_sec.add_suffix('_' + ls_name)\n\n        df_feature = pd.merge(df_feature,df_feature_sec,how='left',left_on='time_id__T0',right_on=f'time_id__{ls_name}')\n        df_feature = df_feature.drop([f'time_id__{ls_name}'],axis=1)\n    \n    \n    df_feature = df_feature.add_prefix('trade_')\n    stock_id = file_path.split('=')[1]\n    df_feature['row_id'] = df_feature['trade_time_id__T0'].apply(lambda x:f'{stock_id}-{x}')\n    df_feature = df_feature.drop(['trade_time_id__T0'],axis=1)\n    \n    return df_feature","metadata":{"execution":{"iopub.status.busy":"2021-10-14T19:30:06.98799Z","iopub.execute_input":"2021-10-14T19:30:06.989467Z","iopub.status.idle":"2021-10-14T19:30:07.008212Z","shell.execute_reply.started":"2021-10-14T19:30:06.989424Z","shell.execute_reply":"2021-10-14T19:30:07.005848Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocessor(list_stock_ids, is_train = True):\n    from joblib import Parallel, delayed # parallel computing to save time\n    df = pd.DataFrame()\n    \n    def for_joblib(stock_id):\n        if is_train:\n            file_path_book = data_dir + \"book_train.parquet/stock_id=\" + str(stock_id)\n            file_path_trade = data_dir + \"trade_train.parquet/stock_id=\" + str(stock_id)\n        else:\n            file_path_book = data_dir + \"book_test.parquet/stock_id=\" + str(stock_id)\n            file_path_trade = data_dir + \"trade_test.parquet/stock_id=\" + str(stock_id)\n            \n        df_tmp = pd.merge(preprocessor_book(file_path_book),preprocessor_trade(file_path_trade),on='row_id',how='left')\n       # df_tmp=preprocessor_book(file_path_book)\n            \n        return pd.concat([df,df_tmp])\n    \n    df = Parallel(n_jobs=-1, verbose=1)(\n        delayed(for_joblib)(stock_id) for stock_id in list_stock_ids\n        )\n\n    df =  pd.concat(df,ignore_index = True)\n    return df","metadata":{"execution":{"iopub.status.busy":"2021-10-14T19:30:07.014724Z","iopub.execute_input":"2021-10-14T19:30:07.015114Z","iopub.status.idle":"2021-10-14T19:30:07.040549Z","shell.execute_reply.started":"2021-10-14T19:30:07.015025Z","shell.execute_reply":"2021-10-14T19:30:07.037961Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ncreate_train=False\nif create_train:\n    \n    train_ids = train.stock_id.unique()\n    df_train = preprocessor(list_stock_ids= train_ids, is_train = True)\n\n    train['row_id'] = train['stock_id'].astype(str) + '-' + train['time_id'].astype(str)\n    df_train = train.merge(df_train, on = ['row_id'], how = 'left')\nelse:\n    df_train = pd.read_csv('../input/optiverricopue30/df_train_local.csv')\n    \ndf_train.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-14T19:30:07.045055Z","iopub.execute_input":"2021-10-14T19:30:07.045449Z","iopub.status.idle":"2021-10-14T19:30:10.986466Z","shell.execute_reply.started":"2021-10-14T19:30:07.045412Z","shell.execute_reply":"2021-10-14T19:30:10.985667Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ntest_ids = test.stock_id.unique()\ndf_test = preprocessor(list_stock_ids= test_ids, is_train = False)\n\ntest['row_id'] = test['stock_id'].astype(str) + '-' + test['time_id'].astype(str)\ndf_test = test.merge(df_test, on = ['row_id'], how = 'left')\n","metadata":{"execution":{"iopub.status.busy":"2021-10-14T19:30:10.987635Z","iopub.execute_input":"2021-10-14T19:30:10.987963Z","iopub.status.idle":"2021-10-14T19:30:12.121845Z","shell.execute_reply.started":"2021-10-14T19:30:10.987931Z","shell.execute_reply":"2021-10-14T19:30:12.120955Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Convert from float64 to float32\nnot_num_cols=['stock_id','target','time_id','row_id']                        \nfeatures_num=[col for col in df_train.columns if col not in not_num_cols]\nfeatures_images=[col for col in df_train.columns if 'T0' in col]\n\ndf_train[features_num]=df_train[features_num].astype('float32')\ndf_train['target']=df_train['target'].astype('float32')\ndf_test[features_num]=df_test[features_num].astype('float32')\n\nif create_train:\n    df_train.to_csv(data_dir +'df_train_local.csv',index = False)","metadata":{"execution":{"iopub.status.busy":"2021-10-14T19:30:12.124764Z","iopub.execute_input":"2021-10-14T19:30:12.125089Z","iopub.status.idle":"2021-10-14T19:30:12.766792Z","shell.execute_reply.started":"2021-10-14T19:30:12.125056Z","shell.execute_reply":"2021-10-14T19:30:12.765936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#fix nan and infitines.\ndf_train.replace(np.inf, np.nan,inplace=True)\ndf_test.replace(np.inf, np.nan,inplace=True)\n\nfor col in features_num:\n    df_train[col] = df_train[col].fillna(df_train.groupby('stock_id')[col].transform('max'))\n    df_test[col] = df_test[col].fillna(df_test.groupby('stock_id')[col].transform('max'))\n    \n    \ndf_train.replace(-np.inf, np.nan,inplace=True)\ndf_test.replace(-np.inf, np.nan,inplace=True)\n\nfor col in features_num:\n    df_train[col] = df_train[col].fillna(df_train.groupby('stock_id')[col].transform('min'))\n    df_test[col] = df_test[col].fillna(df_test.groupby('stock_id')[col].transform('min'))","metadata":{"execution":{"iopub.status.busy":"2021-10-14T19:30:12.768036Z","iopub.execute_input":"2021-10-14T19:30:12.768545Z","iopub.status.idle":"2021-10-14T19:30:13.463515Z","shell.execute_reply.started":"2021-10-14T19:30:12.76851Z","shell.execute_reply":"2021-10-14T19:30:13.462632Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from scipy.special import boxcox1p\nlam=0.1\nsk_f=1000\nfor fet in features_num:    \n    df_train[fet] = boxcox1p(df_train[fet]*sk_f,lam)\n    df_test[fet] = boxcox1p(df_test[fet]*sk_f,lam)","metadata":{"execution":{"iopub.status.busy":"2021-10-14T19:30:13.464891Z","iopub.execute_input":"2021-10-14T19:30:13.465498Z","iopub.status.idle":"2021-10-14T19:30:14.139296Z","shell.execute_reply.started":"2021-10-14T19:30:13.465455Z","shell.execute_reply":"2021-10-14T19:30:14.138454Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocessor_stock_time (df,tw):    \n \n    df_concat=pd.concat([df_train, df_test], ignore_index=True)\n    \n    cols_names=['log_return1_realized_volatility','log_return2_realized_volatility','trade_log_return_realized_volatility']\n    \n    for col in cols_names:\n        df[f'{col}_stock_{tw}']=df_concat.groupby('stock_id')[f'{col}_{tw}'].transform('mean')\n        df[f'rel_{col}_stock_{tw}']=df[f'{col}_{tw}']/df[f'{col}_stock_{tw}']\n        \n        df[f'{col}_time_{tw}']=df.groupby('time_id')[f'{col}_{tw}'].transform('mean')  \n        df[f'rel_{col}_time_{tw}']=df[f'{col}_{tw}']/df[f'{col}_time_{tw}']\n        \n        df[f'{col}_st_{tw}']=df.groupby('time_id')[f'rel_{col}_stock_{tw}'].transform('mean')\n        df[f'rel_{col}_st_{tw}']=df[f'{col}_{tw}']/df[f'{col}_st_{tw}']\n            \n        df = df.drop([f'{col}_stock_{tw}',f'rel_{col}_stock_{tw}',f'{col}_time_{tw}',f'rel_{col}_time_{tw}',\n                     f'{col}_st_{tw}'],axis=1)\n        \n\n    del df_concat\n    \n    return df","metadata":{"execution":{"iopub.status.busy":"2021-10-14T19:30:14.140595Z","iopub.execute_input":"2021-10-14T19:30:14.140947Z","iopub.status.idle":"2021-10-14T19:30:14.149645Z","shell.execute_reply.started":"2021-10-14T19:30:14.140912Z","shell.execute_reply":"2021-10-14T19:30:14.148723Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocessor_diff(df,tw):\n    not_num_cols=['stock_id','target','time_id','row_id']                 \n    features_num=[col for col in df.columns if col not in not_num_cols]\n    \n    cols_names=[col[:-2] for col in features_num if tw in col]\n\n    for col in cols_names:\n        df[col+tw]=df[col+tw]/df[col+'T0']\n        \n    return df","metadata":{"execution":{"iopub.status.busy":"2021-10-14T19:30:14.151171Z","iopub.execute_input":"2021-10-14T19:30:14.151545Z","iopub.status.idle":"2021-10-14T19:30:14.163136Z","shell.execute_reply.started":"2021-10-14T19:30:14.151511Z","shell.execute_reply":"2021-10-14T19:30:14.162279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train=preprocessor_stock_time(df_train,'T0')\ndf_test=preprocessor_stock_time(df_test,'T0')\n\ndf_train=preprocessor_stock_time(df_train,'T1')\ndf_test=preprocessor_stock_time(df_test,'T1')\n\ndf_train=preprocessor_stock_time(df_train,'T2')\ndf_test=preprocessor_stock_time(df_test,'T2')\n\ndf_train=preprocessor_stock_time(df_train,'T3')\ndf_test=preprocessor_stock_time(df_test,'T3')\n\ndf_train=preprocessor_diff(df_train,'T1')\ndf_test=preprocessor_diff(df_test,'T1')\ndf_train=preprocessor_diff(df_train,'T2')\ndf_test=preprocessor_diff(df_test,'T2')\ndf_train=preprocessor_diff(df_train,'T3')\ndf_test=preprocessor_diff(df_test,'T3')","metadata":{"execution":{"iopub.status.busy":"2021-10-14T19:30:14.166279Z","iopub.execute_input":"2021-10-14T19:30:14.166548Z","iopub.status.idle":"2021-10-14T19:30:16.521566Z","shell.execute_reply.started":"2021-10-14T19:30:14.166524Z","shell.execute_reply":"2021-10-14T19:30:16.520651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"not_num_cols=['stock_id','target','time_id','row_id']                        \nfeatures_num=[col for col in df_train.columns if col not in not_num_cols]\n\ndf_train.replace(np.inf, np.nan,inplace=True)\ndf_test.replace(np.inf, np.nan,inplace=True)\n\nfor col in features_num:\n    df_train[col] = df_train[col].fillna(df_train.groupby('stock_id')[col].transform('max'))\n    df_test[col] = df_test[col].fillna(df_test.groupby('stock_id')[col].transform('max'))\n    \n    \ndf_train.replace(-np.inf, np.nan,inplace=True)\ndf_test.replace(-np.inf, np.nan,inplace=True)\n\nfor col in features_num:\n    df_train[col] = df_train[col].fillna(df_train.groupby('stock_id')[col].transform('min'))\n    df_test[col] = df_test[col].fillna(df_test.groupby('stock_id')[col].transform('min'))","metadata":{"execution":{"iopub.status.busy":"2021-10-14T19:30:16.5228Z","iopub.execute_input":"2021-10-14T19:30:16.523125Z","iopub.status.idle":"2021-10-14T19:30:17.452142Z","shell.execute_reply.started":"2021-10-14T19:30:16.523091Z","shell.execute_reply":"2021-10-14T19:30:17.451319Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\ndf_concat=pd.concat([df_train, df_test], ignore_index=True)\n\nscaler = MinMaxScaler(feature_range=(-1, 1))\nscaler.fit(df_concat[features_num])\ndf_train[features_num]=scaler.transform(df_train[features_num])\ndf_test[features_num]=scaler.transform(df_test[features_num])\n\ndel df_concat\n\ndf_train[features_num]=df_train[features_num].astype('float32')\ndf_test[features_num]=df_test[features_num].astype('float32')","metadata":{"execution":{"iopub.status.busy":"2021-10-14T19:30:17.453445Z","iopub.execute_input":"2021-10-14T19:30:17.453946Z","iopub.status.idle":"2021-10-14T19:30:23.941787Z","shell.execute_reply.started":"2021-10-14T19:30:17.453909Z","shell.execute_reply":"2021-10-14T19:30:23.940817Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Process to generate images\n\ndef preprocessor_img_2(df_w):\n    from joblib import Parallel, delayed # parallel computing to save time       \n    \n    def Insert_row_pd(time_id,stock_id,df):\n    \n        row_number=list_of_stocks.index(stock_id)\n        listofzeros = [0] * (df.shape[1]-1)\n        fake_row=[stock_id]+listofzeros\n        df1 = df[0:row_number]\n        df2 = df[row_number:]\n        df1.loc[row_number]=fake_row\n        df1.loc[row_number,'corr_stock']=[[stock_id],[stock_id],[stock_id],[stock_id]]\n        df1.loc[row_number,'row_id']=[f'{stock_id}-{time_id}']\n        df_result = pd.concat([df1, df2])  \n        df_result.index = [*range(df_result.shape[0])]\n        return df_result    \n    \n    list_time_ids=df_w.time_id.unique()\n    \n    df = pd.DataFrame()\n    \n    def for_joblib(time_id):  \n            \n        time_id_df=df_w[df_w.time_id == time_id].sort_values(by=['stock_id'])\n        if time_id_df.shape[0]<112:\n            missing_stock_ids= [i for i in list_of_stocks if i not in time_id_df.stock_id.to_list()]\n            for stock_id in missing_stock_ids:\n                time_id_df=Insert_row_pd(time_id,stock_id,time_id_df)\n                \n        time_id_df['image_matrix']=time_id_df.apply(lambda x: time_id_df[time_id_df['stock_id'].isin(x['corr_stock'])][features_images].values,axis=1)\n             \n            \n        return pd.concat([df,time_id_df[['row_id','image_matrix']]])\n    \n    df = Parallel(n_jobs=-1, verbose=1)(delayed(for_joblib)(time_id) for time_id in list_time_ids)\n\n    df =  pd.concat(df,ignore_index = True)\n    return df","metadata":{"execution":{"iopub.status.busy":"2021-10-14T19:30:23.943109Z","iopub.execute_input":"2021-10-14T19:30:23.943593Z","iopub.status.idle":"2021-10-14T19:30:23.954326Z","shell.execute_reply.started":"2021-10-14T19:30:23.943555Z","shell.execute_reply":"2021-10-14T19:30:23.953459Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get list of corrected stock_ids. 30 most correlated \ndf_train['target_diff']=(df_train.target-df_train.log_return1_realized_volatility_T0)/df_train.log_return1_realized_volatility_T0\n\ntrain_p= df_train.pivot(index='time_id', columns='stock_id', values='target_diff')\ncorr = abs(train_p.corr())\nids = corr.index\n\ncorr_labels=[]\nfor id in ids:\n    corr_label=sorted(corr[[id]].sort_values(id,ascending=False)[1:31].index.values)\n    corr_labels.append(corr_label)\n    \nzip_iterator = zip(ids, corr_labels)\ncorr_dictionary = dict(zip_iterator)\n\ndf_train['corr_stock'] = df_train['stock_id'].map(corr_dictionary)\ndf_test['corr_stock'] = df_test['stock_id'].map(corr_dictionary) \n\ndf_train=df_train.drop(['target_diff'], axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-10-14T19:30:23.955908Z","iopub.execute_input":"2021-10-14T19:30:23.956507Z","iopub.status.idle":"2021-10-14T19:30:24.391832Z","shell.execute_reply.started":"2021-10-14T19:30:23.956435Z","shell.execute_reply":"2021-10-14T19:30:24.390944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"list_of_stocks=df_train.stock_id.unique().tolist()\n\ndf_test =pd.merge(df_test,preprocessor_img_2(df_test),on='row_id',how='left')\n\ncreate_matrix_train=False\n\nif create_matrix_train:\n    df_train =pd.merge(df_train,preprocessor_img_2(df_train),on='row_id',how='left')\n    df_train[['row_id','image_matrix']].to_pickle('./df_train_images-30.pkl')\n    \n\nelse:\n    df_train_images = pd.read_pickle('../input/optiverricopue30/df_train_images-30.pkl')\n    df_train =pd.merge(df_train,df_train_images,on='row_id',how='left')\n    del df_train_images\n    \ndf_test.loc[df_test['image_matrix'].isnull(),['image_matrix']] = df_test.loc[df_test['image_matrix'].isnull(),'image_matrix'].apply(lambda x: np.zeros((30, 14)))\ndf_train.loc[df_train['image_matrix'].isnull(),['image_matrix']] = df_train.loc[df_train['image_matrix'].isnull(),'image_matrix'].apply(lambda x: np.zeros((30, 14)))","metadata":{"execution":{"iopub.status.busy":"2021-10-14T19:30:24.393118Z","iopub.execute_input":"2021-10-14T19:30:24.393496Z","iopub.status.idle":"2021-10-14T19:30:35.881601Z","shell.execute_reply.started":"2021-10-14T19:30:24.393461Z","shell.execute_reply":"2021-10-14T19:30:35.880664Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"not_num_cols=['stock_id','target','time_id','row_id','image_matrix','corr_stock']                        \nfeatures_num=[col for col in df_train.columns if col not in not_num_cols]\n\ntarget=df_train['target']\ngroups=df_train['time_id']\nfeatures_cat=['stock_id']\nfeature_img='image_matrix'\n\nimg_rows=30\nimg_cols=14\nchannnels=1","metadata":{"execution":{"iopub.status.busy":"2021-10-14T19:30:35.883009Z","iopub.execute_input":"2021-10-14T19:30:35.883339Z","iopub.status.idle":"2021-10-14T19:30:35.888739Z","shell.execute_reply.started":"2021-10-14T19:30:35.883304Z","shell.execute_reply":"2021-10-14T19:30:35.887966Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"no_of_unique_cat = max(df_train['stock_id'])+1\nembedding_size = 16\n\ndef build_model(len_cat,len_num,img_rows,img_cols,channnels):    \n\n    \n    img_input = keras.Input(shape=(img_rows,img_cols,channnels))\n    cat_input = keras.Input(shape=(len_cat,), name='stock_id')\n    num_input = keras.Input(shape=(len_num,), name='num_data')    \n    \n    \n    stock_embedded = tf.keras.layers.Embedding(no_of_unique_cat, embedding_size, \n                                           input_length=1, name='stock_embedding')(cat_input)\n    \n    stock_flattened = tf.keras.layers.Reshape(target_shape=(embedding_size,))(stock_embedded) \n        \n        \n    ########################################################\n    \n    cnn2d = tf.keras.layers.Conv2D(16, kernel_size=3,padding='same', activation='relu')(img_input)\n    cnn2d = tf.keras.layers.Conv2D(32, kernel_size=4,padding='same', activation='relu')(cnn2d)\n    cnn2d = tf.keras.layers.GlobalMaxPooling2D()(cnn2d)\n    cnn2d = tf.keras.layers.BatchNormalization()(cnn2d)\n    cnn2d = tf.keras.layers.Dropout(0.5)(cnn2d)\n    cnn2d = tf.keras.layers.Flatten()(cnn2d)  \n   \n\n    concat1 = tf.keras.layers.Concatenate(name='concatenate1')([stock_flattened,num_input])\n    concat2 = tf.keras.layers.Concatenate(name='concatenate2')([stock_flattened,cnn2d]) \n    dense_c2 = tf.keras.layers.Dense(32, activation='swish',name='dense_c2')(concat2)  \n    concat3 = tf.keras.layers.Concatenate(name='concatenate3')([concat1,dense_c2])\n        \n    dense_1 = tf.keras.layers.Dense(128, activation='swish')(concat3) \n    dense_2 = tf.keras.layers.Dense(64, activation='swish')(dense_1)\n    dense_3 = tf.keras.layers.Dense(32, activation='swish')(dense_2)\n    dense_4 = tf.keras.layers.Dense(16, activation='swish')(dense_3)  \n    output=tf.keras.layers.Dense(1)(dense_4)\n    \n    concat4 = tf.keras.layers.Concatenate(name='concatenate_lgb')([concat1,dense_c2])\n                               \n    model = tf.keras.Model(inputs=[img_input,cat_input,num_input], outputs=output)   \n        \n    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate = 0.001),  loss=metric_rmspe_exp)    \n                                   \n\n    return model","metadata":{"execution":{"iopub.status.busy":"2021-10-14T19:30:35.892111Z","iopub.execute_input":"2021-10-14T19:30:35.892669Z","iopub.status.idle":"2021-10-14T19:30:35.958992Z","shell.execute_reply.started":"2021-10-14T19:30:35.892634Z","shell.execute_reply":"2021-10-14T19:30:35.957919Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fold_score_k=[]\noof_k = np.zeros(df_train.shape[0])\npreds_k = np.zeros(df_test.shape[0])\n\ntarget= np.log1p(df_train['target']*1000)\n\ntest_img =np.stack(df_test[feature_img].values, axis=0).reshape(len(df_test),img_rows,img_cols,channnels)\n   \n\n\ngkf = GroupKFold(N_FOLDS)\n\nfor fold, (train_idx, valid_idx) in enumerate(gkf.split(df_train,target,groups=groups)):\n\n    y_train, y_valid = target.iloc[train_idx], target.iloc[valid_idx]\n    \n    X_train_num = df_train[features_num].iloc[train_idx]\n    X_train_cat = df_train[features_cat].iloc[train_idx]\n    X_train_img =np.stack(df_train[feature_img].iloc[train_idx].values, axis=0).reshape(len(train_idx),img_rows,img_cols,channnels)\n    \n    X_valid_num = df_train[features_num].iloc[valid_idx]\n    X_valid_cat = df_train[features_cat].iloc[valid_idx]\n    X_valid_img =np.stack(df_train[feature_img].iloc[valid_idx].values, axis=0).reshape(len(valid_idx),img_rows,img_cols,channnels)\n   \n    \n    \n    \n    model = build_model(len(features_cat),len(features_num),img_rows,img_cols,channnels)\n    \n    rlr = tf.keras.callbacks.ReduceLROnPlateau(monitor = 'val_loss', factor = 0.5, patience = 6, \n                                                 verbose = 0, min_delta = 1e-10, mode = 'min')    \n  \n\n    es = tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', min_delta = 1e-10, patience = 16,\n                                              mode = 'min',  baseline = None, \n                                              restore_best_weights = True, verbose = 0)  \n    \n    model.fit([X_train_img,X_train_cat,X_train_num], y_train, \n                sample_weight = 1/np.square(y_train),\n                validation_data = ([X_valid_img,X_valid_cat,X_valid_num], y_valid), \n                epochs = 300, \n                batch_size = batch_size,               \n              callbacks = [rlr, es], \n              verbose = 1)      \n    \n    \n    \n    oof_k[valid_idx] = model.predict([X_valid_img,X_valid_cat,X_valid_num]).ravel().clip(1.0e-5,1e10)\n    preds_k += model.predict([test_img,df_test[features_cat], df_test[features_num]]).ravel().clip(1.0e-5,1e10)/N_FOLDS\n    print('***********************')\n    rmspe_score = round(rmspe(np.exp(y_valid.values)-1, np.exp(oof_k[valid_idx])-1),6)\n    print(f\"#Keras_fold_{fold}_Original rmse: {rmspe_score}\")\n    fold_score_k.append(rmspe_score)\n    \n    del model","metadata":{"execution":{"iopub.status.busy":"2021-10-14T19:30:35.96072Z","iopub.execute_input":"2021-10-14T19:30:35.961209Z","iopub.status.idle":"2021-10-14T19:30:37.409799Z","shell.execute_reply.started":"2021-10-14T19:30:35.961165Z","shell.execute_reply":"2021-10-14T19:30:37.408376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target=(np.exp(target)-1)/1000\noof_k=((np.exp(oof_k)-1)/1000).clip(1.0e-5,1e10)\n\nprint('#',round(rmspe(df_train['target'].values,oof_k),6))\nprint('# Fold Scores_keras: ', fold_score_k)","metadata":{"execution":{"iopub.status.busy":"2021-10-14T19:30:37.410947Z","iopub.status.idle":"2021-10-14T19:30:37.411674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds_k=((np.exp(preds_k)-1)/1000).clip(1.0e-5,1e10)\ndf_test['target']=preds_k\n\nprint(df_test[['row_id','log_return1_realized_volatility_T0','target']].head(3))\ndf_test[['row_id', 'target']].to_csv('submission.csv',index = False)","metadata":{"execution":{"iopub.status.busy":"2021-10-14T19:30:37.412806Z","iopub.status.idle":"2021-10-14T19:30:37.413519Z"},"trusted":true},"execution_count":null,"outputs":[]}]}