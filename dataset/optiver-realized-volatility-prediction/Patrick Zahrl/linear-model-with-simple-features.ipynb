{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nfrom typing import List, Iterator, Callable\nfrom time import time\nimport glob\nimport os\nfrom functools import lru_cache\nfrom sklearn.metrics import r2_score\nfrom tqdm import tqdm\nfrom sklearn.linear_model import LinearRegression, Ridge\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.base import BaseEstimator\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom utils import DataLoader\n\nimport statsmodels.api as sm","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-11-06T20:10:49.869515Z","iopub.execute_input":"2021-11-06T20:10:49.869846Z","iopub.status.idle":"2021-11-06T20:10:49.877246Z","shell.execute_reply.started":"2021-11-06T20:10:49.869796Z","shell.execute_reply":"2021-11-06T20:10:49.87638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MAX_TIMEIDS = None\nMAX_STOCKS = None\n# MAX_TIMEIDS = 10\n# MAX_STOCKS = 30\ndata_loader = DataLoader(MAX_TIMEIDS, False)","metadata":{"execution":{"iopub.status.busy":"2021-11-06T20:16:44.337036Z","iopub.execute_input":"2021-11-06T20:16:44.337885Z","iopub.status.idle":"2021-11-06T20:16:44.343639Z","shell.execute_reply.started":"2021-11-06T20:16:44.337845Z","shell.execute_reply":"2021-11-06T20:16:44.342644Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = data_loader.read_labels(test=False)\nif MAX_STOCKS and MAX_STOCKS < len(train_df[\"stock_id\"].unique()):\n    print(\"Sampling {} stocks for training.\".format(MAX_STOCKS))\n    sampled_stocks = np.random.choice(train_df[\"stock_id\"].unique(), size=MAX_STOCKS, replace=False)\n    train_df = train_df[train_df[\"stock_id\"].isin(sampled_stocks)]","metadata":{"execution":{"iopub.status.busy":"2021-11-06T20:19:14.91175Z","iopub.execute_input":"2021-11-06T20:19:14.912017Z","iopub.status.idle":"2021-11-06T20:19:15.107113Z","shell.execute_reply.started":"2021-11-06T20:19:14.911991Z","shell.execute_reply":"2021-11-06T20:19:15.10592Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# helpers to extract features from book\ndef calc_realized_volatility(log_returns: pd.Series) -> float:\n    return np.sqrt(np.sum(log_returns ** 2))\n\n\ndef aggregate_book_for_stock_and_time_id(book_time_slice: pd.DataFrame) -> pd.Series:\n    volatilities = dict()\n    for i in range(1, 4):\n        log_returns = np.log(book_time_slice[f\"WAP{i}\"]).diff()\n        volatilities[f\"volatility_{i}\"] = calc_realized_volatility(log_returns)\n        \n        log_returns = np.log(book_time_slice.tail(100)[f\"WAP{i}\"]).diff()\n        volatilities[f\"volatility_tail_{i}\"] = calc_realized_volatility(log_returns)\n\n    return pd.Series(volatilities)\n  \n    \n\ndef aggregate_book_for_stock(df: pd.DataFrame) -> pd.DataFrame:\n    df[\"WAP1\"] = (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']) / (df['bid_size1']+ df['ask_size1'])\n    df[\"WAP2\"] = (df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2']) / (df['bid_size2']+ df['ask_size2'])\n    df[\"WAP3\"] = (\n        (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']) +\n        (df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2'])\n    ) / (\n        (df['bid_size1']+ df['ask_size1']) + (df['bid_size2']+ df['ask_size2'])\n    )\n    \n    start = time()\n    \n    g = df.groupby([\"time_id\"], as_index=False)\n    result = g.apply(aggregate_book_for_stock_and_time_id)\n#     print(time() - start, \"seconds to aggregate book per time_id\")\n    return result\n\ndef get_book_features_for_all_stocks(stock_ids, test):\n    features_all_books = list()\n    print(\"Getting book features for {} stocks.\".format(len(stock_ids)))\n    for stock_id in tqdm(stock_ids):\n        book_df = data_loader.read_book(stock_id, test=test)\n        features_this_book = aggregate_book_for_stock(book_df)\n        features_this_book.insert(0, \"stock_id\", stock_id)\n        features_all_books.append(features_this_book)\n\n    features_all_books = pd.concat(features_all_books)\n    return features_all_books\n    \n","metadata":{"execution":{"iopub.status.busy":"2021-11-06T20:19:18.670843Z","iopub.execute_input":"2021-11-06T20:19:18.671128Z","iopub.status.idle":"2021-11-06T20:19:18.686538Z","shell.execute_reply.started":"2021-11-06T20:19:18.671101Z","shell.execute_reply":"2021-11-06T20:19:18.68556Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get book features per stock\nbook_features_train = get_book_features_for_all_stocks(train_df[\"stock_id\"].unique(), False)","metadata":{"execution":{"iopub.status.busy":"2021-11-06T20:19:19.913994Z","iopub.execute_input":"2021-11-06T20:19:19.9145Z","iopub.status.idle":"2021-11-06T20:19:33.685003Z","shell.execute_reply.started":"2021-11-06T20:19:19.914454Z","shell.execute_reply":"2021-11-06T20:19:33.683706Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_enriched = train_df.merge(book_features_train, how=\"inner\")\ntrain_enriched.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-06T20:19:35.186189Z","iopub.execute_input":"2021-11-06T20:19:35.186466Z","iopub.status.idle":"2021-11-06T20:19:35.216447Z","shell.execute_reply.started":"2021-11-06T20:19:35.186438Z","shell.execute_reply":"2021-11-06T20:19:35.215808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_X(feature_df):\n    stock_dummies = pd.get_dummies(feature_df[[\"stock_id\"]])\n    return pd.concat([\n        feature_df[[\"volatility_1\"]],\n        stock_dummies\n    ], \n        axis=1\n    )","metadata":{"execution":{"iopub.status.busy":"2021-11-06T20:23:51.957815Z","iopub.execute_input":"2021-11-06T20:23:51.958192Z","iopub.status.idle":"2021-11-06T20:23:51.963353Z","shell.execute_reply.started":"2021-11-06T20:23:51.958142Z","shell.execute_reply":"2021-11-06T20:23:51.962676Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Cross Validate Model","metadata":{}},{"cell_type":"code","source":"def rmspe(y_true: pd.Series, y_pred: pd.Series) -> float:\n    return  (np.sqrt(np.mean(np.square((y_true - y_pred) / y_true))))  \n\ndef score_model_on_rmspe(model, X: pd.DataFrame, y_true: pd.Series) -> float:\n    y_pred = model.predict(X)\n    return rmspe(y_true, y_pred)","metadata":{"execution":{"iopub.status.busy":"2021-11-06T20:23:56.064214Z","iopub.execute_input":"2021-11-06T20:23:56.064559Z","iopub.status.idle":"2021-11-06T20:23:56.070505Z","shell.execute_reply.started":"2021-11-06T20:23:56.064522Z","shell.execute_reply":"2021-11-06T20:23:56.069632Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = make_X(train_enriched)\ny = train_enriched[\"target\"]","metadata":{"execution":{"iopub.status.busy":"2021-11-06T20:23:57.489209Z","iopub.execute_input":"2021-11-06T20:23:57.489746Z","iopub.status.idle":"2021-11-06T20:23:57.504305Z","shell.execute_reply.started":"2021-11-06T20:23:57.489703Z","shell.execute_reply":"2021-11-06T20:23:57.502815Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = LinearRegression(fit_intercept=False)\n\nn_splits = 10\ncv_results = cross_val_score(model, X_train, y, scoring=score_model_on_rmspe, cv=n_splits)\nprint(\"RMSPE from cross-validation:\")\nprint(round(cv_results.mean(), 3),  \" +/-\", round(1.96 * cv_results.std(), 2))","metadata":{"execution":{"iopub.status.busy":"2021-11-06T20:23:59.259048Z","iopub.execute_input":"2021-11-06T20:23:59.259577Z","iopub.status.idle":"2021-11-06T20:23:59.314611Z","shell.execute_reply.started":"2021-11-06T20:23:59.259535Z","shell.execute_reply":"2021-11-06T20:23:59.313767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Final Fit and Predict","metadata":{}},{"cell_type":"code","source":"model.fit(X_train, y)","metadata":{"execution":{"iopub.status.busy":"2021-11-06T20:24:09.895886Z","iopub.execute_input":"2021-11-06T20:24:09.896171Z","iopub.status.idle":"2021-11-06T20:24:09.905399Z","shell.execute_reply.started":"2021-11-06T20:24:09.896143Z","shell.execute_reply":"2021-11-06T20:24:09.904759Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Prepare features for test\ntest_df = data_loader.read_labels(test=True)\nbook_features_test = get_book_features_for_all_stocks(test_df[\"stock_id\"].unique(), test=True)\ntest_enriched = test_df.merge(book_features_test, how=\"left\") # to fail early I don't use inner join","metadata":{"execution":{"iopub.status.busy":"2021-11-06T20:24:10.835589Z","iopub.execute_input":"2021-11-06T20:24:10.836498Z","iopub.status.idle":"2021-11-06T20:24:10.880192Z","shell.execute_reply.started":"2021-11-06T20:24:10.836458Z","shell.execute_reply":"2021-11-06T20:24:10.879166Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Missing value prevalence in X_test (will be mean-imputed):\")\nna_pct = test_enriched.isnull().mean()\nprint(na_pct[na_pct>0])\ntest_enriched = test_enriched.fillna(test_enriched.mean(numeric_only=True))","metadata":{"execution":{"iopub.status.busy":"2021-11-06T20:11:32.298999Z","iopub.execute_input":"2021-11-06T20:11:32.299664Z","iopub.status.idle":"2021-11-06T20:11:32.312816Z","shell.execute_reply.started":"2021-11-06T20:11:32.299618Z","shell.execute_reply":"2021-11-06T20:11:32.311919Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test = make_X(test_enriched)\ny_predicted = model.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2021-11-06T20:11:40.961309Z","iopub.execute_input":"2021-11-06T20:11:40.962255Z","iopub.status.idle":"2021-11-06T20:11:40.969697Z","shell.execute_reply.started":"2021-11-06T20:11:40.962217Z","shell.execute_reply":"2021-11-06T20:11:40.968777Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prepare_submissions(test_df, y_predicted):\n    # TODO: ensure that y_predicted is aligned with test_df\n    test_df[\"target\"] = y_predicted\n    \n    submission_df = test_df[[\"row_id\", \"target\"]].fillna(test_df[\"target\"].mean())\n    submission_df.to_csv('submission.csv',index = False)\n    print(\"submissions prepared - done\")\n    \nprepare_submissions(test_df, y_predicted)","metadata":{"execution":{"iopub.status.busy":"2021-11-06T20:11:43.195392Z","iopub.execute_input":"2021-11-06T20:11:43.195892Z","iopub.status.idle":"2021-11-06T20:11:43.207679Z","shell.execute_reply.started":"2021-11-06T20:11:43.195842Z","shell.execute_reply":"2021-11-06T20:11:43.206796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n#     stock_ids = test_df[\"stock_id\"].unique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # simple utils\n# def flatten_hierachical_column_index(df: pd.DataFrame) -> None:\n#     df.columns = ['_'.join(col).strip() for col in df.columns.values]\n    \n# def columns_are_primary_key(df: pd.DataFrame, colnames: List[str]) -> bool:\n#     if not df[colnames].duplicated().any():\n#         print(f\"({', '.join(colnames)}) is a primary key\")\n#         return True","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n \n    \n# def aggregate_trades_for_stock_and_time_id(trade_time_slice: pd.DataFrame) -> pd.Series:\n#     trade_time_slice = trade_time_slice.assign(\n#         trade_price_1 = lambda df: df.price\n#     )\n#     volatilities = dict()\n#     for i in range(1, 2):\n#         log_returns = np.log(trade_time_slice[f\"trade_price_{i}\"]).diff()\n#         volatilities[f\"trade_volatility_{i}\"] = calc_realized_volatility(log_returns)\n\n#     return pd.Series(volatilities)\n    \n# def aggregate_trades_for_stock(df: pd.DataFrame) -> pd.DataFrame:\n#     g = df.groupby([\"time_id\"], as_index=False)\n#     result = g.apply(aggregate_trades_for_stock_and_time_id)\n#     result[\"stock_id\"] = df[\"stock_id\"].iloc[0]\n#     return result\n    \n    \n# # @lru_cache(maxsize=MAX_CACHE)\n# def get_stock_x_time_df_by_stock_id(stock_id: int, test: bool, verbose: bool =True) -> pd.DataFrame:\n#     book_df = df_from_parquet_for_stock(stock_id, test=test, verbose=verbose)\n#     book_agg = aggregate_book_for_stock(book_df)\n    \n#     trade_df = df_from_parquet_for_stock(stock_id, test=test, book_or_trade=\"trade\", verbose=verbose)\n#     trade_agg = aggregate_trades_for_stock(trade_df)\n    \n#     joined = book_agg.merge(trade_agg, on=[\"time_id\", \"stock_id\"], how=\"left\")\n\n#     return joined\n\n\n# def iter_stock_x_time_dfs(stock_ids: int, test: bool, verbose: bool =False) -> Iterator[pd.DataFrame]:\n#     for stock_id in tqdm(stock_ids):\n#         yield get_stock_x_time_df_by_stock_id(stock_id, test=test, verbose=verbose)\n        \n        \n    \n# get_stock_x_time_df_by_stock_id(37, test=False, verbose=False)    \n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Compile Training Data","metadata":{"execution":{"iopub.status.busy":"2021-09-18T20:00:49.348225Z","iopub.execute_input":"2021-09-18T20:00:49.348564Z","iopub.status.idle":"2021-09-18T20:00:49.354003Z","shell.execute_reply.started":"2021-09-18T20:00:49.348521Z","shell.execute_reply":"2021-09-18T20:00:49.352613Z"}}},{"cell_type":"code","source":"# train_df = pd.read_csv(os.path.join(INPUT_DIR, 'train.csv'))\n# # \n# columns_are_primary_key(train_df, [\"stock_id\", \"time_id\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # compile training df\n# possible_stocks = train_df[\"stock_id\"].unique()\n# stock_ids = possible_stocks[:MAX_STOCKS_FOR_TRAINING]\n\n# prepared_training_dfs = list()\n# for i, stock_x_time_df in enumerate(iter_stock_x_time_dfs(stock_ids, test=False)):\n#     if i >= MAX_STOCKS_FOR_TRAINING:\n#         break     \n#     prepared_training_dfs.append(stock_x_time_df)\n\n    \n# prepared_training_df = pd.concat(prepared_training_dfs)\n# prepared_training_df = prepared_training_df.merge(train_df, on=[\"time_id\", \"stock_id\"], how=\"inner\")\n# prepared_training_df.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # check if there are any null values\n# null_count = prepared_training_df.isnull().sum()\n# null_count[null_count > 0].to_frame(\"null count\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# prepared_training_df = prepared_training_df.fillna(prepared_training_df.mean())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Look at simple OLS regression stats for selected features","metadata":{}},{"cell_type":"code","source":"# feature_names = [\"volatility_1\", \"volatility_tail_1\"]\n# y = prepared_training_df[\"target\"]\n# X = prepared_training_df[feature_names]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# lm = sm.OLS(endog=y, exog=X)\n# lm = lm.fit()\n# lm.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Cross validate model using target metric","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# simple_model = LinearRegression(fit_intercept=False)\n\n# n_splits = 10\n# cv_results = cross_val_score(simple_model, X, y, scoring=score_model_on_rmspe, cv=n_splits)\n# print(\"OLS on target - RMSPE from cross-validation:\")\n# print(round(cv_results.mean(), 3),  \" +/-\", round(1.96 * cv_results.std(), 2))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pf = PolynomialFeatures(interaction_only=True, include_bias=False)\n# X_plus = pf.fit_transform(X)\n# cv_results = cross_val_score(simple_model, X_plus, y, scoring=score_model_on_rmspe, cv=n_splits)\n# print(\"OLS on target - RMSPE from cross-validation:\")\n# print(round(cv_results.mean(), 3),  \" +/-\", round(1.96 * cv_results.std(), 2))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Predict on test set and submit","metadata":{}},{"cell_type":"code","source":"# def predict_and_prepare_submission(fitted_model) -> pd.DataFrame:\n    \n#     test_df = pd.read_csv('../input/optiver-realized-volatility-prediction/test.csv')\n#     stock_ids = test_df[\"stock_id\"].unique()\n# #     print(test_df)\n#     predictions = list()\n#     for stock_x_time_df in iter_stock_x_time_dfs(stock_ids, test=True):\n# #         print(stock_x_time_df)\n#         X = stock_x_time_df[feature_names].copy()\n#         X.fillna(0, inplace=True)\n#         y_pred = fitted_model.predict(X)\n#         stock_x_time_df[\"target\"] = y_pred\n        \n#         predictions.append(stock_x_time_df[[\"stock_id\", \"time_id\", \"target\"]])\n        \n#     predictions = pd.concat(predictions)\n#     test_df = test_df.merge(predictions, on=[\"time_id\", \"stock_id\"], how=\"left\")\n#     return test_df[[\"row_id\", \"target\"]].fillna(test_df[\"target\"].mean())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# simple_model.fit(X, y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# submission_df = predict_and_prepare_submission(simple_model)\n# submission_df.to_csv('submission.csv',index = False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}