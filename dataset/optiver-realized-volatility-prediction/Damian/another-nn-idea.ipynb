{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\nimport os\nimport tensorflow as tf\nimport numpy as np\nimport pandas as pd\nfrom joblib import Parallel, delayed\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Files and directories\ntrainFile        = '../input/optiver-realized-volatility-prediction/train.csv'\nbookTrainFolder  = '../input/optiver-realized-volatility-prediction/book_train.parquet/'\nbookTestFolder   = '../input/optiver-realized-volatility-prediction/book_test.parquet/'\ntradeTrainFolder = '../input/optiver-realized-volatility-prediction/trade_train.parquet/'\ntradeTestFolder  = '../input/optiver-realized-volatility-prediction/trade_test.parquet/'\n\n# Load train.csv -- contains the target values\ntrain = pd.read_csv(trainFile)\n\ndef buildData(trainFolders = True):\n\n    if trainFolders:\n        bookFolder  = bookTrainFolder\n        tradeFolder = tradeTrainFolder\n    else:\n        bookFolder  = bookTestFolder\n        tradeFolder = tradeTestFolder\n\n    def buildDataForSingleStockId(stock_id_folder, stock_id_bookFolder, stock_id_tradeFolder):\n\n        # Read in book data for curr stock_id\n        bookData = pd.read_parquet(stock_id_bookFolder + stock_id_folder)\n\n        # create dataframe with all the time_id in the current bookData and all the possible seconds_in_bucket 0-599\n        time_id = []\n        seconds_in_bucket = []\n\n        for x in bookData['time_id'].unique():\n            for y in range(600):\n                time_id.append(x)\n                seconds_in_bucket.append(y)\n\n        allTimes = pd.DataFrame({'time_id' : time_id, 'seconds_in_bucket' : seconds_in_bucket})\n\n        # make sure that all the seconds are accounted for\n        bookData = bookData.merge(allTimes, on=['time_id', 'seconds_in_bucket'], how='outer').sort_values(by=['time_id', 'seconds_in_bucket'])\n        bookData['time_id_cp'] = bookData['time_id']\n        \n        # forward fill and backfill - time_id_cp will get removed\n        bookData = bookData.groupby(['time_id_cp']).fillna(method='ffill').fillna(method='bfill').reset_index(drop=True)\n\n        # calculate the weighted average price\n        bookData['wap1']                = (bookData['bid_price1'] * bookData['ask_size1'] + bookData['ask_price1'] * bookData['bid_size1']) / (bookData['ask_size1'] + bookData['bid_size1'])\n        bookData['wap2']                = (bookData['bid_price2'] * bookData['ask_size2'] + bookData['ask_price2'] * bookData['bid_size2']) / (bookData['ask_size2'] + bookData['bid_size2'])\n\n        # Bid-Ask Spreads\n        bookData['ask1_bid1_spread']    = bookData['ask_price1'] / bookData['bid_price1'] - 1\n\n        # aggregate the data over windows of interval seconds\n        # for each interval calculate\n        #      - log(high/low) using wap1 and wap2\n        def aggregateBookData(interval):\n            df               = bookData.copy()\n            df['interval']   = df['seconds_in_bucket'] // interval\n\n            df_agg = df.groupby(['time_id', 'interval']).agg(\n                                wap1_log_high_low       = pd.NamedAgg(column='wap1',                aggfunc=lambda x: np.log(np.max(x) / np.min(x))),\n                                wap2_log_high_low       = pd.NamedAgg(column='wap2',                aggfunc=lambda x: np.log(np.max(x) / np.min(x))),\n                                ask1_bid1_spread_avg    = pd.NamedAgg(column='ask1_bid1_spread',    aggfunc=np.mean)       ).reset_index()\n\n            df_wide    = pd.pivot_table(df_agg, values=['wap1_log_high_low', 'wap2_log_high_low', 'ask1_bid1_spread_avg'], \n                                             index='time_id', columns='interval').reset_index().fillna(0)\n            df_wide.columns = ['_'.join(str(e) for e in col) for col in df_wide.columns]\n            df_wide = df_wide.add_suffix(f'_{interval}s_wide').rename(columns={f'time_id__{interval}s_wide' : 'time_id'})\n\n            return df_wide\n        \n        finalBookData = aggregateBookData(10)\n\n        # add row_id\n        finalBookData['row_id']   = stock_id_folder.split('=')[1] + '-' + finalBookData['time_id'].astype(str)\n\n        return finalBookData.drop(columns='time_id').fillna(0)\n\n    result = Parallel(n_jobs=-1, verbose=10)(delayed(buildDataForSingleStockId)(curr_stock_id_folder, bookFolder, tradeFolder) for curr_stock_id_folder in os.listdir(bookFolder))\n\n    return pd.concat(result, ignore_index=True).fillna(0)\n\ntrainData_aggregations = buildData(trainFolders = True)\ntestData_aggregations  = buildData(trainFolders = False)\n\n# add stock_id\ntrainData_aggregations['stock_id'] = trainData_aggregations['row_id'].apply(lambda x: x.split('-')[0]).astype(int)\ntestData_aggregations['stock_id']  = testData_aggregations['row_id'].apply(lambda x: x.split('-')[0]).astype(int)\n\n# add time_id\ntrainData_aggregations['time_id'] = trainData_aggregations['row_id'].apply(lambda x: x.split('-')[1]).astype(int)\ntestData_aggregations['time_id']  = testData_aggregations['row_id'].apply(lambda x: x.split('-')[1]).astype(int)\n\n# Merge in targets to trainDataWith_s_wide_vars\ntrain['row_id'] = train['stock_id'].astype(str) + '-' + train['time_id'].astype(str)\ntrainData       = pd.merge(train[['row_id','target']], trainData_aggregations, on = ['row_id'], how = 'inner').fillna(0)\n\n\n# train a nnet with the variable \n#    'wap1_log_high_low'\n#    'wap2_log_high_low'\n#    'ask1_bid1_spread_avg'\n\ns_wide_vars         = [x for x in trainData.columns.tolist() if any(y in x for y in ['s_wide'])]\ntrainData_np_s_wide = trainData[s_wide_vars].to_numpy()\ntestData_np_s_wide  = testData_aggregations[s_wide_vars].to_numpy()\n\ntrainData_np_stock_id = trainData['stock_id'].to_numpy()\ntestData_np_stock_id  = testData_aggregations['stock_id'].to_numpy()\n\n\ntrainTarget_np = trainData['target'].to_numpy()\n\n# custom objective for the keras model\ndef rmspe_tf(y_true, y_pred):\n    return tf.sqrt(tf.reduce_mean(tf.square((y_true - y_pred) / y_true)))\n\n# Network structure\ndef buildModel(input_shape):\n    model = tf.keras.models.Sequential()\n    model.add(tf.keras.layers.InputLayer(input_shape))\n    model.add(tf.keras.layers.Dense(input_shape[0] // 3, activation = 'relu'))\n    model.add(tf.keras.layers.Dense(1, activation = 'relu', name='s_wide_features'))\n    \n    model.compile(loss      = rmspe_tf,\n                  optimizer = tf.keras.optimizers.Adam(learning_rate=0.00001),\n                  metrics   = [rmspe_tf])\n    \n    return model\n\n# embedding from https://www.kaggle.com/tatudoug/stock-embedding-ffnn-features-of-the-best-lgbm/notebook\n# but there are several other notebooks that do the same thing\n# Network structure\n\nmaxStockId = trainData['stock_id'].max()\n\ndef buildModelWithStockEmdedding(input_shape):\n    \n    # stock_id\n    stock_id_input = tf.keras.layers.Input(shape=(1,), name='stock_id')\n    stock_embedding = tf.keras.layers.Embedding(input_dim = maxStockId + 1, output_dim=2, input_length=1, name='stock_embedding')(stock_id_input)\n    flatEmbedding   = tf.keras.layers.Flatten()(stock_embedding)\n    \n    #s_wide\n    s_wide_input   = tf.keras.layers.Input(shape=input_shape, name='s_wide_input')\n    layer1          = tf.keras.layers.Dense(input_shape[0] // 3, activation = 'relu')(s_wide_input)\n    \n    # concat\n    concatLayer     = tf.keras.layers.Concatenate()([flatEmbedding, layer1])\n    \n    # end\n    x               = tf.keras.layers.Dense(10, activation = 'relu')(concatLayer)\n    lastLayer       = tf.keras.layers.Dense(1, activation = 'relu', name='s_wide_features')(x)\n    \n    model = tf.keras.Model(inputs = [stock_id_input, s_wide_input],\n                           outputs = lastLayer)\n        \n    model.compile(loss      = rmspe_tf,\n                  optimizer = tf.keras.optimizers.Adam(learning_rate=0.00001),\n                  metrics   = [rmspe_tf])\n    \n    return model\n\n# 5 fold model / cv\nnum_epochs      = 200\nbatchSize       = 1024\nk               = 5\nnum_val_samples = len(trainData_np_s_wide) // k\n\nnp.random.seed(9999)\nshuffledIndx      = np.random.permutation(len(trainData_np_s_wide))\n\ntestDataRowId       = testData_aggregations['row_id']\nfinalPrediction     = np.zeros(len(testDataRowId))\n\nnp.random.seed(9999)\ntf.random.set_seed(9999)\n\nfor i in range(k):\n    #i = 2\n    print(f'Processing fold #{i}')\n    val_data              = [trainData_np_stock_id[shuffledIndx[i * num_val_samples : (i + 1) * num_val_samples]], trainData_np_s_wide[shuffledIndx[i * num_val_samples : (i + 1) * num_val_samples]]]\n    val_targets           = trainTarget_np[shuffledIndx[i * num_val_samples : (i + 1) * num_val_samples]]\n    partial_train_data    = [np.concatenate([trainData_np_stock_id[shuffledIndx[:i * num_val_samples]],\n                                            trainData_np_stock_id[shuffledIndx[(i + 1) * num_val_samples:]] ],\n                                            axis=0),\n                             np.concatenate([trainData_np_s_wide[shuffledIndx[:i * num_val_samples]],\n                                            trainData_np_s_wide[shuffledIndx[(i + 1) * num_val_samples:]] ],\n                                            axis=0)]\n    partial_train_targets = np.concatenate([trainTarget_np[shuffledIndx[:i * num_val_samples]],\n                                            trainTarget_np[shuffledIndx[(i + 1) * num_val_samples:]] ],\n                                            axis=0)\n    \n    model = buildModelWithStockEmdedding(trainData_np_s_wide.shape[1:])\n    history = model.fit(x=partial_train_data, y=partial_train_targets,\n                        validation_data = (val_data, val_targets),\n                        epochs          = num_epochs, \n                        batch_size      = batchSize, \n                        verbose         = 1)\n\n    finalPrediction += model.predict([testData_np_stock_id, testData_np_s_wide]).reshape(-1) / k\n\n# write submission.csv\npd.DataFrame({'row_id': testDataRowId, 'target': finalPrediction}).to_csv('submission.csv', index=False)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]}]}