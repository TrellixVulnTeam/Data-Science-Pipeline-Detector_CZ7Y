{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Homebrew Kernel\nThings to check\n- Effects of clustering（stock_id）\n- Effects of tau\n- Effects of KNN + tau\n\nThis kernel is the English translation of this kernel.\nIf you prefer the Japanese version, please see here.  \n[https://www.kaggle.com/satoshimts/tau-vs-no-tau-vs-knn-tau]","metadata":{}},{"cell_type":"markdown","source":"# Parameters used\n\nThis is a very detailed part that should be tuned every time if possible, but I'll leave it at that.\n\n\nseed = 29  \nparams = {  \n    'learning_rate': 0.1,          \n    'lambda_l1': 2,  \n    'lambda_l2': 7,  \n    'num_leaves': 800,  \n    'min_sum_hessian_in_leaf': 20,  \n    'feature_fraction': 0.8,  \n    'feature_fraction_bynode': 0.8,  \n    'bagging_fraction': 0.9,  \n    'bagging_freq': 42,  \n    'min_data_in_leaf': 700,  \n    'max_depth': 4,  \n    'seed': seed,  \n    'feature_fraction_seed': seed,  \n    'bagging_seed': seed,  \n    'drop_seed': seed,  \n    'data_random_seed': seed,  \n    'objective': 'rmse',  \n    'boosting': 'gbdt',  \n    'verbosity': -1,  \n    'n_jobs': -1,  \n}   ","metadata":{}},{"cell_type":"markdown","source":"# Baseline\n","metadata":{"tags":[]}},{"cell_type":"code","source":"data_dir = '../input/optiver-realized-volatility-prediction/'","metadata":{"execution":{"iopub.status.busy":"2021-09-17T00:48:27.360079Z","iopub.execute_input":"2021-09-17T00:48:27.36139Z","iopub.status.idle":"2021-09-17T00:48:27.384073Z","shell.execute_reply.started":"2021-09-17T00:48:27.361248Z","shell.execute_reply":"2021-09-17T00:48:27.38318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport glob\nfrom joblib import Parallel, delayed\nimport pandas as pd\nimport numpy as np\nimport scipy as sc\nfrom sklearn.model_selection import KFold\nimport lightgbm as lgb\nimport warnings\nfrom sklearn.cluster import KMeans\nwarnings.filterwarnings('ignore')\npd.set_option('max_columns', 300)","metadata":{"execution":{"iopub.status.busy":"2021-09-17T00:48:27.385988Z","iopub.execute_input":"2021-09-17T00:48:27.386236Z","iopub.status.idle":"2021-09-17T00:48:30.084371Z","shell.execute_reply.started":"2021-09-17T00:48:27.386209Z","shell.execute_reply":"2021-09-17T00:48:30.083457Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to calculate first WAP\ndef calc_wap1(df):\n    wap = (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']) / (df['bid_size1'] + df['ask_size1'])\n    return wap\n\n# Function to calculate second WAP\ndef calc_wap2(df):\n    wap = (df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2']) / (df['bid_size2'] + df['ask_size2'])\n    return wap\n\n# Function to calculate the log of the return\n# Remember that logb(x / y) = logb(x) - logb(y)\ndef log_return(series):\n    return np.log(series).diff()\n\n# Calculate the realized volatility\ndef realized_volatility(series):\n    return np.sqrt(np.sum(series**2))\n\n# Function to count unique elements of a series\ndef count_unique(series):\n    return len(np.unique(series))","metadata":{"execution":{"iopub.status.busy":"2021-09-17T00:48:30.085782Z","iopub.execute_input":"2021-09-17T00:48:30.086195Z","iopub.status.idle":"2021-09-17T00:48:30.097094Z","shell.execute_reply.started":"2021-09-17T00:48:30.086146Z","shell.execute_reply":"2021-09-17T00:48:30.096078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to read our base train and test set\ndef read_train_test():\n    train = pd.read_csv(data_dir + '/train.csv')\n    test = pd.read_csv(data_dir + '/test.csv')\n    # Create a key to merge with book and trade data\n    train['row_id'] = train['stock_id'].astype(str) + '-' + train['time_id'].astype(str)\n    test['row_id'] = test['stock_id'].astype(str) + '-' + test['time_id'].astype(str)\n    print(f'Our training set has {train.shape[0]} rows')\n    return train, test","metadata":{"execution":{"iopub.status.busy":"2021-09-17T00:48:30.098475Z","iopub.execute_input":"2021-09-17T00:48:30.098842Z","iopub.status.idle":"2021-09-17T00:48:30.109294Z","shell.execute_reply.started":"2021-09-17T00:48:30.09881Z","shell.execute_reply":"2021-09-17T00:48:30.108108Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to preprocess book data (for each stock id)\ndef book_preprocessor(file_path):\n    df = pd.read_parquet(file_path)\n    # Calculate Wap\n    df['wap1'] = calc_wap1(df)\n    df['wap2'] = calc_wap2(df)\n    # Calculate log returns\n    df['log_return1'] = df.groupby(['time_id'])['wap1'].apply(log_return)\n    df['log_return2'] = df.groupby(['time_id'])['wap2'].apply(log_return)\n    # Calculate wap balance\n    df['wap_balance'] = abs(df['wap1'] - df['wap2'])\n    # Calculate spread\n    df['price_spread'] = (df['ask_price1'] - df['bid_price1']) / ((df['ask_price1'] + df['bid_price1']) / 2)\n    df['price_spread2'] = (df['ask_price2'] - df['bid_price2']) / ((df['ask_price2'] + df['bid_price2']) / 2)\n    df['bid_spread'] = df['bid_price1'] - df['bid_price2']\n    df['ask_spread'] = df['ask_price1'] - df['ask_price2']\n    df[\"bid_ask_spread\"] = abs(df['bid_spread'] - df['ask_spread'])\n    df['total_volume'] = (df['ask_size1'] + df['ask_size2']) + (df['bid_size1'] + df['bid_size2'])\n    df['volume_imbalance'] = abs((df['ask_size1'] + df['ask_size2']) - (df['bid_size1'] + df['bid_size2']))\n    \n    # Dict for aggregations\n    create_feature_dict = {\n        'wap1': [np.sum, np.mean, np.std],\n        'wap2': [np.sum, np.mean, np.std],\n        'log_return1': [np.sum, realized_volatility, np.mean, np.std],\n        'log_return2': [np.sum, realized_volatility, np.mean, np.std],\n        'wap_balance': [np.sum, np.mean, np.std],\n        'price_spread':[np.sum, np.mean, np.std],\n        'price_spread2':[np.sum, np.mean, np.std],\n        'bid_spread':[np.sum, np.mean, np.std],\n        'ask_spread':[np.sum, np.mean, np.std],\n        'total_volume':[np.sum, np.mean, np.std],\n        'volume_imbalance':[np.sum, np.mean, np.std],\n        \"bid_ask_spread\":[np.sum, np.mean, np.std],\n    }\n    \n    # Function to get group stats for different windows (seconds in bucket)\n    def get_stats_window(seconds_in_bucket, add_suffix = False):\n        # Group by the window\n        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(create_feature_dict).reset_index()\n        # Rename columns joining suffix\n        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n        # Add a suffix to differentiate windows\n        if add_suffix:\n            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n        return df_feature\n    \n    # Get the stats for different windows\n    # きっと100秒刻みはいい結果を得れなかったのだろう\n    df_feature = get_stats_window(seconds_in_bucket = 0, add_suffix = False)\n    df_feature_450 = get_stats_window(seconds_in_bucket = 450, add_suffix = True)\n#     df_feature_500 = get_stats_window(seconds_in_bucket = 500, add_suffix = True)\n#     df_feature_400 = get_stats_window(seconds_in_bucket = 400, add_suffix = True)\n    df_feature_300 = get_stats_window(seconds_in_bucket = 300, add_suffix = True)\n#     df_feature_200 = get_stats_window(seconds_in_bucket = 200, add_suffix = True)\n    df_feature_150 = get_stats_window(seconds_in_bucket = 150, add_suffix = True)\n\n    # Merge all\n    df_feature = df_feature.merge(df_feature_450, how = 'left', left_on = 'time_id_', right_on = 'time_id__450')\n    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n#     df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n    df_feature = df_feature.merge(df_feature_150, how = 'left', left_on = 'time_id_', right_on = 'time_id__150')\n#     df_feature = df_feature.merge(df_feature_100, how = 'left', left_on = 'time_id_', right_on = 'time_id__100')\n    # Drop unnecesary time_ids\n    df_feature.drop(['time_id__450', 'time_id__300', 'time_id__150'], axis = 1, inplace = True)\n    \n    \n    # Create row_id so we can merge\n    stock_id = file_path.split('=')[1]\n    df_feature['row_id'] = df_feature['time_id_'].apply(lambda x: f'{stock_id}-{x}')\n    df_feature.drop(['time_id_'], axis = 1, inplace = True)\n    return df_feature\n","metadata":{"execution":{"iopub.status.busy":"2021-09-17T00:48:30.112275Z","iopub.execute_input":"2021-09-17T00:48:30.112556Z","iopub.status.idle":"2021-09-17T00:48:30.140459Z","shell.execute_reply.started":"2021-09-17T00:48:30.112525Z","shell.execute_reply":"2021-09-17T00:48:30.139477Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to preprocess trade data (for each stock id)\ndef trade_preprocessor(file_path):\n    df = pd.read_parquet(file_path)\n    df['log_return'] = df.groupby('time_id')['price'].apply(log_return)\n    \n    # Dict for aggregations\n    create_feature_dict = {\n        'log_return':[realized_volatility],\n        'seconds_in_bucket':[count_unique],\n        'size':[np.sum, realized_volatility, np.mean, np.std, np.max, np.min],\n        'order_count':[np.mean,np.sum,np.max],\n    }\n    \n    # Function to get group stats for different windows (seconds in bucket)\n    def get_stats_window(seconds_in_bucket, add_suffix = False):\n        # Group by the window\n        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(create_feature_dict).reset_index()\n        # Rename columns joining suffix\n        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n        # Add a suffix to differentiate windows\n        if add_suffix:\n            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n        return df_feature\n    \n\n    # Get the stats for different windows\n    df_feature = get_stats_window(seconds_in_bucket = 0, add_suffix = False)\n    df_feature_450 = get_stats_window(seconds_in_bucket = 450, add_suffix = True)\n#     df_feature_500 = get_stats_window(seconds_in_bucket = 500, add_suffix = True)\n#     df_feature_400 = get_stats_window(seconds_in_bucket = 400, add_suffix = True)\n    df_feature_300 = get_stats_window(seconds_in_bucket = 300, add_suffix = True)\n#     df_feature_200 = get_stats_window(seconds_in_bucket = 200, add_suffix = True)\n    df_feature_150 = get_stats_window(seconds_in_bucket = 150, add_suffix = True)\n    \n    #\n    # ここかスコアに相当影響してると記載してる\n    # なんでこれが効いてるかはいまのところ不明\n    # おって調べることにする。\n    #\n    \n    def tendency(price, vol):    \n        df_diff = np.diff(price)\n        # 差分を動いた後の価格で割って*100してる\n        # 変動額を変動後の価格で割ることで比率にしている(小さい価格だとこの値はおおきくなる)\n        val = (df_diff/price[1:])*100\n        # それにvolをかけてるので変動比率に大きさをかけるのでこの値が大きいと値の比率が大きく動いたことになる\n        power = np.sum(val*vol[1:])\n        return(power)\n    \n    lis = []\n    # time_idに対応するdfを抜き出す。\n    # time_idに統計指標なのでリークしている\n    \n    for n_time_id in df['time_id'].unique():\n        df_id = df[df['time_id'] == n_time_id]\n        \n        # powerって呼ばれる指標を得る。\n        tendencyV = tendency(df_id['price'].values, df_id['size'].values)      \n        \n        # 平均以上のpriceの合計と平均以下のpriceの合計値\n        # 全くもっていらないデータにしか見えない。\n        # 外れ値の影響引く気がするし\n        f_max = np.sum(df_id['price'].values > np.mean(df_id['price'].values))\n        f_min = np.sum(df_id['price'].values < np.mean(df_id['price'].values))\n        \n        #\n        # 正の差分の合計値と負の差分の合計値\n        # いるのかこれ\n        df_max =  np.sum(np.diff(df_id['price'].values) > 0)\n        df_min =  np.sum(np.diff(df_id['price'].values) < 0)\n        \n        \n        # 偏差の中央値\n        abs_diff = np.median(np.abs(df_id['price'].values - np.mean(df_id['price'].values)))  \n        # 価格の二乗の平均値\n        energy = np.mean(df_id['price'].values**2)\n        # 第3-第１\n        iqr_p = np.percentile(df_id['price'].values,75) - np.percentile(df_id['price'].values,25)\n        \n        # sizeに対してもうえと同様のこと\n        abs_diff_v = np.median(np.abs(df_id['size'].values - np.mean(df_id['size'].values)))        \n        energy_v = np.sum(df_id['size'].values**2)\n        iqr_p_v = np.percentile(df_id['size'].values,75) - np.percentile(df_id['size'].values,25)\n        \n        lis.append({'time_id':n_time_id,'tendency':tendencyV,'f_max':f_max,'f_min':f_min,'df_max':df_max,'df_min':df_min,\n                   'abs_diff':abs_diff,'energy':energy,'iqr_p':iqr_p,'abs_diff_v':abs_diff_v,'energy_v':energy_v,'iqr_p_v':iqr_p_v})\n        \n    df_lr = pd.DataFrame(lis)\n        \n   \n    df_feature = df_feature.merge(df_lr, how = 'left', left_on = 'time_id_', right_on = 'time_id')\n    \n    # Merge all\n    df_feature = df_feature.merge(df_feature_450, how = 'left', left_on = 'time_id_', right_on = 'time_id__450')\n    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n#     df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n    df_feature = df_feature.merge(df_feature_150, how = 'left', left_on = 'time_id_', right_on = 'time_id__150')\n#     df_feature = df_feature.merge(df_feature_100, how = 'left', left_on = 'time_id_', right_on = 'time_id__100')\n    # Drop unnecesary time_ids\n    df_feature.drop(['time_id__450', 'time_id__300', 'time_id__150','time_id'], axis = 1, inplace = True)\n    \n    \n    \n    df_feature = df_feature.add_prefix('trade_')\n    stock_id = file_path.split('=')[1]\n    df_feature['row_id'] = df_feature['trade_time_id_'].apply(lambda x:f'{stock_id}-{x}')\n    df_feature.drop(['trade_time_id_'], axis = 1, inplace = True)\n    return df_feature\n","metadata":{"execution":{"iopub.status.busy":"2021-09-17T00:48:30.142456Z","iopub.execute_input":"2021-09-17T00:48:30.143074Z","iopub.status.idle":"2021-09-17T00:48:30.172738Z","shell.execute_reply.started":"2021-09-17T00:48:30.143021Z","shell.execute_reply":"2021-09-17T00:48:30.171627Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to get group stats for the stock_id and time_id\ndef get_time_stock(df):\n    # Get realized volatility columns\n    vol_cols = ['log_return1_realized_volatility', 'log_return2_realized_volatility', 'log_return1_realized_volatility_450', 'log_return2_realized_volatility_450', \n                'log_return1_realized_volatility_300', 'log_return2_realized_volatility_300', 'log_return1_realized_volatility_150', 'log_return2_realized_volatility_150', \n                'trade_log_return_realized_volatility', 'trade_log_return_realized_volatility_450', 'trade_log_return_realized_volatility_300', 'trade_log_return_realized_volatility_150']\n#     vol_cols = ['log_return1_realized_volatility', 'log_return2_realized_volatility',\n#                 'log_return1_realized_volatility_600', 'log_return2_realized_volatility_600', \n#                 'log_return1_realized_volatility_400', 'log_return2_realized_volatility_400',\n# #                 'log_return1_realized_volatility_300', 'log_return2_realized_volatility_300', \n#                 'log_return1_realized_volatility_200', 'log_return2_realized_volatility_200',\n# #                 'log_return1_realized_volatility_100', 'log_return2_realized_volatility_100', \n#                 'trade_log_return_realized_volatility',\n#                 'trade_log_return_realized_volatility_600', \n#                 'trade_log_return_realized_volatility_400',\n# #                 'trade_log_return_realized_volatility_300',\n# #                 'trade_log_return_realized_volatility_100',\n#                 'trade_log_return_realized_volatility_200']\n\n    # Group by the stock id\n    df_stock_id = df.groupby(['stock_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n    # Rename columns joining suffix\n    df_stock_id.columns = ['_'.join(col) for col in df_stock_id.columns]\n    df_stock_id = df_stock_id.add_suffix('_' + 'stock')\n\n    # Group by the stock id\n    df_time_id = df.groupby(['time_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n    # Rename columns joining suffix\n    df_time_id.columns = ['_'.join(col) for col in df_time_id.columns]\n    df_time_id = df_time_id.add_suffix('_' + 'time')\n    \n    # Merge with original dataframe\n    df = df.merge(df_stock_id, how = 'left', left_on = ['stock_id'], right_on = ['stock_id__stock'])\n    df = df.merge(df_time_id, how = 'left', left_on = ['time_id'], right_on = ['time_id__time'])\n    df.drop(['stock_id__stock', 'time_id__time'], axis = 1, inplace = True)\n    return df\n    \n# Funtion to make preprocessing function in parallel (for each stock id)\ndef preprocessor(list_stock_ids, is_train = True):\n    \n    # Parrallel for loop\n    def for_joblib(stock_id):\n        # Train\n        if is_train:\n            file_path_book = data_dir + \"book_train.parquet/stock_id=\" + str(stock_id)\n            file_path_trade = data_dir + \"trade_train.parquet/stock_id=\" + str(stock_id)\n        # Test\n        else:\n            file_path_book = data_dir + \"book_test.parquet/stock_id=\" + str(stock_id)\n            file_path_trade = data_dir + \"trade_test.parquet/stock_id=\" + str(stock_id)\n    \n        # Preprocess book and trade data and merge them\n        df_tmp = pd.merge(book_preprocessor(file_path_book), trade_preprocessor(file_path_trade), on = 'row_id', how = 'left')\n        \n        # Return the merge dataframe\n        return df_tmp\n    \n    # Use parallel api to call paralle for loop\n    df = Parallel(n_jobs = -1, verbose = 1)(delayed(for_joblib)(stock_id) for stock_id in list_stock_ids)\n    # Concatenate all the dataframes that return from Parallel\n    df = pd.concat(df, ignore_index = True)\n    return df\n\n# Function to calculate the root mean squared percentage error\ndef rmspe(y_true, y_pred):\n    return np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\n\n# Function to early stop with root mean squared percentage error\ndef feval_rmspe(y_pred, lgb_train):\n    y_true = lgb_train.get_label()\n    return 'RMSPE', rmspe(y_true, y_pred), False","metadata":{"execution":{"iopub.status.busy":"2021-09-17T00:48:30.174507Z","iopub.execute_input":"2021-09-17T00:48:30.175157Z","iopub.status.idle":"2021-09-17T00:48:30.194935Z","shell.execute_reply.started":"2021-09-17T00:48:30.175109Z","shell.execute_reply":"2021-09-17T00:48:30.193839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to calculate the root mean squared percentage error\ndef rmspe(y_true, y_pred):\n    return np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\n\n# Function to early stop with root mean squared percentage error\ndef feval_rmspe(y_pred, lgb_train):\n    y_true = lgb_train.get_label()\n    return 'RMSPE', rmspe(y_true, y_pred), False","metadata":{"execution":{"iopub.status.busy":"2021-09-17T00:48:30.199201Z","iopub.execute_input":"2021-09-17T00:48:30.199496Z","iopub.status.idle":"2021-09-17T00:48:30.212515Z","shell.execute_reply.started":"2021-09-17T00:48:30.199464Z","shell.execute_reply":"2021-09-17T00:48:30.211582Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 重要度解析\ndef calc_model_importance(model, feature_names=None, importance_type='gain'):\n    importance_df = pd.DataFrame(model.feature_importance(importance_type=importance_type),\n                                 index=feature_names,\n                                 columns=['importance']).sort_values('importance')\n    return importance_df","metadata":{"execution":{"iopub.status.busy":"2021-09-17T00:48:30.213898Z","iopub.execute_input":"2021-09-17T00:48:30.214165Z","iopub.status.idle":"2021-09-17T00:48:30.228084Z","shell.execute_reply.started":"2021-09-17T00:48:30.214132Z","shell.execute_reply":"2021-09-17T00:48:30.227425Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def calc_mean_importance(importance_df_list):\n    mean_importance = np.mean(\n        np.array([df['importance'].values for df in importance_df_list]), axis=0)\n    mean_df = importance_df_list[0].copy()\n    mean_df['importance'] = mean_importance\n    \n    return mean_df","metadata":{"execution":{"iopub.status.busy":"2021-09-17T00:48:30.229459Z","iopub.execute_input":"2021-09-17T00:48:30.229905Z","iopub.status.idle":"2021-09-17T00:48:30.241482Z","shell.execute_reply.started":"2021-09-17T00:48:30.229871Z","shell.execute_reply":"2021-09-17T00:48:30.24026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 画像保存用\nimport matplotlib.pyplot as plt\ndef plot_importance(importance_df, title='',\n                    save_filepath=None, figsize=(8, 12)):\n    fig, ax = plt.subplots(figsize=figsize)\n    importance_df.plot.barh(ax=ax)\n    if title:\n        plt.title(title)\n    plt.tight_layout()\n    if save_filepath is None:\n        plt.show()\n    else:\n        plt.savefig(save_filepath)\n    plt.close()","metadata":{"execution":{"iopub.status.busy":"2021-09-17T00:48:30.2429Z","iopub.execute_input":"2021-09-17T00:48:30.243145Z","iopub.status.idle":"2021-09-17T00:48:30.254476Z","shell.execute_reply.started":"2021-09-17T00:48:30.243117Z","shell.execute_reply":"2021-09-17T00:48:30.253531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Read train and test\ntrain, test = read_train_test()\n\n# Get unique stock ids \ntrain_stock_ids = train['stock_id'].unique()\n# Preprocess them using Parallel and our single stock id functions\ntrain_ = preprocessor(train_stock_ids, is_train = True)\ntrain = train.merge(train_, on = ['row_id'], how = 'left')\n\n# Get unique stock ids \ntest_stock_ids = test['stock_id'].unique()\n# Preprocess them using Parallel and our single stock id functions\ntest_ = preprocessor(test_stock_ids, is_train = False)\ntest = test.merge(test_, on = ['row_id'], how = 'left')\n\n# Get group stats of time_id and stock_id\ntrain = get_time_stock(train)\ntest = get_time_stock(test)","metadata":{"execution":{"iopub.status.busy":"2021-09-17T00:48:30.255698Z","iopub.execute_input":"2021-09-17T00:48:30.255938Z","iopub.status.idle":"2021-09-17T01:10:18.387348Z","shell.execute_reply.started":"2021-09-17T00:48:30.255911Z","shell.execute_reply":"2021-09-17T01:10:18.38655Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.shape","metadata":{"execution":{"iopub.status.busy":"2021-09-17T01:10:18.388863Z","iopub.execute_input":"2021-09-17T01:10:18.389872Z","iopub.status.idle":"2021-09-17T01:10:18.396434Z","shell.execute_reply.started":"2021-09-17T01:10:18.389831Z","shell.execute_reply":"2021-09-17T01:10:18.395654Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.to_pickle(train,'train(307)_notau_noKNN.pkl')\npd.to_pickle(test,'test(307)_notau_noKNN.pkl')","metadata":{"tags":[],"execution":{"iopub.status.busy":"2021-09-17T01:10:18.400014Z","iopub.execute_input":"2021-09-17T01:10:18.400291Z","iopub.status.idle":"2021-09-17T01:10:19.996371Z","shell.execute_reply.started":"2021-09-17T01:10:18.40026Z","shell.execute_reply":"2021-09-17T01:10:19.995435Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# About the clustering index\n\nIn this clustering, time_id is used as an instance, and the values of the objective variable for each stock_id are correlated with each stock_id.  \nIn other words, stock_ids with similar correlation values mean that the distributions of the objective variables are similar.  \nThe idea that the distributions are similar means that new features can be obtained by taking the average of the indicators written for each cluster and substituting the indicators of each cluster for each time_id as features.  \n\nNo tau feature in tihs model","metadata":{}},{"cell_type":"code","source":"train = pd.read_pickle('./train(307)_notau_noKNN.pkl')\ntest = pd.read_pickle('test(307)_notau_noKNN.pkl')\ntrain.shape","metadata":{"execution":{"iopub.status.busy":"2021-09-17T01:10:19.997471Z","iopub.execute_input":"2021-09-17T01:10:19.997732Z","iopub.status.idle":"2021-09-17T01:10:21.321032Z","shell.execute_reply.started":"2021-09-17T01:10:19.997701Z","shell.execute_reply":"2021-09-17T01:10:21.320074Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.shape","metadata":{"execution":{"iopub.status.busy":"2021-09-17T01:10:21.322481Z","iopub.execute_input":"2021-09-17T01:10:21.323302Z","iopub.status.idle":"2021-09-17T01:10:21.329339Z","shell.execute_reply.started":"2021-09-17T01:10:21.323256Z","shell.execute_reply":"2021-09-17T01:10:21.328535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# making agg features\n\n# time_id毎のstockid毎のtarget変数の一覧\ntrain_p = pd.read_csv(data_dir + '/train.csv')\ntrain_p = train_p.pivot(index='time_id', columns='stock_id', values='target')\n\n# \ncorr = train_p.corr()\n\n# stick_idの相関係数のindex、つまりstockid\nids = corr.index\n\n# 7に関しては調べる(シルエット図とかで)\n# time_idをインスタンスとした時のstock_id相関係数\nkmeans = KMeans(n_clusters=7, random_state=0).fit(corr.values)\n# print(kmeans.labels_)\n\n\n# lにrange(7)と等しいクラスタを撮った時のstock_idを格納してる\nl = []\nfor n in range(7):\n    l.append ( [ (x-1) for x in ( (ids+1)*(kmeans.labels_ == n)) if x > 0] )\n    \n\nmat = []\nmatTest = []\n\nn = 0\nfor ind in l:\n    print(ind)\n    # stock_idが指定されてるクラスタと同じもの(全体サンプル)を引いてくる\n    newDf = train.loc[train['stock_id'].isin(ind) ]\n    # time_id毎に平均値を取る(異なるstock_idでもクラスタが同じなもの同士の平均値になる)\n    newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n    \n    #\n    # stock_idそのものにいみがなくなったので注意！！！！！！！！！！！！！！！！！！！！！！\n    # stock_idをクタスタidに変更してその後にc1?っていうのをつけてる\n    #\n    newDf.loc[:,'stock_id'] = str(n)+'c1'\n    mat.append ( newDf )\n    \n    newDf = test.loc[test['stock_id'].isin(ind) ]    \n    newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n    newDf.loc[:,'stock_id'] = str(n)+'c1'\n    matTest.append ( newDf )\n    \n    n+=1\n    \nmat1 = pd.concat(mat).reset_index()\nmat1.drop(columns=['target'],inplace=True)\n\nmat2 = pd.concat(matTest).reset_index()","metadata":{"execution":{"iopub.status.busy":"2021-09-17T01:10:21.330768Z","iopub.execute_input":"2021-09-17T01:10:21.331597Z","iopub.status.idle":"2021-09-17T01:10:24.041102Z","shell.execute_reply.started":"2021-09-17T01:10:21.33156Z","shell.execute_reply":"2021-09-17T01:10:24.040252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"matTest = []\nmat = []\nkmeans = []","metadata":{"execution":{"iopub.status.busy":"2021-09-17T01:10:24.042259Z","iopub.execute_input":"2021-09-17T01:10:24.042491Z","iopub.status.idle":"2021-09-17T01:10:24.047536Z","shell.execute_reply.started":"2021-09-17T01:10:24.042465Z","shell.execute_reply":"2021-09-17T01:10:24.046686Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#mat2 #= mat1.pivot(index='time_id', columns='stock_idmat2\n# 何でそんなことしたんや笑\nmat2 = pd.concat([mat2,mat1.loc[mat1.time_id==5]])","metadata":{"execution":{"iopub.status.busy":"2021-09-17T01:10:24.048933Z","iopub.execute_input":"2021-09-17T01:10:24.049309Z","iopub.status.idle":"2021-09-17T01:10:24.075043Z","shell.execute_reply.started":"2021-09-17T01:10:24.049278Z","shell.execute_reply":"2021-09-17T01:10:24.074046Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# この書き方便利、覚えておく\n\nmat1 = mat1.pivot(index='time_id', columns='stock_id')\nmat1.columns = [\"_\".join(x) for x in mat1.columns.ravel()]\nmat1.reset_index(inplace=True)\n\nmat2 = mat2.pivot(index='time_id', columns='stock_id')\nmat2.columns = [\"_\".join(x) for x in mat2.columns.ravel()]\nmat2.reset_index(inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-09-17T01:10:24.076467Z","iopub.execute_input":"2021-09-17T01:10:24.076727Z","iopub.status.idle":"2021-09-17T01:10:24.299819Z","shell.execute_reply.started":"2021-09-17T01:10:24.076697Z","shell.execute_reply":"2021-09-17T01:10:24.298964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# クラスタリングを行った結果の使う特徴量だけ抜き出してきてる（10/314）\n# 2.5にかんしてはクラスタの中があまりに小さいので除く\n# なぜ、この特徴量にしたのかは不明のため、追加実験が必要\n# log_return2を削った理由はわからん（おそらく相関が似ちゃうから）\n\nnnn = ['time_id',\n     'log_return1_realized_volatility_0c1',\n     'log_return1_realized_volatility_1c1',     \n     'log_return1_realized_volatility_2c1',\n     'log_return1_realized_volatility_3c1',     \n     'log_return1_realized_volatility_4c1',\n     'total_volume_mean_0c1',\n     'total_volume_mean_1c1', \n     'total_volume_mean_2c1',\n     'total_volume_mean_3c1', \n     'total_volume_mean_4c1',\n     'trade_size_mean_0c1',\n     'trade_size_mean_1c1', \n     'trade_size_mean_2c1',\n     'trade_size_mean_3c1', \n     'trade_size_mean_4c1',\n     'trade_order_count_mean_0c1',\n     'trade_order_count_mean_1c1',\n     'trade_order_count_mean_2c1',\n     'trade_order_count_mean_3c1',\n     'trade_order_count_mean_4c1',      \n     'price_spread_mean_0c1',\n     'price_spread_mean_1c1',\n     'price_spread_mean_2c1',\n     'price_spread_mean_3c1',\n     'price_spread_mean_4c1',   \n     'bid_spread_mean_0c1',\n     'bid_spread_mean_1c1',\n     'bid_spread_mean_2c1',\n     'bid_spread_mean_3c1',\n     'bid_spread_mean_4c1',       \n     'ask_spread_mean_0c1',\n     'ask_spread_mean_1c1',\n     'ask_spread_mean_2c1',\n     'ask_spread_mean_3c1',\n     'ask_spread_mean_4c1',   \n     'volume_imbalance_mean_0c1',\n     'volume_imbalance_mean_1c1',\n     'volume_imbalance_mean_2c1',\n     'volume_imbalance_mean_3c1',\n     'volume_imbalance_mean_4c1',       \n     'bid_ask_spread_mean_0c1',\n     'bid_ask_spread_mean_1c1',\n     'bid_ask_spread_mean_2c1',\n     'bid_ask_spread_mean_3c1',\n     'bid_ask_spread_mean_4c1',\n] ","metadata":{"execution":{"iopub.status.busy":"2021-09-17T01:10:24.301425Z","iopub.execute_input":"2021-09-17T01:10:24.301668Z","iopub.status.idle":"2021-09-17T01:10:24.309698Z","shell.execute_reply.started":"2021-09-17T01:10:24.301626Z","shell.execute_reply":"2021-09-17T01:10:24.308727Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# trainとくっつける\ntrain = pd.merge(train,mat1[nnn],how='left',on='time_id')","metadata":{"execution":{"iopub.status.busy":"2021-09-17T01:10:24.310916Z","iopub.execute_input":"2021-09-17T01:10:24.311147Z","iopub.status.idle":"2021-09-17T01:10:48.122212Z","shell.execute_reply.started":"2021-09-17T01:10:24.311121Z","shell.execute_reply":"2021-09-17T01:10:48.12137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.shape","metadata":{"execution":{"iopub.status.busy":"2021-09-17T01:10:48.123606Z","iopub.execute_input":"2021-09-17T01:10:48.123974Z","iopub.status.idle":"2021-09-17T01:10:48.129503Z","shell.execute_reply.started":"2021-09-17T01:10:48.123933Z","shell.execute_reply":"2021-09-17T01:10:48.12889Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-17T01:10:48.130902Z","iopub.execute_input":"2021-09-17T01:10:48.131404Z","iopub.status.idle":"2021-09-17T01:10:48.389219Z","shell.execute_reply.started":"2021-09-17T01:10:48.131347Z","shell.execute_reply":"2021-09-17T01:10:48.387913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-17T01:10:48.390533Z","iopub.execute_input":"2021-09-17T01:10:48.390892Z","iopub.status.idle":"2021-09-17T01:10:48.613787Z","shell.execute_reply.started":"2021-09-17T01:10:48.390851Z","shell.execute_reply":"2021-09-17T01:10:48.61278Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = pd.merge(test,mat2[nnn],how='left',on='time_id')","metadata":{"execution":{"iopub.status.busy":"2021-09-17T01:10:48.615351Z","iopub.execute_input":"2021-09-17T01:10:48.615614Z","iopub.status.idle":"2021-09-17T01:10:48.63467Z","shell.execute_reply.started":"2021-09-17T01:10:48.61558Z","shell.execute_reply":"2021-09-17T01:10:48.633766Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.shape","metadata":{"execution":{"iopub.status.busy":"2021-09-17T01:10:48.637519Z","iopub.execute_input":"2021-09-17T01:10:48.638204Z","iopub.status.idle":"2021-09-17T01:10:48.644138Z","shell.execute_reply.started":"2021-09-17T01:10:48.638156Z","shell.execute_reply":"2021-09-17T01:10:48.643303Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.shape","metadata":{"execution":{"iopub.status.busy":"2021-09-17T01:10:48.645298Z","iopub.execute_input":"2021-09-17T01:10:48.645609Z","iopub.status.idle":"2021-09-17T01:10:48.657728Z","shell.execute_reply.started":"2021-09-17T01:10:48.645568Z","shell.execute_reply":"2021-09-17T01:10:48.656756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-17T01:10:48.659089Z","iopub.execute_input":"2021-09-17T01:10:48.659405Z","iopub.status.idle":"2021-09-17T01:10:48.864074Z","shell.execute_reply.started":"2021-09-17T01:10:48.659364Z","shell.execute_reply":"2021-09-17T01:10:48.862947Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split features and target\nx = train.drop(['row_id', 'target', 'time_id'], axis = 1)\ny = train['target']\nx_test = test.drop(['row_id', 'time_id'], axis = 1)\n# Transform stock id to a numeric value\nx['stock_id'] = x['stock_id'].astype(int)\nx_test['stock_id'] = x_test['stock_id'].astype(int)\n\n# Create out of folds array\noof_predictions = np.zeros(x.shape[0])\n# Create test array to store predictions\ntest_predictions = np.zeros(x_test.shape[0])","metadata":{"execution":{"iopub.status.busy":"2021-09-17T01:10:48.865537Z","iopub.execute_input":"2021-09-17T01:10:48.865882Z","iopub.status.idle":"2021-09-17T01:10:49.949031Z","shell.execute_reply.started":"2021-09-17T01:10:48.865841Z","shell.execute_reply":"2021-09-17T01:10:49.948173Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seed = 29\nparams = {\n    'learning_rate': 0.1,        \n    'lambda_l1': 2,\n    'lambda_l2': 7,\n    'num_leaves': 800,\n    'min_sum_hessian_in_leaf': 20,\n    'feature_fraction': 0.8,\n    'feature_fraction_bynode': 0.8,\n    'bagging_fraction': 0.9,\n    'bagging_freq': 42,\n    'min_data_in_leaf': 700,\n    'max_depth': 4,\n    'seed': seed,\n    'feature_fraction_seed': seed,\n    'bagging_seed': seed,\n    'drop_seed': seed,\n    'data_random_seed': seed,\n    'objective': 'rmse',\n    'boosting': 'gbdt',\n    'verbosity': -1,\n    'n_jobs': -1,\n} ","metadata":{"execution":{"iopub.status.busy":"2021-09-17T01:10:49.951254Z","iopub.execute_input":"2021-09-17T01:10:49.951714Z","iopub.status.idle":"2021-09-17T01:10:49.95776Z","shell.execute_reply.started":"2021-09-17T01:10:49.951665Z","shell.execute_reply":"2021-09-17T01:10:49.956925Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import GroupKFold\ngain_importance_list = []\nsplit_importance_list = []\ngroup = train['time_id']\nkf = GroupKFold(n_splits=5)\n# Iterate through each fold\nfor fold, (trn_ind, val_ind) in enumerate(kf.split(x, groups=group)):\n    print(f'Training fold {fold + 1}')\n    x_train, x_val = x.iloc[trn_ind], x.iloc[val_ind]\n    y_train, y_val = y.iloc[trn_ind], y.iloc[val_ind]\n    # Root mean squared percentage error weights\n    train_weights = 1 / np.square(y_train)\n    val_weights = 1 / np.square(y_val)\n    train_dataset = lgb.Dataset(x_train, y_train, weight = train_weights)\n    val_dataset = lgb.Dataset(x_val, y_val, weight = val_weights)\n    model = lgb.train(params = params, \n                      train_set = train_dataset, \n                      categorical_feature = ['stock_id'],\n                      valid_sets = [train_dataset, val_dataset], \n                      num_boost_round = 5000, \n                      early_stopping_rounds = 30, \n                      verbose_eval = 100,\n                      feval = feval_rmspe)\n\n    # この書き方することで、全データをOOfにしてrmspeが求められる、\n    # 覚えておいた方がいい\n    oof_predictions[val_ind] = model.predict(x_val)\n    # Predict the test set\n    test_predictions += model.predict(x_test) / 5\n\n    feature_names = x_train.columns.values.tolist()\n    gain_importance_df = calc_model_importance(\n        model, feature_names=feature_names, importance_type='gain')\n    gain_importance_list.append(gain_importance_df)\n\n    split_importance_df = calc_model_importance(\n        model, feature_names=feature_names, importance_type='split')\n    split_importance_list.append(split_importance_df)\n\nrmspe_score = rmspe(y, oof_predictions)\nprint(f'Our out of folds RMSPE is {rmspe_score}')","metadata":{"execution":{"iopub.status.busy":"2021-09-17T01:10:49.958932Z","iopub.execute_input":"2021-09-17T01:10:49.959162Z","iopub.status.idle":"2021-09-17T01:17:07.279917Z","shell.execute_reply.started":"2021-09-17T01:10:49.95912Z","shell.execute_reply":"2021-09-17T01:17:07.279201Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mean_gain_df = calc_mean_importance(gain_importance_list)\nmean_gain_df = mean_gain_df.reset_index().rename(columns={'index': 'feature_names'})\nmean_gain_df.to_csv('gain_importance_mean groupkfold 352　KNN notau.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-09-17T01:17:07.281329Z","iopub.execute_input":"2021-09-17T01:17:07.281827Z","iopub.status.idle":"2021-09-17T01:17:07.295859Z","shell.execute_reply.started":"2021-09-17T01:17:07.28179Z","shell.execute_reply":"2021-09-17T01:17:07.294762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test['target'] = test_predictions\ntest[['row_id', 'target']].to_csv('submission.csv',index = False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Discussion\nCV : 0.22013  \nNo clustering CV : 0.2253  \n\nBy adding the clustering index, the cv was significantly improved.\nHowever, when I looked at the feature importance, it didn't seem to have that much of an effect.  \nI wonder if it just happens to match the results of tuning, or if it goes up as well...\n\n~So, what happens if we cluster by time_id?  \nIf you see a correlation between time_id 1 and 2, and you give a statistical measure, the image is  \nNothing time_id  \nMoving time_id  \nand the actual evaluation will be nice and coherent, right?  \nIt's going to be a lot of code to write, but it's worth it~.   \n\nI can't do clustering because target is a per time_id indicator: ....  \nI don't see any features other than stock_id that would make sense for clustering, so I can’t.","metadata":{}},{"cell_type":"markdown","source":"# About TAU\n\ntau1 : train['size_tau'] = np.sqrt( 1/ train['trade_seconds_in_bucket_count_unique'] )\nAs for feature_importance, it is a simple statistical measure and has no effect on the LGBM model.  \nThe feature_importance does not work at all.  \n\n\ntau2 : \ntrain['size_tau2'] = np.sqrt( 1/ train['trade_order_count_sum'] )  \ntrain['size_tau2_150'] = np.sqrt( 0.75/ train['trade_order_count_sum'] )  \nAs for the \"size_tau2_sum\", I think it will change because it is an indicator per unit of time.  \nThe value of the total number of trades after 150 seconds is converted to 600 seconds, so it should be the estimated total number of trades.  \nIf this is different from the actual total number of trades, it should be an indicator that the volatility has moved too much and is different from the estimate.  \n\nWhy do we need to care about the number of trades...?  \nI think you can just use log_return or something...\n\n## Without clustering indicator feature","metadata":{}},{"cell_type":"code","source":"train = pd.read_pickle('./train(307)_notau_noKNN.pkl')\ntest = pd.read_pickle('./test(307)_notau_noKNN.pkl')","metadata":{"execution":{"iopub.status.busy":"2021-09-17T01:17:07.297241Z","iopub.execute_input":"2021-09-17T01:17:07.297563Z","iopub.status.idle":"2021-09-17T01:17:09.529339Z","shell.execute_reply.started":"2021-09-17T01:17:07.297524Z","shell.execute_reply":"2021-09-17T01:17:09.527772Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# こっちは価格が動くどうこうではなくて、取引回数の合計の値\n# 単位をそろえるのと、傾向がなんとなくみえるのか！\n\ntrain['size_tau2'] = np.sqrt( 1/ train['trade_order_count_sum'] )\ntest['size_tau2'] = np.sqrt( 1/ test['trade_order_count_sum'] )\ntrain['size_tau2_450'] = np.sqrt( 0.25/ train['trade_order_count_sum'] )\ntest['size_tau2_450'] = np.sqrt( 0.25/ test['trade_order_count_sum'] )\ntrain['size_tau2_300'] = np.sqrt( 0.5/ train['trade_order_count_sum'] )\ntest['size_tau2_300'] = np.sqrt( 0.5/ test['trade_order_count_sum'] )\ntrain['size_tau2_150'] = np.sqrt( 0.75/ train['trade_order_count_sum'] )\ntest['size_tau2_150'] = np.sqrt( 0.75/ test['trade_order_count_sum'] )\n\n# delta tau\ntrain['size_tau2_600-450'] = train['size_tau2_450'] - train['size_tau2']\ntest['size_tau2_600-450'] = test['size_tau2_450'] - test['size_tau2']\ntrain['size_tau2_600-300'] = train['size_tau2_300'] - train['size_tau2']\ntest['size_tau2_600-300'] = test['size_tau2_300'] - test['size_tau2']\ntrain['size_tau2_600-150'] = train['size_tau2_150'] - train['size_tau2']\ntest['size_tau2_600-150'] = test['size_tau2_150'] - test['size_tau2']\n\ntrain['size_tau2_450-300'] = train['size_tau2_300'] - train['size_tau2_450']\ntest['size_tau2_450-300'] = test['size_tau2_300'] - test['size_tau2_450']\ntrain['size_tau2_450-150'] = train['size_tau2_150'] - train['size_tau2_450']\ntest['size_tau2_450-150'] = test['size_tau2_150'] - test['size_tau2_450']\ntrain['size_tau2_300-150'] = train['size_tau2_150'] - train['size_tau2_300']\ntest['size_tau2_300-150'] = test['size_tau2_150'] - test['size_tau2_300']","metadata":{"execution":{"iopub.status.busy":"2021-09-17T01:17:09.531342Z","iopub.execute_input":"2021-09-17T01:17:09.531735Z","iopub.status.idle":"2021-09-17T01:17:09.58282Z","shell.execute_reply.started":"2021-09-17T01:17:09.531694Z","shell.execute_reply":"2021-09-17T01:17:09.581861Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.to_pickle(train, 'train(317)_tau_noKNN.pkl')\npd.to_pickle(test, 'test(317)_tau_noKNN.pkl')","metadata":{"execution":{"iopub.status.busy":"2021-09-17T01:17:09.5843Z","iopub.execute_input":"2021-09-17T01:17:09.585055Z","iopub.status.idle":"2021-09-17T01:17:11.243511Z","shell.execute_reply.started":"2021-09-17T01:17:09.58501Z","shell.execute_reply":"2021-09-17T01:17:11.242896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.shape","metadata":{"execution":{"iopub.status.busy":"2021-09-17T01:17:11.244829Z","iopub.execute_input":"2021-09-17T01:17:11.245495Z","iopub.status.idle":"2021-09-17T01:17:11.252082Z","shell.execute_reply.started":"2021-09-17T01:17:11.245448Z","shell.execute_reply":"2021-09-17T01:17:11.251106Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split features and target\nx = train.drop(['row_id', 'target', 'time_id'], axis = 1)\ny = train['target']\nx_test = test.drop(['row_id', 'time_id'], axis = 1)\n# Transform stock id to a numeric value\nx['stock_id'] = x['stock_id'].astype(int)\nx_test['stock_id'] = x_test['stock_id'].astype(int)\n\n# Create out of folds array\noof_predictions = np.zeros(x.shape[0])\n# Create test array to store predictions\ntest_predictions = np.zeros(x_test.shape[0])","metadata":{"execution":{"iopub.status.busy":"2021-09-17T01:17:11.253681Z","iopub.execute_input":"2021-09-17T01:17:11.254033Z","iopub.status.idle":"2021-09-17T01:17:12.232936Z","shell.execute_reply.started":"2021-09-17T01:17:11.253992Z","shell.execute_reply":"2021-09-17T01:17:12.231992Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x.shape","metadata":{"execution":{"iopub.status.busy":"2021-09-17T01:17:12.234522Z","iopub.execute_input":"2021-09-17T01:17:12.234855Z","iopub.status.idle":"2021-09-17T01:17:12.241608Z","shell.execute_reply.started":"2021-09-17T01:17:12.234813Z","shell.execute_reply":"2021-09-17T01:17:12.240766Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seed = 29\nparams = {\n    'learning_rate': 0.1,        \n    'lambda_l1': 2,\n    'lambda_l2': 7,\n    'num_leaves': 800,\n    'min_sum_hessian_in_leaf': 20,\n    'feature_fraction': 0.8,\n    'feature_fraction_bynode': 0.8,\n    'bagging_fraction': 0.9,\n    'bagging_freq': 42,\n    'min_data_in_leaf': 700,\n    'max_depth': 4,\n    'seed': seed,\n    'feature_fraction_seed': seed,\n    'bagging_seed': seed,\n    'drop_seed': seed,\n    'data_random_seed': seed,\n    'objective': 'rmse',\n    'boosting': 'gbdt',\n    'verbosity': -1,\n    'n_jobs': -1,\n} ","metadata":{"execution":{"iopub.status.busy":"2021-09-17T01:17:12.243048Z","iopub.execute_input":"2021-09-17T01:17:12.243354Z","iopub.status.idle":"2021-09-17T01:17:12.252689Z","shell.execute_reply.started":"2021-09-17T01:17:12.243313Z","shell.execute_reply":"2021-09-17T01:17:12.251824Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"oof = pd.DataFrame()                 # out-of-fold result\nmodels = []                          # models\nscores = 0.0                         # validation score\n\ngain_importance_list = []\nsplit_importance_list = []\n\nfrom sklearn.model_selection import GroupKFold\ngroup = train['time_id']\nkf = GroupKFold(n_splits=5)\n# Iterate through each fold\nfor fold, (trn_ind, val_ind) in enumerate(kf.split(x, groups=group)):\n    print(f'Training fold {fold + 1}')\n    x_train, x_val = x.iloc[trn_ind], x.iloc[val_ind]\n    y_train, y_val = y.iloc[trn_ind], y.iloc[val_ind]\n    # Root mean squared percentage error weights\n    train_weights = 1 / np.square(y_train)\n    val_weights = 1 / np.square(y_val)\n    train_dataset = lgb.Dataset(x_train, y_train, weight = train_weights)\n    val_dataset = lgb.Dataset(x_val, y_val, weight = val_weights)\n    model = lgb.train(params = params, \n                      train_set = train_dataset, \n                      categorical_feature = ['stock_id'],\n                      valid_sets = [train_dataset, val_dataset], \n                      num_boost_round = 5000, \n                      early_stopping_rounds = 30, \n                      verbose_eval = 100,\n                      feval = feval_rmspe)\n\n    # この書き方することで、全データをOOfにしてrmspeが求められる、\n    # 覚えておいた方がいい\n    oof_predictions[val_ind] = model.predict(x_val)\n    # Predict the test set\n    test_predictions += model.predict(x_test) / 5\n\n    feature_names = x_train.columns.values.tolist()\n    gain_importance_df = calc_model_importance(\n        model, feature_names=feature_names, importance_type='gain')\n    gain_importance_list.append(gain_importance_df)\n\n    split_importance_df = calc_model_importance(\n        model, feature_names=feature_names, importance_type='split')\n    split_importance_list.append(split_importance_df)\n\nrmspe_score = rmspe(y, oof_predictions)\nprint(f'Our out of folds RMSPE is {rmspe_score}')","metadata":{"execution":{"iopub.status.busy":"2021-09-17T01:17:12.25896Z","iopub.execute_input":"2021-09-17T01:17:12.259227Z","iopub.status.idle":"2021-09-17T01:22:07.631727Z","shell.execute_reply.started":"2021-09-17T01:17:12.259199Z","shell.execute_reply":"2021-09-17T01:22:07.630934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.shape","metadata":{"execution":{"iopub.status.busy":"2021-09-17T01:22:07.635381Z","iopub.execute_input":"2021-09-17T01:22:07.63758Z","iopub.status.idle":"2021-09-17T01:22:07.644484Z","shell.execute_reply.started":"2021-09-17T01:22:07.637534Z","shell.execute_reply":"2021-09-17T01:22:07.643727Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mean_gain_df = calc_mean_importance(gain_importance_list)\nmean_gain_df = mean_gain_df.reset_index().rename(columns={'index': 'feature_names'})\nmean_gain_df.to_csv('gain_importance_mean kfold 317 tau.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-09-17T01:22:07.647887Z","iopub.execute_input":"2021-09-17T01:22:07.648129Z","iopub.status.idle":"2021-09-17T01:22:07.671531Z","shell.execute_reply.started":"2021-09-17T01:22:07.648102Z","shell.execute_reply":"2021-09-17T01:22:07.670685Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Discussion\nCV : 0.22507  \nNo tau : 0.2253  \n\nNo effect ....  \nThere is no effect at all when viewed with feture_importance. ","metadata":{}},{"cell_type":"markdown","source":"## KNN + TAU","metadata":{}},{"cell_type":"code","source":"train.shape","metadata":{"execution":{"iopub.status.busy":"2021-09-17T01:22:07.673192Z","iopub.execute_input":"2021-09-17T01:22:07.673422Z","iopub.status.idle":"2021-09-17T01:22:07.688406Z","shell.execute_reply.started":"2021-09-17T01:22:07.673395Z","shell.execute_reply":"2021-09-17T01:22:07.687347Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# making agg features\n\n# time_id毎のstockid毎のtarget変数の一覧\ntrain_p = pd.read_csv(data_dir + '/train.csv')\ntrain_p = train_p.pivot(index='time_id', columns='stock_id', values='target')\n\n# \ncorr = train_p.corr()\n\n# stick_idの相関係数のindex、つまりstockid\nids = corr.index\n\n# 7に関しては調べる(シルエット図とかで)\n# time_idをインスタンスとした時のstock_id相関係数\nkmeans = KMeans(n_clusters=7, random_state=0).fit(corr.values)\n# print(kmeans.labels_)\n\n\n# lにrange(7)と等しいクラスタを撮った時のstock_idを格納してる\nl = []\nfor n in range(7):\n    l.append ( [ (x-1) for x in ( (ids+1)*(kmeans.labels_ == n)) if x > 0] )\n    \n\nmat = []\nmatTest = []\n\nn = 0\nfor ind in l:\n    print(ind)\n    # stock_idが指定されてるクラスタと同じもの(全体サンプル)を引いてくる\n    newDf = train.loc[train['stock_id'].isin(ind) ]\n    # time_id毎に平均値を取る(異なるstock_idでもクラスタが同じなもの同士の平均値になる)\n    newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n    \n    #\n    # stock_idそのものにいみがなくなったので注意！！！！！！！！！！！！！！！！！！！！！！\n    # stock_idをクタスタidに変更してその後にc1?っていうのをつけてる\n    #\n    newDf.loc[:,'stock_id'] = str(n)+'c1'\n    mat.append ( newDf )\n    \n    newDf = test.loc[test['stock_id'].isin(ind) ]    \n    newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n    newDf.loc[:,'stock_id'] = str(n)+'c1'\n    matTest.append ( newDf )\n    \n    n+=1\n    \nmat1 = pd.concat(mat).reset_index()\nmat1.drop(columns=['target'],inplace=True)\n\nmat2 = pd.concat(matTest).reset_index()","metadata":{"execution":{"iopub.status.busy":"2021-09-17T01:22:07.689834Z","iopub.execute_input":"2021-09-17T01:22:07.690062Z","iopub.status.idle":"2021-09-17T01:22:10.161276Z","shell.execute_reply.started":"2021-09-17T01:22:07.690036Z","shell.execute_reply":"2021-09-17T01:22:10.160402Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"matTest = []\nmat = []\nkmeans = []\n#mat2 #= mat1.pivot(index='time_id', columns='stock_idmat2\n# 何でそんなことしたんや笑\nmat2 = pd.concat([mat2,mat1.loc[mat1.time_id==5]])\n\n# この書き方便利、覚えておく\n\nmat1 = mat1.pivot(index='time_id', columns='stock_id')\nmat1.columns = [\"_\".join(x) for x in mat1.columns.ravel()]\nmat1.reset_index(inplace=True)\n\nmat2 = mat2.pivot(index='time_id', columns='stock_id')\nmat2.columns = [\"_\".join(x) for x in mat2.columns.ravel()]\nmat2.reset_index(inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-09-17T01:22:10.162485Z","iopub.execute_input":"2021-09-17T01:22:10.162758Z","iopub.status.idle":"2021-09-17T01:22:10.39182Z","shell.execute_reply.started":"2021-09-17T01:22:10.162721Z","shell.execute_reply":"2021-09-17T01:22:10.391012Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# クラスタリングを行った結果の使う特徴量だけ抜き出してきてる（10/314）\n# 2.5にかんしてはクラスタの中があまりに小さいので除く\n# なぜ、この特徴量にしたのかは不明のため、追加実験が必要\n# log_return2を削った理由はわからん（おそらく相関が似ちゃうから）\n\nnnn = ['time_id',\n     'log_return1_realized_volatility_0c1',\n     'log_return1_realized_volatility_1c1',     \n     'log_return1_realized_volatility_2c1',\n     'log_return1_realized_volatility_3c1',     \n     'log_return1_realized_volatility_4c1',\n     'total_volume_mean_0c1',\n     'total_volume_mean_1c1', \n     'total_volume_mean_2c1',\n     'total_volume_mean_3c1', \n     'total_volume_mean_4c1',\n     'trade_size_mean_0c1',\n     'trade_size_mean_1c1', \n     'trade_size_mean_2c1',\n     'trade_size_mean_3c1', \n     'trade_size_mean_4c1',\n     'trade_order_count_mean_0c1',\n     'trade_order_count_mean_1c1',\n     'trade_order_count_mean_2c1',\n     'trade_order_count_mean_3c1',\n     'trade_order_count_mean_4c1',      \n     'price_spread_mean_0c1',\n     'price_spread_mean_1c1',\n     'price_spread_mean_2c1',\n     'price_spread_mean_3c1',\n     'price_spread_mean_4c1',   \n     'bid_spread_mean_0c1',\n     'bid_spread_mean_1c1',\n     'bid_spread_mean_2c1',\n     'bid_spread_mean_3c1',\n     'bid_spread_mean_4c1',       \n     'ask_spread_mean_0c1',\n     'ask_spread_mean_1c1',\n     'ask_spread_mean_2c1',\n     'ask_spread_mean_3c1',\n     'ask_spread_mean_4c1',   \n     'volume_imbalance_mean_0c1',\n     'volume_imbalance_mean_1c1',\n     'volume_imbalance_mean_2c1',\n     'volume_imbalance_mean_3c1',\n     'volume_imbalance_mean_4c1',       \n     'bid_ask_spread_mean_0c1',\n     'bid_ask_spread_mean_1c1',\n     'bid_ask_spread_mean_2c1',\n     'bid_ask_spread_mean_3c1',\n     'bid_ask_spread_mean_4c1',\n     'size_tau2_0c1',\n     'size_tau2_1c1',\n     'size_tau2_2c1',\n     'size_tau2_3c1',\n     'size_tau2_4c1',\n     'size_tau2_450_0c1',\n     'size_tau2_450_1c1',\n     'size_tau2_450_2c1',\n     'size_tau2_450_3c1',\n     'size_tau2_450_4c1',\n     'size_tau2_300_0c1',\n     'size_tau2_300_1c1',\n     'size_tau2_300_2c1', \n     'size_tau2_300_3c1', \n     'size_tau2_300_4c1', \n     'size_tau2_150_0c1',\n     'size_tau2_150_1c1',\n     'size_tau2_150_2c1', \n     'size_tau2_150_3c1', \n     'size_tau2_150_4c1',          \n     'size_tau2_600-450_0c1',\n     'size_tau2_600-450_1c1',\n     'size_tau2_600-450_2c1',\n     'size_tau2_600-450_3c1',\n     'size_tau2_600-450_4c1',\n     'size_tau2_600-300_0c1',\n     'size_tau2_600-300_1c1',\n     'size_tau2_600-300_2c1',\n     'size_tau2_600-300_3c1',\n     'size_tau2_600-300_4c1',\n     'size_tau2_600-150_0c1',\n     'size_tau2_600-150_1c1',\n     'size_tau2_600-150_2c1',\n     'size_tau2_600-150_3c1',\n     'size_tau2_600-150_4c1',          \n     'size_tau2_450-300_0c1',\n     'size_tau2_450-300_1c1',\n     'size_tau2_450-300_2c1',\n     'size_tau2_450-300_3c1',\n     'size_tau2_450-300_4c1',\n     'size_tau2_450-150_0c1',\n     'size_tau2_450-150_1c1',\n     'size_tau2_450-150_2c1',\n     'size_tau2_450-150_3c1',\n     'size_tau2_450-150_4c1',            \n     'size_tau2_300-150_0c1',\n     'size_tau2_300-150_1c1',\n     'size_tau2_300-150_2c1',\n     'size_tau2_300-150_3c1',\n     'size_tau2_300-150_4c1',            \n      ] ","metadata":{"execution":{"iopub.status.busy":"2021-09-17T01:22:10.39356Z","iopub.execute_input":"2021-09-17T01:22:10.393799Z","iopub.status.idle":"2021-09-17T01:22:10.404847Z","shell.execute_reply.started":"2021-09-17T01:22:10.393764Z","shell.execute_reply":"2021-09-17T01:22:10.403991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# trainとくっつける\ntrain = pd.merge(train,mat1[nnn],how='left',on='time_id')\ntest = pd.merge(test,mat2[nnn],how='left',on='time_id')","metadata":{"execution":{"iopub.status.busy":"2021-09-17T01:22:10.406132Z","iopub.execute_input":"2021-09-17T01:22:10.406364Z","iopub.status.idle":"2021-09-17T01:22:27.389766Z","shell.execute_reply.started":"2021-09-17T01:22:10.406336Z","shell.execute_reply":"2021-09-17T01:22:27.388821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.shape","metadata":{"execution":{"iopub.status.busy":"2021-09-17T01:22:27.391517Z","iopub.execute_input":"2021-09-17T01:22:27.391836Z","iopub.status.idle":"2021-09-17T01:22:27.396985Z","shell.execute_reply.started":"2021-09-17T01:22:27.391804Z","shell.execute_reply":"2021-09-17T01:22:27.396243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.to_pickle(train, 'train(412)_tau_KNN.pkl')\npd.to_pickle(test, 'test(412)_tau_KNN.pkl')","metadata":{"execution":{"iopub.status.busy":"2021-09-17T01:22:27.398101Z","iopub.execute_input":"2021-09-17T01:22:27.398347Z","iopub.status.idle":"2021-09-17T01:22:31.426584Z","shell.execute_reply.started":"2021-09-17T01:22:27.398319Z","shell.execute_reply":"2021-09-17T01:22:31.42535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split features and target\nx = train.drop(['row_id', 'target', 'time_id'], axis = 1)\ny = train['target']\nx_test = test.drop(['row_id', 'time_id'], axis = 1)\n# Transform stock id to a numeric value\nx['stock_id'] = x['stock_id'].astype(int)\nx_test['stock_id'] = x_test['stock_id'].astype(int)\n\n# Create out of folds array\noof_predictions = np.zeros(x.shape[0])\n# Create test array to store predictions\ntest_predictions = np.zeros(x_test.shape[0])","metadata":{"execution":{"iopub.status.busy":"2021-09-17T01:22:31.428281Z","iopub.execute_input":"2021-09-17T01:22:31.428736Z","iopub.status.idle":"2021-09-17T01:22:32.791917Z","shell.execute_reply.started":"2021-09-17T01:22:31.428683Z","shell.execute_reply":"2021-09-17T01:22:32.791096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"oof = pd.DataFrame()                 # out-of-fold result\nmodels = []                          # models\nscores = 0.0                         # validation score\n\ngain_importance_list = []\nsplit_importance_list = []\n\nfrom sklearn.model_selection import GroupKFold\ngroup = train['time_id']\nkf = GroupKFold(n_splits=5)\n# Iterate through each fold\nfor fold, (trn_ind, val_ind) in enumerate(kf.split(x, groups=group)):\n    print(f'Training fold {fold + 1}')\n    x_train, x_val = x.iloc[trn_ind], x.iloc[val_ind]\n    y_train, y_val = y.iloc[trn_ind], y.iloc[val_ind]\n    # Root mean squared percentage error weights\n    train_weights = 1 / np.square(y_train)\n    val_weights = 1 / np.square(y_val)\n    train_dataset = lgb.Dataset(x_train, y_train, weight = train_weights)\n    val_dataset = lgb.Dataset(x_val, y_val, weight = val_weights)\n    model = lgb.train(params = params, \n                      train_set = train_dataset, \n                      categorical_feature = ['stock_id'],\n                      valid_sets = [train_dataset, val_dataset], \n                      num_boost_round = 5000, \n                      early_stopping_rounds = 30, \n                      verbose_eval = 100,\n                      feval = feval_rmspe)\n\n    # この書き方することで、全データをOOfにしてrmspeが求められる、\n    # 覚えておいた方がいい\n    oof_predictions[val_ind] = model.predict(x_val)\n    # Predict the test set\n    test_predictions += model.predict(x_test) / 5\n\n    feature_names = x_train.columns.values.tolist()\n    gain_importance_df = calc_model_importance(\n        model, feature_names=feature_names, importance_type='gain')\n    gain_importance_list.append(gain_importance_df)\n\n    split_importance_df = calc_model_importance(\n        model, feature_names=feature_names, importance_type='split')\n    split_importance_list.append(split_importance_df)\n\nrmspe_score = rmspe(y, oof_predictions)\nprint(f'Our out of folds RMSPE is {rmspe_score}')","metadata":{"execution":{"iopub.status.busy":"2021-09-17T01:22:32.793422Z","iopub.execute_input":"2021-09-17T01:22:32.793687Z","iopub.status.idle":"2021-09-17T01:30:11.937001Z","shell.execute_reply.started":"2021-09-17T01:22:32.793659Z","shell.execute_reply":"2021-09-17T01:30:11.936285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mean_gain_df = calc_mean_importance(gain_importance_list)\nmean_gain_df = mean_gain_df.reset_index().rename(columns={'index': 'feature_names'})\nmean_gain_df.to_csv('gain_importance_mean kfold 412 tau KNN.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-09-17T01:30:11.938485Z","iopub.execute_input":"2021-09-17T01:30:11.938981Z","iopub.status.idle":"2021-09-17T01:30:11.950559Z","shell.execute_reply.started":"2021-09-17T01:30:11.938943Z","shell.execute_reply":"2021-09-17T01:30:11.949816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mean_gain_df.set_index('feature_names').filter(like='tau', axis=0).sort_values('importance', ascending=False).head(15)","metadata":{"execution":{"iopub.status.busy":"2021-09-17T01:30:11.951855Z","iopub.execute_input":"2021-09-17T01:30:11.952272Z","iopub.status.idle":"2021-09-17T01:30:11.971298Z","shell.execute_reply.started":"2021-09-17T01:30:11.952242Z","shell.execute_reply":"2021-09-17T01:30:11.970746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Discussion\n\nCV : 0.21742  \nLB : 0.21906  \nNo clustering tau : CV : 0.22057    \n\nSmall, but effective?  \nLooking at feature importance, size_tau2 might be good?  \n600-450 is also a little higher, so it might be working.  \n\nProbably because the slope of the last 600 seconds is related to the volatility of the next 600 seconds.\n","metadata":{}}]}