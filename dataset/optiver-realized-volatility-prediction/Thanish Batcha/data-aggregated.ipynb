{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# # This Python 3 environment comes with many helpful analytics libraries installed\n# # It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# # For example, here's several helpful packages to load\n\n# import numpy as np # linear algebra\n# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# # Input data files are available in the read-only \"../input/\" directory\n# # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# # You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# # You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2021-06-29T07:00:01.995941Z","iopub.status.busy":"2021-06-29T07:00:01.995374Z","iopub.status.idle":"2021-06-29T07:00:02.000934Z","shell.execute_reply":"2021-06-29T07:00:01.999918Z","shell.execute_reply.started":"2021-06-29T07:00:01.995854Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### About this Notebook\n\nThis is a starter preprocessing notebook to create stats features to build the model. However if you run into resource exhausted error when preprocessing as I ran into, since there are few million records to preprocess, you can check this [dataset](https://www.kaggle.com/c/optiver-realized-volatility-prediction/discussion/249647#1369316) where I have linked the preprocessed train pickle and csv format created using the code below.\n\nNote: I have not run and saved it for the obvious reason that I might run into memory issue again, so just sharing the code used. ","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport glob\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split, KFold\nfrom tqdm import tqdm\n\nimport gc","metadata":{"execution":{"iopub.execute_input":"2021-06-29T07:00:02.003266Z","iopub.status.busy":"2021-06-29T07:00:02.002674Z","iopub.status.idle":"2021-06-29T07:00:03.296936Z","shell.execute_reply":"2021-06-29T07:00:03.295827Z","shell.execute_reply.started":"2021-06-29T07:00:02.00323Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"platform = 'Kaggle'\n\nif platform == 'Kaggle':\n    config = {'input_trade_path': \"../input/optiver-realized-volatility-prediction/trade_\",\n              'input_book_path': \"../input/optiver-realized-volatility-prediction/book_\",\n              'train_path': '../input/optiver-realized-volatility-prediction/train.csv',\n              'test_path' : '../input/optiver-realized-volatility-prediction/test.csv'}\n    \nelse:\n    config = {'input_trade_path': \"../trade_\",\n              'input_book_path': \"../book_\",\n              'train_path': '../train.csv',\n              'test_path' : '../test.csv'}","metadata":{"execution":{"iopub.execute_input":"2021-06-29T07:00:03.299143Z","iopub.status.busy":"2021-06-29T07:00:03.298804Z","iopub.status.idle":"2021-06-29T07:00:03.305425Z","shell.execute_reply":"2021-06-29T07:00:03.30425Z","shell.execute_reply.started":"2021-06-29T07:00:03.299101Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv(config['train_path'])\ntest_df = pd.read_csv(config['test_path'])","metadata":{"execution":{"iopub.execute_input":"2021-06-29T07:00:03.445705Z","iopub.status.busy":"2021-06-29T07:00:03.445282Z","iopub.status.idle":"2021-06-29T07:00:03.711976Z","shell.execute_reply":"2021-06-29T07:00:03.711083Z","shell.execute_reply.started":"2021-06-29T07:00:03.44567Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def read_trade_and_book_data(stock_id, inp_type, data_type):\n    \n    trade_file = glob.glob(config[inp_type]+f'{data_type}.parquet/stock_id={stock_id}/*')[0]\n    trade = pd.read_parquet(trade_file)\n    return trade","metadata":{"execution":{"iopub.execute_input":"2021-06-29T07:00:03.741788Z","iopub.status.busy":"2021-06-29T07:00:03.741464Z","iopub.status.idle":"2021-06-29T07:00:03.752017Z","shell.execute_reply":"2021-06-29T07:00:03.750679Z","shell.execute_reply.started":"2021-06-29T07:00:03.741756Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_consolidated_final_trade_book_df(df, data_type):\n    unique_id = df['stock_id'].unique().tolist()\n    \n    trade_final_df = pd.DataFrame()\n    book_final_df = pd.DataFrame()\n    for stock_id in tqdm(unique_id):\n        # Get book data\n        temp_book_stock_df = read_trade_and_book_data(stock_id=stock_id, \n                                                  inp_type='input_book_path', \n                                                  data_type=data_type)\n        temp_book_stock_df['stock_id'] = stock_id\n        book_final_df = pd.concat([book_final_df, temp_book_stock_df])\n        \n        # Get trade data\n        temp_trade_stock_df = read_trade_and_book_data(stock_id=stock_id, \n                                                   inp_type='input_trade_path', \n                                                   data_type=data_type)\n        temp_trade_stock_df['stock_id'] = stock_id\n        trade_final_df = pd.concat([trade_final_df, temp_trade_stock_df])\n        \n        gc.collect()\n        \n    book_final_df = book_final_df.reset_index(drop=True)\n    trade_final_df = trade_final_df.reset_index(drop=True)\n\n    return book_final_df, trade_final_df","metadata":{"execution":{"iopub.execute_input":"2021-06-29T07:00:25.499461Z","iopub.status.busy":"2021-06-29T07:00:25.499063Z","iopub.status.idle":"2021-06-29T07:00:25.510109Z","shell.execute_reply":"2021-06-29T07:00:25.50891Z","shell.execute_reply.started":"2021-06-29T07:00:25.499425Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()\ntrain_book_final_df, train_trade_final_df = get_consolidated_final_trade_book_df(df=train_df, data_type='train')\ntest_book_final_df, test_trade_final_df = get_consolidated_final_trade_book_df(df=test_df, data_type='test')\n\ntrain_book_final_df.shape, train_trade_final_df.shape, test_book_final_df.shape, test_trade_final_df.shape","metadata":{"execution":{"iopub.execute_input":"2021-06-29T07:00:35.575352Z","iopub.status.busy":"2021-06-29T07:00:35.574995Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_trade_agg_info(df):\n    agg_df = df.groupby(['stock_id', 'time_id']).agg(mean_sec_in_bucket_trade = ('seconds_in_bucket', 'mean'), \n                                                     mean_price = ('price', 'mean'),\n                                                     mean_size = ('size', 'mean'),\n                                                     mean_order = ('order_count', 'mean'),\n                                                     max_sec_in_bucket_trade = ('seconds_in_bucket', 'max'), \n                                                     max_price = ('price', 'max'),\n                                                     max_size = ('size', 'max'),\n                                                     max_order = ('order_count', 'max'),\n                                                     min_sec_in_bucket_trade = ('seconds_in_bucket', 'min'), \n                                                     min_price = ('price', 'min'),\n                                                     min_size = ('size', 'min'),\n                                                     min_order = ('order_count', 'min'),\n                                                     median_sec_in_bucket_trade = ('seconds_in_bucket', 'median'), \n                                                     median_price = ('price', 'median'),\n                                                     median_size = ('size', 'median'),\n                                                     median_order = ('order_count', 'median')\n                                                    ).reset_index()\n    \n    return agg_df\n\ndef get_book_agg_info(df):\n    agg_df = df.groupby(['stock_id', 'time_id']).agg(mean_sec_in_bucket_book = ('seconds_in_bucket', 'mean'),\n                                                     mean_bid_price1 = ('bid_price1', 'mean'),\n                                                     mean_ask_price1 = ('ask_price1', 'mean'),\n                                                     mean_bid_price2 = ('bid_price2',  'mean'),\n                                                     mean_ask_price2 = ('ask_price2',  'mean'),\n                                                     mean_bid_size1 = ('bid_size1',  'mean'),\n                                                     mean_ask_size1 = ('ask_size1',  'mean'),\n                                                     mean_bid_size2 = ('bid_size2', 'mean'),\n                                                     mean_ask_size2 = ('ask_size2', 'mean'),\n                                                     max_sec_in_bucket_book = ('seconds_in_bucket', 'max'),\n                                                     max_bid_price1 = ('bid_price1', 'max'),\n                                                     max_ask_price1 = ('ask_price1', 'max'),\n                                                     max_bid_price2 = ('bid_price2',  'max'),\n                                                     max_ask_price2 = ('ask_price2',  'max'),\n                                                     max_bid_size1 = ('bid_size1',  'max'),\n                                                     max_ask_size1 = ('ask_size1',  'max'),\n                                                     max_bid_size2 = ('bid_size2', 'max'),\n                                                     max_ask_size2 = ('ask_size2', 'max'),\n                                                     min_sec_in_bucket_book = ('seconds_in_bucket', 'min'),\n                                                     min_bid_price1 = ('bid_price1', 'min'),\n                                                     min_ask_price1 = ('ask_price1', 'min'),\n                                                     min_bid_price2 = ('bid_price2',  'min'),\n                                                     min_ask_price2 = ('ask_price2',  'min'),\n                                                     min_bid_size1 = ('bid_size1',  'min'),\n                                                     min_ask_size1 = ('ask_size1',  'min'),\n                                                     min_bid_size2 = ('bid_size2', 'min'),\n                                                     min_ask_size2 = ('ask_size2', 'min'),\n                                                     median_sec_in_bucket_book = ('seconds_in_bucket', 'median'),\n                                                     median_bid_price1 = ('bid_price1', 'median'),\n                                                     median_ask_price1 = ('ask_price1', 'median'),\n                                                     median_bid_price2 = ('bid_price2',  'median'),\n                                                     median_ask_price2 = ('ask_price2',  'median'),\n                                                     median_bid_size1 = ('bid_size1',  'median'),\n                                                     median_ask_size1 = ('ask_size1',  'median'),\n                                                     median_bid_size2 = ('bid_size2', 'median'),\n                                                     median_ask_size2 = ('ask_size2', 'median')\n                                                    ).reset_index()\n    \n    return agg_df","metadata":{"execution":{"iopub.status.busy":"2021-06-29T07:00:23.727551Z","iopub.status.idle":"2021-06-29T07:00:23.728035Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_trade_agg = get_trade_agg_info(df=train_trade_final_df)\ntest_trade_agg = get_trade_agg_info(df=test_trade_final_df)\n\ntrain_trade_agg.shape, test_trade_agg.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_book_agg = get_book_agg_info(df=train_book_final_df)\ntest_book_agg = get_book_agg_info(df=test_book_final_df)\n\ntrain_book_agg.shape, test_book_agg.shape","metadata":{"execution":{"iopub.status.busy":"2021-06-29T07:00:23.73066Z","iopub.status.idle":"2021-06-29T07:00:23.731107Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_agg = pd.merge(train_book_agg, train_trade_agg, \n                     on=['stock_id', 'time_id'], \n                     how='left')\n\ntest_agg = pd.merge(test_book_agg, test_trade_agg, \n                    on=['stock_id', 'time_id'], \n                    how='left')\n\ntrain_agg.shape, test_agg.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Merge to get the labels\ntrain_final_df = pd.merge(train_df, train_agg, on=['stock_id', 'time_id'], how='left')\n\n# Merge to get the row-id for submission\ntest_final_df = pd.merge(test_df, test_agg, on=['stock_id', 'time_id'], how='left')\n\nprint(train_final_df.shape, test_final_df.shape)","metadata":{"execution":{"iopub.status.busy":"2021-06-29T07:00:23.732267Z","iopub.status.idle":"2021-06-29T07:00:23.73274Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_final_df.to_pickle('../train_agg_final_df.pickle')\ntest_final_df.to_pickle('../test_agg_final_df.pickle')","metadata":{},"execution_count":null,"outputs":[]}]}