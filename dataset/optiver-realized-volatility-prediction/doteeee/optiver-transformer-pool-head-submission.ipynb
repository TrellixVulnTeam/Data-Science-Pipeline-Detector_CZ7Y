{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nimport os\nimport gc\nimport numpy as np\nimport pandas as pd","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-09-22T13:50:53.44122Z","iopub.execute_input":"2021-09-22T13:50:53.441621Z","iopub.status.idle":"2021-09-22T13:50:53.446119Z","shell.execute_reply.started":"2021-09-22T13:50:53.441575Z","shell.execute_reply":"2021-09-22T13:50:53.445433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_book_folder='../input/optiver-realized-volatility-prediction/book_test.parquet'\n\nlog_transform_features=['bid_size1', 'ask_size1', 'bid_size2', 'ask_size2']\nfeature_columns=[\n    'bid_price1', 'ask_price1', 'bid_price2', 'ask_price2', \n    'bid_size1', 'ask_size1', 'bid_size2', 'ask_size2'\n]","metadata":{"execution":{"iopub.status.busy":"2021-09-22T13:50:53.447392Z","iopub.execute_input":"2021-09-22T13:50:53.448265Z","iopub.status.idle":"2021-09-22T13:50:53.462184Z","shell.execute_reply.started":"2021-09-22T13:50:53.448217Z","shell.execute_reply":"2021-09-22T13:50:53.460899Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class config:\n    num_buckets= 600\n    num_features= 11\n    batch_size=128\n    epochs=10","metadata":{"execution":{"iopub.status.busy":"2021-09-22T13:50:53.464472Z","iopub.execute_input":"2021-09-22T13:50:53.465299Z","iopub.status.idle":"2021-09-22T13:50:53.476193Z","shell.execute_reply.started":"2021-09-22T13:50:53.46523Z","shell.execute_reply":"2021-09-22T13:50:53.475463Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"window=4\ndef smoothing(x):\n    if x.shape[0] < window:\n        return x\n    x=x.copy()\n    try:\n        cum_x=np.cumsum(x, axis=0)\n        x_rolling=(cum_x[window:, :] - cum_x[:-window, :])/window\n        x[window:, :]=x_rolling\n    except:\n        pass\n    return x\n\ndef smoothing_1d(x):\n    if x.shape[0] < window:\n        return x\n    x=x.copy()\n    try:\n        cum_x=np.cumsum(x)\n        x_rolling=(cum_x[window:] - cum_x[:-window])/window\n        x[window:]=x_rolling\n    except:\n        pass\n    return x","metadata":{"execution":{"iopub.status.busy":"2021-09-22T13:50:53.479175Z","iopub.execute_input":"2021-09-22T13:50:53.48002Z","iopub.status.idle":"2021-09-22T13:50:53.493373Z","shell.execute_reply.started":"2021-09-22T13:50:53.47996Z","shell.execute_reply":"2021-09-22T13:50:53.491463Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# dataset","metadata":{}},{"cell_type":"code","source":"class OptiverDataset(torch.utils.data.Dataset):\n    def __init__(self, features):\n        self.features=features\n    def __len__(self):\n        return len(self.features)\n    \n    def calculate_wap(self, ask_price, bid_price, ask_size, bid_size):\n        ask_size=np.exp(ask_size) - 1\n        bid_size=np.exp(bid_size) - 1\n        wap = (ask_price * bid_size) + (bid_price * ask_size)\n        wap = wap / (ask_size + bid_size)\n        return wap\n    \n    def get_log_returns(self, wap):\n        s=np.append(0, np.diff(np.log(wap)))\n        s[s==-np.inf]=0.0\n        return s\n    \n    def get_realized_volatility(self, s):\n        rv=np.sqrt(np.sum( (s**2) ))\n        return rv\n    \n    def get_price_differences(self, feat, seq_len):\n        bid_price1=feat[:, 0]\n        ask_price1=feat[:, 1]\n        bid_price2= feat[:, 2]\n        ask_price2= feat[:, 3]\n        \n        price_diff1 = bid_price1 - ask_price1\n        price_diff2 = bid_price2 - ask_price2\n        \n        ask_diff = ask_price1 - ask_price2\n        bid_diff = bid_price1 - bid_price2\n        \n        price_diff=np.zeros((4, 600) )\n        price_diff[0, -seq_len:]=price_diff1\n        price_diff[1, -seq_len:]=price_diff2\n        price_diff[2, -seq_len:]=ask_diff\n        price_diff[3, -seq_len:]=bid_diff\n        \n        price_diff=torch.tensor(price_diff, dtype=torch.float32).transpose(1, 0)\n        return price_diff\n    \n    def get_features(self, feat):\n        bid_price1=feat[:, 0]\n        ask_price1=feat[:, 1]\n        bid_price2= feat[:, 2]\n        ask_price2= feat[:, 3]\n        bid_size1=feat[:, 4]\n        ask_size1=feat[:, 5]\n        bid_size2=feat[:, 6]\n        ask_size2=feat[:, 7]\n        \n        wap1=self.calculate_wap(ask_price1, bid_price1, ask_size1, bid_size1)\n        wap2=self.calculate_wap(ask_price2, bid_price2, ask_size2, bid_size2)\n        \n        s1=self.get_log_returns(wap1)\n        s2=self.get_log_returns(wap2)\n        \n        rv1=self.get_realized_volatility(s1)\n        rv2=self.get_realized_volatility(s2)\n        \n        if rv1==0:\n            rv1=rv2\n        elif rv2==0:\n            rv2=rv1\n        \n        return (wap1,wap2, s1, s2, rv1, rv2)\n    \n    def get_binary_target_features(self, y_target, rv1, rv2):\n        y_binary1=(y_target>rv1)\n        y_binary2=(y_target>rv2)\n        \n        weights1=np.abs(y_target - rv1) + 1e-10\n        weights2=np.abs(y_target - rv2) + 1e-10\n        \n        return (y_binary1, y_binary2, weights1, weights2)\n        \n    \n    def __getitem__(self, i):\n        feat=np.array(self.features[i])\n        (seq_len, num_features) = (feat.shape[0], feat.shape[1])\n        \n        (wap1_arr,wap2_arr, s1_arr, s2_arr, rv1, rv2)=self.get_features(feat)\n        price_diff = self.get_price_differences(feat, seq_len)\n        \n        feat=smoothing(feat)\n        wap1_arr=smoothing_1d(wap1_arr)\n        wap2_arr=smoothing_1d(wap2_arr)\n        \n        X=torch.zeros(600, num_features)\n        \n        mask=np.zeros(600)\n        mask[-seq_len:]=1\n        \n        wap1=torch.zeros(600)\n        wap2=torch.zeros(600)\n        s1=torch.zeros(600)\n        s2=torch.zeros(600)\n        \n        mask=torch.tensor(mask, dtype=torch.long)\n        X[-seq_len:]=torch.tensor(feat, dtype=torch.float32)\n        wap1[-seq_len:]=torch.tensor(wap1_arr, dtype=torch.float32)\n        wap2[-seq_len:]=torch.tensor(wap2_arr, dtype=torch.float32)\n        \n        s1[-seq_len:]=torch.tensor(s1_arr, dtype=torch.float32)\n        s2[-seq_len:]=torch.tensor(s2_arr, dtype=torch.float32)\n        \n        \n        rv1=torch.tensor(rv1, dtype=torch.float32)\n        rv2=torch.tensor(rv2, dtype=torch.float32)\n        \n        return (X, price_diff, mask, wap1, wap2, s1, s2, rv1, rv2)","metadata":{"execution":{"iopub.status.busy":"2021-09-22T13:50:53.4961Z","iopub.execute_input":"2021-09-22T13:50:53.496527Z","iopub.status.idle":"2021-09-22T13:50:53.530335Z","shell.execute_reply.started":"2021-09-22T13:50:53.496482Z","shell.execute_reply":"2021-09-22T13:50:53.529483Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# model","metadata":{}},{"cell_type":"code","source":"#https://github.com/KrisKorrel/sparsemax-pytorch\n\nclass Sparsemax(nn.Module):\n    \"\"\"Sparsemax function.\"\"\"\n\n    def __init__(self, dim=None):\n        \"\"\"Initialize sparsemax activation\n        \n        Args:\n            dim (int, optional): The dimension over which to apply the sparsemax function.\n        \"\"\"\n        super(Sparsemax, self).__init__()\n\n        self.dim = -1 if dim is None else dim\n\n    def forward(self, input):\n        \"\"\"Forward function.\n        Args:\n            input (torch.Tensor): Input tensor. First dimension should be the batch size\n        Returns:\n            torch.Tensor: [batch_size x number_of_logits] Output tensor\n        \"\"\"\n        # Sparsemax currently only handles 2-dim tensors,\n        # so we reshape to a convenient shape and reshape back after sparsemax\n        input = input.transpose(0, self.dim)\n        original_size = input.size()\n        input = input.reshape(input.size(0), -1)\n        input = input.transpose(0, 1)\n        dim = 1\n\n        number_of_logits = input.size(dim)\n\n        # Translate input by max for numerical stability\n        input = input - torch.max(input, dim=dim, keepdim=True)[0].expand_as(input)\n\n        # Sort input in descending order.\n        # (NOTE: Can be replaced with linear time selection method described here:\n        # http://stanford.edu/~jduchi/projects/DuchiShSiCh08.html)\n        zs = torch.sort(input=input, dim=dim, descending=True)[0]\n        range = torch.arange(start=1, end=number_of_logits + 1, step=1, device=device, dtype=input.dtype).view(1, -1)\n        range = range.expand_as(zs)\n\n        # Determine sparsity of projection\n        bound = 1 + range * zs\n        cumulative_sum_zs = torch.cumsum(zs, dim)\n        is_gt = torch.gt(bound, cumulative_sum_zs).type(input.type())\n        k = torch.max(is_gt * range, dim, keepdim=True)[0]\n\n        # Compute threshold function\n        zs_sparse = is_gt * zs\n\n        # Compute taus\n        taus = (torch.sum(zs_sparse, dim, keepdim=True) - 1) / k\n        taus = taus.expand_as(input)\n\n        # Sparsemax\n        self.output = torch.max(torch.zeros_like(input), input - taus)\n\n        # Reshape back to original shape\n        output = self.output\n        output = output.transpose(0, 1)\n        output = output.reshape(original_size)\n        output = output.transpose(0, self.dim)\n\n        return output\n\n    def backward(self, grad_output):\n        \"\"\"Backward function.\"\"\"\n        dim = 1\n\n        nonzeros = torch.ne(self.output, 0)\n        sum = torch.sum(grad_output * nonzeros, dim=dim) / torch.sum(nonzeros, dim=dim)\n        self.grad_input = nonzeros * (grad_output - sum.expand_as(grad_output))\n\n        return self.grad_input","metadata":{"execution":{"iopub.status.busy":"2021-09-22T13:50:53.53162Z","iopub.execute_input":"2021-09-22T13:50:53.532065Z","iopub.status.idle":"2021-09-22T13:50:53.557285Z","shell.execute_reply.started":"2021-09-22T13:50:53.532032Z","shell.execute_reply":"2021-09-22T13:50:53.556348Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sparsemax = Sparsemax(dim=-1)\n\ndef get_activation_fn(activation):\n    if activation=='gelu':\n        return nn.GELU()\n    elif activation=='relu':\n        return nn.ReLU()\n    \ndef attention(query, key, value, mask=None, dropout=None):\n    d_k=query.size(-1)\n    scores=torch.matmul( query, key.transpose(-1, -2) )/np.sqrt(d_k)\n    #scores=torch.tril(scores)\n    if mask is not None:\n        scores=scores.masked_fill(mask == 0, -1e9)\n    \n    #p_attn=torch.softmax(scores, dim=-1)\n    p_attn=sparsemax(scores)\n    x_attn=torch.matmul(p_attn, value)\n    if dropout:\n        x_attn=dropout(x_attn)\n        \n    return p_attn, x_attn\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, num_features, dmodel, nhead,activation,norm,dropout):\n        super().__init__()\n        self.dmodel=dmodel\n        self.nhead=nhead\n        self.d_k=dmodel//nhead #Size\n        \n        self.activation=activation\n        self.norm=norm\n        self.dropout=dropout\n        \n        #self.Q=nn.Linear(num_features, dmodel)\n        #self.K=nn.Linear(num_features, dmodel)\n        #self.V=nn.Linear(num_features, dmodel)\n        \n        self.Q=nn.Conv1d(num_features, dmodel, 5, padding=2)\n        self.K=nn.Conv1d(num_features, dmodel, 5, padding=2)\n        self.V=nn.Conv1d(num_features, dmodel, 5, padding=2)\n        \n        \n        self.W=nn.Linear(dmodel, num_features)\n        \n        #nn.init.uniform_(self.Q.weight, -1/np.sqrt(2*num_features), 1/np.sqrt(2*num_features))\n        #nn.init.uniform_(self.K.weight, -1/np.sqrt(2*num_features), 1/np.sqrt(2*num_features))\n        #nn.init.uniform_(self.V.weight, -1/np.sqrt(2*num_features), 1/np.sqrt(2*num_features))\n        #nn.init.uniform_(self.W.weight, -1/np.sqrt(2*num_features), 1/np.sqrt(2*num_features))\n        \n        \n    def forward(self, x, mask=None):\n        bsize=x.size(0)\n        x=self.norm(x)\n        x=x.transpose(2, 1)\n        query=self.Q(x).transpose(2, 1).view(bsize, -1, self.nhead, self.d_k)\n        key=self.K(x).transpose(2, 1).view(bsize, -1, self.nhead, self.d_k)\n        value=self.V(x).transpose(2, 1).view(bsize, -1, self.nhead, self.d_k)\n        mask=mask.unsqueeze(-1).unsqueeze(-1)\n        \n        #query=self.Q(x).view(bsize, -1, self.nhead, self.d_k)\n        #key=self.K(x).view(bsize, -1, self.nhead, self.d_k)\n        #value=self.V(x).view(bsize, -1, self.nhead, self.d_k)\n        \n        \n        p_attn, x_attn=attention(query, key, value, mask, self.dropout)\n        x_attn=x_attn.view(bsize, -1, self.nhead*self.d_k)\n        \n        x_attn=self.W(x_attn)\n        x=x.transpose(2, 1)\n        x=x+x_attn\n        return x\n\nclass TimeSeriesAttentionLayer(nn.Module):\n    def __init__(self,\n                 num_features=32,\n                 dmodel=128,\n                 nhead=4,\n                 dim_feed_forward=512,\n                 activation='relu', \n                 dropout=0.1\n                ):\n        \n        super().__init__()\n        self.num_features=num_features\n        self.dmodel=dmodel\n        self.nhead=nhead\n        self.dim_feed_forward=dim_feed_forward\n        self.activation=get_activation_fn('gelu')\n        self.norm=nn.LayerNorm(num_features)\n        self.dropout=nn.Dropout(dropout)\n        \n        self.multihead_attn=MultiHeadAttention(num_features,\n                                               dmodel,\n                                               nhead,\n                                               self.activation,\n                                               self.norm,\n                                               self.dropout\n                                              )\n        \n        self.conv1=nn.Conv1d(num_features, dim_feed_forward, 5, padding=2)\n        self.conv2=nn.Conv1d(dim_feed_forward, num_features, 5, padding=2)\n        #self.linear1=nn.Linear(num_features, dim_feed_forward)\n        #self.linear2=nn.Linear(dim_feed_forward, num_features)\n        \n    def forward(self, x, mask=None):\n        x=self.multihead_attn(x, mask)\n        x=self.norm(x)\n        #x_ffn=self.linear2(self.dropout(self.activation(self.linear1(x))))\n        x_ffn=self.conv2(self.dropout(self.activation(self.conv1(x.transpose(1, 2))))).transpose(2, 1)\n        x=x+x_ffn\n        return x\n\n\n\nclass FeatureExtractorWith1DConv(nn.Module):\n    def __init__(self, input_size, output_size):\n        super().__init__()\n        #self.pre_layernorm=nn.LayerNorm(input_size)\n        self.pre_bn=nn.BatchNorm1d(input_size)\n        \n        #self.linear1=nn.Linear(input_size, 2*output_size)\n        #self.bn1=nn.BatchNorm1d(2*output_size)\n        \n        #self.linear2=nn.Linear(2*output_size, output_size)\n        #self.bn2=nn.BatchNorm1d(output_size)\n        \n        self.conv1=nn.Conv1d(input_size, 2*output_size, 5, padding=2)\n        self.bn1=nn.BatchNorm1d(2*output_size)\n        \n        self.conv2=nn.Conv1d(2*output_size, output_size, 5, padding=2)\n        self.bn2=nn.BatchNorm1d(output_size)\n        \n        self.activation=nn.GELU()\n        self.dropout=nn.Dropout(0.2)\n        \n    def forward(self, x):\n        #x=self.pre_layernorm(x)\n        x=x.transpose(1, 2)\n        x=self.pre_bn(x)\n        x=self.dropout(self.activation(self.bn1(self.conv1(x))))\n        x=self.activation(self.bn2(self.conv2(x)))\n        x=x.transpose(1, 2)\n        \n        return x","metadata":{"execution":{"iopub.status.busy":"2021-09-22T13:50:53.559097Z","iopub.execute_input":"2021-09-22T13:50:53.559421Z","iopub.status.idle":"2021-09-22T13:50:53.594776Z","shell.execute_reply.started":"2021-09-22T13:50:53.559389Z","shell.execute_reply":"2021-09-22T13:50:53.593679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class FFN(nn.Module):\n    def __init__(self, sz):\n        super().__init__()\n        self.bn1=nn.BatchNorm1d(sz)\n        self.linear1=nn.Linear(sz, 2*sz)\n        \n        self.bn2=nn.BatchNorm1d(2*sz)\n        self.linear2=nn.Linear(2*sz, sz)\n        \n        \n        self.activation=nn.GELU()\n        self.dropout=nn.Dropout(0.2)\n        \n    def forward(self, x):\n        x=self.linear1(self.dropout(self.activation(self.bn1(x))))\n        x=self.linear2(self.dropout(self.activation(self.bn2(x))))\n        return x\n\n\nclass ConvHead(nn.Module):\n    def __init__(self, dmodel, pool_size):\n        super().__init__()\n        self.convs=nn.Sequential(\n            nn.Conv1d(dmodel, 2*dmodel, 7, padding=3, stride=4),\n            nn.BatchNorm1d( 2*dmodel),\n            nn.GELU(),\n            \n            nn.Conv1d(2*dmodel, dmodel, 5, padding=2, stride=4),\n            nn.BatchNorm1d(dmodel),\n            nn.GELU(),\n            \n            nn.AdaptiveAvgPool1d(pool_size)\n        )\n    def forward(self, x):\n        bsize=x.size(0)\n        x=self.convs(x.transpose(1, 2))\n        return x.view(bsize, -1)\n    \n    \nclass MLPHead(nn.Module):\n    def __init__(self, sz, num_layers):\n        super().__init__()\n        self.ffn=nn.ModuleList(\n            [FFN( sz ) for _ in range(num_layers)]\n        )\n        self.dropout=nn.Dropout(0.2)\n        self.out=nn.Linear(sz, 2)\n        \n    def forward(self, x):\n        x=self.dropout(x)\n        for i, _ in enumerate(self.ffn):\n            x=self.ffn[i](x)\n        y=self.out(x)\n        return y\n    \nfrom torch.autograd import Variable\nclass PositionalEncoding(nn.Module):\n    \"Implement the PE function.\"\n    def __init__(self, d_model, dropout, max_len=config.num_buckets):\n        super(PositionalEncoding, self).__init__()\n        self.dropout = nn.Dropout(p=dropout)\n        \n        # Compute the positional encodings once in log space.\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2) *\n                             -(np.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0)\n        self.register_buffer('pe', pe)\n        \n    def forward(self, x):\n        x = x + Variable(self.pe[:, :x.size(1)], \n                         requires_grad=False)\n        return x\n    \nclass OptiverEncoder(nn.Module):\n    def __init__(self, params):\n        super().__init__()\n        self.dmodel=params['dmodel']\n        self.in_features=params['in_features']\n        self.out_features=params['out_features']\n        self.num_ffn_layers=params['num_ffn_layers']\n        self.pool_size=params['pool_size']\n        \n        \n        self.dropout=nn.Dropout(0.1)\n        self.feature_extractor=FeatureExtractorWith1DConv(self.in_features, self.out_features)\n        self.positions = PositionalEncoding(self.out_features, 0.1)        \n        self.attn_layers=nn.ModuleList([TimeSeriesAttentionLayer(num_features=self.out_features,\n                                                                 dmodel=self.dmodel,\n                                                                 nhead=params['nhead'],\n                                                                 dim_feed_forward=params['dim_feed_forward'],\n                                                                ) for _ in range(params['num_attention_layers'])])\n        \n    def forward(self, x, mask):\n        batch_size=x.size(0)\n        seq_len=x.size(1)\n        x=self.positions(self.feature_extractor(x))\n        for attn_layer in self.attn_layers:\n            x=attn_layer(x, mask)\n        return x\n    \nclass OptiverModel(nn.Module):\n    def __init__(self, params):\n        super().__init__()\n        self.dmodel=params['dmodel']\n        self.in_features=params['in_features']\n        self.out_features=params['out_features']\n        self.num_ffn_layers=params['num_ffn_layers']\n        self.pool_size=params['pool_size']\n        \n        self.encoder=OptiverEncoder(params)\n        \n        \n        self.alpha_model=MLPHead(2 * self.out_features , self.num_ffn_layers)\n    def pooling(self, x, mask):\n        mask=mask.unsqueeze(dim=-1)\n        mean_pool=(x * mask).sum(dim=1)/mask.sum(dim=1)\n        \n        max_pool=x.masked_fill(mask == 0, -1e9)\n        max_pool=torch.max(max_pool, dim=1)[0]\n        return torch.cat([mean_pool, max_pool], dim=1)\n    \n    def forward(self, x, mask):\n        batch_size=x.size(0)\n        seq_len=x.size(1)\n        \n        x=self.encoder(x, mask)\n        x=self.pooling(x, mask)\n        \n        yhat_alpha=self.alpha_model(x)\n        return yhat_alpha","metadata":{"execution":{"iopub.status.busy":"2021-09-22T13:50:53.597097Z","iopub.execute_input":"2021-09-22T13:50:53.597546Z","iopub.status.idle":"2021-09-22T13:50:53.63615Z","shell.execute_reply.started":"2021-09-22T13:50:53.597506Z","shell.execute_reply":"2021-09-22T13:50:53.634526Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading models","metadata":{}},{"cell_type":"code","source":"device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(device)\nmodels=[\n    torch.load('../input/optiverpool-head-check2/best_rmspe1.pt', map_location=device)\n]","metadata":{"execution":{"iopub.status.busy":"2021-09-22T13:50:53.638677Z","iopub.execute_input":"2021-09-22T13:50:53.638968Z","iopub.status.idle":"2021-09-22T13:50:53.696412Z","shell.execute_reply.started":"2021-09-22T13:50:53.638938Z","shell.execute_reply":"2021-09-22T13:50:53.695104Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# inference","metadata":{}},{"cell_type":"code","source":"def inference(models, features):\n    test_dataset=OptiverDataset(features)\n    test_dataloader=torch.utils.data.DataLoader(test_dataset,\n                                                batch_size=512,\n                                                shuffle=False,\n                                                num_workers=2,\n                                                pin_memory=True,\n                                                drop_last=False)\n    \n    ypred=[]\n    for (X, price_diff, mask, wap1, wap2, s1, s2, rv1, rv2) in test_dataloader:\n        X=torch.cat([X, price_diff, wap1.unsqueeze(-1), wap2.unsqueeze(-1),\n                             s1.unsqueeze(-1),s2.unsqueeze(-1)], dim=-1)\n        X=X.to(device)\n        mask=mask.to(device)\n        rv1=rv1.to(device)\n        rv2=rv2.to(device)\n        \n        \n        for model in models:\n            model.eval()\n            with torch.no_grad():\n                yhat_alpha=model(X, mask)\n                yhat1 = torch.abs(yhat_alpha[:, 0].view(-1) * rv1)\n                yhat2 = torch.abs(yhat_alpha[:, 1].view(-1) * rv2)\n\n                yhat1=yhat1.view(-1).detach().cpu()\n                yhat1=torch.clamp(yhat1, 0, 1.5)\n                \n                yhat2=yhat2.view(-1).detach().cpu()\n                yhat2=torch.clamp(yhat1, 0, 1.5)\n\n                \n                yhat1=yhat1.numpy()\n                yhat2=yhat2.numpy()\n                \n                ypred+=yhat1.tolist()\n                #ypred+=((yhat1 + yhat2)/2).tolist()\n    return ypred","metadata":{"execution":{"iopub.status.busy":"2021-09-22T13:50:53.698689Z","iopub.execute_input":"2021-09-22T13:50:53.699069Z","iopub.status.idle":"2021-09-22T13:50:53.713396Z","shell.execute_reply.started":"2021-09-22T13:50:53.699026Z","shell.execute_reply":"2021-09-22T13:50:53.711783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_df=[]\nfor i, stock_file in enumerate(os.listdir(test_book_folder)):\n    file_path=os.path.join(test_book_folder, stock_file)\n    stock_id=stock_file.split('=')[-1]\n    \n    stock_df=pd.read_parquet(file_path)\n    stock_df['stock_id']=int(stock_id)\n    \n    stock_df=stock_df.sort_values(['time_id', 'seconds_in_bucket'])\n    for colname in log_transform_features:\n        stock_df[colname]=np.log( 1+stock_df[colname] )\n    \n    \n    stock_df['features']=stock_df[feature_columns].values.tolist()\n    stock_df['features']=stock_df['features'].apply(np.array)\n    \n    \n    stock_df=stock_df.groupby(['stock_id', 'time_id'])[['features']].agg(list).reset_index()\n    stock_df['features']=stock_df['features'].apply(np.array)\n    \n    features=stock_df.features.values\n    \n    \n    stock_df['target']=inference(models, features)\n    stock_df.drop(columns='features', inplace=True)\n    all_df.append(stock_df)\n    gc.collect()\nall_df=pd.concat(all_df)\nall_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-22T13:50:53.714993Z","iopub.execute_input":"2021-09-22T13:50:53.715853Z","iopub.status.idle":"2021-09-22T13:50:54.049219Z","shell.execute_reply.started":"2021-09-22T13:50:53.71579Z","shell.execute_reply":"2021-09-22T13:50:54.047957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df=pd.read_csv('../input/optiver-realized-volatility-prediction/test.csv')\ntest_df=test_df.merge(all_df, how='left')\ntest_df.fillna(0.0, inplace=True)\ntest_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-22T13:50:54.050517Z","iopub.execute_input":"2021-09-22T13:50:54.051364Z","iopub.status.idle":"2021-09-22T13:50:54.073708Z","shell.execute_reply.started":"2021-09-22T13:50:54.051323Z","shell.execute_reply":"2021-09-22T13:50:54.072686Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df[['row_id', 'target']].to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-09-22T13:50:54.076273Z","iopub.execute_input":"2021-09-22T13:50:54.076632Z","iopub.status.idle":"2021-09-22T13:50:54.086365Z","shell.execute_reply.started":"2021-09-22T13:50:54.076601Z","shell.execute_reply":"2021-09-22T13:50:54.085154Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}