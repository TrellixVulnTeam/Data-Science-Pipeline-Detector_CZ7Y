{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport pickle\n\nimport torch\nimport torch.nn as nn\n\nimport matplotlib.pyplot as plt","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-09-02T13:10:21.538633Z","iopub.execute_input":"2021-09-02T13:10:21.539082Z","iopub.status.idle":"2021-09-02T13:10:22.808682Z","shell.execute_reply.started":"2021-09-02T13:10:21.538989Z","shell.execute_reply":"2021-09-02T13:10:22.807646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(device)","metadata":{"execution":{"iopub.status.busy":"2021-09-02T13:10:22.810252Z","iopub.execute_input":"2021-09-02T13:10:22.810556Z","iopub.status.idle":"2021-09-02T13:10:22.817173Z","shell.execute_reply.started":"2021-09-02T13:10:22.810526Z","shell.execute_reply":"2021-09-02T13:10:22.81627Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_volume_features(df):\n    df['ask_vol']=df['ask_size1']+df['ask_size2']\n    df['bid_vol']=df['bid_size1']+df['bid_size2']\n    \n    df['total_volume1']=df['ask_size1']+df['bid_size1']\n    df['total_volume2']=df['ask_size2']+df['bid_size2']\n    df['total_volume']=df['ask_vol']+df['bid_vol']\n    \n    df['volume_spread1']=(df['ask_size1']+1e-12)/(df['bid_size1']+1e-12)\n    df['volume_spread2']=(df['ask_size2']+1e-12)/(df['bid_size2']+1e-12)\n    \n    return df\n\ndef get_price_features(df):\n    df['ask_price_spread']=df['ask_price2'] - df['ask_price1']\n    df['bid_price_spread']=df['bid_price1'] - df['bid_price2']\n    df['price_spread1']=df['ask_price1']-df['bid_price1']\n    df['price_spread2']=df['ask_price2']-df['bid_price2']\n    return df\n\ndef calculate_wap1(df):\n    wap=(df['ask_price1'] * df['bid_size1']) + (df['bid_price1'] * df['ask_size1'])\n    wap/=(df['ask_size1'] + df['bid_size1'])\n    return wap\n\ndef calculate_wap2(df):\n    wap=(df['ask_price2'] * df['bid_size2']) + (df['bid_price2'] * df['ask_size2'])\n    wap/=(df['ask_size2'] + df['bid_size2'])\n    return wap\n\n\ndef calculate_wap3(wap1, wap2):\n    wap3=(wap1+wap2)/2\n    return wap3\n\ndef calculate_log_return(s):\n    s=np.log(s)\n    s=np.diff(s)\n    s=np.append(0, s)\n    return s\n\ndef calculate_realized_volatitlity(s):\n    s=s**2\n    s=s.cumsum()\n    s=np.sqrt(s)\n    return s\n\ndef get_log_return(df, wap_colname):\n    log_return_df=df.groupby('time_id')[[wap_colname]].agg(list).reset_index()\n    log_return_df[wap_colname]=log_return_df[wap_colname].apply(np.array)\n    log_return_df[wap_colname]=log_return_df[wap_colname].apply(calculate_log_return)\n    log_return_values=np.concatenate(log_return_df[wap_colname].values).ravel()\n    return log_return_values\n\ndef get_realized_volatility(df, colname):\n    rv_df=df.groupby('time_id')[[colname]].agg(list).reset_index()\n    rv_df[colname]=rv_df[colname].apply(np.array)\n    rv_df[colname]=rv_df[colname].apply(calculate_realized_volatitlity)\n    rv_values=np.concatenate(rv_df[colname].values).ravel()\n    return rv_values\n\ndef get_features(df):\n    df=df.sort_values(['time_id', 'seconds_in_bucket'])\n    \n    df=get_price_features(df)\n    df=get_volume_features(df)\n    \n    df['wap1']=calculate_wap1(df)\n    df['wap2']=calculate_wap2(df)\n    df['wap3']=calculate_wap3(df['wap1'], df['wap2'])\n    \n    \n    df['log_return1']=get_log_return(df, 'wap1')\n    df['log_return2']=get_log_return(df, 'wap2')\n    df['log_return3']=get_log_return(df, 'wap3')\n    \n    df['rv1']=get_realized_volatility(df, 'log_return1')\n    df['rv2']=get_realized_volatility(df, 'log_return2')\n    df['rv3']=get_realized_volatility(df, 'log_return3')\n    \n    return df\n\n\ndef get_dummy_df(df):\n    time_id=df.time_id.unique()\n    seconds_in_bucket=np.arange(600)\n    \n    \n    dummy_df=pd.DataFrame.from_dict({'time_id': np.repeat(time_id, seconds_in_bucket.shape[0]), \n                                     'seconds_in_bucket': np.tile(seconds_in_bucket, time_id.shape[0])})\n    return dummy_df","metadata":{"execution":{"iopub.status.busy":"2021-09-02T13:10:22.819154Z","iopub.execute_input":"2021-09-02T13:10:22.819769Z","iopub.status.idle":"2021-09-02T13:10:22.845371Z","shell.execute_reply.started":"2021-09-02T13:10:22.819701Z","shell.execute_reply":"2021-09-02T13:10:22.844198Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mean_columns=[\n    'bid_price1', 'ask_price1', 'bid_price2', 'ask_price2', \n    'bid_size1', 'ask_size1', 'bid_size2', 'ask_size2',\n    'ask_price_spread', 'bid_price_spread', 'price_spread1', 'price_spread2', \n    'ask_vol', 'bid_vol', 'total_volume1', 'total_volume2', 'total_volume', \n    'volume_spread1', 'volume_spread2',\n    'wap1', 'wap2', 'wap3', 'log_return1', 'log_return2', 'log_return3', \n]\n\nmax_columns=['rv1', 'rv2', 'rv3']\nlog_transform_features=['bid_size1', 'ask_size1', 'bid_size2', 'ask_size2', 'ask_vol', 'bid_vol',\n                        'total_volume1','total_volume2', 'total_volume', 'volume_spread1', 'volume_spread2']\n\n\ndef aggregate_buckets(df):\n    mean_df=df.groupby(['stock_id', 'time_id', 'bucket'])[mean_columns].mean().reset_index()\n    max_df=df.groupby(['stock_id', 'time_id', 'bucket'])[max_columns].max().reset_index()\n    return mean_df.merge(max_df)","metadata":{"execution":{"iopub.status.busy":"2021-09-02T13:10:22.847253Z","iopub.execute_input":"2021-09-02T13:10:22.848042Z","iopub.status.idle":"2021-09-02T13:10:22.86359Z","shell.execute_reply.started":"2021-09-02T13:10:22.847988Z","shell.execute_reply":"2021-09-02T13:10:22.862432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess_book_data(book_folder):\n    all_df=[]\n    for i, filepath in enumerate(os.listdir(book_folder)):\n        path=os.path.join(book_folder, filepath)\n        stock_id=int(filepath.split('=')[-1])\n        \n        df=pd.read_parquet(path)\n        df['stock_id']=stock_id\n        \n        dummy_df=get_dummy_df(df)\n        df=dummy_df.merge(df, how='left')\n        df['bucket']=df['seconds_in_bucket']//10\n        \n        df=get_features(df)\n        df.fillna(method='ffill', inplace=True)\n        \n        stock_df=aggregate_buckets(df)\n        for colname in log_transform_features:\n            stock_df[colname]=np.log( 1+stock_df[colname] )\n        \n        \n        stock_df['features']=stock_df[mean_columns+max_columns].values.tolist()\n        stock_df['features']=stock_df['features'].apply(np.array)\n        stock_df=stock_df.groupby(['stock_id', 'time_id'])[['bucket', 'features']].agg(list).reset_index()\n\n        stock_df['bucket']=stock_df['bucket'].apply(np.array)\n        stock_df['features']=stock_df['features'].apply(np.array)\n    \n        all_df.append(stock_df)\n        \n    all_df=pd.concat(all_df)\n    all_df.reset_index(drop=True, inplace=True)\n    return all_df","metadata":{"execution":{"iopub.status.busy":"2021-09-02T13:10:22.865609Z","iopub.execute_input":"2021-09-02T13:10:22.866171Z","iopub.status.idle":"2021-09-02T13:10:22.881562Z","shell.execute_reply.started":"2021-09-02T13:10:22.866118Z","shell.execute_reply":"2021-09-02T13:10:22.880358Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features_df=preprocess_book_data('../input/optiver-realized-volatility-prediction/book_test.parquet')\nfeatures_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-02T13:10:22.882935Z","iopub.execute_input":"2021-09-02T13:10:22.883445Z","iopub.status.idle":"2021-09-02T13:10:23.251889Z","shell.execute_reply.started":"2021-09-02T13:10:22.883411Z","shell.execute_reply":"2021-09-02T13:10:23.250966Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class config:\n    num_buckets= 60\n    num_features= 28\n    batch_size=256\n    epochs=20","metadata":{"execution":{"iopub.status.busy":"2021-09-02T13:10:23.253199Z","iopub.execute_input":"2021-09-02T13:10:23.253618Z","iopub.status.idle":"2021-09-02T13:10:23.258853Z","shell.execute_reply.started":"2021-09-02T13:10:23.253579Z","shell.execute_reply":"2021-09-02T13:10:23.257352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset","metadata":{}},{"cell_type":"code","source":"class OptiverDataset(torch.utils.data.Dataset):\n    def __init__(self, df):\n        self.df=df\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        row=self.df.iloc[idx]\n        bucket_num=row.bucket\n        bucket_features=row.features\n        \n        X=torch.zeros((config.num_buckets, config.num_features), dtype=torch.float32)\n        for i, bucket_id in enumerate(bucket_num):\n            X[bucket_id] = torch.tensor(bucket_features[i], dtype=torch.float32)\n        return X","metadata":{"execution":{"iopub.status.busy":"2021-09-02T13:10:23.262517Z","iopub.execute_input":"2021-09-02T13:10:23.263096Z","iopub.status.idle":"2021-09-02T13:10:23.275111Z","shell.execute_reply.started":"2021-09-02T13:10:23.263039Z","shell.execute_reply":"2021-09-02T13:10:23.273256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"def get_activation_fn(activation):\n    if activation=='gelu':\n        return nn.GELU()\n    elif activation=='relu':\n        return nn.ReLU()\n    \ndef attention(query, key, value, dropout=None):\n    d_k=query.size(-1)\n    scores=torch.matmul( query, key.transpose(-1, -2) )/np.sqrt(d_k)\n    scores=torch.tril(scores)\n    scores=scores.masked_fill(scores == 0, -1e9)\n    p_attn=torch.softmax(scores, dim=-1)\n    \n    x_attn=torch.matmul(p_attn, value)\n    \n    if dropout:\n        x_attn=dropout(x_attn)\n    return p_attn, x_attn\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, dmodel, nhead,activation,norm,dropout):\n        super().__init__()\n        self.dmodel=dmodel\n        self.nhead=nhead\n        self.d_k=dmodel//nhead #Size\n        \n        self.activation=activation\n        self.norm=norm\n        self.dropout=dropout\n        \n        self.Q=nn.Linear(dmodel, dmodel)\n        self.K=nn.Linear(dmodel, dmodel)\n        self.V=nn.Linear(dmodel, dmodel)\n        self.W=nn.Linear(dmodel, dmodel)\n        \n    def forward(self, x):\n        bsize=x.size(0)\n        query=self.Q(x).view(bsize, -1, self.nhead, self.d_k)\n        key=self.K(x).view(bsize, -1, self.nhead, self.d_k)\n        value=self.V(x).view(bsize, -1, self.nhead, self.d_k)\n        \n        p_attn, x_attn=attention(query, key, value, self.dropout)\n        x_attn=x_attn.view(bsize, -1, self.nhead*self.d_k)\n        x_attn=self.W(x_attn)\n        x=self.norm(x+x_attn)\n        return x\n\nclass TimeSeriesAttentionLayer(nn.Module):\n    def __init__(self,\n                 dmodel=128,\n                 nhead=4,\n                 dim_feed_forward=512,\n                 activation='gelu', \n                 dropout=0.1):\n        \n        super().__init__()\n        self.dmodel=dmodel\n        self.nhead=nhead\n        self.dim_feed_forward=dim_feed_forward\n        self.activation=get_activation_fn(activation)\n        self.norm=nn.LayerNorm(dmodel)\n        self.dropout=nn.Dropout(dropout)\n        \n        self.multihead_attn=MultiHeadAttention(dmodel,\n                                               nhead,\n                                               self.activation,\n                                               self.norm,\n                                               self.dropout)\n        \n        self.linear1=nn.Linear(dmodel, dim_feed_forward)\n        self.linear2=nn.Linear(dim_feed_forward, dmodel)\n        \n    def forward(self, x):\n        x=self.multihead_attn(x)\n        x_ffn=self.linear2(self.dropout(self.activation(self.linear1(x))))\n        x=self.norm(x+x_ffn)\n        return x\n\nclass FeatureExtractor(nn.Module):\n    def __init__(self, sz):\n        super().__init__()\n        self.pre_bn=nn.BatchNorm1d(config.num_buckets , config.num_features)\n        \n        self.linear1=nn.Linear(config.num_features, 2*sz)\n        self.bn1=nn.BatchNorm1d(config.num_buckets, 2*sz)\n        \n        self.linear2=nn.Linear(2*sz, sz)\n        self.bn2=nn.BatchNorm1d(config.num_buckets, sz)\n        \n        self.dropout=nn.Dropout(0.2)\n        self.activation=nn.GELU()\n    def forward(self, x):\n        x=self.pre_bn(x)\n        x=self.dropout( self.bn1( self.activation(self.linear1(x)) ) )\n        x=self.dropout( self.bn2( self.activation(self.linear2(x)) ) )\n        return x\n    \n    \nclass FeatureExtractorWith1DConv(nn.Module):\n    def __init__(self, sz):\n        super().__init__()\n        self.pre_bn=nn.BatchNorm1d(config.num_buckets , config.num_features)\n        \n        self.conv1=nn.Conv1d(config.num_features, sz, 3, padding=1)\n        self.bn1=nn.BatchNorm1d(config.num_buckets, sz)\n        \n        \n        self.conv2=nn.Conv1d(sz, sz, 3, padding=1)\n        self.bn2=nn.BatchNorm1d(config.num_buckets, sz)\n        \n        self.activation=nn.ReLU()\n        \n    def forward(self, x):\n        x=self.pre_bn(x)\n        x=self.activation( self.bn1( self.conv1(x.transpose(1, 2)).transpose(1, 2) ))\n        #x=self.activation( self.bn2( self.conv2(x.transpose(1, 2)).transpose(1, 2) ))\n        \n        return x","metadata":{"execution":{"iopub.status.busy":"2021-09-02T13:10:23.277304Z","iopub.execute_input":"2021-09-02T13:10:23.277629Z","iopub.status.idle":"2021-09-02T13:10:23.312168Z","shell.execute_reply.started":"2021-09-02T13:10:23.277599Z","shell.execute_reply":"2021-09-02T13:10:23.311004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Auxilary_FFN(nn.Module):\n    def __init__(self, sz):\n        super().__init__()\n        self.linear=nn.Linear(sz, sz)\n        self.bn=nn.BatchNorm1d(config.num_buckets , sz)\n        self.activation=nn.GELU()\n        self.dropout=nn.Dropout(0.2)\n        \n    def forward(self, x):\n        x=self.bn(x)\n        x=self.activation(x)\n        x=self.dropout(x)\n        x=self.linear(x)\n        return x\n\n\nclass FFN(nn.Module):\n    def __init__(self, sz):\n        super().__init__()\n        self.linear=nn.Linear(sz, sz)\n        self.bn=nn.BatchNorm1d(sz)\n        self.activation=nn.GELU()\n        self.dropout=nn.Dropout(0.2)\n        \n    def forward(self, x):\n        x=self.bn(x)\n        x=self.activation(x)\n        x=self.dropout(x)\n        x=self.linear(x)    \n        return x\n\n\nclass AttentionHead(nn.Module):\n    def __init__(self, dmodel, dropout):\n        super().__init__()\n        self.dropout=dropout\n        self.W=nn.Linear(dmodel, 1)\n    def forward(self, x):\n        scores=self.W(x).squeeze(-1)\n        p_attn=torch.softmax(scores, dim=-1)\n        \n        if self.dropout:\n            p_attn=self.dropout(p_attn)\n        x_attn=torch.matmul(p_attn, x)\n        x_attn=x_attn.sum(dim=1)\n        return p_attn, x_attn","metadata":{"execution":{"iopub.status.busy":"2021-09-02T13:10:23.313485Z","iopub.execute_input":"2021-09-02T13:10:23.313985Z","iopub.status.idle":"2021-09-02T13:10:23.332036Z","shell.execute_reply.started":"2021-09-02T13:10:23.313934Z","shell.execute_reply":"2021-09-02T13:10:23.330722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class PrimaryHead(nn.Module):\n    def __init__(self, hsize):\n        super().__init__()\n        self.hsize=hsize\n        self.attn_dropout=nn.Dropout(0.1)\n        self.attn_head=AttentionHead(hsize, self.attn_dropout)\n        self.ffn=nn.ModuleList(\n            [FFN(hsize) for _ in range(3)]\n        )\n        self.primary_out=nn.Linear(hsize, 1)\n        \n    def forward(self, x):\n        p_attn, x=self.attn_head(x)\n        for i, _ in enumerate(self.ffn):\n            if i==0:\n                continue\n            x=self.ffn[i](x + self.ffn[i-1](x))\n        y=self.primary_out(x)\n        return p_attn, y\n\nclass AuxilaryHead(nn.Module):\n    def __init__(self, hsize):\n        super().__init__()\n        self.hsize=hsize\n        self.ffn=nn.ModuleList(\n            [Auxilary_FFN(hsize) for _ in range(3)]\n        )\n        self.aux_out=nn.Linear(hsize, 1)\n    def forward(self, x):\n        for i, _ in enumerate(self.ffn):\n            if i==0:\n                continue\n            x=self.ffn[i](x + self.ffn[i-1](x))\n        y=self.aux_out(x)\n        return y","metadata":{"execution":{"iopub.status.busy":"2021-09-02T13:10:23.333724Z","iopub.execute_input":"2021-09-02T13:10:23.334151Z","iopub.status.idle":"2021-09-02T13:10:23.351008Z","shell.execute_reply.started":"2021-09-02T13:10:23.334115Z","shell.execute_reply":"2021-09-02T13:10:23.350046Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class OptiverModel(nn.Module):\n    def __init__(self, model_size=128):\n        super().__init__()\n        self.model_size=model_size\n        self.feature_extractor=FeatureExtractorWith1DConv(model_size)\n        self.pos_embeddings=nn.Embedding(config.num_buckets, model_size, max_norm=1)\n        self.positions=torch.arange(config.num_buckets, dtype=torch.long).to(device)\n        self.attn_layers=nn.ModuleList([TimeSeriesAttentionLayer() for _ in range(5)])\n        \n        self.primary_model=PrimaryHead(model_size)\n        self.auxilary_ask1=AuxilaryHead(model_size)\n        self.auxilary_bid1=AuxilaryHead(model_size)\n        self.auxilary_wap1=AuxilaryHead(model_size)\n        \n    def forward(self, x):\n        x=self.feature_extractor(x)\n        x=x+(self.pos_embeddings(self.positions).unsqueeze(0)/np.sqrt(self.model_size))\n        for attn_layer in self.attn_layers:\n            x=attn_layer(x)\n            \n        p_attn, yprimary=self.primary_model(x)\n        yaux_ask1=self.auxilary_ask1(x)\n        yaux_bid1=self.auxilary_bid1(x)\n        yaux_wap1=self.auxilary_wap1(x)\n        \n        return {\n            'p_attn': p_attn,\n            'yprimary': yprimary,\n            'yask1': yaux_ask1,\n            'ybid1': yaux_bid1,\n            'ywap1': yaux_wap1\n        }","metadata":{"execution":{"iopub.status.busy":"2021-09-02T13:10:23.352144Z","iopub.execute_input":"2021-09-02T13:10:23.352559Z","iopub.status.idle":"2021-09-02T13:10:23.369453Z","shell.execute_reply.started":"2021-09-02T13:10:23.352529Z","shell.execute_reply":"2021-09-02T13:10:23.368291Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inference","metadata":{}},{"cell_type":"code","source":"def inference(model, test_dataloader):\n    model.eval()\n    ypred=[]\n    for X in test_dataloader:\n        X=X.to(device)\n        with torch.no_grad():\n            output=model(X)\n            y=output['yprimary']\n            y=y.view(-1).detach()\n            y=torch.clamp(y, 0, 1.0)\n            y=y.tolist()\n            ypred+=y\n    return ypred","metadata":{"execution":{"iopub.status.busy":"2021-09-02T13:10:23.370625Z","iopub.execute_input":"2021-09-02T13:10:23.370947Z","iopub.status.idle":"2021-09-02T13:10:23.38841Z","shell.execute_reply.started":"2021-09-02T13:10:23.370916Z","shell.execute_reply":"2021-09-02T13:10:23.387269Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dataset=OptiverDataset(features_df)\ntest_dataloader=torch.utils.data.DataLoader(test_dataset,\n                                            batch_size=1024,\n                                            shuffle=False,\n                                            drop_last=False)\n\nmodel=OptiverModel()\nmodel=torch.load('../input/optiver-attention-multitask-models/best_rmspe.pt', map_location=device)\nmodel=model.to(device)","metadata":{"execution":{"iopub.status.busy":"2021-09-02T13:10:23.392563Z","iopub.execute_input":"2021-09-02T13:10:23.392985Z","iopub.status.idle":"2021-09-02T13:10:23.67676Z","shell.execute_reply.started":"2021-09-02T13:10:23.392934Z","shell.execute_reply":"2021-09-02T13:10:23.675686Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df=pd.read_csv('../input/optiver-realized-volatility-prediction/test.csv')\n\nfeatures_df['target']=inference(model, test_dataloader)\ntest_df=test_df.merge(features_df, how='left')\ntest_df.fillna(0, inplace=True)\ntest_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-02T13:10:23.678345Z","iopub.execute_input":"2021-09-02T13:10:23.678696Z","iopub.status.idle":"2021-09-02T13:10:23.912735Z","shell.execute_reply.started":"2021-09-02T13:10:23.678658Z","shell.execute_reply":"2021-09-02T13:10:23.911653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df[['row_id', 'target']].to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-09-02T13:10:23.914155Z","iopub.execute_input":"2021-09-02T13:10:23.914476Z","iopub.status.idle":"2021-09-02T13:10:23.926025Z","shell.execute_reply.started":"2021-09-02T13:10:23.914444Z","shell.execute_reply":"2021-09-02T13:10:23.92486Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}