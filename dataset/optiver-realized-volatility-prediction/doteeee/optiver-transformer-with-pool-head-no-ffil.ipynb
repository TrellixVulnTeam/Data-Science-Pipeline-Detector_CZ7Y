{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"https://towardsdatascience.com/stock-predictions-with-state-of-the-art-transformer-and-time-embeddings-3a4485237de6\nhttps://papers.nips.cc/paper/2019/file/6775a0635c302542da2c32aa19d86be0-Paper.pdf","metadata":{}},{"cell_type":"code","source":"USE_TPU=False\n\nif USE_TPU:\n    !curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n    !python pytorch-xla-env-setup.py --version nightly --apt-packages libomp5 libopenblas-dev\n\n\n    import torch_xla\n    import torch_xla.core.xla_model as xm\n    import torch_xla.distributed.parallel_loader as pl\n    import torch_xla.distributed.xla_multiprocessing as xmp   \n    import torch_xla.debug.metrics as met\n\n    \n    import torch\n    import torch.nn as nn\n    device=xm.xla_device()\n    !pip install -U numpy\nelse:\n    \n    import torch\n    import torch.nn as nn\n    device=torch.device( 'cuda' if torch.cuda.is_available() else 'cpu' )\n\nprint(device)","metadata":{"execution":{"iopub.status.busy":"2021-09-20T03:42:23.230571Z","iopub.execute_input":"2021-09-20T03:42:23.231116Z","iopub.status.idle":"2021-09-20T03:42:24.566907Z","shell.execute_reply.started":"2021-09-20T03:42:23.231005Z","shell.execute_reply":"2021-09-20T03:42:24.565841Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport gc\nimport json\nimport pickle\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport random\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split","metadata":{"execution":{"iopub.status.busy":"2021-09-20T03:42:24.56853Z","iopub.execute_input":"2021-09-20T03:42:24.568824Z","iopub.status.idle":"2021-09-20T03:42:25.515318Z","shell.execute_reply.started":"2021-09-20T03:42:24.568795Z","shell.execute_reply":"2021-09-20T03:42:25.514351Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def seed_everything(s):\n    random.seed(s)\n    np.random.seed(s)\n    torch.manual_seed(s)\n    torch.cuda.manual_seed_all(s)\n    torch.cuda.manual_seed(s)\n    torch.backends.cudnn.deterministic=True\n    \nseed_everything(2012)","metadata":{"execution":{"iopub.status.busy":"2021-09-20T03:42:25.520072Z","iopub.execute_input":"2021-09-20T03:42:25.520415Z","iopub.status.idle":"2021-09-20T03:42:25.529864Z","shell.execute_reply.started":"2021-09-20T03:42:25.520386Z","shell.execute_reply":"2021-09-20T03:42:25.528763Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class config:\n    num_buckets= 600\n    num_features= 11\n    batch_size=128\n    epochs=15","metadata":{"execution":{"iopub.status.busy":"2021-09-20T03:42:25.53138Z","iopub.execute_input":"2021-09-20T03:42:25.531738Z","iopub.status.idle":"2021-09-20T03:42:25.537104Z","shell.execute_reply.started":"2021-09-20T03:42:25.531674Z","shell.execute_reply":"2021-09-20T03:42:25.536373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_dataset():\n    all_df=[]\n    train_book_folder = \"../input/optiver-no-ffill-dataset/train_book\"\n    for i, stock_file in enumerate(os.listdir(train_book_folder)):\n        if i%20==0:\n            print(i)\n        file_path=os.path.join(train_book_folder, stock_file)\n        stock_df=pd.read_pickle(file_path)\n        all_df.append(stock_df)\n    \n    book_df=pd.concat(all_df)\n    return book_df\n\nbook_df=get_dataset()\nbook_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-20T03:42:25.53823Z","iopub.execute_input":"2021-09-20T03:42:25.538719Z","iopub.status.idle":"2021-09-20T03:43:56.875773Z","shell.execute_reply.started":"2021-09-20T03:42:25.538688Z","shell.execute_reply":"2021-09-20T03:43:56.87505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset","metadata":{}},{"cell_type":"code","source":"window=4\ndef smoothing(x):\n    if x.shape[0] < window:\n        return x\n    x=x.copy()\n    try:\n        cum_x=np.cumsum(x, axis=0)\n        x_rolling=(cum_x[window:, :] - cum_x[:-window, :])/window\n        x[window:, :]=x_rolling\n    except:\n        print('dfsf')\n        pass\n    return x\n\ndef smoothing_1d(x):\n    if x.shape[0] < window:\n        return x\n    x=x.copy()\n    try:\n        cum_x=np.cumsum(x)\n        x_rolling=(cum_x[window:] - cum_x[:-window])/window\n        x[window:]=x_rolling\n    except:\n        pass\n    return x","metadata":{"execution":{"iopub.status.busy":"2021-09-20T03:43:56.876874Z","iopub.execute_input":"2021-09-20T03:43:56.877282Z","iopub.status.idle":"2021-09-20T03:43:56.884581Z","shell.execute_reply.started":"2021-09-20T03:43:56.877251Z","shell.execute_reply":"2021-09-20T03:43:56.883594Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class OptiverDataset(torch.utils.data.Dataset):\n    def __init__(self, indices, features, target):\n        self.indices=indices\n        self.target=target\n        self.features=features\n        \n    def __len__(self):\n        return len(self.indices)\n    \n    def calculate_wap(self, ask_price, bid_price, ask_size, bid_size):\n        ask_size=np.exp(ask_size) - 1\n        bid_size=np.exp(bid_size) - 1\n        wap = (ask_price * bid_size) + (bid_price * ask_size)\n        wap = wap / (ask_size + bid_size)\n        return wap\n    \n    def get_log_returns(self, wap):\n        s=np.diff(np.log(wap))\n        s[s==-np.inf]=0.0\n        return s\n    \n    def get_realized_volatility(self, s):\n        rv=np.sqrt(np.sum( (s**2) ))\n        return rv\n    \n    def get_features(self, feat):\n        bid_price1=feat[:, 0]\n        ask_price1=feat[:, 1]\n        bid_price2= feat[:, 2]\n        ask_price2= feat[:, 3]\n        bid_size1=feat[:, 4]\n        ask_size1=feat[:, 5]\n        bid_size2=feat[:, 6]\n        ask_size2=feat[:, 7]\n        \n        wap1=self.calculate_wap(ask_price1, bid_price1, ask_size1, bid_size1)\n        wap2=self.calculate_wap(ask_price2, bid_price2, ask_size2, bid_size2)\n        \n        s1=self.get_log_returns(wap1)\n        s2=self.get_log_returns(wap2)\n        \n        rv1=self.get_realized_volatility(s1)\n        rv2=self.get_realized_volatility(s2)\n        \n        if rv1==0:\n            rv1=rv2\n        elif rv2==0:\n            rv2=rv1\n        \n        return (wap1,wap2, s1, s2, rv1, rv2)\n    \n    def get_binary_target_features(self, y_target, rv1, rv2):\n        y_binary1=(y_target>rv1)\n        y_binary2=(y_target>rv2)\n        \n        weights1=np.abs(y_target - rv1) + 1e-10\n        weights2=np.abs(y_target - rv2) + 1e-10\n        \n        return (y_binary1, y_binary2, weights1, weights2)\n        \n    \n    def get_price_differences(self, feat, seq_len):\n        bid_price1=feat[:, 0]\n        ask_price1=feat[:, 1]\n        bid_price2= feat[:, 2]\n        ask_price2= feat[:, 3]\n        \n        price_diff1 = bid_price1 - ask_price1\n        price_diff2 = bid_price2 - ask_price2\n        \n        ask_diff = ask_price1 - ask_price2\n        bid_diff = bid_price1 - bid_price2\n        \n        price_diff=np.zeros((4, 600) )\n        price_diff[0, -seq_len:]=price_diff1\n        price_diff[1, -seq_len:]=price_diff2\n        price_diff[2, -seq_len:]=ask_diff\n        price_diff[3, -seq_len:]=bid_diff\n        \n        price_diff=torch.tensor(price_diff, dtype=torch.float32).transpose(1, 0)\n        return price_diff\n    \n    def __getitem__(self, idx):\n        i=self.indices[idx]\n        feat=np.array(self.features[i])\n        y_target=self.target[i]\n        (seq_len, num_features) = (feat.shape[0], feat.shape[1])\n        \n        (wap1_arr,wap2_arr, s1_arr, s2_arr, rv1, rv2)=self.get_features(feat)\n        price_diff = self.get_price_differences(feat, seq_len)\n        ratio=y_target/rv1\n        if ratio >=2.23:#99.95 percent ratio - 2.23\n            ratio=2.23+np.log(1 + ratio - 2.23)\n        \n        y_target=ratio * rv1\n        (y_binary1, y_binary2, weight1, weight2)=self.get_binary_target_features(y_target, rv1, rv2)\n        \n        feat=smoothing(feat)\n        wap1_arr=smoothing_1d(wap1_arr)\n        wap2_arr=smoothing_1d(wap2_arr)\n        \n        X=torch.zeros(600, num_features)\n        \n        mask=np.zeros(600)\n        mask[-seq_len:]=1\n        \n        wap1=torch.zeros(600)\n        wap2=torch.zeros(600)\n        s1=torch.zeros(600)\n        s2=torch.zeros(600)\n        \n        mask=torch.tensor(mask, dtype=torch.long)\n        X[-seq_len:]=torch.tensor(feat, dtype=torch.float32)\n        wap1[-seq_len:]=torch.tensor(wap1_arr, dtype=torch.float32)\n        wap2[-seq_len:]=torch.tensor(wap2_arr, dtype=torch.float32)\n        \n        s1[-seq_len+1:]=torch.tensor(s1_arr, dtype=torch.float32)\n        s2[-seq_len+1:]=torch.tensor(s2_arr, dtype=torch.float32)\n        \n        weight1=torch.tensor(weight1, dtype=torch.float32)\n        weight2=torch.tensor(weight2, dtype=torch.float32)\n        \n        rv1=torch.tensor(rv1, dtype=torch.float32)\n        rv2=torch.tensor(rv2, dtype=torch.float32)\n        \n        y_binary1=torch.tensor(y_binary1, dtype=torch.float32)\n        y_binary2=torch.tensor(y_binary2, dtype=torch.float32)\n        \n        y_target=torch.tensor(y_target, dtype=torch.float32)\n        return (X, price_diff, mask, wap1, wap2, s1, s2, rv1, rv2, weight1, weight2, y_binary1, y_binary2, y_target)","metadata":{"execution":{"iopub.status.busy":"2021-09-20T03:43:56.886172Z","iopub.execute_input":"2021-09-20T03:43:56.886553Z","iopub.status.idle":"2021-09-20T03:43:56.918035Z","shell.execute_reply.started":"2021-09-20T03:43:56.886523Z","shell.execute_reply":"2021-09-20T03:43:56.916768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features=book_df.features.values\ntarget=book_df.target.values\n\ndel book_df\ngc.collect()\nprint(features.shape, target.shape)","metadata":{"execution":{"iopub.status.busy":"2021-09-20T03:43:56.919346Z","iopub.execute_input":"2021-09-20T03:43:56.919706Z","iopub.status.idle":"2021-09-20T03:43:57.049988Z","shell.execute_reply.started":"2021-09-20T03:43:56.919676Z","shell.execute_reply":"2021-09-20T03:43:57.048719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"random.seed(2012)\nindices=np.arange(features.shape[0])\nrandom.shuffle(indices)\n\ntrain_idx=indices[:int(len(indices) * 0.8)]\nvalid_idx=indices[int(len(indices) * 0.8):]\n\nprint(indices.shape, train_idx.shape, valid_idx.shape)\n\ntrain_dataset=OptiverDataset(train_idx, features, target)\nvalid_dataset=OptiverDataset(valid_idx, features, target)\n\n\ntrain_dataloader=torch.utils.data.DataLoader(\n    train_dataset,\n    batch_size=64,\n    shuffle=True,\n    drop_last=True,\n    pin_memory=True,\n    num_workers=0\n)\n\nvalid_dataloader=torch.utils.data.DataLoader(\n    valid_dataset,\n    batch_size=128,\n    shuffle=False,\n    drop_last=False,\n    pin_memory=True,\n    num_workers=2\n)\n\nprint(len(train_dataloader), len(valid_dataloader))\n","metadata":{"execution":{"iopub.status.busy":"2021-09-20T03:43:57.053011Z","iopub.execute_input":"2021-09-20T03:43:57.053411Z","iopub.status.idle":"2021-09-20T03:43:57.827155Z","shell.execute_reply.started":"2021-09-20T03:43:57.053375Z","shell.execute_reply":"2021-09-20T03:43:57.825901Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"#https://github.com/KrisKorrel/sparsemax-pytorch\n\nclass Sparsemax(nn.Module):\n    \"\"\"Sparsemax function.\"\"\"\n\n    def __init__(self, dim=None):\n        \"\"\"Initialize sparsemax activation\n        \n        Args:\n            dim (int, optional): The dimension over which to apply the sparsemax function.\n        \"\"\"\n        super(Sparsemax, self).__init__()\n\n        self.dim = -1 if dim is None else dim\n\n    def forward(self, input):\n        \"\"\"Forward function.\n        Args:\n            input (torch.Tensor): Input tensor. First dimension should be the batch size\n        Returns:\n            torch.Tensor: [batch_size x number_of_logits] Output tensor\n        \"\"\"\n        # Sparsemax currently only handles 2-dim tensors,\n        # so we reshape to a convenient shape and reshape back after sparsemax\n        input = input.transpose(0, self.dim)\n        original_size = input.size()\n        input = input.reshape(input.size(0), -1)\n        input = input.transpose(0, 1)\n        dim = 1\n\n        number_of_logits = input.size(dim)\n\n        # Translate input by max for numerical stability\n        input = input - torch.max(input, dim=dim, keepdim=True)[0].expand_as(input)\n\n        # Sort input in descending order.\n        # (NOTE: Can be replaced with linear time selection method described here:\n        # http://stanford.edu/~jduchi/projects/DuchiShSiCh08.html)\n        zs = torch.sort(input=input, dim=dim, descending=True)[0]\n        range = torch.arange(start=1, end=number_of_logits + 1, step=1, device=device, dtype=input.dtype).view(1, -1)\n        range = range.expand_as(zs)\n\n        # Determine sparsity of projection\n        bound = 1 + range * zs\n        cumulative_sum_zs = torch.cumsum(zs, dim)\n        is_gt = torch.gt(bound, cumulative_sum_zs).type(input.type())\n        k = torch.max(is_gt * range, dim, keepdim=True)[0]\n\n        # Compute threshold function\n        zs_sparse = is_gt * zs\n\n        # Compute taus\n        taus = (torch.sum(zs_sparse, dim, keepdim=True) - 1) / k\n        taus = taus.expand_as(input)\n\n        # Sparsemax\n        self.output = torch.max(torch.zeros_like(input), input - taus)\n\n        # Reshape back to original shape\n        output = self.output\n        output = output.transpose(0, 1)\n        output = output.reshape(original_size)\n        output = output.transpose(0, self.dim)\n\n        return output\n\n    def backward(self, grad_output):\n        \"\"\"Backward function.\"\"\"\n        dim = 1\n\n        nonzeros = torch.ne(self.output, 0)\n        sum = torch.sum(grad_output * nonzeros, dim=dim) / torch.sum(nonzeros, dim=dim)\n        self.grad_input = nonzeros * (grad_output - sum.expand_as(grad_output))\n\n        return self.grad_input","metadata":{"execution":{"iopub.status.busy":"2021-09-20T03:43:57.828788Z","iopub.execute_input":"2021-09-20T03:43:57.829107Z","iopub.status.idle":"2021-09-20T03:43:57.845088Z","shell.execute_reply.started":"2021-09-20T03:43:57.829078Z","shell.execute_reply":"2021-09-20T03:43:57.843922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sparsemax = Sparsemax(dim=-1)\n\ndef get_activation_fn(activation):\n    if activation=='gelu':\n        return nn.GELU()\n    elif activation=='relu':\n        return nn.ReLU()\n    \ndef attention(query, key, value, mask=None, dropout=None):\n    d_k=query.size(-1)\n    scores=torch.matmul( query, key.transpose(-1, -2) )/np.sqrt(d_k)\n    #scores=torch.tril(scores)\n    if mask is not None:\n        scores=scores.masked_fill(mask == 0, -1e9)\n    \n    #p_attn=torch.softmax(scores, dim=-1)\n    p_attn=sparsemax(scores)\n    x_attn=torch.matmul(p_attn, value)\n    if dropout:\n        x_attn=dropout(x_attn)\n        \n    return p_attn, x_attn\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, num_features, dmodel, nhead,activation,norm,dropout):\n        super().__init__()\n        self.dmodel=dmodel\n        self.nhead=nhead\n        self.d_k=dmodel//nhead #Size\n        \n        self.activation=activation\n        self.norm=norm\n        self.dropout=dropout\n        \n        #self.Q=nn.Linear(num_features, dmodel)\n        #self.K=nn.Linear(num_features, dmodel)\n        #self.V=nn.Linear(num_features, dmodel)\n        \n        self.Q=nn.Conv1d(num_features, dmodel, 5, padding=2)\n        self.K=nn.Conv1d(num_features, dmodel, 5, padding=2)\n        self.V=nn.Conv1d(num_features, dmodel, 5, padding=2)\n        \n        \n        self.W=nn.Linear(dmodel, num_features)\n        \n        #nn.init.uniform_(self.Q.weight, -1/np.sqrt(2*num_features), 1/np.sqrt(2*num_features))\n        #nn.init.uniform_(self.K.weight, -1/np.sqrt(2*num_features), 1/np.sqrt(2*num_features))\n        #nn.init.uniform_(self.V.weight, -1/np.sqrt(2*num_features), 1/np.sqrt(2*num_features))\n        #nn.init.uniform_(self.W.weight, -1/np.sqrt(2*num_features), 1/np.sqrt(2*num_features))\n        \n        \n    def forward(self, x, mask=None):\n        bsize=x.size(0)\n        x=self.norm(x)\n        x=x.transpose(2, 1)\n        query=self.Q(x).transpose(2, 1).view(bsize, -1, self.nhead, self.d_k)\n        key=self.K(x).transpose(2, 1).view(bsize, -1, self.nhead, self.d_k)\n        value=self.V(x).transpose(2, 1).view(bsize, -1, self.nhead, self.d_k)\n        mask=mask.unsqueeze(-1).unsqueeze(-1)\n        \n        #query=self.Q(x).view(bsize, -1, self.nhead, self.d_k)\n        #key=self.K(x).view(bsize, -1, self.nhead, self.d_k)\n        #value=self.V(x).view(bsize, -1, self.nhead, self.d_k)\n        \n        \n        p_attn, x_attn=attention(query, key, value, mask, self.dropout)\n        x_attn=x_attn.view(bsize, -1, self.nhead*self.d_k)\n        \n        x_attn=self.W(x_attn)\n        x=x.transpose(2, 1)\n        x=x+x_attn\n        return x\n\nclass TimeSeriesAttentionLayer(nn.Module):\n    def __init__(self,\n                 num_features=32,\n                 dmodel=128,\n                 nhead=4,\n                 dim_feed_forward=512,\n                 activation='relu', \n                 dropout=0.1\n                ):\n        \n        super().__init__()\n        self.num_features=num_features\n        self.dmodel=dmodel\n        self.nhead=nhead\n        self.dim_feed_forward=dim_feed_forward\n        self.activation=get_activation_fn('gelu')\n        self.norm=nn.LayerNorm(num_features)\n        self.dropout=nn.Dropout(dropout)\n        \n        self.multihead_attn=MultiHeadAttention(num_features,\n                                               dmodel,\n                                               nhead,\n                                               self.activation,\n                                               self.norm,\n                                               self.dropout\n                                              )\n        \n        self.conv1=nn.Conv1d(num_features, dim_feed_forward, 5, padding=2)\n        self.conv2=nn.Conv1d(dim_feed_forward, num_features, 5, padding=2)\n        #self.linear1=nn.Linear(num_features, dim_feed_forward)\n        #self.linear2=nn.Linear(dim_feed_forward, num_features)\n        \n    def forward(self, x, mask=None):\n        x=self.multihead_attn(x, mask)\n        x=self.norm(x)\n        #x_ffn=self.linear2(self.dropout(self.activation(self.linear1(x))))\n        x_ffn=self.conv2(self.dropout(self.activation(self.conv1(x.transpose(1, 2))))).transpose(2, 1)\n        x=x+x_ffn\n        return x\n\n\nclass FeatureExtractorWith1DConv(nn.Module):\n    def __init__(self, input_size, output_size):\n        super().__init__()\n        self.pre_bn=nn.BatchNorm1d(input_size)\n        \n        #self.linear1=nn.Linear(input_size, 2*output_size)\n        #self.bn1=nn.BatchNorm1d(2*output_size)\n        \n        #self.linear2=nn.Linear(2*output_size, output_size)\n        #self.bn2=nn.BatchNorm1d(output_size)\n        \n        self.conv1=nn.Conv1d(input_size, 2*output_size, 5, padding=2)\n        self.bn1=nn.BatchNorm1d(2*output_size)\n        \n        self.conv2=nn.Conv1d(2*output_size, output_size, 5, padding=2)\n        self.bn2=nn.BatchNorm1d(output_size)\n        \n        self.activation=nn.GELU()\n        self.dropout=nn.Dropout(0.2)\n        \n    def forward(self, x):\n        x=x.transpose(1, 2)\n        x=self.pre_bn(x)\n        x=self.dropout(self.activation(self.bn1(self.conv1(x))))\n        x=self.activation(self.bn2(self.conv2(x)))\n        x=x.transpose(1, 2)\n        \n        #x=self.pre_bn(x.transpose(2, 1)).transpose(2, 1)\n        #x=self.dropout(self.activation(self.bn1( self.linear1(x).transpose(2, 1) ).transpose(2, 1)))\n        #x=self.activation(self.bn2( self.linear2(x).transpose(2, 1) ).transpose(2, 1))\n        return x","metadata":{"execution":{"iopub.status.busy":"2021-09-20T03:43:57.847032Z","iopub.execute_input":"2021-09-20T03:43:57.847485Z","iopub.status.idle":"2021-09-20T03:43:57.880847Z","shell.execute_reply.started":"2021-09-20T03:43:57.847438Z","shell.execute_reply":"2021-09-20T03:43:57.879954Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class FFN(nn.Module):\n    def __init__(self, sz):\n        super().__init__()\n        self.bn1=nn.BatchNorm1d(sz)\n        self.linear1=nn.Linear(sz, 2*sz)\n        \n        self.bn2=nn.BatchNorm1d(2*sz)\n        self.linear2=nn.Linear(2*sz, sz)\n        \n        \n        self.activation=nn.GELU()\n        self.dropout=nn.Dropout(0.2)\n        \n    def forward(self, x):\n        x=self.linear1(self.dropout(self.activation(self.bn1(x))))\n        x=self.linear2(self.dropout(self.activation(self.bn2(x))))\n        return x\n\n\nclass ConvHead(nn.Module):\n    def __init__(self, dmodel, pool_size):\n        super().__init__()\n        self.convs=nn.Sequential(\n            nn.Conv1d(dmodel, 2*dmodel, 7, padding=3, stride=4),\n            nn.BatchNorm1d( 2*dmodel),\n            nn.GELU(),\n            \n            nn.Conv1d(2*dmodel, dmodel, 5, padding=2, stride=4),\n            nn.BatchNorm1d(dmodel),\n            nn.GELU(),\n            \n            nn.AdaptiveAvgPool1d(pool_size)\n        )\n    def forward(self, x):\n        bsize=x.size(0)\n        x=self.convs(x.transpose(1, 2))\n        return x.view(bsize, -1)","metadata":{"execution":{"iopub.status.busy":"2021-09-20T03:43:57.881997Z","iopub.execute_input":"2021-09-20T03:43:57.882614Z","iopub.status.idle":"2021-09-20T03:43:57.895702Z","shell.execute_reply.started":"2021-09-20T03:43:57.882569Z","shell.execute_reply":"2021-09-20T03:43:57.894453Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MLPHead(nn.Module):\n    def __init__(self, sz, num_layers):\n        super().__init__()\n        self.ffn=nn.ModuleList(\n            [FFN( sz ) for _ in range(num_layers)]\n        )\n        self.dropout=nn.Dropout(0.2)\n        self.out=nn.Linear(sz, 2)\n        \n    def forward(self, x):\n        x=self.dropout(x)\n        for i, _ in enumerate(self.ffn):\n            x=self.ffn[i](x)\n        y=self.out(x)\n        return y","metadata":{"execution":{"iopub.status.busy":"2021-09-20T03:43:57.897573Z","iopub.execute_input":"2021-09-20T03:43:57.898042Z","iopub.status.idle":"2021-09-20T03:43:57.911972Z","shell.execute_reply.started":"2021-09-20T03:43:57.897999Z","shell.execute_reply":"2021-09-20T03:43:57.910837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.autograd import Variable\nclass PositionalEncoding(nn.Module):\n    \"Implement the PE function.\"\n    def __init__(self, d_model, dropout, max_len=config.num_buckets):\n        super(PositionalEncoding, self).__init__()\n        self.dropout = nn.Dropout(p=dropout)\n        \n        # Compute the positional encodings once in log space.\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2) *\n                             -(np.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0)\n        self.register_buffer('pe', pe)\n        \n    def forward(self, x):\n        x = x + Variable(self.pe[:, :x.size(1)], \n                         requires_grad=False)\n        return x","metadata":{"execution":{"iopub.status.busy":"2021-09-20T03:43:57.91315Z","iopub.execute_input":"2021-09-20T03:43:57.913468Z","iopub.status.idle":"2021-09-20T03:43:57.924845Z","shell.execute_reply.started":"2021-09-20T03:43:57.913429Z","shell.execute_reply":"2021-09-20T03:43:57.923977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class OptiverEncoder(nn.Module):\n    def __init__(self, params):\n        super().__init__()\n        self.dmodel=params['dmodel']\n        self.in_features=params['in_features']\n        self.out_features=params['out_features']\n        self.num_ffn_layers=params['num_ffn_layers']\n        self.pool_size=params['pool_size']\n        \n        \n        self.dropout=nn.Dropout(0.1)\n        self.feature_extractor=FeatureExtractorWith1DConv(self.in_features, self.out_features)\n        self.positions = PositionalEncoding(self.out_features, 0.1)        \n        self.attn_layers=nn.ModuleList([TimeSeriesAttentionLayer(num_features=self.out_features,\n                                                                 dmodel=self.dmodel,\n                                                                 nhead=params['nhead'],\n                                                                 dim_feed_forward=params['dim_feed_forward'],\n                                                                ) for _ in range(params['num_attention_layers'])])\n        \n    def forward(self, x, mask):\n        batch_size=x.size(0)\n        seq_len=x.size(1)\n        x=self.positions(self.feature_extractor(x))\n        for attn_layer in self.attn_layers:\n            x=attn_layer(x, mask)\n        return x","metadata":{"execution":{"iopub.status.busy":"2021-09-20T03:43:57.925863Z","iopub.execute_input":"2021-09-20T03:43:57.926171Z","iopub.status.idle":"2021-09-20T03:43:57.942798Z","shell.execute_reply.started":"2021-09-20T03:43:57.926136Z","shell.execute_reply":"2021-09-20T03:43:57.941692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class OptiverModel(nn.Module):\n    def __init__(self, params):\n        super().__init__()\n        self.dmodel=params['dmodel']\n        self.in_features=params['in_features']\n        self.out_features=params['out_features']\n        self.num_ffn_layers=params['num_ffn_layers']\n        self.pool_size=params['pool_size']\n        \n        self.encoder=OptiverEncoder(params)\n        \n        \n        self.alpha_model=MLPHead(2 * self.out_features , self.num_ffn_layers)\n        self.binary_model=MLPHead(2 * self.out_features, self.num_ffn_layers)\n    \n    def pooling(self, x, mask):\n        mask=mask.unsqueeze(dim=-1)\n        mean_pool=(x * mask).sum(dim=1)/mask.sum(dim=1)\n        \n        max_pool=x.masked_fill(mask == 0, -1e9)\n        max_pool=torch.max(max_pool, dim=1)[0]\n        return torch.cat([mean_pool, max_pool], dim=1)\n    \n    def forward(self, x, mask):\n        batch_size=x.size(0)\n        seq_len=x.size(1)\n        \n        x=self.encoder(x, mask)\n        x=self.pooling(x, mask)\n        \n        yhat_alpha=self.alpha_model(x)\n        yhat_binary=self.binary_model(x)\n        \n        return yhat_alpha, yhat_binary","metadata":{"execution":{"iopub.status.busy":"2021-09-20T03:43:57.944441Z","iopub.execute_input":"2021-09-20T03:43:57.944868Z","iopub.status.idle":"2021-09-20T03:43:57.960536Z","shell.execute_reply.started":"2021-09-20T03:43:57.944822Z","shell.execute_reply":"2021-09-20T03:43:57.959583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# custom losses","metadata":{}},{"cell_type":"code","source":"class CustomLosses:\n    @staticmethod\n    def MSE(y, yhat):\n        yerr=y-yhat\n        yerr=torch.square(yerr)\n        return yerr.mean()\n\n    @staticmethod\n    def RMSE(y, yhat):\n        err=(y-yhat)\n        return torch.sqrt( torch.mean(err**2) )\n\n    @staticmethod\n    def RMSPE(y, yhat):\n        err=(y-yhat)\n        err/=y\n        err=torch.square(err)\n        return torch.sqrt( torch.mean(err) )\n\n    \n    @staticmethod\n    def get_auxilary_loss(y, yhat):\n        print(y.shape, yhat.shape)\n        y=y[:, 1:config.num_buckets].squeeze(-1)\n        yhat=yhat[:, 0: config.num_buckets-1].squeeze(-1)\n\n        #Validating only the last 20 time-steps\n        y=y[:, -20:]\n        yhat=yhat[:, -20:]\n\n        yerr=100 * (y - yhat)/y\n        yerr=torch.square(yerr).view(-1).mean()\n        yerr=torch.sqrt(yerr)\n        return yerr\n    \n    @staticmethod\n    def CrossEntropyLoss(y_binary, yhat_binary, weight):\n        yhat_binary=yhat_binary.view(-1)\n        y_binary=y_binary.view(-1)\n        weight=weight.view(-1)\n        \n        weight=weight+1e-10\n        \n        p=torch.sigmoid(yhat_binary)\n        log_p1=torch.log(p+1e-10)\n        log_p0=torch.log((1-p) + 1e-10)\n\n        loss1= (-weight * (y_binary==1) * log_p1).sum()\n        loss2= (-weight * (y_binary==0) * log_p0).sum()\n        loss=(loss1+loss2)/weight.sum()\n        return loss","metadata":{"execution":{"iopub.status.busy":"2021-09-20T03:43:57.961615Z","iopub.execute_input":"2021-09-20T03:43:57.962006Z","iopub.status.idle":"2021-09-20T03:43:57.979241Z","shell.execute_reply.started":"2021-09-20T03:43:57.961978Z","shell.execute_reply":"2021-09-20T03:43:57.978332Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# trainer","metadata":{}},{"cell_type":"code","source":"class Evaluator:\n    def __init__(self, model, val_dataloader):\n        self.model=model\n        self.dataloader=val_dataloader\n        \n    def evaluate(self):\n        self.model.eval()\n        print(\"Evaluating\")\n        yprimary_true=[]; yprimary_pred1=[]; yprimary_pred2=[]\n        for i, (X, price_diff, mask, wap1, wap2, s1, s2, rv1, rv2, weight1, weight2,\n                    y_binary1, y_binary2, y_target) in enumerate(self.dataloader):\n            X=torch.cat([X, price_diff, wap1.unsqueeze(-1), wap2.unsqueeze(-1),\n                             s1.unsqueeze(-1),s2.unsqueeze(-1)], dim=-1)\n            X=X.to(device)\n            mask=mask.to(device)\n            rv1=rv1.to(device)\n            rv2=rv2.to(device)\n            y_target=y_target.to(device)\n            \n            \n            yprimary_true+=y_target.cpu().tolist()\n            with torch.no_grad():\n                yhat_alpha, yhat_binary=self.model(X, mask)\n                yhat1 = yhat_alpha[:, 0].view(-1) * rv1\n                yhat2 = yhat_alpha[:, 1].view(-1) * rv2\n                \n                yprimary_pred1+=yhat1.view(-1).cpu().tolist()\n                yprimary_pred2+=yhat2.view(-1).cpu().tolist()\n            \n            del X\n            del mask\n            del wap1\n            del s1\n        \n        yprimary_true=torch.tensor(yprimary_true, dtype=torch.float32)\n        yprimary_pred1=torch.tensor(yprimary_pred1, dtype=torch.float32)\n        yprimary_pred2=torch.tensor(yprimary_pred2, dtype=torch.float32)\n        \n        rmspe_loss1=CustomLosses.RMSPE(yprimary_true, yprimary_pred1)\n        rmspe_loss2=CustomLosses.RMSPE(yprimary_true, yprimary_pred2)\n        return (rmspe_loss1.item(), rmspe_loss2.item())\n        \nclass Trainer:\n    def __init__(self, model, train_dataloader, val_dataloader,\n                 optimizer, schedular=None):\n        self.best_rmse=None\n        self.best_rmspe1=None\n        self.best_rmspe2=None\n        self.evaluator=Evaluator(model, val_dataloader)\n        \n        self.train_dataloader=train_dataloader\n        self.val_dataloader=val_dataloader\n        \n        self.model=model\n        self.optimizer=optimizer\n        self.schedular=schedular\n        self.steps=0\n        self.acculation_steps=8\n        \n        self.train_loss=[]\n        self.train_alpha1=[]\n        self.train_alpha2=[]\n        self.train_binary1=[]\n        self.train_binary2=[]\n    \n    def train_ops(self, X, mask, rv1, rv2, y_target, y_binary1, y_binary2, weight1, weight2):\n        self.steps+=1\n        self.model.train()\n        X=X.to(device)\n        mask=mask.to(device)\n        weight1=weight1.to(device)\n        weight2=weight2.to(device)\n        rv1=rv1.to(device)\n        rv2=rv2.to(device)\n        y_binary1=y_binary1.to(device)\n        y_binary2=y_binary2.to(device)\n        y_target=y_target.to(device)\n\n        yhat_alpha, yhat_binary=self.model(X, mask)\n        yhat1 = yhat_alpha[:, 0].view(-1) * rv1\n        yhat2 = yhat_alpha[:, 1].view(-1) * rv2\n        \n\n        consistency_loss=torch.abs(yhat1-yhat2).mean()\n        alpha_loss1=CustomLosses.RMSPE(y_target, yhat1)\n        alpha_loss2=CustomLosses.RMSPE(y_target, yhat2)\n\n        binary_loss1=CustomLosses.CrossEntropyLoss(y_binary1, yhat_binary[:, 0], weight1)\n        binary_loss2=CustomLosses.CrossEntropyLoss(y_binary2, yhat_binary[:, 1], weight2)\n\n        loss=(alpha_loss1+alpha_loss2) +(0.5 * (binary_loss1 + binary_loss2) ) + 0.2*consistency_loss\n        loss = loss/self.acculation_steps\n        loss.backward()\n        #torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1)\n        \n        if self.steps%self.acculation_steps==0:\n            if USE_TPU:\n                xm.optimizer_step(self.optimizer)\n            else:\n                self.optimizer.step()\n            self.model.zero_grad(set_to_none=True)\n            self.schedular.step()\n        return {\n            \"loss\": loss.item(),\n            \"alpha1\":alpha_loss1.item(),\n            \"alpha2\":alpha_loss2.item(),\n            \"binary1\": binary_loss1.item(),\n            \"binary2\": binary_loss2.item(),\n            \"consistency\": consistency_loss.item()\n        }\n                \n    def train(self):\n        for e in range(config.epochs):\n            epoch_loss=0.0; \n            epoch_alpha1=0.0; epoch_alpha2=0.0\n            epoch_binary1=0.0; epoch_binary2=0.0;\n            \n            self.model.zero_grad(set_to_none=True)\n            self.model.train()\n            for i,  (X, price_diff, mask, wap1, wap2, s1, s2, rv1, rv2, weight1, weight2,\n                    y_binary1, y_binary2, y_target) in enumerate(self.train_dataloader):\n                X=torch.cat([X, price_diff, wap1.unsqueeze(-1), wap2.unsqueeze(-1),\n                             s1.unsqueeze(-1),s2.unsqueeze(-1)], dim=-1)\n                \n                losses=self.train_ops( X, mask, rv1, rv2, y_target, y_binary1, y_binary2, weight1, weight2)\n                \n                epoch_loss = ((epoch_loss*i)+(losses['loss']))/(i+1)\n                epoch_alpha1 = ((epoch_alpha1*i)+(losses['alpha1']))/(i+1)\n                epoch_alpha2 = ((epoch_alpha2*i)+(losses['alpha2']))/(i+1)\n                epoch_binary1 = ((epoch_binary1*i)+(losses['binary1']))/(i+1)\n                epoch_binary2 = ((epoch_binary2*i)+(losses['binary2']))/(i+1)\n                \n                if i%300==0:\n                    print(\"Iteration:{}|Loss:{:.3f}\".format(i, epoch_loss))\n                    print(\"Alpha1: {:.3f} | Alpha2: {:.3f}\".format(epoch_alpha1, epoch_alpha2))\n                    print(\"binary1: {:.3f} | binary2: {:.3f}\".format(epoch_binary1, epoch_binary2))\n                    print(\"Consistency:{:.4f}\".format(losses['consistency']))\n                   \n                del X\n                del mask\n                del wap1\n                del s1\n            self.train_loss.append(epoch_loss)\n            self.train_alpha1.append(epoch_alpha1)\n            self.train_alpha2.append(epoch_alpha2)\n            self.train_binary1.append(epoch_binary1)\n            self.train_binary2.append(epoch_binary2)\n            \n            eval_rmspe1, eval_rmspe2 = self.evaluator.evaluate()\n            \n            #if self.schedular:\n            #    self.schedular.step(eval_rmspe)\n            \n            if (self.best_rmspe1 is None) or (self.best_rmspe1 > eval_rmspe1):\n                self.best_rmspe1=eval_rmspe1\n                if USE_TPU:\n                    xm.save(self.model.state_dict(),'best_rmspe1.pt')\n                else:\n                    torch.save(self.model, 'best_rmspe1.pt')\n\n            if (self.best_rmspe2 is None) or (self.best_rmspe2 > eval_rmspe2):\n                self.best_rmspe2=eval_rmspe2\n                if USE_TPU:\n                    xm.save(self.model.state_dict(),'best_rmspe2.pt')\n                else:\n                    torch.save(self.model, 'best_rmspe2.pt')\n                    \n            print()\n            print()\n            print(\"***************End of Epoch{}***************\".format(e))\n            print(\"epoch:{}-LOSS:{:.4f}\".format(e, epoch_loss))\n            print(\"Alpha1: {:.3f} | Alpha2: {:.3f}\".format(epoch_alpha1, epoch_alpha2))\n            print(\"binary1: {:.3f} | binary2: {:.3f}\".format(epoch_binary1, epoch_binary2))\n                   \n            print(\"Val RMSPE1:{:.4f}|Val RMSPE2:{:.4f}\".format( eval_rmspe1, eval_rmspe2))\n            gc.collect()\n    \n    def lr_range_test(self):\n        min_lr=1e-5\n        max_lr=1e-3\n        optimizer=torch.optim.AdamW(self.model.parameters(), lr=min_lr, weight_decay=0.001)\n        scheduler=torch.optim.lr_scheduler.StepLR(optimizer, 1, 1.03)\n        \n        losses=[]; alpha_losses1=[]; alpha_losses2=[];\n        consistency_losses=[]\n        lrs=[]\n        self.model.train()\n        self.model.zero_grad(set_to_none=True)\n        for _ in range(100):\n            for i, (X, price_diff, mask, wap1, wap2, s1, s2, rv1, rv2, weight1, weight2,\n                    y_binary1, y_binary2, y_target) in enumerate(self.train_dataloader):\n                \n                X=torch.cat([X, price_diff, wap1.unsqueeze(-1), wap2.unsqueeze(-1),\n                             s1.unsqueeze(-1),s2.unsqueeze(-1)], dim=-1)\n                \n                X=X.to(device)\n                mask=mask.to(device)\n                weight1=weight1.to(device)\n                weight2=weight2.to(device)\n                rv1=rv1.to(device)\n                rv2=rv2.to(device)\n                y_binary1=y_binary1.to(device)\n                y_binary2=y_binary2.to(device)\n                y_target=y_target.to(device)\n                \n                yhat_alpha, yhat_binary=self.model(X, mask)\n                yhat1 = yhat_alpha[:, 0].view(-1) * rv1\n                yhat2 = yhat_alpha[:, 1].view(-1) * rv2\n                \n                \n                consistency_loss=torch.sqrt(((yhat1-yhat2)**2).mean())\n                alpha_loss1=CustomLosses.RMSPE(y_target, yhat1)\n                alpha_loss2=CustomLosses.RMSPE(y_target, yhat2)\n                \n                binary_loss1=CustomLosses.CrossEntropyLoss(y_binary1, yhat_binary[:, 0], weight1)\n                binary_loss2=CustomLosses.CrossEntropyLoss(y_binary2, yhat_binary[:, 1], weight2)\n                \n                loss=(alpha_loss1+alpha_loss2) +(0.5 * (binary_loss1 + binary_loss2) ) + 0.3*consistency_loss\n                \n                loss=loss/8\n                loss.backward()\n                \n                if (i+1)%8==0:\n                    if USE_TPU:\n                        xm.optimizer_step(optimizer)\n                    else:\n                        optimizer.step()\n                    self.model.zero_grad(set_to_none=True)\n                    scheduler.step()\n\n                losses.append(loss.item())\n                alpha_losses1.append(alpha_loss1.item())\n                alpha_losses2.append(alpha_loss2.item())\n                consistency_losses.append(consistency_loss.item())\n                \n                lrs.append(scheduler.get_last_lr()[0])\n                \n                del X\n                del mask\n                \n                if i%10==0:\n                    print(\"Iteration:{}|Loss:{:.3f}|LR:{}\".format(i,loss.item(), lrs[-1]))\n                    print(\"Alpha1:{:.3f}|Alpha2:{:.3f}\".format(alpha_loss1.item(), alpha_loss2.item()))\n                    print(\"Binary1:{:.3f}|Binary2:{:.3f}\".format(binary_loss1.item(), binary_loss2.item()))\n                    print(\"Consistency:{:.4f}\".format(consistency_loss.item()))\n                    print()\n                    print('----')\n                if lrs[-1] > max_lr:\n                    break\n            if lrs[-1] > max_lr:\n                break\n        return lrs, losses, alpha_losses1, alpha_losses2, consistency_losses","metadata":{"execution":{"iopub.status.busy":"2021-09-20T03:44:41.65546Z","iopub.execute_input":"2021-09-20T03:44:41.655964Z","iopub.status.idle":"2021-09-20T03:44:41.72169Z","shell.execute_reply.started":"2021-09-20T03:44:41.655921Z","shell.execute_reply":"2021-09-20T03:44:41.720533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params={\n    'dmodel': 128,\n    'nhead':8,\n    'in_features': 16,\n    'out_features': 128,\n    'pool_size': 4, \n    'dim_feed_forward': 256,\n    'num_attention_layers': 5,\n    'num_ffn_layers': 2\n}\nmodel=OptiverModel(params)\nmodel=model.to(device)\n\n#print(model)","metadata":{"execution":{"iopub.status.busy":"2021-09-20T03:44:41.723811Z","iopub.execute_input":"2021-09-20T03:44:41.7242Z","iopub.status.idle":"2021-09-20T03:44:41.789921Z","shell.execute_reply.started":"2021-09-20T03:44:41.724169Z","shell.execute_reply":"2021-09-20T03:44:41.788942Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_lr=2e-4\noptimizer=torch.optim.AdamW(model.parameters(), lr=max_lr, weight_decay=0.001)\nschedular=torch.optim.lr_scheduler.OneCycleLR(optimizer,\n                                              max_lr=max_lr,\n                                              pct_start=0.15,\n                                              total_steps= config.epochs * len(train_dataloader)//4,\n                                              final_div_factor=10)\n\n\ntrainer=Trainer(model, train_dataloader, valid_dataloader, optimizer, schedular)\ntrainer.train()\n#lrs, losses, alpha_losses1, alpha_losses2, consistency_losses=trainer.lr_range_test()","metadata":{"execution":{"iopub.status.busy":"2021-09-20T03:44:41.791702Z","iopub.execute_input":"2021-09-20T03:44:41.792044Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#window=8\n#new_losses=np.array(losses).copy()\n\n#cum_x=np.cumsum(new_losses)\n#x_rolling=(cum_x[window:] - cum_x[:-window])/window\n#new_losses[window:]=x_rolling\n\n#s=300; e=1000\n\n#plt.xticks(rotation=45)\n#plt.plot(lrs[s:e], new_losses[s:e])","metadata":{"execution":{"iopub.status.busy":"2021-09-20T03:39:45.405321Z","iopub.execute_input":"2021-09-20T03:39:45.405688Z","iopub.status.idle":"2021-09-20T03:39:45.56428Z","shell.execute_reply.started":"2021-09-20T03:39:45.405656Z","shell.execute_reply":"2021-09-20T03:39:45.563283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(trainer.train_loss, label='loss')\nplt.plot(trainer.train_alpha1, label='Alpha1')\nplt.plot(trainer.train_alpha2, label='Alpha2')\n\nplt.legend(loc='best')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-20T03:38:41.613646Z","iopub.execute_input":"2021-09-20T03:38:41.614083Z","iopub.status.idle":"2021-09-20T03:38:41.777131Z","shell.execute_reply.started":"2021-09-20T03:38:41.614044Z","shell.execute_reply":"2021-09-20T03:38:41.776177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#s=20; e=380\n\n#plt.xticks(rotation=45)\n#plt.plot(lrs[s:e], losses[s:e])\n","metadata":{"execution":{"iopub.status.busy":"2021-09-20T03:38:41.77854Z","iopub.execute_input":"2021-09-20T03:38:41.77891Z","iopub.status.idle":"2021-09-20T03:38:41.782589Z","shell.execute_reply.started":"2021-09-20T03:38:41.778874Z","shell.execute_reply":"2021-09-20T03:38:41.781708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#plt.xticks(rotation=45)\n#plt.plot(lrs[s:e], alpha_losses1[s:e])\n#lrs, losses, alpha_losses1, alpha_losses2","metadata":{"execution":{"iopub.status.busy":"2021-09-20T03:38:41.786291Z","iopub.execute_input":"2021-09-20T03:38:41.786653Z","iopub.status.idle":"2021-09-20T03:38:41.791944Z","shell.execute_reply.started":"2021-09-20T03:38:41.786618Z","shell.execute_reply":"2021-09-20T03:38:41.790965Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n#plt.xticks(rotation=45)\n#plt.plot(lrs[s:e], alpha_losses2[s:e])\n#lrs, losses, alpha_losses1, alpha_losses2","metadata":{"execution":{"iopub.status.busy":"2021-09-20T03:38:41.793617Z","iopub.execute_input":"2021-09-20T03:38:41.794052Z","iopub.status.idle":"2021-09-20T03:38:41.800041Z","shell.execute_reply.started":"2021-09-20T03:38:41.794015Z","shell.execute_reply":"2021-09-20T03:38:41.799215Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n#plt.xticks(rotation=45)\n#plt.plot(lrs[s:e], consistency_losses[s:e])\n#lrs, losses, alpha_losses1, alpha_losses2\n","metadata":{"execution":{"iopub.status.busy":"2021-09-20T03:38:41.801317Z","iopub.execute_input":"2021-09-20T03:38:41.8017Z","iopub.status.idle":"2021-09-20T03:38:41.808422Z","shell.execute_reply.started":"2021-09-20T03:38:41.801665Z","shell.execute_reply":"2021-09-20T03:38:41.807636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}