{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        #print(os.path.join(dirname, filename))\n        pass\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-11T03:44:40.377753Z","iopub.execute_input":"2021-08-11T03:44:40.378337Z","iopub.status.idle":"2021-08-11T03:44:40.918513Z","shell.execute_reply.started":"2021-08-11T03:44:40.37824Z","shell.execute_reply":"2021-08-11T03:44:40.917417Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.core.magic import register_cell_magic\n@register_cell_magic\ndef skip(line, cell=None):\n    '''Skips execution of the current line/cell if line evaluates to True.'''\n    if eval(line):\n        return\n        \n    get_ipython().run_cell(cell)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-11T03:44:40.920861Z","iopub.execute_input":"2021-08-11T03:44:40.921182Z","iopub.status.idle":"2021-08-11T03:44:40.927405Z","shell.execute_reply.started":"2021-08-11T03:44:40.921153Z","shell.execute_reply":"2021-08-11T03:44:40.925936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport tensorflow_datasets.public_api as tfds\nimport tensorflow as tf\nimport glob\nimport dill\nfrom kaggle_datasets import KaggleDatasets\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport random","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-11T03:44:40.92975Z","iopub.execute_input":"2021-08-11T03:44:40.930092Z","iopub.status.idle":"2021-08-11T03:44:48.231999Z","shell.execute_reply.started":"2021-08-11T03:44:40.930032Z","shell.execute_reply":"2021-08-11T03:44:48.230828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.random.set_seed(123)\nnp.random.seed(123)\nrandom.seed(123)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-11T03:44:48.234025Z","iopub.execute_input":"2021-08-11T03:44:48.234512Z","iopub.status.idle":"2021-08-11T03:44:48.239904Z","shell.execute_reply.started":"2021-08-11T03:44:48.234451Z","shell.execute_reply":"2021-08-11T03:44:48.238828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# NEW on TPU in TensorFlow 24: shorter cross-compatible TPU/GPU/multi-GPU/cluster-GPU detection code\ntpu = None\ntry: # detect TPUs\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect() # TPU detection\n    strategy = tf.distribute.TPUStrategy(tpu)\nexcept ValueError: # detect GPUs\n    #strategy = tf.distribute.MirroredStrategy() # for GPU or multi-GPU machines\n    strategy = tf.distribute.get_strategy() # default strategy that works on CPU and single GPU\n    #strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy() # for clusters of multi-GPU machines\n\n    \n#strategy,tpu = tf.distribute.MirroredStrategy(devices=[\"TPU:0\", \"TPU:1\",\"TPU:2\"]),True\n\nprint(\"Number of accelerators: \", strategy.num_replicas_in_sync)\n\n\nAUTO = tf.data.experimental.AUTOTUNE\nREPLICAS = strategy.num_replicas_in_sync\nprint(f'REPLICAS: {REPLICAS}')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-11T03:44:48.241669Z","iopub.execute_input":"2021-08-11T03:44:48.242061Z","iopub.status.idle":"2021-08-11T03:44:48.260974Z","shell.execute_reply.started":"2021-08-11T03:44:48.242015Z","shell.execute_reply":"2021-08-11T03:44:48.260089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n* Dataset conversion to TFRecords : https://www.kaggle.com/tchaye59/orvp-dataset\n* TFRecords : https://www.kaggle.com/tchaye59/orvptfrecords","metadata":{}},{"cell_type":"code","source":"data_path = '../input/optiver-realized-volatility-prediction'\nGCS_PATH = '../input/orvptfrecords'\nif tpu:\n    GCS_PATH = KaggleDatasets().get_gcs_path('orvptfrecords')","metadata":{"execution":{"iopub.status.busy":"2021-08-11T03:44:48.262064Z","iopub.execute_input":"2021-08-11T03:44:48.262481Z","iopub.status.idle":"2021-08-11T03:44:48.268825Z","shell.execute_reply.started":"2021-08-11T03:44:48.262449Z","shell.execute_reply":"2021-08-11T03:44:48.268145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TrainDataset(tfds.core.GeneratorBasedBuilder):\n    VERSION = tfds.core.Version('0.1.0')\n    \n    def _split_generators(self, dl_manager):\n        return [\n            tfds.core.SplitGenerator(\n                    name=f'train',\n                    gen_kwargs={\n                    },\n            )\n        ]\n    \n    def _info(self):\n        return tfds.core.DatasetInfo(\n            builder=self,\n            description=(\"\"),\n            features=tfds.features.FeaturesDict({\n                \"stock_id\": tfds.features.Tensor(shape=(),dtype=tf.float64),\n                \"book\": tfds.features.Tensor(shape=(None,9,),dtype=tf.float64),\n                \"trade\": tfds.features.Tensor(shape=(None,4,),dtype=tf.float64),\n                \"target\": tfds.features.Tensor(dtype=tf.float64 ,shape=(1,)),\n            }),\n        )\n    \n    def _generate_examples(self,**args):\n        pass","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-11T03:44:48.269858Z","iopub.execute_input":"2021-08-11T03:44:48.270157Z","iopub.status.idle":"2021-08-11T03:44:48.282073Z","shell.execute_reply.started":"2021-08-11T03:44:48.270129Z","shell.execute_reply":"2021-08-11T03:44:48.281151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BATCH_SIZE_PER_REPLICA = 256\nif tpu:\n    BATCH_SIZE_PER_REPLICA = 256\nGLOBAL_BATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync\nBUFFER_SIZE = 50000\n\nprefetch = 30\nMAX_SEQ = 600\nTRAIN = False\n\nepochs = 60\nLR = 1e-3","metadata":{"execution":{"iopub.status.busy":"2021-08-11T03:44:48.283303Z","iopub.execute_input":"2021-08-11T03:44:48.283586Z","iopub.status.idle":"2021-08-11T03:44:48.292794Z","shell.execute_reply.started":"2021-08-11T03:44:48.28356Z","shell.execute_reply":"2021-08-11T03:44:48.292079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv('../input/optiver-realized-volatility-prediction/train.csv')","metadata":{"execution":{"iopub.status.busy":"2021-08-11T03:44:48.294933Z","iopub.execute_input":"2021-08-11T03:44:48.295394Z","iopub.status.idle":"2021-08-11T03:44:48.573255Z","shell.execute_reply.started":"2021-08-11T03:44:48.295363Z","shell.execute_reply":"2021-08-11T03:44:48.572145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def calculate_wap(bid_price1,ask_price1,bid_size1,ask_size1,\n                 bid_price2,ask_price2,bid_size2,ask_size2):\n    a1 = bid_price1 * ask_size1 + ask_price1 * bid_size1\n    b1 = bid_size1 + ask_size1\n    a2 = bid_price2 * ask_size2 + ask_price2 * bid_size2\n    b2 = bid_size2 + ask_size2\n    x = (a1/b1 + a2/b2)/ 2\n    return x[:,tf.newaxis]\n\n\ndef calculate_wap2(bid_price1,ask_price1,bid_size1,ask_size1,\n                 bid_price2,ask_price2,bid_size2,ask_size2):\n        \n    a1 = bid_price1 * ask_size1 + ask_price1 * bid_size1\n    a2 = bid_price2 * ask_size2 + ask_price2 * bid_size2\n    b = bid_size1 + ask_size1 + bid_size2+ ask_size2\n    \n    x = (a1 + a2)/ b\n    return x[:,tf.newaxis]\n\ndef calculate_wap3(bid_price1,ask_price1,bid_size1,ask_size1,):\n    a1 = bid_price1 * ask_size1 + ask_price1 * bid_size1\n    b1 = bid_size1 + ask_size1\n    x = a1/b1\n    return x[:,tf.newaxis]\n\ndef calculate_wap4(bid_price2,ask_price2,bid_size2,ask_size2,):\n    a2 = bid_price2 * ask_size2 + ask_price2 * bid_size2\n    b2 = bid_size2 + ask_size2\n    x = a2/b2\n    return x[:,tf.newaxis]\n\ndef tf_diff(a):\n    return a[1:]-a[:-1]\n\ndef calculate_log_return(wap):\n    log_return = tf.math.log(wap)\n    log_return = tf.concat([log_return,tf.constant([[0.]],dtype=tf.float64)],axis=0)\n    log_return = tf_diff(log_return)\n    return log_return\n\ndef realized_volatility(log_return):\n    rv = tf.math.sqrt(tf.reduce_sum(log_return**2))\n    return rv","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-11T03:44:48.575619Z","iopub.execute_input":"2021-08-11T03:44:48.576033Z","iopub.status.idle":"2021-08-11T03:44:48.58851Z","shell.execute_reply.started":"2021-08-11T03:44:48.57599Z","shell.execute_reply":"2021-08-11T03:44:48.587782Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def features_builder(stock_id,book,trade):\n    #time_id = book[:,0]\n    seconds_in_bucket = book[:,0]\n    bid_price1 = book[:,1]\n    ask_price1 = book[:,2]\n    bid_price2 = book[:,3]\n    ask_price2 = book[:,4]\n    bid_size1 = book[:,5]\n    ask_size1 = book[:,6]\n    bid_size2 = book[:,7]\n    ask_size2 = book[:,8]\n    \n    #Book features\n    \n    # book_size\n    book_size = tf.cast(tf.shape(book)[0],tf.float64)\n    \n    #wap\n    wap = calculate_wap(bid_price1,ask_price1,bid_size1,ask_size1,bid_price2,ask_price2,bid_size2,ask_size2)\n    wap2 = calculate_wap2(bid_price1,ask_price1,bid_size1,ask_size1,bid_price2,ask_price2,bid_size2,ask_size2)\n    wap3 = calculate_wap3(bid_price1,ask_price1,bid_size1,ask_size1,)\n    wap4 = calculate_wap4(bid_price2,ask_price2,bid_size2,ask_size2,)\n    #log_return\n    log_return = calculate_log_return(wap)\n    log_return2 = calculate_log_return(wap2)\n    log_return3 = calculate_log_return(wap3)\n    log_return4 = calculate_log_return(wap4)\n    # rv\n    rv = realized_volatility(log_return)\n    rv2 = realized_volatility(log_return2)\n    rv3 = realized_volatility(log_return3)\n    rv4 = realized_volatility(log_return4)\n    rv = tf.repeat(rv,tf.shape(book)[0])[:,tf.newaxis]\n    rv2 = tf.repeat(rv2,tf.shape(book)[0])[:,tf.newaxis]\n    rv3 = tf.repeat(rv3,tf.shape(book)[0])[:,tf.newaxis]\n    rv4 = tf.repeat(rv4,tf.shape(book)[0])[:,tf.newaxis]\n    \n    book_data = [wap,wap2,wap3,wap4,\n                 log_return,log_return2,log_return3,log_return4,\n                 rv,rv2,rv3,rv4,\n                 tf.repeat(stock_id,tf.shape(book)[0])[:,tf.newaxis],\n                 tf.repeat(book_size,tf.shape(book)[0])[:,tf.newaxis]\n          ]\n    book_data = tf.concat(book_data,axis=-1)\n    \n    # Trade features\n    #time_id\tseconds_in_bucket\tprice\tsize\torder_count\n    #time_id = trade[:,0]\n    seconds_in_bucket = trade[:,0]\n    price = trade[:,1]\n    size = trade[:,2]\n    order_count = trade[:,3]\n    #price_log_return\n    price_log_return = calculate_log_return(tf.reshape(price,(-1,1)))\n    #trade_size\n    trade_size = tf.cast(tf.shape(trade)[0],tf.float64)\n    \n    trade_data = [\n        #price_log_return,\n        tf.repeat(trade_size,tf.shape(trade)[0])[:,tf.newaxis]\n          ]\n    trade_data = tf.concat(trade_data,axis=-1)\n    \n    return book_data,trade_data\n    \n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-11T03:44:48.589507Z","iopub.execute_input":"2021-08-11T03:44:48.589884Z","iopub.status.idle":"2021-08-11T03:44:48.605739Z","shell.execute_reply.started":"2021-08-11T03:44:48.589856Z","shell.execute_reply":"2021-08-11T03:44:48.605017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_datasets():\n    builder = TrainDataset(data_dir=GCS_PATH)\n    # The following line download the dataset\n    builder.download_and_prepare()\n    dataset = builder.as_dataset()['train']\n    \n    size = len(dataset)\n\n    # pad,shuffle and bacth\n    def preprecoss(x):\n        stock_id,book,trade,target = x['stock_id'],x['book'],x['trade'],x['target']\n        \n        book_data,trade_data = features_builder(stock_id,book,trade)\n        \n        book = tf.concat([book,book_data],axis=-1)\n        trade = tf.concat([trade,trade_data],axis=-1)\n        \n        p1 = [[0,MAX_SEQ-tf.shape(book)[0]],[0,0]]\n        p2 = [[0,MAX_SEQ-tf.shape(trade)[0]],[0,0]]\n        \n        book = tf.pad(book,p1, constant_values=0.)\n        trade = tf.pad(trade,p2, constant_values=0.)\n        \n        return (book,trade),target\n    \n    def shape_fix(inputs,target):\n        book,trade = inputs\n        book = tf.reshape(book,(MAX_SEQ,9+14))\n        trade = tf.reshape(trade,(MAX_SEQ,4+1))\n        return (book,trade),target\n    \n    \n    dataset = dataset.repeat().shuffle(BUFFER_SIZE).map(preprecoss,num_parallel_calls=AUTO)\n    if tpu:\n        dataset = dataset.map(shape_fix,num_parallel_calls=AUTO)\n    dataset = dataset.batch(BATCH_SIZE_PER_REPLICA).prefetch(prefetch)\n    if tpu:\n        dataset = strategy.experimental_distribute_datasets_from_function(lambda x: dataset)\n        \n    return dataset,size//GLOBAL_BATCH_SIZE","metadata":{"execution":{"iopub.status.busy":"2021-08-11T03:44:48.606667Z","iopub.execute_input":"2021-08-11T03:44:48.607079Z","iopub.status.idle":"2021-08-11T03:44:48.623877Z","shell.execute_reply.started":"2021-08-11T03:44:48.60703Z","shell.execute_reply":"2021-08-11T03:44:48.62281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_model():\n    n_seq = MAX_SEQ#None if not tpu else MAX_SEQ\n    book_input = tf.keras.layers.Input(shape=(n_seq,9+14))\n    trade_input = tf.keras.layers.Input(shape=(n_seq,4+1))\n\n    book = tf.keras.layers.Masking()(book_input)\n    trade = tf.keras.layers.Masking()(trade_input)\n\n    book = tf.keras.layers.BatchNormalization()(book)\n    trade = tf.keras.layers.BatchNormalization()(trade)\n\n    book = tf.keras.layers.GRU(256)(book)\n    book = tf.keras.layers.Dropout(0.1)(book)\n\n    trade = tf.keras.layers.GRU(256)(trade)\n    trade = tf.keras.layers.Dropout(0.1)(trade)\n    \n    model = tf.keras.layers.concatenate([book,trade])\n    \n    \n    for _ in range(10):\n        model = keras.layers.Dense(256, activation=keras.activations.swish)(model)\n        model = tf.keras.layers.Dropout(0.2)(model)\n    \n     \n    model = tf.keras.layers.Dense(1,activation=None)(model)\n\n    model = tf.keras.Model([book_input,trade_input],model)\n    return model","metadata":{"execution":{"iopub.status.busy":"2021-08-11T03:44:48.625399Z","iopub.execute_input":"2021-08-11T03:44:48.625982Z","iopub.status.idle":"2021-08-11T03:44:48.637842Z","shell.execute_reply.started":"2021-08-11T03:44:48.625936Z","shell.execute_reply":"2021-08-11T03:44:48.636722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"build_model().summary()","metadata":{"execution":{"iopub.status.busy":"2021-08-11T03:44:48.639216Z","iopub.execute_input":"2021-08-11T03:44:48.639508Z","iopub.status.idle":"2021-08-11T03:44:50.815645Z","shell.execute_reply.started":"2021-08-11T03:44:48.639481Z","shell.execute_reply":"2021-08-11T03:44:50.814468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def rmspe(y_true,y_pred):\n    elements = ((y_true - y_pred) / y_true) ** 2\n    elements = tf.reduce_sum(elements)/tf.cast(tf.size(y_pred),tf.float32)\n    return tf.sqrt(elements)","metadata":{"execution":{"iopub.status.busy":"2021-08-11T03:44:50.816916Z","iopub.execute_input":"2021-08-11T03:44:50.817397Z","iopub.status.idle":"2021-08-11T03:44:50.823045Z","shell.execute_reply.started":"2021-08-11T03:44:50.817349Z","shell.execute_reply":"2021-08-11T03:44:50.821847Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset,train_step = get_datasets()","metadata":{"execution":{"iopub.status.busy":"2021-08-11T03:44:50.824509Z","iopub.execute_input":"2021-08-11T03:44:50.824806Z","iopub.status.idle":"2021-08-11T03:44:52.097191Z","shell.execute_reply.started":"2021-08-11T03:44:50.824777Z","shell.execute_reply":"2021-08-11T03:44:52.096063Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%skip not TRAIN\n\nwith strategy.scope():\n    \n    #model\n    model = build_model()\n    model.compile(\n        optimizer=tf.keras.optimizers.Adam(LR),\n        loss=rmspe,\n    )\n    #callbacks\n    model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=f'weights.h5',\n                                                                   save_weights_only=True,\n                                                                   monitor='loss',\n                                                                   mode='min',verbose=True,\n                                                                   save_best_only=True)\n    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='loss',mode='min', patience=5)\n    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.2, patience=2, min_lr=LR/100)\n    terminate_onNaN = tf.keras.callbacks.TerminateOnNaN()\n    # dataset\n    train_dataset,train_step = get_datasets()\n    # Train\n    history = model.fit(train_dataset,\n                        steps_per_epoch=train_step,\n                        epochs=epochs,\n                        callbacks=[model_checkpoint_callback,early_stopping,reduce_lr,terminate_onNaN],\n                       )","metadata":{"execution":{"iopub.status.busy":"2021-08-11T03:44:52.098888Z","iopub.execute_input":"2021-08-11T03:44:52.099214Z","iopub.status.idle":"2021-08-11T03:44:52.103691Z","shell.execute_reply.started":"2021-08-11T03:44:52.099184Z","shell.execute_reply":"2021-08-11T03:44:52.102893Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%skip not TRAIN\npd.DataFrame(history.history).loss.plot()","metadata":{"execution":{"iopub.status.busy":"2021-08-11T03:44:52.10478Z","iopub.execute_input":"2021-08-11T03:44:52.10503Z","iopub.status.idle":"2021-08-11T03:44:52.122582Z","shell.execute_reply.started":"2021-08-11T03:44:52.105005Z","shell.execute_reply":"2021-08-11T03:44:52.121591Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%skip TRAIN\n! cp  ../input/realized-volatility-keras-rnn-baseline-tpu/weights.h5 ./","metadata":{"execution":{"iopub.status.busy":"2021-08-11T03:44:52.123776Z","iopub.execute_input":"2021-08-11T03:44:52.124214Z","iopub.status.idle":"2021-08-11T03:44:52.995681Z","shell.execute_reply.started":"2021-08-11T03:44:52.124171Z","shell.execute_reply":"2021-08-11T03:44:52.994486Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%skip tpu\nmodel  = build_model()\nmodel.load_weights(f'./weights.h5')","metadata":{"execution":{"iopub.status.busy":"2021-08-11T03:44:52.997137Z","iopub.execute_input":"2021-08-11T03:44:52.997423Z","iopub.status.idle":"2021-08-11T03:44:55.170036Z","shell.execute_reply.started":"2021-08-11T03:44:52.997394Z","shell.execute_reply":"2021-08-11T03:44:55.168772Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TestDataset():\n    \n    def __init__(self,):\n        self.test = pd.read_csv(data_path+'/test.csv')\n        self.stock_ids = self.test.stock_id.unique()\n        \n        \n    def getStockData(self,stock_id):\n        # Read data\n        book_path = f'{data_path}/book_test.parquet/stock_id={stock_id}'\n        trade_path = f'{data_path}/trade_test.parquet/stock_id={stock_id}'\n        book_df = pd.read_parquet(book_path)\n        trade_df = pd.read_parquet(trade_path)\n        \n        book_df_grp = book_df.groupby('time_id')\n        trade_df_grp = trade_df.groupby('time_id')\n        \n        data = []\n        for time_id in self.test.time_id.unique():\n            try:\n                book_items = book_df_grp.get_group(time_id)\n                del book_items['time_id']\n            except:\n                continue\n            try:\n                trade_items = trade_df_grp.get_group(time_id)\n                del trade_items['time_id']\n                trade_items = trade_items.values\n            except:\n                # If No trade\n                trade_items = np.zeros((1,4),dtype=np.float64)\n                \n            \n            row_id = self.test[(self.test.stock_id == stock_id) & (self.test.time_id == time_id) ].row_id.values\n            if len(row_id) == 0:\n                continue\n            row_id = row_id[0]\n            \n            item = row_id,stock_id,book_items.values,trade_items\n            data.append(item)\n            \n        return data\n    \n    def gen(self):\n        for stock_id in self.stock_ids:\n            for item in self.getStockData(stock_id):\n                yield item\n        \n    def __len__(self):\n        return len(self.train)","metadata":{"_kg_hide-input":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-08-11T03:44:55.171578Z","iopub.execute_input":"2021-08-11T03:44:55.171861Z","iopub.status.idle":"2021-08-11T03:44:55.184366Z","shell.execute_reply.started":"2021-08-11T03:44:55.171834Z","shell.execute_reply":"2021-08-11T03:44:55.182839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%skip tpu\ntest_data = TestDataset()","metadata":{"execution":{"iopub.status.busy":"2021-08-11T03:44:55.18601Z","iopub.execute_input":"2021-08-11T03:44:55.186584Z","iopub.status.idle":"2021-08-11T03:44:55.216942Z","shell.execute_reply.started":"2021-08-11T03:44:55.186536Z","shell.execute_reply":"2021-08-11T03:44:55.216002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%skip tpu\nfor x in test_data.gen():\n    break","metadata":{"execution":{"iopub.status.busy":"2021-08-11T03:44:55.218177Z","iopub.execute_input":"2021-08-11T03:44:55.21847Z","iopub.status.idle":"2021-08-11T03:44:55.359521Z","shell.execute_reply.started":"2021-08-11T03:44:55.218442Z","shell.execute_reply":"2021-08-11T03:44:55.358369Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def test_preprecoss(row_id,stock_id,book,trade):\n    book_data,trade_data = features_builder(stock_id,book,trade)\n        \n    book = tf.concat([book,book_data],axis=-1)\n    trade = tf.concat([trade,trade_data],axis=-1)\n        \n    p1 = [[0,MAX_SEQ-tf.shape(book)[0]],[0,0]]\n    p2 = [[0,MAX_SEQ-tf.shape(trade)[0]],[0,0]]\n        \n    book = tf.pad(book,p1, constant_values=0.)\n    trade = tf.pad(trade,p2, constant_values=0.)\n        \n    return row_id,(book,trade)","metadata":{"execution":{"iopub.status.busy":"2021-08-11T03:44:55.360985Z","iopub.execute_input":"2021-08-11T03:44:55.361466Z","iopub.status.idle":"2021-08-11T03:44:55.368426Z","shell.execute_reply.started":"2021-08-11T03:44:55.361417Z","shell.execute_reply":"2021-08-11T03:44:55.367648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%skip tpu\ntest_dataset = tf.data.Dataset.from_generator(test_data.gen,\n                                         output_signature=(\n                                             tf.TensorSpec(shape=(), dtype=tf.string),\n                                             tf.TensorSpec(shape=(),dtype=tf.float64),\n                                             tf.TensorSpec(shape=(None,9,),dtype=tf.float64),\n                                             tf.TensorSpec(shape=(None,4,),dtype=tf.float64),\n                                         )\n                                        )\n\ntest_dataset = test_dataset.map(test_preprecoss,num_parallel_calls=AUTO)\ntest_dataset = test_dataset.batch(64).prefetch(prefetch)","metadata":{"execution":{"iopub.status.busy":"2021-08-11T03:44:55.369778Z","iopub.execute_input":"2021-08-11T03:44:55.370272Z","iopub.status.idle":"2021-08-11T03:44:55.832065Z","shell.execute_reply.started":"2021-08-11T03:44:55.370225Z","shell.execute_reply":"2021-08-11T03:44:55.83119Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%skip tpu\ndef predict_fn(r,X):\n    return r, model(X,training=False)\ntest_dataset = test_dataset.map(predict_fn).prefetch(10)","metadata":{"execution":{"iopub.status.busy":"2021-08-11T03:44:55.833285Z","iopub.execute_input":"2021-08-11T03:44:55.833762Z","iopub.status.idle":"2021-08-11T03:44:57.595339Z","shell.execute_reply.started":"2021-08-11T03:44:55.833713Z","shell.execute_reply":"2021-08-11T03:44:57.594517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%skip tpu\nids,targets= [],[]\nfor (row_id,y) in test_dataset:\n    ids.extend(row_id.numpy().flatten())\n    targets.extend(y.numpy().flatten())\n    \nids = [s.decode('ascii') for s in ids]","metadata":{"execution":{"iopub.status.busy":"2021-08-11T03:44:57.598216Z","iopub.execute_input":"2021-08-11T03:44:57.598688Z","iopub.status.idle":"2021-08-11T03:44:58.232037Z","shell.execute_reply.started":"2021-08-11T03:44:57.598638Z","shell.execute_reply":"2021-08-11T03:44:58.231125Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%skip tpu\n# add missing rows\nmiss_idx = ~test_data.test.row_id.isin(ids)\nmiss = test_data.test.loc[miss_idx,'row_id'].values\nids.extend(miss)\ntargets.extend([0 for _ in miss])\n#targets.extend([train_df.target.min() for _ in miss])","metadata":{"execution":{"iopub.status.busy":"2021-08-11T03:44:58.233454Z","iopub.execute_input":"2021-08-11T03:44:58.23394Z","iopub.status.idle":"2021-08-11T03:44:58.240682Z","shell.execute_reply.started":"2021-08-11T03:44:58.233892Z","shell.execute_reply":"2021-08-11T03:44:58.239866Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%skip tpu\ndf = pd.DataFrame({'row_id':ids,'target':targets})\ndf.to_csv('submission.csv',index=False)\ndf.head(10)","metadata":{"execution":{"iopub.status.busy":"2021-08-11T03:44:58.241937Z","iopub.execute_input":"2021-08-11T03:44:58.242486Z","iopub.status.idle":"2021-08-11T03:44:58.273477Z","shell.execute_reply.started":"2021-08-11T03:44:58.242443Z","shell.execute_reply":"2021-08-11T03:44:58.272675Z"},"trusted":true},"execution_count":null,"outputs":[]}]}