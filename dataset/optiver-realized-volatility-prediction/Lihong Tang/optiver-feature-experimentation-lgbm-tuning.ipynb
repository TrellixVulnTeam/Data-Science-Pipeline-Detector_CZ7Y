{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This notebook explores different stock and volatility predictors. I gathered features engineered by the community, and will model and explain benefit.\n\nThis notebook combines the preprocessing and feature engineering from the following sources. Please check them out!\nhttps://www.kaggle.com/ragnar123/optiver-realized-volatility-lgbm-baseline  \nhttps://www.kaggle.com/konradb/we-need-to-go-deeper-and-validate  \nhttps://www.kaggle.com/tommy1028/lightgbm-starter-with-feature-engineering-idea\n\n\nFeedback is greatly appreciated! Thanks!!!!","metadata":{}},{"cell_type":"code","source":"import os\nimport glob\nfrom joblib import Parallel, delayed\n\nimport pandas as pd\nimport numpy as np\nimport scipy as sc\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split, KFold\nimport lightgbm as lgb\nfrom lightgbm import LGBMRegressor\nimport warnings\nwarnings.filterwarnings('ignore')\npd.set_option('max_columns', 300)\n\nimport optuna\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-20T19:03:53.33161Z","iopub.execute_input":"2021-08-20T19:03:53.332641Z","iopub.status.idle":"2021-08-20T19:03:53.341193Z","shell.execute_reply.started":"2021-08-20T19:03:53.33258Z","shell.execute_reply":"2021-08-20T19:03:53.340244Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"book_example = pd.read_parquet('../input/optiver-realized-volatility-prediction/book_train.parquet/stock_id=0')\nbook_example.head()\n","metadata":{"execution":{"iopub.status.busy":"2021-08-20T19:03:53.346855Z","iopub.execute_input":"2021-08-20T19:03:53.347506Z","iopub.status.idle":"2021-08-20T19:03:53.589402Z","shell.execute_reply.started":"2021-08-20T19:03:53.347452Z","shell.execute_reply":"2021-08-20T19:03:53.588122Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n# Feature Exploration\n\n# WAP\n\nThe Weighted Average Price is explained by the [tutorial notebook](http://www.kaggle.com/jiashenliu/introduction-to-financial-concepts-and-data) and is the measure of stock price as a function of bid/ask price and volume.\n\nThe first two WAP calculations are from the first two levels. The third averages the first two.\n\n# Log Return\n\nThe multiplicative change in price between consecutive snapshots within each time_id. Large returns contribute to more volatility.\n\n# Spread\n\nSpread features measure the gap between competing prices in the book. Larger gaps indicate less liquidity and may lead to bigger movements if the stock is traded as participants may settle for \"worse\" prices on either sides of the gap. On the other hand, large gaps could also indicate that there are not many interested in trading the stock.\n\n# Volume \n\nVolume features show intent. Large volume may act as a resistance that prevents the WAP from easily moving through a price-point. On the other hand, an imbalance could drive the price in a direction as supply outruns demand or vice-versa. Log return function is applied to measure changes in volume. A sudden change in the difference or the volume ratio between bid and ask are explored as features.\n\n# Aggregate Features\n\nAggregate features summarize all of the above. Sum, mean, standard deviation, min, max functions are used to track how our features behave within a single time-window or stock. Sum tracks total activity. Mean averages across all events in the window. Std measures spread. Min and max track extreme values.\n\n# Summarized Results\n\nFrom analysis, the realized volatility of log returns dominates feature importance. Put in another way, volatility values of the past 10 minutes are excellent predictors of volatility the next 10 minutes.\n\nLooking at non-volatility features, stock_id ranks highly. It is confirmed that the same stock_ids will be present in the test set, so the feature is very important to the model. Some volume and spread features are also represented, but their utility falls off compared to volatility features.","metadata":{}},{"cell_type":"code","source":"create_features = True\ntrain_mode = False\n\n\n# data directory\ndata_dir = '../input/optiver-realized-volatility-prediction/'\n\n# Function to calculate first WAP\n# def calc_wap1(df):\n#     wap = (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']) / (df['bid_size1'] + df['ask_size1'])\n#     return wap\n\n# # Function to calculate second WAP\n# def calc_wap2(df):\n#     wap = (df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2']) / (df['bid_size2'] + df['ask_size2'])\n#     return wap\n\n\ndef calc_wap1(df):\n    a1 = df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']\n    b1 = df['bid_size1'] + df['ask_size1']\n    \n    return a1/b1\n\ndef calc_wap2(df):\n    a1 = df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2']\n    b1 = df['bid_size2'] + df['ask_size2']\n    \n    return a1/b1\n\ndef calc_wap3(df):\n    a1 = df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']\n    b1 = df['bid_size1'] + df['ask_size1']\n    a2 = df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2']\n    b2 = df['bid_size2'] + df['ask_size2']\n    \n    x = (a1/b1 + a2/b2)/ 2\n    \n    return x\n\n\ndef calc_wap4(df):\n        \n    a1 = df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']\n    a2 = df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2']\n    b = df['bid_size1'] + df['ask_size1'] + df['bid_size2']+ df['ask_size2']\n    \n    x = (a1 + a2)/ b\n    \n    return x\n\n# Function to calculate the log of the return\n# Remember that logb(x / y) = logb(x) - logb(y)\ndef log_return(series):\n    return np.log(series).diff()\n\n# Calculate the realized volatility\ndef realized_volatility(series):\n    return np.sqrt(np.sum(series**2))\n\n# Function to count unique elements of a series\ndef count_unique(series):\n    return len(np.unique(series))\n\n# Function to read our base train and test set\ndef read_train_test():\n    train = pd.read_csv('../input/optiver-realized-volatility-prediction/train.csv')\n    test = pd.read_csv('../input/optiver-realized-volatility-prediction/test.csv')\n    # Create a key to merge with book and trade data\n    train['row_id'] = train['stock_id'].astype(str) + '-' + train['time_id'].astype(str)\n    test['row_id'] = test['stock_id'].astype(str) + '-' + test['time_id'].astype(str)\n    print(f'Our training set has {train.shape[0]} rows')\n    return train, test\n\n# Function to preprocess book data (for each stock id)\ndef book_preprocessor(file_path):\n    df = pd.read_parquet(file_path)\n    df.sort_values(by=['time_id', 'seconds_in_bucket'])\n    # Calculate Wap\n    df['wap1'] = calc_wap1(df)\n    df['wap2'] = calc_wap2(df)\n    df['wap3'] = calc_wap3(df)\n    #df['wap4'] = calc_wap4(df)\n    \n    \n    # Calculate log returns\n    \n    df['log_return1'] = df.groupby(['time_id'])['wap1'].apply(log_return)\n    df['log_return2'] = df.groupby(['time_id'])['wap2'].apply(log_return)\n    df['log_return3'] = df.groupby(['time_id'])['wap3'].apply(log_return)\n    #df['log_return4'] = df.groupby(['time_id'])['wap4'].apply(log_return)\n \n    # Calculate spread\n    df['price_spread'] = (df['ask_price1'] - df['bid_price1']) / ((df['ask_price1'] + df['bid_price1']) / 2)\n    df['ask_div_bid_price'] = df['ask_price1'] / df['bid_price1']\n    \n    #df['bid_spread'] = df['bid_price1'] - df['bid_price2']\n    #df['ask_spread'] = df['ask_price1'] - df['ask_price2']\n    \n    #Calculate volume\n    df['total_volume'] = (df['ask_size1'] + df['ask_size2']) + (df['bid_size1'] + df['bid_size2'])\n    df['volume_imbalance'] = abs((df['ask_size1'] + df['ask_size2']) - (df['bid_size1'] + df['bid_size2']))\n    \n    df['ask_div_bid_size'] = df['ask_size1'] / df['bid_size1']\n    \n    #Calculate log volume changes\n    #df['log_return_bid_size1'] = df.groupby(['time_id'])['bid_size1'].apply(log_return)\n    #df['log_return_ask_size1'] = df.groupby(['time_id'])['ask_size1'].apply(log_return)\n    df['log_return_ask_div_bid_size'] = df.groupby(['time_id'])['ask_div_bid_size'].apply(log_return)\n    df['log_return_total_volume'] = df.groupby(['time_id'])['total_volume'].apply(log_return)\n    \n    # Dict for aggregations\n    create_feature_dict = {\n        \n        'wap1': [np.sum, np.mean, np.std],\n        'wap2': [np.sum, np.mean, np.std],\n        'wap3': [np.sum, np.mean, np.std],\n        #'wap4': [np.sum, np.mean, np.std, np.max],\n        'log_return1': [np.sum, realized_volatility, np.mean, np.std, np.max, np.min],\n        'log_return2': [np.sum, realized_volatility, np.mean, np.std, np.max, np.min],\n        'log_return3': [np.sum, realized_volatility, np.mean, np.std, np.max, np.min],\n        #'log_return4': [np.sum, realized_volatility, np.mean, np.std, np.max],\n        'price_spread':[np.sum, np.mean, np.std, np.max, np.min],\n        #'bid_spread':[np.sum, np.mean, np.std, np.max],\n        #'ask_spread':[np.sum, np.mean, np.std, np.max],\n        'total_volume':[np.sum, np.mean, np.std, np.max, np.min],\n        'volume_imbalance':[np.sum, np.mean, np.std, np.max, np.min],\n        'ask_div_bid_price': [np.sum, np.mean, np.std, np.max, np.min],\n        'ask_div_bid_size': [np.sum, np.mean, np.std, np.max, np.min],\n        #'log_return_bid_size1': [np.sum, np.mean, np.std, np.max],\n        #'log_return_ask_size1': [np.sum, np.mean, np.std, np.max],\n        'log_return_total_volume': [np.sum, np.mean, np.std, np.max],\n        'log_return_ask_div_bid_size': [np.sum, np.mean, np.std, np.max]\n        \n    }\n    \n    # Function to get group stats for different windows (seconds in bucket)\n    def get_stats_window(seconds_in_bucket, add_suffix = False):\n        # Group by the window\n        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(create_feature_dict).reset_index()\n        # Rename columns joining suffix\n        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n        # Add a suffix to differentiate windows\n        if add_suffix:\n            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n        return df_feature\n    \n    # Get the stats for different windows\n    df_feature = get_stats_window(seconds_in_bucket = 0, add_suffix = False)\n    df_feature_450 = get_stats_window(seconds_in_bucket = 450, add_suffix = True)\n    df_feature_300 = get_stats_window(seconds_in_bucket = 300, add_suffix = True)\n    df_feature_150 = get_stats_window(seconds_in_bucket = 150, add_suffix = True)\n    \n    # Merge all\n    df_feature = df_feature.merge(df_feature_450, how = 'left', left_on = 'time_id_', right_on = 'time_id__450')\n    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n    df_feature = df_feature.merge(df_feature_150, how = 'left', left_on = 'time_id_', right_on = 'time_id__150')\n    # Drop unnecesary time_ids\n    df_feature.drop(['time_id__450', 'time_id__300', 'time_id__150'], axis = 1, inplace = True)\n    \n    # Create row_id so we can merge\n    stock_id = file_path.split('=')[1]\n    df_feature['row_id'] = df_feature['time_id_'].apply(lambda x: f'{stock_id}-{x}')\n    df_feature.drop(['time_id_'], axis = 1, inplace = True)\n    return df_feature\n\n# Function to preprocess trade data (for each stock id)\ndef trade_preprocessor(file_path):\n    df = pd.read_parquet(file_path)\n    df['log_return'] = df.groupby('time_id')['price'].apply(log_return)\n    \n    # Dict for aggregations\n    create_feature_dict = {\n        'log_return':[realized_volatility],\n        'seconds_in_bucket':[count_unique],\n        'size':[np.sum],\n        'order_count':[np.mean],\n    }\n    \n    # Function to get group stats for different windows (seconds in bucket)\n    def get_stats_window(seconds_in_bucket, add_suffix = False):\n        # Group by the window\n        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(create_feature_dict).reset_index()\n        # Rename columns joining suffix\n        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n        # Add a suffix to differentiate windows\n        if add_suffix:\n            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n        return df_feature\n    \n    # Get the stats for different windows\n    df_feature = get_stats_window(seconds_in_bucket = 0, add_suffix = False)\n    df_feature_450 = get_stats_window(seconds_in_bucket = 450, add_suffix = True)\n    df_feature_300 = get_stats_window(seconds_in_bucket = 300, add_suffix = True)\n    df_feature_150 = get_stats_window(seconds_in_bucket = 150, add_suffix = True)\n\n    # Merge all\n    df_feature = df_feature.merge(df_feature_450, how = 'left', left_on = 'time_id_', right_on = 'time_id__450')\n    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n    df_feature = df_feature.merge(df_feature_150, how = 'left', left_on = 'time_id_', right_on = 'time_id__150')\n    # Drop unnecesary time_ids\n    df_feature.drop(['time_id__450', 'time_id__300', 'time_id__150'], axis = 1, inplace = True)\n    \n    df_feature = df_feature.add_prefix('trade_')\n    stock_id = file_path.split('=')[1]\n    df_feature['row_id'] = df_feature['trade_time_id_'].apply(lambda x:f'{stock_id}-{x}')\n    df_feature.drop(['trade_time_id_'], axis = 1, inplace = True)\n    return df_feature\n\n# Function to get group stats for the stock_id and time_id\ndef get_time_stock(df):\n    # Get realized volatility columns\n    vol_cols = ['log_return1_realized_volatility', 'log_return2_realized_volatility', 'log_return1_realized_volatility_450', 'log_return2_realized_volatility_450', \n                'log_return1_realized_volatility_300', 'log_return2_realized_volatility_300', 'log_return1_realized_volatility_150', 'log_return2_realized_volatility_150', \n                'trade_log_return_realized_volatility', 'trade_log_return_realized_volatility_450', 'trade_log_return_realized_volatility_300', 'trade_log_return_realized_volatility_150']\n\n    # Group by the stock id\n    df_stock_id = df.groupby(['stock_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n    # Rename columns joining suffix\n    df_stock_id.columns = ['_'.join(col) for col in df_stock_id.columns]\n    df_stock_id = df_stock_id.add_suffix('_' + 'stock')\n\n    # Group by the stock id\n    df_time_id = df.groupby(['time_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n    # Rename columns joining suffix\n    df_time_id.columns = ['_'.join(col) for col in df_time_id.columns]\n    df_time_id = df_time_id.add_suffix('_' + 'time')\n    \n    # Merge with original dataframe\n    df = df.merge(df_stock_id, how = 'left', left_on = ['stock_id'], right_on = ['stock_id__stock'])\n    df = df.merge(df_time_id, how = 'left', left_on = ['time_id'], right_on = ['time_id__time'])\n    df.drop(['stock_id__stock', 'time_id__time'], axis = 1, inplace = True)\n    return df\n    \n# Funtion to make preprocessing function in parallel (for each stock id)\ndef preprocessor(list_stock_ids, is_train = True):\n    \n    # Parrallel for loop\n    def for_joblib(stock_id):\n        # Train\n        if is_train:\n            file_path_book = data_dir + \"book_train.parquet/stock_id=\" + str(stock_id)\n            file_path_trade = data_dir + \"trade_train.parquet/stock_id=\" + str(stock_id)\n        # Test\n        else:\n            file_path_book = data_dir + \"book_test.parquet/stock_id=\" + str(stock_id)\n            file_path_trade = data_dir + \"trade_test.parquet/stock_id=\" + str(stock_id)\n    \n        # Preprocess book and trade data and merge them\n        df_tmp = pd.merge(book_preprocessor(file_path_book), trade_preprocessor(file_path_trade), on = 'row_id', how = 'left')\n        \n        # Return the merge dataframe\n        return df_tmp\n    \n    # Use parallel api to call paralle for loop\n    df = Parallel(n_jobs = -1, verbose = 1)(delayed(for_joblib)(stock_id) for stock_id in list_stock_ids)\n    # Concatenate all the dataframes that return from Parallel\n    df = pd.concat(df, ignore_index = True)\n    return df\n\n# Function to calculate the root mean squared percentage error\ndef rmspe(y_true, y_pred):\n    return np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\n\n# Function to early stop with root mean squared percentage error\ndef feval_rmspe(y_pred, lgb_train):\n    y_true = lgb_train.get_label()\n    return 'RMSPE', rmspe(y_true, y_pred), False\n\n# def train_and_evaluate(train, test):\n#     # Hyperparammeters (just basic)\n#     params = {\n#       'objective': 'rmse',  \n#       'boosting_type': 'gbdt',\n#       'num_leaves': 100,\n#       'n_jobs': -1,\n#       'learning_rate': 0.1,\n#       'feature_fraction': 0.8,\n#       'bagging_fraction': 0.8,\n#       'verbose': -1\n#     }\n    \n#     # Split features and target\n#     x = train.drop(['row_id', 'target', 'time_id'], axis = 1)\n#     y = train['target']\n#     x_test = test.drop(['row_id', 'time_id'], axis = 1)\n#     # Transform stock id to a numeric value\n#     x['stock_id'] = x['stock_id'].astype(int)\n#     x_test['stock_id'] = x_test['stock_id'].astype(int)\n    \n#     # Create out of folds array\n#     oof_predictions = np.zeros(x.shape[0])\n#     # Create test array to store predictions\n#     test_predictions = np.zeros(x_test.shape[0])\n#     # Create a KFold object\n#     kfold = KFold(n_splits = 5, random_state = 66, shuffle = True)\n#     # Iterate through each fold\n#     for fold, (trn_ind, val_ind) in enumerate(kfold.split(x)):\n#         print(f'Training fold {fold + 1}')\n#         x_train, x_val = x.iloc[trn_ind], x.iloc[val_ind]\n#         y_train, y_val = y.iloc[trn_ind], y.iloc[val_ind]\n#         # Root mean squared percentage error weights\n#         train_weights = 1 / np.square(y_train)\n#         val_weights = 1 / np.square(y_val)\n#         train_dataset = lgb.Dataset(x_train, y_train, weight = train_weights, categorical_feature = ['stock_id'])\n#         val_dataset = lgb.Dataset(x_val, y_val, weight = val_weights, categorical_feature = ['stock_id'])\n#         model = lgb.train(params = params, \n#                           train_set = train_dataset, \n#                           valid_sets = [train_dataset, val_dataset], \n#                           num_boost_round = 10000, \n#                           early_stopping_rounds = 50, \n#                           verbose_eval = 50,\n#                           feval = feval_rmspe)\n#         # Add predictions to the out of folds array\n#         oof_predictions[val_ind] = model.predict(x_val)\n#         # Predict the test set\n#         test_predictions += model.predict(x_test) / 5\n        \n#     rmspe_score = rmspe(y, oof_predictions)\n#     print(f'Our out of folds RMSPE is {rmspe_score}')\n#     # Return test predictions\n#     return test_predictions\n","metadata":{"execution":{"iopub.status.busy":"2021-08-20T19:03:53.593829Z","iopub.execute_input":"2021-08-20T19:03:53.594279Z","iopub.status.idle":"2021-08-20T19:03:53.647722Z","shell.execute_reply.started":"2021-08-20T19:03:53.594197Z","shell.execute_reply":"2021-08-20T19:03:53.646157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#N_TRIALS = 100\nTIME = 3600*6.5\nN_SPLITS = 5\nRANDOM_STATE = 99\nkfold = KFold(N_SPLITS, random_state=RANDOM_STATE, shuffle=True)\n\nFIXED_PARAMS = {'n_estimators': 10000,\n                'learning_rate': 0.1,\n                'metric': 'rmse',\n                'verbosity': -1,\n                'n_jobs': -1,\n                #'max_bin': 127,\n                'seed': RANDOM_STATE}\n\ndef rmspe(y_true, y_pred):\n        return  (np.sqrt(np.mean(np.square((y_true - y_pred) / y_true))))\n\n    \ndef objective(trial, cv=kfold):\n    \n    params = {\n        \n        'num_leaves': trial.suggest_int('num_leaves', 2, 1024),\n        'max_depth': trial.suggest_int('max_depth', 2, 20),\n        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n        'reg_alpha': trial.suggest_float('reg_alpha', 0.001, 10),\n        'reg_lambda': trial.suggest_float('reg_lambda', 0.001, 10),\n        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.4, 1),\n        'subsample': trial.suggest_float('subsample', 0.4, 1),\n        'cat_smooth': trial.suggest_float('cat_smooth', 10, 100.0),  \n        'cat_l2': trial.suggest_int('cat_l2', 1, 20),\n       \n    }\n    \n    params.update(FIXED_PARAMS)\n\n    pruning_callback = optuna.integration.LightGBMPruningCallback(trial, \"rmse\", valid_name='valid_1')\n    rmspe_list = []\n    \n    for kfold, (train_idx, val_idx) in enumerate(cv.split(X, y)):\n        X_train = X.iloc[train_idx]\n        y_train = y.iloc[train_idx]\n        X_val = X.iloc[val_idx]\n        y_val = y.iloc[val_idx]\n\n        d_train = lgb.Dataset(X_train, label=y_train)\n        d_valid = lgb.Dataset(X_val, label=y_val)\n\n        model = lgb.train(params,\n                      train_set=d_train,\n                      valid_sets=[d_train, d_valid],\n                      verbose_eval=0,\n                      early_stopping_rounds=100,\n                      callbacks=[pruning_callback])\n\n        preds = model.predict(X_val)\n        score = rmspe(y_val, preds)\n        \n        rmspe_list.append(score)\n        \n    \n    return np.mean(rmspe_list)\n","metadata":{"execution":{"iopub.status.busy":"2021-08-20T19:03:53.650499Z","iopub.execute_input":"2021-08-20T19:03:53.651332Z","iopub.status.idle":"2021-08-20T19:03:53.669475Z","shell.execute_reply.started":"2021-08-20T19:03:53.651265Z","shell.execute_reply":"2021-08-20T19:03:53.668151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if create_features:\n    # Read train and test\n    train, test = read_train_test()\n\n    # Get unique stock ids \n    train_stock_ids = train['stock_id'].unique()\n    # Preprocess them using Parallel and our single stock id functions\n    train_ = preprocessor(train_stock_ids, is_train = True)\n    train = train.merge(train_, on = ['row_id'], how = 'left')\n\n    # Get unique stock ids \n    test_stock_ids = test['stock_id'].unique()\n    # Preprocess them using Parallel and our single stock id functions\n    test_ = preprocessor(test_stock_ids, is_train = False)\n    test = test.merge(test_, on = ['row_id'], how = 'left')\n\n# Get group stats of time_id and stock_id\n\n    \n    train = get_time_stock(train)\n    test = get_time_stock(test)\n\n    train.to_pickle('train_features_df.pickle')\n    test.to_pickle('test_features_df.pickle')\nelse:\n#     train = pd.read_pickle(\"../input/features/train_features_df.pickle\")\n#     test = pd.read_pickle(\"../input/features/test_features_df.pickle\")\n    train = pd.read_pickle(\"../input/optiver-feats/train_features_df.pickle\")\n    test = pd.read_pickle(\"../input/optiver-feats/test_features_df.pickle\")","metadata":{"execution":{"iopub.status.busy":"2021-08-20T19:03:53.671416Z","iopub.execute_input":"2021-08-20T19:03:53.671814Z","iopub.status.idle":"2021-08-20T19:29:41.138403Z","shell.execute_reply.started":"2021-08-20T19:03:53.671772Z","shell.execute_reply":"2021-08-20T19:29:41.137185Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.shape","metadata":{"execution":{"iopub.status.busy":"2021-08-20T19:29:41.139971Z","iopub.execute_input":"2021-08-20T19:29:41.14028Z","iopub.status.idle":"2021-08-20T19:29:41.146624Z","shell.execute_reply.started":"2021-08-20T19:29:41.14025Z","shell.execute_reply":"2021-08-20T19:29:41.145708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_display = train.drop(['row_id', 'time_id', 'target'], axis = 1)\nX = X_display\ny = train['target']\n","metadata":{"execution":{"iopub.status.busy":"2021-08-20T19:29:41.147859Z","iopub.execute_input":"2021-08-20T19:29:41.148138Z","iopub.status.idle":"2021-08-20T19:29:41.433049Z","shell.execute_reply.started":"2021-08-20T19:29:41.148102Z","shell.execute_reply":"2021-08-20T19:29:41.43221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_display.head(10)","metadata":{"execution":{"iopub.status.busy":"2021-08-20T19:29:41.434213Z","iopub.execute_input":"2021-08-20T19:29:41.434629Z","iopub.status.idle":"2021-08-20T19:29:41.771757Z","shell.execute_reply.started":"2021-08-20T19:29:41.434589Z","shell.execute_reply":"2021-08-20T19:29:41.770704Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if train_mode:\n    study = optuna.create_study(direction='minimize', pruner=optuna.pruners.MedianPruner(n_warmup_steps=25))\n    study.optimize(objective, timeout=TIME)\n    \n    print('Number of finished trials:', len(study.trials))\n    print('Best trial:', study.best_trial.params)","metadata":{"execution":{"iopub.status.busy":"2021-08-20T19:29:41.77431Z","iopub.execute_input":"2021-08-20T19:29:41.774662Z","iopub.status.idle":"2021-08-20T19:29:43.647841Z","shell.execute_reply.started":"2021-08-20T19:29:41.774629Z","shell.execute_reply":"2021-08-20T19:29:43.646989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Trial 24 finished with value: 0.24486020313983187 and parameters: {'num_leaves': 883, 'max_depth': 25, 'min_child_samples': 51, 'reg_alpha': 0.044271390027111224, 'reg_lambda': 4.3452654181772346, 'colsample_bytree': 0.7423339290785697, 'subsample': 0.9574494840064829, 'cat_smooth': 50.81669106814311, 'cat_l2': 14}. Best is trial 24 with value: 0.24486020313983187.","metadata":{}},{"cell_type":"code","source":"if train_mode:\n    optuna.visualization.plot_optimization_history(study)","metadata":{"execution":{"iopub.status.busy":"2021-08-20T19:29:43.662767Z","iopub.execute_input":"2021-08-20T19:29:43.663039Z","iopub.status.idle":"2021-08-20T19:29:43.673555Z","shell.execute_reply.started":"2021-08-20T19:29:43.663013Z","shell.execute_reply":"2021-08-20T19:29:43.672509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if train_mode:\n    optuna.visualization.plot_param_importances(study)","metadata":{"execution":{"iopub.status.busy":"2021-08-20T19:29:43.675041Z","iopub.execute_input":"2021-08-20T19:29:43.675373Z","iopub.status.idle":"2021-08-20T19:29:43.685163Z","shell.execute_reply.started":"2021-08-20T19:29:43.675295Z","shell.execute_reply":"2021-08-20T19:29:43.684178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if train_mode:\n    study.best_params","metadata":{"execution":{"iopub.status.busy":"2021-08-20T19:29:43.686423Z","iopub.execute_input":"2021-08-20T19:29:43.686809Z","iopub.status.idle":"2021-08-20T19:29:43.696813Z","shell.execute_reply.started":"2021-08-20T19:29:43.686768Z","shell.execute_reply":"2021-08-20T19:29:43.695934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if train_mode:\n    best_lgbmparams = study.best_params\n    \nelse:\n    best_lgbmparams = {'num_leaves': 181, 'max_depth': 20, 'min_child_samples': 33, 'reg_alpha': 0.004098886302163573, 'reg_lambda': 4.865281409093663, 'colsample_bytree': 0.43240377400000285, 'subsample': 0.9902166770372244, 'cat_smooth': 54.672676882001234, 'cat_l2': 19}\n    best_lgbmparams.update(FIXED_PARAMS)\n#     best_lgbmparams = {'learning_rate': 0.11352081667311227,\n#                          'max_depth': 206,\n#                          'lambda_l1': 3.802017952502632e-06,\n#                          'lambda_l2': 6.33667047986424,\n#                          'num_leaves': 69,\n#                          'n_estimators': 540,\n#                          'feature_fraction': 0.6675754770916755,\n#                          'bagging_fraction': 0.694980646638328,\n#                          'bagging_freq': 2,\n#                          'min_child_samples': 8}\n#     best_lgbmparams.update(FIXED_PARAMS)","metadata":{"execution":{"iopub.status.busy":"2021-08-20T19:29:43.698078Z","iopub.execute_input":"2021-08-20T19:29:43.698354Z","iopub.status.idle":"2021-08-20T19:29:43.708788Z","shell.execute_reply.started":"2021-08-20T19:29:43.698329Z","shell.execute_reply":"2021-08-20T19:29:43.707807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = test.drop(['time_id'], axis = 1)\n\nX_test = test.drop(['row_id'], axis = 1)","metadata":{"execution":{"iopub.status.busy":"2021-08-20T19:29:43.709922Z","iopub.execute_input":"2021-08-20T19:29:43.71036Z","iopub.status.idle":"2021-08-20T19:29:43.725878Z","shell.execute_reply.started":"2021-08-20T19:29:43.710319Z","shell.execute_reply":"2021-08-20T19:29:43.724807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def calc_model_importance(model, feature_names=None, importance_type='gain'):\n    importance_df = pd.DataFrame(model.feature_importance(importance_type=importance_type),\n                                 index=feature_names,\n                                 columns=['importance']).sort_values('importance')\n    return importance_df\n","metadata":{"execution":{"iopub.status.busy":"2021-08-20T19:29:43.727184Z","iopub.execute_input":"2021-08-20T19:29:43.727618Z","iopub.status.idle":"2021-08-20T19:29:43.735179Z","shell.execute_reply.started":"2021-08-20T19:29:43.727582Z","shell.execute_reply":"2021-08-20T19:29:43.734421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kfold = KFold(N_SPLITS, random_state=RANDOM_STATE, shuffle=True)\n# Create out of folds array\noof_predictions = np.zeros(X.shape[0])\n# Create test array to store predictions\ntest_predictions = np.zeros(X_test.shape[0])\n\n# Importance lists for plotting\ngain_importance_list = []\nsplit_importance_list = []\n\nfor fold, (trn_ind, val_ind) in enumerate(kfold.split(X)):\n        print(f'Training fold {fold + 1}')\n        x_train, x_val = X.iloc[trn_ind], X.iloc[val_ind]\n        y_train, y_val = y.iloc[trn_ind], y.iloc[val_ind]\n        # Root mean squared percentage error weights\n        train_weights = 1 / np.square(y_train)\n        val_weights = 1 / np.square(y_val)\n        train_dataset = lgb.Dataset(x_train, y_train, weight = train_weights, categorical_feature = ['stock_id'])\n        val_dataset = lgb.Dataset(x_val, y_val, weight = val_weights, categorical_feature = ['stock_id'])\n        model = lgb.train(params = best_lgbmparams, \n                          train_set = train_dataset, \n                          valid_sets = [train_dataset, val_dataset], \n                          num_boost_round = 10000, \n                          early_stopping_rounds = 100, \n                          verbose_eval = 50,\n                          feval = feval_rmspe)\n        # Add predictions to the out of folds array\n        oof_predictions[val_ind] = model.predict(x_val)\n        # Predict the test set\n        test_predictions += model.predict(X_test) / 5\n        \n        #Feature importance\n        feature_names = x_train.columns.values.tolist()\n        gain_importance_df = calc_model_importance(\n            model, feature_names=feature_names, importance_type='gain')\n        gain_importance_list.append(gain_importance_df)\n\n        split_importance_df = calc_model_importance(\n            model, feature_names=feature_names, importance_type='split')\n        split_importance_list.append(split_importance_df)\n\nrmspe_score = rmspe(y, oof_predictions)\nprint(f'Our out of folds RMSPE is {rmspe_score}')","metadata":{"execution":{"iopub.status.busy":"2021-08-20T19:29:43.736491Z","iopub.execute_input":"2021-08-20T19:29:43.737074Z","iopub.status.idle":"2021-08-20T19:38:06.848948Z","shell.execute_reply.started":"2021-08-20T19:29:43.737032Z","shell.execute_reply":"2021-08-20T19:38:06.847938Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"non_vol = [x for x in list(X.columns.values) if 'volatility' not in x]\n\ndef calc_mean_importance(importance_df_list):\n    mean_importance = np.mean(\n        np.array([df['importance'].values for df in importance_df_list]), axis=0)\n    mean_df = importance_df_list[0].copy()\n    mean_df['importance'] = mean_importance\n    \n    return mean_df\n\ndef plot_importance(importance_df, title='', save_filepath=None, figsize=(8, 12), exclude_vol=False):\n    fig, ax = plt.subplots(figsize=figsize)\n    if exclude_vol:\n        importance_df = importance_df.loc[importance_df.index.isin(non_vol)]\n    importance_df = importance_df.sort_values(by='importance',ascending=False).head(15)\n    importance_df.plot.barh(ax=ax)\n    if title:\n        plt.title(title)\n    plt.tight_layout()\n    if save_filepath is None:\n        plt.show()\n    else:\n        plt.savefig(save_filepath)\n    plt.close()\n    \n\nmean_gain_df = calc_mean_importance(gain_importance_list)\nplot_importance(mean_gain_df, title='Model feature importance by gain')\n","metadata":{"execution":{"iopub.status.busy":"2021-08-20T19:38:06.850496Z","iopub.execute_input":"2021-08-20T19:38:06.850787Z","iopub.status.idle":"2021-08-20T19:38:07.18521Z","shell.execute_reply.started":"2021-08-20T19:38:06.850759Z","shell.execute_reply":"2021-08-20T19:38:07.184187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_importance(mean_gain_df, title='Non-volatility feature importance', exclude_vol=True)","metadata":{"execution":{"iopub.status.busy":"2021-08-20T19:38:07.186438Z","iopub.execute_input":"2021-08-20T19:38:07.18673Z","iopub.status.idle":"2021-08-20T19:38:07.456442Z","shell.execute_reply.started":"2021-08-20T19:38:07.186701Z","shell.execute_reply":"2021-08-20T19:38:07.45452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test['target'] = test_predictions\ntest[['row_id', 'target']].to_csv('submission.csv',index = False)\n","metadata":{"execution":{"iopub.status.busy":"2021-08-20T19:38:07.457497Z","iopub.execute_input":"2021-08-20T19:38:07.457784Z","iopub.status.idle":"2021-08-20T19:38:07.464674Z","shell.execute_reply.started":"2021-08-20T19:38:07.457754Z","shell.execute_reply":"2021-08-20T19:38:07.463955Z"},"trusted":true},"execution_count":null,"outputs":[]}]}