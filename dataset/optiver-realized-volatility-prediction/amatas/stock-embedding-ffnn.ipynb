{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Realized Volatility Prediction\n\n\nA NN using Embedding.\n\n**References**\n* NOTE: Idea code based on https://www.kaggle.com/syerwin/stock-embedding-ffnn-my-features-a5d6b0\n* https://www.kaggle.com/alexioslyon/stock-embedding-ffnn-my-features\n\n\n* https://www.kaggle.com/jiashenliu/introduction-to-financial-concepts-and-data\n* https://www.kaggle.com/vbmokin/data-science-for-tabular-data-advanced-techniques\n* Embedding layer from : https://www.kaggle.com/colinmorris/embedding-layers\n\n**My Work**\n* Code learning, refactoring and understanding.\n","metadata":{}},{"cell_type":"markdown","source":"# Import Libraries","metadata":{}},{"cell_type":"code","source":"from IPython.core.display import display, HTML\n\nimport pandas as pd\nimport numpy as np\nimport glob\nimport os\nimport gc\n\nfrom joblib import Parallel, delayed\n\nfrom sklearn import preprocessing, model_selection\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import QuantileTransformer\nfrom sklearn.metrics import r2_score\n\nimport matplotlib.pyplot as plt \nimport seaborn as sns\nimport numpy.matlib\n\n\nfrom numpy.random import seed\nseed(114)\nimport tensorflow as tf\ntf.random.set_seed(114)\nfrom tensorflow import keras\nimport numpy as np\nfrom keras import backend as K\n\n\nfrom sklearn.cluster import KMeans\n\n\n#https://bignerdranch.com/blog/implementing-swish-activation-function-in-keras/\nfrom keras.backend import sigmoid\ndef swish(x, beta = 1):\n    return (x * sigmoid(beta * x))\nfrom keras.utils.generic_utils import get_custom_objects\nfrom keras.layers import Activation\nget_custom_objects().update({'swish': Activation(swish)})\n\n","metadata":{"execution":{"iopub.status.busy":"2021-08-27T12:55:13.569005Z","iopub.execute_input":"2021-08-27T12:55:13.569366Z","iopub.status.idle":"2021-08-27T12:55:19.52094Z","shell.execute_reply.started":"2021-08-27T12:55:13.569288Z","shell.execute_reply":"2021-08-27T12:55:19.520079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Variables & Parameters","metadata":{}},{"cell_type":"code","source":"path_submissions = '/'\n# data directory\ndata_dir = '../input/optiver-realized-volatility-prediction/'\ntrain_filename = '../input/optiver-realized-volatility-prediction/train.csv'\ntest_filename = '../input/optiver-realized-volatility-prediction/test.csv'\n\ntarget_name = 'target'\nscores_folds = {}\n\n# Model Parameters\nlearning_rate = 0.0011\nnum_epochs = 200 # 1000","metadata":{"execution":{"iopub.status.busy":"2021-08-27T12:55:19.524666Z","iopub.execute_input":"2021-08-27T12:55:19.524913Z","iopub.status.idle":"2021-08-27T12:55:19.528823Z","shell.execute_reply.started":"2021-08-27T12:55:19.52489Z","shell.execute_reply":"2021-08-27T12:55:19.528051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Functions","metadata":{}},{"cell_type":"code","source":"# Function to calculate first WAP\ndef wap1(df):\n    wap = (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']) / (df['bid_size1'] + df['ask_size1'])\n    return wap\n\n# to calculate second WAP\ndef wap2(df):\n    wap = (df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2']) / (df['bid_size2'] + df['ask_size2'])\n    return wap\n\n# to calculate the log of the return\n# log(xi, xi-1) = log(xi) - log(xi-1)\ndef log_return(series):\n    return np.log(series).diff()\n\n# to calculate the realized volatility\ndef realized_volatility(series):\n    return np.sqrt(np.sum(series**2))\n\n# Function to count unique elements of a series\ndef count_unique(series):\n    return len(np.unique(series))\n\n# Function to read our base train and test set\ndef read_train_test(train_filename, test_filename):\n    ##TOREMOVE: _train = pd.read_csv('../input/optiver-realized-volatility-prediction/train.csv')\n    ##TOREMOVE: _test = pd.read_csv('../input/optiver-realized-volatility-prediction/test.csv')\n    _train = pd.read_csv(train_filename)\n    _test = pd.read_csv(test_filename)\n    # Create a key to merge with book and trade data\n    _train['row_id'] = _train['stock_id'].astype(str) + '-' + _train['time_id'].astype(str)\n    _test['row_id'] = _test['stock_id'].astype(str) + '-' + _test['time_id'].astype(str)\n    print(f'The training set has {_train.shape[0]} rows')\n    print(f'The test set has {_train.shape[0]} rows')\n          \n    ##TODO BORRAR\n    #d = _train[_train['stock_id']==1]\n    #return d, _test\n    \n    return _train, _test\n          \n\ndef root_mean_squared_per_error(y_true, y_pred):\n    return K.sqrt(K.mean(K.square( (y_true - y_pred)/ y_true )))","metadata":{"execution":{"iopub.status.busy":"2021-08-27T12:55:19.530655Z","iopub.execute_input":"2021-08-27T12:55:19.531241Z","iopub.status.idle":"2021-08-27T12:55:19.541733Z","shell.execute_reply.started":"2021-08-27T12:55:19.531188Z","shell.execute_reply":"2021-08-27T12:55:19.540849Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# book_preprocessor Function","metadata":{}},{"cell_type":"code","source":"# to preprocess book data (for each stock id)\ndef book_preprocessor(file_path):\n    \n    print ('Init: book_preprocessor')\n    \n    df = pd.read_parquet(file_path)\n    \n    # Calculate Wap\n    df['wap1'] = wap1(df)\n    df['wap2'] = wap2(df)\n    \n    # Calculate log returns\n    df['log_return1'] = df.groupby(['time_id'])['wap1'].apply(log_return)\n    df['log_return2'] = df.groupby(['time_id'])['wap2'].apply(log_return)\n    # Calculate wap balance\n    df['wap_balance'] = abs(df['wap1'] - df['wap2'])\n    # Calculate spread\n    df['price_spread'] = (df['ask_price1'] - df['bid_price1']) / ((df['ask_price1'] + df['bid_price1']) / 2)\n    df['price_spread2'] = (df['ask_price2'] - df['bid_price2']) / ((df['ask_price2'] + df['bid_price2']) / 2)\n    df['bid_spread'] = df['bid_price1'] - df['bid_price2']\n    df['ask_spread'] = df['ask_price1'] - df['ask_price2']\n    df[\"bid_ask_spread\"] = abs(df['bid_spread'] - df['ask_spread'])\n    df['total_volume'] = (df['ask_size1'] + df['ask_size2']) + (df['bid_size1'] + df['bid_size2'])\n    df['volume_imbalance'] = abs((df['ask_size1'] + df['ask_size2']) - (df['bid_size1'] + df['bid_size2']))\n    \n    # Dict for aggregations\n    create_feature_dict = {\n        'wap1': [np.sum, np.mean, np.std],\n        'wap2': [np.sum, np.mean, np.std],\n        'log_return1': [np.sum, realized_volatility, np.mean, np.std],\n        'log_return2': [np.sum, realized_volatility, np.mean, np.std],\n        'wap_balance': [np.sum, np.mean, np.std],\n        'price_spread':[np.sum, np.mean, np.std],\n        'price_spread2':[np.sum, np.mean, np.std],\n        'bid_spread':[np.sum, np.mean, np.std],\n        'ask_spread':[np.sum, np.mean, np.std],\n        'total_volume':[np.sum, np.mean, np.std],\n        'volume_imbalance':[np.sum, np.mean, np.std],\n        \"bid_ask_spread\":[np.sum, np.mean, np.std],\n    }\n    \n    # Function to get group stats for different windows (seconds in bucket)\n    def get_stats_window(seconds_in_bucket, add_suffix = False):\n        # Group by the window\n        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(create_feature_dict).reset_index()\n        # Rename columns joining suffix\n        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n        # Add a suffix to differentiate windows\n        if add_suffix:\n            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n        return df_feature\n    \n    # Get the stats for different windows\n    df_feature = get_stats_window(seconds_in_bucket = 0, add_suffix = False)\n    #df_feature_450 = get_stats_window(seconds_in_bucket = 450, add_suffix = True)\n    #df_feature_500 = get_stats_window(seconds_in_bucket = 500, add_suffix = True)\n    df_feature_400 = get_stats_window(seconds_in_bucket = 400, add_suffix = True)\n    df_feature_300 = get_stats_window(seconds_in_bucket = 300, add_suffix = True)\n    df_feature_200 = get_stats_window(seconds_in_bucket = 200, add_suffix = True)\n    #df_feature_150 = get_stats_window(seconds_in_bucket = 150, add_suffix = True)\n\n    # Merge all\n    #df_feature = df_feature.merge(df_feature_450, how = 'left', left_on = 'time_id_', right_on = 'time_id__450')\n    df_feature = df_feature.merge(df_feature_400, how = 'left', left_on = 'time_id_', right_on = 'time_id__400')\n    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n    df_feature = df_feature.merge(df_feature_200, how = 'left', left_on = 'time_id_', right_on = 'time_id__200')\n    #df_feature = df_feature.merge(df_feature_150, how = 'left', left_on = 'time_id_', right_on = 'time_id__150')\n    #df_feature = df_feature.merge(df_feature_100, how = 'left', left_on = 'time_id_', right_on = 'time_id__100')\n    # Drop unnecesary time_ids\n    #df_feature.drop(['time_id__450', 'time_id__300', 'time_id__150'], axis = 1, inplace = True)\n    df_feature.drop(['time_id__400', 'time_id__300', 'time_id__200'], axis = 1, inplace = True)\n    \n    # Create row_id so we can merge\n    stock_id = file_path.split('=')[1]\n    df_feature['row_id'] = df_feature['time_id_'].apply(lambda x: f'{stock_id}-{x}')\n    df_feature.drop(['time_id_'], axis = 1, inplace = True)\n    \n    print ('End: book_preprocessor')\n    \n    return df_feature","metadata":{"execution":{"iopub.status.busy":"2021-08-27T12:55:19.543404Z","iopub.execute_input":"2021-08-27T12:55:19.543787Z","iopub.status.idle":"2021-08-27T12:55:19.563536Z","shell.execute_reply.started":"2021-08-27T12:55:19.543748Z","shell.execute_reply":"2021-08-27T12:55:19.562573Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# trade_preprocessor Function","metadata":{}},{"cell_type":"code","source":"# Function to preprocess trade data (for each stock id)\ndef trade_preprocessor(file_path):\n    print ('Init: trade_preprocessor')\n    df = pd.read_parquet(file_path)\n    df['log_return'] = df.groupby('time_id')['price'].apply(log_return)\n    \n    # Dict for aggregations\n    create_feature_dict = {\n        'log_return':[realized_volatility],\n        'seconds_in_bucket':[count_unique],\n        'size':[np.sum, realized_volatility, np.mean, np.std, np.max, np.min],\n        'order_count':[np.mean,np.sum,np.max],\n    }\n    \n    # Function to get group stats for different windows (seconds in bucket)\n    def get_stats_window(seconds_in_bucket, add_suffix = False):\n        # Group by the window\n        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(create_feature_dict).reset_index()\n        # Rename columns joining suffix\n        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n        # Add a suffix to differentiate windows\n        if add_suffix:\n            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n        return df_feature\n    \n\n    # Get the stats for different windows\n    df_feature = get_stats_window(seconds_in_bucket = 0, add_suffix = False)\n    #df_feature_450 = get_stats_window(seconds_in_bucket = 450, add_suffix = True)\n    #df_feature_500 = get_stats_window(seconds_in_bucket = 500, add_suffix = True)\n    df_feature_400 = get_stats_window(seconds_in_bucket = 400, add_suffix = True)\n    df_feature_300 = get_stats_window(seconds_in_bucket = 300, add_suffix = True)\n    df_feature_200 = get_stats_window(seconds_in_bucket = 200, add_suffix = True)\n    #df_feature_150 = get_stats_window(seconds_in_bucket = 150, add_suffix = True)\n    \n    def tendency(price, vol):    \n        df_diff = np.diff(price)\n        val = (df_diff/price[1:])*100\n        power = np.sum(val*vol[1:])\n        return(power)\n    \n    lis = []\n    for n_time_id in df['time_id'].unique():\n        df_id = df[df['time_id'] == n_time_id]        \n        tendencyV = tendency(df_id['price'].values, df_id['size'].values)      \n        f_max = np.sum(df_id['price'].values > np.mean(df_id['price'].values))\n        f_min = np.sum(df_id['price'].values < np.mean(df_id['price'].values))\n        df_max =  np.sum(np.diff(df_id['price'].values) > 0)\n        df_min =  np.sum(np.diff(df_id['price'].values) < 0)\n        # new\n        abs_diff = np.median(np.abs( df_id['price'].values - np.mean(df_id['price'].values)))        \n        energy = np.mean(df_id['price'].values**2)\n        iqr_p = np.percentile(df_id['price'].values,75) - np.percentile(df_id['price'].values,25)\n        \n        # vol vars\n        \n        abs_diff_v = np.median(np.abs( df_id['size'].values - np.mean(df_id['size'].values)))        \n        energy_v = np.sum(df_id['size'].values**2)\n        iqr_p_v = np.percentile(df_id['size'].values,75) - np.percentile(df_id['size'].values,25)\n        \n        lis.append({'time_id':n_time_id,'tendency':tendencyV,'f_max':f_max,'f_min':f_min,'df_max':df_max,'df_min':df_min,\n                   'abs_diff':abs_diff,'energy':energy,'iqr_p':iqr_p,'abs_diff_v':abs_diff_v,'energy_v':energy_v,'iqr_p_v':iqr_p_v})\n    \n    df_lr = pd.DataFrame(lis)\n        \n   \n    df_feature = df_feature.merge(df_lr, how = 'left', left_on = 'time_id_', right_on = 'time_id')\n    \n    # Merge all\n    #df_feature = df_feature.merge(df_feature_450, how = 'left', left_on = 'time_id_', right_on = 'time_id__450')\n    df_feature = df_feature.merge(df_feature_400, how = 'left', left_on = 'time_id_', right_on = 'time_id__400')\n    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n    #df_feature = df_feature.merge(df_feature_150, how = 'left', left_on = 'time_id_', right_on = 'time_id__150')\n    df_feature = df_feature.merge(df_feature_200, how = 'left', left_on = 'time_id_', right_on = 'time_id__200')\n    # Drop unnecesary time_ids\n    #df_feature.drop(['time_id__450', 'time_id__300', 'time_id__150','time_id'], axis = 1, inplace = True)\n    df_feature.drop(['time_id__400', 'time_id__300', 'time_id__200','time_id'], axis = 1, inplace = True)\n    \n    \n    df_feature = df_feature.add_prefix('trade_')\n    stock_id = file_path.split('=')[1]\n    df_feature['row_id'] = df_feature['trade_time_id_'].apply(lambda x:f'{stock_id}-{x}')\n    df_feature.drop(['trade_time_id_'], axis = 1, inplace = True)\n    \n    print ('End: trade_preprocessor')\n    \n    return df_feature\n\n","metadata":{"execution":{"iopub.status.busy":"2021-08-27T12:55:19.565054Z","iopub.execute_input":"2021-08-27T12:55:19.565514Z","iopub.status.idle":"2021-08-27T12:55:19.731645Z","shell.execute_reply.started":"2021-08-27T12:55:19.565478Z","shell.execute_reply":"2021-08-27T12:55:19.730761Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# preprocessor Function","metadata":{}},{"cell_type":"code","source":"# Function to get group stats for the stock_id and time_id\ndef get_time_stock(df):\n    # Get realized volatility columns\n    #vol_cols = ['log_return1_realized_volatility', 'log_return2_realized_volatility', 'log_return1_realized_volatility_450', 'log_return2_realized_volatility_450', \n    #            'log_return1_realized_volatility_300', 'log_return2_realized_volatility_300', 'log_return1_realized_volatility_150', 'log_return2_realized_volatility_150', \n    #           'trade_log_return_realized_volatility', 'trade_log_return_realized_volatility_450', 'trade_log_return_realized_volatility_300', 'trade_log_return_realized_volatility_150']\n    vol_cols = ['log_return1_realized_volatility', 'log_return2_realized_volatility', 'log_return1_realized_volatility_400', 'log_return2_realized_volatility_400', \n                'log_return1_realized_volatility_300', 'log_return2_realized_volatility_300', 'log_return1_realized_volatility_200', 'log_return2_realized_volatility_200', \n                'trade_log_return_realized_volatility', 'trade_log_return_realized_volatility_400', 'trade_log_return_realized_volatility_300', 'trade_log_return_realized_volatility_200']\n    #     vol_cols = ['log_return1_realized_volatility', 'log_return2_realized_volatility',\n    #                 'log_return1_realized_volatility_600', 'log_return2_realized_volatility_600', \n    #                 'log_return1_realized_volatility_400', 'log_return2_realized_volatility_400',\n    # #                 'log_return1_realized_volatility_300', 'log_return2_realized_volatility_300', \n    #                 'log_return1_realized_volatility_200', 'log_return2_realized_volatility_200',\n    # #                 'log_return1_realized_volatility_100', 'log_return2_realized_volatility_100', \n    #                 'trade_log_return_realized_volatility',\n    #                 'trade_log_return_realized_volatility_600', \n    #                 'trade_log_return_realized_volatility_400',\n    # #                 'trade_log_return_realized_volatility_300',\n    # #                 'trade_log_return_realized_volatility_100',\n    #                 'trade_log_return_realized_volatility_200']\n\n    # Group by the stock id\n    df_stock_id = df.groupby(['stock_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n    # Rename columns joining suffix\n    df_stock_id.columns = ['_'.join(col) for col in df_stock_id.columns]\n    df_stock_id = df_stock_id.add_suffix('_' + 'stock')\n\n    # Group by the stock id\n    df_time_id = df.groupby(['time_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n    # Rename columns joining suffix\n    df_time_id.columns = ['_'.join(col) for col in df_time_id.columns]\n    df_time_id = df_time_id.add_suffix('_' + 'time')\n    \n    # Merge with original dataframe\n    df = df.merge(df_stock_id, how = 'left', left_on = ['stock_id'], right_on = ['stock_id__stock'])\n    df = df.merge(df_time_id, how = 'left', left_on = ['time_id'], right_on = ['time_id__time'])\n    df.drop(['stock_id__stock', 'time_id__time'], axis = 1, inplace = True)\n    return df\n    \n# Funtion to make preprocessing function in parallel (for each stock id)\ndef preprocessor(list_stock_ids, is_train = True):\n    \n    # Parrallel for loop\n    def for_joblib(stock_id):\n        # Train\n        if is_train:\n            file_path_book = data_dir + \"book_train.parquet/stock_id=\" + str(stock_id)\n            file_path_trade = data_dir + \"trade_train.parquet/stock_id=\" + str(stock_id)\n        # Test\n        else:\n            file_path_book = data_dir + \"book_test.parquet/stock_id=\" + str(stock_id)\n            file_path_trade = data_dir + \"trade_test.parquet/stock_id=\" + str(stock_id)\n    \n        # Preprocess book and trade data and merge them\n        df_tmp = pd.merge(book_preprocessor(file_path_book), trade_preprocessor(file_path_trade), on = 'row_id', how = 'left')\n        \n        # Return the merge dataframe\n        return df_tmp\n    \n    # Use parallel api to call paralle for loop\n    df = Parallel(n_jobs = -1, verbose = 1)(delayed(for_joblib)(stock_id) for stock_id in list_stock_ids)\n    \n    # Concatenate all the dataframes that return from Parallel\n    df = pd.concat(df, ignore_index = True)\n    return df\n\n# Function to calculate the root mean squared percentage error\ndef rmspe(y_true, y_pred):\n    return np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\n\n# Function to early stop with root mean squared percentage error\n# TODO: Rreview....\ndef feval_rmspe(y_pred, lgb_train):\n    y_true = lgb_train.get_label()\n    return 'RMSPE', rmspe(y_true, y_pred), False","metadata":{"execution":{"iopub.status.busy":"2021-08-27T12:55:19.733407Z","iopub.execute_input":"2021-08-27T12:55:19.733787Z","iopub.status.idle":"2021-08-27T12:55:19.748714Z","shell.execute_reply.started":"2021-08-27T12:55:19.733747Z","shell.execute_reply":"2021-08-27T12:55:19.747896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Funtion to make preprocessing function in parallel (for each stock id)\ndef prepreprocessor(list_stock_ids, is_train = True):\n    \n    # Parrallel for loop\n    def for_joblib(stock_id):\n        # Train\n        if is_train:\n            file_path_book = data_dir + \"book_train.parquet/stock_id=\" + str(stock_id)\n            file_path_trade = data_dir + \"trade_train.parquet/stock_id=\" + str(stock_id)\n        # Test\n        else:\n            file_path_book = data_dir + \"book_test.parquet/stock_id=\" + str(stock_id)\n            file_path_trade = data_dir + \"trade_test.parquet/stock_id=\" + str(stock_id)\n    \n        # Preprocess book and trade data and merge them\n        df_tmp = pd.merge(book_preprocessor(file_path_book), trade_preprocessor(file_path_trade), on = 'row_id', how = 'left')\n        \n        # Return the merge dataframe\n        return df_tmp\n    \n    # Use parallel api to call paralle for loop\n    ##df = Parallel(n_jobs = -1, verbose = 1)(delayed(for_joblib)(stock_id) for stock_id in list_stock_ids)\n    \n    for stock_id in list_stock_ids:\n        print('stock_id: {}'.format(stock_id))\n        d1 = for_joblib(stock_id)\n        # Concatenate all the dataframes that return from Parallel\n        ##df = pd.concat(d1, ignore_index = True)\n        if stock_id==0:\n            df = d1\n        else:\n            df.append(d1, ignore_index = True)\n        \n        ##if stock_id==0: break\n            \n    return df","metadata":{"execution":{"iopub.status.busy":"2021-08-27T13:18:52.952323Z","iopub.execute_input":"2021-08-27T13:18:52.952649Z","iopub.status.idle":"2021-08-27T13:18:52.960189Z","shell.execute_reply.started":"2021-08-27T13:18:52.952619Z","shell.execute_reply":"2021-08-27T13:18:52.959111Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train and Test","metadata":{}},{"cell_type":"code","source":"def train_test_readisness(train_filename, test_filename):\n    # Read train and test\n    print ('Reading files ...')\n    train, test = read_train_test(train_filename, test_filename )\n\n    print ('Train Preprocessing ...')\n\n    # Get unique stock ids \n    train_stock_ids = train['stock_id'].unique()\n\n    # Preprocess them using Parallel and our single stock id functions\n    ##train_ = preprocessor(train_stock_ids, is_train = True)\n    train_ = prepreprocessor(train_stock_ids, is_train = True)\n    train = train.merge(train_, on = ['row_id'], how = 'left')\n\n    print ('Test Preprocessing ...')\n    # Get unique stock ids \n    test_stock_ids = test['stock_id'].unique()\n\n    # Preprocess them using Parallel and our single stock id functions\n    ##test_ = preprocessor(test_stock_ids, is_train = False)\n    test_ = prepreprocessor(test_stock_ids, is_train = False)\n    test = test.merge(test_, on = ['row_id'], how = 'left')\n\n    print ('Grouping stats ...')\n    # Get group stats of time_id and stock_id\n    train = get_time_stock(train)\n    test = get_time_stock(test)\n\n    print('Pre-Procesor done.')\n    \n    # replace by order sum (tau)\n    train['size_tau'] = np.sqrt( 1/ train['trade_seconds_in_bucket_count_unique'] )\n    test['size_tau'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique'] )\n    train['size_tau_400'] = np.sqrt( 1/ train['trade_seconds_in_bucket_count_unique_400'] )\n    test['size_tau_400'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique_400'] )\n    train['size_tau_300'] = np.sqrt( 1/ train['trade_seconds_in_bucket_count_unique_300'] )\n    test['size_tau_300'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique_300'] )\n    train['size_tau_200'] = np.sqrt( 1/ train['trade_seconds_in_bucket_count_unique_200'] )\n    test['size_tau_200'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique_200'] )\n    \n    # tau2 \n    train['size_tau2'] = np.sqrt( 1/ train['trade_order_count_sum'] )\n    test['size_tau2'] = np.sqrt( 1/ test['trade_order_count_sum'] )\n    train['size_tau2_400'] = np.sqrt( 0.25/ train['trade_order_count_sum'] )\n    test['size_tau2_400'] = np.sqrt( 0.25/ test['trade_order_count_sum'] )\n    train['size_tau2_300'] = np.sqrt( 0.5/ train['trade_order_count_sum'] )\n    test['size_tau2_300'] = np.sqrt( 0.5/ test['trade_order_count_sum'] )\n    train['size_tau2_200'] = np.sqrt( 0.75/ train['trade_order_count_sum'] )\n    test['size_tau2_200'] = np.sqrt( 0.75/ test['trade_order_count_sum'] )\n\n    # delta tau\n    train['size_tau2_d'] = train['size_tau2_400'] - train['size_tau2']\n    test['size_tau2_d'] = test['size_tau2_400'] - test['size_tau2']\n    \n    print('Function: train_test_readisness done.')\n    \n    return train, test","metadata":{"execution":{"iopub.status.busy":"2021-08-27T12:55:19.76392Z","iopub.execute_input":"2021-08-27T12:55:19.764353Z","iopub.status.idle":"2021-08-27T12:55:19.779811Z","shell.execute_reply.started":"2021-08-27T12:55:19.764322Z","shell.execute_reply":"2021-08-27T12:55:19.779033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training model and making predictions","metadata":{}},{"cell_type":"code","source":"def kfold_knn():\n    # kfold based on the knn++ algorithm\n\n    out_train = pd.read_csv('../input/optiver-realized-volatility-prediction/train.csv')\n    out_train = out_train.pivot(index='time_id', columns='stock_id', values='target')\n\n    #out_train[out_train.isna().any(axis=1)]\n    out_train = out_train.fillna(out_train.mean())\n    out_train.head()\n\n    # code to add the just the read data after first execution\n\n    # data separation based on knn ++\n    nfolds = 5 # number of folds\n    index = []\n    totDist = []\n    values = []\n    # generates a matriz with the values of \n    mat = out_train.values\n\n    scaler = MinMaxScaler(feature_range=(-1, 1))\n    mat = scaler.fit_transform(mat)\n\n    nind = int(mat.shape[0]/nfolds) # number of individuals\n\n    # adds index in the last column\n    mat = np.c_[mat,np.arange(mat.shape[0])]\n\n    lineNumber = np.random.choice(np.array(mat.shape[0]), size=nfolds, replace=False)\n    lineNumber = np.sort(lineNumber)[::-1]\n\n    for n in range(nfolds):\n        totDist.append(np.zeros(mat.shape[0]-nfolds))\n\n    # saves index\n    for n in range(nfolds):\n        values.append([lineNumber[n]])    \n\n\n    s=[]\n    for n in range(nfolds):\n        s.append(mat[lineNumber[n],:])\n        mat = np.delete(mat, obj=lineNumber[n], axis=0)\n\n    for n in range(nind-1):    \n        luck = np.random.uniform(0,1,nfolds)\n\n        for cycle in range(nfolds):\n             # saves the values of index           \n\n            s[cycle] = np.matlib.repmat(s[cycle], mat.shape[0], 1)\n\n            sumDist = np.sum( (mat[:,:-1] - s[cycle][:,:-1])**2 , axis=1)   \n            totDist[cycle] += sumDist        \n\n            # probabilities\n            f = totDist[cycle]/np.sum(totDist[cycle]) # normalizing the totdist\n            j = 0\n            kn = 0\n            for val in f:\n                j += val        \n                if (j > luck[cycle]): # the column was selected\n                    break\n                kn +=1\n            lineNumber[cycle] = kn\n\n            # delete line of the value added    \n            for n_iter in range(nfolds):\n\n                totDist[n_iter] = np.delete(totDist[n_iter],obj=lineNumber[cycle], axis=0)\n                j= 0\n\n            s[cycle] = mat[lineNumber[cycle],:]\n            values[cycle].append(int(mat[lineNumber[cycle],-1]))\n            mat = np.delete(mat, obj=lineNumber[cycle], axis=0)\n\n\n    for n_mod in range(nfolds):\n        values[n_mod] = out_train.index[values[n_mod]]\n\n    \n    return values","metadata":{"execution":{"iopub.status.busy":"2021-08-27T12:55:19.78194Z","iopub.execute_input":"2021-08-27T12:55:19.782612Z","iopub.status.idle":"2021-08-27T12:55:19.799192Z","shell.execute_reply.started":"2021-08-27T12:55:19.782576Z","shell.execute_reply":"2021-08-27T12:55:19.798402Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def nnnn(train, test):\n    \n    colNames = list(train)\n\n    colNames.remove('time_id')\n    colNames.remove('target')\n    colNames.remove('row_id')\n    colNames.remove('stock_id')\n\n    train.replace([np.inf, -np.inf], np.nan,inplace=True)\n    test.replace([np.inf, -np.inf], np.nan,inplace=True)\n    qt_train = []\n\n    for col in colNames:\n        #print(col)\n        qt = QuantileTransformer(random_state=21,n_quantiles=2000, output_distribution='normal')\n        train[col] = qt.fit_transform(train[[col]])\n        test[col] = qt.transform(test[[col]])    \n        qt_train.append(qt)\n    \n    # making agg features\n    train_p = pd.read_csv('../input/optiver-realized-volatility-prediction/train.csv')\n    train_p = train_p.pivot(index='time_id', columns='stock_id', values='target')\n\n    corr = train_p.corr()\n\n    ids = corr.index\n\n    kmeans = KMeans(n_clusters=7, random_state=0).fit(corr.values)\n    print('------------------------------AAAAAAAAAAAAAA--------------------------')\n    print(kmeans.labels_)\n    print('------------------------------AAAAAAAAAAAAAA--------------------------')\n\n    l = []\n    for n in range(7):\n        l.append ( [ (x-1) for x in ( (ids+1)*(kmeans.labels_ == n)) if x > 0] )\n\n    mat = []\n    matTest = []\n\n    n = 0\n    for ind in l:\n        print(ind)\n        newDf = train.loc[train['stock_id'].isin(ind) ]\n        newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n        newDf.loc[:,'stock_id'] = str(n)+'c1'\n        mat.append ( newDf )\n\n        newDf = test.loc[test['stock_id'].isin(ind) ]    \n        newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n        newDf.loc[:,'stock_id'] = str(n)+'c1'\n        matTest.append ( newDf )\n\n        n+=1\n\n    mat1 = pd.concat(mat).reset_index()\n    mat1.drop(columns=['target'],inplace=True)\n\n    mat2 = pd.concat(matTest).reset_index()\n    \n    \n    \n    matTest = []\n    mat = []\n    kmeans = []\n\n    mat2 = pd.concat([mat2,mat1.loc[mat1.time_id==5]])\n\n    mat1 = mat1.pivot(index='time_id', columns='stock_id')\n    mat1.columns = [\"_\".join(x) for x in mat1.columns.ravel()]\n    mat1.reset_index(inplace=True)\n\n    mat2 = mat2.pivot(index='time_id', columns='stock_id')\n    mat2.columns = [\"_\".join(x) for x in mat2.columns.ravel()]\n    mat2.reset_index(inplace=True)\n    \n    nnn = ['time_id',\n         'log_return1_realized_volatility_0c1',\n         'log_return1_realized_volatility_1c1',     \n         'log_return1_realized_volatility_3c1',\n         'log_return1_realized_volatility_4c1',     \n         'log_return1_realized_volatility_6c1',\n         'total_volume_mean_0c1',\n         'total_volume_mean_1c1', \n         'total_volume_mean_3c1',\n         'total_volume_mean_4c1', \n         'total_volume_mean_6c1',\n         'trade_size_mean_0c1',\n         'trade_size_mean_1c1', \n         'trade_size_mean_3c1',\n         'trade_size_mean_4c1', \n         'trade_size_mean_6c1',\n         'trade_order_count_mean_0c1',\n         'trade_order_count_mean_1c1',\n         'trade_order_count_mean_3c1',\n         'trade_order_count_mean_4c1',\n         'trade_order_count_mean_6c1',      \n         'price_spread_mean_0c1',\n         'price_spread_mean_1c1',\n         'price_spread_mean_3c1',\n         'price_spread_mean_4c1',\n         'price_spread_mean_6c1',   \n         'bid_spread_mean_0c1',\n         'bid_spread_mean_1c1',\n         'bid_spread_mean_3c1',\n         'bid_spread_mean_4c1',\n         'bid_spread_mean_6c1',       \n         'ask_spread_mean_0c1',\n         'ask_spread_mean_1c1',\n         'ask_spread_mean_3c1',\n         'ask_spread_mean_4c1',\n         'ask_spread_mean_6c1',   \n         'volume_imbalance_mean_0c1',\n         'volume_imbalance_mean_1c1',\n         'volume_imbalance_mean_3c1',\n         'volume_imbalance_mean_4c1',\n         'volume_imbalance_mean_6c1',       \n         'bid_ask_spread_mean_0c1',\n         'bid_ask_spread_mean_1c1',\n         'bid_ask_spread_mean_3c1',\n         'bid_ask_spread_mean_4c1',\n         'bid_ask_spread_mean_6c1',\n         'size_tau2_0c1',\n         'size_tau2_1c1',\n         'size_tau2_3c1',\n         'size_tau2_4c1',\n         'size_tau2_6c1'] \n\n    train = pd.merge(train,mat1[nnn],how='left',on='time_id')\n    test = pd.merge(test,mat2[nnn],how='left',on='time_id')\n    mat1= []\n    mat2= []\n    \n    return train, test","metadata":{"execution":{"iopub.status.busy":"2021-08-27T12:55:19.800508Z","iopub.execute_input":"2021-08-27T12:55:19.800863Z","iopub.status.idle":"2021-08-27T12:55:19.821586Z","shell.execute_reply.started":"2021-08-27T12:55:19.800827Z","shell.execute_reply":"2021-08-27T12:55:19.820749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def save_datasets(train, test, trained_filename, tested_filename):\n    train.to_csv(trained_filename)\n    test.to_csv(tested_filename)\n\ndef load_datasets(trained_filename, tested_filename):\n    _train = pd.DataFrame()\n    _test = pd.DataFrame()\n    _train.read_csv(trained_filename)\n    _test.pd.read_csv(tested_filename)\n    return _train, _test\n\n#trained_filename = 'mytrained.csv'\n#tested_filename = 'mytested.csv'\n#save_datasets(train, test, trained_filename, tested_filename)","metadata":{"execution":{"iopub.status.busy":"2021-08-27T12:55:19.822807Z","iopub.execute_input":"2021-08-27T12:55:19.823346Z","iopub.status.idle":"2021-08-27T12:55:19.833694Z","shell.execute_reply.started":"2021-08-27T12:55:19.823294Z","shell.execute_reply":"2021-08-27T12:55:19.832887Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Base Model\nembedding, flatenning and concatenating","metadata":{}},{"cell_type":"code","source":"def define_neural_network():\n    \n    # INPUTS\n    stock_id_input = keras.Input(shape=(1,), name='stock_id')\n    num_input = keras.Input(shape=(362,), name='num_data')\n\n    cat_data = train['stock_id']\n    stock_embedding_size = 24\n    hidden_units = (128,64,32)\n\n    # Embedding, flatenning and concatenating\n    stock_embedded = keras.layers.Embedding(max(cat_data)+1, stock_embedding_size, \n                                           input_length=1, name='stock_embedding')(stock_id_input)\n    stock_flattened = keras.layers.Flatten()(stock_embedded)\n    out = keras.layers.Concatenate()([stock_flattened, num_input])\n    \n    # Add one or more hidden layers\n    for n_hidden in hidden_units:\n        out = keras.layers.Dense(n_hidden, activation='swish')(out)\n        out = keras.layers.Dropout(rate=0.02,seed=111)(out)\n        \n    #out = keras.layers.Concatenate()([out, num_input])\n\n    # A single output: our predicted rating\n    out = keras.layers.Dense(1, activation='linear', name='prediction')(out)\n    \n    model = keras.Model(\n        inputs = [stock_id_input, num_input],\n        outputs = out,\n    )\n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2021-08-27T12:55:19.834998Z","iopub.execute_input":"2021-08-27T12:55:19.835458Z","iopub.status.idle":"2021-08-27T12:55:19.84434Z","shell.execute_reply.started":"2021-08-27T12:55:19.835423Z","shell.execute_reply":"2021-08-27T12:55:19.843314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prediction","metadata":{}},{"cell_type":"code","source":"def prediction(train, test):\n\n    model_name = 'NN'\n    pred_name = 'pred_{}'.format(model_name)\n\n    # https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html\n    # K-Folds cross-validator\n    # Provides train/test indices to split data in train/test sets. Split dataset into k consecutive folds.\n    n_folds = 5\n    kf = model_selection.KFold(n_splits=n_folds, shuffle=True, random_state=2020)\n    scores_folds[model_name] = []\n    counter = 1\n\n    features_to_consider = list(train)\n    features_to_consider.remove('time_id')\n    features_to_consider.remove('target')\n    features_to_consider.remove('row_id')\n    try:\n        features_to_consider.remove('pred_NN')\n    except:\n        pass\n\n    # Fill na values with the mean in train & test\n    train[features_to_consider] = train[features_to_consider].fillna(train[features_to_consider].mean())\n    test[features_to_consider] = test[features_to_consider].fillna(train[features_to_consider].mean())\n\n    train[pred_name] = 0\n    test['target'] = 0\n\n    # looping folds\n    for n_count in range(n_folds):\n        print('CV {}/{}'.format(counter, n_folds))\n\n        ##indexes = np.arange(nfolds).astype(int)\n        indexes = np.arange(n_folds).astype(int)    \n        indexes = np.delete(indexes,obj=n_count, axis=0) \n\n        indexes = np.r_[values[indexes[0]],values[indexes[1]],values[indexes[2]],values[indexes[3]]]\n\n        X_train = train.loc[train.time_id.isin(indexes), features_to_consider]\n        y_train = train.loc[train.time_id.isin(indexes), target_name]\n        X_test = train.loc[train.time_id.isin(values[n_count]), features_to_consider]\n        y_test = train.loc[train.time_id.isin(values[n_count]), target_name]\n\n        # NN   \n        model = define_neural_network()\n\n        es = tf.keras.callbacks.EarlyStopping(\n                monitor='val_loss', patience=20, verbose=0,\n                mode='min',restore_best_weights=True)\n\n        plateau = tf.keras.callbacks.ReduceLROnPlateau(\n                monitor='val_loss', factor=0.2, patience=7, verbose=0,\n                mode='min')\n\n        print('Compile Model')\n        model.compile(\n            keras.optimizers.Adam(learning_rate=learning_rate),\n            loss=root_mean_squared_per_error\n        )\n\n        try:\n            features_to_consider.remove('stock_id')\n        except:\n            pass\n\n        num_data = X_train[features_to_consider]\n\n        scaler = MinMaxScaler(feature_range=(-1, 1))         \n        num_data = scaler.fit_transform(num_data.values)    \n\n        cat_data = X_train['stock_id']    \n        target =  y_train\n\n        num_data_test = X_test[features_to_consider]\n        num_data_test = scaler.transform(num_data_test.values)\n        cat_data_test = X_test['stock_id']\n\n        print('Fit Model')\n        model.fit([cat_data, num_data], \n                  target,               \n                  batch_size=2048,\n                  epochs=num_epochs,\n                  validation_data=([cat_data_test, num_data_test], y_test),\n                  callbacks=[es, plateau],\n                  validation_batch_size=len(y_test),\n                  shuffle=True,\n                  verbose = 1)\n\n        print('Predict Model')\n        preds = model.predict([cat_data_test, num_data_test]).reshape(1,-1)[0]\n\n        score = round(rmspe(y_true = y_test, y_pred = preds),5)\n        print('Fold {} {}: {}'.format(counter, model_name, score))\n        scores_folds[model_name].append(score)\n\n        tt =scaler.transform(test[features_to_consider].values)\n        test[target_name] += model.predict([test['stock_id'], tt]).reshape(1,-1)[0].clip(0,1e10)\n        #test[target_name] += model.predict([test['stock_id'], test[features_to_consider]]).reshape(1,-1)[0].clip(0,1e10)\n\n        counter += 1\n        features_to_consider.append('stock_id')","metadata":{"execution":{"iopub.status.busy":"2021-08-27T12:55:19.845801Z","iopub.execute_input":"2021-08-27T12:55:19.84626Z","iopub.status.idle":"2021-08-27T12:55:19.864995Z","shell.execute_reply.started":"2021-08-27T12:55:19.846227Z","shell.execute_reply":"2021-08-27T12:55:19.864193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_model(train, test, model):\n\n    model_name = 'NN'\n    pred_name = 'pred_{}'.format(model_name)\n\n    features_to_consider = list(train)\n    features_to_consider.remove('time_id')\n    features_to_consider.remove('target')\n    features_to_consider.remove('row_id')\n    try:\n        features_to_consider.remove('pred_NN')   ## No existe!!!!\n    except:\n        pass\n\n    # Fill na values with the mean in train & test\n    train[features_to_consider] = train[features_to_consider].fillna(train[features_to_consider].mean())\n    test[features_to_consider] = test[features_to_consider].fillna(train[features_to_consider].mean())\n\n    train[pred_name] = 0\n    test['target'] = 0\n\n    ##X_train = train.loc[train.time_id.isin(indexes), features_to_consider]\n    ##y_train = train.loc[train.time_id.isin(indexes), target_name]\n    ##X_test = train.loc[train.time_id.isin(values[n_count]), features_to_consider]\n    ##y_test = train.loc[train.time_id.isin(values[n_count]), target_name]\n\n    X_train = train.loc[train, features_to_consider]\n    y_train = train.loc[train, target_name]\n    X_test = train.loc[train, features_to_consider]\n    y_test = train.loc[train, target_name]    \n    \n\n    monitor = tf.keras.callbacks.EarlyStopping(\n          monitor='val_loss', patience=20, verbose=0,\n          mode='min',restore_best_weights=True)\n\n    plateau = tf.keras.callbacks.ReduceLROnPlateau(\n                monitor='val_loss', factor=0.2, patience=7, verbose=0,\n                mode='min')\n\n    print('Compile Model')\n    model.compile(\n            keras.optimizers.Adam(learning_rate=learning_rate),\n            loss=root_mean_squared_per_error\n        )\n\n    try:\n        features_to_consider.remove('stock_id')\n    except:\n        pass\n\n    num_data = X_train[features_to_consider]\n\n    scaler = MinMaxScaler(feature_range=(-1, 1))         \n    num_data = scaler.fit_transform(num_data.values)    \n\n    cat_data = X_train['stock_id']    \n    target =  y_train\n\n    num_data_test = X_test[features_to_consider]\n    num_data_test = scaler.transform(num_data_test.values)\n    cat_data_test = X_test['stock_id']\n\n    print('Fit Model')\n    model.fit([cat_data, num_data], \n                  target,               \n                  batch_size=2048,\n                  epochs=num_epochs,\n                  validation_data=([cat_data_test, num_data_test], y_test),\n                  callbacks=[monitor, plateau],\n                  validation_batch_size=len(y_test),\n                  shuffle=True,\n                  verbose = 1)\n\n\ndef predict_model(model, x, y_test, model_name):\n    print('Predict Model')\n    ##preds = model.predict([cat_data_test, num_data_test]).reshape(1,-1)[0]\n    preds = model.predict(x).reshape(1,-1)[0]\n\n    score = round(rmspe(y_true = y_test, y_pred = preds),5)\n    print('Score {}: {}'.format(model_name, score))\n\n    tt =scaler.transform(test[features_to_consider].values)\n    test[target_name] += model.predict([test['stock_id'], tt]).reshape(1,-1)[0].clip(0,1e10)\n","metadata":{"execution":{"iopub.status.busy":"2021-08-27T12:55:19.866165Z","iopub.execute_input":"2021-08-27T12:55:19.866539Z","iopub.status.idle":"2021-08-27T12:55:19.883803Z","shell.execute_reply.started":"2021-08-27T12:55:19.866507Z","shell.execute_reply":"2021-08-27T12:55:19.882973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"def submission(train, test):\n    test[target_name] = test[target_name]/n_folds\n\n    score = round(rmspe(y_true = train[target_name].values, y_pred = train[pred_name].values),5)\n    print('RMSPE {}: {} - Folds: {}'.format(model_name, score, scores_folds[model_name]))\n\n    display(test[['row_id', target_name]].head(2))\n    test[['row_id', target_name]].to_csv('submission.csv',index = False)\n    ","metadata":{"execution":{"iopub.status.busy":"2021-08-27T12:55:19.886774Z","iopub.execute_input":"2021-08-27T12:55:19.887225Z","iopub.status.idle":"2021-08-27T12:55:19.893522Z","shell.execute_reply.started":"2021-08-27T12:55:19.887178Z","shell.execute_reply":"2021-08-27T12:55:19.892765Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def pre_submission(train, test):\n    #test[target_name] = test[target_name]/n_folds\n\n    score = round(rmspe(y_true = train[target_name].values, y_pred = train[pred_name].values),5)\n    print('RMSPE {}: {} - Folds: {}'.format(model_name, score, scores_folds[model_name]))\n\n    display(test[['row_id', target_name]].head(2))\n    test[['row_id', target_name]].to_csv('submission.csv',index = False)","metadata":{"execution":{"iopub.status.busy":"2021-08-27T12:55:19.894682Z","iopub.execute_input":"2021-08-27T12:55:19.89524Z","iopub.status.idle":"2021-08-27T12:55:19.901495Z","shell.execute_reply.started":"2021-08-27T12:55:19.895179Z","shell.execute_reply":"2021-08-27T12:55:19.900581Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Main","metadata":{}},{"cell_type":"code","source":"%%time\n##############################################################\ntrain, test = train_test_readisness(train_filename, test_filename)\n#values = kfold_knn()\n#train, test = nnnn(train, test)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-08-27T13:19:10.287653Z","iopub.execute_input":"2021-08-27T13:19:10.287978Z","iopub.status.idle":"2021-08-27T14:13:58.869113Z","shell.execute_reply.started":"2021-08-27T13:19:10.287949Z","shell.execute_reply":"2021-08-27T14:13:58.867991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n  #  train, test = read_train_test(train_filename, test_filename )\n\n#train","metadata":{"execution":{"iopub.status.busy":"2021-08-27T12:55:19.910386Z","iopub.execute_input":"2021-08-27T12:55:19.910693Z","iopub.status.idle":"2021-08-27T12:55:19.917244Z","shell.execute_reply.started":"2021-08-27T12:55:19.910638Z","shell.execute_reply":"2021-08-27T12:55:19.916557Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#test","metadata":{"execution":{"iopub.status.busy":"2021-08-27T12:55:19.918352Z","iopub.execute_input":"2021-08-27T12:55:19.918685Z","iopub.status.idle":"2021-08-27T12:55:19.925385Z","shell.execute_reply.started":"2021-08-27T12:55:19.91865Z","shell.execute_reply":"2021-08-27T12:55:19.9246Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#model = define_neural_network()\n\n#predict_model(train, test, model)\n\n#model.fit([cat_data, num_data], \n#                  target,               \n#                  batch_size=2048,\n#                  epochs=num_epochs,\n#                  validation_data=([cat_data_test, num_data_test], y_test),\n#                  callbacks=[es, plateau],\n#                  validation_batch_size=len(y_test),\n#                  shuffle=True,\n#                  verbose = 1)\n    \n    \n","metadata":{"execution":{"iopub.status.busy":"2021-08-27T12:55:19.926453Z","iopub.execute_input":"2021-08-27T12:55:19.926708Z","iopub.status.idle":"2021-08-27T12:55:19.932988Z","shell.execute_reply.started":"2021-08-27T12:55:19.926685Z","shell.execute_reply":"2021-08-27T12:55:19.932175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#s = submission(train, test)\n#print(np.mean(s[model_name]))","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-08-27T12:55:19.934273Z","iopub.execute_input":"2021-08-27T12:55:19.934739Z","iopub.status.idle":"2021-08-27T12:55:19.938871Z","shell.execute_reply.started":"2021-08-27T12:55:19.934704Z","shell.execute_reply":"2021-08-27T12:55:19.937869Z"},"trusted":true},"execution_count":null,"outputs":[]}]}