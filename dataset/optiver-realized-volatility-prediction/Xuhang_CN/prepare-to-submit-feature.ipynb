{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from IPython.core.display import display, HTML\n\nimport pandas as pd\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport glob\nimport os\nimport gc\n\nfrom joblib import Parallel, delayed\n\nfrom sklearn import preprocessing, model_selection\nfrom sklearn.preprocessing import MinMaxScaler,StandardScaler\nfrom sklearn.preprocessing import QuantileTransformer\nfrom sklearn.metrics import r2_score\n\nimport matplotlib.pyplot as plt \nimport seaborn as sns\nimport numpy.matlib\n\nimport scipy as sc\nfrom sklearn.metrics import make_scorer\nfrom sklearn import ensemble\nimport torch\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nimport torch.nn.functional as F\nimport torch.nn as nn\nimport random\nimport warnings\n\n\nfrom sklearn.cluster import KMeans\n!pip install ../input/my-pytorch-tabnet/pytorch_tabnet-3.1.1-py3-none-any.whl\nfrom pytorch_tabnet.tab_model import TabNetRegressor\nfrom sklearn.metrics import make_scorer\nimport torch\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau,CosineAnnealingWarmRestarts\nimport torch.nn.functional as F\nimport torch.nn as nn\nfrom sklearn.preprocessing import LabelEncoder \nimport pickle\nfrom pytorch_tabnet.metrics import Metric\nwarnings.filterwarnings('ignore')\npd.set_option('max_columns', 300)\n# Input data files are available in the read-\n\npath_submissions = '/'\n\ntarget_name = 'target'\nscores_folds = {}\nSEED = 2021\n\ndef random_seed(SEED):\n    \n    random.seed(SEED)\n    os.environ['PYTHONHASHSEED'] = str(SEED)\n    np.random.seed(SEED)\n    torch.manual_seed(SEED)\n    \n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(SEED)\n        torch.cuda.manual_seed_all(SEED)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n\nrandom_seed(SEED)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# data directory\ndata_dir = '../input/optiver-realized-volatility-prediction/'\n\n# Function to calculate first WAP\ndef calc_wap1(df):\n    wap = (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']) / (df['bid_size1'] + df['ask_size1'])\n    return wap\n\n# Function to calculate second WAP\ndef calc_wap2(df):\n    wap = (df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2']) / (df['bid_size2'] + df['ask_size2'])\n    return wap\n\ndef calc_wap3(df):\n    wap = (df['bid_price1'] * df['bid_size1'] + df['ask_price1'] * df['ask_size1']) / (df['bid_size1'] + df['ask_size1'])\n    return wap\n\ndef calc_wap4(df):\n    wap = (df['bid_price2'] * df['bid_size2'] + df['ask_price2'] * df['ask_size2']) / (df['bid_size2'] + df['ask_size2'])\n    return wap\n\n# Function to calculate the log of the return\n# Remember that logb(x / y) = logb(x) - logb(y)\ndef log_return(series):\n    return np.log(series).diff()\n\n# Calculate the realized volatility\ndef realized_volatility(series):\n    return np.sqrt(np.sum(series**2))\n\n# Function to count unique elements of a series\ndef count_unique(series):\n    return len(np.unique(series))\n\n# Function to read our base train and test set\ndef read_train_test():\n    train = pd.read_csv('../input/optiver-realized-volatility-prediction/train.csv')\n    test = pd.read_csv('../input/optiver-realized-volatility-prediction/test.csv')\n    # Create a key to merge with book and trade data\n    train['row_id'] = train['stock_id'].astype(str) + '-' + train['time_id'].astype(str)\n    test['row_id'] = test['stock_id'].astype(str) + '-' + test['time_id'].astype(str)\n    print(f'Our training set has {train.shape[0]} rows')\n    return train, test\n\n# Function to preprocess book data (for each stock id)\ndef book_preprocessor(file_path):\n    df = pd.read_parquet(file_path)\n    # Calculate Wap\n    df['wap1'] = calc_wap1(df)\n    df['wap2'] = calc_wap2(df)\n    df['wap3'] = calc_wap3(df)\n    df['wap4'] = calc_wap4(df)\n    # Calculate log returns\n    df['log_return1'] = df.groupby(['time_id'])['wap1'].apply(log_return)\n    df['log_return2'] = df.groupby(['time_id'])['wap2'].apply(log_return)\n    df['log_return3'] = df.groupby(['time_id'])['wap3'].apply(log_return)\n    df['log_return4'] = df.groupby(['time_id'])['wap4'].apply(log_return)\n    # Calculate wap balance\n    df['wap_balance'] = abs(df['wap1'] - df['wap2'])\n    # Calculate spread\n    df['price_spread'] = (df['ask_price1'] - df['bid_price1']) / ((df['ask_price1'] + df['bid_price1']) / 2)\n    df['price_spread2'] = (df['ask_price2'] - df['bid_price2']) / ((df['ask_price2'] + df['bid_price2']) / 2)\n    df['bid_spread'] = df['bid_price1'] - df['bid_price2']\n    df['ask_spread'] = df['ask_price1'] - df['ask_price2']\n    df[\"bid_ask_spread\"] = abs(df['bid_spread'] - df['ask_spread'])\n    df['total_volume'] = (df['ask_size1'] + df['ask_size2']) + (df['bid_size1'] + df['bid_size2'])\n    df['volume_imbalance'] = abs((df['ask_size1'] + df['ask_size2']) - (df['bid_size1'] + df['bid_size2']))\n    df['mean_price'] = (df['bid_price1']+df['ask_price1'])/2\n    \n    \n    df[\"amt_ab\"] = df[\"bid_price1\"]*df[\"ask_size1\"]+df[\"ask_price1\"]*df[\"bid_size1\"]\n    \n    ordersize50 = pd.DataFrame({\"ordersize50\":df.groupby([\"time_id\"])[\"amt_ab\"].apply(lambda x:np.nanmedian(x))})\n    ordersize25 = pd.DataFrame({\"ordersize25\":df.groupby([\"time_id\"])[\"amt_ab\"].apply(lambda x:np.nanpercentile(x,75))})\n    ordersize75= pd.DataFrame({\"ordersize75\":df.groupby([\"time_id\"])[\"amt_ab\"].apply(lambda x:np.nanpercentile(x,25))})\n    ordersize50.reset_index(inplace=True)\n    ordersize25.reset_index(inplace=True)\n    ordersize75.reset_index(inplace=True)\n    book_df1 = pd.merge(df, ordersize50, on=\"time_id\", how=\"left\")\n    book_df1 = pd.merge(book_df1, ordersize25, on=\"time_id\", how=\"left\")\n    book_df1 = pd.merge(book_df1, ordersize75, on=\"time_id\", how=\"left\")\n    book_df1.loc[:, \"isoversize50\"] = np.where(\n        book_df1[\"amt_ab\"] > book_df1[\"ordersize50\"], \"up50\",\n        np.where(book_df1[\"amt_ab\"] <= book_df1[\"ordersize50\"], \"down50\",\n                 np.nan))\n    book_df1.loc[:, \"isoversize75\"] = np.where(\n        book_df1[\"amt_ab\"] > book_df1[\"ordersize75\"], \"up75\",\n        np.where(book_df1[\"amt_ab\"] <= book_df1[\"ordersize75\"], \"down75\",\n                 np.nan))\n    book_df1.loc[:, \"isoversize25\"] = np.where(\n        book_df1[\"amt_ab\"] > book_df1[\"ordersize25\"], \"up25\",\n        np.where(book_df1[\"amt_ab\"] <= book_df1[\"ordersize25\"], \"down25\",\n                 np.nan))\n    vol = book_df1.groupby('time_id')['wap1'].apply(\n        lambda x: np.sqrt(np.sum(np.log(x).diff()**2)))\n    vol_df = pd.DataFrame(vol)\n    vol_df.rename(columns={'wap1': 'vol_orig'}, inplace=True)\n    data_merge_all = vol_df\n    flagname = \"B\"\n    filtername = \"isBS\"\n    for filtername, flagname in [[\"isoversize50\", \"up50\"],\n                                 [\"isoversize50\", \"down50\"],\n                                 [\"isoversize25\", \"up25\"],\n                                 [\"isoversize25\", \"down25\"],\n                                 [\"isoversize75\", \"up75\"],\n                                 [\"isoversize75\", \"down75\"]]:\n        #print(filtername, flagname)\n        book_df_new = book_df1[book_df1[filtername] == flagname]\n        #个数\n        df_fnum = pd.DataFrame({\n            flagname + \"num\":\n            book_df_new.groupby(\"time_id\")[\"seconds_in_bucket\"].count()\n        })\n        data_merge_all = pd.merge(data_merge_all,\n                                  df_fnum,\n                                  left_index=True,\n                                  right_index=True,\n                                  how=\"left\")\n    del data_merge_all[\"vol_orig\"]\n    # Dict for aggregations\n    create_feature_dict = {\n        'wap1': [np.sum, np.std],\n        'wap2': [np.sum, np.std],\n        'wap3': [np.sum, np.std],\n        'wap4': [np.sum, np.std],\n        'log_return1': [realized_volatility],\n        'log_return2': [realized_volatility],\n        'log_return3': [realized_volatility],\n        'log_return4': [realized_volatility],\n        'wap_balance': [np.sum, np.max],\n        'price_spread':[np.sum, np.max],\n        'price_spread2':[np.sum, np.max],\n        'bid_spread':[np.sum, np.max],\n        'ask_spread':[np.sum, np.max],\n        'total_volume':[np.sum, np.max],\n        'volume_imbalance':[np.sum, np.max],\n        \"bid_ask_spread\":[np.sum,  np.max],\n        \"mean_price\":[np.mean],   \n    }\n    create_feature_dict_time = {\n        'log_return1': [realized_volatility],\n        'log_return2': [realized_volatility],\n        'log_return3': [realized_volatility],\n        'log_return4': [realized_volatility],\n    }\n    \n    # Function to get group stats for different windows (seconds in bucket)\n    def get_stats_window(fe_dict,seconds_in_bucket, add_suffix = False):\n        # Group by the window\n        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(fe_dict).reset_index()\n        # Rename columns joining suffix\n        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n        # Add a suffix to differentiate windows\n        if add_suffix:\n            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n        return df_feature\n    \n    # Get the stats for different windows\n    df_feature = get_stats_window(create_feature_dict,seconds_in_bucket = 0, add_suffix = False)\n    df_feature_500 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 500, add_suffix = True)\n    df_feature_400 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 400, add_suffix = True)\n    df_feature_300 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 300, add_suffix = True)\n    df_feature_200 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 200, add_suffix = True)\n    df_feature_100 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 100, add_suffix = True)\n\n    # Merge all\n    df_feature = df_feature.merge(df_feature_500, how = 'left', left_on = 'time_id_', right_on = 'time_id__500')\n    df_feature = df_feature.merge(df_feature_400, how = 'left', left_on = 'time_id_', right_on = 'time_id__400')\n    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n    df_feature = df_feature.merge(df_feature_200, how = 'left', left_on = 'time_id_', right_on = 'time_id__200')\n    df_feature = df_feature.merge(df_feature_100, how = 'left', left_on = 'time_id_', right_on = 'time_id__100')\n    # Drop unnecesary time_ids\n    df_feature.drop(['time_id__500','time_id__400', 'time_id__300', 'time_id__200','time_id__100'], axis = 1, inplace = True)\n\n    df_feature[\"up50num\"] = data_merge_all[\"up50num\"].values\n    df_feature[\"up25num\"] = data_merge_all[\"up25num\"].values\n    df_feature[\"up75num\"] = data_merge_all[\"up75num\"].values\n    df_feature[\"down50num\"] = data_merge_all[\"down50num\"].values\n    df_feature[\"down25num\"] = data_merge_all[\"down25num\"].values\n    df_feature[\"down75num\"] = data_merge_all[\"down75num\"].values\n    df_feature[\"ordersize50\"] = ordersize50[\"ordersize50\"]\n    df_feature[\"ordersize75\"] = ordersize75[\"ordersize75\"]\n    df_feature[\"ordersize25\"] = ordersize25[\"ordersize25\"]\n    \n    # Create row_id so we can merge\n    stock_id = file_path.split('=')[1]\n    df_feature['row_id'] = df_feature['time_id_'].apply(lambda x: f'{stock_id}-{x}')\n    df_feature.drop(['time_id_'], axis = 1, inplace = True)\n    return df_feature\n\n# Function to preprocess trade data (for each stock id)\ndef trade_preprocessor(file_path):\n    df = pd.read_parquet(file_path)\n    df['log_return'] = df.groupby('time_id')['price'].apply(log_return)\n    df['amount']=df['price']*df['size']\n    # Dict for aggregations\n    create_feature_dict = {\n        'log_return':[realized_volatility],\n        'seconds_in_bucket':[count_unique],\n        'size':[np.sum, np.max],\n        'order_count':[np.sum,np.max],\n        'amount':[np.sum,np.max],\n        \"price\":[np.max,np.min]\n    }\n    create_feature_dict_time = {\n        'log_return':[realized_volatility],\n        'seconds_in_bucket':[count_unique],\n        'size':[np.sum],\n        'order_count':[np.sum],\n    }\n    # Function to get group stats for different windows (seconds in bucket)\n    def get_stats_window(fe_dict,seconds_in_bucket, add_suffix = False):\n        # Group by the window\n        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(fe_dict).reset_index()\n        # Rename columns joining suffix\n        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n        # Add a suffix to differentiate windows\n        if add_suffix:\n            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n        return df_feature\n    \n\n    # Get the stats for different windows\n    df_feature = get_stats_window(create_feature_dict,seconds_in_bucket = 0, add_suffix = False)\n    df_feature_500 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 500, add_suffix = True)\n    df_feature_400 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 400, add_suffix = True)\n    df_feature_300 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 300, add_suffix = True)\n    df_feature_200 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 200, add_suffix = True)\n    df_feature_100 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 100, add_suffix = True)\n    df_feature[\"price_open\"] = df.groupby(\"time_id\")[\"price\"].apply(lambda x:list(x)[0]).values\n    df_feature[\"price_close\"] = df.groupby(\"time_id\")[\"price\"].apply(lambda x:list(x)[-1]).values\n    def tendency(price, vol):    \n        df_diff = np.diff(price)\n        val = (df_diff/price[1:])*100\n        power = np.sum(val*vol[1:])\n        return(power)\n    \n    lis = []\n    for n_time_id in df['time_id'].unique():\n        df_id = df[df['time_id'] == n_time_id]        \n        tendencyV = tendency(df_id['price'].values, df_id['size'].values)      \n        f_max = np.sum(df_id['price'].values > np.mean(df_id['price'].values))\n        f_min = np.sum(df_id['price'].values < np.mean(df_id['price'].values))\n        df_max =  np.sum(np.diff(df_id['price'].values) > 0)\n        df_min =  np.sum(np.diff(df_id['price'].values) < 0)\n        # new\n        abs_diff = np.median(np.abs( df_id['price'].values - np.mean(df_id['price'].values)))        \n        energy = np.mean(df_id['price'].values**2)\n        iqr_p = np.percentile(df_id['price'].values,75) - np.percentile(df_id['price'].values,25)\n        \n        # vol vars\n        abs_diff_v = np.median(np.abs( df_id['size'].values - np.mean(df_id['size'].values)))        \n        energy_v = np.sum(df_id['size'].values**2)\n        iqr_p_v = np.percentile(df_id['size'].values,75) - np.percentile(df_id['size'].values,25)\n        \n        lis.append({'time_id':n_time_id,'tendency':tendencyV,'f_max':f_max,'f_min':f_min,'df_max':df_max,'df_min':df_min,\n                   'abs_diff':abs_diff,'energy':energy,'iqr_p':iqr_p,'abs_diff_v':abs_diff_v,'energy_v':energy_v,'iqr_p_v':iqr_p_v})\n    \n    df_lr = pd.DataFrame(lis)\n        \n   \n    df_feature = df_feature.merge(df_lr, how = 'left', left_on = 'time_id_', right_on = 'time_id')\n    \n    # Merge all\n    df_feature = df_feature.merge(df_feature_500, how = 'left', left_on = 'time_id_', right_on = 'time_id__500')\n    df_feature = df_feature.merge(df_feature_400, how = 'left', left_on = 'time_id_', right_on = 'time_id__400')\n    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n    df_feature = df_feature.merge(df_feature_200, how = 'left', left_on = 'time_id_', right_on = 'time_id__200')\n    df_feature = df_feature.merge(df_feature_100, how = 'left', left_on = 'time_id_', right_on = 'time_id__100')\n    # Drop unnecesary time_ids\n    df_feature.drop(['time_id__500','time_id__400', 'time_id__300', 'time_id__200','time_id','time_id__100'], axis = 1, inplace = True)\n    \n    \n    df_feature = df_feature.add_prefix('trade_')\n    stock_id = file_path.split('=')[1]\n    df_feature['row_id'] = df_feature['trade_time_id_'].apply(lambda x:f'{stock_id}-{x}')\n    df_feature.drop(['trade_time_id_'], axis = 1, inplace = True)\n    return df_feature\n\n# Function to get group stats for the stock_id and time_id\ndef get_time_stock(df):\n    vol_cols = ['log_return1_realized_volatility', 'log_return2_realized_volatility', 'log_return1_realized_volatility_400', 'log_return2_realized_volatility_400', \n                'log_return1_realized_volatility_300', 'log_return2_realized_volatility_300', 'log_return1_realized_volatility_200', 'log_return2_realized_volatility_200', \n                'trade_log_return_realized_volatility', 'trade_log_return_realized_volatility_400', 'trade_log_return_realized_volatility_300', 'trade_log_return_realized_volatility_200']\n\n\n    # Group by the stock id\n    df_stock_id = df.groupby(['stock_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n    # Rename columns joining suffix\n    df_stock_id.columns = ['_'.join(col) for col in df_stock_id.columns]\n    df_stock_id = df_stock_id.add_suffix('_' + 'stock')\n\n    # Group by the stock id\n    df_time_id = df.groupby(['time_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n    # Rename columns joining suffix\n    df_time_id.columns = ['_'.join(col) for col in df_time_id.columns]\n    df_time_id = df_time_id.add_suffix('_' + 'time')\n    \n    # Merge with original dataframe\n    df = df.merge(df_stock_id, how = 'left', left_on = ['stock_id'], right_on = ['stock_id__stock'])\n    df = df.merge(df_time_id, how = 'left', left_on = ['time_id'], right_on = ['time_id__time'])\n    df.drop(['stock_id__stock', 'time_id__time'], axis = 1, inplace = True)\n    return df\n    \n# Funtion to make preprocessing function in parallel (for each stock id)\ndef preprocessor(list_stock_ids, is_train = True):\n    \n    # Parrallel for loop\n    def for_joblib(stock_id):\n        # Train\n        if is_train:\n            file_path_book = data_dir + \"book_train.parquet/stock_id=\" + str(stock_id)\n            file_path_trade = data_dir + \"trade_train.parquet/stock_id=\" + str(stock_id)\n        # Test\n        else:\n            file_path_book = data_dir + \"book_test.parquet/stock_id=\" + str(stock_id)\n            file_path_trade = data_dir + \"trade_test.parquet/stock_id=\" + str(stock_id)\n        # Preprocess book and trade data and merge them\n        df_tmp = pd.merge(book_preprocessor(file_path_book), trade_preprocessor(file_path_trade), on = 'row_id', how = 'left')\n        \n        # Return the merge dataframe\n        return df_tmp\n    \n    # Use parallel api to call paralle for loop\n    df = Parallel(n_jobs = -1, verbose = 1)(delayed(for_joblib)(stock_id) for stock_id in list_stock_ids)\n    # Concatenate all the dataframes that return from Parallel\n    df = pd.concat(df, ignore_index = True)\n    return df\n# Function to calculate the root mean squared percentage error\ndef rmspe(y_true, y_pred):\n    return np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\n\n# Function to early stop with root mean squared percentage error\ndef feval_rmspe(y_pred, lgb_train):\n    y_true = lgb_train.get_label()\n    return 'RMSPE', rmspe(y_true, y_pred), False\ndef make_candle(df_data, price_name, vol_name):\n    df_vol = pd.DataFrame(\n        {vol_name + \"sum\": df_data.groupby(\"time_id\")[vol_name].sum()})\n    df_mean = pd.DataFrame(\n        {price_name + \"mean\": df_data.groupby(\"time_id\")[price_name].mean()})\n    df_high = pd.DataFrame(\n        {price_name + \"high\": df_data.groupby(\"time_id\")[price_name].max()})\n    df_low = pd.DataFrame(\n        {price_name + \"low\": df_data.groupby(\"time_id\")[price_name].min()})\n    df_open = df_data.groupby(\"time_id\").head(1)\n    df_open.index = df_open[\"time_id\"]\n    df_open = pd.DataFrame({price_name + \"open\": df_open[price_name]})\n    df_close = df_data.groupby(\"time_id\").tail(1)\n    df_close.index = df_close[\"time_id\"]\n    df_close = pd.DataFrame({price_name + \"close\": df_close[price_name]})\n    \n\n    df_candle = pd.merge(df_high,\n                         df_low,\n                         left_index=True,\n                         right_index=True,\n                         how=\"inner\")\n    df_candle = pd.merge(df_candle,\n                         df_mean,\n                         left_index=True,\n                         right_index=True,\n                         how=\"inner\")\n    df_candle = pd.merge(df_candle,\n                         df_open,\n                         left_index=True,\n                         right_index=True,\n                         how=\"inner\")\n    df_candle = pd.merge(df_candle,\n                         df_close,\n                         left_index=True,\n                         right_index=True,\n                         how=\"inner\")\n    df_candle = pd.merge(df_candle,\n                     df_vol,\n                     left_index=True,\n                     right_index=True,\n                     how=\"inner\")\n    return df_candle\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train, test = read_train_test()\n\n\ntrain_stock_ids = train['stock_id'].unique()\n\ntrain_ = preprocessor(train_stock_ids, is_train = True)\ntrain = train.merge(train_, on = ['row_id'], how = 'left')\n\n\ntest_stock_ids = test['stock_id'].unique()\n# Preprocess them using Parallel and our single stock id functions\ntest_ = preprocessor(test_stock_ids, is_train = False)\ntest = test.merge(test_, on = ['row_id'], how = 'left')\n\n# Get group stats of time_id and stock_id\ntrain = get_time_stock(train)\ntest = get_time_stock(test)\ntrain.to_csv(\"all_train_data.csv\")\ntest.to_csv(\"all_test_data.csv\")","metadata":{},"execution_count":null,"outputs":[]}]}