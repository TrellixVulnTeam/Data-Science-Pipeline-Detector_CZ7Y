{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import r2_score\nimport glob\nimport tqdm as tqdm\nfrom sklearn import linear_model\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom xgboost import XGBRegressor\nfrom sklearn import model_selection\nfrom sklearn.model_selection import KFold\nimport xgboost as xgb","metadata":{"execution":{"iopub.status.busy":"2021-07-13T12:27:58.430912Z","iopub.execute_input":"2021-07-13T12:27:58.431327Z","iopub.status.idle":"2021-07-13T12:27:58.438263Z","shell.execute_reply.started":"2021-07-13T12:27:58.43129Z","shell.execute_reply":"2021-07-13T12:27:58.436832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# For quickly switching between training and test data\ndef train_test(mode):\n    # mode = \"train\"/\"test\"\n    file_name = '../input/optiver-realized-volatility-prediction/' + mode + '.csv'\n    return pd.read_csv(file_name)\n\ndef my_metrics(y_true, y_pred):\n    return np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\n\ndef rmspe(y_true, y_pred):  \n    output = my_metrics(y_true, y_pred)\n    return 'rmspe', output, False\n\ndef is_in(stock_groups, stock_id):\n    for i in range(len(stock_groups)):\n        if stock_id in stock_groups[i]:\n            return i\n    \n# custom aggregate function\ndef wap2vol(df):\n    # wap2vol stands for WAP to Realized Volatility\n    temp = np.log(df).diff() # calculating tik to tik returns\n    # returning realized volatility\n    return np.sqrt(np.sum(temp**2)) \n\ndef rv(series_log_return):\n    return np.sqrt(np.sum(series_log_return**2))\n\ndef log_return(list_stock_prices):\n    return np.log(list_stock_prices).diff() \n\ndef agg(df_book, stock_stat, feature, func, new_name = None, rename = False):\n    if rename:\n        stock_stat = pd.merge( df_book.groupby(by = ['time_id'])[feature].agg(func).reset_index().rename(columns = {feature : new_name}),\n        stock_stat, on = ['time_id'], how = 'left')    \n    else:\n        stock_stat = pd.merge( df_book.groupby(by = ['time_id'])[feature].agg(func).reset_index(),\n            stock_stat, on = ['time_id'], how = 'left')     \n    return stock_stat\n\n# for inference\ndef linear_inference(models, stock_id, stock_groups, volatility_features, degree, grouped = True):\n    if grouped:\n        model = models[is_in(stock_groups, stock_id)]\n    else:\n        model = models[stock_id]\n    polyfeat = PolynomialFeatures(degree = degree)\n    return model.predict(polyfeat.fit_transform([volatility_features]))[0]","metadata":{"execution":{"iopub.status.busy":"2021-07-13T12:27:58.440745Z","iopub.execute_input":"2021-07-13T12:27:58.441195Z","iopub.status.idle":"2021-07-13T12:27:58.4588Z","shell.execute_reply.started":"2021-07-13T12:27:58.44115Z","shell.execute_reply":"2021-07-13T12:27:58.457724Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = train_test(\"train\")\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-13T12:27:58.46082Z","iopub.execute_input":"2021-07-13T12:27:58.461098Z","iopub.status.idle":"2021-07-13T12:27:58.724047Z","shell.execute_reply.started":"2021-07-13T12:27:58.461072Z","shell.execute_reply":"2021-07-13T12:27:58.722699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = pd.read_csv('../input/optiver-realized-volatility-prediction/test.csv')","metadata":{"execution":{"iopub.status.busy":"2021-07-13T12:27:58.725839Z","iopub.execute_input":"2021-07-13T12:27:58.72616Z","iopub.status.idle":"2021-07-13T12:27:58.739017Z","shell.execute_reply.started":"2021-07-13T12:27:58.726128Z","shell.execute_reply":"2021-07-13T12:27:58.737953Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def trade_stock_stat(path, stock_id):\n    trade =  pd.read_parquet(path)\n    trade['stock_id'] = stock_id\n    trade['trade_log_return'] = trade.groupby(['time_id'])['price'].apply(log_return).fillna(0)\n    trade_features = [\"price\", \"size\", \"order_count\", \"trade_log_return\"]\n  #  print(\"Trade\", trade)\n    for feature in trade_features:\n        if feature == \"price\":\n            stock_stat = agg(trade, trade, feature, func = \"mean\", rename = True, new_name = feature + \"_mean\")\n        else:\n            stock_stat = agg(trade, stock_stat, feature, func = \"mean\", rename = True, new_name = feature + \"_mean\")\n        stock_stat = agg(trade, stock_stat, feature, func = max, rename = True, new_name = feature + \"_max\")\n        stock_stat = agg(trade, stock_stat, feature, func = min, rename = True, new_name = feature + \"_min\")\n        stock_stat = agg(trade, stock_stat, feature, func = sum, rename = True, new_name = feature + \"_sum\")\n#         print(stock_stat)\n        if feature == \"trade_log_return\":\n            stock_stat = agg(trade, stock_stat, feature, func = rv, rename = True, new_name = feature + \"_rv\")\n   #     print(\"End df\", stock_stat)\n   # print(\"Final df\", stock_stat)    \n    return stock_stat\n\ndef get_trade_overall(book):\n    total_df = pd.DataFrame()\n    for i in tqdm.tqdm(book):\n        temp_stock = int(i.split(\"=\")[1])\n        temp_relvol = trade_stock_stat(path = i, stock_id = temp_stock)\n#         print(temp_relvol)\n#         print(temp_relvol.columns)\n        total_df = pd.concat([total_df, temp_relvol])\n    return total_df","metadata":{"execution":{"iopub.status.busy":"2021-07-13T12:27:58.740632Z","iopub.execute_input":"2021-07-13T12:27:58.741025Z","iopub.status.idle":"2021-07-13T12:27:58.753017Z","shell.execute_reply.started":"2021-07-13T12:27:58.740993Z","shell.execute_reply":"2021-07-13T12:27:58.751758Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_stock_stat(path, stock_id):\n    df_book = pd.read_parquet(path) # order book for a stock id loaded\n    # compute different vwap\n    df_book['wap1'] = (df_book['bid_price1'] * df_book['ask_size1'] + df_book['ask_price1'] * df_book['bid_size1']) / (\n                            df_book['bid_size1']+ df_book['ask_size1'])\n\n    # wap2\n    a = df_book['bid_price2'] * df_book['ask_size2'] + df_book['ask_price2'] * df_book['bid_size2']\n    b = df_book['bid_size2']+ df_book['ask_size2']\n    df_book['wap2'] = a/b\n    \n    # wap3\n    a1 = df_book['bid_price1'] * df_book['ask_size1'] + df_book['ask_price1'] * df_book['bid_size1']\n    a2 = df_book['bid_price2'] * df_book['ask_size2'] + df_book['ask_price2'] * df_book['bid_size2']\n    b = df_book['bid_size1'] + df_book['ask_size1'] + df_book['bid_size2']+ df_book['ask_size2']    \n    df_book['wap3'] = (a1 + a2)/ b\n    \n    # wap4 \n    a = (df_book['bid_price1'] * df_book['ask_size1'] + df_book['ask_price1'] * df_book['bid_size1']) / (\n                                       df_book['bid_size1']+ df_book['ask_size1'])\n    b = (df_book['bid_price2'] * df_book['ask_size2'] + df_book['ask_price2'] * df_book['bid_size2']) / (\n                                       df_book['bid_size2']+ df_book['ask_size2'])\n    df_book['wap4'] = (a + b) / 2\n                    \n    df_book['vol_wap1'] = (df_book.groupby(by = ['time_id'])['wap1'].apply(log_return).reset_index(drop = True).fillna(0))\n    df_book['vol_wap2'] = (df_book.groupby(by = ['time_id'])['wap2'].apply(log_return).reset_index(drop = True).fillna(0))\n    df_book['vol_wap3'] = (df_book.groupby(by = ['time_id'])['wap3'].apply(log_return).reset_index(drop = True).fillna(0))\n    df_book['vol_wap4'] = (df_book.groupby(by = ['time_id'])['wap4'].apply(log_return).reset_index(drop = True).fillna(0))\n                \n        \n    df_book['bas'] = (df_book[['ask_price1', 'ask_price2']].min(axis = 1)\n                                / df_book[['bid_price1', 'bid_price2']].max(axis = 1) - 1)   \n    \n\n    # different spreads\n    df_book['h_spread_l1'] = df_book['ask_price1'] - df_book['bid_price1']\n    df_book['h_spread_l2'] = df_book['ask_price2'] - df_book['bid_price2']\n    df_book['v_spread_b'] = df_book['bid_price1'] - df_book['bid_price2']\n    df_book['v_spread_a'] = df_book['ask_price1'] - df_book['ask_price2']\n    df_book['spread_dif1'] = df_book['ask_price1'] - df_book['bid_price2']\n    df_book['spread_dif2'] = df_book['ask_price2'] - df_book['bid_price1']\n    \n    # attach volatitilies based on different VWAPs\n    stock_stat = pd.merge(\n        df_book.groupby(by = ['time_id'])['vol_wap1'].agg(rv).reset_index(),\n        df_book.groupby(by = ['time_id'], as_index = False)['bas'].mean(),\n        on = ['time_id'], how = 'left'\n    )\n    \n    vol_features = [\"vol_wap2\", \"vol_wap3\", \"vol_wap4\"]\n    spread_features = [\"h_spread_l1\", 'h_spread_l2', 'v_spread_b', 'v_spread_a', \"spread_dif1\", \"spread_dif2\"]\n    time_features = [\"seconds_in_bucket\"]\n    for feature in vol_features:\n         stock_stat = agg(df_book, stock_stat, feature, rv)\n            \n    for feature in spread_features:\n        stock_stat = agg(df_book, stock_stat, feature, func = max, rename = True, new_name = feature + \"_max\")\n        stock_stat = agg(df_book, stock_stat, feature, func = min, rename = True, new_name = feature + \"_min\")\n        stock_stat = agg(df_book, stock_stat, feature, func = sum, rename = True, new_name = feature + \"_sum\")\n        stock_stat = agg(df_book, stock_stat, feature, func = \"mean\", rename = True, new_name = feature + \"_mean\")\n    \n#     for feature in time_features:\n#         stock_stat = agg(df_book, stock_stat, feature, func = max, rename = True, new_name = feature + \"_max\")\n#         stock_stat = agg(df_book, stock_stat, feature, func = min, rename = True, new_name = feature + \"_min\")\n#         stock_stat = agg(df_book, stock_stat, feature, func = sum, rename = True, new_name = feature + \"_sum\")\n#         stock_stat = agg(df_book, stock_stat, feature, func = \"mean\", rename = True, new_name = feature + \"_mean\")\n    \n    \n    stock_stat['stock_id'] = stock_id\n    #print(stock_stat[[\"seconds_in_bucket_max\", \"seconds_in_bucket_min\", \"seconds_in_bucket_sum\", \"seconds_in_bucket_mean\"]])\n    #stock_stat = pd.merge(stock_stat, df_book.groupby(\"time_id\")[\"seconds_in_bucket\"].diff().fillna(0).mean().rename(columns = {\"index\" : \"time_id\", \"seconds_in_bucket\" : \"sib_diff\"}), on = [\"time_id\"], how = \"left\")\n\n    return stock_stat","metadata":{"execution":{"iopub.status.busy":"2021-07-13T12:27:58.754519Z","iopub.execute_input":"2021-07-13T12:27:58.754853Z","iopub.status.idle":"2021-07-13T12:27:58.780632Z","shell.execute_reply.started":"2021-07-13T12:27:58.754823Z","shell.execute_reply":"2021-07-13T12:27:58.779503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_overall(book):\n    total_df = pd.DataFrame()\n    for i in tqdm.tqdm(book):\n        temp_stock = int(i.split(\"=\")[1])\n        temp_relvol = get_stock_stat(i, temp_stock)\n    #    print(temp_relvol)\n        total_df = pd.concat([total_df, temp_relvol])\n    return total_df\n\ndef linear_training(X,y,degree):\n    # instantiating polynomial features\n    polyfeat = PolynomialFeatures(degree = degree)\n    weights = 1/np.square(y)\n   # linreg = linear_model.LinearRegression()\n    linreg = XGBRegressor(eval_metric = rmspe, sample_weight = weights)\n    # preprocessing the training data\n    x = np.array(X)\n    x = np.array(X).reshape(-1,len(x[0]))\n    # x = np.array(X).reshape(-1,1)\n    # creating the polynomial features\n    X_ = polyfeat.fit_transform(x)\n    # training the model\n\n    return clf.fit(X_, np.array(y).reshape(-1,1), sample_weight = weights)\n\ndef transform(X, polyfeat):\n    x = np.array(X)\n    x = np.array(X).reshape(-1,len(x[0]))\n    X = polyfeat.fit_transform(x)\n    return X\n\ndef Xgboost(X, y, degree = 1, params = {\"reg_alpha\" : 20, \"reg_lambda\" : 20, \"max_depth\" : 5, \"n_estimators\" : 500}, folds=10):\n    polyfeat = PolynomialFeatures(degree = degree)\n    skf = KFold(n_splits=folds, shuffle=True, random_state=42)\n    for fold, (tr_idx, ts_idx) in enumerate(skf.split(X)):\n        \n        x_tr, y_tr = X.iloc[tr_idx], y.iloc[tr_idx]\n        x_ts, y_ts = X.iloc[ts_idx], y.iloc[ts_idx]\n        \n        x_tr = transform(x_tr, polyfeat)\n        x_ts = transform(x_ts, polyfeat)\n #       print(\"Y tr\", y_tr)\n  #      print(\"Xts\", x_ts)\n  #      print(\"y ts\", y_ts)\n        weights = np.array(1/np.square(y_tr))\n        eval_weights = np.array(1/np.square(y_ts))\n  #      print(\"Weights\", eval_weights)\n      #  print(\"xtr\", x_tr)\n        model = XGBRegressor(**params)\n        model.fit(x_tr, y_tr,\n                 eval_set=[(x_ts, y_ts)],\n                 early_stopping_rounds=10, sample_weight = weights, sample_weight_eval_set = [eval_weights],\n                  verbose=False)   \n     #   print(\"Evaluation result\", model.evals_result())\n              \n    return model\n\n#Groups stocks\ndef chunks(L, n): return [L[x: x+n] for x in range(0, len(L), n)]\n\ndef train_score(joined, models, stock_groups, feature_list, degree = 1):\n    train_pred = create_submission(joined, joined, models, stock_groups, feature_list, degree = degree, merge = True)\n    rmspe_train = rmspe(np.array(train_pred[\"target_x\"]), np.array(train_pred[\"target_y\"]))\n    return rmspe_train\n\n# creating the header for the submission file\ndef create_submission(test_df, joined, models, stock_groups, feature_list, degree = 1, merge = False):\n    submission = pd.DataFrame({\"row_id\" : [], \"target\" : []})  \n    submission[\"row_id\"] = test_df.apply(lambda x: str(int(x.stock_id)) + '-' + str(int(x.time_id)), axis=1)\n    submission[\"target\"] = test_df.apply(lambda x: linear_inference(models,\\\n                                                                            x.stock_id,\\\n                                                                            stock_groups, \\\n                                                                            list(x[feature_list]),\\\n                                                                            degree),\\\n                                                 axis = 1)\n    \n    if merge:\n        submission[\"stock_id\"] = test_df.apply(lambda x: int(x.stock_id), axis = 1)\n        submission[\"time_id\"] = test_df.apply(lambda x: int(x.time_id), axis=1)\n        overall = joined.merge(submission, on = [\"stock_id\", \"time_id\"], how = \"left\")\n        return overall\n    else:\n        return submission\n    \ndef get_model(train, joined, feature_list, group_number = 5, group = True, degree = 1, xgboost = False):\n    stock_groups = chunks(list(joined.groupby(\"stock_id\").mean()[\"target\"].reset_index().sort_values(by = \"target\")[\"stock_id\"]), group_number)\n    stock_id_train = train.stock_id.unique() # all stock_id for the train set\n    models = {} # dictionary for holding trained models for each stock_id\n    if not group:\n        for i in tqdm.tqdm(stock_id_train):\n            temp = joined[joined[\"stock_id\"]==i]\n            X = temp[feature_list]\n            y = temp[\"target\"]\n            if xgboost:\n                models[i] = Xgboost(X, y, degree = degree, folds=5)\n           #     xgb.plot_importance(models[i], title = \"Feature importance for stock id {}:\".format(i))\n            else:\n                models[i] = linear_training(X,y,degree)\n    else:\n        for i in tqdm.tqdm(range(len(stock_groups))):\n            temp = joined[joined[\"stock_id\"].isin(stock_groups[i])]\n            X = temp[feature_list]\n            y = temp[\"target\"]\n            if xgboost:\n                models[i] = Xgboost(X, y, degree = degree, folds=5)\n         #       xgb.plot_importance(models[i], title = \"Feature importance for stock ids {}:\".format(\"\".join(str(stock_groups[i]))))\n            else:\n                models[i] = linear_training(X,y,degree)\n    return models\n\ndef merge_frames(train = True):\n    if train:\n        order_book_training = glob.glob('/kaggle/input/optiver-realized-volatility-prediction/book_train.parquet/*')\n        total_df = get_overall(order_book_training)\n        trade_training = glob.glob('/kaggle/input/optiver-realized-volatility-prediction/trade_train.parquet/*')\n        total_trade = get_trade_overall(trade_training).drop(columns = [\"seconds_in_bucket\", \"price\", \"size\", \"order_count\", \"trade_log_return\"]).drop_duplicates()\n        total_df = total_df.merge(total_trade, on = [\"stock_id\",\"time_id\"], how = \"left\")\n    else:\n        # listing all test order books\n        order_book_test = glob.glob('/kaggle/input/optiver-realized-volatility-prediction/book_test.parquet/*')\n        test_df = get_overall(order_book_test)\n        trade_test = glob.glob('/kaggle/input/optiver-realized-volatility-prediction/trade_test.parquet/*')\n        total_test = get_trade_overall(trade_test).drop(columns = [\"seconds_in_bucket\", \"price\", \"size\", \"order_count\", \"trade_log_return\"]).drop_duplicates()\n        total_df = test_df.merge(total_test, on = [\"stock_id\",\"time_id\"], how = \"left\")\n    return total_df\n\ndef combined(train, save = True, read = False):\n    total_df = merge_frames()\n    if read:\n        joined = pd.read_csv(\"../input/weighted-regression-with-stock-grouping-and-eda/Joined1.csv\")\n    else:    \n        joined = train.merge(total_df, on = [\"stock_id\",\"time_id\"], how = \"left\")\n    \n    if save:\n        joined.to_csv(\"Joined1.csv\", index = False)\n\n    feature_list = list(joined.drop(columns = [\"stock_id\", \"time_id\", \"target\"]).columns)\n    return feature_list, joined\n","metadata":{"execution":{"iopub.status.busy":"2021-07-13T12:27:58.783729Z","iopub.execute_input":"2021-07-13T12:27:58.784042Z","iopub.status.idle":"2021-07-13T12:27:58.821382Z","shell.execute_reply.started":"2021-07-13T12:27:58.784011Z","shell.execute_reply":"2021-07-13T12:27:58.820172Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_list, joined = combined(train, save = True)\nfeature_list","metadata":{"execution":{"iopub.status.busy":"2021-07-13T12:27:58.823066Z","iopub.execute_input":"2021-07-13T12:27:58.823375Z","iopub.status.idle":"2021-07-13T13:13:13.101087Z","shell.execute_reply.started":"2021-07-13T12:27:58.823347Z","shell.execute_reply":"2021-07-13T13:13:13.099846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"joined.head(5)","metadata":{"execution":{"iopub.status.busy":"2021-07-13T13:13:13.102658Z","iopub.execute_input":"2021-07-13T13:13:13.103109Z","iopub.status.idle":"2021-07-13T13:13:13.134028Z","shell.execute_reply.started":"2021-07-13T13:13:13.103063Z","shell.execute_reply":"2021-07-13T13:13:13.132803Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(25,25))\nsns.heatmap(joined.groupby(by = \"stock_id\").mean()[feature_list + [\"target\"]].corr(), center = 0, annot = True, cmap=\"YlGnBu\", linewidths = .05)\nplt.title(\"Correlation heatmap\")\njoined.groupby(\"stock_id\").mean()[\"target\"].reset_index().sort_values(by = \"target\").plot(x = \"stock_id\", y = \"target\", kind = \"bar\", figsize = (20, 8), title = \"Mean target for each stock sorted\")\n","metadata":{"execution":{"iopub.status.busy":"2021-07-13T13:13:13.137625Z","iopub.execute_input":"2021-07-13T13:13:13.13795Z","iopub.status.idle":"2021-07-13T13:13:24.538867Z","shell.execute_reply.started":"2021-07-13T13:13:13.13792Z","shell.execute_reply":"2021-07-13T13:13:24.537582Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df = merge_frames(train = False)\ntest_df","metadata":{"execution":{"iopub.status.busy":"2021-07-13T13:13:24.540417Z","iopub.execute_input":"2021-07-13T13:13:24.540738Z","iopub.status.idle":"2021-07-13T13:13:24.894579Z","shell.execute_reply.started":"2021-07-13T13:13:24.540685Z","shell.execute_reply":"2021-07-13T13:13:24.89359Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"joined[joined[\"order_count_max\"].isnull()]","metadata":{"execution":{"iopub.status.busy":"2021-07-13T13:13:24.895994Z","iopub.execute_input":"2021-07-13T13:13:24.896297Z","iopub.status.idle":"2021-07-13T13:13:24.937222Z","shell.execute_reply.started":"2021-07-13T13:13:24.896266Z","shell.execute_reply":"2021-07-13T13:13:24.935962Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def fill_nan(joined):\n    nan_features = list(joined.columns)[32:]\n    missing_stock_ids = list(joined[joined[\"trade_log_return_rv\"].isnull()][\"stock_id\"].unique())\n    for stock_id in missing_stock_ids:\n        for feature in nan_features:\n            if joined.loc[joined[\"stock_id\"] == stock_id, feature].isnull().all():\n                joined.loc[joined[\"stock_id\"] == stock_id, feature] = joined.loc[joined[\"stock_id\"] == stock_id, feature].fillna(joined[feature].mean())\n            else:\n                joined.loc[joined[\"stock_id\"] == stock_id, feature] = joined.loc[joined[\"stock_id\"] == stock_id, feature].fillna(joined.loc[joined[\"stock_id\"] == stock_id, feature].mean())\n    return joined\njoined = fill_nan(joined)\nprint(\"Missing count after filling\", joined.isnull().any().sum())","metadata":{"execution":{"iopub.status.busy":"2021-07-13T13:13:24.938617Z","iopub.execute_input":"2021-07-13T13:13:24.938943Z","iopub.status.idle":"2021-07-13T13:13:25.482226Z","shell.execute_reply.started":"2021-07-13T13:13:24.938909Z","shell.execute_reply":"2021-07-13T13:13:25.481231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"joined[joined[\"size_sum\"].isnull()]","metadata":{"execution":{"iopub.status.busy":"2021-07-13T13:48:11.869585Z","iopub.execute_input":"2021-07-13T13:48:11.869973Z","iopub.status.idle":"2021-07-13T13:48:11.888144Z","shell.execute_reply.started":"2021-07-13T13:48:11.869943Z","shell.execute_reply":"2021-07-13T13:48:11.886954Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Looping over grouping and the polynomial features degree for best train score\ngroup_numbers = [1]\ndegrees = [1]\nscores = {}\nvalidate = False\nfor group_number in group_numbers:\n    for degree in degrees:\n        stock_groups = chunks(list(joined.groupby(\"stock_id\").mean()[\"target\"].reset_index().sort_values(by = \"target\")[\"stock_id\"]), group_number)\n        models = get_model(train, joined, feature_list, group_number = group_number, group = True, degree = degree, xgboost = True)\n        if validate:\n            overall_train_score = train_score(joined, models, stock_groups, feature_list, degree = degree)\n            print(\"Train score for group number {} and degree {}:\".format(group_number, degree), overall_train_score)\n            scores[str(group_number) + \" - \" + str(degree)] = overall_train_score","metadata":{"execution":{"iopub.status.busy":"2021-07-13T13:13:25.483454Z","iopub.execute_input":"2021-07-13T13:13:25.483772Z","iopub.status.idle":"2021-07-13T13:17:58.430825Z","shell.execute_reply.started":"2021-07-13T13:13:25.483739Z","shell.execute_reply":"2021-07-13T13:17:58.429261Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = create_submission(test_df, joined, models, stock_groups, feature_list, degree = 1, merge = False)\nsubmission.to_csv(\"submission.csv\", index = False)\nsubmission","metadata":{"execution":{"iopub.status.busy":"2021-07-13T13:17:58.434436Z","iopub.execute_input":"2021-07-13T13:17:58.434798Z","iopub.status.idle":"2021-07-13T13:17:58.463207Z","shell.execute_reply.started":"2021-07-13T13:17:58.434762Z","shell.execute_reply":"2021-07-13T13:17:58.462385Z"},"trusted":true},"execution_count":null,"outputs":[]}]}