{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from IPython.core.display import display, HTML\n\nimport pandas as pd\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport glob\nimport os\nimport gc\nimport math\n\nfrom joblib import Parallel, delayed\n\nfrom sklearn import preprocessing, model_selection\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import QuantileTransformer\nfrom sklearn.metrics import r2_score\n\nimport matplotlib.pyplot as plt \nimport seaborn as sns\nimport numpy.matlib\n\n#import warnings\n#warnings.filterwarnings('ignore')\n\npath_submissions = '/'\n\ntarget_name = 'target'\nscores_folds = {}\n%load_ext line_profiler","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":1.105168,"end_time":"2021-09-09T06:28:49.757583","exception":false,"start_time":"2021-09-09T06:28:48.652415","status":"completed"},"tags":[],"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-09-10T12:05:28.624239Z","iopub.execute_input":"2021-09-10T12:05:28.624633Z","iopub.status.idle":"2021-09-10T12:05:28.637992Z","shell.execute_reply.started":"2021-09-10T12:05:28.624584Z","shell.execute_reply":"2021-09-10T12:05:28.636917Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def compare_dataframe(df_orig, df_comp, only_error_columns = True, need_columns=[], skip_columns=[]):\n    goods = []\n    errors = []\n    df = pd.DataFrame()\n    for column in df_orig.columns:\n        if column in skip_columns:\n            continue\n        try:        \n            is_good = numpy.isclose(df_orig[column], df_comp[column], equal_nan=True)\n            errors_sum = (is_good == False).sum()\n            if errors_sum == 0:\n                goods.append(column)\n                if only_error_columns == True and column not in need_columns:\n                    continue\n            df['o_'+column] = df_orig[column]\n            df['n_'+column] = df_comp[column]\n            df['c_'+column] = is_good\n            if errors_sum == 0:\n                print(column, errors_sum)\n            else:\n                errors.append([column, errors_sum])\n                print('Bad column:', column, errors_sum)\n        except Exception as inst:\n            print(f'Not found column: {column}', inst)\n    print('Goods columns: ', goods)\n    return (df.copy(deep=True),errors)","metadata":{"execution":{"iopub.status.busy":"2021-09-10T12:05:28.639956Z","iopub.execute_input":"2021-09-10T12:05:28.640297Z","iopub.status.idle":"2021-09-10T12:05:28.653387Z","shell.execute_reply.started":"2021-09-10T12:05:28.640259Z","shell.execute_reply":"2021-09-10T12:05:28.652667Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# data directory\ndata_dir = '../input/optiver-realized-volatility-prediction/'\n\n# Function to calculate first WAP\ndef calc_wap1(df):\n    wap = (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']) / (df['bid_size1'] + df['ask_size1'])\n    return wap\n\n# Function to calculate second WAP\ndef calc_wap2(df):\n    wap = (df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2']) / (df['bid_size2'] + df['ask_size2'])\n    return wap\n\ndef calc_wap3(df):\n    wap = (df['bid_price1'] * df['bid_size1'] + df['ask_price1'] * df['ask_size1']) / (df['bid_size1'] + df['ask_size1'])\n    return wap\n\ndef calc_wap4(df):\n    wap = (df['bid_price2'] * df['bid_size2'] + df['ask_price2'] * df['ask_size2']) / (df['bid_size2'] + df['ask_size2'])\n    return wap\n\n# Function to calculate the log of the return\n# Remember that logb(x / y) = logb(x) - logb(y)\ndef log_return(series):\n    return np.log(series).diff()\n\n# Calculate the realized volatility\ndef realized_volatility(series):\n    return np.sqrt(np.sum(series**2))\n\n# Function to count unique elements of a series\ndef count_unique(series):\n    return len(np.unique(series))\n\ndef tendency(price, vol):    \n    df_diff = np.diff(price)\n    val = (df_diff/price[1:])*100\n    power = np.sum(val*vol[1:])\n    return(power)\n\n# Function to read our base train and test set\ndef read_train_test():\n    train = pd.read_csv('../input/optiver-realized-volatility-prediction/train.csv')\n    test = pd.read_csv('../input/optiver-realized-volatility-prediction/test.csv')\n    # Create a key to merge with book and trade data\n    train['row_id'] = train['stock_id'].astype(str) + '-' + train['time_id'].astype(str)\n    test['row_id'] = test['stock_id'].astype(str) + '-' + test['time_id'].astype(str)\n    print(f'Our training set has {train.shape[0]} rows')\n    return train, test","metadata":{"papermill":{"duration":0.092933,"end_time":"2021-09-09T06:28:49.870717","exception":false,"start_time":"2021-09-09T06:28:49.777784","status":"completed"},"tags":[],"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-09-10T12:05:28.654635Z","iopub.execute_input":"2021-09-10T12:05:28.655444Z","iopub.status.idle":"2021-09-10T12:05:28.678743Z","shell.execute_reply.started":"2021-09-10T12:05:28.655394Z","shell.execute_reply":"2021-09-10T12:05:28.677536Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to preprocess book data (for each stock id)\ndef book_preprocessor(file_path):\n    df = pd.read_parquet(file_path)\n    # Calculate Wap\n    df['wap1'] = calc_wap1(df)\n    df['wap2'] = calc_wap2(df)\n    df['wap3'] = calc_wap3(df)\n    df['wap4'] = calc_wap4(df)\n    # Calculate log returns\n    df['log_return1'] = df.groupby(['time_id'])['wap1'].apply(log_return)\n    df['log_return2'] = df.groupby(['time_id'])['wap2'].apply(log_return)\n    df['log_return3'] = df.groupby(['time_id'])['wap3'].apply(log_return)\n    df['log_return4'] = df.groupby(['time_id'])['wap4'].apply(log_return)\n    # Calculate wap balance\n    df['wap_balance'] = abs(df['wap1'] - df['wap2'])\n    # Calculate spread\n    df['price_spread'] = (df['ask_price1'] - df['bid_price1']) / ((df['ask_price1'] + df['bid_price1']) / 2)\n    df['price_spread2'] = (df['ask_price2'] - df['bid_price2']) / ((df['ask_price2'] + df['bid_price2']) / 2)\n    df['bid_spread'] = df['bid_price1'] - df['bid_price2']\n    df['ask_spread'] = df['ask_price1'] - df['ask_price2']\n    df[\"bid_ask_spread\"] = abs(df['bid_spread'] - df['ask_spread'])\n    df['total_volume'] = (df['ask_size1'] + df['ask_size2']) + (df['bid_size1'] + df['bid_size2'])\n    df['volume_imbalance'] = abs((df['ask_size1'] + df['ask_size2']) - (df['bid_size1'] + df['bid_size2']))\n    \n    # Dict for aggregations\n    create_feature_dict = {\n        'wap1': [np.sum, np.std],\n        'wap2': [np.sum, np.std],\n        'wap3': [np.sum, np.std],\n        'wap4': [np.sum, np.std],\n        'log_return1': [realized_volatility],\n        'log_return2': [realized_volatility],\n        'log_return3': [realized_volatility],\n        'log_return4': [realized_volatility],\n        'wap_balance': [np.sum, np.max],\n        'price_spread':[np.sum, np.max],\n        'price_spread2':[np.sum, np.max],\n        'bid_spread':[np.sum, np.max],\n        'ask_spread':[np.sum, np.max],\n        'total_volume':[np.sum, np.max],\n        'volume_imbalance':[np.sum, np.max],\n        \"bid_ask_spread\":[np.sum,  np.max],\n    }\n    create_feature_dict_time = {\n        'log_return1': [realized_volatility],\n        'log_return2': [realized_volatility],\n        'log_return3': [realized_volatility],\n        'log_return4': [realized_volatility],\n    }\n    \n    # Function to get group stats for different windows (seconds in bucket)\n    def get_stats_window(fe_dict,seconds_in_bucket, add_suffix = False):\n        # Group by the window\n        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(fe_dict).reset_index()\n        # Rename columns joining suffix\n        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n        # Add a suffix to differentiate windows\n        if add_suffix:\n            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n        return df_feature\n    \n    # Get the stats for different windows\n    df_feature = get_stats_window(create_feature_dict,seconds_in_bucket = 0, add_suffix = False)\n    df_feature_500 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 500, add_suffix = True)\n    df_feature_400 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 400, add_suffix = True)\n    df_feature_300 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 300, add_suffix = True)\n    df_feature_200 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 200, add_suffix = True)\n    df_feature_100 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 100, add_suffix = True)\n\n    # Merge all\n    df_feature = df_feature.merge(df_feature_500, how = 'left', left_on = 'time_id_', right_on = 'time_id__500')\n    df_feature = df_feature.merge(df_feature_400, how = 'left', left_on = 'time_id_', right_on = 'time_id__400')\n    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n    df_feature = df_feature.merge(df_feature_200, how = 'left', left_on = 'time_id_', right_on = 'time_id__200')\n    df_feature = df_feature.merge(df_feature_100, how = 'left', left_on = 'time_id_', right_on = 'time_id__100')\n    # Drop unnecesary time_ids\n    df_feature.drop(['time_id__500','time_id__400', 'time_id__300', 'time_id__200','time_id__100'], axis = 1, inplace = True)\n    \n    \n    # Create row_id so we can merge\n    stock_id = file_path.split('=')[1]\n    df_feature['row_id'] = df_feature['time_id_'].apply(lambda x: f'{stock_id}-{x}')\n    df_feature.drop(['time_id_'], axis = 1, inplace = True)\n    return df_feature","metadata":{"papermill":{"duration":0.092933,"end_time":"2021-09-09T06:28:49.870717","exception":false,"start_time":"2021-09-09T06:28:49.777784","status":"completed"},"tags":[],"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-09-10T12:05:28.727389Z","iopub.execute_input":"2021-09-10T12:05:28.727675Z","iopub.status.idle":"2021-09-10T12:05:28.756148Z","shell.execute_reply.started":"2021-09-10T12:05:28.727646Z","shell.execute_reply":"2021-09-10T12:05:28.754957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to preprocess trade data (for each stock id)\ndef trade_preprocessor(file_path):\n    df = pd.read_parquet(file_path)\n    df['log_return'] = df.groupby('time_id')['price'].apply(log_return)\n    df['amount']=df['price']*df['size']\n    # Dict for aggregations\n    create_feature_dict = {\n        'log_return':[realized_volatility],\n        'seconds_in_bucket':[count_unique],\n        'size':[np.sum, np.max, np.min],\n        'order_count':[np.sum,np.max],\n        'amount':[np.sum,np.max,np.min],\n    }\n    create_feature_dict_time = {\n        'log_return':[realized_volatility],\n        'seconds_in_bucket':[count_unique],\n        'size':[np.sum],\n        'order_count':[np.sum],\n    }\n    # Function to get group stats for different windows (seconds in bucket)\n    def get_stats_window(fe_dict,seconds_in_bucket, add_suffix = False):\n        # Group by the window\n        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(fe_dict).reset_index()\n        # Rename columns joining suffix\n        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n        # Add a suffix to differentiate windows\n        if add_suffix:\n            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n        return df_feature\n    \n\n    # Get the stats for different windows\n    df_feature = get_stats_window(create_feature_dict,seconds_in_bucket = 0, add_suffix = False)\n    df_feature_500 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 500, add_suffix = True)\n    df_feature_400 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 400, add_suffix = True)\n    df_feature_300 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 300, add_suffix = True)\n    df_feature_200 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 200, add_suffix = True)\n    df_feature_100 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 100, add_suffix = True)\n        \n    lis = []\n    for n_time_id in df['time_id'].unique():\n        df_id = df[df['time_id'] == n_time_id]        \n        tendencyV = tendency(df_id['price'].values, df_id['size'].values)      \n        f_max = np.sum(df_id['price'].values > np.mean(df_id['price'].values))\n        f_min = np.sum(df_id['price'].values < np.mean(df_id['price'].values))\n        df_max =  np.sum(np.diff(df_id['price'].values) > 0)\n        df_min =  np.sum(np.diff(df_id['price'].values) < 0)\n        # new\n        abs_diff = np.median(np.abs( df_id['price'].values - np.mean(df_id['price'].values)))        \n        energy = np.mean(df_id['price'].values**2)\n        iqr_p = np.percentile(df_id['price'].values,75) - np.percentile(df_id['price'].values,25)\n        \n        # vol vars\n        \n        abs_diff_v = np.median(np.abs( df_id['size'].values - np.mean(df_id['size'].values)))        \n        energy_v = np.sum(df_id['size'].values**2)\n        iqr_p_v = np.percentile(df_id['size'].values,75) - np.percentile(df_id['size'].values,25)\n        \n        lis.append({'time_id':n_time_id,'tendency':tendencyV,'f_max':f_max,'f_min':f_min,'df_max':df_max,'df_min':df_min,\n                   'abs_diff':abs_diff,'energy':energy,'iqr_p':iqr_p,'abs_diff_v':abs_diff_v,'energy_v':energy_v,'iqr_p_v':iqr_p_v})\n    \n    df_lr = pd.DataFrame(lis)\n        \n   \n    df_feature = df_feature.merge(df_lr, how = 'left', left_on = 'time_id_', right_on = 'time_id')\n    \n    # Merge all\n    df_feature = df_feature.merge(df_feature_500, how = 'left', left_on = 'time_id_', right_on = 'time_id__500')\n    df_feature = df_feature.merge(df_feature_400, how = 'left', left_on = 'time_id_', right_on = 'time_id__400')\n    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n    df_feature = df_feature.merge(df_feature_200, how = 'left', left_on = 'time_id_', right_on = 'time_id__200')\n    df_feature = df_feature.merge(df_feature_100, how = 'left', left_on = 'time_id_', right_on = 'time_id__100')\n    # Drop unnecesary time_ids\n    df_feature.drop(['time_id__500','time_id__400', 'time_id__300', 'time_id__200','time_id','time_id__100'], axis = 1, inplace = True)\n    \n    \n    df_feature = df_feature.add_prefix('trade_')\n    stock_id = file_path.split('=')[1]\n    df_feature['row_id'] = df_feature['trade_time_id_'].apply(lambda x:f'{stock_id}-{x}')\n    df_feature.drop(['trade_time_id_'], axis = 1, inplace = True)\n    return df_feature","metadata":{"papermill":{"duration":0.092933,"end_time":"2021-09-09T06:28:49.870717","exception":false,"start_time":"2021-09-09T06:28:49.777784","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-09-10T12:05:28.758006Z","iopub.execute_input":"2021-09-10T12:05:28.75859Z","iopub.status.idle":"2021-09-10T12:05:28.7857Z","shell.execute_reply.started":"2021-09-10T12:05:28.758552Z","shell.execute_reply":"2021-09-10T12:05:28.784674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to get group stats for the stock_id and time_id\ndef get_time_stock(df):\n    vol_cols = ['log_return1_realized_volatility', 'log_return2_realized_volatility', 'log_return1_realized_volatility_400', 'log_return2_realized_volatility_400', \n                'log_return1_realized_volatility_300', 'log_return2_realized_volatility_300', 'log_return1_realized_volatility_200', 'log_return2_realized_volatility_200', \n                'trade_log_return_realized_volatility', 'trade_log_return_realized_volatility_400', 'trade_log_return_realized_volatility_300', 'trade_log_return_realized_volatility_200']\n\n\n    # Group by the stock id\n    df_stock_id = df.groupby(['stock_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n    # Rename columns joining suffix\n    df_stock_id.columns = ['_'.join(col) for col in df_stock_id.columns]\n    df_stock_id = df_stock_id.add_suffix('_' + 'stock')\n\n    # Group by the stock id\n    df_time_id = df.groupby(['time_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n    # Rename columns joining suffix\n    df_time_id.columns = ['_'.join(col) for col in df_time_id.columns]\n    df_time_id = df_time_id.add_suffix('_' + 'time')\n    \n    # Merge with original dataframe\n    df = df.merge(df_stock_id, how = 'left', left_on = ['stock_id'], right_on = ['stock_id__stock'])\n    df = df.merge(df_time_id, how = 'left', left_on = ['time_id'], right_on = ['time_id__time'])\n    df.drop(['stock_id__stock', 'time_id__time'], axis = 1, inplace = True)\n    return df\n    ","metadata":{"papermill":{"duration":0.092933,"end_time":"2021-09-09T06:28:49.870717","exception":false,"start_time":"2021-09-09T06:28:49.777784","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-09-10T12:05:28.787246Z","iopub.execute_input":"2021-09-10T12:05:28.787517Z","iopub.status.idle":"2021-09-10T12:05:28.805867Z","shell.execute_reply.started":"2021-09-10T12:05:28.787477Z","shell.execute_reply":"2021-09-10T12:05:28.805144Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_np_base_data_with_limits(np_df, time_id_index):\n    np_df_row_id = np_df[:,time_id_index]\n    diff_time_id = np.diff(np_df_row_id).astype(bool)\n    order = np.arange(1,len(diff_time_id)+1)\n    limits = order[diff_time_id]\n    return (limits, np_df_row_id)\n\ndef get_np_com_data_with_limits(df, time_id_index):\n    np_df = np.array(df.values)\n    limits, np_df_row_id = get_np_base_data_with_limits(np_df, time_id_index)\n    return (np_df, limits, np_df_row_id)\n\ndef get_np_bucket_with_limits(df, time_id_index, p_bucket_filter):\n    np_df = np.array(df.to_numpy())\n    np_df = np.delete(np_df, p_bucket_filter, axis = 0)\n    limits, np_df_row_id = get_np_base_data_with_limits(np_df, time_id_index)    \n    uniq_time_id = np.unique(np_df_row_id)\n    return (np_df, limits, uniq_time_id)\n\ndef get_np_data_with_limits(df, time_id_index):\n    np_df, limits, _ = get_np_com_data_with_limits(df, time_id_index)\n    return (np_df, limits)\n\ndef get_np_uniq_data_with_limits(df, time_id_index):\n    np_df, limits, np_df_row_id = get_np_com_data_with_limits(df, time_id_index)\n    uniq_time_id = np.unique(np_df_row_id)\n    return (np_df, limits, uniq_time_id)\n\ndef calc_np_wap(wap_index, np_df, limits):\n    wap_batchs = np.split(np_df[:,wap_index], limits)\n    rv_list = []\n    for wap_part in wap_batchs:\n        rv_diff = np.diff(np.log(wap_part))\n        rv_list.extend([np.NaN])\n        rv_list.extend(rv_diff)\n    return rv_list\n\ndef calc_aggr_nan_feature(feature_index, np_df, limits, aggr_func):\n    wap_batchs = np.split(np_df[:,feature_index], limits)\n    return [aggr_func(wap_part[int(math.isnan(wap_part[0])):]) for wap_part in wap_batchs]\n\ndef create_base_features(p_df_feature, p_np_df, p_limits, p_df_columns, p_create_feature_dict, p_seconds_in_bucket = -1):\n    for create_feature_key, create_feature_values in p_create_feature_dict.items():\n        feature_index = p_df_columns.index(create_feature_key)\n        for create_func in create_feature_values:\n            func_name = create_func.__name__\n            result = calc_aggr_nan_feature(feature_index, p_np_df, p_limits, create_func)\n            feature_name = create_feature_key + '_' + func_name\n            if p_seconds_in_bucket >= 0:\n                feature_name += '_' + str(p_seconds_in_bucket)\n            p_df_feature[feature_name] = result\n    return p_df_feature","metadata":{"execution":{"iopub.status.busy":"2021-09-10T12:05:28.807503Z","iopub.execute_input":"2021-09-10T12:05:28.808114Z","iopub.status.idle":"2021-09-10T12:05:28.826869Z","shell.execute_reply.started":"2021-09-10T12:05:28.808066Z","shell.execute_reply":"2021-09-10T12:05:28.8262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def calc_log_return(df):\n    df.sort_values(['time_id'],kind='stable',inplace=True)\n    columns = df.columns.tolist()\n    np_df, limits = get_np_data_with_limits(df, columns.index('time_id'))\n    df['log_return1'] = calc_np_wap(columns.index('wap1'), np_df, limits)\n    df['log_return2'] = calc_np_wap(columns.index('wap2'), np_df, limits)\n    df['log_return3'] = calc_np_wap(columns.index('wap3'), np_df, limits)\n    df['log_return4'] = calc_np_wap(columns.index('wap4'), np_df, limits)\n    return df\n\ndef get_book_common_features(file_path):\n    df = pd.read_parquet(file_path)\n    \n    # Calculate Wap\n    df['wap1'] = calc_wap1(df)\n    df['wap2'] = calc_wap2(df)\n    df['wap3'] = calc_wap3(df)\n    df['wap4'] = calc_wap4(df)\n    \n    # Calculate log returns\n    df = calc_log_return(df)\n    \n    # Calculate wap balance\n    df['wap_balance'] = abs(df['wap1'] - df['wap2'])\n    # Calculate spread\n    df['price_spread'] = (df['ask_price1'] - df['bid_price1']) / ((df['ask_price1'] + df['bid_price1']) / 2)\n    df['price_spread2'] = (df['ask_price2'] - df['bid_price2']) / ((df['ask_price2'] + df['bid_price2']) / 2)\n    df['bid_spread'] = df['bid_price1'] - df['bid_price2']\n    df['ask_spread'] = df['ask_price1'] - df['ask_price2']\n    df[\"bid_ask_spread\"] = abs(df['bid_spread'] - df['ask_spread'])\n    df['total_volume'] = (df['ask_size1'] + df['ask_size2']) + (df['bid_size1'] + df['bid_size2'])\n    df['volume_imbalance'] = abs((df['ask_size1'] + df['ask_size2']) - (df['bid_size1'] + df['bid_size2']))\n    \n    return df\n\ndef get_base_features(df):\n    # Dict for aggregations\n    create_feature_dict = {\n        'wap1': [np.sum, np.std], #tst_new],\n        'wap2': [np.sum, np.std],\n        'wap3': [np.sum, np.std],\n        'wap4': [np.sum, np.std],\n        'log_return1': [realized_volatility],\n        'log_return2': [realized_volatility],\n        'log_return3': [realized_volatility],\n        'log_return4': [realized_volatility],\n        'wap_balance': [np.sum, np.max],\n        'price_spread':[np.sum, np.max],\n        'price_spread2':[np.sum, np.max],\n        'bid_spread':[np.sum, np.max],\n        'ask_spread':[np.sum, np.max],\n        'total_volume':[np.sum, np.max],\n        'volume_imbalance':[np.sum, np.max],\n        \"bid_ask_spread\":[np.sum,  np.max],\n    }\n    df_columns = df.columns.tolist()\n    np_df, limits, uniq_time_id = get_np_uniq_data_with_limits(df, df_columns.index('time_id'))\n    df_feature = pd.DataFrame()\n    df_feature['time_id'+'_'] = uniq_time_id.astype(int)\n\n    df_feature = create_base_features(df_feature, np_df, limits, df_columns, create_feature_dict)    \n    return df_feature\n\ndef get_book_time_features(df, df_feature):\n    create_feature_dict_time = {\n        'log_return1': [realized_volatility],\n        'log_return2': [realized_volatility],\n        'log_return3': [realized_volatility],\n        'log_return4': [realized_volatility],\n    }         \n    df_columns = df.columns.tolist()\n    for sec_in_bucket in range(500, 0, -100):\n        np_delete_buckets = df['seconds_in_bucket'].to_numpy() < sec_in_bucket\n        np_df, limits, uniq_time_id = get_np_bucket_with_limits(df, df_columns.index('time_id'), np_delete_buckets)\n        df_feature_time = pd.DataFrame()\n        df_feature_time['time_id'+'__'+f'{sec_in_bucket}'] = uniq_time_id.astype(int)\n        df_feature_time = create_base_features(df_feature_time, np_df, limits, df_columns, \n                                               create_feature_dict_time, sec_in_bucket)\n        df_feature = df_feature.merge(df_feature_time, how = 'left', left_on = 'time_id_', right_on = f'time_id__{sec_in_bucket}')\n        df_feature.drop([f'time_id__{sec_in_bucket}'], axis = 1, inplace = True)\n    return df_feature\n\ndef book_preprocessor_new(file_path):\n    df = get_book_common_features(file_path)\n    df_feature = get_base_features(df)   \n    df_feature = get_book_time_features(df, df_feature)\n    # Create row_id so we can merge\n    stock_id = file_path.split('=')[1]\n    df_feature['row_id'] = df_feature['time_id_'].apply(lambda x: f'{stock_id}-{x}')\n    df_feature.drop(['time_id_'], axis = 1, inplace = True)\n    return df_feature  #.copy(deep=True)","metadata":{"execution":{"iopub.status.busy":"2021-09-10T12:05:28.829423Z","iopub.execute_input":"2021-09-10T12:05:28.829949Z","iopub.status.idle":"2021-09-10T12:05:28.858186Z","shell.execute_reply.started":"2021-09-10T12:05:28.829903Z","shell.execute_reply":"2021-09-10T12:05:28.857315Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_trade_base_features(df):\n    # Dict for aggregations\n    create_feature_dict = {\n        'log_return':[realized_volatility],\n        'seconds_in_bucket':[count_unique],\n        'size':[np.sum, np.max, np.min],\n        'order_count':[np.sum,np.max],\n        'amount':[np.sum,np.max,np.min],\n    }    \n    df_columns = df.columns.tolist()\n    np_df, limits, uniq_time_id = get_np_uniq_data_with_limits(df, df_columns.index('time_id'))\n    df_feature = pd.DataFrame()\n    df_feature['time_id'+'_'] = uniq_time_id.astype(int)\n\n    df_feature = create_base_features(df_feature, np_df, limits, df_columns, create_feature_dict)      \n    return df_feature\n\ndef get_trade_time_features(df, df_feature):\n    create_feature_dict_time = {\n        'log_return':[realized_volatility],\n        'seconds_in_bucket':[count_unique],\n        'size':[np.sum],\n        'order_count':[np.sum],\n    }\n    df_columns = df.columns.tolist()\n    for sec_in_bucket in range(500, 0, -100):\n        np_delete_buckets = df['seconds_in_bucket'].to_numpy() < sec_in_bucket\n        np_df, limits, uniq_time_id = get_np_bucket_with_limits(df, df_columns.index('time_id'), np_delete_buckets)\n        df_feature_time = pd.DataFrame()\n        df_feature_time['time_id'+'__'+f'{sec_in_bucket}'] = uniq_time_id.astype(int)\n        df_feature_time = create_base_features(df_feature_time, np_df, limits, df_columns, \n                                               create_feature_dict_time, sec_in_bucket)\n        df_feature = df_feature.merge(df_feature_time, how = 'left', left_on = 'time_id_', right_on = f'time_id__{sec_in_bucket}')\n        df_feature.drop([f'time_id__{sec_in_bucket}'], axis = 1, inplace = True)\n    return df_feature\n\n# Function to preprocess trade data (for each stock id)\ndef trade_preprocessor_new(file_path):\n    df = pd.read_parquet(file_path)\n    \n    #df['log_return'] = df.groupby('time_id')['price'].apply(log_return)\n    df.sort_values(['time_id'],kind='stable',inplace=True)\n    columns = df.columns.tolist()\n    np_df, limits = get_np_data_with_limits(df, columns.index('time_id'))\n    df['log_return'] = calc_np_wap(columns.index('price'), np_df, limits)\n    \n    df['amount'] = df['price'] * df['size']\n    \n    df_feature = get_trade_base_features(df)\n    # Get the stats for different windows\n    df_feature = get_trade_time_features(df, df_feature)\n       \n    df_columns = df.columns.tolist()\n    np_df, limits, uniq_time_id = get_np_uniq_data_with_limits(df, df_columns.index('time_id'))\n\n    lis = []\n    wap_batchs = np.split(np_df, limits)\n    num_price = df_columns.index('price')\n    num_size = df_columns.index('size')\n    for n_time_id, np_id in zip(uniq_time_id, wap_batchs):\n        np_price = np_id[:,num_price]\n        np_size = np_id[:,num_size]\n\n        tendencyV = tendency(np_price, np_size)      \n        \n        mean_price = np.mean(np_price)\n        f_max = np.sum(np_price > mean_price)\n        f_min = np.sum(np_price < mean_price)\n        \n        diff_price = np.diff(np_price)\n        df_max =  np.sum(diff_price > 0)\n        df_min =  np.sum(diff_price < 0)\n        \n        # new\n        abs_diff = np.median(np.abs( np_price - mean_price))        \n        energy = np.mean(np_price**2)\n        iqr_p = np.percentile(np_price,75) - np.percentile(np_price,25)\n        \n        # vol vars\n        abs_diff_v = np.median(np.abs( np_size - np.mean(np_size)))        \n        energy_v = np.sum(np_size**2)\n        iqr_p_v = np.percentile(np_size,75) - np.percentile(np_size,25)\n        \n        lis.append({'time_id':n_time_id,'tendency':tendencyV,'f_max':f_max,'f_min':f_min,'df_max':df_max,'df_min':df_min,\n                   'abs_diff':abs_diff,'energy':energy,'iqr_p':iqr_p,'abs_diff_v':abs_diff_v,'energy_v':energy_v,'iqr_p_v':iqr_p_v})\n        \n    df_lr = pd.DataFrame(lis)\n    df_feature = df_feature.merge(df_lr, how = 'left', left_on = 'time_id_', right_on = 'time_id')\n       \n    df_feature = df_feature.add_prefix('trade_')\n    stock_id = file_path.split('=')[1]\n    df_feature['row_id'] = df_feature['trade_time_id_'].apply(lambda x:f'{stock_id}-{x}')\n    df_feature.drop(['trade_time_id_'], axis = 1, inplace = True)\n    return df_feature  #.copy(deep=True)","metadata":{"execution":{"iopub.status.busy":"2021-09-10T12:05:28.859666Z","iopub.execute_input":"2021-09-10T12:05:28.860447Z","iopub.status.idle":"2021-09-10T12:05:28.88923Z","shell.execute_reply.started":"2021-09-10T12:05:28.860405Z","shell.execute_reply":"2021-09-10T12:05:28.887755Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#train = pd.read_csv('../input/optiver-realized-volatility-prediction/train.csv')","metadata":{"execution":{"iopub.status.busy":"2021-09-10T12:05:28.890864Z","iopub.execute_input":"2021-09-10T12:05:28.891248Z","iopub.status.idle":"2021-09-10T12:05:28.908519Z","shell.execute_reply.started":"2021-09-10T12:05:28.891193Z","shell.execute_reply":"2021-09-10T12:05:28.907481Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"list_trade_file_train = glob.glob('/kaggle/input/optiver-realized-volatility-prediction/trade_train.parquet/*')\ntrade_file_path = list_trade_file_train[0]\n\ntr_new = trade_preprocessor_new(trade_file_path)\ntr_old = trade_preprocessor(trade_file_path)\ntr_c,tr_e = compare_dataframe(tr_old,tr_new,skip_columns=['row_id'])","metadata":{"execution":{"iopub.status.busy":"2021-09-10T12:05:28.910087Z","iopub.execute_input":"2021-09-10T12:05:28.910382Z","iopub.status.idle":"2021-09-10T12:05:46.917613Z","shell.execute_reply.started":"2021-09-10T12:05:28.910355Z","shell.execute_reply":"2021-09-10T12:05:46.916866Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tr_e","metadata":{"execution":{"iopub.status.busy":"2021-09-10T12:05:46.944495Z","iopub.execute_input":"2021-09-10T12:05:46.945195Z","iopub.status.idle":"2021-09-10T12:05:46.950958Z","shell.execute_reply.started":"2021-09-10T12:05:46.94516Z","shell.execute_reply":"2021-09-10T12:05:46.950007Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"tr_c[tr_c['c_trade_tendency']==False]","metadata":{"execution":{"iopub.status.busy":"2021-09-10T12:13:40.182733Z","iopub.execute_input":"2021-09-10T12:13:40.183005Z","iopub.status.idle":"2021-09-10T12:13:40.204869Z","shell.execute_reply.started":"2021-09-10T12:13:40.182979Z","shell.execute_reply":"2021-09-10T12:13:40.203818Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tr_c[tr_c['c_trade_f_max']==False]","metadata":{"execution":{"iopub.status.busy":"2021-09-10T12:11:13.943322Z","iopub.execute_input":"2021-09-10T12:11:13.943629Z","iopub.status.idle":"2021-09-10T12:11:13.962098Z","shell.execute_reply.started":"2021-09-10T12:11:13.943595Z","shell.execute_reply":"2021-09-10T12:11:13.96116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tr_c[tr_c['c_trade_f_min']==False]","metadata":{"execution":{"iopub.status.busy":"2021-09-10T12:12:23.773215Z","iopub.execute_input":"2021-09-10T12:12:23.773544Z","iopub.status.idle":"2021-09-10T12:12:23.79246Z","shell.execute_reply.started":"2021-09-10T12:12:23.773512Z","shell.execute_reply":"2021-09-10T12:12:23.791566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tr_c[tr_c['c_trade_abs_diff']==False].head(5)","metadata":{"execution":{"iopub.status.busy":"2021-09-10T12:14:30.722958Z","iopub.execute_input":"2021-09-10T12:14:30.723272Z","iopub.status.idle":"2021-09-10T12:14:30.74347Z","shell.execute_reply.started":"2021-09-10T12:14:30.723242Z","shell.execute_reply":"2021-09-10T12:14:30.742351Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"list_book_file_train = glob.glob('/kaggle/input/optiver-realized-volatility-prediction/book_train.parquet/*')\nbook_file_path = list_book_file_train[0]\n\nbk_new = book_preprocessor_new(book_file_path)\nbk_old = book_preprocessor(book_file_path)\nbk_c,bk_e = compare_dataframe(bk_old,bk_new,skip_columns=['row_id'])","metadata":{"execution":{"iopub.status.busy":"2021-09-10T12:05:46.952383Z","iopub.execute_input":"2021-09-10T12:05:46.952669Z","iopub.status.idle":"2021-09-10T12:06:19.468608Z","shell.execute_reply.started":"2021-09-10T12:05:46.952629Z","shell.execute_reply":"2021-09-10T12:06:19.467097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bk_c.head(5)","metadata":{"execution":{"iopub.status.busy":"2021-09-10T12:15:22.168191Z","iopub.execute_input":"2021-09-10T12:15:22.168632Z","iopub.status.idle":"2021-09-10T12:15:22.189024Z","shell.execute_reply.started":"2021-09-10T12:15:22.168581Z","shell.execute_reply":"2021-09-10T12:15:22.187638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%time book_preprocessor(book_file_path)\n#%lprun -f book_preprocessor book_preprocessor(book_file_path)","metadata":{"execution":{"iopub.status.busy":"2021-09-10T08:08:08.92805Z","iopub.execute_input":"2021-09-10T08:08:08.928342Z","iopub.status.idle":"2021-09-10T08:09:23.989443Z","shell.execute_reply.started":"2021-09-10T08:08:08.928312Z","shell.execute_reply":"2021-09-10T08:09:23.988241Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%time book_preprocessor_new(book_file_path)\n#%lprun -f book_preprocessor_new book_preprocessor_new(book_file_path)","metadata":{"execution":{"iopub.status.busy":"2021-09-10T08:09:23.9927Z","iopub.execute_input":"2021-09-10T08:09:23.993708Z","iopub.status.idle":"2021-09-10T08:09:35.28036Z","shell.execute_reply.started":"2021-09-10T08:09:23.993655Z","shell.execute_reply":"2021-09-10T08:09:35.279316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%time trade_preprocessor(trade_file_path)\n#%lprun -f trade_preprocessor trade_preprocessor(trade_file_path)","metadata":{"execution":{"iopub.status.busy":"2021-09-10T08:09:35.281735Z","iopub.execute_input":"2021-09-10T08:09:35.282028Z","iopub.status.idle":"2021-09-10T08:10:11.599311Z","shell.execute_reply.started":"2021-09-10T08:09:35.281989Z","shell.execute_reply":"2021-09-10T08:10:11.59813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%time trade_preprocessor_new(trade_file_path)\n#%lprun -f trade_preprocessor_new trade_preprocessor_new(trade_file_path)","metadata":{"execution":{"iopub.status.busy":"2021-09-10T08:10:11.600681Z","iopub.execute_input":"2021-09-10T08:10:11.601394Z","iopub.status.idle":"2021-09-10T08:10:22.714662Z","shell.execute_reply.started":"2021-09-10T08:10:11.601351Z","shell.execute_reply":"2021-09-10T08:10:22.713759Z"},"trusted":true},"execution_count":null,"outputs":[]}]}