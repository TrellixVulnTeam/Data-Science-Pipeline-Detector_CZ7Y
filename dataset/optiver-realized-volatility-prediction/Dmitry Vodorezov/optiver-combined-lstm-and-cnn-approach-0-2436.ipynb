{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Preparing dataset for feeding to NN. Every value is rescaled within  a sample's time window to fit in [0,1] interval.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os\nimport math\n\n\nrawdata = os.scandir(r'../input/optiver-realized-volatility-prediction/book_train.parquet')\nrawdatb = os.scandir(r'../input/optiver-realized-volatility-prediction/trade_train.parquet')\n\ndef log_return(list_stock_prices):\n    return np.log(list_stock_prices).diff() \n\nschet=0\ndatasetB=[]\nlistID=[]\ndatasetT=[]\n\nfor entry1 in rawdata:\n    group=os.scandir(r'../input/optiver-realized-volatility-prediction/book_train.parquet/'+entry1.name)\n    foldername=r'../input/optiver-realized-volatility-prediction/book_train.parquet/'+entry1.name\n    for entry in group:\n        datasetB.append(foldername+'/'+entry.name)\n        listID.append(int(entry1.name[9:]))\n    schet+=1\n\nfor entry1 in rawdatb:\n    group=os.scandir(r'../input/optiver-realized-volatility-prediction/trade_train.parquet/'+entry1.name)\n    foldername=r'../input/optiver-realized-volatility-prediction/trade_train.parquet/'+entry1.name\n    for entry in group:\n        datasetT.append(foldername+'/'+entry.name)\n    schet+=1    \n\ndataset=[]\n\nfor i in range(len(listID)):\n    stringI=[]\n    stringI.append(listID[i])\n    stringI.append(datasetB[i])\n    stringI.append(datasetT[i])\n    dataset.append(stringI)\n    \ndataset.sort()\n\ndataset=pd.DataFrame(dataset)\n\nn=pd.read_parquet(dataset[1][2])\n\n\nx0=6\ndelta=int(600/x0)\n\n     \n        \nlabels=pd.read_csv(r'../input/optiver-realized-volatility-prediction/train.csv')        \n          \n\n\nbigdata=np.zeros((len(labels),(x0)*6+2))\n\n\nsIDP=-50\n\nfor i in range(len(labels)):\n    sID=labels.iat[i,0]\n    tID=labels.iat[i,1]\n    target=labels.iat[i,2]\n    if sIDP!=sID:\n        book=pd.read_parquet(dataset.loc[dataset[0]==sID,[1]].iat[0,0])\n        trade=pd.read_parquet(dataset.loc[dataset[0]==sID,[2]].iat[0,0])\n    sIDP=sID  \n    bigdata[i,(x0)*6+1]=target\n    b=book.loc[book['time_id']==tID]\n    t=trade.loc[trade['time_id']==tID]\n    b.loc[:,'wap']=log_return((b.loc[:,'bid_price1']*b.loc[:,'ask_size1']+b.loc[:,'ask_price1']*b.loc[:,'bid_size1'])/(b.loc[:,'bid_size1']+b.loc[:,'ask_size1']))\n    b.loc[b['wap'].isnull()==1,'wap']=0\n    bmax=b.loc[:,'seconds_in_bucket'].max()\n    bmin=b.loc[:,'seconds_in_bucket'].min()\n    bdelta=(bmax-bmin)/599\n    b.loc[:,'seconds_in_bucket']=(b.loc[:,'seconds_in_bucket']-bmin)/bdelta\n    t.loc[:,'seconds_in_bucket']=(t.loc[:,'seconds_in_bucket']-bmin)/bdelta\n    b.loc[:,'seconds_in_bucket']=b.loc[:,'seconds_in_bucket']//delta\n    t.loc[:,'seconds_in_bucket']=t.loc[:,'seconds_in_bucket']//delta\n    totalo1=np.sum(b.loc[:,'ask_size1' ])\n    totalo2=np.sum(b.loc[:,'bid_size1'])\n    totalt=np.sum(t.loc[:,'size'])\n    totalo3=np.sum((b.loc[:, 'ask_price1']-b.loc[:, 'bid_price1']))\n    #totalo4=np.sum((b.loc[:, 'ask_price1']-b.loc[:, 'ask_price2']))\n\n    for k in range(x0):\n            step=b.loc[b['seconds_in_bucket']<=k]\n            f1=np.sqrt(np.sum(np.square(step.loc[:,'wap'])))\n            if math.isnan(f1):\n                f1=0\n            bigdata[i,k]=f1\n            f2=np.sum(step.loc[:, 'ask_size1'])/totalo1\n            if math.isnan(f2):\n                f2=0\n            bigdata[i,k+1*x0]=f2\n            step1=t.loc[t['seconds_in_bucket']<=k]\n            f3=np.sum(step.loc[:, 'bid_size1'])/totalo2\n            if math.isnan(f3):\n                f3=0\n            bigdata[i,k+2*x0]=f3\n            f4=np.sum(step.loc[:, 'ask_price1']-step.loc[:, 'bid_price1'])/totalo3\n            if math.isnan(f4):\n                f4=0\n            bigdata[i,k+3*x0]=f4\n            f5=np.sum(step1.loc[:, 'size'])/totalt\n            if math.isnan(f5):\n                f5=0\n            bigdata[i,k+4*x0]=f5\n            f6=np.sum(step.loc[:, 'ask_price1']-step.loc[:, 'ask_price2'])/totalo3#wastotalo4\n            if math.isnan(f6):\n                f6=0\n            bigdata[i,k+5*x0]=f6\n    bigdata[i,6*x0]=sIDP         ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"Neural network is trained in a few steps. Each step aims to train separate layers of the network, i.e. the first step trains convolutional part, the second trains the LSTM layer. The experiments has shown, that training with MeanAbsolutePercentageError instead of the  loss function proposed by competition description shows slightly better reuslts in some cases and in some cases the results are identical.","metadata":{}},{"cell_type":"code","source":"from  tensorflow import keras\nfrom tensorflow.keras import layers\n\n\n\nx=6\n\ndataset=bigdata\nnp.random.shuffle(dataset)\nf1,f2,f3,f4,f5,f6,sID,trg=np.hsplit(dataset, [x, 2*x, 3*x, 4*x, 5*x, 6*x, 6*x+1])\nsidB=np.zeros((len(dataset),127))\nfor i in range(len(sidB)):\n    pos=int(sID[i])\n    sidB[i,pos]=1\n\n\ni1=keras.Input(shape=(x,1), name='i1')\ni2=keras.Input(shape=(x,1), name='i2')\ni3=keras.Input(shape=(x,1), name='i3')\ni4=keras.Input(shape=(x,1), name='i4')\ni5=keras.Input(shape=(x,1), name='i5')\ni6=keras.Input(shape=(x,1), name='i6')\n\ni7=keras.Input(shape=(127,1), name='i7')\n\n\n\n\nsid=layers.Conv1D(\n    3,\n    127,\n    strides=1,\n    padding=\"valid\",\n    activation='sigmoid',\n    use_bias=True,\n    kernel_initializer=\"zeros\",\n    bias_initializer=\"zeros\",\n    kernel_regularizer=None,\n    bias_regularizer=None,\n    activity_regularizer=None,\n    kernel_constraint=None,\n    bias_constraint=None,\n    )(i7)\n\n\n\nsid=layers.Flatten()(sid)\n\npt=layers.concatenate([i1,i2,i3,i4,i5,i6],axis=2)\n\npt=layers.LSTM(\n    40,\n    activation=\"tanh\",\n    recurrent_activation=\"sigmoid\",\n    use_bias=True,\n    kernel_initializer=\"glorot_uniform\",\n    recurrent_initializer=\"orthogonal\",\n    bias_initializer=\"zeros\",\n    unit_forget_bias=True,\n    kernel_regularizer=None,\n    recurrent_regularizer=None,\n    bias_regularizer=None,\n    activity_regularizer=None,\n    kernel_constraint=None,\n    recurrent_constraint=None,\n    bias_constraint=None,\n    dropout=0.4, \n    recurrent_dropout=0.0,\n    return_sequences=False,\n    return_state=False,\n    go_backwards=True,\n    stateful=False,\n    time_major=False,\n    unroll=False,)(pt)\n\n\npt=layers.concatenate([pt,sid],axis=1)\n\nxx=layers.Dense(10,activation=\"relu\", use_bias=True)(pt)\n\n\ncatN=layers.Dense(1,activation=\"sigmoid\", use_bias=True, name='output')(xx)\n\ncallback = keras.callbacks.EarlyStopping(monitor='val_loss',  patience=10, baseline=100, mode='auto', restore_best_weights=True)\ncallback1 = keras.callbacks.EarlyStopping(monitor='val_loss',  patience=10, baseline=100, mode='auto', restore_best_weights=True)\n\nmodel = keras.Model(inputs=[i1,i2,i3,i4,i5,i6,i7], outputs=catN)\n\n\n\nopt = keras.optimizers.Adam(learning_rate=0.003)\nmodel.layers[8].trainable = False\nmodel.compile(loss='MeanAbsolutePercentageError', \n              optimizer=opt,  metrics=[keras.metrics.MeanAbsolutePercentageError()])\nmodel.summary()\nfor l in model.layers:\n    print(l.name, l.trainable)\nfitting=model.fit({'i1':f1,'i2':f2, 'i3':f3, 'i4':f4, 'i5':f5, 'i6':f6, 'i7':sidB},{'output':trg}, \n          epochs=100, validation_split=0.5, callbacks=[callback]) \nweights1 = model.get_weights()\n\n\n\nnp.random.shuffle(dataset)\nf1,f2,f3,f4,f5,f6,sID,trg=np.hsplit(dataset, [x, 2*x, 3*x, 4*x, 5*x, 6*x, 6*x+1])\nsidB=np.zeros((len(dataset),127))\nfor i in range(len(sidB)):\n    pos=int(sID[i])\n    sidB[i,pos]=1\n\nmodel1=keras.Model(inputs=[i1,i2,i3,i4,i5,i6,i7], outputs=catN)\n\nmodel1.set_weights(weights1)\nmodel1.layers[9].trainable = False\nmodel1.layers[8].trainable = True\n\n\nmodel1.compile(loss='MeanAbsolutePercentageError', \n              optimizer=opt,  metrics=[keras.metrics.MeanAbsolutePercentageError()])\n#for l in model.layers:\n#    print(l.name, l.trainable)\n#model.summary()\nfitting=model1.fit({'i1':f1,'i2':f2, 'i3':f3, 'i4':f4, 'i5':f5, 'i6':f6, 'i7':sidB},{'output':trg}, \n          epochs=30, validation_split=0.5, callbacks=[callback1]) \n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Preparing test dataset:","metadata":{}},{"cell_type":"code","source":"\nrawdataT = os.scandir(r'../input/optiver-realized-volatility-prediction/book_test.parquet')\nrawdatbT = os.scandir(r'../input/optiver-realized-volatility-prediction/trade_test.parquet')\n\nschet=0\ndatasetB=[]\nlistID=[]\ndatasetT=[]\n\nfor entry1 in rawdataT:\n    group=os.scandir(r'../input/optiver-realized-volatility-prediction/book_test.parquet/'+entry1.name)\n    foldername=r'../input/optiver-realized-volatility-prediction/book_test.parquet/'+entry1.name\n    for entry in group:\n        datasetB.append(foldername+'/'+entry.name)\n        listID.append(int(entry1.name[9:]))\n    schet+=1\n\nfor entry1 in rawdatbT:\n    group=os.scandir(r'../input/optiver-realized-volatility-prediction/trade_test.parquet/'+entry1.name)\n    foldername=r'../input/optiver-realized-volatility-prediction/trade_test.parquet/'+entry1.name\n    for entry in group:\n        datasetT.append(foldername+'/'+entry.name)\n    schet+=1    \n\ndataset=[]\n\nfor i in range(len(listID)):\n    stringI=[]\n    stringI.append(listID[i])\n    stringI.append(datasetB[i])\n    stringI.append(datasetT[i])\n    dataset.append(stringI)\n    \ndataset.sort()\n\n\nsubmission=[]\nlabels=[]\n\nfor i in range(len(dataset)):\n    n=pd.read_parquet(dataset[i][1])\n    stockID=dataset[i][0]\n    timeID=n['time_id'].unique()\n    for j in range(len(timeID)):\n        submission.append(str(stockID)+'-'+str(timeID[j]))\n        labels.append([stockID,timeID[j]])\n        \n\n\nx0=6\ndelta=int(600/x0)\n\n\nbigdata=np.zeros((len(submission),(x0)*6+1))\n\nsIDP=-50\ndataset=pd.DataFrame(dataset)\nfor i in range(len(labels)):\n    sID=labels[i][0]\n    tID=labels[i][1]\n    if sIDP!=sID:\n        book=pd.read_parquet(dataset.loc[dataset[0]==sID,[1]].iat[0,0])\n        trade=pd.read_parquet(dataset.loc[dataset[0]==sID,[2]].iat[0,0])\n    sIDP=sID  \n    b=book.loc[book['time_id']==tID]\n    t=trade.loc[trade['time_id']==tID]\n    b.loc[:,'wap']=log_return((b.loc[:,'bid_price1']*b.loc[:,'ask_size1']+b.loc[:,'ask_price1']*b.loc[:,'bid_size1'])/(b.loc[:,'bid_size1']+b.loc[:,'ask_size1']))\n    b.loc[b['wap'].isnull()==1,'wap']=0\n    bmax=b.loc[:,'seconds_in_bucket'].max()\n    bmin=b.loc[:,'seconds_in_bucket'].min()\n    bdelta=(bmax-bmin)/599\n    b.loc[:,'seconds_in_bucket']=(b.loc[:,'seconds_in_bucket']-bmin)/bdelta\n    t.loc[:,'seconds_in_bucket']=(t.loc[:,'seconds_in_bucket']-bmin)/bdelta\n    b.loc[:,'seconds_in_bucket']=b.loc[:,'seconds_in_bucket']//delta\n    t.loc[:,'seconds_in_bucket']=t.loc[:,'seconds_in_bucket']//delta\n    totalo1=np.sum(b.loc[:,'ask_size1' ])\n    totalo2=np.sum(b.loc[:,'bid_size1'])\n    totalt=np.sum(t.loc[:,'size'])\n    totalo3=np.sum((b.loc[:, 'ask_price1']-b.loc[:, 'bid_price1']))\n    totalo4=np.sum((b.loc[:, 'ask_price1']-b.loc[:, 'ask_price2']))\n\n    for k in range(x0):\n            step=b.loc[b['seconds_in_bucket']<=k]\n            f1=np.sqrt(np.sum(np.square(step.loc[:,'wap'])))\n            if math.isnan(f1):\n                f1=0\n            bigdata[i,k]=f1\n            f2=np.sum(step.loc[:, 'ask_size1'])/totalo1\n            if math.isnan(f2):\n                f2=0\n            bigdata[i,k+1*x0]=f2\n            step1=t.loc[t['seconds_in_bucket']<=k]\n            f3=np.sum(step.loc[:, 'bid_size1'])/totalo2\n            if math.isnan(f3):\n                f3=0\n            bigdata[i,k+2*x0]=f3\n            f4=np.sum(step.loc[:, 'ask_price1']-step.loc[:, 'bid_price1'])/totalo3\n            if math.isnan(f4):\n                f4=0\n            bigdata[i,k+3*x0]=f4\n            f5=np.sum(step1.loc[:, 'size'])/totalt\n            if math.isnan(f5):\n                f5=0\n            bigdata[i,k+4*x0]=f5\n            f6=np.sum(step.loc[:, 'ask_price1']-step.loc[:, 'ask_price2'])/totalo4\n            if math.isnan(f6):\n                f6=0\n            bigdata[i,k+5*x0]=f6\n    bigdata[i,6*x0]=sIDP  ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Submitting results:","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset=bigdata\n\nf1,f2,f3,f4,f5,f6,sID=np.hsplit(dataset, [x0, 2*x0, 3*x0, 4*x0, 5*x0, 6*x0])\nsidB=np.zeros((len(dataset),127))\nfor i in range(len(sidB)):\n    pos=int(sID[i])\n    sidB[i,pos]=1\n    \nnn=model1.predict(\n    x={'i1':f1,'i2':f2, 'i3':f3, 'i4':f4, 'i5':f5, 'i6':f6, 'i7':sidB}, batch_size=None, verbose=0, steps=None,\n    callbacks=None)\n\nn=nn.reshape((len(nn)))\npred=n.tolist()\n\nlst1 = submission\nlst2 = pred\n\nitog = pd.DataFrame(\n    {'row_id': lst1,\n     'target': lst2\n     \n    })\n\n\nitog.to_csv('submission.csv',index=False)\n    ","metadata":{},"execution_count":null,"outputs":[]}]}