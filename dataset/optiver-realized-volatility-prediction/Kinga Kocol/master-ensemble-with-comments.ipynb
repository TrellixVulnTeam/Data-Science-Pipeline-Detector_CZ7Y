{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Introduction\nThis notebook creates the submission file for the competition [Optiver Realized Volatility Prediction](https://www.kaggle.com/c/optiver-realized-volatility-prediction).\n\n### Structure of the notebook\n1. **Load the data** - In the beginning, training and test sets are loaded. \n2. **Preprocess the data** - The test set is preprocessed using functions contained in this notebook.\n3. **Encode and normalize features** - Data from the sets are standardized and labels are encoded. \n4. **Combine the models and run inference** - The notebook uses nine models to predict the solution.\n5. **Get average of the results** - In the end, the weighted average is calculated (using each model public score as weight) and the result is saved to the submission file.\n\n### Models used\nThe first three models were created by me, the rest were made by other Kagglers. Models were trained in separate notebooks to be able to check the public score of each model. Models include models uses tree-based learning algorithms, simple models like linear regression and deep neural model.\n\n**My models**:\n1. `tab_net_norm` - [TabNet (with normalization)](https://www.kaggle.com/kingakocol/master-tabnet-with-normalization)\n2. `xgb_norm` - [XGB (with normalization)](https://www.kaggle.com/kingakocol/master-xgb-with-normalization)\n3. `lgbm_norm` - [LGBM (with normalization)](https://www.kaggle.com/kingakocol/master-lgbm-with-normalization)\n4. `catboost_norm` - [CatBoost (with normalization)](https://www.kaggle.com/kingakocol/master-catboost-with-normalization)\n\n**Kaggle community models**:\n\n5. `linreg_norm` - [LinearRegression (with normalization)](https://www.kaggle.com/kingakocol/master-linearregression-with-normalization)\n6. `bayridge_norm` - [Bayesian Ridge (with normalization)](https://www.kaggle.com/kingakocol/master-bayesian-ridge-with-normalization)\n7. `gradboost_norm` - [GradientBoostingRegressor (with normalization)](https://www.kaggle.com/kingakocol/gradientboostingregressor-with-normalization)\n8. `lightGBM_norm` - [LightGBM (with normalization)](https://www.kaggle.com/kingakocol/master-lightgbm-with-normalization)\n9. `dnm_norm` - [Deep Neural Model (with normalization)](https://www.kaggle.com/kingakocol/master-deep-neural-model-with-normalization)","metadata":{}},{"cell_type":"markdown","source":"## Loading and preprocessing the data\n\n### Import of libraries\n\nFirstly, I import the necessary libraries.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport tensorflow as tf\nimport joblib\nimport xgboost as xgb\nimport numpy as np\nfrom catboost import Pool, CatBoostRegressor\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import QuantileTransformer\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import BayesianRidge\nfrom sklearn.experimental import enable_hist_gradient_boosting\nfrom sklearn.ensemble import HistGradientBoostingRegressor\nfrom sklearn.model_selection import KFold\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.regularizers import l2\nfrom tensorflow.keras.optimizers import Adamax\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\nfrom tensorflow.keras.layers import SpatialDropout1D\nfrom tensorflow.keras.layers import LeakyReLU, Reshape\nfrom tensorflow.keras.layers import Dropout, Concatenate\nfrom tensorflow.keras.layers import Embedding, Dense, Flatten\nfrom tensorflow.keras.layers import Input, BatchNormalization\nfrom lightgbm import LGBMRegressor\nfrom joblib import Parallel, delayed\nfrom tqdm import tqdm","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-09-25T07:16:29.377017Z","iopub.execute_input":"2021-09-25T07:16:29.377786Z","iopub.status.idle":"2021-09-25T07:16:29.387909Z","shell.execute_reply.started":"2021-09-25T07:16:29.377722Z","shell.execute_reply":"2021-09-25T07:16:29.387082Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Functions used in preprocessing\n\nThis notebook (https://www.kaggle.com/jiashenliu/introduction-to-financial-concepts-and-data) shared by Optiver contains an introduction to financial concepts. The notebook explains how to calculate the instantaneous stock valuation (WAP), how to compare prices of a stock (log returns) and target (realized volatility).\n\nThis section includes functions used to calculate parameters from the data contained in the test and training set, such as wap (weighted average price), log returns or spreads, for aggregation and loading data from files.\n\nI used code from this notebook: https://www.kaggle.com/tensorchoko/optiver-tabnet-beginner.","metadata":{}},{"cell_type":"code","source":"def calc_wap1(df):\n    wap = (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1'])/(df['bid_size1'] + df['ask_size1'])\n    return wap\n\ndef calc_wap2(df):\n    wap = (df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2'])/(df['bid_size2'] + df['ask_size2'])\n    return wap\n\ndef calc_wap3(df):\n    wap = (df['bid_price1'] * df['bid_size1'] + df['ask_price1'] * df['ask_size1'])/(df['bid_size1'] + df['ask_size1'])\n    return wap\n\ndef calc_wap4(df):\n    wap = (df['bid_price2'] * df['bid_size2'] + df['ask_price2'] * df['ask_size2'])/(df['bid_size2'] + df['ask_size2'])\n    return wap\n\ndef log_return(list_stock_prices):\n    return np.log(list_stock_prices).diff() \n\ndef realized_volatility(series_log_return):\n    return np.sqrt(np.sum(series_log_return**2))\n\ndef count_unique(series):\n    return len(np.unique(series))\n\ndef preprocessor_book(file_path):\n    df = pd.read_parquet(file_path)\n\n    df['wap1'] = calc_wap1(df)\n    df['log_return1'] = df.groupby('time_id')['wap1'].apply(log_return)\n    \n    df['wap2'] = calc_wap2(df)\n    df['log_return2'] = df.groupby('time_id')['wap2'].apply(log_return)\n    \n    df['wap3'] = calc_wap3(df)\n    df['log_return3'] = df.groupby('time_id')['wap3'].apply(log_return)\n    \n    df['wap4'] = calc_wap4(df)\n    df['log_return4'] = df.groupby('time_id')['wap4'].apply(log_return)\n    \n    df['wap_avg'] = (df['wap1'] + df['wap2']) / 2\n    df['log_return_avg'] = df.groupby('time_id')['wap_avg'].apply(log_return)\n    \n    df['wap_balance'] = abs(df['wap1'] - df['wap2'])\n    \n    df['price_spread1'] = (df['ask_price1'] - df['bid_price1']) / ((df['ask_price1'] + df['bid_price1'])/2)\n    df['price_spread2'] = (df['ask_price2'] - df['bid_price2']) / ((df['ask_price2'] + df['bid_price2'])/2)\n    \n    df['bid_spread'] = df['bid_price1'] - df['bid_price2']\n    df['ask_spread'] = df['ask_price1'] - df['ask_price2']\n    df['bid_ask_spread'] = abs(df['bid_spread'] - df['ask_spread'])\n    \n    df['total_volume'] = (df['ask_size1'] + df['ask_size2']) + (df['bid_size1'] + df['bid_size2'])\n    df['volume_imbalance'] = abs((df['ask_size1'] + df['ask_size2']) - (df['bid_size1'] + df['bid_size2']))\n    df['invent_liquidity'] = (df['ask_size1'] + df['ask_size2']) / (df['bid_size1'] + df['bid_size2'])\n\n    #dict for aggregate\n    create_feature_dict = {\n        'log_return1':[realized_volatility, np.sum, np.mean, np.std],\n        'log_return2':[realized_volatility, np.sum, np.mean, np.std],\n        'log_return3':[realized_volatility, np.sum, np.mean, np.std],\n        'log_return4':[realized_volatility, np.sum, np.mean, np.std],\n        'log_return_avg':[realized_volatility, np.sum, np.mean, np.std],\n        'wap_balance':[np.sum, np.mean, np.std],\n        'wap_avg':[np.sum, np.mean, np.std],\n        'price_spread1':[np.sum, np.mean, np.std],\n        'price_spread2':[np.sum, np.mean, np.std],\n        'bid_spread':[np.sum, np.mean, np.std],\n        'ask_spread':[np.sum, np.mean, np.std],\n        'bid_ask_spread':[np.sum, np.mean, np.std],\n        'volume_imbalance':[np.sum, np.mean, np.std],\n        'total_volume':[np.sum, np.mean, np.std],\n        'invent_liquidity':[np.sum, np.mean, np.std],\n        'wap1':[np.sum, np.mean, np.std],\n        'wap2':[np.sum, np.mean, np.std],\n        'wap3':[np.sum, np.mean, np.std],\n        'wap4':[np.sum, np.mean, np.std],\n            }\n\n    #groupby / last XX seconds (dla ostatnich 300 sekund - druga połowa przedziału czasu)\n    df_feature = pd.DataFrame(df.groupby(['time_id']).agg(create_feature_dict)).reset_index()\n    \n    df_feature.columns = ['_'.join(col) for col in df_feature.columns] #time_id is changed to time_id_\n     \n    #groupby / last XX seconds (dla ostatnich 300 sekund - druga połowa przedziału czasu)\n    last_seconds = [300]\n    \n    for second in last_seconds:\n        second = 600 - second \n    \n        df_feature_sec = pd.DataFrame(df.query(f'seconds_in_bucket >= {second}').groupby(['time_id']).agg(create_feature_dict)).reset_index()\n\n        df_feature_sec.columns = ['_'.join(col) for col in df_feature_sec.columns] #time_id is changed to time_id_\n     \n        df_feature_sec = df_feature_sec.add_suffix('_' + str(second))\n\n        df_feature = pd.merge(df_feature,df_feature_sec,how='left',left_on='time_id_',right_on=f'time_id__{second}')\n        df_feature = df_feature.drop([f'time_id__{second}'],axis=1)\n    \n    #create row_id\n    stock_id = file_path.split('=')[1]\n    df_feature.insert(loc=0, column='row_id', value=df_feature['time_id_'].apply(lambda x:f'{stock_id}-{x}'))\n    df_feature = df_feature.drop(['time_id_'],axis=1)\n\n    return df_feature\n\ndef preprocessor_trade(file_path):\n    df = pd.read_parquet(file_path)\n    df['log_return'] = df.groupby('time_id')['price'].apply(log_return)\n    \n    aggregate_dictionary = {\n        'log_return':[realized_volatility],\n        'seconds_in_bucket':[count_unique],\n        'size':[np.sum],\n        'order_count':[np.mean],\n    }\n    \n    #groupby / last XX seconds (dla ostatnich 300 sekund - druga połowa przedziału czasu)\n    df_feature = df.groupby('time_id').agg(aggregate_dictionary)\n    \n    df_feature = df_feature.reset_index()\n    df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n    \n    #groupby / last XX seconds (dla ostatnich 300 sekund - druga połowa przedziału czasu)\n    last_seconds = [300]\n    \n    for second in last_seconds:\n        second = 600 - second\n    \n        df_feature_sec = df.query(f'seconds_in_bucket >= {second}').groupby('time_id').agg(aggregate_dictionary)\n        df_feature_sec = df_feature_sec.reset_index()\n        \n        df_feature_sec.columns = ['_'.join(col) for col in df_feature_sec.columns]\n        df_feature_sec = df_feature_sec.add_suffix('_' + str(second))\n        \n        df_feature = pd.merge(df_feature,df_feature_sec,how='left',left_on='time_id_',right_on=f'time_id__{second}')\n        df_feature = df_feature.drop([f'time_id__{second}'],axis=1)\n    \n    df_feature = df_feature.add_prefix('trade_')\n    stock_id = file_path.split('=')[1]\n    df_feature['row_id'] = df_feature['trade_time_id_'].apply(lambda x:f'{stock_id}-{x}')\n    df_feature = df_feature.drop(['trade_time_id_'],axis=1)\n    \n    return df_feature\n\ndef preprocessor(list_stock_ids, is_train = True):\n    df = pd.DataFrame()\n    \n    def for_joblib(stock_id):\n        if is_train:\n            file_path_book = '../input/optiver-realized-volatility-prediction/book_train.parquet/stock_id=' + str(stock_id)\n            file_path_trade = '../input/optiver-realized-volatility-prediction/trade_train.parquet/stock_id=' + str(stock_id)\n        else:\n            file_path_book = '../input/optiver-realized-volatility-prediction/book_test.parquet/stock_id=' + str(stock_id)\n            file_path_trade = '../input/optiver-realized-volatility-prediction/trade_test.parquet/stock_id=' + str(stock_id)\n            \n        df_tmp = pd.merge(preprocessor_book(file_path_book),preprocessor_trade(file_path_trade),on='row_id',how='left')\n     \n        return pd.concat([df,df_tmp])\n    \n    df = Parallel(n_jobs=1, verbose=1)(delayed(for_joblib)(stock_id) for stock_id in list_stock_ids)\n\n    df =  pd.concat(df,ignore_index = True)\n    return df","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-09-25T07:16:29.389808Z","iopub.execute_input":"2021-09-25T07:16:29.390387Z","iopub.status.idle":"2021-09-25T07:16:29.424546Z","shell.execute_reply.started":"2021-09-25T07:16:29.390343Z","shell.execute_reply":"2021-09-25T07:16:29.423395Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Training set\n\nThe training set is loaded from a parquet file. In a separate notebook data from the training set was preprocessed and saved to the file to save time.\n\nThe notebook: https://www.kaggle.com/kingakocol/dataset-optiver.","metadata":{}},{"cell_type":"code","source":"df_train = pd.read_parquet('../input/dataset-optiver/df_train.parquet')\ndf_train_catboost = df_train.copy()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-09-25T07:16:29.426559Z","iopub.execute_input":"2021-09-25T07:16:29.426876Z","iopub.status.idle":"2021-09-25T07:16:30.606397Z","shell.execute_reply.started":"2021-09-25T07:16:29.426844Z","shell.execute_reply":"2021-09-25T07:16:30.605407Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Test set\n\nPreprocessing of the test set.","metadata":{}},{"cell_type":"code","source":"test = pd.read_csv('../input/optiver-realized-volatility-prediction/test.csv')\n\ntest_ids = test.stock_id.unique()\ndf_test = preprocessor(list_stock_ids= test_ids, is_train = False)\ndf_test = test.merge(df_test, on = ['row_id'], how = 'left')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-09-25T07:16:30.607931Z","iopub.execute_input":"2021-09-25T07:16:30.608226Z","iopub.status.idle":"2021-09-25T07:16:30.83516Z","shell.execute_reply.started":"2021-09-25T07:16:30.608177Z","shell.execute_reply":"2021-09-25T07:16:30.834252Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Normalization and label encoding\n\nIn this section, the test set was standardized (all columns except row_id, target, time_id, stock_id) and the columns (time_id, stock_id) were encoded.","metadata":{}},{"cell_type":"code","source":"for col in df_test.columns.to_list()[3:]:\n    df_test[col] = df_test[col].fillna(df_test[col].mean())\n    df_test[col] = df_test[col].fillna(0)\n\ndf_test = df_test.drop(['row_id'], axis = 1)\n\ndf_train[['stock_id', 'time_id']] = df_train['row_id'].str.split('-', expand=True)\ncol = df_train.pop('stock_id')\ndf_train.insert(1, 'stock_id', col)\ncol = df_train.pop('time_id')\ndf_train.insert(2, 'time_id', col)\n\nfor col in df_train.columns.to_list()[4:]:\n    df_train[col] = df_train[col].fillna(df_train[col].mean())","metadata":{"execution":{"iopub.status.busy":"2021-09-25T07:16:30.836793Z","iopub.execute_input":"2021-09-25T07:16:30.837205Z","iopub.status.idle":"2021-09-25T07:16:32.791093Z","shell.execute_reply.started":"2021-09-25T07:16:30.837144Z","shell.execute_reply":"2021-09-25T07:16:32.790224Z"},"_kg_hide-input":true,"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scales = df_train.drop(['row_id', 'target', 'time_id','stock_id'], axis = 1).columns.to_list()\n\nscaler = StandardScaler()\nscaler.fit(df_train[scales])\n\ntest = df_test.drop(['time_id','stock_id'], axis = 1)\ntest = scaler.transform(test)\ntest = pd.DataFrame(test)\ntest = test.set_axis(scales, axis=1, inplace=False)\n\ntest_x = test.copy()\n\ntest['time_id'] = df_test['time_id']\ntest.set_index(test.columns[-1], inplace=True)\ntest.reset_index(inplace=True)\n\ntest['stock_id'] = df_test['stock_id']\ntest.set_index(test.columns[-1], inplace=True)\ntest.reset_index(inplace=True)\n\nle = LabelEncoder()\nle.fit(df_test['stock_id'])\ntest['stock_id'] = le.transform(test['stock_id'])\n\nle = LabelEncoder()\nle.fit(df_test['time_id'])\ntest['time_id'] = le.transform(test['time_id'])\n\ntest_dnm = test.copy()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-09-25T07:16:32.793487Z","iopub.execute_input":"2021-09-25T07:16:32.793916Z","iopub.status.idle":"2021-09-25T07:16:34.97804Z","shell.execute_reply.started":"2021-09-25T07:16:32.79387Z","shell.execute_reply":"2021-09-25T07:16:34.977136Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load the trained models\n### `tab_net_norm`\n\nIn this notebook: https://www.kaggle.com/kingakocol/tabnet-gpu-with-normalization, I trained the TabNet model using Optuna for optimization. The notebook shows different kinds of plots after the model training. The plots enable analyze what parameters were selected during training.","metadata":{}},{"cell_type":"code","source":"model_path = '../input/tabnet-gpu-with-normalization/best_model'\nmodel = tf.keras.models.load_model(model_path)","metadata":{"execution":{"iopub.status.busy":"2021-09-25T07:16:34.979647Z","iopub.execute_input":"2021-09-25T07:16:34.979911Z","iopub.status.idle":"2021-09-25T07:16:41.921236Z","shell.execute_reply.started":"2021-09-25T07:16:34.979885Z","shell.execute_reply":"2021-09-25T07:16:41.92034Z"},"_kg_hide-input":true,"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### `tab_net_norm` inference ","metadata":{}},{"cell_type":"code","source":"col = test.columns.to_list()\n\npreds_tab_net_norm = model.predict(test[col])\npreds_tab_net_norm = preds_tab_net_norm.flatten()","metadata":{"execution":{"iopub.status.busy":"2021-09-25T07:16:41.922962Z","iopub.execute_input":"2021-09-25T07:16:41.923261Z","iopub.status.idle":"2021-09-25T07:16:42.323262Z","shell.execute_reply.started":"2021-09-25T07:16:41.923231Z","shell.execute_reply":"2021-09-25T07:16:42.322302Z"},"_kg_hide-input":true,"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### `xgb_norm`\n\nIn this notebook: https://www.kaggle.com/kingakocol/xgboost-with-normalization-model, I trained the XGBoost model. I created the simple model.","metadata":{}},{"cell_type":"code","source":"model_path = '../input/xgboost-with-normalization-model/xg'\nmodel = joblib.load(model_path)","metadata":{"execution":{"iopub.status.busy":"2021-09-25T07:16:42.3245Z","iopub.execute_input":"2021-09-25T07:16:42.324759Z","iopub.status.idle":"2021-09-25T07:16:42.748417Z","shell.execute_reply.started":"2021-09-25T07:16:42.324733Z","shell.execute_reply":"2021-09-25T07:16:42.747423Z"},"_kg_hide-input":true,"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### `xgb_norm` inference","metadata":{}},{"cell_type":"code","source":"col = test_x.columns.to_list()\npreds_xgb_norm = model.predict(test_x[col])","metadata":{"execution":{"iopub.status.busy":"2021-09-25T07:16:42.750235Z","iopub.execute_input":"2021-09-25T07:16:42.750571Z","iopub.status.idle":"2021-09-25T07:16:42.764649Z","shell.execute_reply.started":"2021-09-25T07:16:42.75054Z","shell.execute_reply":"2021-09-25T07:16:42.763374Z"},"_kg_hide-input":true,"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### `lgbm_norm`\n\nIn this notebook: https://www.kaggle.com/kingakocol/lgbm-with-normalization-model, I trained the LightGBM model.","metadata":{}},{"cell_type":"code","source":"model_path = '../input/lgbm-with-normalization-model/lgbm'\nmodel = joblib.load(model_path)","metadata":{"execution":{"iopub.status.busy":"2021-09-25T07:16:42.766177Z","iopub.execute_input":"2021-09-25T07:16:42.766651Z","iopub.status.idle":"2021-09-25T07:16:42.780474Z","shell.execute_reply.started":"2021-09-25T07:16:42.766601Z","shell.execute_reply":"2021-09-25T07:16:42.779388Z"},"_kg_hide-input":true,"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### `lgbm_norm` inference","metadata":{}},{"cell_type":"code","source":"col = test_x.columns.to_list()\npreds_lgbm_norm = model.predict(test_x[col])","metadata":{"execution":{"iopub.status.busy":"2021-09-25T07:16:42.78196Z","iopub.execute_input":"2021-09-25T07:16:42.78249Z","iopub.status.idle":"2021-09-25T07:16:42.795119Z","shell.execute_reply.started":"2021-09-25T07:16:42.782443Z","shell.execute_reply":"2021-09-25T07:16:42.793997Z"},"_kg_hide-input":true,"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### `catboost_norm`\n\nNext model I used from this notebook: https://www.kaggle.com/sweetjane/catboost. I trained the model in a separate notebook and saved it: https://www.kaggle.com/kingakocol/catboost-with-normalization-model.","metadata":{}},{"cell_type":"code","source":"df_train_catboost[['stock_id', 'time_id']] = df_train_catboost['row_id'].str.split('-', expand=True)\ncol = df_train_catboost.pop('stock_id')\ndf_train_catboost.insert(1, 'stock_id', col)\ncol = df_train_catboost.pop('time_id')\ndf_train_catboost.insert(2, 'time_id', col)","metadata":{"execution":{"iopub.status.busy":"2021-09-25T07:16:42.796651Z","iopub.execute_input":"2021-09-25T07:16:42.797346Z","iopub.status.idle":"2021-09-25T07:16:45.128823Z","shell.execute_reply.started":"2021-09-25T07:16:42.797286Z","shell.execute_reply":"2021-09-25T07:16:45.127899Z"},"_kg_hide-input":true,"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for col in df_train_catboost.columns.to_list()[4:]:\n    df_train_catboost[col] = df_train_catboost[col].fillna(df_train_catboost[col].mean())","metadata":{"execution":{"iopub.status.busy":"2021-09-25T07:16:45.131527Z","iopub.execute_input":"2021-09-25T07:16:45.131788Z","iopub.status.idle":"2021-09-25T07:16:45.50562Z","shell.execute_reply.started":"2021-09-25T07:16:45.131761Z","shell.execute_reply":"2021-09-25T07:16:45.504543Z"},"_kg_hide-input":true,"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scales = df_train_catboost.drop(['row_id', 'target', 'time_id','stock_id'], axis = 1).columns.to_list()\n\nscaler = StandardScaler()\nscaler.fit(df_train_catboost[scales])\n\ntarget = df_train_catboost['target']\ntrain = df_train_catboost.drop(['row_id', 'target', 'time_id','stock_id'], axis = 1)\ntrain = scaler.transform(train)\ntrain = pd.DataFrame(train)\ntrain = train.set_axis(scales, axis=1, inplace=False)\n\ntrain_dnm = train.copy()","metadata":{"execution":{"iopub.status.busy":"2021-09-25T07:16:45.507369Z","iopub.execute_input":"2021-09-25T07:16:45.507647Z","iopub.status.idle":"2021-09-25T07:16:48.807377Z","shell.execute_reply.started":"2021-09-25T07:16:45.50762Z","shell.execute_reply":"2021-09-25T07:16:48.806428Z"},"_kg_hide-input":true,"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### `catboost_norm` inference","metadata":{}},{"cell_type":"code","source":"model_path = '../input/catboost-with-normalization-model/catboost'","metadata":{"execution":{"iopub.status.busy":"2021-09-25T07:16:48.808657Z","iopub.execute_input":"2021-09-25T07:16:48.808932Z","iopub.status.idle":"2021-09-25T07:16:48.812805Z","shell.execute_reply.started":"2021-09-25T07:16:48.808904Z","shell.execute_reply":"2021-09-25T07:16:48.811756Z"},"_kg_hide-input":true,"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds_xgb_catboost = np.zeros(test.shape[0])\n\nkfold = KFold(n_splits = 5, random_state = 66, shuffle = True)\n\nfor fold, (trn_ind, val_ind) in enumerate(kfold.split(df_train)):\n    print(f'Training fold {fold + 1}')\n    \n    test_pool = Pool(test) \n    model = joblib.load(model_path)\n    preds_xgb_catboost += model.predict(test_pool) / 5","metadata":{"execution":{"iopub.status.busy":"2021-09-25T07:16:48.814104Z","iopub.execute_input":"2021-09-25T07:16:48.814424Z","iopub.status.idle":"2021-09-25T07:16:49.303381Z","shell.execute_reply.started":"2021-09-25T07:16:48.814394Z","shell.execute_reply":"2021-09-25T07:16:49.302478Z"},"_kg_hide-input":true,"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next five models I used from this notebook: https://www.kaggle.com/dlaststark/orvp-pulp-fiction/notebook#Base-models.\n\nLinks:\n1. https://www.kaggle.com/kingakocol/linearregression-with-normalization-model\n2. https://www.kaggle.com/kingakocol/bayesian-ridge-with-normalization-model\n3. https://www.kaggle.com/kingakocol/gradientboostingregressor-model\n4. https://www.kaggle.com/kingakocol/lightgbm-with-normalization-model\n5. https://www.kaggle.com/kingakocol/deep-neural-model-with-normalization-model","metadata":{}},{"cell_type":"markdown","source":"### `linreg_norm`,`bayridge_norm`,`gradboost_norm`,`lightGBM_norm`","metadata":{}},{"cell_type":"code","source":"FOLD = 10\nSEEDS = [2018, 2020]\nCOUNTER = 0\n\ny_pred_final_lr = 0\ny_pred_final_ridge = 0\ny_pred_final_gbr = 0\ny_pred_final_lgb = 0","metadata":{"execution":{"iopub.status.busy":"2021-09-25T07:16:49.311497Z","iopub.execute_input":"2021-09-25T07:16:49.311787Z","iopub.status.idle":"2021-09-25T07:16:49.322924Z","shell.execute_reply.started":"2021-09-25T07:16:49.311755Z","shell.execute_reply":"2021-09-25T07:16:49.322003Z"},"_kg_hide-input":true,"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Ytrain_strat = pd.qcut(target, q=10, labels=range(0,10))","metadata":{"execution":{"iopub.status.busy":"2021-09-25T07:16:49.324301Z","iopub.execute_input":"2021-09-25T07:16:49.324615Z","iopub.status.idle":"2021-09-25T07:16:49.399384Z","shell.execute_reply.started":"2021-09-25T07:16:49.324585Z","shell.execute_reply":"2021-09-25T07:16:49.39836Z"},"_kg_hide-input":true,"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### `linreg_norm`,`bayridge_norm`,`gradboost_norm`,`lightGBM_norm` inference","metadata":{}},{"cell_type":"code","source":"model_path_lreg = '../input/linearregression-with-normalization-model/lreg'\nmodel_path_ridge = '../input/bayesian-ridge-with-normalization-model/ridge'\nmodel_path_gbr = '../input/gradientboostingregressor-model/GBR'\nmodel_path_lgb = '../input/lightgbm-with-normalization-model/lightGBM'","metadata":{"execution":{"iopub.status.busy":"2021-09-25T07:16:49.40079Z","iopub.execute_input":"2021-09-25T07:16:49.401092Z","iopub.status.idle":"2021-09-25T07:16:49.405732Z","shell.execute_reply.started":"2021-09-25T07:16:49.401062Z","shell.execute_reply":"2021-09-25T07:16:49.404805Z"},"_kg_hide-input":true,"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for sidx, seed in enumerate(SEEDS):\n\n    kfold = StratifiedKFold(n_splits=FOLD, shuffle=True, random_state=seed)\n\n    for idx, (xtrain, val) in enumerate(kfold.split(train, Ytrain_strat)):\n        COUNTER += 1\n\n        model_lreg = joblib.load(model_path_lreg)\n        y_pred_final_lr += model_lreg.predict(test_x)\n        \n        model_ridge = joblib.load(model_path_ridge)\n        y_pred_final_ridge += model_ridge.predict(test_x)\n        \n        model_gbr = joblib.load(model_path_gbr)\n        y_pred_final_gbr += model_gbr.predict(test_x)\n        \n        model_lgb = joblib.load(model_path_lgb)\n        y_pred_final_lgb += model_lgb.predict(test_x, num_iteration=model.best_iteration_)","metadata":{"execution":{"iopub.status.busy":"2021-09-25T07:16:49.406958Z","iopub.execute_input":"2021-09-25T07:16:49.407224Z","iopub.status.idle":"2021-09-25T07:16:53.856735Z","shell.execute_reply.started":"2021-09-25T07:16:49.407186Z","shell.execute_reply":"2021-09-25T07:16:53.855821Z"},"_kg_hide-input":true,"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred_final_lr = y_pred_final_lr / float(COUNTER)\ny_pred_final_lr = np.array([y_pred_final_lr]).T\n\ny_pred_final_ridge = y_pred_final_ridge / float(COUNTER)\ny_pred_final_ridge = np.array([y_pred_final_ridge]).T\n\ny_pred_final_gbr = y_pred_final_gbr / float(COUNTER)\ny_pred_final_gbr = np.array([y_pred_final_gbr]).T\n\ny_pred_final_lgb = y_pred_final_lgb / float(COUNTER)\ny_pred_final_lgb = np.array([y_pred_final_lgb]).T\n\ny_pred_final_lr = y_pred_final_lr.flatten()\ny_pred_final_ridge = y_pred_final_ridge.flatten()\ny_pred_final_gbr = y_pred_final_gbr.flatten()\ny_pred_final_lgb = y_pred_final_lgb.flatten()","metadata":{"execution":{"iopub.status.busy":"2021-09-25T07:16:53.858097Z","iopub.execute_input":"2021-09-25T07:16:53.858697Z","iopub.status.idle":"2021-09-25T07:16:53.866313Z","shell.execute_reply.started":"2021-09-25T07:16:53.858641Z","shell.execute_reply":"2021-09-25T07:16:53.865326Z"},"_kg_hide-input":true,"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### `dnm_norm`","metadata":{}},{"cell_type":"code","source":"train_dnm['time_id'] = df_train['time_id']\ntrain_dnm.set_index(train_dnm.columns[-1], inplace=True)\ntrain_dnm.reset_index(inplace=True)\n\ntrain_dnm['stock_id'] = df_train['stock_id']\ntrain_dnm.set_index(train.columns[-1], inplace=True)\ntrain_dnm.reset_index(inplace=True)\n\nle = LabelEncoder()\nle.fit(df_train['stock_id'])\ntrain_dnm['stock_id'] = le.transform(train_dnm['stock_id'])\n\nle.fit(df_train['time_id'])\ntrain_dnm['time_id'] = le.transform(train_dnm['time_id'])","metadata":{"execution":{"iopub.status.busy":"2021-09-25T07:16:53.868009Z","iopub.execute_input":"2021-09-25T07:16:53.86898Z","iopub.status.idle":"2021-09-25T07:16:54.407671Z","shell.execute_reply.started":"2021-09-25T07:16:53.868929Z","shell.execute_reply":"2021-09-25T07:16:54.406712Z"},"_kg_hide-input":true,"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cat_cols = ['stock_id','time_id']\n\ntrain_dnm[cat_cols] = train_dnm[cat_cols].astype(int)\ntest[cat_cols] = test[cat_cols].astype(int)\ncat_cols_indices = [train_dnm.columns.get_loc(col) for col in cat_cols]\n\nnum_cols = [col for col in train_dnm.columns if col not in cat_cols]","metadata":{"execution":{"iopub.status.busy":"2021-09-25T07:16:54.408875Z","iopub.execute_input":"2021-09-25T07:16:54.409134Z","iopub.status.idle":"2021-09-25T07:16:54.956979Z","shell.execute_reply.started":"2021-09-25T07:16:54.409107Z","shell.execute_reply":"2021-09-25T07:16:54.956181Z"},"_kg_hide-input":true,"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for col in tqdm(num_cols):\n    transformer = QuantileTransformer(n_quantiles=5000, \n                                      random_state=2020, \n                                      output_distribution=\"normal\")\n    \n    vec_len = len(train_dnm[col].values)\n    vec_len_test = len(test[col].values)\n\n    raw_vec = train_dnm[col].values.reshape(vec_len, 1)\n    test_vec = test_dnm[col].values.reshape(vec_len_test, 1)\n    transformer.fit(raw_vec)\n    \n    train_dnm[col] = transformer.transform(raw_vec).reshape(1, vec_len)[0]\n    test[col] = transformer.transform(test_vec).reshape(1, vec_len_test)[0]\n\nprint(f\"train: {train_dnm.shape} \\ntest: {test.shape}\")","metadata":{"execution":{"iopub.status.busy":"2021-09-25T07:16:54.958177Z","iopub.execute_input":"2021-09-25T07:16:54.95853Z","iopub.status.idle":"2021-09-25T07:17:18.387906Z","shell.execute_reply.started":"2021-09-25T07:16:54.958497Z","shell.execute_reply":"2021-09-25T07:17:18.386888Z"},"_kg_hide-input":true,"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_path = '../input/deep-neural-model-with-normalization-model/dnm'\nmodel = tf.keras.models.load_model(model_path)","metadata":{"execution":{"iopub.status.busy":"2021-09-25T07:17:18.389147Z","iopub.execute_input":"2021-09-25T07:17:18.389477Z","iopub.status.idle":"2021-09-25T07:17:18.393368Z","shell.execute_reply.started":"2021-09-25T07:17:18.389444Z","shell.execute_reply":"2021-09-25T07:17:18.392423Z"},"_kg_hide-input":true,"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"VERBOSE = 0\nBATCH_SIZE = 2048\n\ny_pred_final_dnn = 0\ncounter = 0","metadata":{"execution":{"iopub.status.busy":"2021-09-25T07:17:18.394594Z","iopub.execute_input":"2021-09-25T07:17:18.394859Z","iopub.status.idle":"2021-09-25T07:17:18.406406Z","shell.execute_reply.started":"2021-09-25T07:17:18.394831Z","shell.execute_reply":"2021-09-25T07:17:18.405291Z"},"_kg_hide-input":true,"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### `dnm_norm` inference","metadata":{}},{"cell_type":"code","source":"for sidx, seed in enumerate(SEEDS):\n    seed_score = 0\n    \n    kfold = StratifiedKFold(n_splits=FOLD, shuffle=True, random_state=seed)\n\n    for idx, (xtrain, val) in enumerate(kfold.split(train, Ytrain_strat)):\n        counter += 1\n\n        y_pred_final_dnn += model.predict([[test[col] for col in cat_cols], test[num_cols]], batch_size=BATCH_SIZE)","metadata":{"execution":{"iopub.status.busy":"2021-09-25T07:17:18.407919Z","iopub.execute_input":"2021-09-25T07:17:18.408266Z","iopub.status.idle":"2021-09-25T07:17:18.420983Z","shell.execute_reply.started":"2021-09-25T07:17:18.408232Z","shell.execute_reply":"2021-09-25T07:17:18.419969Z"},"_kg_hide-input":true,"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred_final_dnn = y_pred_final_dnn / float(counter)\ny_pred_final_dnn = y_pred_final_dnn.flatten()","metadata":{"execution":{"iopub.status.busy":"2021-09-25T07:17:18.422564Z","iopub.execute_input":"2021-09-25T07:17:18.42284Z","iopub.status.idle":"2021-09-25T07:17:18.429702Z","shell.execute_reply.started":"2021-09-25T07:17:18.422814Z","shell.execute_reply":"2021-09-25T07:17:18.42859Z"},"_kg_hide-input":true,"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Inference\n\nIn this section, the weighted average is calculated. Thanks to the fact that I had separate notebooks with models, I could get to know their public scores. I used each model public score as weight. The result was saved to the submission file.","metadata":{}},{"cell_type":"code","source":"w_xgb_norm = 1 / 0.27199\nw_tab_net_norm = 1 / 0.26633\nw_lgbm_norm = 1 / 0.29614\nw_catboost_norm = 1 / 0.29050\nw_lr_norm = 1 / 0.23332\nw_ridge_norm = 1 / 0.23339\nw_gbr_norm = 1 / 0.22499\nw_lgb_norm = 1 / 0.22344\nw_dnm_norm = 1 / 0.30256\n\nweight_sum = (w_xgb_norm + w_tab_net_norm + w_lgbm_norm + w_catboost_norm + w_lr_norm + w_ridge_norm\n             + w_gbr_norm + w_lgb_norm + w_dnm_norm)\n\npreds = (preds_xgb_norm * w_xgb_norm + preds_tab_net_norm * w_tab_net_norm + preds_xgb_norm * w_lgbm_norm\n        + preds_xgb_catboost * w_catboost_norm + y_pred_final_lr * w_lr_norm + y_pred_final_ridge * w_ridge_norm\n        + y_pred_final_gbr * w_gbr_norm + y_pred_final_lgb * w_lgb_norm + y_pred_final_dnn * w_dnm_norm) / weight_sum","metadata":{"execution":{"iopub.status.busy":"2021-09-25T07:28:23.732467Z","iopub.execute_input":"2021-09-25T07:28:23.733139Z","iopub.status.idle":"2021-09-25T07:28:23.741434Z","shell.execute_reply.started":"2021-09-25T07:28:23.733087Z","shell.execute_reply":"2021-09-25T07:28:23.740412Z"},"_kg_hide-input":true,"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub = pd.read_csv('../input/optiver-realized-volatility-prediction/sample_submission.csv')\nsub.target = preds\n\nsub.to_csv('submission.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2021-09-25T07:17:18.5355Z","iopub.execute_input":"2021-09-25T07:17:18.535949Z","iopub.status.idle":"2021-09-25T07:17:18.550176Z","shell.execute_reply.started":"2021-09-25T07:17:18.535919Z","shell.execute_reply":"2021-09-25T07:17:18.54923Z"},"_kg_hide-input":true,"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]}]}