{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport gc\nimport glob\n\nimport numpy as np \nimport pandas as pd \n\nfrom itertools import islice\n\nfrom multiprocessing import Pool\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\n\nimport tensorflow as tf\nimport keras.backend as K\n\nfrom tensorflow.keras.layers import Dense, Lambda, Dot, Activation, Concatenate\nfrom tensorflow.keras.layers import Layer\n\nfrom sklearn.model_selection import train_test_split\n\nfrom tqdm.auto import tqdm\ntqdm.pandas()\n\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-09-07T07:51:05.419818Z","iopub.execute_input":"2021-09-07T07:51:05.420631Z","iopub.status.idle":"2021-09-07T07:51:12.251342Z","shell.execute_reply.started":"2021-09-07T07:51:05.420488Z","shell.execute_reply":"2021-09-07T07:51:12.250086Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"NTHREADS = 4\nSEED = 42\nTRAIN_BATCH_SIZE = 256\nTEST_BATCH_SIZE = 256\nBUCKET_WINDOWS2 = [(0, 100), (100, 200), (200, 300), (300, 400), (400, 500), (500, 600)]\n\nDATA_PATH = '../input/optiver-realized-volatility-prediction'\nBOOK_TRAIN_PATH = '../input/optiver-realized-volatility-prediction/book_train.parquet'\nTRADE_TRAIN_PATH = '../input/optiver-realized-volatility-prediction/trade_train.parquet'\nBOOK_TEST_PATH = '../input/optiver-realized-volatility-prediction/book_test.parquet'\nTRADE_TEST_PATH = '../input/optiver-realized-volatility-prediction/trade_test.parquet'\nCHECKPOINT = './model_checkpoint/model_01'\n\nbook_skip_columns = trade_skip_columns = ['time_id', 'row_id', 'target']","metadata":{"execution":{"iopub.status.busy":"2021-09-07T07:51:12.253291Z","iopub.execute_input":"2021-09-07T07:51:12.253701Z","iopub.status.idle":"2021-09-07T07:51:12.261404Z","shell.execute_reply.started":"2021-09-07T07:51:12.253654Z","shell.execute_reply":"2021-09-07T07:51:12.260447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_path_dict(f, v):\n\n    f_dict = {}\n    for i in tqdm(v):\n        fpath = f'{f}/stock_id={i}'\n        flist = glob.glob(os.path.join(fpath, '*.parquet'))\n    \n        if len(flist) > 0:\n            f_dict[i] = flist[0]\n    \n    return f_dict","metadata":{"execution":{"iopub.status.busy":"2021-09-07T07:51:12.263397Z","iopub.execute_input":"2021-09-07T07:51:12.263741Z","iopub.status.idle":"2021-09-07T07:51:12.275191Z","shell.execute_reply.started":"2021-09-07T07:51:12.263708Z","shell.execute_reply":"2021-09-07T07:51:12.274221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_ds = pd.read_csv(os.path.join(DATA_PATH, 'train.csv'))\ntest_ds = pd.read_csv(os.path.join(DATA_PATH, 'test.csv'))\nprint(f'Train ds shape: {train_ds.shape}')\nprint(f'Test ds shape: {test_ds.shape}')\ntrain_ds['row_id'] = train_ds['stock_id'].astype(str) + '-' + train_ds['time_id'].astype(str)","metadata":{"execution":{"iopub.status.busy":"2021-09-07T07:51:12.276803Z","iopub.execute_input":"2021-09-07T07:51:12.277106Z","iopub.status.idle":"2021-09-07T07:51:13.904378Z","shell.execute_reply.started":"2021-09-07T07:51:12.277075Z","shell.execute_reply":"2021-09-07T07:51:13.903096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"book_train_dict = get_path_dict(BOOK_TRAIN_PATH, train_ds['stock_id'].unique())\ntrade_train_dict = get_path_dict(TRADE_TRAIN_PATH, train_ds['stock_id'].unique())\n\nbook_test_dict = get_path_dict(BOOK_TEST_PATH, test_ds['stock_id'].unique())\ntrade_test_dict = get_path_dict(TRADE_TEST_PATH, test_ds['stock_id'].unique())","metadata":{"execution":{"iopub.status.busy":"2021-09-07T07:51:13.906284Z","iopub.execute_input":"2021-09-07T07:51:13.906746Z","iopub.status.idle":"2021-09-07T07:51:14.554829Z","shell.execute_reply.started":"2021-09-07T07:51:13.906691Z","shell.execute_reply":"2021-09-07T07:51:14.55383Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def calc_wap1(df):\n    # Function to calculate first WAP\n    wap = (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']) / (df['bid_size1'] + df['ask_size1'])\n    return wap\n\ndef calc_wap2(df):\n    # Function to calculate second WAP\n    wap = (df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2']) / (df['bid_size2'] + df['ask_size2'])\n    return wap\n\ndef calc_wap3(df):\n    wap = (df['bid_price1'] * df['bid_size1'] + df['ask_price1'] * df['ask_size1']) / (df['bid_size1'] + df['ask_size1'])\n    return wap\n\ndef calc_wap4(df):\n    wap = (df['bid_price2'] * df['bid_size2'] + df['ask_price2'] * df['ask_size2']) / (df['bid_size2'] + df['ask_size2'])\n    return wap\n\ndef calc_ma(df, colname, window_size):\n    a = df[colname].rolling(window=window_size).mean()\n    return a\n\ndef calc_mstd(df, colname, window_size):\n    a = df[colname].rolling(window=window_size).std()\n    return a\n\ndef calc_memw(df, colname, window_size):\n    a = df[colname].ewm(span=10).mean()\n    return a\n\ndef log_return(series):\n    # Function to calculate the log of the return\n    return np.log(series).diff()\n\ndef realized_volatility(series):\n    # Calculate the realized volatility\n    return np.sqrt(np.sum(series**2))\n\ndef count_unique(series):\n    # Function to count unique elements of a series\n    return len(np.unique(series))\n\ndef book_ds_fe(df):\n    # Calculate Wap\n    df['wap1'] = calc_wap1(df)\n    df['wap2'] = calc_wap2(df)\n    \n    # Calculate log returns\n    df['log_return1'] = df.groupby(['time_id'])['wap1'].apply(log_return)\n    df['log_return2'] = df.groupby(['time_id'])['wap2'].apply(log_return)\n    \n    # Calculate wap balance\n    df['wap_balance'] = abs(df['wap1'] - df['wap2'])\n    \n    # Calculate spread\n    df['price_spread'] = (df['ask_price1'] - df['bid_price1']) / ((df['ask_price1'] + df['bid_price1']) / 2)\n    df['price_spread2'] = (df['ask_price2'] - df['bid_price2']) / ((df['ask_price2'] + df['bid_price2']) / 2)\n    \n    df['bid_spread'] = df['bid_price1'] - df['bid_price2']\n    df['ask_spread'] = df['ask_price1'] - df['ask_price2']\n    df[\"bid_ask_spread\"] = abs(df['bid_spread'] - df['ask_spread'])\n    \n    df['total_volume'] = (df['ask_size1'] + df['ask_size2']) + (df['bid_size1'] + df['bid_size2'])\n    df['volume_imbalance'] = abs((df['ask_size1'] + df['ask_size2']) - (df['bid_size1'] + df['bid_size2']))\n    \n    #####\n    win_size = 10\n    bid_price1_ma = calc_ma(df, 'bid_price1', win_size)\n    ask_size1_ma = calc_ma(df, 'ask_size1', win_size)\n    ask_price1_ma = calc_ma(df, 'ask_price1', win_size)\n    bid_size1_ma = calc_ma(df, 'bid_size1', win_size)\n    \n    bid_price2_ma = calc_ma(df, 'bid_price2', win_size)\n    ask_size2_ma = calc_ma(df, 'ask_size2', win_size)\n    ask_price2_ma = calc_ma(df, 'ask_price2', win_size)\n    bid_size2_ma = calc_ma(df, 'bid_size2', win_size)\n    \n    df['wap1_ma'] = (bid_price1_ma * ask_size1_ma + ask_price1_ma * bid_size1_ma) / (bid_size1_ma + ask_size1_ma)\n    df['wap2_ma'] = (bid_price2_ma * ask_size2_ma + ask_price2_ma * bid_size2_ma) / (bid_size2_ma + ask_size2_ma)\n    \n    return df\n\ndef trade_ds_fe(df):\n    df['log_return'] = df.groupby('time_id')['price'].apply(log_return)\n    df['amount'] = df['price'] * df['size']\n    return df\n","metadata":{"execution":{"iopub.status.busy":"2021-09-07T07:51:14.556295Z","iopub.execute_input":"2021-09-07T07:51:14.556603Z","iopub.status.idle":"2021-09-07T07:51:14.5801Z","shell.execute_reply.started":"2021-09-07T07:51:14.55657Z","shell.execute_reply":"2021-09-07T07:51:14.579108Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import librosa\n\n''' MFCC coefficients contain information about the rate changes in the different spectrum bands '''\ndef get_mfcc(a):\n    r = np.zeros((1, a.shape[1]))\n    for i in range(a.shape[1]):\n        mfcc = librosa.feature.mfcc(a[:, i])\n        mfcc_mean = mfcc.mean(axis=1)\n        #print(mfcc_mean)\n        #r[:, i] = np.mean(mfcc_mean)\n        r[:, i] = mfcc_mean[1]\n    return r","metadata":{"execution":{"iopub.status.busy":"2021-09-07T07:51:14.581676Z","iopub.execute_input":"2021-09-07T07:51:14.582187Z","iopub.status.idle":"2021-09-07T07:51:16.266185Z","shell.execute_reply.started":"2021-09-07T07:51:14.582145Z","shell.execute_reply":"2021-09-07T07:51:16.264844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tsfresh.feature_extraction import feature_calculators\n\n''' Number of peaks '''\ndef get_number_peaks(a):\n    r = np.zeros((1, a.shape[1]))\n    for i in range(a.shape[1]):\n        r[:, i] = feature_calculators.number_peaks(a[:, i], 2)\n    return r","metadata":{"execution":{"iopub.status.busy":"2021-09-07T07:51:16.26917Z","iopub.execute_input":"2021-09-07T07:51:16.269518Z","iopub.status.idle":"2021-09-07T07:51:17.599292Z","shell.execute_reply.started":"2021-09-07T07:51:16.269481Z","shell.execute_reply":"2021-09-07T07:51:17.598061Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def np_seq_stat(a, s):\n    ''' a - array, s - seconds_in_bucket'''\n    \n    r = []\n    for w in BUCKET_WINDOWS2:\n        \n        idx = np.where(np.logical_and(s >= w[0], s < w[1]))[0]\n       \n        s_min = np.zeros((1, a.shape[1]))\n        s_max = np.zeros((1, a.shape[1]))\n        s_mean = np.zeros((1, a.shape[1]))\n        s_std = np.zeros((1, a.shape[1]))\n        s_median = np.zeros((1, a.shape[1]))\n        s_sum = np.zeros((1, a.shape[1]))\n        #s_mfcc = np.zeros((1, a.shape[1]))\n        #s_peaks = np.zeros((1, a.shape[1]))\n        \n        if a[idx].shape[0] > 0:\n            s_min = np.min(a[idx], axis=0, keepdims=True)\n            s_max = np.max(a[idx], axis=0, keepdims=True)\n            s_mean = np.mean(a[idx], axis=0, keepdims=True)\n            s_std = np.std(a[idx], axis=0, keepdims=True)\n            s_median = np.median(a[idx], axis=0, keepdims=True)\n            s_sum = np.sum(a[idx], axis=0, keepdims=True)\n            \n            #s_peaks = get_number_peaks(a[idx]) # <- it gives small boost\n            #s_mfcc = get_mfcc(a[idx])\n            \n        r.append(np.concatenate((s_min, s_max, s_mean, s_std, s_median, s_sum), axis=0))\n        \n    return np.nan_to_num(np.concatenate(r, axis=0).transpose())","metadata":{"execution":{"iopub.status.busy":"2021-09-07T07:51:17.600959Z","iopub.execute_input":"2021-09-07T07:51:17.601285Z","iopub.status.idle":"2021-09-07T07:51:17.615358Z","shell.execute_reply.started":"2021-09-07T07:51:17.601252Z","shell.execute_reply":"2021-09-07T07:51:17.614061Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def process_optiver_ds(ds, f_dict, fe_func, skip_cols, train_flg=True):\n    \n    x = []\n    y = []\n    \n    for stock_id, stock_fnmame in tqdm(f_dict.items()):\n\n        optiver_ds = pd.read_parquet(stock_fnmame)\n        optiver_ds['row_id'] = str(stock_id) + '-' + optiver_ds['time_id'].astype(str)\n\n        sds = ds[ds['stock_id'] == stock_id]\n\n        cols = ['time_id', 'target']\n        if train_flg == False:\n            cols = ['time_id']\n            \n        merge_ds = pd.merge(sds[cols], optiver_ds, on='time_id', how='left')\n        merge_ds = fe_func(merge_ds).fillna(0)\n        \n        cols = [c for c in merge_ds.columns if c not in skip_cols]\n\n        np_ds = merge_ds[cols].to_numpy(dtype=np.float16)\n        seconds_in_bucket = merge_ds['seconds_in_bucket'].to_numpy()\n        g_idx = merge_ds[['time_id']].to_numpy()\n        \n        l = np.unique(g_idx, return_index=True)[1][1:]        \n        a_list = np.split(np_ds, l)\n        s_list = np.split(seconds_in_bucket, l)\n\n        stat = list(map(np_seq_stat, a_list, s_list))\n        b = np.transpose(np.dstack(stat), (2, 1, 0))\n        b = b.astype(np.float16)\n        \n        r = []\n        if train_flg:\n            targets = merge_ds[['target']].to_numpy(dtype=np.float16)\n            t_list = np.split(targets, l)\n            r = [t[0][0] for t in t_list]\n        \n        x.append(b)\n        y.append(r)\n        #break\n    return x, y","metadata":{"execution":{"iopub.status.busy":"2021-09-07T07:51:17.617021Z","iopub.execute_input":"2021-09-07T07:51:17.617353Z","iopub.status.idle":"2021-09-07T07:51:17.631882Z","shell.execute_reply.started":"2021-09-07T07:51:17.617316Z","shell.execute_reply":"2021-09-07T07:51:17.63041Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def chunks(data, SIZE=10000):\n    it = iter(data)\n    for i in range(0, len(data), SIZE):\n        yield {k:data[k] for k in islice(it, SIZE)}\n        \ndef process_book_train_chunk(chunk_ds):\n    return process_optiver_ds(train_ds, chunk_ds, book_ds_fe, book_skip_columns)\ndef process_trade_train_chunk(chunk_ds):\n    return process_optiver_ds(train_ds, chunk_ds, trade_ds_fe, trade_skip_columns)\ndef process_book_test_chunk(chunk_ds):\n    return process_optiver_ds(test_ds, chunk_ds, book_ds_fe, book_skip_columns, False)\ndef process_trade_test_chunk(chunk_ds):\n    return process_optiver_ds(test_ds, chunk_ds, trade_ds_fe, trade_skip_columns, False)\n\nbook_train_chunks = [i for i in chunks(book_train_dict, int(len(book_train_dict)/NTHREADS))]\ntrade_train_chunks = [i for i in chunks(trade_train_dict, int(len(trade_train_dict)/NTHREADS))]\n\nz = 1 if len(book_test_dict) < NTHREADS else NTHREADS\nbook_test_chunks = [i for i in chunks(book_test_dict, int(len(book_test_dict)/z))]\ntrade_test_chunks = [i for i in chunks(trade_test_dict, int(len(trade_test_dict)/z))]","metadata":{"execution":{"iopub.status.busy":"2021-09-07T07:51:17.63369Z","iopub.execute_input":"2021-09-07T07:51:17.63407Z","iopub.status.idle":"2021-09-07T07:51:17.650089Z","shell.execute_reply.started":"2021-09-07T07:51:17.634035Z","shell.execute_reply":"2021-09-07T07:51:17.648996Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\npool = Pool(NTHREADS)\nr = pool.map(process_book_train_chunk, book_train_chunks)\npool.close()\n\na1, a2 = zip(*r)\nnp_books = [np.concatenate(a1[i], axis=0) for i in range(len(a1))]\nnp_books = np.concatenate(np_books, axis=0)\n\ntargets = [np.concatenate(a2[i], axis=0) for i in range(len(a2))]\ntargets = np.concatenate(targets, axis=0)","metadata":{"execution":{"iopub.status.busy":"2021-09-07T07:51:17.651996Z","iopub.execute_input":"2021-09-07T07:51:17.652876Z","iopub.status.idle":"2021-09-07T08:24:04.657259Z","shell.execute_reply.started":"2021-09-07T07:51:17.652745Z","shell.execute_reply":"2021-09-07T08:24:04.655388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\npool = Pool(NTHREADS)\nr = pool.map(process_trade_train_chunk, trade_train_chunks)\npool.close()\n\na1, _ = zip(*r)\nnp_trades = [np.concatenate(a1[i], axis=0) for i in range(len(a1))]\nnp_trades = np.concatenate(np_trades, axis=0)","metadata":{"execution":{"iopub.status.busy":"2021-09-07T08:24:04.661313Z","iopub.execute_input":"2021-09-07T08:24:04.661813Z","iopub.status.idle":"2021-09-07T08:33:41.570391Z","shell.execute_reply.started":"2021-09-07T08:24:04.661746Z","shell.execute_reply":"2021-09-07T08:33:41.569082Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(np_books.shape, np_trades.shape, targets.shape)\nnp_train = np.concatenate((np_books, np_trades), axis=2)\nprint(np_train.shape, targets.shape)","metadata":{"execution":{"iopub.status.busy":"2021-09-07T08:45:05.581035Z","iopub.execute_input":"2021-09-07T08:45:05.581659Z","iopub.status.idle":"2021-09-07T08:45:06.264892Z","shell.execute_reply.started":"2021-09-07T08:45:05.581589Z","shell.execute_reply":"2021-09-07T08:45:06.263911Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"idx = np.arange(np_train.shape[0])\ntrain_idx, valid_idx = train_test_split(idx, shuffle=False, test_size=0.1, random_state=SEED)","metadata":{"execution":{"iopub.status.busy":"2021-09-07T08:45:06.514396Z","iopub.execute_input":"2021-09-07T08:45:06.515273Z","iopub.status.idle":"2021-09-07T08:45:06.556019Z","shell.execute_reply.started":"2021-09-07T08:45:06.515161Z","shell.execute_reply":"2021-09-07T08:45:06.554338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Scaler\ntransformers = []\nfor i in tqdm(range(np_train.shape[1])):\n    a = np.nan_to_num(np_train[train_idx, i, :])\n    b = np.nan_to_num(np_train[valid_idx, i, :])\n\n    transformer = StandardScaler() # StandardScaler is very useful!\n    np_train[train_idx, i, :] = transformer.fit_transform(a)\n    np_train[valid_idx, i, :] = transformer.transform(b)\n    transformers.append(transformer) # Save Scalers for the inference stage","metadata":{"execution":{"iopub.status.busy":"2021-09-07T08:45:36.399366Z","iopub.execute_input":"2021-09-07T08:45:36.399972Z","iopub.status.idle":"2021-09-07T08:46:56.770128Z","shell.execute_reply.started":"2021-09-07T08:45:36.399918Z","shell.execute_reply":"2021-09-07T08:46:56.769059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np_train = np.nan_to_num(np_train)","metadata":{"execution":{"iopub.status.busy":"2021-09-07T08:47:04.233403Z","iopub.execute_input":"2021-09-07T08:47:04.233901Z","iopub.status.idle":"2021-09-07T08:47:12.392999Z","shell.execute_reply.started":"2021-09-07T08:47:04.233854Z","shell.execute_reply":"2021-09-07T08:47:12.391846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Loss function\ndef rmspe(y_true, y_pred):\n    return K.sqrt(K.mean(K.square((y_true - y_pred) / y_true)))","metadata":{"execution":{"iopub.status.busy":"2021-09-07T08:47:14.445464Z","iopub.execute_input":"2021-09-07T08:47:14.445946Z","iopub.status.idle":"2021-09-07T08:47:14.452929Z","shell.execute_reply.started":"2021-09-07T08:47:14.445903Z","shell.execute_reply":"2021-09-07T08:47:14.450632Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# https://github.com/philipperemy/keras-attention-mechanism\nclass Attention(Layer):\n\n    def __init__(self, units=128, **kwargs):\n        self.units = units\n        super().__init__(**kwargs)\n\n    def __call__(self, inputs):\n        \"\"\"\n        Many-to-one attention mechanism for Keras.\n        @param inputs: 3D tensor with shape (batch_size, time_steps, input_dim).\n        @return: 2D tensor with shape (batch_size, 128)\n        @author: felixhao28, philipperemy.\n        \"\"\"\n        hidden_states = inputs\n        hidden_size = int(hidden_states.shape[2])\n        # Inside dense layer\n        #              hidden_states            dot               W            =>           score_first_part\n        # (batch_size, time_steps, hidden_size) dot (hidden_size, hidden_size) => (batch_size, time_steps, hidden_size)\n        # W is the trainable weight matrix of attention Luong's multiplicative style score\n        score_first_part = Dense(hidden_size, use_bias=False, name='attention_score_vec')(hidden_states)\n        #            score_first_part           dot        last_hidden_state     => attention_weights\n        # (batch_size, time_steps, hidden_size) dot   (batch_size, hidden_size)  => (batch_size, time_steps)\n        h_t = Lambda(lambda x: x[:, -1, :], output_shape=(hidden_size,), name='last_hidden_state')(hidden_states)\n        score = Dot(axes=[1, 2], name='attention_score')([h_t, score_first_part])\n        attention_weights = Activation('softmax', name='attention_weight')(score)\n        # (batch_size, time_steps, hidden_size) dot (batch_size, time_steps) => (batch_size, hidden_size)\n        context_vector = Dot(axes=[1, 1], name='context_vector')([hidden_states, attention_weights])\n        pre_activation = Concatenate(name='attention_output')([context_vector, h_t])\n        attention_vector = Dense(self.units, use_bias=False, activation='tanh', name='attention_vector')(pre_activation)\n        return attention_vector\n\n    def get_config(self):\n        return {'units': self.units}\n\n    @classmethod\n    def from_config(cls, config):\n        return cls(**config)","metadata":{"execution":{"iopub.status.busy":"2021-09-07T08:47:18.361756Z","iopub.execute_input":"2021-09-07T08:47:18.362212Z","iopub.status.idle":"2021-09-07T08:47:18.375284Z","shell.execute_reply.started":"2021-09-07T08:47:18.362167Z","shell.execute_reply":"2021-09-07T08:47:18.374401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class DataGenerator(tf.keras.utils.Sequence):\n    'Generates data for Keras'\n    def __init__(self, ds, targets, batch_size, shape=(32,32,32), shuffle=True):\n        'Initialization'\n        self.batch_size = batch_size\n        self.targets = targets\n        self.shape = shape\n        self.ds = ds\n        self.ids = np.arange(ds.shape[0])\n        self.shuffle = shuffle\n        self.on_epoch_end()\n\n    def __len__(self):\n        'Denotes the number of batches per epoch'\n        return int(np.floor(len(self.ids) / self.batch_size))\n\n    def __getitem__(self, index):\n        \n        'Generate one batch of data'\n        # Generate indexes of the batch\n        indexes = self.ids[index*self.batch_size:(index+1)*self.batch_size]\n\n        # Find list of IDs\n        ids_temp = [self.ids[k] for k in indexes]\n\n\n        x = self.ds[ids_temp, :, :]\n        y = self.targets[ids_temp]\n        \n        return x, y\n\n    def on_epoch_end(self):\n        'Updates indexes after each epoch'\n        self.ids = np.arange(self.ds.shape[0])\n        if self.shuffle == True:\n            np.random.shuffle(self.ids)","metadata":{"execution":{"iopub.status.busy":"2021-09-07T08:47:19.030449Z","iopub.execute_input":"2021-09-07T08:47:19.031047Z","iopub.status.idle":"2021-09-07T08:47:19.041474Z","shell.execute_reply.started":"2021-09-07T08:47:19.031009Z","shell.execute_reply":"2021-09-07T08:47:19.04028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_model_v1():\n    model = tf.keras.models.Sequential()\n    model.add(tf.keras.layers.LSTM(50, input_shape=(np_train.shape[1], np_train.shape[2]), return_sequences=True))\n    model.add(tf.keras.layers.LSTM(50, input_shape=(np_train.shape[1], np_train.shape[2]), return_sequences=True))\n    model.add(Attention(256)) # the gain is small, but ...\n    model.add(tf.keras.layers.Dense(1))\n\n    model.compile(loss=rmspe, optimizer='adam')\n    model.summary()\n    return model","metadata":{"execution":{"iopub.status.busy":"2021-09-07T08:47:19.668295Z","iopub.execute_input":"2021-09-07T08:47:19.66871Z","iopub.status.idle":"2021-09-07T08:47:19.677447Z","shell.execute_reply.started":"2021-09-07T08:47:19.668674Z","shell.execute_reply":"2021-09-07T08:47:19.676543Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_generator = DataGenerator(np_train[train_idx, :, :], targets[train_idx], batch_size=TRAIN_BATCH_SIZE)\nvalidation_generator = DataGenerator(np_train[valid_idx, :, :], targets[valid_idx], batch_size=TRAIN_BATCH_SIZE)\n\nmodel = get_model_v1()\n\ncheckpoint_filepath = CHECKPOINT\nmodel_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n    filepath=checkpoint_filepath,\n    save_weights_only=True,\n    monitor='val_loss',\n    mode='min',\n    save_best_only=True)\n\nmodel_earlystopping_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=20)\n\nNEPOCHS = 100\nhistory = model.fit_generator(generator=training_generator, \n                              callbacks=[model_checkpoint_callback, model_earlystopping_callback], \n                              epochs=NEPOCHS, \n                              validation_data=validation_generator, \n                              use_multiprocessing=False, \n                              workers=NTHREADS)","metadata":{"execution":{"iopub.status.busy":"2021-09-07T08:47:22.176563Z","iopub.execute_input":"2021-09-07T08:47:22.177245Z","iopub.status.idle":"2021-09-07T08:47:32.996414Z","shell.execute_reply.started":"2021-09-07T08:47:22.177202Z","shell.execute_reply":"2021-09-07T08:47:32.993982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"a = np.min(history.history['val_loss'])\nprint(f'The best val_loss is {a:.4f}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del np_train, np_books, np_trades\nz = gc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\npool = Pool(NTHREADS)\nr = pool.map(process_book_test_chunk, book_test_chunks)\npool.close()\n\na1, _ = zip(*r)\nnp_books = [np.concatenate(a1[i], axis=0) for i in range(len(a1))]\nnp_books = np.concatenate(np_books, axis=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\npool = Pool(NTHREADS)\nr = pool.map(process_trade_test_chunk, trade_test_chunks)\npool.close()\n\na1, _ = zip(*r)\nnp_trades = [np.concatenate(a1[i], axis=0) for i in range(len(a1))]\nnp_trades = np.concatenate(np_trades, axis=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(np_books.shape, np_trades.shape)\nnp_test = np.concatenate((np_books, np_trades), axis=2)\nprint(np_test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Scaler\nfor i in tqdm(range(np_test.shape[1])):\n    transformer = transformers[i]\n    np_test[:, i, :] = transformer.transform(np.nan_to_num(np_test[:, i, :]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np_test = np.nan_to_num(np_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.load_weights(checkpoint_filepath)\nres = model.predict(np_test, batch_size=TEST_BATCH_SIZE)\nres = np.clip(res, 0, 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"res[:3]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import shutil\nshutil.rmtree('./model_checkpoint')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_ds = pd.DataFrame()\nsubmission_ds['row_id'] = test_ds['row_id']\nsubmission_ds['target'] = res\nsubmission_ds.to_csv('submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_ds[:3]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}