{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# I ‚ù§ parquet\n\nParquet is my favorite file format for large amounts of columnar data.  Here are a few patterns to help you get started!  The [Apache parquet docs](https://arrow.apache.org/docs/python/parquet.html) are also helpful.","metadata":{"execution":{"iopub.status.busy":"2021-06-29T01:23:52.468611Z","iopub.execute_input":"2021-06-29T01:23:52.469124Z","iopub.status.idle":"2021-06-29T01:23:52.473588Z","shell.execute_reply.started":"2021-06-29T01:23:52.469084Z","shell.execute_reply":"2021-06-29T01:23:52.472088Z"}}},{"cell_type":"code","source":"import pyarrow as pa\nimport pyarrow.parquet as pq","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-29T02:40:46.342618Z","iopub.execute_input":"2021-06-29T02:40:46.343403Z","iopub.status.idle":"2021-06-29T02:40:46.353169Z","shell.execute_reply.started":"2021-06-29T02:40:46.343257Z","shell.execute_reply":"2021-06-29T02:40:46.352268Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"When reading parquet files I like to use `pq.read_table`.  You can also use `pd.read_table` as shown in the [competition introduction notebook](https://www.kaggle.com/jiashenliu/introduction-to-financial-concepts-and-data]).","metadata":{"execution":{"iopub.status.busy":"2021-06-29T01:29:39.381068Z","iopub.execute_input":"2021-06-29T01:29:39.381383Z","iopub.status.idle":"2021-06-29T01:29:39.386673Z","shell.execute_reply.started":"2021-06-29T01:29:39.381356Z","shell.execute_reply":"2021-06-29T01:29:39.385531Z"}}},{"cell_type":"code","source":"table = pq.read_table('../input/optiver-realized-volatility-prediction/book_train.parquet/stock_id=0/c439ef22282f412ba39e9137a3fdabac.parquet')\ndf = table.to_pandas()\ndf.head()\n","metadata":{"execution":{"iopub.status.busy":"2021-06-29T02:40:46.354458Z","iopub.execute_input":"2021-06-29T02:40:46.35478Z","iopub.status.idle":"2021-06-29T02:40:46.988229Z","shell.execute_reply.started":"2021-06-29T02:40:46.354751Z","shell.execute_reply":"2021-06-29T02:40:46.987197Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The competition has been structured with *partitioned* datasets.  That means the value for `stock_id` is in the path and not in the file.  Its called the *partition key*.  The `pyarrow.parquet` library has a `ParquetDataset` that takes advantage of partitions.","metadata":{"execution":{"iopub.status.busy":"2021-06-29T01:25:57.918877Z","iopub.execute_input":"2021-06-29T01:25:57.919218Z","iopub.status.idle":"2021-06-29T01:25:57.925411Z","shell.execute_reply.started":"2021-06-29T01:25:57.919189Z","shell.execute_reply":"2021-06-29T01:25:57.924066Z"}}},{"cell_type":"code","source":"%%time\ndataset = pq.ParquetDataset('../input/optiver-realized-volatility-prediction/trade_train.parquet/')  \ntable = dataset.read()\ntrades = table.to_pandas()\ntrades.info()","metadata":{"execution":{"iopub.status.busy":"2021-06-29T02:40:46.990212Z","iopub.execute_input":"2021-06-29T02:40:46.990655Z","iopub.status.idle":"2021-06-29T02:40:52.674309Z","shell.execute_reply.started":"2021-06-29T02:40:46.990613Z","shell.execute_reply":"2021-06-29T02:40:52.673392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We just loaded ALL the trades for every stock!  The *partition key* became the categorical column `stock_id` automatically! You can do this for the book data too.","metadata":{}},{"cell_type":"code","source":"dataset = pq.ParquetDataset('../input/optiver-realized-volatility-prediction/book_train.parquet/') \nbooks = dataset.read()\nbooks = books.to_pandas()  # I overwrite the pyarrow table object here to save memory\nbooks.info()","metadata":{"execution":{"iopub.status.busy":"2021-06-29T02:40:52.675932Z","iopub.execute_input":"2021-06-29T02:40:52.676247Z","iopub.status.idle":"2021-06-29T02:41:23.566534Z","shell.execute_reply.started":"2021-06-29T02:40:52.676219Z","shell.execute_reply":"2021-06-29T02:41:23.565643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'Found {books.time_id.nunique()} unique time ids')\nprint(f'Found {books.stock_id.nunique()} unique stock ids.')","metadata":{"execution":{"iopub.status.busy":"2021-06-29T02:41:23.56764Z","iopub.execute_input":"2021-06-29T02:41:23.567893Z","iopub.status.idle":"2021-06-29T02:41:29.330672Z","shell.execute_reply.started":"2021-06-29T02:41:23.567867Z","shell.execute_reply":"2021-06-29T02:41:29.32954Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"My kernel shows 7 out of 16GB used which might cause problems when you start unleashing your pandas magic.  One way to save memory, which you already know, is to load one stock at a time.  But you might not know there's a way to get a single stock using the `filters` option!","metadata":{}},{"cell_type":"code","source":"dataset = pq.ParquetDataset('../input/optiver-realized-volatility-prediction/book_train.parquet/', \n                            filters =[('stock_id', '=', '5')]) \ntable = dataset.read()\nstock = table.to_pandas()\nprint(f'Found {stock.time_id.nunique()} unique time ids')\nprint(f'Found {stock.stock_id.nunique()} unique stock ids.')","metadata":{"execution":{"iopub.status.busy":"2021-06-29T02:41:29.33224Z","iopub.execute_input":"2021-06-29T02:41:29.332638Z","iopub.status.idle":"2021-06-29T02:41:29.934627Z","shell.execute_reply.started":"2021-06-29T02:41:29.332604Z","shell.execute_reply":"2021-06-29T02:41:29.933769Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"You could have also iterated though the files, but this way the `stock_id` column is already in the dataframe.  The `filter` option can be used to select rows for any column.  Here we get every stock for `time_id=5`.","metadata":{"execution":{"iopub.status.busy":"2021-06-29T01:58:00.445142Z","iopub.execute_input":"2021-06-29T01:58:00.44546Z","iopub.status.idle":"2021-06-29T01:58:00.451035Z","shell.execute_reply.started":"2021-06-29T01:58:00.445437Z","shell.execute_reply":"2021-06-29T01:58:00.449669Z"}}},{"cell_type":"code","source":"dataset = pq.ParquetDataset('../input/optiver-realized-volatility-prediction/book_train.parquet/', \n                            use_legacy_dataset=False,\n                            filters =[('time_id', '=', '5')]) \ntable = dataset.read()\nstock = table.to_pandas()\nprint(f'Found {stock.time_id.nunique()} unique time ids')\nprint(f'Found {stock.stock_id.nunique()} unique stock ids.')","metadata":{"execution":{"iopub.status.busy":"2021-06-29T02:41:29.93577Z","iopub.execute_input":"2021-06-29T02:41:29.936056Z","iopub.status.idle":"2021-06-29T02:41:38.282979Z","shell.execute_reply.started":"2021-06-29T02:41:29.936021Z","shell.execute_reply":"2021-06-29T02:41:38.281926Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Note that I used the option `use_legacy_dataset=False`.  This is required to filter on something other than *partition keys*. ","metadata":{}}]}