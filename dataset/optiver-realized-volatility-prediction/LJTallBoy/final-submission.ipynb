{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-09-27T14:17:57.106082Z","iopub.execute_input":"2021-09-27T14:17:57.106369Z","iopub.status.idle":"2021-09-27T14:17:57.33421Z","shell.execute_reply.started":"2021-09-27T14:17:57.106343Z","shell.execute_reply":"2021-09-27T14:17:57.33341Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport glob\nimport pandas as pd\npd.options.mode.chained_assignment = None\nfrom sklearn.linear_model import LinearRegression\nimport time \nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.compose import ColumnTransformer\n#from sklearn.neural_network import MLPRegressor\nfrom xgboost import XGBRegressor\n\nstart = time.time()\n\n \ndef get_book_df(train_test, stock_id):\n    target_path = '/kaggle/input/optiver-realized-volatility-prediction/book_' + train_test + '.parquet/stock_id=' + str(stock_id) +'/' \n    df = pd.read_parquet(target_path)\n    df['mid'] = (df['bid_price1']+df['ask_price1'])/2\n    df['wap'] =(df['bid_price1'] * df['ask_size1']+df['ask_price1'] * df['bid_size1'])  / (\n                                      df['bid_size1']+ df['ask_size1'])\n    \n    return df\n    \n\n\ndef get_trade_df(train_test, stock_id):\n    target_path = '/kaggle/input/optiver-realized-volatility-prediction/trade_' + train_test + '.parquet/stock_id=' + str(stock_id) +'/'\n    df1 = pd.read_parquet(target_path)\n    return df1\n\ndef log_return(list_stock_prices):\n    return np.log(list_stock_prices).diff()\n\ndef logged_vol(series_log_return):\n        return np.log1p((np.sqrt(np.sum(series_log_return**2))))\n\ndef bipower_variation(series_log_return):\n    u = np.sqrt(np.pi / 2) ** -2\n    pre_log = u * sum([abs(f) * abs(p) for f, p in zip(series_log_return[2:], series_log_return[1:])])\n    return np.log1p(pre_log)\n\ndef filter_(raw_df, time_id):\n    \n    df = raw_df.loc[raw_df['time_id'] == time_id]\n    #df['seconds_in_bucket'] = df['seconds_in_bucket'] - df['seconds_in_bucket'].min()\n    return df\n\n        \ndef filled_book_trade_df(book_df, trade_df, time_id):\n    book_df = filter_(book_df, time_id).set_index('seconds_in_bucket')\n    trade_df = filter_(trade_df, time_id).set_index('seconds_in_bucket')\n    df = pd.concat([book_df, trade_df], axis=1)\n    df['time_id'] = time_id\n    df[['bid_price1','ask_price1','bid_price2','ask_price2','bid_size1','ask_size1','bid_size2','ask_size2', 'wap']] = df[['bid_price1','ask_price1','bid_price2','ask_price2','bid_size1','ask_size1','bid_size2','ask_size2', 'wap']].fillna(method = 'ffill')\n    df['log_return'] = log_return(df['wap'])\n    df['log_return'] = df['log_return'].fillna(0)\n    df[['size', 'order_count']] = df[['size', 'order_count']].fillna(0)\n    df['price'] = df['price'].astype(float)\n    df['price'] = df['price'].fillna(df['wap'])\n    return df\n\ndef s_book_trade_df(book_df, trade_df, time_id):\n    book_df = filter_(book_df, time_id).set_index('seconds_in_bucket')\n    trade_df = filter_(trade_df, time_id).set_index('seconds_in_bucket')\n    df = pd.concat([book_df, trade_df], axis=1)\n    df['time_id'] = time_id\n    df[['bid_price1','ask_price1','bid_price2','ask_price2','bid_size1','ask_size1','bid_size2','ask_size2', 'wap']] = df[['bid_price1','ask_price1','bid_price2','ask_price2','bid_size1','ask_size1','bid_size2','ask_size2', 'wap']].fillna(method = 'ffill')\n    df['log_return'] = log_return(df['wap'])\n    df['log_return'] = df['log_return'].fillna(0)\n    df[['size', 'order_count']] = df[['size', 'order_count']].fillna(0)\n    df['price'] = df['price'].astype(float)\n    df['price'] = df['price'].fillna(df['wap'])\n    return df\n\ndef depth(bid_or_ask_1, bid_or_ask_2, bid_or_ask_1_volume, bid_or_ask_2_volume):\n    x = np.sum((np.multiply(bid_or_ask_2,bid_or_ask_2_volume)))\n    y = np.sum((np.multiply(bid_or_ask_1,bid_or_ask_1_volume)))\n    return np.log1p((x+y))\n\ndef slope(best_order, second_best_order, mid_price):\n    return np.log1p(np.mean((best_order-second_best_order)/mid_price))\n\ndef trade_dist(series_trade_price,series_best_bid, series_best_ask):\n    z = np.mean(series_trade_price/((series_best_ask+series_best_ask)/2))\n    return z\n\n#path_list = glob.glob('/kaggle/input/optiver-realized-volatility-prediction/book_test.parquet/*')\n\ntest_df = pd.read_csv('/kaggle/input/optiver-realized-volatility-prediction/test.csv')\n\nstock_id_list = test_df['stock_id'].unique()\n#stock_id_list = [0]\ntargets_df = pd.read_csv('/kaggle/input/optiver-realized-volatility-prediction/train.csv')\n\nfeatures = np.array([])\nagg_targets = np.array([])\nagg_index= np.array([])\nagg_predictions = np.array([])\nrescaled_agg_targets = np.array([])\n\nstartloop = time.time()\n\n\nfor stock_id in stock_id_list:\n    \n    ## retrieve data for training ##\n    train_stock_book_df = get_book_df('train', stock_id)\n    train_stock_trade_df = get_trade_df('train',stock_id)\n    train_time_id_list = train_stock_book_df['time_id'].unique()\n    train_stock_target_df = targets_df.loc[lambda df: df['stock_id'] == stock_id]\n\n    \n    ## initiate lists to fill with augmented/feature engineereed training data ##\n    m_bid_1 = []\n    m_bid_avg = []\n    m_ask_1 = []\n    m_ask_avg = []\n    bid_depth_1 = []\n    bid_depth_avg = []\n    ask_depth_1 = []\n    ask_depth_avg = []\n    rv_1 = []\n    rv_2 =[]\n    bpv_1 = []\n    bpv_2 = []\n    targets = []\n    index= []\n\n\n    \n    for train_time_id in train_time_id_list:\n        \n        raw = s_book_trade_df(train_stock_book_df, train_stock_trade_df, train_time_id)\n        df1 = train_stock_target_df.loc[lambda df: df['time_id'] == train_time_id]\n        \n        t1_log_return = raw['log_return']\n        t1_bid1 = raw['bid_price1']\n        t1_bids2 = raw['bid_price2']\n        t1_mid = raw['mid']\n        t1_ask1 = raw['ask_price1']\n        t1_ask2 = raw['ask_price2']\n        t1_bid1_size = raw['bid_size1']\n        t1_bid2_size = raw['bid_size2']\n        t1_ask1_size = raw['ask_size1']\n        t1_ask2_size = raw['ask_size2']\n        t1_trade_price = raw['price']\n        \n        \n      \n        index.append(str(stock_id) + '-' + str(train_time_id))\n        #m_bid_1.append(slope(t1_bid1.head(300), t1_bids2.head(300), t1_mid.head(300)))\n        m_bid_avg.append(slope(t1_bid1, t1_bids2, t1_mid))\n        #m_ask_1.append(slope(t1_ask1.head(300), t1_ask2.head(300), t1_mid.head(300)))\n        m_ask_avg.append(slope(t1_ask1, t1_ask2, t1_mid))\n        #bid_depth_1.append(depth(t1_bid1.head(300), t1_bids2.head(300), t1_bid1_size.head(300), t1_bid2_size.head(300)))\n        bid_depth_avg.append(depth(t1_bid1, t1_bids2, t1_bid1_size, t1_bid2_size))\n        #ask_depth_1.append(depth(t1_ask1_size.head(300), t1_ask2.head(300), t1_ask1_size.head(300), t1_ask1_size.head(300)))\n        ask_depth_avg.append(depth(t1_ask1, t1_ask2, t1_ask1_size, t1_ask2_size))\n        #rv_1.append(((logged_vol(t1_log_return.head(300)))))\n        rv_2.append((logged_vol(t1_log_return)))\n        bpv_1.append(trade_dist(t1_trade_price,t1_bid1, t1_ask1))\n        bpv_2.append(bipower_variation(t1_log_return))\n        targets.append(np.log1p(df1['target'].values[0]))\n    \n    ind_df = pd.DataFrame({#'m_bid_1': m_bid_1, \n                           'm_bid_avg': m_bid_avg, \n                           #'m_ask_1': m_ask_1, \n                           'm_ask_avg': m_ask_avg, \n                           #'bid_depth_1': bid_depth_1, \n                           'bid_depth_avg': bid_depth_avg, \n                           #'ask_depth_1': ask_depth_1, \n                           'ask_depth_avg': ask_depth_avg, \n                           #'rv_1' : rv_1, \n                           'rv_2': rv_2, \n                           'bpv_1' : bpv_1, \n                           'bpv_2' : bpv_2})\n    \n\n   ## put training features and targets into trainable structure ##                 \n    dep_df = pd.DataFrame({'targets': targets})\n    row_id_df = pd.DataFrame({'row_id': index})\n    ind_np_train = ind_df.to_numpy()\n    dep_np_train = dep_df.to_numpy()\n    sample_size = int((ind_np_train.size)/7)\n    \n    ## fit XGBoost to the training data ##\n    X = ind_np_train.reshape(sample_size,7)\n    y = dep_np_train.reshape(sample_size).ravel()\n    reg = XGBRegressor(max_depth = 3, eta = 0.15)\n    reg.fit(X,y)\n    \n    ## retrieve data for testing ##\n    test_stock_book_df = get_book_df('test', stock_id)\n    test_stock_trade_df = get_trade_df('test',stock_id)\n    #test_time_id_list = test_stock_book_df['time_id'].unique()\n    test_time_id_list = test_stock_book_df['time_id'].unique()\n    \n    ## initiate lists tof ill with augmented/feature engineered test data ##\n    ts_m_bid_1 = []\n    ts_m_bid_avg = []\n    ts_m_ask_1 = []\n    ts_m_ask_avg = []\n    ts_bid_depth_1 = []\n    ts_bid_depth_avg = []\n    ts_ask_depth_1 = []\n    ts_ask_depth_avg = []\n    ts_rv_1 = []\n    ts_rv_2 =[]\n    ts_bpv_1 = []\n    ts_bpv_2 = []\n    ts_targets = []\n    ts_index= []\n    \n    for test_time_id in test_time_id_list:\n        \n        raw = s_book_trade_df(test_stock_book_df, test_stock_trade_df, test_time_id)\n        #df1 = test_stock_target_df.loc[lambda df: df['time_id'] == test_time_id]\n        \n        t1_log_return = raw['log_return']\n        t1_bid1 = raw['bid_price1']\n        t1_bids2 = raw['bid_price2']\n        t1_mid = raw['mid']\n        t1_ask1 = raw['ask_price1']\n        t1_ask2 = raw['ask_price2']\n        t1_bid1_size = raw['bid_size1']\n        t1_bid2_size = raw['bid_size2']\n        t1_ask1_size = raw['ask_size1']\n        t1_ask2_size = raw['ask_size2']\n        t1_trade_price = raw['price']\n        \n      \n        ts_index.append(str(stock_id) + '-' + str(test_time_id))\n        #ts_m_bid_1.append(slope(t1_bid1.head(300), t1_bids2.head(300), t1_mid.head(300)))\n        ts_m_bid_avg.append(slope(t1_bid1, t1_bids2, t1_mid))\n        #ts_m_ask_1.append(slope(t1_ask1.head(300), t1_ask2.head(300), t1_mid.head(300)))\n        ts_m_ask_avg.append(slope(t1_ask1, t1_ask2, t1_mid))\n        #ts_bid_depth_1.append(depth(t1_bid1.head(300), t1_bids2.head(300), t1_bid1_size.head(300), t1_bid2_size.head(300)))\n        ts_bid_depth_avg.append(depth(t1_bid1, t1_bids2, t1_bid1_size, t1_bid2_size))\n        #ts_ask_depth_1.append(depth(t1_ask1_size.head(300), t1_ask2.head(300), t1_ask1_size.head(300), t1_ask1_size.head(300)))\n        ts_ask_depth_avg.append(depth(t1_ask1, t1_ask2, t1_ask1_size, t1_ask2_size))\n        #ts_rv_1.append(((logged_vol(t1_log_return.head(300)))))\n        ts_rv_2.append((logged_vol(t1_log_return)))\n        ts_bpv_1.append(trade_dist(t1_trade_price,t1_bid1, t1_ask1))\n        ts_bpv_2.append(bipower_variation(t1_log_return))\n \n    \n    train_feat_df = pd.DataFrame({#'m_bid_1': ts_m_bid_1, \n                           'm_bid_avg': ts_m_bid_avg, \n                           #'m_ask_1': ts_m_ask_1, \n                           'm_ask_avg': ts_m_ask_avg, \n                           #'bid_depth_1': ts_bid_depth_1, \n                           'bid_depth_avg': ts_bid_depth_avg, \n                           #'ask_depth_1': ts_ask_depth_1, \n                           'ask_depth_avg': ts_ask_depth_avg, \n                           #'rv_1' : ts_rv_1, \n                           'rv_2': ts_rv_2, \n                           'dist' : ts_bpv_1, \n                           'bpv_2' : ts_bpv_2})\n    \n    \n \n    \n    ## put test features into structure that XGBoost can predict ##                     \n    row_id_df = pd.DataFrame({'row_id': ts_index})\n    feat_np_test = train_feat_df.to_numpy()\n    agg_index = np.append(agg_index, ts_index)\n    test_sample_size = int((feat_np_test.size)/7)\n    \n    ## make predcitions and save as pandas DataFrame##                  \n    unscaled_pred = reg.predict(feat_np_test.reshape(test_sample_size, 7))\n    #scaler.fit(dep_np_unscaled)\n    scaled_pred = np.expm1(unscaled_pred.reshape(test_sample_size,1))\n    agg_predictions = np.append(agg_predictions, scaled_pred)\n\nend = time.time()\n\nprint (end-start)\n\npred_df = pd.DataFrame(agg_predictions, columns = ['target'])\nrow_id_df = pd.DataFrame(agg_index, columns = ['row_id'])\nfinal_df = pd.concat([row_id_df, pred_df], axis = 1)\nfinal_df.to_csv('submission.csv', index=False)\n","metadata":{"execution":{"iopub.status.busy":"2021-09-27T14:17:57.361028Z","iopub.execute_input":"2021-09-27T14:17:57.361323Z","iopub.status.idle":"2021-09-27T14:19:26.266839Z","shell.execute_reply.started":"2021-09-27T14:17:57.361293Z","shell.execute_reply":"2021-09-27T14:19:26.26518Z"},"trusted":true},"execution_count":null,"outputs":[]}]}