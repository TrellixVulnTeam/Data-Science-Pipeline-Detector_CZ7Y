{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Feature engineering pipeline with optimized LightGBM\n\nThis notebook is an adaptation of [this notebook](https://www.kaggle.com/tommy1028/lightgbm-starter-with-feature-engineering-idea) with improvements in organization, performance, code generalization and readability. I also changed the modelling and feature importance parts to use the scikit-learn API and focus on normalized gain. A lot more could be done in the feature engineering but I'm going to leave it this way, since for me now it has a good organization to keep progressing. Hyperparameter optimization with Optuna was implemented as well.\n\n**If you think this is relevant or helped you, please give it an upvote. Thanks!**","metadata":{}},{"cell_type":"markdown","source":"## Preparation","metadata":{}},{"cell_type":"code","source":"# Libs to deal with tabular data\nimport numpy as np\nimport pandas as pd\npd.options.mode.chained_assignment = None\npd.set_option('display.max_columns', None)\n\n# Statistics\nfrom scipy.stats import chi2_contingency\nfrom scipy.stats.contingency import expected_freq\n\n# Plotting packages\nimport seaborn as sns\nsns.axes_style(\"darkgrid\")\n%matplotlib inline\nimport matplotlib.pyplot as plt\nplt.style.use('seaborn')\n\n# Machine Learning\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split, KFold, GridSearchCV\nfrom sklearn.inspection import permutation_importance\nfrom sklearn.feature_selection import mutual_info_classif\nfrom boruta import BorutaPy\n\nfrom lightgbm import LGBMRegressor\n\n# Optimization\nimport optuna\nfrom optuna.samplers import TPESampler\nfrom optuna.visualization import plot_contour, plot_optimization_history\nfrom optuna.visualization import plot_param_importances, plot_slice\n\n# To display stuff in notebook\nfrom IPython.display import display, Markdown\n\n# Misc \nfrom joblib import Parallel, delayed\nfrom tqdm.notebook import tqdm\nimport time\nimport os\nimport glob","metadata":{"execution":{"iopub.status.busy":"2021-08-16T22:12:48.849963Z","iopub.execute_input":"2021-08-16T22:12:48.850379Z","iopub.status.idle":"2021-08-16T22:12:52.273119Z","shell.execute_reply.started":"2021-08-16T22:12:48.850346Z","shell.execute_reply":"2021-08-16T22:12:52.271781Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# data directory\nDATA_DIR = '../input/optiver-realized-volatility-prediction/'","metadata":{"execution":{"iopub.status.busy":"2021-08-16T22:12:52.274673Z","iopub.execute_input":"2021-08-16T22:12:52.275017Z","iopub.status.idle":"2021-08-16T22:12:52.281651Z","shell.execute_reply.started":"2021-08-16T22:12:52.274987Z","shell.execute_reply":"2021-08-16T22:12:52.280391Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Simple feature creation functions","metadata":{}},{"cell_type":"code","source":"def compute_wap(df, index):\n    numerator = df[f'bid_price{index}'] * df[f'ask_size{index}'] + df[f'ask_price{index}'] * df[f'bid_size{index}'] \n    wap = numerator / (df[f'bid_size{index}'] + df[f'ask_size{index}'])\n    return wap\n\ndef compute_realized_volatility(returns):\n    return np.sqrt(np.sum(returns**2))","metadata":{"execution":{"iopub.status.busy":"2021-08-16T22:12:52.284274Z","iopub.execute_input":"2021-08-16T22:12:52.284786Z","iopub.status.idle":"2021-08-16T22:12:52.296582Z","shell.execute_reply.started":"2021-08-16T22:12:52.284737Z","shell.execute_reply":"2021-08-16T22:12:52.295521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Main function for preprocessing book data","metadata":{}},{"cell_type":"code","source":"def create_book_features(df, windows = [600]):\n    # compute prices and returns\n    for idx in [1,2]:\n        df[f'wap{idx}'] = compute_wap(df, idx)\n        df[f'log_wap{idx}'] = np.log(df[f'wap{idx}'])\n        df[f'log_return{idx}'] = df.groupby('time_id')[f'log_wap{idx}'].diff()\n    \n    # compute general book features\n    df['wap_balance'] = abs(df['wap1'] - df['wap2'])\n    df['price_spread'] = (df['ask_price1'] - df['bid_price1']) / ((df['ask_price1'] + df['bid_price1'])/2)\n    df['bid_spread'] = df['bid_price1'] - df['bid_price2']\n    df['ask_spread'] = df['ask_price1'] - df['ask_price2']\n    df['total_volume'] = (df['ask_size1'] + df['ask_size2']) + (df['bid_size1'] + df['bid_size2'])\n    df['volume_imbalance'] = abs((df['ask_size1'] + df['ask_size2']) - (df['bid_size1'] + df['bid_size2']))\n    \n    # compute aggregations over the features created above for different time windows\n    feature_creation_dict = {\n        'log_return1':[compute_realized_volatility],\n        'log_return2':[compute_realized_volatility],\n        'wap_balance':[np.mean],\n        'price_spread':[np.mean],\n        'bid_spread':[np.mean],\n        'ask_spread':[np.mean],\n        'volume_imbalance':[np.mean],\n        'total_volume':[np.mean],\n        'wap1':[np.mean]\n    }\n    \n    window_features = []\n    for seconds in windows:\n        df_window = df.loc[df['seconds_in_bucket'] >= (600 - seconds), :] if seconds != 600 else df\n        df_features = df_window.groupby(['time_id']).agg(feature_creation_dict)\n        df_features.columns = ['_'.join(col) + f'_l{seconds}' for col in df_features.columns] # join multi-index column names\n        window_features.append(df_features)\n        \n    df_features = pd.concat(window_features, axis=1, copy=False)     \n    return df_features","metadata":{"execution":{"iopub.status.busy":"2021-08-16T22:12:52.298303Z","iopub.execute_input":"2021-08-16T22:12:52.298799Z","iopub.status.idle":"2021-08-16T22:12:52.313417Z","shell.execute_reply.started":"2021-08-16T22:12:52.298757Z","shell.execute_reply":"2021-08-16T22:12:52.312445Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nstock_id = 0\nfile_path = DATA_DIR + f\"book_train.parquet/stock_id={stock_id}\"\ndf = pd.read_parquet(file_path)\nbook_features = create_book_features(df, [600, 300])","metadata":{"execution":{"iopub.status.busy":"2021-08-16T22:12:52.314744Z","iopub.execute_input":"2021-08-16T22:12:52.315189Z","iopub.status.idle":"2021-08-16T22:12:59.497492Z","shell.execute_reply.started":"2021-08-16T22:12:52.31513Z","shell.execute_reply":"2021-08-16T22:12:59.496282Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"book_features.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-16T22:12:59.499149Z","iopub.execute_input":"2021-08-16T22:12:59.499479Z","iopub.status.idle":"2021-08-16T22:12:59.530564Z","shell.execute_reply.started":"2021-08-16T22:12:59.499447Z","shell.execute_reply":"2021-08-16T22:12:59.529225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Main function for preprocessing trade data","metadata":{}},{"cell_type":"code","source":"def create_trade_features(df, windows = [600]):\n    # compute return\n    df['log_price'] = np.log(df['price'])\n    df['log_return'] = df.groupby('time_id')['log_price'].diff()\n    \n    # compute aggregations for different time windows\n    feature_creation_dict = {\n        'log_return':[compute_realized_volatility],\n        'seconds_in_bucket':'nunique',\n        'size':[np.sum],\n        'order_count':[np.mean],\n    }\n    \n    window_features = []\n    for seconds in windows:\n        df_window = df.loc[df['seconds_in_bucket'] >= (600 - seconds), :] if seconds != 600 else df\n        df_features = df_window.groupby(['time_id']).agg(feature_creation_dict)\n        df_features.columns = ['_'.join(col) + f'_l{seconds}' for col in df_features.columns] # join multi-index column names\n        window_features.append(df_features)\n    \n    df_features = pd.concat(window_features, axis=1, copy=False)   \n    return df_features","metadata":{"execution":{"iopub.status.busy":"2021-08-16T22:12:59.532226Z","iopub.execute_input":"2021-08-16T22:12:59.532533Z","iopub.status.idle":"2021-08-16T22:12:59.541491Z","shell.execute_reply.started":"2021-08-16T22:12:59.532504Z","shell.execute_reply":"2021-08-16T22:12:59.54041Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nstock_id = 0\nfile_path = DATA_DIR + f\"trade_train.parquet/stock_id={stock_id}\"\ndf = pd.read_parquet(file_path)\ntrade_features = create_trade_features(df, [600, 300])","metadata":{"execution":{"iopub.status.busy":"2021-08-16T22:12:59.5428Z","iopub.execute_input":"2021-08-16T22:12:59.543069Z","iopub.status.idle":"2021-08-16T22:13:02.668567Z","shell.execute_reply.started":"2021-08-16T22:12:59.543043Z","shell.execute_reply":"2021-08-16T22:13:02.667374Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trade_features.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-16T22:13:02.672628Z","iopub.execute_input":"2021-08-16T22:13:02.672985Z","iopub.status.idle":"2021-08-16T22:13:02.690658Z","shell.execute_reply.started":"2021-08-16T22:13:02.672954Z","shell.execute_reply":"2021-08-16T22:13:02.68925Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Combined preprocessor function","metadata":{}},{"cell_type":"code","source":"def preprocessor(stock_id_list, mode = 'train', windows = [600, 300]):\n    # the function above will be parallelized\n    def create_stock_features(stock_id):\n        book = pd.read_parquet(f\"{DATA_DIR}book_{mode}.parquet/stock_id={stock_id}\")\n        trade = pd.read_parquet(f\"{DATA_DIR}trade_{mode}.parquet/stock_id={stock_id}\")\n\n        features = create_book_features(book, windows).join(\n            create_trade_features(trade, windows), \n            how='outer'\n        )\n        \n        # create row_id\n        features['row_id'] = features.index.map(lambda x: f'{stock_id}-{x}')\n        features = features.reset_index(drop=True)\n\n        return features\n    \n    features_list = Parallel(n_jobs=-1, verbose=1)(\n        delayed(create_stock_features)(stock_id) for stock_id in stock_id_list\n    )\n    features = pd.concat(features_list, ignore_index = True)\n    return features","metadata":{"execution":{"iopub.status.busy":"2021-08-16T22:13:02.692656Z","iopub.execute_input":"2021-08-16T22:13:02.692957Z","iopub.status.idle":"2021-08-16T22:13:02.701275Z","shell.execute_reply.started":"2021-08-16T22:13:02.692932Z","shell.execute_reply":"2021-08-16T22:13:02.700051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"list_stock_ids = [0,1]\nfeatures = preprocessor(list_stock_ids, 'train')\nfeatures.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-16T22:13:02.702665Z","iopub.execute_input":"2021-08-16T22:13:02.702974Z","iopub.status.idle":"2021-08-16T22:13:15.219263Z","shell.execute_reply.started":"2021-08-16T22:13:02.702946Z","shell.execute_reply":"2021-08-16T22:13:15.218022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training set","metadata":{}},{"cell_type":"code","source":"# Reading train file, which maps stock_id and time_it to the target\ntrain = pd.read_csv(DATA_DIR + 'train.csv')\ntrain_stock_ids = train.stock_id.unique()\ntrain['row_id'] = train['stock_id'].astype(str) + '-' + train['time_id'].astype(str)\ntrain = train[['row_id','target']]\n\n# Creating train dataset\ndf_train = preprocessor(train_stock_ids, 'train')\ndf_train = train.merge(df_train, on=['row_id'], how='left')\n\ndf_train['stock_id'] = df_train['row_id'].apply(lambda x: x.split('-')[0]).astype(int)","metadata":{"execution":{"iopub.status.busy":"2021-08-16T22:13:15.220658Z","iopub.execute_input":"2021-08-16T22:13:15.220974Z","iopub.status.idle":"2021-08-16T22:22:23.524078Z","shell.execute_reply.started":"2021-08-16T22:13:15.220944Z","shell.execute_reply":"2021-08-16T22:22:23.522946Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.shape","metadata":{"execution":{"iopub.status.busy":"2021-08-16T22:22:24.722695Z","iopub.execute_input":"2021-08-16T22:22:24.723319Z","iopub.status.idle":"2021-08-16T22:22:24.731827Z","shell.execute_reply.started":"2021-08-16T22:22:24.723272Z","shell.execute_reply":"2021-08-16T22:22:24.730617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## LightGBM","metadata":{}},{"cell_type":"code","source":"X = df_train.drop(['row_id','target'],axis=1)\ny = df_train['target']","metadata":{"execution":{"iopub.status.busy":"2021-08-16T22:22:29.345145Z","iopub.execute_input":"2021-08-16T22:22:29.345614Z","iopub.status.idle":"2021-08-16T22:22:29.372902Z","shell.execute_reply.started":"2021-08-16T22:22:29.345571Z","shell.execute_reply":"2021-08-16T22:22:29.372108Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def rmspe(y_true, y_pred):\n    metric_val = (np.sqrt(np.mean(np.square((y_true - y_pred) / y_true))))\n    return  'rmspe', metric_val, False","metadata":{"execution":{"iopub.status.busy":"2021-08-16T22:22:29.374365Z","iopub.execute_input":"2021-08-16T22:22:29.374987Z","iopub.status.idle":"2021-08-16T22:22:29.390215Z","shell.execute_reply.started":"2021-08-16T22:22:29.374945Z","shell.execute_reply":"2021-08-16T22:22:29.389308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Hyperparameter optimization","metadata":{}},{"cell_type":"code","source":"class Light_GBM_CV:\n    def __init__(self, X, y, folds=5, random_state=42):\n        self.X = X\n        self.y = y\n        self.folds = folds\n        self.random_state = random_state\n\n    def __call__(self, trial):\n        cv = KFold(\n            self.folds, \n            random_state = self.random_state, \n            shuffle=True\n        )\n        \n        clf = LGBMRegressor(\n            boosting_type = 'gbdt',\n            objective = 'rmse',\n            random_state = self.random_state,\n            first_metric_only = True,\n            num_leaves = trial.suggest_int('num_leaves', 16, 256),\n            max_depth = trial.suggest_int('max_depth', 4, 8),\n            learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1.0),\n            min_child_samples = trial.suggest_int('min_child_samples', 5, 100),\n            n_estimators = trial.suggest_int('n_estimators', 10, 3000),\n            lambda_l1 = trial.suggest_loguniform('lambda_l1', 1e-5, 1.0),\n            lambda_l2 = trial.suggest_loguniform('lambda_l2', 1e-5, 1.0),\n            max_bin = trial.suggest_int('max_bin', 10, 1000),\n            feature_fraction = trial.suggest_float('feature_fraction', 0.1, 1),\n            bagging_fraction = trial.suggest_float('bagging_fraction', 0.1, 1),\n            categorical_feature = ['stock_id']\n        )\n        \n        cv_scores = []\n\n        for array_idxs in cv.split(self.X):\n            train_index, val_index = array_idxs[0], array_idxs[1]\n            X_train, X_val = self.X.loc[train_index], self.X.loc[val_index]\n            y_train, y_val = self.y.loc[train_index], self.y.loc[val_index]\n            \n            clf.fit(\n                X_train, y_train,\n                sample_weight = 1 / np.square(y_train),\n                eval_set = [(X_val, y_val), (X_train, y_train)],\n                eval_metric = rmspe,\n                early_stopping_rounds = 10,\n                verbose = False,\n                categorical_feature = ['stock_id']\n            )\n            cv_scores.append(clf.best_score_['valid_0']['rmspe'])\n\n        return sum(cv_scores) / len(cv_scores)","metadata":{"execution":{"iopub.status.busy":"2021-08-16T22:22:29.391637Z","iopub.execute_input":"2021-08-16T22:22:29.392268Z","iopub.status.idle":"2021-08-16T22:22:29.408353Z","shell.execute_reply.started":"2021-08-16T22:22:29.392226Z","shell.execute_reply":"2021-08-16T22:22:29.407373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lgbm_cv = Light_GBM_CV(X, y)\nstudy = optuna.create_study(sampler=TPESampler(seed = 42), direction='minimize')\nstudy.optimize(lgbm_cv, n_trials=250)","metadata":{"scrolled":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-08-16T22:22:29.409729Z","iopub.execute_input":"2021-08-16T22:22:29.410334Z","iopub.status.idle":"2021-08-16T22:23:20.429124Z","shell.execute_reply.started":"2021-08-16T22:22:29.410289Z","shell.execute_reply":"2021-08-16T22:23:20.42799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Best model')\nprint('Mean validation RMSPE: ', study.best_value, '\\n')","metadata":{"execution":{"iopub.status.busy":"2021-08-16T22:59:04.610242Z","iopub.execute_input":"2021-08-16T22:59:04.610776Z","iopub.status.idle":"2021-08-16T22:59:04.618485Z","shell.execute_reply.started":"2021-08-16T22:59:04.610712Z","shell.execute_reply":"2021-08-16T22:59:04.616738Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"study.best_params","metadata":{"execution":{"iopub.status.busy":"2021-08-16T22:59:07.075312Z","iopub.execute_input":"2021-08-16T22:59:07.075719Z","iopub.status.idle":"2021-08-16T22:59:07.08389Z","shell.execute_reply.started":"2021-08-16T22:59:07.075656Z","shell.execute_reply":"2021-08-16T22:59:07.082733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Cross validation","metadata":{}},{"cell_type":"code","source":"%%time\n\nmodels_list = []\ncv_scores = []\n\nkf = KFold(n_splits=5, random_state=19901028, shuffle=True)\nfor fold, (trn_idx, val_idx) in enumerate(kf.split(X, y)):\n    print(\"Fold :\", fold+1)\n    \n    # Dataset creation\n    X_train, y_train = X.loc[trn_idx], y[trn_idx]\n    X_valid, y_valid = X.loc[val_idx], y[val_idx]\n    \n    # Modelling\n    model = LGBMRegressor(\n        objective = \"rmse\",\n        boosting_type = \"gbdt\",\n        importance_type = 'gain',\n        first_metric_only = True,\n        random_state = 42,\n        categorical_feature = ['stock_id'],\n        **study.best_params\n    )\n    \n    model.fit(\n        X_train, y_train,\n        sample_weight = 1 / np.square(y_train),\n        eval_set = [(X_valid, y_valid), (X_train, y_train)],\n        eval_metric = rmspe,\n        early_stopping_rounds = 30,\n        verbose = 100,\n        categorical_feature = ['stock_id']\n    )\n    \n    # validation\n    rmspe_val = model.best_score_['valid_0']['rmspe']\n    print(f'Performance fold #{fold+1}: {rmspe_val}')\n\n    #keep scores and models\n    cv_scores.append(rmspe_val)\n    models_list.append(model)\n    print(\"*\" * 100)","metadata":{"execution":{"iopub.status.busy":"2021-08-16T22:28:05.79751Z","iopub.execute_input":"2021-08-16T22:28:05.798033Z","iopub.status.idle":"2021-08-16T22:28:57.414453Z","shell.execute_reply.started":"2021-08-16T22:28:05.797994Z","shell.execute_reply":"2021-08-16T22:28:57.413479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'CV score:', pd.Series(cv_scores).mean())\ncv_scores","metadata":{"execution":{"iopub.status.busy":"2021-08-16T22:30:04.624379Z","iopub.execute_input":"2021-08-16T22:30:04.625176Z","iopub.status.idle":"2021-08-16T22:30:04.635628Z","shell.execute_reply.started":"2021-08-16T22:30:04.625119Z","shell.execute_reply":"2021-08-16T22:30:04.634302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature importance","metadata":{}},{"cell_type":"code","source":"raw_imp_vetors = [model.feature_importances_.reshape(1, -1) for model in models_list]\nraw_imp_matrix = np.concatenate(raw_imp_vetors, axis=0)\nnorm_imp = raw_imp_matrix / raw_imp_matrix.sum(1).reshape(-1, 1)\nmean_imp = norm_imp.mean(0)\nimp_series = pd.Series(mean_imp, index=X.columns).sort_values(ascending=False)","metadata":{"execution":{"iopub.status.busy":"2021-08-16T22:30:17.373612Z","iopub.execute_input":"2021-08-16T22:30:17.37404Z","iopub.status.idle":"2021-08-16T22:30:17.384287Z","shell.execute_reply.started":"2021-08-16T22:30:17.374003Z","shell.execute_reply":"2021-08-16T22:30:17.383383Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"imp_series","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-08-16T22:30:18.934052Z","iopub.execute_input":"2021-08-16T22:30:18.934566Z","iopub.status.idle":"2021-08-16T22:30:18.941708Z","shell.execute_reply.started":"2021-08-16T22:30:18.934533Z","shell.execute_reply":"2021-08-16T22:30:18.940964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10, 8))\nsns.barplot(x = imp_series.values, y = imp_series.index, color='lightblue')\nplt.title('Normalized CV feature importance (gain)', fontsize=16)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-16T22:30:19.141923Z","iopub.execute_input":"2021-08-16T22:30:19.142328Z","iopub.status.idle":"2021-08-16T22:30:19.645283Z","shell.execute_reply.started":"2021-08-16T22:30:19.142296Z","shell.execute_reply":"2021-08-16T22:30:19.638634Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Test set","metadata":{}},{"cell_type":"code","source":"# Reading test file\ntest = pd.read_csv(DATA_DIR + 'test.csv')\ntest_stock_ids = test.stock_id.unique()\ntest['row_id'] = test['stock_id'].astype(str) + '-' + test['time_id'].astype(str)\ntest = test[['row_id']]\n\n# Creating train dataset\ndf_test = preprocessor(test_stock_ids, 'test')\ndf_test = test.merge(df_test, on=['row_id'], how='left')\n\ndf_test['stock_id'] = df_test['row_id'].apply(lambda x: x.split('-')[0]).astype(int)","metadata":{"execution":{"iopub.status.busy":"2021-08-16T22:23:20.491721Z","iopub.status.idle":"2021-08-16T22:23:20.492157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = df_test[['row_id']]\nX_test = df_test.drop(columns=['row_id'])\n\n# Scoring ensemble\ntarget = np.zeros(len(X_test))\nfor model in models_list:\n    pred = model.predict(X_test, num_iteration=model.best_iteration_)\n    target += pred / len(models_list)\n\nsubmission = submission.assign(target = target)    ","metadata":{"execution":{"iopub.status.busy":"2021-08-16T22:23:20.493455Z","iopub.status.idle":"2021-08-16T22:23:20.494046Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-16T22:23:20.495018Z","iopub.status.idle":"2021-08-16T22:23:20.495466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv('submission.csv', index = False)","metadata":{"execution":{"iopub.status.busy":"2021-08-16T22:23:20.496359Z","iopub.status.idle":"2021-08-16T22:23:20.496866Z"},"trusted":true},"execution_count":null,"outputs":[]}]}