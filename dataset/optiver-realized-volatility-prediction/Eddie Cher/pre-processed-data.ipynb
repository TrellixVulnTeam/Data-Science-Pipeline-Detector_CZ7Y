{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import time\nt1 = time.time()","metadata":{"executionInfo":{"elapsed":573,"status":"ok","timestamp":1631166890394,"user":{"displayName":"Eddie Zhang","photoUrl":"","userId":"01983443237361224085"},"user_tz":-480},"id":"oboKKAwdWM1g","execution":{"iopub.status.busy":"2021-09-15T07:48:23.366152Z","iopub.execute_input":"2021-09-15T07:48:23.367022Z","iopub.status.idle":"2021-09-15T07:48:23.391114Z","shell.execute_reply.started":"2021-09-15T07:48:23.366877Z","shell.execute_reply":"2021-09-15T07:48:23.390358Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Setup","metadata":{"id":"9GlfW1rbUsei"}},{"cell_type":"markdown","source":"## Google Drive\n","metadata":{}},{"cell_type":"code","source":"# from google.colab import drive\n# drive.mount('/content/gdrive')","metadata":{"execution":{"iopub.status.busy":"2021-09-15T07:48:25.437076Z","iopub.execute_input":"2021-09-15T07:48:25.437389Z","iopub.status.idle":"2021-09-15T07:48:25.442055Z","shell.execute_reply.started":"2021-09-15T07:48:25.437356Z","shell.execute_reply":"2021-09-15T07:48:25.440755Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# cd \"/content/gdrive/My Drive/Optiver Kaggle/Data\"","metadata":{"execution":{"iopub.status.busy":"2021-09-15T07:48:26.322244Z","iopub.execute_input":"2021-09-15T07:48:26.322578Z","iopub.status.idle":"2021-09-15T07:48:26.329887Z","shell.execute_reply.started":"2021-09-15T07:48:26.322541Z","shell.execute_reply":"2021-09-15T07:48:26.328906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# output_file = r'/content/gdrive/My Drive/Optiver Kaggle/\\working/submission.csv'\n# train_file = r'/content/gdrive/My Drive/Optiver Kaggle/working/train.csv'","metadata":{"execution":{"iopub.status.busy":"2021-09-15T07:48:26.654575Z","iopub.execute_input":"2021-09-15T07:48:26.654882Z","iopub.status.idle":"2021-09-15T07:48:26.66049Z","shell.execute_reply.started":"2021-09-15T07:48:26.654851Z","shell.execute_reply":"2021-09-15T07:48:26.658821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Kaggle","metadata":{}},{"cell_type":"code","source":"cd \"/kaggle/input/optiver-realized-volatility-prediction/\"","metadata":{"execution":{"iopub.status.busy":"2021-09-15T07:48:27.685815Z","iopub.execute_input":"2021-09-15T07:48:27.686849Z","iopub.status.idle":"2021-09-15T07:48:27.69483Z","shell.execute_reply.started":"2021-09-15T07:48:27.686795Z","shell.execute_reply":"2021-09-15T07:48:27.693624Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output_file = r'/kaggle/working/submission.csv'\ntrain_file = r'/kaggle/working/train.csv'","metadata":{"execution":{"iopub.status.busy":"2021-09-15T09:33:37.948086Z","iopub.execute_input":"2021-09-15T09:33:37.948908Z","iopub.status.idle":"2021-09-15T09:33:37.953171Z","shell.execute_reply.started":"2021-09-15T09:33:37.948867Z","shell.execute_reply":"2021-09-15T09:33:37.952441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## PC","metadata":{}},{"cell_type":"code","source":"# cd D:\\kaggle\\input\\optiver-realized-volatility-prediction","metadata":{"execution":{"iopub.status.busy":"2021-09-15T07:48:29.647091Z","iopub.execute_input":"2021-09-15T07:48:29.647586Z","iopub.status.idle":"2021-09-15T07:48:29.651913Z","shell.execute_reply.started":"2021-09-15T07:48:29.647543Z","shell.execute_reply":"2021-09-15T07:48:29.650596Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# output_file = r'D:\\kaggle\\working\\submission.csv'\n# train_file = r'D:\\kaggle\\working\\train.csv'","metadata":{"execution":{"iopub.status.busy":"2021-09-15T07:48:29.979756Z","iopub.execute_input":"2021-09-15T07:48:29.980206Z","iopub.status.idle":"2021-09-15T07:48:29.983907Z","shell.execute_reply.started":"2021-09-15T07:48:29.980175Z","shell.execute_reply":"2021-09-15T07:48:29.982948Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Libraries","metadata":{"id":"it-rah_eSmEj"}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport glob\nimport os\nimport gc\n\nfrom joblib import Parallel, delayed\n\nfrom sklearn import preprocessing, model_selection\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import QuantileTransformer\nfrom sklearn.metrics import r2_score\n\nimport matplotlib.pyplot as plt \nimport seaborn as sns\nimport numpy.matlib\n\nseed0 = 2021\ntarget_name = 'target'\nscores_folds = {}\n\npd.set_option('display.max_rows',None)\npd.set_option('display.max_columns',None)","metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1631166891550,"user":{"displayName":"Eddie Zhang","photoUrl":"","userId":"01983443237361224085"},"user_tz":-480},"id":"YwWaHuldSlqn","execution":{"iopub.status.busy":"2021-09-15T07:48:31.104422Z","iopub.execute_input":"2021-09-15T07:48:31.104856Z","iopub.status.idle":"2021-09-15T07:48:33.14413Z","shell.execute_reply.started":"2021-09-15T07:48:31.104826Z","shell.execute_reply":"2021-09-15T07:48:33.143028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Preprocess","metadata":{"id":"0f0C8QoMdhVF"}},{"cell_type":"markdown","source":"## Read Target Data ","metadata":{"id":"ZFVvIBwpVnGd"}},{"cell_type":"code","source":"def read_train_test():\n    train = pd.read_csv('train.csv')\n    test = pd.read_csv('test.csv')\n    # Create a key to merge with book and trade data\n    train['row_id'] = train['stock_id'].astype(str) + '-' + train['time_id'].astype(str)\n    test['row_id'] = test['stock_id'].astype(str) + '-' + test['time_id'].astype(str)\n    print(f'Our training set has {train.shape[0]} rows')\n    return train, test","metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1631166891550,"user":{"displayName":"Eddie Zhang","photoUrl":"","userId":"01983443237361224085"},"user_tz":-480},"id":"nCFM9L4ydj35","execution":{"iopub.status.busy":"2021-09-15T07:48:36.621415Z","iopub.execute_input":"2021-09-15T07:48:36.621928Z","iopub.status.idle":"2021-09-15T07:48:36.628716Z","shell.execute_reply.started":"2021-09-15T07:48:36.621884Z","shell.execute_reply":"2021-09-15T07:48:36.627497Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train,test = read_train_test()","metadata":{"executionInfo":{"elapsed":831,"status":"ok","timestamp":1631166892379,"user":{"displayName":"Eddie Zhang","photoUrl":"","userId":"01983443237361224085"},"user_tz":-480},"id":"tQPHe2WYSf3h","outputId":"7fc612cd-a443-4c3e-d529-61c837b7a247","execution":{"iopub.status.busy":"2021-09-15T07:48:37.107438Z","iopub.execute_input":"2021-09-15T07:48:37.10863Z","iopub.status.idle":"2021-09-15T07:48:37.117393Z","shell.execute_reply.started":"2021-09-15T07:48:37.10854Z","shell.execute_reply":"2021-09-15T07:48:37.115108Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train.head(5)","metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1631166892380,"user":{"displayName":"Eddie Zhang","photoUrl":"","userId":"01983443237361224085"},"user_tz":-480},"id":"x5sSPjpTTXkP","outputId":"a91e94da-0152-465f-a488-ba81f283235e","execution":{"iopub.status.busy":"2021-09-15T07:48:37.800336Z","iopub.execute_input":"2021-09-15T07:48:37.800684Z","iopub.status.idle":"2021-09-15T07:48:37.805384Z","shell.execute_reply.started":"2021-09-15T07:48:37.800652Z","shell.execute_reply":"2021-09-15T07:48:37.804377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Book and Trade Data Preprocess\n\n","metadata":{"id":"h0MfOE0PVrSG"}},{"cell_type":"markdown","source":"### Pre functions","metadata":{"id":"oZsWzqIINq1A"}},{"cell_type":"code","source":"# calculate weighted average price\ndef wap1(df):\n    wap = (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']) / (df['bid_size1'] + df['ask_size1'])\n    return wap\n\ndef wap2(df):\n    wap = (df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2']) / (df['bid_size2'] + df['ask_size2'])\n    return wap\n\ndef wap3(df):\n    wap = (df['bid_price1'] * df['bid_size1'] + df['ask_price1'] * df['ask_size1']) / (df['bid_size1'] + df['ask_size1'])\n    return wap\n\ndef wap4(df):\n    wap = (df['bid_price2'] * df['bid_size2'] + df['ask_price2'] * df['ask_size2']) / (df['bid_size2'] + df['ask_size2'])\n    return wap\n\n# calculate log return\ndef log_return(series):\n    return np.log(series).diff()\n\n# calculate the realized volatility\ndef realized_volatility(series):\n    return np.sqrt(np.sum(series**2))\n\n# calculate the realized volatility\ndef realized_volatility_constant(df, col_name, new_col):\n    df[new_col] = np.sqrt(np.sum(df[col_name]**2))\n    return df\n\n# count unique elements of a series\ndef count_unique(series):\n    return len(np.unique(series))\n\n# count unique elements of a series\ndef count_unique_constant(df, col_name, new_col):\n    df[new_col] = len(np.unique(df[col_name]))\n    return df\n\n# calculate order flow using tick method\ndef order_flow_tick(df):\n    df['order_flow_tick'] = np.nan\n    df['price_difference'] = df['price'].diff()\n    df.loc[df['price_difference']>0,'order_flow_tick'] = df['size']\n    df.loc[df['price_difference']<0,'order_flow_tick'] = -df['size']\n    # for those with price not change, assign a same sign with the previous value\n    constant_index = df.index[df['price_difference']==0].values\n    for index in constant_index:\n        df.loc[index,'order_flow_tick'] = np.sign(df.loc[index-1,'order_flow_tick']) * df.loc[index,'size']\n    df.drop('price_difference',inplace=True,axis=1)\n    return df\n\ndef order_flow_quote(df,df_ob):\n    # select the time id of orderbook to be consistent with trade\n    time_id = df['time_id'].iloc[0]\n    df_ob = df_ob.loc[df_ob['time_id']==time_id,:]\n\n    df_ob['mid_price'] = (df_ob['ask_price1'] + df_ob['bid_price1']) / 2\n    df = df.merge(df_ob[['seconds_in_bucket','mid_price']],on='seconds_in_bucket',how='left')\n    # if there are none values, let the quote equals to tick\n    df['order_flow_quote'] = df['order_flow_tick']\n    df.loc[df['price']<df['mid_price'],'order_flow_quote'] = -df['size']\n    df.loc[df['price']>df['mid_price'],'order_flow_quote'] = df['size']\n    df.loc[df['price']==df['mid_price'],'order_flow_quote'] = 0\n\n    df.drop('mid_price',inplace=True,axis=1)\n\n    return df\n\ndef tendency_factors(df):\n    def tendency(price, vol):    \n        df_diff = np.diff(price)\n        val = (df_diff/price[1:])*100\n        power = np.sum(val*vol[1:])\n        return power\n\n    lis = []\n    for n_time_id in df['time_id'].unique():\n        df_id = df[df['time_id'] == n_time_id]        \n        tendencyV = tendency(df_id['price'].values, df_id['size'].values)      \n        f_max = np.sum(df_id['price'].values > np.mean(df_id['price'].values))\n        f_min = np.sum(df_id['price'].values < np.mean(df_id['price'].values))\n        df_max =  np.sum(np.diff(df_id['price'].values) > 0)\n        df_min =  np.sum(np.diff(df_id['price'].values) < 0)\n        # new\n        abs_diff = np.median(np.abs( df_id['price'].values - np.mean(df_id['price'].values)))        \n        energy = np.mean(df_id['price'].values**2)\n        iqr_p = np.percentile(df_id['price'].values,75) - np.percentile(df_id['price'].values,25)\n\n        # vol vars\n\n        abs_diff_v = np.median(np.abs( df_id['size'].values - np.mean(df_id['size'].values)))        \n        energy_v = np.sum(df_id['size'].values**2)\n        iqr_p_v = np.percentile(df_id['size'].values,75) - np.percentile(df_id['size'].values,25)\n\n        lis.append({'time_id':n_time_id,'tendency':tendencyV,'f_max':f_max,'f_min':f_min,'df_max':df_max,'df_min':df_min,\n                    'abs_diff':abs_diff,'energy':energy,'iqr_p':iqr_p,'abs_diff_v':abs_diff_v,'energy_v':energy_v,'iqr_p_v':iqr_p_v})\n\n    df_lr = pd.DataFrame(lis)\n\n    return df_lr","metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1631166892381,"user":{"displayName":"Eddie Zhang","photoUrl":"","userId":"01983443237361224085"},"user_tz":-480},"id":"f4EUlaN3V7jh","execution":{"iopub.status.busy":"2021-09-15T07:48:39.07802Z","iopub.execute_input":"2021-09-15T07:48:39.078326Z","iopub.status.idle":"2021-09-15T07:48:39.112548Z","shell.execute_reply.started":"2021-09-15T07:48:39.078263Z","shell.execute_reply":"2021-09-15T07:48:39.111619Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Main functions","metadata":{"id":"P4AL2vn4Ntfo"}},{"cell_type":"markdown","source":"#### Book","metadata":{"id":"k3c3RSIyN6a_"}},{"cell_type":"code","source":"# preprocess book data (for each stock id)\ndef book_preprocessor(file_path):\n    df = pd.read_parquet(file_path)\n\n    # calculate weighted average price\n    df['wap1'] = wap1(df)\n    df['wap2'] = wap2(df)\n    df['wap3'] = wap3(df)\n    df['wap4'] = wap4(df)\n    # calculate log returns\n    df['log_return1'] = df.groupby(['time_id'])['wap1'].apply(log_return)\n    df['log_return2'] = df.groupby(['time_id'])['wap2'].apply(log_return)\n    df['log_return3'] = df.groupby(['time_id'])['wap3'].apply(log_return)\n    df['log_return4'] = df.groupby(['time_id'])['wap4'].apply(log_return)\n    # calculate wap balance\n    df['wap_gap1'] = abs(df['wap1'] - df['wap2'])\n    df['wap_gap2'] = abs(df['wap3'] - df['wap4'])\n\n    # calculate spread\n    df['price_spread1'] = (df['ask_price1'] - df['bid_price1']) / ((df['ask_price1'] + df['bid_price1']))\n    df['price_spread2'] = (df['ask_price2'] - df['bid_price2']) / ((df['ask_price2'] + df['bid_price2']))\n    df['bid_spread'] = df['bid_price1'] - df['bid_price2']\n    df['ask_spread'] = df['ask_price2'] - df['ask_price1']\n    df['bid_ask_spread1'] = df['ask_price1'] - df['bid_price1']\n    df['bid_ask_spread2'] = df['ask_price2'] - df['bid_price2']\n    df[\"bid_ask_spread_spread\"] = abs(df['bid_spread'] - df['ask_spread'])\n\n    # calculate imbalance\n    df['volume1'] = (df['ask_size1']) + (df['bid_size1'])\n    df['volume_imbalance1'] = (df['ask_size1']) - (df['bid_size1'])\n    df['imbalance_percentage1'] = df['volume_imbalance1'] / df['volume1']\n    df['volume2'] = (df['ask_size2']) + (df['bid_size2'])\n    df['volume_imbalance2'] = (df['ask_size2']) - (df['bid_size2'])\n    df['imbalance_percentage2'] = df['volume_imbalance2'] / df['volume2']\n    df['total_volume'] = (df['ask_size1'] + df['ask_size2']) + (df['bid_size1'] + df['bid_size2'])\n    df['total_volume_imbalance'] = (df['ask_size1'] + df['ask_size2']) - (df['bid_size1'] + df['bid_size2'])\n    df['total_imbalance_percentage'] = df['total_volume_imbalance'] / df['total_volume']\n\n    # create dict of functions for each feature for aggregations\n    create_feature_dict = {\n      'wap1': [np.sum, np.std],\n      'wap2': [np.sum, np.std],\n      'wap3': [np.sum, np.std],\n      'wap4': [np.sum, np.std],\n      'log_return1': [realized_volatility],\n      'log_return2': [realized_volatility],\n      'log_return3': [realized_volatility],\n      'log_return4': [realized_volatility],\n      'wap_gap1': [np.sum, np.max],\n      'wap_gap2': [np.sum, np.max],\n      'price_spread1':[np.sum, np.max],\n      'price_spread2':[np.sum, np.max],\n      'bid_spread':[np.sum, np.max],\n      'ask_spread':[np.sum, np.max],\n      'bid_ask_spread1':[np.sum, np.max],\n      'bid_ask_spread2':[np.sum, np.max],\n      'bid_ask_spread_spread':[np.sum, np.max],\n      'volume1':[np.sum, np.max],\n      'volume_imbalance1':[np.sum, np.max],\n      \"imbalance_percentage1\":[np.sum,  np.max],\n      \"volume2\":[np.sum,  np.max],\n      \"volume_imbalance2\":[np.sum,  np.max],\n      \"imbalance_percentage2\":[np.sum,  np.max],\n      \"total_volume\":[np.sum,  np.max],\n      \"total_volume_imbalance\":[np.sum,  np.max],\n      \"total_imbalance_percentage\":[np.sum,  np.max],\n    }\n    create_feature_dict_time = {\n      'log_return1': [realized_volatility],\n      'log_return2': [realized_volatility],\n      'log_return3': [realized_volatility],\n      'log_return4': [realized_volatility],\n    }\n\n    # Function to get group stats for different windows (seconds in bucket)\n    def get_stats_window(fe_dict,seconds_in_bucket, add_suffix = False):\n        # Group by the window\n        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(fe_dict).reset_index()\n        # Rename columns joining suffix (get wap_sum from the multi index wap and sum)\n        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n        # Add a suffix to differentiate windows\n        if add_suffix:\n            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n        return df_feature\n\n    # Get the stats for different windows\n    df_feature = get_stats_window(create_feature_dict,seconds_in_bucket = 0, add_suffix = False)\n    df_feature_500 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 500, add_suffix = True)\n    df_feature_400 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 400, add_suffix = True)\n    df_feature_300 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 300, add_suffix = True)\n    df_feature_200 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 200, add_suffix = True)\n    df_feature_100 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 100, add_suffix = True)\n\n    # Merge all\n    df_feature = df_feature.merge(df_feature_500, how = 'left', left_on = 'time_id_', right_on = 'time_id__500')\n    df_feature = df_feature.merge(df_feature_400, how = 'left', left_on = 'time_id_', right_on = 'time_id__400')\n    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n    df_feature = df_feature.merge(df_feature_200, how = 'left', left_on = 'time_id_', right_on = 'time_id__200')\n    df_feature = df_feature.merge(df_feature_100, how = 'left', left_on = 'time_id_', right_on = 'time_id__100')\n    # Drop unnecesary time_ids\n    df_feature.drop(['time_id__500','time_id__400', 'time_id__300', 'time_id__200','time_id__100'], axis = 1, inplace = True)\n\n    # Create row_id so we can merge later\n    stock_id = file_path.split('=')[1]\n    df_feature['row_id'] = df_feature['time_id_'].apply(lambda x: f'{stock_id}-{x}')\n    df_feature.drop(['time_id_'], axis = 1, inplace = True)\n\n    return df_feature","metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1631166892814,"user":{"displayName":"Eddie Zhang","photoUrl":"","userId":"01983443237361224085"},"user_tz":-480},"id":"GGGbBOWTN1ZU","execution":{"iopub.status.busy":"2021-09-15T07:48:40.376742Z","iopub.execute_input":"2021-09-15T07:48:40.377051Z","iopub.status.idle":"2021-09-15T07:48:40.410267Z","shell.execute_reply.started":"2021-09-15T07:48:40.377022Z","shell.execute_reply":"2021-09-15T07:48:40.409542Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df = book_preprocessor('Data/book_train.parquet/stock_id=0')\n# df.describe()","metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1631166892814,"user":{"displayName":"Eddie Zhang","photoUrl":"","userId":"01983443237361224085"},"user_tz":-480},"id":"AD8irCjsQ6mR","execution":{"iopub.status.busy":"2021-09-15T07:48:40.944363Z","iopub.execute_input":"2021-09-15T07:48:40.94468Z","iopub.status.idle":"2021-09-15T07:48:40.949316Z","shell.execute_reply.started":"2021-09-15T07:48:40.944651Z","shell.execute_reply":"2021-09-15T07:48:40.948012Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df[df['row_id']=='0-5']","metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1631166892814,"user":{"displayName":"Eddie Zhang","photoUrl":"","userId":"01983443237361224085"},"user_tz":-480},"id":"1F9L3wZ9RSkz","execution":{"iopub.status.busy":"2021-09-15T07:48:41.852385Z","iopub.execute_input":"2021-09-15T07:48:41.852658Z","iopub.status.idle":"2021-09-15T07:48:41.85685Z","shell.execute_reply.started":"2021-09-15T07:48:41.852623Z","shell.execute_reply":"2021-09-15T07:48:41.855875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Trade","metadata":{"id":"xzaEI44wN8Gq"}},{"cell_type":"code","source":"# df = pd.read_parquet('Data/trade_train.parquet/stock_id=1')\n# df_ob = pd.read_parquet('Data/book_train.parquet/stock_id=1')\n# df = df.groupby('time_id').apply(order_flow_tick)\n# df = df.groupby('time_id').apply(order_flow_quote,df_ob=df_ob)\n# df = df.droplevel(0).reset_index()\n# df = df[df['time_id']==5]\n# df_ob = df_ob[df_ob['time_id']==5]\n\n# df_ob['mid_price'] = (df_ob['ask_price1'] + df_ob['bid_price1']) / 2\n# overlap = df_ob['seconds_in_bucket'].isin(df['seconds_in_bucket'])\n# df['mid_price'] = df_ob.loc[overlap, ['mid_price']].values\n# df['order_flow'] = df['size']\n# df.loc[df['price']>df['mid_price'],'order_flow'] = -df['order_flow']\n# df.loc[df['price']==df['mid_price'],'order_flow'] = 0","metadata":{"executionInfo":{"elapsed":60156,"status":"ok","timestamp":1631166952967,"user":{"displayName":"Eddie Zhang","photoUrl":"","userId":"01983443237361224085"},"user_tz":-480},"id":"E4p09nZjcJwF","execution":{"iopub.status.busy":"2021-09-15T07:48:43.063268Z","iopub.execute_input":"2021-09-15T07:48:43.063595Z","iopub.status.idle":"2021-09-15T07:48:43.068063Z","shell.execute_reply.started":"2021-09-15T07:48:43.063565Z","shell.execute_reply":"2021-09-15T07:48:43.067133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df_ob","metadata":{"executionInfo":{"elapsed":17,"status":"ok","timestamp":1631166952969,"user":{"displayName":"Eddie Zhang","photoUrl":"","userId":"01983443237361224085"},"user_tz":-480},"id":"UJ_DdKLD99-e","execution":{"iopub.status.busy":"2021-09-15T07:48:43.561775Z","iopub.execute_input":"2021-09-15T07:48:43.562065Z","iopub.status.idle":"2021-09-15T07:48:43.566197Z","shell.execute_reply.started":"2021-09-15T07:48:43.562037Z","shell.execute_reply":"2021-09-15T07:48:43.565144Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# volume_per_bar = 50","metadata":{"executionInfo":{"elapsed":16,"status":"ok","timestamp":1631166952969,"user":{"displayName":"Eddie Zhang","photoUrl":"","userId":"01983443237361224085"},"user_tz":-480},"id":"mZYcoYhpnIEK","execution":{"iopub.status.busy":"2021-09-15T07:48:44.237655Z","iopub.execute_input":"2021-09-15T07:48:44.237945Z","iopub.status.idle":"2021-09-15T07:48:44.241807Z","shell.execute_reply.started":"2021-09-15T07:48:44.237917Z","shell.execute_reply":"2021-09-15T07:48:44.240938Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# preprocess trade data (for each stock id)\ndef trade_preprocessor(file_path, file_path_book):\n    df = pd.read_parquet(file_path)\n    df_ob = pd.read_parquet(file_path_book)\n\n    # calculate log return\n    df['log_return'] = df.groupby('time_id')['price'].apply(log_return)\n    \n    # calculate trade amount\n    df['amount']=df['price']*df['size']\n\n    # calculate average volume of each order\n    df['average_volume_per_trade'] = df['size'] / df['order_count']\n\n    # calculate trading order flow\n    df = df.groupby('time_id').apply(order_flow_tick)\n    df = df.groupby('time_id').apply(order_flow_quote,df_ob=df_ob)\n    df = df.droplevel(0).reset_index() # drop multi-index created when doing groupby\n\n    # create dict of functions for each feature for aggregations\n    create_feature_dict = {\n        'log_return':[realized_volatility],\n        'seconds_in_bucket':[count_unique],\n        'size':[np.sum, np.max, np.min],\n        'order_count':[np.sum,np.max],\n        'amount':[np.sum,np.max,np.min],\n        'average_volume_per_trade':[np.sum,np.max,np.min],\n        'order_flow_tick':[np.sum,np.max,np.min],\n        'order_flow_quote':[np.sum,np.max,np.min]\n    }\n    create_feature_dict_time = {\n        'log_return':[realized_volatility],\n        'seconds_in_bucket':[count_unique],\n        'size':[np.sum],\n        'order_count':[np.sum],\n        'amount':[np.sum,np.max,np.min],\n        'average_volume_per_trade':[np.sum,np.max,np.min],\n        'order_flow_tick':[np.sum,np.max,np.min],\n        'order_flow_quote':[np.sum,np.max,np.min]\n    }\n\n    # Function to get group stats for different windows (seconds in bucket)\n    def get_stats_window(fe_dict,seconds_in_bucket, add_suffix = False):\n        # Group by the window\n        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(fe_dict).reset_index()\n        # Rename columns joining suffix\n        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n        # Add a suffix to differentiate windows\n        if add_suffix:\n            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n        return df_feature\n    \n\n    # Get the stats for different windows\n    df_feature = get_stats_window(create_feature_dict,seconds_in_bucket = 0, add_suffix = False)\n    df_feature_500 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 500, add_suffix = True)\n    df_feature_400 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 400, add_suffix = True)\n    df_feature_300 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 300, add_suffix = True)\n    df_feature_200 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 200, add_suffix = True)\n    df_feature_100 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 100, add_suffix = True)\n    \n\n    # calculate tendency factors\n    df_lr = tendency_factors(df) \n    df_feature = df_feature.merge(df_lr, how = 'left', left_on = 'time_id_', right_on = 'time_id')\n        \n    # Merge all\n    df_feature = df_feature.merge(df_feature_500, how = 'left', left_on = 'time_id_', right_on = 'time_id__500')\n    df_feature = df_feature.merge(df_feature_400, how = 'left', left_on = 'time_id_', right_on = 'time_id__400')\n    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n    df_feature = df_feature.merge(df_feature_200, how = 'left', left_on = 'time_id_', right_on = 'time_id__200')\n    df_feature = df_feature.merge(df_feature_100, how = 'left', left_on = 'time_id_', right_on = 'time_id__100')\n    # Drop unnecesary time_ids\n    df_feature.drop(['time_id__500','time_id__400', 'time_id__300', 'time_id__200','time_id','time_id__100'], axis = 1, inplace = True)\n    \n    \n    df_feature = df_feature.add_prefix('trade_')\n    stock_id = file_path.split('=')[1]\n    df_feature['row_id'] = df_feature['trade_time_id_'].apply(lambda x:f'{stock_id}-{x}')\n    df_feature.drop(['trade_time_id_'], axis = 1, inplace = True)\n    return df_feature","metadata":{"executionInfo":{"elapsed":419,"status":"ok","timestamp":1631167102728,"user":{"displayName":"Eddie Zhang","photoUrl":"","userId":"01983443237361224085"},"user_tz":-480},"id":"Cz7XwpIvN-AO","execution":{"iopub.status.busy":"2021-09-15T07:48:44.620236Z","iopub.execute_input":"2021-09-15T07:48:44.620539Z","iopub.status.idle":"2021-09-15T07:48:44.6436Z","shell.execute_reply.started":"2021-09-15T07:48:44.620508Z","shell.execute_reply":"2021-09-15T07:48:44.642421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df = trade_preprocessor('Data/trade_train.parquet/stock_id=0','Data/book_train.parquet/stock_id=0')","metadata":{"executionInfo":{"elapsed":15,"status":"ok","timestamp":1631166952970,"user":{"displayName":"Eddie Zhang","photoUrl":"","userId":"01983443237361224085"},"user_tz":-480},"id":"mqUarXx_je1R","execution":{"iopub.status.busy":"2021-09-15T07:48:45.241047Z","iopub.execute_input":"2021-09-15T07:48:45.241408Z","iopub.status.idle":"2021-09-15T07:48:45.246219Z","shell.execute_reply.started":"2021-09-15T07:48:45.24137Z","shell.execute_reply":"2021-09-15T07:48:45.244968Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df[df['row_id']=='0-11']","metadata":{"executionInfo":{"elapsed":14,"status":"ok","timestamp":1631166952970,"user":{"displayName":"Eddie Zhang","photoUrl":"","userId":"01983443237361224085"},"user_tz":-480},"id":"y0iIH9O0jkYv","execution":{"iopub.status.busy":"2021-09-15T07:48:45.625743Z","iopub.execute_input":"2021-09-15T07:48:45.626065Z","iopub.status.idle":"2021-09-15T07:48:45.630132Z","shell.execute_reply.started":"2021-09-15T07:48:45.626032Z","shell.execute_reply":"2021-09-15T07:48:45.629072Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Merge","metadata":{"id":"5TzVy5d_PbtK"}},{"cell_type":"code","source":"# make preprocessing function in parallel (for each stock id)\ndef preprocessor(list_stock_ids, is_train = True):\n    \n    # Parrallel for loop\n    def for_joblib(stock_id):\n        # Train\n        if is_train:\n            file_path_book = \"book_train.parquet/stock_id=\" + str(stock_id)\n            file_path_trade = \"trade_train.parquet/stock_id=\" + str(stock_id)\n        # Test\n        else:\n            file_path_book = \"book_test.parquet/stock_id=\" + str(stock_id)\n            file_path_trade = \"trade_test.parquet/stock_id=\" + str(stock_id)\n    \n        # Preprocess book and trade data and merge them\n        df_tmp = pd.merge(book_preprocessor(file_path_book), trade_preprocessor(file_path_trade,file_path_book), on = 'row_id', how = 'left')\n        \n        # Return the merge dataframe\n        return df_tmp\n    \n    # Use parallel api to call paralle for loop\n    df = Parallel(n_jobs = -1, verbose = 1)(delayed(for_joblib)(stock_id) for stock_id in list_stock_ids)\n    # Concatenate all the dataframes that return from Parallel\n    df = pd.concat(df, ignore_index = True)\n    return df\n\n# get group stats for the stock_id and time_id\ndef get_time_stock(df):\n    vol_cols = ['log_return1_realized_volatility', 'log_return2_realized_volatility', 'log_return1_realized_volatility_400', 'log_return2_realized_volatility_400', \n                'log_return1_realized_volatility_300', 'log_return2_realized_volatility_300', 'log_return1_realized_volatility_200', 'log_return2_realized_volatility_200', \n                'trade_log_return_realized_volatility', 'trade_log_return_realized_volatility_400', 'trade_log_return_realized_volatility_300', 'trade_log_return_realized_volatility_200']\n\n\n    # Group by the stock id\n    df_stock_id = df.groupby(['stock_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n    # Rename columns joining suffix\n    df_stock_id.columns = ['_'.join(col) for col in df_stock_id.columns]\n    df_stock_id = df_stock_id.add_suffix('_' + 'stock')\n\n    # Group by the stock id\n    df_time_id = df.groupby(['time_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n    # Rename columns joining suffix\n    df_time_id.columns = ['_'.join(col) for col in df_time_id.columns]\n    df_time_id = df_time_id.add_suffix('_' + 'time')\n    \n    # Merge with original dataframe\n    df = df.merge(df_stock_id, how = 'left', left_on = ['stock_id'], right_on = ['stock_id__stock'])\n    df = df.merge(df_time_id, how = 'left', left_on = ['time_id'], right_on = ['time_id__time'])\n    df.drop(['stock_id__stock', 'time_id__time'], axis = 1, inplace = True)\n    return df","metadata":{"executionInfo":{"elapsed":14,"status":"ok","timestamp":1631166952970,"user":{"displayName":"Eddie Zhang","photoUrl":"","userId":"01983443237361224085"},"user_tz":-480},"id":"K_3-VoXMPdEM","execution":{"iopub.status.busy":"2021-09-15T07:48:46.937175Z","iopub.execute_input":"2021-09-15T07:48:46.937526Z","iopub.status.idle":"2021-09-15T07:48:46.952381Z","shell.execute_reply.started":"2021-09-15T07:48:46.937492Z","shell.execute_reply":"2021-09-15T07:48:46.951227Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Main","metadata":{"id":"E-Yya_qUivY_"}},{"cell_type":"code","source":"# Read train and test\ntrain, test = read_train_test()\n\n# Get unique stock ids \ntrain_stock_ids = train['stock_id'].unique()\n# Preprocess them using Parallel and our single stock id functions\ntrain_ = preprocessor(train_stock_ids, is_train = True)\ntrain = train.merge(train_, on = ['row_id'], how = 'left')\n\n# Get unique stock ids \ntest_stock_ids = test['stock_id'].unique()\n# Preprocess them using Parallel and our single stock id functions\ntest_ = preprocessor(test_stock_ids, is_train = False)\ntest = test.merge(test_, on = ['row_id'], how = 'left')\n\n# Get group stats of time_id and stock_id\ntrain = get_time_stock(train)\ntest = get_time_stock(test)\n\n# train = pd.read_csv(train_file)\n# train.drop('Unnamed: 0',axis=1,inplace=True)","metadata":{"id":"LNTHq72Niw7I","outputId":"25a641a1-2a91-4dae-91d1-b00682e5e1a7","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# save the train\ntrain.to_csv(train_file)","metadata":{"execution":{"iopub.status.busy":"2021-09-15T07:48:54.019787Z","iopub.status.idle":"2021-09-15T07:48:54.021114Z","shell.execute_reply.started":"2021-09-15T07:48:54.020182Z","shell.execute_reply":"2021-09-15T07:48:54.020414Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}