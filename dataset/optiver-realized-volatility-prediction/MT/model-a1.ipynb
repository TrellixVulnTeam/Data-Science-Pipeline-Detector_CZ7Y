{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from IPython.core.display import display, HTML\n\nimport pandas as pd\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport glob\nimport os\nimport gc\n\nfrom joblib import Parallel, delayed\n\nfrom sklearn import preprocessing, model_selection\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import QuantileTransformer\nfrom sklearn.metrics import r2_score\n\nimport matplotlib.pyplot as plt \nimport seaborn as sns\nimport numpy.matlib\n\n\npath_submissions = '/'\n\ntarget_name = 'target'\nscores_folds = {}\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\npd.set_option('max_rows', 300)\npd.set_option('max_columns', 300)\n\nimport os\nimport glob\n# data directory\ndata_dir = '../input/optiver-realized-volatility-prediction/'\ndef calc_wap(df):\n    wap = (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1'])/(df['bid_size1'] + df['ask_size1'])\n    return wap\ndef calc_wap2(df):\n    wap = (df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2'])/(df['bid_size2'] + df['ask_size2'])\n    return wap\ndef log_return(list_stock_prices):\n    return np.log(list_stock_prices).diff()\ndef realized_volatility(series):\n    return np.sqrt(np.sum(series**2))\ndef count_unique(series):\n    return len(np.unique(series))\ndef ffill(data_df):\n    data_df=data_df.set_index(['time_id', 'seconds_in_bucket'])\n    data_df = data_df.reindex(pd.MultiIndex.from_product([data_df.index.levels[0], \n                                                          np.arange(0,600)], \n                                                         names = ['time_id', 'seconds_in_bucket']), method='ffill')\n    return data_df.reset_index()\n\ndef grab_more_f(trade_path, book_path, stock_id):\n    \n    trade_train = pd.read_parquet(trade_path)\n    book_train = pd.read_parquet(book_path)\n    \n    merge = trade_train.merge(book_train,on=['time_id','seconds_in_bucket'],how='right')\n    merge = merge.sort_values(['time_id','seconds_in_bucket'])\n    merge.reset_index(inplace=True,drop=True)\n    merge['new_feature'] = merge['bid_price1']-merge['ask_price1']\n    merge['new_feature'] = abs(merge['new_feature'])\n    merge['new_feature'] = merge['new_feature']/merge['price']\n    \n    \n    merge['stock_id']=stock_id\n    merge['row_id']=merge['stock_id'].astype(str)+'-'+merge['time_id'].astype(str)\n    dict_1 = merge.groupby(['row_id'])['new_feature'].mean().to_dict()\n    dict_2 = {}#merge.groupby(['row_id'])['new_feature2'].mean().to_dict()\n    dict_3 = {}#merge.groupby(['row_id'])['new_feature3'].mean().to_dict()\n    dict_4 = {}#merge.groupby(['row_id'])['new_feature4'].mean().to_dict()\n\n    #dict_8 = merge.groupby(['row_id'])['order_count_isna'].max().to_dict()\n    #dict_7 = merge.groupby(['row_id'])['bid_price1_isna'].sum().to_dict()\n    \n    merge = merge[merge['seconds_in_bucket']>300]\n    dict_7 = {}#merge.groupby(['row_id'])['new_feature'].mean().to_dict()\n    dict_5 = {}#merge.groupby(['row_id'])['new_feature2'].mean().to_dict()\n    dict_6 = {}#merge.groupby(['row_id'])['new_feature3'].mean().to_dict()\n    dict_8 = {}#merge.groupby(['row_id'])['new_feature4'].mean().to_dict()\n    #print(len(merge.time_id.unique()))\n    return pd.DataFrame(),pd.DataFrame(),dict_1,dict_2,dict_3,dict_4,dict_5,dict_6,dict_7,dict_8\nfrom scipy.stats import kurtosis\nfrom scipy.stats import skew\n#kurtosis(x).\n\ndef preprocessor_book(file_path):\n    \n    df = pd.read_parquet(file_path)\n    #df = ffill(df)\n    #calculate return etc\n    df['wap'] = calc_wap(df)\n    df['log_return'] = df.groupby('time_id')['wap'].apply(log_return)\n    \n    df['wap2'] = calc_wap2(df)\n    df['log_return2'] = df.groupby('time_id')['wap2'].apply(log_return)\n    \n    #df['wap_balance'] = abs(df['wap'] - df['wap2'])\n    \n    df['price_spread'] = (df['ask_price1'] - df['bid_price1']) / ((df['ask_price1'] + df['bid_price1'])/2)\n    #df['bid_spread'] = df['bid_price1'] - df['bid_price2']\n    df['ask_spread'] = df['ask_price1'] - df['ask_price2']\n    df['total_volume'] = (df['ask_size1'] + df['ask_size2']) + (df['bid_size1'] + df['bid_size2'])\n    #df['volume_imbalance'] = abs((df['ask_size1'] + df['ask_size2']) - (df['bid_size1'] + df['bid_size2']))\n    #df['wap_dif']=df['wap'] - df['wap2']\n    #df[\"bid_ask_spread\"] = abs(df['bid_spread'] - df['ask_spread'])\n    df['log_return_diff']=abs(df['log_return'] - df['log_return2'])\n    \n    #df['h_spread_l1'] = df['ask_price1'] - df['bid_price1']\n    #df['h_spread_l2'] = df['ask_price2'] - df['bid_price2']\n    #df['v_spread_b'] = df['bid_price1'] - df['bid_price2']\n    #df['v_spread_a'] = df['ask_price1'] - df['ask_price2']\n    \n\n    #dict for aggregate\n    create_feature_dict = {\n        #'ask_price1':[count_unique],\n        #'ask_price2':[count_unique],\n        #'bid_price1':[count_unique],\n        #'bid_price2':[count_unique],\n        'log_return':[realized_volatility,np.std],\n        'log_return2':[realized_volatility,np.std],\n        #'wap_balance':[np.mean,np.max,np.min,np.std, kurtosis, skew],\n        'price_spread':[np.mean],#,np.max,np.min,np.std, kurtosis, skew],\n        #'bid_spread':[np.mean,np.max,np.min,np.std, kurtosis, skew],\n        'ask_spread':[np.max],\n        #'volume_imbalance':[np.mean,np.max,np.min,np.std, kurtosis, skew],\n        'total_volume':[np.min],\n        'wap':[np.mean],\n        #'wap2':[np.mean,np.max,np.min,np.std, kurtosis, skew],\n        #'wap_dif':[np.mean,np.std,np.max,np.min,kurtosis,skew],\n        'log_return_diff':[np.mean],#,np.std,np.max,np.min,kurtosis,skew],\n        #\"bid_ask_spread\":[np.mean]\n        \n        \n        #'h_spread_l1':[np.mean],\n        #'h_spread_l2':[np.mean],\n        #'v_spread_b':[np.mean],\n        #'v_spread_a':[np.mean],\n            }\n\n    #####groupby / all seconds\n    df_feature = pd.DataFrame(df.groupby(['time_id']).agg(create_feature_dict)).reset_index()\n    df_feature.columns = ['_'.join(col) for col in df_feature.columns] #time_id is changed to time_id_\n      \n        \n\n        \n        \n    ######groupby / last XX seconds\n    last_seconds = [300, 150, 50]\n    \n    for second in last_seconds:\n        second = 600 - second \n    \n        df_feature_sec = pd.DataFrame(df.query(f'seconds_in_bucket >= {second}').groupby(['time_id']).agg(create_feature_dict)).reset_index()\n\n        df_feature_sec.columns = ['_'.join(col) for col in df_feature_sec.columns] #time_id is changed to time_id_\n     \n        df_feature_sec = df_feature_sec.add_suffix('_' + str(second))\n\n        df_feature = pd.merge(df_feature,df_feature_sec,how='left',left_on='time_id_',right_on=f'time_id__{second}')\n        df_feature = df_feature.drop([f'time_id__{second}'],axis=1)\n                \n    \n    #create row_id\n    stock_id = file_path.split('=')[1]\n    df_feature['row_id'] = df_feature['time_id_'].apply(lambda x:f'{stock_id}-{x}')\n    df_feature = df_feature.drop(['time_id_'],axis=1)        \n        \n        \n        \n    df['stock_id']=stock_id\n    df['row_id']=df['stock_id'].astype(str)+'-'+df['time_id'].astype(str)\n    \n  \n    return df_feature\n\ndef preprocessor_trade(file_path):\n    df = pd.read_parquet(file_path)\n    df['log_return'] = df.groupby('time_id')['price'].apply(log_return)\n\n    aggregate_dictionary = {\n        'log_return':[realized_volatility],#,np.max,np.min,np.std, kurtosis, skew],\n        'seconds_in_bucket':[count_unique, sum],#,np.max,np.min,np.std, kurtosis, skew],\n        'size':[np.sum],#,np.max,np.min,np.std, kurtosis, skew],\n       # 'order_count':[np.mean,np.max,np.min,np.std, kurtosis, skew, np.sum],\n        #'my_trade_feature_1':[np.mean,np.max,np.min,np.std, kurtosis, skew],\n        'price':[np.std]#, np.max, np.min, kurtosis, skew, count_unique]#,np.std,np.max,np.min\n    }\n    \n    df_feature = df.groupby('time_id').agg(aggregate_dictionary)\n    \n    df_feature = df_feature.reset_index()\n    df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n\n    \n    ######groupby / last XX seconds\n    last_seconds = [300, 150, 50]\n    \n    for second in last_seconds:\n        second = 600 - second\n    \n        df_feature_sec = df.query(f'seconds_in_bucket >= {second}').groupby('time_id').agg(aggregate_dictionary)\n        df_feature_sec = df_feature_sec.reset_index()\n        \n        df_feature_sec.columns = ['_'.join(col) for col in df_feature_sec.columns]\n        df_feature_sec = df_feature_sec.add_suffix('_' + str(second))\n        \n        df_feature = pd.merge(df_feature,df_feature_sec,how='left',left_on='time_id_',right_on=f'time_id__{second}')\n        df_feature = df_feature.drop([f'time_id__{second}'],axis=1)\n            \n    \n    df_feature = df_feature.add_prefix('trade_')\n    stock_id = file_path.split('=')[1]\n    df_feature['row_id'] = df_feature['trade_time_id_'].apply(lambda x:f'{stock_id}-{x}')\n    df_feature = df_feature.drop(['trade_time_id_'],axis=1)  \n    \n    \n    df['stock_id']=stock_id\n    df['row_id']=df['stock_id'].astype(str)+'-'+df['time_id'].astype(str)\n    \n    return df_feature\n\ndef preprocessor(list_stock_ids, is_train = True):\n    from joblib import Parallel, delayed # parallel computing to save time\n    df = pd.DataFrame()\n    \n    def for_joblib(stock_id):\n        if is_train:\n            file_path_book = data_dir + \"book_train.parquet/stock_id=\" + str(stock_id)\n            file_path_trade = data_dir + \"trade_train.parquet/stock_id=\" + str(stock_id)\n        else:\n            file_path_book = data_dir + \"book_test.parquet/stock_id=\" + str(stock_id)\n            file_path_trade = data_dir + \"trade_test.parquet/stock_id=\" + str(stock_id)\n            \n        df_tmp = pd.merge(preprocessor_book(file_path_book),preprocessor_trade(file_path_trade),on='row_id',how='left')\n\n        \n        df_tmp2,df_tmp3,dict_1,dict_2,dict_3,dict_4,dict_5,dict_6,dict_7,dict_8= grab_more_f(file_path_trade, file_path_book, stock_id)\n\n        df_tmp['new_feat']=[dict_1.get(n,np.nan) for n in df_tmp['row_id']]\n        \n        return pd.concat([df,df_tmp])       \n        \n    \n    df = Parallel(n_jobs=-1, verbose=1)(\n        delayed(for_joblib)(stock_id) for stock_id in list_stock_ids\n        )\n\n    df =  pd.concat(df,ignore_index = True)\n    return df\n\n#kurtosis median quantile\ndef add_features_by_time(df, feature, by='time_id'):\n    temp = df.groupby(by)[feature].mean().to_dict()\n    df['avg1_'+feature+'_by_'+by] = [temp[n] for n in df[by]]\n\n    temp = df.groupby(by)[feature].std().to_dict()\n    df['std1_'+feature+'_by_'+by] = [temp[n] for n in df[by]]\n\n    temp = df.groupby(by)[feature].max().to_dict()\n    df['max1_'+feature+'_by_'+by] = [temp[n] for n in df[by]]\n\n    temp = df.groupby(by)[feature].min().to_dict()\n    df['min1_'+feature+'_by_'+by] = [temp[n] for n in df[by]]\n    \n    temp = df.groupby(by)[feature].skew().to_dict()\n    df['skew1_'+feature+'_by_'+by] = [temp[n] for n in df[by]]\n    \n    temp = df.groupby(by)[feature].median().to_dict()\n    df['median1_'+feature+'_by_'+by] = [temp[n] for n in df[by]]    \n    \n    temp = df.groupby(by)[feature].quantile(.25).to_dict()\n    df['quantile_025_1_'+feature+'_by_'+by] = [temp[n] for n in df[by]]  \n\n    temp = df.groupby(by)[feature].quantile(.75).to_dict()\n    df['quantile_075_1_'+feature+'_by_'+by] = [temp[n] for n in df[by]]  \n    \n    temp = df.groupby(by)[feature].apply(pd.DataFrame.kurt).to_dict()\n    df['kurt1_'+feature+'_by_'+by] = [temp[n] for n in df[by]] \n    \n    #.apply(pd.DataFrame.kurt)\n    \n    return df\n\n\n\n\n\n\ntest = pd.read_csv(data_dir + 'test.csv')\ntest_ids = test.stock_id.unique()\ndf_test = preprocessor(list_stock_ids= test_ids, is_train = False)\ndf_test = test.merge(df_test, on = ['row_id'], how = 'left')\ndf_test.head()\n\n\ntrain = pd.read_csv('../input/optiver-realized-volatility-prediction/train.csv')\ntrain['row_id'] = train['stock_id'].astype(str)+'-'+train['time_id'].astype(str)\ntrain_ids = train.stock_id.unique()\ndf_train = preprocessor(list_stock_ids= train_ids, is_train = True)\ndf_train.drop(['stock_id','time_id'],axis=1,inplace=True,errors='ignore')\ndf_train = train.merge(df_train, on = ['row_id'], how = 'left')\n\ncols = [\n\n        'log_return_realized_volatility',\n        'log_return2_realized_volatility_450',\n        'log_return2_realized_volatility_300',\n    \n        'trade_log_return_realized_volatility_300',\n        'trade_seconds_in_bucket_count_unique',\n        'trade_seconds_in_bucket_count_unique_450',\n        'trade_size_sum',\n        'trade_size_sum_300',\n        'trade_size_sum_450',\n        \n       ]\n\nfrom tqdm import tqdm\nfor col in tqdm(cols):\n    df_train = add_features_by_time(df_train, col, by='time_id')\n    df_test = add_features_by_time(df_test, col, by='time_id')\n\ndf_test['ff1']=df_test['log_return_realized_volatility_300']/df_test['log_return_realized_volatility']\ndf_train['ff1']=df_train['log_return_realized_volatility_300']/df_train['log_return_realized_volatility']\n\n\ndf_train['size_tau'] = np.sqrt( 1/ df_train['trade_seconds_in_bucket_count_unique'] )\ndf_test['size_tau'] = np.sqrt( 1/ df_test['trade_seconds_in_bucket_count_unique'] )\n\ndf_train['stock_id'] = df_train['stock_id'].astype(int)\ndf_test['stock_id'] = df_test['stock_id'].astype(int)\nimport os\nimport sys\nimport numpy as np\nimport pandas as pd\nimport random\nfrom collections import Counter, defaultdict\nfrom sklearn import model_selection\nimport lightgbm as lgbm\n# ---- GroupKFold ----\nclass GroupKFold(object):\n    \"\"\"\n    GroupKFold with random shuffle with a sklearn-like structure\n    \"\"\"\n\n    def __init__(self, n_splits=4, shuffle=True, random_state=42):\n        self.n_splits = n_splits\n        self.shuffle = shuffle\n        self.random_state = random_state\n\n    def get_n_splits(self, X=None, y=None, group=None):\n        return self.n_splits\n\n    def split(self, X, y, group):\n        kf = model_selection.KFold(n_splits=self.n_splits, shuffle=self.shuffle, random_state=self.random_state)\n        unique_ids = group.unique()\n        for fold, (tr_group_idx, va_group_idx) in enumerate(kf.split(unique_ids)):\n            # split group\n            tr_group, va_group = unique_ids[tr_group_idx], unique_ids[va_group_idx]\n            train_idx = np.where(group.isin(tr_group))[0]\n            val_idx = np.where(group.isin(va_group))[0]\n            yield train_idx, val_idx\n            \n\ndef rmspe(y_true, y_pred):\n    return  (np.sqrt(np.mean(np.square((y_true - y_pred) / y_true))))\n\ndef feval_RMSPE(preds, lgbm_train):\n    labels = lgbm_train.get_label()\n    return 'RMSPE', round(rmspe(y_true = labels, y_pred = preds),5), False\n\nparams = {\n      \"objective\": \"rmse\", \n      \"metric\": \"rmse\", \n      \"boosting_type\": \"gbdt\",\n      'early_stopping_rounds': 500,\n      'learning_rate': 0.01,\n      'lambda_l1': 1,\n      'lambda_l2': 1,\n      'feature_fraction': 0.8,\n      'bagging_fraction': 0.8,\n    'verbose': -1\n  }\n\ncols = '''0.2955 ± 0.0117\tlog_return_realized_volatility\n0.1010 ± 0.0011\tlog_return_realized_volatility_300\n0.0439 ± 0.0012\tstock_id\n0.0396 ± 0.0010\tlog_return2_realized_volatility\n0.0267 ± 0.0001\tlog_return2_realized_volatility_300\n0.0071 ± 0.0001\tlog_return2_realized_volatility_450\n0.0070 ± 0.0002\tlog_return_realized_volatility_450\n0.0047 ± 0.0002\ttrade_log_return_realized_volatility\n0.0067 ± 0.0003\ttrade_log_return_realized_volatility_450\n0.0035 ± 0.0001\ttrade_log_return_realized_volatility_300\n0.0113 ± 0.0003\tmin1_log_return2_realized_volatility_300_by_time_id\n0.0101 ± 0.0007\tquantile_025_1_log_return2_realized_volatility_450_by_time_id\n0.0077 ± 0.0002\tprice_spread_mean_550\n0.0057 ± 0.0001\tmax1_trade_seconds_in_bucket_count_unique_450_by_time_id\n0.0055 ± 0.0001\tnew_feat\n0.0052 ± 0.0002\tquantile_025_1_trade_size_sum_450_by_time_id\n0.0043 ± 0.0001\tmax1_trade_seconds_in_bucket_count_unique_by_time_id\n0.0031 ± 0.0000\tlog_return_realized_volatility_550\n0.0025 ± 0.0006\tavg1_trade_size_sum_300_by_time_id\n0.0024 ± 0.0001\tlog_return2_realized_volatility_550\n0.0024 ± 0.0001\ttrade_price_std_300\n0.0023 ± 0.0002\tstd1_log_return_realized_volatility_by_time_id\n0.0020 ± 0.0001\ttrade_price_std_550\n0.0017 ± 0.0002\tmedian1_trade_seconds_in_bucket_count_unique_by_time_id\n0.0016 ± 0.0002\tkurt1_trade_size_sum_by_time_id\n0.0008 ± 0.0003\tff1\n0.0007 ± 0.0001\tmin1_trade_log_return_realized_volatility_300_by_time_id\n0.0007 ± 0.0001\ttotal_volume_amin_450\n0.0007 ± 0.0001\tquantile_075_1_trade_size_sum_by_time_id\n0.0007 ± 0.0001\tmedian1_log_return2_realized_volatility_300_by_time_id\n0.0007 ± 0.0001\twap_mean_550\n0.0007 ± 0.0001\task_spread_amax_450\n0.0007 ± 0.0001\tlog_return2_std_550\n0.0007 ± 0.0001\tlog_return_diff_mean_300\n0.0007 ± 0.0001\tbid_spread_amin_300\n0.0007 ± 0.0001\tsize_tau'''\n\n\ncols = [n.split('\\t')[1] for n in cols.split('\\n')]\n\nmodels = []                          # models\nscores = 0.0                         # validation score\n\ngroups = df_train['time_id']\nncols = cols#+[n]\nX_copy = df_train[[n for n in df_train.columns if n in ncols]]\ny =df_train['target']\n\nn_splits = 24\n\nmodels =[]\nf_importance=[]\nrandom_states=[42, 1976, 888893, 524]\n\nnew_params = [\n    {'num_leaves': 337, 'max_depth': 5, 'min_data_in_leaf': 25, \n 'bagging_fraction': 0.25669443049008234,\n 'feature_fraction': 0.9828506707021959, \n 'lambda_l1': 4.560681832518118, 'lambda_l2': 6.551040117920411},\n    \n    {'num_leaves': 368, 'max_depth': 7, 'min_data_in_leaf': 86, \n 'bagging_fraction': 0.8214445330138478, 'feature_fraction': 0.6898653647476735, \n 'lambda_l1': 1.186553228787509, 'lambda_l2': 5.543011768727234, \n 'max_bin': 485, 'bagging_freq': 56},\n    \n    {'num_leaves': 33, 'max_depth': 8, 'min_data_in_leaf': 88,\n               'bagging_fraction': 0.7332102735163316, 'feature_fraction': 0.8186377930627372,\n               'lambda_l1': 6.126340153869034, 'lambda_l2': 9.609672774031658, \n               'max_bin': 412, 'boosting': 'gbdt'},\n    \n        {'num_leaves': 337, 'max_depth': 5, 'min_data_in_leaf': 25, \n 'bagging_fraction': 0.25669443049008234,\n 'feature_fraction': 0.9828506707021959, \n 'lambda_l1': 4.560681832518118, 'lambda_l2': 6.551040117920411},\n\n             ]\n\n    \nfor index_fold, random_state in enumerate(random_states):\n    \n    params.update(new_params[index_fold])\n    print(params)\n\n    cv = GroupKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n    kf = cv.split(X_copy, y, groups)\n    for fold, (trn_idx, val_idx) in enumerate(kf):\n\n        #print(\"Fold :\", fold+1)\n\n        # create dataset\n        X_train, y_train = X_copy.loc[trn_idx], y[trn_idx]\n        X_valid, y_valid = X_copy.loc[val_idx], y[val_idx]\n\n        #RMSPE weight\n        weights = 1/np.square(y_train)\n        lgbm_train = lgbm.Dataset(X_train,y_train,weight = weights)\n\n        weights = 1/np.square(y_valid)\n        lgbm_valid = lgbm.Dataset(X_valid,y_valid,reference = lgbm_train,weight = weights)\n\n        # model \n        model = lgbm.train(params=params,\n                          train_set=lgbm_train,\n                          valid_sets=[lgbm_train, lgbm_valid],\n                          num_boost_round=50000,         \n                          feval=feval_RMSPE,\n                          verbose_eval=500,\n                          categorical_feature = ['stock_id']                \n                         )\n\n        # validation \n        y_pred = model.predict(X_valid, num_iteration=model.best_iteration)\n\n        RMSPE = round(rmspe(y_true = y_valid, y_pred = y_pred),5)\n        print(f'Performance of the　prediction: , RMSPE: {RMSPE}')\n\n        #keep scores and models\n        scores += RMSPE / n_splits\n        models.append(model)\n        #print(\"*\" * 100)\n\n        model.save_model('lgb_a1_model_{seed}_{fold}.txt'.format(seed=random_state,fold=fold)\n                         , num_iteration=model.best_iteration)\n    \nprint('final_score',scores/len(random_states))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target = np.zeros(len(df_test))\n#light gbm models\nfor model in models:\n    pred = model.predict(df_test[list(X_train.columns)])\n    target += pred / len(models)\n    \ny_pred = df_test[['row_id']]\ny_pred = y_pred.assign(target =target)\ny_pred.to_csv('submission.csv',index = False)","metadata":{},"execution_count":null,"outputs":[]}]}