{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from IPython.core.display import display, HTML\n\nimport pandas as pd\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport glob\nimport os\nimport gc\n\nfrom joblib import Parallel, delayed\n\nfrom sklearn import preprocessing, model_selection\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import QuantileTransformer\nfrom sklearn.metrics import r2_score\n\nimport matplotlib.pyplot as plt \nimport seaborn as sns\nimport numpy.matlib\n\n\npath_submissions = '/'\n\ntarget_name = 'target'\nscores_folds = {}","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-09-01T03:42:39.018839Z","iopub.execute_input":"2021-09-01T03:42:39.0193Z","iopub.status.idle":"2021-09-01T03:42:40.177152Z","shell.execute_reply.started":"2021-09-01T03:42:39.019216Z","shell.execute_reply":"2021-09-01T03:42:40.17598Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# data directory\ndata_dir = '../input/optiver-realized-volatility-prediction/'\n\n# Function to calculate first WAP\ndef calc_wap1(df):\n    wap = (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']) / (df['bid_size1'] + df['ask_size1'])\n    return wap\n\n# Function to calculate second WAP\ndef calc_wap2(df):\n    wap = (df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2']) / (df['bid_size2'] + df['ask_size2'])\n    return wap\n\ndef calc_wap3(df):\n    wap = (df['bid_price1'] * df['bid_size1'] + df['ask_price1'] * df['ask_size1']) / (df['bid_size1'] + df['ask_size1'])\n    return wap\n\ndef calc_wap4(df):\n    wap = (df['bid_price2'] * df['bid_size2'] + df['ask_price2'] * df['ask_size2']) / (df['bid_size2'] + df['ask_size2'])\n    return wap\n\n# Function to calculate the log of the return\n# Remember that logb(x / y) = logb(x) - logb(y)\ndef log_return(series):\n    return np.log(series).diff()\n\n# Calculate the realized volatility\ndef realized_volatility(series):\n    return np.sqrt(np.sum(series**2))\n\n# Function to count unique elements of a series\ndef count_unique(series):\n    return len(np.unique(series))\n\n# Function to read our base train and test set\ndef read_train_test():\n    #train = pd.read_csv('../input/optiver-realized-volatility-prediction/train.csv')\n    test = pd.read_csv('../input/optiver-realized-volatility-prediction/test.csv')\n    # Create a key to merge with book and trade data\n    #train['row_id'] = train['stock_id'].astype(str) + '-' + train['time_id'].astype(str)\n    test['row_id'] = test['stock_id'].astype(str) + '-' + test['time_id'].astype(str)\n    #print(f'Our training set has {train.shape[0]} rows')\n    return test\n\n# Function to preprocess book data (for each stock id)\ndef book_preprocessor(file_path):\n    df = pd.read_parquet(file_path)\n    # Calculate Wap\n    df['wap1'] = calc_wap1(df)\n    df['wap2'] = calc_wap2(df)\n    df['wap3'] = calc_wap3(df)\n    df['wap4'] = calc_wap4(df)\n    # Calculate log returns\n    df['log_return1'] = df.groupby(['time_id'])['wap1'].apply(log_return)\n    df['log_return2'] = df.groupby(['time_id'])['wap2'].apply(log_return)\n    df['log_return3'] = df.groupby(['time_id'])['wap3'].apply(log_return)\n    df['log_return4'] = df.groupby(['time_id'])['wap4'].apply(log_return)\n    # Calculate wap balance\n    df['wap_balance'] = abs(df['wap1'] - df['wap2'])\n    # Calculate spread\n    df['price_spread'] = (df['ask_price1'] - df['bid_price1']) / ((df['ask_price1'] + df['bid_price1']) / 2)\n    df['price_spread2'] = (df['ask_price2'] - df['bid_price2']) / ((df['ask_price2'] + df['bid_price2']) / 2)\n    df['bid_spread'] = df['bid_price1'] - df['bid_price2']\n    df['ask_spread'] = df['ask_price1'] - df['ask_price2']\n    df[\"bid_ask_spread\"] = abs(df['bid_spread'] - df['ask_spread'])\n    df['total_volume'] = (df['ask_size1'] + df['ask_size2']) + (df['bid_size1'] + df['bid_size2'])\n    df['volume_imbalance'] = abs((df['ask_size1'] + df['ask_size2']) - (df['bid_size1'] + df['bid_size2']))\n    \n    # Dict for aggregations\n    create_feature_dict = {\n        'wap1': [np.sum, np.std],\n        'wap2': [np.sum, np.std],\n        'wap3': [np.sum, np.std],\n        'wap4': [np.sum, np.std],\n        'log_return1': [realized_volatility],\n        'log_return2': [realized_volatility],\n        'log_return3': [realized_volatility],\n        'log_return4': [realized_volatility],\n        'wap_balance': [np.sum, np.max],\n        'price_spread':[np.sum, np.max],\n        'price_spread2':[np.sum, np.max],\n        'bid_spread':[np.sum, np.max],\n        'ask_spread':[np.sum, np.max],\n        'total_volume':[np.sum, np.max],\n        'volume_imbalance':[np.sum, np.max],\n        \"bid_ask_spread\":[np.sum,  np.max],\n    }\n    create_feature_dict_time = {\n        'log_return1': [realized_volatility],\n        'log_return2': [realized_volatility],\n        'log_return3': [realized_volatility],\n        'log_return4': [realized_volatility],\n    }\n    \n    # Function to get group stats for different windows (seconds in bucket)\n    def get_stats_window(fe_dict,seconds_in_bucket, add_suffix = False):\n        # Group by the window\n        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(fe_dict).reset_index()\n        # Rename columns joining suffix\n        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n        # Add a suffix to differentiate windows\n        if add_suffix:\n            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n        return df_feature\n    \n    # Get the stats for different windows\n    df_feature = get_stats_window(create_feature_dict,seconds_in_bucket = 0, add_suffix = False)\n    df_feature_500 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 500, add_suffix = True)\n    df_feature_400 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 400, add_suffix = True)\n    df_feature_300 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 300, add_suffix = True)\n    df_feature_200 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 200, add_suffix = True)\n    df_feature_100 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 100, add_suffix = True)\n\n    # Merge all\n    df_feature = df_feature.merge(df_feature_500, how = 'left', left_on = 'time_id_', right_on = 'time_id__500')\n    df_feature = df_feature.merge(df_feature_400, how = 'left', left_on = 'time_id_', right_on = 'time_id__400')\n    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n    df_feature = df_feature.merge(df_feature_200, how = 'left', left_on = 'time_id_', right_on = 'time_id__200')\n    df_feature = df_feature.merge(df_feature_100, how = 'left', left_on = 'time_id_', right_on = 'time_id__100')\n    # Drop unnecesary time_ids\n    df_feature.drop(['time_id__500','time_id__400', 'time_id__300', 'time_id__200','time_id__100'], axis = 1, inplace = True)\n    \n    \n    # Create row_id so we can merge\n    stock_id = file_path.split('=')[1]\n    df_feature['row_id'] = df_feature['time_id_'].apply(lambda x: f'{stock_id}-{x}')\n    df_feature.drop(['time_id_'], axis = 1, inplace = True)\n    return df_feature\n\n# Function to preprocess trade data (for each stock id)\ndef trade_preprocessor(file_path):\n    df = pd.read_parquet(file_path)\n    df['log_return'] = df.groupby('time_id')['price'].apply(log_return)\n    df['amount']=df['price']*df['size']\n    # Dict for aggregations\n    create_feature_dict = {\n        'log_return':[realized_volatility],\n        'seconds_in_bucket':[count_unique],\n        'size':[np.sum, np.max, np.min],\n        'order_count':[np.sum,np.max],\n        'amount':[np.sum,np.max,np.min],\n    }\n    create_feature_dict_time = {\n        'log_return':[realized_volatility],\n        'seconds_in_bucket':[count_unique],\n        'size':[np.sum],\n        'order_count':[np.sum],\n    }\n    # Function to get group stats for different windows (seconds in bucket)\n    def get_stats_window(fe_dict,seconds_in_bucket, add_suffix = False):\n        # Group by the window\n        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(fe_dict).reset_index()\n        # Rename columns joining suffix\n        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n        # Add a suffix to differentiate windows\n        if add_suffix:\n            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n        return df_feature\n    \n\n    # Get the stats for different windows\n    df_feature = get_stats_window(create_feature_dict,seconds_in_bucket = 0, add_suffix = False)\n    df_feature_500 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 500, add_suffix = True)\n    df_feature_400 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 400, add_suffix = True)\n    df_feature_300 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 300, add_suffix = True)\n    df_feature_200 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 200, add_suffix = True)\n    df_feature_100 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 100, add_suffix = True)\n    \n    def tendency(price, vol):    \n        df_diff = np.diff(price)\n        val = (df_diff/price[1:])*100\n        power = np.sum(val*vol[1:])\n        return(power)\n    \n    lis = []\n    for n_time_id in df['time_id'].unique():\n        df_id = df[df['time_id'] == n_time_id]        \n        tendencyV = tendency(df_id['price'].values, df_id['size'].values)      \n        f_max = np.sum(df_id['price'].values > np.mean(df_id['price'].values))\n        f_min = np.sum(df_id['price'].values < np.mean(df_id['price'].values))\n        df_max =  np.sum(np.diff(df_id['price'].values) > 0)\n        df_min =  np.sum(np.diff(df_id['price'].values) < 0)\n        # new\n        abs_diff = np.median(np.abs( df_id['price'].values - np.mean(df_id['price'].values)))        \n        energy = np.mean(df_id['price'].values**2)\n        iqr_p = np.percentile(df_id['price'].values,75) - np.percentile(df_id['price'].values,25)\n        \n        # vol vars\n        \n        abs_diff_v = np.median(np.abs( df_id['size'].values - np.mean(df_id['size'].values)))        \n        energy_v = np.sum(df_id['size'].values**2)\n        iqr_p_v = np.percentile(df_id['size'].values,75) - np.percentile(df_id['size'].values,25)\n        \n        lis.append({'time_id':n_time_id,'tendency':tendencyV,'f_max':f_max,'f_min':f_min,'df_max':df_max,'df_min':df_min,\n                   'abs_diff':abs_diff,'energy':energy,'iqr_p':iqr_p,'abs_diff_v':abs_diff_v,'energy_v':energy_v,'iqr_p_v':iqr_p_v})\n    \n    df_lr = pd.DataFrame(lis)\n        \n   \n    df_feature = df_feature.merge(df_lr, how = 'left', left_on = 'time_id_', right_on = 'time_id')\n    \n    # Merge all\n    df_feature = df_feature.merge(df_feature_500, how = 'left', left_on = 'time_id_', right_on = 'time_id__500')\n    df_feature = df_feature.merge(df_feature_400, how = 'left', left_on = 'time_id_', right_on = 'time_id__400')\n    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n    df_feature = df_feature.merge(df_feature_200, how = 'left', left_on = 'time_id_', right_on = 'time_id__200')\n    df_feature = df_feature.merge(df_feature_100, how = 'left', left_on = 'time_id_', right_on = 'time_id__100')\n    # Drop unnecesary time_ids\n    df_feature.drop(['time_id__500','time_id__400', 'time_id__300', 'time_id__200','time_id','time_id__100'], axis = 1, inplace = True)\n    \n    \n    df_feature = df_feature.add_prefix('trade_')\n    stock_id = file_path.split('=')[1]\n    df_feature['row_id'] = df_feature['trade_time_id_'].apply(lambda x:f'{stock_id}-{x}')\n    df_feature.drop(['trade_time_id_'], axis = 1, inplace = True)\n    return df_feature\n\n# Function to get group stats for the stock_id and time_id\ndef get_time_stock(df):\n    vol_cols = ['log_return1_realized_volatility', 'log_return2_realized_volatility', 'log_return1_realized_volatility_400', 'log_return2_realized_volatility_400', \n                'log_return1_realized_volatility_300', 'log_return2_realized_volatility_300', 'log_return1_realized_volatility_200', 'log_return2_realized_volatility_200', \n                'trade_log_return_realized_volatility', 'trade_log_return_realized_volatility_400', 'trade_log_return_realized_volatility_300', 'trade_log_return_realized_volatility_200']\n\n\n    # Group by the stock id\n    df_stock_id = df.groupby(['stock_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n    # Rename columns joining suffix\n    df_stock_id.columns = ['_'.join(col) for col in df_stock_id.columns]\n    df_stock_id = df_stock_id.add_suffix('_' + 'stock')\n\n    # Group by the stock id\n    df_time_id = df.groupby(['time_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n    # Rename columns joining suffix\n    df_time_id.columns = ['_'.join(col) for col in df_time_id.columns]\n    df_time_id = df_time_id.add_suffix('_' + 'time')\n    \n    # Merge with original dataframe\n    df = df.merge(df_stock_id, how = 'left', left_on = ['stock_id'], right_on = ['stock_id__stock'])\n    df = df.merge(df_time_id, how = 'left', left_on = ['time_id'], right_on = ['time_id__time'])\n    df.drop(['stock_id__stock', 'time_id__time'], axis = 1, inplace = True)\n    return df\n    \n# Funtion to make preprocessing function in parallel (for each stock id)\ndef preprocessor(list_stock_ids, is_train = True):\n    \n    # Parrallel for loop\n    def for_joblib(stock_id):\n        # Train\n        if is_train:\n            file_path_book = data_dir + \"book_train.parquet/stock_id=\" + str(stock_id)\n            file_path_trade = data_dir + \"trade_train.parquet/stock_id=\" + str(stock_id)\n        # Test\n        else:\n            file_path_book = data_dir + \"book_test.parquet/stock_id=\" + str(stock_id)\n            file_path_trade = data_dir + \"trade_test.parquet/stock_id=\" + str(stock_id)\n    \n        # Preprocess book and trade data and merge them\n        df_tmp = pd.merge(book_preprocessor(file_path_book), trade_preprocessor(file_path_trade), on = 'row_id', how = 'left')\n        \n        # Return the merge dataframe\n        return df_tmp\n    \n    # Use parallel api to call paralle for loop\n    df = Parallel(n_jobs = -1, verbose = 1)(delayed(for_joblib)(stock_id) for stock_id in list_stock_ids)\n    # Concatenate all the dataframes that return from Parallel\n    df = pd.concat(df, ignore_index = True)\n    return df\n\n# Function to calculate the root mean squared percentage error\ndef rmspe(y_true, y_pred):\n    return np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\n\n# Function to early stop with root mean squared percentage error\ndef feval_rmspe(y_pred, lgb_train):\n    y_true = lgb_train.get_label()\n    return 'RMSPE', rmspe(y_true, y_pred), False","metadata":{"execution":{"iopub.status.busy":"2021-09-01T03:42:40.624674Z","iopub.execute_input":"2021-09-01T03:42:40.625027Z","iopub.status.idle":"2021-09-01T03:42:40.691741Z","shell.execute_reply.started":"2021-09-01T03:42:40.624997Z","shell.execute_reply":"2021-09-01T03:42:40.690563Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Read train and test\ntrain =pd.read_pickle(\"../input/optiver006/train.pkl\")\ntest = read_train_test()\n\n# Get unique stock ids \n#train_stock_ids = train['stock_id'].unique()\n# Preprocess them using Parallel and our single stock id functions\n#train_ = preprocessor(train_stock_ids, is_train = True)\n#train = train.merge(train_, on = ['row_id'], how = 'left')\n\n# Get unique stock ids \ntest_stock_ids = test['stock_id'].unique()\n# Preprocess them using Parallel and our single stock id functions\ntest_ = preprocessor(test_stock_ids, is_train = False)\ntest = test.merge(test_, on = ['row_id'], how = 'left')\n\n# Get group stats of time_id and stock_id\n#train = get_time_stock(train)\ntest = get_time_stock(test)\n\ntrain1=train\ntest1=test","metadata":{"execution":{"iopub.status.busy":"2021-09-01T03:42:41.403437Z","iopub.execute_input":"2021-09-01T03:42:41.40382Z","iopub.status.idle":"2021-09-01T03:42:48.500897Z","shell.execute_reply.started":"2021-09-01T03:42:41.403786Z","shell.execute_reply":"2021-09-01T03:42:48.499634Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# replace by order sum (tau)\ntrain['size_tau'] = np.sqrt( 1/ train['trade_seconds_in_bucket_count_unique'] )\ntest['size_tau'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique'] )\n#train['size_tau_450'] = np.sqrt( 1/ train['trade_seconds_in_bucket_count_unique_450'] )\n#test['size_tau_450'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique_450'] )\ntrain['size_tau_400'] = np.sqrt( 1/ train['trade_seconds_in_bucket_count_unique_400'] )\ntest['size_tau_400'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique_400'] )\ntrain['size_tau_300'] = np.sqrt( 1/ train['trade_seconds_in_bucket_count_unique_300'] )\ntest['size_tau_300'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique_300'] )\n#train['size_tau_150'] = np.sqrt( 1/ train['trade_seconds_in_bucket_count_unique_150'] )\n#test['size_tau_150'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique_150'] )\ntrain['size_tau_200'] = np.sqrt( 1/ train['trade_seconds_in_bucket_count_unique_200'] )\ntest['size_tau_200'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique_200'] )","metadata":{"execution":{"iopub.status.busy":"2021-09-01T03:43:59.419178Z","iopub.execute_input":"2021-09-01T03:43:59.419629Z","iopub.status.idle":"2021-09-01T03:43:59.460023Z","shell.execute_reply.started":"2021-09-01T03:43:59.41959Z","shell.execute_reply":"2021-09-01T03:43:59.459127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['size_tau2'] = np.sqrt( 1/ train['trade_order_count_sum'] )\ntest['size_tau2'] = np.sqrt( 1/ test['trade_order_count_sum'] )\n#train['size_tau2_450'] = np.sqrt( 0.25/ train['trade_order_count_sum'] )\n#test['size_tau2_450'] = np.sqrt( 0.25/ test['trade_order_count_sum'] )\ntrain['size_tau2_400'] = np.sqrt( 0.33/ train['trade_order_count_sum'] )\ntest['size_tau2_400'] = np.sqrt( 0.33/ test['trade_order_count_sum'] )\ntrain['size_tau2_300'] = np.sqrt( 0.5/ train['trade_order_count_sum'] )\ntest['size_tau2_300'] = np.sqrt( 0.5/ test['trade_order_count_sum'] )\n#train['size_tau2_150'] = np.sqrt( 0.75/ train['trade_order_count_sum'] )\n#test['size_tau2_150'] = np.sqrt( 0.75/ test['trade_order_count_sum'] )\ntrain['size_tau2_200'] = np.sqrt( 0.66/ train['trade_order_count_sum'] )\ntest['size_tau2_200'] = np.sqrt( 0.66/ test['trade_order_count_sum'] )\n\n# delta tau\ntrain['size_tau2_d'] = train['size_tau2_400'] - train['size_tau2']\ntest['size_tau2_d'] = test['size_tau2_400'] - test['size_tau2']","metadata":{"execution":{"iopub.status.busy":"2021-09-01T03:43:59.983353Z","iopub.execute_input":"2021-09-01T03:43:59.983906Z","iopub.status.idle":"2021-09-01T03:44:00.030987Z","shell.execute_reply.started":"2021-09-01T03:43:59.98387Z","shell.execute_reply":"2021-09-01T03:44:00.029871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"colNames = [col for col in list(train.columns)\n            if col not in {\"stock_id\", \"time_id\", \"target\", \"row_id\"}]\nlen(colNames)","metadata":{"execution":{"iopub.status.busy":"2021-09-01T03:44:01.213762Z","iopub.execute_input":"2021-09-01T03:44:01.214311Z","iopub.status.idle":"2021-09-01T03:44:01.225088Z","shell.execute_reply.started":"2021-09-01T03:44:01.214277Z","shell.execute_reply":"2021-09-01T03:44:01.223924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.cluster import KMeans\n# making agg features\n\ntrain_p = pd.read_csv('../input/optiver-realized-volatility-prediction/train.csv')\ntrain_p = train_p.pivot(index='time_id', columns='stock_id', values='target')\n\ncorr = train_p.corr()\n\nids = corr.index\n\nkmeans = KMeans(n_clusters=7, random_state=0).fit(corr.values)\nprint(kmeans.labels_)\n\nl = []\nfor n in range(7):\n    l.append ( [ (x-1) for x in ( (ids+1)*(kmeans.labels_ == n)) if x > 0] )\n    \n\nmat = []\nmatTest = []\n\nn = 0\nfor ind in l:\n    print(ind)\n    newDf = train.loc[train['stock_id'].isin(ind) ]\n    newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n    newDf.loc[:,'stock_id'] = str(n)+'c1'\n    mat.append ( newDf )\n    \n    newDf = test.loc[test['stock_id'].isin(ind) ]    \n    newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n    newDf.loc[:,'stock_id'] = str(n)+'c1'\n    matTest.append ( newDf )\n    \n    n+=1\n    \nmat1 = pd.concat(mat).reset_index()\nmat1.drop(columns=['target'],inplace=True)\n\nmat2 = pd.concat(matTest).reset_index()","metadata":{"execution":{"iopub.status.busy":"2021-09-01T03:44:01.889058Z","iopub.execute_input":"2021-09-01T03:44:01.889503Z","iopub.status.idle":"2021-09-01T03:44:05.178034Z","shell.execute_reply.started":"2021-09-01T03:44:01.889464Z","shell.execute_reply":"2021-09-01T03:44:05.176908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mat2 = pd.concat([mat2,mat1.loc[mat1.time_id==5]])\nmat1 = mat1.pivot(index='time_id', columns='stock_id')\nmat1.columns = [\"_\".join(x) for x in mat1.columns.ravel()]\nmat1.reset_index(inplace=True)\n\nmat2 = mat2.pivot(index='time_id', columns='stock_id')\nmat2.columns = [\"_\".join(x) for x in mat2.columns.ravel()]\nmat2.reset_index(inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-09-01T03:44:05.18059Z","iopub.execute_input":"2021-09-01T03:44:05.181049Z","iopub.status.idle":"2021-09-01T03:44:05.362073Z","shell.execute_reply.started":"2021-09-01T03:44:05.181005Z","shell.execute_reply":"2021-09-01T03:44:05.360662Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nnn = ['time_id',\n     'log_return1_realized_volatility_0c1',\n     'log_return1_realized_volatility_1c1',     \n     'log_return1_realized_volatility_3c1',\n     'log_return1_realized_volatility_4c1',     \n     'log_return1_realized_volatility_6c1',\n     'total_volume_sum_0c1',\n     'total_volume_sum_1c1', \n     'total_volume_sum_3c1',\n     'total_volume_sum_4c1', \n     'total_volume_sum_6c1',\n     'trade_size_sum_0c1',\n     'trade_size_sum_1c1', \n     'trade_size_sum_3c1',\n     'trade_size_sum_4c1', \n     'trade_size_sum_6c1',\n     'trade_order_count_sum_0c1',\n     'trade_order_count_sum_1c1',\n     'trade_order_count_sum_3c1',\n     'trade_order_count_sum_4c1',\n     'trade_order_count_sum_6c1',      \n     'price_spread_sum_0c1',\n     'price_spread_sum_1c1',\n     'price_spread_sum_3c1',\n     'price_spread_sum_4c1',\n     'price_spread_sum_6c1',   \n     'bid_spread_sum_0c1',\n     'bid_spread_sum_1c1',\n     'bid_spread_sum_3c1',\n     'bid_spread_sum_4c1',\n     'bid_spread_sum_6c1',       \n     'ask_spread_sum_0c1',\n     'ask_spread_sum_1c1',\n     'ask_spread_sum_3c1',\n     'ask_spread_sum_4c1',\n     'ask_spread_sum_6c1',   \n     'volume_imbalance_sum_0c1',\n     'volume_imbalance_sum_1c1',\n     'volume_imbalance_sum_3c1',\n     'volume_imbalance_sum_4c1',\n     'volume_imbalance_sum_6c1',       \n     'bid_ask_spread_sum_0c1',\n     'bid_ask_spread_sum_1c1',\n     'bid_ask_spread_sum_3c1',\n     'bid_ask_spread_sum_4c1',\n     'bid_ask_spread_sum_6c1',\n     'size_tau2_0c1',\n     'size_tau2_1c1',\n     'size_tau2_3c1',\n     'size_tau2_4c1',\n     'size_tau2_6c1'] \ntrain = pd.merge(train,mat1[nnn],how='left',on='time_id')\ntest = pd.merge(test,mat2[nnn],how='left',on='time_id')","metadata":{"execution":{"iopub.status.busy":"2021-09-01T03:44:05.364689Z","iopub.execute_input":"2021-09-01T03:44:05.365147Z","iopub.status.idle":"2021-09-01T03:44:16.08135Z","shell.execute_reply.started":"2021-09-01T03:44:05.365103Z","shell.execute_reply":"2021-09-01T03:44:16.080169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gc\ndel mat1,mat2\ngc.collect()\n","metadata":{"execution":{"iopub.status.busy":"2021-09-01T03:44:16.083015Z","iopub.execute_input":"2021-09-01T03:44:16.083312Z","iopub.status.idle":"2021-09-01T03:44:16.251643Z","shell.execute_reply.started":"2021-09-01T03:44:16.083283Z","shell.execute_reply":"2021-09-01T03:44:16.250288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import KFold\nimport lightgbm as lgb\n\nseed0=2021\nparams0 = {\n    'objective': 'rmse',\n    'boosting_type': 'gbdt',\n    'max_depth': -1,\n    'max_bin':100,\n    'min_data_in_leaf':500,\n    'learning_rate': 0.05,\n    'subsample': 0.72,\n    'subsample_freq': 4,\n    'feature_fraction': 0.5,\n    'lambda_l1': 0.5,\n    'lambda_l2': 1.0,\n    'categorical_column':[0],\n    'seed':seed0,\n    'feature_fraction_seed': seed0,\n    'bagging_seed': seed0,\n    'drop_seed': seed0,\n    'data_random_seed': seed0,\n    'n_jobs':-1,\n    'verbose': -1}\nseed1=42\nparams1 = {\n        'learning_rate': 0.1,        \n        'lambda_l1': 2,\n        'lambda_l2': 7,\n        'num_leaves': 800,\n        'min_sum_hessian_in_leaf': 20,\n        'feature_fraction': 0.8,\n        'feature_fraction_bynode': 0.8,\n        'bagging_fraction': 0.9,\n        'bagging_freq': 42,\n        'min_data_in_leaf': 700,\n        'max_depth': 4,\n        'categorical_column':[0],\n        'seed': seed1,\n        'feature_fraction_seed': seed1,\n        'bagging_seed': seed1,\n        'drop_seed': seed1,\n        'data_random_seed': seed1,\n        'objective': 'rmse',\n        'boosting': 'gbdt',\n        'verbosity': -1,\n        'n_jobs':-1,\n    }\n# Function to early stop with root mean squared percentage error\ndef rmspe(y_true, y_pred):\n    return np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\n\ndef feval_rmspe(y_pred, lgb_train):\n    y_true = lgb_train.get_label()\n    return 'RMSPE', rmspe(y_true, y_pred), False\n\ndef train_and_evaluate_lgb(train, test, params):\n    # Hyperparammeters (just basic)\n    \n    features = [col for col in train.columns if col not in {\"time_id\", \"target\", \"row_id\"}]\n    y = train['target']\n    # Create out of folds array\n    oof_predictions = np.zeros(train.shape[0])\n    # Create test array to store predictions\n    test_predictions = np.zeros(test.shape[0])\n    # Create a KFold object\n    kfold = KFold(n_splits = 5, random_state = 2021, shuffle = True)\n    # Iterate through each fold\n    for fold, (trn_ind, val_ind) in enumerate(kfold.split(train)):\n        print(f'Training fold {fold + 1}')\n        x_train, x_val = train.iloc[trn_ind], train.iloc[val_ind]\n        y_train, y_val = y.iloc[trn_ind], y.iloc[val_ind]\n        # Root mean squared percentage error weights\n        train_weights = 1 / np.square(y_train)\n        val_weights = 1 / np.square(y_val)\n        train_dataset = lgb.Dataset(x_train[features], y_train, weight = train_weights)\n        val_dataset = lgb.Dataset(x_val[features], y_val, weight = val_weights)\n        model = lgb.train(params = params,\n                          num_boost_round=1000,\n                          train_set = train_dataset, \n                          valid_sets = [train_dataset, val_dataset], \n                          verbose_eval = 250,\n                          early_stopping_rounds=50,\n                          feval = feval_rmspe)\n        # Add predictions to the out of folds array\n        oof_predictions[val_ind] = model.predict(x_val[features])\n        # Predict the test set\n        test_predictions += model.predict(test[features]) / 5\n    rmspe_score = rmspe(y, oof_predictions)\n    print(f'Our out of folds RMSPE is {rmspe_score}')\n    lgb.plot_importance(model,max_num_features=20)\n    # Return test predictions\n    return test_predictions\n# Traing and evaluate\npredictions_lgb1= train_and_evaluate_lgb(train, test,params0)\n#test['target'] = predictions_lgb\n#test[['row_id', 'target']].to_csv('submission.csv',index = False)","metadata":{"execution":{"iopub.status.busy":"2021-09-01T03:45:10.82645Z","iopub.execute_input":"2021-09-01T03:45:10.826836Z","iopub.status.idle":"2021-09-01T03:56:09.118859Z","shell.execute_reply.started":"2021-09-01T03:45:10.826805Z","shell.execute_reply":"2021-09-01T03:56:09.117618Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import KFold\nimport lightgbm as lgb\n\nseed0=1111\nparams111 = {\n    'objective': 'rmse',\n    'boosting_type': 'gbdt',\n    'max_depth': -1,\n    'max_bin':100,\n    'min_data_in_leaf':500,\n    'learning_rate': 0.05,\n    'subsample': 0.72,\n    'subsample_freq': 4,\n    'feature_fraction': 0.5,\n    'lambda_l1': 0.5,\n    'lambda_l2': 1.0,\n    'categorical_column':[0],\n    'seed':seed0,\n    'feature_fraction_seed': seed0,\n    'bagging_seed': seed0,\n    'drop_seed': seed0,\n    'data_random_seed': seed0,\n    'n_jobs':-1,\n    'verbose': -1}\nseed1=42\nparams1 = {\n        'learning_rate': 0.1,        \n        'lambda_l1': 2,\n        'lambda_l2': 7,\n        'num_leaves': 800,\n        'min_sum_hessian_in_leaf': 20,\n        'feature_fraction': 0.8,\n        'feature_fraction_bynode': 0.8,\n        'bagging_fraction': 0.9,\n        'bagging_freq': 42,\n        'min_data_in_leaf': 700,\n        'max_depth': 4,\n        'categorical_column':[0],\n        'seed': seed1,\n        'feature_fraction_seed': seed1,\n        'bagging_seed': seed1,\n        'drop_seed': seed1,\n        'data_random_seed': seed1,\n        'objective': 'rmse',\n        'boosting': 'gbdt',\n        'verbosity': -1,\n        'n_jobs':-1,\n    }\n# Function to early stop with root mean squared percentage error\ndef rmspe(y_true, y_pred):\n    return np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\n\ndef feval_rmspe(y_pred, lgb_train):\n    y_true = lgb_train.get_label()\n    return 'RMSPE', rmspe(y_true, y_pred), False\n\ndef train_and_evaluate_lgb(train, test, params):\n    # Hyperparammeters (just basic)\n    \n    features = [col for col in train.columns if col not in {\"time_id\", \"target\", \"row_id\"}]\n    y = train['target']\n    # Create out of folds array\n    oof_predictions = np.zeros(train.shape[0])\n    # Create test array to store predictions\n    test_predictions = np.zeros(test.shape[0])\n    # Create a KFold object\n    kfold = KFold(n_splits = 5, random_state = 1111, shuffle = True)\n    # Iterate through each fold\n    for fold, (trn_ind, val_ind) in enumerate(kfold.split(train)):\n        print(f'Training fold {fold + 1}')\n        x_train, x_val = train.iloc[trn_ind], train.iloc[val_ind]\n        y_train, y_val = y.iloc[trn_ind], y.iloc[val_ind]\n        # Root mean squared percentage error weights\n        train_weights = 1 / np.square(y_train)\n        val_weights = 1 / np.square(y_val)\n        train_dataset = lgb.Dataset(x_train[features], y_train, weight = train_weights)\n        val_dataset = lgb.Dataset(x_val[features], y_val, weight = val_weights)\n        model = lgb.train(params = params,\n                          num_boost_round=1200,\n                          train_set = train_dataset, \n                          valid_sets = [train_dataset, val_dataset], \n                          verbose_eval = 250,\n                          early_stopping_rounds=50,\n                          feval = feval_rmspe)\n        # Add predictions to the out of folds array\n        oof_predictions[val_ind] = model.predict(x_val[features])\n        # Predict the test set\n        test_predictions += model.predict(test[features]) / 5\n    rmspe_score = rmspe(y, oof_predictions)\n    print(f'Our out of folds RMSPE is {rmspe_score}')\n    lgb.plot_importance(model,max_num_features=20)\n    # Return test predictions\n    return test_predictions\n# Traing and evaluate\npredictions_lgb2= train_and_evaluate_lgb(train, test,params111)\n#test['target'] = predictions_lgb\n#test[['row_id', 'target']].to_csv('submission.csv',index = False)","metadata":{"execution":{"iopub.status.busy":"2021-09-01T03:58:28.836609Z","iopub.execute_input":"2021-09-01T03:58:28.837056Z","iopub.status.idle":"2021-09-01T04:11:01.042457Z","shell.execute_reply.started":"2021-09-01T03:58:28.837022Z","shell.execute_reply":"2021-09-01T04:11:01.041309Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# predictions_lgb2,predictions_lgb1","metadata":{"execution":{"iopub.status.busy":"2021-09-01T04:11:04.284345Z","iopub.execute_input":"2021-09-01T04:11:04.285049Z","iopub.status.idle":"2021-09-01T04:11:04.295285Z","shell.execute_reply.started":"2021-09-01T04:11:04.28498Z","shell.execute_reply":"2021-09-01T04:11:04.293929Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nfrom numpy.random import seed\nseed(42)\nimport tensorflow as tf\ntf.random.set_seed(42)\nfrom tensorflow import keras\nimport numpy as np\nfrom keras import backend as K\ndef root_mean_squared_per_error(y_true, y_pred):\n         return K.sqrt(K.mean(K.square( (y_true - y_pred)/ y_true )))\n    \nes = tf.keras.callbacks.EarlyStopping(\n    monitor='val_loss', patience=20, verbose=0,\n    mode='min',restore_best_weights=True)\n\nplateau = tf.keras.callbacks.ReduceLROnPlateau(\n    monitor='val_loss', factor=0.2, patience=7, verbose=0,\n    mode='min')\n","metadata":{"execution":{"iopub.status.busy":"2021-09-01T04:12:12.025059Z","iopub.execute_input":"2021-09-01T04:12:12.025684Z","iopub.status.idle":"2021-09-01T04:12:19.165687Z","shell.execute_reply.started":"2021-09-01T04:12:12.025646Z","shell.execute_reply":"2021-09-01T04:12:19.164144Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# kfold based on the knn++ algorithm\n\nout_train = pd.read_csv('../input/optiver-realized-volatility-prediction/train.csv')\nout_train = out_train.pivot(index='time_id', columns='stock_id', values='target')\n\n#out_train[out_train.isna().any(axis=1)]\nout_train = out_train.fillna(out_train.mean())\nout_train.head()\n\n# code to add the just the read data after first execution\n\n# data separation based on knn ++\nnfolds = 5 # number of folds\nindex = []\ntotDist = []\nvalues = []\n# generates a matriz with the values of \nmat = out_train.values\n\nscaler = MinMaxScaler(feature_range=(-1, 1))\nmat = scaler.fit_transform(mat)\n\nnind = int(mat.shape[0]/nfolds) # number of individuals\n\n# adds index in the last column\nmat = np.c_[mat,np.arange(mat.shape[0])]\n\n\nlineNumber = np.random.choice(np.array(mat.shape[0]), size=nfolds, replace=False)\n\nlineNumber = np.sort(lineNumber)[::-1]\n\nfor n in range(nfolds):\n    totDist.append(np.zeros(mat.shape[0]-nfolds))\n\n# saves index\nfor n in range(nfolds):\n    \n    values.append([lineNumber[n]])    \n\n\ns=[]\nfor n in range(nfolds):\n    s.append(mat[lineNumber[n],:])\n    \n    mat = np.delete(mat, obj=lineNumber[n], axis=0)\n\nfor n in range(nind-1):    \n\n    luck = np.random.uniform(0,1,nfolds)\n    \n    for cycle in range(nfolds):\n         # saves the values of index           \n\n        s[cycle] = np.matlib.repmat(s[cycle], mat.shape[0], 1)\n\n        sumDist = np.sum( (mat[:,:-1] - s[cycle][:,:-1])**2 , axis=1)   \n        totDist[cycle] += sumDist        \n                \n        # probabilities\n        f = totDist[cycle]/np.sum(totDist[cycle]) # normalizing the totdist\n        j = 0\n        kn = 0\n        for val in f:\n            j += val        \n            if (j > luck[cycle]): # the column was selected\n                break\n            kn +=1\n        lineNumber[cycle] = kn\n        \n        # delete line of the value added    \n        for n_iter in range(nfolds):\n            \n            totDist[n_iter] = np.delete(totDist[n_iter],obj=lineNumber[cycle], axis=0)\n            j= 0\n        \n        s[cycle] = mat[lineNumber[cycle],:]\n        values[cycle].append(int(mat[lineNumber[cycle],-1]))\n        mat = np.delete(mat, obj=lineNumber[cycle], axis=0)\n\n\nfor n_mod in range(nfolds):\n    values[n_mod] = out_train.index[values[n_mod]]","metadata":{"execution":{"iopub.status.busy":"2021-09-01T04:12:19.170147Z","iopub.execute_input":"2021-09-01T04:12:19.170509Z","iopub.status.idle":"2021-09-01T04:12:32.942907Z","shell.execute_reply.started":"2021-09-01T04:12:19.170477Z","shell.execute_reply":"2021-09-01T04:12:32.941646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#colNames.remove('row_id')\ntrain.replace([np.inf, -np.inf], np.nan,inplace=True)\ntest.replace([np.inf, -np.inf], np.nan,inplace=True)\nqt_train = []\ntrain_nn=train[colNames].copy()\ntest_nn=test[colNames].copy()\nfor col in colNames:\n    #print(col)\n    qt = QuantileTransformer(random_state=21,n_quantiles=2000, output_distribution='normal')\n    train_nn[col] = qt.fit_transform(train_nn[[col]])\n    test_nn[col] = qt.transform(test_nn[[col]])    \n    qt_train.append(qt)\n","metadata":{"execution":{"iopub.status.busy":"2021-09-01T04:12:32.945045Z","iopub.execute_input":"2021-09-01T04:12:32.945401Z","iopub.status.idle":"2021-09-01T04:13:03.562151Z","shell.execute_reply.started":"2021-09-01T04:12:32.945368Z","shell.execute_reply":"2021-09-01T04:13:03.561103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_nn[['stock_id','time_id','target']]=train[['stock_id','time_id','target']]\ntest_nn[['stock_id','time_id']]=test[['stock_id','time_id']]","metadata":{"execution":{"iopub.status.busy":"2021-09-01T04:13:03.563934Z","iopub.execute_input":"2021-09-01T04:13:03.564293Z","iopub.status.idle":"2021-09-01T04:13:03.582072Z","shell.execute_reply.started":"2021-09-01T04:13:03.564261Z","shell.execute_reply":"2021-09-01T04:13:03.580925Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# making agg features\nfrom sklearn.cluster import KMeans\ntrain_p = pd.read_csv('../input/optiver-realized-volatility-prediction/train.csv')\ntrain_p = train_p.pivot(index='time_id', columns='stock_id', values='target')\n\ncorr = train_p.corr()\n\nids = corr.index\n\nkmeans = KMeans(n_clusters=7, random_state=0).fit(corr.values)\nprint(kmeans.labels_)\n\nl = []\nfor n in range(7):\n    l.append ( [ (x-1) for x in ( (ids+1)*(kmeans.labels_ == n)) if x > 0] )\n    \n\nmat = []\nmatTest = []\n\nn = 0\nfor ind in l:\n    print(ind)\n    newDf = train_nn.loc[train_nn['stock_id'].isin(ind) ]\n    newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n    newDf.loc[:,'stock_id'] = str(n)+'c1'\n    mat.append ( newDf )\n    \n    newDf = test_nn.loc[test_nn['stock_id'].isin(ind) ]    \n    newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n    newDf.loc[:,'stock_id'] = str(n)+'c1'\n    matTest.append ( newDf )\n    \n    n+=1\n    \nmat1 = pd.concat(mat).reset_index()\nmat1.drop(columns=['target'],inplace=True)\n\nmat2 = pd.concat(matTest).reset_index()\nmat2 = pd.concat([mat2,mat1.loc[mat1.time_id==5]])","metadata":{"execution":{"iopub.status.busy":"2021-09-01T04:13:03.583715Z","iopub.execute_input":"2021-09-01T04:13:03.584033Z","iopub.status.idle":"2021-09-01T04:13:05.363809Z","shell.execute_reply.started":"2021-09-01T04:13:03.584004Z","shell.execute_reply":"2021-09-01T04:13:05.362668Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nnn = ['time_id',\n     'log_return1_realized_volatility_0c1',\n     'log_return1_realized_volatility_1c1',     \n     'log_return1_realized_volatility_3c1',\n     'log_return1_realized_volatility_4c1',     \n     'log_return1_realized_volatility_6c1',\n     'total_volume_sum_0c1',\n     'total_volume_sum_1c1', \n     'total_volume_sum_3c1',\n     'total_volume_sum_4c1', \n     'total_volume_sum_6c1',\n     'trade_size_sum_0c1',\n     'trade_size_sum_1c1', \n     'trade_size_sum_3c1',\n     'trade_size_sum_4c1', \n     'trade_size_sum_6c1',\n     'trade_order_count_sum_0c1',\n     'trade_order_count_sum_1c1',\n     'trade_order_count_sum_3c1',\n     'trade_order_count_sum_4c1',\n     'trade_order_count_sum_6c1',      \n     'price_spread_sum_0c1',\n     'price_spread_sum_1c1',\n     'price_spread_sum_3c1',\n     'price_spread_sum_4c1',\n     'price_spread_sum_6c1',   \n     'bid_spread_sum_0c1',\n     'bid_spread_sum_1c1',\n     'bid_spread_sum_3c1',\n     'bid_spread_sum_4c1',\n     'bid_spread_sum_6c1',       \n     'ask_spread_sum_0c1',\n     'ask_spread_sum_1c1',\n     'ask_spread_sum_3c1',\n     'ask_spread_sum_4c1',\n     'ask_spread_sum_6c1',   \n     'volume_imbalance_sum_0c1',\n     'volume_imbalance_sum_1c1',\n     'volume_imbalance_sum_3c1',\n     'volume_imbalance_sum_4c1',\n     'volume_imbalance_sum_6c1',       \n     'bid_ask_spread_sum_0c1',\n     'bid_ask_spread_sum_1c1',\n     'bid_ask_spread_sum_3c1',\n     'bid_ask_spread_sum_4c1',\n     'bid_ask_spread_sum_6c1',\n     'size_tau2_0c1',\n     'size_tau2_1c1',\n     'size_tau2_3c1',\n     'size_tau2_4c1',\n     'size_tau2_6c1'] ","metadata":{"execution":{"iopub.status.busy":"2021-09-01T04:13:05.365492Z","iopub.execute_input":"2021-09-01T04:13:05.366122Z","iopub.status.idle":"2021-09-01T04:13:05.375798Z","shell.execute_reply.started":"2021-09-01T04:13:05.366074Z","shell.execute_reply":"2021-09-01T04:13:05.374296Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mat1 = mat1.pivot(index='time_id', columns='stock_id')\nmat1.columns = [\"_\".join(x) for x in mat1.columns.ravel()]\nmat1.reset_index(inplace=True)\n\nmat2 = mat2.pivot(index='time_id', columns='stock_id')\nmat2.columns = [\"_\".join(x) for x in mat2.columns.ravel()]\nmat2.reset_index(inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-09-01T04:13:05.377505Z","iopub.execute_input":"2021-09-01T04:13:05.37785Z","iopub.status.idle":"2021-09-01T04:13:05.576022Z","shell.execute_reply.started":"2021-09-01T04:13:05.377818Z","shell.execute_reply":"2021-09-01T04:13:05.574629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gc\ntrain_nn = pd.merge(train_nn,mat1[nnn],how='left',on='time_id')\ntest_nn = pd.merge(test_nn,mat2[nnn],how='left',on='time_id')\n\ntrain1=train_nn\ntest1=test_nn\ndel mat1,mat2\ndel train,test\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-09-01T04:13:05.578289Z","iopub.execute_input":"2021-09-01T04:13:05.578645Z","iopub.status.idle":"2021-09-01T04:13:11.522463Z","shell.execute_reply.started":"2021-09-01T04:13:05.578612Z","shell.execute_reply":"2021-09-01T04:13:11.521132Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#https://bignerdranch.com/blog/implementing-swish-activation-function-in-keras/\nfrom keras.backend import sigmoid\ndef swish(x, beta = 1):\n    return (x * sigmoid(beta * x))\n\nfrom keras.utils.generic_utils import get_custom_objects\nfrom keras.layers import Activation\nget_custom_objects().update({'swish': Activation(swish)})\n\nhidden_units = (128,64,32)\nstock_embedding_size = 24\n\ncat_data = train_nn['stock_id']\n\ndef base_model():\n    \n    # Each instance will consist of two inputs: a single user id, and a single movie id\n    stock_id_input = keras.Input(shape=(1,), name='stock_id')\n    num_input = keras.Input(shape=(244,), name='num_data')\n\n\n    #embedding, flatenning and concatenating\n    stock_embedded = keras.layers.Embedding(max(cat_data)+1, stock_embedding_size, \n                                           input_length=1, name='stock_embedding')(stock_id_input)\n    stock_flattened = keras.layers.Flatten()(stock_embedded)\n    out = keras.layers.Concatenate()([stock_flattened, num_input])\n    \n    # Add one or more hidden layers\n    for n_hidden in hidden_units:\n\n        out = keras.layers.Dense(n_hidden, activation='swish')(out)\n        \n\n    #out = keras.layers.Concatenate()([out, num_input])\n\n    # A single output: our predicted rating\n    out = keras.layers.Dense(1, activation='linear', name='prediction')(out)\n    \n    model = keras.Model(\n    inputs = [stock_id_input, num_input],\n    outputs = out,\n    )\n    \n    return model\n","metadata":{"execution":{"iopub.status.busy":"2021-09-01T04:13:11.526827Z","iopub.execute_input":"2021-09-01T04:13:11.527212Z","iopub.status.idle":"2021-09-01T04:13:11.554753Z","shell.execute_reply.started":"2021-09-01T04:13:11.527181Z","shell.execute_reply":"2021-09-01T04:13:11.553483Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to calculate the root mean squared percentage error\ndef rmspe(y_true, y_pred):\n    return np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\n\n# Function to early stop with root mean squared percentage error\ndef feval_rmspe(y_pred, lgb_train):\n    y_true = lgb_train.get_label()\n    return 'RMSPE', rmspe(y_true, y_pred), False","metadata":{"execution":{"iopub.status.busy":"2021-09-01T04:13:11.556516Z","iopub.execute_input":"2021-09-01T04:13:11.556831Z","iopub.status.idle":"2021-09-01T04:13:11.56449Z","shell.execute_reply.started":"2021-09-01T04:13:11.556802Z","shell.execute_reply":"2021-09-01T04:13:11.56277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target_name='target'\nscores_folds = {}\nmodel_name = 'NN'\npred_name = 'pred_{}'.format(model_name)\n\nn_folds = 5\nkf = model_selection.KFold(n_splits=n_folds, shuffle=True, random_state=2020)\nscores_folds[model_name] = []\ncounter = 1\n\nfeatures_to_consider = list(train_nn)\n\nfeatures_to_consider.remove('time_id')\nfeatures_to_consider.remove('target')\ntry:\n    features_to_consider.remove('pred_NN')\nexcept:\n    pass\n\n\ntrain_nn[features_to_consider] = train_nn[features_to_consider].fillna(train_nn[features_to_consider].mean())\ntest_nn[features_to_consider] = test_nn[features_to_consider].fillna(train_nn[features_to_consider].mean())\n\ntrain_nn[pred_name] = 0\ntest_nn[target_name] = 0\ntest_predictions_nn = np.zeros(test_nn.shape[0])\n\nfor n_count in range(n_folds):\n    print('CV {}/{}'.format(counter, n_folds))\n    \n    indexes = np.arange(nfolds).astype(int)    \n    indexes = np.delete(indexes,obj=n_count, axis=0) \n    \n    indexes = np.r_[values[indexes[0]],values[indexes[1]],values[indexes[2]],values[indexes[3]]]\n    \n    X_train = train_nn.loc[train_nn.time_id.isin(indexes), features_to_consider]\n    y_train = train_nn.loc[train_nn.time_id.isin(indexes), target_name]\n    X_test = train_nn.loc[train_nn.time_id.isin(values[n_count]), features_to_consider]\n    y_test = train_nn.loc[train_nn.time_id.isin(values[n_count]), target_name]\n    \n    #############################################################################################\n    # NN\n    #############################################################################################\n    \n    model = base_model()\n    \n    model.compile(\n        keras.optimizers.Adam(learning_rate=0.006),\n        loss=root_mean_squared_per_error\n    )\n    \n    try:\n        features_to_consider.remove('stock_id')\n    except:\n        pass\n    \n    num_data = X_train[features_to_consider]\n    \n    scaler = MinMaxScaler(feature_range=(-1, 1))         \n    num_data = scaler.fit_transform(num_data.values)    \n    \n    cat_data = X_train['stock_id']    \n    target =  y_train\n    \n    num_data_test = X_test[features_to_consider]\n    num_data_test = scaler.transform(num_data_test.values)\n    cat_data_test = X_test['stock_id']\n\n    model.fit([cat_data, num_data], \n              target,               \n              batch_size=2048,\n              epochs=1000,\n              validation_data=([cat_data_test, num_data_test], y_test),\n              callbacks=[es, plateau],\n              validation_batch_size=len(y_test),\n              shuffle=True,\n             verbose = 1)\n\n    preds = model.predict([cat_data_test, num_data_test]).reshape(1,-1)[0]\n    \n    score = round(rmspe(y_true = y_test, y_pred = preds),5)\n    print('Fold {} {}: {}'.format(counter, model_name, score))\n    scores_folds[model_name].append(score)\n    \n    tt =scaler.transform(test_nn[features_to_consider].values)\n    #test_nn[target_name] += model.predict([test_nn['stock_id'], tt]).reshape(1,-1)[0].clip(0,1e10)\n    test_predictions_nn += model.predict([test_nn['stock_id'], tt]).reshape(1,-1)[0].clip(0,1e10)/n_folds\n    #test[target_name] += model.predict([test['stock_id'], test[features_to_consider]]).reshape(1,-1)[0].clip(0,1e10)\n       \n    counter += 1\n    features_to_consider.append('stock_id')","metadata":{"execution":{"iopub.status.busy":"2021-09-01T04:13:11.566209Z","iopub.execute_input":"2021-09-01T04:13:11.566518Z","iopub.status.idle":"2021-09-01T04:30:58.595072Z","shell.execute_reply.started":"2021-09-01T04:13:11.566489Z","shell.execute_reply":"2021-09-01T04:30:58.5937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_predictions_nn","metadata":{"execution":{"iopub.status.busy":"2021-09-01T04:31:33.093784Z","iopub.execute_input":"2021-09-01T04:31:33.094293Z","iopub.status.idle":"2021-09-01T04:31:33.10093Z","shell.execute_reply.started":"2021-09-01T04:31:33.094262Z","shell.execute_reply":"2021-09-01T04:31:33.099613Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from numpy.random import seed\nseed(41)\nimport tensorflow as tf\ntf.random.set_seed(41)\nfrom tensorflow import keras\n\ntarget_name='target'\nscores_folds = {}\nmodel_name = 'NN'\npred_name = 'pred_{}'.format(model_name)\n\nn_folds = 5\nkf = model_selection.KFold(n_splits=n_folds, shuffle=True, random_state=2021)\nscores_folds[model_name] = []\ncounter = 1\n\nfeatures_to_consider = list(train1)\n\nfeatures_to_consider.remove('time_id')\nfeatures_to_consider.remove('target')\ntry:\n    features_to_consider.remove('pred_NN')\nexcept:\n    pass\n\n\ntrain1[features_to_consider] = train1[features_to_consider].fillna(train1[features_to_consider].mean())\ntest1[features_to_consider] = test1[features_to_consider].fillna(train1[features_to_consider].mean())\n\ntrain1[pred_name] = 0\ntest1[target_name] = 0\ntest_predictions_nn1 = np.zeros(test_nn.shape[0])\n\nfor n_count in range(n_folds):\n    print('CV {}/{}'.format(counter, n_folds))\n    \n    indexes = np.arange(nfolds).astype(int)    \n    indexes = np.delete(indexes,obj=n_count, axis=0) \n    \n    indexes = np.r_[values[indexes[0]],values[indexes[1]],values[indexes[2]],values[indexes[3]]]\n    \n    X_train = train1.loc[train1.time_id.isin(indexes), features_to_consider]\n    y_train = train1.loc[train1.time_id.isin(indexes), target_name]\n    X_test = train1.loc[train1.time_id.isin(values[n_count]), features_to_consider]\n    y_test = train1.loc[train1.time_id.isin(values[n_count]), target_name]\n    \n    #############################################################################################\n    # NN\n    #############################################################################################\n    \n    model = base_model()\n    \n    model.compile(\n        keras.optimizers.Adam(learning_rate=0.006),\n        loss=root_mean_squared_per_error\n    )\n    \n    try:\n        features_to_consider.remove('stock_id')\n    except:\n        pass\n    \n    num_data = X_train[features_to_consider]\n    \n    scaler = MinMaxScaler(feature_range=(-1, 1))         \n    num_data = scaler.fit_transform(num_data.values)    \n    \n    cat_data = X_train['stock_id']    \n    target =  y_train\n    \n    num_data_test = X_test[features_to_consider]\n    num_data_test = scaler.transform(num_data_test.values)\n    cat_data_test = X_test['stock_id']\n\n    model.fit([cat_data, num_data], \n              target,               \n              batch_size=2048,\n              epochs=1000,\n              validation_data=([cat_data_test, num_data_test], y_test),\n              callbacks=[es, plateau],\n              validation_batch_size=len(y_test),\n              shuffle=True,\n             verbose = 1)\n\n    preds = model.predict([cat_data_test, num_data_test]).reshape(1,-1)[0]\n    \n    score = round(rmspe(y_true = y_test, y_pred = preds),5)\n    print('Fold {} {}: {}'.format(counter, model_name, score))\n    scores_folds[model_name].append(score)\n    \n    tt =scaler.transform(test_nn[features_to_consider].values)\n    #test_nn[target_name] += model.predict([test_nn['stock_id'], tt]).reshape(1,-1)[0].clip(0,1e10)\n#     test_predictions_nn1 += model.predict([test1['stock_id'], tt]).reshape(1,-1)[0].clip(0,1e10)/n_folds\n    #test[target_name] += model.predict([test['stock_id'], test[features_to_consider]]).reshape(1,-1)[0].clip(0,1e10)\n       \n    counter += 1\n    features_to_consider.append('stock_id')","metadata":{"execution":{"iopub.status.busy":"2021-09-01T04:31:39.015582Z","iopub.execute_input":"2021-09-01T04:31:39.016116Z","iopub.status.idle":"2021-09-01T04:48:45.678995Z","shell.execute_reply.started":"2021-09-01T04:31:39.016074Z","shell.execute_reply":"2021-09-01T04:48:45.67742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test1","metadata":{"execution":{"iopub.status.busy":"2021-09-01T04:59:39.839324Z","iopub.execute_input":"2021-09-01T04:59:39.839723Z","iopub.status.idle":"2021-09-01T04:59:39.869807Z","shell.execute_reply.started":"2021-09-01T04:59:39.839691Z","shell.execute_reply":"2021-09-01T04:59:39.868593Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test=pd.read_csv(\"../input/optiver-realized-volatility-prediction/test.csv\")\na=test_predictions_nn*0.60+predictions_lgb2*0.40\nb=test_predictions_nn1*0.55+predictions_lgb1*0.45\ntest[target_name] = (a+b)/2\n\ndisplay(test[['row_id', target_name]].head(3))\ntest[['row_id', target_name]].to_csv('submission.csv',index = False)\n#test[['row_id', target_name]].to_csv('submission.csv',index = False)\n#kmeans N=5 [0.2101, 0.21399, 0.20923, 0.21398, 0.21175]","metadata":{"execution":{"iopub.status.busy":"2021-09-01T05:04:11.548897Z","iopub.execute_input":"2021-09-01T05:04:11.549316Z","iopub.status.idle":"2021-09-01T05:04:11.583162Z","shell.execute_reply.started":"2021-09-01T05:04:11.549283Z","shell.execute_reply":"2021-09-01T05:04:11.581796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"RES_TMP = test[target_name] ","metadata":{"execution":{"iopub.status.busy":"2021-09-01T04:49:10.044636Z","iopub.execute_input":"2021-09-01T04:49:10.045134Z","iopub.status.idle":"2021-09-01T04:49:10.055087Z","shell.execute_reply.started":"2021-09-01T04:49:10.045089Z","shell.execute_reply":"2021-09-01T04:49:10.05377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del train1, test1, train_nn, test_nn, model, tt, X_train, X_test \ngc.collect()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## TabNeT","metadata":{}},{"cell_type":"code","source":"!pip -q install ../input/pytorchtabnet/pytorch_tabnet-3.1.1-py3-none-any.whl","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy.matlib\n\nimport matplotlib.gridspec as gridspec\nfrom matplotlib.ticker import MaxNLocator\n\nfrom scipy import stats\nfrom scipy.stats import norm\nfrom joblib import Parallel, delayed\n\nimport shutil\nimport glob\n\nfrom sklearn.preprocessing import MinMaxScaler, LabelEncoder, StandardScaler\nfrom sklearn.metrics import r2_score\nfrom sklearn.cluster import KMeans\nfrom sklearn.model_selection import KFold\n\nfrom pytorch_tabnet.metrics import Metric\nfrom pytorch_tabnet.tab_model import TabNetRegressor\n\nimport torch\nfrom torch.optim import Adam, SGD\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingWarmRestarts\n\n\n# setting some globl config\n\nplt.style.use('ggplot')\norange_black = [\n    '#fdc029', '#df861d', '#FF6347', '#aa3d01', '#a30e15', '#800000', '#171820'\n]\nplt.rcParams['figure.figsize'] = (16,9)\nplt.rcParams[\"figure.facecolor\"] = '#FFFACD'\nplt.rcParams[\"axes.facecolor\"] = '#FFFFE0'\nplt.rcParams[\"axes.grid\"] = True\nplt.rcParams[\"grid.color\"] = orange_black[3]\nplt.rcParams[\"grid.alpha\"] = 0.5\nplt.rcParams[\"grid.linestyle\"] = '--'\n\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import psutil\npsutil.cpu_count()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gpu_info = !nvidia-smi\ngpu_info = '\\n'.join(gpu_info)\nprint(gpu_info)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def read_train_test():\n    # Function to read our base train and test set\n    \n    train = pd.read_csv('../input/optiver-realized-volatility-prediction/train.csv')\n    test = pd.read_csv('../input/optiver-realized-volatility-prediction/test.csv')\n\n    # Create a key to merge with book and trade data\n    train['row_id'] = train['stock_id'].astype(str) + '-' + train['time_id'].astype(str)\n    test['row_id'] = test['stock_id'].astype(str) + '-' + test['time_id'].astype(str)\n    print(f'Our training set has {train.shape[0]} rows')\n    print(f'Our test set has {test.shape[0]} rows')\n    print(f'Our training set has {train.isna().sum().sum()} missing values')\n    print(f'Our test set has {test.isna().sum().sum()} missing values')\n    \n    return train, test","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train, test = read_train_test()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# data directory\ndata_dir = '../input/optiver-realized-volatility-prediction/'\n\ndef calc_wap1(df):\n    # Function to calculate first WAP\n    wap = (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']) / (df['bid_size1'] + df['ask_size1'])\n    return wap\n\ndef calc_wap2(df):\n    # Function to calculate second WAP\n    wap = (df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2']) / (df['bid_size2'] + df['ask_size2'])\n    return wap\n\ndef log_return(series):\n    # Function to calculate the log of the return\n    return np.log(series).diff()\n\ndef realized_volatility(series):\n    # Calculate the realized volatility\n    return np.sqrt(np.sum(series**2))\n\ndef count_unique(series):\n    # Function to count unique elements of a series\n    return len(np.unique(series))\n\ndef book_preprocessor(file_path):\n    # Function to preprocess book data (for each stock id)\n    \n    df = pd.read_parquet(file_path)\n    \n    # Calculate Wap\n    df['wap1'] = calc_wap1(df)\n    df['wap2'] = calc_wap2(df)\n    \n    # Calculate log returns\n    df['log_return1'] = df.groupby(['time_id'])['wap1'].apply(log_return)\n    df['log_return2'] = df.groupby(['time_id'])['wap2'].apply(log_return)\n    \n    # Calculate wap balance\n    df['wap_balance'] = abs(df['wap1'] - df['wap2'])\n    \n    # Calculate spread\n    df['price_spread'] = (df['ask_price1'] - df['bid_price1']) / ((df['ask_price1'] + df['bid_price1']) / 2)\n    df['price_spread2'] = (df['ask_price2'] - df['bid_price2']) / ((df['ask_price2'] + df['bid_price2']) / 2)\n    df['bid_spread'] = df['bid_price1'] - df['bid_price2']\n    df['ask_spread'] = df['ask_price1'] - df['ask_price2']\n    df[\"bid_ask_spread\"] = abs(df['bid_spread'] - df['ask_spread'])\n    df['total_volume'] = (df['ask_size1'] + df['ask_size2']) + (df['bid_size1'] + df['bid_size2'])\n    df['volume_imbalance'] = abs((df['ask_size1'] + df['ask_size2']) - (df['bid_size1'] + df['bid_size2']))\n    \n    # Dict for aggregations\n    create_feature_dict = {\n        'wap1': [np.sum, np.mean, np.std],\n        'wap2': [np.sum, np.mean, np.std],\n        'log_return1': [np.sum, realized_volatility, np.mean, np.std],\n        'log_return2': [np.sum, realized_volatility, np.mean, np.std],\n        'wap_balance': [np.sum, np.mean, np.std],\n        'price_spread':[np.sum, np.mean, np.std],\n        'price_spread2':[np.sum, np.mean, np.std],\n        'bid_spread':[np.sum, np.mean, np.std],\n        'ask_spread':[np.sum, np.mean, np.std],\n        'total_volume':[np.sum, np.mean, np.std],\n        'volume_imbalance':[np.sum, np.mean, np.std],\n        \"bid_ask_spread\":[np.sum, np.mean, np.std],\n    }\n    \n    def get_stats_window(seconds_in_bucket, add_suffix = False):\n        # Function to get group stats for different windows (seconds in bucket)\n        \n        # Group by the window\n        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(create_feature_dict).reset_index()\n        \n        # Rename columns joining suffix\n        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n        \n        # Add a suffix to differentiate windows\n        if add_suffix:\n            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n        return df_feature\n    \n    # Get the stats for different windows\n    df_feature = get_stats_window(seconds_in_bucket = 0, add_suffix = False)\n    df_feature_400 = get_stats_window(seconds_in_bucket = 400, add_suffix = True)\n    df_feature_300 = get_stats_window(seconds_in_bucket = 300, add_suffix = True)\n    df_feature_200 = get_stats_window(seconds_in_bucket = 200, add_suffix = True)\n    \n    # Merge all\n    df_feature = df_feature.merge(df_feature_400, how = 'left', left_on = 'time_id_', right_on = 'time_id__400')\n    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n    df_feature = df_feature.merge(df_feature_200, how = 'left', left_on = 'time_id_', right_on = 'time_id__200')\n\n    # Drop unnecesary time_ids\n    df_feature.drop(['time_id__400', 'time_id__300', 'time_id__200'], axis = 1, inplace = True)\n    \n    \n    # Create row_id so we can merge\n    stock_id = file_path.split('=')[1]\n    df_feature['row_id'] = df_feature['time_id_'].apply(lambda x: f'{stock_id}-{x}')\n    df_feature.drop(['time_id_'], axis = 1, inplace = True)\n    \n    return df_feature\n\n\ndef trade_preprocessor(file_path):\n    # Function to preprocess trade data (for each stock id)\n    \n    df = pd.read_parquet(file_path)\n    df['log_return'] = df.groupby('time_id')['price'].apply(log_return)\n    \n    # Dict for aggregations\n    create_feature_dict = {\n        'log_return':[realized_volatility],\n        'seconds_in_bucket':[count_unique],\n        'size':[np.sum, realized_volatility, np.mean, np.std, np.max, np.min],\n        'order_count':[np.mean,np.sum,np.max],\n    }\n    \n    def get_stats_window(seconds_in_bucket, add_suffix = False):\n        # Function to get group stats for different windows (seconds in bucket)\n        \n        # Group by the window\n        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(create_feature_dict).reset_index()\n        \n        # Rename columns joining suffix\n        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n        \n        # Add a suffix to differentiate windows\n        if add_suffix:\n            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n        return df_feature\n    \n    # Get the stats for different windows\n    df_feature = get_stats_window(seconds_in_bucket = 0, add_suffix = False)\n    df_feature_400 = get_stats_window(seconds_in_bucket = 400, add_suffix = True)\n    df_feature_300 = get_stats_window(seconds_in_bucket = 300, add_suffix = True)\n    df_feature_200 = get_stats_window(seconds_in_bucket = 200, add_suffix = True)\n    \n    def tendency(price, vol):    \n        df_diff = np.diff(price)\n        val = (df_diff/price[1:])*100\n        power = np.sum(val*vol[1:])\n        return(power)\n    \n    lis = []\n    for n_time_id in df['time_id'].unique():\n        df_id = df[df['time_id'] == n_time_id]        \n        tendencyV = tendency(df_id['price'].values, df_id['size'].values)      \n        f_max = np.sum(df_id['price'].values > np.mean(df_id['price'].values))\n        f_min = np.sum(df_id['price'].values < np.mean(df_id['price'].values))\n        df_max =  np.sum(np.diff(df_id['price'].values) > 0)\n        df_min =  np.sum(np.diff(df_id['price'].values) < 0)\n        abs_diff = np.median(np.abs( df_id['price'].values - np.mean(df_id['price'].values)))        \n        energy = np.mean(df_id['price'].values**2)\n        iqr_p = np.percentile(df_id['price'].values,75) - np.percentile(df_id['price'].values,25)\n        abs_diff_v = np.median(np.abs( df_id['size'].values - np.mean(df_id['size'].values)))        \n        energy_v = np.sum(df_id['size'].values**2)\n        iqr_p_v = np.percentile(df_id['size'].values,75) - np.percentile(df_id['size'].values,25)\n        \n        lis.append({'time_id':n_time_id,'tendency':tendencyV,'f_max':f_max,'f_min':f_min,'df_max':df_max,'df_min':df_min,\n                   'abs_diff':abs_diff,'energy':energy,'iqr_p':iqr_p,'abs_diff_v':abs_diff_v,'energy_v':energy_v,'iqr_p_v':iqr_p_v})\n    \n    df_lr = pd.DataFrame(lis)\n        \n   \n    df_feature = df_feature.merge(df_lr, how = 'left', left_on = 'time_id_', right_on = 'time_id')\n    \n    # Merge all\n    df_feature = df_feature.merge(df_feature_400, how = 'left', left_on = 'time_id_', right_on = 'time_id__400')\n    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n    df_feature = df_feature.merge(df_feature_200, how = 'left', left_on = 'time_id_', right_on = 'time_id__200')\n\n    # Drop unnecesary time_ids\n    df_feature.drop(['time_id__400', 'time_id__300', 'time_id__200','time_id'], axis = 1, inplace = True)\n    df_feature = df_feature.add_prefix('trade_')\n    stock_id = file_path.split('=')[1]\n    df_feature['row_id'] = df_feature['trade_time_id_'].apply(lambda x:f'{stock_id}-{x}')\n    df_feature.drop(['trade_time_id_'], axis = 1, inplace = True)\n    \n    def order_sum(df, sec:str):\n        new_col = 'size_tau' + sec\n        bucket_col = 'trade_seconds_in_bucket_count_unique' + sec\n        df[new_col] = np.sqrt(1/df[bucket_col])\n        \n        new_col2 = 'size_tau2' + sec\n        order_col = 'trade_order_count_sum' + sec\n        df[new_col2] = np.sqrt(1/df[order_col])\n        \n        if sec == '400_':\n            df['size_tau2_d'] = df['size_tau2_400'] - df['size_tau2']\n        \n\n    \n    for sec in ['','_200','_300','_400']:\n        order_sum(df_feature, sec)\n        \n    df_feature['size_tau2_d'] = df_feature['size_tau2_400'] - df_feature['size_tau2']\n    \n    return df_feature\n\n\ndef get_time_stock(df):\n    # Function to get group stats for the stock_id and time_id\n    \n    # Get realized volatility columns\n    vol_cols = ['log_return1_realized_volatility', 'log_return2_realized_volatility', 'log_return1_realized_volatility_400', 'log_return2_realized_volatility_400', \n                'log_return1_realized_volatility_300', 'log_return2_realized_volatility_300', 'log_return1_realized_volatility_200', 'log_return2_realized_volatility_200', \n                'trade_log_return_realized_volatility', 'trade_log_return_realized_volatility_400', 'trade_log_return_realized_volatility_300', 'trade_log_return_realized_volatility_200']\n\n    # Group by the stock id\n    df_stock_id = df.groupby(['stock_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n    \n    # Rename columns joining suffix\n    df_stock_id.columns = ['_'.join(col) for col in df_stock_id.columns]\n    df_stock_id = df_stock_id.add_suffix('_' + 'stock')\n\n    # Group by the stock id\n    df_time_id = df.groupby(['time_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n    \n    # Rename columns joining suffix\n    df_time_id.columns = ['_'.join(col) for col in df_time_id.columns]\n    df_time_id = df_time_id.add_suffix('_' + 'time')\n    \n    # Merge with original dataframe\n    df = df.merge(df_stock_id, how = 'left', left_on = ['stock_id'], right_on = ['stock_id__stock'])\n    df = df.merge(df_time_id, how = 'left', left_on = ['time_id'], right_on = ['time_id__time'])\n    df.drop(['stock_id__stock', 'time_id__time'], axis = 1, inplace = True)\n    \n    return df\n\ndef create_agg_features(train, test):\n\n    # Making agg features\n\n    train_p = pd.read_csv('../input/optiver-realized-volatility-prediction/train.csv')\n    train_p = train_p.pivot(index='time_id', columns='stock_id', values='target')\n    corr = train_p.corr()\n    ids = corr.index\n    kmeans = KMeans(n_clusters=7, random_state=0).fit(corr.values)\n    l = []\n    for n in range(7):\n        l.append ( [ (x-1) for x in ( (ids+1)*(kmeans.labels_ == n)) if x > 0] )\n\n    mat = []\n    matTest = []\n    n = 0\n    for ind in l:\n        newDf = train.loc[train['stock_id'].isin(ind) ]\n        newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n        newDf.loc[:,'stock_id'] = str(n)+'c1'\n        mat.append ( newDf )\n        newDf = test.loc[test['stock_id'].isin(ind) ]    \n        newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n        newDf.loc[:,'stock_id'] = str(n)+'c1'\n        matTest.append ( newDf )\n        n+=1\n\n    mat1 = pd.concat(mat).reset_index()\n    mat1.drop(columns=['target'],inplace=True)\n    mat2 = pd.concat(matTest).reset_index()\n    \n    mat2 = pd.concat([mat2,mat1.loc[mat1.time_id==5]])\n    \n    mat1 = mat1.pivot(index='time_id', columns='stock_id')\n    mat1.columns = [\"_\".join(x) for x in mat1.columns.ravel()]\n    mat1.reset_index(inplace=True)\n    \n    mat2 = mat2.pivot(index='time_id', columns='stock_id')\n    mat2.columns = [\"_\".join(x) for x in mat2.columns.ravel()]\n    mat2.reset_index(inplace=True)\n    \n    prefix = ['log_return1_realized_volatility', 'total_volume_mean', 'trade_size_mean', 'trade_order_count_mean','price_spread_mean','bid_spread_mean','ask_spread_mean',\n              'volume_imbalance_mean', 'bid_ask_spread_mean','size_tau2']\n    selected_cols=mat1.filter(regex='|'.join(f'^{x}.(0|1|3|4|6)c1' for x in prefix)).columns.tolist()\n    selected_cols.append('time_id')\n    \n    train_m = pd.merge(train,mat1[selected_cols],how='left',on='time_id')\n    test_m = pd.merge(test,mat2[selected_cols],how='left',on='time_id')\n    \n    # filling missing values with train means\n\n    features = [col for col in train_m.columns.tolist() if col not in ['time_id','target','row_id']]\n    train_m[features] = train_m[features].fillna(train_m[features].mean())\n    test_m[features] = test_m[features].fillna(train_m[features].mean())\n\n    return train_m, test_m\n    \n    \ndef preprocessor(list_stock_ids, is_train = True):\n    # Funtion to make preprocessing function in parallel (for each stock id)\n    \n    # Parrallel for loop\n    def for_joblib(stock_id):\n        # Train\n        if is_train:\n            file_path_book = data_dir + \"book_train.parquet/stock_id=\" + str(stock_id)\n            file_path_trade = data_dir + \"trade_train.parquet/stock_id=\" + str(stock_id)\n        # Test\n        else:\n            file_path_book = data_dir + \"book_test.parquet/stock_id=\" + str(stock_id)\n            file_path_trade = data_dir + \"trade_test.parquet/stock_id=\" + str(stock_id)\n    \n        # Preprocess book and trade data and merge them\n        df_tmp = pd.merge(book_preprocessor(file_path_book), trade_preprocessor(file_path_trade), on = 'row_id', how = 'left')\n        \n        # Return the merge dataframe\n        return df_tmp\n    \n    # Use parallel api to call paralle for loop\n    df = Parallel(n_jobs = -1, verbose = 1)(delayed(for_joblib)(stock_id) for stock_id in list_stock_ids)\n    \n    # Concatenate all the dataframes that return from Parallel\n    df = pd.concat(df, ignore_index = True)\n    \n    return df","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get unique stock ids \ntrain_stock_ids = train['stock_id'].unique()\n\n# Preprocess them using Parallel and our single stock id functions\ntrain_ = preprocessor(train_stock_ids, is_train = True)\ntrain = train.merge(train_, on = ['row_id'], how = 'left')\n\n# Get unique stock ids \ntest_stock_ids = test['stock_id'].unique()\n\n# Preprocess them using Parallel and our single stock id functions\ntest_ = preprocessor(test_stock_ids, is_train = False)\ntest = test.merge(test_, on = ['row_id'], how = 'left')\n\n# Get group stats of time_id and stock_id\ntrain = get_time_stock(train)\ntest = get_time_stock(test)\n\n# Fill inf values\ntrain.replace([np.inf, -np.inf], np.nan,inplace=True)\ntest.replace([np.inf, -np.inf], np.nan,inplace=True)\n\n# Aggregating some features\ntrain, test = create_agg_features(train,test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = train.drop(['row_id', 'target', 'time_id'], axis = 1)\ny = train['target']\nX_test=test.copy()\nX_test.drop(['time_id','row_id'], axis=1,inplace=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def rmspe(y_true, y_pred):\n    # Function to calculate the root mean squared percentage error\n    return np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\n\nclass RMSPE(Metric):\n    def __init__(self):\n        self._name = \"rmspe\"\n        self._maximize = False\n\n    def __call__(self, y_true, y_score):\n        \n        return np.sqrt(np.mean(np.square((y_true - y_score) / y_true)))\n    \n\n\ndef RMSPELoss(y_pred, y_true):\n    return torch.sqrt(torch.mean( ((y_true - y_pred) / y_true) ** 2 )).clone()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nunique = X.nunique()\ntypes = X.dtypes\n\ncategorical_columns = []\ncategorical_dims =  {}\n\nfor col in X.columns:\n    if  col == 'stock_id':\n        l_enc = LabelEncoder()\n        X[col] = l_enc.fit_transform(X[col].values)\n        X_test[col] = l_enc.transform(X_test[col].values)\n        categorical_columns.append(col)\n        categorical_dims[col] = len(l_enc.classes_)\n    else:\n        scaler = StandardScaler()\n        X[col] = scaler.fit_transform(X[col].values.reshape(-1, 1))\n        X_test[col] = scaler.transform(X_test[col].values.reshape(-1, 1))\n        \n\n\ncat_idxs = [ i for i, f in enumerate(X.columns.tolist()) if f in categorical_columns]\n\ncat_dims = [ categorical_dims[f] for i, f in enumerate(X.columns.tolist()) if f in categorical_columns]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tabnet_params = dict(\n    cat_idxs=cat_idxs,\n    cat_dims=cat_dims,\n    cat_emb_dim=1,\n    n_d = 16,\n    n_a = 16,\n    n_steps = 2,\n    gamma = 2,\n    n_independent = 2,\n    n_shared = 2,\n    lambda_sparse = 0,\n    optimizer_fn = Adam,\n    optimizer_params = dict(lr = (2e-2)),\n    mask_type = \"entmax\",\n    scheduler_params = dict(T_0=200, T_mult=1, eta_min=1e-4, last_epoch=-1, verbose=False),\n    scheduler_fn = CosineAnnealingWarmRestarts,\n    seed = 42,\n    verbose = 10\n    \n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kfold = KFold(n_splits = 5, random_state = 42, shuffle = True)\n# Create out of folds array\noof_predictions = np.zeros((X.shape[0], 1))\ntest_predictions = np.zeros(X_test.shape[0])\nfeature_importances = pd.DataFrame()\nfeature_importances[\"feature\"] = X.columns.tolist()\nstats = pd.DataFrame()\nexplain_matrices = []\nmasks_ =[]\n\nfor fold, (trn_ind, val_ind) in enumerate(kfold.split(X)):\n    print(f'Training fold {fold + 1}')\n    X_train, X_val = X.iloc[trn_ind].values, X.iloc[val_ind].values\n    y_train, y_val = y.iloc[trn_ind].values.reshape(-1,1), y.iloc[val_ind].values.reshape(-1,1)\n\n\n    clf =  TabNetRegressor(**tabnet_params)\n    clf.fit(\n      X_train, y_train,\n      eval_set=[(X_val, y_val)],\n      max_epochs = 200,\n      patience = 50,\n      batch_size = 1024*20, \n      virtual_batch_size = 128*20,\n      num_workers = 4,\n      drop_last = False,\n      eval_metric=[RMSPE],\n      loss_fn=RMSPELoss\n      )\n    \n    saving_path_name = f\"./fold{fold}\"\n    saved_filepath = clf.save_model(saving_path_name)\n    \n    explain_matrix, masks = clf.explain(X_val)\n    explain_matrices.append(explain_matrix)\n    masks_.append(masks[0])\n    masks_.append(masks[1])\n      \n    oof_predictions[val_ind] = clf.predict(X_val)\n    test_predictions+=clf.predict(X_test.values).flatten()/5\n    feature_importances[f\"importance_fold{fold}+1\"] = clf.feature_importances_\n    \n    stats[f'fold{fold+1}_train_rmspe']=clf.history['loss']\n    stats[f'fold{fold+1}_val_rmspe']=clf.history['val_0_rmspe']\n    \nprint(f'OOF score across folds: {rmspe(y, oof_predictions.flatten())}')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in stats.filter(like='train', axis=1).columns.tolist():\n    plt.plot(stats[i], label=str(i))\nplt.title('Train RMSPE')\nplt.legend()  ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in stats.filter(like='val', axis=1).columns.tolist():\n    plt.plot(stats[i], label=str(i))\nplt.title('Train RMSPE')\nplt.legend() ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_importances['mean_importance']=feature_importances[['importance_fold0+1','importance_fold1+1']].mean(axis=1)\nfeature_importances.sort_values(by='mean_importance', ascending=False, inplace=True)\nsns.barplot(y=feature_importances['feature'][:25],x=feature_importances['mean_importance'][:25], palette='inferno')\nplt.title('Mean Feature Importance by Folds')\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axs = plt.subplots(5, 2, figsize=(16,16))\naxs = axs.flatten()\n\nk=-1    \nfor i, (mask, j) in enumerate(zip(masks_, axs)):\n    sns.heatmap(mask[:150], ax=j)\n    if i%2 == 0:\n        k+=1\n    j.set_title((f\"Fold{k} Mask for First 150 Instances\"))\nplt.tight_layout()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axs = plt.subplots(len(explain_matrices), 1, figsize=(20,8))\nfor i,matrix in enumerate(explain_matrices):\n    axs[i].set_title(f'Fold{i} Explain Matrix for First 150 Instances')\n    sns.heatmap(matrix[:150], ax=axs[i])\nplt.tight_layout() \n    ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test['target'] = test_predictions*0.05 + RES_TMP*0.95\ntest[['row_id', 'target']].to_csv('submission.csv',index = False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}