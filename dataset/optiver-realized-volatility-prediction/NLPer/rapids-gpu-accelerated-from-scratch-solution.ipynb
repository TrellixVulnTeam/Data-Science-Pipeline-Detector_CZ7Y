{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# GPU accelerated solution using NVIDIA RAPIDS cudf and cuml\n# Data loading, preprocessing and feature engineering takes less than 3min in GPU.","metadata":{}},{"cell_type":"code","source":"import pandas\nimport numpy as np\nimport cudf as pd\nimport cupy as cp\n\nimport glob\nimport os\nimport gc\nimport time\n\nfrom joblib import Parallel, delayed\n\nfrom sklearn import preprocessing, model_selection\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import QuantileTransformer\nfrom sklearn.metrics import r2_score\nfrom sklearn.model_selection import GroupKFold\nfrom scipy.optimize import minimize\n\n\nimport matplotlib.pyplot as plt \nimport seaborn as sns\nimport numpy.matlib\nfrom catboost import Pool, CatBoostRegressor\nfrom tqdm import tqdm\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import KFold\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom catboost import CatBoostRegressor\nimport cuml\nfrom cuml.neighbors import KNeighborsRegressor\nfrom cuml import LinearRegression\nfrom cuml import Ridge\nfrom cuml.ensemble import RandomForestRegressor\n\n\npath_submissions = '/'\n\ntarget_name = 'target'\nscores_folds = {}\n\ndef convert_to_32bit(df):\n    for f in df.columns:\n        if df[f].dtype == 'int64':\n            df[f] = df[f].astype('int32')\n        if df[f].dtype == 'float64':\n            df[f] = df[f].astype('float32')\n    return df","metadata":{"execution":{"iopub.status.busy":"2021-09-20T12:03:03.098362Z","iopub.execute_input":"2021-09-20T12:03:03.098952Z","iopub.status.idle":"2021-09-20T12:03:08.468016Z","shell.execute_reply.started":"2021-09-20T12:03:03.098859Z","shell.execute_reply":"2021-09-20T12:03:08.467264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading train and test sets","metadata":{}},{"cell_type":"code","source":"# data directory\ndata_dir = '../input/optiver-realized-volatility-prediction/'\n\ntrain = pd.read_csv('../input/optiver-realized-volatility-prediction/train.csv')\ntest = pd.read_csv('../input/optiver-realized-volatility-prediction/test.csv')\n\ntrain['row_id'] = train['stock_id'].astype(str) + '-' + train['time_id'].astype(str)\ntest['row_id'] = test['stock_id'].astype(str) + '-' + test['time_id'].astype(str)\n\ntrain['is_train'] = 1\ntest['is_train'] = 0\n\ntrain = convert_to_32bit(train)\ntest = convert_to_32bit(test)\n\nprint( train.shape )\nprint( test.shape )","metadata":{"execution":{"iopub.status.busy":"2021-09-20T12:03:08.46941Z","iopub.execute_input":"2021-09-20T12:03:08.469696Z","iopub.status.idle":"2021-09-20T12:03:12.671378Z","shell.execute_reply.started":"2021-09-20T12:03:08.469659Z","shell.execute_reply":"2021-09-20T12:03:12.669961Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train.head(20))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(test.head(20))","metadata":{"execution":{"iopub.status.busy":"2021-09-20T12:03:12.672552Z","iopub.execute_input":"2021-09-20T12:03:12.672947Z","iopub.status.idle":"2021-09-20T12:03:12.685374Z","shell.execute_reply.started":"2021-09-20T12:03:12.672911Z","shell.execute_reply":"2021-09-20T12:03:12.684588Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Checking how many stock_id there are in train and test","metadata":{}},{"cell_type":"code","source":"train_stock_ids = train['stock_id'].to_pandas().unique()\ntest_stock_ids = test['stock_id'].to_pandas().unique()\nprint( 'Sizes:', len(train_stock_ids), len(test_stock_ids) )\nprint( 'Train stocks:', train_stock_ids )\nprint( 'Test stocks:', test_stock_ids )","metadata":{"execution":{"iopub.status.busy":"2021-09-20T12:03:12.687464Z","iopub.execute_input":"2021-09-20T12:03:12.687791Z","iopub.status.idle":"2021-09-20T12:03:12.703073Z","shell.execute_reply.started":"2021-09-20T12:03:12.687758Z","shell.execute_reply":"2021-09-20T12:03:12.702294Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to preprocess book data (for each stock id)\ndef book_preprocessor(file_path):\n    df = pd.read_parquet(file_path)\n    df = convert_to_32bit(df)\n    \n    # Calculate Wap\n    df['wap1'] = (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']) / (df['bid_size1'] + df['ask_size1'])\n    df['wap2'] = (df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2']) / (df['bid_size2'] + df['ask_size2'])\n    df['wap3'] = (df['bid_price1'] * df['bid_size1'] + df['ask_price1'] * df['ask_size1']) / (df['bid_size1'] + df['ask_size1'])\n    df['wap4'] = (df['bid_price2'] * df['bid_size2'] + df['ask_price2'] * df['ask_size2']) / (df['bid_size2'] + df['ask_size2'])\n    \n    # Calculate log returns\n    df['log_return1'] = df['wap1'].log()\n    df['log_return1'] = df['log_return1'] - df.groupby(['time_id'])['log_return1'].shift(1).reset_index(drop=True)\n\n    df['log_return2'] = df['wap2'].log()\n    df['log_return2'] = df['log_return2'] - df.groupby(['time_id'])['log_return2'].shift(1).reset_index(drop=True)\n\n    df['log_return3'] = df['wap3'].log()\n    df['log_return3'] = df['log_return3'] - df.groupby(['time_id'])['log_return3'].shift(1).reset_index(drop=True)\n\n    df['log_return4'] = df['wap4'].log()\n    df['log_return4'] = df['log_return4'] - df.groupby(['time_id'])['log_return4'].shift(1).reset_index(drop=True)\n    \n    # Calculate wap balance\n    df['wap_balance'] = abs(df['wap1'] - df['wap2'])\n    \n    # Calculate spread\n    df['price_spread'] = (df['ask_price1'] - df['bid_price1']) / ((df['ask_price1'] + df['bid_price1']) / 2)\n    df['price_spread2'] = (df['ask_price2'] - df['bid_price2']) / ((df['ask_price2'] + df['bid_price2']) / 2)\n    df['bid_spread'] = df['bid_price1'] - df['bid_price2']\n    df['ask_spread'] = df['ask_price1'] - df['ask_price2']\n    df[\"bid_ask_spread\"] = abs(df['bid_spread'] - df['ask_spread'])\n    df['total_volume'] = (df['ask_size1'] + df['ask_size2']) + (df['bid_size1'] + df['bid_size2'])\n    df['volume_imbalance'] = abs((df['ask_size1'] + df['ask_size2']) - (df['bid_size1'] + df['bid_size2']))\n    \n    df['log_return1_sqr'] = df['log_return1'] ** 2\n    df['log_return2_sqr'] = df['log_return2'] ** 2\n    df['log_return3_sqr'] = df['log_return3'] ** 2\n    df['log_return4_sqr'] = df['log_return4'] ** 2\n    # Dict for aggregations\n    create_feature_dict = {\n        'wap1': ['sum', 'std', 'min','max'],\n        'wap2': ['sum', 'std', 'min','max'],\n        'wap3': ['sum', 'std', 'min','max'],\n        'wap4': ['sum', 'std', 'min','max'],\n        'log_return1_sqr': ['sum', 'std', 'min','max'],\n        'log_return2_sqr': ['sum', 'std', 'min','max'],\n        'log_return3_sqr': ['sum', 'std', 'min','max'],\n        'log_return4_sqr': ['sum', 'std', 'min','max'],\n        'wap_balance': ['sum', 'mean', 'min','max'],\n        'price_spread':['sum', 'mean', 'min','max'],\n        'price_spread2':['sum', 'mean', 'min','max'],\n        'bid_spread':['sum', 'mean', 'min','max'],\n        'ask_spread':['sum', 'mean', 'min','max'],\n        'total_volume':['sum', 'mean', 'min','max'],\n        'volume_imbalance':['sum', 'mean', 'min','max'],\n        \"bid_ask_spread\":['sum',  'mean', 'min','max'],\n    }\n    create_feature_dict_time = {\n        'log_return1_sqr': ['sum', 'std', 'min','max'],\n        'log_return2_sqr': ['sum', 'std', 'min','max'],\n        'log_return3_sqr': ['sum', 'std', 'min','max'],\n        'log_return4_sqr': ['sum', 'std', 'min','max'],\n    }\n    \n    def get_stats_window(fe_dict,seconds_in_bucket, add_suffix = False):\n        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(fe_dict).reset_index()\n        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n        if add_suffix:\n            df_feature.columns = [col + '_' + str(seconds_in_bucket) for col in df_feature.columns]\n        return df_feature\n    \n    # Get the stats for different windows\n    df_feature = get_stats_window(create_feature_dict,seconds_in_bucket = 0, add_suffix = False)\n    df_feature_500 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 500, add_suffix = True)\n    df_feature_400 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 400, add_suffix = True)\n    df_feature_300 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 300, add_suffix = True)\n    df_feature_200 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 200, add_suffix = True)\n    df_feature_100 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 100, add_suffix = True)\n\n    # Merge all\n    df_feature = df_feature.merge(df_feature_500, how = 'left', left_on = 'time_id_', right_on = 'time_id__500')\n    df_feature = df_feature.merge(df_feature_400, how = 'left', left_on = 'time_id_', right_on = 'time_id__400')\n    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n    df_feature = df_feature.merge(df_feature_200, how = 'left', left_on = 'time_id_', right_on = 'time_id__200')\n    df_feature = df_feature.merge(df_feature_100, how = 'left', left_on = 'time_id_', right_on = 'time_id__100')\n    \n    # Drop tmp columns\n    df_feature.drop(['time_id__500','time_id__400', 'time_id__300', 'time_id__200','time_id__100'], axis = 1, inplace = True)\n    \n    # Create row_id so we can merge\n    stock_id = file_path.split('=')[1]\n    df_feature['stock_id'] = str(stock_id) + '-'\n\n    df_feature['row_id'] = df_feature['stock_id'] + df_feature['time_id_'].astype(str)\n    \n    return df_feature","metadata":{"execution":{"iopub.status.busy":"2021-09-20T12:03:12.704408Z","iopub.execute_input":"2021-09-20T12:03:12.704823Z","iopub.status.idle":"2021-09-20T12:03:12.733437Z","shell.execute_reply.started":"2021-09-20T12:03:12.70479Z","shell.execute_reply":"2021-09-20T12:03:12.732745Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\ndef transform(df, groupby='time_id', feat='price', agg='mean' ):\n    return df.merge( \n        df.groupby(groupby)[feat].agg(agg).reset_index().rename({feat:feat+'_'+agg}, axis=1),\n        on=groupby,\n        how='left' \n    )\n\n# Function to preprocess trade data (for each stock id)\ndef trade_preprocessor(file_path):\n    df = pd.read_parquet(file_path)\n    df = convert_to_32bit(df)\n    \n    df['log_return'] = df['price'].log()\n    df['log_return'] = df['log_return'] - df.groupby(['time_id'])['log_return'].shift(1).reset_index(drop=True)\n    df['log_return_sqr'] = df['log_return'] ** 2\n    \n    df['amount']=df['price']*df['size']\n    \n    # Dict for aggregations\n    create_feature_dict = {\n        'log_return_sqr': ['sum', 'std','max', 'min'],\n        'seconds_in_bucket':['nunique','std', 'mean','max', 'min'],\n        'size':['sum', 'nunique','std','max', 'min'],\n        'order_count':['sum','nunique','max','min','std'],\n        'amount':['sum','std','max','min'],\n    }\n    create_feature_dict_time = {\n        'log_return_sqr': ['sum', 'std','max','min'],\n        'seconds_in_bucket':['nunique'],\n        'size':['sum','mean','std','min','max'],\n        'order_count':['sum','mean','std','min','max'],\n    }\n    # Function to get group stats for different windows (seconds in bucket)\n    def get_stats_window(fe_dict,seconds_in_bucket, add_suffix = False):\n        # Group by the window\n        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(fe_dict).reset_index()\n        # Rename columns joining suffix\n        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n        # Add a suffix to differentiate windows\n        if add_suffix:\n            df_feature.columns = [col + '_' + str(seconds_in_bucket) for col in df_feature.columns]\n        return df_feature\n\n    # Get the stats for different windows\n    df_feature = get_stats_window(create_feature_dict,seconds_in_bucket = 0, add_suffix = False)\n    df_feature_500 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 500, add_suffix = True)\n    df_feature_400 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 400, add_suffix = True)\n    df_feature_300 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 300, add_suffix = True)\n    df_feature_200 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 200, add_suffix = True)\n    df_feature_100 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 100, add_suffix = True)\n    df = df.sort_values(['time_id','seconds_in_bucket']).reset_index(drop=True)\n    \n    df = transform(df, groupby='time_id', feat='price', agg='mean' )\n    df = transform(df, groupby='time_id', feat='price', agg='sum' )\n    df = transform(df, groupby='time_id', feat='size', agg='mean' )\n    df['price_dif'] = ((df['price'] - df.groupby(['time_id'])['price'].shift(1).reset_index(drop=True)) / df['price']).fillna(0.)\n    df['tendencyV'] = df['size'] * df['price_dif']\n    df['f_max'] = 1 * (df['price'] >= df['price_mean'])\n    df['f_min'] = 1 * (df['price'] < df['price_mean'])\n    df['df_max'] = 1 * (df['price_dif'] >= 0)\n    df['df_min'] = 1 * (df['price_dif'] < 0)\n    df['abs_dif'] = (df['price'] - df['price_mean']).abs()\n    df['price_sqr'] = df['price']**2\n    df['size_dif'] = (df['size'] - df['size_mean']).abs()\n    df['size_sqr'] = df['size']**2\n    df['iqr_p25'] = df.groupby(['time_id'])['price'].quantile(0.15).reset_index(drop=True)\n    df['iqr_p75'] = df.groupby(['time_id'])['price'].quantile(0.85).reset_index(drop=True)\n    df['iqr_p_v25'] = df.groupby(['time_id'])['size'].quantile(0.15).reset_index(drop=True)\n    df['iqr_p_v75'] = df.groupby(['time_id'])['size'].quantile(0.85).reset_index(drop=True)\n\n    dt = df.groupby('time_id')[['tendencyV','price','price_dif','f_max','f_min','df_max','df_min','abs_dif','price_sqr','size_dif','size_sqr','iqr_p25','iqr_p75','iqr_p_v25','iqr_p_v75']].agg(\n        {\n            'tendencyV':['sum','std','max', 'min'],\n            'price':['mean','std','max', 'min'],\n            'price_dif':['mean','std','max', 'min'],\n            'f_max':['mean','std','max', 'min'],\n            'f_min':['mean','std','max', 'min'],\n            'df_max':['mean','std','max', 'min'],\n            'df_min':['mean','std','max', 'min'],\n            'abs_dif':['median','std','max', 'min'],\n            'price_sqr':['sum','std','max', 'min'],\n            'size_dif':['median','std','max', 'min'],\n            'size_sqr':['sum','std','max', 'min'],\n            'iqr_p25':['mean','std','max', 'min'],\n            'iqr_p75':['mean','std','max', 'min'],\n            'iqr_p_v25':['mean','std','max', 'min'],\n            'iqr_p_v75':['mean','std','max', 'min'],\n        }\n    )\n    dt.columns = [i+'_'+j for i, j in dt.columns] \n    df_feature = df_feature.merge(dt, left_on='time_id_', right_index=True, how='left')\n    \n    # Merge all\n    df_feature = df_feature.merge(df_feature_500, how = 'left', left_on = 'time_id_', right_on = 'time_id__500')\n    df_feature = df_feature.merge(df_feature_400, how = 'left', left_on = 'time_id_', right_on = 'time_id__400')\n    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n    df_feature = df_feature.merge(df_feature_200, how = 'left', left_on = 'time_id_', right_on = 'time_id__200')\n    df_feature = df_feature.merge(df_feature_100, how = 'left', left_on = 'time_id_', right_on = 'time_id__100')\n    \n    # Drop tmp columns\n    df_feature = df_feature.sort_values(['time_id_' ]).reset_index(drop=True)\n    \n    stock_id = file_path.split('=')[1]\n    df_feature['stock_id'] = str(stock_id) + '-'\n    df_feature['row_id'] = df_feature['stock_id'] + df_feature['time_id_'].astype(str)\n    df_feature.drop(['time_id__500','time_id__400', 'time_id__300', 'time_id__200', 'time_id_','time_id__100','stock_id'], axis = 1, inplace = True)\n\n    fnames = ['trade_' + f for f in df_feature.columns]\n    fnames[-1] = 'row_id'\n    df_feature.columns = fnames\n\n    return df_feature","metadata":{"execution":{"iopub.status.busy":"2021-09-20T12:03:12.734869Z","iopub.execute_input":"2021-09-20T12:03:12.73519Z","iopub.status.idle":"2021-09-20T12:03:12.759879Z","shell.execute_reply.started":"2021-09-20T12:03:12.735159Z","shell.execute_reply":"2021-09-20T12:03:12.759218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Process all train .parquet files. Create features using cudf (GPU)\n# Note cudf speed to load and apply all feature engineering in all train set stocks.","metadata":{}},{"cell_type":"code","source":"%%time\nDF_TRAIN = []\nfor stock_id in tqdm(train_stock_ids):\n    df_tmp = pd.merge( \n        book_preprocessor(data_dir + \"book_train.parquet/stock_id=\" + str(stock_id)),\n        trade_preprocessor(data_dir + \"trade_train.parquet/stock_id=\" + str(stock_id)),\n        on = 'row_id',\n        how = 'left'\n    )\n    df_tmp['stock_id'] = stock_id\n    df_tmp = convert_to_32bit(df_tmp) # to save memory\n    #df_tmp.to_parquet( 'train_parquet/'+str(stock_id)+'.parquet' )\n    DF_TRAIN.append(df_tmp)\n\n# Concatenate all stock_id in the same dataframe\nDF_TRAIN = pd.concat(DF_TRAIN, ignore_index=True )\n_ = gc.collect()\n\n# Flag to filter train/test rows\nDF_TRAIN['is_test'] = 0\nDF_TRAIN.shape","metadata":{"execution":{"iopub.status.busy":"2021-09-20T12:03:12.807644Z","iopub.execute_input":"2021-09-20T12:03:12.807838Z","iopub.status.idle":"2021-09-20T12:05:44.456874Z","shell.execute_reply.started":"2021-09-20T12:03:12.807817Z","shell.execute_reply":"2021-09-20T12:05:44.45616Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Process all test .parquet files. Create features using cudf (GPU)","metadata":{}},{"cell_type":"code","source":"%%time\nDF_TEST = []\nfor stock_id in tqdm(test_stock_ids):\n    df_tmp = pd.merge( \n        book_preprocessor(data_dir + \"book_test.parquet/stock_id=\" + str(stock_id)),\n        trade_preprocessor(data_dir + \"trade_test.parquet/stock_id=\" + str(stock_id)),\n        on = 'row_id',\n        how = 'left'\n    )\n    df_tmp['stock_id'] = stock_id\n    df_tmp = convert_to_32bit(df_tmp) # to save memory\n    #df_tmp.to_parquet( 'test_parquet/'+str(stock_id)+'.parquet' )\n    DF_TEST.append(df_tmp)\n    \n# Concatenate all stock_id in the same dataframe\nDF_TEST = pd.concat(DF_TEST, ignore_index=True )\n_ = gc.collect()\n\n# Flag to filter train/test rows\nDF_TEST['is_test'] = 1\nDF_TEST.shape","metadata":{"execution":{"iopub.status.busy":"2021-09-20T12:05:44.45837Z","iopub.execute_input":"2021-09-20T12:05:44.459087Z","iopub.status.idle":"2021-09-20T12:05:45.176711Z","shell.execute_reply.started":"2021-09-20T12:05:44.459049Z","shell.execute_reply":"2021-09-20T12:05:45.175979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TRAIN = pd.concat( [DF_TRAIN, DF_TEST] ).sort_values(['stock_id','time_id_']).reset_index(drop=True)\n\n# del DF_TRAIN, DF_TEST\n# _ = gc.collect()\nTRAIN.shape","metadata":{"execution":{"iopub.status.busy":"2021-09-20T12:05:45.178124Z","iopub.execute_input":"2021-09-20T12:05:45.178402Z","iopub.status.idle":"2021-09-20T12:05:45.709157Z","shell.execute_reply.started":"2021-09-20T12:05:45.178368Z","shell.execute_reply":"2021-09-20T12:05:45.708489Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\ndef get_time_stock(df_):\n    vol_cols = ['log_return1_sqr_sum_500', 'log_return2_sqr_sum_500', 'log_return3_sqr_sum_500', 'log_return4_sqr_sum_500', 'trade_log_return_sqr_sum', 'trade_log_return_sqr_std', 'trade_seconds_in_bucket_nunique' ]\n\n    df = df_.copy()\n    df_stock_id = df.groupby(['stock_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n    df_stock_id.columns = ['_'.join(col) + '_stock' for col in df_stock_id.columns]\n\n    df_time_id = df.groupby(['time_id_'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n    df_time_id.columns = ['_'.join(col)+ '_time' for col in df_time_id.columns]\n    \n    df = df.merge(df_stock_id, how = 'left', left_on = ['stock_id'], right_on = ['stock_id__stock'])\n    df = df.merge(df_time_id, how = 'left', left_on = ['time_id_'], right_on = ['time_id___time'])\n    df.drop(['stock_id__stock', 'time_id___time'], axis = 1, inplace = True)\n    return df\n\nTRAIN_ = get_time_stock(DF_TRAIN)\nTEST_ = get_time_stock(DF_TEST)\nTRAIN_.drop(['stock_id','time_id_'], axis = 1, inplace = True)\nTEST_.drop(['stock_id','time_id_'], axis = 1, inplace = True)\nprint(TRAIN_.shape)\nprint(TRAIN_.head())","metadata":{"execution":{"iopub.status.busy":"2021-09-20T12:05:45.711156Z","iopub.execute_input":"2021-09-20T12:05:45.711423Z","iopub.status.idle":"2021-09-20T12:05:46.334172Z","shell.execute_reply.started":"2021-09-20T12:05:45.711387Z","shell.execute_reply":"2021-09-20T12:05:46.33335Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = train.merge(TRAIN_, on='row_id', how='left' )\ntest  = test.merge(TEST_, on='row_id', how='left' )\n\ndel TRAIN_, TRAIN\n_ = gc.collect()\n\ntrain.shape, test.shape","metadata":{"execution":{"iopub.status.busy":"2021-09-20T12:05:46.335471Z","iopub.execute_input":"2021-09-20T12:05:46.335817Z","iopub.status.idle":"2021-09-20T12:05:46.666185Z","shell.execute_reply.started":"2021-09-20T12:05:46.335778Z","shell.execute_reply":"2021-09-20T12:05:46.665377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-20T12:05:46.667417Z","iopub.execute_input":"2021-09-20T12:05:46.667895Z","iopub.status.idle":"2021-09-20T12:05:47.170999Z","shell.execute_reply.started":"2021-09-20T12:05:46.667857Z","shell.execute_reply":"2021-09-20T12:05:47.170163Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Now time to calculate correlation between all stock. The best way is using a correlation matrix, so first pivot all target variables by stock_id, then just calculate the correlation matrix.\n# To Find correlated stocks use Kmeans algorithm on the correlation matrix. This procedure is a bit leak because it not being processed using crossvalidation, but it won't leak much since only 6 clusters are being calculated.","metadata":{}},{"cell_type":"code","source":"%%time\ntrain_p = pd.read_csv('../input/optiver-realized-volatility-prediction/train.csv')\ntrain_p = train_p.pivot(index='time_id', columns=['stock_id'], values=['target']).fillna(0.)\ncorr = train_p.corr()\n\nkm = cuml.KMeans(n_clusters=7, max_iter=2000, n_init=5).fit(corr)\ndf = pd.DataFrame( {'stock_id': [ f[1] for f in corr.columns ], 'cluster': km.labels_} )\ndf = convert_to_32bit(df)\n\ntrain = train.merge(df, on='stock_id', how='left')\ntest = test.merge(df, on='stock_id', how='left')\n\n\ndel train_p, df, corr, km\n_ = gc.collect()\n\n# Clusters found\ntrain.groupby('cluster')['time_id'].agg('count')","metadata":{"execution":{"iopub.status.busy":"2021-09-20T12:05:47.90577Z","iopub.execute_input":"2021-09-20T12:05:47.90609Z","iopub.status.idle":"2021-09-20T12:05:52.199019Z","shell.execute_reply.started":"2021-09-20T12:05:47.906053Z","shell.execute_reply":"2021-09-20T12:05:52.195643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"matTrain = []\nmatTest = []\n\n# 6 clusters\nfor ind in range(train.cluster.max()+1):\n    print(ind)\n    newDf = train.loc[train['cluster']==ind].copy()\n    newDf = newDf.groupby(['time_id']).agg('mean')\n    newDf.loc[:,'stock_id'] = 127+ind\n    matTrain.append ( newDf )\n    \n    newDf = test.loc[test['cluster']==ind].copy()\n    newDf = newDf.groupby(['time_id']).agg('mean')\n    newDf.loc[:,'stock_id'] = 127+ind\n    matTest.append ( newDf )\n    \nmatTrain = pd.concat(matTrain).reset_index()\nmatTrain.drop(columns=['target'],inplace=True)\n\nmatTest = pd.concat(matTest).reset_index()\n\nmatTrain.shape, matTest.shape","metadata":{"execution":{"iopub.status.busy":"2021-09-20T12:05:52.200212Z","iopub.execute_input":"2021-09-20T12:05:52.200445Z","iopub.status.idle":"2021-09-20T12:05:54.992118Z","shell.execute_reply.started":"2021-09-20T12:05:52.200415Z","shell.execute_reply":"2021-09-20T12:05:54.991459Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"matTest = pd.concat([matTest , matTrain.loc[matTrain.time_id==5]])\nmatTrain = matTrain.pivot(index='time_id', columns='stock_id')\nmatTrain.columns = [x[0]+'_stock'+str(int(x[1])) for x in matTrain.columns]\nmatTrain.reset_index(inplace=True)\n\nmatTest = matTest.pivot(index='time_id', columns='stock_id')\nmatTest.columns = [x[0]+'_stock'+str(int(x[1])) for x in matTest.columns]\nmatTest.reset_index(inplace=True)\n\nmatTrain.shape, matTest.shape","metadata":{"execution":{"iopub.status.busy":"2021-09-20T12:05:54.995445Z","iopub.execute_input":"2021-09-20T12:05:54.995655Z","iopub.status.idle":"2021-09-20T12:05:56.337311Z","shell.execute_reply.started":"2021-09-20T12:05:54.995632Z","shell.execute_reply":"2021-09-20T12:05:56.336666Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kfeatures = [\n    'time_id',\n        \n    'wap1_sum_stock127',\n    'wap1_sum_stock128',     \n    'wap1_sum_stock129',\n    'wap1_sum_stock130',     \n    'wap1_sum_stock131',\n    'wap1_sum_stock132',\n        \n    'wap2_sum_stock127',\n    'wap2_sum_stock128',     \n    'wap2_sum_stock129',\n    'wap2_sum_stock130',     \n    'wap2_sum_stock131',\n    'wap2_sum_stock132',\n        \n    'wap3_sum_stock127',\n    'wap3_sum_stock128',     \n    'wap3_sum_stock129',\n    'wap3_sum_stock130',     \n    'wap3_sum_stock131',\n    'wap3_sum_stock132',\n        \n    'wap4_sum_stock127',\n    'wap4_sum_stock128',     \n    'wap4_sum_stock129',\n    'wap4_sum_stock130',     \n    'wap4_sum_stock131',\n    'wap4_sum_stock132',\n    \n    'log_return1_sqr_sum_stock127',\n    'log_return1_sqr_sum_stock128',     \n    'log_return1_sqr_sum_stock129',\n    'log_return1_sqr_sum_stock130',     \n    'log_return1_sqr_sum_stock131',\n    'log_return1_sqr_sum_stock132',\n\n    'log_return2_sqr_sum_stock127',\n    'log_return2_sqr_sum_stock128',     \n    'log_return2_sqr_sum_stock129',\n    'log_return2_sqr_sum_stock130',     \n    'log_return2_sqr_sum_stock131',\n    'log_return2_sqr_sum_stock132',\n\n    'log_return3_sqr_sum_stock127',\n    'log_return3_sqr_sum_stock128',     \n    'log_return3_sqr_sum_stock129',\n    'log_return3_sqr_sum_stock130',     \n    'log_return3_sqr_sum_stock131',\n    'log_return3_sqr_sum_stock132',\n\n    'log_return4_sqr_sum_stock127',\n    'log_return4_sqr_sum_stock128',     \n    'log_return4_sqr_sum_stock129',\n    'log_return4_sqr_sum_stock130',     \n    'log_return4_sqr_sum_stock131',\n    'log_return4_sqr_sum_stock132',\n    \n    'total_volume_sum_stock127',\n    'total_volume_sum_stock128', \n    'total_volume_sum_stock129',\n    'total_volume_sum_stock130', \n    'total_volume_sum_stock131',\n    'total_volume_sum_stock132',\n    \n    'trade_size_sum_stock127',\n    'trade_size_sum_stock128', \n    'trade_size_sum_stock129',\n    'trade_size_sum_stock130', \n    'trade_size_sum_stock131',\n    'trade_size_sum_stock132',\n    \n    'trade_order_count_sum_stock127',\n    'trade_order_count_sum_stock128',\n    'trade_order_count_sum_stock129',\n    'trade_order_count_sum_stock130',\n    'trade_order_count_sum_stock131',      \n    'trade_order_count_sum_stock132',\n    \n    'price_spread_sum_stock127',\n    'price_spread_sum_stock128',\n    'price_spread_sum_stock129',\n    'price_spread_sum_stock130',\n    'price_spread_sum_stock131',   \n    'price_spread_sum_stock132',\n    \n    'bid_spread_sum_stock127',\n    'bid_spread_sum_stock128',\n    'bid_spread_sum_stock129',\n    'bid_spread_sum_stock130',\n    'bid_spread_sum_stock131',\n    'bid_spread_sum_stock132',\n    \n    'ask_spread_sum_stock127',\n    'ask_spread_sum_stock128',\n    'ask_spread_sum_stock129',\n    'ask_spread_sum_stock130',\n    'ask_spread_sum_stock131',   \n    'ask_spread_sum_stock132',\n    \n    'volume_imbalance_sum_stock127',\n    'volume_imbalance_sum_stock128',\n    'volume_imbalance_sum_stock129',\n    'volume_imbalance_sum_stock130',\n    'volume_imbalance_sum_stock131',       \n    'volume_imbalance_sum_stock132',\n    \n    'bid_ask_spread_sum_stock127',\n    'bid_ask_spread_sum_stock128',\n    'bid_ask_spread_sum_stock129',\n    'bid_ask_spread_sum_stock130',\n    'bid_ask_spread_sum_stock131',\n    'bid_ask_spread_sum_stock132',\n]\nmatTrain = convert_to_32bit(matTrain)\nmatTest = convert_to_32bit(matTest)\n\ntrain = pd.merge(train,matTrain[kfeatures],how='left',on='time_id')\ntest = pd.merge(test,matTest[kfeatures],how='left',on='time_id')\n_ = gc.collect()\n\nprint( train.shape, test.shape )","metadata":{"execution":{"iopub.status.busy":"2021-09-20T12:05:56.338568Z","iopub.execute_input":"2021-09-20T12:05:56.339022Z","iopub.status.idle":"2021-09-20T12:05:58.232256Z","shell.execute_reply.started":"2021-09-20T12:05:56.338984Z","shell.execute_reply":"2021-09-20T12:05:58.231553Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train=train[~(train[\"stock_id\"]==31)].reset_index(drop=True)\n# _= gc.collect()\n\ntrain = convert_to_32bit(train)\ntest  = convert_to_32bit(test)\n_= gc.collect()\n\ntrain.shape, test.shape","metadata":{"execution":{"iopub.status.busy":"2021-09-20T12:05:58.233508Z","iopub.execute_input":"2021-09-20T12:05:58.233929Z","iopub.status.idle":"2021-09-20T12:05:58.463199Z","shell.execute_reply.started":"2021-09-20T12:05:58.233892Z","shell.execute_reply":"2021-09-20T12:05:58.462487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_target = train.target.to_pandas() #need to be numpy or pandas for sklearn \ntime_id = train.time_id.to_pandas()\nNFOLD = 5\n\ndef rmspe(y_true, y_pred):\n    return np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\n\n# Target min and max values\nnp.min(y_target), np.max(y_target)","metadata":{"execution":{"iopub.status.busy":"2021-09-20T12:05:58.46461Z","iopub.execute_input":"2021-09-20T12:05:58.464867Z","iopub.status.idle":"2021-09-20T12:05:58.476952Z","shell.execute_reply.started":"2021-09-20T12:05:58.464833Z","shell.execute_reply":"2021-09-20T12:05:58.47608Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# XGBoost GPU","metadata":{}},{"cell_type":"code","source":"xgbtime = time.time()\n\n# Define the custom metric to optimize\ndef evalerror(preds, dtrain):\n    labels = dtrain.get_label()\n    err = rmspe(labels, preds)\n    return 'rmspe', err\n\ndef train_and_evaluate_xgb(train, test, params, colNames):\n    # Sample weight\n    train['target_sqr'] = 1. / (train['target'] ** 1.55 + 9e-7)    \n\n    dtest = xgb.DMatrix(test[colNames])\n\n    y_train = np.zeros(len(train))\n    y_test = np.zeros(len(test))\n\n    kf = GroupKFold(n_splits=NFOLD)\n    for fold, (train_idx, valid_idx) in enumerate(kf.split(train, y_target, time_id)):\n        print('Fold:', fold)\n        dtrain = xgb.DMatrix(train.loc[train_idx, colNames], train.loc[train_idx, 'target'], weight=train.loc[train_idx, 'target_sqr'])\n        dvalid = xgb.DMatrix(train.loc[valid_idx, colNames], train.loc[valid_idx, 'target'])\n        model = xgb.train(\n            params,\n            dtrain,\n            3000,\n            #[(dtrain, \"train\"), (dvalid, \"valid\")],\n            [(dvalid, \"valid\")],\n            verbose_eval=250,\n            early_stopping_rounds=50,\n            feval=evalerror,\n        )\n        y_train[valid_idx] = np.clip(model.predict(dvalid), 2e-4, 0.072)\n        y_test += np.clip((model.predict(dtest)), 2e-4, 0.072)\n        print( 'Rmspe Fold:', rmspe(y_target[valid_idx], y_train[valid_idx]) )\n    y_test /= NFOLD\n    \n    print( 'XGBoost Rmspe CV:', rmspe(y_target, y_train) )\n    print( pandas.DataFrame.from_dict( model.get_score(), orient='index').sort_values(0, ascending=False).head(20) )\n    print()\n    \n    del model, dtest, dtrain, dvalid\n    _ = gc.collect()\n    \n    return y_train, y_test\n\n\ncolNames = [col for col in list(train.columns) if col not in {'is_train', 'time_id', 'target', 'row_id', 'target_sqr', 'is_train'}]\ncolNames = [col for col in colNames if col.find('min')<0 ]\nparams = {\n        \"subsample\": 0.60,\n        \"colsample_bytree\": 0.40,\n        \"max_depth\": 6,\n        \"learning_rate\": 0.02,\n        \"objective\": \"reg:squarederror\",\n        'disable_default_eval_metric': 1, # <- necessary for XGBoost to earlystop by Rmspe and not the default rmse\n        \"nthread\": -1,\n        \"tree_method\": \"gpu_hist\",\n        \"gpu_id\": 0,\n        \"max_bin\": 128, \n        'min_child_weight': 2,\n        'reg_lambda': 0.001,\n        'reg_alpha': 0.01, \n        'seed' : 2021,\n    }\ny_train1a, y_test1a = train_and_evaluate_xgb(train, test, params, colNames)\n\n\ncolNames = [col for col in list(train.columns) if col not in {'is_train', 'time_id', 'target', 'row_id', 'target_sqr', 'is_train'}]\ncolNames = [col for col in colNames if col.find('max')<0 ]\nparams = {\n        \"subsample\": 0.85,\n        \"colsample_bytree\": 0.25,\n        \"max_depth\": 7,\n        \"learning_rate\": 0.02,\n        \"objective\": \"reg:squarederror\",\n        'disable_default_eval_metric': 1, # <- necessary for XGBoost to earlystop by Rmspe and not the default rmse\n        \"nthread\": -1,\n        \"tree_method\": \"gpu_hist\",\n        \"gpu_id\": 0,\n        \"max_bin\": 128, \n        'min_child_weight': 2,\n        'reg_lambda': 0.001,\n        'reg_alpha': 0.01, \n        'seed' : 2022,\n    }\ny_train1b, y_test1b = train_and_evaluate_xgb(train, test, params, colNames)\n\n\ny_train1 = 0.75*y_train1a + 0.25*y_train1b\ny_test1  = 0.75*y_test1a  + 0.25*y_test1b\n\n\nxgbtime = time.time() - xgbtime\n\nprint( 'XGBoost Rmspe CV:', rmspe(y_target, y_train1), 'time: ', int(xgbtime), 's', y_test1[:3] )","metadata":{"execution":{"iopub.status.busy":"2021-09-20T13:07:03.429062Z","iopub.execute_input":"2021-09-20T13:07:03.429338Z","iopub.status.idle":"2021-09-20T13:12:05.979544Z","shell.execute_reply.started":"2021-09-20T13:07:03.429308Z","shell.execute_reply":"2021-09-20T13:12:05.978014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"catbtime = time.time()\n\ndef train_and_evaluate_catb(train, test, params):\n\n    # Sample weight\n    train['target_sqr'] = 1. / (train['target'] ** 1.75 + 1e-6)\n\n    colNames = [col for col in list(train.columns) if col not in {'is_train', 'time_id', 'target', 'row_id', 'target_sqr', 'is_train'}]\n\n    y_train = np.zeros(len(train))\n    y_test = np.zeros(len(test))\n\n    kf = GroupKFold(n_splits=NFOLD)\n    for fold, (train_idx, valid_idx) in enumerate(kf.split(train, y_target, time_id)):\n        print('Fold:', fold)\n\n        model = CatBoostRegressor(\n            iterations=3000,\n            learning_rate=0.05,\n            depth=7,\n            loss_function='RMSE',\n            #l2_leaf_reg = 0.001,\n            #random_strength = 0.5,\n            #bagging_temperature = 1.0,\n            task_type=\"GPU\",\n            random_seed = 2021,\n        )        \n        model.fit(\n            X=train.loc[train_idx, colNames].to_pandas(), y=train.loc[train_idx, 'target'].to_pandas(),\n            sample_weight = train.loc[train_idx, 'target_sqr'].to_pandas(),\n            eval_set = (train.loc[valid_idx, colNames].to_pandas(), train.loc[valid_idx, 'target'].to_pandas(),),\n            early_stopping_rounds = 20,\n            cat_features = [0],\n            verbose=False)\n\n        y_train[valid_idx] = np.clip(model.predict(train.loc[valid_idx, colNames].to_pandas()), 2e-4, 0.072)\n        y_test += np.clip((model.predict(test[colNames].to_pandas())), 2e-4, 0.072)\n        print( 'Catboost Rmspe Fold:', rmspe(y_target[valid_idx], y_train[valid_idx]) )        \n        print()\n    y_test /= NFOLD\n    return y_train, y_test\n\n\ny_train2, y_test2 = train_and_evaluate_catb(train, test, params)\n_= gc.collect()\ncatbtime = time.time() - catbtime\n     \nprint( 'Catboost Rmspe CV:', rmspe(y_target, y_train2), 'time: ', int(catbtime), 's', y_test2[:3]  )","metadata":{"execution":{"iopub.status.busy":"2021-09-20T12:17:59.24643Z","iopub.execute_input":"2021-09-20T12:17:59.246631Z","iopub.status.idle":"2021-09-20T12:23:39.802994Z","shell.execute_reply.started":"2021-09-20T12:17:59.246608Z","shell.execute_reply":"2021-09-20T12:23:39.802253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# LightGBM GPU","metadata":{}},{"cell_type":"code","source":"lgbtime = time.time()\n\n# Define the custom metric to optimize\ndef rmspe(y_true, y_pred):\n    return np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\n\ndef feval_rmspe(y_pred, lgb_train):\n    y_true = lgb_train.get_label()\n    return 'RMSPE', rmspe(y_true, y_pred), False\n\ndef train_and_evaluate_lgb(train, test, params):\n    \n    features = [col for col in train.columns if col not in {\"time_id\", \"target\", \"target_sqr\", \"row_id\", 'is_train'}]\n    y = train['target']\n    \n    y_train = np.zeros(train.shape[0])\n    y_test = np.zeros(test.shape[0])\n    \n    kf = GroupKFold(n_splits=NFOLD)\n    for fold, (trn_ind, val_ind) in enumerate(kf.split(train, y_target, time_id)):\n        print('Fold:', fold)\n        x_train, x_val = train.iloc[trn_ind], train.iloc[val_ind]\n        y_tra, y_val = y.iloc[trn_ind], y.iloc[val_ind]\n        \n        train_dataset = lgb.Dataset(x_train[features], y_tra, weight = (1. / (np.square(y_tra) + 1e-6)) )\n        valid_dataset = lgb.Dataset(x_val[features], y_val)\n        model = lgb.train(params = params,\n                          num_boost_round=3000,\n                          train_set = train_dataset, \n                          valid_sets = [train_dataset, valid_dataset], \n                          verbose_eval = 100,\n                          early_stopping_rounds=20,\n                          feval = feval_rmspe)\n        \n        y_train[val_ind] = np.clip(model.predict(x_val[features]), 2e-4, 0.072)\n        y_test += np.clip((model.predict(test[features])), 2e-4, 0.072)        \n    y_test/=NFOLD\n    \n    print('LightGBM Rmspe Fold:', rmspe(y_target, y_train))\n    lgb.plot_importance(model,max_num_features=20)\n    \n    return y_train, y_test\n\n\nparams = {\n    'objective': 'rmse',\n    'boosting_type': 'gbdt',\n    'max_depth': -1,\n    'max_bin':255,\n    'min_data_in_leaf':750,\n    'learning_rate': 0.05,\n    'subsample': 0.72,\n    'subsample_freq': 3,\n    'feature_fraction': 0.5,\n    'lambda_l1': 0.5,\n    'lambda_l2': 1.0,\n    'categorical_column':[0],\n    'seed':2021,\n    'n_jobs':-1,\n    'verbose': -1,\n    'device': 'gpu',\n    'num_gpu': 1,\n    'gpu_platform_id':-1,\n    'gpu_device_id':-1,\n    'gpu_use_dp': False,\n}\n\ny_train3, y_test3 = train_and_evaluate_lgb(train.to_pandas(), test.to_pandas(), params)\n_= gc.collect()\n\nprint( 'LightGBM Rmspe CV:', rmspe(y_target, y_train3), 'time: ', int(time.time() - lgbtime), 's', y_test3[:3]   )","metadata":{"execution":{"iopub.status.busy":"2021-09-20T12:23:39.804414Z","iopub.execute_input":"2021-09-20T12:23:39.804683Z","iopub.status.idle":"2021-09-20T12:29:21.497443Z","shell.execute_reply.started":"2021-09-20T12:23:39.80465Z","shell.execute_reply":"2021-09-20T12:29:21.495322Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Ensembling Time","metadata":{}},{"cell_type":"code","source":"print( 'LightGBM Rmspe:', rmspe(y_target, y_train3) )\nprint( 'XGBoost Rmspe:', rmspe(y_target, y_train1) )\nprint( 'CatBoost Rmspe:', rmspe(y_target, y_train2) )","metadata":{"execution":{"iopub.status.busy":"2021-09-20T13:06:33.386504Z","iopub.execute_input":"2021-09-20T13:06:33.387228Z","iopub.status.idle":"2021-09-20T13:06:33.403727Z","shell.execute_reply.started":"2021-09-20T13:06:33.387192Z","shell.execute_reply":"2021-09-20T13:06:33.402878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def minimize_arit(W):\n    ypred = W[0] * y_train1 + W[1] * y_train2 + W[2] * y_train3\n    return rmspe(y_target, ypred )\n\nW0 = minimize(minimize_arit, [1./3]*3, options={'gtol': 1e-6, 'disp': True}).x\nprint('Weights arit:',W0)","metadata":{"execution":{"iopub.status.busy":"2021-09-20T13:12:11.87012Z","iopub.execute_input":"2021-09-20T13:12:11.870368Z","iopub.status.idle":"2021-09-20T13:12:12.259751Z","shell.execute_reply.started":"2021-09-20T13:12:11.870341Z","shell.execute_reply":"2021-09-20T13:12:12.258962Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def signed_power(var, p=2):\n    return np.sign(var) * np.abs(var)**p\n\ndef minimize_geom(W):\n    ypred = signed_power(y_train1, W[0]) * signed_power(y_train2, W[1]) * signed_power(y_train3, W[2])\n    return rmspe(y_target, ypred)\n\nW1 = minimize(minimize_geom, [1./3]*3, options={'gtol': 1e-6, 'disp': True}).x\n\nprint('weights geom:',W1)","metadata":{"execution":{"iopub.status.busy":"2021-09-20T13:12:18.213319Z","iopub.execute_input":"2021-09-20T13:12:18.214046Z","iopub.status.idle":"2021-09-20T13:12:29.390465Z","shell.execute_reply.started":"2021-09-20T13:12:18.214004Z","shell.execute_reply":"2021-09-20T13:12:29.389734Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ypred0 = W0[0] * y_train1 + W0[1] * y_train2 + W0[2] * y_train3\nprint( np.min(ypred0), np.max(ypred0))\n\nypred1 = signed_power(y_train1, W1[0]) * signed_power(y_train2, W1[1]) * signed_power(y_train3, W1[2])\nprint( np.min(ypred1) , np.max(ypred1) )\n\nprint( 'Ensemble:', rmspe(y_target, np.clip((ypred0+ypred1)/2 ,0.0002, 0.071) ) )","metadata":{"execution":{"iopub.status.busy":"2021-09-20T13:12:29.392103Z","iopub.execute_input":"2021-09-20T13:12:29.392513Z","iopub.status.idle":"2021-09-20T13:12:29.488461Z","shell.execute_reply.started":"2021-09-20T13:12:29.392478Z","shell.execute_reply":"2021-09-20T13:12:29.487758Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print( np.min(ypred0),np.mean(ypred0),np.max(ypred0),np.std(ypred0) )\nprint( np.min(ypred1),np.mean(ypred1),np.max(ypred1),np.std(ypred1) )","metadata":{"execution":{"iopub.status.busy":"2021-09-20T13:13:05.986123Z","iopub.execute_input":"2021-09-20T13:13:05.987007Z","iopub.status.idle":"2021-09-20T13:13:06.001037Z","shell.execute_reply.started":"2021-09-20T13:13:05.98695Z","shell.execute_reply":"2021-09-20T13:13:06.000108Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.hist(ypred0, bins=100)\nplt.hist(ypred1, bins=100, alpha=0.5)","metadata":{"execution":{"iopub.status.busy":"2021-09-20T13:13:27.81535Z","iopub.execute_input":"2021-09-20T13:13:27.815725Z","iopub.status.idle":"2021-09-20T13:13:28.686301Z","shell.execute_reply.started":"2021-09-20T13:13:27.815685Z","shell.execute_reply":"2021-09-20T13:13:28.685504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['ypred'] = np.clip((ypred0+ypred1)/2 ,0.0002, 0.071)\ntrain['error'] = (train['target'] - train['ypred']) / train['target']\ntrain['error'] = train['error']**2\n\ndt = train.groupby('stock_id')['error'].agg('mean').reset_index()\ndt['error'] = np.sqrt(dt['error'])\ndt = dt.sort_values('error', ascending=False)\ndt.to_csv('error-contribution.csv', index=False)\ndel train['ypred'], train['error']\ndt.head(10)","metadata":{"execution":{"iopub.status.busy":"2021-09-20T13:13:31.998368Z","iopub.execute_input":"2021-09-20T13:13:31.999122Z","iopub.status.idle":"2021-09-20T13:13:32.04368Z","shell.execute_reply.started":"2021-09-20T13:13:31.999082Z","shell.execute_reply":"2021-09-20T13:13:32.042831Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dt.tail(10)","metadata":{"execution":{"iopub.status.busy":"2021-09-20T12:58:44.073446Z","iopub.execute_input":"2021-09-20T12:58:44.074245Z","iopub.status.idle":"2021-09-20T12:58:44.095375Z","shell.execute_reply.started":"2021-09-20T12:58:44.074197Z","shell.execute_reply":"2021-09-20T12:58:44.09444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ypred0 = W0[0] * y_test1 + W0[1] * y_test2 + W0[2] * y_test3\nypred1 = signed_power(y_test1, W1[0]) * signed_power(y_test2, W1[1]) * signed_power(y_test3, W1[2])\n\nypredtest = np.clip((ypred0+ypred1)/2,0.0002, 0.071)\nprint( ypred0[:3],  ypred1[:3], ypredtest[:3] )\n\ntest['target'] = ypredtest\ntest[['row_id', 'target']].to_csv('submission.csv',index = False)\ntest[['row_id', 'target']].head(3)","metadata":{"execution":{"iopub.status.busy":"2021-09-20T12:34:06.483527Z","iopub.execute_input":"2021-09-20T12:34:06.48413Z","iopub.status.idle":"2021-09-20T12:34:06.511914Z","shell.execute_reply.started":"2021-09-20T12:34:06.484091Z","shell.execute_reply":"2021-09-20T12:34:06.51112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}