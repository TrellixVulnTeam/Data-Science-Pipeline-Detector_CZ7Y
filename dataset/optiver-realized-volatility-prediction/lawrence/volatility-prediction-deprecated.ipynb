{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\n\nimport time\nimport gc\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport plotly\nimport plotly.express as px\n\nfrom IPython import display\nimport sklearn\nimport gc\nimport time\n\ngc.collect()\n\nprint('Library imported.')\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-02T04:52:36.391401Z","iopub.execute_input":"2021-08-02T04:52:36.392044Z","iopub.status.idle":"2021-08-02T04:52:36.515682Z","shell.execute_reply.started":"2021-08-02T04:52:36.391991Z","shell.execute_reply":"2021-08-02T04:52:36.514867Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 定義整個資料預處理\n\ndef preprocess_trade(trade_data):\n\n    trade_data['mean_price'] = trade_data.groupby(['stock_id', 'time_id'])\\\n         ['price'].transform('mean')\n    \n    trade_data['max_price'] = trade_data.groupby(['stock_id', 'time_id'])\\\n         ['price'].transform('max')\n    # total number of traders in same time_id for each stock_id\n    trade_data['total_trader'] = trade_data.groupby(['stock_id', 'time_id'])\\\n         ['order_count'].transform('sum')\n    # total sold: price * size\n    trade_data['total_sold'] = trade_data['price']*trade_data['size']\n    trade_data['mean_totalsold'] = trade_data.groupby(['stock_id', 'time_id'])\\\n        ['total_sold'].transform('mean')\n    trade_data['max_totalsold'] = trade_data.groupby(['stock_id', 'time_id'])\\\n        ['total_sold'].transform('max')\n    \n    trade_data.drop('total_sold', axis = 1, inplace = True)\n    \n    return trade_data\n    \ndef log_return(list_stock_prices):\n    # Give series of stock price, 計算每時段與下一筆資料間的log return\n    return np.log(list_stock_prices).diff() \n\ndef realized_volatility(series_log_return):\n    # Give log return, 計算realized volatility\n    return np.sqrt(np.sum(series_log_return**2))\n\n\n# 計算每個stock_id-time_id單位範圍內的realized volatility\ndef compute_realized_vol(file_path, realized_vol_colname):\n    # file_path: \n    # realized_vol:\n    df_book_data = pd.read_parquet(file_path)\n    # 用公式計算order book資料的WAP\n    df_book_data['wap'] =(df_book_data['bid_price1'] * df_book_data['ask_size1']\n                          +df_book_data['ask_price1'] * df_book_data['bid_size1'])/  \\\n                            (df_book_data['bid_size1'] + \n                             df_book_data['ask_size1'])\n    # 用WAP計算log return\n    df_book_data['log_return'] = df_book_data.groupby(['time_id'])['wap'].apply(log_return)\n    # 去除 log return 的NULL值\n    df_book_data = df_book_data[~df_book_data['log_return'].isnull()]\n    # 用公式計算realized volatility\n    df_realized_vol_per_stock =  pd.DataFrame(df_book_data.groupby(['time_id'])['log_return'].agg(realized_volatility)).reset_index()\n    df_realized_vol_per_stock = df_realized_vol_per_stock.rename(columns = {'log_return':realized_vol_colname})\n    stock_id = file_path.split('=')[1]\n    # 建立新欄位: stock_id-time_id 即每個樣本的index\n    df_realized_vol_per_stock['row_id'] = df_realized_vol_per_stock['time_id'].apply(lambda x:f'{stock_id}-{x}')\n    return df_realized_vol_per_stock[['row_id',realized_vol_colname]]\n\n\ndef generate_realized_vol(list_file,realized_vol_colname):\n    # Give list of order book files( ../stock_id/filename.parquet), generate realized vol.\n    # and return Dataframe for each stock_id\n    # list_file: list of filepath for multiple stock_id files\n    # realized_vol_colname: column name for calculated realized_vol. value\n    df_past_realized = pd.DataFrame()\n    for file in list_file:\n        df_past_realized = pd.concat([df_past_realized,\n                                     compute_realized_vol(file,realized_vol_colname)])\n    return df_past_realized\n\nimport glob\norder_book_list = glob.glob('../input/optiver-realized-volatility-prediction/book_train.parquet/*')\n\nif os.path.exists('../input/train-processed/train_processed.csv'):\n    df_train = pd.read_csv('../input/train-processed/train_processed.csv')\nelse:\n    df_train = generate_realized_vol(list_file=order_book_list, realized_vol_colname='realized_vol')\n\nprint('必要function已經設定完成')\n\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-08-02T04:52:36.517024Z","iopub.execute_input":"2021-08-02T04:52:36.517471Z","iopub.status.idle":"2021-08-02T04:52:37.436494Z","shell.execute_reply.started":"2021-08-02T04:52:36.517438Z","shell.execute_reply":"2021-08-02T04:52:37.435732Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import r2_score\ndef rmspe(y_true, y_pred):\n    return  (np.sqrt(np.mean(np.square((y_true - y_pred) / y_true))))\n\nprint('RMSPE 定義完畢。')","metadata":{"execution":{"iopub.status.busy":"2021-08-02T04:52:37.438343Z","iopub.execute_input":"2021-08-02T04:52:37.438993Z","iopub.status.idle":"2021-08-02T04:52:37.445567Z","shell.execute_reply.started":"2021-08-02T04:52:37.438886Z","shell.execute_reply":"2021-08-02T04:52:37.444545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pickle\nimport lightgbm as lgb\nfrom scipy.stats import randint as sp_randint\nfrom sklearn.metrics import make_scorer\n\nmu = 1e-9\n\ndef gradient(y_true, y_pred):\n    # gradient for loss function\n    # y_true: (n_samples)\n    \n    return -2*((y_true - y_pred)/(y_true + mu))\n\ndef hessian(y_true, y_pred):\n    \n    return 2/(y_true + mu)\n\ndef rmspe_loss(y_true, y_pred):\n    # grad: 1-order derivative for each sample point\n    # hess: 2-order derivative for each sample point\n    grad = gradient(y_true, y_pred)\n    hess = hessian(y_true, y_pred)\n    \n    return grad, hess\n\n# lgb parameters for grid search\nlgb_params_grid = {\n    \"num_leaves\":[128, 256, 512],\n    \"min_data_in_leaf\":np.arange(100, 300, 100),\n    'learning_rate':[1e-3],\n    'n_estimators':[2000, 3000, 5000],\n    'reg_alpha':[0.1, 0.01, 1],\n    'reg_lambda':[0.1, 0.01, 1],\n    'random_state':[99],\n    'silent':[True],\n    'objective':[rmspe_loss],\n    'n_jobs':[-1]\n}\n\n# custom scorer (RMSPE)\ncustom_score = make_scorer(rmspe, greater_is_better = False)\n\n\nlgbm = lgb.LGBMRegressor()\n\nprint('LGBM Model parameter 搜尋池建立完畢.')","metadata":{"execution":{"iopub.status.busy":"2021-08-02T04:52:37.450152Z","iopub.execute_input":"2021-08-02T04:52:37.450512Z","iopub.status.idle":"2021-08-02T04:52:37.460911Z","shell.execute_reply.started":"2021-08-02T04:52:37.450472Z","shell.execute_reply":"2021-08-02T04:52:37.460199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 每個stock_id在特定的時間區段time_id內都有一個目標realized vol. 值，如stock_id= 0在time_id = 5時的target即為0.004136\n\n#### time_id代表某個時間區段的代號ID, target代表此time_id**未來10分鐘後的feature data計算出來的值**, 也就是我們要預測的目標。\n\n#### 每個time_id時間區段內歷經長度為10分鐘, 即seconds_in_bucket的值會由0~600秒(但不一定每一秒都有紀錄)\n\n#### 我們要用**現在的time_id, stock_id對應的order book, trade資料**建立數學模型，預測未來10分鐘的realized volatility","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import TimeSeriesSplit, GridSearchCV, RandomizedSearchCV\n\ny = df_train['target']\nx = df_train[['realized_vol', 'mean_price', 'max_price', \\\n              'total_trader', 'mean_totalsold', 'max_totalsold']]\n\ntime_cv = TimeSeriesSplit(n_splits = 3)\nval_metric = []\n\n# hyperparameter tuning\ngrid_search = GridSearchCV(\n    estimator = lgbm,\n    param_grid = lgb_params_grid,\n    cv = time_cv,\n    scoring = custom_score,\n    verbose = 100\n)\n\n\ngrid_search.fit(x, y,\n               verbose = 1000)\n","metadata":{"execution":{"iopub.status.busy":"2021-08-02T04:52:37.462195Z","iopub.execute_input":"2021-08-02T04:52:37.462609Z","iopub.status.idle":"2021-08-02T04:52:57.574861Z","shell.execute_reply.started":"2021-08-02T04:52:37.462555Z","shell.execute_reply":"2021-08-02T04:52:57.572368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"grid_search.best_params_","metadata":{"execution":{"iopub.status.busy":"2021-08-02T04:52:57.575759Z","iopub.status.idle":"2021-08-02T04:52:57.576188Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pickle\n# save best model as output\nwith open('best_lgbm.pickle', 'wb') as file:\n    pickle.dump(grid_search.best_estimator_, file)","metadata":{"execution":{"iopub.status.busy":"2021-08-02T04:52:57.577073Z","iopub.status.idle":"2021-08-02T04:52:57.577482Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}