{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nimport time\n#for dirname, _, filenames in os.walk('/kaggle/input'):\n#    for filename in filenames:\n#        print(os.path.join(dirname, filename))\ndef read_parquet_from_folder(folder_name): \n    df = pd.DataFrame()\n    list_filename = []\n    \n    for filename in os.listdir(folder_name):   \n        list_filename.append(filename) \n        new_foldername = folder_name + '/' + filename  \n        for filename1 in os.listdir(new_foldername):  \n            with open(os.path.join(new_foldername, filename1), errors = 'ignore', mode='r') as f1: \n                 df1 = pd.read_parquet(f1.name) \n                 df1 = df1.dropna()\n                 df1['stock_id'] = list_filename[-1].strip('stock_id=')\n                 df = df.append(df1)     \n    return df, list_filename  \ntrade_df, trade_filename = read_parquet_from_folder('/kaggle/input/optiver-realized-volatility-prediction/trade_train.parquet') \n\n\ndf_train = pd.read_csv('/kaggle/input/optiver-realized-volatility-prediction/train.csv')\nu1 = df_train['stock_id'].unique()       \nv1 = df_train['time_id'].unique()\nlist_timeid = v1.tolist()\nlist_timeid.sort()\nstock_timeid = np.zeros([max(u1)+1, len(list_timeid)])     # output of stock_id vs time_id matrix \n\nfor i in range(len(df_train)):\n    z1 = df_train['stock_id'][i]\n    z2 = df_train['time_id'][i]\n    z3 = list_timeid.index(z2)\n    stock_timeid[z1][z3] = df_train['target'][i]\n    \n\ndef func(trade_df, trade_filename, list_timeid):\n    list_filename1 = []\n    StockId_timeId = []\n    list_stocks = []\n\n    folder_name1 = os.listdir('/kaggle/input/optiver-realized-volatility-prediction/book_train.parquet')   \n    for i in range(56):    \n        for filename1 in folder_name1[i*2:i*2+2]:\n            list_filename1.append(filename1)\n            new_foldername = '/kaggle/input/optiver-realized-volatility-prediction/book_train.parquet' + '/' + filename1  \n            for filename2 in os.listdir(new_foldername):  \n                with open(os.path.join(new_foldername, filename2), errors = 'ignore', mode='r') as f1: \n                     df = pd.read_parquet(f1.name) \n                     df = df.dropna()\n                     df['stock_id'] = list_filename1[-1].strip('stock_id=')  \n                 \n    \n            timeId_SecondBucket1 = np.zeros([len(stock_timeid[0])]) \n            if list_filename1[-1] in trade_filename: \n               c = trade_filename.index(list_filename1[-1])\n               ids1 = trade_filename[c].strip('stock_id=') \n         \n               list_stocks.append(int(ids1))\n               df1 = trade_df[trade_df['stock_id'] == ids1] \n               timeid_unique1 = df1['time_id'].unique() \n               timeid_unique1 = timeid_unique1.tolist()\n               secondbucket1_unique = df1['seconds_in_bucket'].unique() \n             \n               timeid_unique2 = df['time_id'].unique() \n               timeid_unique2 = timeid_unique2.tolist()\n               secondbucket2_unique = df['seconds_in_bucket'].unique() \n           \n\n               for q in range(len(timeid_unique1)):\n                   df11 = df1[df1['time_id'] == timeid_unique1[q]] \n                   df22 = df[df['time_id'] == timeid_unique1[q]] \n                   df11 = df11.reset_index(drop=True) \n                   df22 = df22.reset_index(drop=True) \n               \n                   secondsbucket1 = df11['seconds_in_bucket'].unique()  \n                   secondsbucket1 = secondsbucket1.tolist()\n     \n                   secondsbucket2 = df22['seconds_in_bucket'].unique()  \n                   secondsbucket2 = secondsbucket2.tolist()\n    \n            \n            \n                   c1 = []\n                   c2 = []\n                   c3 = [] \n                   timeid_seconds1 = np.zeros([len(secondsbucket1)])\n             \n                   for r in range(len(df22)): \n                       timeid = df22.iloc[r]['time_id']  \n                       seconds = df22.iloc[r]['seconds_in_bucket']  \n\n                       if seconds in secondsbucket1: \n                          a1 = secondsbucket1.index(seconds) \n                          b1 = df11.iloc[a1]['size'] \n                          d1 = df11.iloc[a1]['order_count']\n                          a, b, c, d, e, f, g, h = df22.iloc[r]['bid_price1'], df22.iloc[r]['ask_price1'], df22.iloc[r]['bid_price2'], df22.iloc[r]['ask_price2'], df22.iloc[r]['bid_size1'], df22.iloc[r]['ask_size1'], df22.iloc[r]['bid_size2'], df22.iloc[r]['ask_size2']     \n                          c1.append((a1,b1,d1))\n                          c2.append((a,b,c,d,e,f,g,h)) \n                          c3.append((timeid, seconds))\n               \n              \n                   if ((c2[0][4] != 0) & (c2[0][5] != 0)) == 1: \n                      WAP1 = ((c2[0][0]*c2[0][5])+(c2[0][1]*c2[0][4]))/(c2[0][4]+c2[0][5])            #((a*f)+(b*e))/(e+f) \n                   elif c2[0][4] == 0:\n                      x22 = np.array(c2).transpose()\n                      WAP1 = ((c2[0][0]*c2[0][5])+(c2[0][1]*c2[0][4]))/(np.mean(x22[4])+c2[0][5])\n                   elif c2[0][5] == 0:     \n                      x22 = np.array(c2).transpose()\n                      WAP1 = ((c2[0][0]*c2[0][5])+(c2[0][1]*c2[0][4]))/(c2[0][4]+np.mean(x22[5]))\n                   \n                   if ((c2[0][6] != 0) & (c2[0][7] != 0)) == 1: \n                      WAP2 = ((c2[0][2]*c2[0][7])+(c2[0][3]*c2[0][6]))/(c2[0][6]+c2[0][7])            #((c*h)+(d*g))/(g+h) \n                   elif c2[0][6] == 0:\n                      x22 = np.array(c2).transpose()\n                      WAP2 = ((c2[0][2]*c2[0][7])+(c2[0][3]*c2[0][6]))/(np.mean(x22[6])+c2[0][7])\n                   elif c2[0][5] == 0:     \n                      x22 = np.array(c2).transpose()\n                      WAP2 = ((c2[0][2]*c2[0][7])+(c2[0][3]*c2[0][6]))/(c2[0][6]+np.mean(x22[7]))   \n                    \n                   if WAP1 > WAP2:\n                      timeid_seconds1[c1[0][0]] = WAP1*(c1[0][2]/c1[0][1])\n                   else:\n                      timeid_seconds1[c1[0][0]] = WAP2*(c1[0][2]/c1[0][1])       \n                       \n                   for s in range(len(c1)-1):\n                      # bidaskSpread1 = c2[s+1][1]/c2[s+1][0]     #b/a \n                      # bidaskSpread2 = c2[s+1][3]/c2[s+1][2]     #d/c \n                       if c1[s][1] != 0:\n                          ratio_price = int(c1[s+1][1]/c1[s][1])     #b1 \n                       else:\n                          x11 = np.array(c1).transpose()\n                          ratio_price = int(c1[s+1][1]/np.mean(x11[1]))   \n                          \n                       if ((c2[s+1][0] != 0) & (c2[s+1][3] != 0)) == 1:   \n                          ratio_Spread = int(c2[s+1][1]*c2[s+1][2]/c2[s+1][0]*c2[s+1][3])\n                       elif c2[s+1][0] == 0:\n                          x22 = np.array(c2).transpose()\n                          ratio_Spread = int(c2[s+1][1]*c2[s+1][2]/np.mean(x22[0])*c2[s+1][3])  \n                       elif c2[s+1][3] == 0:\n                          x22 = np.array(c2).transpose()\n                          ration_Spread = int(c2[s+1][1]*c2[s+1][2]/c2[s+1][0]*np.mean(x22[3]))      \n                   \n                   \n                       if ((c2[s+1][4] != 0) & (c2[s+1][5] != 0)) == 1: \n                          WAP1 = ((c2[s+1][0]*c2[s+1][5])+(c2[s+1][1]*c2[s+1][4]))/(c2[s+1][4]+c2[s+1][5])            #((a*f)+(b*e))/(e+f) \n                       elif c2[s+1][4] == 0:\n                          x22 = np.array(c2).transpose()\n                          WAP1 = ((c2[s+1][0]*c2[s+1][5])+(c2[s+1][1]*c2[s+1][4]))/(np.mean(x22[4])+c2[s+1][5])\n                       elif c2[s+1][5] == 0:     \n                          x22 = np.array(c2).transpose()\n                          WAP1 = ((c2[s+1][0]*c2[s+1][5])+(c2[s+1][1]*c2[s+1][4]))/(c2[s+1][4]+np.mean(x22[5]))\n                   \n                   \n                       if ((c2[s+1][6] != 0) & (c2[s+1][7] != 0)) == 1: \n                          WAP2 = ((c2[s+1][2]*c2[s+1][7])+(c2[s+1][3]*c2[s+1][6]))/(c2[s+1][6]+c2[s+1][7])            #((c*h)+(d*g))/(g+h) \n                       elif c2[s+1][6] == 0:\n                          x22 = np.array(c2).transpose()\n                          WAP2 = ((c2[s+1][2]*c2[s+1][7])+(c2[s+1][3]*c2[s+1][6]))/(np.mean(x22[6])+c2[s+1][7])\n                       elif c2[s+1][5] == 0:     \n                          x22 = np.array(c2).transpose()\n                          WAP2 = ((c2[s+1][2]*c2[s+1][7])+(c2[s+1][3]*c2[s+1][6]))/(c2[s+1][6]+np.mean(x22[7]))   \n                      \n                  \n                       if ((ratio_price >= 1 & ratio_Spread >= 1) or (ratio_price <= 1 & ratio_Spread <= 1)) == 1: \n                          WAP = WAP1\n                       else:\n                          WAP = WAP2\n                       timeid_seconds1[c1[s+1][0]] = WAP*(c1[s+1][2]/c1[s+1][1]) \n          \n                   timeid_seconds11 = timeid_seconds1[timeid_seconds1 != 0]     \n          \n          \n                   timeId_SecondBucket11 = []\n                   for t in range(len(timeid_seconds11)-1):\n                       x = np.log(timeid_seconds11[t+1]/timeid_seconds11[t])  \n                       timeId_SecondBucket11.append(x**2)\n                                   \n                   Var_SecondBucket = (np.sum(timeId_SecondBucket11)/len(timeid_seconds11))**0.5\n                   timeId_SecondBucket1[list_timeid.index(timeid_unique1[q])] = Var_SecondBucket\n               print('hey shikha') \n               StockId_timeId.append(timeId_SecondBucket1)\n       \n    return StockId_timeId, list_stocks                      \n \nStockId_timeId, list_stocks = func(trade_df, trade_filename, list_timeid)\nStockId_timeId = np.array(StockId_timeId)\nprint('hello')   \n\ntrain_x = StockId_timeId        #stockid_timeid matrix \n     \nalpha = 0.0000056   # hyperparameters\nbeta = 0.0000068\nconst1 = 0.000006\n\np_value = []       # so we can check on diff p_value means how much test_data lags from train data\nfor i in range(3, 50):\n    p_value.append(i)\n\n\n\ndef ARIMA_model(StockId_timeId, list_stocks, alpha, p_value, const1, stock_timeid):\n    Alpha = []\n    Error = []\n    \n    while alpha <= 0.00003:\n        error1 = []\n        error2 = []\n        for i in p_value:\n            input_mat = np.zeros([len(StockId_timeId), len(StockId_timeId[0])-i])\n            for j in range(len(StockId_timeId)): \n                for k in range(i, len(StockId_timeId[j])): \n                    if StockId_timeId[j][k] != 0: \n                       x1 = 0\n                       for l in range(0, i): \n                           x1 += StockId_timeId[j][k-l]*alpha + const1\n                       input_mat[j][k-i] = x1\n                \n            output_mat = stock_timeid[list_stocks, i:len(StockId_timeId[0])]   \n            err_mat = output_mat - input_mat \n            error1.append(err_mat) \n            error2.append(np.sum(err_mat))  \n           \n        Error.append(error2)\n        Alpha.append(alpha)    \n        alpha += 0.000005\n       \n    pvalue_list = []\n    errsum_list = []\n    err_list = []\n    for i in range(len(Error)): \n        s = min(Error[i]) \n        t = Error[i].index(s) \n        errsum_list.append(error2[t])     \n        err_list.append(error1[t])    \n        pvalue_list.append(t+p_value[0]) \n    return pvalue_list, errsum_list, err_list, Alpha\n\n\n\npvalue_list, errsum_list, err_list, Alpha = ARIMA_model(StockId_timeId, list_stocks, alpha, p_value, const1, stock_timeid)\n\n\ninde = errsum_list.index(min(errsum_list)) # so we get minimum error index\nalpha1 = Alpha[inde]     # true alpha for minimum error\nconst2 = 0.00001\n\nminimum_err = err_list[inde]  # minimum error\n\n\ndf_pvalue = pd.DataFrame(pvalue_list)\nunique_pvalue = df_pvalue[0].unique()\np_index = []\nfor i in range(len(unique_pvalue)):\n    t = 0\n    for j in range(len(pvalue_list)):\n        if unique_pvalue[i] == pvalue_list[j]:\n           t += 1\n    p_index.append(t)       \n\ne = p_index.index(max(p_index))\nPvalue = unique_pvalue[e]        # so we get how much test data lag from train data for minimum error\n\n\n\ndef ARIMA_final(alpha1, beta, const2, StockId_timeId, list_stocks, minimum_err, Pvalue, stock_timeid):\n    Beta = []\n    err1 = []\n    err2 = []\n    \n    while beta <= 0.00002:\n        g1 = np.zeros([len(StockId_timeId), len(StockId_timeId[0])-Pvalue])\n        g2 = stock_timeid[list_stocks, Pvalue:len(StockId_timeId[0])] \n               \n        for i in range(len(StockId_timeId)):\n            for j in range(Pvalue, len(StockId_timeId[i])): \n                if StockId_timeId[i][j] != 0:\n                   x3 = 0\n                   for k in range(0, Pvalue):\n                       x3 += StockId_timeId[i][j-k]*alpha1 + const2\n                   g1[i][j-Pvalue] = x3 + beta*minimum_err[i][j-Pvalue]\n            \n        g3 = g2 - g1\n        err1.append(g3) \n        err2.append(np.sum(g3))\n                \n        Beta.append(beta)     \n        beta += 0.000003    \n        \n    Min_sumerr = min(err2)\n    t1 = err2.index(Min_sumerr)\n    true_beta = Beta[t1]\n    Min_err = err1[t1]\n    return Min_sumerr, true_beta, Min_err      \n \nMin_sumerr, true_beta, Min_err = ARIMA_final(alpha1, beta, const2, StockId_timeId, list_stocks, minimum_err, Pvalue, stock_timeid)\n  \ndf_test = pd.read_csv('/kaggle/input/optiver-realized-volatility-prediction/test.csv')\ntest_stockid = df_test['stock_id'].unique()\nprint(test_stockid)\ntest_timeid = df_test['time_id'].unique()\n  \ntrain_y1 = stock_timeid[0]       # we have to use only shopid = 0 for test bcz we only have one shopid=0 \nindex2 = list_stocks.index(test_stockid)  # for shopid=0\nresult1 = Min_err[index2]        # min error for shopid=0\nresult2 = np.zeros(len(train_y1)-len(result1))  # for put intial value for same dimension\nresult2 = result2.tolist()\n\nfor i in range(len(result1)):\n    result2.append(result1[i])\n    \n    \ntrain_y = train_y1 - result2     # train_y for shopid=0\ntrain_y = train_y.reshape(len(train_y), 1)     \n\ninv_trainX = np.linalg.pinv(train_x)  # so we project train data into shopid=0\nProj_stockid = inv_trainX*train_y\nProj_stockid = Proj_stockid.transpose()\n\nFinal_result = []\ntest_ID = []\nfor i in range(len(df_test)):\n    Final_result.append(np.abs(Proj_stockid[df_test['stock_id'][i]][df_test['time_id'][i]]))\n    test_ID.append(df_test['row_id']) \n\nsubmission = pd.DataFrame({'row_id':test_ID, 'target':Final_result}) # this is the final submission\nsubmission.to_csv('/kaggle/working/submission.csv', index=False)\n\n    \n  \n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-09-27T08:33:13.186564Z","iopub.execute_input":"2021-09-27T08:33:13.186903Z"},"trusted":true},"execution_count":null,"outputs":[]}]}