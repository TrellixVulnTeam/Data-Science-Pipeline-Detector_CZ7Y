{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# 0909d -1-1-1\n# epoch 长一点70->100\n# thrd 0.01 -> 0.005\n# 改为 pytorch lightning\n# pyg 172->201\n# 0917a 512 256 128\n# 0918a 改为128 128 128 +mlp\n# 发散","metadata":{"execution":{"iopub.status.busy":"2021-09-24T12:08:08.513719Z","iopub.execute_input":"2021-09-24T12:08:08.514142Z","iopub.status.idle":"2021-09-24T12:08:08.519963Z","shell.execute_reply.started":"2021-09-24T12:08:08.514055Z","shell.execute_reply":"2021-09-24T12:08:08.518868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nnn = ['time_id',\n     'log_return1_realized_volatility_0c1',\n     'log_return1_realized_volatility_1c1',     \n     'log_return1_realized_volatility_3c1',\n     'log_return1_realized_volatility_4c1',     \n     'log_return1_realized_volatility_6c1',\n     'total_volume_mean_0c1',\n     'total_volume_mean_1c1', \n     'total_volume_mean_3c1',\n     'total_volume_mean_4c1', \n     'total_volume_mean_6c1',\n     'trade_size_mean_0c1',\n     'trade_size_mean_1c1', \n     'trade_size_mean_3c1',\n     'trade_size_mean_4c1', \n     'trade_size_mean_6c1',\n     'trade_order_count_mean_0c1',\n     'trade_order_count_mean_1c1',\n     'trade_order_count_mean_3c1',\n     'trade_order_count_mean_4c1',\n     'trade_order_count_mean_6c1',      \n     'price_spread_mean_0c1',\n     'price_spread_mean_1c1',\n     'price_spread_mean_3c1',\n     'price_spread_mean_4c1',\n     'price_spread_mean_6c1',   \n     'bid_spread_mean_0c1',\n     'bid_spread_mean_1c1',\n     'bid_spread_mean_3c1',\n     'bid_spread_mean_4c1',\n     'bid_spread_mean_6c1',       \n     'ask_spread_mean_0c1',\n     'ask_spread_mean_1c1',\n     'ask_spread_mean_3c1',\n     'ask_spread_mean_4c1',\n     'ask_spread_mean_6c1',   \n     'volume_imbalance_mean_0c1',\n     'volume_imbalance_mean_1c1',\n     'volume_imbalance_mean_3c1',\n     'volume_imbalance_mean_4c1',\n     'volume_imbalance_mean_6c1',       \n     'bid_ask_spread_mean_0c1',\n     'bid_ask_spread_mean_1c1',\n     'bid_ask_spread_mean_3c1',\n     'bid_ask_spread_mean_4c1',\n     'bid_ask_spread_mean_6c1',\n     'size_tau2_0c1',\n     'size_tau2_1c1',\n     'size_tau2_3c1',\n     'size_tau2_4c1',\n     'size_tau2_6c1'] ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-09-24T12:08:08.527354Z","iopub.execute_input":"2021-09-24T12:08:08.527733Z","iopub.status.idle":"2021-09-24T12:08:08.638525Z","shell.execute_reply.started":"2021-09-24T12:08:08.527703Z","shell.execute_reply":"2021-09-24T12:08:08.637105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install ../input/pyg201whl/*.whl","metadata":{"execution":{"iopub.status.busy":"2021-09-24T12:08:08.640676Z","iopub.execute_input":"2021-09-24T12:08:08.641506Z","iopub.status.idle":"2021-09-24T12:08:39.684762Z","shell.execute_reply.started":"2021-09-24T12:08:08.641457Z","shell.execute_reply":"2021-09-24T12:08:39.683458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''机器学习区'''\nimport numpy as np\nimport numpy.matlib\nimport pandas as pd\nfrom sklearn.model_selection import GroupKFold,KFold\nfrom sklearn.preprocessing import MinMaxScaler, QuantileTransformer\nfrom sklearn.cluster import KMeans\n\n'''绘图区'''\nimport matplotlib.pyplot as plt\nimport matplotlib.style as style\nimport seaborn as sns\nfrom matplotlib import pyplot\nfrom matplotlib.ticker import ScalarFormatter\nsns.set_context(\"talk\")\n\n'''pytroch'''\nimport torch\nimport torch.nn.functional as F\nimport torch.nn as nn\nfrom torch.nn import ModuleList, BatchNorm1d\nimport torch.optim as optim\nfrom torch.utils.data import Dataset,DataLoader\nfrom torch import Tensor\nimport pytorch_lightning as pl\nfrom pytorch_lightning import Trainer, seed_everything\n\nfrom pytorch_lightning.callbacks import EarlyStopping\n'''pyg'''\nimport torch_geometric\nfrom torch_geometric.datasets import Reddit\nfrom torch_geometric.data import Data,NeighborSampler\nfrom torch_geometric.data import NeighborSampler as RawNeighborSampler\nif torch_geometric.__version__ != '1.7.2':\n    from torch_geometric.loader.neighbor_sampler import EdgeIndex\nelse:\n    from torch_geometric.data.sampler import EdgeIndex\nfrom torch_geometric.nn import GCNConv,GATConv, ChebConv,SAGEConv\n\n'''系统资源区'''\nfrom tqdm.notebook import tqdm\nfrom typing import Optional, List, NamedTuple\nimport multiprocessing\nfrom joblib import Parallel, delayed\nfrom glob import glob\nimport warnings\nwarnings.filterwarnings('ignore')\nimport pickle\nimport gc\nfrom torch_geometric.nn import TransformerConv\nheads = 4","metadata":{"execution":{"iopub.status.busy":"2021-09-24T12:08:39.689792Z","iopub.execute_input":"2021-09-24T12:08:39.690114Z","iopub.status.idle":"2021-09-24T12:08:44.736346Z","shell.execute_reply.started":"2021-09-24T12:08:39.69008Z","shell.execute_reply":"2021-09-24T12:08:44.735139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DEBUG = 0\nTHRD = 0.02","metadata":{"execution":{"iopub.status.busy":"2021-09-24T12:08:44.738725Z","iopub.execute_input":"2021-09-24T12:08:44.739265Z","iopub.status.idle":"2021-09-24T12:08:44.744957Z","shell.execute_reply.started":"2021-09-24T12:08:44.739221Z","shell.execute_reply":"2021-09-24T12:08:44.743113Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BASE = '../input/optiver-realized-volatility-prediction/'\nEBASE = '../input/optiver-clf/'\nEBASE1 = '../input/opt-0924/'\nBOOK_TST = BASE+'book_test.parquet/'\nBOOK_TRN = BASE+'book_train.parquet/'\nTRADE_TST = BASE+'trade_test.parquet/'\nTRADE_TRN = BASE+'trade_train.parquet/'\nTRN = BASE+'train.csv'\nTST = BASE+'test.csv'\n\nif DEBUG == 1:\n    TST = TRN\n    BOOK_TST = BOOK_TRN\n    \n    trn = pd.read_csv(TRN)\n    trn['row_id'] = trn['stock_id'].astype(str) + '-' + trn['time_id'].astype(str)\n    tst = trn.copy()\n    y_true = tst['target']\n    tst.drop('target',axis=1,inplace =True)\nelse:\n    trn = pd.read_csv(TRN)\n    trn['row_id'] = trn['stock_id'].astype(str) + '-' + trn['time_id'].astype(str)\n    tst = pd.read_csv(TST)\nprint('trn',trn.shape,'tst',tst.shape)","metadata":{"execution":{"iopub.status.busy":"2021-09-24T12:08:44.747118Z","iopub.execute_input":"2021-09-24T12:08:44.747928Z","iopub.status.idle":"2021-09-24T12:08:46.229208Z","shell.execute_reply.started":"2021-09-24T12:08:44.747826Z","shell.execute_reply":"2021-09-24T12:08:46.226885Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trn_time_id = trn.time_id.unique()\nall_stock_id = trn.stock_id.unique()","metadata":{"execution":{"iopub.status.busy":"2021-09-24T12:08:46.231033Z","iopub.execute_input":"2021-09-24T12:08:46.23146Z","iopub.status.idle":"2021-09-24T12:08:46.25199Z","shell.execute_reply.started":"2021-09-24T12:08:46.231414Z","shell.execute_reply":"2021-09-24T12:08:46.251005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_book_data_by_stock_id(stock_id,is_train=True):\n    if is_train == True:\n        df = pd.read_parquet(f'{BOOK_TRN}stock_id={stock_id}')\n    else:\n        df = pd.read_parquet(f'{BOOK_TST}stock_id={stock_id}')\n    return df\n\ndef book_feature_by_stock_id_for_trn(stock_id):\n    return book_feature_by_stock_id(stock_id)\n\ndef book_feature_by_stock_id_for_tst(stock_id):\n    return book_feature_by_stock_id(stock_id,is_train=False)\n\ndef calc_wap1(df):\n    return (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']) / (df['bid_size1'] + df['ask_size1'])\n\ndef calc_price(df):\n    diff = abs(df[['bid_price1','ask_price1','bid_price2','ask_price2']].diff())\n    min_diff = np.nanmin(diff.where(lambda x: x > 0))\n    n_ticks = (diff / min_diff).round()\n    scale = 0.01 / np.nanmean(diff / n_ticks)\n    return scale\n\ndef calc_prices(stock_id,is_train):\n    try:\n        book = load_book_data_by_stock_id(stock_id,is_train)\n    except:\n        return pd.DataFrame()\n    book['wap1'] = calc_wap1(book)\n    df = book.groupby('time_id').apply(calc_price).to_frame('price').reset_index()\n    df['stock_id'] = stock_id\n    df['first_wap'] = df['price'] * book.groupby('time_id')['wap1'].first().values\n    df['last_wap'] = df['price'] * book.groupby('time_id')['wap1'].last().values\n    df['mean_wap'] = df['price'] * book.groupby('time_id')['wap1'].mean().values\n    return df","metadata":{"execution":{"iopub.status.busy":"2021-09-24T12:08:46.253492Z","iopub.execute_input":"2021-09-24T12:08:46.253937Z","iopub.status.idle":"2021-09-24T12:08:46.268091Z","shell.execute_reply.started":"2021-09-24T12:08:46.253892Z","shell.execute_reply":"2021-09-24T12:08:46.266524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stock_ids = trn.stock_id.unique()","metadata":{"execution":{"iopub.status.busy":"2021-09-24T12:08:46.271647Z","iopub.execute_input":"2021-09-24T12:08:46.272778Z","iopub.status.idle":"2021-09-24T12:08:46.286362Z","shell.execute_reply.started":"2021-09-24T12:08:46.272514Z","shell.execute_reply":"2021-09-24T12:08:46.285133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SEED=42\n\ndef set_all_seed(SEED=42):\n    np.random.seed(SEED)\n    torch.manual_seed(SEED)\n    torch.cuda.manual_seed(SEED)\n    torch.cuda.manual_seed_all(SEED)\n\n    torch.backends.cudnn.deterministic=True\n    torch.backends.cudnn.benchmark = False\n    seed_everything(SEED)\nset_all_seed(SEED)","metadata":{"execution":{"iopub.status.busy":"2021-09-24T12:08:51.715543Z","iopub.execute_input":"2021-09-24T12:08:51.715966Z","iopub.status.idle":"2021-09-24T12:08:51.730207Z","shell.execute_reply.started":"2021-09-24T12:08:51.71593Z","shell.execute_reply":"2021-09-24T12:08:51.728985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_cluster_knn_values(out_train):\n    np.random.seed(SEED)\n    out_train = out_train.pivot(index='time_id', columns='stock_id', values='target')\n\n    # out_train[out_train.isna().any(axis=1)]\n    out_train = out_train.fillna(out_train.mean())\n    out_train.head()\n\n    # Code to add the just the read data after first execution\n\n    # Data separation based on knn ++\n    nfolds = 5 # number of folds\n    index = []\n    totDist = []\n    values = []\n\n    # Generates a matriz with the values of \n    mat = out_train.values\n    scaler = MinMaxScaler(feature_range=(-1, 1))\n    mat = scaler.fit_transform(mat)\n    nind = int(mat.shape[0]/nfolds) # number of individuals\n\n    # Adds index in the last column\n    mat = np.c_[mat,np.arange(mat.shape[0])]\n    lineNumber = np.random.choice(np.array(mat.shape[0]), size=nfolds, replace=False)\n    lineNumber = np.sort(lineNumber)[::-1]\n    for n in range(nfolds):\n        totDist.append(np.zeros(mat.shape[0]-nfolds))\n\n    # Saves index\n    for n in range(nfolds):    \n        values.append([lineNumber[n]])\n\n    s=[]\n    for n in range(nfolds):\n        s.append(mat[lineNumber[n],:])\n        mat = np.delete(mat, obj=lineNumber[n], axis=0)\n\n    for n in range(nind-1):    \n        luck = np.random.uniform(0,1,nfolds)\n\n        for cycle in range(nfolds):\n            # Saves the values of index           \n            s[cycle] = np.matlib.repmat(s[cycle], mat.shape[0], 1)\n            sumDist = np.sum( (mat[:,:-1] - s[cycle][:,:-1])**2 , axis=1)   \n            totDist[cycle] += sumDist        \n\n            # Probabilities\n            f = totDist[cycle]/np.sum(totDist[cycle]) # normalizing the totDist\n            j = 0\n            kn = 0\n            for val in f:\n                j += val        \n                if (j > luck[cycle]): # the column was selected\n                    break\n                kn +=1\n            lineNumber[cycle] = kn\n\n            # Delete line of the value added    \n            for n_iter in range(nfolds):\n                totDist[n_iter] = np.delete(totDist[n_iter],obj=lineNumber[cycle], axis=0)\n                j= 0\n\n            s[cycle] = mat[lineNumber[cycle],:]\n            values[cycle].append(int(mat[lineNumber[cycle],-1]))\n            mat = np.delete(mat, obj=lineNumber[cycle], axis=0)\n\n    for n_mod in range(nfolds):\n        values[n_mod] = out_train.index[values[n_mod]]    \n    return values \n    \nknn_values = get_cluster_knn_values(trn)","metadata":{"execution":{"iopub.status.busy":"2021-09-24T12:08:52.935044Z","iopub.execute_input":"2021-09-24T12:08:52.935415Z","iopub.status.idle":"2021-09-24T12:09:02.012095Z","shell.execute_reply.started":"2021-09-24T12:08:52.935369Z","shell.execute_reply":"2021-09-24T12:09:02.010647Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_cluster_labels(train_p):\n    train_p = train_p.pivot(index='time_id', columns='stock_id', values='target')\n    corr = train_p.corr()\n    ids = corr.index\n    kmeans = KMeans(n_clusters=7, random_state=0).fit(corr.values)\n    print(kmeans.labels_)\n    l = []\n    for n in range(7):\n        l.append ( [ (x-1) for x in ( (ids+1)*(kmeans.labels_ == n)) if x > 0] )\n    return l\n\ncluster_labels = get_cluster_labels(trn)","metadata":{"execution":{"iopub.status.busy":"2021-09-24T12:09:02.01415Z","iopub.execute_input":"2021-09-24T12:09:02.01455Z","iopub.status.idle":"2021-09-24T12:09:02.430176Z","shell.execute_reply.started":"2021-09-24T12:09:02.014507Z","shell.execute_reply":"2021-09-24T12:09:02.42681Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# node","metadata":{}},{"cell_type":"code","source":"from IPython.core.display import display, HTML\n\nimport glob\nimport os\nimport gc\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt \nimport seaborn as sns\nimport numpy.matlib\n\nimport plotly.express as px\nimport plotly.graph_objects as go\n\nfrom joblib import Parallel, delayed\n\nfrom sklearn import preprocessing, model_selection\nfrom sklearn.preprocessing import MinMaxScaler, QuantileTransformer\nfrom sklearn.metrics import r2_score\nfrom sklearn.cluster import KMeans\n\nfrom numpy.random import seed\nseed(42)\n\nimport tensorflow as tf\ntf.random.set_seed(42)\nfrom tensorflow import keras\nfrom keras import backend as K\nfrom keras.backend import sigmoid\nfrom keras.utils.generic_utils import get_custom_objects\nfrom keras.layers import Activation\n\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2021-09-24T12:09:02.4342Z","iopub.execute_input":"2021-09-24T12:09:02.434668Z","iopub.status.idle":"2021-09-24T12:09:09.921635Z","shell.execute_reply.started":"2021-09-24T12:09:02.434623Z","shell.execute_reply":"2021-09-24T12:09:09.920382Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def read_train_test():\n    # Function to read our base train and test set\n    \n    train = pd.read_csv('../input/optiver-realized-volatility-prediction/train.csv')\n    test = pd.read_csv('../input/optiver-realized-volatility-prediction/test.csv')\n\n    # Create a key to merge with book and trade data\n    train['row_id'] = train['stock_id'].astype(str) + '-' + train['time_id'].astype(str)\n    test['row_id'] = test['stock_id'].astype(str) + '-' + test['time_id'].astype(str)\n    print(f'Our training set has {train.shape[0]} rows')\n    \n    return train, test\n\n# Read train and test\ntrain, test = read_train_test()","metadata":{"execution":{"iopub.status.busy":"2021-09-24T12:09:09.924139Z","iopub.execute_input":"2021-09-24T12:09:09.924583Z","iopub.status.idle":"2021-09-24T12:09:11.298888Z","shell.execute_reply.started":"2021-09-24T12:09:09.924536Z","shell.execute_reply":"2021-09-24T12:09:11.297674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# data directory\ndata_dir = '../input/optiver-realized-volatility-prediction/'\n\ndef calc_wap1(df):\n    # Function to calculate first WAP\n    wap = (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']) / (df['bid_size1'] + df['ask_size1'])\n    return wap\n\ndef calc_wap2(df):\n    # Function to calculate second WAP\n    wap = (df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2']) / (df['bid_size2'] + df['ask_size2'])\n    return wap\n\ndef log_return(series):\n    # Function to calculate the log of the return\n    return np.log(series).diff()\n\ndef realized_volatility(series):\n    # Calculate the realized volatility\n    return np.sqrt(np.sum(series**2))\n\ndef count_unique(series):\n    # Function to count unique elements of a series\n    return len(np.unique(series))\n\ndef book_preprocessor(file_path):\n    # Function to preprocess book data (for each stock id)\n    \n    df = pd.read_parquet(file_path)\n    \n    # Calculate Wap\n    df['wap1'] = calc_wap1(df)\n    df['wap2'] = calc_wap2(df)\n    \n    # Calculate log returns\n    df['log_return1'] = df.groupby(['time_id'])['wap1'].apply(log_return)\n    df['log_return2'] = df.groupby(['time_id'])['wap2'].apply(log_return)\n    \n    # Calculate wap balance\n    df['wap_balance'] = abs(df['wap1'] - df['wap2'])\n    \n    # Calculate spread\n    df['price_spread'] = (df['ask_price1'] - df['bid_price1']) / ((df['ask_price1'] + df['bid_price1']) / 2)\n    df['price_spread2'] = (df['ask_price2'] - df['bid_price2']) / ((df['ask_price2'] + df['bid_price2']) / 2)\n    df['bid_spread'] = df['bid_price1'] - df['bid_price2']\n    df['ask_spread'] = df['ask_price1'] - df['ask_price2']\n    df[\"bid_ask_spread\"] = abs(df['bid_spread'] - df['ask_spread'])\n    df['total_volume'] = (df['ask_size1'] + df['ask_size2']) + (df['bid_size1'] + df['bid_size2'])\n    df['volume_imbalance'] = abs((df['ask_size1'] + df['ask_size2']) - (df['bid_size1'] + df['bid_size2']))\n    \n    # Dict for aggregations\n    create_feature_dict = {\n        'wap1': [np.sum, np.mean, np.std],\n        'wap2': [np.sum, np.mean, np.std],\n        'log_return1': [np.sum, realized_volatility, np.mean, np.std],\n        'log_return2': [np.sum, realized_volatility, np.mean, np.std],\n        'wap_balance': [np.sum, np.mean, np.std],\n        'price_spread':[np.sum, np.mean, np.std],\n        'price_spread2':[np.sum, np.mean, np.std],\n        'bid_spread':[np.sum, np.mean, np.std],\n        'ask_spread':[np.sum, np.mean, np.std],\n        'total_volume':[np.sum, np.mean, np.std],\n        'volume_imbalance':[np.sum, np.mean, np.std],\n        \"bid_ask_spread\":[np.sum, np.mean, np.std],\n    }\n    \n    def get_stats_window(seconds_in_bucket, add_suffix = False):\n        # Function to get group stats for different windows (seconds in bucket)\n        \n        # Group by the window\n        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(create_feature_dict).reset_index()\n        \n        # Rename columns joining suffix\n        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n        \n        # Add a suffix to differentiate windows\n        if add_suffix:\n            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n        return df_feature\n    \n    # Get the stats for different windows\n    df_feature = get_stats_window(seconds_in_bucket = 0, add_suffix = False)\n    df_feature_400 = get_stats_window(seconds_in_bucket = 400, add_suffix = True)\n    df_feature_300 = get_stats_window(seconds_in_bucket = 300, add_suffix = True)\n    df_feature_200 = get_stats_window(seconds_in_bucket = 200, add_suffix = True)\n    \n    # Merge all\n    df_feature = df_feature.merge(df_feature_400, how = 'left', left_on = 'time_id_', right_on = 'time_id__400')\n    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n    df_feature = df_feature.merge(df_feature_200, how = 'left', left_on = 'time_id_', right_on = 'time_id__200')\n\n    # Drop unnecesary time_ids\n    df_feature.drop(['time_id__400', 'time_id__300', 'time_id__200'], axis = 1, inplace = True)\n    \n    \n    # Create row_id so we can merge\n    stock_id = file_path.split('=')[1]\n    df_feature['row_id'] = df_feature['time_id_'].apply(lambda x: f'{stock_id}-{x}')\n    df_feature.drop(['time_id_'], axis = 1, inplace = True)\n    \n    return df_feature\n\n\ndef trade_preprocessor(file_path):\n    # Function to preprocess trade data (for each stock id)\n    \n    df = pd.read_parquet(file_path)\n    df['log_return'] = df.groupby('time_id')['price'].apply(log_return)\n    \n    # Dict for aggregations\n    create_feature_dict = {\n        'log_return':[realized_volatility],\n        'seconds_in_bucket':[count_unique],\n        'size':[np.sum, realized_volatility, np.mean, np.std, np.max, np.min],\n        'order_count':[np.mean,np.sum,np.max],\n    }\n    \n    def get_stats_window(seconds_in_bucket, add_suffix = False):\n        # Function to get group stats for different windows (seconds in bucket)\n        \n        # Group by the window\n        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(create_feature_dict).reset_index()\n        \n        # Rename columns joining suffix\n        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n        \n        # Add a suffix to differentiate windows\n        if add_suffix:\n            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n        return df_feature\n    \n    # Get the stats for different windows\n    df_feature = get_stats_window(seconds_in_bucket = 0, add_suffix = False)\n    df_feature_400 = get_stats_window(seconds_in_bucket = 400, add_suffix = True)\n    df_feature_300 = get_stats_window(seconds_in_bucket = 300, add_suffix = True)\n    df_feature_200 = get_stats_window(seconds_in_bucket = 200, add_suffix = True)\n    \n    def tendency(price, vol):    \n        df_diff = np.diff(price)\n        val = (df_diff/price[1:])*100\n        power = np.sum(val*vol[1:])\n        return(power)\n    \n    lis = []\n    for n_time_id in df['time_id'].unique():\n        df_id = df[df['time_id'] == n_time_id]        \n        tendencyV = tendency(df_id['price'].values, df_id['size'].values)      \n        f_max = np.sum(df_id['price'].values > np.mean(df_id['price'].values))\n        f_min = np.sum(df_id['price'].values < np.mean(df_id['price'].values))\n        df_max =  np.sum(np.diff(df_id['price'].values) > 0)\n        df_min =  np.sum(np.diff(df_id['price'].values) < 0)\n        abs_diff = np.median(np.abs( df_id['price'].values - np.mean(df_id['price'].values)))        \n        energy = np.mean(df_id['price'].values**2)\n        iqr_p = np.percentile(df_id['price'].values,75) - np.percentile(df_id['price'].values,25)\n        abs_diff_v = np.median(np.abs( df_id['size'].values - np.mean(df_id['size'].values)))        \n        energy_v = np.sum(df_id['size'].values**2)\n        iqr_p_v = np.percentile(df_id['size'].values,75) - np.percentile(df_id['size'].values,25)\n        \n        lis.append({'time_id':n_time_id,'tendency':tendencyV,'f_max':f_max,'f_min':f_min,'df_max':df_max,'df_min':df_min,\n                   'abs_diff':abs_diff,'energy':energy,'iqr_p':iqr_p,'abs_diff_v':abs_diff_v,'energy_v':energy_v,'iqr_p_v':iqr_p_v})\n    \n    df_lr = pd.DataFrame(lis)\n        \n   \n    df_feature = df_feature.merge(df_lr, how = 'left', left_on = 'time_id_', right_on = 'time_id')\n    \n    # Merge all\n    df_feature = df_feature.merge(df_feature_400, how = 'left', left_on = 'time_id_', right_on = 'time_id__400')\n    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n    df_feature = df_feature.merge(df_feature_200, how = 'left', left_on = 'time_id_', right_on = 'time_id__200')\n\n    # Drop unnecesary time_ids\n    df_feature.drop(['time_id__400', 'time_id__300', 'time_id__200','time_id'], axis = 1, inplace = True)\n    df_feature = df_feature.add_prefix('trade_')\n    stock_id = file_path.split('=')[1]\n    df_feature['row_id'] = df_feature['trade_time_id_'].apply(lambda x:f'{stock_id}-{x}')\n    df_feature.drop(['trade_time_id_'], axis = 1, inplace = True)\n    \n    return df_feature\n\n\ndef get_time_stock(df):\n    # Function to get group stats for the stock_id and time_id\n    \n    # Get realized volatility columns\n    vol_cols = ['log_return1_realized_volatility', 'log_return2_realized_volatility', 'log_return1_realized_volatility_400', 'log_return2_realized_volatility_400', \n                'log_return1_realized_volatility_300', 'log_return2_realized_volatility_300', 'log_return1_realized_volatility_200', 'log_return2_realized_volatility_200', \n                'trade_log_return_realized_volatility', 'trade_log_return_realized_volatility_400', 'trade_log_return_realized_volatility_300', 'trade_log_return_realized_volatility_200']\n\n    # Group by the stock id\n    df_stock_id = df.groupby(['stock_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n    \n    # Rename columns joining suffix\n    df_stock_id.columns = ['_'.join(col) for col in df_stock_id.columns]\n    df_stock_id = df_stock_id.add_suffix('_' + 'stock')\n\n    # Group by the stock id\n    df_time_id = df.groupby(['time_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n    \n    # Rename columns joining suffix\n    df_time_id.columns = ['_'.join(col) for col in df_time_id.columns]\n    df_time_id = df_time_id.add_suffix('_' + 'time')\n    \n    # Merge with original dataframe\n    df = df.merge(df_stock_id, how = 'left', left_on = ['stock_id'], right_on = ['stock_id__stock'])\n    df = df.merge(df_time_id, how = 'left', left_on = ['time_id'], right_on = ['time_id__time'])\n    df.drop(['stock_id__stock', 'time_id__time'], axis = 1, inplace = True)\n    \n    return df\n    \n    \ndef preprocessor(list_stock_ids, is_train = True):\n    # Funtion to make preprocessing function in parallel (for each stock id)\n    \n    # Parrallel for loop\n    def for_joblib(stock_id):\n        # Train\n        if is_train:\n            file_path_book = data_dir + \"book_train.parquet/stock_id=\" + str(stock_id)\n            file_path_trade = data_dir + \"trade_train.parquet/stock_id=\" + str(stock_id)\n        # Test\n        else:\n            file_path_book = data_dir + \"book_test.parquet/stock_id=\" + str(stock_id)\n            file_path_trade = data_dir + \"trade_test.parquet/stock_id=\" + str(stock_id)\n    \n        # Preprocess book and trade data and merge them\n        df_tmp = pd.merge(book_preprocessor(file_path_book), trade_preprocessor(file_path_trade), on = 'row_id', how = 'left')\n        \n        # Return the merge dataframe\n        return df_tmp\n    \n    # Use parallel api to call paralle for loop\n    df = Parallel(n_jobs = -1, verbose = 1)(delayed(for_joblib)(stock_id) for stock_id in list_stock_ids)\n    \n    # Concatenate all the dataframes that return from Parallel\n    df = pd.concat(df, ignore_index = True)\n    \n    return df\n\n\ndef rmspe(y_true, y_pred):\n    # Function to calculate the root mean squared percentage error\n    return np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\n\ndef feval_rmspe(y_pred, lgb_train):\n    # Function to early stop with root mean squared percentage error\n    y_true = lgb_train.get_label()\n    return 'RMSPE', rmspe(y_true, y_pred), False","metadata":{"execution":{"iopub.status.busy":"2021-09-24T12:09:11.30083Z","iopub.execute_input":"2021-09-24T12:09:11.301324Z","iopub.status.idle":"2021-09-24T12:09:11.357892Z","shell.execute_reply.started":"2021-09-24T12:09:11.301262Z","shell.execute_reply":"2021-09-24T12:09:11.356607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get unique stock ids \ntrain_stock_ids = train['stock_id'].unique()\n\n# Preprocess them using Parallel and our single stock id functions\n#train_ = preprocessor(train_stock_ids, is_train = True)\n#train = train.merge(train_, on = ['row_id'], how = 'left')\n\n# Get unique stock ids \ntest_stock_ids = test['stock_id'].unique()\n\n# Preprocess them using Parallel and our single stock id functions\ntest_ = preprocessor(test_stock_ids, is_train = False)\ntest = test.merge(test_, on = ['row_id'], how = 'left')\n\n# Get group stats of time_id and stock_id\n#train = get_time_stock(train)\ntest = get_time_stock(test)","metadata":{"execution":{"iopub.status.busy":"2021-09-24T12:09:11.359519Z","iopub.execute_input":"2021-09-24T12:09:11.360228Z","iopub.status.idle":"2021-09-24T12:09:13.184207Z","shell.execute_reply.started":"2021-09-24T12:09:11.360179Z","shell.execute_reply":"2021-09-24T12:09:13.182932Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# replace by order sum (tau)\n#train['size_tau'] = np.sqrt(1/train['trade_seconds_in_bucket_count_unique'])\ntest['size_tau'] = np.sqrt(1/test['trade_seconds_in_bucket_count_unique'])\n#train['size_tau_400'] = np.sqrt(1/train['trade_seconds_in_bucket_count_unique_400'])\ntest['size_tau_400'] = np.sqrt(1/test['trade_seconds_in_bucket_count_unique_400'])\n#train['size_tau_300'] = np.sqrt(1/train['trade_seconds_in_bucket_count_unique_300'])\ntest['size_tau_300'] = np.sqrt(1/test['trade_seconds_in_bucket_count_unique_300'])\n#train['size_tau_200'] = np.sqrt(1/train['trade_seconds_in_bucket_count_unique_200'])\ntest['size_tau_200'] = np.sqrt(1/test['trade_seconds_in_bucket_count_unique_200'])","metadata":{"execution":{"iopub.status.busy":"2021-09-24T12:09:13.18619Z","iopub.execute_input":"2021-09-24T12:09:13.186915Z","iopub.status.idle":"2021-09-24T12:09:13.200295Z","shell.execute_reply.started":"2021-09-24T12:09:13.186861Z","shell.execute_reply":"2021-09-24T12:09:13.199203Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# tau2 \n#train['size_tau2'] = np.sqrt(1/train['trade_order_count_sum'])\ntest['size_tau2'] = np.sqrt(1/test['trade_order_count_sum'])\n#train['size_tau2_400'] = np.sqrt(0.25/train['trade_order_count_sum'])\ntest['size_tau2_400'] = np.sqrt(0.25/test['trade_order_count_sum'])\n#train['size_tau2_300'] = np.sqrt(0.5/train['trade_order_count_sum'])\ntest['size_tau2_300'] = np.sqrt(0.5/test['trade_order_count_sum'])\n#train['size_tau2_200'] = np.sqrt(0.75/train['trade_order_count_sum'])\ntest['size_tau2_200'] = np.sqrt(0.75/test['trade_order_count_sum'])\n\n# delta tau\n#train['size_tau2_d'] = train['size_tau2_400'] - train['size_tau2']\ntest['size_tau2_d'] = test['size_tau2_400'] - test['size_tau2']","metadata":{"execution":{"iopub.status.busy":"2021-09-24T12:09:13.203772Z","iopub.execute_input":"2021-09-24T12:09:13.204443Z","iopub.status.idle":"2021-09-24T12:09:13.218713Z","shell.execute_reply.started":"2021-09-24T12:09:13.204397Z","shell.execute_reply":"2021-09-24T12:09:13.2174Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trn_df = pd.read_parquet('../input/opt-0924/train0923.parq')\ntst_df = test.copy()","metadata":{"execution":{"iopub.status.busy":"2021-09-24T12:09:13.220791Z","iopub.execute_input":"2021-09-24T12:09:13.221364Z","iopub.status.idle":"2021-09-24T12:09:21.191963Z","shell.execute_reply.started":"2021-09-24T12:09:13.221317Z","shell.execute_reply":"2021-09-24T12:09:21.190807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_col = trn_df.columns.difference(['time_id','target','row_id','stock_id']).tolist()\nnum_col = [col for col in num_col if '_stock' not in col ]\ncat_col = 'stock_id'\nfea_col = [cat_col]+num_col\nprint(len(fea_col),len(num_col),cat_col)\nprint(np.nanmean(trn_df[num_col],axis=0).round(5).tolist())","metadata":{"execution":{"iopub.status.busy":"2021-09-24T12:09:25.933182Z","iopub.execute_input":"2021-09-24T12:09:25.933622Z","iopub.status.idle":"2021-09-24T12:09:27.469428Z","shell.execute_reply.started":"2021-09-24T12:09:25.933589Z","shell.execute_reply":"2021-09-24T12:09:27.468151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''先qt'''\ntrn_df.replace([np.inf, -np.inf], np.nan,inplace=True)\ntst_df.replace([np.inf, -np.inf], np.nan,inplace=True)\nqt_train = []\n\nfor col in num_col:\n    qt = QuantileTransformer(random_state=21,n_quantiles=2000, output_distribution='normal')\n    trn_df[col] = qt.fit_transform(trn_df[[col]])\n    tst_df[col] = qt.transform(tst_df[[col]])    \n    qt_train.append(qt)\n","metadata":{"execution":{"iopub.status.busy":"2021-09-24T12:09:27.471519Z","iopub.execute_input":"2021-09-24T12:09:27.47225Z","iopub.status.idle":"2021-09-24T12:10:14.546375Z","shell.execute_reply.started":"2021-09-24T12:09:27.472201Z","shell.execute_reply":"2021-09-24T12:10:14.545283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(np.nanmean(trn_df[num_col],axis=0).round(5).tolist())","metadata":{"execution":{"iopub.status.busy":"2021-09-24T12:10:14.548402Z","iopub.execute_input":"2021-09-24T12:10:14.548917Z","iopub.status.idle":"2021-09-24T12:10:16.18929Z","shell.execute_reply.started":"2021-09-24T12:10:14.54887Z","shell.execute_reply":"2021-09-24T12:10:16.187996Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_agg_fea(trn_df,tst_df,cluster_labels):\n    mat = []\n    matTest = []\n    n = 0\n    for ind in cluster_labels:\n        print(ind)\n        newDf = trn_df.loc[trn_df['stock_id'].isin(ind) ]\n        newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n        newDf.loc[:,'stock_id'] = str(n)+'c1'\n        mat.append ( newDf )\n        newDf = tst_df.loc[tst_df['stock_id'].isin(ind) ]    \n        newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n        newDf.loc[:,'stock_id'] = str(n)+'c1'\n        matTest.append ( newDf )\n        n+=1\n\n    mat1 = pd.concat(mat).reset_index()\n    mat1.drop(columns=['target'],inplace=True)\n    mat2 = pd.concat(matTest).reset_index()\n\n    '''变换形状，改名'''\n    mat2 = pd.concat([mat2,mat1.loc[mat1.time_id==5]])#下面是left join\n    mat1 = mat1.pivot(index='time_id', columns='stock_id')\n    mat1.columns = [\"_\".join(x) for x in mat1.columns.ravel()]\n    mat1.reset_index(inplace=True)\n\n    mat2 = mat2.pivot(index='time_id', columns='stock_id')\n    mat2.columns = [\"_\".join(x) for x in mat2.columns.ravel()]\n    mat2.reset_index(inplace=True)    \n    \n    '''拼接'''\n    trn_df = pd.merge(trn_df,mat1[nnn].set_index('time_id'),how='left',left_on='time_id' ,right_index=True)\n    tst_df = pd.merge(tst_df,mat2[nnn].set_index('time_id'),how='left',left_on='time_id' ,right_index=True)\n    return trn_df,tst_df","metadata":{"execution":{"iopub.status.busy":"2021-09-24T12:10:16.191628Z","iopub.execute_input":"2021-09-24T12:10:16.192353Z","iopub.status.idle":"2021-09-24T12:10:16.207279Z","shell.execute_reply.started":"2021-09-24T12:10:16.192306Z","shell.execute_reply":"2021-09-24T12:10:16.20602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trn_df,tst_df = get_agg_fea(trn_df,tst_df,cluster_labels)#主要是agg和scaler部分","metadata":{"execution":{"iopub.status.busy":"2021-09-24T12:10:16.210461Z","iopub.execute_input":"2021-09-24T12:10:16.210948Z","iopub.status.idle":"2021-09-24T12:10:19.974166Z","shell.execute_reply.started":"2021-09-24T12:10:16.210891Z","shell.execute_reply":"2021-09-24T12:10:19.973035Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prices_df1 = pd.read_parquet(EBASE+'/df_prices1.pq') #明示数据的价格矩阵\nprices_df2 = pd.concat(Parallel(n_jobs=4, verbose=51)(delayed(calc_prices)(r,False) for r in all_stock_id))\nprices_df = pd.concat([prices_df1,prices_df2]).reset_index(drop=True)\nprint(f\"出现time_id:{prices_df.time_id.nunique()}\")\ntrn_df = trn_df.merge(prices_df[['time_id','stock_id','price']],how='left')\ntst_df = tst_df.merge(prices_df[['time_id','stock_id','price']],how='left')","metadata":{"execution":{"iopub.status.busy":"2021-09-24T12:10:19.975842Z","iopub.execute_input":"2021-09-24T12:10:19.976412Z","iopub.status.idle":"2021-09-24T12:10:24.595325Z","shell.execute_reply.started":"2021-09-24T12:10:19.976322Z","shell.execute_reply":"2021-09-24T12:10:24.593975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_col = trn_df.columns.difference(['time_id','target','row_id','stock_id','price']).tolist()\nnum_col = [col for col in num_col if '_stock' not in col ]\ncat_col = 'stock_id'\nfea_col = [cat_col]+num_col\nprint(len(fea_col),len(num_col),cat_col)\nprint(np.nanmean(trn_df[num_col],axis=0).round(5).tolist())","metadata":{"execution":{"iopub.status.busy":"2021-09-24T12:10:24.599275Z","iopub.execute_input":"2021-09-24T12:10:24.599756Z","iopub.status.idle":"2021-09-24T12:10:27.323926Z","shell.execute_reply.started":"2021-09-24T12:10:24.599702Z","shell.execute_reply":"2021-09-24T12:10:27.322621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trn_df[num_col] = trn_df[num_col].fillna(trn_df[num_col].mean())\ntst_df[num_col] = tst_df[num_col].fillna(trn_df[num_col].mean())\nscaler = MinMaxScaler(feature_range=(-1, 1))\ngg = 50\nfor i in range(len(num_col)//gg+1):\n    col = num_col[i*gg:i*gg+gg]\n    trn_df[col] = scaler.fit_transform(trn_df[col])\n    tst_df[col] = scaler.transform(tst_df[col])\nnode_df = pd.concat([trn_df,tst_df]).drop_duplicates().reset_index(drop=True)\nprint(np.nanmean(trn_df[num_col],axis=0).round(5).tolist())","metadata":{"execution":{"iopub.status.busy":"2021-09-24T12:10:27.325928Z","iopub.execute_input":"2021-09-24T12:10:27.326275Z","iopub.status.idle":"2021-09-24T12:11:49.44714Z","shell.execute_reply.started":"2021-09-24T12:10:27.326212Z","shell.execute_reply":"2021-09-24T12:11:49.445978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# edge","metadata":{}},{"cell_type":"code","source":"# prices_df1 = pd.read_parquet(EBASE+'/df_prices1.pq') #明示数据的价格矩阵\n# prices_df2 = pd.concat(Parallel(n_jobs=4, verbose=51)(delayed(calc_prices)(r,False) for r in stock_ids)) #在线上是test部分的价格矩阵\n# prices_df = pd.concat([prices_df1,prices_df2]).reset_index(drop=True)\n# print(f\"出现time_id:{prices_df.time_id.nunique()}\")","metadata":{"execution":{"iopub.status.busy":"2021-09-24T12:16:21.370439Z","iopub.execute_input":"2021-09-24T12:16:21.37096Z","iopub.status.idle":"2021-09-24T12:16:21.37738Z","shell.execute_reply.started":"2021-09-24T12:16:21.370904Z","shell.execute_reply":"2021-09-24T12:16:21.375983Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''每个time_id的起始价格和结束价格的query矩阵'''\nfirst_df = prices_df[['time_id', 'stock_id', 'first_wap']].pivot('time_id', 'stock_id', 'first_wap')\nlast_df = prices_df[['time_id', 'stock_id', 'last_wap']].pivot('time_id', 'stock_id', 'last_wap')\nfirst_df = first_df.fillna(first_df.mean())\nlast_df = last_df.fillna(first_df.mean())","metadata":{"execution":{"iopub.status.busy":"2021-09-24T12:16:21.379818Z","iopub.execute_input":"2021-09-24T12:16:21.380833Z","iopub.status.idle":"2021-09-24T12:16:21.739102Z","shell.execute_reply.started":"2021-09-24T12:16:21.380776Z","shell.execute_reply":"2021-09-24T12:16:21.737961Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_nearby_time_id(query_df,template_df):\n    def get_nearby_time_id_(time_id):\n        template_df_=template_df.drop(time_id,axis=1)\n        query = query_df[time_id].values.repeat(len(template_df_.columns)).reshape(-1,len(template_df_.columns))\n        diffs = np.square((template_df_- query)/query).sum(axis=0)\n        diffs = diffs.sort_values()[:2].reset_index().rename(columns = {'time_id':'tid',0:'loss'})\n        diffs['time_id'] = time_id\n        return diffs    \n    edge_df = pd.concat(Parallel(n_jobs=4, verbose=1)(delayed(get_nearby_time_id_)(time_id) for time_id in tqdm(query_df.columns)))\n    edge_df = edge_df[['time_id','tid','loss']]\n    edge_df .columns = ['a','b','w']\n    return edge_df","metadata":{"execution":{"iopub.status.busy":"2021-09-24T12:11:49.926116Z","iopub.execute_input":"2021-09-24T12:11:49.928644Z","iopub.status.idle":"2021-09-24T12:11:49.941933Z","shell.execute_reply.started":"2021-09-24T12:11:49.928581Z","shell.execute_reply":"2021-09-24T12:11:49.940536Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''每个time_id分别按照收尾两头各找5个，然后做无向图且去重，边的权重取出现过的最小值'''\nlast_edge_df = get_nearby_time_id(last_df.T,first_df.T)\nfirst_edge_df = get_nearby_time_id(first_df.T,last_df.T)\n# last_edge_df = pd.read_parquet(EBASE1+'last_edge_df.pq')\n# first_edge_df = pd.read_parquet(EBASE1+'first_edge_df.pq')\nedge_df = pd.concat([first_edge_df,last_edge_df])\n#edge_df = pd.concat([edge_df,edge_df.rename(columns={'a':'b','b':'a'})])\nedge_df = edge_df.groupby(['a','b'])['w'].min().to_frame('w').reset_index().rename(columns={'a':'b','b':'a'})","metadata":{"execution":{"iopub.status.busy":"2021-09-24T12:11:49.947371Z","iopub.execute_input":"2021-09-24T12:11:49.950823Z","iopub.status.idle":"2021-09-24T12:16:21.320238Z","shell.execute_reply.started":"2021-09-24T12:11:49.950766Z","shell.execute_reply":"2021-09-24T12:16:21.319056Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"edge_df.shape","metadata":{"execution":{"iopub.status.busy":"2021-09-24T12:16:21.322174Z","iopub.execute_input":"2021-09-24T12:16:21.322643Z","iopub.status.idle":"2021-09-24T12:16:21.332663Z","shell.execute_reply.started":"2021-09-24T12:16:21.322599Z","shell.execute_reply":"2021-09-24T12:16:21.331256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''阈值截断'''\nedge_df = edge_df.query('w < @THRD').reset_index(drop = True)\nedge_df.shape","metadata":{"execution":{"iopub.status.busy":"2021-09-24T12:16:21.334958Z","iopub.execute_input":"2021-09-24T12:16:21.335515Z","iopub.status.idle":"2021-09-24T12:16:21.355936Z","shell.execute_reply.started":"2021-09-24T12:16:21.335453Z","shell.execute_reply":"2021-09-24T12:16:21.35492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"edge_df.shape","metadata":{"execution":{"iopub.status.busy":"2021-09-24T12:16:21.357414Z","iopub.execute_input":"2021-09-24T12:16:21.35819Z","iopub.status.idle":"2021-09-24T12:16:21.366952Z","shell.execute_reply.started":"2021-09-24T12:16:21.35814Z","shell.execute_reply":"2021-09-24T12:16:21.365256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''拓展到所有stock'''\nedge_dfs = []\nfor stock_id in all_stock_id:\n    edge_df_ = edge_df.copy()\n    edge_df_['a'] = str(stock_id)+'-' +edge_df.a.astype(str)\n    edge_df_['b'] = str(stock_id)+'-' +edge_df.b.astype(str)\n    edge_dfs.append(edge_df_)\nedge_df = pd.concat(edge_dfs).reset_index(drop=True)\nedge_df","metadata":{"execution":{"iopub.status.busy":"2021-09-24T12:16:50.816841Z","iopub.execute_input":"2021-09-24T12:16:50.817375Z","iopub.status.idle":"2021-09-24T12:16:58.001047Z","shell.execute_reply.started":"2021-09-24T12:16:50.817341Z","shell.execute_reply":"2021-09-24T12:16:57.999993Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''孤立点'''\nisolate_node = list(set(node_df['row_id']) - set(edge_df['a'].unique()))\nisolate_edge_df = pd.DataFrame({'a':isolate_node,'b':isolate_node,'w':0})\nedge_df = pd.concat([edge_df,isolate_edge_df]).reset_index(drop=True)\n\n'''重新编码'''\nrow_id_index_dict = dict(zip(node_df.row_id,node_df.index))\n\nedge_df['a'] = edge_df['a'].map(row_id_index_dict)\nedge_df['b'] = edge_df['b'].map(row_id_index_dict)\n\n'''构造边使用了笛卡尔积，但是有些点不存在'''\nedge_df = edge_df[~edge_df.a.isna() & ~edge_df.b.isna()]\nedge_df.shape","metadata":{"execution":{"iopub.status.busy":"2021-09-24T12:17:04.855429Z","iopub.execute_input":"2021-09-24T12:17:04.855833Z","iopub.status.idle":"2021-09-24T12:17:07.915534Z","shell.execute_reply.started":"2021-09-24T12:17:04.855799Z","shell.execute_reply":"2021-09-24T12:17:07.914374Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MyFLod:\n    def __init__(self,time_id_list):\n        self.time_id_list=time_id_list\n        \n    def __iter__(self):\n        self.flod = 0\n        self.n_folds = len(self.time_id_list)\n        return self\n    \n    def __next__(self):\n        if self.flod >= self.n_folds:\n            raise StopIteration\n        time_ids = np.arange(self.n_folds).astype(int)    \n        time_ids = np.delete(time_ids,obj=self.flod, axis=0) \n        val_time_id = list(self.time_id_list[self.flod])\n        trn_time_id = \\\n        list(self.time_id_list[time_ids[0]])+\\\n        list(self.time_id_list[time_ids[1]])+\\\n        list(self.time_id_list[time_ids[2]])+\\\n        list(self.time_id_list[time_ids[3]])\n        self.flod += 1\n        return trn_time_id,val_time_id\n            \n\nknn_flod = MyFLod(knn_values)","metadata":{"execution":{"iopub.status.busy":"2021-09-24T12:17:11.580265Z","iopub.execute_input":"2021-09-24T12:17:11.580663Z","iopub.status.idle":"2021-09-24T12:17:11.590284Z","shell.execute_reply.started":"2021-09-24T12:17:11.580631Z","shell.execute_reply":"2021-09-24T12:17:11.588719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# GNN","metadata":{}},{"cell_type":"code","source":"class GS_CFG():\n    save_path = 'gs.pth' #模型路径\n    lr = 0.0005 #学习率\n    label_col = 'fraud'\n    batch_size = 512  # 每批的样本数量\n    epoch = 80  # 遍历多少遍样本集\n    verbose = 1\n    output_num = 1\n    fea_num = len(num_col)\n    stock_embedding_size = 24","metadata":{"execution":{"iopub.status.busy":"2021-09-24T12:18:32.66415Z","iopub.execute_input":"2021-09-24T12:18:32.664607Z","iopub.status.idle":"2021-09-24T12:18:32.672627Z","shell.execute_reply.started":"2021-09-24T12:18:32.664571Z","shell.execute_reply":"2021-09-24T12:18:32.671139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if len(tst) == 3:\n    GS_CFG.epoch=2","metadata":{"execution":{"iopub.status.busy":"2021-09-24T12:18:35.277183Z","iopub.execute_input":"2021-09-24T12:18:35.277583Z","iopub.status.idle":"2021-09-24T12:18:35.284571Z","shell.execute_reply.started":"2021-09-24T12:18:35.277549Z","shell.execute_reply":"2021-09-24T12:18:35.281475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def RMSPELoss(y_pred, y_true):\n    return torch.sqrt(torch.mean(((y_true - y_pred) / y_true) ** 2 ))\ndef rmspe(y_true, y_pred):\n    # Function to calculate the root mean squared percentage error\n    return np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))","metadata":{"execution":{"iopub.status.busy":"2021-09-24T12:18:49.138253Z","iopub.execute_input":"2021-09-24T12:18:49.1387Z","iopub.status.idle":"2021-09-24T12:18:49.14582Z","shell.execute_reply.started":"2021-09-24T12:18:49.138666Z","shell.execute_reply":"2021-09-24T12:18:49.144248Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Batch(NamedTuple):\n    x: Tensor\n    y: Tensor\n    adjs: List[EdgeIndex]\n\nclass OptGNNDM(pl.LightningDataModule):\n    def __init__(self, train_mask,val_mask,test_mask):\n        super().__init__()\n        self.x = torch.FloatTensor(node_df[['stock_id']+num_col].values)\n        self.y = torch.tensor(node_df['target']).reshape(-1,1)\n        self.edge_index = torch.LongTensor(edge_df.drop_duplicates(['a','b'])[['a','b']].values).t()\n        self.train_mask = train_mask\n        self.val_mask = val_mask\n        self.test_mask = test_mask\n        \n    def train_dataloader(self):\n        return NeighborSampler(self.edge_index, node_idx=self.train_mask,\n                               sizes=[-1,-1,-1], return_e_id=True,\n                               transform=self.convert_batch, batch_size=GS_CFG.batch_size,\n                               num_workers=8, shuffle=True)\n\n    def val_dataloader(self):\n        return NeighborSampler(self.edge_index, node_idx=self.val_mask,\n                               sizes=[-1,-1,-1], return_e_id=True,\n                               transform=self.convert_batch, batch_size=GS_CFG.batch_size,\n                               num_workers=4, shuffle=False)\n\n    def test_dataloader(self):\n        return NeighborSampler(self.edge_index, node_idx=self.test_mask,\n                               sizes=[-1,-1,-1], return_e_id=True,\n                               transform=self.convert_batch, batch_size=GS_CFG.batch_size,\n                               num_workers=4, shuffle=False)\n\n    def convert_batch(self, batch_size, n_id, adjs):\n        return Batch(\n            x=self.x[n_id],\n            y=self.y[n_id[:batch_size]],\n            adjs=adjs,\n        )","metadata":{"execution":{"iopub.status.busy":"2021-09-24T12:19:20.814702Z","iopub.execute_input":"2021-09-24T12:19:20.815227Z","iopub.status.idle":"2021-09-24T12:19:20.829177Z","shell.execute_reply.started":"2021-09-24T12:19:20.815189Z","shell.execute_reply":"2021-09-24T12:19:20.827657Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class GraphSAGE(pl.LightningModule):\n    def __init__(self, in_channels: int, out_channels: int, hidden_channels: int = 128,num_layers=3):\n        super().__init__()\n        #self.save_hyperparameters()\n        self.num_layers = num_layers\n        self.hidden_channels = hidden_channels\n        \n        hidden_channels = self.hidden_channels\n        self.convs = ModuleList()\n        self.convs.append(TransformerConv(in_channels, hidden_channels//heads,heads=heads))\n        self.convs.append(TransformerConv(hidden_channels, hidden_channels//heads,heads=heads))\n        self.convs.append(TransformerConv(hidden_channels, hidden_channels//heads,heads=heads))\n        self.embedding = nn.Embedding(127, GS_CFG.stock_embedding_size)\n        self.Lin1 = nn.Linear(hidden_channels,out_channels)\n                 \n    def forward(self, x: Tensor, adjs: List[EdgeIndex]) -> Tensor:\n        cat_data = x[:,0].long()\n        num_data = x[:,1:]\n        x = torch.cat([self.embedding(cat_data),num_data],axis=1)\n        for i, (edge_index, _, size) in enumerate(adjs):\n            x_target = x[:size[1]]  # Target nodes are always placed first.\n            x = self.convs[i]((x, x_target), edge_index)\n            x = F.relu(x)\n        x = self.Lin1(x)\n        return x\n\n    def training_step(self, batch: Batch, batch_idx: int):\n        x, y, adjs = batch\n        out = model(x, adjs)\n        loss = RMSPELoss(out, y)\n        self.log(\"train_loss\", loss, prog_bar=True, on_step=False, on_epoch=True, logger=True)\n        return loss\n    \n    def validation_step(self, batch: Batch, batch_idx: int):\n        x, y, adjs = batch\n        y_pred = model(x, adjs)\n        \n        return {'y_pred':y_pred,'y':y}\n        #loss = RMSPELoss(out, y)\n        #self.log(\"valid_loss\", loss, prog_bar=True, on_step=False, on_epoch=True, logger=True)\n\n    def validation_epoch_end(self, outputs):\n        y_pred = torch.cat([out['y_pred'] for out in outputs], dim=0)\n        y = torch.cat([out['y'] for out in outputs], dim=0)\n        loss = RMSPELoss(y_pred,y)\n        self.log(\"valid_loss\", loss, prog_bar=True, on_step=False, on_epoch=True, logger=True)\n        return {'dummy_item': 0} \n    \n    def test_step(self, batch: Batch, batch_idx: int):\n        x, y, adjs = batch\n        y_pred = model(x, adjs)\n        return {'y_pred': y_pred}\n    \n    def test_epoch_end(self, outputs):\n        y_pred = torch.cat([out['y_pred'] for out in outputs], dim=0)\n        y_pred = y_pred.detach().cpu().numpy().reshape(-1)\n        self.test_y_pred = y_pred  # Save prediction internally for easy access\n        # We need to return something \n        return {'dummy_item': 0} \n    \n    def configure_optimizers(self):\n        optimizer = torch.optim.Adam(self.parameters(), lr=GS_CFG.lr)\n        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience = 3,threshold=0.001,  factor = 0.5, verbose =True)\n        return {\n           'optimizer': optimizer,\n           'lr_scheduler': scheduler, # Changed scheduler to lr_scheduler\n           'monitor': 'valid_loss'\n       }","metadata":{"execution":{"iopub.status.busy":"2021-09-24T12:19:26.257729Z","iopub.execute_input":"2021-09-24T12:19:26.258219Z","iopub.status.idle":"2021-09-24T12:19:26.285024Z","shell.execute_reply.started":"2021-09-24T12:19:26.258183Z","shell.execute_reply":"2021-09-24T12:19:26.28344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gc\ndel trn_df\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-09-24T12:19:34.134207Z","iopub.execute_input":"2021-09-24T12:19:34.134609Z","iopub.status.idle":"2021-09-24T12:19:34.475816Z","shell.execute_reply.started":"2021-09-24T12:19:34.134575Z","shell.execute_reply":"2021-09-24T12:19:34.474161Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"edge_index = torch.LongTensor(edge_df.drop_duplicates(['a','b'])[['a','b']].values).t()\nx = torch.FloatTensor(node_df[['stock_id']+num_col].values)","metadata":{"execution":{"iopub.status.busy":"2021-09-24T12:19:48.347224Z","iopub.execute_input":"2021-09-24T12:19:48.347636Z","iopub.status.idle":"2021-09-24T12:19:50.30123Z","shell.execute_reply.started":"2021-09-24T12:19:48.347603Z","shell.execute_reply":"2021-09-24T12:19:50.299934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_list = pickle.load(open(EBASE1+\"0928b_model_list25\", \"rb\"))","metadata":{"execution":{"iopub.status.busy":"2021-09-24T12:20:42.745625Z","iopub.execute_input":"2021-09-24T12:20:42.746138Z","iopub.status.idle":"2021-09-24T12:20:43.106154Z","shell.execute_reply.started":"2021-09-24T12:20:42.746102Z","shell.execute_reply":"2021-09-24T12:20:43.105053Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tst_preds = 0","metadata":{"execution":{"iopub.status.busy":"2021-09-24T12:20:47.007593Z","iopub.execute_input":"2021-09-24T12:20:47.008057Z","iopub.status.idle":"2021-09-24T12:20:47.013274Z","shell.execute_reply.started":"2021-09-24T12:20:47.008021Z","shell.execute_reply":"2021-09-24T12:20:47.011653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_list_ = model_list[0:5]\nn_folds = 5\nscores = []\ntst_pred = 0\n\nprint(edge_index.shape)\nfor n_count,(trn_time_id,val_time_id) in enumerate(iter(knn_flod)):\n    flod = n_count+1\n    print('CV {}/{}'.format(flod, n_folds))\n    \n    train_mask = torch.tensor(node_df.time_id.isin(trn_time_id))\n    val_mask = torch.tensor(node_df.time_id.isin(val_time_id))\n    test_mask = ~(train_mask | val_mask)\n    opt_gnn_dm = OptGNNDM(train_mask,val_mask,test_mask)\n\n    model = GraphSAGE(GS_CFG.fea_num+GS_CFG.stock_embedding_size, \n                      GS_CFG.output_num)\n    model.load_state_dict(model_list_[n_count])\n    trainer = Trainer(gpus=1)\n    trainer.test(model, opt_gnn_dm.test_dataloader())\n    tst_pred += model.test_y_pred.reshape(1,-1)[0].clip(0,0.09)\n    scores.append(trainer.validate(model, opt_gnn_dm.val_dataloader())[0]['valid_loss'])\n    del trainer,model,opt_gnn_dm\n    gc.collect()\nprint(np.array(scores).round(5),'cv',np.mean(scores).round(5))\nprint(tst_pred/flod)\ntst_preds += tst_pred/flod","metadata":{"execution":{"iopub.status.busy":"2021-09-24T12:23:22.656195Z","iopub.execute_input":"2021-09-24T12:23:22.656617Z","iopub.status.idle":"2021-09-24T12:25:38.276385Z","shell.execute_reply.started":"2021-09-24T12:23:22.656584Z","shell.execute_reply":"2021-09-24T12:25:38.274893Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_list_ = model_list[5:10]\nn_folds = 5\nscores = []\ntst_pred = 0\n\nprint(edge_index.shape)\nfor n_count,(trn_time_id,val_time_id) in enumerate(iter(knn_flod)):\n    flod = n_count+1\n    print('CV {}/{}'.format(flod, n_folds))\n    \n    train_mask = torch.tensor(node_df.time_id.isin(trn_time_id))\n    val_mask = torch.tensor(node_df.time_id.isin(val_time_id))\n    test_mask = ~(train_mask | val_mask)\n    opt_gnn_dm = OptGNNDM(train_mask,val_mask,test_mask)\n\n    model = GraphSAGE(GS_CFG.fea_num+GS_CFG.stock_embedding_size, \n                      GS_CFG.output_num)\n    model.load_state_dict(model_list_[n_count])\n    trainer = Trainer(gpus=1)\n    trainer.test(model, opt_gnn_dm.test_dataloader())\n    tst_pred += model.test_y_pred.reshape(1,-1)[0].clip(0,0.09)\n    scores.append(trainer.validate(model, opt_gnn_dm.val_dataloader())[0]['valid_loss'])\n    del trainer,model,opt_gnn_dm\n    gc.collect()\nprint(np.array(scores).round(5),'cv',np.mean(scores).round(5))\nprint(tst_pred/flod)\ntst_preds += tst_pred/flod","metadata":{"execution":{"iopub.status.busy":"2021-09-24T12:27:07.664233Z","iopub.execute_input":"2021-09-24T12:27:07.664637Z","iopub.status.idle":"2021-09-24T12:28:36.143619Z","shell.execute_reply.started":"2021-09-24T12:27:07.664602Z","shell.execute_reply":"2021-09-24T12:28:36.142217Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_list_ = model_list[10:15]\nn_folds = 5\nscores = []\ntst_pred = 0\n\nprint(edge_index.shape)\nfor n_count,(trn_time_id,val_time_id) in enumerate(iter(knn_flod)):\n    flod = n_count+1\n    print('CV {}/{}'.format(flod, n_folds))\n    \n    train_mask = torch.tensor(node_df.time_id.isin(trn_time_id))\n    val_mask = torch.tensor(node_df.time_id.isin(val_time_id))\n    test_mask = ~(train_mask | val_mask)\n    opt_gnn_dm = OptGNNDM(train_mask,val_mask,test_mask)\n\n    model = GraphSAGE(GS_CFG.fea_num+GS_CFG.stock_embedding_size, \n                      GS_CFG.output_num)\n    model.load_state_dict(model_list_[n_count])\n    trainer = Trainer(gpus=1)\n    trainer.test(model, opt_gnn_dm.test_dataloader())\n    tst_pred += model.test_y_pred.reshape(1,-1)[0].clip(0,0.09)\n    scores.append(trainer.validate(model, opt_gnn_dm.val_dataloader())[0]['valid_loss'])\n    del trainer,model,opt_gnn_dm\n    gc.collect()\nprint(np.array(scores).round(5),'cv',np.mean(scores).round(5))\nprint(tst_pred/flod)\ntst_preds += tst_pred/flod","metadata":{"execution":{"iopub.status.busy":"2021-09-24T12:29:01.989192Z","iopub.execute_input":"2021-09-24T12:29:01.989626Z","iopub.status.idle":"2021-09-24T12:30:31.687654Z","shell.execute_reply.started":"2021-09-24T12:29:01.989591Z","shell.execute_reply":"2021-09-24T12:30:31.686106Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_list_ = model_list[15:20]\nn_folds = 5\nscores = []\ntst_pred = 0\n\nprint(edge_index.shape)\nfor n_count,(trn_time_id,val_time_id) in enumerate(iter(knn_flod)):\n    flod = n_count+1\n    print('CV {}/{}'.format(flod, n_folds))\n    \n    train_mask = torch.tensor(node_df.time_id.isin(trn_time_id))\n    val_mask = torch.tensor(node_df.time_id.isin(val_time_id))\n    test_mask = ~(train_mask | val_mask)\n    opt_gnn_dm = OptGNNDM(train_mask,val_mask,test_mask)\n\n    model = GraphSAGE(GS_CFG.fea_num+GS_CFG.stock_embedding_size, \n                      GS_CFG.output_num)\n    model.load_state_dict(model_list_[n_count])\n    trainer = Trainer(gpus=1)\n    trainer.test(model, opt_gnn_dm.test_dataloader())\n    tst_pred += model.test_y_pred.reshape(1,-1)[0].clip(0,0.09)\n    scores.append(trainer.validate(model, opt_gnn_dm.val_dataloader())[0]['valid_loss'])\n    del trainer,model,opt_gnn_dm\n    gc.collect()\nprint(np.array(scores).round(5),'cv',np.mean(scores).round(5))\nprint(tst_pred/flod)\ntst_preds += tst_pred/flod","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_list_ = model_list[20:25]\nn_folds = 5\nscores = []\ntst_pred = 0\n\nprint(edge_index.shape)\nfor n_count,(trn_time_id,val_time_id) in enumerate(iter(knn_flod)):\n    flod = n_count+1\n    print('CV {}/{}'.format(flod, n_folds))\n    \n    train_mask = torch.tensor(node_df.time_id.isin(trn_time_id))\n    val_mask = torch.tensor(node_df.time_id.isin(val_time_id))\n    test_mask = ~(train_mask | val_mask)\n    opt_gnn_dm = OptGNNDM(train_mask,val_mask,test_mask)\n\n    model = GraphSAGE(GS_CFG.fea_num+GS_CFG.stock_embedding_size, \n                      GS_CFG.output_num)\n    model.load_state_dict(model_list_[n_count])\n    trainer = Trainer(gpus=1)\n    trainer.test(model, opt_gnn_dm.test_dataloader())\n    tst_pred += model.test_y_pred.reshape(1,-1)[0].clip(0,0.09)\n    scores.append(trainer.validate(model, opt_gnn_dm.val_dataloader())[0]['valid_loss'])\n    del trainer,model,opt_gnn_dm\n    gc.collect()\nprint(np.array(scores).round(5),'cv',np.mean(scores).round(5))\nprint(tst_pred/flod)\ntst_preds += tst_pred/flod","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tst_preds = tst_preds/(len(model_list)//5)\ntst_preds","metadata":{"execution":{"iopub.status.busy":"2021-09-24T12:30:32.160237Z","iopub.execute_input":"2021-09-24T12:30:32.160729Z","iopub.status.idle":"2021-09-24T12:30:32.172506Z","shell.execute_reply.started":"2021-09-24T12:30:32.160679Z","shell.execute_reply":"2021-09-24T12:30:32.171058Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tst_preds1 = tst_preds","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# edge2","metadata":{}},{"cell_type":"code","source":"'''每个time_id的起始价格和结束价格的query矩阵'''\nfirst_df = prices_df[['time_id', 'stock_id', 'first_wap']].pivot('time_id', 'stock_id', 'first_wap')\nlast_df = prices_df[['time_id', 'stock_id', 'last_wap']].pivot('time_id', 'stock_id', 'last_wap')\nfirst_df = first_df.fillna(first_df.mean())\nlast_df = last_df.fillna(first_df.mean())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_nearby_time_id(query_df,template_df):\n    def get_nearby_time_id_(time_id):\n        template_df_=template_df.drop(time_id,axis=1)\n        query = query_df[time_id].values.repeat(len(template_df_.columns)).reshape(-1,len(template_df_.columns))\n        diffs = np.square((template_df_- query)/query).sum(axis=0)\n        diffs = diffs.sort_values()[:3].reset_index().rename(columns = {'time_id':'tid',0:'loss'})\n        diffs['time_id'] = time_id\n        return diffs    \n    edge_df = pd.concat(Parallel(n_jobs=4, verbose=1)(delayed(get_nearby_time_id_)(time_id) for time_id in tqdm(query_df.columns)))\n    edge_df = edge_df[['time_id','tid','loss']]\n    edge_df .columns = ['a','b','w']\n    return edge_df","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''每个time_id分别按照收尾两头各找5个，然后做无向图且去重，边的权重取出现过的最小值'''\nlast_edge_df = get_nearby_time_id(last_df.T,first_df.T)\nfirst_edge_df = get_nearby_time_id(first_df.T,last_df.T)\n# last_edge_df = pd.read_parquet(EBASE1+'last_edge_df.pq')\n# first_edge_df = pd.read_parquet(EBASE1+'first_edge_df.pq')\nedge_df = pd.concat([first_edge_df,last_edge_df])\n#edge_df = pd.concat([edge_df,edge_df.rename(columns={'a':'b','b':'a'})])\nedge_df = edge_df.groupby(['a','b'])['w'].min().to_frame('w').reset_index().rename(columns={'a':'b','b':'a'})","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"edge_df.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''阈值截断'''\nedge_df = edge_df.query('w < @THRD').reset_index(drop = True)\nedge_df.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''拓展到所有stock'''\nedge_dfs = []\nfor stock_id in all_stock_id:\n    edge_df_ = edge_df.copy()\n    edge_df_['a'] = str(stock_id)+'-' +edge_df.a.astype(str)\n    edge_df_['b'] = str(stock_id)+'-' +edge_df.b.astype(str)\n    edge_dfs.append(edge_df_)\nedge_df = pd.concat(edge_dfs).reset_index(drop=True)\nedge_df","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''孤立点'''\nisolate_node = list(set(node_df['row_id']) - set(edge_df['a'].unique()))\nisolate_edge_df = pd.DataFrame({'a':isolate_node,'b':isolate_node,'w':0})\nedge_df = pd.concat([edge_df,isolate_edge_df]).reset_index(drop=True)\n\n'''重新编码'''\nrow_id_index_dict = dict(zip(node_df.row_id,node_df.index))\n\nedge_df['a'] = edge_df['a'].map(row_id_index_dict)\nedge_df['b'] = edge_df['b'].map(row_id_index_dict)\n\n'''构造边使用了笛卡尔积，但是有些点不存在'''\nedge_df = edge_df[~edge_df.a.isna() & ~edge_df.b.isna()]\nedge_df.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MyFLod:\n    def __init__(self,time_id_list):\n        self.time_id_list=time_id_list\n        \n    def __iter__(self):\n        self.flod = 0\n        self.n_folds = len(self.time_id_list)\n        return self\n    \n    def __next__(self):\n        if self.flod >= self.n_folds:\n            raise StopIteration\n        time_ids = np.arange(self.n_folds).astype(int)    \n        time_ids = np.delete(time_ids,obj=self.flod, axis=0) \n        val_time_id = list(self.time_id_list[self.flod])\n        trn_time_id = \\\n        list(self.time_id_list[time_ids[0]])+\\\n        list(self.time_id_list[time_ids[1]])+\\\n        list(self.time_id_list[time_ids[2]])+\\\n        list(self.time_id_list[time_ids[3]])\n        self.flod += 1\n        return trn_time_id,val_time_id\n            \n\nknn_flod = MyFLod(knn_values)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# GNN2","metadata":{}},{"cell_type":"code","source":"class GS_CFG():\n    save_path = 'gs.pth' #模型路径\n    lr = 0.0005 #学习率\n    label_col = 'fraud'\n    batch_size = 512  # 每批的样本数量\n    epoch = 80  # 遍历多少遍样本集\n    verbose = 1\n    output_num = 1\n    fea_num = len(num_col)\n    stock_embedding_size = 24","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_list = pickle.load(open(EBASE1+\"0928_model_list25\", \"rb\"))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tst_preds = 0","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_list_ = model_list[0:5]\nn_folds = 5\nscores = []\ntst_pred = 0\n\nprint(edge_index.shape)\nfor n_count,(trn_time_id,val_time_id) in enumerate(iter(knn_flod)):\n    flod = n_count+1\n    print('CV {}/{}'.format(flod, n_folds))\n    \n    train_mask = torch.tensor(node_df.time_id.isin(trn_time_id))\n    val_mask = torch.tensor(node_df.time_id.isin(val_time_id))\n    test_mask = ~(train_mask | val_mask)\n    opt_gnn_dm = OptGNNDM(train_mask,val_mask,test_mask)\n\n    model = GraphSAGE(GS_CFG.fea_num+GS_CFG.stock_embedding_size, \n                      GS_CFG.output_num)\n    model.load_state_dict(model_list_[n_count])\n    trainer = Trainer(gpus=1)\n    trainer.test(model, opt_gnn_dm.test_dataloader())\n    tst_pred += model.test_y_pred.reshape(1,-1)[0].clip(0,0.09)\n    scores.append(trainer.validate(model, opt_gnn_dm.val_dataloader())[0]['valid_loss'])\n    del trainer,model,opt_gnn_dm\n    gc.collect()\nprint(np.array(scores).round(5),'cv',np.mean(scores).round(5))\nprint(tst_pred/flod)\ntst_preds += tst_pred/flod","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_list_ = model_list[5:10]\nn_folds = 5\nscores = []\ntst_pred = 0\n\nprint(edge_index.shape)\nfor n_count,(trn_time_id,val_time_id) in enumerate(iter(knn_flod)):\n    flod = n_count+1\n    print('CV {}/{}'.format(flod, n_folds))\n    \n    train_mask = torch.tensor(node_df.time_id.isin(trn_time_id))\n    val_mask = torch.tensor(node_df.time_id.isin(val_time_id))\n    test_mask = ~(train_mask | val_mask)\n    opt_gnn_dm = OptGNNDM(train_mask,val_mask,test_mask)\n\n    model = GraphSAGE(GS_CFG.fea_num+GS_CFG.stock_embedding_size, \n                      GS_CFG.output_num)\n    model.load_state_dict(model_list_[n_count])\n    trainer = Trainer(gpus=1)\n    trainer.test(model, opt_gnn_dm.test_dataloader())\n    tst_pred += model.test_y_pred.reshape(1,-1)[0].clip(0,0.09)\n    scores.append(trainer.validate(model, opt_gnn_dm.val_dataloader())[0]['valid_loss'])\n    del trainer,model,opt_gnn_dm\n    gc.collect()\nprint(np.array(scores).round(5),'cv',np.mean(scores).round(5))\nprint(tst_pred/flod)\ntst_preds += tst_pred/flod","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_list_ = model_list[10:15]\nn_folds = 5\nscores = []\ntst_pred = 0\n\nprint(edge_index.shape)\nfor n_count,(trn_time_id,val_time_id) in enumerate(iter(knn_flod)):\n    flod = n_count+1\n    print('CV {}/{}'.format(flod, n_folds))\n    \n    train_mask = torch.tensor(node_df.time_id.isin(trn_time_id))\n    val_mask = torch.tensor(node_df.time_id.isin(val_time_id))\n    test_mask = ~(train_mask | val_mask)\n    opt_gnn_dm = OptGNNDM(train_mask,val_mask,test_mask)\n\n    model = GraphSAGE(GS_CFG.fea_num+GS_CFG.stock_embedding_size, \n                      GS_CFG.output_num)\n    model.load_state_dict(model_list_[n_count])\n    trainer = Trainer(gpus=1)\n    trainer.test(model, opt_gnn_dm.test_dataloader())\n    tst_pred += model.test_y_pred.reshape(1,-1)[0].clip(0,0.09)\n    scores.append(trainer.validate(model, opt_gnn_dm.val_dataloader())[0]['valid_loss'])\n    del trainer,model,opt_gnn_dm\n    gc.collect()\nprint(np.array(scores).round(5),'cv',np.mean(scores).round(5))\nprint(tst_pred/flod)\ntst_preds += tst_pred/flod","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_list_ = model_list[15:20]\nn_folds = 5\nscores = []\ntst_pred = 0\n\nprint(edge_index.shape)\nfor n_count,(trn_time_id,val_time_id) in enumerate(iter(knn_flod)):\n    flod = n_count+1\n    print('CV {}/{}'.format(flod, n_folds))\n    \n    train_mask = torch.tensor(node_df.time_id.isin(trn_time_id))\n    val_mask = torch.tensor(node_df.time_id.isin(val_time_id))\n    test_mask = ~(train_mask | val_mask)\n    opt_gnn_dm = OptGNNDM(train_mask,val_mask,test_mask)\n\n    model = GraphSAGE(GS_CFG.fea_num+GS_CFG.stock_embedding_size, \n                      GS_CFG.output_num)\n    model.load_state_dict(model_list_[n_count])\n    trainer = Trainer(gpus=1)\n    trainer.test(model, opt_gnn_dm.test_dataloader())\n    tst_pred += model.test_y_pred.reshape(1,-1)[0].clip(0,0.09)\n    scores.append(trainer.validate(model, opt_gnn_dm.val_dataloader())[0]['valid_loss'])\n    del trainer,model,opt_gnn_dm\n    gc.collect()\nprint(np.array(scores).round(5),'cv',np.mean(scores).round(5))\nprint(tst_pred/flod)\ntst_preds += tst_pred/flod","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_list_ = model_list[20:25]\nn_folds = 5\nscores = []\ntst_pred = 0\n\nprint(edge_index.shape)\nfor n_count,(trn_time_id,val_time_id) in enumerate(iter(knn_flod)):\n    flod = n_count+1\n    print('CV {}/{}'.format(flod, n_folds))\n    \n    train_mask = torch.tensor(node_df.time_id.isin(trn_time_id))\n    val_mask = torch.tensor(node_df.time_id.isin(val_time_id))\n    test_mask = ~(train_mask | val_mask)\n    opt_gnn_dm = OptGNNDM(train_mask,val_mask,test_mask)\n\n    model = GraphSAGE(GS_CFG.fea_num+GS_CFG.stock_embedding_size, \n                      GS_CFG.output_num)\n    model.load_state_dict(model_list_[n_count])\n    trainer = Trainer(gpus=1)\n    trainer.test(model, opt_gnn_dm.test_dataloader())\n    tst_pred += model.test_y_pred.reshape(1,-1)[0].clip(0,0.09)\n    scores.append(trainer.validate(model, opt_gnn_dm.val_dataloader())[0]['valid_loss'])\n    del trainer,model,opt_gnn_dm\n    gc.collect()\nprint(np.array(scores).round(5),'cv',np.mean(scores).round(5))\nprint(tst_pred/flod)\ntst_preds += tst_pred/flod","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tst_preds = tst_preds/(len(model_list)//5)\ntst_preds","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tst['target'] = tst_preds1*0.4+tst_preds*0.6\ntst[['row_id','target']].to_csv('submission.csv', index=False)\ntst.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-24T12:30:32.174728Z","iopub.execute_input":"2021-09-24T12:30:32.175269Z","iopub.status.idle":"2021-09-24T12:30:32.200504Z","shell.execute_reply.started":"2021-09-24T12:30:32.175207Z","shell.execute_reply":"2021-09-24T12:30:32.199534Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sys as sys\nobject_v_li = []\nobject_size_li = []\nfor object_v in dir():\n    \n    getsizeof_str = 'sys.getsizeof('+ object_v +')'\n    try:\n        size = eval(getsizeof_str)\n    except NameError as e:\n        print('except:', e)\n        continue\n        \n    #print(object_v)\n    object_v_li.append(object_v)\n    object_size_li.append(size)\n    \ndddd = pd.DataFrame({'object_name':object_v_li,'size':pd.Series(object_size_li)/1024/1024})\ndddd.sort_values('size',ascending = False).head(10)","metadata":{"execution":{"iopub.status.busy":"2021-09-24T12:31:07.223266Z","iopub.execute_input":"2021-09-24T12:31:07.223733Z","iopub.status.idle":"2021-09-24T12:31:08.332094Z","shell.execute_reply.started":"2021-09-24T12:31:07.223669Z","shell.execute_reply":"2021-09-24T12:31:08.330535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}