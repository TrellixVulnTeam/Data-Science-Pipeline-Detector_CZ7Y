{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Optimal Realized Volatility Prediction\n\nThis notebook contains some techniques of modeling process. If you are interested in EDA of this data, please check [here](https://www.kaggle.com/hyewon328/understand-and-visualize-volatility-data)!\n\n## Process\n1. [Preprocessing](#pre)\n2. [Feature Selection](#fs)\n3. [Bayesian Optimization](#opt)\n4. [Modeling](#model)\n5. [Prediction & Submission](#pred)","metadata":{}},{"cell_type":"markdown","source":"# Load package & data","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport os\nimport glob\nfrom joblib import Parallel, delayed\n\nimport eli5\nfrom eli5.sklearn import PermutationImportance\nfrom bayes_opt import BayesianOptimization\n\nfrom tqdm import tqdm\nfrom sklearn.metrics import make_scorer\nfrom sklearn.model_selection import train_test_split, GridSearchCV, KFold\nimport lightgbm as lgb\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\n\n\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-23T10:58:14.488798Z","iopub.execute_input":"2021-07-23T10:58:14.489182Z","iopub.status.idle":"2021-07-23T10:58:14.496011Z","shell.execute_reply.started":"2021-07-23T10:58:14.48915Z","shell.execute_reply":"2021-07-23T10:58:14.495056Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv(\"../input/optiver-realized-volatility-prediction/train.csv\")\ntest = pd.read_csv(\"../input/optiver-realized-volatility-prediction/test.csv\")\nsub = pd.read_csv(\"../input/optiver-realized-volatility-prediction/sample_submission.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-07-23T10:58:16.33562Z","iopub.execute_input":"2021-07-23T10:58:16.335937Z","iopub.status.idle":"2021-07-23T10:58:16.485912Z","shell.execute_reply.started":"2021-07-23T10:58:16.335907Z","shell.execute_reply":"2021-07-23T10:58:16.485092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"book_train_filepath = \"/kaggle/input/optiver-realized-volatility-prediction/book_train.parquet\"\ntrade_train_filepath = \"/kaggle/input/optiver-realized-volatility-prediction/trade_train.parquet\"\nbook_test_filepath = \"/kaggle/input/optiver-realized-volatility-prediction/book_test.parquet\"\ntrade_test_filepath = \"/kaggle/input/optiver-realized-volatility-prediction/trade_test.parquet\"\n\n# get filenames in book and trade files\nbook_train_filenames = os.listdir(book_train_filepath)\ntrade_train_filenames = os.listdir(trade_train_filepath)\nbook_test_filenames = os.listdir(book_test_filepath)\ntrade_test_filenames = os.listdir(trade_test_filepath)","metadata":{"execution":{"iopub.status.busy":"2021-07-23T10:58:17.849766Z","iopub.execute_input":"2021-07-23T10:58:17.850078Z","iopub.status.idle":"2021-07-23T10:58:17.86137Z","shell.execute_reply.started":"2021-07-23T10:58:17.850048Z","shell.execute_reply":"2021-07-23T10:58:17.860426Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Preprocessing <a class=\"anchor\" id=\"pre\"></a>\n\nIn **preprocessing part**, we generate some additional variables for LGBM Modeling.\n\nReference:\n<https://www.kaggle.com/manels/lgb-starter>","metadata":{}},{"cell_type":"code","source":"def log_return(list_stock_prices):\n    return np.log(list_stock_prices).diff() \n\ndef realized_volatility(series_log_return):\n    return np.sqrt(np.sum(series_log_return**2))\n\ndef RMSPE(true, pred):\n    rmspe = np.sqrt(np.mean(np.square((true-pred)/true)))\n    return rmspe\n\ndef feval_RMSPE(preds, train_data):\n    labels = train_data.get_label()\n    return 'RMSPE', round(RMSPE(true = labels, pred = preds),5), False","metadata":{"execution":{"iopub.status.busy":"2021-07-23T10:58:20.148625Z","iopub.execute_input":"2021-07-23T10:58:20.148934Z","iopub.status.idle":"2021-07-23T10:58:20.156137Z","shell.execute_reply.started":"2021-07-23T10:58:20.148905Z","shell.execute_reply":"2021-07-23T10:58:20.153878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def pre_data(stock_id, train = True):\n    \n    if train == True:\n        book_filepath = book_train_filepath\n        trade_filepath = trade_train_filepath\n    else:\n        book_filepath = book_test_filepath\n        trade_filepath = trade_test_filepath\n    \n    path = os.path.join(book_filepath, f\"stock_id={stock_id}\".format(stock_id))\n    data = pd.read_parquet(path)\n    \n    data['stock_id'] = stock_id\n    \n    data['wap1'] = (data['bid_price1'] * data['ask_size1'] + data['ask_price1'] * data['bid_size1']) / (data['bid_size1']+ data['ask_size1'])\n    data['wap2'] = (data['bid_price2'] * data['ask_size2'] + data['ask_price2'] * data['bid_size2']) / (data['bid_size2']+ data['ask_size2'])\n    \n    data['wap_balance'] = data['wap1'] - data['wap2']\n    \n    data['log_return1'] = data.groupby(['time_id'])['wap1'].apply(log_return)\n    data['log_return2'] = data.groupby(['time_id'])['wap2'].apply(log_return)\n    \n    data['spread1'] = (data['ask_price1'] - data['bid_price1'])/data['bid_price1']\n    data['spread2'] = data['ask_price2'] - data['bid_price2']/data['bid_price2']\n    \n    data['net_size1'] = data['bid_size1'] - data['ask_size1']\n    data['net_size2'] = data['bid_size2'] - data['ask_size2']\n    \n    \n    data = data.groupby(['stock_id', 'time_id']).agg(wap_balance_mean = ('wap_balance', 'mean'),\n                                                     volatility1 = ('log_return1', realized_volatility),\n                                                     volatility2 = ('log_return2', realized_volatility),\n                                                     spread1_mean = ('spread1', 'mean'),\n                                                     spread2_mean = ('spread2', 'mean'),\n                                                     net_size1_mean = ('net_size1', 'mean'),\n                                                     net_size2_mean = ('net_size2', 'mean'))\n    \n    \n    \n    # trade \n\n    trade_path = os.path.join(trade_filepath, f\"stock_id={stock_id}\".format(stock_id))\n    trade_data = pd.read_parquet(trade_path)\n    trade_data['stock_id'] = stock_id\n    \n    trade_data = trade_data.groupby(['stock_id', 'time_id']).agg(trade_price_mean = ('price', 'mean'),\n                                                                 trade_size_mean = ('size', 'mean'),\n                                                                 trade_order_mean = ('order_count', 'mean')).reset_index()\n    \n    \n\n    final = data.merge(trade_data, how = 'left', on = ['stock_id', 'time_id'])\n    #final = final.merge(volatility, how = 'left', on = ['stock_id', 'time_id'])\n    \n    #final['row_id'] = final['time_id'].apply(lambda x: f'{stock_id}-{x}')\n        \n    return final\n\n\ndef get_dataset(id_list, train = True):\n\n    stock = Parallel(n_jobs=-1)(\n        delayed(pre_data)(stock_id, train = True) \n        for stock_id in id_list\n    )\n    \n    stock_df = pd.concat(stock, ignore_index = True)\n\n    return stock_df","metadata":{"execution":{"iopub.status.busy":"2021-07-23T11:01:19.155439Z","iopub.execute_input":"2021-07-23T11:01:19.155824Z","iopub.status.idle":"2021-07-23T11:01:19.170843Z","shell.execute_reply.started":"2021-07-23T11:01:19.155791Z","shell.execute_reply":"2021-07-23T11:01:19.169842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stock_train_df = get_dataset(train['stock_id'].unique(), train = True)\ntrain_df = train.merge(stock_train_df, how = 'left', on = ['stock_id', 'time_id'])\n\nstock_test_df = pre_data(0, train = False)\ntest_df = test.merge(stock_test_df, how = 'left', on = ['stock_id', 'time_id']).fillna(-999)","metadata":{"execution":{"iopub.status.busy":"2021-07-23T11:01:20.990514Z","iopub.execute_input":"2021-07-23T11:01:20.990889Z","iopub.status.idle":"2021-07-23T11:06:24.845932Z","shell.execute_reply.started":"2021-07-23T11:01:20.990858Z","shell.execute_reply":"2021-07-23T11:06:24.844708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize = (8, 8))\ncorr = train_df.corr()\nmask = np.zeros_like(corr, dtype = np.bool)\nmask[np.triu_indices_from(mask)] = True\n\nsns.heatmap(corr, cmap = 'coolwarm', mask = mask, linewidth = 0.5, vmin = -1, vmax = 1,\n           cbar_kws = {'shrink': .5})\nplt.title('Correlation Heatmap of train features', fontsize = 20, fontweight = 'bold')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-23T11:06:50.216118Z","iopub.execute_input":"2021-07-23T11:06:50.21646Z","iopub.status.idle":"2021-07-23T11:06:50.687887Z","shell.execute_reply.started":"2021-07-23T11:06:50.216429Z","shell.execute_reply":"2021-07-23T11:06:50.687069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kfold = KFold(n_splits = 5, shuffle = True, random_state = 0)\n\nfeatures = train_df.drop(['target'], axis = 1)\ntarget = train_df['target']\n\n# define LGBM model\nlgbm = LGBMRegressor(random_state = 0)\nX_train, X_val, y_train, y_val = train_test_split(features, target, test_size = 0.2, random_state = 0)","metadata":{"execution":{"iopub.status.busy":"2021-07-23T11:07:03.532193Z","iopub.execute_input":"2021-07-23T11:07:03.532551Z","iopub.status.idle":"2021-07-23T11:07:03.602936Z","shell.execute_reply.started":"2021-07-23T11:07:03.532521Z","shell.execute_reply":"2021-07-23T11:07:03.602107Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Hyperparameter Tuning: Bayesian Optimization <a class=\"anchor\" id=\"opt\"></a>\nTo select optimal parameter, we need to conduct hyperparameter tuning.\n\n**GridSearchCV** take too long since this tests all possible combinations of parameters. **RandomSearchCV** take less time but it chooses set of parameters randomly(does not test all combinations of parameters), selected parameter may not be an optimal parameter. Both algorithms do not contains prior knowledge information\n\n**Bayesian Optimization** keep track of past evaluation results which they use to form a probabilistic model mapping hyperparameters to a probability of a score on the objective function. Also it's fatster than GridSearchCV, and more precise than RandomSearchCV.\n(Ref: <https://towardsdatascience.com/a-conceptual-explanation-of-bayesian-model-based-hyperparameter-optimization-for-machine-learning-b8172278050f>)\n","metadata":{}},{"cell_type":"code","source":"X_train, X_val, y_train, y_val = train_test_split(features, target, test_size = 0.2, random_state = 0)\n\n\ndef lgbm_cv(learning_rate, n_estimators, max_depth, num_leaves, subsample, min_data_in_leaf, silent = True):\n    params = {'learning_rate': learning_rate,\n          'n_estimators': int(n_estimators),\n          'max_depth': int(max_depth),\n          'num_leaves': int(num_leaves),\n          'subsample': subsample,\n          'min_data_in_leaf': int(min_data_in_leaf),\n          'verbose': -1,\n          'force_col_wise': True\n             }\n    \n    lgbm_train = lgb.Dataset(X_train, label = y_train, categorical_feature = ['stock_id'], weight = 1/np.square(y_train))\n    lgbm_val = lgb.Dataset(X_val, label = y_val, categorical_feature = ['stock_id'], weight = 1/np.square(y_val))\n    \n    model = lgb.train(params = params,\n                      train_set = lgbm_train,\n                      valid_sets = [lgbm_train, lgbm_val],\n                      feval = feval_RMSPE,\n                      verbose_eval = 1,\n                      early_stopping_rounds = 100,\n                      num_boost_round = 1000)\n    \n    pred = model.predict(X_val)\n    rmspe_score = RMSPE(y_val, pred)\n    \n    return rmspe_score","metadata":{"execution":{"iopub.status.busy":"2021-07-23T11:07:08.169414Z","iopub.execute_input":"2021-07-23T11:07:08.169739Z","iopub.status.idle":"2021-07-23T11:07:08.243064Z","shell.execute_reply.started":"2021-07-23T11:07:08.169711Z","shell.execute_reply":"2021-07-23T11:07:08.242239Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pbounds = {'learning_rate': (0.005, 0.1),\n          'n_estimators': (1000, 2000),\n          'max_depth': (10, 20),\n          'num_leaves': (7, 14),\n          'subsample': (0.5, 0.9),\n          'min_data_in_leaf': (5, 20)\n          }\n\nlgbm_bo = BayesianOptimization(f = lgbm_cv, pbounds = pbounds, verbose = 2, random_state = 0)\n\n# init_points :  initial number of Random Search points \n# n_iter : iteration number\n# acq : Acquisition Function - we use EI\n# xi : exploration (default: 0)\nlgbm_bo.maximize(init_points = 2, n_iter = 10, acq = 'ei', xi = 0.01)","metadata":{"execution":{"iopub.status.busy":"2021-07-23T11:07:11.545647Z","iopub.execute_input":"2021-07-23T11:07:11.54596Z","iopub.status.idle":"2021-07-23T11:10:06.222249Z","shell.execute_reply.started":"2021-07-23T11:07:11.545932Z","shell.execute_reply":"2021-07-23T11:10:06.221097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# optimal parameter\nprint(lgbm_bo.max)\n\nopt_params = lgbm_bo.max['params']","metadata":{"execution":{"iopub.status.busy":"2021-07-23T11:10:52.258202Z","iopub.execute_input":"2021-07-23T11:10:52.258558Z","iopub.status.idle":"2021-07-23T11:10:52.264426Z","shell.execute_reply.started":"2021-07-23T11:10:52.258528Z","shell.execute_reply":"2021-07-23T11:10:52.263087Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# GridSearchCV\n\n# scoring function\n#rmspe = make_scorer(RMSPE, greater_is_better = False)\n\n#X_train, X_val, y_train, y_val = train_test_split(new_features, target, test_size = 0.2, random_state = 0)\n\n#grid_model = GridSearchCV(lgbm, param_grid = params, cv = 5, scoring = rmspe)\n#grid_model.fit(X_train, y_train)\n\n#print('Best parameter: ', grid_model.best_params_)\n#print('Best score: ', grid_model.best_score_)","metadata":{"execution":{"iopub.status.busy":"2021-07-22T14:02:28.217497Z","iopub.status.idle":"2021-07-22T14:02:28.217944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## LGBM Modeling <a class=\"anchor\" id=\"model\"></a>","metadata":{}},{"cell_type":"code","source":"params = {'learning_rate': opt_params['learning_rate'],\n          'objective': 'regression',\n          'n_estimators': int(np.round(opt_params['n_estimators'], 0)), \n          'max_depth': int(np.round(opt_params['max_depth'], 0)),\n          'num_leaves': int(np.round(opt_params['num_leaves'], 0)),\n          'min_data_in_leaf': int(np.round(opt_params['min_data_in_leaf'], 0)),\n          'subsample': opt_params['subsample'],\n          'force_col_wise': True,\n          'verbose': -1\n          }\n\n    \n# KFold\nrmspe_list = []\nmodel_list = []\nfor i, (train_idx, val_idx) in enumerate(kfold.split(features)):\n    print(f'################# {i+1}th Fold #################')\n    X_train, X_val = features.iloc[train_idx, :], features.iloc[val_idx, :]\n    y_train, y_val = target[train_idx], target[val_idx]\n        \n    train_set = lgb.Dataset(X_train, label = y_train, categorical_feature = ['stock_id'], weight = 1/np.square(y_train))\n    val_set = lgb.Dataset(X_val, label = y_val, categorical_feature = ['stock_id'], weight = 1/np.square(y_val))\n\n    model = lgb.train(params = params,\n                      train_set = train_set,\n                      valid_sets = [train_set, val_set],\n                      feval = feval_RMSPE,\n                      early_stopping_rounds = 100,\n                      num_boost_round = 1000,\n                      verbose_eval = 100)\n    \n    model_list.append(model)\n    \n    pred = model.predict(X_val)\n    rmspe_score = RMSPE(y_val, pred)\n    \n    print(f'RMSPE: {np.round(rmspe_score, 4)}')\n    rmspe_list.append(rmspe_score)","metadata":{"execution":{"iopub.status.busy":"2021-07-23T11:14:47.995319Z","iopub.execute_input":"2021-07-23T11:14:47.995701Z","iopub.status.idle":"2021-07-23T11:16:26.986007Z","shell.execute_reply.started":"2021-07-23T11:14:47.995671Z","shell.execute_reply":"2021-07-23T11:16:26.985197Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feat_imp = pd.DataFrame()\nfeat_imp['feature'] = features.columns.tolist()\nfeat_imp['importance'] = model.feature_importance(importance_type = 'gain')\n\nfeat_imp = feat_imp.sort_values(by = ['importance'], ascending = False).reset_index(drop = True)\nfeat_imp","metadata":{"execution":{"iopub.status.busy":"2021-07-23T11:11:53.55948Z","iopub.execute_input":"2021-07-23T11:11:53.559881Z","iopub.status.idle":"2021-07-23T11:11:53.584629Z","shell.execute_reply.started":"2021-07-23T11:11:53.559847Z","shell.execute_reply":"2021-07-23T11:11:53.583685Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize = (8, 6))\nax = sns.barplot(data = feat_imp, x = 'importance', y = 'feature', color = '#006699', edgecolor = 'black')\n\nfor i in ['right', 'top']:\n        ax.spines[i].set_visible(False)\n        \nplt.title('Feature Importance', fontsize = 20, fontweight = 'bold')\nplt.xlabel('Importance', fontsize = 10, fontweight = 'bold')\nplt.ylabel('Feature', fontsize = 10, fontweight = 'bold')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-23T11:17:27.263205Z","iopub.execute_input":"2021-07-23T11:17:27.263646Z","iopub.status.idle":"2021-07-23T11:17:27.441609Z","shell.execute_reply.started":"2021-07-23T11:17:27.263613Z","shell.execute_reply":"2021-07-23T11:17:27.440535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ax = plt.subplot(1, 1, 1)\n#ax.plot(range(1, 6), rmspe_list, color ='#006699', marker = 'o')\nax.fill_between(range(1, 6), 0, rmspe_list, alpha = 0.4, color = '#d9e6f2')\nax.scatter(range(1, 6), rmspe_list, color = '#006699')\nplt.axhline(y = np.mean(rmspe_list), color = '#cc0000', linestyle = ':', linewidth = 2)\nplt.text(1 ,np.mean(rmspe_list)+0.0015, 'mean', \n         bbox = dict(facecolor ='#cc0000', edgecolor='#cc0000', boxstyle='round', alpha = 0.2))\n\n\nax.set_ylim([0.23, 0.26])\nfor i in ['left', 'right', 'top']:\n        ax.spines[i].set_visible(False)\n\nax.set_xlabel('Fold', fontweight = 'bold')\nax.set_ylabel('RMSPE', fontweight = 'bold')\nax.set_xticks(range(1,6))\nplt.title('RMSPE in each fold', fontsize = 15, fontweight = 'bold', pad = 20)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-23T11:12:03.005541Z","iopub.execute_input":"2021-07-23T11:12:03.005862Z","iopub.status.idle":"2021-07-23T11:12:03.197693Z","shell.execute_reply.started":"2021-07-23T11:12:03.005833Z","shell.execute_reply":"2021-07-23T11:12:03.1969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Prediction & Submission <a class=\"anchor\" id=\"pred\"></a>","metadata":{}},{"cell_type":"code","source":"test_df_new = test_df.drop(['row_id'], axis = 1)","metadata":{"execution":{"iopub.status.busy":"2021-07-23T11:18:43.405898Z","iopub.execute_input":"2021-07-23T11:18:43.406223Z","iopub.status.idle":"2021-07-23T11:18:43.417507Z","shell.execute_reply.started":"2021-07-23T11:18:43.406192Z","shell.execute_reply":"2021-07-23T11:18:43.416796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_pred = np.zeros(len(test_df_new))\nfor model in model_list:\n    pred = model.predict(test_df_new)\n    test_pred += pred/len(model_list)","metadata":{"execution":{"iopub.status.busy":"2021-07-23T11:18:44.971068Z","iopub.execute_input":"2021-07-23T11:18:44.971426Z","iopub.status.idle":"2021-07-23T11:18:44.97741Z","shell.execute_reply.started":"2021-07-23T11:18:44.971391Z","shell.execute_reply":"2021-07-23T11:18:44.976421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df['target'] = test_pred\nsub = test_df[['row_id', 'target']]\nsub","metadata":{"execution":{"iopub.status.busy":"2021-07-23T11:16:58.38025Z","iopub.execute_input":"2021-07-23T11:16:58.380617Z","iopub.status.idle":"2021-07-23T11:16:58.392244Z","shell.execute_reply.started":"2021-07-23T11:16:58.380585Z","shell.execute_reply":"2021-07-23T11:16:58.39128Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub.to_csv('submission.csv', index = False)","metadata":{"execution":{"iopub.status.busy":"2021-07-23T11:17:03.946106Z","iopub.execute_input":"2021-07-23T11:17:03.946447Z","iopub.status.idle":"2021-07-23T11:17:03.955611Z","shell.execute_reply.started":"2021-07-23T11:17:03.946415Z","shell.execute_reply":"2021-07-23T11:17:03.954762Z"},"trusted":true},"execution_count":null,"outputs":[]}]}