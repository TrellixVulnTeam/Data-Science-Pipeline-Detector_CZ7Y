{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport glob\nfrom tqdm import tqdm\nimport sys, os\n\n\ndef load_data(mode, path=\"/kaggle/input/optiver-realized-volatility-prediction\"):\n    # mode = \"train\"/\"test\"\n    file_name = f'{path}/{mode}.csv'\n    return pd.read_csv(file_name)\n\ndf = load_data(\"test\")\nprint(df.shape, df[\"stock_id\"].max())\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-06T05:51:58.742832Z","iopub.status.idle":"2021-09-06T05:51:58.743196Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SCALE = 100\nPATH = \"/kaggle/input/optiver-realized-volatility-prediction\"\n\norder_book_paths = glob.glob(f'{PATH}/book_test.parquet/*/*')\nlen(order_book_paths)","metadata":{"execution":{"iopub.status.busy":"2021-09-06T05:52:01.433205Z","iopub.execute_input":"2021-09-06T05:52:01.433582Z","iopub.status.idle":"2021-09-06T05:52:01.448345Z","shell.execute_reply.started":"2021-09-06T05:52:01.433553Z","shell.execute_reply":"2021-09-06T05:52:01.447562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trade_paths = glob.glob(f'{PATH}/trade_test.parquet/*/*')\nlen(trade_paths)","metadata":{"execution":{"iopub.status.busy":"2021-09-06T05:52:03.463592Z","iopub.execute_input":"2021-09-06T05:52:03.463919Z","iopub.status.idle":"2021-09-06T05:52:03.476844Z","shell.execute_reply.started":"2021-09-06T05:52:03.463892Z","shell.execute_reply":"2021-09-06T05:52:03.476084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"order_books = dict()\n\n\nfor path in tqdm(order_book_paths):\n    stock_id = int(path.split(\"=\")[1].split(\"/\")[0])\n    book_df = pd.read_parquet(path)\n    books_by_time = dict()\n    \n    for time_id in book_df.time_id.unique():\n        books_by_time[time_id] = book_df[book_df[\"time_id\"] == time_id].reset_index(drop=True)\n    \n    order_books[stock_id] = books_by_time","metadata":{"execution":{"iopub.status.busy":"2021-09-06T05:52:06.801918Z","iopub.execute_input":"2021-09-06T05:52:06.802325Z","iopub.status.idle":"2021-09-06T05:52:06.939207Z","shell.execute_reply.started":"2021-09-06T05:52:06.802294Z","shell.execute_reply":"2021-09-06T05:52:06.938381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trades = dict()\n\n\nfor path in tqdm(trade_paths):\n    stock_id = int(path.split(\"=\")[1].split(\"/\")[0])\n    trade_df = pd.read_parquet(path)\n    trade_by_time = dict()\n    \n    for time_id in trade_df.time_id.unique():\n        trade_by_time[time_id] = trade_df[trade_df[\"time_id\"] == time_id].reset_index(drop=True)\n    \n    trades[stock_id] = trade_by_time","metadata":{"execution":{"iopub.status.busy":"2021-09-06T05:52:08.579201Z","iopub.execute_input":"2021-09-06T05:52:08.579605Z","iopub.status.idle":"2021-09-06T05:52:08.602209Z","shell.execute_reply.started":"2021-09-06T05:52:08.579576Z","shell.execute_reply":"2021-09-06T05:52:08.601305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\n\n\nmeans_order = torch.FloatTensor([  0.9997,   1.0003, 769.9902, 766.7346,   0.9995,   1.0005, 959.3417,\n        928.2203, 300])\nstds_order = torch.FloatTensor([3.6881e-03, 3.6871e-03, 5.3541e+03, 4.9549e+03, 3.7009e-03, 3.6991e-03,\n        6.6838e+03, 5.7353e+03, 300])\n\nmeans_trade = torch.FloatTensor([300, 1.0, 100, 3.0])\nstds_trade = torch.FloatTensor([300, 0.004, 153, 3.5])\n\n\n\nclass OptiverDataset(Dataset):\n    \n    def __init__(self, df, aug=False):\n        super().__init__()\n        self.df = df.reset_index(drop=True)\n        self.aug = aug\n        self.seq_len = 600\n        self.order_features = ['bid_price1', 'ask_price1', 'bid_size1', 'ask_size1','bid_price2', \n                         'ask_price2', 'bid_size2', 'ask_size2', \"seconds_in_bucket\"]\n        self.trade_features = [\"seconds_in_bucket\", \"price\", \"size\", \"order_count\"]\n        \n    \n    def extract_features(self, data_dict, stock_id, time_id, features, means, stds):\n        X = -torch.ones((self.seq_len, len(features)))\n        try:\n            df = data_dict[stock_id][time_id]\n            feature_array = df[features].values\n            X[-feature_array.shape[0]:] = (torch.FloatTensor(feature_array) - means)/stds\n        except:\n            pass\n        return X\n\n\n    def __getitem__(self, index):\n        row = self.df.iloc[index]\n        \n        X1 = self.extract_features(order_books, row.stock_id, row.time_id, self.order_features,\n                                  means_order, stds_order)\n        try:\n            X2 = self.extract_features(trades, row.stock_id, row.time_id, self.trade_features,\n                                      means_trade, stds_trade) \n        except:\n            X2 = -torch.ones((self.seq_len, len(self.trade_features)))\n        target = torch.FloatTensor([0.0])\n        stock = torch.LongTensor([row.stock_id])\n        return X1, X2, stock, target\n\n    def __len__(self):\n        return self.df.shape[0]\n    \nds = OptiverDataset(df)\nds[1]","metadata":{"execution":{"iopub.status.busy":"2021-09-06T05:52:12.991386Z","iopub.execute_input":"2021-09-06T05:52:12.991881Z","iopub.status.idle":"2021-09-06T05:52:13.019786Z","shell.execute_reply.started":"2021-09-06T05:52:12.991835Z","shell.execute_reply":"2021-09-06T05:52:13.018693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ConvBlock(nn.Module):\n    def __init__(self, in_dim, out_dim, kernel_size, stride=1):\n        super().__init__()\n        self.lin = nn.Conv1d(in_dim, out_dim, kernel_size, stride=stride)\n        self.bn = nn.BatchNorm1d(out_dim)\n        self.activation = nn.ReLU()\n        \n    def forward(self, x):\n        x = self.lin(x)\n        x = self.bn(x)\n        return self.activation(x)\n        \n\nclass SubModel(nn.Module):\n    def __init__(self, in_dim):\n        super().__init__()\n        self.convs1 = nn.Sequential(ConvBlock(in_dim, 16, 3),\n                                   ConvBlock(16, 32, 3))\n        self.stock_conv = ConvBlock(36, 64, 4, stride=4)\n        self.avg_pool = nn.AdaptiveAvgPool1d(8)\n        self.max_pool = nn.AdaptiveMaxPool1d(8)\n        self.convs2 = nn.Sequential(ConvBlock(128, 128, 2, stride=2),\n                                    ConvBlock(128, 32, 2, stride=2),\n                                    ConvBlock(32, 8, 2, stride=2))\n        \n    def forward(self, x, s):\n        x = self.convs1(x.transpose(2, 1))\n        x = self.stock_conv(torch.cat([x, s.repeat(1, 1, x.shape[2])], axis=1))\n        x = torch.cat([self.avg_pool(x), self.max_pool(x)], axis=1)\n        x = self.convs2(x).squeeze(-1)\n        return x\n    \n    \nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.order_model = SubModel(in_dim=9)\n        self.trade_model = SubModel(in_dim=4)\n        self.top = nn.Linear(16, 1)\n        self.stock_emb = nn.Embedding(127, 4)\n        \n    def forward(self, inputs):\n        x1, x2, s = inputs\n        s = self.stock_emb(s).transpose(2, 1)\n        \n        x1 = self.order_model(x1, s)\n        x2 = self.trade_model(x2, s)\n        x = self.top(torch.cat([x1, x2], axis=1))\n        return x","metadata":{"execution":{"iopub.status.busy":"2021-09-06T05:52:27.720451Z","iopub.execute_input":"2021-09-06T05:52:27.720785Z","iopub.status.idle":"2021-09-06T05:52:27.738185Z","shell.execute_reply.started":"2021-09-06T05:52:27.720756Z","shell.execute_reply":"2021-09-06T05:52:27.737027Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def read_data(data):\n    return tuple(d.cuda() for d in data[:-1]), data[-1].cuda()\n\ndef inference(model, loader):\n    model.eval()\n    \n    tbar = tqdm(loader, file=sys.stdout)\n    \n    preds = []\n\n    with torch.no_grad():\n        for idx, data in enumerate(tbar):\n            inputs, target = read_data(data)\n\n            pred = model(inputs)\n\n            preds.append(pred.detach().cpu().numpy().ravel())\n    \n    return np.concatenate(preds)\n\nNW = 4\nBS = 256\nNUM_FOLDS = 5\nloader = DataLoader(ds, batch_size=BS, shuffle=False, num_workers=NW, pin_memory=False, drop_last=False)\n\n\nmodel = Model()\nmodel = model.cuda()\nmodel.eval()\n\nmodel.load_state_dict(torch.load(f\"/kaggle/input/models/optiver_nn_v01_0.pth\"))\ny = inference(model, loader)/NUM_FOLDS\n\nfor i in range(1, NUM_FOLDS):\n    model.load_state_dict(torch.load(f\"/kaggle/input/models/optiver_nn_v01_{i}.pth\"))\n    y += inference(model, loader)/NUM_FOLDS","metadata":{"execution":{"iopub.status.busy":"2021-09-06T05:53:32.494408Z","iopub.execute_input":"2021-09-06T05:53:32.494734Z","iopub.status.idle":"2021-09-06T05:53:34.247005Z","shell.execute_reply.started":"2021-09-06T05:53:32.494704Z","shell.execute_reply":"2021-09-06T05:53:34.245918Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[\"target\"] = np.clip(y, 0.0, None)/SCALE\n\ndf.to_csv(\"submission.csv\", index=False, columns=[\"row_id\", \"target\"])\n","metadata":{"execution":{"iopub.status.busy":"2021-09-06T05:54:12.49836Z","iopub.execute_input":"2021-09-06T05:54:12.49868Z","iopub.status.idle":"2021-09-06T05:54:12.506341Z","shell.execute_reply.started":"2021-09-06T05:54:12.498653Z","shell.execute_reply":"2021-09-06T05:54:12.505309Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()\n","metadata":{"execution":{"iopub.status.busy":"2021-09-06T05:53:56.988108Z","iopub.execute_input":"2021-09-06T05:53:56.98851Z","iopub.status.idle":"2021-09-06T05:53:57.000754Z","shell.execute_reply.started":"2021-09-06T05:53:56.98848Z","shell.execute_reply":"2021-09-06T05:53:56.999562Z"},"trusted":true},"execution_count":null,"outputs":[]}]}