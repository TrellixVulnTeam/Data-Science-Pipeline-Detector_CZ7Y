{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"A simplified reproduction of the [tutorial notebook](http://)https://www.kaggle.com/jiashenliu/introduction-to-financial-concepts-and-data provided by the content organizers. By \"simplified\" I am meaning that the usage of ultra familiar tools is maximized. Hope it will ease the on-boarding of new joiners in this competition. Heavily inspired by this [notebook](http://)https://www.kaggle.com/lucasmorin/realised-vol-weighted-regression-baseline?select=sample_submission.csv. ","metadata":{}},{"cell_type":"markdown","source":"### Importing all necessary libraries","metadata":{}},{"cell_type":"code","source":"\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import r2_score\nimport glob","metadata":{"execution":{"iopub.status.busy":"2021-07-06T03:39:17.357124Z","iopub.execute_input":"2021-07-06T03:39:17.358098Z","iopub.status.idle":"2021-07-06T03:39:17.363596Z","shell.execute_reply.started":"2021-07-06T03:39:17.358006Z","shell.execute_reply":"2021-07-06T03:39:17.362494Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# For quickly switching between training and test data\ndef train_test(mode):\n    # mode = \"train\"/\"test\"\n    file_name = '../input/optiver-realized-volatility-prediction/' + mode + '.csv'\n    return pd.read_csv(file_name)\n\n","metadata":{"execution":{"iopub.status.busy":"2021-07-06T03:39:17.365176Z","iopub.execute_input":"2021-07-06T03:39:17.365482Z","iopub.status.idle":"2021-07-06T03:39:17.380459Z","shell.execute_reply.started":"2021-07-06T03:39:17.365454Z","shell.execute_reply":"2021-07-06T03:39:17.379357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = train_test(\"train\")\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-06T03:39:17.382797Z","iopub.execute_input":"2021-07-06T03:39:17.383273Z","iopub.status.idle":"2021-07-06T03:39:17.547138Z","shell.execute_reply.started":"2021-07-06T03:39:17.383222Z","shell.execute_reply":"2021-07-06T03:39:17.54597Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The *order_book* data are partitioned on the basis of *stock_id*. The following command lists all the parquet file names, it will help us to iterate over all stocks in later section.","metadata":{}},{"cell_type":"code","source":"order_book_training = glob.glob('/kaggle/input/optiver-realized-volatility-prediction/book_train.parquet/*')","metadata":{"execution":{"iopub.status.busy":"2021-07-06T03:39:17.54918Z","iopub.execute_input":"2021-07-06T03:39:17.549678Z","iopub.status.idle":"2021-07-06T03:39:17.560594Z","shell.execute_reply.started":"2021-07-06T03:39:17.549629Z","shell.execute_reply":"2021-07-06T03:39:17.559395Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Each member in *order_book_training* corresponds to a single stock. Each stock contains several *time_id*. The goal is to predict volatility for each (*stock_id, time_id*) tuples.\n\nHere we utilize the fact that the Panda's groupby operation retains the order of the rows. We write a custom aggregate function to calculate WAP -> tik to tik returns -> realized volatility.\n\n**Warning**: Custom aggregate functions are not Cythonized and they are slow. Basically cutom aggregate functions are syntactic sugars with a sprinkle of enhanced readability.","metadata":{}},{"cell_type":"code","source":"# custom aggregate function\ndef wap2vol(df):\n    # wap2vol stands for WAP to Realized Volatility\n    temp = np.log(df).diff() # calculating tik to tik returns\n    # returning realized volatility\n    return np.sqrt(np.sum(temp**2)) ","metadata":{"execution":{"iopub.status.busy":"2021-07-06T03:39:17.56221Z","iopub.execute_input":"2021-07-06T03:39:17.563109Z","iopub.status.idle":"2021-07-06T03:39:17.572765Z","shell.execute_reply.started":"2021-07-06T03:39:17.563051Z","shell.execute_reply":"2021-07-06T03:39:17.571477Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# function for calculating realized volatility per time id for a given stock\ndef rel_vol_time_id(path):\n    # book: book is an order book\n    book = pd.read_parquet(path) # order book for a stock id loaded\n    # calculating WAP\n    p1 = book[\"bid_price1\"]\n    p2 = book[\"ask_price1\"]\n    s1 = book[\"bid_size1\"]\n    s2 = book[\"ask_size1\"]\n    \n    book[\"WAP\"] = (p1*s2 + p2*s1) / (s1 + s2)\n    # calculating realized volatility for each time_id\n    transbook = book.groupby(\"time_id\")[\"WAP\"].agg(wap2vol)\n    return transbook","metadata":{"execution":{"iopub.status.busy":"2021-07-06T03:39:17.574123Z","iopub.execute_input":"2021-07-06T03:39:17.574443Z","iopub.status.idle":"2021-07-06T03:39:17.588342Z","shell.execute_reply.started":"2021-07-06T03:39:17.574412Z","shell.execute_reply":"2021-07-06T03:39:17.587126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we iterate over all order books and compute realized volatility of each (*stock_id, time_id*) tuples.","metadata":{}},{"cell_type":"markdown","source":"Instead of concatenating dataframes corresponding to each stock, I am rather taking an unified approach by listing all *stock_id, time_id* and their realized volatility. Later these lists will be converted to a dataframe. This approach is recommended/reiterated in this [stack exchange discussion](http://)https://stackoverflow.com/questions/13784192/creating-an-empty-pandas-dataframe-then-filling-it ","metadata":{}},{"cell_type":"code","source":"%%time \nstock_id = []\ntime_id = []\nrelvol = []\nfor i in order_book_training:\n    # finding the stock_id\n    temp_stock = int(i.split(\"=\")[1])\n    # find the realized volatility for all time_id of temp_stock\n    temp_relvol = rel_vol_time_id(i)\n    stock_id += [temp_stock]*temp_relvol.shape[0]\n    time_id += list(temp_relvol.index)\n    relvol += list(temp_relvol)","metadata":{"execution":{"iopub.status.busy":"2021-07-06T03:39:17.591145Z","iopub.execute_input":"2021-07-06T03:39:17.591642Z","iopub.status.idle":"2021-07-06T03:44:20.043762Z","shell.execute_reply.started":"2021-07-06T03:39:17.591602Z","shell.execute_reply":"2021-07-06T03:44:20.042775Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we create the dataframe containing realized volatilities for all _(stock_id, time_id)_ tuples.","metadata":{}},{"cell_type":"code","source":"past_volatility = pd.DataFrame({\"stock_id\": stock_id, \"time_id\": time_id, \"volatility\": relvol})","metadata":{"execution":{"iopub.status.busy":"2021-07-06T03:44:20.04539Z","iopub.execute_input":"2021-07-06T03:44:20.045689Z","iopub.status.idle":"2021-07-06T03:44:20.490553Z","shell.execute_reply.started":"2021-07-06T03:44:20.045659Z","shell.execute_reply":"2021-07-06T03:44:20.489468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we join *past_volatility* with *training* to calculate the error metrics, mainly for a sanity check to confirm that it is a correct reproduction. Here we are naively assuming that **past level of volatility = future level of volatility**.","metadata":{}},{"cell_type":"code","source":"joined = train.merge(past_volatility, on = [\"stock_id\",\"time_id\"], how = \"left\")\nR2 = round(r2_score(y_true = joined['target'], y_pred = joined['volatility']),3)\nprint(f'The R2 score of the naive prediction for training set is {R2}')","metadata":{"execution":{"iopub.status.busy":"2021-07-06T03:44:20.492073Z","iopub.execute_input":"2021-07-06T03:44:20.492391Z","iopub.status.idle":"2021-07-06T03:44:20.622251Z","shell.execute_reply.started":"2021-07-06T03:44:20.49236Z","shell.execute_reply":"2021-07-06T03:44:20.620992Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def rmspe(y_true, y_pred):\n    return  (np.sqrt(np.mean(np.square((y_true - y_pred) / y_true))))\n\nrmspe = rmspe(joined[\"target\"], joined[\"volatility\"])\nprint(f'The RMSPE score of the native prediciton for the training set is {rmspe}')","metadata":{"execution":{"iopub.status.busy":"2021-07-06T03:44:20.623564Z","iopub.execute_input":"2021-07-06T03:44:20.623901Z","iopub.status.idle":"2021-07-06T03:44:20.636114Z","shell.execute_reply.started":"2021-07-06T03:44:20.623846Z","shell.execute_reply":"2021-07-06T03:44:20.634525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's train a simple OLS model for each stock_id. Use *degree* to specify the degree of the linear model.","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\n\n# for training\ndef linear_training(X,y,degree):\n    # instantiating polynomial features\n    polyfeat = PolynomialFeatures(degree = degree)\n    linreg = LinearRegression()\n    # preprocessing the training data\n    x = np.array(X).reshape(-1,1)\n    # creating the polynomial features\n    X_ = polyfeat.fit_transform(x)\n    # training the model\n    weights = 1/np.square(y)\n    return linreg.fit(X_, np.array(y).reshape(-1,1), sample_weight = weights)\n\n\nstock_id_train = train.stock_id.unique() # all stock_id for the train set\nmodels = {} # dictionary for holding trained models for each stock_id\ndegree = 2\nfor i in stock_id_train:\n    temp = joined[joined[\"stock_id\"]==i]\n    X = temp[\"volatility\"]\n    y = temp[\"target\"]\n    models[i] = linear_training(X,y,degree)\n    ","metadata":{"execution":{"iopub.status.busy":"2021-07-06T03:50:14.601082Z","iopub.execute_input":"2021-07-06T03:50:14.601423Z","iopub.status.idle":"2021-07-06T03:50:14.982207Z","shell.execute_reply.started":"2021-07-06T03:50:14.601395Z","shell.execute_reply":"2021-07-06T03:50:14.981301Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"models","metadata":{"execution":{"iopub.status.busy":"2021-07-06T03:44:21.082304Z","iopub.execute_input":"2021-07-06T03:44:21.082871Z","iopub.status.idle":"2021-07-06T03:44:21.114125Z","shell.execute_reply.started":"2021-07-06T03:44:21.082818Z","shell.execute_reply":"2021-07-06T03:44:21.112438Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's make prediction on the test set and submit a sample submission","metadata":{}},{"cell_type":"code","source":"# listing all test order books\norder_book_test = glob.glob('/kaggle/input/optiver-realized-volatility-prediction/book_test.parquet/*')","metadata":{"execution":{"iopub.status.busy":"2021-07-06T03:50:20.344347Z","iopub.execute_input":"2021-07-06T03:50:20.344705Z","iopub.status.idle":"2021-07-06T03:50:20.350939Z","shell.execute_reply.started":"2021-07-06T03:50:20.344676Z","shell.execute_reply":"2021-07-06T03:50:20.350004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We start by calculating the past volatility of the test set.","metadata":{}},{"cell_type":"code","source":"%%time \nstock_id = []\ntime_id = []\nrelvol = []\nfor i in order_book_test:\n    # finding the stock_id\n    temp_stock = int(i.split(\"=\")[1])\n    # find the realized volatility for all time_id of temp_stock\n    temp_relvol = rel_vol_time_id(i)\n    stock_id += [temp_stock]*temp_relvol.shape[0]\n    time_id += list(temp_relvol.index)\n    relvol += list(temp_relvol)\n    \npast_test_volatility = pd.DataFrame({\"stock_id\": stock_id, \"time_id\": time_id, \"volatility\": relvol})","metadata":{"execution":{"iopub.status.busy":"2021-07-06T03:50:23.136957Z","iopub.execute_input":"2021-07-06T03:50:23.137284Z","iopub.status.idle":"2021-07-06T03:50:23.157948Z","shell.execute_reply.started":"2021-07-06T03:50:23.137255Z","shell.execute_reply":"2021-07-06T03:50:23.156999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will be using *linear_inference* for prediction","metadata":{}},{"cell_type":"code","source":"# for inference\ndef linear_inference(models, stock_id, past_volatility, degree):\n    model = models[stock_id]\n    polyfeat = PolynomialFeatures(degree = degree)\n    return model.predict(polyfeat.fit_transform([[past_volatility]]))[0][0]\n    ","metadata":{"execution":{"iopub.status.busy":"2021-07-06T03:50:27.109451Z","iopub.execute_input":"2021-07-06T03:50:27.109801Z","iopub.status.idle":"2021-07-06T03:50:27.115185Z","shell.execute_reply.started":"2021-07-06T03:50:27.109769Z","shell.execute_reply":"2021-07-06T03:50:27.114238Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# creating the header for the submission file\nsubmission = pd.DataFrame({\"row_id\" : [], \"target\" : []})  \nsubmission[\"row_id\"] = past_test_volatility.apply(lambda x: str(int(x.stock_id)) + '-' + str(int(x.time_id)), axis=1)\n# prediction for test data\nsubmission[\"target\"] = past_test_volatility.apply(lambda x: linear_inference(models,\\\n                                                                            x.stock_id,\\\n                                                                            x.volatility,\\\n                                                                            degree),\\\n                                                 axis = 1)","metadata":{"execution":{"iopub.status.busy":"2021-07-06T03:50:30.454748Z","iopub.execute_input":"2021-07-06T03:50:30.455107Z","iopub.status.idle":"2021-07-06T03:50:30.46396Z","shell.execute_reply.started":"2021-07-06T03:50:30.455077Z","shell.execute_reply":"2021-07-06T03:50:30.463279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv('submission.csv',index = False)","metadata":{"execution":{"iopub.status.busy":"2021-07-06T03:44:21.187125Z","iopub.execute_input":"2021-07-06T03:44:21.187414Z","iopub.status.idle":"2021-07-06T03:44:21.197112Z","shell.execute_reply.started":"2021-07-06T03:44:21.187376Z","shell.execute_reply":"2021-07-06T03:44:21.196096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission","metadata":{"execution":{"iopub.status.busy":"2021-07-06T03:50:35.509728Z","iopub.execute_input":"2021-07-06T03:50:35.510202Z","iopub.status.idle":"2021-07-06T03:50:35.517846Z","shell.execute_reply.started":"2021-07-06T03:50:35.510171Z","shell.execute_reply":"2021-07-06T03:50:35.517162Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}