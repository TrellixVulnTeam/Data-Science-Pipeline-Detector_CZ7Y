{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"All data wrangling + volatility calculation codes were pulled from my another notebook [Overly simplified OLS prediction](https://www.kaggle.com/shahmahdihasan/overly-simplified-ols-prediction). The goal of this codebook to cluster the *stock_id* based on their *realized volatility*. ","metadata":{}},{"cell_type":"markdown","source":"### Importing all the necessary librarires","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import r2_score\nimport glob\nfrom collections import Counter\nfrom sklearn.decomposition import PCA\nfrom mpl_toolkits.mplot3d import Axes3D","metadata":{"execution":{"iopub.status.busy":"2021-07-07T08:57:42.911708Z","iopub.execute_input":"2021-07-07T08:57:42.91235Z","iopub.status.idle":"2021-07-07T08:57:44.027359Z","shell.execute_reply.started":"2021-07-07T08:57:42.912257Z","shell.execute_reply":"2021-07-07T08:57:44.026457Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"order_book_training = glob.glob('/kaggle/input/optiver-realized-volatility-prediction/book_train.parquet/*')\n\n# custom aggregate function\ndef wap2vol(df):\n    # wap2vol stands for WAP to Realized Volatility\n    temp = np.log(df).diff() # calculating tik to tik returns\n    # returning realized volatility\n    return np.sqrt(np.sum(temp**2)) \n\n\n\n# function for calculating realized volatility per time id for a given stock\ndef rel_vol_time_id(path):\n    # book: book is an order book\n    book = pd.read_parquet(path) # order book for a stock id loaded\n    # calculating WAP\n    p1 = book[\"bid_price1\"]\n    p2 = book[\"ask_price1\"]\n    s1 = book[\"bid_size1\"]\n    s2 = book[\"ask_size1\"]\n    \n    book[\"WAP\"] = (p1*s2 + p2*s1) / (s1 + s2)\n    # calculating realized volatility for each time_id\n    transbook = book.groupby(\"time_id\")[\"WAP\"].agg(wap2vol)\n    return transbook\n\n","metadata":{"execution":{"iopub.status.busy":"2021-07-07T08:57:44.028959Z","iopub.execute_input":"2021-07-07T08:57:44.029349Z","iopub.status.idle":"2021-07-07T08:57:44.045301Z","shell.execute_reply.started":"2021-07-07T08:57:44.029309Z","shell.execute_reply":"2021-07-07T08:57:44.044315Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"All the necessary functions are there, now let's calculate the realized volatility for each *(stock_id, time_id)* tuples.","metadata":{}},{"cell_type":"code","source":"%%time \nstock_id = []\ntime_id = []\nrelvol = []\nfor i in order_book_training:\n    # finding the stock_id\n    temp_stock = int(i.split(\"=\")[1])\n    # find the realized volatility for all time_id of temp_stock\n    temp_relvol = rel_vol_time_id(i)\n    stock_id += [temp_stock]*temp_relvol.shape[0]\n    time_id += list(temp_relvol.index)\n    relvol += list(temp_relvol)\n\npast_volatility = pd.DataFrame({\"stock_id\": stock_id, \"time_id\": time_id, \"volatility\": relvol})","metadata":{"execution":{"iopub.status.busy":"2021-07-07T08:57:44.046878Z","iopub.execute_input":"2021-07-07T08:57:44.047138Z","iopub.status.idle":"2021-07-07T09:03:34.147413Z","shell.execute_reply.started":"2021-07-07T08:57:44.047113Z","shell.execute_reply":"2021-07-07T09:03:34.146468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('../input/optiver-realized-volatility-prediction/train.csv')\njoined = train.merge(past_volatility, on = [\"stock_id\",\"time_id\"], how = \"left\")\nstockID = 0\nsns.scatterplot(data = joined[joined[\"stock_id\"]==stockID], x = \"target\", y = \"volatility\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-07T09:08:14.21187Z","iopub.execute_input":"2021-07-07T09:08:14.212244Z","iopub.status.idle":"2021-07-07T09:08:14.630919Z","shell.execute_reply.started":"2021-07-07T09:08:14.21221Z","shell.execute_reply":"2021-07-07T09:08:14.629859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# finding the number of time_id each stock_id has \ncount_stock_id = Counter(stock_id)","metadata":{"execution":{"iopub.status.busy":"2021-07-07T09:03:34.631149Z","iopub.status.idle":"2021-07-07T09:03:34.631556Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"count_stock_id","metadata":{"execution":{"iopub.status.busy":"2021-07-07T09:03:34.632662Z","iopub.status.idle":"2021-07-07T09:03:34.633084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see not all the *stock_id* has equal number of time_id. For now I am proceeding with *stock_id* with number of *time_id* = 3830 ","metadata":{}},{"cell_type":"code","source":"eligible_stock_id = []\nfor i in count_stock_id:\n    if count_stock_id[i] == 3830:\n        eligible_stock_id.append(i)","metadata":{"execution":{"iopub.status.busy":"2021-07-07T09:03:34.633887Z","iopub.status.idle":"2021-07-07T09:03:34.634302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"past_volatility = past_volatility.loc[past_volatility[\"stock_id\"].isin(eligible_stock_id),:]","metadata":{"execution":{"iopub.status.busy":"2021-07-07T09:03:34.635148Z","iopub.status.idle":"2021-07-07T09:03:34.635583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vecx = np.array(past_volatility[\"volatility\"])","metadata":{"execution":{"iopub.status.busy":"2021-07-07T09:03:34.636669Z","iopub.status.idle":"2021-07-07T09:03:34.637091Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = np.reshape(vecx, (3830,-1))","metadata":{"execution":{"iopub.status.busy":"2021-07-07T09:03:34.638131Z","iopub.status.idle":"2021-07-07T09:03:34.638569Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Principal Component Analysis (PCA) is scale sensitive, hence I am preprocessing the data using *StandardScalar* from sklearn. I am also using a 2-component PCA for the ease of visualization to see if there actually exists any stock classes.","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nX = StandardScaler().fit_transform(X.T)\npca = PCA(n_components=3)\nPC = pca.fit_transform(X)\nPC.shape","metadata":{"execution":{"iopub.status.busy":"2021-07-07T09:03:34.639554Z","iopub.status.idle":"2021-07-07T09:03:34.639975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.set_style(\"whitegrid\", {'axes.grid' : False})\nfig = plt.figure(figsize=(6,6))\nax = Axes3D(fig) \n\nx = PC[:,0]\ny = PC[:,1]\nz = PC[:,2]\n\n\nax.scatter(x, y, z, c=x, marker='o')\nax.set_xlabel('PC_0')\nax.set_ylabel('PC_1')\nax.set_zlabel('PC_2')\n\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2021-07-07T09:03:34.640953Z","iopub.status.idle":"2021-07-07T09:03:34.641375Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(pca.explained_variance_ratio_)","metadata":{"execution":{"iopub.status.busy":"2021-07-07T09:03:34.642276Z","iopub.status.idle":"2021-07-07T09:03:34.642705Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.set_theme(style=\"white\")\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\nsns.heatmap(np.corrcoef(X), cmap=cmap)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-07T09:03:34.643571Z","iopub.status.idle":"2021-07-07T09:03:34.644008Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Conclusion being: in terms of volatility, the stocks are almost uncorrelated. ","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}