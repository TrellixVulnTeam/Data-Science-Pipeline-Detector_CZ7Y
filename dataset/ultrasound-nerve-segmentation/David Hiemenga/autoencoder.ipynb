{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# I started with the following notebook and made deletions and additions as necessary:\n# https://www.kaggle.com/kakauandme/digit-recognizer/tensorflow-deep-nn\nfrom __future__ import print_function\n\nimport numpy as np\nimport pandas as pd\n#from sklearn import preprocessing\nfrom sklearn.preprocessing import StandardScaler, RobustScaler\n\n#%matplotlib inline\n#import matplotlib.pyplot as plt\n#import matplotlib.cm as cm\n\nimport tensorflow as tf\n\nimport time\n\nnow = time.time()\ntag = str(now)\n\n#\nDEBUG = True\n# settings\nLEARNING_RATE = 5e-6\nTRAINING_ITERATIONS = 20000\n\n# sizes of the hidden layers\nNN_HL_ARCH = [100, 100, 100, 50]  \n    \n# probability of keeping a neuron = 1-prob(dropout)\nDROPOUT = 1.0\n\n#\nBATCH_SIZE = 1 # there may be some errors if set to > 1. \n\n# set to 0 to train on all available data\n# (currently validation is not used)\nVALIDATION_SIZE = 0 #2000\n\n# read training data from CSV file\nif DEBUG:\n    print('reading CSV input...')\ndata = pd.read_csv('../input/train.csv')\nheaders = list(data)\nheadertext = ','.join(headers)\n\n"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"\ndef normalize(X,has_outliers=True,has_missing=True,is_sparse=True):\n    robust_scaler = RobustScaler(with_centering=True)\n    data = robust_scaler.fit_transform(X)\n    if has_outliers:\n        pass\n        #data = preprocessing.scale(X,with_centering=False)\n    return data\n    #print('data({0[0]},{0[1]})'.format(data.shape))\n    #print (data.head())\ninputs = data.iloc[:,1:].values\ninputs = inputs.astype(np.float)\n"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"if DEBUG:\n    print('finished reading data...')\n\n# convert from [0:255] => [0.0:1.0]\n# inputs = np.multiply(inputs, 1.0 / 255.0)\n\nif DEBUG:\n    print('normalizing data...')\ninputs = normalize(inputs)\n\n\n# split data into training & validation\n# for an autoencoder labels are the same as inputs\nif DEBUG:\n    print('performing train/validation split...')\nvalidation_inputs = inputs[:VALIDATION_SIZE]\nvalidation_labels = inputs[:VALIDATION_SIZE]\n\ntrain_inputs = inputs[VALIDATION_SIZE:]\ntrain_labels = inputs[VALIDATION_SIZE:]\n\ninput_size = len(inputs[0])\n\nif DEBUG:\n    print('finished performing train/validation split...')\n\n\ndef weight_variable(shape):\n    initial = tf.truncated_normal(shape, stddev=0.1)\n    return tf.Variable(initial)\n\ndef bias_variable(shape):\n    initial = tf.constant(0.1, shape=shape)\n    return tf.Variable(initial)\n\nif DEBUG:\n    print('creating placeholders for input and output...')\n\n\n# inputs\nx = tf.placeholder('float', shape=[None,input_size])\n# outputs = labels\ny_ = tf.placeholder('float', shape=[None,input_size])\n\n# To prevent overfitting, we  apply [dropout](https://en.wikipedia.org/wiki/Convolutional_neural_network#Dropout) before the readout layer.\n# \n# Dropout removes some nodes from the network at each training stage. Each of the nodes is either kept in the network with probability *keep_prob* or dropped with probability *1 - keep_prob*. After the training stage is over the nodes are returned to the NN with their original weights.\nkeep_prob = tf.placeholder('float')\nif DEBUG:\n    print('creating W,b,h variables for various layers...')\n\n\nW = []\nb = []\nh = []\nh_tmp = []\n\nif DEBUG:\n    print('\\tnow creating input layer...')\n    print('\\t\\t',input_size,',',NN_HL_ARCH[0])\n\nW.append(weight_variable([input_size,NN_HL_ARCH[0]]))\nb.append(bias_variable([NN_HL_ARCH[0]]))\nh.append(tf.nn.relu(tf.matmul(x,W[0]) + b[0]))\n\nfor i in range(1,len(NN_HL_ARCH)):\n    u = NN_HL_ARCH[i-1]\n    v = NN_HL_ARCH[i]\n    if DEBUG:\n        print('\\tnow creating layer:', i)\n        print('\\t\\t',u,',',v)\n\n    W.append(weight_variable([u,v]))\n    b.append(bias_variable([v]))\n    if DEBUG:\n        pass\n        #print('h', tf.shape(h[i-1]), ', W:', tf.shape(W[i]), ', b:', tf.shape(b[i]))\n\n    h_tmp.append(tf.nn.relu(tf.matmul(h[i-1],W[i]) + b[i]))\n    # dropout\n    h.append(tf.nn.dropout(h_tmp[-1], keep_prob))\n\nif DEBUG:\n    print('\\tnow creating output layer...')\n    print('\\t\\t',NN_HL_ARCH[-1],',',input_size)\n\nW.append(weight_variable([NN_HL_ARCH[-1],input_size]))\nb.append(bias_variable([input_size]))\nif DEBUG:\n    print('\\tsetting up output vector...')\ny = tf.nn.relu(tf.matmul(h[-1],W[-1]) + b[-1])\n\n# \n# ADAM optimiser is a gradient based optimization algorithm, based on adaptive estimates, it's more sophisticated than steepest gradient descent and is well suited for problems with large data or many parameters.\n# cost function\ncross_entropy = -tf.reduce_sum(y_*tf.log(y))\nif DEBUG:\n    print('defining objective function...')\n# but we shall use rmse\nrmse = tf.sqrt(tf.reduce_mean(tf.pow(y-y_, 2)))\n\nif DEBUG:\n    print('defining optimization step...')\n# optimisation function\ntrain_step = tf.train.AdamOptimizer(LEARNING_RATE).minimize(rmse)\n\n# evaluation\nif DEBUG:\n    print('defining optimization step...')\n\n# CHECK\nerror_sq_vector = tf.pow(y - y_,2)\n\n# CHECK\naccuracy = tf.sqrt(tf.reduce_mean(error_sq_vector))\npredict = tf.identity(y)\n\n"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# *Finally neural network structure is defined and TensorFlow graph is ready for training.*\n# ## Train, validate and predict\n# #### Helper functions\n# \n# Ideally, we should use all data for every step of the training, but that's expensive. So, instead, we use small \"batches\" of random data. \n# \n# This method is called [stochastic training](https://en.wikipedia.org/wiki/Stochastic_gradient_descent). It is cheaper, faster and gives much of the same result.\nepochs_completed = 0\nindex_in_epoch = 0\nnum_examples = train_inputs.shape[0]\n\n# serve data by batches\ndef next_batch(batch_size):\n    \n    global train_inputs\n    global train_labels\n    global index_in_epoch\n    global epochs_completed\n    \n    start = index_in_epoch\n    index_in_epoch += batch_size\n    \n    # when all trainig data have been already used, it is reorder randomly    \n    if index_in_epoch > num_examples:\n        # finished epoch\n        epochs_completed += 1\n        # shuffle the data\n        perm = np.arange(num_examples)\n        np.random.shuffle(perm)\n        train_inputs = train_inputs[perm]\n        train_labels = train_labels[perm]\n        # start next epoch\n        start = 0\n        index_in_epoch = batch_size\n        assert batch_size <= num_examples\n    end = index_in_epoch\n    return train_inputs[start:end], train_labels[start:end]\n# Now when all operations for every variable are defined in TensorFlow graph all computations will be performed outside Python environment.\n# start TensorFlow session\ninit = tf.initialize_all_variables()\nsess = tf.InteractiveSession()\n\nsess.run(init)\n# Each step of the loop, we get a \"batch\" of data points from the training set and feed it to the graph to replace the placeholders.  In this case, it's:  *x, y* and *dropout.*\n# \n# Also, once in a while, we check training accuracy on an upcoming \"batch\".\n# \n# On the local environment, we recommend [saving training progress](https://www.tensorflow.org/versions/master/api_docs/python/state_ops.html#Saver), so it can be recovered for further training, debugging or evaluation.\n# visualisation variables\ntrain_accuracies = []\nvalidation_accuracies = []\nx_range = []\n\ndisplay_step=1\n\nfor i in range(TRAINING_ITERATIONS):\n\n    #get new batch\n    batch_xs, batch_ys = next_batch(BATCH_SIZE)        \n\n    # check progress on every 1st,2nd,...,10th,20th,...,100th... step\n    if i%display_step == 0 or (i+1) == TRAINING_ITERATIONS:\n        \n        train_accuracy = accuracy.eval(feed_dict={x:batch_xs, \n                                                  y_: batch_ys, \n                                                  keep_prob: 1.0})       \n        if(VALIDATION_SIZE):\n            validation_accuracy = accuracy.eval(feed_dict={ x: validation_inputs[0:BATCH_SIZE], \n                                                            y_: validation_labels[0:BATCH_SIZE], \n                                                            keep_prob: 1.0})                                  \n            print('training_accuracy / validation_accuracy => %.2f / %.2f for step %d'%(train_accuracy, validation_accuracy, i))\n            \n            validation_accuracies.append(validation_accuracy)\n            \n        else:\n             print('training_accuracy => %.4f for step %d'%(train_accuracy, i))\n        train_accuracies.append(train_accuracy)\n        x_range.append(i)\n        \n        # increase display_step\n        if i%(display_step*10) == 0 and i:\n            display_step *= 10\n    # train on batch\n    sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys, keep_prob: DROPOUT})\n# After training is done, it's good to check accuracy on data that wasn't used in training.\n# check final accuracy on validation set  \nif(VALIDATION_SIZE):\n    validation_accuracy = accuracy.eval(feed_dict={x: validation_inputs, \n                                                   y_: validation_labels, \n                                                   keep_prob: 1.0})\n    print('validation_accuracy => %.4f'%validation_accuracy)\n    plt.plot(x_range, train_accuracies,'-b', label='Training')\n    plt.plot(x_range, validation_accuracies,'-g', label='Validation')\n    plt.legend(loc='lower right', frameon=False)\n    plt.ylim(ymax = 1.1, ymin = 0.7)\n    plt.ylabel('accuracy')\n    plt.xlabel('step')\n    plt.show()\n# When, we're happy with the outcome, we read test data from *test.csv* and predict labels for provided inputs.\n# \n# Test data contains only inputs and labels are missing. Otherwise, the structure is similar to training data.\n# \n# Predicted labels are stored into CSV file for future submission.\n# read test data from CSV file \ntest_inputs = pd.read_csv('../input/test.csv').values\ntest_inputs = test_inputs.astype(np.float)\n\n# convert from [0:255] => [0.0:1.0]\ntest_inputs = normalize(test_inputs) #np.multiply(test_inputs, 1.0 / 255.0)\n\n#print('test_inputs({0[0]},{0[1]})'.format(test_inputs.shape))\n\n\n# predict test set\noutput_rows = predict.eval(feed_dict={x: train_inputs, keep_prob: 1.0})\ntest_rows = predict.eval(feed_dict={x: test_inputs, keep_prob: 1.0})\n'''\n# using batches is more resource efficient\npredicted_lables = np.zeros(test_inputs.shape[0])\nfor i in range(0,test_inputs.shape[0]//BATCH_SIZE):\n    predicted_lables[i*BATCH_SIZE : (i+1)*BATCH_SIZE] = predict.eval(feed_dict={x: test_inputs[i*BATCH_SIZE : (i+1)*BATCH_SIZE], keep_prob: 1.0})\n'''\n\nnp.savetxt('normalized_input_'+tag+'.csv', \n           train_inputs,\n           delimiter=',', \n           header = headertext,\n           comments = '', \n           fmt='%f')\n\nnp.savetxt('normalized_testset_'+tag+'.csv', \n           test_inputs,\n           delimiter=',', \n           header = headertext,\n           comments = '', \n           fmt='%f')\n\n# save results\nnp.savetxt('autoencoded_input_'+tag+'.csv', \n           output_rows, \n           delimiter=',', \n           header = headertext,\n           comments = '', \n           fmt='%f')\n\nnp.savetxt('testset_autoencoded_'+tag+'.csv', \n           test_rows, \n           delimiter=',', \n           header = headertext,\n           comments = '', \n           fmt='%f')\n\n\ndiff = np.array(train_inputs) - np.array(output_rows)\n\nnp.savetxt('diff_'+tag+'.csv', \n           diff, \n           delimiter=',', \n           header = headertext,\n           comments = '', \n           fmt='%f')\n\n\n\n# ## Appendix\n# As it was mentioned before, it is good to output some variables for a better understanding of the process. \n# \n# Here we pull an output of the first convolution layer from TensorFlow graph. 32 features are transformed into an image grid, and it's quite interesting to see how filters picked by NN outline characteristics of different digits.\n\n\n#layer1_grid = layer1.eval(feed_dict={x: test_inputs[IMAGE_TO_DISPLAY:IMAGE_TO_DISPLAY+1], keep_prob: 1.0})\n#plt.axis('off')\n#plt.imshow(layer1_grid[0], cmap=cm.seismic )\n\nsaver = tf.train.Saver()\n# Save the variables to disk.\nsave_path = saver.save(sess, \"./model_\"+tag+\".ckpt\")\nprint(\"Model saved in file: %s\" % save_path)\n\n\nfor w_,b_ in zip(W,b):\n    print(\"W:\")\n    print(w_.eval())\n    print(\"b:\")\n    print(b_.eval())\n\nsess.close()"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":0}