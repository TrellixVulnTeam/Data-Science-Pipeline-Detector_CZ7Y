{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import re\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport pickle\n\nimport cv2\nfrom tqdm import tqdm\nfrom glob import glob\nfrom PIL import Image\nfrom skimage.transform import resize\nfrom sklearn.model_selection import train_test_split, KFold\n\nimport tensorflow as tf\nimport tensorflow.keras\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","id":"V000z_TVFFnk","executionInfo":{"status":"ok","timestamp":1621255093482,"user_tz":-360,"elapsed":100970,"user":{"displayName":"Md. Badiuzzaman Shuvo","photoUrl":"","userId":"15863578466039424430"}},"execution":{"iopub.status.busy":"2021-06-23T16:40:49.369678Z","iopub.execute_input":"2021-06-23T16:40:49.370041Z","iopub.status.idle":"2021-06-23T16:40:49.376326Z","shell.execute_reply.started":"2021-06-23T16:40:49.370011Z","shell.execute_reply":"2021-06-23T16:40:49.375412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.random.set_seed(43)\nnp.random.seed(43)","metadata":{"execution":{"iopub.status.busy":"2021-06-23T16:35:24.137375Z","iopub.execute_input":"2021-06-23T16:35:24.137684Z","iopub.status.idle":"2021-06-23T16:35:24.141865Z","shell.execute_reply.started":"2021-06-23T16:35:24.137656Z","shell.execute_reply":"2021-06-23T16:35:24.140731Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Building the training dataset.\nLet's look at the train image list","metadata":{"id":"H36EHCoUFFoL","jupyter":{"source_hidden":true}}},{"cell_type":"code","source":"import os\n\npath = \"../input/ultrasound-nerve-segmentation/train/\"\nfile_list = os.listdir(path)\nfile_list[:20]","metadata":{"_kg_hide-output":false,"id":"5IQvMMbEFFoO","executionInfo":{"status":"ok","timestamp":1621255093487,"user_tz":-360,"elapsed":100119,"user":{"displayName":"Md. Badiuzzaman Shuvo","photoUrl":"","userId":"15863578466039424430"}},"outputId":"c6a17f14-e33f-486b-dce3-9c0cb142d00c","execution":{"iopub.status.busy":"2021-06-23T16:35:30.396592Z","iopub.execute_input":"2021-06-23T16:35:30.396946Z","iopub.status.idle":"2021-06-23T16:35:30.832203Z","shell.execute_reply.started":"2021-06-23T16:35:30.396913Z","shell.execute_reply":"2021-06-23T16:35:30.831183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Sort the file list in ascending order and seperate it into images and masks**<br/>\nEach file has the form of either \"subject_imageNum.tif\" or \"subject_imageNum_mask.tif\", so we can extract `subject` and `imageNum` from each file name by using regular expression. `\"[0-9]+\"` means to find the first consecutive number.<br/>","metadata":{"id":"w0rkSYxsFFoc"}},{"cell_type":"code","source":"train_image = []\ntrain_mask = glob(path + '*_mask*')\n\nfor i in train_mask:\n    train_image.append(i.replace('_mask', ''))\n        \nprint(train_image[:10],\"\\n\" ,train_mask[:10])","metadata":{"id":"UiEiHaJHFFor","executionInfo":{"status":"ok","timestamp":1621255093492,"user_tz":-360,"elapsed":98834,"user":{"displayName":"Md. Badiuzzaman Shuvo","photoUrl":"","userId":"15863578466039424430"}},"outputId":"a1b2e8a3-3ab5-4103-dd24-6d7096b173fd","execution":{"iopub.status.busy":"2021-06-23T16:35:38.729007Z","iopub.execute_input":"2021-06-23T16:35:38.729322Z","iopub.status.idle":"2021-06-23T16:35:38.766693Z","shell.execute_reply.started":"2021-06-23T16:35:38.729294Z","shell.execute_reply":"2021-06-23T16:35:38.765876Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Display the first image and mask of the first subject.\nimage1 = np.array(Image.open(path+\"1_1.tif\"))\nimage1_mask = np.array(Image.open(path+\"1_1_mask.tif\"))\nimage1_mask = np.ma.masked_where(image1_mask == 0, image1_mask)\n\nfig, ax = plt.subplots(1,3,figsize = (16,12))\nax[0].imshow(image1, cmap = 'gray')\n\nax[1].imshow(image1_mask, cmap = 'gray')\n\nax[2].imshow(image1, cmap = 'gray', interpolation = 'none')\nax[2].imshow(image1_mask, cmap = 'jet', interpolation = 'none', alpha = 0.7)","metadata":{"id":"hjv-upiCFFo3","executionInfo":{"status":"ok","timestamp":1621255095346,"user_tz":-360,"elapsed":98630,"user":{"displayName":"Md. Badiuzzaman Shuvo","photoUrl":"","userId":"15863578466039424430"}},"outputId":"0b3541e6-b0fb-44a0-adfe-f559ec0c38c3","execution":{"iopub.status.busy":"2021-06-23T16:35:44.861542Z","iopub.execute_input":"2021-06-23T16:35:44.86188Z","iopub.status.idle":"2021-06-23T16:35:45.352825Z","shell.execute_reply.started":"2021-06-23T16:35:44.861848Z","shell.execute_reply":"2021-06-23T16:35:45.351917Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, I try to load all image files and store them variables X and y. Afther doing this, I recognize that it takes very much memory.<br/>\nPlease let me know if there are several efficient ways to store image file","metadata":{"id":"fxk0ICSrFFpD"}},{"cell_type":"markdown","source":"## How to deal with train_masks.csv ?","metadata":{"id":"31B8z8M9FFpF"}},{"cell_type":"code","source":"width = 128\nheight = 128","metadata":{"id":"YPvr7aOOFFpQ","executionInfo":{"status":"ok","timestamp":1621255095349,"user_tz":-360,"elapsed":96658,"user":{"displayName":"Md. Badiuzzaman Shuvo","photoUrl":"","userId":"15863578466039424430"}},"execution":{"iopub.status.busy":"2021-06-20T04:54:08.704916Z","iopub.execute_input":"2021-06-20T04:54:08.705278Z","iopub.status.idle":"2021-06-20T04:54:08.709601Z","shell.execute_reply.started":"2021-06-20T04:54:08.705242Z","shell.execute_reply":"2021-06-20T04:54:08.708505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's check that I did well","metadata":{"id":"FQhxK5L0FFps"}},{"cell_type":"markdown","source":"Let's modularize this work.","metadata":{"id":"F21ZRTvPFFpu"}},{"cell_type":"code","source":"from tensorflow.keras.models import Model, load_model\nfrom tensorflow.keras import Input\nfrom tensorflow.keras.layers import Input, Activation, BatchNormalization, Dropout, Lambda, Conv2D, Conv2DTranspose, MaxPooling2D, concatenate,add\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom tensorflow.keras.losses import binary_crossentropy, MSE","metadata":{"id":"Gpbv3Zc4FFqi","executionInfo":{"status":"ok","timestamp":1621255095350,"user_tz":-360,"elapsed":95208,"user":{"displayName":"Md. Badiuzzaman Shuvo","photoUrl":"","userId":"15863578466039424430"}},"execution":{"iopub.status.busy":"2021-06-20T04:54:08.71178Z","iopub.execute_input":"2021-06-20T04:54:08.712121Z","iopub.status.idle":"2021-06-20T04:54:08.721174Z","shell.execute_reply.started":"2021-06-20T04:54:08.712083Z","shell.execute_reply":"2021-06-20T04:54:08.720148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def dice_coef(y_true, y_pred, smooth = 1.0):\n    y_true_f = K.flatten(y_true)\n    y_pred_f = K.flatten(y_pred)\n    intersection = K.sum(y_true_f * y_pred_f)\n    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n\ndef iou(y_true, y_pred, smooth = 1.0):\n    y_true_f = K.flatten(y_true)\n    y_pred_f = K.flatten(y_pred)\n    intersection = K.sum ( y_true_f * y_pred_f) + smooth\n    union = K.sum ( y_true_f + y_pred_f - y_true_f * y_pred_f) + smooth\n\n    return intersection/union\n\ndef iou_loss(y_true, y_pred):\n    return 1 - iou(y_true, y_pred)\n\ndef dice_coef_loss(y_true, y_pred):\n    return 1 - dice_coef(y_true, y_pred)\n\ndef bbox_loss(y_true, y_pred):\n    y_cls = K.sum(y_true, axis=-1)\n    \n    loss = K.mean(K.square(y_true - y_pred), axis=-1)\n    loss = loss * y_cls\n    return loss\n\ndef cgm_loss(y_true, y_pred):\n    y_tr_cls = y_true[:, 0]\n    y_tr_bb = y_true[:, 1:]\n    y_pr_cls = y_pred[:, 0]\n    y_pr_bb = y_pred[:, 1:]\n    \n    return binary_crossentropy(y_tr_cls, y_pr_cls) + MSE(y_tr_bb, y_pr_bb) * y_tr_cls\n\n# From : https://github.com/symoon94/YOLO-keras/blob/master/yolo2/loss.py\ndef box_diou(b_true, b_pred):\n    \"\"\"\n    Calculate DIoU/CIoU loss on anchor boxes\n    Reference Paper:\n        \"Distance-IoU Loss: Faster and Better Learning for Bounding Box Regression\"\n        https://arxiv.org/abs/1911.08287\n    Parameters\n    ----------\n    b_true: GT boxes tensor, shape=(batch, feat_w, feat_h, anchor_num, 4), xywh\n    b_pred: predict boxes tensor, shape=(batch, feat_w, feat_h, anchor_num, 4), xywh\n    use_ciou: bool flag to indicate whether to use CIoU loss type\n    Returns\n    -------\n    diou: tensor, shape=(batch, feat_w, feat_h, anchor_num, 1)\n    \"\"\"\n    b_true_xy = b_true[:, :2]\n    b_true_wh = b_true[:, 2:4]\n    b_true_wh_half = b_true_wh/2.\n    b_true_mins = b_true_xy - b_true_wh_half\n    b_true_maxes = b_true_xy + b_true_wh_half\n\n    b_pred_xy = b_pred[:, :2]\n    b_pred_wh = b_pred[:, 2:4]\n    b_pred_wh_half = b_pred_wh/2.\n    b_pred_mins = b_pred_xy - b_pred_wh_half\n    b_pred_maxes = b_pred_xy + b_pred_wh_half\n\n    intersect_mins = K.maximum(b_true_mins, b_pred_mins)\n    intersect_maxes = K.minimum(b_true_maxes, b_pred_maxes)\n    intersect_wh = K.maximum(intersect_maxes - intersect_mins, 0.)\n    intersect_area = intersect_wh[:, 0] * intersect_wh[:, 1]\n    b_true_area = b_true_wh[:, 0] * b_true_wh[:, 1]\n    b_pred_area = b_pred_wh[:, 0] * b_pred_wh[:, 1]\n    union_area = b_true_area + b_pred_area - intersect_area\n    # calculate IoU, add epsilon in denominator to avoid dividing by 0\n    iou = intersect_area / (union_area + K.epsilon())\n\n    # box center distance\n    center_distance = K.sum(K.square(b_true_xy - b_pred_xy), axis=-1)\n    # get enclosed area\n    enclose_mins = K.minimum(b_true_mins, b_pred_mins)\n    enclose_maxes = K.maximum(b_true_maxes, b_pred_maxes)\n    enclose_wh = K.maximum(enclose_maxes - enclose_mins, 0.0)\n    # get enclosed diagonal distance\n    enclose_diagonal = K.sum(K.square(enclose_wh), axis=-1)\n    # calculate DIoU, add epsilon in denominator to avoid dividing by 0\n    diou = iou - 1.0 * (center_distance) / (enclose_diagonal + K.epsilon())\n    \n    return diou","metadata":{"id":"xyAgRz_tFFqp","executionInfo":{"status":"ok","timestamp":1621255095352,"user_tz":-360,"elapsed":94466,"user":{"displayName":"Md. Badiuzzaman Shuvo","photoUrl":"","userId":"15863578466039424430"}},"execution":{"iopub.status.busy":"2021-06-20T04:54:08.722623Z","iopub.execute_input":"2021-06-20T04:54:08.723039Z","iopub.status.idle":"2021-06-20T04:54:08.741019Z","shell.execute_reply.started":"2021-06-20T04:54:08.723005Z","shell.execute_reply":"2021-06-20T04:54:08.740015Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !rmdir /Q /S ..\\ultrasound\\generated","metadata":{"id":"bQ0sF9KaDmDs","executionInfo":{"status":"ok","timestamp":1621255095354,"user_tz":-360,"elapsed":93191,"user":{"displayName":"Md. Badiuzzaman Shuvo","photoUrl":"","userId":"15863578466039424430"}},"execution":{"iopub.status.busy":"2021-06-20T04:54:08.744001Z","iopub.execute_input":"2021-06-20T04:54:08.74426Z","iopub.status.idle":"2021-06-20T04:54:08.753569Z","shell.execute_reply.started":"2021-06-20T04:54:08.744236Z","shell.execute_reply":"2021-06-20T04:54:08.752827Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pos_mask = []\npos_img = []\nneg_mask = []\nneg_img = []\n\nfor mask_path, img_path in zip(train_mask, train_image):\n    mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n    if np.sum(mask) == 0:\n        neg_mask.append(mask_path)\n        neg_img.append(img_path)\n    else:\n        pos_mask.append(mask_path)\n        pos_img.append(img_path)","metadata":{"id":"zO4WD60TQ_HL","executionInfo":{"status":"ok","timestamp":1621255103334,"user_tz":-360,"elapsed":100393,"user":{"displayName":"Md. Badiuzzaman Shuvo","photoUrl":"","userId":"15863578466039424430"}},"execution":{"iopub.status.busy":"2021-06-20T04:54:08.756922Z","iopub.execute_input":"2021-06-20T04:54:08.757232Z","iopub.status.idle":"2021-06-20T04:54:19.367596Z","shell.execute_reply.started":"2021-06-20T04:54:08.757196Z","shell.execute_reply":"2021-06-20T04:54:19.366751Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!mkdir generated\n!mkdir generated/img","metadata":{"id":"YGNJWh6ZQ_HW","executionInfo":{"status":"ok","timestamp":1621255122738,"user_tz":-360,"elapsed":1208,"user":{"displayName":"Md. Badiuzzaman Shuvo","photoUrl":"","userId":"15863578466039424430"}},"outputId":"d95b3c9a-1b9b-401e-9920-a7eacd76b9d2","execution":{"iopub.status.busy":"2021-06-20T04:54:19.368855Z","iopub.execute_input":"2021-06-20T04:54:19.369176Z","iopub.status.idle":"2021-06-20T04:54:20.657652Z","shell.execute_reply.started":"2021-06-20T04:54:19.369143Z","shell.execute_reply":"2021-06-20T04:54:20.656823Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def flip_up_down(img):\n    newImg = img.copy()\n    return cv2.flip(newImg, 0)\n\ndef flip_right_left(img):\n    newImg = img.copy()\n    return cv2.flip(newImg, 1)","metadata":{"id":"_vN5uEKUjzeK","executionInfo":{"status":"ok","timestamp":1621255123171,"user_tz":-360,"elapsed":1624,"user":{"displayName":"Md. Badiuzzaman Shuvo","photoUrl":"","userId":"15863578466039424430"}},"execution":{"iopub.status.busy":"2021-06-20T04:54:20.66014Z","iopub.execute_input":"2021-06-20T04:54:20.660525Z","iopub.status.idle":"2021-06-20T04:54:20.667216Z","shell.execute_reply.started":"2021-06-20T04:54:20.660486Z","shell.execute_reply":"2021-06-20T04:54:20.66634Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gen_img = []\ngen_mask = []\n\nfor (img_path, mask_path) in tqdm(zip(pos_img, pos_mask)):\n    image_name = img_path.split('/')[-1].split('.')[0]\n\n    uf_img_path = 'generated/img/'+image_name+'_uf.jpg'\n    uf_mask_path = 'generated/img/'+image_name+'_uf_mask.jpg'\n    rf_img_path = 'generated/img/'+image_name+'_rf.jpg'\n    rf_mask_path = 'generated/img/'+image_name+'_rf_mask.jpg'\n\n    img = cv2.imread(img_path)\n    mask = cv2.imread(mask_path)\n\n    uf_img = flip_up_down(img)\n    uf_mask = flip_up_down(mask)\n    cv2.imwrite(uf_img_path, uf_img)\n    cv2.imwrite(uf_mask_path, uf_mask)\n    \n    rf_img = flip_right_left(img)\n    rf_mask = flip_right_left(mask)\n    cv2.imwrite(rf_img_path, rf_img)\n    cv2.imwrite(rf_mask_path, rf_mask)\n    \n    gen_img.append(uf_img_path)\n    gen_mask.append(uf_mask_path)\n    gen_img.append(rf_img_path)\n    gen_mask.append(rf_mask_path)","metadata":{"id":"bL-tRF1DhR-J","executionInfo":{"status":"ok","timestamp":1621255181793,"user_tz":-360,"elapsed":60231,"user":{"displayName":"Md. Badiuzzaman Shuvo","photoUrl":"","userId":"15863578466039424430"}},"outputId":"8b9fc3b2-0f6e-42e3-ed3d-cb8f174efbba","execution":{"iopub.status.busy":"2021-06-20T04:54:20.668789Z","iopub.execute_input":"2021-06-20T04:54:20.669121Z","iopub.status.idle":"2021-06-20T04:55:29.794184Z","shell.execute_reply.started":"2021-06-20T04:54:20.669084Z","shell.execute_reply":"2021-06-20T04:55:29.792498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"intruders = []\n\ntry:\n    with open('intruders.dat', 'rb') as f:\n        intruders = pickle.load(f)\nexcept FileNotFoundError:\n    pos_images = np.array([cv2.imread(i, cv2.IMREAD_GRAYSCALE) for i in pos_img])\n    \n    for im_path in tqdm(neg_img):\n        neg = cv2.imread(im_path, cv2.IMREAD_GRAYSCALE)\n\n        comp = pos_images == neg\n        for i, j in enumerate(comp):\n            if j.all():\n                # print(im_path.replace('\\\\', '\\\\\\\\'))\n                intruders.append(im_path)\n                break\n    \n    with open('intruders.dat', 'wb') as f:\n        pickle.dump(intruders, f)","metadata":{"id":"KRL0dG06DmDw","executionInfo":{"status":"ok","timestamp":1621255814494,"user_tz":-360,"elapsed":692919,"user":{"displayName":"Md. Badiuzzaman Shuvo","photoUrl":"","userId":"15863578466039424430"}},"outputId":"5fb29dd9-9def-4994-a000-b3a1191efe74","execution":{"iopub.status.busy":"2021-06-20T04:55:29.795587Z","iopub.execute_input":"2021-06-20T04:55:29.795937Z","iopub.status.idle":"2021-06-20T05:06:57.523906Z","shell.execute_reply.started":"2021-06-20T04:55:29.795899Z","shell.execute_reply":"2021-06-20T05:06:57.523165Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Throwing away False Negative from dataset\n\ntrain_image_polished = train_image.copy()\ntrain_mask_polished = train_mask.copy()\n\nfor i in intruders:\n    index = train_image.index(i)\n    train_image_polished.pop(index)\n    train_mask_polished.pop(index)","metadata":{"id":"55wsk21UDmDx","executionInfo":{"status":"ok","timestamp":1621255814498,"user_tz":-360,"elapsed":692912,"user":{"displayName":"Md. Badiuzzaman Shuvo","photoUrl":"","userId":"15863578466039424430"}},"execution":{"iopub.status.busy":"2021-06-20T05:06:57.53092Z","iopub.execute_input":"2021-06-20T05:06:57.531634Z","iopub.status.idle":"2021-06-20T05:06:57.538485Z","shell.execute_reply.started":"2021-06-20T05:06:57.531595Z","shell.execute_reply":"2021-06-20T05:06:57.537578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"aug_img = gen_img + train_image_polished\naug_mask = gen_mask + train_mask_polished\n\ndf_ = pd.DataFrame(data={\"filename\": aug_img, 'mask' : aug_mask})\ndf = df_.sample(frac=1).reset_index(drop=True)\n\nkf = KFold(n_splits = 5, shuffle=False)","metadata":{"id":"dXlzh9nwFFq_","executionInfo":{"status":"ok","timestamp":1621255814500,"user_tz":-360,"elapsed":692904,"user":{"displayName":"Md. Badiuzzaman Shuvo","photoUrl":"","userId":"15863578466039424430"}},"execution":{"iopub.status.busy":"2021-06-20T05:06:57.540128Z","iopub.execute_input":"2021-06-20T05:06:57.540701Z","iopub.status.idle":"2021-06-20T05:06:57.572923Z","shell.execute_reply.started":"2021-06-20T05:06:57.540664Z","shell.execute_reply":"2021-06-20T05:06:57.571969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.applications.vgg16 import VGG16\nfrom tensorflow.keras.layers import *\n\ndef res_block(inputs,filter_size):\n    \"\"\"\n    res_block -- Residual block for building res path\n    \n    Arguments:\n    inputs {<class 'tensorflow.python.framework.ops.Tensor'>} -- input for residual block\n    filter_size {int} -- convolutional filter size \n    \n    Returns:\n    add {<class 'tensorflow.python.framework.ops.Tensor'>} -- addition of two convolutional filter output  \n    \"\"\"\n    # First Conv2D layer\n    cb1 = Conv2D(filter_size,(3,3),padding = 'same',activation=\"relu\")(inputs)\n    # Second Conv2D layer parallel to the first one\n    cb2 = Conv2D(filter_size,(1,1),padding = 'same',activation=\"relu\")(inputs)\n    # Addition of cb1 and cb2\n    add = Add()([cb1,cb2])\n    \n    return add\n\n# def res_path(inputs,filter_size,path_number):\n#     \"\"\"\n#     res_path -- residual path / modified skip connection\n    \n#     Arguments:\n#     inputs {<class 'tensorflow.python.framework.ops.Tensor'>} -- input for res path\n#     filter_size {int} -- convolutional filter size \n#     path_number {int} -- path identifier \n    \n#     Returns:\n#     skip_connection {<class 'tensorflow.python.framework.ops.Tensor'>} -- final res path\n#     \"\"\"\n#     # Minimum one residual block for every res path\n#     skip_connection = res_block(inputs, filter_size)\n\n#     # Two serial residual blocks for res path 2\n#     if path_number == 2:\n#         skip_connection = res_block(skip_connection,filter_size)\n    \n#     # Three serial residual blocks for res path 1\n#     elif path_number == 1:\n#         skip_connection = res_block(skip_connection,filter_size)\n#         skip_connection = res_block(skip_connection,filter_size)\n    \n#     return skip_connection\n\ndef decoder_block(inputs, out_channels, depth):\n    \n    \"\"\"\n    decoder_block -- decoder block formation\n    \n    Arguments:\n    inputs {<class 'tensorflow.python.framework.ops.Tensor'>} -- input for decoder block\n    mid_channels {int} -- no. of mid channels \n    out_channels {int} -- no. of out channels\n    \n    Returns:\n    db {<class 'tensorflow.python.framework.ops.Tensor'>} -- returning the decoder block\n    \"\"\"\n    conv_kwargs = dict(\n        activation='relu',\n        padding='same',\n        kernel_initializer='he_normal',\n        data_format='channels_last'  \n    )\n    \n    # UpConvolutional layer\n    db = UpSampling2D((2, 2), interpolation='bilinear')(inputs)\n    #db = concatenate([db, res], axis=3)\n    # First conv2D layer \n    db = Conv2D(out_channels, 3, **conv_kwargs)(db)\n    # Second conv2D layer\n    db = Conv2D(out_channels, 3, **conv_kwargs)(db)\n\n    if depth > 2:\n        # Third conv2D layer\n        db = Conv2D(out_channels, 3, **conv_kwargs)(db)\n\n    return db\n\ndef TransCGUNet(input_size=(512, 512, 1), pruned=False):\n    \"\"\"\n    TransResUNet -- main architecture of TransResUNet\n    \n    Arguments:\n    input_size {tuple} -- size of input image\n    \n    Returns:\n    model {<class 'tensorflow.python.keras.engine.training.Model'>} -- final model\n    \"\"\"\n    \n    # Input \n    inputs = Input(input_size)\n\n    # VGG16 with imagenet weights\n    encoder = VGG16(include_top=False, weights='imagenet', input_shape=input_size)\n       \n    # First encoder block\n    enc1 = encoder.get_layer(name='block1_conv1')(inputs)\n    enc1 = encoder.get_layer(name='block1_conv2')(enc1)\n    enc2 = MaxPooling2D(pool_size=(2, 2))(enc1)\n    \n    # Second encoder block\n    enc2 = encoder.get_layer(name='block2_conv1')(enc2)\n    enc2 = encoder.get_layer(name='block2_conv2')(enc2)\n    enc3 = MaxPooling2D(pool_size=(2, 2))(enc2)\n    \n    # Third encoder block\n    enc3 = encoder.get_layer(name='block3_conv1')(enc3)\n    enc3 = encoder.get_layer(name='block3_conv2')(enc3)\n    enc3 = encoder.get_layer(name='block3_conv3')(enc3)\n    center = MaxPooling2D(pool_size=(2, 2))(enc3)\n\n    # Center block\n    center = Conv2D(512, (3, 3), activation='relu', padding='same', name='center1')(center)\n    center = Conv2D(512, (3, 3), activation='relu', padding='same', name='center2')(center)\n    \n    # classification pred\n    cls = Conv2D(32, (3,3), activation='relu', padding='same')(center)\n    cls = Conv2D(1, (1,1))(cls)\n    cls = GlobalAveragePooling2D()(cls)\n    cls = Activation('sigmoid', name='class')(cls)\n    clsr = Reshape((1, 1, 1), name='reshape')(cls)\n\n    # Decoder block corresponding to third encoder\n    #res_path3 = res_path(enc3,128,3)\n    dec3 = decoder_block(center, 256, 3)\n    \n    # Decoder block corresponding to second encoder\n    #res_path2 = res_path(enc2,64,2)\n    dec2 = decoder_block(dec3, 128, 2)\n    \n    # Final Block concatenation with first encoded feature \n    #res_path1 = res_path(enc1,32,1)\n    dec1 = decoder_block(dec2, 64, 1)\n\n    # Output\n    out = Conv2D(1, 1)(dec1)\n    out = Activation('sigmoid', name='pre')(out)\n    out_2 = multiply(inputs=[out,clsr], name='seg')\n    \n    # Final model\n    if pruned:\n        model = Model(inputs=[inputs], outputs=[out])\n    else:\n        model = Model(inputs=[inputs], outputs=[out_2, cls])\n        # Adding BBox\n        model = add_bbox(model)\n    \n    return model\n\ndef add_bbox(model):\n    # bbox branch\n    cls_ = Conv2D(256, (3,3), activation='relu', padding='same')(model.get_layer('center2').output)\n    cls_ = Conv2D(256, (3,3), activation='relu', padding='same')(cls_)\n    cls_ = MaxPooling2D(pool_size=(2, 2))(cls_)\n\n    cls_ = Conv2D(128, (3,3), activation='relu', padding='same')(cls_)\n    cls_ = Conv2D(128, (3,3), activation='relu', padding='same')(cls_)\n    cls_ = MaxPooling2D(pool_size=(2, 2))(cls_)\n\n    cls_ = Conv2D(64, (3,3), activation='relu', padding='same')(cls_)\n    cls_ = Conv2D(64, (3,3), activation='relu', padding='same')(cls_)\n    cls_ = MaxPooling2D(pool_size=(2, 2))(cls_)\n\n    cls_ = Conv2D(32, (3,3), activation='relu', padding='same')(cls_)\n    cls_ = Conv2D(32, (3,3), activation='relu', padding='same')(cls_)\n\n    bbox = Conv2D(4, (1,1))(cls_)\n    bbox = GlobalAveragePooling2D()(bbox)\n    bbox = Activation('sigmoid', name='bbox')(bbox)\n\n    return Model(inputs=[model.input], outputs=[model.output[0], model.output[1], bbox])","metadata":{"id":"M_d0LZZj3zOz","executionInfo":{"status":"ok","timestamp":1621255815423,"user_tz":-360,"elapsed":693820,"user":{"displayName":"Md. Badiuzzaman Shuvo","photoUrl":"","userId":"15863578466039424430"}},"execution":{"iopub.status.busy":"2021-06-20T05:15:13.46902Z","iopub.execute_input":"2021-06-20T05:15:13.469411Z","iopub.status.idle":"2021-06-20T05:15:13.495602Z","shell.execute_reply.started":"2021-06-20T05:15:13.469369Z","shell.execute_reply":"2021-06-20T05:15:13.494734Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# From: https://github.com/zhixuhao/unet/blob/master/data.py\nfrom tensorflow.keras.applications.vgg16 import preprocess_input\n\ndef train_generator(data_frame, batch_size, train_path, aug_dict,\n        image_color_mode=\"rgb\",\n        mask_color_mode=\"grayscale\",\n        image_save_prefix=\"image\",\n        mask_save_prefix=\"mask\",\n        save_to_dir=None,\n        target_size=(256,256),\n        seed=1):\n    '''\n    can generate image and mask at the same time use the same seed for\n    image_datagen and mask_datagen to ensure the transformation for image\n    and mask is the same if you want to visualize the results of generator,\n    set save_to_dir = \"your path\"\n    '''\n    image_datagen = ImageDataGenerator(**aug_dict)\n    mask_datagen = ImageDataGenerator(**aug_dict)\n    \n    image_generator = image_datagen.flow_from_dataframe(\n        data_frame,\n        directory = train_path,\n        x_col = \"filename\",\n        class_mode = None,\n        color_mode = image_color_mode,\n        target_size = target_size,\n        batch_size = batch_size,\n        save_to_dir = save_to_dir,\n        save_prefix  = image_save_prefix,\n        seed = seed)\n\n    mask_generator = mask_datagen.flow_from_dataframe(\n        data_frame,\n        directory = train_path,\n        x_col = \"mask\",\n        class_mode = None,\n        color_mode = mask_color_mode,\n        target_size = target_size,\n        batch_size = batch_size,\n        save_to_dir = save_to_dir,\n        save_prefix  = mask_save_prefix,\n        seed = seed)\n\n    train_gen = zip(image_generator, mask_generator)\n    \n    for (img, mask) in train_gen:\n        img, mask, label, bbox = adjust_data(img, mask)\n        yield (img, [mask, label, bbox])\n\ndef adjust_data(img,mask):\n    img = preprocess_input(img)\n    \n    bbox = np.zeros((len(img), 4))\n    for i, m in enumerate(mask):\n        m_ = np.array(m, dtype='uint8')\n        _, thresh = cv2.threshold(m_,127,255,0)\n        contours, _ = cv2.findContours(thresh, 1, 2)\n        if len(contours) > 0:\n            cnt = contours[0]\n            x,y,w,h = cv2.boundingRect(cnt)\n\n            bbox[i, :] = x, y, w, h\n        \n#     Assuming height == width\n    bbox /= height\n    \n    mask = mask / 255    \n    mask[mask > 0.5] = 1\n    mask[mask <= 0.5] = 0\n    masks_sum = np.sum(mask, axis=(1,2,3)).reshape((-1, 1))\n    class_lab = (masks_sum != 0) + 0.\n    \n    return (img, mask, class_lab, bbox)","metadata":{"id":"IEX0f8GhFFq3","executionInfo":{"status":"ok","timestamp":1621255815425,"user_tz":-360,"elapsed":693818,"user":{"displayName":"Md. Badiuzzaman Shuvo","photoUrl":"","userId":"15863578466039424430"}},"execution":{"iopub.status.busy":"2021-06-20T05:15:20.737641Z","iopub.execute_input":"2021-06-20T05:15:20.737955Z","iopub.status.idle":"2021-06-20T05:15:20.75005Z","shell.execute_reply.started":"2021-06-20T05:15:20.737925Z","shell.execute_reply":"2021-06-20T05:15:20.749212Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gen_img[0]","metadata":{"id":"3rfELa6YDmD1","executionInfo":{"status":"ok","timestamp":1621255815427,"user_tz":-360,"elapsed":693804,"user":{"displayName":"Md. Badiuzzaman Shuvo","photoUrl":"","userId":"15863578466039424430"}},"outputId":"de8b0636-3f87-4973-c763-489cf3a21919","execution":{"iopub.status.busy":"2021-06-20T05:15:24.48446Z","iopub.execute_input":"2021-06-20T05:15:24.48478Z","iopub.status.idle":"2021-06-20T05:15:24.490186Z","shell.execute_reply.started":"2021-06-20T05:15:24.48475Z","shell.execute_reply":"2021-06-20T05:15:24.489276Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"histories = []\nlosses = []\naccuracies = []\ndicecoefs = []\nious = []\n\ntrain_generator_args = dict(rotation_range=0.2,\n                            width_shift_range=0.05,\n                            height_shift_range=0.05,\n                            shear_range=0.05,\n                            zoom_range=0.05,\n                            horizontal_flip=True,\n                            fill_mode='nearest')\n\nEPOCHS = 120\nBATCH_SIZE = 32\n\nfor k, (train_index, test_index) in enumerate(kf.split(df)):\n    print('\\nFold no. :', k+1)\n    \n    train_data_frame = df.iloc[train_index]\n    test_data_frame = df.iloc[test_index]\n    #continue\n    \n    train_gen = train_generator(train_data_frame, BATCH_SIZE,\n                                None,\n                                train_generator_args,\n                                target_size=(height, width))\n\n    test_gener = train_generator(test_data_frame, BATCH_SIZE,\n                                None,\n                                dict(),\n                                target_size=(height, width))\n\n    model = TransCGUNet(input_size=(height, width, 3))\n    model.summary()\n    \n    model.compile(optimizer=Adam(lr=1e-5),\n                    loss={'seg':dice_coef_loss, 'class':'binary_crossentropy', 'bbox':bbox_loss},\n                    loss_weights={'seg':1, 'class':1, 'bbox':1},\n                    metrics={'seg':[iou, dice_coef, 'binary_accuracy'], 'class':['accuracy'], 'bbox':['accuracy']})\n\n    model_checkpoint = ModelCheckpoint(str(k+1) + '_unet_ner_seg.hdf5', \n                                       verbose=1, \n                                       save_best_only=True)\n\n    history = model.fit(train_gen,\n                          steps_per_epoch=len(train_data_frame) // BATCH_SIZE, \n                          epochs=EPOCHS, \n                          callbacks=[model_checkpoint],\n                          validation_data = test_gener,\n                          validation_steps=len(test_data_frame) // BATCH_SIZE)\n\n    print('\\n\\nTesting the model. Fold :', (k+1), '\\n\\n')\n    model = load_model(str(k+1) + '_unet_ner_seg.hdf5',\n                       custom_objects={'dice_coef_loss': dice_coef_loss, 'iou': iou, 'dice_coef': dice_coef, 'bbox_loss': bbox_loss})\n    test_gen = train_generator(test_data_frame, BATCH_SIZE,\n                                None,\n                                dict(),\n                                target_size=(height, width))\n    results = model.evaluate(test_gen, steps=len(test_data_frame) // BATCH_SIZE)\n    results = dict(zip(model.metrics_names,results))\n\n    histories.append(history)\n    accuracies.append(results['seg_seg_binary_accuracy'])    \n    losses.append(results['seg_loss'])\n    dicecoefs.append(results['seg_seg_dice_coef'])\n    ious.append(results['seg_seg_iou'])\n        \n    break","metadata":{"id":"ZlGRbdQ2FFrG","scrolled":true,"executionInfo":{"status":"ok","timestamp":1621261188205,"user_tz":-360,"elapsed":966,"user":{"displayName":"Md. Badiuzzaman Shuvo","photoUrl":"","userId":"15863578466039424430"}},"outputId":"f2946602-8d94-4746-ff26-b753d0857993","execution":{"iopub.status.busy":"2021-06-20T05:15:26.713651Z","iopub.execute_input":"2021-06-20T05:15:26.713961Z","iopub.status.idle":"2021-06-20T05:16:01.265915Z","shell.execute_reply.started":"2021-06-20T05:15:26.713932Z","shell.execute_reply":"2021-06-20T05:16:01.262518Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pickle\n\nfor h, history in enumerate(histories):\n\n    keys = history.history.keys()\n    fig, axs = plt.subplots(1, len(keys)//2, figsize = (25, 5))\n    fig.suptitle('No. ' + str(h+1) + ' Fold Results', fontsize=30)\n\n    for k, key in enumerate(list(keys)[:len(keys)//2]):\n        training = history.history[key]\n        validation = history.history['val_' + key]\n\n        epoch_count = range(1, len(training) + 1)\n\n        axs[k].plot(epoch_count, training, 'r--')\n        axs[k].plot(epoch_count, validation, 'b-')\n        axs[k].legend(['Training ' + key, 'Validation ' + key])\n                \n    with open(str(h+1) + '_lungs_trainHistoryDict', 'wb') as file_pi:\n        pickle.dump(history.history, file_pi)","metadata":{"id":"JG6XfYBQFFrO","scrolled":true,"outputId":"642d7ebd-c841-4630-849f-ea15bf8dd836","execution":{"iopub.status.busy":"2021-06-20T05:07:01.117956Z","iopub.status.idle":"2021-06-20T05:07:01.118363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('accuracies : ', accuracies)\nprint('losses : ', losses)\nprint('dicecoefs : ', dicecoefs)\nprint('ious : ', ious)\n\nprint('-----------------------------------------------------------------------------')\nprint('-----------------------------------------------------------------------------')\n\nprint('average accuracy : ', np.mean(np.array(accuracies)))\nprint('average loss : ', np.mean(np.array(losses)))\nprint('average dicecoefs : ', np.mean(np.array(dicecoefs)))\nprint('average ious : ', np.mean(np.array(ious)))\nprint()\n\nprint('standard deviation of accuracy : ', np.std(np.array(accuracies)))\nprint('standard deviation of loss : ', np.std(np.array(losses)))\nprint('standard deviation of dicecoefs : ', np.std(np.array(dicecoefs)))\nprint('standard deviation of ious : ', np.std(np.array(ious)))","metadata":{"id":"_EhgaH9kFFrU","outputId":"c9d58d0f-11e8-4589-e1d4-025f744f949c","execution":{"iopub.status.busy":"2021-06-20T05:07:01.11955Z","iopub.status.idle":"2021-06-20T05:07:01.120084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(20):\n    index=np.random.randint(0,len(test_data_frame.index))\n    print(i+1, index)\n    img = cv2.imread(test_data_frame['filename'].iloc[index])\n    img = cv2.resize(img, (height, width))\n    img = preprocess_input(img)\n    img = img[np.newaxis, :, :, :]\n    pred = model.predict(img)\n    pre_pred = Model(model.inputs, model.get_layer('pre').output).predict(img)\n\n    m_ = np.array(cv2.resize(cv2.imread(test_data_frame['mask'].iloc[index], cv2.IMREAD_GRAYSCALE), (height, width)), dtype='uint8')\n    _, thresh = cv2.threshold(m_,127,255,0)\n    contours, _ = cv2.findContours(thresh, 1, 2)\n\n    bbox = np.zeros(shape=4)\n    if len(contours) > 0:\n        cnt = contours[0]\n        x,y,w,h = cv2.boundingRect(cnt)\n\n        bbox[:] = x, y, w, h\n        \n#     Assuming height == width\n    bbox /= height\n\n    plt.figure(figsize=(12,12))\n    plt.subplot(1,6,1)\n    plt.imshow(cv2.resize(cv2.imread(test_data_frame['filename'].iloc[index]), (height, width)))\n    plt.title('Original Image')\n    plt.subplot(1,6,2)\n    plt.imshow(np.squeeze(cv2.resize(cv2.imread(test_data_frame['mask'].iloc[index]), (height, width))))\n    plt.title('Original Mask')\n    plt.subplot(1,6,3)\n    plt.imshow(np.squeeze(pre_pred) > .5)\n    plt.title('Pre-Prediction')\n    plt.subplot(1,6,4)\n    plt.imshow(np.squeeze(pred[0]) > .5)\n    plt.title('Prediction')\n    x, y, w, h = np.array(bbox * height, dtype='int')\n    bb = cv2.resize(cv2.imread(test_data_frame['filename'].iloc[index], cv2.IMREAD_GRAYSCALE), (height, width))\n    bb = cv2.rectangle(bb, (x, y), (x+w, y+h), 1, 2)\n    plt.subplot(1,6,5)\n    plt.imshow(bb)\n    plt.title('BBox_original')\n    x, y, w, h = np.array(pred[2][0] * height, dtype='int')\n    print(pred[1],x,y,w,h)\n    bb = cv2.resize(cv2.imread(test_data_frame['filename'].iloc[index], cv2.IMREAD_GRAYSCALE), (height, width))\n    bb = cv2.rectangle(bb, (x, y), (x+w, y+h), 1, 2)\n    plt.subplot(1,6,6)\n    plt.imshow(bb)\n    plt.title('BBox')\n    plt.show()","metadata":{"scrolled":true,"id":"11T8lIVcDmEC","executionInfo":{"status":"ok","timestamp":1621264878278,"user_tz":-360,"elapsed":29389,"user":{"displayName":"Md. Badiuzzaman Shuvo","photoUrl":"","userId":"15863578466039424430"}},"outputId":"fffa9edb-d2d9-45ca-a891-c22cb00aeb5f","execution":{"iopub.status.busy":"2021-06-20T05:07:01.121095Z","iopub.status.idle":"2021-06-20T05:07:01.121802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tp_cls = tp_seg = tn_cls = tn_seg = fp_cls = fp_seg = fn_cls = fn_seg = tp_mod = tn_mod = fp_mod = fn_mod = 0\n# conf.shape = (gt_cls, cls_cls, seg_cls)\nconf = np.zeros((2,2,2))\nconf2 = np.zeros((2,2,2))\n\n# gt = 0\n# cls = 1\n# seg = 2\n\nfalse_negatives = [] \nfalse_positives = [] \n\nfor i, j in tqdm(zip(train_image_polished, train_mask_polished)):\n    img = cv2.imread(i)\n    mask_img = cv2.imread(j)\n\n    img = cv2.resize(img, (height, width))\n    img = preprocess_input(img)\n    img = img[np.newaxis, :, :, :]\n    pred = model.predict(img)\n    pre_pred = Model(model.inputs, model.get_layer('pre').output).predict(img)\n    \n    if(np.sum(mask_img >= 0.5) == 0):\n        ground_truth = 0\n    else:\n        ground_truth = 1\n\n    # Have to try moving the threshhold value\n    if(pred[1] >= 0.5):\n        cls_pred = 1\n    else:\n        cls_pred = 0\n\n    if(np.sum(pred[0] >= 0.5) == 0):\n        seg_pred = 0\n    else:\n        seg_pred = 1\n\n    if(np.sum(pre_pred[0] >= 0.5) == 0):\n        seg_pre_pred = 0\n    else:\n        seg_pre_pred = 1\n\n    if(ground_truth == 1 and cls_pred == 1):\n        tp_cls = tp_cls+1\n    elif(ground_truth == 0 and cls_pred == 0):\n        tn_cls = tn_cls+1\n    elif(ground_truth == 1 and cls_pred == 0):\n        fn_cls = fn_cls+1\n    else:\n        fp_cls = fp_cls+1\n        \n    if(ground_truth == 1 and seg_pred == 1):\n        tp_seg = tp_seg+1\n    elif(ground_truth == 0 and seg_pred == 0):\n        tn_seg = tn_seg+1\n    elif(ground_truth == 1 and seg_pred == 0):\n        fn_seg = fn_seg+1\n    else:\n        fp_seg = fp_seg+1\n        \n    if(cls_pred == 1 and seg_pred == 1):\n        tp_mod = tp_mod+1\n    elif(cls_pred == 0 and seg_pred == 0):\n        tn_mod = tn_mod+1\n    elif(cls_pred == 1 and seg_pred == 0):\n        fn_mod = fn_mod+1\n        false_negatives.append((i,j))\n    else:\n        fp_mod = fp_mod+1\n        \n    if(seg_pre_pred == 1 and cls_pred == 0):\n        false_positives.append((i,j))\n        \n    conf[ground_truth, cls_pred, seg_pred] += 1\n    conf2[ground_truth, cls_pred, seg_pre_pred] += 1","metadata":{"scrolled":true,"id":"sDwZirnWDmEF","executionInfo":{"status":"ok","timestamp":1621260958985,"user_tz":-360,"elapsed":861989,"user":{"displayName":"Md. Badiuzzaman Shuvo","photoUrl":"","userId":"15863578466039424430"}},"outputId":"bc6de9e6-65f4-40c9-a23a-2e71ec698422","execution":{"iopub.status.busy":"2021-06-20T05:07:01.126891Z","iopub.status.idle":"2021-06-20T05:07:01.127643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Modes\\t', 'TP', 'TN', 'FP', 'FN', sep='\\t')\nprint('Class\\t', tp_cls, tn_cls, fp_cls, fn_cls, sep='\\t')\nprint('Seg\\t', tp_seg, tn_seg, fp_seg, fn_seg, sep='\\t')\nprint('Seg wrt Class', tp_mod, tn_mod, fp_mod, fn_mod, sep='\\t')","metadata":{"scrolled":true,"id":"wX9I7eJiDmEH","executionInfo":{"status":"ok","timestamp":1621260959044,"user_tz":-360,"elapsed":98,"user":{"displayName":"Md. Badiuzzaman Shuvo","photoUrl":"","userId":"15863578466039424430"}},"outputId":"a3396eb2-1eca-46b9-8010-d884649e06f7","execution":{"iopub.status.busy":"2021-06-20T05:07:01.128997Z","iopub.status.idle":"2021-06-20T05:07:01.129726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Pos examples:\\n')\nprint(conf[1])\nprint('Neg exaples:\\n')\nprint(conf[0])","metadata":{"id":"uAZZ9t0yDmEI","executionInfo":{"status":"ok","timestamp":1621260959046,"user_tz":-360,"elapsed":89,"user":{"displayName":"Md. Badiuzzaman Shuvo","photoUrl":"","userId":"15863578466039424430"}},"outputId":"8b21025d-10fb-405e-fb6c-cb3c8767edf2","execution":{"iopub.status.busy":"2021-06-20T05:07:01.130963Z","iopub.status.idle":"2021-06-20T05:07:01.131729Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Pos examples:\\n')\nprint(conf2[1])\nprint('Neg exaples:\\n')\nprint(conf2[0])","metadata":{"id":"T_WAarguUKvL","executionInfo":{"status":"ok","timestamp":1621260960806,"user_tz":-360,"elapsed":1839,"user":{"displayName":"Md. Badiuzzaman Shuvo","photoUrl":"","userId":"15863578466039424430"}},"outputId":"744a9f3a-aa48-4dda-fe3e-c0dfa66812e4","execution":{"iopub.status.busy":"2021-06-20T05:07:01.132878Z","iopub.status.idle":"2021-06-20T05:07:01.133711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!rm -r generated ","metadata":{"id":"F7RpNHfBDmEO","outputId":"e4afc797-f916-4a7a-966b-cc5ef9ff99f4","execution":{"iopub.status.busy":"2021-06-20T05:07:01.152721Z","iopub.status.idle":"2021-06-20T05:07:01.153505Z"},"trusted":true},"execution_count":null,"outputs":[]}]}