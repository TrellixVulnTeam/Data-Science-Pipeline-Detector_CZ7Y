{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nimport sys\nimport random\nimport warnings\n\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n\nfrom tqdm import tqdm\n\nfrom keras.models import Model, load_model\nfrom keras.layers import Input\nfrom keras.layers.core import Dropout, Lambda\nfrom keras.layers.convolutional import Conv2D, Conv2DTranspose\nfrom keras.layers.pooling import MaxPooling2D\nfrom keras.layers.merge import concatenate\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom keras import backend as K\nfrom keras.optimizers import Adam\nimport tensorflow as tf\n\nfrom skimage.io import imread\nfrom skimage.transform import resize","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"TRAIN_PATH = '../input/train/'\nTEST_PATH = '../input/test/'\n\nseed = 42\nrandom.seed = seed\nnp.random.seed = seed\n\ntot_num = 5635\nIMG_HEIGHT = 128\nIMG_WIDTH = 128\n\nfiles = os.listdir(TRAIN_PATH)\nmasks_list = []\nimgs_list = []\n\nfor f in files:\n    if 'mask' in f:\n        masks_list.append(f)\n    else:\n        imgs_list.append(f)\n\nmasks_list = sorted(masks_list)\nimgs_list = sorted(imgs_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = np.zeros((tot_num, IMG_HEIGHT, IMG_WIDTH), dtype=np.float32)\nY_train = np.zeros((tot_num, IMG_HEIGHT, IMG_WIDTH), dtype=np.float32)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_train_one = []\nX_train_one = []\nY_train_zero = []\nX_train_zero = []\n\nfor i, file in tqdm(enumerate(imgs_list), total=len(imgs_list)):\n    img_path = file\n    mask_path = img_path[:-4] + '_mask.tif'\n   \n    mask = imread(TRAIN_PATH + mask_path)\n    mask = resize(mask, (IMG_HEIGHT, IMG_WIDTH), mode='constant', preserve_range=True)\n    \n    img = imread(TRAIN_PATH + img_path)\n    img = resize(img, (IMG_HEIGHT, IMG_WIDTH), mode='constant', preserve_range=True)\n    \n    if mask.any() == False:\n        Y_train_zero.append(mask)\n        X_train_zero.append(img)\n    else:\n        Y_train_one.append(mask)\n        X_train_one.append(img)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fig, ax = plt.subplots(5, 2, figsize=(10, 50))\n# for i in range(5):\n#     ax[i, 0].imshow(X_train_zero[i])\n#     ax[i, 1].imshow(Y_train_zero[i])\n#     print(np.unique(Y_train_zero[i]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fig, ax = plt.subplots(5, 2, figsize=(10, 50))\n# for i in range():\n#     ax[i, 0].imshow(X_train_one[i], 'gray')\n#     Y_train_one = np.array(Y_train_one, dtype='bool')\n# #     Y_train_one[i][Y_train_one]=1\n#     ax[i, 1].imshow(Y_train_one[i], 'gray')\n#     print(np.unique(Y_train_one[i]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_one = np.array(X_train_one)\nY_train_one = np.array(Y_train_one)\nX_train_zero = np.array(X_train_zero)\nY_train_zero = np.array(Y_train_zero)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = []\nY_train = []","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def augmentation(imgs, masks):\n    for img, mask in zip(imgs, masks):\n        img_lr = np.fliplr(img)\n        mask_lr = np.fliplr(mask)\n        img_up = np.flipud(img)\n        mask_up = np.flipud(mask)\n        #img_lr_up = np.flipud(img_lr)\n        #mask_lr_up = np.flipud(mask_lr)\n        #img_up_lr = np.fliplr(img_up)\n        #mask_up_lr = np.fliplr(mask_up)\n        X_train.append(img)\n        Y_train.append(mask)\n        X_train.append(img_lr)\n        Y_train.append(mask_lr)\n        X_train.append(img_up)\n        Y_train.append(mask_up)\n        #X_train.append(img_lr_up)\n        #Y_train.append(mask_lr_up)\n        #X_train.append(img_up_lr)\n        #Y_train.append(mask_up_lr)\n        \naugmentation(X_train_one, Y_train_one)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for img, mask in zip(X_train_zero, Y_train_zero):\n#     X_train.append(img)\n#     Y_train.append(mask)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = np.array(X_train)\nY_train = np.array(Y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# X_train_all = np.concatenate((X_train, X_train_zero[:1000]), axis=0)\n# Y_train_all = np.concatenate((Y_train, Y_train_zero[:1000]), axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_ax = X_train[:,:,:,np.newaxis]/255.\nY_train_ax = Y_train[:,:,:,np.newaxis]/255.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Mask 1로 만들기"},{"metadata":{"trusted":true},"cell_type":"code","source":"def to_one(x):\n    if x==0:\n        return 0\n    else:\n        return 1\n\nto_one = np.vectorize(to_one)\n\nY_train_ax = to_one(Y_train_ax)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"smooth = 1.\n\ndef dice_coef(y_true, y_pred):\n    y_true_f = K.flatten(y_true)\n    y_pred_f = K.flatten(y_pred)\n    intersection = K.sum(y_true_f * y_pred_f)\n    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n\n\ndef dice_coef_loss(y_true, y_pred):\n    return -dice_coef(y_true, y_pred)\n\ndef lovasz_grad(gt_sorted):\n    \"\"\"\n    Computes gradient of the Lovasz extension w.r.t sorted errors\n    See Alg. 1 in paper\n    \"\"\"\n    gts = tf.reduce_sum(gt_sorted)\n    intersection = gts - tf.cumsum(gt_sorted)\n    union = gts + tf.cumsum(1. - gt_sorted)\n    jaccard = 1. - intersection / union\n    jaccard = tf.concat((jaccard[0:1], jaccard[1:] - jaccard[:-1]), 0)\n    return jaccard\n\ndef flatten_binary_scores(scores, labels, ignore=None):\n    \"\"\"\n    Flattens predictions in the batch (binary case)\n    Remove labels equal to 'ignore'\n    \"\"\"\n    scores = tf.reshape(scores, (-1,))\n    labels = tf.reshape(labels, (-1,))\n    if ignore is None:\n        return scores, labels\n    valid = tf.not_equal(labels, ignore)\n    vscores = tf.boolean_mask(scores, valid, name='valid_scores')\n    vlabels = tf.boolean_mask(labels, valid, name='valid_labels')\n    return vscores, vlabels\n\ndef lovasz_hinge_flat(logits, labels):\n    \"\"\"\n    Binary Lovasz hinge loss\n      logits: [P] Variable, logits at each prediction (between -\\infty and +\\infty)\n      labels: [P] Tensor, binary ground truth labels (0 or 1)\n      ignore: label to ignore\n    \"\"\"\n\n    def compute_loss():\n        labelsf = tf.cast(labels, logits.dtype)\n        signs = 2. * labelsf - 1.\n        errors = 1. - logits * tf.stop_gradient(signs)\n        errors_sorted, perm = tf.nn.top_k(errors, k=tf.shape(errors)[0], name=\"descending_sort\")\n        gt_sorted = tf.gather(labelsf, perm)\n        grad = lovasz_grad(gt_sorted)\n        loss = tf.tensordot(tf.nn.relu(errors_sorted), tf.stop_gradient(grad), 1, name=\"loss_non_void\")\n        return loss\n\n    # deal with the void prediction case (only void pixels)\n    loss = tf.cond(tf.equal(tf.shape(logits)[0], 0),\n                   lambda: tf.reduce_sum(logits) * 0.,\n                   compute_loss,\n                   strict=True,\n                   name=\"loss\"\n                   )\n    return loss\n\ndef lovasz_hinge(logits, labels, per_image=True, ignore=None):\n    \"\"\"\n    Binary Lovasz hinge loss\n      logits: [B, H, W] Variable, logits at each pixel (between -\\infty and +\\infty)\n      labels: [B, H, W] Tensor, binary ground truth masks (0 or 1)\n      per_image: compute the loss per image instead of per batch\n      ignore: void class id\n    \"\"\"\n    if per_image:\n        def treat_image(log_lab):\n            log, lab = log_lab\n            log, lab = tf.expand_dims(log, 0), tf.expand_dims(lab, 0)\n            log, lab = flatten_binary_scores(log, lab, ignore)\n            return lovasz_hinge_flat(log, lab)\n        losses = tf.map_fn(treat_image, (logits, labels), dtype=tf.float32)\n        loss = tf.reduce_mean(losses)\n    else:\n        loss = lovasz_hinge_flat(*flatten_binary_scores(logits, labels, ignore))\n    return loss\n\ndef lovasz_softmax(y_true, y_pred):\n  return lovasz_hinge(labels=y_true, logits=y_pred)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"IMG_CHANNELS = 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip3 install git+https://github.com/qubvel/segmentation_models\nfrom segmentation_models import Unet\n\n# model = Unet('densenet121',encorder_weights='imagenet',freeze_encorder=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# model = Unet('densenet121',encorder_weights='imagenet',freeze_encorder=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# N = 1\n\n# base_model = Unet(backbone_name='resnet34', encoder_weights='imagenet')\n\n# inp = Input(shape=(None, None, N))\n# l1 = Conv2D(3, (1, 1))(inp) # map N channels data to 3 channels\n# out = base_model(l1)\n\n# model = Model(inp, out, name=base_model.name)\n# model.compile(optimizer=Adam(lr = 1e-5), loss=dice_coef_loss, metrics=[dice_coef])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"inputs = Input((IMG_HEIGHT, IMG_WIDTH, 1))\n\nconv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\nconv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv1)\npool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n\nconv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(pool1)\nconv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv2)\npool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n\nconv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(pool2)\nconv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv3)\npool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n\nconv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(pool3)\nconv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv4)\npool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n\nconv5 = Conv2D(512, (3, 3), activation='relu', padding='same')(pool4)\nconv5 = Conv2D(512, (3, 3), activation='relu', padding='same')(conv5)\n\nup6 = concatenate([Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(conv5), conv4], axis=3)\nconv6 = Conv2D(256, (3, 3), activation='relu', padding='same')(up6)\nconv6 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv6)\n\nup7 = concatenate([Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(conv6), conv3], axis=3)\nconv7 = Conv2D(128, (3, 3), activation='relu', padding='same')(up7)\nconv7 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv7)\n\nup8 = concatenate([Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(conv7), conv2], axis=3)\nconv8 = Conv2D(64, (3, 3), activation='relu', padding='same')(up8)\nconv8 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv8)\n\nup9 = concatenate([Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(conv8), conv1], axis=3)\nconv9 = Conv2D(32, (3, 3), activation='relu', padding='same')(up9)\nconv9 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv9)\n\nconv10 = Conv2D(1, (1, 1), activation='sigmoid')(conv9)\n\nmodel = Model(inputs=[inputs], outputs=[conv10])\nmodel.compile(optimizer=Adam(lr = 1e-5), loss=lovasz_softmax, metrics=[dice_coef])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results = model.fit(X_train_ax, Y_train_ax, validation_split=0.1, batch_size=8, epochs=18)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\n\nsub = pd.read_csv(\"../input/sample_submission.csv\")\ntest_list = os.listdir(\"../input/test\")\n\nprint(\"The number of test data : \", len(test_list))\n\n# Sort the test set in ascending order.\nreg = re.compile(\"[0-9]+\")\n\ntemp1 = list(map(lambda x: reg.match(x).group(), test_list)) \ntemp1 = list(map(int, temp1))\n\ntest_list = [x for _,x in sorted(zip(temp1, test_list))]\n\ntest_list[:15]\n\nimport cv2\nimport re\n\nX_test = np.empty((len(test_list), IMG_HEIGHT, IMG_WIDTH), dtype = 'float32')\nfor i, item in enumerate(test_list):\n    image = cv2.imread(\"../input/test/\" + item, 0)\n    image = cv2.resize(image, (IMG_HEIGHT, IMG_WIDTH), interpolation = cv2.INTER_AREA)\n    X_test[i] = image\nX_test = X_test[:,:,:,np.newaxis] / 255\n\ny_pred = model.predict(X_test)\n\ndef run_length_enc(label):\n    from itertools import chain\n    x = label.transpose().flatten()\n    y = np.where(x > 0)[0]\n    if len(y) < 10:  # consider as empty\n        return ''\n    z = np.where(np.diff(y) > 1)[0]\n    start = np.insert(y[z+1], 0, y[0])\n    end = np.append(y[z], y[-1])\n    length = end - start\n    res = [[s+1, l+1] for s, l in zip(list(start), list(length))]\n    res = list(chain.from_iterable(res))\n    return ' '.join([str(r) for r in res])\n\nrles = []\nfor i in range(X_test.shape[0]):\n    img = y_pred[i, :, :, 0]\n    img = img > 0.5\n    img = resize(img, (420, 580), preserve_range=True)\n    rle = run_length_enc(img)\n    rles.append(rle)\n    if i % 100 == 0:\n            print('{}/{}'.format(i, X_test.shape[0]), end = \"\\r\")\n            \nsub['pixels'] = rles\nsub.to_csv(\"submission.csv\", index = False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}