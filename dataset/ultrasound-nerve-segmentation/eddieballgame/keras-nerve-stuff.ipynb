{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5c3d1f4c-e96b-5ff0-64d3-b7dd67452840"},"outputs":[],"source":"# -*- coding: utf-8 -*-\n__author__ = 'ZFTurbo: https://kaggle.com/zfturbo'\n\nimport numpy as np\nnp.random.seed(2016)\nimport os\nimport matplotlib.pyplot as plt\nimport glob\nimport cv2\nimport datetime\nimport time\nfrom sklearn.cross_validation import KFold\nfrom keras.models import Sequential\nfrom keras.layers.core import Dense, Dropout, Activation, Flatten\nfrom keras.layers.convolutional import Convolution2D, MaxPooling2D\nfrom keras.optimizers import Adam\nfrom keras.optimizers import SGD\nfrom keras.callbacks import EarlyStopping\nfrom keras.utils import np_utils\nfrom sklearn.metrics import log_loss\n\nIMAGES_TO_READ = 100\n\ndef plot_image(img, title=None):\n    #plt.figure(figsize=(15,20))\n    plt.title(title)\n    plt.imshow(img)\n    plt.show()\n\ndef get_im_cv2(path, img_rows, img_cols):\n    img = cv2.imread(path, 0)\n    resized = cv2.resize(img, (img_cols, img_rows), cv2.INTER_LINEAR)\n    #plot_image(resized)\n    #plot_image(img)\n    return resized\n\n\ndef load_train(img_rows, img_cols):\n    X_train = []\n    X_train_id = []\n    mask_train = []\n    start_time = time.time()\n    images_read = 0\n    \n    print('Read train images')\n    files = glob.glob(\"../input/train/*[0-9].tif\")\n    for fl in files:\n        flbase = os.path.basename(fl)\n        print(\"flbase%s\"%flbase)\n        img = get_im_cv2(fl, img_rows, img_cols)\n        X_train.append(img)\n        X_train_id.append(flbase[:-4])\n        mask_path = \"../input/train/\" + flbase[:-4] + \"_mask.tif\"\n        mask = get_im_cv2(mask_path, img_rows, img_cols)\n        mask_train.append(mask)\n        images_read += 1\n        if images_read >= IMAGES_TO_READ:\n            break\n\n    print('Read train data time: {} seconds'.format(round(time.time() - start_time, 2)))\n    return X_train, mask_train, X_train_id\n\n\ndef load_test(img_rows, img_cols):\n    print('Read test images')\n    files = glob.glob(\"../input/test/*[0-9].tif\")\n    X_test = []\n    X_test_id = []\n    total = 0\n    start_time = time.time()\n    for fl in files:\n        flbase = os.path.basename(fl)\n        img = get_im_cv2(fl, img_rows, img_cols)\n        X_test.append(img)\n        X_test_id.append(flbase[:-4])\n        total += 1\n        if total >= IMAGES_TO_READ:\n            break\n\n    print('Read test data time: {} seconds'.format(round(time.time() - start_time, 2)))\n    return X_test, X_test_id\n\n\ndef rle_encode(img, order='F'):\n    bytes = img.reshape(img.shape[0] * img.shape[1], order=order)\n    runs = []\n    r = 0\n    pos = 1\n    for c in bytes:\n        if c == 0:\n            if r != 0:\n                runs.append((pos, r))\n                pos += r\n                r = 0\n            pos += 1\n        else:\n            r += 1\n\n    if r != 0:\n        runs.append((pos, r))\n        pos += r\n\n    z = ''\n    for rr in runs:\n        z += str(rr[0]) + ' ' + str(rr[1]) + ' '\n    return z[:-1]\n\n\ndef find_best_mask():\n    files = glob.glob(os.path.join(\"..\", \"input\", \"train\", \"*_mask.tif\"))\n    overall_mask = cv2.imread(files[0], cv2.IMREAD_GRAYSCALE)\n    overall_mask.fill(0)\n    overall_mask = overall_mask.astype(np.float32)\n\n    for fl in files:\n        mask = cv2.imread(fl, cv2.IMREAD_GRAYSCALE)\n        overall_mask += mask\n    overall_mask /= 255\n    max_value = overall_mask.max()\n    koeff = 0.5\n    overall_mask[overall_mask < koeff * max_value] = 0\n    overall_mask[overall_mask >= koeff * max_value] = 255\n    overall_mask = overall_mask.astype(np.uint8)\n    return overall_mask\n\n\ndef create_submission(predictions, test_id, info):\n    sub_file = os.path.join('submission_' + info + '_' + str(datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M\")) + '.csv')\n    subm = open(sub_file, \"w\")\n    mask = find_best_mask()\n    encode = rle_encode(mask)\n    subm.write(\"img,pixels\\n\")\n    for i in range(len(test_id)):\n        subm.write(str(test_id[i]) + ',')\n        if predictions[i][1] > 0.5:\n            subm.write(encode)\n        subm.write('\\n')\n    subm.close()\n\n\ndef get_empty_mask_state(mask):\n    out = []\n    for i in range(len(mask)):\n        if mask[i].sum() == 0:\n            out.append(0)\n        else:\n            out.append(1)\n    return np.array(out)\n\n\ndef read_and_normalize_train_data(img_rows, img_cols):\n    train_data, train_target, train_id = load_train(img_rows, img_cols)\n    train_data = np.array(train_data, dtype=np.uint8)\n    train_target = np.array(train_target, dtype=np.uint8)\n    train_data = train_data.reshape(train_data.shape[0], 1, img_rows, img_cols)\n    # Convert to 0 or 1\n    train_target = get_empty_mask_state(train_target)\n    train_target = np_utils.to_categorical(train_target, 2)\n    train_data = train_data.astype('float32')\n    train_data /= 255\n    print('Train shape:', train_data.shape)\n    print(train_data.shape[0], 'train samples')\n    return train_data, train_target, train_id\n\n\ndef read_and_normalize_test_data(img_rows, img_cols):\n    test_data, test_id = load_test(img_rows, img_cols)\n    test_data = np.array(test_data, dtype=np.uint8)\n    test_data = test_data.reshape(test_data.shape[0], 1, img_rows, img_cols)\n    test_data = test_data.astype('float32')\n    print(\"test data%s\"%test_data)\n    test_data /= 255\n    print(\"test data transformed%s\"%test_data)    \n    print('Test shape:', test_data.shape)\n    print(test_data.shape[0], 'test samples')\n    return test_data, test_id\n\n\ndef merge_several_folds_mean(data, nfolds):\n    a = np.array(data[0])\n    for i in range(1, nfolds):\n        a += np.array(data[i])\n    a /= nfolds\n    return a.tolist()\n\n\ndef create_model(img_rows, img_cols):\n    model = Sequential()\n    model.add(Convolution2D(4, 3, 3, border_mode='same', init='he_normal',\n                            input_shape=(1, img_rows, img_cols)))\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(Dropout(0.15))\n\n    model.add(Convolution2D(8, 3, 3, border_mode='same', init='he_normal'))\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(Dropout(0.15))\n\n    model.add(Flatten())\n    model.add(Dense(2))\n    model.add(Activation('softmax'))\n\n    sgd = SGD(lr=1e-3, decay=1e-6, momentum=0.9, nesterov=True)\n    model.compile(optimizer=sgd, loss='categorical_crossentropy')\n    return model\n\n\ndef get_validation_predictions(train_data, predictions_valid):\n    pv = []\n    for i in range(len(train_data)):\n        pv.append(predictions_valid[i])\n    return pv\n\n\ndef getPredScorePercent(train_target, train_id, predictions_valid):\n    perc = 0\n    for i in range(len(train_target)):\n        pred = 1\n        if predictions_valid[i][0] > 0.5:\n            pred = 0\n        real = 1\n        if train_target[i][0] > 0.5:\n            real = 0\n        if real == pred:\n            perc += 1\n    perc /= len(train_target)\n    return perc\n\n\ndef run_cross_validation(nfolds=10):\n    img_rows, img_cols = 32, 32\n    batch_size = 32\n    nb_epoch = 50\n    random_state = 51\n\n    train_data, train_target, train_id = read_and_normalize_train_data(img_rows, img_cols)\n    test_data, test_id = read_and_normalize_test_data(img_rows, img_cols)\n\n    yfull_train = dict()\n    yfull_test = []\n    kf = KFold(len(train_data), n_folds=nfolds, shuffle=True, random_state=random_state)\n    num_fold = 0\n    sum_score = 0\n    for train_index, test_index in kf:\n        model = create_model(img_rows, img_cols)\n        X_train, X_valid = train_data[train_index], train_data[test_index]\n        Y_train, Y_valid = train_target[train_index], train_target[test_index]\n\n        num_fold += 1\n        print('Start KFold number {} from {}'.format(num_fold, nfolds))\n        print('Split train: ', len(X_train), len(Y_train))\n        print('Split valid: ', len(X_valid), len(Y_valid))\n\n        callbacks = [\n            EarlyStopping(monitor='val_loss', patience=2, verbose=0),\n        ]\n        model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch,\n              shuffle=True, verbose=2, validation_data=(X_valid, Y_valid),\n              callbacks=callbacks)\n\n        predictions_valid = model.predict(X_valid, batch_size=batch_size, verbose=1)\n        score = log_loss(Y_valid, predictions_valid)\n        print('Score log_loss: ', score)\n\n        # Store valid predictions\n        for i in range(len(test_index)):\n            yfull_train[test_index[i]] = predictions_valid[i]\n\n        # Store test predictions\n        test_prediction = model.predict(test_data, batch_size=batch_size, verbose=2)\n        yfull_test.append(test_prediction)\n\n    predictions_valid = get_validation_predictions(train_data, yfull_train)\n    score = log_loss(train_target, predictions_valid)\n    print(\"Log_loss train independent avg: \", score)\n\n    print('Final log_loss: {}, rows: {} cols: {} nfolds: {} epoch: {}'.format(score, img_rows, img_cols, nfolds, nb_epoch))\n    perc = getPredScorePercent(train_target, train_id, predictions_valid)\n    print('Percent success: {}'.format(perc))\n\n    info_string = 'loss_' + str(score) \\\n                    + '_r_' + str(img_rows) \\\n                    + '_c_' + str(img_cols) \\\n                    + '_folds_' + str(nfolds) \\\n                    + '_ep_' + str(nb_epoch)\n\n    test_res = merge_several_folds_mean(yfull_test, nfolds)\n    create_submission(test_res, test_id, info_string)\n\n\nif __name__ == '__main__':\n    run_cross_validation(10)"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}