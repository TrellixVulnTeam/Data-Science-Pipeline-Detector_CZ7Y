{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Introduction\n\nThis notebook is for beginners.\n\n**Sorry but there are few explanations yet, so I will write more.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# reference\n# https://github.com/ghmagazine/kagglebook/blob/master/ch01/ch01-01-titanic.py","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Read training data and test data"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/tabular-playground-series-apr-2021/train.csv')\ntest = pd.read_csv('/kaggle/input/tabular-playground-series-apr-2021/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Profiling"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas_profiling","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.profile_report()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Divide the training data into features and objective variables\ntrain_x = train.drop(['Survived'], axis=1)\ntrain_y = train['Survived']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The test data is only features, so you can leave it as it is.\ntest_x = test.copy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Create feature"},{"metadata":{"trusted":true},"cell_type":"code","source":"# remove the variable PassengerId\ntrain_x = train_x.drop(['PassengerId'], axis=1)\ntest_x = test_x.drop(['PassengerId'], axis=1)\n# remove variables Name, Ticket, and Cabin\ntrain_x = train_x.drop(['Name', 'Ticket', 'Cabin'], axis=1)\ntest_x = test_x.drop(['Name', 'Ticket', 'Cabin'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\n# Apply label encoding to each categorical variable\nle = LabelEncoder()\nfor c in ['Sex', 'Embarked']:\n    le.fit(train_x[c].fillna('NA'))    \n    train_x[c] = le.transform(train_x[c].fillna('NA'))\n    test_x[c] = le.transform(test_x[c].fillna('NA'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_x.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Create model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import XGBClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create model and learn with training data\nmodel = XGBClassifier(n_estimators=20, random_state=71)\nmodel.fit(train_x, train_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Output the predicted value of test data with probability\npred = model.predict_proba(test_x)[:, 1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Convert the predicted value of test data to binary\npred_label = np.where(pred > 0.5, 1, 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# submission\nsubmission = pd.DataFrame({'PassengerId': test['PassengerId'], 'Survived': pred_label})\nsubmission.to_csv('submission_00.csv', index=False)\nsubmission","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import log_loss, accuracy_score\nfrom sklearn.model_selection import KFold","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# cross validation\nscores_accuracy = []\nscores_logloss = []\nkf = KFold(n_splits=4, shuffle=True, random_state=71)\nfor tr_idx, va_idx in kf.split(train_x):\n    # divide into training data and validation data\n    tr_x, va_x = train_x.iloc[tr_idx], train_x.iloc[va_idx]\n    tr_y, va_y = train_y.iloc[tr_idx], train_y.iloc[va_idx]\n    # train\n    model = XGBClassifier(n_estimators=20, random_state=71)\n    model.fit(tr_x, tr_y)\n    # predict\n    va_pred = model.predict_proba(va_x)[:, 1]\n    # Calculate the score in the validation data\n    logloss = log_loss(va_y, va_pred)\n    accuracy = accuracy_score(va_y, va_pred > 0.5)\n    # Save the scores\n    scores_logloss.append(logloss)\n    scores_accuracy.append(accuracy)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(scores_logloss)\nprint(scores_accuracy)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Output the average score of each fold\nlogloss = np.mean(scores_logloss)\naccuracy = np.mean(scores_accuracy)\nprint(f'logloss: {logloss:.4f}, accuracy: {accuracy:.4f}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model tuning"},{"metadata":{"trusted":true},"cell_type":"code","source":"import itertools","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Prepare tuning candidate parameters\nparam_space = {\n    'max_depth': [3, 5, 7],\n    'min_child_weight': [1.0, 2.0, 4.0]\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Combination of hyperparameters to search\nparam_combinations = itertools.product(param_space['max_depth'], param_space['min_child_weight'])\n\nparams = []\nscores = []\n# Cross-validation for each combination of parameters\nfor max_depth, min_child_weight in param_combinations:\n    score_folds = []\n    # cross validation\n    kf = KFold(n_splits=4, shuffle=True, random_state=123456)\n    for tr_idx, va_idx in kf.split(train_x):\n        # Divide into training data and validation data\n        tr_x, va_x = train_x.iloc[tr_idx], train_x.iloc[va_idx]\n        tr_y, va_y = train_y.iloc[tr_idx], train_y.iloc[va_idx]\n        # train\n        model = XGBClassifier(n_estimators=20, random_state=71,\n                              max_depth=max_depth, min_child_weight=min_child_weight)\n        model.fit(tr_x, tr_y)\n        # predict\n        va_pred = model.predict_proba(va_x)[:, 1]\n        logloss = log_loss(va_y, va_pred)\n        score_folds.append(logloss)\n    # Average the score for each fold\n    score_mean = np.mean(score_folds)\n    # Save the combination of parameters and the score of it\n    params.append((max_depth, min_child_weight))\n    scores.append(score_mean)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Choose the parameter with the best score\nbest_idx = np.argsort(scores)[0]\nbest_param = params[best_idx]\nprint(f'max_depth: {best_param[0]}, min_child_weight: {best_param[1]}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Create features for logistic regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_x2 = train.drop(['Survived'], axis=1)\ntest_x2 = test.copy()\n\ntrain_x2 = train_x2.drop(['PassengerId'], axis=1)\ntest_x2 = test_x2.drop(['PassengerId'], axis=1)\ntrain_x2 = train_x2.drop(['Name', 'Ticket', 'Cabin'], axis=1)\ntest_x2 = test_x2.drop(['Name', 'Ticket', 'Cabin'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# one-hot encoding\ncat_cols = ['Sex', 'Embarked', 'Pclass']\nohe = OneHotEncoder(categories='auto', sparse=False)\nohe.fit(train_x2[cat_cols].fillna('NA'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a column of one-hot encoding\nohe_columns = []\nfor i, c in enumerate(cat_cols):\n    ohe_columns += [f'{c}_{v}' for v in ohe.categories_[i]]\n\nohe_columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# one-hot encoding\nohe_train_x2 = pd.DataFrame(ohe.transform(train_x2[cat_cols].fillna('NA')), columns=ohe_columns)\nohe_test_x2 = pd.DataFrame(ohe.transform(test_x2[cat_cols].fillna('NA')), columns=ohe_columns)\n# remove unnecessary old variables\ntrain_x2 = train_x2.drop(cat_cols, axis=1)\ntest_x2 = test_x2.drop(cat_cols, axis=1)\n# Join columns of one-hot encoding\ntrain_x2 = pd.concat([train_x2, ohe_train_x2], axis=1)\ntest_x2 = pd.concat([test_x2, ohe_test_x2], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_x2.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_x2.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_x2.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# replace missing values in numeric variables with mean values\n# num_cols = ['Age', 'SibSp', 'Parch', 'Fare']\nnum_cols = ['Age', 'Fare']\nfor col in num_cols:\n    train_x2[col].fillna(train_x2[col].mean(), inplace=True)\n    test_x2[col].fillna(train_x2[col].mean(), inplace=True)\n    \ntrain_x2.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_x2.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install ptitprince","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from ptitprince import RainCloud\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(10, 10))\nRainCloud(data=train_x2, y='Fare', orient='h')\nax.grid()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# change the variable Fare x to log(x+1)\ntrain_x2['Fare'] = np.log1p(train_x2['Fare'])\ntest_x2['Fare'] = np.log1p(test_x2['Fare'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(10, 10))\nRainCloud(data=train_x2, y='Fare', orient='h')\nax.grid()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Ensemble"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# xgboost\nmodel_xgb = XGBClassifier(n_estimators=20, random_state=71)\nmodel_xgb.fit(train_x, train_y)\npred_xgb = model_xgb.predict_proba(test_x)[:, 1]\n# logistic regression model\n# Since the features are different from the xgboost, we created train_x2 and test_x2.\nmodel_lr = LogisticRegression(solver='lbfgs', max_iter=300)\nmodel_lr.fit(train_x2, train_y)\npred_lr = model_lr.predict_proba(test_x2)[:, 1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Take a weighted average of the predicted values\npred = pred_xgb * 0.8 + pred_lr * 0.2\npred_label = np.where(pred > 0.5, 1, 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# submission\nsubmission = pd.DataFrame({'PassengerId': test['PassengerId'], 'Survived': pred_label})\nsubmission","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# submission\nsubmission = pd.DataFrame({'PassengerId': test['PassengerId'], 'Survived': pred_label})\nsubmission.to_csv('submission_01.csv', index=False)\nsubmission","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}