{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 1.Beginner's Friendly notebooks\n\n[Logistic Regression model](https://www.kaggle.com/zhaodianwen/tps-april-3-model-development) by Dianwen (David) Zhao with an accuracy score of 0.79+ ,he is also leading the tournament! If you are a beginner you need to definitely check this notebook.\n\n[For detailed EDA of the dataset](https://www.kaggle.com/dwin183287/tps-apr-2021-eda) by Sharlto Cope, with maximum upvotes in EDA with complete detailed analysis of each feature in the dataset, it will be great for you if you want to know better your dataset features for using better Featuring Engineering Techniques.\n\n[A guide for EDA, mostly Featuring Engineering](https://www.kaggle.com/dimitriirfan/tps-apr-2021-exploratory-data-analysis-baseline) by Dimitri Irfan, one of the most appealing EDA I have seen with detailed conclusion at the end of the notebook, if you are in a hurry and probably just want to know what to do with the dataset, just have a look into this notebook.","metadata":{}},{"cell_type":"markdown","source":"# 2.Hyper-Parameter Tuning using OPTUNA\n\n[Ultimate Guide for Paramater tuning using OPTUNA for beginners](https://www.kaggle.com/hamzaghanmi/tps-apr-hyperparameter-tuning-using-optuna) by Hamza. Anything less than a beginners treasure book guide for parameter tuning will be an absolute insult to this beautiful gem, if you ever think of starting tuning your parameter using OPTUNA and you know nothing about OPTUNA, Voila! you will need this gem. You can also check this two notebooks [XGB-hyp-par-Tuning](https://www.kaggle.com/hamzaghanmi/xgboost-hyperparameter-tuning-using-optuna) and [LGBM-hyp-par-Tuning](https://www.kaggle.com/hamzaghanmi/lgbm-hyperparameter-tuning-using-optuna).\n\n\n","metadata":{}},{"cell_type":"markdown","source":"# 3.Stacking and Blending Notebooks\n\n \"Stacking is an ensemble machine learning technique. It involves combining the predictions from multiple machine learning models on the same dataset.  The overall idea of stacking is to train several models, usually with different algorithm types (aka base-learners), on the train data, and then rather than picking the best model, all the models are aggregated/fronted using another model (meta learner), to make the final prediction. The inputs for the meta-learner is the prediction outputs of the base-learners\".\n\nHere is an easy to understand [Stacking notebook](https://www.kaggle.com/tarushi89/tps-april-your-baseline-model) by Tarushi Pathak\n\n  \"Blending is an ensemble machine learning technique that uses a machine learning model to learn how to best combine the predictions from multiple contributing ensemble member models\"\n\nSounds a bit confusing between 'Stacking' & 'Blending'. Here is a great [article](https://machinelearningmastery.com/blending-ensemble-machine-learning-with-python/) to have a clear grasp of the two.\n\nEveryone is using this [Easy Blending solution of the Public Leaderboard](https://www.kaggle.com/manwithaflower/lazy-and-boring-blending) to get better accuracy\n\nAlso, you can use \"Voting\" , it is also ensemble machine learning technique like above two. To get started from the very basics you can read this article by [machinelearningmastery.com](https://machinelearningmastery.com/voting-ensembles-with-python/)","metadata":{}},{"cell_type":"markdown","source":"# 4.Auto ML Notebooks\n\nSo, here is the catch, so there are lots of AutoML libraries you can use to build yourself an ML Pipeline and to know which ML Algorithm may work better.\nSome of the best AutoML notebooks to get started in this competition are - \n\n[LightAutoML](https://www.kaggle.com/alexryzhkov/n3-tps-april-21-lightautoml-starter) by Alexander Ryzhkov\n\n[H20 AutoML](https://www.kaggle.com/tunguz/apr-21-tps-h2o-automl) by Bojan Tunguz\n\nTo build yourself a ML Pipeline, [EvalML](https://www.kaggle.com/saurabhshahane/eval-ml-automl) by Saurabh Shahane would be great to start with.\n\nIf you are a beginner, I would suggest you to use [Auto-Sklearn](https://automl.github.io/auto-sklearn/master/) as it is more easy to understand what's going on!\nHere are some reference videos for you to begin with [Auto-Sklearn] by [Krish Naik](https://www.youtube.com/watch?v=uMWJls5Roqs&t=287s)\nand [AIEngineering](https://www.youtube.com/watch?v=SMUNVfbr3Mc&t=146s)","metadata":{}},{"cell_type":"markdown","source":"# 5.Probably a HOT TOPIC of Discussion Forum\n\n\"Pseudo Labelling\" [Caution: If you are a beginner may be this is not for you, I repeat in clauses 'MAY BE']\n\nStill you can have a look in this [Pseudo Labelling Notebook](https://www.kaggle.com/hiro5299834/tps-apr-2021-voting-pseudo-labeling) by BIZEN ","metadata":{}},{"cell_type":"markdown","source":"These notebooks are all very good and will provide you enough knowledge about the dataset to get started with the Competition. Personally, I have followed these notebooks to get a better accuracy score and learned some techniques that are my go to starting approach to every tabular tabular data problem.","metadata":{}}]}