{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Support Vector Machines**\n\nI have not seen anyone using a Support Vector Classifier in this competition yet, so how about we try it and see how it performs? üëÄ\n\nIf this method is new to you, I strongly recommend the following videos from the great **StatQuest** channel. They will give you a good grasp of what is going on.\n\n* [Support Vector Machines: Main Ideas!!!](https://www.youtube.com/watch?v=efR1C6CvhmE)\n* [Support Vector Machines: The Polynomial Kernel](https://www.youtube.com/watch?v=Toet3EiSFcM)\n* [Support Vector Machines: The Radial (RBF) Kernel](https://www.youtube.com/watch?v=Qc5IyLW_hns)\n\nIf you are curious about other ML techniques as well, check the other videos in the channel. They are clear and also funny. BAM!!! üòÉ","metadata":{}},{"cell_type":"markdown","source":"### Updates\n\n**Version 4**: added 10-fold cross validation.\n\n**Version 5**: added *scikit-learn-intelex* extension, reccomended by @napetrov (see [this notebook](https://www.kaggle.com/napetrov/tps04-svm-with-intel-extension-for-scikit-learn)).","metadata":{}},{"cell_type":"markdown","source":"## Enabling scikit-learn-intelex","metadata":{}},{"cell_type":"code","source":"!pip install scikit-learn-intelex --progress-bar off >> /tmp/pip_sklearnex.log\nfrom sklearnex import patch_sklearn\npatch_sklearn()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load libraries and data","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n\nfrom sklearn.svm import LinearSVC\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import SGDClassifier\n\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import accuracy_score\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import train_test_split","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('../input/tabular-playground-series-apr-2021/train.csv', index_col='PassengerId')\ntest = pd.read_csv('../input/tabular-playground-series-apr-2021/test.csv', index_col='PassengerId')\nsubmission = pd.read_csv('../input/tabular-playground-series-apr-2021/sample_submission.csv', index_col='PassengerId')\n\ntarget = train.pop('Survived')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"By glancing at the train dataframe, it seems like we have missing data and some features that we may not need.","metadata":{}},{"cell_type":"code","source":"train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Preprocessing\n\nThe features **Name**, **Ticket** and **Cabin** don't seem to be useful, so let's drop them.","metadata":{}},{"cell_type":"code","source":"train.drop(['Name', 'Ticket', 'Cabin'], axis=1, inplace=True)\ntest.drop(['Name', 'Ticket', 'Cabin'], axis=1, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let's see if we have missing data.","metadata":{}},{"cell_type":"code","source":"train.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We do. Here we take a simple approach and just fill **Age** and **Fare** with their median value (using the train data). For **Embarked**, we fill the NAs with its most frequent value, which is 'S'.","metadata":{}},{"cell_type":"code","source":"test_prepared = test.copy()\ntrain_prepared = train.copy()\n\ntest_prepared['Age'].fillna((train['Age'].median()), inplace=True)\ntrain_prepared['Age'].fillna((train['Age'].median()), inplace=True)\n\ntest_prepared['Fare'].fillna((train['Fare'].median()), inplace=True)\ntrain_prepared['Fare'].fillna((train['Fare'].median()), inplace=True)\n\ntest_prepared['Embarked'].fillna('S', inplace=True)\ntrain_prepared['Embarked'].fillna('S', inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_prepared.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_prepared.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now the data is complete. Let's encode the categorical variables **Pclass**, **Sex** and **Embarked**.","metadata":{}},{"cell_type":"code","source":"for col in ['Pclass', 'Sex', 'Embarked']:\n    le = LabelEncoder()\n    le.fit(train_prepared[col])\n    train_prepared[col] = le.transform(train_prepared[col])\n    test_prepared[col] = le.transform(test_prepared[col])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_prepared.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_prepared.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The last preprocessing step will be scaling all variables because Support Vector Machines do not work well with variables in different scales.","metadata":{}},{"cell_type":"code","source":"train_prepared_scaled = train_prepared.copy()\ntest_prepared_scaled = test_prepared.copy()\n\nscaler = StandardScaler()\nscaler.fit(train_prepared)\ntrain_prepared_scaled = scaler.transform(train_prepared_scaled)\ntest_prepared_scaled = scaler.transform(test_prepared_scaled)\n\ntrain_prepared_scaled = pd.DataFrame(train_prepared_scaled, columns=train_prepared.columns)\ntest_prepared_scaled = pd.DataFrame(test_prepared_scaled, columns=train_prepared.columns)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_prepared_scaled.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We are good to go! :)\n\nLet's split the train set into 90% training and 10% validation data.","metadata":{}},{"cell_type":"code","source":"X_train, X_valid, y_train, y_valid = train_test_split(train_prepared_scaled, target, test_size=0.1, random_state=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# SVC with linear kernel\n\nIf we want to use a linear kernel, `LinearSVC` is the recommended class because of its speed. Here it runs in a fraction of a second!","metadata":{}},{"cell_type":"code","source":"%%time\nlinear_svc = LinearSVC(random_state=0, C=0.01, loss='hinge')\nlinear_svc.fit(X_train, y_train)\ny_pred = linear_svc.predict(X_valid)\naccuracy_score(y_pred, y_valid)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nfinal_pred = linear_svc.predict(test_prepared_scaled)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The Public LB score for the following submission file is **0.78505**. After some investigation I realized that this classifier is pratically using only the variable **Sex** for classification. Specifically, it predicts that all women survive and all men die. So the SVC with a linear kernel does not help us at all. But hey, we had to try it!","metadata":{}},{"cell_type":"code","source":"submission['Survived'] = np.round(final_pred).astype(int)\nsubmission.to_csv('svc_kernel_linear.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# SVC with RBF kernel\n\nLet's now use the *kernel trick* with an RBF kernel. For this, we need to use the class SVC and set `kernel='rbf'`. The parameter `C` did not have an influence for some of the values I tested, but it ran faster when I set it to low values, so that's why I chose 0.01.\n\nBefore Version 5 of this notebook, the following cell would take 10 minutes to run. With **scikit-learn-intelex** enabled, it takes less than a minute.","metadata":{}},{"cell_type":"code","source":"%%time\nsvc_kernel_rbf = SVC(kernel='rbf', random_state=0, C=0.01)\nsvc_kernel_rbf.fit(X_train, y_train)\ny_pred = svc_kernel_rbf.predict(X_valid)\naccuracy_score(y_pred, y_valid)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Prediction is now very fast with scikit-learn-intelex. Without it, the followging cell would take about 5 minutes.","metadata":{}},{"cell_type":"code","source":"%%time\nfinal_pred = svc_kernel_rbf.predict(test_prepared_scaled)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The Public LB score for the following submission file is **0.79062**, \"which is an improvement from our previous score\". üòè","metadata":{}},{"cell_type":"code","source":"submission['Survived'] = np.round(final_pred).astype(int)\nsubmission.to_csv('svc_kernel_rbf.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# SVC with polynomial kernel\n\nAnother popular choice is to use a polynomial kernel. Here I tried a 3rd degree polynomial, but other values could be used as well. \n\nThe Extension **scikit-learn-intelex** did not speed up the computations in this case. The following cell will take several minutes to run.","metadata":{}},{"cell_type":"code","source":"%%time\nsvc_kernel_poly_3 = SVC(kernel='poly', degree=3, random_state=0, C=0.01)\nsvc_kernel_poly_3.fit(X_train, y_train)\ny_pred = svc_kernel_poly_3.predict(X_valid)\naccuracy_score(y_pred, y_valid)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Prediction will also take a few minutes here.","metadata":{}},{"cell_type":"code","source":"%%time\nfinal_pred = svc_kernel_poly_3.predict(test_prepared_scaled)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The Public LB score for the following submission file is **0.76709**, \"which is *not* an improvement from our previous score\". In fact, it's even worse than what we obtained with a linear kernel. üòê","metadata":{}},{"cell_type":"code","source":"submission['Survived'] = np.round(final_pred).astype(int)\nsubmission.to_csv('svc_kernel_poly_3.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Final submission with 10 folds\n\nTo finish off, let's predict survival with 10-fold cross-validation using the RBF kernel. The following cell will take over 2 hours to run.","metadata":{}},{"cell_type":"code","source":"%%time\nn_folds = 10\nkf = KFold(n_splits=n_folds, shuffle=True, random_state=0)\ny_pred = np.zeros(test.shape[0])\n\nfor fold, (train_index, valid_index) in enumerate(kf.split(train_prepared_scaled, target)):\n    print(\"Running Fold {}\".format(fold + 1))\n    X_train, X_valid = pd.DataFrame(train_prepared_scaled.iloc[train_index]), pd.DataFrame(train_prepared_scaled.iloc[valid_index])\n    y_train, y_valid = target.iloc[train_index], target.iloc[valid_index]\n    svc_kernel_rbf = SVC(kernel='rbf', random_state=0, C=0.01)\n    svc_kernel_rbf.fit(X_train, y_train)\n    print(\"  Accuracy: {}\".format(accuracy_score(y_valid, svc_kernel_rbf.predict(X_valid))))\n    y_pred += svc_kernel_rbf.predict(test_prepared_scaled)\n\ny_pred /= n_folds\n\nprint(\"\")\nprint(\"Done!\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The Public LB score for the following submission file is **0.79062**, the same obtained with only one split.","metadata":{}},{"cell_type":"code","source":"submission['Survived'] = np.round(y_pred).astype(int)\nsubmission.to_csv('svc_kernel_rbf_10_folds.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Conclusion\n\nI found it interesting that the Support Vector Classifier with a linear kernel seems to be influenced only by one of the variables. If you have an explanation (or a guess) for this fact, please leave a comment.\n\nThis was the first time that I used this technique and it was fun to play with it. Hope you enjoyed this brief notebook too! üòâ","metadata":{}}]}