{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Voting classifier**\n\nIn this notebook, the following algorithms/methods are used:\n\n* Support Vector Machines\n* Logistic Regression\n* Random Forest\n* XGBoost\n* LGBM\n* CatBoost\n* Neural Network\n\nFor each one of them, an output file was submitted for evaluation and thre LB scores were noted. Then, two voting classifiers were used to combine all results:\n\n* one with **hard voting** using the binary outcomes of all agorithms and choosing the majority classification for each test instance\n* one with **soft voting** using the probability outcomes of all algorithms, computing an average and rounding the result for each test instance\n\nThe results show that both voting classifiers outperform all individual results. Also, hard voting results in a better LB score than soft voting.","metadata":{}},{"cell_type":"markdown","source":"# Load libraries and data","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import accuracy_score\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.model_selection import train_test_split","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('../input/tabular-playground-series-apr-2021/train.csv', index_col='PassengerId')\ntest = pd.read_csv('../input/tabular-playground-series-apr-2021/test.csv', index_col='PassengerId')\nsubmission = pd.read_csv('../input/tabular-playground-series-apr-2021/sample_submission.csv', index_col='PassengerId')\n\ntarget = train.pop('Survived')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Preprocessing\n\nBased on [this notebook](https://www.kaggle.com/ekozyreff/tps-2021-04-support-vector-machines).","metadata":{}},{"cell_type":"code","source":"train.drop(['Name', 'Ticket', 'Cabin'], axis=1, inplace=True)\ntest.drop(['Name', 'Ticket', 'Cabin'], axis=1, inplace=True)\n\ntest['Age'].fillna((train['Age'].median()), inplace=True)\ntrain['Age'].fillna((train['Age'].median()), inplace=True)\n\ntest['Fare'].fillna((train['Fare'].median()), inplace=True)\ntrain['Fare'].fillna((train['Fare'].median()), inplace=True)\n\ntest['Fare'] = test['Fare'].map(lambda i: np.log(i) if i > 0 else 0)\ntrain['Fare'] = train['Fare'].map(lambda i: np.log(i) if i > 0 else 0)\n\ntest['FamilySize'] = test['SibSp'] + test['Parch'] + 1\ntrain['FamilySize'] = train['SibSp'] + train['Parch'] + 1\n\ntest['Embarked'].fillna('S', inplace=True)\ntrain['Embarked'].fillna('S', inplace=True)\n\nfor col in ['Pclass', 'Sex', 'Embarked']:\n    le = LabelEncoder()\n    le.fit(train[col])\n    test[col] = le.transform(test[col])\n    train[col] = le.transform(train[col])    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_valid, y_train, y_valid = train_test_split(train, target, test_size=0.1, random_state=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For SVM, it is recommended that the input be scaled.","metadata":{}},{"cell_type":"code","source":"X_train_scaled = X_train.copy()\nX_valid_scaled = X_valid.copy()\ntest_scaled = test.copy()\n\nscaler = StandardScaler()\nscaler.fit(train)\nX_train_scaled = scaler.transform(X_train)\nX_valid_scaled = scaler.transform(X_valid)\ntest_scaled = scaler.transform(test)\n\nX_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns)\nX_valid_scaled = pd.DataFrame(X_valid_scaled, columns=X_valid.columns)\ntest_scaled = pd.DataFrame(test_scaled, columns=test.columns)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# SVM with RBF kernel\n\nIn order to the probabilities associated with each prediction, we need to set `probability=True` here, and this makes the algorithm much slower. The following cell takes approximately 30 minutes to run. \n\nFor the other methods, this is not necessary and we can use directly `predict_proba`.","metadata":{}},{"cell_type":"code","source":"%%time\nsvc_kernel_rbf = SVC(kernel='rbf', random_state=0, C=0.01, probability=True)\nsvc_kernel_rbf.fit(X_train_scaled, y_train)\ny_pred = svc_kernel_rbf.predict(X_valid_scaled)\nprint(\"Accuracy: {}\".format(accuracy_score(y_pred, y_valid)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nsvc_kernel_rbf_final_pred_probs = svc_kernel_rbf.predict_proba(test_scaled)[:,1]\nsvc_kernel_rbf_final_pred_binary = svc_kernel_rbf.predict(test_scaled)\nsubmission['Survived'] = svc_kernel_rbf_final_pred_binary\nsubmission.to_csv('svm_kernel_rbf.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Public LB score: **0.78642**.","metadata":{}},{"cell_type":"markdown","source":"# Logistic Regression","metadata":{}},{"cell_type":"code","source":"%%time\nlog_reg = LogisticRegression(random_state=0)\nlog_reg.fit(X_train_scaled, y_train)\ny_pred = log_reg.predict(X_valid_scaled)\nprint(\"Accuracy: {}\".format(accuracy_score(y_pred, y_valid)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nlog_reg_final_pred_probs = log_reg.predict_proba(test_scaled)[:,1]\nlog_reg_final_pred_binary = log_reg.predict(test_scaled)\nsubmission['Survived'] = log_reg_final_pred_binary\nsubmission.to_csv('logistic_regression.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Public LB score: **0.79341**.","metadata":{}},{"cell_type":"markdown","source":"# Random Forest","metadata":{}},{"cell_type":"code","source":"%%time\nrandom_forest = RandomForestClassifier(random_state=0, n_estimators=1000, max_features=2, min_samples_split=0.1)\nrandom_forest.fit(X_train, y_train)\ny_pred = random_forest.predict(X_valid)\nprint(\"Accuracy: {}\".format(accuracy_score(y_pred, y_valid)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nrandom_forest_final_pred_probs = random_forest.predict_proba(test)[:,1]\nrandom_forest_final_pred_binary = random_forest.predict(test)\nsubmission['Survived'] = random_forest_final_pred_binary\nsubmission.to_csv('random_forest.csv')","metadata":{"_kg_hide-output":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Public LB score: **0.79506**.","metadata":{}},{"cell_type":"markdown","source":"# XGBoost","metadata":{}},{"cell_type":"code","source":"%%time\nxgboost = XGBClassifier(random_state=0, n_estimators=1000, use_label_encoder=False, eval_metric='logloss')\nxgboost.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], early_stopping_rounds=10, verbose=False)\ny_pred = xgboost.predict(X_valid)\nprint(\"Accuracy: {}\".format(accuracy_score(y_pred, y_valid)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nxgboost_final_pred_probs = xgboost.predict_proba(test)[:,1]\nxgboost_final_pred_binary = xgboost.predict(test)\nsubmission['Survived'] = xgboost_final_pred_binary\nsubmission.to_csv('xgboost.csv')","metadata":{"_kg_hide-output":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Public LB score: **0.78247**.","metadata":{}},{"cell_type":"markdown","source":"# LGBM","metadata":{}},{"cell_type":"code","source":"%%time\nlgbm = LGBMClassifier(random_state=0, n_estimators=1000)\nlgbm.fit(X_train, y_train, eval_set=(X_valid, y_valid), eval_metric='logloss', early_stopping_rounds=10, verbose=0)\ny_pred = lgbm.predict(X_valid)\nprint(\"Accuracy: {}\".format(accuracy_score(y_pred, y_valid)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nlgbm_final_pred_probs = lgbm.predict_proba(test)[:,1]\nlgbm_final_pred_binary = lgbm.predict(test)\nsubmission['Survived'] = lgbm_final_pred_binary\nsubmission.to_csv('lgbm.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Public LB score: **0.78711**.","metadata":{}},{"cell_type":"markdown","source":"# CatBoost","metadata":{}},{"cell_type":"code","source":"%%time\ncatboost = CatBoostClassifier(random_state=0, n_estimators=1000)\ncatboost.fit(X_train, y_train, eval_set=(X_valid, y_valid), verbose=False, early_stopping_rounds=10)\ny_pred = catboost.predict(X_valid)\nprint(\"Accuracy: {}\".format(accuracy_score(y_pred, y_valid)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ncatboost_final_pred_probs = catboost.predict_proba(test)[:,1]\ncatboost_final_pred_binary = catboost.predict(test)\nsubmission['Survived'] = catboost_final_pred_binary\nsubmission.to_csv('catboost.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Public LB score: **0.78550**.","metadata":{}},{"cell_type":"markdown","source":"# Neural Network","metadata":{}},{"cell_type":"code","source":"tf.random.set_seed(0)\n\nearly_stopping = keras.callbacks.EarlyStopping(\n    patience = 10,\n    min_delta = 0.001,\n    restore_best_weights = True,\n)\n\nneural_net = keras.Sequential([\n    layers.Dense(units=100, activation='relu', input_shape=[X_train_scaled.shape[1]]),\n    layers.Dropout(rate=0.3),\n    layers.BatchNormalization(),\n    layers.Dense(units=100, activation='relu'),\n    layers.Dropout(rate=0.3),\n    layers.BatchNormalization(),\n    layers.Dense(units=50, activation='relu'),\n    layers.BatchNormalization(),\n    layers.Dense(units=1, activation='sigmoid'),\n])\nneural_net.compile(\n    optimizer=\"adam\",\n    loss=\"binary_crossentropy\",\n    metrics = ['binary_accuracy']\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nhistory = neural_net.fit(X_train, y_train,\n                     validation_data = (X_valid, y_valid),\n                     batch_size = 512,\n                     epochs = 50,\n                     callbacks = [early_stopping],\n                    )","metadata":{"_kg_hide-input":false,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nneural_net_final_pred_probs = neural_net.predict(test).reshape(100000,)\nneural_net_final_pred_binary = np.round(neural_net_final_pred_probs).astype(int).reshape(100000,)\nsubmission['Survived'] = neural_net_final_pred_binary\nsubmission.to_csv('neural_net.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Public LB score: **0.79284**.","metadata":{}},{"cell_type":"markdown","source":"# Hard voting classifier","metadata":{}},{"cell_type":"code","source":"binary_average = np.mean([svc_kernel_rbf_final_pred_binary,\n                          log_reg_final_pred_binary,\n                          random_forest_final_pred_binary,\n                          xgboost_final_pred_binary,\n                          lgbm_final_pred_binary,\n                          catboost_final_pred_binary,\n                          neural_net_final_pred_binary], axis=0)\n\nhard_classifier_predictions = np.round(binary_average).astype(int)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission['Survived'] = hard_classifier_predictions\nsubmission.to_csv('hard_voting_classifier.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Public LB score: **0.79692**.","metadata":{}},{"cell_type":"markdown","source":"# Soft voting classifier","metadata":{}},{"cell_type":"code","source":"probs_average = np.mean([svc_kernel_rbf_final_pred_probs,\n                          log_reg_final_pred_probs,\n                          random_forest_final_pred_probs,\n                          xgboost_final_pred_probs,\n                          lgbm_final_pred_probs,\n                          catboost_final_pred_probs,\n                          neural_net_final_pred_probs], axis=0)\n\nsoft_classifier_predictions = np.round(probs_average).astype(int)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission['Survived'] = soft_classifier_predictions\nsubmission.to_csv('soft_voting_classifier.csv')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Public LB score: **0.79603**.","metadata":{}},{"cell_type":"markdown","source":"## Summary of results\n\n| Algorithm               | LB score |\n| --- | --- |\n| Support Vector Machines    | 0.78642 |\n| Logistic Regression        | 0.79341 |\n| Random Forest              | 0.79506 |\n| XGBoost                    | 0.78247 |\n| LGBM                       | 0.78711 |\n| CatBoost                   | 0.78550 |\n| Neural Network             | 0.79284 |\n| **Hard voting classifier** | **0.79692** |\n| **Soft voting classifier** | **0.79603** |","metadata":{}},{"cell_type":"markdown","source":"# Final remarks\n\n1. The main purpose of this notebook was to test whether a simple ensemble method such as a voting classifier would improve on individual results and, as expected, it did.\n\n2. The best individual result was 0.79506 (Random Forest) and both voting classifiers outperformed this value (0.79692 with hard voting and 0.79603 with soft voting). However, I was expecting soft voting to do better than hard voting, since in theory there is \"more information\" in the probabilites that each method produces. That did not happen here.\n\n3. I realize that the individual results for XGBoost, LGBM and CatBoost are low compared to other kernels, so my next step is to work on them individually. Hopefully when I have them performing better I will get a much higher score with the ensemble.\n\nIf you have any thoughts to share, please do so in the comments. And thanks for stopping by! :)","metadata":{}}]}