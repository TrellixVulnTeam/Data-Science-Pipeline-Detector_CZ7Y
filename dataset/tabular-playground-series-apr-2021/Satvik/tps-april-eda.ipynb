{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Imports","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.compose import make_column_selector\nfrom sklearn.impute import KNNImputer\nfrom sklearn.metrics import classification_report\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.compose import make_column_selector\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import StackingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n%matplotlib inline","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### This is my first Kaggle Competition and I thought it would be great to share my notebook. I really appreciate any feedback and suggestions. And if you find it helpful -please vote.","metadata":{}},{"cell_type":"markdown","source":"# Loading the data","metadata":{}},{"cell_type":"code","source":"df_train=pd.read_csv('../input/tabular-playground-series-apr-2021/train.csv')\ndf_test=pd.read_csv('../input/tabular-playground-series-apr-2021/test.csv')\ndf_train","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train=df_train.drop(columns=[\"Survived\"])\nY_train=df_train[\"Survived\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_joined=pd.concat([X_train,df_test],ignore_index=True)\ndf_joined","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EDA","metadata":{}},{"cell_type":"code","source":"df_joined.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We can see we have Null values in our data\n# Let's count the number of examples having NULL values\ndf_joined.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Lets's explore the numerical features i.e. Age and Fare, to see if we need to drop them or impute them**\n### Fare","metadata":{}},{"cell_type":"code","source":"fig,axs = plt.subplots(ncols = 2,figsize = (17,4))\n\nsns.distplot(x = df_joined['Fare'] , ax = axs[0])\nsns.boxplot(x = 'Fare' , data = df_joined , color = 'red' , ax = axs[1])\n\nfor i in range(2):\n    axs[i].set_ylabel('')\n    axs[i].set_xlabel('Fare')\n    axs[i].tick_params(axis='x', labelsize=10)\n    axs[i].tick_params(axis='y', labelsize=10)\n\naxs[0].set_title('Distribution (Fare)', fontsize=13)\naxs[1].set_title('Boxplot For Fare', fontsize=13)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see from the Boxplot that anything greater than 100 looks like an outlier, and from the distribution graph we see that the data is left skewed. Thus we will go up to impute the NaN's with the median. \n\n**Let's look over the Age feature**\n\n\n### Age","metadata":{}},{"cell_type":"code","source":"fig,axs = plt.subplots(ncols = 2,figsize = (17,4))\n\nsns.distplot(x = df_joined['Age'] , ax = axs[0])\nsns.boxplot(x = 'Age' , data = df_joined , color = 'red' , ax = axs[1])\n\nfor i in range(2):\n    axs[i].set_ylabel('')\n    axs[i].set_xlabel('Age')\n    axs[i].tick_params(axis='x', labelsize=10)\n    axs[i].tick_params(axis='y', labelsize=10)\n\naxs[0].set_title('Distribution (Age)', fontsize=13)\naxs[1].set_title('Boxplot For Age', fontsize=13)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Boxplot clearly depicts that our data has no outliers and the distribution looks like a normal distribution, so we'll go up to impute the NaN's with the mean\n\n#### **Now we know how to impute Age and Fare, So let's move on further**\n\n### Embarked","metadata":{}},{"cell_type":"code","source":"# First we'll encode the data\ndf_joined['Embarked'].unique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dic_embarked={'S':1,'C':2,'Q':3}\ndf_joined['Embarked'] = df_joined['Embarked'].map(dic_embarked)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_joined['Embarked'].unique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig,axs = plt.subplots(ncols = 2,figsize = (17,4))\n\nsns.distplot(x = df_joined['Embarked'] , ax = axs[0])\nsns.boxplot(x = 'Embarked' , data = df_joined , color = 'red' , ax = axs[1])\n\nfor i in range(2):\n    axs[i].set_ylabel('')\n    axs[i].set_xlabel('Embarked')\n    axs[i].tick_params(axis='x', labelsize=10)\n    axs[i].tick_params(axis='y', labelsize=10)\n\naxs[0].set_title('Distribution (Embarked)', fontsize=13)\naxs[1].set_title('Boxplot For Embarked', fontsize=13)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see in the Boxplot that majority of are data belong to class S(1) and C(2), and that most of them belong to s(1), as seen from distribution graph.\nThus we can use KNN Imputer to replace the NaN's\n\n### Let's Impute the values so far","metadata":{}},{"cell_type":"code","source":"imputer = KNNImputer()\ndf_joined['Embarked']= pd.Series(imputer.fit_transform(df_joined['Embarked'].values.reshape(-1,1)).flatten())\ndf_joined['Age'] = df_joined['Age'].replace(np.nan,df_joined['Age'].mean())\ndf_joined['Fare'] = df_joined['Fare'].replace(np.nan,df_joined['Fare'].median())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Having a look over the NaN count again\nmissing_count=df_joined.isnull().sum()\npercent_missing = missing_count * 100 / len(df_joined)\nprint(f\"Missing Count \\n\\n{missing_count}\\n\\nPercentage Missing\\n\\n{percent_missing}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see from the percentage table 70%(approx.) of teh Cabin data is missing. Thus, we'll be dropping the Cabin column as of now and will come back to it and see if it will affect our model score.\nAlso 5%(approx.) of the Ticket data is missing which is 9804 examples out of 200000 thus droping them off will not adversly affect our data size.","metadata":{}},{"cell_type":"code","source":"df_joined.drop(columns=['Cabin'],inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We'll seperate the X_train and X_test data and append Y_train to X_train to drop the corresponding target variables\n\n# Mapping the Sex attribute\ndic_sex={'male':0,'female':1}\ndf_joined['Sex']=df_joined['Sex'].map(dic_sex)\n\nX_train =  df_joined[df_joined['PassengerId']<df_train.shape[0]]\nX_test =  df_joined[df_joined['PassengerId']>=df_train.shape[0]]\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# X_train.dropna(inplace=True)\n# X_test.dropna(inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Seperating X_train and Y_train\n# Y_train=X_train.iloc[:,-1]\n# X_train=X_train.iloc[:,:-1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Missing Count  \\n\\n{X_train.isnull().sum()}\\n\\nMissing Count in X_test\\n\\n{X_test.isnull().sum()}\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **Now, we have removed all the NaN's from our data and since we are working on a supervised ML problem we should also look at the relationship between the dependent variable and independent variable.**\n\nBefore that we'll be dropping off the Name and Ticket features as they are a bit random. But we'll come back to them to if they add anything to the model performance","metadata":{}},{"cell_type":"code","source":"X_train.drop(columns=['Name','Ticket'],inplace=True)\nX_test.drop(columns=['Name','Ticket'],inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.corrwith(Y_train).plot.bar(figsize=(15,10),title=\"Correlation with response variable\",fontsize=15,rot=90, color = 'red', grid=True )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that PassengerId , SibSp , Parch aren't that much correlated with the target variable. So what we'll do is we'll drop the PassengerId, SibSp , Parch. ","metadata":{}},{"cell_type":"code","source":"X_train.drop(columns=['SibSp','Parch','PassengerId'],inplace=True)\nX_test.drop(columns=['SibSp','Parch','PassengerId'],inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.corrwith(Y_train).plot.bar(figsize=(15,10),title=\"Correlation with response variable\",fontsize=15,rot=90, color = 'red', grid=True )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.reset_index(drop=True,inplace=True)\nX_test.reset_index(drop=True,inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def stand_scaler(df):\n    scaler=StandardScaler()\n    scaled_value=scaler.fit_transform(df[['Age','Fare']].values)\n    df[['Age','Fare']]=pd.DataFrame(scaled_value,index=df[['Age','Fare']].index,columns=df[['Age','Fare']].columns)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stand_scaler(X_train)\nstand_scaler(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def encoder(df):\n    transformer=ColumnTransformer([('encoder',OneHotEncoder(),[0,1,4])],remainder='passthrough')\n    df=np.array(transformer.fit_transform(df))\n    return df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train=encoder(X_train)\nX_test=encoder(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Implementation","metadata":{}},{"cell_type":"code","source":"# let's split our X_train data\nx_train,x_test,y_train,y_test=train_test_split(X_train,Y_train,test_size=0.2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr=LogisticRegression()\nlr.fit(x_train,y_train)\ny_pred=lr.predict(x_test)\nprint(classification_report(y_test,y_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"random_forest = RandomForestClassifier()\nrandom_forest.fit(x_train, y_train)\ny_pred = random_forest.predict(x_test)\nprint(classification_report(y_test,y_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"knn=KNeighborsClassifier(n_neighbors=5)\nknn.fit(x_train,y_train)\ny_pred=knn.predict(x_test)\nprint(classification_report(y_test,y_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"estimators = [('knn',KNeighborsClassifier()),('lr',LogisticRegression()),('dtr',DecisionTreeClassifier()),('rf',random_forest)]\nclf = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression())\nclf.fit(x_train,y_train)\ny_pred=clf.predict(x_test)\nprint(classification_report(y_test,y_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# It seen out that stacking the classifiers performed much better.","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Submitting the Predictions","metadata":{}},{"cell_type":"code","source":"result = lr.predict(X_test)\npd.DataFrame({'PassengerId' : df_test['PassengerId'] , 'Survived': result}).to_csv(\"my_submission2.csv\",index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result = lr.predict(X_test)\nresult","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}