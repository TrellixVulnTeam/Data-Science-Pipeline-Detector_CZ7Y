{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Utils","metadata":{}},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import classification_report, roc_auc_score, make_scorer, accuracy_score, roc_curve\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC \nfrom xgboost import XGBClassifier\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.cluster import KMeans\nfrom kmodes.kmodes import KModes\nfrom sklearn.preprocessing import PowerTransformer\nfrom sklearn.model_selection import StratifiedKFold\nimport lightgbm\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import uniform as sp_uniform\nfrom scipy.stats import randint as sp_randint\nfrom scipy.stats import uniform as sp_uniform\nfrom sklearn.metrics import pairwise_distances\nfrom sklearn.metrics.pairwise import pairwise_kernels\nfrom sklearn.metrics.pairwise  import cosine_similarity\nfrom sklearn.metrics.pairwise import chi2_kernel\nfrom sklearn.feature_selection import SelectFromModel","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.set_option('display.max_rows', None)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load Data","metadata":{}},{"cell_type":"code","source":"train= pd.read_csv('/kaggle/input/tabular-playground-series-apr-2021/train.csv', sep=',')\nsub_sample = pd.read_csv('/kaggle/input/tabular-playground-series-apr-2021/sample_submission.csv', sep=',')\ntest= pd.read_csv('/kaggle/input/tabular-playground-series-apr-2021/test.csv', sep=',')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train.shape, test.shape, sub_sample.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.isna().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data Preprocessing","metadata":{}},{"cell_type":"code","source":"train = train.set_index('PassengerId')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Df with train + test data\ndfg = pd.concat([train, test], axis=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"using the Median to fill all the NAN for Age","metadata":{}},{"cell_type":"code","source":"dfg['Age'].median()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['Age'] = train['Age'].replace(np.nan, dfg['Age'].median())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['Age'] = round(train['Age'],0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#Create a complex Feature that includes Age_bin, Sex","metadata":{}},{"cell_type":"code","source":"bins = ['Y1', 'Y2', 'Y3', 'Y4', 'M1', 'M2', 'E']\ntrain['Age_Bin'] = pd.cut(x=train['Age'],\n                            bins=[0, 5 , 10, 15, 20, 30, 50,1000],\n                            labels=bins,right=False)\ntrain['Age_Bin'] = train['Age_Bin'].astype('str')\ntrain['Age_Bin'] = train['Age_Bin']+train['Sex']\ndf_Age_bin = pd.get_dummies(train['Age_Bin'], prefix='Age_bin')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Handle missing data in \"Fare\" field","metadata":{}},{"cell_type":"code","source":"dfg['Age_Bin'] = pd.cut(x=dfg['Age'],\n                            bins=[0, 5 , 10, 15, 20, 30, 50,1000],\n                            labels=bins,right=False)\ndfg.groupby(by=['Sex','Pclass'])['Fare'].median()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.loc[(train['Fare'].isna()) & (train['Pclass']==1) & (train['Sex']=='female'), 'Fare']=85.40\ntrain.loc[(train['Fare'].isna()) & (train['Pclass']==2) & (train['Sex']=='female'), 'Fare']=24.75\ntrain.loc[(train['Fare'].isna()) & (train['Pclass']==3) & (train['Sex']=='female'), 'Fare']=12.54\n\ntrain.loc[(train['Fare'].isna()) & (train['Pclass']==1) & (train['Sex']=='male'), 'Fare']=64.51\ntrain.loc[(train['Fare'].isna()) & (train['Pclass']==2) & (train['Sex']=='male'), 'Fare']=14.23\ntrain.loc[(train['Fare'].isna()) & (train['Pclass']==3) & (train['Sex']=='male'), 'Fare']=11.02","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bins2 = ['L1', 'L2', 'L3', 'L4']\ntrain['Fare_Bin'] = pd.cut(x=train['Fare'],\n                            bins=[0,11, 30 , 60, 10000],\n                            labels=bins2,right=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['Fare_Bin'] = train['Fare_Bin'].astype('str')\ndf_Fare_bin = pd.get_dummies(train['Fare_Bin'], prefix='Fare_bin')\ndf_Fare_bin.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For Cabin Feature, estract only the first letter, then fillna with Z","metadata":{}},{"cell_type":"code","source":"dfg['Cabin'] =dfg['Cabin'].str[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dfg['Cabin'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['Cabin'] =train['Cabin'].str[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['Cabin'] = train['Cabin'].fillna('Z')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.groupby(by=['Cabin'])['Survived'].mean()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Remove T because of few instances","metadata":{}},{"cell_type":"code","source":"train.loc[(train['Cabin']=='T'), 'Cabin']='Z'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_cabin = pd.get_dummies(train['Cabin'], prefix='Cabin')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Analysis of possible strategy to fillna for Embarked field.\nSince the Passenger Class seems to be correlated to the possibility to Survive. 3 different embark classes will be created to fillna","metadata":{}},{"cell_type":"code","source":"train.groupby(by=['Embarked'])[['Fare','Survived']].mean()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train[train['Embarked'].isna()].groupby(by=['Pclass'])['Survived'].mean()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Used the most frequent caracter\ntrain['Embarked'] = train['Embarked'].fillna('S')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_embarked = pd.get_dummies(train['Embarked'], prefix='Embark')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Keep only the first letter for ticket and replace space with N and nan wi1th ZZ","metadata":{}},{"cell_type":"code","source":"train['Ticket'] = train['Ticket'].str.replace('[^a-zA-Z]', '').str[:1]\ntrain['Ticket'] = train['Ticket'].str.strip()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Convert NAN to ZZ","metadata":{}},{"cell_type":"code","source":"train['Ticket'] = train['Ticket'].fillna('ZZ')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Convert '' with NN","metadata":{}},{"cell_type":"code","source":"train.loc[train['Ticket']=='', 'Ticket']='ZZ'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Removed L because of few instances","metadata":{}},{"cell_type":"code","source":"train.loc[train['Ticket']=='L', 'Ticket']='ZZ'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.groupby(by=['Ticket'])['Survived'].mean()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['Ticket'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_tiket = pd.get_dummies(train['Ticket'], prefix='ticket')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Handling \"Name\"","metadata":{}},{"cell_type":"code","source":"df_name = pd.concat([train['Name'], test['Name']], axis=0)\ndf_name = pd.DataFrame(df_name, columns=['Name'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_name['FirstName'] = df_name['Name'].apply(lambda x:x.split(', ')[0])\ndf_name['SecondName'] = df_name['Name'].str.split(', ', 1, expand=True)[1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"le = LabelEncoder()\nle1 = LabelEncoder()\ndf_name['FirstName'] = le.fit_transform(df_name['FirstName'])\ndf_name['SecondName'] = le1.fit_transform(df_name['SecondName'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['FirstName'] = train['Name'].apply(lambda x:x.split(', ')[0])\ntrain['SecondName'] = train['Name'].str.split(', ', 1, expand=True)[1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['FirstName'] = le.transform(train['FirstName'])\ntrain['SecondName'] = le1.transform(train['SecondName'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Encoding the sex feature","metadata":{}},{"cell_type":"code","source":"train['Sex'] = train['Sex'].apply(lambda x: 1 if x=='female' else 0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['Sex'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"One Hot encoder for Pclass","metadata":{}},{"cell_type":"code","source":"train['Pclass'] = train['Pclass'].astype('str')\ndf_pclass = pd.get_dummies(train['Pclass'], prefix='class')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_pclass.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Get the Fatures related to Family size. An idea taken from:\nhttps://medium.datadriveninvestor.com/start-with-kaggle-a-comprehensive-guide-to-solve-the-titanic-challenge-8ac5815b0473","metadata":{}},{"cell_type":"code","source":"# introducing a new feature : the size of families (including the passenger)\ntrain['FamilySize'] = train['Parch'] + train['SibSp'] + 1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# introducing other features based on the family size\ntrain['Singleton'] = train['FamilySize'].map(lambda s: 1 if s == 1 else 0)\ntrain['SmallFamily'] = train['FamilySize'].map(lambda s: 1 if 2 <= s <= 4 else 0)\ntrain['LargeFamily'] = train['FamilySize'].map(lambda s: 1 if 5 <= s else 0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.concat([train['Fare'], train['Age'],train['FamilySize'], train['Singleton'], train['SmallFamily'], train['LargeFamily'],train['Sex'], df_cabin,df_tiket, df_pclass, df_embarked ,train['FirstName'],train['SecondName'],df_Age_bin,df_Fare_bin,train['Survived']], axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(15,10))\nsns.heatmap(data=df.corr())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df.drop(columns='Survived')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Adding 3 more features with Kmeans","metadata":{}},{"cell_type":"code","source":"km = KMeans(n_clusters=3, random_state=22, n_init=20)\ndf_km = km.fit_predict(df)\ndf_km = pd.DataFrame(df_km, index=df.index)\ndf_km = df_km.astype('str')\ndf_km = pd.get_dummies(df_km)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_km.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.concat([df, df_km], axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_target = train['Survived']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clf = RandomForestClassifier(n_estimators=200, max_features='sqrt')\nclf = clf.fit(df, df_target)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features = pd.DataFrame()\nfeatures['feature'] = df.columns\nfeatures['importance'] = clf.feature_importances_\nfeatures.sort_values(by=['importance'], ascending=True, inplace=True)\nfeatures.set_index('feature', inplace=True)\n\nfeatures.plot(kind='barh', figsize=(25, 25))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Using Optuna with Lgbm","metadata":{}},{"cell_type":"code","source":"import optuna","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def objective(trial , data = df , target = df_target):\n    train_x , test_x , train_y , test_y = train_test_split(data , target , \\\n            test_size = 0.028059109276941666 , random_state = 2)\n\n    #test_size = 0.028059109276941666\n    params = {\n        'reg_alpha' : trial.suggest_loguniform('reg_alpha' , 1e-5 , 12),\n        'reg_lambda' : trial.suggest_loguniform('reg_lambda' , 1e-5 , 12),\n        'num_leaves' : trial.suggest_int('num_leaves' , 11 , 900),\n        'learning_rate' : trial.suggest_uniform('learning_rate' , 0.0000001 , 0.2),\n        'max_depth' : trial.suggest_int('max_depth' , 5 , 400),\n        'n_estimators' : trial.suggest_int('n_estimators' , 1 , 9999),\n        'min_child_samples' : trial.suggest_int('min_child_samples' , 1 , 110),\n        'min_child_weight' : trial.suggest_loguniform('min_child_weight' , 1e-5 , 1),\n        'subsample' : trial.suggest_uniform('subsample' , 1e-5 , 1.0),\n        'colsample_bytree' : trial.suggest_loguniform('colsample_bytree' , 1e-5 , 1),\n        'random_state' : trial.suggest_categorical('random_state' , [2,22,222,2222]),\n        'metric' : 'accuracy',\n        'device_type' : 'cpu',\n    }\n    model = lightgbm.LGBMClassifier(**params)\n    model.fit(train_x , train_y , eval_set = [(test_x , test_y)] ,eval_metric='logloss', early_stopping_rounds = 3000 , \\\n             verbose = False)\n    preds = model.predict(test_x)\n    acc = accuracy_score(test_y , preds)\n    return acc","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"study = optuna.create_study(direction = 'maximize' , study_name = 'lgbm')\nstudy.optimize(objective , n_trials = 1)\nprint('numbers of the finished trials:' , len(study.trials))\nprint('the best params:' , study.best_trial.params)\nprint('the best value:' , study.best_value)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#the best value: 0.7808267997148967\nparams= {'reg_alpha': 0.000493095633250276, 'reg_lambda': 0.2799468729577344, 'num_leaves': 220, 'learning_rate': 0.058683299033376934, 'max_depth': 97, 'n_estimators': 9161, 'min_child_samples': 108, 'min_child_weight': 1.7359084365325016e-05, 'subsample': 0.7381682823837273, 'colsample_bytree': 0.29845810314125426, 'random_state': 1509}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#the best value: 0.7811831789023521\nparams2 = {'reg_alpha': 0.02242367265240423, 'reg_lambda': 0.0006085533155144086, 'num_leaves': 238, 'learning_rate': 0.03240605916351265, 'max_depth': 65, 'n_estimators': 5361, 'min_child_samples': 27, 'min_child_weight': 0.00011308353926700071, 'subsample': 0.5688435861948473, 'colsample_bytree': 0.06746586089945723, 'random_state': 22}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#the best value: 0.7804704205274412\nparams1= {'reg_alpha': 0.009415444471348289, 'reg_lambda': 1.2556528225033043, 'num_leaves': 25, 'learning_rate': 0.00835886426230468, 'max_depth': 230, 'n_estimators': 3653, 'min_child_samples': 9, 'min_child_weight': 0.0002224399318225647, 'subsample': 0.9780174338845454, 'colsample_bytree': 0.7969641118752326, 'random_state': 1}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Preprocessing the Test_set","metadata":{}},{"cell_type":"code","source":"test = test.set_index('PassengerId')\n\n#Age\ntest['Age'] = test['Age'].replace(np.nan, dfg['Age'].median())\ntest['Age_Bin'] = pd.cut(x=test['Age'],\n                            bins=[0, 5 , 10, 15, 20, 30, 50,1000],\n                            labels=bins,right=False)\ntest['Age_Bin'] = test['Age_Bin'].astype('str')\ntest['Age_Bin'] = test['Age_Bin']+test['Sex']\ndft_Age_bin = pd.get_dummies(test['Age_Bin'], prefix='Age_bin')\n\n\n#Fare\ntest.loc[(test['Fare'].isna()) & (test['Pclass']==1) & (test['Sex']=='female'), 'Fare']=85.40\ntest.loc[(test['Fare'].isna()) & (test['Pclass']==2) & (test['Sex']=='female'), 'Fare']=24.75\ntest.loc[(test['Fare'].isna()) & (test['Pclass']==3) & (test['Sex']=='female'), 'Fare']=12.54\n\ntest.loc[(test['Fare'].isna()) & (test['Pclass']==1) & (test['Sex']=='male'), 'Fare']=64.51\ntest.loc[(test['Fare'].isna()) & (test['Pclass']==2) & (test['Sex']=='male'), 'Fare']=14.23\ntest.loc[(test['Fare'].isna()) & (test['Pclass']==3) & (test['Sex']=='male'), 'Fare']=11.02\n\ntest['Fare_Bin'] = pd.cut(x=test['Fare'],\n                            bins=[0,11, 30 , 60, 10000],\n                            labels=bins2,right=False)\n\ntest['Fare_Bin'] = test['Fare_Bin'].astype('str')\ndft_Fare_bin = pd.get_dummies(test['Fare_Bin'], prefix='Fare_bin')\n\n\n#Cabin\ntest['Cabin'] =test['Cabin'].str[0]\ntest['Cabin'] = test['Cabin'].fillna('Z')\ntest.loc[(test['Cabin']=='T'), 'Cabin']='Z'\ndft_cabin = pd.get_dummies(test['Cabin'], prefix='Cabin')\n\n#Embarked\ntest['Embarked'] = test['Embarked'].fillna('S')\ndft_embarked = pd.get_dummies(test['Embarked'], prefix='Embark')\n\n#Ticket\ntest['Ticket'] = test['Ticket'].str.replace('[^a-zA-Z]', '').str[:1]\ntest['Ticket'] = test['Ticket'].str.strip()\ntest['Ticket'] = test['Ticket'].fillna('ZZ')\ntest.loc[test['Ticket']=='', 'Ticket']='ZZ'\ntest.loc[test['Ticket']=='L', 'Ticket']='ZZ'\ndft_tiket = pd.get_dummies(test['Ticket'], prefix='ticket')\n\n#Name\ntest['FirstName'] = test['Name'].apply(lambda x:x.split(', ')[0])\ntest['SecondName'] = test['Name'].str.split(', ', 1, expand=True)[1]\n\ntest['FirstName'] = le.transform(test['FirstName'])\ntest['SecondName'] = le1.transform(test['SecondName'])\n\n#Sex\ntest['Sex'] = test['Sex'].apply(lambda x: 1 if x=='female' else 0)\n\n#Pclass\ntest['Pclass'] = test['Pclass'].astype('str')\ndft_pclass = pd.get_dummies(test['Pclass'], prefix='class')\n\n#Family Size\ntest['FamilySize'] = test['Parch'] + test['SibSp'] + 1\n\ntest['Singleton'] = test['FamilySize'].map(lambda s: 1 if s == 1 else 0)\ntest['SmallFamily'] = test['FamilySize'].map(lambda s: 1 if 2 <= s <= 4 else 0)\ntest['LargeFamily'] = test['FamilySize'].map(lambda s: 1 if 5 <= s else 0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dft = pd.concat([test['Fare'], test['Age'],test['FamilySize'], test['Singleton'], test['SmallFamily'], test['LargeFamily'], test['Sex'],dft_cabin,dft_tiket, dft_pclass, dft_embarked, test['FirstName'],test['SecondName'], dft_Age_bin, dft_Fare_bin], axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dft_km = km.predict(dft)\ndft_km = pd.DataFrame(dft_km, index=dft.index)\ndft_km = dft_km.astype('str')\ndft_km = pd.get_dummies(dft_km)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dft = pd.concat([dft, dft_km], axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(dft.columns)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(df.columns)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"list(set(df.columns)-set(dft.columns))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_target.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params1['metric'] = 'accuracy'\nparams1['device'] = 'cpu'\npreds = np.zeros(dft.shape[0])\noof_preds = np.zeros(df.shape[0])\nkf = StratifiedKFold(n_splits = 50 , random_state = 22 , shuffle = True)\nroc = []\nn = 0\nfor trn_idx , val_idx in kf.split(df , df_target):\n    train_x = df.iloc[trn_idx]\n    train_y = df_target.iloc[trn_idx]\n    val_x = df.iloc[val_idx]\n    val_y = df_target.iloc[val_idx]\n    \n    model = lightgbm.LGBMClassifier(**params1)\n    model.fit(train_x , train_y , eval_set = [(val_x , val_y)] ,eval_metric='logloss', early_stopping_rounds = 8000 , verbose = False)\n    clf = CalibratedClassifierCV(model, cv='prefit', method='sigmoid')\n    clf.fit(train_x , train_y)\n    preds += clf.predict_proba(dft)[:,1]/kf.n_splits\n    oof_preds += clf.predict_proba(df)[:,1]/kf.n_splits\n    roc.append(accuracy_score(val_y , clf.predict(val_x)))\n    fpr, tpr, thresholds = roc_curve(val_y , clf.predict_proba(val_x)[:,1])\n    gmeans = np.sqrt(tpr * (1-fpr))\n    ix = np.argmax(gmeans)\n    print(n+1 , roc[n], 'Best Threshold=%f, G-Mean=%.3f' % (thresholds[ix], gmeans[ix]))\n    n+=1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fpr, tpr, thresholds = roc_curve(df_target , oof_preds)\ngmeans = np.sqrt(tpr * (1-fpr))\nix = np.argmax(gmeans)\nthresholds[ix]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub_sample['Survived'] = preds","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#simple threshold 0.405749\nsub_sample['Survived'] = sub_sample['Survived'].apply(lambda x: 1 if x>0.43147162440098286 else 0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub_sample.to_csv('submission.csv',index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}