{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Define the Problem\n\nFor this project, the problem statement is given to us on a golden plater, develop an algorithm to predict the survival outcome of passengers on the Titanic.\nIn this challenge, we ask you to complete the analysis of what sorts of people were likely to survive. In particular, we ask you to apply the tools of machine learning to predict which passengers survived the tragedy. <br>\n- Tools\n    - It's a classic **Binary classification**. \n- Data\n    - The dataset is used for this competition is synthetic but based on a real dataset (in this case, the actual [Titanic data](https://www.kaggle.com/c/titanic/data)!) and generated using a CTGAN. The statistical properties of this dataset are very similar to the original Titanic dataset, but there's no way to \"cheat\" by using public labels for predictions. How well does your model perform on truly private test labels?\n\nGood luck and have fun!","metadata":{}},{"cell_type":"markdown","source":"# Data Load In","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_raw = pd.read_csv('../input/tabular-playground-series-apr-2021/train.csv')\ndata_val = pd.read_csv('../input/tabular-playground-series-apr-2021/test.csv')\ndata1 = data_raw.copy(deep=True)\ndata_cleaner = [data1, data_val]\n\nTarget = ['Survived']","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for dataset in data_cleaner:\n    print(dataset.info())\n    print(dataset.describe(include='all'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Preprocessing","metadata":{}},{"cell_type":"markdown","source":"## Fill NA","metadata":{}},{"cell_type":"code","source":"for dataset in data_cleaner:\n    dataset.Age.fillna(dataset.Age.median(), inplace=True)\n    dataset.Embarked.fillna('S', inplace=True)\n    dataset.Fare.fillna(dataset.Fare.median(), inplace=True)\n    # dataset['Title'] = dataset.Name.str.split(', ', expand=True)[1].str.split('.', expand=True)[0]\n    dataset['Family_members'] = dataset.Parch + dataset.SibSp","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data1.sample(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## A Bit Visualization\n\nCheck distribution for setting bins","metadata":{}},{"cell_type":"code","source":"data1.drop(['Name', 'PassengerId', 'Ticket', 'SibSp', 'Parch'], axis=1, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\nfor i in data1.columns:\n    sns.countplot(data1[i])\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Cabin","metadata":{}},{"cell_type":"code","source":"# data1.groupby(data1['Cabin'].isnull()).mean()\ndata1.groupby(data1['Cabin'].isnull())['Survived'].mean()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for dataset in data_cleaner:\n    dataset['Cabin_Allotted'] = np.where(dataset.Cabin.isnull(), 0, 1)\n    dataset.drop('Cabin', axis=1, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data1.sample(5)\n# data1['Title'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Encoder\n\nIt all goes on how you explain your feature's (X) effect on the final prediction (Y).<br>\nLet's take a look at the feature `Fare`.<br>\nDo you think the Fare's increment like 1 dollar would make any significant impact on their survival rate?<br>\nThe answer is quite obvious, right? But what if we raise the increment to i.e. 30 dollars..? Hard to tell huh!<br>\nAgain, it all goes on how you interpret data. In this case, I've tried them so many times. And I received the better result by *categoricalization* on `Fare` in most of them.","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\nlb = LabelEncoder()\n\nfor dataset in data_cleaner:\n    dataset['Sex_labeled'] = lb.fit_transform(dataset.Sex)\n    \n    dataset['AgeBin'] = pd.qcut(dataset.Age, 3)\n    dataset['Age_labeled'] = lb.fit_transform(dataset['AgeBin'])\n\n    dataset['FareBin'] = pd.qcut(dataset.Fare, 4)\n    dataset['Fare_labeled'] = lb.fit_transform(dataset['FareBin'])\n\n    dataset['Embarked_labeled'] = lb.fit_transform(dataset.Embarked)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data1.sample(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(data1['Age_labeled'].value_counts())\nprint(data1['Fare_labeled'].value_counts())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data1_X = [\n    'Pclass', \n    'Family_members', \n    'Cabin_Allotted', \n    'Sex_labeled', \n    'Age_labeled', \n    'Fare_labeled', \n    'Embarked_labeled'\n]\n\nfor i in data1[data1_X].columns:\n    sns.lineplot(i, 'Survived', data=data1)\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data1[data1_X].info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Selection\n\nAn easy basic way to compare different models within the same dataset.","metadata":{}},{"cell_type":"code","source":"from sklearn import ensemble, tree, neighbors\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn import model_selection\n\nMLA = [\n    ensemble.AdaBoostClassifier(),\n    ensemble.BaggingClassifier(),\n    ensemble.ExtraTreesClassifier(),\n    ensemble.GradientBoostingClassifier(),\n    ensemble.RandomForestClassifier(),\n\n    neighbors.KNeighborsClassifier(), \n\n    tree.DecisionTreeClassifier(),\n    tree.ExtraTreeClassifier(), \n\n    XGBClassifier(objective='binary:logistic', eval_metric='logloss'),\n    LGBMClassifier()\n]\n\ncv_split = model_selection.ShuffleSplit(n_splits=10, test_size=0.2, train_size=0.8, random_state=1)\n\nMLA_columns = ['MLA Name', 'MLA Parameters', 'MLA Train Accuracy Mean', 'MLA Test Accuracy Mean', 'MLA Test Accuracy 3*STD', 'MLA Time']\nMLA_compare = pd.DataFrame(columns = MLA_columns)\n\nrow_index = 0\nfor alg in MLA:\n    MLA_name = alg.__class__.__name__\n    MLA_compare.loc[row_index, 'MLA Name'] = MLA_name\n    MLA_compare.loc[row_index, 'MLA Parameters'] = str(alg.get_params())\n\n    cv_results = model_selection.cross_validate(alg, data1[data1_X], data1[Target].values.reshape(-1,), cv=cv_split, return_train_score=True)\n\n    MLA_compare.loc[row_index, 'MLA Time'] = cv_results['fit_time'].mean()\n    MLA_compare.loc[row_index, 'MLA Train Accuracy Mean'] = cv_results['train_score'].mean()\n    MLA_compare.loc[row_index, 'MLA Test Accuracy Mean'] = cv_results['test_score'].mean() \n    MLA_compare.loc[row_index, 'MLA Test Accuracy 3*STD'] = cv_results['test_score'].std()*3\n\n    row_index += 1\n\n# MLA_compare.sort_values(by=['MLA Test Accuracy Mean'], ascending=False, inplace=True)\nMLA_compare.sort_values(by=['MLA Test Accuracy Mean'], ascending=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Next: Hyperparameters\n\nWhat is the importance of hyperparameter tuning? <br>\nHyperparameters are crucial as they control the overall behaviour of a machine learning model. The ultimate goal is to find an optimal combination of hyperparameters that minimizes a predefined loss function to give better results. To improve your ML skills, see [Titanic Top 11%| Starter II: Hyperparameter Tuning](https://www.kaggle.com/chienhsianghung/titanic-top-11-starter-ii-hyperparameter-tuning).<br>\n\nFurther discussion: [The Best Hyperparameters](https://www.kaggle.com/c/tabular-playground-series-apr-2021/discussion/231152)","metadata":{}},{"cell_type":"markdown","source":"## Tuning (Not Yet Finished)\n\nClearly, LightGBM received the highest grade in all models using `default hyperparameters`.<br>\nIn the starter Titanic challenge, using RandomForest and applying a *grid searching* technique, I can improve my score by 0.02 (from 0.76-0.78).<br> \nAnd now, I'm trying to use LightGBM in this competition according to my [Models Selection](https://www.kaggle.com/chienhsianghung/tps-apr-starter-pack-all-models?scriptVersionId=58964521&cellId=23) result but the `grid searching` part couldn't go well (failed in 9hrs session) because the amount of data is way too massive than the starter Titanic. It's 12GB in total.\n\n~~So, I applied my previous [grid searching result](https://www.kaggle.com/chienhsianghung/titanic-top-11-starter-ii-hyperparameter-tuning?scriptVersionId=58924477&cellId=25) which used small Titanic data for tuning. And, it didn't improve well enough as the RandomForest does though.<br>\nI still want to make a `hyperparameter searching` to see how far the LightGBM could go. Because, obviously, this's not a proper neither practicable way when we encounter a massive dataset.~~\n\n2021 Apr. 8th update___<br>\nI've tried reducing grid size, it still took me to run 27 thousand seconds and return me a *not better than RF's* improvement.","metadata":{}},{"cell_type":"code","source":"param = [{\n    'n_estimators': [1000, 1500, 2000], # [1000, 1500, 2000, 2500]\n    'max_depth':  [4, 8, 11], # [4, 5, 8, 11, -1]\n    'num_leaves': [15, 31, 58, 63], # [15, 31, 58, 63, 127]\n    'subsample': [0.6, 0.708, 0.8], # [0.6, 0.7, 0.708, 0.8, 1.0]\n    'colsample_bytree': [0.613, 0.8, 1.0], # [0.6, 0.613, 0.7, 0.8, 1.0]\n    # 'learning_rate' : [0.01, 0.02, 0.03]\n}]\n\n# https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n# https://scikit-learn.org/stable/auto_examples/model_selection/plot_multi_metric_evaluation.html\nbest_search = model_selection.GridSearchCV(estimator=LGBMClassifier(), param_grid=param, cv=cv_split, scoring='roc_auc')\nbest_search.fit(data1[data1_X], data1[Target].values.reshape(-1,))\n\n# best_param = best_search.best_params_\n# clf.set_params(**best_param)\nclf = best_search.best_estimator_\n\nprint(f\"Best Params:\\n{str(best_search.best_params_)}\")\nbest_search.best_score_ # Compare this with LB","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Predicting","metadata":{}},{"cell_type":"code","source":"# after trying several times submissions, I chose RandomForestClassifier as my highest score\n# model = ensemble.RandomForestClassifier(**{'criterion': 'entropy', 'max_depth': 5, 'n_estimators': 50, 'random_state': 1})\n\n# model = LGBMClassifier(**{'colsample_bytree': 0.8, 'learning_rate': 0.01, 'max_depth': 4, 'n_estimators': 1000, 'num_leaves': 15, 'subsample': 0.6})\n# model.fit(data1[data1_X], data1[Target].values.reshape(-1, ))\n# predictions = model.predict(data_val[data1_X])\npredictions = clf.predict(data_val[data1_X])\n\noutput = pd.DataFrame({'PassengerId': data_val.PassengerId, 'Survived': predictions})\noutput.to_csv('./my_submission_RandomForestClassifier_tunned_F4.csv', index=False)\nprint(\"Your submission was successfully saved!\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Features Importance","metadata":{}},{"cell_type":"code","source":"# https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.plot_importance.html\nfrom lightgbm import plot_importance\n\nplot_importance(clf)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# References\n* [A Data Science Framework: To Achieve 99% Accuracy](https://www.kaggle.com/ldfreeman3/a-data-science-framework-to-achieve-99-accuracy)\n* [MY FIRST KAGGLE WORK TITANIC](https://www.kaggle.com/saptarshisit/my-first-kaggle-work-titanic)","metadata":{}}]}