{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \n\nimport shap\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.graph_objects as go\n\nfrom sklearn.preprocessing import StandardScaler","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Hi Kagglers\nFinally we have some meaningful features we can do some feature engineering so in this notebook I just want to play with those features and have some fun. ","metadata":{}},{"cell_type":"markdown","source":"pclass - Passenger Ticket class : Class 1, 2 and 3.\n\nName - Name of the passenger\n\nsex - Sex of the Passenger\n\nAge - Age in years of the Passenger\n\nsibsp - Number of siblings / spouses aboard the Titanic\n\nparch - Number of parents / children aboard the Titanic\n\nTicket - Ticket number\n\nFare - Passenger fare\n\nCabin - Cabin number\n\nEmbarked - Port of Embarkation shows the port from which the passenger boarded the titanic\n\n       C - Cherbourg\n       Q - Queenstown\n       S - Southampton","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv(\"/kaggle/input/tabular-playground-series-apr-2021/train.csv\")\ntest_df = pd.read_csv(\"/kaggle/input/tabular-playground-series-apr-2021/test.csv\")\nsample = pd.read_csv(\"/kaggle/input/tabular-playground-series-apr-2021/sample_submission.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import missingno as no","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"no.matrix(train_df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"no.matrix(test_df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"100 * train_df.isnull().sum()/ len(train_df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"100 * test_df.isnull().sum()/ len(test_df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Fill NaN values first","metadata":{}},{"cell_type":"markdown","source":"**Age column**","metadata":{}},{"cell_type":"markdown","source":"I am going to fill nan values with median of this column.","metadata":{}},{"cell_type":"code","source":"train_df.drop(\"PassengerId\", axis=1).groupby(\"Sex\").median()[\"Age\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def fill_age(df):\n    \"\"\"\n    Function impute NaN values in Age column based\n    on age median per Sex\n    \"\"\"\n    age_median_per_sex = train_df.drop(\"PassengerId\", axis=1).groupby(\"Sex\").median()[\"Age\"]\n    m_idx = df[df[\"Sex\"] == \"male\"].index\n    f_idx = df[df[\"Sex\"] == \"female\"].index\n    \n    df.loc[m_idx, \"Age\"] = df[df[\"Sex\"] == \"male\"].fillna(value=age_median_per_sex.values[1])\n    df.loc[f_idx, \"Age\"] = df[df[\"Sex\"] == \"female\"].fillna(value=age_median_per_sex.values[0])\n    df[\"Age\"] = df[\"Age\"].apply(lambda x: int(x))\n\nfill_age(train_df)\nfill_age(test_df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df[\"Age_interval\"] = pd.cut(train_df[\"Age\"], 9, \n                                  labels=[\"0-9\",\"10-19\",\"20-29\",\"30-39\",\"40-49\",\"50-59\",\"60-69\",\"70-79\",\"80-89\"])\ntest_df[\"Age_interval\"] = pd.cut(test_df[\"Age\"], 9, \n                                 labels=[\"0-9\",\"10-19\",\"20-29\",\"30-39\",\"40-49\",\"50-59\",\"60-69\",\"70-79\",\"80-89\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_barplot(df, col):\n    x = df[col].value_counts().index\n    y = df[col].value_counts().values\n    \n    fig = go.Figure(data=[go.Bar(x=x, y=y)])\n    fig.update_traces(marker_color=px.colors.sequential.Greens, \n                      marker_line_color=\"rgb(8, 48, 107)\",\n                      marker_line_width=1.5, opacity=0.6)\n    fig.update_layout(title_text=f\"{col}\")\n    #fig.update_layout(xaxis=dict(ticktext=[\"Not Survived\", \"Survived\"],\n                                 #tickvals=[0,1]))\n    fig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_barplot(train_df, \"Age\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Cabin**","metadata":{}},{"cell_type":"code","source":"def cabin_feat(df, col):\n    # Fill NaN values with None string\n    df[col] = df[col].fillna(\"None\")\n    \n    # Create new features\n    df[f\"has_{col}\"] = df[col].apply(lambda x: 1 if x != \"None\" else 0)\n    df[\"Deck\"] = df[col].apply(lambda x: x[0])\n    df.drop(col, axis=1, inplace=True)\n    return df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = cabin_feat(train_df, \"Cabin\")\ntest_df = cabin_feat(test_df, \"Cabin\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_barplot(train_df, \"has_Cabin\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Ticket**","metadata":{}},{"cell_type":"code","source":"train_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df[\"Ticket\"] = train_df[\"Ticket\"].fillna(\"N\").map(lambda x:str(x).split()[0] if len(str(x).split()) > 1 else \"N\")\ntest_df[\"Ticket\"] = test_df[\"Ticket\"].fillna(\"N\").map(lambda x:str(x).split()[0] if len(str(x).split()) > 1 else \"N\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df['Ticket'].unique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Name**","metadata":{}},{"cell_type":"code","source":"train_df[\"Name\"].apply(lambda x: x.split(\",\")[0]).value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I wonder how often and what name was in the orginal titanic dataset. Does anyone remember? ","metadata":{}},{"cell_type":"code","source":"train_df[\"last_name\"] = train_df[\"Name\"].apply(lambda x: x.split(\",\")[0])\ntest_df[\"last_name\"] = test_df[\"Name\"].apply(lambda x: x.split(\",\")[0])\n\ntrain_df.drop(\"Name\", axis=1, inplace=True)\ntest_df.drop(\"Name\", axis=1, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#last_name = train_df[\"last_name\"].value_counts()\n#last_name = last_name[last_name.values > 100]\n\n#train_df[\"last_name\"] = train_df[\"last_name\"].apply(lambda x: \"others\" if x not in last_name.index else x)\n#test_df[\"last_name\"] = test_df[\"last_name\"].apply(lambda x: \"others\" if x not in last_name.index else x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I am going to drop \"Name\" column and leave first_name column for label encoding. I want to check if any feature engineering will have any efect on our results by training model with this feature and evaluate effect.","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\ncol_names = [\"last_name\", \"Ticket\"]\nfor col in col_names:\n    le = LabelEncoder()\n    le.fit(train_df[col].values.tolist() + test_df[col].values.tolist())\n    train_df[col] = le.transform(train_df[col].values)\n    test_df[col] = le.transform(test_df[col].values)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**SibSp and Parch**\n\nWe could create a family column divided into e.g. categories like (no family, small, medium, large)","metadata":{}},{"cell_type":"code","source":"plot_barplot(train_df, \"SibSp\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_barplot(train_df, \"Parch\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df[\"family\"] = train_df[\"SibSp\"] + train_df[\"Parch\"]\ntest_df[\"family\"] = test_df[\"SibSp\"] + test_df[\"Parch\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df[\"family\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def family_size(x):\n    if x == 0:\n        return \"alone\"\n    elif  0 < x <= 5:\n        return \"small\"\n    elif 5 < x <= 10:\n        return \"medium\"\n    else:\n        return \"large\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df[\"family\"] = train_df[\"family\"].apply(family_size)\ntest_df[\"family\"] = test_df[\"family\"].apply(family_size)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_barplot(train_df, \"family\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import OrdinalEncoder","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"family_df = pd.DataFrame({\"family\": train_df[\"family\"].values})\nfamily_df_test = pd.DataFrame({\"family\": test_df[\"family\"].values})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ordinal_enc = OrdinalEncoder()\ntrain_df[\"family\"] = ordinal_enc.fit_transform(family_df).reshape(-1)\ntest_df[\"family\"] = ordinal_enc.transform(family_df_test).reshape(-1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Pclass**","metadata":{}},{"cell_type":"code","source":"plot_barplot(train_df, \"Pclass\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Fare**","metadata":{}},{"cell_type":"code","source":"print(f\" Missing values in Fare column = {train_df.Fare.isna().sum()}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df[\"Fare\"] = train_df[\"Fare\"].fillna(value=train_df[\"Fare\"].median())\ntest_df[\"Fare\"] = test_df[\"Fare\"].fillna(value=train_df[\"Fare\"].median())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Embarked**","metadata":{}},{"cell_type":"markdown","source":"Fill missing values with column mode.","metadata":{}},{"cell_type":"code","source":"train_df['Embarked'].mode()[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df['Embarked'].value_counts(dropna=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df[\"Embarked\"].fillna(value=train_df[\"Embarked\"].mode()[0], inplace=True)\ntest_df[\"Embarked\"].fillna(value=test_df[\"Embarked\"].mode()[0], inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Dummy variables**","metadata":{}},{"cell_type":"code","source":"train2 = train_df.drop(\"PassengerId\", axis=1).copy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test2 = test_df.drop(\"PassengerId\", axis=1).copy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dummy_cols = [\"Sex\", \"Age\", \"Embarked\", \"Deck\",\"Age_interval\"] ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def convert_columns(df, cols):\n    \n    dummies_df = pd.get_dummies(df[cols], drop_first=True)\n    df.drop(cols, axis=1, inplace=True)\n    new_df = pd.concat([df, dummies_df], axis=1)\n    new_df.drop(\"PassengerId\", axis=1, inplace=True)\n    \n    return new_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_train = convert_columns(train_df, dummy_cols)\nnew_test = convert_columns(test_df, dummy_cols)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Heatmap**","metadata":{}},{"cell_type":"code","source":"corr_map = new_train.corr(method=\"spearman\")\nmask = np.zeros_like(corr_map)\nmask[np.triu_indices_from(mask)] = True\nplt.figure(figsize=(20,10))\nsns.heatmap(corr_map,\n            mask=mask,\n            annot=True,\n            linewidth=1,\n            linecolor=\"w\",\n            #square=True,\n            cbar=False,\n            cmap=\"coolwarm\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that newly created column has_Cabin is perfectly correlated with Deck_N column, therefore I will drop has_Cabin column. ","metadata":{}},{"cell_type":"code","source":"new_train = new_train.drop(\"has_Cabin\", axis=1)\nnew_test = new_test.drop(\"has_Cabin\", axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Columns distribution**","metadata":{}},{"cell_type":"code","source":"cols = new_train.drop(\"Survived\", axis=1).columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axes = plt.subplots(6, 3, figsize=(16, 20))\n\nfor col, ax in zip(cols, axes.flatten()):\n    sns.histplot(x=new_train[col], ax=ax)\n    sns.histplot(x=new_test[col], ax=ax, color=\"red\", alpha=0.4)\n    plt.tight_layout()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Creating folds with StratifiedKFold","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold\n\nskf = StratifiedKFold(n_splits=10)\nnew_train[\"kfold\"] = -1\n\nnew_train = new_train.sample(frac=1).reset_index(drop=True)\n\nfor fold, (train_idx, valid_idx) in enumerate(skf.split(X=new_train, y=new_train[\"Survived\"])):\n    new_train.loc[valid_idx, \"kfold\"] = fold","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def run_training(algo, df, test_df, fold, oof):\n    train_df = df[df.kfold != fold].reset_index(drop=True)\n    valid_df = df[df.kfold == fold].reset_index(drop=True)\n    \n    xtrain = train_df.drop([\"Survived\", \"kfold\"], axis=1)\n    xvalid = valid_df.drop([\"Survived\", \"kfold\"], axis=1)\n    \n    sc = StandardScaler()\n    xtrain = sc.fit_transform(xtrain)\n    xvalid = sc.transform(xvalid)\n    test_df = sc.transform(test_df)\n    \n    ytrain = train_df[\"Survived\"].values\n    yvalid = valid_df[\"Survived\"].values\n    \n    algo.fit(xtrain, ytrain)\n    preds = algo.predict(xvalid)\n    sub_proba = algo.predict_proba(test_df)[:, 1]\n    train_proba = algo.predict_proba(xvalid)[:, 1]\n    \n    fold_acc = accuracy_score(yvalid, preds)\n    \n    print(f\"fold={fold+1}, accuracy={fold_acc}\")\n    oof[valid_idx] += fold_acc\n    \n    return oof, sub_proba, algo, train_proba","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, roc_auc_score\n\nrfc = RandomForestClassifier(n_estimators=150)\n\nlevel2_df = pd.DataFrame()\ndf_proba = pd.DataFrame()\n\ntest_proba = np.zeros(len(new_test))\noof = np.zeros(len(new_train))\ntrain_pred = []\nfor fold in range(10):\n    oof, proba, rfc_model, tt_pred = run_training(rfc,new_train, new_test, fold, oof)\n    test_proba += proba\n    train_pred.append(tt_pred)\n    \nlevel2_df[\"randomforest\"] = np.hstack(train_pred)  \ndf_proba[\"randomforest\"] = test_proba / 10\nprint(f\"Mean accuracy after 10 folds {np.mean(oof)}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#from sklearn.neighbors import KNeighborsClassifier\n#knn = KNeighborsClassifier(n_jobs=-1)\n\n#test_proba = np.zeros(len(new_test))\n#oof = np.zeros(len(new_train))\n#for fold in range(5):\n    #oof, proba = run_training(knn,new_train, new_test, fold, oof)\n    #test_proba += proba\n    \n#df_proba[\"knn\"] = test_proba / 5\n#print(f\"Mean accuracy after 5 folds {np.mean(oof)}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from xgboost import XGBClassifier\n\nxgb = XGBClassifier(use_label_encoder=False)\n\ntest_proba = np.zeros(len(new_test))\noof = np.zeros(len(new_train))\ntrain_pred = []\nfor fold in range(10):\n    oof, proba, xgb_model, tt_pred = run_training(xgb,new_train, new_test, fold, oof)\n    test_proba += proba\n    train_pred.append(tt_pred)\n    \nlevel2_df[\"xgboost\"] = np.hstack(train_pred)\ndf_proba[\"xgboost\"] = test_proba / 10\nprint(f\"Mean accuracy after 10 folds {np.mean(oof)}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from lightgbm import LGBMClassifier\n\nlgbm = LGBMClassifier()\n\ntest_proba = np.zeros(len(new_test))\noof = np.zeros(len(new_train))\ntrain_pred = []\nfor fold in range(10):\n    oof, proba, lgbm_model, tt_pred = run_training(lgbm,new_train, new_test, fold, oof)\n    test_proba += proba\n    train_pred.append(tt_pred)\n\nlevel2_df[\"lgbm\"] = np.hstack(train_pred)\ndf_proba[\"lgbm\"] = test_proba / 10\nprint(f\"Mean accuracy after 10 folds {np.mean(oof)}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#df_proba[\"sum\"] = df_proba.sum(axis=1) / 4\n#df_proba[\"binary\"] = np.where(df_proba[\"sum\"] > 0.5, 1, 0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_proba[\"wavg\"] = 0.2 * df_proba[\"randomforest\"] + 0.2 * df_proba[\"xgboost\"] + 0.5 * df_proba[\"lgbm\"]\ndf_proba[\"binary_wavg\"] = np.where(df_proba[\"wavg\"] > 0.5, 1, 0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_proba.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Ensemble submission","metadata":{}},{"cell_type":"code","source":"submission = sample.copy()\n\nsubmission[\"Survived\"] = df_proba[\"binary_wavg\"].values\nsubmission.to_csv(\"ensemble_sub_avg.csv\",index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Lightgbm hyperparameter optimalization with optuna","metadata":{}},{"cell_type":"code","source":"import optuna\nimport lightgbm as lgbm\nfrom sklearn.model_selection import train_test_split\nfrom optuna.pruners import SuccessiveHalvingPruner","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = new_train.drop([\"Survived\", \"kfold\"], axis=1)\ny = new_train[\"Survived\"].values","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def objective(trial):\n    \n    xtrain, xvalid, ytrain, yvalid = train_test_split(X, y, test_size=0.2, random_state=45)\n    sc = StandardScaler()\n    xtrain = sc.fit_transform(xtrain)\n    xvalid = sc.transform(xvalid)\n    \n    dtrain = lgbm.Dataset(xtrain, label=ytrain)\n    dvalid = lgbm.Dataset(xvalid, label=yvalid)\n    \n    params = {\n        \"objective\":\"binary\",\n        \"metric\":\"binary_logloss\",\n        \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-4, 1e-1),\n        \"lambda_l1\": trial.suggest_loguniform(\"lambda_l1\", 1e-8, 10.0),\n        \"lambda_l2\": trial.suggest_loguniform(\"lambda_l1\", 1e-8, 10.0),\n        \"num_leaves\": trial.suggest_int(\"num_leaves\", 2, 256),\n        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 15),\n        \"feature_fraction\": trial.suggest_uniform(\"feature_fraction\", 0.4, 1.0),\n        \"bagging_fraction\": trial.suggest_uniform(\"bagging_fraction\", 0.4, 1.0),\n        \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 1, 7),\n        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 100)\n    }\n    \n    gbm = lgbm.train(params, dtrain)\n    preds = gbm.predict(xvalid)\n    pred_labels = np.rint(preds)\n    accuracy = accuracy_score(yvalid, pred_labels)\n    return accuracy\n\nstudy = optuna.create_study(direction=\"maximize\", pruner=SuccessiveHalvingPruner())\nstudy.optimize(objective, n_trials=100)\n\nprint(\"Number of finished trials:\", len(study.trials))\nprint(\"Best trial:\", study.best_trial.params)","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"study.best_value","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lgbm_final = LGBMClassifier(**study.best_params)\n\ntest_proba = np.zeros(len(new_test))\noof = np.zeros(len(new_train))\ntrain_pred = []\nfor fold in range(10):\n    oof, proba, lgbm_optuna, tt_pred = run_training(lgbm_final,new_train, new_test, fold, oof)\n    test_proba += proba\n    train_pred.append(tt_pred)\n\nlevel2_df[\"lgbm_optuna_10fold\"] = np.hstack(train_pred)\ndf_proba[\"lgbm_optuna_10fold\"] = test_proba / 5\nprint(f\"Mean accuracy after 10 folds {np.mean(oof)}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Submission","metadata":{}},{"cell_type":"code","source":"submission = sample.copy()\n\nsc = StandardScaler()\nXtrain_full = sc.fit_transform(X)\nXtest = sc.transform(new_test)\n\nlgbm_model = LGBMClassifier(**study.best_params)\nlgbm_model.fit(Xtrain_full, y)\n\nsub_preds = lgbm_model.predict(Xtest)\nsubmission[\"Survived\"] = sub_preds\nsubmission.to_csv(\"lgbm_optuna10fold.csv\",index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Different approach","metadata":{}},{"cell_type":"code","source":"train2.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"price_per_deck = train2.groupby(\"Deck\").mean()[\"Fare\"]\ntrain2[\"fare_mean_per_deck\"] = train2[\"Deck\"].map(price_per_deck.to_dict())\ntest2[\"fare_mean_per_deck\"] = test2[\"Deck\"].map(price_per_deck.to_dict())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cols = [\"Sex\", \"Embarked\", \"Deck\", \"Age_interval\"]\nle = LabelEncoder()\nfor col in cols:\n    le.fit(train2[col].values.tolist() + test2[col].values.tolist())\n    train2[col] = le.transform(train2[col].values)\n    test2[col] = le.transform(test2[col].values)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axes = plt.subplots(5, 3, figsize=(16, 15))\n\nfor col, ax in zip(train2.drop(\"Survived\", axis=1).columns, axes.flatten()):\n    sns.histplot(x=train2[col], ax=ax)\n    sns.histplot(x=test2[col], ax=ax, color=\"red\",alpha=0.4)\n    plt.tight_layout()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"skf = StratifiedKFold(n_splits=10)\ntrain2[\"kfold\"] = -1\n\ntrain2 = train2.sample(frac=1).reset_index(drop=True)\n\nfor fold, (train_idx, valid_idx) in enumerate(skf.split(X=train2, y=train2[\"Survived\"])):\n    train2.loc[valid_idx, \"kfold\"] = fold","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"log_cols = [\"Fare\"]\nfor col in log_cols:\n    train2[col] = np.log1p(train2[col])\n    test2[col] = np.log1p(test2[col])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12,8))\nsns.histplot(x=train2[\"Fare\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clf = RandomForestClassifier(n_estimators=150)\n\ntest_proba = np.zeros(len(test2))\noof = np.zeros(len(train2))\ntrain_pred = []\nfor fold in range(10):\n    oof, proba, rfc_model2, tt_pred = run_training(rfc,train2, test2, fold, oof)\n    test_proba += proba\n    train_pred.append(tt_pred)\n\nlevel2_df[\"randomforest2\"] = np.hstack(train_pred)\ndf_proba[\"randomforest2\"] = test_proba / 10\nprint(f\"Mean accuracy after 10 folds {np.mean(oof)}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Mean accuracy after 5 folds 0.7647400000000004","metadata":{}},{"cell_type":"code","source":"xgb = XGBClassifier(use_label_encoder=False)\n\ntest_proba = np.zeros(len(test2))\noof = np.zeros(len(train2))\ntrain_pred = []\nfor fold in range(10):\n    oof, proba, xgb_model2, tt_pred = run_training(xgb,train2, test2, fold, oof)\n    test_proba += proba\n    train_pred.append(tt_pred)\n    \nlevel2_df[\"xgboost2\"] = np.hstack(train_pred)\ndf_proba[\"xgboost2\"] = test_proba / 10\nprint(f\"Mean accuracy after 10 folds {np.mean(oof)}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Mean accuracy after 5 folds 0.7789","metadata":{}},{"cell_type":"code","source":"lgbm = LGBMClassifier()\n\ntest_proba = np.zeros(len(new_test))\noof = np.zeros(len(new_train))\ntrain_pred = []\nfor fold in range(10):\n    oof, proba, lgbm_model2, tt_pred = run_training(lgbm, train2, test2, fold, oof)\n    test_proba += proba\n    train_pred.append(tt_pred)\n    \nlevel2_df[\"lgbm2\"] = np.hstack(train_pred)\ndf_proba[\"lgbm2\"] = test_proba / 10\nprint(f\"Mean accuracy after 10 folds {np.mean(oof)}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Mean accuracy after 5 folds 0.7819000000000002 to beat","metadata":{}},{"cell_type":"code","source":"df_proba[\"wavg2\"] = 0.2 * df_proba[\"randomforest2\"] + 0.2 * df_proba[\"xgboost2\"] + 0.5 * df_proba[\"lgbm2\"]\ndf_proba[\"binary_wavg2\"] = np.where(df_proba[\"wavg2\"] > 0.5, 1, 0)\ndf_proba.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Ensemble submission 2","metadata":{}},{"cell_type":"code","source":"submission = sample.copy()\n\nsubmission[\"Survived\"] = df_proba[\"binary_wavg2\"].values\nsubmission.to_csv(\"ensemble_sub1.csv\",index=False)\nsubmission[\"Survived\"] = np.where(df_proba[\"lgbm2\"] > 0.5, 1, 0)\nsubmission.to_csv(\"lgbm_sub10fold.csv\",index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## LGBM + optuna","metadata":{}},{"cell_type":"code","source":"X = train2.drop([\"kfold\", \"Survived\"], axis=1)\ny = train2[\"Survived\"].values","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import lightgbm as lgbm\n\ndef objective(trial):\n    \n    xtrain, xvalid, ytrain, yvalid = train_test_split(X, y, test_size=0.2, random_state=45)\n    sc = StandardScaler()\n    xtrain = sc.fit_transform(xtrain)\n    xvalid = sc.transform(xvalid)\n    \n    dtrain = lgbm.Dataset(xtrain, label=ytrain)\n    dvalid = lgbm.Dataset(xvalid, label=yvalid)\n    \n    params = {\n        \"objective\":\"binary\",\n        \"metric\":\"binary_logloss\",\n        \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-4, 1e-1),\n        \"lambda_l1\": trial.suggest_loguniform(\"lambda_l1\", 1e-8, 10.0),\n        \"lambda_l2\": trial.suggest_loguniform(\"lambda_l1\", 1e-8, 10.0),\n        \"num_leaves\": trial.suggest_int(\"num_leaves\", 2, 256),\n        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 15),\n        \"feature_fraction\": trial.suggest_uniform(\"feature_fraction\", 0.4, 1.0),\n        \"bagging_fraction\": trial.suggest_uniform(\"bagging_fraction\", 0.4, 1.0),\n        \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 1, 7),\n        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 100)\n    }\n    \n    gbm = lgbm.train(params, dtrain)\n    preds = gbm.predict(xvalid)\n    pred_labels = np.rint(preds)\n    accuracy = accuracy_score(yvalid, pred_labels)\n    return accuracy\n\nstudy = optuna.create_study(direction=\"maximize\", pruner=SuccessiveHalvingPruner())\nstudy.optimize(objective, n_trials=100)\n\nprint(\"Number of finished trials:\", len(study.trials))\nprint(\"Best trial:\", study.best_trial.params)","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"study.best_value","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = sample.copy()\n\nsc = StandardScaler()\nXtrain_full = sc.fit_transform(X)\nXtest = sc.transform(test2)\n\nlgbm_model = LGBMClassifier(**study.best_params)\nlgbm_model.fit(Xtrain_full, y)\n\nsub_preds = lgbm_model.predict(Xtest)\nsubmission[\"Survived\"] = sub_preds\nsubmission.to_csv(\"lgbm_optuna2.csv\",index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"study.best_value","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lgbm = LGBMClassifier(**study.best_params)\n\ntest_proba = np.zeros(len(new_test))\noof = np.zeros(len(new_train))\ntrain_pred = []\nfor fold in range(10):\n    oof, proba, lgbm_model, tt_pred = run_training(lgbm,train2, test2, fold, oof)\n    test_proba += proba\n    train_pred.append(tt_pred)\n\nlevel2_df[\"lgbm_optuna2\"] = np.hstack(train_pred)\ndf_proba[\"lgbm_optuna2\"] = test_proba / 10\nprint(f\"Mean accuracy after 10 folds {np.mean(oof)}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = sample.copy()\nsubmission[\"Survived\"] = np.where(df_proba[\"lgbm_optuna2\"] > 0.5, 1, 0)\nsubmission.to_csv(\"lgbm_optuna2_10folds_2.csv\", index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Explain LGBMClassifier predictions with Shap","metadata":{}},{"cell_type":"code","source":"shap.initjs()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"explainer = shap.TreeExplainer(lgbm_model)\nshap_values = explainer.shap_values(X)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"shap.force_plot(explainer.expected_value[1], shap_values[1][0,:], X.iloc[0,:])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"shap.force_plot(explainer.expected_value[1], shap_values[1][:1000,:], X.iloc[:1000,:])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"shap.summary_plot(shap_values[1], X)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"shap.summary_plot(shap_values, X)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Xgboost + optuna","metadata":{}},{"cell_type":"code","source":"train2.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = train2.drop([\"Survived\", \"kfold\"], axis=1)\ny = train2[\"Survived\"].values","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import xgboost as xgb","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def objective(trial):\n    \n    Xtrain, Xvalid, ytrain, yvalid = train_test_split(X, \n                                                      y, \n                                                      test_size=0.2,\n                                                      random_state=101)\n    \n    sc = StandardScaler()\n    Xtrain = sc.fit_transform(Xtrain)\n    Xvalid = sc.transform(Xvalid)\n    \n    dtrain = xgb.DMatrix(Xtrain, label=ytrain)\n    dvalid = xgb.DMatrix(Xvalid, label=yvalid)\n    \n    params = {\n        \"objective\": \"binary:logistic\",\n        \"use_label_encoder\": False,\n        \"eta\": trial.suggest_loguniform(\"eta\", 1e-2, 2e-1),\n        \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 12),\n        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10),\n        \"gamma\": trial.suggest_loguniform(\"gamma\", 1e-4, 1.0),\n        \"lambda\": trial.suggest_loguniform(\"lambda\", 1e-4, 1.0),\n        \"alpha\": trial.suggest_loguniform(\"alpha\", 1e-4, 1.0),\n        \"subsample\": trial.suggest_uniform(\"subsample\", 0.3, 1.0),\n        \"colsample_bytree\": trial.suggest_uniform(\"colsample_bytree\", 0.3, 1.0)      \n    }\n    xgb_optuna = xgb.train(params, dtrain)\n    preds = xgb_optuna.predict(dvalid)\n    pred_labels = np.rint(preds)\n    accuracy = accuracy_score(yvalid, pred_labels)\n    return accuracy\n\nstudy = optuna.create_study(direction=\"maximize\", pruner=SuccessiveHalvingPruner())\nstudy.optimize(objective, n_trials=100)\n\nprint(\"Number of finished trials:\", len(study.trials))\nprint(\"Best trial:\", study.best_trial.params)","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"study.best_value","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgb = XGBClassifier(**study.best_params, use_label_encoder=False)\n\ntest_proba = np.zeros(len(test2))\noof = np.zeros(len(train2))\ntrain_pred = []\nfor fold in range(10):\n    oof, proba, xgb_model2, tt_pred = run_training(xgb,train2, test2, fold, oof)\n    test_proba += proba\n    train_pred.append(tt_pred)\n    \nlevel2_df[\"xgboost_optuna_5fold2\"] = np.hstack(train_pred)\ndf_proba[\"xgboost_optuna_5fold2\"] = test_proba / 10\nprint(f\"Mean accuracy after 10 folds {np.mean(oof)}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Xgboost predictions with shap","metadata":{}},{"cell_type":"code","source":"explainer = shap.TreeExplainer(xgb_model2, X)\nshap_values = explainer.shap_values(X)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"shap.initjs()\nshap.force_plot(explainer.expected_value, shap_values[1], X.iloc[0,:].index)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"shap.summary_plot(shap_values, X)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"shap.force_plot(explainer.expected_value, shap_values[:1000,:], X.iloc[:1000,:])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission[\"Survived\"] = np.where(df_proba[\"xgboost_optuna_5fold2\"] > 0.5, 1, 0)\nsubmission.to_csv(\"xgboost_optuna_10fold_2.csv\", index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The best prediction I've achieved so far is with lgbm_optuna2_5folds_2 which scored 0.80423","metadata":{}},{"cell_type":"markdown","source":"## Level 2 model aproach","metadata":{}},{"cell_type":"code","source":"df_proba = df_proba.drop(['wavg', 'binary_wavg','wavg2','binary_wavg2'], axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_train2 = pd.concat([level2_df, train2], axis=1)\nnew_test2 = pd.concat([df_proba, test2], axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegressionCV\n\nlr_model = LogisticRegressionCV(max_iter=100000)\n\ntest_proba = np.zeros(len(test2))\noof = np.zeros(len(train2))\nfor fold in range(10):\n    oof, proba, lr_model,_ = run_training(lr_model, new_train2, new_test2, fold, oof)\n    test_proba += proba\n    \nfinal_preds = test_proba / 10\nprint(f\"Mean accuracy after 10 folds {np.mean(oof)}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission[\"Survived\"] = np.where(final_preds > 0.5, 1, 0)\nsubmission.to_csv(\"level2_sub2.csv\", index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = new_train2.drop([\"Survived\", \"kfold\"], axis=1)\ny = new_train2[\"Survived\"].values\n\nimport lightgbm as lgbm\n\ndef objective(trial):\n    \n    xtrain, xvalid, ytrain, yvalid = train_test_split(X, y, test_size=0.2, random_state=45)\n    sc = StandardScaler()\n    xtrain = sc.fit_transform(xtrain)\n    xvalid = sc.transform(xvalid)\n    \n    dtrain = lgbm.Dataset(xtrain, label=ytrain)\n    dvalid = lgbm.Dataset(xvalid, label=yvalid)\n    \n    params = {\n        \"objective\":\"binary\",\n        \"metric\":\"binary_logloss\",\n        \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-4, 1e-1),\n        \"lambda_l1\": trial.suggest_loguniform(\"lambda_l1\", 1e-8, 10.0),\n        \"lambda_l2\": trial.suggest_loguniform(\"lambda_l1\", 1e-8, 10.0),\n        \"num_leaves\": trial.suggest_int(\"num_leaves\", 2, 256),\n        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 15),\n        \"feature_fraction\": trial.suggest_uniform(\"feature_fraction\", 0.4, 1.0),\n        \"bagging_fraction\": trial.suggest_uniform(\"bagging_fraction\", 0.4, 1.0),\n        \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 1, 7),\n        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 100)\n    }\n    \n    gbm = lgbm.train(params, dtrain)\n    preds = gbm.predict(xvalid)\n    pred_labels = np.rint(preds)\n    accuracy = accuracy_score(yvalid, pred_labels)\n    return accuracy\n\nstudy = optuna.create_study(direction=\"maximize\", pruner=SuccessiveHalvingPruner())\nstudy.optimize(objective, n_trials=100)\n\nprint(\"Number of finished trials:\", len(study.trials))\nprint(\"Best trial:\", study.best_trial.params)","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"study.best_value","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lgbm = LGBMClassifier(**study.best_params)\n\ntest_proba = np.zeros(len(test2))\noof = np.zeros(len(train2))\nfor fold in range(10):\n    oof, proba,_ ,_ = run_training(lgbm, new_train2, new_test2, fold, oof)\n    test_proba += proba\n    \nfinal_preds = test_proba / 10\nprint(f\"Mean accuracy after 10 folds {np.mean(oof)}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission[\"Survived\"] = np.where(final_preds > 0.48, 1, 0)\nsubmission.to_csv(\"level2_sub3.csv\", index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Finding optimal weights","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\nfrom functools import partial\nfrom scipy.optimize import fmin","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class OptimizerAUC:\n    def __init__(self):\n        self.coef_ = 0\n        \n    def auc_(self, coef, X, y):\n        x_coef = X * coef\n        predictions = np.sum(x_coef, axis=1)\n        auc_score = roc_auc_score(y, predictions)\n        return -1.0 * auc_score\n    \n    def fit(self, X, y):\n        partial_loss = partial(self.auc_, X=X, y=y)\n        init_coef = np.random.dirichlet(np.ones(X.shape[1]))\n        self.coef_ = fmin(partial_loss, init_coef, disp=True)\n        \n    def predict(self, X):\n        x_coef = X * self.coef_\n        predictions = np.sum(x_coef, axis=1)\n        return predictions\n    \n    \ndef run_training2(pred_df, fold, col_names):\n    train_df = pred_df[pred_df.kfold != fold].reset_index(drop=True)\n    valid_df = pred_df[pred_df.kfold == fold].reset_index(drop=True)\n    \n    xtrain = train_df[col_names].values\n    xvalid = valid_df[col_names].values\n    \n    sc = StandardScaler()\n    xtrain = sc.fit_transform(xtrain)\n    yvalid = sc.transform(xvalid)\n    \n    ytrain = train_df.Survived.values\n    yvalid = valid_df.Survived.values\n    \n    opt = OptimizerAUC()\n    opt.fit(xtrain, ytrain)\n    preds = opt.predict(xvalid)\n    \n    fold_auc = roc_auc_score(yvalid, preds)\n    print(f\"fold={fold}, auc={fold_auc}\")\n    \n    return opt.coef_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"level2_df[\"Survived\"] = train2[\"Survived\"].values\nlevel2_df[\"kfold\"] = train2[\"kfold\"].values","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"col_names = [col for col in level2_df.columns if col not in [\"Survived\", \"kfold\"]]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"level2_df.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"col_names = ['lgbm_optuna_10fold', 'lgbm2', 'xgboost_optuna_5fold2']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"coef = []\nfor j in range(10):\n    coef.append(run_training2(level2_df, j, col_names))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"coef = np.array(coef)\ncoef_mean = np.mean(coef, axis=0)\nprint(coef_mean)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"col_names","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wt_avg = (\n    coef_mean[0] * level2_df[\"lgbm_optuna_10fold\"].values\n    + coef_mean[1] * level2_df[\"lgbm2\"].values\n    + coef_mean[2] * level2_df[\"xgboost_optuna_5fold2\"].values\n)\nprint(\"Optimal acc after finding coefs\")\nwt_acc = accuracy_score(level2_df[\"Survived\"], np.where(wt_avg > 0.5, 1, 0))\nprint(f\"Optimized weighted avg of acc: {wt_acc}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wt_avg_sub = (\n    coef_mean[0] * df_proba[\"lgbm_optuna_10fold\"].values\n    + coef_mean[1] * df_proba[\"lgbm2\"].values\n    + coef_mean[2] * df_proba[\"xgboost_optuna_5fold2\"].values\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission[\"Survived\"] = np.where(wt_avg_sub > 0.5, 1, 0)\nsubmission.to_csv(\"optimal_weights_sub.csv\", index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}