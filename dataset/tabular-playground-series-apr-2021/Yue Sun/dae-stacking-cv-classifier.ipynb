{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport itertools\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nimport warnings\nwarnings.simplefilter('ignore')\nfrom sklearn import model_selection\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB \nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom catboost import CatBoostClassifier\nfrom mlxtend.classifier import StackingCVClassifier\nimport time\nimport lightgbm as lgb\nfrom pathlib import Path\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import roc_auc_score, confusion_matrix\nimport warnings\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","metadata":{"_uuid":"051d70d956493feee0c6d64651c6a088724dca2a","_execution_state":"idle","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install kaggler","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import kaggler\nfrom kaggler.model import AutoLGB\nfrom kaggler.preprocessing import DAE, TargetEncoder, LabelEncoder\n\nprint(f'Kaggler: {kaggler.__version__}')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_name = 'dae_te'\nalgo_name = 'lgb'\nmodel_name = f'{algo_name}_{feature_name}'\n\ndata_dir = Path('/kaggle/input/tabular-playground-series-apr-2021/')\ntrn_file = data_dir / 'train.csv'\ntst_file = data_dir / 'test.csv'\nsample_file = data_dir / 'sample_submission.csv'\npseudo_label_file = ''\n\nfeature_file = f'{feature_name}.csv'\npredict_val_file = f'{model_name}.val.txt'\npredict_tst_file = f'{model_name}.tst.txt'\nsubmission_file = f'{model_name}.sub.csv'\n\ntarget_col = 'Survived'\nid_col = 'PassengerId'\nfeature_name = 'dae_te'\nalgo_name = 'lgb'\nmodel_name = f'{algo_name}_{feature_name}'\n\ndata_dir = Path('/kaggle/input/tabular-playground-series-apr-2021/')\ntrn_file = data_dir / 'train.csv'\ntst_file = data_dir / 'test.csv'\nsample_file = data_dir / 'sample_submission.csv'\npseudo_label_file = ''\n\nfeature_file = f'{feature_name}.csv'\npredict_val_file = f'{model_name}.val.txt'\npredict_tst_file = f'{model_name}.tst.txt'\nsubmission_file = f'{model_name}.sub.csv'\n\ntarget_col = 'Survived'\nid_col = 'PassengerId'","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trn = pd.read_csv(trn_file, index_col=id_col)\ntst = pd.read_csv(tst_file, index_col=id_col)\nsub = pd.read_csv(sample_file, index_col=id_col)\npseudo_label = pd.read_csv(pseudo_label_file, index_col=id_col)\nprint(trn.shape, tst.shape, sub.shape, pseudo_label.shape)\ntst[target_col] = pseudo_label[target_col]\nn_trn = trn.shape[0]\ndf = pd.concat([trn, tst], axis=0)\ndf.head()\n# Feature engineering code from https://www.kaggle.com/udbhavpangotra/tps-apr21-eda-model\n\ndf['Embarked'] = df['Embarked'].fillna('No')\ndf['Cabin'] = df['Cabin'].fillna('_')\ndf['CabinType'] = df['Cabin'].apply(lambda x:x[0])\ndf.Ticket = df.Ticket.map(lambda x:str(x).split()[0] if len(str(x).split()) > 1 else 'X')\n\ndf['Age'].fillna(round(df['Age'].median()), inplace=True,)\ndf['Age'] = df['Age'].apply(round).astype(int)\n\n# Fare, fillna with mean value\nfare_map = df[['Fare', 'Pclass']].dropna().groupby('Pclass').median().to_dict()\ndf['Fare'] = df['Fare'].fillna(df['Pclass'].map(fare_map['Fare']))\n\ndf['FirstName'] = df['Name'].str.split(', ').str[0]\ndf['SecondName'] = df['Name'].str.split(', ').str[1]\n\ndf['n'] = 1\n\ngb = df.groupby('FirstName')\ndf_names = gb['n'].sum()\ndf['SameFirstName'] = df['FirstName'].apply(lambda x:df_names[x]).fillna(1)\n\ngb = df.groupby('SecondName')\ndf_names = gb['n'].sum()\ndf['SameSecondName'] = df['SecondName'].apply(lambda x:df_names[x]).fillna(1)\n\ndf['Sex'] = (df['Sex'] == 'male').astype(int)\n\ndf['FamilySize'] = df.SibSp + df.Parch + 1\n\nfeature_cols = ['Pclass', 'Age','Embarked','Parch','SibSp','Fare','CabinType','Ticket','SameFirstName', 'SameSecondName', 'Sex',\n                'FamilySize', 'FirstName', 'SecondName']\ncat_cols = ['Pclass','Embarked','CabinType','Ticket', 'FirstName', 'SecondName']\nnum_cols = [x for x in feature_cols if x not in cat_cols]\nprint(len(feature_cols), len(cat_cols), len(num_cols))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for col in ['SameFirstName', 'SameSecondName', 'Fare', 'FamilySize', 'Parch', 'SibSp']:\n    df[col] = np.log2(1 + df[col])\n    \nscaler = StandardScaler()\ndf[num_cols] = scaler.fit_transform(df[num_cols])\nlbe = LabelEncoder(min_obs=50)\ndf[cat_cols] = lbe.fit_transform(df[cat_cols]).astype(int)\ncv = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=seed)\nte = TargetEncoder(cv=cv)\ndf_te = te.fit_transform(df[cat_cols], df[target_col])\ndf_te.columns = [f'te_{col}' for col in cat_cols]\ndf_te.head()\ndae = DAE(cat_cols=cat_cols, num_cols=num_cols, encoding_dim=encoding_dim)\nX = dae.fit_transform(df[feature_cols])\ndf_dae = pd.DataFrame(X, columns=[f'dae_{i}' for i in range(encoding_dim)])\nprint(df_dae.shape)\nX = pd.concat([df[feature_cols], df_te, df_dae], axis=1)\ny = df[target_col]\nto_test = X.iloc[n_trn:].reset_index(drop = True)\nX.to_csv('dae X.csv', index = False)\ny.to_csv('dae y.csv', index = False)\nto_test.to_csv('dae to_test.csv', index = False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = pd.read_csv(\"../input/yue-sun-dae/dae X.csv\")\ny = pd.read_csv(\"../input/yue-sun-dae/dae y.csv\")\nto_test = pd.read_csv(\"../input/yue-sun-dae/dae to_test.csv\")\ny = y.Survived","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"RANDOM_SEED = 2021\nPROBAS = True\nFOLDS = 5\nTARGET = 'Survived'\nparams = {\n    'metric': 'binary_logloss',\n    'n_estimators': 10000,\n    'objective': 'binary',\n    'learning_rate': 0.02,\n    'min_child_samples': 150,\n    'reg_alpha': 3e-5,\n    'reg_lambda': 9e-2,\n    'num_leaves': 20,\n    'max_depth': 16,\n    'colsample_bytree': 0.8,\n    'subsample': 0.8,\n    'subsample_freq': 2,\n    'max_bin': 240,\n    'device_type' : 'gpu'\n}\ncb_params = {\n    'max_depth':6,\n    'max_ctr_complexity': 5,\n    'num_trees': 50000,\n    'od_wait': 500,\n    'od_type':'Iter', \n    'learning_rate': 0.04,\n    'min_data_in_leaf': 3,\n    'task_type': 'GPU'\n}\nmlr = LogisticRegression(**{'tol': 0.0001738334848471753, 'C': 0.015550647561600984, \n        'fit_intercept': False, 'random_state': 555, \"n_jobs\" : -1})\n#cl1 = KNeighborsClassifier(n_neighbors = 1)\ncl2 = RandomForestClassifier(n_estimators = 200, max_depth = 5, random_state = RANDOM_SEED)\n#cl3 = GaussianNB()\n#cl4 = DecisionTreeClassifier(max_depth = 5)\ncl5 = CatBoostClassifier(**cb_params, verbose = None, logging_level = 'Silent')\ncl6 = LGBMClassifier(**params)\n# I used some hyperparameter search (ExtraTrees - Genetic search)\ncl7 = ExtraTreesClassifier(bootstrap=False, criterion='entropy', max_features=0.55, min_samples_leaf=8, min_samples_split=4, n_estimators=100) # Optimized using TPOT\n#cl8 = MLPClassifier(activation = \"relu\", alpha = 0.1, hidden_layer_sizes = (10,10,10),\n#                            learning_rate = \"constant\", max_iter = 2000, random_state = RANDOM_SEED)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cnt_trained = 0     \nfor it, rs in enumerate(range(2020, 2025)):    # (2020, 2048)一直让结果更好\n    print('=' * 30)\n    print('START RANDOM_STATE = {}'.format(rs))\n    print('=' * 30)\n    model = StackingCVClassifier(classifiers= [cl2, cl5, cl6, cl7],\n                            meta_classifier = mlr, \n                            use_probas = PROBAS,   \n                            random_state = rs)\n    model.fit(X, y)\n    test_pred = model.predict_proba(to_test)[:,1]\n    cnt_trained += 1\n    if it == 0:\n        test_pred_full = test_pred.copy()\n    else:        \n        test_pred_full += test_pred\ny_pred = test_pred_full / cnt_trained\npid = pd.read_csv(\"../input/tps-apr-2021-pseudo-label-dae/tps04-sub-006.csv\")\n\noutput = pd.DataFrame({'PassengerId': pid.PassengerId, 'Survived': y_pred})\nthreshold = 0.5\noutput.Survived =  (output.Survived > threshold).astype(int)\noutput.to_csv('dae + stacking.csv', index=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"check = pd.read_csv(\"../input/proba-dae-stacking/th 0.43.csv\")\ncheck.to_csv('dae + stacking.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}