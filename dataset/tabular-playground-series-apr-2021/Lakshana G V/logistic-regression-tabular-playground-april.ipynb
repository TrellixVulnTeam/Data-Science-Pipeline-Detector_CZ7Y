{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Introduction\n\nThis is my first ever kernal on Kaggle and also my first Machine Learning project.\nThis is based on the 'Tabular Playground Sythanic April competition'.\nI have done Exploratory data analysis of the given data, cleansed the data, applied some feature engineering techniques and then fit the data to a Logistic Regression model.\nI am open to feedback and evaluation on this kernel.\nBelow are the steps followed in the analysis\n1. Import Packages and Read data\n2. Data Preprocessing\n     2.1. Explore shape and attribues\n     2.2. Describe data\n     2.3. Check for missing values\n         2.3.1. Cabin\n         2.3.2. Ticket\n         2.3.3. Age\n         2.3.4. Fare\n         2.3.5. Embarked\n     2.4. Handle Outliers     \n3. Exploratory Data Analysis\n4. Feature Engineering\n    4.1. Handle missing values for continuos features\n        4.1.1. Impute null values in Age\n        4.1.2. Impute missing values in Fare\n    4.2. Handle missing values for categorical features\n        4.2.1. Impute null values in Embarked\n        4.2.2. Create new feature from SibSp and Parch\n        4.2.3. One-Hot Encoding for Pclass, Sex, Embarked\n    4.3. Drop unnecessary features\n5. Fit data to Logistic Regression model\n6. Prediction for Kaggle test data\n","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"## 1. Import Packages and Read data","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsns.color_palette(\"Set2\",10)\nsns.set()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, confusion_matrix, roc_curve, roc_auc_score\nfrom sklearn.preprocessing import StandardScaler\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Read Training dataset","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv('../input/tabular-playground-series-apr-2021/train.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# preview data\ntrain_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Read Test dataset","metadata":{}},{"cell_type":"code","source":"test_df = pd.read_csv('../input/tabular-playground-series-apr-2021/test.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# preview test data\ntest_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. Data Preprocessing\n\n### 2.1 Explore data and its attribues","metadata":{}},{"cell_type":"code","source":"train_df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Total number of passenger records in the training set is {}\".format(train_df.shape[0]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"'Survived' is the attribute to the predicted. Hence its not part of the test dataset","metadata":{}},{"cell_type":"code","source":"print(\"Total number of passenger records in the training set is {}\".format(test_df.shape[0]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.2 Describe data","metadata":{}},{"cell_type":"code","source":"train_df.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The above table gives  a brief decription of the numerical attributes in training data","metadata":{}},{"cell_type":"markdown","source":"### 2.3 Check for missing values\n\nLet's check the data for missing values, so that we will handle them in the Feature Engineering section","metadata":{}},{"cell_type":"code","source":"train_df.isnull().sum().sort_values(ascending=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### 2.3.1 Cabin","metadata":{}},{"cell_type":"code","source":"print(\"Percentage of null-values in Cabin %.2f%% \" %(train_df['Cabin'].isnull().sum()/train_df.shape[0]*100))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Cabin has more than 67.87% of null values, and it doesn't make much sense to impute this attribute. So we'll drop this attribute during Feature engineering process","metadata":{}},{"cell_type":"markdown","source":"##### 2.3.2 Ticket","metadata":{}},{"cell_type":"code","source":"print(\"Percentage of null-values in Ticket %.2f%% \" %(train_df['Ticket'].isnull().sum()/train_df.shape[0]*100))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Ticket column refernces the ticket number and we wouldn't use it for our analysis. So we'll drop this attribute as well","metadata":{}},{"cell_type":"markdown","source":"##### 2.3.3 Age","metadata":{}},{"cell_type":"code","source":"print(\"Percentage of null-values in Age %.2f%% \" %(train_df['Age'].isnull().sum()/train_df.shape[0]*100))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Age will be one of the most important attribute to predict the survived.\n\nWe wil impute the missing age values with either mean or median, depending on the data.\n\nLet's look at the distribution of Age","metadata":{}},{"cell_type":"code","source":"#Plot the distribution of Age\nplt.figure(figsize=(7,5))\nsns.histplot(data = train_df['Age'],bins=20,kde=True, color='coral',element=\"step\", alpha=0.4)\nplt.title('Distribution of Age')\nplt.ylabel('No of Passengers')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('The mean Age is',train_df['Age'].mean())\nprint('The median Age is',train_df['Age'].median())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.boxplot(x=train_df['Age'], color='turquoise')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see Age is slightly left skewed, so we will consider median for imputing null values.","metadata":{}},{"cell_type":"markdown","source":"##### 2.3.4 Fare","metadata":{}},{"cell_type":"code","source":"print(\"Percentage of null-values in Fare %.2f%% \" %(train_df['Fare'].isnull().sum()/train_df.shape[0]*100))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Fare is the ticket fare and its based on the class of the Passenger. So we'll group passenger records by Passenger class, calculate the average fare and impute missing fare values based on this.\n\nLet's look at the distribution of Fare attribute","metadata":{}},{"cell_type":"code","source":"# Visualize distribution of Fare\n\nplt.figure(figsize=(7,5))\nsns.histplot(data = train_df['Fare'],bins=50, kde=True, color='green',element=\"step\", alpha=0.3)\nplt.title('Distribution of Fare')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Clearly Fare is left skewed and also has lot of outliers at the upper extreme. We will remove some of the outliers using quartiles and IQR.\n\nLet's look at the outliers more clearly using boxplot","metadata":{}},{"cell_type":"code","source":"sns.boxplot(data=train_df, y=train_df['Fare'], x=train_df['Pclass'], palette='Set2')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will group the passenger records by Passenger Class and then remvove the Fare outliers from each group. We will perform this in the next section","metadata":{}},{"cell_type":"markdown","source":"##### 2.3.5 Embarked","metadata":{}},{"cell_type":"code","source":"print(\"Percentage of null-values in Embarked %.2f%% \" %(train_df['Embarked'].isnull().sum()/train_df.shape[0]*100))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are only 0.25% null values in Port of Embarkation. So we'll impute this with the most frequent port embarked.","metadata":{}},{"cell_type":"code","source":"#Visualize the distribution of Embarked\nplt.figure(figsize=(7,5))\nsns.countplot(x='Embarked',data=train_df, palette='Set2')\nplt.title('Distribution of Embarked')\nplt.ylabel('No of passengers')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"All of the attributes with null values have been explored. We will look at the distribution of all the attributes after handling outliers and null-values","metadata":{}},{"cell_type":"markdown","source":"### 2.4 Handle Outliers\n\nFare column has outliers.\n\n1. Because Fare is associated with PClass, We will group the records by Pclass \n2. We can calculate IQR --> Inter Quartile Range and Quantile(0.75).\n3. Compute Quantile(0.75) + (1.5 * IQR)\n4. Any value above this can be treated as outlier and dropped.","metadata":{}},{"cell_type":"code","source":"#Group based on Pclass\ntrain_pclass1 = train_df.iloc[np.where(train_df['Pclass'] == 1)]\ntrain_pclass2 = train_df.iloc[np.where(train_df['Pclass'] == 2)]\ntrain_pclass3 = train_df.iloc[np.where(train_df['Pclass'] == 3)]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Handling outliers where Pclass=1\nq1 = train_pclass1['Fare'].quantile(0.25)\nq3 = train_pclass1['Fare'].quantile(0.75)\nIQR = q3-q1\nm = q3 + 1.5*IQR","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_pclass1 = train_pclass1.iloc[np.where(train_pclass1['Fare'] < m)]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Handling outliers where Pclass=2\nq1 = train_pclass2['Fare'].quantile(0.25)\nq3 = train_pclass2['Fare'].quantile(0.75)\nIQR = q3-q1\nm = q3 + 1.5*IQR\n\ntrain_pclass2 = train_pclass2.iloc[np.where(train_pclass2['Fare'] < m)]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Handling outliers where Pclass=3\nq1 = train_pclass3['Fare'].quantile(0.25)\nq3 = train_pclass3['Fare'].quantile(0.75)\nIQR = q3-q1\nm = q3 + 1.5*IQR\n\ntrain_pclass3 = train_pclass3.iloc[np.where(train_pclass3['Fare'] < m)]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#concatenate the 3 dataframes\ntrain_df = pd.concat([train_pclass1,train_pclass2,train_pclass3], axis=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"6777 records are dropped as outliers in Fare attribute\n\nLet's draw the boxplot of Fare to check if the outliers are handled well","metadata":{}},{"cell_type":"code","source":"#Boxplot of Fare\nplt.figure(figsize=(7,5))\nsns.boxplot(data=train_df, y=train_df['Fare'], x=train_df['Pclass'], palette='Set2')\nplt.title(\"Checking for outliers\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we have already handled the extreme outliers, we can leave the outliers still present in the data untreated. ","metadata":{}},{"cell_type":"markdown","source":"## 3. Exploratory data analysis\n\nIn this section, we will explore some more features and look for relationship with 'Survived'\n\nLet's draw some plots and gather insights","metadata":{}},{"cell_type":"markdown","source":"Let's see if there is any relationship between PClass and Survived passengers","metadata":{}},{"cell_type":"code","source":"upper = train_df.loc[train_df['Pclass'] == 1]['Survived']\nupper_percent = (sum(upper)/len(upper))*100\nprint(\" %.2f%% of upper class passengers survived\" %upper_percent)\n\nmiddle = train_df.loc[train_df['Pclass'] == 2]['Survived']\nmiddle_percent = (sum(middle)/len(middle))*100\nprint(\" %.2f%% of middle class passengers survived\" %middle_percent)\n\nlower = train_df.loc[train_df['Pclass'] == 3]['Survived']\nlower_percent = (sum(lower)/len(lower))*100\nprint(\" %.2f%% of lower class passengers survived\" %lower_percent)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(7,5))\nsns.countplot(x='Survived',hue='Pclass',data=train_df, palette='Set2')\nplt.title('Analysing Survived passengers by Pclass')\nplt.ylabel('No of passengers')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the graph, its obvious that passengers belonging to upper class have higher chances of survival.\n\nAlso most passengers who didnt survive are from lower class","metadata":{}},{"cell_type":"markdown","source":"#### Did more women survive than men?","metadata":{}},{"cell_type":"code","source":"male = train_df.loc[train_df['Sex'] == 'male']['Survived']\nmale_percent = (sum(male)/len(male))*100\nprint(\"Percentage of men who survived %.2f%% \" %male_percent)\n\nfemale = train_df.loc[train_df['Sex'] == 'female']['Survived']\nfemale_percent = (sum(female)/len(female))*100\nprint(\"Percentage of women who survived %.2f%%\" %female_percent)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Its is true that being a women increased the chances of being on life boat and survival\n\n71 % of women survived when compared to 20 % of men who survived the disaster.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(7,5))\nsns.countplot(x='Survived',hue='Sex',data=train_df, palette='Set2' )\nplt.title('Analysing Survived by Sex')\nplt.ylabel('No of Passengers')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Is there any relationship between Fare and survival?","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(7,5))\nax = sns.kdeplot(train_df[train_df['Survived'] == 1]['Fare'], shade=True, legend=True, color='coral')\nax = sns.kdeplot(train_df[train_df['Survived'] == 0]['Fare'], shade=True, legend=True, color='teal')\nplt.title('Relationship between Fare and Survival')\nax.legend(['Survived','Deceased'])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the plot, we can infer that people who paid less fare have low probability of survival.","metadata":{}},{"cell_type":"markdown","source":"## 4. Feature Engineering\n1. Handle missing values for continuous features\n2. One Hot Encoding for categorical features","metadata":{}},{"cell_type":"markdown","source":"### 4.1 Handle missing values for continuous features\n\nAs we have already explored the different features, we have a fair idea of how to impute missing values for different features\n\nWe will impute missing in our training data and then we'll apply the same logic and impute missing values in final test data (kaggle) as well.","metadata":{}},{"cell_type":"markdown","source":"#### 4.1.1 Impute null values in Age","metadata":{}},{"cell_type":"code","source":"# Impute train data\nage_median = train_df['Age'].median()\ntrain_df['Age'] = train_df['Age'].fillna(age_median)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Impute test data\ntest_df['Age'] = test_df['Age'].fillna(age_median)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 4.1.2 Impute missing values in Fare","metadata":{}},{"cell_type":"code","source":"# Impute train data\ntrain_mean_fare = lambda x: x.fillna(x.mean())\ntrain_df['Fare'] = train_df.groupby(['Pclass'])['Fare'].apply(lambda x: x.fillna(x.mean()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Store fare mean by Pclass for handling missing values in test data\nimpute_fare = train_df.groupby(['Pclass'])['Fare'].mean()\nimpute_fare","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pclass1_mean_fare = impute_fare.iloc[0]\npclass2_mean_fare = impute_fare.iloc[1]\npclass3_mean_fare = impute_fare.iloc[2]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Impute missing values in test data\ntest_df['Fare'] = np.where(test_df['Pclass']==1, test_df['Pclass'].fillna(pclass1_mean_fare),test_df['Fare'])\ntest_df['Fare'] = np.where(test_df['Pclass']==2, test_df['Pclass'].fillna(pclass2_mean_fare),test_df['Fare'])\ntest_df['Fare'] = np.where(test_df['Pclass']==3, test_df['Pclass'].fillna(pclass3_mean_fare),test_df['Fare'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df['Fare'].isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4.2 Handle missing values for categorical features\n\nThere are 3 categorical features to be imputed.\n* Embarked\n* Pclass\n* SibSp & Parch --> combine to create new attribute\n\nWe will perform One-Hot encoding for Pclass and Embarked features.\n\nSibSp --> Presence of Sibling/Spouse on board\nParch --> Presence of Parent/Child on board\nWe will create a new feature based on these two, to indicate the presence of Family on board\n\nWe will impute missing in our training data and then we'll apply the same logic and impute missing values in final test data (kaggle) as well.","metadata":{}},{"cell_type":"markdown","source":"#### 4.2.1 Impute null values in Embarked\n* We will impute missing values with the most frequent port of Embarkment","metadata":{}},{"cell_type":"code","source":"# Imputing train data\nfreq_embarked = train_df['Embarked'].mode()[0]\ntrain_df['Embarked'] = train_df['Embarked'].fillna(freq_embarked)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Imputing test data\ntest_df['Embarked'] = test_df['Embarked'].fillna(freq_embarked)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 4.2.1 Create new feature from SibSp and Parch","metadata":{}},{"cell_type":"code","source":"# Impute train data\ntrain_df['Family'] = np.where(train_df['SibSp']+train_df['Parch'] > 0, 1,0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Impute test data\ntest_df['Family'] = np.where(test_df['SibSp']+test_df['Parch'] > 0, 1,0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 4.2.2 One-Hot Encoding for Pclass, Sex, Embarked","metadata":{}},{"cell_type":"code","source":"# Impute train data\ntrain_df['Pclass'] = train_df['Pclass'].astype(str)\ntrain_onehot = pd.get_dummies(train_df[['Pclass','Sex','Embarked']], drop_first=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_onehot.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.concat([train_df,train_onehot], axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#  Impute test data\ntest_df['Pclass'] = test_df['Pclass'].astype(str)\ntest_onehot = pd.get_dummies(test_df[['Pclass','Sex','Embarked']], drop_first=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_onehot.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df = pd.concat([test_df,test_onehot], axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4.3. Drop unnecessary features\nLet's drop the features that are no longer needed after one hot encoding.","metadata":{}},{"cell_type":"code","source":"train_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Drop features in train data\ntrain_df.drop(['Sex','Embarked','SibSp','Parch','Pclass'], axis=1, inplace=True)\ntrain_df.drop(['PassengerId','Name','Cabin','Ticket'], axis=1, inplace=True)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Drop features in test data\ntest_df.drop(['Sex','Embarked','SibSp','Parch','Pclass'], axis=1, inplace=True)\ntest_df.drop(['Name','Cabin','Ticket'], axis=1, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5. Fit data to Logistic Regression model\n","metadata":{}},{"cell_type":"code","source":"X = train_df.drop(['Survived'], axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y = train_df['Survived']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since the attributes are of different scales, we will scale the data using standard scaler\n\nThen we have to check for multicollinearity in the dependent variables using VIF (Variance Inflation Factor)","metadata":{}},{"cell_type":"code","source":"scaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_scaled","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This is transformed data after scaling","metadata":{}},{"cell_type":"code","source":"vif = pd.DataFrame()\nvif['vif'] = [variance_inflation_factor(X_scaled,i) for i in range(X_scaled.shape[1])] \nvif['Features'] = X.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vif","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"All Vif values are very low --> no multicollinearity\n\nWe will split the training data into train and test data, so that we can check the accuracy of prediction\n\nThe test data provided by kaggle is for final prediction and submission\n\nSo, let's split the training data into train (80%) and test(20%)","metadata":{}},{"cell_type":"code","source":"# Split data\nX_train, X_test, y_train, y_test = train_test_split(X_scaled,y,test_size=0.20, random_state=355)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"log_reg = LogisticRegression()\nlog_reg.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = log_reg.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"log_reg.score(X_test,y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print confusion matrix\nconf_matrix = confusion_matrix(y_test,y_pred)\nconf_matrix","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot confusion matrix on heatmap\nplt.figure(figsize=(4,4))\nsns.heatmap(confusion_matrix(y_test,y_pred), annot=True, fmt='.0f', cbar=False, cmap='Greys')\nplt.xlabel('Actual Values')\nplt.ylabel('Predicted Values')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accuracy = accuracy_score(y_test,y_pred)\nprint(\"Accuracy of the logistic regression model is %2.3f\" % accuracy)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"true_positive = conf_matrix[0][0]\nfalse_positive = conf_matrix[0][1]\nfalse_negative = conf_matrix[1][0]\ntrue_negative = conf_matrix[1][1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Precision = true_positive / (true_positive + false_positive)\nRecall = true_positive / (true_positive + false_negative)\nSpecificity = true_negative / (true_negative + false_positive)\nFalse_positive_rate = 1-Specificity\nauc = roc_auc_score(y_test,y_pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Accuracy of the logistic regression model is %2.3f\" % accuracy)\nprint(\"Precision of the logistic regression model is %2.3f\" % Precision)\nprint(\"Recall of the logistic regression model is %2.3f\" % Recall)\nprint(\"Specificity of the logistic regression model is %2.3f\" % Specificity)\nprint(\"False positive rate of the logistic regression model is %2.3f\" % False_positive_rate)\nprint(\"AUC of the logistic regression model is %2.3f\" % auc)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 6. Prediction for Kaggle Test data","metadata":{}},{"cell_type":"code","source":"test_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_result = pd.DataFrame()\ntest_result['PassengerId']=test_df['PassengerId']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.drop(['PassengerId'],axis=1,inplace=True)\ntest_df.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_result['Survived'] = log_reg.predict(test_df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_result","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_result.to_csv('submission.csv',header=True,index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}