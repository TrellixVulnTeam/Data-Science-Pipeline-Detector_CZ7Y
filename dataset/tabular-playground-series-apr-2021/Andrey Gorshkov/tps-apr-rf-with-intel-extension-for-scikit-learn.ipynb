{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"In Kaggle competitions and in solving real-life problems, we tend to use complex models: combinations of various algorithms, neural networks, various boosts, and much more. However, for a quick start and plunge into the field of machine learning and big data, it is enough to know how to use the algorithms of classical machine learning. An example of a classical machine learning algorithm is Random Forest. Using only this algorithm may already give you a fairly good solution.\n\nFor classical machine learning algorithms, we often use the most popular Python library, Scikit-learn. With Scikit-learn you can fit models and search for optimal parameters, but it sometimes works for hours. Speeding up this process is something anyone who uses Scikit-learn would be interested in.\n\nI want to show you how to use Scikit-learn library and get the results faster without changing the code. To do this, we will make use of another Python library, [**Intel® Extension for Scikit-learn***](https://github.com/intel/scikit-learn-intelex). It accelerates Scikit-learn and does not require you to change the code written for Scikit-learn.\n\nI will show you how to speed up your kernel from **25 minutes to 14 minutes** without changing your code!","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport re\nimport optuna\n\nimport sklearn\nfrom sklearn.model_selection import train_test_split","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train =pd.read_csv(\"../input/tabular-playground-series-apr-2021/train.csv\")\ntest = pd.read_csv(\"../input/tabular-playground-series-apr-2021/test.csv\")\nsample_submission = pd.read_csv('../input/tabular-playground-series-apr-2021/sample_submission.csv')\n\ntrain.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessing\n\nMost of the preprocessing was taken from Anisotropic's [Introduction to Ensembling/Stacking in Python](https://www.kaggle.com/arthurtok/introduction-to-ensembling-stacking-in-python) notebook. I have also added several new features  that are based on the existing ones. While doing this will allow us to take previously unknown regularities into account, it might also lead to a strong correlation in data and, consequently, to overfitting. We would need to find a balance.\n\n","metadata":{}},{"cell_type":"code","source":"PassengerId = test['PassengerId']\nfull_data = [train, test]\n\n# Feature engineering steps taken from Anisotropic\n# Create a new feature with the length of a name \ntrain['Name_length'] = train['Name'].apply(len)\ntest['Name_length'] = test['Name'].apply(len)\n# Crate an new feature that tells whether a passenger had a cabin on the Titanic\ntrain['Has_Cabin'] = train[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)\ntest['Has_Cabin'] = test[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)\n\n# Feature engineering steps taken from Sina\n# Create a new feature FamilySize as a combination of SibSp and Parch\nfor dataset in full_data:\n    dataset['FamilySize'] = (dataset['SibSp'] + dataset['Parch'] + 1)*100\n    dataset['Pclass'] = dataset['Pclass']*10\n# Create a new feature IsAlone from FamilySize\nfor dataset in full_data:\n    dataset['IsAlone'] = -1\n    dataset.loc[dataset['FamilySize'] == 100, 'IsAlone'] = 1\n# Remove all NULLS in the Embarked column\nfor dataset in full_data:\n    dataset['Embarked'] = dataset['Embarked'].fillna('S')\n# Remove all NULLS in the Fare column and create a new feature CategoricalFare\nfor dataset in full_data:\n    dataset['Fare'] = dataset['Fare'].fillna(train['Fare'].median())\ntrain['CategoricalFare'] = pd.qcut(train['Fare'], 4)\n\n# Create a new feature CategoricalAge\nfor dataset in full_data:\n    dataset['Age'].fillna((dataset['Age'].mean()), inplace=True)\n    dataset['Age'] = dataset['Age'].astype(int)\ntrain['CategoricalAge'] = pd.cut(train['Age'], 5)\n# Define a function to extract titles from passenger names\ndef get_title(name):\n    title_search = re.search(' ([A-Za-z]+)\\.', name)\n    # If the title exists, extract and return it.\n    if title_search:\n        return title_search.group(1)\n    return \"\"\n# Create a new feature Title, containing the titles of passenger names\nfor dataset in full_data:\n    dataset['Title'] = dataset['Name'].apply(get_title)\n# Group all non-common titles into one single grouping \"Rare\"\nfor dataset in full_data:\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n\nfor dataset in full_data:\n    # Mapping Sex\n    dataset['Sex'] = dataset['Sex'].map( {'female': 2, 'male': 1} ).astype(int)\n    \n    # Mapping titles\n    title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\n    dataset['Title'] = dataset['Title'].map(title_mapping)\n    dataset['Title'] = dataset['Title'].fillna(0)\n    \n    # Mapping Embarked\n    dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\n    \n    # Mapping Fare\n    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare']                               = 0\n    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n    dataset.loc[ dataset['Fare'] > 31, 'Fare']                                  = 3\n    dataset['Fare'] = dataset['Fare'].astype(int)\n    \n    # Mapping Age\n    dataset.loc[ dataset['Age'] <= 16, 'Age']                          = 0\n    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\n    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\n    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\n    dataset.loc[ dataset['Age'] > 64, 'Age']                           = 4 ;\n    \nfor dataset in full_data:\n    # Some features of my own, it's a product of the most value previous features\n    dataset['Pclass_Sex'] = (dataset['Pclass']*dataset['Sex'])\n    dataset['Pclass_FS'] = (dataset['Pclass']*dataset['FamilySize'])\n    dataset['Pclass_IsAlone'] = (dataset['Pclass']*dataset['IsAlone'])\n    dataset['Sex_FS'] = (dataset['Sex']*dataset['FamilySize'])\n    dataset['Sex_IsAlone'] = (dataset['Sex']*dataset['IsAlone'])\n    dataset['FS_IsAlone'] = (dataset['FamilySize']*dataset['IsAlone'])\n    dataset['Pclass_Sex_FS'] = (dataset['Pclass']*dataset['Sex']*dataset['FamilySize'])\n    dataset['Pclass_Sex_IsAlone'] = (dataset['Pclass']*dataset['Sex']*dataset['IsAlone'])\n    dataset['Pclass_Sex_FS_IsAlone'] = (dataset['Pclass']*dataset['Sex']*dataset['FamilySize']*dataset['IsAlone'])\n    \n# Feature selection\ndrop_elements = ['PassengerId', 'Name', 'Ticket', 'Cabin', 'SibSp']\ntrain = train.drop(drop_elements, axis = 1)\ntrain = train.drop(['CategoricalAge'], axis = 1)\ntrain = train.drop(['CategoricalFare'], axis = 1)\ntest  = test.drop(drop_elements, axis = 1)\n\ny = train[\"Survived\"]\ntrain.drop(columns=[\"Survived\"], inplace=True)\n\ntrain.head(5)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Split the data into two parts: for training and prediction**","metadata":{}},{"cell_type":"code","source":"x_train, x_val, y_train, y_val = train_test_split(train, y, test_size=0.2)\nx_train.shape, x_val.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Random Forest\n\nRandom Forest is an ensemble of Decision Trees. The work of this algorithm can be represented as a collective decision made by some expert committee. First, each expert (decision tree) expresses their opinion. The opinions are aggregated and the final decision is reached by the head of the committee.\n\nLet's select some of the hyperparameters that are available for Random Forest: the number of trees to be used in the algorithm `n_estimators`, the depth of each tree `max_depth`, the minimum number of samples in a tree leaf `min_samples_leaf` and the maximum number of features in a tree `max_features`. More information about parameters can be found in [**Scikit-learn library documentation**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html).","metadata":{}},{"cell_type":"markdown","source":"# Intel® Extension for Scikit-learn\n\nAs was mentioned earlier, we will use a library that accelerates Scikit-learn. Patch Scikit-learn using and compare a training time to get optimizations:","metadata":{}},{"cell_type":"code","source":"!pip install scikit-learn-intelex --progress-bar off >> /tmp/pip_sklearnex.log\nfrom sklearnex import patch_sklearn\npatch_sklearn()\n\n# import Random Forest classifier\nfrom sklearn.ensemble import RandomForestClassifier","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Find optimal parameters using Optuna\n\nIt's time to adjust the hyperparameters of the algorithm to our data. To search for the optimal values of the hyperparameters, let's use [**Optuna**](https://optuna.readthedocs.io/en/stable/index.html), a hyperparameter optimization framework.","metadata":{}},{"cell_type":"code","source":"def objective(trial):\n    params = {\n        'n_estimators': trial.suggest_int('n_estimators', 100, 2000, 100),\n        'max_depth': trial.suggest_int('max_depth', 5, 20, 5),\n        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 5, 1),\n        'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2'])\n    }\n\n    rf = RandomForestClassifier(**params, random_state=777, n_jobs=-1)\n    rf.fit(x_train, y_train)\n    return rf.score(x_val, y_val)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\nsearch_space = {'n_estimators': [500, 1000, 2000],\n                'max_depth': [10, 15, 20],\n                'min_samples_leaf': [1, 2, 3],\n                'max_features': ['sqrt', 'log2']}\nstudy = optuna.create_study(sampler=optuna.samplers.GridSampler(search_space),\n                            direction=\"maximize\",\n                            pruner=optuna.pruners.MedianPruner())\nstudy.optimize(objective, show_progress_bar=True)","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Best Value: {study.best_trial.value}\")\nprint(f\"Best Params: {study.best_params}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Train the final model**\n\nNow let's train the final model using the best parameters.","metadata":{}},{"cell_type":"code","source":"%%time\n\ncl = RandomForestClassifier(**study.best_params, random_state=777)\ncl.fit(train, y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Predict using test data**","metadata":{}},{"cell_type":"code","source":"%%time\n\npredict = cl.predict(test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Save the result**","metadata":{}},{"cell_type":"code","source":"sample_submission[\"Survived\"] = predict\nsample_submission.head()\nsample_submission.to_csv('sklearnex.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Call stock Scikit-learn\n\nLet’s run the same Scikit-learn code without the patching offered by Intel® Extension for Scikit-learn and compare its execution time with the execution time of the patched Scikit-learn.","metadata":{}},{"cell_type":"code","source":"from sklearnex import unpatch_sklearn\nunpatch_sklearn()\n\n# import Random Forest classifier\nfrom sklearn.ensemble import RandomForestClassifier","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def objective(trial):\n    params = {\n        'n_estimators': trial.suggest_int('n_estimators', 100, 2000, 100),\n        'max_depth': trial.suggest_int('max_depth', 5, 20, 5),\n        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 5, 1),\n        'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2'])\n    }\n\n    rf = RandomForestClassifier(**params, random_state=777, n_jobs=-1)\n    rf.fit(x_train, y_train)\n    return rf.score(x_val, y_val)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\nsearch_space = {'n_estimators': [500, 1000, 2000],\n                'max_depth': [10, 15, 20],\n                'min_samples_leaf': [1, 2, 3],\n                'max_features': ['sqrt', 'log2']}\nstudy = optuna.create_study(sampler=optuna.samplers.GridSampler(search_space),\n                            direction=\"maximize\",\n                            pruner=optuna.pruners.MedianPruner())\nstudy.optimize(objective, show_progress_bar=True)","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Best Value: {study.best_trial.value}\")\nprint(f\"Best Params: {study.best_params}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\ncl = RandomForestClassifier(**study.best_params, random_state=777, n_jobs=-1)\ncl.fit(train, y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\npredict = cl.predict(test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Conclusions\n\nWe can see that using only one classical machine learning algorithm may give you a pretty hight accuracy score. We also use well-known libraries [Scikit-learn](https://scikit-learn.org/stable/) and [Optuna](https://optuna.readthedocs.io/en/stable/index.html), as well as the increasingly popular library [**Intel® Extension for Scikit-learn**](https://github.com/intel/scikit-learn-intelex). Noted that Intel® Extension for Scikit-learn gives you opportunities to:\n\n* Use your Scikit-learn code for training and inference without modification.\n* Train Scikit-learn models and use them for prediction up to 1.7 - 2 times faster.\n* Get predictions of the similar quality as the other tested frameworks.\n\n*Please upvote if you liked it.*","metadata":{}}]}