{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nimport plotly.io as pio\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport plotly.express as pex\nfrom plotly.subplots import make_subplots\nfrom plotly.offline import init_notebook_mode, iplot\nimport seaborn as sns\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Import & load data","metadata":{}},{"cell_type":"code","source":"df_train = pd.read_csv('../input/tabular-playground-series-apr-2021/train.csv')\ndf_test = pd.read_csv('../input/tabular-playground-series-apr-2021/test.csv')\ndf_train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from cycler import cycler\n\nmpl.rcParams['figure.dpi'] = 120\nmpl.rcParams['axes.spines.top'] = False\nmpl.rcParams['axes.spines.right'] = False\n# mpl.rcParams['font.family'] = 'serif'\n\nraw_light_palette = [\n    (0, 122, 255), # Blue\n    (255, 149, 0), # Orange\n    (52, 199, 89), # Green\n    (255, 59, 48), # Red\n    (175, 82, 222),# Purple\n    (255, 45, 85), # Pink\n    (88, 86, 214), # Indigo\n    (90, 200, 250),# Teal\n    (255, 204, 0)  # Yellow\n]\n\nlight_palette = np.array(raw_light_palette)/255\n\n\nmpl.rcParams['axes.prop_cycle'] = cycler('color',light_palette)\n\nsurvived_palette = ['#dddddd', light_palette[2]]\nsex_palette = [light_palette[0], light_palette[3]]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datetime import datetime\n\n\ntl_dates = [\n    \"WED April 10\",\n    \"SUN April 14\",\n    \"MON April 15\",\n    \"THU April 18\"\n]\n\ntl_x = [1, 2, 6, 9]\n\ntl_sub_x = [1.5, 2.4, 2.9, 3.4, 3.8, 4.5, 5.0, 6.5, 7, 7.6, 8]\ntl_sub_times = [\n    \"1:30 PM\",\n    \"9:00 AM\",\n    \"1:42 PM\",\n    \"7:15 PM\",\n    \"10:00 PM\",\n    \"11:30 PM\",\n    \"11:40 PM\",\n    \"12:20 AM\",\n    \"12:45 AM\",\n    \"2:00 AM\",\n    \"2:20 AM\",\n]\n\ntl_text = [\n    \"Titanic sets sail.\",\n    \"Recieve Message.\",\n    \"Baltic Warns Titanic\\nof icebergs.\", \n    \"Smith requests the\\n return of the message.\",\n    \"Second Officer\\n Lightroller is\\n relievced from duty.\",\n    \"Warning bells, iceberg\\n sighting.\",\n    \"Titanic hits an iceberg.\",\n    \"Life boats are being\\n lowered.\",\n    \"Passengers slowly arrive\\n on deck.\",\n    \"Rear of boat begins to\\n raise.\",\n    \"Titanic sinks.\"\n]\n\n# Set figure & Axes\nfig, ax = plt.subplots(figsize=(15, 5), constrained_layout=True)\nax.set_ylim(-2, 2)\nax.set_xlim(0, 10)\n\n\n# Timeline : line\nax.axhline(0, xmin=0.1, xmax=0.95, c='#4a4a4a', zorder=1)\n# Timeline : Date Points\nax.scatter(tl_x, np.zeros(len(tl_x)), s=120, c='#4a4a4a', zorder=2)\nax.scatter(tl_x, np.zeros(len(tl_x)), s=30, c='#fafafa', zorder=3)\n# Timeline : Time Points\nax.scatter(tl_sub_x, np.zeros(len(tl_sub_x)), s=50, c='#4a4a4a',zorder=4)\n\n# Date Text\nfor x, date in zip(tl_x, tl_dates):\n    ax.text(x, -0.2, date, ha='center', \n            fontfamily='serif', fontweight='bold',\n            color='#4a4a4a')\n    \n\n# Stemplot : vertical line\nlevels = np.zeros(len(tl_sub_x))    \nlevels[::2] = 0.3\nlevels[1::2] = -0.3\nmarkerline, stemline, baseline = ax.stem(tl_sub_x, levels, use_line_collection=True)    \nplt.setp(baseline, zorder=0)\nplt.setp(markerline, marker=',', color='#4a4a4a')\nplt.setp(stemline, color='#4a4a4a')\n\n# Text\nfor idx, x, time, txt in zip(range(1, len(tl_sub_x)+1), tl_sub_x, tl_sub_times, tl_text):\n    ax.text(x, 1.3*(idx%2)-0.5, time, ha='center', \n            fontfamily='serif', fontweight='bold',\n            color='#4a4a4a' if idx!=len(tl_sub_x) else '#e3120b', fontsize=11)\n    \n    ax.text(x, 1.3*(idx%2)-0.6, txt, va='top', ha='center', \n        fontfamily='serif',color='#4a4a4a' if idx!=len(tl_sub_x) else '#e3120b')\n\n# Spine\nfor spine in [\"left\", \"top\", \"right\", \"bottom\"]:\n    ax.spines[spine].set_visible(False)\n\n# Ticks    \nax.set_xticks([]) \nax.set_yticks([]) \n\n# Title\nax.set_title(\"Titanic Timeline\", fontweight=\"bold\", fontfamily='serif', fontsize=16, color='#4a4a4a')\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EDA","metadata":{}},{"cell_type":"code","source":"#First we chect train and test files null data -> Null data have to be fill in!!!\nfor col in df_train.columns:\n    wsg = 'column: {:>10}â‚©t Percent of NaN value: {:.2f}%'.format(col, 100*(df_train[col].isnull().sum()/df_train[col].shape[0]))\n    print(wsg)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import missingno as msno  #show the nerd data in the dataframe \n\n#.iloc[] = index location. it brings the index that we need (distribution)\n#the blank below the graph is the null\nmsno.matrix(df=df_train.iloc[:, :],figsize=(8,8),color=(0.8,0.5,0.2))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Other way to find null data - using bar (percentage)\nmsno.bar(df=df_train.iloc[:, :],figsize=(8,8),color=(0.8,0.5,0.2))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f, ax = plt.subplots(1,2,figsize=(8,6))  \n\n#explode = make a distance between the picture\n#autopct = make a percentage \n#  ax[0], ax[1] = So which part you gona put in between 0 and 1\n\ndf_train['Survived'].value_counts().plot.pie(explode=[0,0.1], autopct='%1.1f%%', ax=ax[0], shadow=True)\nax[0].set_title('Pie plot - Survived')\nax[0].set_ylabel('') #ylabel = blank\nsns.countplot('Survived', data=df_train, ax=ax[1])  #Count the Survived in the file df_train\nax[1].set_title('Count plot - Survived')\nplt.show()\n\n#The result show that this data is balanced ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize=(10, 6))\n\ngs = fig.add_gridspec(3, 4)\n\ntrain=df_train\nsurvived_palette = ['#dddddd', light_palette[2]]\nsex_palette = [light_palette[0], light_palette[3]]\n\nax_sex_survived = fig.add_subplot(gs[:2,:2])\nsns.countplot(x='Sex',hue='Survived', data=train, ax=ax_sex_survived, \n              palette=survived_palette)\n\nax_survived_sex = fig.add_subplot(gs[:2,2:4], sharey=ax_sex_survived)\nsns.countplot(x='Survived',hue='Sex', data=train, ax=ax_survived_sex,\n              palette=sex_palette\n             )\n# ax_survived_sex.set_yticks([])\nax_survived_sex.set_ylabel('')\n\nax_pie_male = fig.add_subplot(gs[2, 0])\nax_pie_female = fig.add_subplot(gs[2, 1])\nax_pie_notsurvived = fig.add_subplot(gs[2, 2])\nax_pie_survived = fig.add_subplot(gs[2, 3])\n\n# Sex\nmale = train[train['Sex']=='male']['Survived'].value_counts().sort_index()\nax_pie_male.pie(male, labels=male.index, autopct='%1.1f%%',explode = (0, 0.1), startangle=90,\n               colors=survived_palette\n               )\n\nfemale = train[train['Sex']=='female']['Survived'].value_counts().sort_index()\nax_pie_female.pie(female, labels=female.index, autopct='%1.1f%%',explode = (0, 0.1), startangle=90,\n                colors=survived_palette\n                 )\n\n# Survived\nnotsurvived = train[train['Survived']==0]['Sex'].value_counts()[['male', 'female']]\nax_pie_notsurvived.pie(notsurvived, labels=notsurvived.index, autopct='%1.1f%%',startangle=90,\n                      colors=sex_palette, textprops={'color':\"w\"}\n                      )\n\nsurvived = train[train['Survived']==1]['Sex'].value_counts()[['male', 'female']]\nax_pie_survived.pie(survived, labels=survived.index, autopct='%1.1f%%', startangle=90,\n                    colors=sex_palette, textprops={'color':\"w\"}\n                   )\n\nfig.suptitle('[Sex & Survived] Conditional Distribution', fontweight='bold', fontsize=20)\nfig.text(s='''Gender and survival are the most important features of the existing Titanic problem.\\nLook at each conditional probability and think of the minimum score''', \n         x=0.5, y= 0.94, ha='center', va='top')\n\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train[['Pclass','Survived']].groupby(['Pclass'], as_index=True).count()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.crosstab(df_train['Pclass'],df_train['Survived'], margins=True).style.background_gradient(cmap='cool')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f, ax = plt.subplots(1,2,figsize=(10,5))\ndf_train[['Sex','Survived']].groupby(['Sex'],as_index=True).mean().plot.bar(ax=ax[0])\nax[0].set_title('Survived vs Sex')\nsns.countplot('Sex', hue= 'Survived', data = df_train, ax=ax[1])\nax[1].set_title('Sex: Survived vs Dead')\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.crosstab(df_train['Sex'], df_train['Survived'], margins=True).style.background_gradient(cmap='summer_r')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Let me draw a histogram of the Age of Survival.\nfig, ax = plt.subplots(1,1,figsize=(9,5))\nsns.kdeplot(df_train[df_train['Survived']==1]['Age'],ax=ax)\nsns.kdeplot(df_train[df_train['Survived']==0]['Age'],ax=ax)\nplt.legend(['Survived == 1', 'Survived == 0'])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Survived0 = train[train['Survived'] == 0]['Age']\nSurvived1 = train[train['Survived'] == 1]['Age']\n\nfig = go.Figure()\nfig.add_trace(go.Violin(x=Survived0, line_color='salmon', name='Survived0'))\nfig.add_trace(go.Violin(x=Survived1, line_color='gold', name= 'Survived1'))\n\n\nfig.update_traces(orientation='h', side='positive', width=3, points=False, meanline_visible=True)\nfig.update_layout(xaxis_showgrid=True, xaxis_zeroline=False)\n\nfig.update_layout(title='Survival-Age distn.',\n                  xaxis_title='Age', \n    width=750,\n    template=\"plotly_dark\",\n    showlegend=False,\n    paper_bgcolor=\"black\",\n    font=dict(\n        color ='white', \n    )\n )\nfig.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def age_band(num):\n    for i in range(1, 100):\n        if num < 10*i :  return f'{(i-1) * 10} ~ {i*10}'\n\ntrain['Age band'] = train['Age'].apply(age_band)\ntitanic_age = train[['Age band', 'Survived']].groupby('Age band')['Survived'].value_counts().sort_index().unstack().fillna(0)\ntitanic_age['Survival rate'] = titanic_age[1] / (titanic_age[0] + titanic_age[1]) * 100\nage_band = train['Age band'].value_counts().sort_index()\n\n\nfrom mpl_toolkits.axes_grid1.axes_divider import make_axes_locatable\n\nfig = plt.figure(figsize=(10, 7))\ngs = fig.add_gridspec(3, 4)\nax = fig.add_subplot(gs[:-1,:])\n\ncolor_map = ['#d4dddd' for _ in range(9)]\ncolor_map[2] = light_palette[3]\ncolor_map[8] = light_palette[2]\n\n\nbars = ax.bar(titanic_age['Survival rate'].index, titanic_age['Survival rate'], \n       color=color_map, width=0.55, \n       edgecolor='black', \n       linewidth=0.7)\n\nax.spines[[\"top\",\"right\",\"left\"]].set_visible(False)\nax.bar_label(bars, fmt='%.2f%%')\n\n\n# mean line + annotation\nmean = train['Survived'].mean() *100\nax.axhline(mean ,color='black', linewidth=0.4, linestyle='dashdot')\nax.annotate(f\"mean : {mean :.4}%\", \n            xy=('20 ~ 30', mean + 4),\n            va = 'center', ha='center',\n            color='#4a4a4a',\n            bbox=dict(boxstyle='round', pad=0.4, facecolor='#efe8d1', linewidth=0))\n    \n\n\nax.set_yticks(np.arange(0, 81, 20))\nax.grid(axis='y', linestyle='-', alpha=0.4)\nax.set_ylim(0, 85)\n\n\nax_bottom = fig.add_subplot(gs[-1,:])\nbars = ax_bottom.bar(age_band.index, age_band, width=0.55, \n       edgecolor='black', \n       linewidth=0.7)\n\nax_bottom.spines[[\"top\",\"right\",\"left\"]].set_visible(False)\nax_bottom.bar_label(bars, fmt='%d', label_type='center', color='white')\nax_bottom.grid(axis='y', linestyle='-', alpha=0.4)\n\n# Title & Subtitle    \nfig.text(0.1, 1, 'Age Band & Survival Rate', fontsize=15, fontweight='bold', fontfamily='serif', ha='left')\nfig.text(0.1, 0.96, 'Unlike before, the survival rate of infants and toddlers is very low.', fontsize=12, fontweight='light', fontfamily='serif', ha='left')\n\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sibsp = train.groupby('SibSp')['Survived'].mean().sort_index()*100\nparch = train.groupby('Parch')['Survived'].mean().sort_index()*100\n\nfig, axes = plt.subplots(1, 2, figsize=(10, 5), sharey=True)\naxes[0].barh(width=100, y=sibsp.index, color='#dedede')\nhbar1 = axes[0].barh(width=sibsp, y=sibsp.index, color=light_palette[2])\n\naxes[0].bar_label(hbar1, fmt='%.02f%%', padding=2)\n\naxes[1].barh(width=100, y=parch.index, color='#dedede')\nhbar2 = axes[1].barh(width=parch, y=parch.index, color=light_palette[2])\naxes[1].bar_label(hbar2, fmt='%.02f%%', padding=2)\n\naxes[0].set_yticks(range(0, max(parch.index)+1))\naxes[0].invert_yaxis()\n\naxes[0].set_xticks([])\naxes[1].set_xticks([])\naxes[0].spines[['bottom', 'left']].set_visible(False)\naxes[1].spines[['bottom', 'left']].set_visible(False)\n\naxes[0].axvline(mean ,color='black', linewidth=0.4, linestyle='dashdot')\naxes[1].axvline(mean ,color='black', linewidth=0.4, linestyle='dashdot')\n\nfor ax in axes:\n    ax.annotate(f\"mean : {mean :.4}%\", \n            xy=(mean + 4, 7),\n            va = 'center', ha='left',\n            color='#4a4a4a',\n            bbox=dict(boxstyle='round', pad=0.4, facecolor='#efe8d1', linewidth=0))\n    \n\naxes[0].set_title('SibSp Survived Ratio', zorder=0)\naxes[1].set_title('Parch Survived Ratio', zorder=0)\n\n\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Age distribution withing classes, by using hist plot we can see easily \nplt.figure(figsize=(8,4))\ndf_train['Age'][df_train['Pclass']==1].plot(kind='hist')\ndf_train['Age'][df_train['Pclass']==2].plot(kind='hist')\ndf_train['Age'][df_train['Pclass']==3].plot(kind='hist')\n\nplt.xlabel('Age')\nplt.title('Age Distribution within classes')\nplt.legend(['1st Class', '2nd Class', '3rd Class'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize=(8, 5))\ngs = fig.add_gridspec(3,1)\ngs.update(hspace= -0.55)\n\naxes = list()\ncolors = light_palette[-3:]\n\nfor idx, cls, c in zip(range(3), sorted(train['Pclass'].unique()), colors):\n    axes.append(fig.add_subplot(gs[idx, 0]))\n    \n    # you can also draw density plot with matplotlib + scipy.\n    sns.kdeplot(x='Age', data=train[train['Pclass']==cls], \n                fill=True, ax=axes[idx], cut=0, bw_method=0.20, \n                lw=1.4, edgecolor='lightgray',color=c, alpha=1) \n    \n    axes[idx].set_ylim(0, 0.035)\n    axes[idx].set_xlim(0, 85)\n    \n    axes[idx].set_yticks([])\n    if idx != 2 : axes[idx].set_xticks([])\n    axes[idx].set_ylabel('')\n    axes[idx].set_xlabel('')\n\n\n    axes[idx].spines[[\"top\",\"right\",\"left\",\"bottom\"]].set_visible(False)\n        \n    axes[idx].patch.set_alpha(0)\n    axes[idx].text(-0.2,0,f'Pclass {cls}',fontweight=\"light\", fontfamily='serif', fontsize=11,ha=\"right\")\n\nfig.text(0.13,0.81,\"Age distribution by Pclass in Titanic\", fontweight=\"bold\", fontfamily='serif', fontsize=16)\nplt.show()    \n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cummulate_survival_ratio = []\n#Survival of age by showing the trend.\n\nfor i in range(1, 80):\n    cummulate_survival_ratio.append(df_train[df_train['Age'] < i]['Survived'].sum() / len(df_train[df_train['Age'] < i]['Survived']))\n\nplt.figure(figsize=(7, 7))\nplt.plot(cummulate_survival_ratio)\nplt.title('Survival rate change depending on range of Age', y=1.02)\nplt.ylabel('Survival rate')\nplt.xlabel('Range of Age(0~x)')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f, ax = plt.subplots(1,2,figsize=(18,8))\nsns.violinplot('Pclass','Age', hue='Survived', data=df_train, scale='count', split=True, ax=ax[0])\nax[0].set_title('Pclass and Age vs Survived')\nax[0].set_yticks(range(0,110,10))\n\nsns.violinplot('Sex','Age', hue='Survived', data=df_train, scale='count',split=True, ax=ax[1])\nax[1].set_title('sex and Age vs Survived')\nax[1].set_yticks(range(0,110,10))\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axes = plt.subplots(2, 1, figsize=(8 , 5), sharex=True)\n\nfor idx, feature in enumerate(['Pclass', 'Embarked']):\n    sns.heatmap(train.groupby([feature, 'Age band'])['Survived'].aggregate('mean').unstack()*100, ax=axes[idx],\n                square=True, annot=True, fmt='.2f', center=mean, linewidth=2,\n                cbar=False, cmap=sns.diverging_palette(240, 10, as_cmap=True)\n               ) \n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#We can combine data because it is combined with number \ndf_train['FamilySize'] = df_train['SibSp'] + df_train['Parch'] + 1 #we have to add 1 because we have include oneself\ndf_test['FamilySize'] = df_test['SibSp'] + df_train['Parch'] + 1 #we have to add 1 because we have include oneself\n\nf,ax=plt.subplots(1, 3, figsize=(40,10))\nsns.countplot('FamilySize', data=df_train, ax=ax[0])\nax[0].set_title('(1) No. Of Passengers Boarded', y=1.02)\n\nsns.countplot('FamilySize', hue='Survived', data=df_train, ax=ax[1])\nax[1].set_title('(2) Survived countplot depending on FamilySize',  y=1.02)\n\ndf_train[['FamilySize', 'Survived']].groupby(['FamilySize'], as_index=True).mean().sort_values(by='Survived', ascending=False).plot.bar(ax=ax[2])\nax[2].set_title('(3) Survived rate depending on FamilySize',  y=1.02)\n\nplt.subplots_adjust(wspace=0.2, hspace=0.5)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feat. importance","metadata":{}},{"cell_type":"markdown","source":"## Boruta","metadata":{}},{"cell_type":"code","source":"df_train","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from boruta import BorutaPy\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import LabelEncoder\n\n###initialize Boruta\nforest = RandomForestRegressor(\n   n_jobs = -1, \n   max_depth = 5\n)\nboruta = BorutaPy(\n   estimator = forest, \n   n_estimators = 'auto',\n   max_iter = 100, # number of trials to perform\n   verbose=1\n)\nprint(df_train.isnull().sum(axis = 0))\n#df_boruta=df_train.drop(['Name', 'Fare', 'PassengerId','Ticket','Cabin'], axis=1)\ndf_boruta=df_train.drop(['Name',  'PassengerId','Ticket','Cabin'], axis=1)\n#df_boruta=df_train.drop(['Name', 'Fare', 'Ticket','Cabin'], axis=1)\n# integer encode\nlabel_encoder = LabelEncoder()\ndf_boruta.Sex= label_encoder.fit_transform(df_boruta.Sex)\ndf_boruta.Embarked = label_encoder.fit_transform(df_boruta.Embarked)\ndf_boruta = df_boruta.dropna()\n\nprint(df_boruta.head())\n\n### fit Boruta (it accepts np.array, not pd.DataFrame)\nboruta.fit(np.array(df_boruta.drop(['Survived'], axis=1)), np.array(df_boruta.Survived))\n### print results\ngreen_area = df_boruta.drop(['Survived'], axis=1).columns[boruta.support_].to_list()\nblue_area = df_boruta.drop(['Survived'], axis=1).columns[boruta.support_weak_].to_list()\nprint('features in the green area:', green_area)\nprint('features in the blue area:', blue_area)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### store feature importances\nfeat_imp_X = forest.feature_importances_[:len(df_boruta.drop(['Survived'], axis=1).columns)]\nfeat_imp_shadow = forest.feature_importances_[len(df_boruta.drop(['Survived'],axis=1).columns):]\n### compute hits\nhits = feat_imp_X > feat_imp_shadow.max()\nfeat_imp_X","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_features = list()\nindexes = np.where(boruta.support_ == True)\nfor x in np.nditer(indexes):\n    final_features.append(feat_imp_X[x])\nprint(final_features)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## SHAP","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\ndf_shap=df_train.drop(['Name',  'PassengerId','Ticket','Cabin'], axis=1)\n#df_boruta=df_train.drop(['Name', 'Fare', 'Ticket','Cabin'], axis=1)\n# integer encode\nlabel_encoder = LabelEncoder()\ndf_shap.Sex= label_encoder.fit_transform(df_shap.Sex)\ndf_shap.Embarked = label_encoder.fit_transform(df_shap.Embarked)\ndf_shap = df_shap.dropna()\nprint(df_shap.head(10))\n\nx_train, x_test, y_train, y_test = train_test_split(df_shap.drop(['Survived'], axis=1), df_shap.Survived, test_size=0.02, random_state=0)\n\nfrom sklearn.linear_model import LogisticRegression\n\nLR = LogisticRegression()\nLR.fit(x_train, y_train)\nLR.score(x_test, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import shap\nexplainer = shap.LinearExplainer(LR, x_train, feature_perturbation=\"interventional\")\nshap_values = explainer.shap_values(x_test)\nshap.summary_plot(shap_values, x_test)\nshap.summary_plot(shap_values, x_train, plot_type=\"bar\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"shap.initjs()\nshap.force_plot(explainer.expected_value, shap_values[3,:], x_test.iloc[3,:], link=\"logit\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"shap.initjs()\nshap.force_plot(explainer.expected_value, shap_values, x_test, link=\"logit\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Cramer","metadata":{}},{"cell_type":"code","source":"from scipy import stats\n\n## I borrowred this code snippet from https://towardsdatascience.com/the-search-for-categorical-correlation-a1cf7f1888c9\ndef cramers_v(x, y):\n    confusion_matrix = pd.crosstab(x,y)\n    chi2 = stats.chi2_contingency(confusion_matrix)[0]\n    n = confusion_matrix.sum().sum()\n    phi2 = chi2/n\n    r,k = confusion_matrix.shape\n    phi2corr = max(0, phi2-((k-1)*(r-1))/(n-1))\n    rcorr = r-((r-1)**2)/(n-1)\n    kcorr = k-((k-1)**2)/(n-1)\n    return np.sqrt(phi2corr/min((kcorr-1),(rcorr-1)))\n\ncategoricals = ['Sex', 'Pclass', 'Embarked', 'SibSp', 'Parch']\nprint(\"Cramer's -V categorival features correlation with Survival \")\nprint('**********************************************************')\nfor cats in categoricals:\n    print('Correlation between {} and survival is {:.2f}'.format(cats, cramers_v(df_train[cats], df_train['Survived'])))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train=df_train\nPClass1_1 = train[(train['Pclass'] == 1) &(train['Survived'] == 1)]['Age']\nPClass1_0 = train[(train['Pclass'] == 1) &(train['Survived'] == 0)]['Age']\nPClass2_1 = train[(train['Pclass'] == 2) &(train['Survived'] == 1)]['Age']\nPClass2_0 = train[(train['Pclass'] == 2) &(train['Survived'] == 0)]['Age']\nPClass3_1 = train[(train['Pclass'] == 3) &(train['Survived'] == 1)]['Age']\nPClass3_0 = train[(train['Pclass'] == 3) &(train['Survived'] == 0)]['Age']\n\n\nfig = go.Figure()\n\nfig.add_trace(go.Violin(x=PClass1_1, line_color='salmon', name='PClass1_[1]', ))\nfig.add_trace(go.Violin(x=PClass1_0, line_color='lightsalmon', name= 'PClass1_[0]', ))\nfig.add_trace(go.Violin(x=PClass2_1, line_color='seagreen', name='PClass2_[1]', ))\nfig.add_trace(go.Violin(x=PClass2_0, line_color='lightseagreen', name='PClass2_[0]', ))\nfig.add_trace(go.Violin(x=PClass3_1, line_color='gold', name= 'PClass3_[1]', ))\nfig.add_trace(go.Violin(x=PClass3_0, line_color='silver', name='PClass3_[0]', ))\n\nfig.update_traces(orientation='h', side='positive', width=3,\n                  bandwidth = None, points=False, meanline_visible=True, scalemode='count')\nfig.update_layout(xaxis_showgrid=True, xaxis_zeroline=False)\n\nfig.update_layout(title='Pclass-Age Survival',\n                  font_family=\"San Serif\",\n                  xaxis_title='Age',\n                  width=600,height=400,\n                  template=\"plotly_dark\",\n                  showlegend=False,\n                  titlefont={'size': 24},\n                  paper_bgcolor=\"black\",\n                  font=dict(\n                      color ='white',\n                      )\n                  )\nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train=df_train\nsuv = train.groupby(['Survived', 'Sex', 'Pclass']).agg({'Survived': 'count'}).rename(columns = {'Survived': 'count'}).reset_index()\nsuv.iloc[0:6, 0] = 'Not survived'\nsuv.iloc[6:, 0] = 'Survived'\nfor i in range(len(suv.index)):\n    suv.iloc[i,2] = str(suv.iloc[i,2]) + ' class'\n\nfig = px.sunburst(suv, path = ['Survived', 'Sex', 'Pclass'], values = 'count', color = 'Survived',\n                 color_discrete_map = {'Not survived': '#A01D26', 'Survived': '#ACBEBE'},\n                 width = 700, height = 700)\n\nfig.update_layout(annotations = [dict(text = 'Distribution of male and female survival rates by class', \n                                      x = 0.5, y = 1.1, font_size = 24, showarrow = False, \n                                      font_family = 'Calibri Black',\n                                      font_color = 'black')])\n\nfig.update_traces(textinfo = 'label + percent parent')\n                  \nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## LIME","metadata":{}},{"cell_type":"code","source":"import lime\nimport lime.lime_tabular\n\ndf_lime=df_train.drop(['Name',  'PassengerId','Ticket','Cabin'], axis=1)\n#df_boruta=df_train.drop(['Name', 'Fare', 'Ticket','Cabin'], axis=1)\n# integer encode\nlabel_encoder = LabelEncoder()\ndf_lime.Sex= label_encoder.fit_transform(df_lime.Sex)\ndf_lime.Embarked = label_encoder.fit_transform(df_lime.Embarked)\ndf_lime = df_lime.dropna()\n\n\nx_train, x_test, y_train, y_test = train_test_split(df_lime.drop(['Survived'], axis=1), df_lime.Survived, test_size=0.2, random_state=0)\ndef predict_fn(x):\n    preds = clf.predict(x, num_iteration=clf.best_iteration).reshape(-1,1)\n    p0 = 1 - preds\n    return np.hstack((p0, preds))\n\nexplainerLime = lime.lime_tabular.LimeTabularExplainer(\n    x_train.values,\n    mode='classification',\n    feature_names=df_lime.columns,\n   class_names=[\"NotSurvived\", \"Survived\"],\n   verbose=True\n    )\n\nnp.random.seed(1)\ni = 0\nexp = explainerLime.explain_instance(x_train[df_lime.columns].values[i], predict_fn, num_features=10)\nexp.show_in_notebook(show_all=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# PrÃ©dictions","metadata":{}},{"cell_type":"markdown","source":"## mljar","metadata":{}},{"cell_type":"code","source":"!pip install -q -U git+https://github.com/mljar/mljar-supervised.git@master\nfrom supervised.automl import AutoML # mljar-supervised\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train=df_train\nx_cols = train.columns[2:].tolist()\ny_col = train.columns[1]\n\nautoml = AutoML(\n    mode=\"Compete\", \n    eval_metric=\"auc\",\n    total_time_limit=2*360,\n#    total_time_limit=2*3600,\n    features_selection=False # switch off feature selection\n)\nautoml.fit(train[x_cols], train[y_col])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"automl.report()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = automl.predict(test[x_cols])\nsubmission = pd.DataFrame({'PassengerId':test.iloc[:,0], 'Survived': preds})\nsubmission.to_csv('1_submission.csv', index=False)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Naive Bayes Classifier\n\nNotice we have the Name of each passenger. We won't use that feature for our classifier because it is not significant for our problem. We'll also get rid of the Fare feature because it is continuous and our features need to be discrete.\n\nThere are Naive Bayes Classifiers that support continuous features. For example, the Gaussian Naive Bayes Classifier.","metadata":{}},{"cell_type":"code","source":"df_nbc_train=df_train.drop(['Name', 'Fare', 'PassengerId','Ticket','Cabin'], axis=1)\ndf_nbc_test=df_train.drop(['Name', 'Fare','PassengerId','Ticket','Cabin'], axis=1)\n#print(len(df_nbc_train.drop(['Survived'], axis=1)[0]))\nprint(df_nbc_train.drop(['Survived'], axis=1).values[0])\n\ndf_nbc_train.head()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class NaiveBayesClassifier:\n    \n    def __init__(self, X, y):\n        \n        '''\n        X and y denotes the features and the target labels respectively\n        '''\n        self.X, self.y = X, y \n        \n        self.N = len(self.X) # Length of the training set\n\n        self.dim = len(self.X[0]) # Dimension of the vector of features\n\n        self.attrs = [[] for _ in range(self.dim)] # Here we'll store the columns of the training set\n\n        self.output_dom = {} # Output classes with the number of ocurrences in the training set. In this case we have only 2 classes\n\n        self.data = [] # To store every row [Xi, yi]\n        \n        \n        for i in range(len(self.X)):\n            for j in range(self.dim):\n                # if we have never seen this value for this attr before, \n                # then we add it to the attrs array in the corresponding position\n                if not self.X[i][j] in self.attrs[j]:\n                    self.attrs[j].append(self.X[i][j])\n                    \n            # if we have never seen this output class before,\n            # then we add it to the output_dom and count one occurrence for now\n            if not self.y[i] in self.output_dom.keys():\n                self.output_dom[self.y[i]] = 1\n            # otherwise, we increment the occurrence of this output in the training set by 1\n            else:\n                self.output_dom[self.y[i]] += 1\n            # store the row\n            self.data.append([self.X[i], self.y[i]])\n            \n            \n\n    def classify(self, entry):\n\n        solve = None # Final result\n        max_arg = -1 # partial maximum\n\n        for y in self.output_dom.keys():\n\n            prob = self.output_dom[y]/self.N # P(y)\n\n            for i in range(self.dim):\n                cases = [x for x in self.data if x[0][i] == entry[i] and x[1] == y] # all rows with Xi = xi\n                n = len(cases)\n                prob *= n/self.N # P *= P(Xi = xi)\n                \n            # if we have a greater prob for this output than the partial maximum...\n            if prob > max_arg:\n                max_arg = prob\n                solve = y\n\n        return solve","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"values=df_nbc_test.drop(['Survived'],axis=1).values\nprint (len(values))\n\n\n## Creating the Naive Bayes Classifier instance with the training data\nnbc = NaiveBayesClassifier(df_nbc_train.drop(['Survived'],axis=1).values,df_nbc_train['Survived'].values)\n\n\ntotal_cases = len(df_nbc_test) # size of validation set\n\n# Well classified examples and bad classified examples\ngood = 0\nbad = 0\nfor i in range(total_cases):\n    predict = nbc.classify(values[i])\n    if (i % 100 == 0):\n        print(i)\n#     print(y_val[i] + ' --------------- ' + predict)\n    if df_nbc_test.Survived.values[i] == predict:\n        good += 1\n    else:\n        bad += 1\n\nprint('TOTAL EXAMPLES:', total_cases)\nprint('RIGHT:', good)\nprint('WRONG:', bad)\nprint('ACCURACY:', good/total_cases)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## LightAutoML","metadata":{}},{"cell_type":"code","source":"pip install -U lightautoml","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}