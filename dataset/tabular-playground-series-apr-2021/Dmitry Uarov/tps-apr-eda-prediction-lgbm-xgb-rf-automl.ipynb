{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"![](https://storage.googleapis.com/kaggle-competitions/kaggle/26478/logos/header.png?t=2021-03-29-17-07-0)","metadata":{}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport plotly as py\nimport plotly.graph_objs as go\nimport plotly.express as px\nfrom plotly.offline import init_notebook_mode\ninit_notebook_mode(connected = True)\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import LabelEncoder\n\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc, accuracy_score\nfrom sklearn.metrics import roc_auc_score, precision_score, recall_score, f1_score\n\nfrom sklearn.model_selection import train_test_split, cross_val_score\n\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.model_selection import KFold\nimport optuna\n\npd.set_option('display.max_columns', None)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('../input/tabular-playground-series-apr-2021/train.csv')\ntest = pd.read_csv('../input/tabular-playground-series-apr-2021/test.csv')\nID = test['PassengerId']","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Basic information","metadata":{}},{"cell_type":"code","source":"train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Great, we have missing values! Let's impute them.","metadata":{}},{"cell_type":"markdown","source":"# Preprocessing","metadata":{}},{"cell_type":"markdown","source":"**I will use all information to imputing missing values**","metadata":{}},{"cell_type":"code","source":"all_data = pd.concat([train, test])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Imputing embarked**","metadata":{}},{"cell_type":"code","source":"emb = pd.DataFrame(all_data.groupby(['Pclass', 'Sex', 'SibSp'])['Embarked'].apply(pd.Series.mode).reset_index()).drop('level_3', axis = 1)\n\nfor i in range(len(train.index)):\n    if pd.isna(train.iloc[i,11]) == True:\n        if train.iloc[i,2] == 1 and train.iloc[i,4] == 'female' and train.iloc[i,6] <= 2:\n            train.iloc[i,11] = 'C'\n        else:\n            train.iloc[i,11] = 'S'\n            \nfor i in range(len(test.index)):\n    if pd.isna(test.iloc[i,10]) == True:\n        if test.iloc[i,1] == 1 and test.iloc[i,3] == 'female' and test.iloc[i,5] <= 2:\n            test.iloc[i,10] = 'C'\n        else:\n            test.iloc[i,10] = 'S'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Imputing age**","metadata":{}},{"cell_type":"code","source":"ages = all_data.groupby(['Pclass', 'Sex', 'SibSp', 'Embarked']).agg({'Age': 'mean'}).reset_index()\n\nfor i in range(len(train.index)):\n    if pd.isna(train.iloc[i,5]) == True:\n        for k in range(len(ages.index)):\n            if train.iloc[i,2] == ages.iloc[k,0] and train.iloc[i,4] == ages.iloc[k,1] and train.iloc[i,6] == ages.iloc[k,2] and train.iloc[i,11] == ages.iloc[k,3]:\n                train.iloc[i,5] = ages.iloc[k,4]\n                \nfor i in range(len(test.index)):\n    if pd.isna(test.iloc[i,4]) == True:\n        for k in range(len(ages.index)):\n            if test.iloc[i,1] == ages.iloc[k,0] and test.iloc[i,3] == ages.iloc[k,1] and test.iloc[i,5] == ages.iloc[k,2] and test.iloc[i,10] == ages.iloc[k,3]:\n                test.iloc[i,4] = ages.iloc[k,4]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Imputing fare**","metadata":{}},{"cell_type":"code","source":"fare = all_data.groupby(['Pclass', 'Sex', 'Embarked']).agg({'Fare': 'mean'}).reset_index()\n\nfor i in range(len(train.index)):\n    if pd.isna(train.iloc[i,9]) == True:\n        for k in range(len(fare.index)):\n            if train.iloc[i,2] == fare.iloc[k,0] and train.iloc[i,4] == fare.iloc[k,1] and train.iloc[i,11] == fare.iloc[k,2]:\n                train.iloc[i,9] = fare.iloc[k,3]\n                \nfor i in range(len(test.index)):\n    if pd.isna(test.iloc[i,8]) == True:\n        for k in range(len(fare.index)):\n            if test.iloc[i,1] == fare.iloc[k,0] and test.iloc[i,3] == fare.iloc[k,1] and test.iloc[i,10] == fare.iloc[k,2]:\n                test.iloc[i,8] = fare.iloc[k,3]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Delete unnecessary columns**","metadata":{}},{"cell_type":"code","source":"train = train.drop(['PassengerId', 'Ticket', 'Cabin', 'Name'], axis = 1)\ntest = test.drop(['PassengerId', 'Ticket', 'Cabin', 'Name'], axis = 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EDA","metadata":{}},{"cell_type":"markdown","source":"**Distribution of survivors**","metadata":{}},{"cell_type":"code","source":"fig = px.pie(train['Survived'].value_counts().reset_index(), values = 'Survived', names = ['Not survived', 'Survived'],\n                 width = 600, height = 600)\nfig.update_traces(textposition = 'inside', \n                  textinfo = 'percent + label', \n                  hole = 0.78, \n                  marker = dict(colors = ['#A01D26','#ACBEBE'], line = dict(color = 'white', width = 2)))\n\nfig.update_layout(title_text = 'Survivors', title_x = 0.5, title_y = 0.53, title_font_size = 32, title_font_family = 'Calibri Black', title_font_color = 'black',\n                  showlegend = False)\n                  \nfig.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Affect of class, gender and embarked on survival**","metadata":{}},{"cell_type":"code","source":"pclass = train.groupby(['Pclass', 'Survived']).agg({'Survived': 'count'}).rename(columns = {'Survived': 'count'}).reset_index()\nsex = train.groupby(['Sex', 'Survived']).agg({'Survived': 'count'}).rename(columns = {'Survived': 'count'}).reset_index()\nembarked = train.groupby(['Embarked', 'Survived']).agg({'Survived': 'count'}).rename(columns = {'Survived': 'count'}).reset_index()\n\ndef percent(data):\n    data['percent'] = 0\n    for i in range(len(data.index)):\n        if data.index[i] % 2 == 0:\n            data.iloc[i, 3] = round((data.iloc[i, 2] / (data.iloc[i, 2] + data.iloc[i+1, 2])) * 100, 1)\n        else:\n            data.iloc[i, 3] = 100 - data.iloc[i-1, 3]\n            \npercent(pclass)\npercent(sex)\npercent(embarked)\n\npclass.iloc[[0,2,4], 1] = 'Not survived'\npclass.iloc[[1,3,5], 1] = 'Survived'","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize = (18, 18))\nfig.patch.set_facecolor('#fafafa')\n\nplt.subplot(321)\nsns.set_style('white')\nplt.title('Class', size = 20, x = 1.1, y = 1.03)\nplt.grid(color = 'gray', linestyle = ':', axis = 'y', zorder = 0,  dashes = (1,7))\na = sns.barplot(data = pclass, x = pclass['Pclass'], y = pclass['count'], hue = pclass['Survived'], palette = ['#A01D26','#ACBEBE'])\nfor p in a.patches:\n    height = p.get_height()\n    a.annotate(f'{height:g}', (p.get_x() + p.get_width() / 2, p.get_height()), \n                   ha = 'center', va = 'center', \n                   size = 10,\n                   xytext = (0, 5), \n                   textcoords = 'offset points')\nplt.ylabel('')\nplt.xlabel('')\nplt.legend(loc = 'upper left')\n\n\nplt.subplot(322)\nplt.grid(color = 'gray', linestyle = ':', axis = 'y', zorder = 0,  dashes = (1,7))\naa = sns.barplot(data = pclass, x = pclass['Pclass'], y = pclass['percent'], hue = pclass['Survived'], palette = ['#A01D26','#ACBEBE'])\nfor p in aa.patches:\n    height = p.get_height()\n    aa.annotate(f'{height:g}%', (p.get_x() + p.get_width() / 2, p.get_height()), \n                   ha = 'center', va = 'center', \n                   size = 10,\n                   xytext = (0, 5), \n                   textcoords = 'offset points')\nplt.ylabel('')\nplt.xlabel('')\nplt.legend('').set_visible(False)\n\nplt.subplot(323)\nplt.title('Gender', size = 20, x = 1.1, y = 1.03)\na2 = sns.barplot(data = sex, x = sex['Sex'], y = sex['count'], hue = sex['Survived'], palette = ['#A01D26','#ACBEBE'])\nfor p in a2.patches:\n    height = p.get_height()\n    a2.annotate(f'{height:g}', (p.get_x() + p.get_width() / 2, p.get_height()), \n                   ha = 'center', va = 'center', \n                   size = 10,\n                   xytext = (0, 5), \n                   textcoords = 'offset points')\nplt.grid(color = 'gray', linestyle = ':', axis = 'y', zorder = 0,  dashes = (1,7))\nplt.ylabel('')\nplt.xlabel('')\nplt.legend('').set_visible(False)\n\nplt.subplot(324)\nplt.grid(color = 'gray', linestyle = ':', axis = 'y', zorder = 0,  dashes = (1,7))\naa2 = sns.barplot(data = sex, x = sex['Sex'], y = sex['percent'], hue = sex['Survived'], palette = ['#A01D26','#ACBEBE'])\nfor p in aa2.patches:\n    height = p.get_height()\n    aa2.annotate(f'{height:g}%', (p.get_x() + p.get_width() / 2, p.get_height()), \n                   ha = 'center', va = 'center', \n                   size = 10,\n                   xytext = (0, 5), \n                   textcoords = 'offset points')\nplt.ylabel('')\nplt.xlabel('')\nplt.legend('').set_visible(False)\n\nplt.subplot(325)\nplt.title('Embarked', size = 20, x = 1.1, y = 1.03)\na3 = sns.barplot(data = embarked, x = embarked['Embarked'], y = embarked['count'], hue = embarked['Survived'], palette = ['#A01D26','#ACBEBE'])\nfor p in a3.patches:\n    height = p.get_height()\n    a3.annotate(f'{height:g}', (p.get_x() + p.get_width() / 2, p.get_height()), \n                   ha = 'center', va = 'center', \n                   size = 10,\n                   xytext = (0, 5), \n                   textcoords = 'offset points')\nplt.grid(color = 'gray', linestyle = ':', axis = 'y', zorder = 0,  dashes = (1,7))\nplt.ylabel('')\nplt.xlabel('')\nplt.legend('').set_visible(False)\n\nplt.subplot(326)\nplt.grid(color = 'gray', linestyle = ':', axis = 'y', zorder = 0,  dashes = (1,7))\naa3 = sns.barplot(data = embarked, x = embarked['Embarked'], y = embarked['percent'], hue = embarked['Survived'], palette = ['#A01D26','#ACBEBE'])\nfor p in aa3.patches:\n    height = p.get_height()\n    aa3.annotate(f'{height:g}%', (p.get_x() + p.get_width() / 2, p.get_height()), \n                   ha = 'center', va = 'center', \n                   size = 10,\n                   xytext = (0, 5), \n                   textcoords = 'offset points')\nplt.ylabel('')\nplt.xlabel('')\nplt.legend('').set_visible(False)\n\n\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Affect of age, SibSp, parch and fare on survival**","metadata":{}},{"cell_type":"code","source":"fig = plt.figure(figsize = (18, 18))\nfig.patch.set_facecolor('#fafafa')\n\nplt.subplot(221)\nsns.set_style('white')\nplt.title('Age', size = 14)\nsns.kdeplot(train.query('Survived == 0')['Age'], color = '#A01D26', shade = True, label = 'Not survived', alpha = 0.7)\nsns.kdeplot(train.query('Survived == 1')['Age'], color = '#ACBEBE', shade = True, label = 'Survived', alpha = 0.7)\nplt.grid(color = 'gray', linestyle = ':', axis = 'x', zorder = 0,  dashes = (1,7))\nplt.ylabel('')\nplt.xlabel('')\nplt.yticks([])\nplt.legend(loc = 'upper left')\n\nplt.subplot(222)\nplt.title('SibSp', size = 14)\nsns.kdeplot(train.query('Survived == 0')['SibSp'], color = '#A01D26', shade = True, label = 'Not survived', alpha = 0.7)\nsns.kdeplot(train.query('Survived == 1')['SibSp'], color = '#ACBEBE', shade = True, label = 'Survived', alpha = 0.7)\nplt.grid(color = 'gray', linestyle = ':', axis = 'x', zorder = 0,  dashes = (1,7))\nplt.ylabel('')\nplt.xlabel('')\nplt.yticks([])\n\nplt.subplot(223)\nplt.title('Parch', size = 14)\nsns.kdeplot(train.query('Survived == 0')['Parch'], color = '#A01D26', shade = True, label = 'Not survived', alpha = 0.7)\nsns.kdeplot(train.query('Survived == 1')['Parch'], color = '#ACBEBE', shade = True, label = 'Survived', alpha = 0.7)\nplt.grid(color = 'gray', linestyle = ':', axis = 'x', zorder = 0,  dashes = (1,7))\nplt.ylabel('')\nplt.xlabel('')\nplt.yticks([])\n\nplt.subplot(224)\nplt.title('Fare', size = 14)\nsns.kdeplot(train.query('Survived == 0')['Fare'], color = '#A01D26', shade = True, label = 'Not survived', alpha = 0.7)\nsns.kdeplot(train.query('Survived == 1')['Fare'], color = '#ACBEBE', shade = True, label = 'Survived', alpha = 0.7)\nplt.grid(color = 'gray', linestyle = ':', axis = 'x', zorder = 0,  dashes = (1,7))\nplt.ylabel('')\nplt.xlabel('')\nplt.yticks([])\n\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Distribution of male and female survival rates by class**","metadata":{}},{"cell_type":"code","source":"suv = train.groupby(['Survived', 'Sex', 'Pclass']).agg({'Survived': 'count'}).rename(columns = {'Survived': 'count'}).reset_index()\nsuv.iloc[0:6, 0] = 'Not survived'\nsuv.iloc[6:, 0] = 'Survived'\nfor i in range(len(suv.index)):\n    suv.iloc[i,2] = str(suv.iloc[i,2]) + ' class'\n\nfig = px.sunburst(suv, path = ['Survived', 'Sex', 'Pclass'], values = 'count', color = 'Survived',\n                 color_discrete_map = {'Not survived': '#A01D26', 'Survived': '#ACBEBE'},\n                 width = 700, height = 700)\n\nfig.update_layout(annotations = [dict(text = 'Distribution of male and female survival rates by class', \n                                      x = 0.5, y = 1.1, font_size = 24, showarrow = False, \n                                      font_family = 'Calibri Black',\n                                      font_color = 'black')])\n\nfig.update_traces(textinfo = 'label + percent parent')\n                  \nfig.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"matrix = np.triu(train.corr())\npalette = ['#ACBEBE', '#A01D26']\nplt.figure(figsize=(13, 10))\nsns.heatmap(train.corr(), annot = True, cmap = palette, fmt=\".2f\", mask = matrix,\n            vmin = -1, vmax = 1, linewidths = 0.1, linecolor = 'white', cbar = False)\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Conclusions of EDA","metadata":{}},{"cell_type":"markdown","source":"1. As in the original Titanic dataset, the most important factors affecting survival are the class and gender of the passenger. \n2. Also a great importance have a port of embarkation. \n3. Age also affects survival, but not much. \n4. Count of of siblings / spouses / parents / children have almost no effect on survival.","metadata":{}},{"cell_type":"markdown","source":"# Prepare for modeling","metadata":{}},{"cell_type":"code","source":"X = train.drop(['Survived'], axis = 1)\ny = train['Survived']\n\nnum_cols = ['Age', 'SibSp', 'Parch', 'Fare']\ncat_cols = ['Sex', 'Embarked']\n\ndef label_encoder(df):\n    for i in cat_cols:\n        le = LabelEncoder()\n        df[i] = le.fit_transform(df[i])\n    return df\n\nsc = StandardScaler()\nX[num_cols] = sc.fit_transform(X[num_cols])\ntest[num_cols] = sc.fit_transform(test[num_cols])\n\nX = label_encoder(X)\ntest = label_encoder(test)\n\nfor i in ['Pclass', 'Sex', 'Embarked']:\n    X[i] = X[i].astype('category')\n    test[i] = test[i].astype('category')\n\nX.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modeling 1","metadata":{}},{"cell_type":"markdown","source":"For modeling 1 I will use LGBM tuned with Optuna (100 trials)","metadata":{}},{"cell_type":"code","source":"def objective(trial, data = X, target = y):\n    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.2, random_state = 2021)\n\n    params = {\n        'reg_alpha': trial.suggest_float('reg_alpha', 0.001, 10.0),\n        'reg_lambda': trial.suggest_float('reg_lambda', 0.001, 10.0),\n        'num_leaves': trial.suggest_int('num_leaves', 10, 500),\n        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n        'max_depth': trial.suggest_int('max_depth', 3, 10),\n        'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.1),\n        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.1, 0.6),\n        'cat_feature': ['Pclass', 'Sex', 'Embarked'],\n        'cat_smooth' : trial.suggest_int('cat_smooth', 10, 100),\n        'cat_l2': trial.suggest_int('cat_l2', 1, 20),\n        'min_data_per_group': trial.suggest_int('min_data_per_group', 50, 200),\n        'n_estimators': 10000,\n        'random_state': 2021,\n        'metric': 'binary_logloss'\n    }\n    \n    model = LGBMClassifier(**params)  \n    model.fit(X_train, y_train, eval_set = [(X_val,y_val)], early_stopping_rounds = 100, verbose = False)\n    y_pred = model.predict(X_val)\n    \n    accuracy = accuracy_score(y_val, y_pred)\n    \n    return accuracy\n\nstudy = optuna.create_study(direction = 'maximize')\nstudy.optimize(objective, n_trials = 100)\nprint('Number of finished trials:', len(study.trials))\nprint('Best trial:', study.best_trial.params)\nprint('Best value:', study.best_value)\n\nparamsLGBM = study.best_trial.params\nparams = {'n_estimators': 10000, 'random_state': 2021, 'metric': 'binary_logloss', 'cat_feature': ['Pclass', 'Sex', 'Embarked']}\nparamsLGBM.update(params)\n\nfolds = KFold(n_splits = 10, shuffle = True, random_state = 2021)\npredictions = np.zeros(len(test))\nfor fold, (trn_idx, val_idx) in enumerate(folds.split(X, y)):\n    \n    X_train, X_val = X.iloc[trn_idx], X.iloc[val_idx]\n    y_train, y_val = y.iloc[trn_idx], y.iloc[val_idx]\n\n    model = LGBMClassifier(**paramsLGBM)\n   \n    model.fit(X_train, y_train, eval_set = [(X_val, y_val)], verbose = False, early_stopping_rounds = 100)\n    \n    predictions += model.predict_proba(test)[:,1] / folds.n_splits ","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.histplot(predictions)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.DataFrame({'PassengerId': ID, 'Survived': np.where(predictions > 0.5, 1, 0)})\nsubmission.to_csv('submissionLGBM.csv', index = False)\nsubmission.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Result - 0.78945 (from past notebook version)","metadata":{}},{"cell_type":"markdown","source":"Let's try to change threshold","metadata":{}},{"cell_type":"code","source":"submission = pd.DataFrame({'PassengerId': ID, 'Survived': np.where(predictions > 0.45, 1, 0)})\nsubmission.to_csv('submissionLGBM2.csv', index = False)\nsubmission.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Result - 0.79450 (from past notebook version)","metadata":{}},{"cell_type":"markdown","source":"# Modeling 2","metadata":{}},{"cell_type":"markdown","source":"For modeling 2 I try to use tuned with Optuna (30 minutes) LGBM with cross-validation (10 folds)","metadata":{}},{"cell_type":"code","source":"pin_params = {'n_estimators': 10000, 'learning_rate': 0.05, 'metric': 'binary_logloss', 'cat_feature': ['Pclass', 'Sex', 'Embarked']}\n\ndef objective(trial):\n\n    params = {\n        'reg_alpha': trial.suggest_float('reg_alpha', 0.001, 10.0),\n        'reg_lambda': trial.suggest_float('reg_lambda', 0.001, 10.0),\n        'num_leaves': trial.suggest_int('num_leaves', 10, 500),\n        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n        'max_depth': trial.suggest_int('max_depth', 3, 10),\n        'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.1),\n        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.1, 0.6),\n        'cat_smooth' : trial.suggest_int('cat_smooth', 10, 100),\n        'cat_l2': trial.suggest_int('cat_l2', 1, 20),\n        'min_data_per_group': trial.suggest_int('min_data_per_group', 50, 200)\n    }\n\n    params.update(pin_params)\n    \n    model = LGBMClassifier(**params) \n    scores = []\n    k = KFold(n_splits = 10, shuffle = True, random_state = 2021)\n    for i, (trn_idx, val_idx) in enumerate(k.split(X)):\n        print(f\"\\n----- FOLD {i} -----\")\n        \n        X_train, X_val = X.iloc[trn_idx], X.iloc[val_idx]\n        y_train, y_val = y.iloc[trn_idx], y.iloc[val_idx]\n\n        model.fit(X, y, eval_set = [(X_val, y_val)], early_stopping_rounds = 25, verbose = 1000)\n        \n        tr_preds = model.predict(X_train)\n        tr_score = accuracy_score(y_train, tr_preds)\n        \n        val_preds = model.predict(X_val)\n        val_score = accuracy_score(y_val, val_preds)\n\n        scores.append((tr_score, val_score))\n        \n        print(f\"Fold {i} | Accuracy: {val_score}\")\n        \n        \n    scores = pd.DataFrame(scores, columns = ['train score', 'validation score'])\n    \n    return scores['validation score'].mean()\n\nstudy = optuna.create_study(direction = 'maximize')\nstudy.optimize(objective, timeout = 1800)\nprint('Number of finished trials:', len(study.trials))\nprint('Best trial:', study.best_trial.params)\nprint('Best value:', study.best_value)","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pin_params.update(study.best_params)\n\nmodel = LGBMClassifier(**pin_params) \npredictions = np.zeros(len(test))\nk = KFold(n_splits = 10, shuffle = True, random_state = 2021)\nfor i, (trn_idx, val_idx) in enumerate(k.split(X, y)):\n    print(f\"\\n----- FOLD {i} -----\")\n    \n    X_train, X_val = X.iloc[trn_idx], X.iloc[val_idx]\n    y_train, y_val = y.iloc[trn_idx], y.iloc[val_idx]\n\n    model.fit(X, y, eval_set = [(X_val, y_val)], early_stopping_rounds = 25, verbose = 1000)\n    predictions += model.predict_proba(test)[:,1] / k.n_splits","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.DataFrame({'PassengerId': ID, 'Survived': np.where(predictions > 0.5, 1, 0)})\nsubmission.to_csv('submissionLGBM3.csv', index = False)\nsubmission.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"result - 0.77387. It's sad :(\n\n(from past notebook version)","metadata":{}},{"cell_type":"markdown","source":"# Modeling 3","metadata":{}},{"cell_type":"markdown","source":"For modeling 3 I try to use tuned XGB with Optuna (100 trials)","metadata":{}},{"cell_type":"code","source":"for i in ['Pclass', 'Sex', 'Embarked']:\n    X[i] = X[i].astype('int')\n    test[i] = test[i].astype('int')","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def objective(trial, data = X, target = y):\n    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.2, random_state = 2021)\n    \n    params = {\n        'max_depth': trial.suggest_int('max_depth', 3, 10),\n        'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.1),\n        'min_child_weight': trial.suggest_int('min_child_weight', 1, 300),\n        'gamma': trial.suggest_float('gamma', 0.0001, 1.0, log = True),\n        'alpha': trial.suggest_float('alpha', 0.0001, 10.0, log = True),\n        'lambda': trial.suggest_float('lambda', 0.0001, 10.0, log = True),\n        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.1, 0.6),\n        'subsample': trial.suggest_float('subsample', 0.1, 0.6),\n        'max_bin': trial.suggest_int('max_bin', 50, 500),\n        'n_estimators': 10000,\n        'random_state': 2021,\n        'objective': 'binary:logistic',\n        'eval_metric': 'logloss'\n\n    }\n    \n    model = XGBClassifier(**params)  \n    model.fit(X_train, y_train, eval_set = [(X_val,y_val)], early_stopping_rounds = 100, verbose = False)\n    y_pred = model.predict(X_val)\n    \n    accuracy = accuracy_score(y_val, y_pred)\n    \n    return accuracy\n\nstudy = optuna.create_study(direction = 'maximize')\nstudy.optimize(objective, n_trials = 100)\nprint('Number of finished trials:', len(study.trials))\nprint('Best trial:', study.best_trial.params)\nprint('Best value:', study.best_value)","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"paramsXGB = study.best_trial.params\nparams = {'n_estimators': 10000, 'random_state': 2021}\nparamsXGB.update(params)\n\nfolds = KFold(n_splits = 10, shuffle = True, random_state = 2021)\npredictions = np.zeros(len(test))\nfor fold, (trn_idx, val_idx) in enumerate(folds.split(X, y)):\n    \n    X_train, X_val = X.iloc[trn_idx], X.iloc[val_idx]\n    y_train, y_val = y.iloc[trn_idx], y.iloc[val_idx]\n\n    model = XGBClassifier(**paramsXGB)\n   \n    model.fit(X_train, y_train, eval_set = [(X_val, y_val)], eval_metric = 'logloss', verbose = False, early_stopping_rounds = 100)\n    \n    predictions += model.predict_proba(test)[:,1] / folds.n_splits ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.histplot(predictions)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.DataFrame({'PassengerId': ID, 'Survived': np.where(predictions > 0.5, 1, 0)})\nsubmission.to_csv('submissionXGB.csv', index = False)\nsubmission.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Result - 0.79288 (from past notebook version)","metadata":{}},{"cell_type":"markdown","source":"Let's change threshold","metadata":{}},{"cell_type":"code","source":"submission = pd.DataFrame({'PassengerId': ID, 'Survived': np.where(predictions > 0.45, 1, 0)})\nsubmission.to_csv('submissionXGB2.csv', index = False)\nsubmission.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Result - 0.79591 (from past notebook version)","metadata":{}},{"cell_type":"markdown","source":"# Random Forest (why not?)","metadata":{}},{"cell_type":"code","source":"rf = RandomForestClassifier(random_state = 2021)\n\nparams = { \n    'n_estimators': [200, 500, 1000, 2000, 5000],\n    'max_depth' : range(3,8)\n}\n\nCV_rf = GridSearchCV(estimator = rf, param_grid = params, cv = 5, scoring = 'accuracy')\nCV_rf.fit(X, y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_rf = CV_rf.best_estimator_\nbest_rf","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = best_rf.predict_proba(test)[:,1]\nsubmission = pd.DataFrame({'PassengerId': ID, 'Survived': np.where(predictions > 0.5, 1, 0)})\nsubmission.to_csv('submissionRF.csv', index = False)\nsubmission.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Result - 0.78961","metadata":{}},{"cell_type":"markdown","source":"Let's try to change threshold","metadata":{}},{"cell_type":"code","source":"submission = pd.DataFrame({'PassengerId': ID, 'Survived': np.where(predictions > 0.45, 1, 0)})\nsubmission.to_csv('submissionRF2.csv', index = False)\n\nsubmission = pd.DataFrame({'PassengerId': ID, 'Survived': np.where(predictions > 0.55, 1, 0)})\nsubmission.to_csv('submissionRF3.csv', index = False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Results - 0.79401 and 0.77871","metadata":{}},{"cell_type":"markdown","source":"# AutoML","metadata":{}},{"cell_type":"markdown","source":"According to recent observations, I have noticed that many masters successfully use AutoML, so I'll try to keep up.","metadata":{}},{"cell_type":"markdown","source":"First, let's try to create new features. For this I repeat my preproceesing without deleting columns (except PassengerId and Name).","metadata":{}},{"cell_type":"code","source":"train['Cabin'] = train['Cabin'].map(lambda x: str(x)[0].strip())\ntest['Cabin'] = test['Cabin'].map(lambda x: str(x)[0].strip())\n\ntrain['FamilySize'] = train['SibSp'] + train['Parch'] + 1\ntest['FamilySize'] = test['SibSp'] + test['Parch'] + 1\n\ntrain['is_alone'] = train['FamilySize'].apply(lambda x: 1 if x == 1 else 0)\ntest['is_alone'] = test['FamilySize'].apply(lambda x: 1 if x == 1 else 0)\n\ntrain['ticket_type'] = 0\nfor i in range(len(train)):\n    if pd.isna(train.iloc[i,8]) == True:\n        train.iloc[i,14] = 'Na'\n    elif train.iloc[i,8].isdigit() == True:\n        train.iloc[i,14] = 'N'\n    else:\n        train.iloc[i,14] = train.iloc[i,8].split(' ')[0]\n        \ntest['ticket_type'] = 0\nfor i in range(len(test)):\n    if pd.isna(test.iloc[i,7]) == True:\n        test.iloc[i,13] = 'Na'\n    elif test.iloc[i,7].isdigit() == True:\n        test.iloc[i,13] = 'N'\n    else:\n        test.iloc[i,13] = test.iloc[i,7].split(' ')[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = train.drop(['PassengerId', 'Name', 'Ticket'], axis = 1)\ntest = test.drop(['PassengerId', 'Name', 'Ticket'], axis = 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install -U lightautoml","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from lightautoml.automl.presets.tabular_presets import TabularAutoML, TabularUtilizedAutoML\nfrom lightautoml.dataset.roles import DatetimeRole\nfrom lightautoml.tasks import Task\nfrom lightautoml.utils.profiler import Profiler\nimport torch","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"N_THREADS = 4 # threads cnt for lgbm and linear models\nN_FOLDS = 10 # folds cnt for AutoML\nRANDOM_STATE = 2021 # fixed random state for various reasons\nTEST_SIZE = 0.2 # Test size for metric check\nTIMEOUT = 300 # Time in seconds for automl run\n\nnp.random.seed(RANDOM_STATE)\ntorch.set_num_threads(N_THREADS)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Task\ndef acc_score(y_true, y_pred, **kwargs):\n    return accuracy_score(y_true, (y_pred > 0.5).astype(int), **kwargs)\n\ntask = Task('binary', metric = 'logloss')\n\n# Column role\nroles = {'target': 'Survived'}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nautoml = TabularUtilizedAutoML(task = task, \n                       timeout = TIMEOUT,\n                       cpu_limit = N_THREADS,\n                       general_params = {'use_algos': [['linear_l2', 'lgb', 'lgb_tuned']]},\n                       reader_params = {'n_jobs': N_THREADS})\noof_pred = automl.fit_predict(train, roles = roles)\nprint(f'oof_pred:\\n{oof_pred[:10]}\\nShape = {oof_pred.shape}')","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = automl.predict(test)\nprint(f'Prediction for test data:\\n{predictions[:10]}\\nShape = {predictions.shape}')\n\nprint('Check scores...')\nprint('OOF score: {}'.format(acc_score(train['Survived'].values, oof_pred.data[:, 0])))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.DataFrame({'PassengerId': ID, 'Survived': (predictions.data[:, 0] > 0.5).astype(int)})\nsubmission.to_csv('submissionAutoML.csv', index = False)\nsubmission.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Result - 0.79151","metadata":{}}]}