{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Tabular Playground Series - Apr-2021\nhttps://www.kaggle.com/c/tabular-playground-series-apr-2021","metadata":{}},{"cell_type":"markdown","source":"# Libraries","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\npd.set_option('display.max_columns', 500)\npd.set_option('display.max_rows', 500)\n\nimport seaborn as sns\nimport sklearn\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport h2o\n\nprint(\"seaborn\", sns.__version__)\nprint(\"pandas\", pd.__version__)\n\nfrom pathlib import Path\n\nPATH = Path.cwd().parent\nDATA_PATH = \"../input/tabular-playground-series-apr-2021/\"\nSEED = 42","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load the Data","metadata":{}},{"cell_type":"code","source":"h2o.init(min_mem_size=\"15G\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = h2o.import_file(DATA_PATH + \"train.csv\")\nprint(train.shape)\ntrain.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Basic Stats","metadata":{}},{"cell_type":"code","source":"train.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Features","metadata":{}},{"cell_type":"code","source":"target = \"Survived\"\n\nremove = ['PassengerId', 'Ticket', 'Cabin']\nnum_cols = ['Age', 'SibSp', 'Parch', 'Fare']\ntext_cols = ['Name']\ncat_cols = ['Sex', 'Embarked', 'Pclass']\nfeatures = num_cols + cat_cols + text_cols\n\nprint(\"missing\", [f for f in train.columns if f not in features + remove + [target]])\nprint(\"not founded\", [f for f in features + remove + [target] if f not in train.columns])\nprint(\"features\", len(features), features)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Check Nulls","metadata":{}},{"cell_type":"code","source":"dfs = {\"train\":train}\nfor key in dfs:\n    print(\"--------\", key, dfs[key].shape, \"--------\")\n    for col in features:\n        print(col, dfs[key][col].isna().sum())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessing","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import KBinsDiscretizer\n\ndef mk_discrete_values(hdf, col, n_bins=10, sufix=\"_disc\"):\n    discretizer = KBinsDiscretizer(n_bins=n_bins, encode=\"ordinal\")\n    discrete_values = discretizer.fit_transform(hdf[col].as_data_frame().values)\n    hdf[col+sufix] = h2o.H2OFrame(discrete_values.reshape(-1))\n    return hdf\n\ndef mk_imputation(hdf, col, by, method=\"median\"):\n    _ = hdf.impute(col, method=method, by=by)    \n    return hdf\n\n# split names\ndef get_names(hdf):\n    names = hdf[\"Name\"].strsplit(\",\")\n    names.columns = [\"Last_Name\", \"First_Name\"]\n    hdf = hdf.cbind(names)\n    return hdf\n\ndef preprocess(hdf, as_trainning=True):\n    # ensure the categorical variables\n    hdf[cat_cols] = hdf[cat_cols].asfactor()\n    if as_trainning:\n        hdf[target] = hdf[target].asfactor()\n        \n    \n    # some imputation\n    imputation = {\"Age\":[\"Pclass\", \"Sex\"], \"Fare\":[\"Pclass\"]}\n    for keys in imputation:\n        hdf = mk_imputation(hdf, keys, imputation[keys])\n        \n    hdf = mk_discrete_values(hdf, \"Fare\")\n    hdf = mk_imputation(hdf, \"Embarked\", [\"Pclass\", \"Fare_disc\"])\n    \n    hdf = get_names(hdf)\n    \n    return hdf\n\ntrain = preprocess(train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train[features].describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"from h2o.automl import H2OAutoML\n\nfeatures.remove(\"Name\")\nfeatures.append(\"Last_Name\")\n\naml = H2OAutoML(max_runtime_secs = 6 * 3600,\n                seed = SEED, \n                stopping_metric ='logloss', \n                sort_metric ='logloss', \n                max_models=100)\n\naml.train(x=features, y=target, training_frame=train)\n\nlb = aml.leaderboard \nlb.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"test = h2o.import_file(DATA_PATH + \"test.csv\")\nprint(test.shape)\ntest.head()\n\ntest = preprocess(test, False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = aml.leader.predict(test)\npredictions","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.concat([test.as_data_frame()[\"PassengerId\"],\n                        predictions.as_data_frame()[\"predict\"]], axis=1)\nsubmission.columns = [\"PassengerId\", target]\nsubmission.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv(\"submission.csv\", index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}