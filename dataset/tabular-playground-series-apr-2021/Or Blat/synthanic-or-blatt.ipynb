{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport dask.dataframe as dd\npd.options.mode.chained_assignment = None  # default='warn'\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-05-23T18:30:01.112446Z","iopub.execute_input":"2021-05-23T18:30:01.112844Z","iopub.status.idle":"2021-05-23T18:30:01.779535Z","shell.execute_reply.started":"2021-05-23T18:30:01.112765Z","shell.execute_reply":"2021-05-23T18:30:01.778601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Measure memory","metadata":{}},{"cell_type":"code","source":"# https://gdmarmerola.github.io/big-data-ml-training/\n# https://github.com/gdmarmerola/big-data-ml-training/blob/master/track_memory.py\n\n# libs to help us track memory via sampling\nimport numpy as np\nimport tracemalloc\nfrom time import sleep\nimport matplotlib.pyplot as plt\n\n# sampling time in seconds\nSAMPLING_TIME = 0.001\n\nclass MemoryMonitor:\n    def __init__(self, close=True):\n        \n        # start tracemalloc and sets\n        # measurement atribute to True\n        tracemalloc.start()\n        self.keep_measuring = True\n        self.close = close\n        \n    def measure_usage(self):\n        \n        \"\"\"\n        Takes measurements of used memory on\n        regular intevals determined by the \n        global SAMPLING_TIME constant\n        \"\"\"\n        \n        # list to store memory usage samples\n        usage_list = []\n        \n        # keeps going until someone changes this parameter to false\n        while self.keep_measuring:\n            \n            # takes a sample, stores it in the usage_list and sleeps\n            current, peak = tracemalloc.get_traced_memory()\n            usage_list.append(current/1e6)\n            sleep(SAMPLING_TIME)\n            \n        # stop tracemalloc and returns list\n        if self.close:\n            tracemalloc.stop()\n        return usage_list\n\n# imports executor\nfrom concurrent.futures import ThreadPoolExecutor\nfrom functools import wraps\n\ndef plot_memory_use(history, fn_name, open_figure=True, offset=0, **kwargs):\n    \n    \"\"\"Function to plot memory use from a history collected\n        by the MemoryMonitor class\n    \"\"\"\n\n    # getting times from counts and sampling time\n    times = (offset + np.arange(len(history))) * SAMPLING_TIME\n    \n    # opening figure and plotting\n    if open_figure:\n        plt.figure(figsize=(10,3), dpi=120)\n    plt.plot(times, history, 'k--', linewidth=1)\n    plt.fill_between(times, history, alpha=0.5, **kwargs)\n    \n    # axes titles\n    plt.ylabel('Memory usage [MB]')\n    plt.xlabel('Time [seconds]')\n    plt.title(f'{fn_name} memory usage over time')\n    \n    # legend\n    plt.legend();\n\ndef track_memory_use(plot=True, close=True, return_history=False):\n    \n    def meta_wrapper(fn):\n    \n        \"\"\"\n        This function is meant to be used as a decorator\n        that informs wrapped function memory usage\n        \"\"\"\n        \n        # decorator so we can retrieve original fn\n        @wraps(fn)\n        def wrapper(*args, **kwargs):\n\n            \"\"\"\n            Starts wrapped function and holds a process \n            to sample memory usage while executing it\n            \"\"\"\n\n            # context manager for executor\n            with ThreadPoolExecutor() as executor:\n\n                # start memory monitor\n                monitor = MemoryMonitor(close=close)\n                mem_thread = executor.submit(monitor.measure_usage)\n\n                # start wrapped function and get its result\n                try:\n                    fn_thread = executor.submit(fn, *args, **kwargs)\n                    fn_result = fn_thread.result()\n\n                # when wrapped function ends, stop measuring\n                finally:\n                    monitor.keep_measuring = False\n                    history = mem_thread.result()\n\n                # inform results via prints and plot\n                print(f'Current memory usage: {history[-1]:2f}')\n                print(f'Peak memory usage: {max(history):2f}')\n                if plot:\n                    plot_memory_use(history, fn.__name__)\n            if return_history:\n                return fn_result, history\n            else:\n                return fn_result\n\n        return wrapper\n    \n    return meta_wrapper","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-05-23T18:30:01.781845Z","iopub.execute_input":"2021-05-23T18:30:01.78212Z","iopub.status.idle":"2021-05-23T18:30:01.800689Z","shell.execute_reply.started":"2021-05-23T18:30:01.782092Z","shell.execute_reply":"2021-05-23T18:30:01.799676Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import multiprocessing\n# using a function so we can track memory usage\n@track_memory_use(close=False, return_history=False)\ndef pandas_read():\n    \n    # reading train data\n    train_df = pd.read_csv('/kaggle/input/tabular-playground-series-apr-2021/train.csv')\n    test_df = pd.read_csv('/kaggle/input/tabular-playground-series-apr-2021/test.csv')\n    ddX = dd.from_pandas(train_df.drop(['Survived'], axis=1), npartitions=4*multiprocessing.cpu_count())\n    ddy = dd.from_pandas(train_df.Survived, npartitions=4*multiprocessing.cpu_count())\n    test_df = dd.from_pandas(test_df, npartitions=4*multiprocessing.cpu_count())\n    \n#     X = train_df.drop(['Survived'], axis=1)\n#     y = train_df.Survived\n    return train_df, test_df, ddX, ddy\n\n# executing\ntrain_df, test_df, X, y = pandas_read()","metadata":{"execution":{"iopub.status.busy":"2021-05-23T18:30:01.802033Z","iopub.execute_input":"2021-05-23T18:30:01.802331Z","iopub.status.idle":"2021-05-23T18:30:03.234022Z","shell.execute_reply.started":"2021-05-23T18:30:01.802304Z","shell.execute_reply":"2021-05-23T18:30:03.232845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"numerical_features = [cname for cname in X.columns if X[cname].dtype in ['int64', 'float64']]\nnumerical_features.remove('Pclass')\nprint('\\nNumerical columns:', numerical_features)","metadata":{"execution":{"iopub.status.busy":"2021-05-23T18:30:03.235393Z","iopub.execute_input":"2021-05-23T18:30:03.235799Z","iopub.status.idle":"2021-05-23T18:30:03.250848Z","shell.execute_reply.started":"2021-05-23T18:30:03.235763Z","shell.execute_reply":"2021-05-23T18:30:03.24817Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"categorical_features = [col for col in X.columns if X[col].dtype == \"object\"] + ['Pclass']\nprint('\\nCategorical columns:', categorical_features)","metadata":{"execution":{"iopub.status.busy":"2021-05-23T18:30:03.255623Z","iopub.execute_input":"2021-05-23T18:30:03.256155Z","iopub.status.idle":"2021-05-23T18:30:03.267329Z","shell.execute_reply.started":"2021-05-23T18:30:03.256112Z","shell.execute_reply":"2021-05-23T18:30:03.265965Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create Transformers","metadata":{}},{"cell_type":"code","source":"import numpy as np \nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder, MinMaxScaler, StandardScaler, RobustScaler, QuantileTransformer, PowerTransformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import FeatureUnion, Pipeline \nimport string\n\n#Custom Transformer that extracts columns passed as argument to its constructor \nclass FeatureSelector( BaseEstimator, TransformerMixin ):\n    #Class Constructor \n    def __init__( self, feature_names ):\n        self.feature_names = feature_names \n    \n    #Return self nothing else to do here    \n    def fit( self, X, y = None ):\n        return self \n    \n    #Method that describes what we need this transformer to do\n    def transform( self, X, y = None ):\n        return X[ self.feature_names ] ","metadata":{"execution":{"iopub.status.busy":"2021-05-23T18:30:03.27233Z","iopub.execute_input":"2021-05-23T18:30:03.272883Z","iopub.status.idle":"2021-05-23T18:30:04.692332Z","shell.execute_reply.started":"2021-05-23T18:30:03.272844Z","shell.execute_reply":"2021-05-23T18:30:04.69124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Categorical Transformers\nhttps://towardsdatascience.com/custom-transformers-and-ml-data-pipelines-with-python-20ea2a7adb65","metadata":{}},{"cell_type":"code","source":"#Custom transformer \nclass CategoricalTransformer( BaseEstimator, TransformerMixin ):\n    #Class constructor method that takes in a list of values as its argument\n    def __init__(self):\n        pass\n        \n    #Return self nothing else to do here\n    def fit( self, X, y = None  ):\n        return self\n    \n    ### Helper Functions\n    def sex_label_encoder(self, sex):\n        if(sex == 'male'):\n            return 0\n        elif(sex == 'female'):\n            return 1\n        return 2 # fallback - unknown sex\n    \n    #Helper function to extract cabin_prefix from column 'cabin' \n    def get_cabin_prefix(self, obj):\n        return str(obj)[0].upper()\n#         if (isinstance(obj, str)):\n#             return obj[0]\n#         return obj # fallback for np.nan (float type) --> missing value\n        \n#         try:\n# #             return obj[0]\n#             if (obj[0] == 'n'):\n#                 return obj\n#             return str(obj)[0]\n#         except Exception as e: # handle np.nan\n#             return e\n# #             return 'U' # stands for unkown cabin\n    \n    def remove_bracket_from_name(self, name):\n        if('(' in name):\n            name_no_bracket = name.split('(')[0] \n        else:\n            name_no_bracket = name\n        return name_no_bracket\n    \n    def extract_title_from_name(self, name):\n        try:\n            title = name.split(',')[1].strip().split(' ')[0].replace('.','')\n        except:\n            title = \"\"\n        return title\n    \n    def extract_family_from_name(self, name):    \n        family = name.split(',')[0]\n        for c in string.punctuation:\n            family = family.replace(c, '').strip()\n        return family\n    \n    def correct_title_names(self, title):\n        if(title == 'Mlle'):\n            return 'Miss'\n        elif(title == 'Ms'):\n            return 'Miss'\n        elif(title == 'Mme'):\n            return 'Mrs'\n        return title\n    \n    def clean_rare_title_names(self, X): \n#     def clean_rare_title_names(self): \n        stat_min = 10 #common minimum in statistics: http://nicholasjjackson.com/2012/03/08/sample-size-is-10-a-magic-number/\n        title_names = (X['Title'].value_counts() < stat_min)\n        return X['Title'].apply(lambda x: 'Rare' if title_names.loc[x] == True else x)\n    \n    def extract_first_name(self, name: str) -> int:\n        return len(name.split(',')[0])\n    def extract_last_name(self, name:str) -> int:\n        return len(name.split(',')[1][1:])\n        \n    def extract_ticket_alphabetic_code(self, ticket):\n        import re\n        ticket_code = re.sub(r'[^\\w\\s]', '', str(ticket))\n        ticket_code = ticket_code.replace(' ', '')\n        ticket_code = re.sub(r'(\\d)', '', ticket_code)\n        if (ticket_code == np.nan or ticket_code == '' or ticket_code == 'nan'):\n            ticket_code = 'NA'            \n        return ticket_code\n        \n    def extract_ticket_number(self, ticket):\n        import re            \n        ticket = str(ticket)\n        ticket_search = re.search(r'(\\d+)', ticket)\n        if ticket_search:\n            ticket_number = ticket_search.group(1)\n        else:\n            ticket_number = str(0)\n        return ticket_number\n#                 combine['TicketCode'] = combine['Ticket'].str.replace('[^\\w\\s]','')\n# combine['TicketCode'] = combine['TicketCode'].str.replace(' ','')\n# combine['TicketCode'] = combine['TicketCode'].fillna('NA')\n# combine['TicketCode'] = combine['TicketCode'].replace('(\\d)', '', regex=True)\n    \n    #Transformer method we wrote for this transformer \n    @track_memory_use(close=False, return_history=False)\n    def transform(self, X , y = None ):\n#         X.Sex = X.Sex.apply(func = self.sex_label_encoder, meta=('Sex', 'int64'))\n        # Label encode Sex\n        X.Sex = X.Sex.map_partitions(lambda row: row.apply(lambda sex: self.sex_label_encoder(sex)), meta=('Sex', 'int64'))\n        # Create Cabin_Prefix\n        X['Cabin_Prefix'] = X.Cabin.map_partitions(lambda row: row.apply(lambda cabin: self.get_cabin_prefix(cabin)), meta=('Cabin_Prefix', 'O'))\n        \n        # https://www.kaggle.com/dwin183287/tps-april-2021-models-feature-enginering/execution#4.3.6.-First-name-and-last-Name---Continuous\n        # Name\n        X['First_Name'] = X.Name.map_partitions(lambda row: row.apply(lambda name: self.extract_first_name(name)), meta=('First_Name', 'i8'))\n        X['Last_Name'] = X.Name.map_partitions(lambda row: row.apply(lambda name: self.extract_last_name(name)), meta=('Last_Name', 'O'))\n        \n        # https://www.kaggle.com/dwin183287/tps-april-2021-models-feature-enginering/execution#4.4.4.-Ticket-Code---Categorical-&-Ticket-Number---Continuous\n        # Ticket\n        X['Ticket_Alphabetic_Code'] = X.Ticket.map_partitions(lambda row: row.apply(lambda ticket: self.extract_ticket_alphabetic_code(ticket)), meta=('Ticket_Alphabetic_Code', 'O'))\n#         X['TicketNumber'] = X.Ticket.map_partitions(lambda row: row.apply(lambda ticket: self.extract_ticket_number(ticket)), meta=('TicketNumber', 'O'))\n        \n\n# combine['TicketNumber'] = combine['Ticket'].str.extract('(\\d+)')\n# combine['TicketNumber'] = combine['TicketNumber'].astype(float)\n# combine['TicketNumber'] = combine['TicketNumber'].fillna(0)\n# #         # normalize Name\n#         X.Name = X.Name.map_partitions(lambda row: row.apply(lambda name: self.remove_bracket_from_name(name)), meta=('Name', 'O'))\n#         # create Family_Name\n#         X['Family_Name'] = X.Name.map_partitions(lambda row: row.apply(lambda name: self.extract_family_from_name(name)), meta=('Family_Name', 'O'))\n    \n    \n    \n    \n    \n    \n    \n# #         # create Title\n#         X['Title'] = X.Name.map_partitions(lambda row: row.apply(lambda name: self.extract_title_from_name(name)), meta=('Title', 'O'))\n# #         # correct title names\n#         X.Title = X.Title.map_partitions(lambda row: row.apply(lambda title: self.correct_title_names(title)), meta=('Title', 'O'))\n        \n        # clean rare title names\n#         X.Title = X.map_partitions(lambda df: self.clean_rare_title_names(df))\n    \n                           \n\n\n        \n#         X.Sex = X.Sex.map_partitions(lambda row: row.apply(lambda sex: self.sex_label_encoder(sex)), meta=('Sex', 'int64'))\n#         X.Sex = X.Sex.map_partitions(lambda row: row.apply(lambda sex: self.sex_label_encoder(sex)), meta=('Sex', 'int64'))\n#         X.Sex = X.Sex.map_partitions(lambda row: row.apply(lambda sex: self.sex_label_encoder(sex)), meta=('Sex', 'int64'))\n    \n    \n    \n#         X.Sex = X.\\\n#                 map_partitions(\\\n#                                lambda df: df.apply\\\n#                                     ((lambda row: self.sex_label_encoder(row.Sex)), axis = 1)\\\n#                                , meta=X)\n# #                                , meta=pd.Series([], dtype=int, name='Sex'))\n        \n#        #using the helper functions written above \n#         # label encode sex\n#         X.loc[:, 'Sex'] = X['Sex'].apply( self.sex_label_encoder )\n#         # Create Cabin_Prefix\n#         X.loc[:,'Cabin_Prefix'] = X['Cabin'].apply( self.get_cabin_prefix )\n#         # normalize Name\n#         X.loc[:, 'Name'] = X['Name'].apply( self.remove_bracket_from_name )\n#         # create Title\n#         X.loc[:, 'Title'] = X['Name'].apply( self.extract_title_from_name )\n#         # correct title names\n#         X.loc[:,'Title'] = X['Title'].apply( self.correct_title_names )\n#         # clean rare title names\n# #         X.loc[:,'Title'] = X['Title'].apply( self.clean_rare_title_names )\n# #         X.loc[:,'Title'] = self.clean_rare_title_names(X)\n#         # create Family_Name\n#         X.loc[:, 'Family_Name'] = X['Name'].apply( self.extract_family_from_name )\n#         # create Ticket_Frequency\n#         X.loc[:, 'Ticket_Frequency'] = X.groupby('Ticket')['Ticket'].transform('count')\n       \n       # drop columns\n        X = X.drop(['Name', 'Ticket', 'Cabin'], axis = 1)\n       # debugging Dask:\n        X = X.compute(scheduler='processes')\n       #returns numpy array\n        return X.values","metadata":{"execution":{"iopub.status.busy":"2021-05-23T18:30:04.694193Z","iopub.execute_input":"2021-05-23T18:30:04.694891Z","iopub.status.idle":"2021-05-23T18:30:04.745618Z","shell.execute_reply.started":"2021-05-23T18:30:04.694849Z","shell.execute_reply":"2021-05-23T18:30:04.744003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Numerical Transformers","metadata":{}},{"cell_type":"code","source":"#Custom transformer\nclass NumericalTransformer(BaseEstimator, TransformerMixin):\n    #Class Constructor\n    def __init__( self ):\n        pass\n        \n    #Return self, nothing else to do here\n    def fit( self, X, y = None ):\n        return self \n    \n    def create_family_size(self, df):\n        return df.SibSp + df.Parch + 1\n    \n    def create_calculated_fare(self, df):\n        return df.Fare / df.Family_Size\n    \n    #Custom transform method we wrote that creates aformentioned features and drops redundant ones \n    @track_memory_use(close=False, return_history=False)\n    def transform(self, X, y = None):\n        # create Family_Size\n        X['Family_Size'] = X.map_partitions(lambda df: self.create_family_size(df), meta=('Family_Size', 'int64'))\n        # create Calculated_Fare                                 \n#         X['Calculated_Fare'] = X.map_partitions(lambda df: self.create_calculated_fare(df), meta=('Calculated_Fare', 'int64'))\n#         # create Calculated_Fare\n#         X.loc[:, 'Calculated_Fare'] = X['Fare'] / X['Family_Size']\n#         # drop redundant features\n#         X = X.drop(['PassengerId', 'Fare'], axis=1)\n        X = X.drop(['PassengerId'] ,axis = 1) # 'Age', 'SibSp', 'Parch', \n#         X = X.drop(['PassengerId', 'Fare'] ,axis = 1) # 'Age', 'SibSp', 'Parch', \n        # debugging Dask:\n        X = X.compute(scheduler='processes')\n        \n        #returns a numpy array\n        return X.values","metadata":{"execution":{"iopub.status.busy":"2021-05-23T18:30:04.747794Z","iopub.execute_input":"2021-05-23T18:30:04.748287Z","iopub.status.idle":"2021-05-23T18:30:04.761769Z","shell.execute_reply.started":"2021-05-23T18:30:04.748211Z","shell.execute_reply":"2021-05-23T18:30:04.760359Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Pipeline","metadata":{}},{"cell_type":"code","source":"print('\\nCategorical columns:', categorical_features)\nprint('\\nNumerical columns:', numerical_features)","metadata":{"execution":{"iopub.status.busy":"2021-05-23T18:30:04.763481Z","iopub.execute_input":"2021-05-23T18:30:04.764234Z","iopub.status.idle":"2021-05-23T18:30:04.782035Z","shell.execute_reply.started":"2021-05-23T18:30:04.764195Z","shell.execute_reply":"2021-05-23T18:30:04.780678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nfrom sklearn.decomposition import PCA\n\n#Defining the steps in the categorical pipeline \ncategorical_pipeline = Pipeline( steps = \\\n    [ ( 'cat_selector', FeatureSelector(categorical_features) ),\n      ( 'cat_transformer', CategoricalTransformer() ), \n      ( 'imputer', SimpleImputer(strategy = 'most_frequent') ),\n     ( 'one_hot_encoder', OneHotEncoder(handle_unknown='ignore', sparse = False ) ),\n     #PCA Worse score\n# https://www.mikulskibartosz.name/pca-how-to-choose-the-number-of-components/\n#      ( 'min_max_scaler', MinMaxScaler(copy = False)),\n#      ( 'PCA', PCA(n_components = 0.95)) # PCA(n_components = 0.99)\n     ])\n\n\n#Defining the steps in the numerical pipeline     \nnumerical_pipeline = Pipeline( steps = \\\n  [ ( 'num_selector', FeatureSelector(numerical_features) ),\n    ( 'num_transformer', NumericalTransformer() ),                                  \n    ( 'imputer', SimpleImputer(strategy = 'median') ),\n    ( 'std_scaler', MinMaxScaler()), # MinMaxScaler, StandardScaler, RobustScaler # other scalers has no affect\n    ( 'power_transform', PowerTransformer(method = 'yeo-johnson'))\n   ])\n\n#Combining numerical and categorical piepline into one full big pipeline horizontally \n#using FeatureUnion\nfull_pipeline = FeatureUnion( transformer_list = \\\n    [ ( 'categorical_pipeline', categorical_pipeline ),         \n      ( 'numerical_pipeline', numerical_pipeline ) \n    ])","metadata":{"execution":{"iopub.status.busy":"2021-05-23T18:30:04.784001Z","iopub.execute_input":"2021-05-23T18:30:04.784848Z","iopub.status.idle":"2021-05-23T18:30:04.797665Z","shell.execute_reply.started":"2021-05-23T18:30:04.78474Z","shell.execute_reply":"2021-05-23T18:30:04.796177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from sklearn.model_selection import train_test_split\n# X_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2,\n# random_state = 42)","metadata":{"execution":{"iopub.status.busy":"2021-05-23T18:30:04.79957Z","iopub.execute_input":"2021-05-23T18:30:04.800397Z","iopub.status.idle":"2021-05-23T18:30:04.81751Z","shell.execute_reply.started":"2021-05-23T18:30:04.800354Z","shell.execute_reply":"2021-05-23T18:30:04.816034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# https://stackoverflow.com/a/54363480 - DEBUG --> CHANGE Dask loc assignment\n# https://stackoverflow.com/a/38776838 - DEBUG --> Change df.loc[row, column] syntax\n# full_pipeline.fit(X, y)","metadata":{"_kg_hide-output":false,"execution":{"iopub.status.busy":"2021-05-23T18:30:04.819348Z","iopub.execute_input":"2021-05-23T18:30:04.819824Z","iopub.status.idle":"2021-05-23T18:30:04.831321Z","shell.execute_reply.started":"2021-05-23T18:30:04.819781Z","shell.execute_reply":"2021-05-23T18:30:04.829917Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"full_pipeline.fit(X, y)","metadata":{"execution":{"iopub.status.busy":"2021-05-23T18:30:04.833327Z","iopub.execute_input":"2021-05-23T18:30:04.833859Z","iopub.status.idle":"2021-05-23T18:30:19.079272Z","shell.execute_reply.started":"2021-05-23T18:30:04.833818Z","shell.execute_reply":"2021-05-23T18:30:19.077704Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_transformed = pd.DataFrame(full_pipeline.transform(X))\ny_transformed = y.compute()\nX_transformed","metadata":{"execution":{"iopub.status.busy":"2021-05-23T18:30:19.081892Z","iopub.execute_input":"2021-05-23T18:30:19.082594Z","iopub.status.idle":"2021-05-23T18:30:32.53822Z","shell.execute_reply.started":"2021-05-23T18:30:19.08253Z","shell.execute_reply":"2021-05-23T18:30:32.53674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_transformed.head(50)","metadata":{"execution":{"iopub.status.busy":"2021-05-23T18:30:32.540143Z","iopub.execute_input":"2021-05-23T18:30:32.540606Z","iopub.status.idle":"2021-05-23T18:30:32.750923Z","shell.execute_reply.started":"2021-05-23T18:30:32.540566Z","shell.execute_reply":"2021-05-23T18:30:32.749457Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# a.loc[:, a.columns[3]].unique()","metadata":{"execution":{"iopub.status.busy":"2021-05-23T18:30:32.753019Z","iopub.execute_input":"2021-05-23T18:30:32.753848Z","iopub.status.idle":"2021-05-23T18:30:32.75932Z","shell.execute_reply.started":"2021-05-23T18:30:32.753792Z","shell.execute_reply":"2021-05-23T18:30:32.758347Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# XGBoost Modeling","metadata":{}},{"cell_type":"code","source":"# !pip install dask_ml\n# ! python -m pip install --upgrade dask\n# ! python -m pip install fsspec\n# ! python -m pip install --upgrade joblib","metadata":{"execution":{"iopub.status.busy":"2021-05-23T18:30:32.761436Z","iopub.execute_input":"2021-05-23T18:30:32.762506Z","iopub.status.idle":"2021-05-23T18:30:32.772346Z","shell.execute_reply.started":"2021-05-23T18:30:32.762449Z","shell.execute_reply":"2021-05-23T18:30:32.771119Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from dask_ml.model_selection import train_test_split\n# X_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state = 42)\nfrom sklearn.model_selection import train_test_split\nrandom_state = 42\nX_train, X_valid, y_train, y_valid = train_test_split(X_transformed, y_transformed, train_size=0.8, test_size=0.2, random_state = random_state, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2021-05-23T18:30:32.774173Z","iopub.execute_input":"2021-05-23T18:30:32.774596Z","iopub.status.idle":"2021-05-23T18:30:32.83348Z","shell.execute_reply.started":"2021-05-23T18:30:32.774561Z","shell.execute_reply":"2021-05-23T18:30:32.832076Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import xgboost as xgb\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import mean_absolute_error\nfrom hyperopt import STATUS_OK, Trials, fmin, hp, tpe\n\n# # Define the model\n# clf = XGBClassifier(n_estimators = 1000, learning_rate = 0.05, eval_metric = 'error') # Your code here\n\n# params = {'booster': ['gbtree', 'gblinear', 'dart']}\n\n# class XGBTransformer(BaseEstimator, TransformerMixin):\n#     #Class Constructor\n#     def __init__( self ):\n#         pass\n        \n#     #Return self, nothing else to do here\n#     def fit( self, X, y = None):\n#         return self \n    \n#     @track_memory_use(close=False, return_history=False)\n#     def transform(self, X, y = None):\n#         return xgb.DMatrix(X)\n\n# xgb_pipeline = Pipeline(steps=[\\\n#                                 ('data_wrangling', full_pipeline),\n#                                 ('xgb_dmatrix', XGBTransformer())\n#                                ])\n# xgb_pipeline.fit(X, y)\n# X_DMatrix = xgb_pipeline.transform(X)","metadata":{"execution":{"iopub.status.busy":"2021-05-23T18:30:32.835478Z","iopub.execute_input":"2021-05-23T18:30:32.835939Z","iopub.status.idle":"2021-05-23T18:30:33.678443Z","shell.execute_reply.started":"2021-05-23T18:30:32.835884Z","shell.execute_reply":"2021-05-23T18:30:33.676942Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dask HyperbandSearchCV","metadata":{}},{"cell_type":"code","source":"# ###### WORKING\n# # https://www.kaggle.com/prashant111/a-guide-on-xgboost-hyperparameters-tuning\n# from hyperopt import STATUS_OK, Trials, fmin, hp, tpe\n# from sklearn.metrics import accuracy_score\n# from hyperopt.pyll import scope\n# import warnings\n# warnings.filterwarnings(\"ignore\")\n\n# space={'max_depth': hp.quniform(\"max_depth\", 3, 18, 1),\n#         'gamma': hp.uniform ('gamma', 1,9),\n#         'reg_alpha' : hp.quniform('reg_alpha', 40,180,1),\n#         'reg_lambda' : hp.uniform('reg_lambda', 0,1),\n#         'colsample_bytree' : hp.uniform('colsample_bytree', 0.5,1),\n#         'min_child_weight' : hp.quniform('min_child_weight', 0, 10, 1),\n#         'n_estimators': scope.int(hp.quniform('n_estimators', 100, 300, q=1)),\n#         'seed': 0,\n#         'learning_rate' : hp.uniform('learning_rate', 0.01,0.1),\n#         'booster' : [None, 'gbtree', 'gblinear', 'dart']\n#     }\n\n# def objective(space):\n#     clf=xgb.XGBClassifier(\n#                     n_estimators =space['n_estimators'], max_depth = int(space['max_depth']), gamma = space['gamma'],\n#                     reg_alpha = int(space['reg_alpha']),min_child_weight=int(space['min_child_weight']),\n#                     colsample_bytree=int(space['colsample_bytree']), random_state = 42)\n    \n#     evaluation = [( X_train, y_train), ( X_valid, y_valid)]\n    \n#     clf.fit(X_train, y_train,\n#             eval_set=evaluation, eval_metric=\"auc\",\n#             early_stopping_rounds=10,verbose=False)\n    \n\n#     pred = clf.predict(X_valid)\n#     accuracy = accuracy_score(y_valid, pred>0.5)\n#     print (\"SCORE:\", accuracy)\n#     return {'loss': -accuracy, 'status': STATUS_OK }\n\n# trials = Trials()\n\n# best_hyperparams = fmin(fn = objective,\n#                         space = space,\n#                         algo = tpe.suggest,\n#                         max_evals = 1000,\n#                         trials = trials,\n#                         rstate=np.random.RandomState(42))","metadata":{"execution":{"iopub.status.busy":"2021-05-23T18:33:00.371422Z","iopub.execute_input":"2021-05-23T18:33:00.371944Z","iopub.status.idle":"2021-05-23T18:33:07.986672Z","shell.execute_reply.started":"2021-05-23T18:33:00.371901Z","shell.execute_reply":"2021-05-23T18:33:07.98154Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from pprint import pprint\n# print('best loss: -0.77535\\n')\n# print(\"The best hyperparameters are : \")\n# pprint(best_hyperparams)\n\n# # best loss: -0.77535\n\n# # The best hyperparameters are : \n# # {'colsample_bytree': 0.8378618694781489,\n# #  'gamma': 2.8242880151334067,\n# #  'learning_rate': 0.07069892867801585,\n# #  'max_depth': 14.0,\n# #  'min_child_weight': 5.0,\n# #  'n_estimators': 202.0,\n# #  'reg_alpha': 40.0,\n# #  'reg_lambda': 0.9194811165570513}","metadata":{"execution":{"iopub.status.busy":"2021-05-23T18:30:33.996842Z","iopub.status.idle":"2021-05-23T18:30:33.997547Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from pprint import pprint\n# print('best loss: -0.77535\\n')\n# print(\"The best hyperparameters are : \")\n# pprint(best_hyperparams)\n\n# best loss: -0.77535\n\n# The best hyperparameters are : \n# {'colsample_bytree': 0.8585598855009294,\n#  'gamma': 1.3997603558344998,\n#  'learning_rate': 0.03708950942418698,\n#  'max_depth': 10.0,\n#  'min_child_weight': 7.0,\n#  'n_estimators': 231.0,\n#  'reg_alpha': 90.0,\n#  'reg_lambda': 0.44294659050314755}","metadata":{"execution":{"iopub.status.busy":"2021-05-23T18:30:33.998788Z","iopub.status.idle":"2021-05-23T18:30:33.999514Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# clf = xgb.XGBClassifier().set_params(**best_hyperparams)","metadata":{"execution":{"iopub.status.busy":"2021-05-23T18:30:34.000991Z","iopub.status.idle":"2021-05-23T18:30:34.001702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Double Cross-Validation / Nested Cross-Validation","metadata":{}},{"cell_type":"code","source":"######### WORKING HYPER PARAM SELECTION\nfrom sklearn.model_selection import KFold, cross_validate\n\nrandom_state=42\nn_iter = 100# 50\nnum_folds=10\nkf = KFold(n_splits=num_folds, shuffle = True,random_state=random_state)","metadata":{"execution":{"iopub.status.busy":"2021-05-23T18:33:45.832916Z","iopub.execute_input":"2021-05-23T18:33:45.833427Z","iopub.status.idle":"2021-05-23T18:33:45.840585Z","shell.execute_reply.started":"2021-05-23T18:33:45.833394Z","shell.execute_reply":"2021-05-23T18:33:45.839212Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ######### WORKING HYPER PARAM SELECTION\n\n# https://www.kaggle.com/ilialar/hyperparameters-tunning-with-hyperopt#Hyperopt\nfrom sklearn.model_selection import KFold, cross_validate, cross_val_score\n\ndef xgb_clf_acc_f1_cv(params, random_state = random_state, cv=kf, X=X_train, y=y_train):\n    # the function gets a set of variable parameters in \"param\"\n    params = {'max_depth': int(params['max_depth']), \n              'gamma': params['gamma'], \n              'reg_alpha': params['reg_alpha'], \n              'reg_lambda': params['reg_lambda'], \n              'colsample_bytree': params['colsample_bytree'], \n              'min_child_weight': int(params['min_child_weight']), \n              'n_estimators': int(params['n_estimators']), \n              'seed': int(params['seed']), \n              'learning_rate': params['learning_rate'],\n#               'booster': params['booster'],\n             }\n    \n    # we use this params to create a new LGBM Regressor\n    model = xgb.XGBClassifier(random_state = random_state, **params)\n\n    # https://stackoverflow.com/a/35886445/15830024\n#     scoring = {'acc': 'accuracy',\n#                'f1': 'f1'\n#               }\n    scoring = 'accuracy'\n    # and then conduct the cross validation with the same folds as before\n    score = -cross_val_score(model, X, y, cv=cv, scoring=scoring, n_jobs=-1).mean()\n\n    return score","metadata":{"execution":{"iopub.status.busy":"2021-05-23T18:33:47.252262Z","iopub.execute_input":"2021-05-23T18:33:47.252797Z","iopub.status.idle":"2021-05-23T18:33:47.266412Z","shell.execute_reply.started":"2021-05-23T18:33:47.252739Z","shell.execute_reply":"2021-05-23T18:33:47.264944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ######### WORKING HYPER PARAM SELECTION\n\nfrom sklearn.metrics import f1_score, accuracy_score\nfrom hyperopt.pyll import scope\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# possible values of parameters\nspace={'max_depth': hp.quniform(\"max_depth\", 3, 18, 1),\n        'gamma': hp.uniform ('gamma', 1,9),\n        'reg_alpha' : hp.quniform('reg_alpha', 40,180,1),\n        'reg_lambda' : hp.uniform('reg_lambda', 0,1),\n        'colsample_bytree' : hp.uniform('colsample_bytree', 0.5,1),\n        'min_child_weight' : hp.quniform('min_child_weight', 0, 10, 1),\n        'n_estimators': scope.int(hp.quniform('n_estimators', 100, 300, q=1)),\n        'seed': 0,\n        'learning_rate' : hp.uniform('learning_rate', 0.01,0.1),\n#         'booster' : [None, 'gbtree', 'gblinear', 'dart']\n    }\n\n# trials will contain logging information\ntrials = Trials()\n\nbest=fmin(fn=xgb_clf_acc_f1_cv, # function to optimize\n          space=space, \n          algo=tpe.suggest, # optimization algorithm, hyperotp will select its parameters automatically\n          max_evals=n_iter, # maximum number of iterations\n          trials=trials, # logging\n          rstate=np.random.RandomState(random_state) # fixing random state for the reproducibility\n         )\n\n# computing the score on the test set\nmodel = xgb.XGBClassifier(\n                    n_estimators = int(space['n_estimators']), \n                    max_depth = int(space['max_depth']), \n#                     gamma = space['gamma'],\n                    reg_alpha = int(space['reg_alpha']),\n                    reg_lambda = int(space['reg_lambda']),\n                    min_child_weight=int(space['min_child_weight']),\n                    colsample_bytree=int(space['colsample_bytree']),\n                    learning_rate = space['learning_rate'],\n                    seed = int(space['seed']),\n#                     booster = space['booster'],\n                    random_state = int(random_state)\n                    )\nmodel.fit(X_train,y_train)\n# tpe_test_f1_score=f1_score(y_valid, model.predict(X_valid))\ntpe_test_accuracy_score=accuracy_score(y_valid, model.predict(X_valid))\n\n# print(\"Best f1 score {:.3f} params {}\".format( tpe_test_f1_score(best), best))\nprint(\"Best accuracy score {:.3f} params {}\".format( tpe_test_accuracy_score(best), best))","metadata":{"execution":{"iopub.status.busy":"2021-05-23T18:33:49.373166Z","iopub.execute_input":"2021-05-23T18:33:49.373664Z","iopub.status.idle":"2021-05-23T18:34:21.592324Z","shell.execute_reply.started":"2021-05-23T18:33:49.373626Z","shell.execute_reply":"2021-05-23T18:34:21.587336Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# best","metadata":{"execution":{"iopub.status.busy":"2021-05-23T18:30:34.00872Z","iopub.status.idle":"2021-05-23T18:30:34.009419Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best","metadata":{"execution":{"iopub.status.busy":"2021-05-23T18:30:34.010522Z","iopub.status.idle":"2021-05-23T18:30:34.011289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ######### WORKING HYPER PARAM SELECTION\n\ntpe_results=np.array([[x['result']['loss'],\n                      x['misc']['vals']['learning_rate'][0],\n                      x['misc']['vals']['max_depth'][0],\n                      x['misc']['vals']['n_estimators'][0],\n                      x['misc']['vals']['min_child_weight'][0],\n                      x['misc']['vals']['colsample_bytree'][0],\n                      x['misc']['vals']['reg_alpha'][0],\n                      x['misc']['vals']['reg_lambda'][0]]\n                      for x in trials.trials])\n\ntry:\n    tpe_results_df=pd.DataFrame(tpe_results,\n                           columns=['score', 'learning_rate', 'max_depth', 'n_estimators', 'min_child_weight', 'colsample_bytree', 'reg_alpha', 'reg_lambda'])\n    tpe_results_df.plot(subplots=True,figsize=(10, 10))\nexcept:\n    pass","metadata":{"execution":{"iopub.status.busy":"2021-05-23T18:30:34.012516Z","iopub.status.idle":"2021-05-23T18:30:34.013159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # https://machinelearningmastery.com/nested-cross-validation-for-machine-learning-with-python/\n\n# # manual nested cross-validation for random forest on a classification dataset\n# from numpy import mean\n# from numpy import std\n# from sklearn.model_selection import KFold\n# from sklearn.model_selection import GridSearchCV\n# from sklearn.ensemble import RandomForestClassifier\n# from sklearn.metrics import accuracy_score\n\n# # configure the cross-validation procedure\n# cv_outer = KFold(n_splits=10, shuffle=True, random_state=1)\n# # enumerate splits\n# outer_results = list()\n# for train_ix, test_ix in cv_outer.split(X_transformed):\n#     # split data\n#     X_train, X_test = X[train_ix, :], X[test_ix, :]\n#     y_train, y_test = y[train_ix], y[test_ix]\n#     # configure the cross-validation procedure\n#     cv_inner = KFold(n_splits=3, shuffle=True, random_state=1)\n#     # define the model\n#     model = xgb.XGBClassifier(\n#                     n_estimators =space['n_estimators'], \n#                     max_depth = int(space['max_depth']), \n#                     gamma = space['gamma'],\n#                     reg_alpha = int(space['reg_alpha']),\n#                     min_child_weight=int(space['min_child_weight']),\n#                     colsample_bytree=int(space['colsample_bytree']),\n#                     random_state = 42)\n#     # define search space\n#     space={'max_depth': hp.quniform(\"max_depth\", 3, 18, 1),\n#         'gamma': hp.uniform ('gamma', 1,9),\n#         'reg_alpha' : hp.quniform('reg_alpha', 40,180,1),\n#         'reg_lambda' : hp.uniform('reg_lambda', 0,1),\n#         'colsample_bytree' : hp.uniform('colsample_bytree', 0.5,1),\n#         'min_child_weight' : hp.quniform('min_child_weight', 0, 10, 1),\n#         'n_estimators': scope.int(hp.quniform('n_estimators', 100, 300, q=1)),\n#         'seed': 0,\n#         'learning_rate' : hp.uniform('learning_rate', 0.01,0.1),\n#         'booster' : [None, 'gbtree', 'gblinear', 'dart']\n#     }\n#     # define search\n#     search = GridSearchCV(model, space, scoring='accuracy', cv=cv_inner, refit=True)\n#     # execute search\n#     result = search.fit(X_train, y_train)\n#     # get the best performing model fit on the whole training set\n#     best_model = result.best_estimator_\n#     # evaluate model on the hold out dataset\n#     yhat = best_model.predict(X_test)\n#     # evaluate the model\n#     acc = accuracy_score(y_test, yhat)\n#     # store the result\n#     outer_results.append(acc)\n#     # report progress\n#     print('>acc=%.3f, est=%.3f, cfg=%s' % (acc, result.best_score_, result.best_params_))\n# # summarize the estimated performance of the model\n# print('Accuracy: %.3f (%.3f)' % (mean(outer_results), std(outer_results)))","metadata":{"execution":{"iopub.status.busy":"2021-05-23T18:30:34.01446Z","iopub.status.idle":"2021-05-23T18:30:34.01516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from distributed import Client\n# client = Client(processes=False, n_workers=1, memory_limit='16GB')\n# client","metadata":{"execution":{"iopub.status.busy":"2021-05-23T18:30:34.016242Z","iopub.status.idle":"2021-05-23T18:30:34.016911Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# client","metadata":{"execution":{"iopub.status.busy":"2021-05-23T18:30:34.017939Z","iopub.status.idle":"2021-05-23T18:30:34.018619Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from dask_ml.model_selection import HyperbandSearchCV\n\n# clf = XGBClassifier(n_estimators = 1000, learning_rate = 0.05, eval_metric = 'error') # Your code here\n# model = clf\n# params = {}\n# space={'max_depth': range(3, 18),\n#         'gamma': range(1,9),\n#         'reg_alpha' : range('reg_alpha', 40,180),\n#         'reg_lambda' : range(0,1),\n#         'colsample_bytree' : hp.uniform('colsample_bytree', 0.5,1),\n#         'min_child_weight' : hp.quniform('min_child_weight', 0, 10, 1),\n#         'n_estimators': scope.int(hp.quniform('n_estimators', 100, 300, q=1)),\n#         'seed': 0,\n#         'learning_rate' : hp.uniform('learning_rate', 0.01,0.1),\n#         'booster' : [None, 'gbtree', 'gblinear', 'dart']\n#     }\n\n# n_examples = 15 * len(X_train)\n# n_params = 15\n\n# max_iter = n_params  # number of times partial_fit will be called\n# chunks = n_examples // n_params  # number of examples each call sees\n\n# print((max_iter, chunks))\n# search = HyperbandSearchCV(\n#     model,\n#     params,\n#     max_iter=max_iter,\n#     patience=True,\n# )\n# search.metadata[\"partial_fit_calls\"]","metadata":{"execution":{"iopub.status.busy":"2021-05-23T18:30:34.021215Z","iopub.status.idle":"2021-05-23T18:30:34.022294Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# %%time\n# search.fit(X_train_dask, y_train_dask, classes=[0, 1])","metadata":{"execution":{"iopub.status.busy":"2021-05-23T18:30:34.024181Z","iopub.status.idle":"2021-05-23T18:30:34.025266Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# before_mean_train_score, before_mean_test_score, before_mean_test_score_std = [], [], []\n# after_mean_train_score, after_mean_test_score, after_mean_test_score_std = [], [], []\n# best_param = []\n# from tqdm import tqdm\n# for model_name, model in tqdm(MLA_dict.items()):\n#     base_results = model_selection.cross_validate(model, train_df[train_df_x_calc], train_df[Target], cv  = cv_split, return_train_score = True)\n#     model.fit(train_df[train_df_x_calc], train_df[Target])\n#     before_mean_train_score.append(base_results['train_score'].mean()*100)\n#     before_mean_test_score.append(base_results['test_score'].mean()*100)\n#     before_mean_test_score_std.append(base_results['test_score'].std()*100*3)\n    \n#     param_grid = parameters_dict[model_name]\n#     print(\"\")\n#     print(model_name)\n#     print(param_grid)\n#     tune_model = model_selection.GridSearchCV(model, param_grid=param_grid, scoring = 'roc_auc', cv = cv_split, return_train_score = True)\n#     tune_model.fit(train_df[train_df_x_calc], train_df[Target])\n#     try:\n#         print(\"Best parameters for model: \" + model_name)\n#     except Exception as e:\n#         pass\n#     print(tune_model.best_params_)\n    \n#     after_mean_train_score.append(tune_model.cv_results_['mean_train_score'][tune_model.best_index_]*100)\n#     after_mean_test_score.append(tune_model.cv_results_['mean_test_score'][tune_model.best_index_]*100)\n#     after_mean_test_score_std.append(tune_model.cv_results_['std_test_score'][tune_model.best_index_]*100*3)\n#     best_param.append(tune_model.best_params_)\n# print(\"DONE\")\n","metadata":{"execution":{"iopub.status.busy":"2021-05-23T18:30:34.02728Z","iopub.status.idle":"2021-05-23T18:30:34.028363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# param = {}\n# param['booster'] = 'gbtree'\n# param['objective'] = 'binary:logistic'\n# param[\"eval_metric\"] = \"error\"\n# param['eta'] = 0.3\n# param['gamma'] = 0\n# param['max_depth'] = 6\n# param['min_child_weight']=1\n# param['max_delta_step'] = 0\n# param['subsample']= 1\n# param['colsample_bytree']=1\n# param['silent'] = 0\n# param['seed'] = 0\n# param['base_score'] = 0.5\n# param['random_state'] = 42\n# param['learning_rate'] = 0.05\n# param['n_estimators'] = 1000\n\n# clf = XGBClassifier(use_label_encoder=False)\n# clf.set_params(**param)\n# full_pipeline.fit(X, y)\n# # full_pipeline.transform(X_train)\n# clf.fit(pd.DataFrame(full_pipeline.transform(X)), y.compute())\n# # clf.fit(pd.DataFrame(full_pipeline.transform(X_train)), y_train.compute())\n# # clf.score(pd.DataFrame(full_pipeline.transform(X_valid)), y_valid.compute())","metadata":{"execution":{"iopub.status.busy":"2021-05-23T18:30:34.030378Z","iopub.status.idle":"2021-05-23T18:30:34.031451Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# param = {'colsample_bytree': 0.8585598855009294,\n#  'gamma': 1.3997603558344998,\n#  'learning_rate': 0.03708950942418698,\n#  'max_depth': 10,\n#  'min_child_weight': 7,\n#  'n_estimators': 231,\n#  'reg_alpha': 90,\n#  'reg_lambda': 0.44294659050314755}\nclf = XGBClassifier(eval_metric = 'error')\nclf.set_params(**best) # param\nclf.fit(pd.DataFrame(full_pipeline.transform(X)), y.compute())\n\n# make predictions which we will submit. \ntest_preds = clf.predict(pd.DataFrame(full_pipeline.transform(test_df)))\n\n# The lines below shows how to save predictions in format used for competition scoring\n# Just uncomment them.\n\noutput = pd.DataFrame({'PassengerId': test_df.compute().PassengerId,\n                      'Survived': test_preds})\noutput.to_csv('submission_gpu.csv', index=False)\noutput","metadata":{"execution":{"iopub.status.busy":"2021-05-23T18:30:34.033307Z","iopub.status.idle":"2021-05-23T18:30:34.034004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# output.to_csv('submission_6.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-05-23T18:30:34.035131Z","iopub.status.idle":"2021-05-23T18:30:34.035833Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from xgboost import XGBClassifier\n# from sklearn.metrics import mean_absolute_error\n\n# # Define the model\n# clf = XGBClassifier(n_estimators = 1000, learning_rate = 0.05, eval_metric = 'error') # Your code here\n\n# # # Fit the model\n# clf.fit(X_train, y_train, early_stopping_rounds=5, \n#              eval_set=[(X_valid, y_valid)], \n#              verbose=False) \n\n# # # Get predictions\n# predictions = clf.predict(X_valid) \n\n# # # Calculate MAE\n# mae = mean_absolute_error(y_valid, predictions) # Your code here\n\n# # # Uncomment to print MAE\n\n# print(\"Mean Absolute Error:\" , mae)","metadata":{"execution":{"iopub.status.busy":"2021-05-23T18:30:34.03695Z","iopub.status.idle":"2021-05-23T18:30:34.037629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from sklearn.model_selection import cross_val_score\n\n# # import joblib\n\n\n\n# model_pipeline = Pipeline( steps = \\\n# [ ( 'data_processing', X_transformed),\n#     ( 'num_transformer', NumericalTransformer() ),                                  \n#     ( 'imputer', SimpleImputer(strategy = 'median') ),\n#     ( 'std_scaler', StandardScaler()),\n#    # MinMaxScaler, StandardScaler, RobustScaler\n#     ( 'power_transform', PowerTransformer(method = 'yeo-johnson'))\n#    ])\n\n# full_pipeline = FeatureUnion( transformer_list = \\\n#     [ ( 'full_pipeline', full_pipeline ),         \n#       ( 'model_pipeline', model_pipeline ) \n#     ])\n\n# from dask.distributed import Client\n# import joblib\n\n# client = Client(processes=False)             # create local cluster\n# # client = Client(\"scheduler-address:8786\")  # or connect to remote cluster\n\n# with joblib.parallel_backend('dask', scatter = [X, y]):\n#     cross_val_score(full_pipeline, X, y)\n\n# # # Multiply by -1 since sklearn calculates *negative* MAE\n# # scores = -1 * cross_val_score(full_pipeline, X, y_transformed,\n# #                               cv=5,\n# #                               scoring='neg_mean_absolute_error')\n\n# # print(\"Average MAE score:\", scores.mean())","metadata":{"execution":{"iopub.status.busy":"2021-05-23T18:30:34.038835Z","iopub.status.idle":"2021-05-23T18:30:34.039632Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ERROR - pipeline.transform(X)\n## Your notebook tried to allocate more memory than is available. It has restarted.\nWhat's wrong with the transformers' loc[:, column_name]?  \nHow should I avoid:  \nSettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead","metadata":{}},{"cell_type":"code","source":"### ERROR ###\n# full_pipeline.transform(X)\n### ERROR ###","metadata":{"execution":{"iopub.status.busy":"2021-05-23T18:30:34.041217Z","iopub.status.idle":"2021-05-23T18:30:34.041952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# XGBoost Optimization\n# https://gist.github.com/dirusali\n# import xgboost as xgb\n\n# class XGBTransformer(BaseEstimator, TransformerMixin):\n#     #Class Constructor\n#     def __init__( self ):\n#         pass\n        \n#     #Return self, nothing else to do here\n#     def fit( self, X, y = None ):\n#         return self \n    \n#     #Custom transform method we wrote that creates aformentioned features and drops redundant ones \n#     def transform(self, X, y = None):\n#         return xgb.DMatrix(X, label=y)\n\n# cat_num_pipeline = FeatureUnion( transformer_list = [\\\n#                                                   ( 'categorical_pipeline', categorical_pipeline ),         \n#                                                   ( 'numerical_pipeline', numerical_pipeline ) \n#                                                  ])\n# xgb_pipeline = Pipeline(steps=[\\\n#                                 ('data_wrangling', cat_num_pipeline),\n#                                 ('xgb_dmatrix', XGBTransformer())\n#                                ])\n# xgb_pipeline.fit(X, y)\n# # help(xgb_pipeline.transform(X_train))","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-05-23T18:30:34.043524Z","iopub.status.idle":"2021-05-23T18:30:34.044312Z"},"trusted":true},"execution_count":null,"outputs":[]}]}