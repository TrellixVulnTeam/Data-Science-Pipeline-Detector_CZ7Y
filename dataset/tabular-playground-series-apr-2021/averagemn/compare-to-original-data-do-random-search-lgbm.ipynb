{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nfrom tqdm.auto import tqdm\ntqdm.pandas()\n\nimport matplotlib.pyplot as plt\nimport os\n\n#i added the original titanic dataset also here, just to compare a bit\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I added the original dataset, just so I can do a bit of comparison. Because I tried copying features from my earlier Titanic notebooks and some did not work.","metadata":{}},{"cell_type":"code","source":"%%time\ndf_train_orig = pd.read_csv(\"/kaggle/input/titanic/train.csv\")\ndf_train_orig.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = pd.read_csv(\"/kaggle/input/tabular-playground-series-apr-2021/train.csv\")\ndf_train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There seems to be the difference, that the synthetic data in this dataset has no titles for people. Thus the title as a feature is not useful in this case.","metadata":{}},{"cell_type":"code","source":"df_test = pd.read_csv(\"/kaggle/input/tabular-playground-series-apr-2021/test.csv\")\ndf_test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Combine train and test data to build some common features.","metadata":{}},{"cell_type":"code","source":"df_train[\"train\"] = 1\ndf_test[\"train\"] = 0\ndf_all = pd.concat([df_train, df_test], sort=False)\ndf_all.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def parse_cabin_type(x):\n    if pd.isnull(x):\n        return None\n    cab_id = x[0]\n    return cab_id","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def parse_cabin_num(x):\n    if pd.isnull(x):\n        return -1\n    cab_num = x[1:]\n    return cab_num","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cabin_type = df_all[\"Cabin\"].apply(lambda x: parse_cabin_type(x))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cabin_type","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cabin_num = df_all[\"Cabin\"].apply(lambda x: parse_cabin_num(x))\ncabin_num","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cabin_num.unique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Also, the original dataset had people who were marked as having multiple cabins. This synthetic dataset does not have that.","metadata":{}},{"cell_type":"code","source":"def parse_cabin_count(x):\n    if pd.isnull(x):\n        return np.nan\n    #a typical passenger has a single cabin but some had multiple. in that case they are space separated\n    cabs = x.split()\n    return len(cabs)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_all[\"cabin_type\"] = df_all[\"Cabin\"].apply(lambda x: parse_cabin_type(x))\ndf_all[\"cabin_num\"] = df_all[\"Cabin\"].apply(lambda x: parse_cabin_num(x))\n#no multiple cabins in this set\n#df_all[\"cabin_count\"] = df_all[\"Cabin\"].apply(lambda x: parse_cabin_count(x))\ndf_all[\"cabin_num\"] = df_all[\"cabin_num\"].astype(int)\ndf_all.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_all[\"family_size\"] = df_all[\"SibSp\"] + df_all[\"Parch\"] + 1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#there are no titles in this dataset\n\n#df_all['Title'] = df_all['Name'].str.extract('([A-Za-z]+)\\.', expand=True)\n#df_all.head()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train_orig['Title'] = df_train_orig['Name'].str.extract('([A-Za-z]+)\\.', expand=True)\ndf_train_orig.head()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So the above shows the titles in the original dataset, extracted to the \"Title\" column. The new synthetic one gives nothing if you run that on int.","metadata":{}},{"cell_type":"markdown","source":"There are some missing values. Age is one, so need to imputate that. Meaning, fill in the blanks..","metadata":{}},{"cell_type":"code","source":"df_all['Age'].isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_all[\"Age\"].value_counts().count()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are 175 different age values, so the age must be reported in fractions of a year. The following confirms this:","metadata":{}},{"cell_type":"code","source":"df_all[\"Age\"].unique()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_all[df_all[\"Fare\"].isnull()]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The number of missing fares by passenger group (Pclass):","metadata":{}},{"cell_type":"code","source":"df_all.groupby('Pclass').agg({'Fare': lambda x: x.isnull().sum()})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Strangely, the fare seems also to vary quite a lot inside each passenger class as well. The following shows over 20k different values for class 1 alone:","metadata":{}},{"cell_type":"code","source":"df_all.groupby('Pclass')[\"Fare\"].nunique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"p3_median_fare = df_all[df_all[\"Pclass\"] == 2][\"Fare\"].median()\np3_median_fare","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The values are largely collected on the bottom part, with the above median of 21.7:","metadata":{}},{"cell_type":"code","source":"df_all[df_all[\"Pclass\"] == 3].hist(column=\"Fare\", bins=100)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"See how the fares are quite changing within the class, and the diffs are mostly not too big:","metadata":{}},{"cell_type":"code","source":"df_all[df_all[\"Pclass\"] == 3][\"Fare\"].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This fills the missing fares by the passengers class medium. So passenger in class 1 with a missing fare gets a new fare value that is the median of all reported fares in class 1:","metadata":{}},{"cell_type":"code","source":"df_all['Fare'] = df_all['Fare'].fillna(df_all.groupby('Pclass')['Fare'].transform('median'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_all[\"Fare\"].isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_all.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Change all categorical columns to pandas categorical data type to make use of LGBM's built-in categorical data handling. Thus no need for one-hot encoding:","metadata":{}},{"cell_type":"code","source":"#pd.Int64Dtype seems to be some kind of int that takes NaN also. \n#however, using it here causes unknown label type for LGBM, so have stick with float\n#df_all[\"Survived\"] = df_all[\"Survived\"].astype(pd.Int64Dtype())\ndf_all[\"Sex\"] = df_all[\"Sex\"].astype('category')\ndf_all[\"Embarked\"] = df_all[\"Embarked\"].astype('category')\ndf_all[\"cabin_type\"] = df_all[\"cabin_type\"].astype('category')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#passenger id got ranked high at some point in feature importance.. \n#no idea why. better to remove it anyway\n#although i guess it could indicate the order in which people boarded\ndf_all = df_all.drop([\"Cabin\", \"Name\", \"Ticket\", \"PassengerId\"], axis=1)\ndf_all.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_all.dtypes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now that the data is all processed and features added, split it back to the original train/test set:","metadata":{}},{"cell_type":"code","source":"df_train = df_all[df_all[\"train\"] == 1]\ndf_train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test = df_all[df_all[\"train\"] == 0]\ndf_test = df_test.drop([\"Survived\", \"train\"], axis=1)\ndf_test.head()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y = df_train[\"Survived\"]\nX = df_train.drop([\"Survived\", \"train\"], axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=314, stratify=y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nimport lightgbm as lgbm\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.dtypes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Check our categorical columns are still correct:","metadata":{}},{"cell_type":"code","source":"cat_cols = df_train.select_dtypes(include=['category']).columns\ncat_cols","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Set the parameters to use for LGBM fit() function:","metadata":{}},{"cell_type":"code","source":"fit_params = {\"eval_metric\": [\"binary_logloss\", \"auc\"]}\n#fit_params[\"n_estimators\"] = [2000, 5000, 10000, 15000]\nfit_params[\"early_stopping_rounds\"] = 50\nfit_params[\"eval_set\"] = [(X_test,y_test)]\nfit_params['verbose'] = 100 #this results in printing info every 100th round\nfit_params['categorical_feature'] = 'auto'\n#fit_params['categorical_feature'] = cat_cols\n\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This is a method to define a range of values to explore for the Random Search algorithm:","metadata":{}},{"cell_type":"code","source":"from scipy.stats import randint as sp_randint\n\nfrozen = sp_randint(6, 50)\nfrozen_results = frozen.rvs(size=1000)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Just to see what types of data the above generated:","metadata":{}},{"cell_type":"code","source":"plt.hist(frozen_results)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Do a randomized search over the search space:","metadata":{}},{"cell_type":"code","source":"%%time\nfrom scipy.stats import randint as sp_randint\nfrom scipy.stats import uniform as sp_uniform\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n\nparam_space={'num_leaves': sp_randint(6, 100), \n             'min_child_samples': sp_randint(100, 500), \n             'min_child_weight': [1e-5, 1e-3, 1e-2, 1e-1, 1, 1e1, 1e2, 1e3, 1e4],\n             'subsample': sp_uniform(loc=0.2, scale=0.8), \n             'colsample_bytree': sp_uniform(loc=0.4, scale=0.6),\n             'reg_alpha': [0, 1e-1, 1, 2, 5, 7, 10, 50, 100],\n             'reg_lambda': [0, 1e-1, 1, 5, 10, 20, 50, 100]}\n\nclf = lgbm.LGBMClassifier(max_depth=-1, random_state=314, \n                         silent=True, metric='None', \n                         n_jobs=4, n_estimators=5000)\n\ngs = RandomizedSearchCV(\n    estimator=clf, param_distributions=param_space, \n    n_iter=100,\n    scoring='roc_auc',\n    cv=3,\n    refit=True,\n    random_state=314,\n    verbose=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now run the search that was just configured above.\n\nUnfortunately the following will also print excessive error messages about overriding some categorical value. Quick search turned no solution, so just leaving it here.","metadata":{}},{"cell_type":"code","source":"%%time\ngs.fit(X_train, y_train, **fit_params)\n","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'Best score reached: {gs.best_score_} with params: {gs.best_params_} ')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Take the best parameters that the above search found, and re-train the LGBM with those.","metadata":{}},{"cell_type":"code","source":"clf = lgbm.LGBMClassifier(max_depth=-1, random_state=314, \n                         silent=True, metric='None', \n                         n_jobs=4, n_estimators=5000, **gs.best_params_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clf.fit(X_train, y_train, **fit_params)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Just for interest, plot the highest ranked features:","metadata":{}},{"cell_type":"code","source":"importances = clf.feature_importances_\nfeatures = X.columns\nfeat_importances = pd.Series(importances, index=features)\nfeat_importances.nlargest(30).sort_values().plot(kind='barh', color='#86bf91', figsize=(10, 8))\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Make the predictions for submission:","metadata":{}},{"cell_type":"code","source":"predictions = clf.predict(df_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"And save them. The Kaggle system does not seem to like floats for 1/0 survived here, so have to convert them to ints.","metadata":{}},{"cell_type":"code","source":"sub_df = pd.read_csv(\"/kaggle/input/tabular-playground-series-apr-2021/test.csv\")\nsub_df = sub_df[[\"PassengerId\"]]\nsub_df[\"Survived\"] = predictions\nsub_df[\"Survived\"] = sub_df[\"Survived\"].astype(int)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub_df.to_csv(\"sub.csv\", index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!head sub.csv","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!tail sub.csv","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}