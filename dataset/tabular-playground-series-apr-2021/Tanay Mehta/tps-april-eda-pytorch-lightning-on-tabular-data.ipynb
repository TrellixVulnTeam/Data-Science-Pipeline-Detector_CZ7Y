{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Data and Library Imports ðŸ“š","metadata":{}},{"cell_type":"code","source":"! pip install -q rich dabl","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nfrom scipy import stats\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom rich import print as _pprint\nfrom rich.progress import track\nfrom tqdm import tqdm\nfrom colorama import Fore, Style\nimport random\nimport dabl\nfrom wordcloud import WordCloud\n\nimport plotly.express as px\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import KFold\nfrom IPython.display import HTML\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport pytorch_lightning as pl\nfrom torch.utils.data import DataLoader, Dataset\n\npd.set_option('display.max_columns', 500)\npd.set_option('display.max_rows', 500)\n\nf = open(\"../input/notebookassets/orange.css\").read()\nHTML(f\"<style>{f}</style>\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def cprint(string):\n    _pprint(f\"[black]{string}[/black]\")\n    \ndef cout(string: str, color=Fore.RED, end='\\n'):\n    \"\"\"\n    Saves some work\n    \"\"\"\n    print(color+string+Style.RESET_ALL, end=end)\n\ndef stats(scol, col):\n    cout(f\"Average Value in the Column: {scol} is: {np.mean(col):.4f}\", Fore.RED)\n    cout(f\"Median Value in the Column: {scol} is: {np.median(col):.4f}\", Fore.BLUE)\n    cout(f\"Maxmimum Value in the Column: {scol} is: {np.max(col):.4f}\", Fore.GREEN)\n    cout(f\"Minimum Value in the Column: {scol} is: {np.min(col):.4f}\", Fore.YELLOW)\n    cout(f\"50th Quantile of the Column: {scol} is: {np.quantile(col, 0.5):.4f}\", Fore.CYAN)\n    cout(f\"75th Quantile of the Column: {scol} is: {np.quantile(col, 0.75):.4f}\", Fore.MAGENTA)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cprint('[red bold]If you like my notebook, please leave an Upvote![/red bold]')","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_file = pd.read_csv(\"../input/tabular-playground-series-apr-2021/train.csv\")\ntest_file = pd.read_csv(\"../input/tabular-playground-series-apr-2021/test.csv\")\n\ntrain_file.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Exploratory Data Analysis ðŸ“Š\n\nLet's now start with exploratory data analysis!","metadata":{}},{"cell_type":"code","source":"train_file.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\">\n    This month's competition's data can be thought of as an extension to the Titanic competition data.\n    The points in this dataset is generated by a CTGAN.\n</div>","metadata":{}},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-danger\">\n    The statistical properties of this dataset are very similar to the original Titanic dataset, but there's no way to \"cheat\" by using public labels for predictions.\n</div>","metadata":{}},{"cell_type":"markdown","source":"### Null Value Percentage in different columns","metadata":{}},{"cell_type":"code","source":"cprint(\"[cyan]Percentage of Null values in every column:[/cyan]\")\nfor col in train_file.columns:\n    percent_null = (train_file[col].isna().sum() / train_file.shape[0]) * 100\n    print(\"Percentage of Null values in\", end=' ')\n    cout(f\"{col}\", Fore.GREEN, end=' ')\n    print(\"column:\", end=' ')\n    cout(f\"{percent_null} %\", Fore.RED)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see from above, <strong style=\"color:green;\">Cabin</strong> has more than **67%** of it's values Null.\n\nI'm thus dropping the column for now (may include later, idk)","metadata":{}},{"cell_type":"markdown","source":"### Target Class - `Survived` Distribution","metadata":{}},{"cell_type":"code","source":"sns.set_style('whitegrid')\nnames = [\"Didn't Survive\", \"Survived\"]\nvalues = train_file['Survived'].value_counts().tolist()\n\nplt.figure(figsize=(9, 9))\nplt.pie(x=values, labels=names, autopct=\"%1.2f%%\", colors=[\"blue\", \"green\"], shadow=True)\nplt.title(\"Survived Passengers Pie-Chart\", fontdict={'fontsize': 14})\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Passenger Class (`Pclass`) column\n\nThis column represents the Ticket class of a passenger.","metadata":{}},{"cell_type":"code","source":"sns.set(style=\"whitegrid\")\nnames = [\"3rd Class\", \"1st Class\", \"2nd Class\"]\nvalues = train_file['Pclass'].value_counts().tolist()\n\nplt.figure(figsize=(9, 9))\nplt.pie(x=values, labels=names, autopct=\"%1.2f%%\", colors=[\"#1f6193\", \"#0b8bed\", \"#307bb5\"], shadow=True)\nplt.title(\"Passenger Classes Pie-Chart\", fontdict={'fontsize': 14})\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Age (`Age`) Column\n\nBefore we plot the age column, we must remove the null values by either dropping them or by imputing them.\nI choose the latter option.","metadata":{}},{"cell_type":"code","source":"train_file['Age'] = train_file['Age'].fillna(train_file['Age'].mean())\n\nplt.style.use(\"classic\")\nplt.figure(figsize=(9, 8))\nsns.histplot(train_file['Age'], color='blue', kde=True, bins=35)\nplt.axvline(train_file['Age'].mean(), color='pink', linestyle='-', linewidth=0.9)\nmin_ylim, max_ylim = plt.ylim()\nplt.text(train_file['Age'].mean()*1.05, max_ylim*0.95, 'Mean (Î¼): {:.2f} years'.format(train_file['Age'].mean()))\nplt.xlabel(\"Age (in years)\")\nplt.title(f\"Distribution of Ages\")\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Gender (`Sex`) Column ","metadata":{}},{"cell_type":"code","source":"sns.set(style=\"whitegrid\")\nlabels = ['Male', 'Female']\nvalues = train_file['Sex'].value_counts().tolist()\n\nplt.figure(figsize=(9, 9))\nplt.pie(x=values, labels=labels, autopct=\"%1.2f%%\", colors=['blue', 'magenta'], explode=[0, 0.005], shadow=True)\nplt.title(\"Gender Distribution Pie Chart\", fontdict={'fontsize': 14})\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Siblings / Spouses Aboard the Titanic (`SibSp`)","metadata":{}},{"cell_type":"code","source":"sns.set(style=\"whitegrid\")\nlabels = train_file['SibSp'].value_counts().index.tolist()\nvalues = train_file['SibSp'].value_counts().tolist()\n\ncolors=['#03045E', '#023E8A', '#0077B6', '#0096C7', '#00B4D8', '#48CAE4', '#90E0EF']\n\nplt.figure(figsize=(9, 9))\ntext, patches = plt.pie(x=values, labels=None, colors=colors, explode=[0, 0, 0, 0, 0.1, 0.2, 0.3])\nplt.legend(labels=labels)\n\nplt.title(\"Number of Siblings/Spouses Aboard\", fontdict={'fontsize': 14})\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Parents / Childrens aboard the Titanic (`Parch`)","metadata":{}},{"cell_type":"code","source":"sns.set(style=\"whitegrid\")\nlabels = train_file['Parch'].value_counts().index.tolist()\nvalues = train_file['Parch'].value_counts().tolist()\n\ncolors = ['#b5179e', '#7209b7', '#560bad', '#480ca8', '#3a0ca3', '#3f37c9', '#4361ee', '#4895ef'][::-1]\n\nplt.figure(figsize=(9, 9))\n\ntext, patches = plt.pie(x=values, labels=None, colors=colors)\nplt.legend(labels=labels)\n\nplt.title(\"Number of Parents/Children Aboard\", fontdict={'fontsize': 14})\nplt.show()","metadata":{"_kg_hide-output":false,"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Top-10 Most Popular Tickets (`Ticket`)","metadata":{}},{"cell_type":"code","source":"ixs = train_file['Ticket'].value_counts().index.tolist()[:10]\ncat5_fl = train_file[train_file['Ticket'].isin(ixs)]['Ticket']\nsns.set_style('whitegrid')\nplt.figure(figsize=(8, 7))\nax = sns.histplot(cat5_fl, color='orange')\nplt.xlabel(\"Category\")\nplt.ylabel(\"Count\")\nplt.title(\"Top-10 most popular Tickets\")\n\ntotal = len(train_file['Ticket'])\nfor p in ax.patches:\n        percentage = '{:.2f}%'.format(100 * p.get_height()/total)\n        x = p.get_x() + p.get_width() / 5.2\n        y = p.get_y() + p.get_height() + 0.05\n        ax.annotate(percentage, (x, y))\n\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Embarked at (`Embarked`)","metadata":{}},{"cell_type":"code","source":"ixs = train_file['Embarked'].value_counts().index.tolist()\ncat5_fl = train_file[train_file['Embarked'].isin(ixs)]['Embarked']\n\nfig, ax = plt.subplots(nrows=1, ncols=2, figsize=(12, 7))\nfig.suptitle(\"Embarked Station\", fontdict={'fontsize': 20})\n\nsns.histplot(cat5_fl, color='magenta', ax=ax[0])\nplt.xlabel(\"Station\")\nplt.ylabel(\"Count\")\n\nlabels = train_file['Embarked'].value_counts().index.tolist()\nvalues = train_file['Embarked'].value_counts().tolist()\n\nax[1].pie(x=values, labels=labels, autopct=\"%1.1f%%\", colors=['#0742f2', '#093ac1', '#072c91'])\n\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### First and Last Names\n\nLet's take a look at what first and last names are popular.\n\n<div class=\"alert alert-block alert-info\">\n    Keep in mind that in the <code>Name</code> column, the first string is actually the Last Name (or surname)\n    and the second string is the First name.\n</div>","metadata":{}},{"cell_type":"code","source":"# Get a list of first and last names from the dataframe\nfirst_names = []\nlast_names = []\nfor name in train_file['Name']:\n    first_names.append(name.split(',')[1][1:])\n    last_names.append(name.split(',')[0])\n\nidx = random.randint(a=0, b=len(first_names)-1)\nprint(f\"Example name: {first_names[idx]} {last_names[idx]}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Wordcloud\nfnames = \" \".join(first_names)\nlnames = \" \".join(last_names)\nfwc = WordCloud(width=1024, height=1024, collocations=False).generate(fnames)\nlwc = WordCloud(width=1024, height=1024, collocations=False).generate(lnames)\n\nfig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10, 10))\n# fig.suptitle(\"First and Last Names WordClouds\")\n\nax[0].imshow(fwc)\nax[0].axis('off')\nax[0].set_title('First Names')\n\nax[1].imshow(lwc)\nax[1].axis('off')\nax[1].set_title('Last Names')\n\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Preprocessing\n\nLet's do some really quick data processing so we can focus more on modelling part!\n\nFor this stage, I will be doing the following:\n1. Dropping features that aren't much use or have a lot of missing values\n2. Dropping any remaining NaN values\n3. Encoding any Categorical Features","metadata":{}},{"cell_type":"code","source":"def process_data(data: pd.DataFrame, is_test=False):\n    if is_test:\n        ids = data['PassengerId'].values\n    data = data.drop(['PassengerId', 'Name', 'Ticket', 'Cabin'], axis=1)\n    data = data.dropna()\n    data['Fare'] = data['Fare'].fillna(data['Fare'].mean())\n    data['Sex'] = data['Sex'].map({'male': 0, 'female': 1})\n    data['Embarked'] = data['Embarked'].map({'Q': 0, 'C': 1, 'S': 2})\n    \n    if is_test:\n        return (data, ids)\n    else:\n        return (data, None)\n\ntrain_data, _ = process_data(train_file)\ntest_data, test_ids = process_data(test_file, is_test=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Modelling using PyTorch Lightning\n\nLet's make a Simple Deep Feed forward Neural Network using PyTorch Lightning and train it on GPUs or TPUs!","metadata":{}},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\">\n    Keep in mind, the model in this notebook isn't very great but it's here to show you guys how you can build a PyTorch lightning model for this comeptition!\n</div>","metadata":{}},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-danger\">\n    I highly encourage you all to fork this notebook (if you do, please leave an upvote!) and change the model architecture and maybe even fine-tune them to get better results.\n</div>","metadata":{}},{"cell_type":"code","source":"# Custom dataset\nclass TPSData(Dataset):\n    def __init__(self, data: pd.core.frame.DataFrame, is_test: bool=False):\n        self.is_test = is_test\n        self.target = data['Survived'].values\n        self.features = data.drop(['Survived'], axis=1).values\n    \n    def __getitem__(self, idx):\n        data = self.features[idx]\n        if self.is_test:\n            return torch.tensor(data, dtype=torch.float32)\n        else:\n            target = self.target[idx]\n            return torch.tensor(data, dtype=torch.float32), torch.tensor(target, dtype=torch.long)\n    \n    def __len__(self):\n        return len(self.features)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_datasets(data: pd.core.frame.DataFrame, split: int=0.2):\n    \"\"\"\n    Split the data into training and validation splits\n    Make them into Torch Dataset format\n    \"\"\"\n    # Shuffle the data\n    data = data.sample(frac=1).reset_index(drop=True)\n    \n    # Split the data\n    split_nb = int(split * len(data))\n    train_split = data[split_nb:]\n    val_split = data[:split_nb]\n    \n    # Make them Torch Datasets\n    training_set = TPSData(\n        train_split,\n        is_test=False\n    )\n    validation_set = TPSData(\n        val_split,\n        is_test=False\n    )\n    \n    return {'train': training_set, 'val' : validation_set}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_config = {\n    'data': train_data,\n    'split_pcent': 0.2,\n    'data_ret': get_datasets,\n    'num_workers': 4,\n    'train_bs': 64,\n    'val_bs': 128\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Model\nclass TPSModel(pl.LightningModule):\n    def __init__(self,\n                 input_size: int = 7, \n                 classes: int = 2,\n                 learning_rate: float = 1e-3,\n                 data_config: dict = data_config\n        ):\n        super(TPSModel, self).__init__()\n        \n        if not data_config:\n            raise ValueError(\"Data Config Cannot be empty\")\n        \n        self.data_config = data_config\n        self.input_size = input_size\n        self.learning_rate = learning_rate\n        \n        # Mode Architecture\n        self.fc1 = nn.Linear(self.input_size, 1024)\n        self.fc2 = nn.Linear(1024, 768)\n        self.fc3 = nn.Linear(768, 128)\n        self.fc4 = nn.Linear(128, classes)\n        self.relu = nn.ReLU()\n        \n    def forward(self, x):\n        # Model Compuatation Code\n        out = self.fc1(x)\n        out = self.relu(out)\n        out = self.fc2(out)\n        out = self.relu(out)\n        out = self.fc3(out)\n        out = self.relu(out)\n        out = self.fc4(out)\n        \n        return out\n\n    def prepare_data(self):\n        \"\"\"\n        Get the datasets related variables and functions from the data config dictionary\n        Then use it to split data.\n        \"\"\"\n        # Get stuff from dict\n        data = self.data_config['data']\n        split_pcent = self.data_config['split_pcent']\n        data_ret_fn = self.data_config['data_ret']\n        \n        # Call the retriever function to split data and make datasets\n        # Also extract the datasets from the returned dictionary\n        dataset_cache = data_ret_fn(data, split_pcent)\n        self.train_set = dataset_cache['train']\n        self.val_set = dataset_cache['val']\n        \n    def train_dataloader(self):\n        \"\"\"\n        Initializes and returns the training dataloader\n        \"\"\"\n        num_workers = self.data_config['num_workers']\n        train_bs = self.data_config['train_bs']\n        \n        train_loader = DataLoader(\n            dataset = self.train_set,\n            shuffle = True,\n            batch_size = train_bs,\n            num_workers = num_workers\n        )\n        \n        return train_loader\n        \n    def val_dataloader(self):\n        \"\"\"\n        Initializes and returns the validation dataloader\n        \"\"\"\n        num_workers = self.data_config['num_workers']\n        val_bs = self.data_config['val_bs']\n        \n        val_loader = DataLoader(\n            dataset = self.val_set,\n            shuffle = False,\n            batch_size = val_bs,\n            num_workers = num_workers,\n        )\n        \n        return val_loader\n    \n    def training_step(self, batch, batch_idx):\n        data, targets = batch\n        outputs = self(data)\n        loss = F.cross_entropy(outputs, targets)\n        return {'loss': loss}\n    \n    def validation_step(self, batch, batch_idx):\n        data, targets = batch\n        outputs = self(data)\n        val_loss = F.cross_entropy(outputs, targets)\n        return {'val_loss': val_loss}\n        \n    def validation_epoch_end(self, outputs):\n        # 'outputs' is a list of dictionaries containing validation loss of each batch\n        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n        # return {'val_loss': avg_loss}\n        # self.log(f\"Average loss: {avg_loss}\")\n    \n    def configure_optimizers(self):\n        return torch.optim.Adam(self.parameters(), lr=self.learning_rate)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Run the training loop\nmodel = TPSModel()\ntrainer = pl.Trainer(max_epochs=10, gpus=1)\ntrainer.fit(model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cprint('[red bold]If you like my notebook, please leave an Upvote![/red bold]')","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]}]}