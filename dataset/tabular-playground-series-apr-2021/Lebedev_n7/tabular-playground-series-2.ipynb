{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pickle\nimport scipy.io\nimport tarfile\nimport csv\nimport sys\nimport os,shutil\nimport PIL\nimport re\nimport random \nfrom PIL import ImageOps, ImageFilter\nfrom sklearn.preprocessing import LabelEncoder\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler,StandardScaler\n\nimport lightgbm as lgb\nfrom catboost import CatBoostClassifier\nimport tensorflow as tf\nimport tensorflow.keras as keras\nimport tensorflow.keras.models as M\nimport tensorflow.keras.layers as L\nimport tensorflow.keras.backend as K\nimport tensorflow.keras.callbacks as C\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\nfrom tensorflow.keras.callbacks import LearningRateScheduler, ModelCheckpoint, EarlyStopping\nfrom tensorflow.keras.callbacks import Callback\nfrom tensorflow.keras.applications.xception import Xception\nfrom tensorflow.keras import optimizers\nfrom tensorflow.keras import regularizers\nimport zipfile","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.python.client import device_lib\nprint(device_lib.list_local_devices())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DATA_DIR = '/kaggle/input/tabular-playground-series-apr-2021/'\ndf_train = pd.read_csv(DATA_DIR + 'train.csv')\ndf_test = pd.read_csv(DATA_DIR + 'test.csv')\nsample_submission = pd.read_csv(DATA_DIR + 'sample_submission.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EDA\n\nКолонки данных: \n- PassengerId: Id пассажира \n- Survived: Жив\\Мертв\n- Pclass: Класс пасажира\n    * 1st = Высший \n    + 2nd = Средний\n    - 3rd = Низкий\n- Name: Имя пассажира \n- Sex: Пол\n- Age: Возраст\n- SibSp: братьях и сестрах / супругах на борту\n    * Sibling = brother, sister, stepbrother, stepsister\n    + Spouse = husband, wife (mistresses and fiancés were ignored)\n- Parch: родители\\ дети на борту \n    * Parent = mother, father\n    + Child = daughter, son, stepdaughter, stepson\n    - Некоторые дети путешествуют с нянями, поэтому parch=0 \n- Ticket: Билет\n- Fare:\n- Cabin: Номер\\Кабина\n- Embarked: Порт посадки ","metadata":{}},{"cell_type":"markdown","source":"### ВАЖНО! для корректной обработки признаков объединяем трейн и тест в один датасет","metadata":{}},{"cell_type":"code","source":"df_train['sample'] = 1  # помечаем где у нас трейн\ndf_test['sample'] = 0  # помечаем где у нас тест\n# в тесте у нас нет значения price, мы его должны предсказать, поэтому пока просто заполняем нулями\ndf_test['Survived'] = 0\n\ndata = df_test.append(df_train, sort=False).reset_index(drop=True)  # объединяем","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Посчитаем количество пустых строк для каждого столбца в дасасете","metadata":{}},{"cell_type":"code","source":"NaN_Sum = lambda col: col.isnull().sum()  # Функция для определения количества нулевых знчений  \ncolumns = list(data.columns)\nfor col in columns:\n    print(\"Количество пустых значений\", col, NaN_Sum(data[col]), sep=' ')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Скопируем исхоный датасет и будем работаь уже с ним, попробуем исследовать каждый столбец, найдем зависимости между стобцами и решим, что делать с каждым из них ","metadata":{}},{"cell_type":"code","source":"df=data.copy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Ticket\n- Вытащим из билетов буквенные обазначения \n- Билеты которые имеют только момера обозначим как r\n- Пустые билеты заменим на 0 (пока что )","metadata":{}},{"cell_type":"code","source":"def ticket_label(tickets):\n    try:    \n        patern = re.compile('\\D+')\n        tic = (patern.findall(tickets.Ticket)[0])\n        return tic.rstrip()\n    except IndexError:\n        return 'r'\n    except TypeError:\n        return 'X'\n\ndf['Ticket']=df.apply(lambda df: ticket_label(df),axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.Ticket.unique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(29,8))\nplt.title('Распределение пассажиров по билетам в зависимости от класса')\nsns.countplot(x=\"Ticket\", hue=\"Pclass\", data=df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Parch & SibSp","metadata":{}},{"cell_type":"code","source":"# Суммируем количества членов семьи на борту \ndf['FamOnBoard'] = df['Parch']+df['SibSp']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Создадим столбец для тех кто плывет один \ndf['Alone'] = 1\n\ndef alone_on_board(dat):\n    if dat.SibSp+dat.Parch==0:\n        return 1\n    else:\n        return 0 \n    \ndf['Alone'] = df.apply(lambda df: alone_on_board(df),axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Pclass & Cabin\n- Количество пустых строк в Cabin: 138697, Pclass: 0 ","metadata":{}},{"cell_type":"code","source":"print('Общее количество пассажиров первого класса: %s'%len(df[df.Pclass==1].Cabin))\nprint('Количество пассажиров первого класса не имеющих данных о номере кабины: %s'%df[df.Pclass==1].Cabin.isnull().sum())\nprint('------------------------------------------------------------')\nprint('Общее количество пассажиров второго класса: %s'%len(df[df.Pclass==2].Cabin))\nprint('Количество пассажиров второго класса не имеющих данных о номере кабины: %s'%df[df.Pclass==2].Cabin.isnull().sum())\nprint('------------------------------------------------------------')\nprint('Общее количество пассажиров третьего класса: %s'%len(df[df.Pclass==3].Cabin))\nprint('Количество пассажиров третьего класса не имеющих данных о номере кабины: %s'%df[df.Pclass==3].Cabin.isnull().sum())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Напишем функцию которая вытащит из номера кабины её тип (ряд)","metadata":{}},{"cell_type":"code","source":"def cabin_type(cabin):\n    try:\n        pattern_cabin=re.compile('\\w')\n        name=str((pattern_cabin.findall(cabin.Cabin))[0])\n        return name\n    except:\n        np.nan\n\ndf['Cabin']=df.apply(lambda df: cabin_type(df),axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Оценим уровень выживаемости в зависимости от каюты, а также класс пассажира и закрепленная за ним тип каюты ","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(29,8))\nplt.subplot(1, 4, 1)\nplt.title('Выжимаемость по кабинам')\nsns.barplot(data=df, x=\"Cabin\", y=\"Survived\")\nplt.subplot(1, 4, 2)\nplt.title('Выжимаемость по классам')\nsns.barplot(data=df, x=\"Pclass\", y=\"Survived\")\nplt.subplot(1, 4, 3)\nplt.title('Выжимаемость по кабинам внутри классов')\nsns.barplot(x = \"Pclass\", y = \"Survived\", hue = \"Cabin\", data = df)\nplt.subplot(1, 4, 4)\nplt.title('Распределение пассажиров по кабинам в зависимости от класса')\nsns.countplot(x=\"Cabin\", hue=\"Pclass\", data=df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Построим график в котором в процентах покажем распределение между кабинами в зависимости от класса","metadata":{}},{"cell_type":"code","source":"x,y = 'Pclass', 'Cabin'\n\ndf1 = df.groupby(x)[y].value_counts(normalize=True)\ndf1 = df1.mul(100)\ndf1 = df1.rename('percent').reset_index()\n\n\ng = sns.catplot(x=x,y='percent',hue=y,kind='bar',data=df1)\ng.ax.set_ylim(0,100)\n\nfor p in g.ax.patches:\n    txt = str(p.get_height().round(2)) + '%'\n    txt_x = p.get_x() \n    txt_y = p.get_height()\n    g.ax.text(txt_x,txt_y,txt)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### На основании этих графиков можно сказать:\n- Пассажиры 1 класса занимали кабины A,B,C,D,E\n- Пассажиры 2 класса занимали кабины F,A,D,C,E,G\n- Пассажиры 3 класса занимали кабины A,F,C,D,B,G\n\n### Теперь на основании эти данных заменим пустые значения кабины с помощью данных о классе пассажира c привязкой к вероятности ","metadata":{}},{"cell_type":"code","source":"df.Cabin=df.Cabin.fillna(0)\n\ndef cabin_zero(dat):\n    cab=0\n    class_ = ['C','B','A','D','E','F','G','T']\n    if dat.Cabin == 0: \n        if dat.Pclass == 1:\n            cab = random.choices(class_,weights=[0.3481, 0.2935, 0.2113,0.0938,0.0456,0.0058,0.0001,0.0018])\n        elif dat.Pclass == 2:\n            cab = random.choices(class_,weights=[0.1397,0.0848,0.2002,0.1517,0.1286,0.2299,0.0617,0.0034])\n        else:\n            cab = random.choices(class_,weights=[0.1358,0.0937,0.3043,0.1094,0.0979,0.2254,0.0309,0.0026])\n        return cab[0]\n    else:\n        return dat.Cabin\n\ndf['Cabin']=df.apply(lambda df: cabin_zero(df),axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Построим гарфик распределения еще раз и убедимся, что пропорции остались верными ","metadata":{}},{"cell_type":"code","source":"x,y = 'Pclass', 'Cabin'\n\ndf1 = df.groupby(x)[y].value_counts(normalize=True)\ndf1 = df1.mul(100)\ndf1 = df1.rename('percent').reset_index()\n\n\ng = sns.catplot(x=x,y='percent',hue=y,kind='bar',data=df1)\ng.ax.set_ylim(0,100)\n\nfor p in g.ax.patches:\n    txt = str(p.get_height().round(2)) + '%'\n    txt_x = p.get_x() \n    txt_y = p.get_height()\n    g.ax.text(txt_x,txt_y,txt)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Embarked \nНайдем зависимость между классом пассажира и метом посадки и заменим пропущенные значения ","metadata":{}},{"cell_type":"code","source":"x,y = 'Pclass', 'Embarked'\n\ndf1 = df.groupby(x)[y].value_counts(normalize=True)\ndf1 = df1.mul(100)\ndf1 = df1.rename('percent').reset_index()\n\n\ng = sns.catplot(x=x,y='percent',hue=y,kind='bar',data=df1)\ng.ax.set_ylim(0,100)\n\nfor p in g.ax.patches:\n    txt = str(p.get_height().round(2)) + '%'\n    txt_x = p.get_x() \n    txt_y = p.get_height()\n    g.ax.text(txt_x,txt_y,txt)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.Embarked=df.Embarked.fillna(0)\n\ndef embarked_zero(dat):\n    embark=0\n    port_ = ['S','C','Q']\n    if dat.Embarked == 0: \n        if dat.Pclass == 1:\n            embark = random.choices(port_,weights=[0.4757,0.3944,0.1299])\n        elif dat.Pclass == 2:\n            embark = random.choices(port_,weights=[0.7712,0.1982,0.0305])\n        else:\n            embark = random.choices(port_,weights=[0.80,0.1385,0.0519])\n        return embark[0]\n    else:\n        return dat.Embarked\n\ndf['Embarked']=df.apply(lambda df: embarked_zero(df),axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Fare \nПропущенных значений очень мало поэтому заменим неизвестные на медиану","metadata":{}},{"cell_type":"code","source":"df.groupby('Pclass').Fare.mean()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.Fare=df.Fare.fillna(0)\n\ndef fare_zero(dat):\n    class_1 = 103.209898\n    class_2 = 25.595244\n    class_3 = 19.498357\n    fare =0\n    if dat.Pclass == 1: \n        fare = class_1 + random.randint(0,20)\n    elif dat.Pclass == 2: \n        fare = class_2 + random.randint(0,7)\n    else:\n        fare = class_3 + random.randint(0,4)\n    return fare\n\ndf['Fare']=df.apply(lambda df: fare_zero(df),axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['Fare_Medu'] = df['Fare'].fillna(df.Fare.median())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Age\n\nПропущенных значений не так много, но стоит попробывать заменить их по умному ","metadata":{}},{"cell_type":"code","source":"df.groupby('Pclass').Age.mean()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#df.Age=df.Age.fillna(df.Age.median())\ndf.Age=df.Age.fillna(0)\n\ndef age_zero(dat):\n    class_1 = 40.672757\n    class_2 = 36.855067\n    class_3 = 30.205570\n    fare =0\n    if dat.Pclass == 1: \n        fare = class_1 + random.randint(0,10)\n    elif dat.Pclass == 2: \n        fare = class_2 + random.randint(0,7)\n    else:\n        fare = class_3 + random.randint(0,4)\n    return fare\n\ndf['Age']=df.apply(lambda df: age_zero(df),axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Name\n\nСоздадим два столбца для имени и фамилии, возьмем самые популярные фамилии и имена","metadata":{}},{"cell_type":"code","source":"df['FirstName'] = 'f'\ndf['LastName'] = 'l'\n\ndef first_name(dat):\n    patern_1 = re.compile('[A-Z][a-z]+')\n    name = patern_1.findall(dat.Name)\n    first = name[1]\n    return first\n\ndef second_name(dat):\n    patern_2 = re.compile('[A-Z][a-z]+')\n    name = patern_2.findall(dat.Name)\n    last = name[0]\n    return last\n    \ndf['FirstName'] = df.apply(lambda df: first_name(df),axis=1)       \ndf['LastName'] = df.apply(lambda df: second_name(df),axis=1)  ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Количество уникальных имен: %s'%len(df.FirstName.unique()))\nprint('Количество уникальных фамилий: %s'%len(df.LastName.unique()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Подготовка даных для нейронных сетей","metadata":{}},{"cell_type":"code","source":"df_final = df.copy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_final.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encoder = LabelEncoder()\nscaler = MinMaxScaler()\n\nlabel_futures = ['LastName',\n                 'FirstName',\n                 'Name',\n                 #'Ticket',\n                 #'Sex',\n                 #'Cabin',\n                 #'Embarked',\n                ]\ncategorical_features = ['Sex',\n                        'Cabin',\n                        'Embarked',\n                        'Pclass',\n                        'Alone',\n                        'FamOnBoard',\n                        'Ticket',\n                       ]\nnumerical_features = ['Age',\n                      'Fare',\n                      #'Fare_Medu',\n                      #'Parch',\n                      #'SibSp',\n                      #'Pclass',\n                     ]\n\n#for column in label:\n#    df_final[column] = encoder.fit_transform(df_final[column] )\n    \nfor column in label_futures:\n    df_final[column] = encoder.fit_transform(df_final[column] )\n    \ndf_final = pd.get_dummies(\n        df_final, columns=categorical_features, dummy_na=False)    \n    \n# Нормализация данных\nfor column in numerical_features:\n    df_final[column] = scaler.fit_transform(df_final[[column]])[:, 0]    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_final = df_final.drop(columns=[#'Ticket',\n                                  'Fare_Medu',\n                                  #'FirstName',\n                                  #'LastName'\n                                 ],\n                         axis=1)\ndf_final","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Split data","metadata":{}},{"cell_type":"code","source":"# Теперь выделим тестовую часть\ntrain_data = df_final.query('sample == 1').drop(['sample'], axis=1)\ntest_data = df_final.query('sample == 0').drop(['sample'], axis=1)\n\ny = train_data.Survived.values     # наш таргет\nX = train_data.drop(['Survived'], axis=1)\nX_sub = test_data.drop(['Survived'], axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.15, shuffle=True, random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model 1: CatBoostRegressor","metadata":{}},{"cell_type":"code","source":"model = CatBoostClassifier(iterations=1000,\n                          depth=4,\n                          reg_lambda=0.1,\n                          learning_rate=0.13,\n                          eval_metric='Accuracy',\n                           use_best_model=True,\n                           random_seed=42\n                          )\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.fit(X_train, y_train,\n          eval_set=(X_test, y_test),\n          verbose_eval=100,\n          #early_stopping_rounds=50,\n          use_best_model=True,\n          plot=True\n          )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- version_1 0.778\n- version_2 0.7805333 noramalization Fare\n- version_4 0.7805333 noramalization Fare Age\n- version_4 0.7834000 normalization !=Fare, + Tickets(Random)\n- version_10 0.7869333 normalization !=Fare, + My_Tickets(Random)\n- version_11 0.7872000 normalization Fare, Age + My_Tickets(Random)\n","metadata":{}},{"cell_type":"markdown","source":"## Выделим наиболее полезные признаки и сохраним только первые 20 ","metadata":{}},{"cell_type":"code","source":"feature_importances = pd.DataFrame()\nfi_tmp = pd.DataFrame()\nfi_tmp[\"feature\"] = X_test.columns.to_list()\nfi_tmp[\"importance\"] = model.get_feature_importance()\nfeature_importances = feature_importances.append(fi_tmp)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# just to get ideas to improve\norder = list(feature_importances.groupby(\"feature\").mean().sort_values(\"importance\", ascending=False).index)\nplt.figure(figsize=(10, 10))\nsns.barplot(x=\"importance\", y=\"feature\", data=feature_importances, order=order)\nplt.title(\"{} importance\".format(\"CatBoostClassifier\"))\nplt.tight_layout()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"not_importante_df=feature_importances.sort_values(by='importance',ascending=False)[20:]\nctb_impotante_fut=df_final.drop(columns=list(not_importante_df.feature),axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Теперь выделим тестовую часть\ntrain_data = ctb_impotante_fut.query('sample == 1').drop(['sample'], axis=1)\ntest_data = ctb_impotante_fut.query('sample == 0').drop(['sample'], axis=1)\n\ny = train_data.Survived.values     # наш таргет\nX = train_data.drop(['Survived'], axis=1)\nX_sub_ctb = test_data.drop(['Survived'], axis=1)\n\nX_train_ctb, X_test_ctb, y_train_ctb, y_test_ctb = train_test_split(\n    X, y, test_size=0.15, shuffle=True, random_state=42)\n\nmodel = CatBoostClassifier(iterations=1000,\n                          depth=4,\n                          #reg_lambda=0.1,\n                          learning_rate=0.12,\n                          eval_metric='Accuracy',\n                           use_best_model=True,random_seed=42\n                          )\n\nmodel.fit(X_train_ctb, y_train_ctb,\n          eval_set=(X_test_ctb, y_test_ctb),\n          verbose_eval=100,\n          #early_stopping_rounds=50,\n          use_best_model=True,\n          plot=True\n          )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model 2: RandomForest","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn import metrics\n\nregressor = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n            max_depth=None, max_features='auto', max_leaf_nodes=None,\n            min_impurity_decrease=0.0, min_impurity_split=None,\n            min_samples_leaf=1, min_samples_split=2,\n            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n            oob_score=False, random_state=None, verbose=0,\n            warm_start=False)\nregressor.fit(X_train, y_train)\ny_pred = regressor.predict(X_test)\n\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\npredictions = regressor.predict(X_sub)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Выделим наиболее полезные признаки и сохраним только первые 20 ","metadata":{}},{"cell_type":"code","source":"rdf_feature_importances = pd.DataFrame()\nrdf_fi_tmp = pd.DataFrame()\nrdf_fi_tmp[\"feature\"] = X_test.columns.to_list()\nrdf_fi_tmp[\"importance\"] = regressor.feature_importances_\nrdf_feature_importances = rdf_feature_importances.append(rdf_fi_tmp)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# just to get ideas to improve\norder = list(rdf_feature_importances.groupby(\"feature\").mean().sort_values(\"importance\", ascending=False).index)\nplt.figure(figsize=(10, 10))\nsns.barplot(x=\"importance\", y=\"feature\", data=rdf_feature_importances, order=order)\nplt.title(\"{} importance\".format(\"RandomForest\"))\nplt.tight_layout()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"not_importante_df=rdf_feature_importances.sort_values(by='importance',ascending=False)[20:]\nrdf_impotante_fut=df_final.drop(columns=list(not_importante_df.feature),axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Теперь выделим тестовую часть\ntrain_data_rdf = rdf_impotante_fut.query('sample == 1').drop(['sample'], axis=1)\ntest_data_rdf = rdf_impotante_fut.query('sample == 0').drop(['sample'], axis=1)\n\ny = train_data_rdf.Survived.values     # наш таргет\nX = train_data_rdf.drop(['Survived'], axis=1)\nX_sub_rdf = test_data_rdf.drop(['Survived'], axis=1)\n\nX_train_rdf, X_test_rdf, y_train_rdf, y_test_rdf = train_test_split(\n    X, y, test_size=0.15, shuffle=True, random_state=42)\n\nregressor = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n            max_depth=None, max_features='auto', max_leaf_nodes=None,\n            min_impurity_decrease=0.0, min_impurity_split=None,\n            min_samples_leaf=1, min_samples_split=2,\n            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n            oob_score=False, random_state=None, verbose=0,\n            warm_start=False)\nregressor.fit(X_train_rdf, y_train_rdf)\ny_pred = regressor.predict(X_test_rdf)\n\nprint(\"Accuracy:\",metrics.accuracy_score(y_test_rdf, y_pred))\npredictions = regressor.predict(X_sub_rdf)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model 3: LightGBM","metadata":{}},{"cell_type":"code","source":"train_data = df_final.query('sample == 1').drop(['sample'], axis=1)\n\nX_train, X_test, Y_train, Y_test = train_test_split(train_data.drop(['Survived'], axis=1), train_data.Survived,test_size=0.15)\nfeature_n = train_data.drop(columns='Survived',axis=1)\ntrain_dataset = lgb.Dataset(X_train, Y_train, feature_name=feature_n.columns.tolist())\ntest_dataset = lgb.Dataset(X_test, Y_test, feature_name=feature_n.columns.tolist())\n\nlgb_params = {\n        'boosting_type': 'gbdt',\n        'objective': 'binary',\n        'metric':'acc',\n        'learning_rate': 0.17,\n        #'is_unbalance': 'true',  #because training data is unbalance (replaced with scale_pos_weight)\n        'num_leaves': 31,  # we should let it be smaller than 2^(max_depth)\n        'max_depth': -1,  # -1 means no limit\n        'min_child_samples': 20,  # Minimum number of data need in a child(min_data_in_leaf)\n        'max_bin': 255,  # Number of bucketed bin for feature values\n        'subsample': 0.6,  # Subsample ratio of the training instance.\n        'subsample_freq': 0,  # frequence of subsample, <=0 means no enable\n        'colsample_bytree': 0.3,  # Subsample ratio of columns when constructing each tree.\n        'min_child_weight': 5,  # Minimum sum of instance weight(hessian) needed in a child(leaf)\n        'subsample_for_bin': 200000,  # Number of samples for constructing bin\n        'min_split_gain': 0,  # lambda_l1, lambda_l2 and min_gain_to_split to regularization\n        'reg_alpha': 0,  # L1 regularization term on weights\n        'reg_lambda': 0.13,  # L2 regularization term on weights\n        'nthread': 8,\n        'verbose': 0,\n    }\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clf = lgb.LGBMClassifier()\nclf.fit(X_train, Y_train)\n\n# predict the results\ny_pred=clf.predict(X_test)\n# view accuracy\nfrom sklearn.metrics import accuracy_score\naccuracy=accuracy_score(y_pred, Y_test)\nprint('LightGBM Model accuracy score: {0:0.4f}'.format(accuracy_score(Y_test, y_pred)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lgbm_feature_importances = pd.DataFrame()\nlgbm_fi_tmp = pd.DataFrame()\nlgbm_fi_tmp[\"feature\"] = X_test.columns.to_list()\nlgbm_fi_tmp[\"importance\"] = clf.feature_importances_\nlgbm_feature_importances = lgbm_feature_importances.append(lgbm_fi_tmp)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# just to get ideas to improve\norder = list(lgbm_feature_importances.groupby(\"feature\").mean().sort_values(\"importance\", ascending=False).index)\nplt.figure(figsize=(10, 10))\nsns.barplot(x=\"importance\", y=\"feature\", data=lgbm_feature_importances, order=order)\nplt.title(\"{} importance\".format(\"LGBMClassifier\"))\nplt.tight_layout()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"not_importante_df=lgbm_feature_importances.sort_values(by='importance',ascending=False)[20:]\nlgbm_impotante_fut=df_final.drop(columns=list(not_importante_df.feature),axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data_clf = lgbm_impotante_fut.query('sample == 1').drop(['sample'], axis=1)\ntest_data_clf = lgbm_impotante_fut.query('sample == 0').drop(['sample'], axis=1)\nX_sub_clf = test_data_clf.drop(['Survived'], axis=1)\n\nX_train_clf, X_test_clf, Y_train_clf, Y_test_clf = train_test_split(train_data_clf.drop(['Survived'], axis=1), train_data_clf.Survived,test_size=0.15)\nfeature_n = train_data.drop(columns='Survived',axis=1)\ntrain_dataset = lgb.Dataset(X_train_clf, Y_train_clf, feature_name=feature_n.columns.tolist())\ntest_dataset = lgb.Dataset(X_test_clf, Y_test_clf, feature_name=feature_n.columns.tolist())\n\nclf = lgb.LGBMClassifier()\nclf.fit(X_train_clf, Y_train_clf)\n\n# predict the results\ny_pred_clf=clf.predict(X_test_clf)\n\n# view accuracy\nfrom sklearn.metrics import accuracy_score\naccuracy=accuracy_score(y_pred_clf, Y_test_clf)\nprint('LightGBM Model accuracy score: {0:0.4f}'.format(accuracy_score(Y_test_clf, y_pred_clf)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model 4: Keras NN","metadata":{}},{"cell_type":"code","source":"# Теперь выделим тестовую часть\ntrain_data_nn = df_final.query('sample == 1').drop(['sample'], axis=1)\ntest_data_nn = df_final.query('sample == 0').drop(['sample'], axis=1)\n\ny_nn = train_data_nn.Survived.values     # наш таргет\nX_nn = train_data_nn.drop(['Survived'], axis=1).values\nSub_nn = test_data_nn.drop(['Survived'], axis=1).values\n\nX_train_nn, X_test_nn, y_train_nn, y_test_nn = train_test_split(\n    X_nn, y_nn, test_size=0.15, shuffle=True, random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Feature Scaling\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train_nn = sc.fit_transform(X_train_nn)\nX_test_nn = sc.fit_transform(X_test_nn)\n#y_train_nn = sc.fit_transform(y_train_nn)\n#y_test_nn = sc.fit_transform(y_test_nn)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_nn = M.Sequential()\nmodel_nn.add(L.Dense(256, activation='relu', input_dim = X_train_nn.shape[1], kernel_regularizer=regularizers.l1_l2(\n    l1=0.000000001, l2=0.000000001),))\nmodel_nn.add(L.Dense(128, activation=\"relu\", kernel_regularizer=regularizers.l1_l2(\n    l1=0.000000001, l2=0.000000001),))\nmodel_nn.add(L.Dropout(0.5))\nmodel_nn.add(L.Dense(1, activation='sigmoid'))\n\n\nsgd = optimizers.SGD(lr = 0.008, momentum = 0.99)\n\nmodel_nn.compile(loss='binary_crossentropy',\n              optimizer=sgd,\n              metrics=['accuracy'])\n\ncheckpoint = ModelCheckpoint('/kaggle/best_model_nn.hdf5', monitor=['val_accuracy'], verbose=0, mode='max')\nearlystop = EarlyStopping(monitor='val_accuracy', patience=50, restore_best_weights=True,)\ncallbacks_list = [checkpoint, earlystop]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model_nn.fit(X_train_nn, y_train_nn,\n                    batch_size=512,\n                    epochs=20,  # фактически мы обучаем пока EarlyStopping не остановит обучение\n                    validation_data=(X_test_nn, y_test_nn),\n                    callbacks=callbacks_list,\n                    verbose=2,\n                    )","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"acc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(1,len(acc)+1)\n\nplt.plot(epochs, acc, 'b', label='Training accuracy')\nplt.plot(epochs, val_acc, 'r', label='Validation accuracy')\nplt.title('Точность на обучении и проверки')\nplt.legend()\n\nplt.figure()\n\nplt.plot(epochs, loss, 'b', label='Training loss')\nplt.plot(epochs, val_loss, 'r', label='Validation loss')\nplt.title('Потери на обучении и проверки')\nplt.legend()\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Ансамблируем модели","metadata":{}},{"cell_type":"code","source":"predictions_cat_0 = model.predict(X_test_ctb)\npredictions_rdf_0 = regressor.predict(X_test_rdf)\npredictions_lgb_0 = clf.predict(X_test_clf)\nprediction_keras_nn = model_nn.predict(X_test_nn)\nprediction_keras_nn = prediction_keras_nn[prediction_keras_nn>0]\n\nresult_0=(predictions_cat_0+predictions_rdf_0+predictions_lgb_0+prediction_keras_nn)/4\nresult_0 = np.around(result_0).astype(int)\n\nprint('All Model accuracy score: {0:0.4f}'.format(accuracy_score(y_test, result_0)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions_cat = model.predict(X_sub)\npredictions_rdf = regressor.predict(X_sub_rdf)\npredictions_lgb = clf.predict(X_sub_clf)\nprediction_keras = model_nn.predict(Sub_nn)\nprediction_keras = prediction_keras_nn[prediction_keras_nn>0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result = (predictions_cat+predictions_rdf+predictions_lgb+prediction_keras)/4\nresult = np.around(result).astype(int)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"my_submission = pd.DataFrame({'PassengerId': df_test.PassengerId ,\n                              'Survived': (result.astype(int)) })\n\nmy_submission.to_csv(\"submission.csv\",index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"my_submission","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}