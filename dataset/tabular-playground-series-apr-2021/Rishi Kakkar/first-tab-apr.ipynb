{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Preface\n* This is the first submission after preprocessing the data\n* Filling the NaN values with Imblearn","metadata":{}},{"cell_type":"markdown","source":"# Indexing\n\n* [Library for NAN](#Imbalance-Learn)\n* [Model 1](#LightBGM)\n* [Val and train](#Validating-and-training)\n* [Classification Report 1](#Classification-report)\n* [Model 2](#XGBoost)\n* [Classification Report 2](#Classification-report2)\n* [Winning Model](#Winner)\n* [Submission](#Submission)","metadata":{}},{"cell_type":"markdown","source":"![Idhar](https://imbalanced-learn.org/stable/_static/logo.png)","metadata":{}},{"cell_type":"markdown","source":"## Imbalance-Learn\n* This is the library which is useful when data is less or imbalanced data in the classification problem\n* **Imblearn is a classic library which I used in order to remove NaN values**\n* Using Kmeans clustering algorithm to find out relative features\n* **KMeans Algorithm Helps to cluster similiar data points and figure out the NaN values from avilable data**","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/d/darknez/modified/Final.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.drop('Unnamed: 0',axis=1,inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pid = df['PassengerId']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.drop('PassengerId',axis=1,inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = df.iloc[:, 1:]\ny = df.iloc[:, 0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xnewed = x","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Validating-and-training\n* Training with a model and validating a result is fun part of the machine learning so that we can have a closer idea of model performance\n* It is also effective to see a 5% decrease in actual leader boards or 2% increase if used properly\n* The validation can be improved by introducing scaling, normalization, feature selection etc.","metadata":{}},{"cell_type":"code","source":"x_train_new, x_val_new, y_train_new, y_val_new = train_test_split(xnewed, y, test_size=0.01, random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## LightBGM\n* As this problem stems from the basic machine learning task a beginner does, It needs to be graph rarely and understood\n* Thus jumping straight into the model created by microsoft, for tree search\n* Tip: If you GridSearchCV you may attain better results","metadata":{}},{"cell_type":"code","source":"from lightgbm import LGBMClassifier","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lgbm = LGBMClassifier(n_estimators=500,max_depth=7,n_jobs=-1,\n                      num_leaves=256,colsample_bytree=0.9)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lgbm.fit(x_train_new, y_train_new)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Classification-report\n* An excellent function for classification problems providing overall metrics of how your model performed\n* Thus makes sure to notice f-score(i.e. testing score metric which combines recall and precision), sometimes a better metric for model comparison","metadata":{}},{"cell_type":"code","source":"print(classification_report(y_val_new,lgbm.predict(x_val_new)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lgbm.fit(x, y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from xgboost import XGBClassifier","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## XGBoost\n* Kaggle Compitetion winner, But only effective if you search the parameters\n* N_estimators = 350 because moderate number of trees provide better classification\n* max_depth = 7 because it has significance suggested to be n_estimators/(50-70) range\n* min_child_weight = 6 the weight of each child of the trees constructed\n* n_jobs to speed up the training by using all threads\n* Label encoder False to avoid warnings","metadata":{}},{"cell_type":"code","source":"xgbc = XGBClassifier(n_estimators = 350, use_label_encoder=False,max_depth=5,\n                     min_child_weight=6,n_jobs = -1,num_parallel_tree=2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgbc.fit(x_train_new, y_train_new)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Classification-report2\n* This is the report for XGBoost with my above defined parameters\n* For better results vary the parameters","metadata":{}},{"cell_type":"code","source":"print(classification_report(y_val_new, xgbc.predict(x_val_new)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Winner\n* Currently LightBGM in present setting is an better estimator than the XGB\n* But keep trying with [300,500] range estimators with [3,4,7,6] max_depth\n* Probably attain more","metadata":{}},{"cell_type":"code","source":"df_test = pd.read_csv('/kaggle/input/d/darknez/modified/Finals2.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test.drop(['Unnamed: 0','PassengerId'],inplace=True,axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ans = pd.DataFrame(lgbm.predict(df_test),columns=['Survived'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ans['PassengerId'] = range(100000,200000)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ans.set_index('PassengerId',inplace=True)\nans.reset_index(inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Submission\n* Getting the data as described in the submission rules and thus setting index to false to avoid a counting","metadata":{}},{"cell_type":"code","source":"ans.to_csv('submission28.csv',index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}