{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This idea is copied from the following notebook which has both great implemnetation and a much better LB score:\nhttps://www.kaggle.com/hiro5299834/tps-apr-2021-voting-pseudo-labeling","metadata":{}},{"cell_type":"markdown","source":"# Data Preprocessing","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler,OneHotEncoder\nfrom sklearn.model_selection import train_test_split, KFold, StratifiedKFold\nfrom sklearn.feature_selection import RFECV \n\nfrom sklearn.linear_model import LogisticRegression, LinearRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\n\nimport optuna\n\nimport graphviz\nimport matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv('../input/tabular-playground-series-apr-2021/train.csv')\ntest_df = pd.read_csv('../input/tabular-playground-series-apr-2021/test.csv')\nsubmission = pd.read_csv('../input/tabular-playground-series-apr-2021/sample_submission.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Age fillna with mean age for each class\ntrain_df['Age'] = train_df['Age'].fillna(train_df['Age'].mean())\ntest_df['Age'] = test_df['Age'].fillna(train_df['Age'].mean())\n\n# Cabin, fillna with 'N' and take first letter\ntrain_df['Cabin'] = train_df['Cabin'].fillna('N').map(lambda x: x[0].strip())\ntest_df['Cabin'] = test_df['Cabin'].fillna('N').map(lambda x: x[0].strip())\n\n# Ticket, fillna with 'X', split string and take first split \ntrain_df['Ticket'] = train_df['Ticket'].fillna('X').map(lambda x:str(x).split()[0] if len(str(x).split()) > 1 else 'X')\ntest_df['Ticket'] = test_df['Ticket'].fillna('X').map(lambda x:str(x).split()[0] if len(str(x).split()) > 1 else 'X')\n\n# Fare, fillna with mean value\nfare_map = train_df[['Fare', 'Pclass']].dropna().groupby('Pclass').median().to_dict()\ntrain_df['Fare'] = train_df['Fare'].fillna(train_df['Pclass'].map(fare_map['Fare']))\ntest_df['Fare'] = test_df['Fare'].fillna(test_df['Pclass'].map(fare_map['Fare']))\n\n# Embarked, fillna with 'N' value\ntrain_df['Embarked'] = train_df['Embarked'].fillna('N')\ntest_df['Embarked'] = test_df['Embarked'].fillna('N')\n\n# Creating family Size Variable\ntrain_df['family_size'] = train_df['SibSp'] + train_df['Parch']\ntest_df['family_size'] = test_df['SibSp'] + test_df['Parch']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"label_col = ['Pclass', 'Sex', 'Cabin', 'Embarked','Ticket']\n\nlc = LabelEncoder()\nfor col in label_col:\n    train_df[col] = lc.fit_transform(train_df[col])\n    test_df[col] = lc.transform(test_df[col])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features = ['Age','Fare','Pclass', 'Sex', 'Cabin', 'Embarked','Ticket','SibSp','Parch']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sc = StandardScaler()\ntrain_df[features] = sc.fit_transform(train_df[features])\ntest_df[features] = sc.transform(test_df[features])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Logistic Regression Model","metadata":{}},{"cell_type":"code","source":"n_folds = 10\nkf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\ny_pred = np.zeros(test_df.shape[0])\ny_oof = np.zeros(train_df.shape[0])\n\ntrain = train_df[features]\ntest = test_df[features]\ntarget = train_df['Survived']\nacc_mean = 0\n\nfor fold, (train_index, valid_index) in enumerate(kf.split(train, target)):\n    print(\"Running Fold {}\".format(fold + 1))\n    X_train, X_valid = pd.DataFrame(train.iloc[train_index]), pd.DataFrame(train.iloc[valid_index])\n    y_train, y_valid = target.iloc[train_index], target.iloc[valid_index]\n    model = LogisticRegression(max_iter=500)\n    model.fit(X_train, y_train)\n    y_oof[valid_index] = model.predict_proba(X_valid)[:,1] \n    accuracy = accuracy_score(y_valid, np.where(y_oof[valid_index]>0.5, 1, 0)) \n    print(\"  Accuracy: {}\".format(accuracy))\n    acc_mean += accuracy\n    y_pred += model.predict_proba(test)[:,1]    \n\ny_pred /= n_folds\nacc_mean /= n_folds\n\nprint(\"\")\nprint(\"Mean Accuracy: {}\".format(acc_mean))\nprint(\"Done!\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"oof_lr = y_oof\npred_lr = y_pred","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Decision Tree Model","metadata":{}},{"cell_type":"code","source":"params = {'max_depth': 10, 'min_samples_split': 818, 'min_samples_leaf': 35}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_folds = 10\nkf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\ny_pred = np.zeros(test_df.shape[0])\ny_oof = np.zeros(train_df.shape[0])\n\ntrain = train_df[features]\ntest = test_df[features]\ntarget = train_df['Survived']\nacc_mean = 0\n\nfor fold, (train_index, valid_index) in enumerate(kf.split(train, target)):\n    print(\"Running Fold {}\".format(fold + 1))\n    X_train, X_valid = pd.DataFrame(train.iloc[train_index]), pd.DataFrame(train.iloc[valid_index])\n    y_train, y_valid = target.iloc[train_index], target.iloc[valid_index]\n    model = DecisionTreeClassifier(**params)\n    model.fit(X_train , y_train) \n    y_oof[valid_index] = model.predict_proba(X_valid)[:,1] \n    accuracy = accuracy_score(y_valid, np.where(y_oof[valid_index]>0.5, 1, 0)) \n    print(\"  Accuracy: {}\".format(accuracy))\n    acc_mean += accuracy\n    y_pred += model.predict_proba(test)[:,1]   \n\ny_pred /= n_folds\nacc_mean /= n_folds\n\nprint(\"\")\nprint(\"Mean Accuracy: {}\".format(acc_mean))\nprint(\"Done!\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"oof_dt = y_oof\npred_dt = y_pred","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Random Forest Model","metadata":{}},{"cell_type":"code","source":"params = {'n_estimators': 400,\n    'max_depth': 80,\n    'min_samples_split': 41,\n    'min_samples_leaf': 36}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_folds = 10\nkf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\ny_pred = np.zeros(test_df.shape[0])\ny_oof = np.zeros(train_df.shape[0])\n\ntrain = train_df[features]\ntest = test_df[features]\ntarget = train_df['Survived']\nacc_mean = 0\n\nfor fold, (train_index, valid_index) in enumerate(kf.split(train, target)):\n    print(\"Running Fold {}\".format(fold + 1))\n    X_train, X_valid = pd.DataFrame(train.iloc[train_index]), pd.DataFrame(train.iloc[valid_index])\n    y_train, y_valid = target.iloc[train_index], target.iloc[valid_index]\n    model = RandomForestClassifier(**params)\n    model.fit(X_train , y_train)\n    y_oof[valid_index] = model.predict_proba(X_valid)[:,1] \n    accuracy = accuracy_score(y_valid, np.where(y_oof[valid_index]>0.5, 1, 0)) \n    print(\"  Accuracy: {}\".format(accuracy))\n    acc_mean += accuracy\n    y_pred += model.predict_proba(test)[:,1]   \n\ny_pred /= n_folds\nacc_mean /= n_folds\n\nprint(\"\")\nprint(\"Mean Accuracy: {}\".format(acc_mean))\nprint(\"Done!\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"oof_rf = y_oof\npred_rf = y_pred","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Light GBM","metadata":{}},{"cell_type":"code","source":"params = {\n    'metric': 'binary_logloss',\n    'n_estimators': 1000,\n    'objective': 'binary',\n    'learning_rate': 0.01,\n    'reg_lambda': 2.343120757686286,\n    'reg_alpha': 9.978349316502767,\n    'colsample_bytree': 0.4,\n    'min_child_samples': 100,\n    'max_depth': 70,\n    'num_leaves': 50,\n    'device_type' : 'gpu'\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_folds = 10\nkf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\ny_pred = np.zeros(test_df.shape[0])\ny_oof = np.zeros(train_df.shape[0])\n\ntrain = train_df[features]\ntest = test_df[features]\ntarget = train_df['Survived']\nacc_mean = 0\n\nfor fold, (train_index, valid_index) in enumerate(kf.split(train, target)):\n    print(\"Running Fold {}\".format(fold + 1))\n    X_train, X_valid = pd.DataFrame(train.iloc[train_index]), pd.DataFrame(train.iloc[valid_index])\n    y_train, y_valid = target.iloc[train_index], target.iloc[valid_index]\n    model = LGBMClassifier(**params)\n    model.fit(X_train , y_train , eval_set = [(X_valid , y_valid)] ,eval_metric='logloss', early_stopping_rounds = 300 , verbose = False)\n    y_oof[valid_index] = model.predict_proba(X_valid)[:,1] \n    accuracy = accuracy_score(y_valid, np.where(y_oof[valid_index]>0.5, 1, 0)) \n    print(\"  Accuracy: {}\".format(accuracy))\n    acc_mean += accuracy\n    y_pred += model.predict_proba(test)[:,1]   \n\ny_pred /= n_folds\nacc_mean /= n_folds\n\nprint(\"\")\nprint(\"Mean Accuracy: {}\".format(acc_mean))\nprint(\"Done!\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"oof_lgbm = y_oof\npred_lgbm = y_pred","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ANN Model","metadata":{}},{"cell_type":"code","source":"model = keras.Sequential([\n    layers.Dense(64, activation='relu', input_shape=[9]),\n    layers.Dropout(0.5),\n    layers.Dense(16, activation='relu'),\n    layers.Dropout(0.2),\n    layers.Dense(1, activation='sigmoid'),\n])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(\n    optimizer='adam',\n    loss='binary_crossentropy',\n    metrics=['binary_accuracy'],\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"early_stopping = keras.callbacks.EarlyStopping(\n    patience=20,\n    min_delta=0.001,\n    restore_best_weights=True,\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_folds = 10\nkf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\ny_pred = np.zeros(test_df.shape[0])\ny_oof = np.zeros(train_df.shape[0])\n\ntrain = train_df[features]\ntest = test_df[features]\ntarget = train_df['Survived']\nacc_mean = 0\n\nfor fold, (train_index, valid_index) in enumerate(kf.split(train, target)):\n    print(\"Running Fold {}\".format(fold + 1))\n    X_train, X_valid = pd.DataFrame(train.iloc[train_index]), pd.DataFrame(train.iloc[valid_index])\n    y_train, y_valid = target.iloc[train_index], target.iloc[valid_index]\n    model.fit(X_train, y_train, validation_data=(X_valid, y_valid), batch_size=512, epochs=100, callbacks=[early_stopping], verbose=0,)\n    y_oof[valid_index] = model.predict(X_valid).flatten()\n    accuracy = accuracy_score(y_valid, np.where(y_oof[valid_index]>0.5, 1, 0)) \n    print(\"  Accuracy: {}\".format(accuracy))\n    acc_mean += accuracy\n    y_pred += model.predict(test).flatten()  \n\ny_pred /= n_folds\nacc_mean /= n_folds\n\nprint(\"\")\nprint(\"Mean Accuracy: {}\".format(acc_mean))\nprint(\"Done!\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"oof_ann = y_oof\npred_ann = y_pred","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"oof_df = pd.DataFrame()\noof_df['oof_lr'] = oof_lr\noof_df['oof_dt'] = oof_dt\noof_df['oof_rf'] = oof_rf\noof_df['oof_lgbm'] = oof_lgbm\noof_df['oof_ann'] = oof_ann","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corrMatrix_oof = oof_df.corr()\nsns.heatmap(corrMatrix_oof, annot=True)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_df = pd.DataFrame()\npred_df['pred_lr'] = pred_lr\npred_df['pred_dt'] = pred_dt\npred_df['pred_rf'] = pred_rf\npred_df['pred_lgbm'] = pred_lgbm\npred_df['pred_ann'] = pred_ann","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corrMatrix_pr = oof_df.corr()\nsns.heatmap(corrMatrix_pr, annot=True)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"oof_df['Survived'] = train_df['Survived']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_folds = 10\nkf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)\ny_pred = np.zeros(test_df.shape[0])\ny_oof = np.zeros(train_df.shape[0])\n\ntrain = oof_df.drop('Survived', axis=1)\ntest = pred_df\ntarget = oof_df['Survived']\nacc_mean = 0\n\nfor fold, (train_index, valid_index) in enumerate(kf.split(train, target)):\n    print(\"Running Fold {}\".format(fold + 1))\n    X_train, X_valid = pd.DataFrame(train.iloc[train_index]), pd.DataFrame(train.iloc[valid_index])\n    y_train, y_valid = target.iloc[train_index], target.iloc[valid_index]\n    model = LogisticRegression()\n    model.fit(X_train, y_train)\n    y_oof[valid_index] = model.predict_proba(X_valid)[:,1] \n    accuracy = accuracy_score(y_valid, np.where(y_oof[valid_index]>0.5, 1, 0)) \n    print(\"  Accuracy: {}\".format(accuracy))\n    acc_mean += accuracy\n    y_pred += model.predict_proba(test)[:,1]   \n\ny_pred /= n_folds\nacc_mean /= n_folds\n\nprint(\"\")\nprint(\"Mean Accuracy: {}\".format(acc_mean))\nprint(\"Done!\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission['Survived'] = np.where(y_pred>0.5, 1, 0)\nsubmission.to_csv('submission.csv',index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}