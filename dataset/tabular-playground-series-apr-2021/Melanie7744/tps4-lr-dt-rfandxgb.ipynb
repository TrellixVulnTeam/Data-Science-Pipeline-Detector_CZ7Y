{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport matplotlib\nimport matplotlib.pyplot as plt # plotting\n%matplotlib inline \nprint(\"matplotlib version: {}\". format(matplotlib.__version__))\n\nimport seaborn as sns\nprint(\"seaborn version: {}\". format(sns.__version__))\n\nimport sklearn # machine learning algorithms\nprint(\"scikit-learn version: {}\". format(sklearn.__version__))\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nimport xgboost as xgb\nprint(\"xgboost version: {}\". format(xgb.__version__))\nfrom sklearn.metrics import confusion_matrix # creates a confusion matrix\nfrom sklearn.metrics import plot_confusion_matrix # draws a confusion matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import GridSearchCV # cross validation\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# About\nThis is the first notebook I publish on Kaggle. It's for the Tabular Playground Series - April 2021 \"Synthanic\". I tried four different models (Logistic Regression, Decision Tree, Random Forest and XGB) with different preprocessing and evolving features. My best public score I got was 0.79341 with a Logistic Regression. Code for analysing the wrong predicions on the validation set and overwriting predicions for all lines with a Cabin is also included. ","metadata":{}},{"cell_type":"code","source":"df_train = pd.read_csv('../input/tabular-playground-series-apr-2021/train.csv')\ndf_test = pd.read_csv('../input/tabular-playground-series-apr-2021/test.csv')\nsample_submission = pd.read_csv('../input/tabular-playground-series-apr-2021/sample_submission.csv') #the sample submission predicts everybody survived\ndf_all = df_train.append(df_test, ignore_index = True) # created because sometimes it is convenient to work on train and test set together","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1 Quick baseline in order to know which score my model must beat \n# -> 0.78505\nNote: this is based on previous knowledge of the original Titanic challenge. ","metadata":{}},{"cell_type":"code","source":"female_passengers = df_train[df_train.Sex == \"female\"]\nprint(\"Female passengers:\\n\", female_passengers.Survived.value_counts())\nmale_passengers = df_train[df_train.Sex == \"male\"]\nprint(\"Male passengers: \\n\", male_passengers.Survived.value_counts())\n\n# pretend that all women survived\nsurvived = (df_test.Sex == \"female\").astype('uint8')\n\n# create submission file \n#submission = pd.DataFrame({\n#    \"PassengerId\": df_test.PassengerId, \n#    \"Survived\": survived\n#})\n#submission.to_csv('submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2 EDA (Exploratory Data Analysis)","metadata":{}},{"cell_type":"code","source":"df_train.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.dtypes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.isna().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test.isna().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Column \"Cabin\" has more than half of the values missing. But the position of the Cabin might still give valuable information on survival, i.e. passengers from higher decks are more likely to reach the lifeboats (-> create feature). Drop column \"Ticket\" as it has nearly as many unique values as there are rows (and I can't make any sense of it). \nWe have to deal with the missing values in \"Age\", \"Fare\" and \"Embarked\". Let's fill it with a median. To check which one let's explore how much the median varies if it is computed overall/ by Sex / by Pclass. ","metadata":{}},{"cell_type":"code","source":"df_all.groupby([\"Sex\",\"Pclass\"]).agg({\"Age\":\"mean\",\"Fare\":\"mean\",\"Embarked\":pd.Series.mode})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.1 Deal with missing values \nAs shown above the mean varies between age and Plcass groups. So use the mean from the specific subgroup to replace missing values.","metadata":{}},{"cell_type":"code","source":"# for a Sex-Pclass combination replace the missing Ages/Fares/Embarked with the mean/mode\nfor sex in [\"male\",\"female\"]:\n    age_by_Pclass = df_all[df_all.Sex == sex].groupby([\"Pclass\"])[\"Age\"].mean().round(2)\n    fare_by_Pclass = df_all[df_all.Sex == sex].groupby([\"Pclass\"])[\"Fare\"].mean().round(2)\n    emb_by_Pclass = df_all[df_all.Sex == sex].groupby([\"Pclass\"])[\"Embarked\"].agg(pd.Series.mode)\n\n    for i in range(1,4):\n        df_all.loc[(df_all.Sex == sex) & (df_all.Pclass == i) & df_all.Age.isna(),\"Age\"] = age_by_Pclass[i]\n        df_all.loc[(df_all.Sex == sex) & (df_all.Pclass == i) & df_all.Fare.isna(),\"Fare\"] = fare_by_Pclass[i]\n        df_all.loc[(df_all.Sex == sex) & (df_all.Pclass == i) & df_all.Embarked.isna(),\"Embarked\"] = emb_by_Pclass[i]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_all.isna().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# this is an experiment with target encoding... I'm not convinced by it's usefulness, remove for now\n#mean_sex = df_all[0:len(df_train)].groupby([\"Sex\"])[\"Survived\"].mean()\n#mean_deck = df_all[0:len(df_train)].groupby([\"Deck\"])[\"Survived\"].mean()\n#mean_embarked = df_all[0:len(df_train)].groupby([\"Embarked\"])[\"Survived\"].mean()\n#df_all[\"Sex\"] = df_all[\"Sex\"].map(mean_sex)\n#df_all[\"Deck\"] = df_all[\"Deck\"].map(mean_deck)\n#df_all[\"Embarked\"] = df_all[\"Embarked\"].map(mean_embarked)\n#df_all.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_2 = df_all.copy() # make a copy for the seperate deck experiment","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.2 Visualize Distribution","metadata":{}},{"cell_type":"code","source":"fig = plt.figure() # create figure\nfsize = (14,7)\nax0 = fig.add_subplot(2, 4, 1) # add subplot 1 (2 rows, 4 columns, first plot)\nax1 = fig.add_subplot(2, 4, 2) \nax2 = fig.add_subplot(2, 4, 3) \nax3 = fig.add_subplot(2, 4, 4) \nax4 = fig.add_subplot(2, 4, 5) \nax5 = fig.add_subplot(2, 4, 6) \nax6 = fig.add_subplot(2, 4, 7) \nax7 = fig.add_subplot(2, 4, 8)\n\ndf_train.Survived.hist(figsize=fsize, ax=ax0)\ndf_train.Pclass.hist(figsize=fsize, ax=ax1)\ndf_train.Sex.hist(figsize=fsize, ax=ax2)\ndf_train.Age.hist(figsize=fsize, ax=ax3)\ndf_train.SibSp.hist(figsize=fsize, ax=ax4)\ndf_train.Parch.hist(figsize=fsize, ax=ax5)\ndf_train.Fare.hist(figsize=fsize, ax=ax6)\ndf_train.Embarked.hist(figsize=fsize, ax=ax7)\n\nax0.set_title(\"Survived\")\nax1.set_title(\"Plcass\")\nax2.set_title(\"Sex\")\nax3.set_title(\"Age\")\nax4.set_title(\"SibSp\")\nax5.set_title(\"Parch\")\nax6.set_title(\"Fare\")\nax7.set_title(\"Embarked\")\nplt.suptitle(\"Distributions in training data (df_train)\", fontsize=14)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# alternative, much shorter version to get the distributions. However this does not include the categorical variables. Could convert them before plotting...\n#fsize = (10,12)\n#plot_columns = [\"Survived\",\"Pclass\",\"Sex\", \"Age\",\"SibSp\",\"Parch\",\"Fare\",\"Embarked\" ]\n#df_train[plot_columns].hist(figsize=fsize)\n#plt.suptitle(\"Distributions in training data (df_train)\", fontsize=14)\n#plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure() # create figure\nfsize = (14,7)\nax0 = fig.add_subplot(2, 4, 1) # add subplot 1 (2 rows, 4 columns, first plot)\nax1 = fig.add_subplot(2, 4, 2) \nax2 = fig.add_subplot(2, 4, 3) \nax3 = fig.add_subplot(2, 4, 4) \nax4 = fig.add_subplot(2, 4, 5) \nax5 = fig.add_subplot(2, 4, 6) \nax6 = fig.add_subplot(2, 4, 7) \nax7 = fig.add_subplot(2, 4, 8)\n\ndf_test.Pclass.hist(figsize=fsize, ax=ax1)\ndf_test.Sex.hist(figsize=fsize, ax=ax2)\ndf_test.Age.hist(figsize=fsize, ax=ax3)\ndf_test.SibSp.hist(figsize=fsize, ax=ax4)\ndf_test.Parch.hist(figsize=fsize, ax=ax5)\ndf_test.Fare.hist(figsize=fsize, ax=ax6)\ndf_test.Embarked.hist(figsize=fsize, ax=ax7)\n\n\nax1.set_title(\"Plcass\")\nax2.set_title(\"Sex\")\nax3.set_title(\"Age\")\nax4.set_title(\"SibSp\")\nax5.set_title(\"Parch\")\nax6.set_title(\"Fare\")\nax7.set_title(\"Embarked\")\nplt.suptitle(\"Distributions in testing data (df_test)\", fontsize=14)\nplt.show()","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Most notable difference: In the test set the age distribution is different. So are Pclass and Sex ratio.","metadata":{}},{"cell_type":"code","source":"fig = plt.figure() # create figure\nfsize = (14,7)\nax0 = fig.add_subplot(2, 4, 1) # add subplot 1 (2 rows, 4 columns, first plot)\nax1 = fig.add_subplot(2, 4, 2) \nax2 = fig.add_subplot(2, 4, 3) \nax3 = fig.add_subplot(2, 4, 4) \nax4 = fig.add_subplot(2, 4, 5) \nax5 = fig.add_subplot(2, 4, 6) \nax6 = fig.add_subplot(2, 4, 7) \nax7 = fig.add_subplot(2, 4, 8)\n\n#df_all.Survived.hist(figsize=fsize, ax=ax0)\ndf_all.Pclass.hist(figsize=fsize, ax=ax1)\ndf_all.Sex.hist(figsize=fsize, ax=ax2)\ndf_all.Age.hist(figsize=fsize, ax=ax3)\ndf_all.SibSp.hist(figsize=fsize, ax=ax4)\ndf_all.Parch.hist(figsize=fsize, ax=ax5)\ndf_all.Fare.hist(figsize=fsize, ax=ax6)\ndf_all.Embarked.hist(figsize=fsize, ax=ax7)\n\n#ax0.set_title(\"Survived\")\nax1.set_title(\"Plcass\")\nax2.set_title(\"Sex\")\nax3.set_title(\"Age\")\nax4.set_title(\"SibSp\")\nax5.set_title(\"Parch\")\nax6.set_title(\"Fare\")\nax7.set_title(\"Embarked\")\nplt.suptitle(\"Distributions in combined data (df_all)\", fontsize=14)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# let's have a different look at the fare column\nfsize = (20,7)\nfig = plt.figure(figsize=fsize) # create figure\nax0 = fig.add_subplot(1, 1, 1) # add subplot 1 (1 row, 1 columns, first plot)\nsns.boxplot(x='Fare', data=df_train, ax=ax0)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3 Feature Engineering","metadata":{}},{"cell_type":"code","source":"#df_train[(df_train.Pclass == 1) & (df_train.Embarked == \"S\")].sort_values(by=[\"Name\"]).head(25)\n#df_train[(df_train.SibSp == 8)].sort_values(by=[\"Name\"]).head(25)\ndf_all.sort_values(by=\"Name\")[5:23]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It seems, that unlike in the original dataset, the synthetic dataset has no \"real\" family relations. Let's see if it is completely useless to create features like in the original dataset.\n\nEdit: it is not, the final score improved a bit after creating \"is_alone\" and \"family_size\".","metadata":{}},{"cell_type":"code","source":"# create new features\ndf_all[\"is_alone\"] = 1\ndf_all.loc[(df_all.SibSp > 0) | (df_all.Parch > 0),[\"is_alone\"]] = 0\ndf_all[\"family_size\"] = df_all.SibSp + df_all.Parch + 1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_all[\"Deck\"]= df_all[\"Cabin\"].str[0] # get the Deck from the Cabin number\ndf_all.loc[df_all.Deck.isna(), \"Deck\"] = \"N\" # treat the NaNs as a seperate category, maybe it means something for survival if the deck is unknown\ndf_all[0:len(df_train)].groupby(\"Deck\").agg({\"Deck\":\"count\",'Survived': 'mean'}) # check survival rates per deck","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Decks F, G and T have few values, based on their survival rates I group them. \ndf_all.loc[df_all.Deck == \"G\", \"Deck\"] = \"F\"\ndf_all.loc[df_all.Deck == \"T\", \"Deck\"] = \"N\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_all.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4 Data Preprocessing\nPrepare data for modelling. \n\nFor Logistic Regression: One hot encoding of categorical values (\"Sex\", \"Embarked\", \"Deck\"). Scale values to zero mean and same variance.\n\nFor Tree based models: One hot encoding of categorical values (\"Sex\", \"Embarked\", \"Deck\").","metadata":{}},{"cell_type":"code","source":"# drop unused columns\nprint(df_all.shape)\ndf_all.drop(columns=[\"Name\",\"Cabin\",\"Ticket\"], inplace=True)    \nprint(df_all.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# One Hot Encoding\ndf_all['Sex'].replace(to_replace=['male','female'], value=[1,0],inplace=True)\ndf_all = pd.get_dummies(df_all, columns= [\"Embarked\",\"Deck\"])\ndf_all.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# let's bin the features \"Age\" and \"Fare\" and add a new binned column for them\nno_bins = 6\ndf_all[\"Age_bin\"] = pd.cut(df_all.Age,no_bins,labels=False)\ndf_all[\"Fare_bin\"] = pd.cut(df_all.Fare,no_bins,labels=False)\ndf_all.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_all.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Mean normalization. I use the complete dataset here, but I am not sure yet if this is the correct place to do normalization\n# is it better to do it for training, validation and test set seperately?\n#columns = [\"Pclass\",\"Sex\",\"Age\",\"SibSp\",\"Parch\",\"Fare\",\"Embarked\", \"Deck\", \"is_alone\", \"family_size\"]\n#columns = [\"Pclass\",\"Sex\",\"Age\",\"SibSp\",\"Parch\",\"Fare\",\"is_alone\", \"family_size\",\"C\",\"Q\",\"S\"]\ncolumns = ['Pclass', 'Sex', 'SibSp','Parch', 'is_alone', 'family_size', 'Embarked_C','Embarked_Q', 'Embarked_S', 'Deck_A', 'Deck_B', 'Deck_C', 'Deck_D', 'Deck_E', 'Deck_F', 'Deck_N', 'Age_bin', 'Fare_bin']\nprint(\"columns: \", columns)\ncolumns_n = []  # create columns names for the normalized columns\nfor i in columns:\n    temp = i + \"_n\"\n    columns_n.append(temp)\nprint(\"columns_n: \", columns_n)\n\nscaler = StandardScaler() # all features are centered around 0 and have variance in the same order\ntemp = scaler.fit_transform(df_all[columns])\ndf_all = df_all.join(pd.DataFrame(data=temp, columns = columns_n)) # add normalized columns to df_all\ndf_all.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5 Start Modelling\n## 5.1 Setup Validation Scheme\nI use holdhout validation as the number of training examples is much larger than the number of features.","metadata":{}},{"cell_type":"code","source":"# split df_all again in training and testing part\ntraining_data = df_all[df_all.PassengerId<100000] # I dont overwrite df_train, df_train has still the values without encoding/normalization\ntesting_data = df_all[df_all.PassengerId>=100000]\ntraining_data.head(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# split training data and target variable\ny = training_data.Survived\nX = training_data.drop(columns=[\"PassengerId\",\"Survived\"])\n\n# split training set into training and validation part\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n# now I have X_train with 80% of the rows and X_val with 20% of the rows","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# quick check that there are a comparable percentage of survivers in the train and validation set, this should have been done by stratify=y\nprint(sum(y_train)/len(y_train))\nprint(sum(y_val)/len(y_val))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5.2.1 Train the first model: Logistic Regression","metadata":{}},{"cell_type":"code","source":"# initially without k-fold cross validation\nclf = LogisticRegression()\nclf.fit(X_train[columns_n], y_train) # use only the normalized columns for logistic regression\n#clf.fit(X_train[limited_features], y_train)\n# display the used parameters, regularization is on by default\nclf.get_params()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# make a prediction\ny_val_hat = clf.predict(X_val[columns_n]) # predicitions, 1dim np array\ny_val_hat_pr = clf.predict_proba(X_val[columns_n]) # contains the probabilitys of the predictions 2dim np array","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clf.score(X_val[columns_n], y_val) #  this the accuracy: number of correct predictions / number of predictions\n#(sum((y_val_hat == 1) & (y_val ==1)) + sum((y_val_hat == 0) & (y_val ==0))) / len(y_val)\n# previous: 0.76895\n# 0.7687 with one hot encoding\n# 0.76655 with Age and Fare as bins\n# 0.77245 with Deck\n# 0.77 with target encoded and normalized variables(v23)\n# 0.77185 with limited features","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# alternative method to get the accurarcy score\naccuracy_score(y_val, y_val_hat)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# quick check if cross validation would be useful. As the resulting accuracy is not much different, I decide not.\n\n#clf3 = LogisticRegression()\n#scores = cross_val_score(clf3, X[columns_n], y, cv=5)\n#scores","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# find the best parameter for regularization\nreg = [0.0001, 0.001, 0.01, 0.1, 1, 10]\nresult = []\nfor r in reg:\n    print(r)\n    clf = LogisticRegression(C=r)\n    clf.fit(X_train[columns_n], y_train) # train on training set\n    accuracy = clf.score(X_val[columns_n], y_val) # predict with validation set\n    print(accuracy)\n    result.append(accuracy)\n\nprint(\"\\n The best accuracy score is: \", max(result))\nindex_of_best_score = result.index(max(result))\nprint(\" with C: \", reg[index_of_best_score])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# make a prediction that can be submitted\nclf = LogisticRegression(C=0.01)\nclf.fit(X[columns_n], y) # retrain on whole dataset\ny_hat = clf.predict(testing_data[columns_n]) # predict for test set\ny_hat = y_hat.astype(int) # clf.predicts outputs float, which will give 0 score in submission","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check the thetas for each feature\npd.DataFrame({\"Variable\":X[columns_n].columns, \"Weights\": clf.coef_.round(2).reshape(-1)})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create final submission file\nsubmission = pd.DataFrame({\n    \"PassengerId\": df_test.PassengerId, \n    \"Survived\": y_hat\n})\nsubmission.to_csv('submission_logReg.csv', index=False)\n\n# private score on Leaderboard\n# 0.79337\n# 0.79341 with binned Age and Fare\n# 0.79183 using \"Deck\" and Target Encoding\n# 0.79296 using \"Deck\"\n# 0.78307 with limited features\n# limited_features = ['Pclass_n', 'Sex_n','Embarked_C_n', 'Embarked_Q_n', 'Embarked_S_n','Deck_A_n', 'Deck_B_n', 'Deck_C_n', 'Deck_D_n', 'Deck_E_n', 'Deck_F_n','Deck_N_n', 'Age_bin_n', 'Fare_bin_n']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 5.2.2 Try to predict survival of persons with known Cabin seperately","metadata":{}},{"cell_type":"code","source":"# let's try something: make a 2nd model and train it only for the rows having a \"Deck\"\nmask = df_2.Cabin.isna()\ndf_2 = df_2[~mask]\ndf_2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# feature creation and preprocessing\n# create new features\ndf_2[\"is_alone\"] = 1\ndf_2.loc[(df_2.SibSp > 0) | (df_2.Parch > 0),[\"is_alone\"]] = 0\ndf_2[\"family_size\"] = df_2.SibSp + df_2.Parch + 1\ndf_2[\"Deck\"]= df_2[\"Cabin\"].str[0] # get the Deck from the Cabin number\ndf_2.loc[df_2.Deck.isna(), \"Deck\"] = \"N\" # treat the NaNs as a seperate category, maybe it means something for survival if the deck is unknown\n#df_2[0:len(df_train)].groupby(\"Deck\").agg({\"Deck\":\"count\",'Survived': 'mean'}) # check survival rates per deck\n#df_2.loc[df_2.Deck == \"G\", \"Deck\"] = \"F\" # put small groups together\n#df_2.loc[df_2.Deck == \"T\", \"Deck\"] = \"F\"\ndf_2.drop(columns=[\"Name\",\"Cabin\",\"Ticket\"], inplace=True) \ndf_2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# preprocessing\ndf_2['Sex'].replace(to_replace=['male','female'], value=[1,0],inplace=True)\ndf_2 = pd.get_dummies(df_2, columns= [\"Embarked\",\"Deck\"])\ndf_2[\"Age_bin\"] = pd.cut(df_2.Age,no_bins,labels=False)\ndf_2[\"Fare_bin\"] = pd.cut(df_2.Fare,no_bins,labels=False)\ndf_2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"columns = ['Pclass', 'Sex', 'SibSp','Parch', 'is_alone', 'family_size', 'Embarked_C','Embarked_Q', 'Embarked_S', 'Deck_A', 'Deck_B', 'Deck_C', 'Deck_D', 'Deck_E', 'Deck_F', 'Age_bin', 'Fare_bin']\nprint(\"columns: \", columns)\ncolumns_n = []  # create columns names for the normalized columns\nfor i in columns:\n    temp = i + \"_n\"\n    columns_n.append(temp)\nprint(\"columns_n: \", columns_n)\n\nscaler = StandardScaler() # all features are centered around 0 and have variance in the same order\ntemp = scaler.fit_transform(df_2[columns])\ndf_2 = df_2.reset_index(drop=True) # reset index before join \ndf_2 = df_2.join(pd.DataFrame(data=temp, columns = columns_n)) # add normalized columns to df_2\n\n#df_2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# split data before modelling\ntraining_data_2 = df_2[df_2.PassengerId<100000] # I dont overwrite df_train, df_train has still the values without encoding/normalization\ntesting_data_2 = df_2[df_2.PassengerId>=100000]\ny_2 = training_data_2.Survived\nX_2 = training_data_2.drop(columns=[\"PassengerId\",\"Survived\"])\nX_train_2, X_val_2, y_train_2, y_val_2 = train_test_split(X_2, y_2, test_size=0.2, random_state=42, stratify=y_2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clf_2 = LogisticRegression()\nclf_2.fit(X_train_2[columns_n], y_train_2) # use only the normalized columns for logistic regression\ny_val_hat_2 = clf_2.predict(X_val_2[columns_n])\nclf_2.score(X_val_2[columns_n], y_val_2) # accuracy score: 0.7560292515948343 - worse than I hoped for","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# find the best parameter for regularization\nreg = [0.0001, 0.001, 0.01, 0.1, 1, 10]\nresult = []\nfor r in reg:\n    print(r)\n    clf_2 = LogisticRegression(C=r)\n    clf_2.fit(X_train_2[columns_n], y_train_2) # train on training set\n    accuracy = clf_2.score(X_val_2[columns_n], y_val_2) # predict with validation set\n    print(accuracy)\n    result.append(accuracy)\n\nprint(\"\\n The best accuracy score is: \", max(result))\nindex_of_best_score = result.index(max(result))\nprint(\" with C: \", reg[index_of_best_score])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# make a prediction that can be submitted\nclf_2 = LogisticRegression(C=0.01)\nclf_2.fit(X_2[columns_n], y_2) # retrain on whole dataset\ny_hat_2 = clf_2.predict(testing_data_2[columns_n]) # predict for test set\ny_hat_2 = y_hat_2.astype(int) # clf.predicts outputs float, which will give 0 score in submission","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_subset = pd.DataFrame({\"PassengerId\": testing_data_2.PassengerId, \n    \"Survived\": y_hat_2})\ndf_subset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create a dataframe with the original predictions from Log Reg and the new ones on the \"Cabin subset\"\ndf_both = submission.merge(df_subset, on=\"PassengerId\", how=\"left\")\ndf_both.columns= [\"PassengerId\", \"y_hat\", \"y_hat_2\"]\n\ndf_both","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# when there is Nan in y_hat_2 because this row did not have a Cabin, take the value from the original prediction y_hat instead\ndf_both.loc[df_both.y_hat_2.isna(),\"y_hat_2\"] = df_both.y_hat\ndf_both.y_hat_2 = df_both.y_hat_2.astype(int)\nprint(\"Number of predictions that differ: \",df_both[df_both.y_hat != df_both.y_hat_2].shape[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create submission file\nsubmission = pd.DataFrame({\n    \"PassengerId\": df_both.PassengerId, \n    \"Survived\": df_both.y_hat_2\n})\nsubmission.to_csv('submission_logReg_2.csv', index=False)\n\n# unfortunatelly it did not help, in contrary it made the private score worse\n# 0.79062 vs 0.79296","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5.3 Second Model: Decision Tree","metadata":{}},{"cell_type":"code","source":"tree = DecisionTreeClassifier(criterion = \"entropy\", random_state=0)\ntree.fit(X_train[columns],y_train) # use the not normalized columns for tree models\ny_val_hat = tree.predict(X_val[columns])\nprint(\"Training accuracy: \", tree.score(X_train[columns],y_train))\nprint(\"Validation accuracy: \", tree.score(X_val[columns],y_val))\nprint(\"---\")\nprint(\"with cross validation: \", cross_val_score(tree, X[columns], y, cv=5))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train[columns]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A tree with default parameters performs badly. Overfits to the training set. \n\n> columns = [\"Pclass\",\"Sex\",\"Age\",\"SibSp\",\"Parch\",\"Fare\",\"is_alone\", \"family_size\",\"C\",\"Q\",\"S\"]\n\n> Training accuracy:  0.994225\n> Validation accuracy:  0.6847\n> \n> with cross validation:  [0.68588 0.68956 0.68536 0.68344]\n\nOverfitting is reduced when Age and Fare bins are used!\n\n> columns = [\"Pclass\",\"Sex\",\"Age_bin\",\"SibSp\",\"Parch\",\"Fare_bin\",\"is_alone\", \"family_size\",\"C\",\"Q\",\"S\"]\n\n> Training accuracy:  0.7806\n> Validation accuracy:  0.76045\n> \n> with cross validation:  [0.76216 0.76216 0.76132 0.76056]\n\nwhen \"Deck\" is used\n\n> columns = ['Pclass', 'Sex', 'SibSp','Parch', 'is_alone', 'family_size', 'Embarked_C','Embarked_Q', 'Embarked_S', 'Deck_A', 'Deck_B', 'Deck_C', 'Deck_D', 'Deck_E', 'Deck_F', 'Deck_N', 'Age_bin', 'Fare_bin']\n> \n> Training accuracy:  0.7980125\n> Validation accuracy:  0.765\n> \n> with cross validation:  [0.7638  0.76475 0.7655  0.76455 0.76765]","metadata":{}},{"cell_type":"markdown","source":"## 5.4 Third model: Random Forest (first ensemble model using bagging)","metadata":{}},{"cell_type":"code","source":"forest = RandomForestClassifier(criterion=\"gini\", max_depth=5, min_samples_leaf = 2, random_state=0)\nforest.fit(X_train[columns],y_train)\nprint(\"Training accuracy: \", forest.score(X_train[columns],y_train))\nprint(\"Validation accuracy: \", forest.score(X_val[columns],y_val))\n\n# with default parameters:\n# Training accuracy:  0.9938875\n# Validation accuracy:  0.72675\n\n# with max_depth=5\n# Training accuracy:  0.770725\n# Validation accuracy:  0.76895\n\n# with max_depth=5 and min_samples_leaf = 2\n# Training accuracy:  0.7707\n# Validation accuracy:  0.76885\n\n# using bins\n# Training accuracy:  0.7689875\n# Validation accuracy:  0.7675\n\n# adding \"Deck\"\n# Training accuracy:  0.7737375\n# Validation accuracy:  0.774","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This is comparable to what I got from Logistic Regression. Let's retrain and make a submission.\nforest = RandomForestClassifier(criterion=\"gini\", max_depth=5, random_state=0)\nforest.fit(X[columns],y)\ny_hat = forest.predict(testing_data[columns])\ny_hat = y_hat.astype(int)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create final submission file\nsubmission = pd.DataFrame({\n    \"PassengerId\": df_test.PassengerId, \n    \"Survived\": y_hat\n})\nsubmission.to_csv('submission_forest.csv', index=False)\n\n# private score on Leaderboard\n# 0.78037\n# 0.77823 with binned Age and Fare\n# 0.79058 using \"Deck\" and Target Encoding\n# 0.78380 using \"Deck\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5.5 4th model: XGBoost (enseble mode using boosting)","metadata":{}},{"cell_type":"code","source":"X_train[columns].head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clf_xgb = xgb.XGBClassifier(objective='binary:logistic', missing=None, seed=42)\nclf_xgb.fit(X_train[columns], \n        y_train,\n        verbose=True,\n        ## the next three arguments set up early stopping.\n        early_stopping_rounds=10,\n        eval_metric='error',\n        eval_set=[(X_val[columns], y_val)])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_confusion_matrix(clf_xgb, \n                      X_val[columns], \n                      y_val,\n                      values_format='d',\n                      display_labels=[\"Drowned\", \"Survived\"]\n                     )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# try to improve the classification accuracy with better hyperparameters\n# Round 1\n#param_grid = {\n#     'max_depth': [3, 4, 5],\n#     'learning_rate': [0.1, 0.01, 0.05],\n#     'gamma': [0, 0.25, 1.0],\n#     'reg_lambda': [0, 1.0, 10.0]\n# }\n# Output: {'gamma': 0, 'learning_rate': 0.1, 'max_depth': 4, 'reg_lambda': 0}\n# Round 2\nparam_grid = {\n     'max_depth': [4],\n     'learning_rate': [0.1, 0.5, 1],\n     'gamma': [0],\n     'reg_lambda': [0, 0.5, 1]\n }\n# Output: {'gamma': 0, 'learning_rate': 0.1, 'max_depth': 4, 'reg_lambda': 0}\n#To speed up cross validiation, and to further prevent overfitting.\n# We are only using a random subset of the data (90%) and are only\n# using a random subset of the features (columns) (50%) per tree.\noptimal_params = GridSearchCV(\n     estimator=xgb.XGBClassifier(objective='binary:logistic', \n                                 seed=42,\n                                 subsample=0.9,\n                                 colsample_bytree=0.5),\n     param_grid=param_grid,\n     scoring='accuracy', ## see https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n     verbose=0, # NOTE: If you want to see what Grid Search is doing, set verbose=2\n     n_jobs = 10,\n     cv = 3\n )\n# only uncomment if you want to run grid search again, time consuming!\n#optimal_params.fit(X_train[columns], \n#        y_train,\n#        verbose=False,\n#        early_stopping_rounds=10,\n#        eval_metric='error',\n#        eval_set=[(X_val[columns], y_val)])\n#print(optimal_params.best_params_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clf_xgb = xgb.XGBClassifier(seed=42,\n                        objective='binary:logistic',\n                        gamma=0,\n                        learning_rate=0.1, \n                        max_depth=4,\n                        reg_lambda=0,\n                        subsample=0.9,\n                        colsample_bytree=0.5)\nclf_xgb.fit(X_train[columns], \n            y_train, \n            verbose=True, \n            early_stopping_rounds=10,\n            eval_metric='error',\n            eval_set=[(X_val[columns], y_val)])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_confusion_matrix(clf_xgb, \n                      X_val[columns], \n                      y_val,\n                      values_format='d',\n                      display_labels=[\"Drowned\", \"Survived\"]\n                     )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Looks like there is no improvement with better hyperparameters.","metadata":{}},{"cell_type":"code","source":"y_val_hat = clf_xgb.predict(X_val[columns]).astype(int)\n# get accuracy score on validation set like for the other models\naccuracy_score(y_val, y_val_hat)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# predict for testing data\ny_hat = clf_xgb.predict(testing_data[columns]).astype(int)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create final submission file\nsubmission = pd.DataFrame({\n    \"PassengerId\": df_test.PassengerId, \n    \"Survived\": y_hat\n})\nsubmission.to_csv('submission_xgb.csv', index=False)\n\n# private score on Leaderboard\n# 0.7834\n# 0.78529 tuned\n# 0.77000 with binned Age and Fare\n# 0.79090 using \"Deck\" and Target Encoding\n# 0.78630 using \"Deck\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 6 Analysis of wrong predictions\n\nThis has been done with the predicitions from logistic regression. Code might have to be adapted when a tree model has been run.","metadata":{}},{"cell_type":"code","source":"# let's compare the ground truth with the predictions and prediction probabilities on the validation set\ndf_compare = pd.DataFrame({ 'y_val': y_val, 'y_val_hat': y_val_hat}).reset_index()\ndf_compare2 = pd.DataFrame(y_val_hat_pr)\ndf_compare = df_compare.join(df_compare2)\ndf_compare.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get the rows that have been incorectly classified (predicted) and look at them in the original unnormalized data\nindex_of_wrong_class = df_compare[df_compare.y_val != y_val_hat][\"index\"]\ndf_train.iloc[index_of_wrong_class].head(50)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Sadly, I can't see anything obvious here.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}