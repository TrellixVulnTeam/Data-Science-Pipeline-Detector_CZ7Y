{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<h2>If the notebook helps you in learning new things and getting introduced to any new concepts,please do upvote the notebook.\n\n- Any improvement suggestions would be very much helpful","metadata":{}},{"cell_type":"markdown","source":"<h2>Tabular Playground Series-April'21</h2>\n\nIn the notebook, a series of steps in being followed in order to attain desired accuracy.\nThe steps being taken are as follows:\n-  Normal Data Exploration\n- Exploratory Data Analysis\n- NULL Value Treatment\n- Feature Transformation\n- Splitting and Testing models \n- Generating the results","metadata":{}},{"cell_type":"markdown","source":"<h1><b>1. Normal Data Exploration</b></h1>","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder\n\ntrain = pd.read_csv('/kaggle/input/tabular-playground-series-apr-2021/train.csv')\ntest = pd.read_csv('/kaggle/input/tabular-playground-series-apr-2021/test.csv')\n\n\ntrain.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3>As from the above information displayed, it can be clearly seen that there are NaN values in the dataset and hence the data needs further NULL value treatment and this is done after performing EDA</h3>","metadata":{}},{"cell_type":"markdown","source":"<h1><b>2. Exploratory Data Analysis</b></h1>","metadata":{}},{"cell_type":"code","source":"##Null values are present hence we need to do NULL Value treatment at an initial stage.\n#num = [x for x in train.columns if train[x].dtypes!='O']\nfor i in ['Age','Fare']:\n    sns.boxplot(train[i])\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##NULL Values Heatmap Visualization\nsns.heatmap(train.isnull())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.distplot(train.Age,hist = False)\n##It can be clearly seen from the distplot that most of the people were belonging to the age group of >20 and <80","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Here no class imbalance is found in case of the target variable and the disrtibution can be seen as follows\nsns.countplot(train['Survived'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##No class imbalance as such in case of Gender\nsns.countplot(train['Sex'],hue = train['Survived'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##This can be clearly observerd from the plot that most of the values for Embarked are S followed by C and then Q which are less in number\nsns.countplot(train['Embarked'],hue = train['Survived'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##For most of the families, there were families with 0 siblings together and very few of them had 1 or 2 siblings.\n# Count is found to be very less for more than 2 siblings\nsns.countplot(train['SibSp'],hue = train['Survived'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##This value of Parch was also found to be 0 for most of the times and rest of the values are very less in number\nsns.countplot(train['Parch'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2><b>3. NULL Value Treatment</b></h2>\n\n- <h3>Keeping the above plots in mind and the distribution of all variables,  appropriate values are being used for replacing with the NULL values</h3>","metadata":{}},{"cell_type":"code","source":"##Removing NULL values from the Age \ntest['Age'].fillna((train['Age'].median()), inplace=True)\ntrain['Age'].fillna((train['Age'].median()), inplace=True)\n\n##bfill is being done instead of any particular value since using any constant value at times reduced accuracy\n# test['Embarked'].fillna(method = 'bfill', inplace=True)\n# train['Embarked'].fillna(method = 'bfill', inplace=True)\n\ntest['Embarked'].fillna((train['Embarked'].mode()), inplace=True)\ntrain['Embarked'].fillna((train['Embarked'].mode()), inplace=True)\n\n##Removal of NULL values from Fare Attribute\ntest['Fare'].fillna((train['Fare'].mean()), inplace=True)\ntrain['Fare'].fillna((train['Fare'].mean()), inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3><b>Creating Extra Features</b></h3>\n\n- At times creating extra features using existing features in the dataset increases the overall model accuracy.\n- The new features being generated are based on the there correlation values.","metadata":{}},{"cell_type":"code","source":"##Creating new, extra features from increasing accuracy. \ntrain['Age*Fare'] = train['Age']*train['Fare']\ntest['Age*Fare'] = test['Age']*test['Fare']\n\n\ntrain['Age/Fare'] = train['Age']/train['Fare']\ntest['Age/Fare'] = test['Age']/test['Fare']\n\n\ntrain['FamilySize'] = train['SibSp'] + train['Parch'] + 1 \ntest['FamilySize'] = test['SibSp'] + test['Parch'] + 1 \n\n##Dropping unnecessary Categorical features. \ntrain.drop(['Name','Cabin','PassengerId','Ticket'],axis = 1,inplace = True)\ntest.drop(['Name','Cabin','PassengerId','Ticket'],axis = 1,inplace = True)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Also a common observation is being seen that:\n- For most of the instances, both the columns, Ticket and Cabin were found to be NULL at the same time. and hence can be considered for dropping.","metadata":{}},{"cell_type":"markdown","source":"<h3> Now that we have done all the required things for NULL value treatment, we further go on to work with Encoding Techniques in order to encode the available categorical features to convert them to numerical ones</h3>","metadata":{}},{"cell_type":"code","source":"cate = [x for x in train.columns if train[x].dtypes == 'O']\ncate1 = [x for x in test.columns if test[x].dtypes == 'O']\n\nfor i in cate:\n    label = LabelEncoder()\n    train[i] = label.fit_transform(train[i])\n    test[i] = label.transform(test[i])\n    \n\nX = train.drop('Survived',axis = 1)\ny = train.Survived","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##Correlation Heatmap\nplt.figure(figsize=(8, 8))\nsns.heatmap(train.corr(),annot = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2><b>4. Feature Transformation</b></h2>","metadata":{}},{"cell_type":"markdown","source":"Here various trials of different types of scaling techniques were being tried. Some amongst them are as:\n- 1. MinMaxScaler\n- 2. QuantileTransformer\n- 3. PowerTransformer\n- 4. RobustScaler.\n\nAll of the above scaling techniques should be applied to the data and this is much useful in increasing accuracy for Linear models. ","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import RobustScaler,PowerTransformer\nscaler = PowerTransformer()\nX2 = scaler.fit_transform(X)\ntest3 = scaler.transform(test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(X2,y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2>5. Basic Modelling</h2>","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression \nmdl = LogisticRegression().fit(X2,y)\nmdl.score(X_test,y_test)\n#0.76932","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred = mdl.predict(test3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred = pd.DataFrame(pred,columns = ['Survived'])\ntest = pd.concat([pred,test],axis = 1)\n\nclms = [train,test]\nfinal = pd.concat(clms)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- <h2>Extra Attempt: PsuedoLabelling</h2>\n\n- Using Results predicted on test data from models for actual predictions","metadata":{}},{"cell_type":"code","source":"xf = final.drop('Survived',axis = 1)\nyf = final.Survived\n\ntest11 = test.drop('Survived',axis = 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import RobustScaler,PowerTransformer\nscalerf = PowerTransformer()\nX2f = scalerf.fit_transform(xf)\ntest3 = scalerf.transform(test11)\n\nfrom sklearn.model_selection import train_test_split\nX_trainf,X_testf,y_trainf,y_testf = train_test_split(X2f,yf)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression \n\nmdl_final = LogisticRegression().fit(X2f,yf)\nmdl_final.score(X_testf,y_testf)\n#0.76932","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from xgboost import XGBClassifier\nxgb = XGBClassifier(max_depth = 6)\nxgb.fit(X2f,yf)\nxgb.score(X_testf,y_testf)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2>StackingClassifier</h2>\n\n- An efficient ensemble technique that can used to combine different models stacking them one on other \n- Various models could be combined together and a final model to make predictions on the one made by all other models results in the overall prediction and hence increases the overall performance","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import StackingClassifier\n\nestimators = [('m1',mdl_final),('ridge',xgb)]\nclf_mdl = StackingClassifier(estimators = estimators,final_estimator = XGBClassifier() )\nclf_mdl.fit(X2f,yf)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2>6. Generating Results for Submission</h2>","metadata":{}},{"cell_type":"code","source":"##Generating the results for submission\npredxgb = xgb.predict(test3)\npredxgb = pd.DataFrame(predxgb,columns = ['Survived'])\n\ntestt = pd.read_csv('/kaggle/input/tabular-playground-series-apr-2021/test.csv')\nres = pd.DataFrame(predxgb,columns = ['Survived'])\nres = pd.concat([testt.PassengerId,res],axis = 1)\n#res.to_csv('14_2.csv',index = False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##Generating the results for submission\npredxgb = clf_mdl.predict(test3)\npredxgb = pd.DataFrame(predxgb,columns = ['Survived'])\n\ntestt = pd.read_csv('/kaggle/input/tabular-playground-series-apr-2021/test.csv')\nres = pd.DataFrame(predxgb,columns = ['Survived'])\nres = pd.concat([testt.PassengerId,res],axis = 1)\n#res.to_csv('16_1.csv',index = False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"res.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Using LGBM\nfrom lightgbm import LGBMClassifier\nfrom sklearn.ensemble import VotingClassifier\n\nlgbm = LGBMClassifier(boosting_type = 'dart',num_leaves = 32,max_depth = 8,colsample_bytree = 0.8,extra_trees = True,n_jobs = -1)\nlgbm.fit(X2f, yf)\ny_pred = lgbm.predict(test3).astype(int)\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##Combining all best models\nest_vote = [('m1',clf_mdl),('m2',mdl_final),('ridge',xgb),('m3',lgbm)]\nclf_vote = VotingClassifier(estimators = est_vote,voting = 'soft')\nclf_vote.fit(X2f,yf)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = clf_vote.predict(test3)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##Generating the results for submission\npredvote = clf_vote.predict(test3)\npredvote = pd.DataFrame(predvote,columns = ['Survived'])\n\ntestt = pd.read_csv('/kaggle/input/tabular-playground-series-apr-2021/test.csv')\nres1 = pd.DataFrame(predvote,columns = ['Survived'])\nres1 = pd.concat([testt.PassengerId,res1],axis = 1)\nres1.to_csv('17.csv',index = False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2> More updates will be made soon!</h2>","metadata":{}}]}