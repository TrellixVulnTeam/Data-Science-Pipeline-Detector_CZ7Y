{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"\n\n# **What will we cover?**  \n##  *This code will help you get started with the problem and will also focus on getting some key insights into the data*\n##  *It also captures RF model with CV and submission file*\n\n##   *Contents*\n  \n### 1. Data Understanding  \n### 2. Data Cleaning  \n### 3. Data manipulation and preprocessing  \n### 4. Exploratory Data Analysis (Univariate and Bivariate analysis)  + Vizualizations\n### 5. Model\n### 6. Submission","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"![](https://preview.redd.it/0izq0428pe661.jpg?width=960&format=pjpg&auto=webp&s=15022053715fc50198a17c401be035445592fee2)","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Importing important packages","metadata":{}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom datetime import datetime\nfrom sklearn.preprocessing import LabelEncoder\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Read the data","metadata":{}},{"cell_type":"code","source":"df_train = pd.read_csv(\"../input/tabular-playground-series-apr-2021/train.csv\")\ndf_test = pd.read_csv(\"../input/tabular-playground-series-apr-2021/test.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Check the records in the data\nSurvived is the target variable, while other variables are the raw features in the train data.","metadata":{}},{"cell_type":"code","source":"# Check top 10 records of train data\ndf_train.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check top 10 records of test data\ndf_test.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## % of passenges who survived - 42.8%","metadata":{}},{"cell_type":"code","source":"# % of passenges who survived\ndf_train.Survived.value_counts()/ df_train.shape[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### A good strategy to understand features as a whole would be to combine trainig and testing data and then perform univariate analysis\n### So let's do it!!","metadata":{}},{"cell_type":"code","source":"df_test['Survived'] = 1  #temporarily kept as 1 for all test data passengers\n\n# To be able to filter the data later\ndf_train['Set'] = \"Train\" \ndf_test['Set'] = \"Test\"\n\n# Complete data\ndf_comp = pd.concat([df_train, df_test]).reset_index(drop = True)\ndf_comp.index.nunique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Check Missing Values of the features in the complete data\n\n## 1. Age ~3.38% missing values  \n## 2. Ticket ~ 4.9% missing values\n## 3. Fare ~ 0.13% missing values\n## 4. Cabin ~ 69.34% missing values\n## 4. Embarked ~ 0.26% missing values","metadata":{}},{"cell_type":"code","source":"#check missing values\nprint(\"Missing Values in the data \\n\",df_comp.isnull().sum()/df_comp.shape[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Treating Missing values","metadata":{}},{"cell_type":"markdown","source":"### Age  \n#### Treat by Median","metadata":{}},{"cell_type":"code","source":"# Distribution of Age\nprint(df_comp.Age.describe())\nsns.displot(df_comp.Age)\n\n# We can perfom median treatment for missing values of Age \ndf_comp.Age = df_comp.Age.fillna(df_comp.Age.median())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create Age Bucket\ndf_comp[\"Age_bucket\"] = pd.cut(df_comp[\"Age\"], 9, \n                                  labels=[\"0-9\",\"10-19\",\"20-29\",\"30-39\",\"40-49\",\"50-59\",\"60-69\",\"70-79\",\"80-89\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Fare\n#### Multimodal skewed distribution. This could be due to the class of the passengers  \n#### It is worth to check if Fare is dependent on Pclass and then do the treatment of missing values as per the passenger class\n#### Also because of heavy tail and outliers, we may have to perform a log transform to control the variation in Fare\n#### The Fare also has some values < 1, which means ln(Fare) can go negaitve. So we could do ln(1+Fare) transformation","metadata":{}},{"cell_type":"code","source":"# Distribution of Fare\nprint(df_comp.Fare.describe())\nsns.displot(df_comp.Fare)\n\n# Highly skewed distribution for Fare and seems multimodal histogram","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Clearly, the fare if dependent on Passenger Class\nsns.boxplot(x= df_comp.Pclass, y= np.log(1+df_comp.Fare))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's get the median values of Fare by Pclass for missing replacement\nFareByClass = pd.crosstab(index = df_comp.Pclass, columns = 'MedianFare', \\\n                          values = np.log(1+df_comp.Fare), aggfunc= 'median').to_dict()['MedianFare']\nFareByClass","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_comp.Pclass.map(FareByClass)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get a column with log of fare\ndf_comp['LnFare'] = np.log(1+df_comp['Fare'])\n\n# Replace missing fare values with log transformed values by PClass\ndf_comp['LnFare'].fillna(df_comp.Pclass.map(FareByClass),inplace=True)\n\n# Validating if missing values are treated\ndf_comp.loc[:,['Fare','LnFare']][df_comp.Fare.isna()]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Passengers who paid more fare had higher chances of survival","metadata":{}},{"cell_type":"code","source":"# Bivariate\nsns.boxplot(x = df_comp.Survived[df_comp.Set == \"Train\"],y= df_comp.LnFare[df_comp.Set == \"Train\"] )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Embarked\n#### Can be replaced by mode value of embarked which is 'S'","metadata":{}},{"cell_type":"code","source":"print(df_comp.Embarked.value_counts())\ndf_comp.Embarked.fillna('S', inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## For passengers who boarded the Ship in 'Southampton' had least chances of survival\n## while who boarded in Cherbourg had highest chances of survival","metadata":{}},{"cell_type":"code","source":"# Bivariate\npd.crosstab(index= df_comp.Embarked[df_comp.Set == \"Train\"] , columns= df_comp.Survived[df_comp.Set == \"Train\"], normalize='index' ). \\\nsort_values(by = 1).plot.bar(figsize=(15, 7),stacked = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Cabin\n### Cabin is interesting with approx 70% missing values\n### It may mean that 70% of the passengers did not have dedicated Cabin assigned.. maybe they were using some common/ shared rooms\n### It will be interesting to see if passengers with no cabin alloted had lower chances of survival\n### We also observe that the 1st alphbet of the cabin number may mean the Deck number (could be an important feature. Not done in this version of code)","metadata":{}},{"cell_type":"code","source":"# Let's create a new column CabinAlloted (1/0)\ndf_comp['CabinNotAlloted'] = df_comp.Cabin.isna().astype(int)\ndf_comp['CabinNotAlloted'].value_counts()/df_comp.shape[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Passengers with no cabin alloted had relatively lower chances of survival","metadata":{}},{"cell_type":"code","source":"# Bivariate\n# Passengers with no cabin alloted had relatively lower chances of survival\npd.crosstab(index = df_train.Cabin.isna(), columns= df_train.Survived, normalize = 'index').plot.bar(stacked = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's also fetch the first char of the cabin (which could signify Deck)\ndf_comp['Deck'] = df_comp.Cabin.str[0:1]\nprint(\"Unique decks \\n\", df_comp['Deck'].unique())\n\n# replace missing deck by 'X'\ndf_comp['Deck'].fillna('X',inplace= True)\n\nprint(\"Passengers in each deck \\n\", df_comp['Deck'].value_counts())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Ticket\n### Some ticket numbers have some special alpha charachters like A/5,CA, SC PARIS etc., while many are just numeric. May mean some categorization of Special class tickets\n#### Let's call the ones with only numbers as XX tickets\n### Let us try and do some text cleaning and feature extraction and then also put the missing values as 'XX'","metadata":{}},{"cell_type":"code","source":"# Check this out\nset(df_comp.Ticket.map(lambda x:str(x).split()[0] if len(str(x).split()) > 1 else 'XX'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let us create a new Ticket column with the above feature extracted and also do the missing value at the same time\ndf_comp['TicketType'] = df_comp.Ticket.fillna('XX')\ndf_comp['TicketType'] = df_comp.TicketType.map(lambda x:str(x).split()[0] if len(str(x).split()) > 1 else 'XX')\nprint(set(df_comp.TicketType))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let us do some text cleaning on TicketType\n# Convert to lower\ndf_comp['TicketType'] = df_comp['TicketType'].str.lower()\n\n# Get rid of dots and slash\nimport re\ndf_comp['TicketType'] = df_comp.TicketType.map(lambda x: re.sub(\"[^\\w\\s]+\",\"\",x))\nset(df_comp.TicketType.to_list())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's check the freq of passengers\ndf_comp.TicketType.value_counts()/ df_comp.shape[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.crosstab(index= df_comp.TicketType, columns= df_comp.Pclass, normalize='index' )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### df_comp['Pclass'].value_counts()/ df_comp.shape[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## The ticket type is related to Pclass \n## we also observe for the ticket types with less than 20% of 1st class passengers\n###  - stono, stono2, sotono2, stonoq, aq3, a and a5 have death rates >= 80%\n###  - sotonoq, fa, ca, fcc, scow, caston, wc and c have death rates >=65% and <80%\n###  - rest are <65% death rate (>35% survival rate)  \n  \n    \n\n### ** also most of the 'pc' ticket type passengers belong to first class\n","metadata":{}},{"cell_type":"code","source":"# Let's see if ticket type is related to Pclass\npd.crosstab(index= df_comp.TicketType, columns= df_comp.Pclass, normalize='index' ).sort_values(by = 1).plot.bar(figsize=(15, 7), stacked = True)\n\nplt.axhline(y = 0.2, color = 'r', linestyle = '-')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Bivariate\n# Check the ticket types when more than 60% did not survive \npd.crosstab(index= df_comp.TicketType[df_comp.Set == \"Train\"] , columns= df_comp.Survived[df_comp.Set == \"Train\"], normalize='index' ). \\\nsort_values(by = 1).plot.bar(figsize=(15, 7),stacked = True)\n\nplt.axhline(y = 0.8, color = 'r', linestyle = '-')\nplt.axhline(y = 0.65, color = 'g', linestyle = '-')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's create bucket for ticket type\ndf_comp['TT_bucket'] = df_comp.TicketType.map(lambda x: 0 if x == 'pc' else 3 if x in ['stono', 'stono2', 'sotono2', 'stonoq', 'aq3', 'a', 'a5'] else 2 if \\\n                                             x in ['sotonoq', 'fa', 'ca', 'fcc', 'scow', 'caston', 'wc', 'c'] else 1)\n\ndf_comp['TT_bucket'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Ticket Type Bucket shows survival rate varies by ticket types","metadata":{}},{"cell_type":"code","source":"#Bivariate\npd.crosstab(index= df_comp.TT_bucket[df_comp.Set == \"Train\"] , columns= df_comp.Survived[df_comp.Set == \"Train\"], normalize='index' ). \\\nsort_values(by = 1).plot.bar(figsize=(15, 7),stacked = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Change Sex variable to 1 and 0\ndf_comp['Sex'] = df_comp['Sex'].map(lambda i: 1 if i == 'male' else 0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Surival rate for women was higher","metadata":{}},{"cell_type":"code","source":"# Bivariate\npd.crosstab(index= df_comp.Sex[df_comp.Set == \"Train\"] , columns= df_comp.Survived[df_comp.Set == \"Train\"], normalize='index' ). \\\nsort_values(by = 1).plot.bar(figsize=(15, 7),stacked = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##combine the number of SibSp & Parch +1 to be FamilySize, a new feature synthesized:\ndf_comp['FamilySize'] = df_comp['SibSp'] + df_comp['Parch'] + 1 ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def family_size(x):\n    if x == 1:\n        return \"alone\"\n    else:\n        return \"notalone\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_comp[\"Group\"] = df_comp[\"FamilySize\"].apply(family_size)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Being Alone or not did not make much difference","metadata":{}},{"cell_type":"code","source":"# Bivariate\npd.crosstab(index= df_comp.Group[df_comp.Set == \"Train\"] , columns= df_comp.Survived[df_comp.Set == \"Train\"], normalize='index' ). \\\nsort_values(by = 1).plot.bar(stacked = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's create a backup for our analytical data with all features\ndf_copy_comp = df_comp.copy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Drop the unnecessary columns","metadata":{}},{"cell_type":"code","source":"# Check column names\ndf_comp.columns\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Drop unnecessary columns\ndf_comp.drop(columns = ['Name', 'Age', 'Ticket', 'Fare', 'Cabin', 'TicketType'], inplace= True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check data\ndf_comp.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Change group to 0 and 1 (0 mean Alone and 1 mean Group)\ndf_comp['Group'] = df_comp['Group'].map(lambda x: 0 if x == \"alone\" else 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get dummies for Embarked\ndf_comp = pd.get_dummies(df_comp, columns= ['Embarked'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get dummies for Age bucket\ndf_comp = pd.get_dummies(df_comp, columns= ['Age_bucket'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Label encode deck\n# Import label encoder\nfrom sklearn import preprocessing\n  \n# label_encoder object knows how to understand word labels.\nlabel_encoder = preprocessing.LabelEncoder()\n  \n# Encode labels in column 'species'.\ndf_comp['Deck']= label_encoder.fit_transform(df_comp['Deck'])\n  \ndf_comp['Deck'].unique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_comp.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Model\n# Get train and test data from comp data\n\nnew_train = df_comp[df_comp.Set == \"Train\"].drop([\"Set\", \"PassengerId\"], axis = 1)\nnew_test =  df_comp[df_comp.Set == \"Test\"].drop([\"Set\", \"PassengerId\",\"Survived\"], axis = 1)\n\nnew_train.head()\n#df_test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creat X feature set and y target\nX=new_train.drop(\"Survived\",axis=1).values\ny = new_train.Survived.values","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X.shape)\nprint(y.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## CROSS VALIDATION","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import KFold\n\n# Creating 5 folds (samples)\nkf = KFold(n_splits=5,random_state=42,shuffle=True)\n\n# Train test split\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.20,random_state=400)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn import ensemble\nfrom sklearn import metrics\ndef cross_val_fn(n_trees):\n    AUC =[]\n    for dev_index, validation_index in kf.split(X_train):\n        print(\"TRAIN:\", dev_index, \"TEST:\", validation_index)\n        X_dev, X_validation = X_train[dev_index], X_train[validation_index]\n        y_dev, y_validation = y_train[dev_index], y_train[validation_index]\n        clf=ensemble.RandomForestClassifier(n_jobs=-1,n_estimators=n_trees,random_state=400)\n        clf = clf.fit(X_dev, y_dev)\n        ### train my model on dev set and obtain some accuracy measure on validation set\n        # preds=clf.predict(X_validation)\n        probs=clf.predict_proba(X_validation)[:,1]\n        auc = metrics.roc_auc_score(y_validation,probs)\n        AUC.append(auc)\n    print(\"Mean AUC = \",np.array(AUC).mean())\n    return np.array(AUC).mean()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Blank dictionary for AUC for different iterations of n_estimators\nn_estimator_dict={}\n\n# 2 keys- # tress and AUC\n# Value pairs - n_tress and AUC output by the function\nn_estimator_dict['trees']=[]\nn_estimator_dict['AUC']=[]\n\n# Run many iterations of ensemble models starting from 10 trees till 200 tress with 20 steps frequency.. 10, 30, 50, 70,....190\nfor tree in range(10,200,10):\n    AUC=cross_val_fn(tree)\n    n_estimator_dict['trees'].append(tree)\n    n_estimator_dict['AUC'].append(AUC)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking the outputs in dictionary\ndf_auc = pd.DataFrame(n_estimator_dict)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.lineplot(x = df_auc.trees, y = df_auc.AUC)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trees=[150,170,190,210, 230, 250]\nmin_samples_split=[2,4,6]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import the library for creating cross product of tress and min sample split\nimport itertools","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a function to run multiple ensembles by lopping over cross product of multiple hyperparameters\ndef cross_val_fn(n_trees, min_samples):\n    AUC =[]\n    for dev_index, validation_index in kf.split(X_train):\n        print(\"TRAIN:\", dev_index, \"TEST:\", validation_index)\n        X_dev, X_validation = X_train[dev_index], X_train[validation_index]\n        y_dev, y_validation = y_train[dev_index], y_train[validation_index]\n        clf=ensemble.RandomForestClassifier(n_jobs=-1,n_estimators=n_trees,min_samples_split=min_samples,\n                                       random_state=400)\n        clf = clf.fit(X_dev, y_dev)\n        ### train my model on dev set and obtain some accuracy measure on validation set\n        # preds=clf.predict(X_validation)\n        \n        probs=clf.predict_proba(X_validation)[:,1]\n        auc = metrics.roc_auc_score(y_validation,probs)\n        AUC.append(auc)\n    print(\"Mean AUC = \",np.array(AUC).mean())\n    return np.array(AUC).mean()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Blank dictionary for AUC for different iterations of n_estimators\nn_estimator_dict={}\nn_estimator_dict['trees']=[]\nn_estimator_dict['Min Sample'] =[]\nn_estimator_dict['AUC']=[]\n\nfor tree,min_samples in itertools.product(trees,min_samples_split):\n    AUC=cross_val_fn(tree, min_samples)\n    n_estimator_dict['trees'].append(tree)\n    n_estimator_dict['Min Sample'].append(min_samples)\n    n_estimator_dict['AUC'].append(AUC)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create dataframe of dictionary\npd.DataFrame(n_estimator_dict)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Finalize RF with n_estimators = 250 and min_sample_split = 6\n# Import random forest classifier \nfrom sklearn.ensemble import RandomForestClassifier\n\n# Create classifier object\nclf=RandomForestClassifier(n_estimators=250, min_samples_split=6 ,oob_score=True,n_jobs=-1,random_state=400)\n\n# Fit model\nclf.fit(X,y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#clf.oob_score_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_names = new_train.drop(\"Survived\", axis = 1).columns\npd.Series(clf.feature_importances_,index=feature_names.tolist()).sort_values(ascending=False).plot(kind='barh', figsize = (15,7))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"y_hat = clf.predict(new_test)\nresults_df = pd.DataFrame(data={'PassengerId':df_test['PassengerId'], 'Survived':y_hat})\nresults_df.to_csv('submission-random_forest_kfold.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## I hope you like the notebook. Feel free to use it. Make sure you upvote and give credits.\n### All the best \n#### -JM","metadata":{}}]}