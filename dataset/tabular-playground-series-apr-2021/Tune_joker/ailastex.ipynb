{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# データ読み込み\nimport os\nimport numpy as np\nimport pandas as pd\n\n# フォルダ名からコンペ名を取得\ncompe_name = os.listdir('/kaggle/input')[0]\nprint(\"Competition name: \" + compe_name)\n\n# データ読み込み\ntrain = pd.read_csv('../input/%s/train.csv' % (compe_name))\ntest = pd.read_csv('../input/%s/test.csv' % (compe_name))\nprint(\"Train: \", train.shape)\nprint(\"Test: \", test.shape)\n\n# 提出用ファイルの見本からIndexとObjectiveの列名を取得\nsub_sample = pd.read_csv('../input/%s/sample_submission.csv' % (compe_name))\nsub_idxcol = sub_sample.columns[0]\nsub_objcol = sub_sample.columns[1]\n\nprint(\"Variables: \", train.columns)\nprint(\"Submission format: \")\nprint(sub_sample)\nprint(\"Index column name: \" + sub_idxcol)\nprint(\"Objective column name: \" + sub_objcol)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-01-13T15:58:01.601567Z","iopub.execute_input":"2022-01-13T15:58:01.602363Z","iopub.status.idle":"2022-01-13T15:58:02.101019Z","shell.execute_reply.started":"2022-01-13T15:58:01.602303Z","shell.execute_reply":"2022-01-13T15:58:02.099867Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 学習データ・テストデータの準備\nobj_var = 'Survived' # train.csvの目的変数の列名を指定する\n\nxtrn = train.drop([obj_var], axis=1)\nxtst = test.copy()\nytrn = train[obj_var]\n\n# 目的変数（Survived）以外の変数一覧\n#     True: 欠損値を含む変数\n#     False: 欠損値を含まない変数\nprint(pd.merge(xtrn, xtst, 'outer').isnull().any())","metadata":{"execution":{"iopub.status.busy":"2022-01-13T15:58:02.102908Z","iopub.execute_input":"2022-01-13T15:58:02.103362Z","iopub.status.idle":"2022-01-13T15:58:02.911793Z","shell.execute_reply.started":"2022-01-13T15:58:02.103324Z","shell.execute_reply":"2022-01-13T15:58:02.910387Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 絶対に使用しない変数をリストアップ\ndrop_vars = ['PassengerId']\n\n# 何らかの理由で使用しない変数をリストに追加\n# drop_vars += ['Name', 'Sex', 'Age', 'Ticket', 'Fare', 'Cabin', 'Embarked']\ndrop_vars += ['Name', 'Ticket', 'Cabin']\n\n# 欠損値を含む変数のリスト\ncat_miss_vars = ['Embarked'] # カテゴリ変数\nnum_miss_vars = ['Age', 'Fare'] # 数値変数\n\n# 使用する予定の変数（欠損値のない変数）\ncat_vars = ['Sex'] # カテゴリ変数\nnum_vars = ['Pclass', 'SibSp', 'Parch'] # 数値変数\n\n# 欠損値を含む変数を使用する変数リストに追加\ncat_vars += cat_miss_vars\nnum_vars += num_miss_vars","metadata":{"execution":{"iopub.status.busy":"2022-01-13T15:58:02.913261Z","iopub.execute_input":"2022-01-13T15:58:02.91349Z","iopub.status.idle":"2022-01-13T15:58:02.921242Z","shell.execute_reply.started":"2022-01-13T15:58:02.913464Z","shell.execute_reply":"2022-01-13T15:58:02.919989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 使用しない変数を削除\nxtrn.drop(drop_vars, axis=1, inplace=True)\nxtst.drop(drop_vars, axis=1, inplace=True)\nprint(xtrn)","metadata":{"execution":{"iopub.status.busy":"2022-01-13T15:58:02.922907Z","iopub.execute_input":"2022-01-13T15:58:02.923748Z","iopub.status.idle":"2022-01-13T15:58:02.96492Z","shell.execute_reply.started":"2022-01-13T15:58:02.923675Z","shell.execute_reply":"2022-01-13T15:58:02.963872Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 欠損値補完の処理\n\n# カテゴリ変数の欠損箇所を「__NA__」という文字列で埋める\nfor c in cat_miss_vars:\n    replace_cat = '__NA__'\n    xtrn[c].fillna(replace_cat, inplace=True)\n    xtst[c].fillna(replace_cat, inplace=True)\n\n# 数値変数の欠損値を「学習データにおける平均値」で埋める\nfor c in num_miss_vars:\n    replace_num = xtrn[c].mean()\n    xtrn[c].fillna(replace_num, inplace=True)\n    xtst[c].fillna(replace_num, inplace=True)\n    \nprint(pd.merge(xtrn, xtst, 'outer').isnull().any())","metadata":{"execution":{"iopub.status.busy":"2022-01-13T15:58:02.968802Z","iopub.execute_input":"2022-01-13T15:58:02.969446Z","iopub.status.idle":"2022-01-13T15:58:03.29184Z","shell.execute_reply.started":"2022-01-13T15:58:02.969399Z","shell.execute_reply":"2022-01-13T15:58:03.290619Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 変数の組み合わせ\n# # AgeとParchを組み合わせ\n# xtrn[\"AgeParch\"] = xtrn[\"Age\"] * (xtrn[\"Parch\"]+1)\n# xtst[\"AgeParch\"] = xtst[\"Age\"] * (xtst[\"Parch\"]+1)\n\nxtrn[\"FarePclass\"] = xtrn[\"Fare\"] * xtrn[\"Pclass\"]\nxtst[\"FarePclass\"] = xtst[\"Fare\"] * xtst[\"Pclass\"]\n\nnum_vars += ['FarePclass']\nprint(xtrn)","metadata":{"execution":{"iopub.status.busy":"2022-01-13T15:58:03.293255Z","iopub.execute_input":"2022-01-13T15:58:03.293552Z","iopub.status.idle":"2022-01-13T15:58:03.310539Z","shell.execute_reply.started":"2022-01-13T15:58:03.293516Z","shell.execute_reply":"2022-01-13T15:58:03.309513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 変換前の数値変数\nprint(xtrn[num_vars])\n\nfrom sklearn.preprocessing import MinMaxScaler\n\n# # 学習データに基づいて複数列のYeo-Johnson変換を定義\n# pt = PowerTransformer(method='box-cox')\n# pt.fit(xtrn[num_vars])\n\n#学習データに基づいて複数列のMin-Maxスケーリングを定義\npt = MinMaxScaler()\npt.fit(xtrn[num_vars])\n\n# 変換後のデータで各列を置換\nxtrn[num_vars] = pt.transform(xtrn[num_vars])\nxtst[num_vars] = pt.transform(xtst[num_vars])\n\n# 変換後の数値変数\nprint(xtrn[num_vars])","metadata":{"execution":{"iopub.status.busy":"2022-01-13T15:58:03.312213Z","iopub.execute_input":"2022-01-13T15:58:03.312507Z","iopub.status.idle":"2022-01-13T15:58:03.364619Z","shell.execute_reply.started":"2022-01-13T15:58:03.312473Z","shell.execute_reply":"2022-01-13T15:58:03.363961Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 変換前のカテゴリ変数\nprint(xtrn[cat_vars])\n\nfrom sklearn.model_selection import KFold\n\n# クロスバリデーションのfoldを定義\nkf = KFold(n_splits=4, shuffle=True, random_state=71)\n\n# 変数をループしてtarget encoding\nfor c in cat_vars:\n\n    # targetを付加\n    data_tmp = pd.DataFrame({c: xtrn[c], 'target': ytrn})\n    target_mean = data_tmp.groupby(c)['target'].mean()\n    \n    # テストデータのカテゴリを置換\n    xtst[c] = xtst[c].map(target_mean)\n    \n    # 変換後の値を格納する配列を準備\n    tmp = np.repeat(np.nan, xtrn.shape[0])\n\n    # 学習データからバリデーションデータを分ける\n    for i, (tr_idx, va_idx) in enumerate(kf.split(xtrn)):\n        # 学習データについて、各カテゴリにおける目的変数の平均を計算\n        target_mean = data_tmp.iloc[tr_idx].groupby(c)['target'].mean()\n        # バリデーションデータについて、変換後の値を一時配列に格納\n        tmp[va_idx] = xtrn[c].iloc[va_idx].map(target_mean)\n\n    # 変換後のデータで元の変数を置換\n    xtrn[c] = tmp\n\n\n# 変換後のカテゴリ変数\nprint(xtrn[cat_vars])","metadata":{"execution":{"iopub.status.busy":"2022-01-13T15:58:03.366089Z","iopub.execute_input":"2022-01-13T15:58:03.366433Z","iopub.status.idle":"2022-01-13T15:58:03.571139Z","shell.execute_reply.started":"2022-01-13T15:58:03.366403Z","shell.execute_reply":"2022-01-13T15:58:03.569911Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ---------------------------------\n# 相関係数\n# ---------------------------------\nimport scipy.stats as st\n\n# 相関係数\ncorrs = []\nfor c in xtrn.columns:\n    corr = np.corrcoef(xtrn[c], ytrn)[0, 1]\n    corrs.append(corr)\ncorrs = np.array(corrs)\n\n# ---------------------------------\n# カイ二乗統計量\n# ---------------------------------\nfrom sklearn.feature_selection import chi2\nfrom sklearn.preprocessing import MinMaxScaler\n\n# カイ二乗統計量\n# x = MinMaxScaler().fit_transform(xtrn) # すでにMinMaxスケーリングを行っているので、省略\nc2, _ = chi2(xtrn, ytrn)\n\n# 重要度の上位を出力する（上位5個まで）\nidx = np.argsort(c2)[::-1]\ntop_cols, top_importances = xtrn.columns.values[idx][7], corrs[idx][7]\nprint(top_cols, top_importances)\n\n# 上位5つ以外の特徴量を削除\nxtrn.drop(top_cols, axis=1, inplace=True)\nxtst.drop(top_cols, axis=1, inplace=True)\nprint(xtrn)","metadata":{"execution":{"iopub.status.busy":"2022-01-13T15:58:03.57263Z","iopub.execute_input":"2022-01-13T15:58:03.572898Z","iopub.status.idle":"2022-01-13T15:58:03.648587Z","shell.execute_reply.started":"2022-01-13T15:58:03.572864Z","shell.execute_reply":"2022-01-13T15:58:03.644818Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# PCA\nfrom sklearn.decomposition import PCA\n\n# データは標準化などのスケールを揃える前処理が行われているものとする\ntrain_x_saved = xtrn.copy()\n# 学習データに基づいてPCAによる変換を定義\npca = PCA() # PCA後に全主成分を取り出す場合\n#pca = PCA(n_components=5) # PCA後に第1～5主成分のみ取り出す場合\npca.fit(xtrn)\n\n# 変換の適用\nxtrn = pca.transform(xtrn)\nxtst = pca.transform(xtst)\n\nprint(train_x_saved[:1])\nprint(xtrn[:1])","metadata":{"execution":{"iopub.status.busy":"2022-01-13T15:58:03.650611Z","iopub.execute_input":"2022-01-13T15:58:03.653675Z","iopub.status.idle":"2022-01-13T15:58:03.726105Z","shell.execute_reply.started":"2022-01-13T15:58:03.653617Z","shell.execute_reply":"2022-01-13T15:58:03.725094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # ---------------------------------\n# # スタッキング\n# # ----------------------------------\n# from sklearn.metrics import log_loss\n# from sklearn.model_selection import KFold\n\n# # models.pyにModel1Xgb, Model1NN, Model2Linearを定義しているものとする\n# # 各クラスは、fitで学習し、predictで予測値の確率を出力する\n\n# from models import Model1Xgb, Model1NN, Model2Linear\n\n\n# # 学習データに対する「目的変数を知らない」予測値と、テストデータに対する予測値を返す関数\n# def predict_cv(model, train_x, train_y, test_x):\n#     preds = []\n#     preds_test = []\n#     va_idxes = []\n\n#     kf = KFold(n_splits=4, shuffle=True, random_state=71)\n\n#     # クロスバリデーションで学習・予測を行い、予測値とインデックスを保存する\n#     for i, (tr_idx, va_idx) in enumerate(kf.split(train_x)):\n#         tr_x, va_x = train_x.iloc[tr_idx], train_x.iloc[va_idx]\n#         tr_y, va_y = train_y.iloc[tr_idx], train_y.iloc[va_idx]\n#         model.fit(tr_x, tr_y, va_x, va_y)\n#         pred = model.predict(va_x)\n#         preds.append(pred)\n#         pred_test = model.predict(test_x)\n#         preds_test.append(pred_test)\n#         va_idxes.append(va_idx)\n\n#     # バリデーションデータに対する予測値を連結し、その後元の順序に並べ直す\n#     va_idxes = np.concatenate(va_idxes)\n#     preds = np.concatenate(preds, axis=0)\n#     order = np.argsort(va_idxes)\n#     pred_train = preds[order]\n\n#     # テストデータに対する予測値の平均をとる\n#     preds_test = np.mean(preds_test, axis=0)\n\n#     return pred_train, preds_test\n\n\n# # 1層目のモデル\n# # pred_train_1a, pred_train_1bは、学習データのクロスバリデーションでの予測値\n# # pred_test_1a, pred_test_1bは、テストデータの予測値\n# model_1a = Model1Xgb()\n# pred_train_1a, pred_test_1a = predict_cv(model_1a, xtrn, ytrn, xtst)\n\n# model_1b = Model1NN()\n# pred_train_1b, pred_test_1b = predict_cv(model_1b, train_x_nn, train_y, test_x_nn)\n\n# # 1層目のモデルの評価\n# print(f'logloss: {log_loss(train_y, pred_train_1a, eps=1e-7):.4f}')\n# print(f'logloss: {log_loss(train_y, pred_train_1b, eps=1e-7):.4f}')\n\n# # 予測値を特徴量としてデータフレームを作成\n# train_x_2 = pd.DataFrame({'pred_1a': pred_train_1a, 'pred_1b': pred_train_1b})\n# test_x_2 = pd.DataFrame({'pred_1a': pred_test_1a, 'pred_1b': pred_test_1b})\n\n# # 2層目のモデル\n# # pred_train_2は、2層目のモデルの学習データのクロスバリデーションでの予測値\n# # pred_test_2は、2層目のモデルのテストデータの予測値\n# model_2 = Model2Linear()\n# pred_train_2, pred_test_2 = predict_cv(model_2, train_x_2, train_y, test_x_2)\n# print(f'logloss: {log_loss(train_y, pred_train_2, eps=1e-7):.4f}')","metadata":{"execution":{"iopub.status.busy":"2022-01-13T15:58:03.731485Z","iopub.execute_input":"2022-01-13T15:58:03.731932Z","iopub.status.idle":"2022-01-13T15:58:03.745608Z","shell.execute_reply.started":"2022-01-13T15:58:03.731886Z","shell.execute_reply":"2022-01-13T15:58:03.744551Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# XGBoostモデルの学習\nfrom xgboost import XGBClassifier\n\n# モデルの作成および学習データを与えての学習\nmodel = XGBClassifier()\nmodel.fit(xtrn, ytrn)","metadata":{"execution":{"iopub.status.busy":"2022-01-13T15:58:03.747689Z","iopub.execute_input":"2022-01-13T15:58:03.748379Z","iopub.status.idle":"2022-01-13T15:58:15.106155Z","shell.execute_reply.started":"2022-01-13T15:58:03.748335Z","shell.execute_reply":"2022-01-13T15:58:15.105264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 欠損値を含む特徴量を使わない 0.761950\n# NAと平均値で補完           0.792430\n# PCA                      0.794210\n# 重要度最下位の特徴量を削除　 0.789500\n# 重要度最下位から2つを削除　 0.787500\n# Age*Parch               0.791590\n# Age+Parch               0.792660\n# Age*Pclass              0.791270\n# Fare*Pclass             0.793480\n# Fare/Pclass             0.792920\n# 学習済みモデルによる予測\nfrom sklearn.metrics import accuracy_score\n\nptrn = model.predict_proba(xtrn)\nptst = model.predict_proba(xtst)\nltrn = np.argmax(ptrn, axis=1)\nltst = np.argmax(ptst, axis=1)\n\nprint(ptrn)\nprint(ltrn)\nprint(ytrn)\n\nacctrn = accuracy_score(ytrn, ltrn)\nprint(\"XGBoost training accuracy: %f\" % (acctrn))","metadata":{"execution":{"iopub.status.busy":"2022-01-13T15:58:15.107496Z","iopub.execute_input":"2022-01-13T15:58:15.107782Z","iopub.status.idle":"2022-01-13T15:58:15.341484Z","shell.execute_reply.started":"2022-01-13T15:58:15.107751Z","shell.execute_reply":"2022-01-13T15:58:15.340109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 提出用ファイルを作成\ncsvname = 'submission_xgboost.csv'\nprint(csvname)\nsubmission = pd.DataFrame({sub_idxcol: sub_sample[sub_idxcol], sub_objcol: ltst})\nsubmission.to_csv(csvname, index=False)\nprint(submission)","metadata":{"execution":{"iopub.status.busy":"2022-01-13T15:58:15.344546Z","iopub.execute_input":"2022-01-13T15:58:15.345125Z","iopub.status.idle":"2022-01-13T15:58:15.552422Z","shell.execute_reply.started":"2022-01-13T15:58:15.345082Z","shell.execute_reply":"2022-01-13T15:58:15.551311Z"},"trusted":true},"execution_count":null,"outputs":[]}]}