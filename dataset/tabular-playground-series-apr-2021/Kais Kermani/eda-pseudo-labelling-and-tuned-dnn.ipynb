{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#13:56\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ntrain_data = pd.read_csv('/kaggle/input/tabular-playground-series-apr-2021/train.csv')\ntest_data = pd.read_csv('/kaggle/input/tabular-playground-series-apr-2021/test.csv')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Setting up graphics and color palette\nfrom pylab import rcParams\nrcParams['figure.figsize'] = 9, 7\n\nsns.set_context('notebook')\nsns.set_style('whitegrid')\npal = sns.color_palette('Set2')\nsns.set_palette(pal)\n\nimport warnings  \nwarnings.filterwarnings('ignore')","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EDA:","metadata":{}},{"cell_type":"code","source":"print(train_data.info())\nprint(test_data.info())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = train_data.drop(columns=['PassengerId', 'Survived', 'Name'])\ny_train = train_data['Survived']\nX_test = test_data.drop(columns=['PassengerId', 'Name'])\nplot_data = X_train.join(y_train)\nnum_cols = ['Pclass', 'Age', 'SibSp', 'Parch', 'Fare']\ncat_cols = ['Sex', 'Embarked']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train[num_cols].describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.histplot(data=plot_data, x='Age', hue='Survived' , kde=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(data=plot_data, x='Embarked', hue='Survived')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(data=plot_data, x='Sex', hue='Survived')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.kdeplot(data=plot_data, x='Parch', hue='Survived', multiple='stack', bw_method=.3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.kdeplot(data=plot_data, x='SibSp', hue='Survived', multiple='stack', bw_method=.3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.stripplot(data=plot_data, x='Pclass', y='Fare', hue='Survived', dodge=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Ticket and Cabin","metadata":{}},{"cell_type":"code","source":"cabins = list(X_train['Cabin'].unique())\ncabins = [str(x) for x in cabins]\ncabins.remove('nan')\ncabins_cat = pd.Series([s[0] for s in cabins])\ncabins_num = pd.Series([int(s[1:]) for s in cabins])\nprint(cabins_cat.unique())\nsns.histplot(data = cabins_num)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tickets = list(train_data['Ticket'].unique())\ntickets.remove(np.nan)\ntickets_formatted = [('xxx', x) if not x.lower().islower() else (x.split(' ')[0].lower().replace('.', ''), x.split(' ')[-1]) for x in tickets]\ntickets_cat, tickets_num = list(zip(*tickets_formatted))\ntickets_temp = np.array([np.array(x.split('/')) for x in tickets_cat])\ntickets_cat_list = []\nfor x in tickets_temp:\n    for y in x:\n        tickets_cat_list.append(y)\ntickets_cat_list = pd.Series(tickets_cat_list)\ntickets_cat = pd.Series(tickets_cat)\ntickets_num = pd.Series(tickets_num)\nprint(tickets_cat_list.unique())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(pd.Series(train_data['Ticket'].unique()).count())\ntickets_num = pd.Series([int(x) for x in tickets_num if x != ''])\nsns.histplot(data=tickets_num)\ntickets_num","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preproc:","metadata":{}},{"cell_type":"markdown","source":"## Cabin and Ticket","metadata":{}},{"cell_type":"code","source":"def split_cabin(df):\n    data = df.copy()\n    data['Cabin'] = data['Cabin'].fillna(value='Z0')\n    data['Cabin_cat'] = list(map(lambda s: s[0], data['Cabin']))\n    data['Cabin_num'] = list(map(lambda s: int(s[1:]), data['Cabin']))\n    return data.drop(columns=['Cabin'])\n\nX_train = split_cabin(X_train)\nX_test = split_cabin(X_test)\nX_train","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def split_ticket(df):\n    data = df.copy()\n    data['Ticket'] = data['Ticket'].fillna(value='xxx 0')\n    tickets_formatted = [('xxx', x) if not x.lower().islower() else (x.split(' ')[0].lower().replace('.', ''), x.split(' ')[-1]) for x in data['Ticket']]\n    tickets_cat, tickets_num = list(zip(*tickets_formatted))\n    tickets_cat = pd.Series(tickets_cat)\n    tickets_num = pd.Series(tickets_num).replace('', '0').astype(int)\n    data['Ticket_cat'] = pd.Series(tickets_cat)\n    data['Ticket_num'] = pd.Series(tickets_num)\n    return data.drop(columns=['Ticket'])\n\nX_train = split_ticket(X_train)\nX_test = split_ticket(X_test)\nX_train","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Encoding\n","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import OrdinalEncoder\nencoder = OrdinalEncoder()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def enc_cat(df, enc, train=True):\n    data = df.copy()\n    data['Embarked'].fillna(value='Z', inplace=True)\n    if train:\n        data[['Sex', 'Embarked', 'Cabin_cat', 'Ticket_cat']] = enc.fit_transform(data[['Sex', 'Embarked', 'Cabin_cat', 'Ticket_cat']])\n    else:\n        data[['Sex', 'Embarked', 'Cabin_cat', 'Ticket_cat']] = enc.transform(data[['Sex', 'Embarked', 'Cabin_cat', 'Ticket_cat']])\n    data.loc[data['Embarked']==3, ['Embarked']] = np.nan\n    return data\n    \nX_train = enc_cat(X_train, encoder)\nX_test = enc_cat(X_test, encoder, train=False)\nX_train","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\ncolumns = X_train.columns\n\nX_train = pd.DataFrame(scaler.fit_transform(X_train), columns=columns)\nX_test = pd.DataFrame(scaler.transform(X_test), columns=columns)\nX_train.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.neighbors import KNeighborsRegressor\nknn_reg = KNeighborsRegressor(n_neighbors=50, weights='distance')\n\nfor col in ['Fare', 'Embarked', 'Age']:\n    train_na = X_train[X_train[col].isna()].drop(columns=[col]).dropna()\n    knn_reg.fit(X_train.dropna().drop(columns = [col]), X_train.dropna()[col])\n    X_train.loc[train_na.index, col] = knn_reg.predict(train_na)\n    test_na = X_test[X_test[col].isna()].drop(columns=[col]).dropna()\n    X_test.loc[test_na.index, col] = knn_reg.predict(test_na)\n    print('Filled', col, 'missing values.')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.fillna(value=0, inplace=True)\nX_test.fillna(value=0, inplace=True)\nX_test.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Pseudo Labeling with QDA","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import classification_report\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import f1_score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis as QDA\n\nqda_model = QDA(reg_param=.35)\nqda_model.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"qda_pred = qda_model.predict_proba(X_test)[:, -2]\nthresh = .06\npseudo_labeled_X = np.argwhere(np.logical_or(qda_pred>1-thresh, qda_pred<thresh)).ravel()\npseudo_labeled_X = X_test.loc[pseudo_labeled_X, :]\npseudo_labeled_y = pd.Series(qda_model.predict(pseudo_labeled_X))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = pd.concat([X_train, pseudo_labeled_X], ignore_index=True)\ny_train = pd.concat([y_train, pseudo_labeled_y], ignore_index=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nprint(\"# of samples: \" + str(y_train.shape[0]))\n\n# Splitting data into train (85%) CV (15%)\nX, y = (X_train, y_train)\nX_train, X_dev, y_train, y_dev = train_test_split(X, y, test_size = .1, stratify = y_train, random_state = 69)\ny_train = np.array(y_train).astype(np.float32).reshape((-1,1))\ny_dev = np.array(y_dev).astype(np.float32).reshape((-1,1))\n\nprint(\"X_train shape: \" + str(X_train.shape) + \"\\t y_train shape:\" + str(y_train.shape))\nprint(\"X_dev shape:  \" + str(X_dev.shape) + \"\\t y_dev shape: \" + str(y_dev.shape))\n\nprint(sum(y_train==1))\nprint(sum(y_dev==1))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Machine Learning:","metadata":{}},{"cell_type":"markdown","source":"## 1st Layer:","metadata":{}},{"cell_type":"markdown","source":"### Random Forest ","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Grid-Search Tuning Hyper-Params\nrf_params= [{\n    'min_samples_split': [70, 90, 100, 120]\n}]\n\nrf_model = GridSearchCV(\n    RandomForestClassifier(), rf_params, scoring='accuracy', verbose=3\n)\nrf_model.fit(X, np.ravel(y))","metadata":{"_kg_hide-output":true,"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Best score achieved:', rf_model.best_score_)\nprint('With params:\\n', rf_model.best_params_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# rf_model = RandomForestClassifier(n_estimators=100, criterion='entropy', min_samples_leaf=20)\n# rf_model.fit(X_train, np.ravel(y_train))\n\n# y_pred = rf_model.predict_proba(X_train)\n# print('Score on the training set:')\n# print(classification_report(y_train, np.around(y_pred[:, 1])))\n# print('roc_auc score: ', end='')\n# print(roc_auc_score(y_train, y_pred[:, 1]))\n# print('f1 score:', f1_score(y_train,np.around(y_pred[:, 1])), end='\\n\\n')\n\n# y_pred = rf_model.predict_proba(X_dev)\n# print('Score on the dev set:')\n# print(classification_report(y_dev, np.around(y_pred[:, 1])))\n# print('roc_auc score: ', end='')\n# print(roc_auc_score(y_dev, y_pred[:, 1]))\n# print('f1 score:', f1_score(y_dev,np.around(y_pred[:, 1])), end='\\n\\n')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### XGBoost ","metadata":{}},{"cell_type":"code","source":"import xgboost as xgb\n\nxgb_params= [{\n    'max_depth': [2, 3],\n    'min_child_weight': [60, 70, 80],\n    'lambda': [2.2, 2.5, 2.8]\n}]\n\nxgb_model = GridSearchCV(\n    xgb.XGBClassifier(objective=\"binary:logistic\", eval_metric='rmse', use_label_encoder = False, random_state=42),\n    xgb_params, scoring='accuracy', verbose=3\n)\nxgb_model.fit(X, np.ravel(y))","metadata":{"_kg_hide-output":true,"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Best score achieved:', xgb_model.best_score_)\nprint('With params:\\n', xgb_model.best_params_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# y_pred = xgb_model.predict_proba(X_train)\n# print('Score on the training set:')\n# print(classification_report(y_train, np.around(y_pred[:, 1])))\n# print('roc_auc score: ', end='')\n# print(roc_auc_score(y_train, y_pred[:, 1]))\n# print('f1 score:', f1_score(y_train,np.around(y_pred[:, 1])), end='\\n\\n')\n\n# y_pred = xgb_model.predict_proba(X_dev)\n# print('Score on the dev set:')\n# print(classification_report(y_dev, np.around(y_pred[:, 1])))\n# print('roc_auc score: ', end='')\n# print(roc_auc_score(y_dev, y_pred[:, 1]))\n# print('f1 score:', f1_score(y_dev,np.around(y_pred[:, 1])), end='\\n\\n')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### ANN:","metadata":{}},{"cell_type":"code","source":"def dfify(hist):\n\tdf = pd.DataFrame(hist.history)\n\tdf['epoch'] = df.index\n\tval_cols = [x for x in df.columns if x.startswith('val')]\n\tdf_val = df[val_cols+['epoch']]\n\tdf.drop(columns=val_cols, inplace=True)\n\tdf_val.rename(columns={col: col.split('val_')[-1] for col in df_val.columns}, inplace=True)\n\tdf['phase'] = 'train'\n\tdf_val['phase'] = 'val'\n\treturn pd.concat([df, df_val], ignore_index=True)\n\ndef visu_history(hist):\n    rcParams['figure.figsize'] = 20, 7\n    hist_df = dfify(hist)\n    fig, axes = plt.subplots(1, 2)\n    grid = sns.lineplot(data = hist_df, x='epoch', y='loss', hue='phase', ax=axes[0])\n    grid.set(yscale='log')\n    sns.lineplot(data = hist_df, x='epoch', y='accuracy', hue='phase', ax=axes[1])\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nimport tensorflow_addons as tfa\nfrom sklearn.metrics import classification_report\n\ntf.keras.backend.clear_session()\n\nregu = lambda y : tf.keras.regularizers.L2(l2=y)\n\ndef make_model(optimizer, loss_fn, metrics, output_bias='zeros', dropout=0, l2regu=0):\n    model = tf.keras.Sequential([\n        tf.keras.layers.Flatten(input_shape=(X_train.shape[-1],)),\n        tf.keras.layers.Dense(100, activation='relu', kernel_initializer=tf.keras.initializers.HeNormal, kernel_regularizer=regu(l2regu)),\n        tf.keras.layers.Dropout(dropout),\n        tf.keras.layers.Dense(50, activation='relu', kernel_initializer=tf.keras.initializers.HeNormal, kernel_regularizer=regu(l2regu)),\n        tf.keras.layers.Dropout(dropout),\n        tf.keras.layers.Dense(30, activation='relu', kernel_initializer=tf.keras.initializers.HeNormal, kernel_regularizer=regu(l2regu)),\n        tf.keras.layers.Dropout(dropout),\n        tf.keras.layers.Dense(10, activation='relu', kernel_initializer=tf.keras.initializers.HeNormal, kernel_regularizer=regu(l2regu)),\n        tf.keras.layers.Dropout(dropout),\n        tf.keras.layers.Dense(3, activation='tanh', kernel_initializer=tf.keras.initializers.GlorotNormal, kernel_regularizer=regu(l2regu)),\n        tf.keras.layers.Dropout(dropout),\n        tf.keras.layers.Dense(1, activation='sigmoid', kernel_initializer=tf.initializers.GlorotNormal, bias_initializer=output_bias)\n    ])\n    \n    model.compile(\n        optimizer=optimizer,\n        loss=loss_fn,\n        metrics=metrics\n    )\n    \n    return model\n\n\nloss_fn = tf.losses.BinaryCrossentropy()\n\noptimizer = tf.keras.optimizers.SGD(learning_rate=1e-3, momentum=1)\n\nann_model = make_model(optimizer, loss_fn, ['accuracy'])\nann_model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Overfitting a random 20 rows:","metadata":{}},{"cell_type":"code","source":"history = ann_model.fit(X_train[100:120], y_train[100:120], epochs=300, batch_size=1024, verbose=2)","metadata":{"scrolled":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"visu_history(history)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Overfitting:","metadata":{}},{"cell_type":"code","source":"ann_model = make_model(tf.keras.optimizers.Adam(learning_rate=.004), loss_fn, ['accuracy'])\nhistory = ann_model.fit(X_train, y_train, epochs=1000, batch_size=2048, verbose=2, validation_data=(X_dev, y_dev))","metadata":{"scrolled":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"visu_history(history)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Regularization:","metadata":{}},{"cell_type":"code","source":"callback = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=200, mode='max', restore_best_weights=True, verbose=2)\nann_model = make_model(tfa.optimizers.AdamW(learning_rate=.004, weight_decay=7e-5), loss_fn, ['accuracy'], dropout=0.05, l2regu=2e-5)\nhistory = ann_model.fit(\n    X_train, y_train, epochs=500, batch_size=2048, \n    callbacks=[callback], validation_data=(X_dev, y_dev), verbose=2\n)","metadata":{"_kg_hide-output":true,"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"visu_history(history)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Hyper parameter Tuning","metadata":{}},{"cell_type":"code","source":"iterations = 50\n# Parameters:\nl_rate_range = 10**np.random.uniform(-4.5, -1.5, iterations)\nw_decay_range = 10**np.random.uniform(-2.5, -5.5, iterations)\ndropout_range = np.random.uniform(.01, .1, iterations)\nlambd_regu_range = 10**np.random.uniform(-3, -7, iterations)\n\n#combos = [(0.00018754084977224016, 2.4085576475004506e-05, 0.04475658897898782, 0.0002886421264356717)]\ncombos = list(zip(l_rate_range, w_decay_range, dropout_range, lambd_regu_range))\nbest_accuracy = 0\n\ncombo_scores = pd.DataFrame(columns=['l_rate', 'w_decay', 'droupout', 'lambd_regu', 'score'])\n\n# Same initial weights for consistency:\nann_model = make_model(tfa.optimizers.AdamW(learning_rate=2e-4, weight_decay=2e-5), loss_fn, ['accuracy'], dropout=.045, l2regu=2e-5)\nann_model.save_weights('initial_weights')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"i=0\nfor l_rate, w_decay, dropout, lambd_regu in combos:\n    i=i+1\n    print('********* iteration', i, '/', iterations,'*********')\n    print('L_rate:', l_rate, '\\tw_decay:', w_decay, '\\tdropout:', dropout, '\\tlambd_regu:', lambd_regu)\n    callback = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=500, mode='max', restore_best_weights=True, verbose=2)\n    ann_model = make_model(tfa.optimizers.AdamW(learning_rate=l_rate, weight_decay=w_decay), loss_fn, ['accuracy'], dropout=dropout, l2regu=lambd_regu)\n    ann_model.load_weights('initial_weights')\n    history = ann_model.fit(\n        X_train, y_train, epochs=1500, batch_size=2048, \n        callbacks=[callback], validation_data=(X_dev, y_dev), verbose=0\n    )\n    \n    val_acc = ann_model.evaluate(X_dev, y_dev, batch_size=2048)[1]\n    combo_scores.loc[i-1] = (*combos[i-1], val_acc)\n    \n    print('score:', val_acc)\n    if val_acc > best_accuracy:\n        best_accuracy = val_acc\n        best_history = history\n        ann_model.save_weights('best_weights')\n        print('BEST SCORE YET!!!')\n        \nann_model.load_weights('best_weights')","metadata":{"scrolled":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ann_model.load_weights('best_weights')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"combo_scores.loc[combo_scores['score'].argmax()]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"visu_history(best_history)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rcParams['figure.figsize'] = 20, 13\nmin_score = .838\nmax_score = .84\nfig, axes = plt.subplots(2,3)\ng = sns.scatterplot(data=combo_scores, x='l_rate', y='w_decay', hue='score', hue_norm=(min_score, max_score), ax=axes[0,0])\ng.set(xscale='log', yscale='log')\ng = sns.scatterplot(data=combo_scores, x='l_rate', y='droupout', hue='score', hue_norm=(min_score, max_score), ax=axes[0,1])\ng.set(xscale='log')\ng = sns.scatterplot(data=combo_scores, x='l_rate', y='lambd_regu', hue='score', hue_norm=(min_score, max_score), ax=axes[0,2])\ng.set(xscale='log', yscale='log')\ng = sns.scatterplot(data=combo_scores, x='droupout', y='w_decay', hue='score', hue_norm=(min_score, max_score), ax=axes[1,0])\ng.set(yscale='log')\ng = sns.scatterplot(data=combo_scores, x='lambd_regu', y='w_decay', hue='score', hue_norm=(min_score, max_score), ax=axes[1,1])\ng.set(xscale='log', yscale='log')\ng = sns.scatterplot(data=combo_scores, x='droupout', y='lambd_regu', hue='score', hue_norm=(min_score, max_score), ax=axes[1,2])\ng.set(yscale='log')\n\nfor ax in np.ravel(axes):\n    ax.get_legend().remove()\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = ann_model.predict(X_train)\nprint('Score on the training set:')\nprint(classification_report(y_train, np.around(y_pred)))\nprint('roc_auc score: ', end='')\nprint(roc_auc_score(y_train, y_pred))\nprint('f1 score:', f1_score(y_train,np.around(y_pred)), end='\\n\\n')\n\ny_pred = ann_model.predict(X_dev)\nprint('Score on the dev set:')\nprint(classification_report(y_dev, np.around(y_pred)))\nprint('roc_auc score: ', end='')\nprint(roc_auc_score(y_dev, y_pred))\nprint('f1 score:', f1_score(y_dev,np.around(y_pred)), end='\\n\\n')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Ensemble Model (Layer 2)","metadata":{}},{"cell_type":"code","source":"class ensemble:\n    def __init__(self, prev_layer, esbl_model):\n        self.esbl_model = esbl_model\n        self.prev_layer = prev_layer\n        self.prev_layer_pred_train = np.array([])\n        self.prev_layer_pred = np.array([])\n        \n    def fit(self, X_loc, y_loc):\n        self.prev_layer_pred_train = np.zeros(shape=(len(X_loc), len(self.prev_layer)))\n        for i in range(len(self.prev_layer)):\n            self.prev_layer_pred_train[:, i] = self.prev_layer[i].predict_proba(X_loc)[:, -1]\n        \n        self.esbl_model.fit(self.prev_layer_pred_train, np.ravel(y_loc))\n        \n    def predict_prev(self, X_loc):\n        self.prev_layer_pred = np.zeros(shape=(len(X_loc), len(self.prev_layer)))\n        for i in range(len(self.prev_layer)):\n            self.prev_layer_pred[:, i] = self.prev_layer[i].predict_proba(X_loc)[:, -1]\n        return pd.DataFrame(self.prev_layer_pred)\n    \n    def predict_proba(self, X_loc):\n        self.predict_prev(X_loc)\n        return self.esbl_model.predict_proba(self.prev_layer_pred)\n    \n    def predict(self, X_loc):\n        self.predict_prev(X_loc)\n        return self.esbl_model.predict(self.prev_layer_pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgb_esbl = xgb.XGBClassifier(objective=\"binary:logistic\", eval_metric='rmse', use_label_encoder = False, random_state=42)\nrf_esbl = RandomForestClassifier(min_samples_split=100)\nesbl_model = ensemble([rf_model, xgb_model, ann_model], xgb_esbl)\nesbl_model.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = esbl_model.predict(X_train)\nprint('Score on the training set:')\nprint(classification_report(y_train, np.around(y_pred)))\nprint('roc_auc score: ', end='')\nprint(roc_auc_score(y_train, y_pred))\nprint('f1 score:', f1_score(y_train,np.around(y_pred)), end='\\n\\n')\n\ny_pred = esbl_model.predict(X_dev)\nprint('Score on the dev set:')\nprint(classification_report(y_dev, np.around(y_pred)))\nprint('roc_auc score: ', end='')\nprint(roc_auc_score(y_dev, y_pred))\nprint('f1 score:', f1_score(y_dev,np.around(y_pred)), end='\\n\\n')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission:","metadata":{}},{"cell_type":"code","source":"first_layer_pred = esbl_model.predict_prev(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Choose the model:\nmodel = ann_model\ndecision = np.around(model.predict(X_test).ravel()).astype(int)\n# decision = np.around(first_layer_pred.mean(axis=1)).astype(int)\n\nsubmission = pd.DataFrame({'PassengerId': test_data['PassengerId'], 'Survived': decision})\n\nfrom IPython.display import HTML\nimport base64\ndef create_download_link(df, title = \"Download CSV file\", filename = \"submission.csv\"):  \n    csv = df.to_csv(index=False)\n    b64 = base64.b64encode(csv.encode())\n    payload = b64.decode()\n    html = '<a download=\"{filename}\" href=\"data:text/csv;base64,{payload}\" target=\"_blank\">{title}</a>'\n    html = html.format(payload=payload,title=title,filename=filename)\n    return HTML(html)\n\ncreate_download_link(submission)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}