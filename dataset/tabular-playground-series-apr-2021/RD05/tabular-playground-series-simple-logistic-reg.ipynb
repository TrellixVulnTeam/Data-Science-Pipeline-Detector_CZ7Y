{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Tabular Playground Series - April 2021\n\nThe aim of this notebook is to present a really simple data handling, data preparation & logistic regression. \nThis notebook will not contain an EDA and you can find plenty of really nice EDA example already for this competition. \n","metadata":{}},{"cell_type":"markdown","source":"## Library Importation\nSeveral libraries will be used during this project, let's import all of them. ","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LogisticRegression\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Function Definition\nTo help me looking & understand the data I am used to used below functions: \n* **var_meaning** : It will give the definition of a column name whenever I want based on a dictionnary I'll build at the beginning of the project\n* **missing_value** : It will help me identifying if some variables contain or not missing value. ","metadata":{}},{"cell_type":"code","source":"def var_meaning(dict, var):\n    if var in dict:\n        print(dict[var])\n    else:\n        print('Variable not found')\n\n\ndef missing_value(df, column_names):\n    for col in column_names:\n        try:\n            if sum(df[col].isnull()) >= 1:\n                print(col, \" does contain null value. The number of null values equal \", sum(df[col].isnull()))\n                print('')\n        except KeyError:\n            print(col, 'not present')\n            print('')\n    print('The total number of null values = ', df.isnull().sum().sum())\n    print('------------------------------------')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Preparation\n### 1. Reading the data\n\nLet's first read the csv files we have ","metadata":{}},{"cell_type":"code","source":"# Read the training data\ntraining = pd.read_csv('/kaggle/input/tabular-playground-series-apr-2021/train.csv')\ntest = pd.read_csv('/kaggle/input/tabular-playground-series-apr-2021/test.csv')\nsubmission = pd.read_csv('/kaggle/input/tabular-playground-series-apr-2021/sample_submission.csv')\npd.set_option('display.max_columns', None)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As explained above, I am creating a dictionnary of the variable defition. ","metadata":{}},{"cell_type":"code","source":"# Variable definition\nvariable_definition = {'PassengerId': 'Unique Passenger Identifier',\n                       'Survived': 'Binary Variable | 0 = No, 1 = Yes',\n                       'Pclass': 'Ticket Class',\n                       'Name': 'Name of the passenger',\n                       'Sex': 'Sex of the passenger',\n                       'Age': 'Age of the passenger',\n                       'SibSp': 'Number of siblings / spouses aboard the boat',\n                       'Parch': '# of parents / children aboard the boat',\n                       'Ticket': 'Ticket Number',\n                       'Fare': 'Passenger fare',\n                       'Cabin': 'Cabin number',\n                       'Embarked': 'Port of Embarkation | C = Cherbourg, Q = Queenstown, S = Southampton'\n                       }","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"By using the above function I can now have the definition of a column whenever. \nExample :","metadata":{}},{"cell_type":"code","source":"var_meaning(variable_definition, 'Pclass')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2. Do we have missing value to handle ? ","metadata":{}},{"cell_type":"code","source":"missing_value(training, list(variable_definition.keys()))\nmissing_value(test, list(variable_definition.keys()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From above, one can see that several variables contains null values. Let's keep them in mind to make sure we are handling those use case before modelling our Logistic Regression ! The variables having missing values are the same in both our training and testing sets. \n\n* Age\n* Ticket\n* Fare\n* Cabin\n* Embarked","metadata":{}},{"cell_type":"markdown","source":"### 3. Analysis variable per variable\n#### 3.1. Pclass\n\nLet's have a look to the first variable. Again, this notebook will not contain any EDA to not make it super long. However I would definitely recommand anyone to start by an exhaustive EDA. I have myself done this step and some decisions are taken in this notebook based on my previous EDA. \n\n* **Missing Value** : No missing value for both train and test set. \n* **Outlier** : No outlier spotted\n* Only three unique values | Class 1, Class 2, Class 3\n\nThe only thing I'll do for this variable is to transform into dummy variables. Indeed, here, even though the variable is an integer, the number itself doesn't mean anything and should be analysis as a categorical variables.\n","metadata":{}},{"cell_type":"code","source":"training['Pclass'].unique()  # Only three unique value.\ntraining = training.join(pd.get_dummies(training['Pclass'], prefix='Pclass', drop_first=True))\n\ntest['Pclass'].unique()  # Only three unique value.\ntest = test.join(pd.get_dummies(test['Pclass'], prefix='Pclass', drop_first=True))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 3.2. Name\n* **Missing value** : No missing value for both train and test sets\n\nI haven't done anything with the variable name. I didn't identified any class (Dr, Lady, etc.). This variable will simply bet dropped at the end of the data preparation step. ","metadata":{}},{"cell_type":"markdown","source":"#### 3.3. Sex\n\n* **Missing Value** : No missing value for both train and test set. \n* Only two unique values | Male , Female\n\nThe only thing I'll do for this variable is to transform into dummy variables","metadata":{}},{"cell_type":"code","source":"training['Sex'].unique()  # Only 2 unique value.\ntest['Sex'].unique()  # Only 2 unique value.\n\ntraining = training.join(pd.get_dummies(training['Sex'], prefix='Sex', drop_first=True))\ntest = test.join(pd.get_dummies(test['Sex'], prefix='Sex', drop_first=True))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 3.4. Age\n\n* **Missing Value** : We do have missing values for Age in both Train & Test sets. \n* **Outlier** :  Not outlier spotted\n\nBased on my EDA, I have noticed that the Age of the observations was varying based on its Pclass. I'll therefore replace the missing value by the median age of the observation in the same Pclass. \n\nNotice that to do this observation, I've concatenated both Test & Training set to have a more representative datasets of the entire population onboard. ","metadata":{}},{"cell_type":"code","source":"df = pd.concat([training, test])\nsns.displot(df, x='Age', kind='kde', hue='Pclass', fill='True')\nplt.show()\n\nmap_age_pclass = df[['Age', 'Pclass']].dropna().groupby('Pclass').median().to_dict()\n\ntraining['Age'] = training['Age'].mask(training['Age'].isna(), training['Pclass'].map(map_age_pclass['Age']))\ntest['Age'] = test['Age'].mask(test['Age'].isna(), test['Pclass'].map(map_age_pclass['Age']))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 3.5. SibSp & Parch\n\n* **Missing Value** : No missing values \n* **Outlier** :  Not outlier spotted\n\nI've decided to join those two together as they are both like to the family size. Two new features will be created: \n* **Size_Family** = SibSp + Parch\n* **is_alone** = IF Size_Family = 0 THEN 1 ELSE 0","metadata":{}},{"cell_type":"code","source":"training['Size_Family'] = training['SibSp'] + training['Parch']\ntest['Size_Family'] = test['SibSp'] + test['Parch']\n\ntraining['is_alone'] = np.where(training['Size_Family'] == 0, 1, 0)\ntest['is_alone'] = np.where(test['Size_Family'] == 0, 1, 0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 3.6. Ticket\n\nNothing has been done with the Ticket variable. This variable will be dropped before starting the modelling phase. ","metadata":{}},{"cell_type":"markdown","source":"#### 3.7. Fare\n\n* **Missing Value** : We have some missing value in both train & test sets. \n* **Outlier** :  Not outlier spotted\n\nSimilarly to Age, based on the EDA, I have notice that the variable Fare was varying based on the Pclass. I'll also replace the missing value by the median fare paid by the observation in the same Pclass. ","metadata":{}},{"cell_type":"code","source":"map_fare_pclass = df[['Fare', 'Pclass']].dropna().groupby('Pclass').median().to_dict()\n\ntraining['Fare'] = training['Fare'].mask(training['Fare'].isna(), training['Pclass'].map(map_fare_pclass['Fare']))\ntest['Fare'] = test['Fare'].mask(test['Fare'].isna(), test['Pclass'].map(map_fare_pclass['Fare']))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 3.8. Cabin\n\n* **Missing Value** : We have some missing value in both training & testing sets.\n* **Outlier** :  Not outlier spotted\n\nFew features will be created based on the Cabin variable. \n* **Has_Cabin** : IF cabin IS NULL THEN 0 ELSE 1\n* **Cabin_Type** : First letter if the Cabin variable. \n\nThe has_cabin variable is solving our missing values issue while the cabin type variable might indicate where in the boat the passenger was living. \nThis cabin_type will then be transformed into dummy variables. ","metadata":{}},{"cell_type":"code","source":"training['Has_cabin'] = np.where(training['Cabin'].isnull() == True, 0, 1)\ntest['Has_cabin'] = np.where(test['Cabin'].isnull() == True, 0, 1)\n\ntraining['Cabin_type'] = training['Cabin'].str[0]\ntest['Cabin_type'] = test['Cabin'].str[0]\n\ntraining = training.join(pd.get_dummies(training['Cabin_type'], prefix='Cabin_type', drop_first=True))\ntest = test.join(pd.get_dummies(test['Cabin_type'], prefix='Cabin_type', drop_first=True))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 3.9. Embarked \n\n* **Missing Value** : We have some missing value in both training & testing sets.\n* **Outlier** :  Not outlier spotted\n\nBased on my EDA, I've notice that most of the passenger embarked at Southampton. I'll assume that my missing value also embarked on Southampton. \nThen I am creating dummies variables. ","metadata":{}},{"cell_type":"code","source":"training['Embarked'] = np.where(training['Cabin'].isnull() == True, 'S', training['Embarked'])\ntest['Embarked'] = np.where(test['Cabin'].isnull() == True, 'S', test['Embarked'])\n\ntraining = training.join(pd.get_dummies(training['Embarked'], prefix='Embarked', drop_first=True))\ntest = test.join(pd.get_dummies(test['Embarked'], prefix='Embarked', drop_first=True))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 3.10. Dropping the unnecessary variables\n\nDropping from both the training & the testing set,  the columns that have either been transformed into dummy variables and the columns that I've decided not to use. ","metadata":{}},{"cell_type":"code","source":"# Dropping the unnecessary column.\ntraining = training.drop('Pclass', axis=1)\ntest = test.drop('Pclass', axis=1)\ntraining = training.drop('Name', axis=1)\ntest = test.drop('Name', axis=1)\ntraining = training.drop('Sex', axis=1)\ntest = test.drop('Sex', axis=1)\ntraining = training.drop('Ticket', axis=1)\ntest = test.drop('Ticket', axis=1)\ntraining = training.drop('Cabin', axis=1)\ntest = test.drop('Cabin', axis=1)\ntraining = training.drop('Embarked', axis=1)\ntest = test.drop('Embarked', axis=1)\ntraining = training.drop('Cabin_type', axis=1)\ntest = test.drop('Cabin_type', axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4. Logistic Regression Modelling\n\nMy datasets are now ready. I'll be modelling a really simply logistic regression to see how it is performing. \n\nFirst thing is to split my datasets into X_train, Y_train & X_test. ","metadata":{}},{"cell_type":"code","source":"y_train = training['Survived']\nx_train = training.drop('Survived', axis=1)\nx_train = x_train.drop('PassengerId', axis=1)\nx_test = test.drop('PassengerId', axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I can now use those to do my logistic regression and saved my results in my submission csv file. ","metadata":{}},{"cell_type":"code","source":"lr = LogisticRegression(max_iter = 500)\nlr.fit(x_train, y_train)\n\ny_pred = lr.predict(x_test).astype(int)\nprint('Mean =', y_pred.mean(), ' Std =', y_pred.std())\n\nsubmission['Survived'] = y_pred\nsubmission.to_csv(\"submission.csv\", index=False)\nprint('Done')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 5. Result & next steps\n\nWith this really simple data processing & logistic regression, I had an accuracy of 0.795 which placed which led me to the top 35%. \n\nTo improve the score : \n* Additional feature creation\n* Feature Selection \n* Additional Model testing\n* Model finetuning","metadata":{}}]}