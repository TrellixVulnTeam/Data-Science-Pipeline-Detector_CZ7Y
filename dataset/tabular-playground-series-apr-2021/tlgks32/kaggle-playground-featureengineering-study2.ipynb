{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"https://www.kaggle.com/udbhavpangotra/tps-apr21-eda-model\n\n\nhttps://www.kaggle.com/hiro5299834/tps-apr-2021-voting-pseudo-labeling","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# KAGGLE 스터디 ","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport random\nimport os\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.model_selection import train_test_split, KFold, StratifiedKFold\n\nimport lightgbm as lgb\nimport catboost as ctb\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.tree import DecisionTreeClassifier, export_graphviz\n\nimport graphviz\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.simplefilter('ignore')","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TARGET = 'Survived'\n\nN_ESTIMATORS = 1000\nN_SPLITS = 10\nSEED = 2021\nEARLY_STOPPING_ROUNDS = 100\nVERBOSE = 100","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#랜덤 시드 생성\ndef set_seed(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    \nset_seed(SEED)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 데이터 전처리","metadata":{}},{"cell_type":"markdown","source":"### lode data","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv('../input/tabular-playground-series-apr-2021/train.csv')\ntest_df = pd.read_csv('../input/tabular-playground-series-apr-2021/test.csv')\nsubmission = pd.read_csv('../input/tabular-playground-series-apr-2021/sample_submission.csv')\n#test_df['Survived'] = pd.read_csv(\"../input/submission-merged3/submission_merged3.csv\")['Survived']\n\nall_df = pd.concat([train_df, test_df]).reset_index(drop=True)\n#reset_index : 인덱스를 세팅한다. drop=True를 하면 인덱스를 세팅한걸 삭제함. \n","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Rows and Columns in train dataset:', train_df.shape)\nprint('Rows and Columns in test dataset:', test_df.shape)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 결측치 갯수 출력","metadata":{}},{"cell_type":"code","source":"print('Missing values per columns in train dataset')\nfor col in train_df.columns:\n    temp_col = train_df[col].isnull().sum()\n    print(f'{col}: {temp_col}')\nprint()\nprint('Missing values per columns in test dataset')\nfor col in test_df.columns:\n    temp_col = test_df[col].isnull().sum()\n    print(f'{col}: {temp_col}')","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Filling missing values","metadata":{}},{"cell_type":"code","source":"#나이는 나이의 평균치로 채운다.\nall_df['Age'] = all_df['Age'].fillna(all_df['Age'].mean())\n\n#cabin은 문자열을 분할하고, 제일 첫번째 글자를 따와서 넣는다. 결측치엔 X를 넣는다.\n#strip() : 양쪽 공백을 지운다. 여기서느 x[0]외엔 다 지우는듯. \nall_df['Cabin'] = all_df['Cabin'].fillna('X').map(lambda x: x[0].strip())\n\n\n#print(all_df['Ticket'].head(10))\n#Ticket, fillna with 'X', split string and take first split \n#split() : 문자열 나누기. 디폴트는 ' '이고, 문자를 가진 데이터들이 전부 띄워쓰기로 구분되어있기때문에 가능. \nall_df['Ticket'] = all_df['Ticket'].fillna('X').map(lambda x:str(x).split()[0] if len(str(x).split()) > 1 else 'X')\n\n#pclass에 따른 Fare의 평균을 구해서 dictionary형태로 만든다. \nfare_map = all_df[['Fare', 'Pclass']].dropna().groupby('Pclass').median().to_dict()\n#fare의 결측치에 본인 행의 pclass 값을 넣고, 그 값을 fare 평균에 맵핑시킨다.  \nall_df['Fare'] = all_df['Fare'].fillna(all_df['Pclass'].map(fare_map['Fare']))\n#유독 높은 가격이나 낮은 가격이 있기때문에, 이상치의 영향을 줄이기 위해서 Fare에 log를 취해준다.\nall_df['Fare'] = np.log1p(all_df['Fare'])\n\n\n#항구의 결측치를 X로 채운다. \nall_df['Embarked'] = all_df['Embarked'].fillna('X')\n\n#이름은 성만 사용한다.\nall_df['Name'] = all_df['Name'].map(lambda x: x.split(',')[0])\n","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_1=all_df.loc[all_df['Pclass']==1].groupby('Ticket')['Ticket'].count().sort_values(ascending=False)\nprint(data_1)\nprint()\ndata_2=all_df.loc[all_df['Pclass']==2].groupby('Ticket')['Ticket'].count().sort_values(ascending=False)\nprint(data_2)\nprint()\ndata_3=all_df.loc[all_df['Pclass']==3].groupby('Ticket')['Ticket'].count().sort_values(ascending=False)\nprint(data_3)\nprint()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 인코딩 ","metadata":{}},{"cell_type":"markdown","source":"변수별로 인코딩을 다르게 해준다. ","metadata":{}},{"cell_type":"code","source":"label_cols = ['Name', 'Ticket', 'Sex','Pclass','Embarked']\nonehot_cols = [ 'Cabin',]\nnumerical_cols = [ 'Age', 'SibSp', 'Parch', 'Fare']","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#라벨 인코딩 함수. c라는 매개변수를 받아서 맞게 트렌스폼 해준다. \ndef label_encoder(c):\n    le = LabelEncoder()\n    return le.fit_transform(c)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n#StandardScaler(): 평균을 제거하고 데이터를 단위 분산으로 조정한다. \n#그러나 이상치가 있다면 평균과 표준편차에 영향을 미쳐 변환된 데이터의 확산은 매우 달라지게 되는 함수\nscaler = StandardScaler()\n\nonehot_encoded_df = pd.get_dummies(all_df[onehot_cols])\nlabel_encoded_df = all_df[label_cols].apply(label_encoder)\nnumerical_df = pd.DataFrame(scaler.fit_transform(all_df[numerical_cols]), columns=numerical_cols)\ntarget_df = all_df[TARGET]\n\nall_df = pd.concat([numerical_df, label_encoded_df,onehot_encoded_df, target_df], axis=1)\n#all_df = pd.concat([numerical_df, label_encoded_df, target_df], axis=1)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 모델링","metadata":{}},{"cell_type":"code","source":"drop_list=['Survived','Parch']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## not pseudo","metadata":{}},{"cell_type":"code","source":"train = all_df.iloc[:100000, :]#0개~100000개\ntest = all_df.iloc[100000:, :] #100000개~ \n#iloc은 정수형 인덱싱\ntest = test.drop('Survived', axis=1) #test에서 종속변수를 드랍한다. \nmodel_results = pd.DataFrame()\nfolds = 5","metadata":{"_kg_hide-input":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y= train.loc[:,'Survived']\nX= train.drop(drop_list,axis=1)","metadata":{"_kg_hide-input":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## pseudo","metadata":{}},{"cell_type":"code","source":"# y=all_df.loc[:,'Survived']\n# X=all_df.drop('Survived',axis=1)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_valid, y_train, y_valid = train_test_split(X,y,test_size=0.25, random_state=21)\n","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn import metrics  \nfrom sklearn.metrics import accuracy_score\nimport numpy as np","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params = {\n    'metric': 'binary_logloss',\n    'n_estimators': N_ESTIMATORS,\n    'objective': 'binary',\n    'random_state': SEED,\n    'learning_rate': 0.01,\n    'min_child_samples': 150,\n    'reg_alpha': 3e-5,\n    'reg_lambda': 9e-2,\n    'num_leaves': 20,\n    'max_depth': 16,\n    'colsample_bytree': 0.8,\n    'subsample': 0.8,\n    'subsample_freq': 2,\n    'max_bin': 240,\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lgbm_model=lgb.LGBMClassifier(**params)\nlgbm_model.fit(X_train,y_train)\nlgbm_pred=lgbm_model.predict(X_valid)\n\nlgbm_R2=metrics.accuracy_score(y_valid,lgbm_pred)\n#lgbm_rmse = np.sqrt(mean_squared_error(lgbm_pred,y_valid))\nprint('R2 : ',lgbm_R2)\n#print(\"RMSE : \", lgbm_rmse)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(X_train.columns))\nprint(X_train.columns)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def cal_adjust_r2(r2):\n    n=80000\n    k= len(X_train.columns)\n    temp=(1-r2)*(n-1)\n    temp2=n-k-1\n    ad_r2=1-(temp/temp2)\n    return ad_r2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ad_r2_lgbm=cal_adjust_r2(lgbm_R2)\nprint(ad_r2_lgbm)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"#NOT Pseudo\ntrain_kf_feature=train.drop(drop_list,axis=1)\ntrain_kf_label=train.loc[:,'Survived']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Pseudo\n# train_kf_feature=all_df.drop(drop_list,axis=1)\n# train_kf_label=all_df.loc[:,'Survived']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_iter=0\nkfold=StratifiedKFold(n_splits=5)\ncv_accuracy=[]\nfeature_importances = pd.DataFrame()\n\nfor train_idx, test_idx in kfold.split(train_kf_feature,train_kf_label):\n\n    X_train=train_kf_feature.iloc[train_idx]\n    X_test=train_kf_feature.iloc[test_idx]\n    y_train,y_test=train_kf_label.iloc[train_idx],train_kf_label.iloc[test_idx]\n    #학습 진행\n    lgbm_model.fit(X_train,y_train)\n    #예측\n    fold_pred=lgbm_model.predict(X_test)\n    \n    #정확도\n    n_iter+=1\n    fold_accuracy=metrics.accuracy_score(y_test,fold_pred)\n    print(\"\\n {}번째  교차 검증 정확도 : {} , 학습 데이터 크기:{}, 검증 데이터 크기 :{} \".\n          format(n_iter,fold_accuracy,X_train.shape[0],X_test.shape[0]))\n    cv_accuracy.append(fold_accuracy)\n    \n    #중요도 \n    fi_tmp = pd.DataFrame()\n    fi_tmp[\"feature\"] = lgbm_model.feature_name_\n    fi_tmp[\"importance\"] = lgbm_model.feature_importances_\n    feature_importances = feature_importances.append(fi_tmp)\n\nprint('\\n 평균 검증 정확도 : ',np.mean(cv_accuracy))\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"order = list(feature_importances.groupby(\"feature\").\n             mean().sort_values(\"importance\", ascending=False).index)\nplt.figure(figsize=(10, 10))\nsns.barplot(x=\"importance\", y=\"feature\", data=feature_importances, order=order)\nplt.title(\"{} importance\".format(\"LGBMRegressor\"))\nplt.tight_layout()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### CATBoost\n","metadata":{}},{"cell_type":"code","source":"params_cat = {\n    'bootstrap_type': 'Poisson',\n    'loss_function': 'Logloss',\n    'eval_metric': 'Logloss',\n    'random_seed': SEED,\n    'task_type': 'GPU',\n    'max_depth': 8,\n    'learning_rate': 0.01,\n    'n_estimators': N_ESTIMATORS,\n    'max_bin': 280,\n    'min_data_in_leaf': 64,\n    'l2_leaf_reg': 0.01,\n    'subsample': 0.8\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#새로운 트레인 valid 셋\nX_train, X_valid, y_train, y_valid = train_test_split(X,y,test_size=0.25, random_state=21)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ncat_model=ctb.CatBoostClassifier(**params_cat)\ncat_model.fit(X_train, y_train,verbose=300)\ncat_pred=cat_model.predict(X_valid)\nprint(\"\\n정확도: \", metrics.accuracy_score(y_valid, cat_pred))\ncat_R2=metrics.accuracy_score(y_valid,cat_pred)\n#lgbm_rmse = np.sqrt(mean_squared_error(lgbm_pred,y_valid))\nprint('R2 : ',cat_R2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cv_accuracy=[]\nfeature_importances = pd.DataFrame()\n\nfor train_idx, test_idx in kfold.split(train_kf_feature,train_kf_label):\n\n    X_train=train_kf_feature.iloc[train_idx]\n    X_test=train_kf_feature.iloc[test_idx]\n    y_train,y_test=train_kf_label.iloc[train_idx],train_kf_label.iloc[test_idx]\n    #학습 진행\n    cat_model.fit(X_train,y_train,verbose=500)\n    #예측\n    fold_pred=cat_model.predict(X_test)\n    \n    #정확도\n    n_iter+=1\n    fold_accuracy=metrics.accuracy_score(y_test,fold_pred)\n    print(\"\\n {}번째  교차 검증 정확도 : {} , 학습 데이터 크기:{}, 검증 데이터 크기 :{} \".\n          format(n_iter,fold_accuracy,X_train.shape[0],X_test.shape[0]))\n    cv_accuracy.append(fold_accuracy)\n    \n    #중요도 . lgbm이랑 명령어가 다르다.\n    fi_tmp = pd.DataFrame()\n    fi_tmp[\"feature\"] = X_test.columns.to_list()\n    fi_tmp[\"importance\"] = cat_model.get_feature_importance()\n    feature_importances = feature_importances.append(fi_tmp)\n\nprint('\\n 평균 검증 정확도 : ',np.mean(cv_accuracy))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# just to get ideas to improve\norder = list(feature_importances.groupby(\"feature\").mean().sort_values(\"importance\", ascending=False).index)\nplt.figure(figsize=(10, 10))\nsns.barplot(x=\"importance\", y=\"feature\", data=feature_importances, order=order)\nplt.title(\"{} importance\".format(\"CatBoostClassifier\"))\nplt.tight_layout()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Submission\n","metadata":{}},{"cell_type":"code","source":"def create_submission(model, test, test_passenger_id, model_name):\n    y_pred_test = model.predict_proba(test)[:, 1]\n    submission = pd.DataFrame(\n        {\n            'PassengerId': test_passenger_id, \n            'Survived': (y_pred_test >= 0.5).astype(int),\n        }\n    )\n    submission.to_csv(f\"submission_{model_name}.csv\", index=False)\n    \n    return y_pred_test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#X_test=test.drop('Pclass',axis=1)\ntest = all_df.iloc[100000:, :] #100000개~ \nX_test=test.drop(drop_list,axis=1)\nX_test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_pred_lightgbm = create_submission(\n    lgbm_model, X_test, test_df[\"PassengerId\"], \"lightgbm\"\n)\ntest_pred_catboost = create_submission(\n    cat_model, X_test, test_df[\"PassengerId\"], \"catboost\"\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_pred_merged = (\n\n    test_pred_lightgbm + \n    test_pred_catboost \n)\ntest_pred_merged = np.round(test_pred_merged / 2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.DataFrame(\n    {\n        'PassengerId': test_df[\"PassengerId\"], \n        'Survived': test_pred_merged.astype(int),\n    }\n)\nsubmission.to_csv(f\"submission_merged3.csv\", index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}