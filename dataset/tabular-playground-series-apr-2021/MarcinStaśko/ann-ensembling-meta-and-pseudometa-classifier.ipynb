{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport itertools\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.model_selection import train_test_split\nimport warnings\nwarnings.simplefilter('ignore')\n\nfrom sklearn import model_selection\nfrom sklearn import metrics\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\nfrom mlxtend.classifier import StackingCVClassifier","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## According to the ensemble inspiration from [Remek Kinas](https://www.kaggle.com/remekkinas) I have explored the Ensembing but with the ANN instad of Logistic Regression. If you like the code or have some ideas to improve please upvote and comment :) ","metadata":{}},{"cell_type":"markdown","source":"# 1. Description","metadata":{}},{"cell_type":"markdown","source":"The ensemble technique works best when the base models are not correlated.\nWe have 3 basics concept of ensembling techniques <br> \n***Max Voting***<br>\nThe prediction from each model is a vote. In max voting the final prediction come from the most votes\n- classifier 1 – class A\n- classifier 2 – class B\n- classifier 3 – class B\n- Output:     **Class B**\n\n***Averaging***<br>\nThe final output is an average of all predictons (regression problems)\n- regressor 1 – 200\n- regressor 2 – 300 \n- regressor 3 – 400\n- Output:    **300**   \n\n***Weighted Averaging***<br>\nThe base model with higher predictive power is more important.\n- Output:     **Weighted Average*\n\n","metadata":{}},{"cell_type":"markdown","source":"# 2. Set up script parameters","metadata":{}},{"cell_type":"code","source":"SEED = 1992\nPROBAS = True\nFOLDS = 5\n\nTARGET = 'Survived'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Load TPS-04 competition data","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('../input/tabular-playground-series-apr-2021/train.csv')\ntest = pd.read_csv('../input/tabular-playground-series-apr-2021/test.csv')\nsubmission = pd.read_csv('../input/tabular-playground-series-apr-2021/sample_submission.csv')\npseudo_labels = pd.read_csv(\"../input/tps04preds/dae.csv\")\ntest[TARGET] = pseudo_labels[TARGET]\n\nall_df = pd.concat([train, test]).reset_index(drop=True)\ntest['Survived'] = [x for x in pseudo_labels.Survived]\nall_df = pd.concat([train, test]).reset_index(drop=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Checking the null Data\nnull_data = (train.isna().sum().sort_values(ascending=False) / len(train) * 100)[:6] \nfig, ax = plt.subplots(1,1,figsize=(10, 7)) \nax.bar(null_data.index, 100, color='#dadada', width=0.6) \nbar = ax.bar(null_data.index,null_data, width=0.6) \nax.bar_label(bar, fmt='%.01f %%') \nax.spines.left.set_visible(False) \nax.set_yticks([]) \nax.set_title('Null Data Ratio', fontweight='bold') \nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Preprocess data\nPreprocessing Logic: [BIZEN](https://www.kaggle.com/hiro5299834/tps-apr-2021-pseudo-labeling-voting-ensemble) notebook (as a benchmark) to compare results.","metadata":{}},{"cell_type":"code","source":"#Age fillna with mean age for each class\nall_df['Age'] = all_df['Age'].fillna(all_df['Age'].mean())\n\n# Cabin, fillna with 'X' and take first letter\nall_df['Cabin'] = all_df['Cabin'].fillna('X').map(lambda x: x[0].strip())\n\n# Ticket, fillna with 'X', split string and take first split \nall_df['Ticket'] = all_df['Ticket'].fillna('X').map(lambda x:str(x).split()[0] if len(str(x).split()) > 1 else 'X')\n\n# Fare, fillna with mean value\nfare_map = all_df[['Fare', 'Pclass']].dropna().groupby('Pclass').median().to_dict()\nall_df['Fare'] = all_df['Fare'].fillna(all_df['Pclass'].map(fare_map['Fare']))\nall_df['Fare'] = np.log1p(all_df['Fare'])\n\n# Embarked, fillna with 'X' value\nall_df['Embarked'] = all_df['Embarked'].fillna('X')\n\n# Name, take only surnames\nall_df['Name'] = all_df['Name'].map(lambda x: x.split(',')[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#The Feature Engineering Results\nall_df.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5. Label Encoding","metadata":{}},{"cell_type":"code","source":"label_cols = ['Name', 'Ticket', 'Sex']\nonehot_cols = ['Cabin', 'Embarked']\nnumerical_cols = ['Pclass', 'Age', 'SibSp', 'Parch', 'Fare']\n\ndef label_encoder(c):\n    le = LabelEncoder()\n    return le.fit_transform(c)\n\nscaler = StandardScaler()\n\nonehot_encoded_df = pd.get_dummies(all_df[onehot_cols])\nlabel_encoded_df = all_df[label_cols].apply(label_encoder)\nnumerical_df = pd.DataFrame(scaler.fit_transform(all_df[numerical_cols]), columns=numerical_cols)\ntarget_df = all_df[TARGET]\n\n#Remove Duplicates\nall_df = all_df.loc[~all_df.index.duplicated(keep='first')]\n\n#Concat all dataframes\nall_df = pd.concat([numerical_df, label_encoded_df, onehot_encoded_df, target_df], axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 6. Create Train and Test Datasets","metadata":{}},{"cell_type":"code","source":"X = all_df.drop([TARGET], axis = 1)\ny = all_df[TARGET]\n\nprint (f'X:{X.shape} y: {y.shape} \\n')\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1, random_state = SEED)\nprint (f'X_train:{X_train.shape} y_train: {y_train.shape}')\nprint (f'X_test:{X_test.shape} y_test: {y_test.shape}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = all_df[len(train):].drop([TARGET], axis = 1)\nprint (f'test:{test.shape}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 7. Create Meta Classifier","metadata":{}},{"cell_type":"code","source":"lgbm_params = {\n    'metric': 'binary_logloss',\n    'n_estimators': 9000,\n    'objective': 'binary',\n    'random_state': SEED,\n    'learning_rate': 0.02,\n    'min_child_samples': 150,\n    'reg_alpha': 3e-5,\n    'reg_lambda': 9e-2,\n    'num_leaves': 20,\n    'max_depth': 16,#16\n    'colsample_bytree': 0.8,\n    'subsample': 0.7,\n    'subsample_freq': 2,\n    'max_bin': 240,\n    'device':'gpu'\n}\n\ncat_params = {#'iterations': 5000,\n          'eval_metric': 'AUC',\n          'loss_function':'Logloss',\n          'od_type':'Iter',\n          'num_trees':50000,\n          'max_depth': 6, \n          'l2_leaf_reg': 3,\n          'bootstrap_type': 'Bayesian',\n          'max_bin': 254,\n          'grow_policy': \"Lossguide\",\n          'random_seed': 314,\n          'min_data_in_leaf': 64,\n          'verbose': None,\n          'logging_level': 'Silent',\n          'task_type': 'GPU'\n}\n\nETC_params = {\n    'bootstrap':True,\n    'criterion': 'entropy',\n    'max_features': 0.55,\n    'min_samples_leaf': 8,\n    'min_samples_split': 4,\n    'n_estimators': 100\n}\n\nrf_params = {\n    'max_depth': 15,\n    'min_samples_leaf': 8,\n    'random_state': SEED\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.layers import Input, Dense, Dropout,Reshape \nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.wrappers.scikit_learn import KerasClassifier #To teat ANN as classifier\n\ndef ann_network():\n    i = Input(shape=(None, 32, 6))\n    x = Dense(60, activation='relu')(i)  \n    x = Dense(1, activation='sigmoid')(x) \n    model = Model(i, x)\n    \n    opt = tf.keras.optimizers.SGD(lr=1e-4, decay=1e-6, momentum=0.8, nesterov=True)\n    model.compile(\n        loss=\"binary_crossentropy\",\n        optimizer=opt,\n        metrics=[\"accuracy\"]\n    )\n    \n    return model\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from mlxtend.classifier import StackingCVClassifier,EnsembleVoteClassifier\nfrom sklearn.linear_model import LogisticRegression\n\ncl1 = CatBoostClassifier(**cat_params)\ncl2 = LGBMClassifier(**lgbm_params)\ncl3 = ExtraTreesClassifier(**ETC_params)\n\nmlr = LogisticRegression()\n\nann_clf = KerasClassifier(lambda: ann_network(), epochs=4)\n\n# ANN Ensembling\nclf = StackingCVClassifier(classifiers= [cl1,cl2,cl3], \n                            meta_classifier = ann_clf, \n                            use_probas = True, \n                            random_state = SEED) \n\n# Hard Voting Ensemble\nS_eclf = EnsembleVoteClassifier(clfs=[cl1, cl2, cl3],\n                              weights=[1, 1, 2], voting='soft')\n\n#Soft Voting Ensemble\nH_eclf = EnsembleVoteClassifier(clfs=[cl1, cl2, cl3],\n                              weights=[1, 1, 3], voting='hard')\n\n#PseudoMeta classifier\nAnnStakced_clf =  StackingCVClassifier(classifiers= [cl1,cl2,cl3],\n                            meta_classifier = S_eclf, \n                            use_probas = True,    \n                            random_state = SEED) \n\nclassifiers = [clf,H_eclf,S_eclf,AnnStakced_clf]\n\n# Fit the classifier variations\nclf.fit(X_train, y_train) \nH_eclf.fit(X_train, y_train) \nS_eclf.fit(X_train, y_train)\nAnnStakced_clf.fit(X_train, y_train) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 8. Predict and Validate (AUC)","metadata":{}},{"cell_type":"code","source":"preds = pd.DataFrame()\nclassifiers = {\"stacked CLF\": clf,\n              'Soft voted CLF':S_eclf,\n               'Hard voted CLF':H_eclf,\n              'Ann Clf':AnnStakced_clf} \nNUM_CLAS = 4\nfor key in classifiers:\n    try:\n        y_pred = classifiers[key].predict_meta_features(X_test)[:,1]\n    except:\n        y_pred = classifiers[key].predict_proba(X_test)[:,1]\n    preds[f\"{key}\"] = y_pred\n    auc = metrics.roc_auc_score(y_test, y_pred)\n    print(f\"{key} -> AUC: {auc:.3f}\")\n\npreds[TARGET] = pd.DataFrame(y_test).reset_index(drop=True)\n\nprint(preds.sample(10))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 9. Plot Results","metadata":{}},{"cell_type":"code","source":"sns.set(font_scale = 1)\nsns.set_style({\"axes.facecolor\": \"1.0\", \"axes.edgecolor\": \"0.85\", \"grid.color\": \"0.85\",\n               \"grid.linestyle\": \"-\", 'axes.labelcolor': '0.4', \"xtick.color\": \"0.4\",\n               'ytick.color': '0.4'})\n\nf, ax = plt.subplots(figsize=(13, 4), nrows=1, ncols = NUM_CLAS)\n\nfor key, counter in zip(classifiers, range(NUM_CLAS)):\n    \n    y_pred = preds[key]\n   \n    auc = metrics.roc_auc_score(y_test, y_pred)\n    textstr = f\"AUC: {auc:.3f}\"\n\n\n    false_pred = preds[preds[TARGET] == 0]\n    sns.distplot(false_pred[key], hist=True, kde=True, \n                 bins=int(50), color = 'red', \n                 hist_kws={'edgecolor':'black'}, ax = ax[counter])\n    \n\n    true_pred = preds[preds[TARGET] == 1]\n    sns.distplot(true_pred[key], hist=True, kde=True, \n                 bins=int(50), color = 'green', \n                 hist_kws={'edgecolor':'black'}, ax = ax[counter])\n    \n    \n    props = dict(boxstyle='round', facecolor='white', alpha=0.5)\n    \n    ax[counter].text(0.05, 0.95, textstr, transform=ax[counter].transAxes, fontsize=14,\n                    verticalalignment = \"top\", bbox=props)\n    \n    ax[counter].set_title(f\"{key}\")\n    ax[counter].set_xlim(0,1)\n    ax[counter].set_xlabel(\"Probability\")\n\nplt.tight_layout()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 10. Final Prediction","metadata":{}},{"cell_type":"code","source":"# For ANN Classifier we can not use predict_proba or .predict. Instead of this .predict_meta_features\n\ntest_preds1 = clf.predict_meta_features(test)[:,1]\n#test_preds2 = H_eclf.predict_meta_features(test)[:,1]\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# tip -> Alexander Ryzhkov\n\nthreshold = pd.Series(test_preds1).sort_values(ascending = False).head(34911).values[-1]\nthreshold=threshold\nprint(f\"Current threshold is: {threshold}\")\n\n# Creation of Multiple Submission for Voting\nsubmission['submit_1'] = (test_preds1 > threshold).astype(int)\nsubmission['submit_2'] = pseudo_labels[TARGET]\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nsubmission[[col for col in submission.columns if col.startswith('submit_')]].sum(axis = 1).value_counts()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission[TARGET] = (submission[[col for col in submission.columns if col.startswith('submit_')]].sum(axis=1) >= 2).astype(int)\nsubmission[TARGET].mean()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Final File preparation\nsubmission[['PassengerId', TARGET]].to_csv(\"Mstasko_final.csv\", index = False)\nsubmission[\"Survived\"].hist()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}}]}