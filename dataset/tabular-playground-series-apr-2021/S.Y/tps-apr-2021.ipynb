{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('../input/tabular-playground-series-apr-2021/train.csv')\ntest = pd.read_csv('../input/tabular-playground-series-apr-2021/test.csv')\n\nPassengerId = test[\"PassengerId\"]\n\ntrain.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.info()\nprint('_'*40)\ntest.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# two way to display nun-nunmeric\nprint(train.describe(include=['O']))\nprint('-'*60)\nprint(train.describe(exclude=['number']))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check the percentage of survivors at 'Pclass'\n# using mean since Survived is expressed by 0 or 1 (1 = 'suvived')\ntrain[['Pclass', 'Survived']].groupby(['Pclass'], as_index=False).mean().sort_values(by='Survived', ascending=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train[[\"Sex\", \"Survived\"]].groupby(['Sex'], as_index=False).mean().sort_values(by='Survived', ascending=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check the percentage of survivors at 'Sex'\ntrain[[\"Parch\", \"Survived\"]].groupby(['Parch'], as_index=False).mean().sort_values(by='Survived', ascending=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train[[\"SibSp\", \"Survived\"]].groupby(['SibSp'], as_index=False).mean().sort_values(by='Survived', ascending=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"missing_val_count_by_column_train = (train.isnull().sum())\nprint(missing_val_count_by_column_train[missing_val_count_by_column_train > 0])\n\nmissing_val_count_by_column_test = (test.isnull().sum())\nprint(missing_val_count_by_column_test[missing_val_count_by_column_test > 0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import random\n# missing value completion at train data\nTicket_nullcount_train = train['Ticket'].isnull().sum()\nrand_train_Ticket_ar = np.random.randint(100,999999, size = Ticket_nullcount_train)\n\n#print(Ticket_nullcount_train)\n\n# too much time below code\n\"\"\"\nfor num in range(Ticket_nullcount_train):\n    rand_train_Ticket = rand_train_Ticket_ar[num]\n    train['Ticket'].fillna(value=rand_train_Ticket, inplace=True, limit=1)\n\"\"\"   \n\n# because ticket and ticket_type was dropped after cell\ntrain['Ticket'].fillna(value=rand_train_Ticket_ar[1], inplace=True)\n\n# **replace same value\n# replace random value with missing value\n\n#print(rand_train_Ticket) # check\n\n#train['Ticket'][np.isnan(train['Ticket'])] = rand_train_Ticket #???\ntrain['Ticket'] = train['Ticket'].astype(str)\n\ntrain['Ticket_type'] = train['Ticket'].apply(lambda x: x[0:3])\ntrain['Ticket_type'] = train['Ticket_type'].astype('category').cat.codes\ntrain.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# missing value completion at test data\n\nTicket_nullcount_test = test['Ticket'].isnull().sum()\nrand_test_Ticket_ar = np.random.randint(100,999999, size = Ticket_nullcount_test)\n\n\"\"\"\nfor num in range(Ticket_nullcount_test):\n    rand_test_Ticket = rand_test_Ticket_ar[num]\n    test['Ticket'].fillna(value=rand_test_Ticket, inplace=True, limit=1) # replace random value with missing value\n\"\"\"\n    \n# because ticket and ticket_type was dropped after cell\ntrain['Ticket'].fillna(value=rand_test_Ticket_ar[1], inplace=True)\n\n# print(rand_test_Ticket) # check\n\n#test['Ticket'][np.isnan(test['Ticket'])] = rand_test_Ticket ???\ntest['Ticket'] = test['Ticket'].astype(str)\n\ntest['Ticket_type'] = test['Ticket'].apply(lambda x: x[0:3])\ntest['Ticket_type'] = test['Ticket_type'].astype('category').cat.codes\ntest.head()\n\ntest.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"missing_val_count_by_column_train = (train.isnull().sum())\nprint(missing_val_count_by_column_train[missing_val_count_by_column_train > 0])\n\nmissing_val_count_by_column_test = (test.isnull().sum())\nprint(missing_val_count_by_column_test[missing_val_count_by_column_test > 0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# age\nfull_data = train + test\n\nAge_average = full_data[\"Age\"].mean() #average\nAge_std = full_data[\"Age\"].std()  # standard deviation\nAge_nullcount_train = train[\"Age\"].isnull().sum() # count missing value\nAge_nullcount_test = test[\"Age\"].isnull().sum() # count missing value\n\n# uniform distribution\nrand_train = np.random.randint(Age_average - Age_std, Age_average + Age_std , size = Age_nullcount_train)\nrand_test = np.random.randint(Age_average - Age_std, Age_average + Age_std , size = Age_nullcount_test)\n\n#train['Age'].fillna(value=rand_train, inplace=True)\n#test['Age'].fillna(value=rand_test, inplace=True)\n\ntrain[\"Age\"][np.isnan(train[\"Age\"])] = rand_train\ntest[\"Age\"][np.isnan(test[\"Age\"])] = rand_test\n\ntrain['Age'] = train['Age'].astype(int)\ntest['Age'] = test['Age'].astype(int)\n\ntrain.loc[train['Age'] <= 16, 'Age'] = 0\ntest.loc[test['Age'] <= 16, 'Age'] = 0\ntrain.loc[(train['Age'] > 16) & (train['Age'] <= 32), 'Age'] = 1\ntest.loc[(test['Age'] > 16) & (test['Age'] <= 32), 'Age'] = 1\ntrain.loc[(train['Age'] > 32) & (train['Age'] <= 48), 'Age'] = 2\ntest.loc[(test['Age'] > 32) & (test['Age'] <= 48), 'Age'] = 2\ntrain.loc[(train['Age'] > 48) & (train['Age'] <= 64), 'Age'] = 3\ntest.loc[(test['Age'] > 48) & (test['Age'] <= 64), 'Age'] = 3\ntrain.loc[train['Age'] > 64, 'Age'] = 4\ntest.loc[test['Age'] > 64, 'Age'] = 4\n\ntrain.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"missing_val_count_by_column_train = (train.isnull().sum())\nprint(missing_val_count_by_column_train[missing_val_count_by_column_train > 0])\n\nmissing_val_count_by_column_test = (test.isnull().sum())\nprint(missing_val_count_by_column_test[missing_val_count_by_column_test > 0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['Sex'] = train['Sex'].map( {'female': 0, 'male': 1} ).astype(int)\ntest['Sex'] = test['Sex'].map( {'female': 0, 'male': 1} ).astype(int)    \ntrain.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"missing_val_count_by_column_train = (train.isnull().sum())\nprint(missing_val_count_by_column_train[missing_val_count_by_column_train > 0])\n\nmissing_val_count_by_column_test = (test.isnull().sum())\nprint(missing_val_count_by_column_test[missing_val_count_by_column_test > 0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['Has_Cabin'] = train[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)\ntrain['Family_Size'] = train['SibSp'] + train['Parch'] + 1\n\ntest['Has_Cabin'] = test[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)\ntest['Family_Size'] = test['SibSp'] + test['Parch'] + 1\n\ntrain.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"missing_val_count_by_column_train = (train.isnull().sum())\nprint(missing_val_count_by_column_train[missing_val_count_by_column_train > 0])\n\nmissing_val_count_by_column_test = (test.isnull().sum())\nprint(missing_val_count_by_column_test[missing_val_count_by_column_test > 0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Fare_average = full_data[\"Fare\"].mean() #average\nFare_std = full_data[\"Fare\"].std()  # standard deviation\nFare_nullcount_train = train[\"Fare\"].isnull().sum() # count missing value\nFare_nullcount_test = test[\"Fare\"].isnull().sum() # count missing value\n\n# uniform distribution\nrand_train_Fare = np.random.randint(Fare_average - Fare_std, Fare_average + Fare_std , size = Fare_nullcount_train)\nrand_test_Fare = np.random.randint(Fare_average - Fare_std, Fare_average + Fare_std , size = Fare_nullcount_test)\n\n\ntrain[\"Fare\"][np.isnan(train[\"Fare\"])] = rand_train_Fare\ntest[\"Fare\"][np.isnan(test[\"Fare\"])] = rand_test_Fare\n\ntrain['Fare'] = train['Fare'].astype(int)\ntest['Fare'] = test['Fare'].astype(int)\n\ntrain.loc[train['Fare'] <= 7.91, 'Fare'] = 0\ntest.loc[test['Fare'] <= 7.91, 'Fare'] = 0\ntrain.loc[(train['Fare'] > 7.91) & (train['Fare'] <= 14.454), 'Fare'] = 1\ntest.loc[(test['Fare'] > 7.91) & (test['Fare'] <= 14.454), 'Fare'] = 1\ntrain.loc[(train['Fare'] > 14.454) & (train['Fare'] <= 31), 'Fare'] = 2\ntest.loc[(test['Fare'] > 14.454) & (test['Fare'] <= 31), 'Fare'] = 2\ntrain.loc[train['Fare'] > 31, 'Fare'] = 3\ntest.loc[test['Fare'] > 31, 'Fare'] = 3\n\ntrain.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"missing_val_count_by_column_train = (train.isnull().sum())\nprint(missing_val_count_by_column_train[missing_val_count_by_column_train > 0])\n\nmissing_val_count_by_column_test = (test.isnull().sum())\nprint(missing_val_count_by_column_test[missing_val_count_by_column_test > 0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_train = train\nsample_test = test\n\nimport matplotlib.pyplot as plt\n\nprint(sample_train['Embarked'].value_counts())\nsample_train['Embarked'].hist()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\" # replace previous value\nsample_train['Embarked'].fillna(method='ffill', inplace=True)\nsample_test['Embarked'].fillna(method='ffill', inplace=True)\n\"\"\"\n\n# replace 'S'\nsample_train['Embarked'].fillna(value='S', inplace=True)\nsample_test['Embarked'].fillna(value='S', inplace=True)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"missing_val_count_by_column_train = (train.isnull().sum())\nprint(missing_val_count_by_column_train[missing_val_count_by_column_train > 0])\n\nmissing_val_count_by_column_test = (test.isnull().sum())\nprint(missing_val_count_by_column_test[missing_val_count_by_column_test > 0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\nfrom pylab import rcParams\n\nsample_train.hist(figsize=(15, 15))\nplt.show() ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"g = sns.FacetGrid(sample_train, col='Survived')\ng.map(plt.hist, 'Age', bins=20)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"drop_elements = ['PassengerId', 'Name', 'Ticket', 'Cabin', 'SibSp', 'Parch']\nsample_train = sample_train.drop(drop_elements, axis = 1)\nsample_test  = sample_test.drop(drop_elements, axis = 1)\n\nsample_train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_train.loc[sample_train['Embarked'] == 'S', 'Embarked'] = 0\nsample_test.loc[sample_test['Embarked'] == 'S', 'Embarked'] = 0\nsample_train.loc[sample_train['Embarked'] == 'C', 'Embarked'] = 1\nsample_test.loc[sample_test['Embarked'] == 'C', 'Embarked'] = 1\nsample_train.loc[sample_train['Embarked'] == 'Q', 'Embarked'] = 2\nsample_test.loc[sample_test['Embarked'] == 'Q', 'Embarked'] = 2\n\nsample_train = sample_train.astype({'Embarked': int})\nsample_test = sample_test.astype({'Embarked': int})\n\nsample_train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"missing_val_count_by_column_train = (sample_train.isnull().sum())\nprint(missing_val_count_by_column_train[missing_val_count_by_column_train > 0])\n\nmissing_val_count_by_column_test = (sample_test.isnull().sum())\nprint(missing_val_count_by_column_test[missing_val_count_by_column_test > 0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check correlation\ntrain[[\"Embarked\", \"Survived\"]].groupby(['Embarked'], as_index=False).mean().sort_values(by='Survived', ascending=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_train.hist(figsize=(15, 15))\nplt.show() ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#correlation matrix\ncorrmat = sample_train.corr()\nf, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(corrmat, annot=True, vmax=.8, square=True);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_train = sample_train.drop(['Ticket_type', 'Family_Size'], axis = 1)\ntarget = sample_test.drop(['Ticket_type', 'Family_Size'], axis = 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#correlation matrix\ncorrmat = final_train.corr()\nf, ax = plt.subplots(figsize=(8, 6))\nsns.heatmap(corrmat, annot=True, vmax=.8, square=True);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_train.hist(figsize=(12, 12))\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_train.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# standardization\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import scale\n\nfinal_train['Pclass'] = scale(final_train['Pclass'])\nfinal_train['Age'] = scale(final_train['Age'])\nfinal_train['Fare'] = scale(final_train['Fare'])\nfinal_train['Embarked'] = scale(final_train['Embarked'])\n#final_train['Family_Size'] = scale(final_train['Family_Size'])\n\ntarget['Pclass'] = scale(target['Pclass'])\ntarget['Age'] = scale(target['Age'])\ntarget['Fare'] = scale(target['Fare'])\ntarget['Embarked'] = scale(target['Embarked'])\n#target['Family_Size'] = scale(target['Family_Size'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_train.hist(figsize=(12, 12))\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nx_train_b, x_test_b = train_test_split(final_train, test_size=0.1, random_state=1)\n\ny_train = x_train_b['Survived'].ravel()\nx_train = x_train_b.drop(['Survived'], axis=1)\nx_train = x_train.values # Creates an array of the train data\n\ny_test = x_test_b['Survived'].ravel()\nx_test = x_test_b.drop(['Survived'], axis=1)\nx_test = x_test.values # Creates an array of the train data\n\nfinal_train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# machine learning\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ロジスティックス回帰\n# Random Search\nfrom scipy.stats import uniform\n\nlogistic = LogisticRegression(tol=1e-4, max_iter=300, random_state=0)\ndistributions = dict(C=uniform(loc=0, scale=4),\n                     tol=uniform(loc=1e-4, scale=1))\nclf_log = RandomizedSearchCV(logistic, distributions, random_state=0)\nsearch = clf_log.fit(x_train, y_train)\nprint(search.best_params_, search.best_score_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Grid Search\n\nparameters = {'C':[0.01,0.1,1,2], 'tol':[1e-4, 1e-3, 1e-2, 0.1, 1]}\nclf_g = GridSearchCV(logistic, parameters)\nclf_g.fit(x_train, y_train)\nsorted(clf_g.cv_results_.keys())\nclf_g.score(x_test, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"LR = LogisticRegression(C=0.08, tol=0.8, max_iter=500, random_state=0)\nLR.fit(x_train, y_train)\nprint(LR.score(x_train, y_train))\nLR_predict = LR.predict(target)\nLR.score(x_test, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\n# svc *not converge\nsvc = SVC()\nsvc.fit(x_train, y_train)\nsvc.score(x_test, y_test)\n\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gauss = GaussianNB()\ndistributions = dict(var_smoothing=uniform(loc=1e-10, scale=1e-6))\nclf_gauss = RandomizedSearchCV(gauss, distributions, random_state=0)\nsearch = clf_gauss.fit(x_train, y_train)\nprint(search.best_params_, search.best_score_)\nsearch.score(x_test, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# GaussianNB\ngaussian = GaussianNB()\ngaussian.fit(x_train, y_train)\ngauss_predict = gaussian.predict(target)\ngaussian.score(x_test, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Perceptron\nperceptron = Perceptron(random_state=0)\nperceptron.fit(x_train, y_train)\nprint(perceptron.score(x_train, y_train))\nperceptron_predict = perceptron.predict(target)\nperceptron.score(x_test, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\n# need to edit using SGD\nlogistic = LogisticRegression(tol=1e-4, max_iter=300, random_state=0)\ndistributions = dict(C=uniform(loc=0, scale=4),\n                     tol=uniform(loc=1e-4, scale=1))\nclf_log = RandomizedSearchCV(logistic, distributions, random_state=0)\nsearch = clf_log.fit(x_train, y_train)\nprint(search.best_params_, search.best_score_)\nsearch.score(x_test, y_test)\n\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Stochastic Gradient Descent\nsgd = SGDClassifier()\nsgd.fit(x_train, y_train)\nsgd_predict = sgd.predict(target)\nsgd.score(x_test, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"param_DTC = {\"criterion\": [\"entropy\"] #, \"gini\"]\n             ,\"splitter\": [\"random\"]\n             ,\"max_depth\": [8]  #[4,5,6,7,8,9]\n             ,\"min_samples_split\": [6] #[4,5,6,7,8]\n             ,\"min_samples_leaf\": [7] #[6,7,8,9,10]\n             }\nDTC = DecisionTreeClassifier()\n\nclf_DTC = GridSearchCV(DTC, param_DTC)\nclf_DTC.fit(x_train, y_train)\nprint(sorted(clf_DTC.cv_results_.keys()))\nprint(clf_DTC.best_params_)\nprint(clf_DTC.best_score_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 決定木\nDTC = DecisionTreeClassifier(criterion='entropy', max_depth=8, min_samples_leaf=7, min_samples_split=6, splitter='random',random_state=0)\nDTC.fit(x_train, y_train)\nDTC_predict = DTC.predict(target)\n# 正解率を表示\nDTC.score(x_test, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"param_RFC = {\"max_depth\": [7] # [4,5,6,7,8]\n             }\nRFC = RandomForestClassifier(n_estimators=300, random_state=4)\n\nclf_RFC = GridSearchCV(RFC, param_RFC)\nclf_RFC.fit(x_train, y_train)\nprint(sorted(clf_RFC.cv_results_.keys()))\nprint(clf_RFC.best_params_)\nprint(clf_RFC.best_score_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ランダムフォレスト\nRFC = RandomForestClassifier(max_depth=7, n_estimators=300, random_state=4)\nRFC.fit(x_train, y_train)\n\nRFC_predict = RFC.predict(target)\n\nRFC.score(x_test, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#LinearSVC\nfrom sklearn import svm #??収束させるには？\n\nLsvc = svm.LinearSVC(random_state=2, max_iter=3000)\nLsvc.fit(x_train, y_train)\nLsvc_predict = Lsvc.predict(target)\nLsvc.score(x_test, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nn_neighbors\nweights* ‘uniform’ or ‘distance’\nalgorithm: ‘ball_tree’,‘kd_tree’, ‘brute’, ‘auto’\nleaf_size\n\"\"\"\nparam_KNN =  {'n_neighbors':[7], # [6,7,8,9]\n              'weights':['uniform'],\n              'algorithm':['ball_tree'],\n              'leaf_size':[9], #[7,8,9,10]\n              'p':[1]}\nKNN = KNeighborsClassifier()\n\nclf_KNN = GridSearchCV(KNN, param_KNN)\nclf_KNN.fit(x_train, y_train)\nsorted(clf_KNN.cv_results_.keys())\nprint(clf_KNN.best_params_)\nprint(clf_KNN.best_score_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#K近傍法\nKNN = KNeighborsClassifier(n_neighbors=7,algorithm='ball_tree', leaf_size=9, p=1, weights='uniform')\nKNN.fit(x_train, y_train)\nKNN_predict = KNN.predict(target)\nKNN.score(x_test, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#xgb model\nimport xgboost as xgb\n\nclf = xgb.XGBClassifier(max_depth=2, n_estimators=300)\nclf.fit(x_train, y_train)\npred = clf.predict(target.values)\nxgb_predict = pd.Series(pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nfrom sklearn.kernel_ridge import KernelRidge\nKRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)\nKRR.fit(x_train, y_train)\nscore = rmsle_cv(KRR)\nprint(\"Kernel Ridge score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# [0.757, 0.7291, 0.4982, 0.7579, 0.7674, 0.7672, 0.7577, 0.7313]\nmodel_score = [LR.score(x_test, y_test),\n               gaussian.score(x_test, y_test),\n#               perceptron.score(x_test, y_test),\n               sgd.score(x_test, y_test),\n               DTC.score(x_test, y_test),\n               RFC.score(x_test, y_test),\n               Lsvc.score(x_test, y_test),\n               KNN.score(x_test, y_test),\n              ]\nprint(model_score)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# change weight ver. 18\nmodel_predict = LR_predict + gauss_predict + sgd_predict + 2*DTC_predict + 2*RFC_predict + Lsvc_predict + KNN_predict + xgb_predict #+ perceptron_predict \nmodel_predict = model_predict / 10\n\n#print(len(model_score))\n#print(model_predict)\nfor i in range(len(model_predict)):\n    if model_predict[i] >=0.5:\n        model_predict[i] = 1\n    else:\n        model_predict[i] =0\n\n#print(model_predict)\n#print(model_predict.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(model_predict.dtype)\nmodel_predict = model_predict.astype(np.int64)\nprint(model_predict.dtype)\nprint(model_predict.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nfrom sklearn.metrics import accuracy_score\n\nscore = accuracy_score(x_train, xgb_predictions)\nprint('score:{0:.4f}'.format(score))\n\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# my_prediction(予測データ）とPassengerIdをデータフレームへ落とし込む\nStackingSubmission = pd.DataFrame({ 'PassengerId': PassengerId,\n                            'Survived': model_predict})\n\n# my_tree_one.csvとして書き出し\nStackingSubmission.to_csv(\"submission.csv\", index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}