{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# TPS April'21 (ANN + Pseudolabel) ","metadata":{}},{"cell_type":"markdown","source":"**********************************************************************\n## Current Score : 81.101 %\n**********************************************************************\n### Previous score : 80.782 %\n\nhttps://www.kaggle.com/pranjalverma08/tps-april-21-the-ann-approach-score-80-782","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":0.024042,"end_time":"2021-04-14T09:46:40.827782","exception":false,"start_time":"2021-04-14T09:46:40.80374","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport theano\nimport tensorflow\nimport keras\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom tensorflow.keras.utils import plot_model\nfrom sklearn.model_selection import GridSearchCV\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Dropout\nfrom sklearn.experimental import enable_halving_search_cv  \nfrom sklearn.model_selection import HalvingGridSearchCV\nfrom sklearn.model_selection import HalvingRandomSearchCV","metadata":{"papermill":{"duration":7.864114,"end_time":"2021-04-14T09:46:48.705635","exception":false,"start_time":"2021-04-14T09:46:40.841521","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train=pd.read_csv('../input/tabular-playground-series-apr-2021/train.csv')\ntest=pd.read_csv('../input/tabular-playground-series-apr-2021/test.csv')\npseudo_label = pd.read_csv('../input/pseudosubforann/predforpseudo.csv', index_col=0)","metadata":{"papermill":{"duration":0.608321,"end_time":"2021-04-14T09:46:49.327487","exception":false,"start_time":"2021-04-14T09:46:48.719166","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train.shape)\nprint(test.shape)\nprint(pseudo_label.shape)","metadata":{"papermill":{"duration":0.022421,"end_time":"2021-04-14T09:46:49.363877","exception":false,"start_time":"2021-04-14T09:46:49.341456","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Engineering","metadata":{"papermill":{"duration":0.013891,"end_time":"2021-04-14T09:46:49.393547","exception":false,"start_time":"2021-04-14T09:46:49.379656","status":"completed"},"tags":[]}},{"cell_type":"code","source":"test['Survived'] = [x for x in pseudo_label.Survived]\ntrain['Survived'].value_counts()\ntrain.isnull().sum()\ntest.isnull().sum()\n#train.head()\ntrain['Ticket'].nunique()\ntrain[[\"SibSp\", \"Survived\"]].groupby(['SibSp'], as_index=False).mean().sort_values(by='Survived', ascending=False)\ntrain[[\"Sex\", \"Survived\"]].groupby(['Sex'], as_index=False).mean().sort_values(by='Survived', ascending=False)\ntrain[[\"Parch\", \"Survived\"]].groupby(['Parch'], as_index=False).mean().sort_values(by='Survived', ascending=False)\ntrain[[\"Embarked\", \"Survived\"]].groupby(['Embarked'], as_index=False).mean().sort_values(by='Survived', ascending=False)\ntrain_copy = train.copy()\ntest_copy = test.copy()\ntrain = train.drop(['PassengerId', 'Ticket'], axis = 1)\ntest = test.drop(['PassengerId', 'Ticket',], axis = 1)\ncombine = [train, test]\n\ntrain['Cabin'].fillna('U', inplace=True)\ntrain['Cabin'] = train['Cabin'].apply(lambda x: x[0])\n\ntest['Cabin'].fillna('U', inplace=True)\ntest['Cabin'] = test['Cabin'].apply(lambda x: x[0])\n\ntrain['Cabin'].unique()\nfor dataset in combine:\n  dataset['Cabin'] = dataset['Cabin'].fillna('U')\n  dataset['Cabin'] = dataset['Cabin'].apply(lambda x: x[0])\n  \npd.crosstab(train['Cabin'], train['Survived'])\ntrain[['Cabin', 'Survived']].groupby(['Cabin'], as_index = False).mean().sort_values(by = 'Survived', ascending = True)\ncabin_mapping = {\"T\": 0, \"U\": 1, \"A\": 2, \"G\": 3, \"C\": 4, \"F\": 5, \"B\": 6, \"E\": 7, \"D\": 8}\nfor dataset in combine:\n    dataset['Cabin'] = dataset['Cabin'].map(cabin_mapping)\n    dataset['Cabin'] = dataset['Cabin'].fillna(0)\n\n#train.head()\nfor dataset in combine:\n    dataset['Title'] = dataset['Name'].map(lambda x: x.split(',')[1].split('.')[0].strip())\n\npd.crosstab(train['Title'], train['Sex'])\n\n\n# for dataset in combine:\n#     dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Major', 'Rev', 'Jonkheer', 'Dona'], 'Rare')\n\n#     dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n#     dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n#     dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n#     dataset['Title'] = dataset['Title'].replace('Sir', 'Mr')\n#     dataset['Title'] = dataset['Title'].replace('Dr', 'Mr')\n    \n# train[['Title', 'Survived']].groupby(['Title'], as_index=False).mean()\n\n# title_mapping = {\"Mrs\": 4, \"Miss\": 3, \"Mr\": 1, \"Master\": 2, \"Rare\": 0}\n# for dataset in combine:\n#     dataset['Title'] = dataset['Title'].map(title_mapping)\n#     dataset['Title'] = dataset['Title'].fillna(0)\n\n#train.head()\n\nfor dataset in combine:\n    dataset['Sex'] = dataset['Sex'].map( {'female': 1, 'male': 0} ).astype(int)\n\n#train.head()\n# Imputing Missing Values\n\ntrain['Age'].fillna(train['Age'].dropna().median(), inplace=True)\ntest['Age'].fillna(train['Age'].mean(), inplace = True)\ntest['Fare'].fillna(train['Fare'].dropna().median(), inplace = True)\ntrain['Embarked'].fillna('C', inplace = True)\ntest['Embarked'].fillna('C', inplace = True)\ntrain['Fare'].fillna(train['Fare'].dropna().median(), inplace = True)\ntrain.drop('Title',axis=1,inplace=True)\ntest.drop('Title',axis=1,inplace=True)\n\ntrain.isnull().sum()\ntest.isnull().sum()\n\ntrain['AgeBand'] = pd.cut(train['Age'], 5)\ntrain[['AgeBand', 'Survived']].groupby(['AgeBand'], as_index=False).mean().sort_values(by='AgeBand', ascending=True)\n\n\nfor dataset in combine:    \n    dataset.loc[ dataset['Age'] <= 16, 'Age'] = 4\n    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\n    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\n    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\n    dataset.loc[ dataset['Age'] > 64, 'Age'] = 0\n    \n#train.head()\n\n\n\nfor dataset in combine:\n    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\n\ntrain[['FamilySize', 'Survived']].groupby(['FamilySize'], as_index=False).mean().sort_values(by='Survived', ascending=False)\n'''\nfor dataset in combine:    \n    dataset.loc[ dataset['FamilySize'] > 4, 'FamilySize'] = 0  \n    dataset.loc[ dataset['FamilySize'] <= 4, 'FamilySize'] = 1\n    \ntrain.head()\n\n'''\nfor dataset in combine:\n    dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\n\n#train.head()\n\ntrain['FareBand'] = pd.qcut(train['Fare'], 4)\ntrain[['FareBand', 'Survived']].groupby(['FareBand'], as_index=False).mean().sort_values(by='FareBand', ascending=True)\n\n\n\nfor dataset in combine:\n    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] = 0\n    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n    dataset.loc[ dataset['Fare'] > 31, 'Fare'] = 3\n    dataset['Fare'] = dataset['Fare'].astype(int)\n\ntrain = train.drop(['FareBand'], axis=1)\n#combine = [train, test]\ndata = pd.concat([train, test], axis=0)\n\n#train.head(5)\n\n# train = train.drop(['AgeBand', 'Name', 'SibSp', 'Parch' ], axis = 1)\n# test = test.drop(['Name', 'SibSp', 'Parch'], axis = 1)\n# # splitting the dataset into x(independent variables) and y(dependent variables)\n\n# x_train = train.drop('Survived', axis = 1)\n# y_train = train.Survived\n\n# # print(x_train.shape)\n# # print(y_train.shape)\n\n# x_test = test\n\n# # print(x_test.shape)\n# horizontal_stack = pd.concat([x_train, y_train], axis=1)\n# horizontal_stack.index+=1\n# train_df=horizontal_stack\n# # train_df\n# x_test.index+=100000\n# test_df=x_test\n# #testcopy=test_df.copy()\n","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"papermill":{"duration":1.200125,"end_time":"2021-04-14T09:46:50.607649","exception":false,"start_time":"2021-04-14T09:46:49.407524","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.concat([train, test], axis=0)\ntrain = data.iloc[:train.shape[0]]\ntest = data.iloc[train.shape[0]:].drop(columns=['Survived'])","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.columns","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.columns","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lab_cols = ['Pclass','Age', 'Ticket', 'Fare', 'Cabin', 'Embarked']\ntarget = 'Survived'\n\nfeatures_selected = ['Pclass', 'Sex', 'Age','Embarked','Parch','SibSp','Fare','Cabin']\n\nX = data.drop(target, axis=1)\nX = X[features_selected]\ny = data[target]\n\ntest = test[features_selected]","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X.shape","metadata":{"_kg_hide-input":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Building Model","metadata":{"papermill":{"duration":0.015593,"end_time":"2021-04-14T09:46:50.746591","exception":false,"start_time":"2021-04-14T09:46:50.730998","status":"completed"},"tags":[]}},{"cell_type":"code","source":"X_train,X_test,y_train,y_test = train_test_split(X,y,random_state=101, test_size=0.25)","metadata":{"papermill":{"duration":0.048987,"end_time":"2021-04-14T09:46:50.811595","exception":false,"start_time":"2021-04-14T09:46:50.762608","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_classifier(optimizer):\n    classifier = Sequential()\n    classifier.add(Dense(units=10,kernel_initializer='uniform',activation='relu',input_dim=8))\n    classifier.add(Dropout(rate = 0.2))\n    classifier.add(Dense(units=64,kernel_initializer='uniform',activation='relu'))\n    classifier.add(Dropout(rate = 0.2))\n#     classifier.add(Dense(units = 8, kernel_initializer = 'uniform', activation = 'relu'))\n#     classifier.add(Dropout(rate = 0.2))\n    classifier.add(Dense(units=1,kernel_initializer='uniform',activation='sigmoid'))\n    classifier.compile(optimizer=optimizer,loss='binary_crossentropy',metrics=['accuracy'])\n    return classifier","metadata":{"papermill":{"duration":0.025214,"end_time":"2021-04-14T09:46:50.852631","exception":false,"start_time":"2021-04-14T09:46:50.827417","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nclassifier = KerasClassifier(build_fn = build_classifier)\nparam_grid = dict(optimizer = ['Adam'],\n                  epochs=[50,100,200,250],\n                  batch_size=[16,32,64,128])\ngrid = HalvingRandomSearchCV(classifier, param_grid, scoring='accuracy',cv=10,n_jobs=-1,random_state=101)\ngrid_result = grid.fit(X_train, y_train)\nbest_parameters = grid.best_params_\nbest_accuracy = grid.best_score_","metadata":{"papermill":{"duration":358.961216,"end_time":"2021-04-14T09:52:49.829956","exception":false,"start_time":"2021-04-14T09:46:50.86874","status":"completed"},"scrolled":true,"tags":[],"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(grid.best_params_)\nprint(grid.best_score_)","metadata":{"papermill":{"duration":1.814068,"end_time":"2021-04-14T09:52:53.39835","exception":false,"start_time":"2021-04-14T09:52:51.584282","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Submission File**","metadata":{"papermill":{"duration":3.482228,"end_time":"2021-04-14T09:57:51.870843","exception":false,"start_time":"2021-04-14T09:57:48.388615","status":"completed"},"tags":[]}},{"cell_type":"code","source":"test_df=test.copy()\ntest_df.index+=100000","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_preds = grid.predict_proba(test_df)[:,1]\ntest_preds","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"threshold = pd.Series(test_preds).sort_values(ascending = False).head(34911).values[-1]\nprint(f\"Current threshold is: {threshold}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# yfinal = (test_preds > threshold).astype(int).reshape(test_df.shape[0])\n# output = pd.DataFrame({'PassengerId': test_df.index, 'Survived': yfinal})\n# output.to_csv('thresholdpseudoann.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Do upvote if you find this Notebook helpful.  ","metadata":{"papermill":{"duration":3.476687,"end_time":"2021-04-14T09:58:06.013677","exception":false,"start_time":"2021-04-14T09:58:02.53699","status":"completed"},"tags":[]}}]}