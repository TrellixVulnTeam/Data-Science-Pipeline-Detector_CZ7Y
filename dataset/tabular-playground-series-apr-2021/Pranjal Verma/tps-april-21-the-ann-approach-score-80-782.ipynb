{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"***********************************************\n## Updated Score : 81.101 %\nhttps://www.kaggle.com/pranjalverma08/tps-april-21-ann-pseudo-label-score-81-101\n**********************************************\n### Score of this Notebook : 80.782 %","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport theano\nimport tensorflow\nimport keras\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom tensorflow.keras.utils import plot_model\nfrom sklearn.model_selection import GridSearchCV\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Dropout\nfrom sklearn.experimental import enable_halving_search_cv  \nfrom sklearn.model_selection import HalvingGridSearchCV\nfrom sklearn.model_selection import HalvingRandomSearchCV","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train=pd.read_csv('../input/tabular-playground-series-apr-2021/train.csv')\ntest=pd.read_csv('../input/tabular-playground-series-apr-2021/test.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train.shape)\nprint(test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Engineering","metadata":{}},{"cell_type":"code","source":"train['Survived'].value_counts()\ntrain.isnull().sum()\ntest.isnull().sum()\n#train.head()\ntrain['Ticket'].nunique()\ntrain[[\"SibSp\", \"Survived\"]].groupby(['SibSp'], as_index=False).mean().sort_values(by='Survived', ascending=False)\ntrain[[\"Sex\", \"Survived\"]].groupby(['Sex'], as_index=False).mean().sort_values(by='Survived', ascending=False)\ntrain[[\"Parch\", \"Survived\"]].groupby(['Parch'], as_index=False).mean().sort_values(by='Survived', ascending=False)\ntrain[[\"Embarked\", \"Survived\"]].groupby(['Embarked'], as_index=False).mean().sort_values(by='Survived', ascending=False)\ntrain_copy = train.copy()\ntest_copy = test.copy()\ntrain = train.drop(['PassengerId', 'Ticket'], axis = 1)\ntest = test.drop(['PassengerId', 'Ticket',], axis = 1)\ncombine = [train, test]\n\ntrain['Cabin'].fillna('U', inplace=True)\ntrain['Cabin'] = train['Cabin'].apply(lambda x: x[0])\n\ntest['Cabin'].fillna('U', inplace=True)\ntest['Cabin'] = test['Cabin'].apply(lambda x: x[0])\n\ntrain['Cabin'].unique()\nfor dataset in combine:\n  dataset['Cabin'] = dataset['Cabin'].fillna('U')\n  dataset['Cabin'] = dataset['Cabin'].apply(lambda x: x[0])\n  \npd.crosstab(train['Cabin'], train['Survived'])\ntrain[['Cabin', 'Survived']].groupby(['Cabin'], as_index = False).mean().sort_values(by = 'Survived', ascending = True)\ncabin_mapping = {\"T\": 0, \"U\": 1, \"A\": 2, \"G\": 3, \"C\": 4, \"F\": 5, \"B\": 6, \"E\": 7, \"D\": 8}\nfor dataset in combine:\n    dataset['Cabin'] = dataset['Cabin'].map(cabin_mapping)\n    dataset['Cabin'] = dataset['Cabin'].fillna(0)\n\n#train.head()\nfor dataset in combine:\n    dataset['Title'] = dataset['Name'].map(lambda x: x.split(',')[1].split('.')[0].strip())\n\npd.crosstab(train['Title'], train['Sex'])\n\n\n# for dataset in combine:\n#     dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Major', 'Rev', 'Jonkheer', 'Dona'], 'Rare')\n\n#     dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n#     dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n#     dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n#     dataset['Title'] = dataset['Title'].replace('Sir', 'Mr')\n#     dataset['Title'] = dataset['Title'].replace('Dr', 'Mr')\n    \n# train[['Title', 'Survived']].groupby(['Title'], as_index=False).mean()\n\n# title_mapping = {\"Mrs\": 4, \"Miss\": 3, \"Mr\": 1, \"Master\": 2, \"Rare\": 0}\n# for dataset in combine:\n#     dataset['Title'] = dataset['Title'].map(title_mapping)\n#     dataset['Title'] = dataset['Title'].fillna(0)\n\n#train.head()\n\nfor dataset in combine:\n    dataset['Sex'] = dataset['Sex'].map( {'female': 1, 'male': 0} ).astype(int)\n\n#train.head()\n# Imputing Missing Values\n\ntrain['Age'].fillna(train['Age'].dropna().median(), inplace=True)\ntest['Age'].fillna(train['Age'].mean(), inplace = True)\ntest['Fare'].fillna(train['Fare'].dropna().median(), inplace = True)\ntrain['Embarked'].fillna('C', inplace = True)\ntest['Embarked'].fillna('C', inplace = True)\ntrain['Fare'].fillna(train['Fare'].dropna().median(), inplace = True)\ntrain.drop('Title',axis=1,inplace=True)\ntest.drop('Title',axis=1,inplace=True)\n\ntrain.isnull().sum()\ntest.isnull().sum()\n\ntrain['AgeBand'] = pd.cut(train['Age'], 5)\ntrain[['AgeBand', 'Survived']].groupby(['AgeBand'], as_index=False).mean().sort_values(by='AgeBand', ascending=True)\n\n\nfor dataset in combine:    \n    dataset.loc[ dataset['Age'] <= 16, 'Age'] = 4\n    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\n    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\n    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\n    dataset.loc[ dataset['Age'] > 64, 'Age'] = 0\n    \n#train.head()\n\n\n\nfor dataset in combine:\n    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\n\ntrain[['FamilySize', 'Survived']].groupby(['FamilySize'], as_index=False).mean().sort_values(by='Survived', ascending=False)\n'''\nfor dataset in combine:    \n    dataset.loc[ dataset['FamilySize'] > 4, 'FamilySize'] = 0  \n    dataset.loc[ dataset['FamilySize'] <= 4, 'FamilySize'] = 1\n    \ntrain.head()\n\n'''\nfor dataset in combine:\n    dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\n\n#train.head()\n\ntrain['FareBand'] = pd.qcut(train['Fare'], 4)\ntrain[['FareBand', 'Survived']].groupby(['FareBand'], as_index=False).mean().sort_values(by='FareBand', ascending=True)\n\n\n\nfor dataset in combine:\n    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] = 0\n    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n    dataset.loc[ dataset['Fare'] > 31, 'Fare'] = 3\n    dataset['Fare'] = dataset['Fare'].astype(int)\n\ntrain = train.drop(['FareBand'], axis=1)\ncombine = [train, test]\n    \n#train.head(5)\n\ntrain = train.drop(['AgeBand', 'Name', 'SibSp', 'Parch' ], axis = 1)\ntest = test.drop(['Name', 'SibSp', 'Parch'], axis = 1)\n# splitting the dataset into x(independent variables) and y(dependent variables)\n\nx_train = train.drop('Survived', axis = 1)\ny_train = train.Survived\n\n# print(x_train.shape)\n# print(y_train.shape)\n\nx_test = test\n\n# print(x_test.shape)\nhorizontal_stack = pd.concat([x_train, y_train], axis=1)\nhorizontal_stack.index+=1\ntrain_df=horizontal_stack\n# train_df\nx_test.index+=100000\ntest_df=x_test\n#testcopy=test_df.copy()\n","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Building Model","metadata":{}},{"cell_type":"code","source":"y=train_df['Survived']\nX=train_df.drop('Survived', axis =1)\nX_train,X_test,y_train,y_test = train_test_split(X,y,random_state=101, test_size=0.25)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_classifier(optimizer):\n    classifier = Sequential()\n    classifier.add(Dense(units=10,kernel_initializer='uniform',activation='relu',input_dim=7))\n    classifier.add(Dropout(rate = 0.2))\n    classifier.add(Dense(units=64,kernel_initializer='uniform',activation='relu'))\n    classifier.add(Dropout(rate = 0.2))\n#     classifier.add(Dense(units = 8, kernel_initializer = 'uniform', activation = 'relu'))\n#     classifier.add(Dropout(rate = 0.2))\n    classifier.add(Dense(units=1,kernel_initializer='uniform',activation='sigmoid'))\n    classifier.compile(optimizer=optimizer,loss='binary_crossentropy',metrics=['accuracy'])\n    return classifier","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nclassifier = KerasClassifier(build_fn = build_classifier)\nparam_grid = dict(optimizer = ['Adam'],\n                  epochs=[50,100,200],\n                  batch_size=[16,32,64])\ngrid = HalvingRandomSearchCV(classifier, param_grid, scoring='accuracy',cv=10,n_jobs=-1)\ngrid_result = grid.fit(X_train, y_train)\nbest_parameters = grid.best_params_\nbest_accuracy = grid.best_score_","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(grid.best_params_)\nprint(grid.best_score_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"classifier = KerasClassifier(build_fn = build_classifier,\n                             optimizer=best_parameters['optimizer'],\n                             batch_size=best_parameters['batch_size'],\n                             epochs=best_parameters['epochs'])\n\nclassifier.fit(X_train,y_train)\npreds = classifier.predict(test_df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Submission File**","metadata":{}},{"cell_type":"code","source":"# y_final = (preds > 0.5).astype(int).reshape(test_df.shape[0])\n\n# output = pd.DataFrame({'PassengerId': test_df.index, 'Survived': y_final})\n# output.to_csv('pred_ann.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Do upvote if you find this notebook helpful.**\n\nThis idea is still in intial phase so drop your views in comment section.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}