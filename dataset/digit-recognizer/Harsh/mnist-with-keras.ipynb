{"cells":[{"metadata":{"id":"UTJmsXnXQKbG"},"cell_type":"markdown","source":"## Keras -- MLPs on MNIST","execution_count":null},{"metadata":{"executionInfo":{"elapsed":8880,"status":"ok","timestamp":1532670072534,"user":{"displayName":"Applied AI Course","photoUrl":"//lh3.googleusercontent.com/-EsJzSyawCkQ/AAAAAAAAAAI/AAAAAAAAAWk/jhKHALKaHag/s50-c-k-no/photo.jpg","userId":"116292885805316472049"},"user_tz":-330},"id":"2UILABPAQKbH","outputId":"10fbb305-017f-45e7-ab0c-8787e2224880","trusted":true},"cell_type":"code","source":"\nfrom keras.utils import np_utils \nfrom keras.datasets import mnist \nimport seaborn as sns\nfrom keras.initializers import RandomNormal","execution_count":null,"outputs":[]},{"metadata":{"id":"lOO-TM_7QKbM","trusted":true},"cell_type":"code","source":"%matplotlib notebook\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport time\n# https://gist.github.com/greydanus/f6eee59eaf1d90fcb3b534a25362cea4\n# https://stackoverflow.com/a/14434334\n# this function is used to update the plots for each epoch and error\ndef plt_dynamic(x, vy, ty, ax, colors=['b']):\n    ax.plot(x, vy, 'b', label=\"Validation Loss\")\n    ax.plot(x, ty, 'r', label=\"Train Loss\")\n    plt.legend()\n    plt.grid()\n    fig.canvas.draw()","execution_count":null,"outputs":[]},{"metadata":{"executionInfo":{"elapsed":4700,"status":"ok","timestamp":1532670084617,"user":{"displayName":"Applied AI Course","photoUrl":"//lh3.googleusercontent.com/-EsJzSyawCkQ/AAAAAAAAAAI/AAAAAAAAAWk/jhKHALKaHag/s50-c-k-no/photo.jpg","userId":"116292885805316472049"},"user_tz":-330},"id":"TKmYKnibQKbQ","outputId":"d861b5ef-2bba-4884-fec3-43f1b6e5646a","trusted":true},"cell_type":"code","source":"# the data, shuffled and split between train and test sets \n(X_train, y_train), (X_test, y_test) = mnist.load_data()","execution_count":null,"outputs":[]},{"metadata":{"executionInfo":{"elapsed":1000,"status":"ok","timestamp":1532670089916,"user":{"displayName":"Applied AI Course","photoUrl":"//lh3.googleusercontent.com/-EsJzSyawCkQ/AAAAAAAAAAI/AAAAAAAAAWk/jhKHALKaHag/s50-c-k-no/photo.jpg","userId":"116292885805316472049"},"user_tz":-330},"id":"CCwnZae-QKbT","outputId":"b5f379d4-7ebc-4112-96c5-1a4c2ab4a97c","trusted":true},"cell_type":"code","source":"print(\"Number of training examples :\", X_train.shape[0], \"and each image is of shape (%d, %d)\"%(X_train.shape[1], X_train.shape[2]))\nprint(\"Number of training examples :\", X_test.shape[0], \"and each image is of shape (%d, %d)\"%(X_test.shape[1], X_test.shape[2]))","execution_count":null,"outputs":[]},{"metadata":{"id":"BlotuCmoQKbW","trusted":true},"cell_type":"code","source":"# if you observe the input shape its 2 dimensional vector\n# for each image we have a (28*28) vector\n# we will convert the (28*28) vector into single dimensional vector of 1 * 784 \n\nX_train = X_train.reshape(X_train.shape[0], X_train.shape[1]*X_train.shape[2]) \nX_test = X_test.reshape(X_test.shape[0], X_test.shape[1]*X_test.shape[2]) ","execution_count":null,"outputs":[]},{"metadata":{"executionInfo":{"elapsed":1001,"status":"ok","timestamp":1532670095751,"user":{"displayName":"Applied AI Course","photoUrl":"//lh3.googleusercontent.com/-EsJzSyawCkQ/AAAAAAAAAAI/AAAAAAAAAWk/jhKHALKaHag/s50-c-k-no/photo.jpg","userId":"116292885805316472049"},"user_tz":-330},"id":"8JvpNJy4QKbY","outputId":"cf0b2d6d-4976-45d3-a4ad-53b2ba3eed49","trusted":true},"cell_type":"code","source":"# after converting the input images from 3d to 2d vectors\n\nprint(\"Number of training examples :\", X_train.shape[0], \"and each image is of shape (%d)\"%(X_train.shape[1]))\nprint(\"Number of training examples :\", X_test.shape[0], \"and each image is of shape (%d)\"%(X_test.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{"executionInfo":{"elapsed":1009,"status":"ok","timestamp":1532670101067,"user":{"displayName":"Applied AI Course","photoUrl":"//lh3.googleusercontent.com/-EsJzSyawCkQ/AAAAAAAAAAI/AAAAAAAAAWk/jhKHALKaHag/s50-c-k-no/photo.jpg","userId":"116292885805316472049"},"user_tz":-330},"id":"qvuaHDk0QKbb","outputId":"1e5d1263-db82-4799-d47b-8a991bc2c89a","trusted":true},"cell_type":"code","source":"# An example data point\nprint(X_train[0])","execution_count":null,"outputs":[]},{"metadata":{"id":"lfohtdKPQKbd","trusted":true},"cell_type":"code","source":"# if we observe the above matrix each cell is having a value between 0-255\n# before we move to apply machine learning algorithms lets try to normalize the data\n# X => (X - Xmin)/(Xmax-Xmin) = X/255\n\nX_train = X_train/255\nX_test = X_test/255","execution_count":null,"outputs":[]},{"metadata":{"executionInfo":{"elapsed":1017,"status":"ok","timestamp":1532670108312,"user":{"displayName":"Applied AI Course","photoUrl":"//lh3.googleusercontent.com/-EsJzSyawCkQ/AAAAAAAAAAI/AAAAAAAAAWk/jhKHALKaHag/s50-c-k-no/photo.jpg","userId":"116292885805316472049"},"user_tz":-330},"id":"4C-dmsXJQKbf","outputId":"39118d5d-7c79-4f3e-c1f8-2f921de2b59d","trusted":true},"cell_type":"code","source":"# example data point after normlizing\nprint(X_train[0])","execution_count":null,"outputs":[]},{"metadata":{"executionInfo":{"elapsed":1026,"status":"ok","timestamp":1532670116894,"user":{"displayName":"Applied AI Course","photoUrl":"//lh3.googleusercontent.com/-EsJzSyawCkQ/AAAAAAAAAAI/AAAAAAAAAWk/jhKHALKaHag/s50-c-k-no/photo.jpg","userId":"116292885805316472049"},"user_tz":-330},"id":"3Ruu-vXzQKbi","outputId":"95160c17-cdc3-4127-c834-e64ce0ac9cc1","trusted":true},"cell_type":"code","source":"# here we are having a class number for each image\nprint(\"Class label of first image :\", y_train[0])\n\n# lets convert this into a 10 dimensional vector\n# ex: consider an image is 5 convert it into 5 => [0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n# this conversion needed for MLPs \n\nY_train = np_utils.to_categorical(y_train, 10) \nY_test = np_utils.to_categorical(y_test, 10)\n\nprint(\"After converting the output into a vector : \",Y_train[0])","execution_count":null,"outputs":[]},{"metadata":{"id":"u0pWB6UyQKbk"},"cell_type":"markdown","source":"<h2>  Softmax classifier  </h2>","execution_count":null},{"metadata":{"id":"tLAGMubCQKbm","trusted":true},"cell_type":"code","source":"# https://keras.io/getting-started/sequential-model-guide/\n\n# The Sequential model is a linear stack of layers.\n# you can create a Sequential model by passing a list of layer instances to the constructor:\n\n# model = Sequential([\n#     Dense(32, input_shape=(784,)),\n#     Activation('relu'),\n#     Dense(10),\n#     Activation('softmax'),\n# ])\n\n# You can also simply add layers via the .add() method:\n\n# model = Sequential()\n# model.add(Dense(32, input_dim=784))\n# model.add(Activation('relu'))\n\n###\n\n# https://keras.io/layers/core/\n\n# keras.layers.Dense(units, activation=None, use_bias=True, kernel_initializer='glorot_uniform', \n# bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, \n# kernel_constraint=None, bias_constraint=None)\n\n# Dense implements the operation: output = activation(dot(input, kernel) + bias) where\n# activation is the element-wise activation function passed as the activation argument, \n# kernel is a weights matrix created by the layer, and \n# bias is a bias vector created by the layer (only applicable if use_bias is True).\n\n# output = activation(dot(input, kernel) + bias)  => y = activation(WT. X + b)\n\n####\n\n# https://keras.io/activations/\n\n# Activations can either be used through an Activation layer, or through the activation argument supported by all forward layers:\n\n# from keras.layers import Activation, Dense\n\n# model.add(Dense(64))\n# model.add(Activation('tanh'))\n\n# This is equivalent to:\n# model.add(Dense(64, activation='tanh'))\n\n# there are many activation functions ar available ex: tanh, relu, softmax\n\n\nfrom keras.models import Sequential \nfrom keras.layers import Dense, Activation \n","execution_count":null,"outputs":[]},{"metadata":{"id":"I0s7jzhVQKbn","trusted":true},"cell_type":"code","source":"# some model parameters\n\noutput_dim = 10\ninput_dim = X_train.shape[1]\n\nbatch_size = 128 \nnb_epoch = 20","execution_count":null,"outputs":[]},{"metadata":{"id":"HdQg5wGDQKbr","trusted":true},"cell_type":"code","source":"# start building a model\nmodel = Sequential()\n\n# The model needs to know what input shape it should expect. \n# For this reason, the first layer in a Sequential model \n# (and only the first, because following layers can do automatic shape inference)\n# needs to receive information about its input shape. \n# you can use input_shape and input_dim to pass the shape of input\n\n# output_dim represent the number of nodes need in that layer\n# here we have 10 nodes\n\nmodel.add(Dense(output_dim, input_dim=input_dim, activation='softmax'))","execution_count":null,"outputs":[]},{"metadata":{"executionInfo":{"elapsed":43776,"status":"ok","timestamp":1532670180018,"user":{"displayName":"Applied AI Course","photoUrl":"//lh3.googleusercontent.com/-EsJzSyawCkQ/AAAAAAAAAAI/AAAAAAAAAWk/jhKHALKaHag/s50-c-k-no/photo.jpg","userId":"116292885805316472049"},"user_tz":-330},"id":"EVA11VpmQKbt","outputId":"08359e28-6c25-49ff-c806-248b1fa79a25","trusted":true},"cell_type":"code","source":"# Before training a model, you need to configure the learning process, which is done via the compile method\n\n# It receives three arguments:\n# An optimizer. This could be the string identifier of an existing optimizer , https://keras.io/optimizers/\n# A loss function. This is the objective that the model will try to minimize., https://keras.io/losses/\n# A list of metrics. For any classification problem you will want to set this to metrics=['accuracy'].  https://keras.io/metrics/\n\n\n# Note: when using the categorical_crossentropy loss, your targets should be in categorical format \n# (e.g. if you have 10 classes, the target for each sample should be a 10-dimensional vector that is all-zeros except \n# for a 1 at the index corresponding to the class of the sample).\n\n# that is why we converted out labels into vectors\n\nmodel.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Keras models are trained on Numpy arrays of input data and labels. \n# For training a model, you will typically use the  fit function\n\n# fit(self, x=None, y=None, batch_size=None, epochs=1, verbose=1, callbacks=None, validation_split=0.0, \n# validation_data=None, shuffle=True, class_weight=None, sample_weight=None, initial_epoch=0, steps_per_epoch=None, \n# validation_steps=None)\n\n# fit() function Trains the model for a fixed number of epochs (iterations on a dataset).\n\n# it returns A History object. Its History.history attribute is a record of training loss values and \n# metrics values at successive epochs, as well as validation loss values and validation metrics values (if applicable).\n\n# https://github.com/openai/baselines/issues/20\n\nhistory = model.fit(X_train, Y_train, batch_size=batch_size, epochs=nb_epoch, verbose=1, validation_data=(X_test, Y_test)) \n","execution_count":null,"outputs":[]},{"metadata":{"executionInfo":{"elapsed":1679,"status":"ok","timestamp":1532671653039,"user":{"displayName":"Applied AI Course","photoUrl":"//lh3.googleusercontent.com/-EsJzSyawCkQ/AAAAAAAAAAI/AAAAAAAAAWk/jhKHALKaHag/s50-c-k-no/photo.jpg","userId":"116292885805316472049"},"user_tz":-330},"id":"HNpwTMHcQKbw","outputId":"d4b72284-e854-4931-9e3c-194cde835168","trusted":true},"cell_type":"code","source":"score = model.evaluate(X_test, Y_test, verbose=0) \nprint('Test score:', score[0]) \nprint('Test accuracy:', score[1])\n\nfig,ax = plt.subplots(1,1)\nax.set_xlabel('epoch') ; ax.set_ylabel('Categorical Crossentropy Loss')\n\n# list of epoch numbers\nx = list(range(1,nb_epoch+1))\n\n# print(history.history.keys())\n# dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])\n# history = model_drop.fit(X_train, Y_train, batch_size=batch_size, epochs=nb_epoch, verbose=1, validation_data=(X_test, Y_test))\n\n# we will get val_loss and val_acc only when you pass the paramter validation_data\n# val_loss : validation loss\n# val_acc : validation accuracy\n\n# loss : training loss\n# acc : train accuracy\n# for each key in histrory.histrory we will have a list of length equal to number of epochs\n\nvy = history.history['val_loss']\nty = history.history['loss']\nplt_dynamic(x, vy, ty, ax)","execution_count":null,"outputs":[]},{"metadata":{"id":"eh4VWYrHQKbz"},"cell_type":"markdown","source":" <h3>  MLP + Sigmoid activation + SGDOptimizer </h3>","execution_count":null},{"metadata":{"executionInfo":{"elapsed":1062,"status":"ok","timestamp":1532672535990,"user":{"displayName":"Applied AI Course","photoUrl":"//lh3.googleusercontent.com/-EsJzSyawCkQ/AAAAAAAAAAI/AAAAAAAAAWk/jhKHALKaHag/s50-c-k-no/photo.jpg","userId":"116292885805316472049"},"user_tz":-330},"id":"7rzbngqHQKbz","outputId":"c7b2d034-a61d-4676-91ca-1e2f0c8a20c0","trusted":true},"cell_type":"code","source":"# Multilayer perceptron\n\nmodel_sigmoid = Sequential()\nmodel_sigmoid.add(Dense(512, activation='sigmoid', input_shape=(input_dim,)))\nmodel_sigmoid.add(Dense(128, activation='sigmoid'))\nmodel_sigmoid.add(Dense(output_dim, activation='softmax'))\n\nmodel_sigmoid.summary()","execution_count":null,"outputs":[]},{"metadata":{"executionInfo":{"elapsed":50953,"status":"ok","timestamp":1532672588508,"user":{"displayName":"Applied AI Course","photoUrl":"//lh3.googleusercontent.com/-EsJzSyawCkQ/AAAAAAAAAAI/AAAAAAAAAWk/jhKHALKaHag/s50-c-k-no/photo.jpg","userId":"116292885805316472049"},"user_tz":-330},"id":"48vVImSpQKb5","outputId":"989df292-6e16-4eca-cfcb-b193ca431f72","trusted":true},"cell_type":"code","source":"model_sigmoid.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])\n\nhistory = model_sigmoid.fit(X_train, Y_train, batch_size=batch_size, epochs=nb_epoch, verbose=1, validation_data=(X_test, Y_test))","execution_count":null,"outputs":[]},{"metadata":{"executionInfo":{"elapsed":2060,"status":"ok","timestamp":1532672590601,"user":{"displayName":"Applied AI Course","photoUrl":"//lh3.googleusercontent.com/-EsJzSyawCkQ/AAAAAAAAAAI/AAAAAAAAAWk/jhKHALKaHag/s50-c-k-no/photo.jpg","userId":"116292885805316472049"},"user_tz":-330},"id":"6ffjzr8hQKb8","outputId":"8caf663e-e8cc-4c1e-9fee-278ab9591642","trusted":true},"cell_type":"code","source":"score = model_sigmoid.evaluate(X_test, Y_test, verbose=0) \nprint('Test score:', score[0]) \nprint('Test accuracy:', score[1])\n\nfig,ax = plt.subplots(1,1)\nax.set_xlabel('epoch') ; ax.set_ylabel('Categorical Crossentropy Loss')\n\n# list of epoch numbers\nx = list(range(1,nb_epoch+1))\n\n# print(history.history.keys())\n# dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])\n# history = model_drop.fit(X_train, Y_train, batch_size=batch_size, epochs=nb_epoch, verbose=1, validation_data=(X_test, Y_test))\n\n# we will get val_loss and val_acc only when you pass the paramter validation_data\n# val_loss : validation loss\n# val_acc : validation accuracy\n\n# loss : training loss\n# acc : train accuracy\n# for each key in histrory.histrory we will have a list of length equal to number of epochs\n\nvy = history.history['val_loss']\nty = history.history['loss']\nplt_dynamic(x, vy, ty, ax)","execution_count":null,"outputs":[]},{"metadata":{"executionInfo":{"elapsed":2682,"status":"ok","timestamp":1532672813648,"user":{"displayName":"Applied AI Course","photoUrl":"//lh3.googleusercontent.com/-EsJzSyawCkQ/AAAAAAAAAAI/AAAAAAAAAWk/jhKHALKaHag/s50-c-k-no/photo.jpg","userId":"116292885805316472049"},"user_tz":-330},"id":"Ejn01XXdQKb-","outputId":"d301a744-1e86-4c2c-8e86-ea233376d0b7","trusted":true},"cell_type":"code","source":"w_after = model_sigmoid.get_weights()\n\nh1_w = w_after[0].flatten().reshape(-1,1)\nh2_w = w_after[2].flatten().reshape(-1,1)\nout_w = w_after[4].flatten().reshape(-1,1)\n\n\nfig = plt.figure()\nplt.title(\"Weight matrices after model trained\")\nplt.subplot(1, 3, 1)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=h1_w,color='b')\nplt.xlabel('Hidden Layer 1')\n\nplt.subplot(1, 3, 2)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=h2_w, color='r')\nplt.xlabel('Hidden Layer 2 ')\n\nplt.subplot(1, 3, 3)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=out_w,color='y')\nplt.xlabel('Output Layer ')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"ExLv_CAeQKcA"},"cell_type":"markdown","source":"<h2>MLP + Sigmoid activation + ADAM </h2>","execution_count":null},{"metadata":{"executionInfo":{"elapsed":62441,"status":"ok","timestamp":1532672891495,"user":{"displayName":"Applied AI Course","photoUrl":"//lh3.googleusercontent.com/-EsJzSyawCkQ/AAAAAAAAAAI/AAAAAAAAAWk/jhKHALKaHag/s50-c-k-no/photo.jpg","userId":"116292885805316472049"},"user_tz":-330},"id":"w9BtnNGaQKcD","outputId":"9077226e-e62a-440f-fc87-439b21e31d70","trusted":true},"cell_type":"code","source":"model_sigmoid = Sequential()\nmodel_sigmoid.add(Dense(512, activation='sigmoid', input_shape=(input_dim,)))\nmodel_sigmoid.add(Dense(128, activation='sigmoid'))\nmodel_sigmoid.add(Dense(output_dim, activation='softmax'))\n\nmodel_sigmoid.summary()\n\nmodel_sigmoid.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\nhistory = model_sigmoid.fit(X_train, Y_train, batch_size=batch_size, epochs=nb_epoch, verbose=1, validation_data=(X_test, Y_test))","execution_count":null,"outputs":[]},{"metadata":{"id":"AslSnqQ2QKcF","outputId":"3bf10216-1082-42f1-dbb5-2910e779d524","trusted":true},"cell_type":"code","source":"score = model_sigmoid.evaluate(X_test, Y_test, verbose=0) \nprint('Test score:', score[0]) \nprint('Test accuracy:', score[1])\n\nfig,ax = plt.subplots(1,1)\nax.set_xlabel('epoch') ; ax.set_ylabel('Categorical Crossentropy Loss')\n\n# list of epoch numbers\nx = list(range(1,nb_epoch+1))\n\n# print(history.history.keys())\n# dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])\n# history = model_drop.fit(X_train, Y_train, batch_size=batch_size, epochs=nb_epoch, verbose=1, validation_data=(X_test, Y_test))\n\n# we will get val_loss and val_acc only when you pass the paramter validation_data\n# val_loss : validation loss\n# val_acc : validation accuracy\n\n# loss : training loss\n# acc : train accuracy\n# for each key in histrory.histrory we will have a list of length equal to number of epochs\n\nvy = history.history['val_loss']\nty = history.history['loss']\nplt_dynamic(x, vy, ty, ax)","execution_count":null,"outputs":[]},{"metadata":{"executionInfo":{"elapsed":2602,"status":"ok","timestamp":1532674029296,"user":{"displayName":"Applied AI Course","photoUrl":"//lh3.googleusercontent.com/-EsJzSyawCkQ/AAAAAAAAAAI/AAAAAAAAAWk/jhKHALKaHag/s50-c-k-no/photo.jpg","userId":"116292885805316472049"},"user_tz":-330},"id":"HEp-AZCSQKcI","outputId":"f1416fcd-c07c-43ec-906e-2a775972e4a7","trusted":true},"cell_type":"code","source":"w_after = model_sigmoid.get_weights()\n\nh1_w = w_after[0].flatten().reshape(-1,1)\nh2_w = w_after[2].flatten().reshape(-1,1)\nout_w = w_after[4].flatten().reshape(-1,1)\n\n\nfig = plt.figure()\nplt.title(\"Weight matrices after model trained\")\nplt.subplot(1, 3, 1)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=h1_w,color='b')\nplt.xlabel('Hidden Layer 1')\n\nplt.subplot(1, 3, 2)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=h2_w, color='r')\nplt.xlabel('Hidden Layer 2 ')\n\nplt.subplot(1, 3, 3)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=out_w,color='y')\nplt.xlabel('Output Layer ')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"bRVrzqvSQKcL"},"cell_type":"markdown","source":"<h2> MLP + ReLU +SGD </h2>","execution_count":null},{"metadata":{"id":"O8lG2jzmQKcM","outputId":"72a68439-c243-4569-86b9-f636757c60f7","trusted":true},"cell_type":"code","source":"# Multilayer perceptron\n\n# https://arxiv.org/pdf/1707.09725.pdf#page=95\n# for relu layers\n# If we sample weights from a normal distribution N(0,σ) we satisfy this condition with σ=√(2/(ni). \n# h1 =>  σ=√(2/(fan_in) = 0.062  => N(0,σ) = N(0,0.062)\n# h2 =>  σ=√(2/(fan_in) = 0.125  => N(0,σ) = N(0,0.125)\n# out =>  σ=√(2/(fan_in+1) = 0.120  => N(0,σ) = N(0,0.120)\n\nmodel_relu = Sequential()\nmodel_relu.add(Dense(512, activation='relu', input_shape=(input_dim,), kernel_initializer=RandomNormal(mean=0.0, stddev=0.062, seed=None)))\nmodel_relu.add(Dense(128, activation='relu', kernel_initializer=RandomNormal(mean=0.0, stddev=0.125, seed=None)) )\nmodel_relu.add(Dense(output_dim, activation='softmax'))\n\nmodel_relu.summary()","execution_count":null,"outputs":[]},{"metadata":{"id":"QhtoviFKQKcP","outputId":"e25de732-19a7-4178-d784-411170e6d810","trusted":true},"cell_type":"code","source":"model_relu.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])\n\nhistory = model_relu.fit(X_train, Y_train, batch_size=batch_size, epochs=nb_epoch, verbose=1, validation_data=(X_test, Y_test))","execution_count":null,"outputs":[]},{"metadata":{"id":"0rKzoKxgQKcS","outputId":"7893c859-5d0d-4cbb-ef05-d7dd854e325c","trusted":true},"cell_type":"code","source":"score = model_relu.evaluate(X_test, Y_test, verbose=0) \nprint('Test score:', score[0]) \nprint('Test accuracy:', score[1])\n\nfig,ax = plt.subplots(1,1)\nax.set_xlabel('epoch') ; ax.set_ylabel('Categorical Crossentropy Loss')\n\n# list of epoch numbers\nx = list(range(1,nb_epoch+1))\n\n# print(history.history.keys())\n# dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])\n# history = model_drop.fit(X_train, Y_train, batch_size=batch_size, epochs=nb_epoch, verbose=1, validation_data=(X_test, Y_test))\n\n# we will get val_loss and val_acc only when you pass the paramter validation_data\n# val_loss : validation loss\n# val_acc : validation accuracy\n\n# loss : training loss\n# acc : train accuracy\n# for each key in histrory.histrory we will have a list of length equal to number of epochs\n\nvy = history.history['val_loss']\nty = history.history['loss']\nplt_dynamic(x, vy, ty, ax)","execution_count":null,"outputs":[]},{"metadata":{"id":"z386BnBlQKcV","outputId":"dbfc21b4-4c17-4c29-86f0-9ea53c26d112","trusted":true},"cell_type":"code","source":"w_after = model_relu.get_weights()\n\nh1_w = w_after[0].flatten().reshape(-1,1)\nh2_w = w_after[2].flatten().reshape(-1,1)\nout_w = w_after[4].flatten().reshape(-1,1)\n\n\nfig = plt.figure()\nplt.title(\"Weight matrices after model trained\")\nplt.subplot(1, 3, 1)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=h1_w,color='b')\nplt.xlabel('Hidden Layer 1')\n\nplt.subplot(1, 3, 2)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=h2_w, color='r')\nplt.xlabel('Hidden Layer 2 ')\n\nplt.subplot(1, 3, 3)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=out_w,color='y')\nplt.xlabel('Output Layer ')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"9ccmfiVGQKcc"},"cell_type":"markdown","source":"<h2> MLP + ReLU + ADAM </h2>","execution_count":null},{"metadata":{"id":"-CtwrinkQKcc","outputId":"c46c7912-e77b-48c5-a506-b90cc13ba9f2","trusted":true},"cell_type":"code","source":"model_relu = Sequential()\nmodel_relu.add(Dense(512, activation='relu', input_shape=(input_dim,), kernel_initializer=RandomNormal(mean=0.0, stddev=0.062, seed=None)))\nmodel_relu.add(Dense(128, activation='relu', kernel_initializer=RandomNormal(mean=0.0, stddev=0.125, seed=None)) )\nmodel_relu.add(Dense(output_dim, activation='softmax'))\n\nprint(model_relu.summary())\n\nmodel_relu.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\nhistory = model_relu.fit(X_train, Y_train, batch_size=batch_size, epochs=nb_epoch, verbose=1, validation_data=(X_test, Y_test))","execution_count":null,"outputs":[]},{"metadata":{"id":"niCdh2TUQKce","outputId":"9620cc68-260a-4f70-e2a2-2296d55cade6","trusted":true},"cell_type":"code","source":"score = model_relu.evaluate(X_test, Y_test, verbose=0) \nprint('Test score:', score[0]) \nprint('Test accuracy:', score[1])\n\nfig,ax = plt.subplots(1,1)\nax.set_xlabel('epoch') ; ax.set_ylabel('Categorical Crossentropy Loss')\n\n# list of epoch numbers\nx = list(range(1,nb_epoch+1))\n\n# print(history.history.keys())\n# dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])\n# history = model_drop.fit(X_train, Y_train, batch_size=batch_size, epochs=nb_epoch, verbose=1, validation_data=(X_test, Y_test))\n\n# we will get val_loss and val_acc only when you pass the paramter validation_data\n# val_loss : validation loss\n# val_acc : validation accuracy\n\n# loss : training loss\n# acc : train accuracy\n# for each key in histrory.histrory we will have a list of length equal to number of epochs\n\n\nvy = history.history['val_loss']\nty = history.history['loss']\nplt_dynamic(x, vy, ty, ax)","execution_count":null,"outputs":[]},{"metadata":{"id":"PDtptoSnQKcf","outputId":"4fd88cc4-6452-4f28-970f-bd7a0c5fe462","trusted":true},"cell_type":"code","source":"w_after = model_relu.get_weights()\n\nh1_w = w_after[0].flatten().reshape(-1,1)\nh2_w = w_after[2].flatten().reshape(-1,1)\nout_w = w_after[4].flatten().reshape(-1,1)\n\n\nfig = plt.figure()\nplt.title(\"Weight matrices after model trained\")\nplt.subplot(1, 3, 1)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=h1_w,color='b')\nplt.xlabel('Hidden Layer 1')\n\nplt.subplot(1, 3, 2)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=h2_w, color='r')\nplt.xlabel('Hidden Layer 2 ')\n\nplt.subplot(1, 3, 3)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=out_w,color='y')\nplt.xlabel('Output Layer ')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"TjduBU7hQKch"},"cell_type":"markdown","source":"<h2> MLP + Batch-Norm on hidden Layers + AdamOptimizer </2>","execution_count":null},{"metadata":{"id":"KGHnPnoTQKci","outputId":"8abd1dfb-b838-47f8-9e9b-0a4a757bfd8a","trusted":true},"cell_type":"code","source":"# Multilayer perceptron\n\n# https://intoli.com/blog/neural-network-initialization/ \n# If we sample weights from a normal distribution N(0,σ) we satisfy this condition with σ=√(2/(ni+ni+1). \n# h1 =>  σ=√(2/(ni+ni+1) = 0.039  => N(0,σ) = N(0,0.039)\n# h2 =>  σ=√(2/(ni+ni+1) = 0.055  => N(0,σ) = N(0,0.055)\n# h1 =>  σ=√(2/(ni+ni+1) = 0.120  => N(0,σ) = N(0,0.120)\n\nfrom keras.layers.normalization import BatchNormalization\n\nmodel_batch = Sequential()\n\nmodel_batch.add(Dense(512, activation='sigmoid', input_shape=(input_dim,), kernel_initializer=RandomNormal(mean=0.0, stddev=0.039, seed=None)))\nmodel_batch.add(BatchNormalization())\n\nmodel_batch.add(Dense(128, activation='sigmoid', kernel_initializer=RandomNormal(mean=0.0, stddev=0.55, seed=None)) )\nmodel_batch.add(BatchNormalization())\n\nmodel_batch.add(Dense(output_dim, activation='softmax'))\n\n\nmodel_batch.summary()","execution_count":null,"outputs":[]},{"metadata":{"id":"yMPo5fcUQKcm","outputId":"345fa9de-3920-49c3-b03e-2ba99ddfb7c7","trusted":true},"cell_type":"code","source":"model_batch.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\nhistory = model_batch.fit(X_train, Y_train, batch_size=batch_size, epochs=nb_epoch, verbose=1, validation_data=(X_test, Y_test))","execution_count":null,"outputs":[]},{"metadata":{"id":"HThyBcHUQKcn","outputId":"3da2f50e-c25f-4903-da11-d294ec1a63b8","trusted":true},"cell_type":"code","source":"score = model_batch.evaluate(X_test, Y_test, verbose=0) \nprint('Test score:', score[0]) \nprint('Test accuracy:', score[1])\n\nfig,ax = plt.subplots(1,1)\nax.set_xlabel('epoch') ; ax.set_ylabel('Categorical Crossentropy Loss')\n\n# list of epoch numbers\nx = list(range(1,nb_epoch+1))\n\n# print(history.history.keys())\n# dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])\n# history = model_drop.fit(X_train, Y_train, batch_size=batch_size, epochs=nb_epoch, verbose=1, validation_data=(X_test, Y_test))\n\n# we will get val_loss and val_acc only when you pass the paramter validation_data\n# val_loss : validation loss\n# val_acc : validation accuracy\n\n# loss : training loss\n# acc : train accuracy\n# for each key in histrory.histrory we will have a list of length equal to number of epochs\n\nvy = history.history['val_loss']\nty = history.history['loss']\nplt_dynamic(x, vy, ty, ax)","execution_count":null,"outputs":[]},{"metadata":{"id":"Ilj0es2yQKcp","outputId":"55ee6f5e-c253-4cf2-fe1e-4d9be82e1c88","trusted":true},"cell_type":"code","source":"w_after = model_batch.get_weights()\n\nh1_w = w_after[0].flatten().reshape(-1,1)\nh2_w = w_after[2].flatten().reshape(-1,1)\nout_w = w_after[4].flatten().reshape(-1,1)\n\n\nfig = plt.figure()\nplt.title(\"Weight matrices after model trained\")\nplt.subplot(1, 3, 1)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=h1_w,color='b')\nplt.xlabel('Hidden Layer 1')\n\nplt.subplot(1, 3, 2)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=h2_w, color='r')\nplt.xlabel('Hidden Layer 2 ')\n\nplt.subplot(1, 3, 3)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=out_w,color='y')\nplt.xlabel('Output Layer ')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"IWYP8-3pQKct"},"cell_type":"markdown","source":"<h2> 5. MLP + Dropout + AdamOptimizer </h2>","execution_count":null},{"metadata":{"id":"DJj0hu0-QKct","outputId":"674bedfc-e833-4fb7-f0d6-908fb9906c36","trusted":true},"cell_type":"code","source":"# https://stackoverflow.com/questions/34716454/where-do-i-call-the-batchnormalization-function-in-keras\n\nfrom keras.layers import Dropout\n\nmodel_drop = Sequential()\n\nmodel_drop.add(Dense(512, activation='sigmoid', input_shape=(input_dim,), kernel_initializer=RandomNormal(mean=0.0, stddev=0.039, seed=None)))\nmodel_drop.add(BatchNormalization())\nmodel_drop.add(Dropout(0.5))\n\nmodel_drop.add(Dense(128, activation='sigmoid', kernel_initializer=RandomNormal(mean=0.0, stddev=0.55, seed=None)) )\nmodel_drop.add(BatchNormalization())\nmodel_drop.add(Dropout(0.5))\n\nmodel_drop.add(Dense(output_dim, activation='softmax'))\n\n\nmodel_drop.summary()","execution_count":null,"outputs":[]},{"metadata":{"id":"J5pc4W_9QKcw","outputId":"fc6c250a-49f9-46d7-de24-ae1977ea2ccc","trusted":true},"cell_type":"code","source":"model_drop.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\nhistory = model_drop.fit(X_train, Y_train, batch_size=batch_size, epochs=nb_epoch, verbose=1, validation_data=(X_test, Y_test))","execution_count":null,"outputs":[]},{"metadata":{"id":"i5WcfQRFQKc0","outputId":"67d7cea2-34bc-41f4-a62e-53710bd6100b","trusted":true},"cell_type":"code","source":"score = model_drop.evaluate(X_test, Y_test, verbose=0) \nprint('Test score:', score[0]) \nprint('Test accuracy:', score[1])\n\nfig,ax = plt.subplots(1,1)\nax.set_xlabel('epoch') ; ax.set_ylabel('Categorical Crossentropy Loss')\n\n# list of epoch numbers\nx = list(range(1,nb_epoch+1))\n\n# print(history.history.keys())\n# dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])\n# history = model_drop.fit(X_train, Y_train, batch_size=batch_size, epochs=nb_epoch, verbose=1, validation_data=(X_test, Y_test))\n\n# we will get val_loss and val_acc only when you pass the paramter validation_data\n# val_loss : validation loss\n# val_acc : validation accuracy\n\n# loss : training loss\n# acc : train accuracy\n# for each key in histrory.histrory we will have a list of length equal to number of epochs\n\nvy = history.history['val_loss']\nty = history.history['loss']\nplt_dynamic(x, vy, ty, ax)","execution_count":null,"outputs":[]},{"metadata":{"id":"U2N2u7xoQKc2","outputId":"fb67995c-2b23-4967-c398-3a621d1b63f4","trusted":true},"cell_type":"code","source":"w_after = model_drop.get_weights()\n\nh1_w = w_after[0].flatten().reshape(-1,1)\nh2_w = w_after[2].flatten().reshape(-1,1)\nout_w = w_after[4].flatten().reshape(-1,1)\n\n\nfig = plt.figure()\nplt.title(\"Weight matrices after model trained\")\nplt.subplot(1, 3, 1)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=h1_w,color='b')\nplt.xlabel('Hidden Layer 1')\n\nplt.subplot(1, 3, 2)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=h2_w, color='r')\nplt.xlabel('Hidden Layer 2 ')\n\nplt.subplot(1, 3, 3)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=out_w,color='y')\nplt.xlabel('Output Layer ')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"ayBTjaZGQKc6"},"cell_type":"markdown","source":"<h2> Hyper-parameter tuning of Keras models using Sklearn </h2>","execution_count":null},{"metadata":{"id":"8k1L9fZcQKc6","trusted":true},"cell_type":"code","source":"from keras.optimizers import Adam,RMSprop,SGD\ndef best_hyperparameters(activ):\n\n    model = Sequential()\n    model.add(Dense(512, activation=activ, input_shape=(input_dim,), kernel_initializer=RandomNormal(mean=0.0, stddev=0.062, seed=None)))\n    model.add(Dense(128, activation=activ, kernel_initializer=RandomNormal(mean=0.0, stddev=0.125, seed=None)) )\n    model.add(Dense(output_dim, activation='softmax'))\n\n\n    model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"id":"Mj1klnwWQKc7","trusted":true},"cell_type":"code","source":"# https://machinelearningmastery.com/grid-search-hyperparameters-deep-learning-models-python-keras/\n\nactiv = ['sigmoid','relu']\n\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.model_selection import GridSearchCV\n\nmodel = KerasClassifier(build_fn=best_hyperparameters, epochs=nb_epoch, batch_size=batch_size, verbose=0)\nparam_grid = dict(activ=activ)\n\n# if you are using CPU\n# grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\n# if you are using GPU dont use the n_jobs parameter\n\ngrid = GridSearchCV(estimator=model, param_grid=param_grid)\ngrid_result = grid.fit(X_train, Y_train)","execution_count":null,"outputs":[]},{"metadata":{"id":"9UiW9qkmQKc8","outputId":"625e907e-e347-4826-953d-e09c26a2e397","trusted":true},"cell_type":"code","source":"print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}