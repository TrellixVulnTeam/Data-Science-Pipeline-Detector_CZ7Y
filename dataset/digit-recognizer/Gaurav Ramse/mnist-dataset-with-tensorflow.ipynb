{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\npd.set_option('display.max_columns', None)\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-25T12:19:39.339679Z","iopub.execute_input":"2021-07-25T12:19:39.340091Z","iopub.status.idle":"2021-07-25T12:19:39.352574Z","shell.execute_reply.started":"2021-07-25T12:19:39.340003Z","shell.execute_reply":"2021-07-25T12:19:39.351313Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/digit-recognizer/train.csv')\n\ndf.sample(5)","metadata":{"execution":{"iopub.status.busy":"2021-07-25T12:19:39.355727Z","iopub.execute_input":"2021-07-25T12:19:39.356165Z","iopub.status.idle":"2021-07-25T12:19:42.925128Z","shell.execute_reply.started":"2021-07-25T12:19:39.356122Z","shell.execute_reply":"2021-07-25T12:19:42.921137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = pd.read_csv('/kaggle/input/digit-recognizer/test.csv')","metadata":{"execution":{"iopub.status.busy":"2021-07-25T12:19:42.926889Z","iopub.execute_input":"2021-07-25T12:19:42.92733Z","iopub.status.idle":"2021-07-25T12:19:44.842258Z","shell.execute_reply.started":"2021-07-25T12:19:42.927288Z","shell.execute_reply":"2021-07-25T12:19:44.841361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**The classsification algorithm that we can use**  \n1) Logistic Regression \\\n2) SVC \\\n3) Addaboost \\\n4) SGDClassifier \\\n3) Descision Tree \\\n4) Random Forest \\\n5) Ensemble Methods \\\n6) BernolliNB \\\n7) PassiveaggressiveClassifier \\\n8) LGBM \\\n9) BaggingClassifier \\\n10) CalibratedClassifierCV","metadata":{}},{"cell_type":"code","source":"# Let's find shape of our data, So we have around 42000 images with 786 pixel\ndf.shape","metadata":{"execution":{"iopub.status.busy":"2021-07-25T12:19:53.162543Z","iopub.execute_input":"2021-07-25T12:19:53.163114Z","iopub.status.idle":"2021-07-25T12:19:53.167504Z","shell.execute_reply.started":"2021-07-25T12:19:53.163064Z","shell.execute_reply":"2021-07-25T12:19:53.166861Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2021-07-25T12:19:54.538275Z","iopub.execute_input":"2021-07-25T12:19:54.538812Z","iopub.status.idle":"2021-07-25T12:19:55.429926Z","shell.execute_reply.started":"2021-07-25T12:19:54.538768Z","shell.execute_reply":"2021-07-25T12:19:55.428973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lets try to find out how sparse our dataset is, So we can see that data is eually distrubuted among all classes, So\n# We don't need to balance classes\n \nsns.histplot(data = df, x = 'label')","metadata":{"execution":{"iopub.status.busy":"2021-07-25T12:19:56.042878Z","iopub.execute_input":"2021-07-25T12:19:56.043225Z","iopub.status.idle":"2021-07-25T12:19:56.296024Z","shell.execute_reply.started":"2021-07-25T12:19:56.043196Z","shell.execute_reply":"2021-07-25T12:19:56.295065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Prepare dta to feed the ML algorithms","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(df.drop('label', axis = 1), df['label'])","metadata":{"execution":{"iopub.status.busy":"2021-07-25T12:19:57.530389Z","iopub.execute_input":"2021-07-25T12:19:57.530914Z","iopub.status.idle":"2021-07-25T12:19:58.110089Z","shell.execute_reply.started":"2021-07-25T12:19:57.53086Z","shell.execute_reply":"2021-07-25T12:19:58.108885Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\nAs our data is in the range 0-255, To scale our data into 0-1.  \nWe have to scale our dataset, So we have multiple ways to do that,  \nI will mention most used one  \n1) MinMaxScaler  \n$ \\frac {value - min(feature)}{max(feature) - min(feature)}$  \n\n2) StandardScaler  \n$ \\frac {value - mean}{standard_deviation} $\n\nSo we don't need to use this scaling function , Just devide by 255 will work in our case","metadata":{}},{"cell_type":"code","source":"x_train = x_train/255.0\nx_test = x_test/255.0","metadata":{"execution":{"iopub.status.busy":"2021-07-25T12:19:58.242876Z","iopub.execute_input":"2021-07-25T12:19:58.24322Z","iopub.status.idle":"2021-07-25T12:19:58.308718Z","shell.execute_reply.started":"2021-07-25T12:19:58.243191Z","shell.execute_reply":"2021-07-25T12:19:58.307622Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Classification Algorithms\n","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.model_selection import GridSearchCV","metadata":{"execution":{"iopub.status.busy":"2021-07-25T12:20:00.882814Z","iopub.execute_input":"2021-07-25T12:20:00.883161Z","iopub.status.idle":"2021-07-25T12:20:00.96754Z","shell.execute_reply.started":"2021-07-25T12:20:00.883132Z","shell.execute_reply":"2021-07-25T12:20:00.96652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Tensorflow","metadata":{}},{"cell_type":"code","source":"import tensorflow\nfrom tensorflow import keras\n","metadata":{"execution":{"iopub.status.busy":"2021-07-25T12:20:40.49134Z","iopub.execute_input":"2021-07-25T12:20:40.492085Z","iopub.status.idle":"2021-07-25T12:20:46.158599Z","shell.execute_reply.started":"2021-07-25T12:20:40.492036Z","shell.execute_reply":"2021-07-25T12:20:46.157538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train, x_test, y_train, y_test = train_test_split(df.drop('label', axis = 1), df['label'])\nx_train_valid, x_test_valid, y_train_valid, y_test_valid  = train_test_split(x_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2021-07-25T12:21:18.093673Z","iopub.execute_input":"2021-07-25T12:21:18.094192Z","iopub.status.idle":"2021-07-25T12:21:18.663538Z","shell.execute_reply.started":"2021-07-25T12:21:18.094157Z","shell.execute_reply":"2021-07-25T12:21:18.662797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train = x_train/255.0\nx_test = x_test/255.0\nx_train_valid = x_train_valid/255.0\nx_test_valid = x_test_valid/255.0","metadata":{"execution":{"iopub.status.busy":"2021-07-25T12:21:18.664629Z","iopub.execute_input":"2021-07-25T12:21:18.665048Z","iopub.status.idle":"2021-07-25T12:21:18.776422Z","shell.execute_reply.started":"2021-07-25T12:21:18.665016Z","shell.execute_reply":"2021-07-25T12:21:18.775366Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = keras.models.Sequential([\n keras.layers.Dense(400, activation = 'relu', input_shape = x_train.shape[1:]),\n keras.layers.BatchNormalization(),\n keras.layers.Dense(200, activation=\"elu\", kernel_initializer=\"he_normal\"),\n keras.layers.BatchNormalization(),\n keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n keras.layers.BatchNormalization(),\n keras.layers.Dense(10, activation=\"softmax\")\n])\nmodel.compile(\n    optimizer =  keras.optimizers.SGD(),\n    loss = keras.losses.sparse_categorical_crossentropy,\n    metrics=[keras.metrics.SparseCategoricalAccuracy()]\n)\n\nhistory = model.fit(x_train, y_train, epochs = 20, validation_data=(x_train_valid, y_train_valid))","metadata":{"execution":{"iopub.status.busy":"2021-07-25T12:21:18.778178Z","iopub.execute_input":"2021-07-25T12:21:18.778599Z","iopub.status.idle":"2021-07-25T12:23:10.510797Z","shell.execute_reply.started":"2021-07-25T12:21:18.778563Z","shell.execute_reply":"2021-07-25T12:23:10.509897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = test/255.0","metadata":{"execution":{"iopub.status.busy":"2021-07-25T12:23:10.512271Z","iopub.execute_input":"2021-07-25T12:23:10.512545Z","iopub.status.idle":"2021-07-25T12:23:10.557725Z","shell.execute_reply.started":"2021-07-25T12:23:10.512517Z","shell.execute_reply":"2021-07-25T12:23:10.556764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred = np.argmax(model.predict(test), axis = 1)","metadata":{"execution":{"iopub.status.busy":"2021-07-25T12:23:10.559274Z","iopub.execute_input":"2021-07-25T12:23:10.559557Z","iopub.status.idle":"2021-07-25T12:23:12.641174Z","shell.execute_reply.started":"2021-07-25T12:23:10.559528Z","shell.execute_reply":"2021-07-25T12:23:12.640028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub = pd.DataFrame()\nsub['ImageId'] = list(test.index)[1:] + [28000]\nsub['Label'] = pred\nsub.to_csv(r'/kaggle/working/sub_tens.csv', index = False)","metadata":{"execution":{"iopub.status.busy":"2021-07-25T12:23:12.64244Z","iopub.execute_input":"2021-07-25T12:23:12.642686Z","iopub.status.idle":"2021-07-25T12:23:12.702074Z","shell.execute_reply.started":"2021-07-25T12:23:12.642662Z","shell.execute_reply":"2021-07-25T12:23:12.701062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}