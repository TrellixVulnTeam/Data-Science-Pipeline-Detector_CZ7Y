{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Summary\n\nThis note is an MNIST digit recognizer implemented in numpy from scratch.\n\nThis is a simple demonstration mainly for pedagogical purposes, which shows the basic workflow of a machine learning algorithm using a simple feedforward neural network. The derivative at the backpropagation stage is computed explicitly through the chain rule.\n\n* The model is a 3-layer feedforward neural network (FNN), in which the input layer has 784 units, and the 256-unit hidden layer is activated by ReLU, while the output layer is activated by softmax function to produce a discrete probability distribution for each input. \n* The loss function, model hypothesis function, and the gradient of the loss function are all implemented from ground-up in numpy in a highly vectorized fashion (no FOR loops).\n* The training is through a standard gradient descent with step size adaptively changing by Root Mean Square prop (RMSprop), and there is no cross-validation set reserved nor model averaging for simplicity.\n\nThe code is vectorized and is adapted from the Softmax regression and Neural Network lectures used in [UCI Math 10](https://github.com/scaomath/UCI-Math10). \n\nCaveat lector: fluency in linear algebra and multivariate calculus.\n\n\n#### References:\n* [Stanford Deep Learning tutorial in MATLAB](http://ufldl.stanford.edu/tutorial/supervised/MultiLayerNeuralNetworks/).\n* [Learning PyTorch with examples](https://pytorch.org/tutorials/beginner/pytorch_with_examples.html).\n* [An overview of gradient descent optimization algorithms](http://ruder.io/optimizing-gradient-descent/)."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n%matplotlib inline","execution_count":47,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data input"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\npath = os.listdir(\"../input\")\nprint(path)","execution_count":6,"outputs":[{"output_type":"stream","text":"['train.csv', 'sample_submission.csv', 'test.csv']\n","name":"stdout"}]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# Read the data\ntrain_data = pd.read_csv('../input/train.csv')\ntest_data = pd.read_csv(\"../input/test.csv\")","execution_count":7,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# Set up the data\ny_train = train_data['label'].values\nX_train = train_data.drop(columns=['label']).values/255\nX_test = test_data.values/255","execution_count":48,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Some examples of the input data\nWe randomly choose 10 samples from the training set and visualize it."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(2,5, figsize=(12,5))\naxes = axes.flatten()\nidx = np.random.randint(0,42000,size=10)\nfor i in range(10):\n    axes[i].imshow(X_train[idx[i],:].reshape(28,28), cmap='gray')\n    axes[i].axis('off') # hide the axes ticks\n    axes[i].set_title(str(int(y_train[idx[i]])), color= 'black', fontsize=25)\nplt.show()","execution_count":49,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 864x360 with 10 Axes>","image/png":"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\n"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"# Network structures\n\n<img src=\"https://faculty.sites.uci.edu/shuhaocao/files/2019/06/nn-3layers.png\" alt=\"drawing\" width=\"700\"/>\n\nThe figure above is a simplication of the neural network used in this example. The circles labeled \"+1\" are the bias units. Layer 1 is the input layer, and Layer 3 is the output layer. The middle layer, Layer 2, is the hidden layer.\n\nThe neural network in the figure above has 2 input units (not counting the bias unit), 3 hidden units, and 1 output unit. In this actual computation below, the input layer has 784 units, the hidden layer has 256 units, and the output layers has 10 units ($K =10$ classes).\n\nThe weight matrix $W^{(0)}$ mapping input $\\mathbf{x}$ from the input layer (Layer 1) to the hidden layer (Layer 2) is of shape `(784,256)` together with a `(256,)` bias. Then $\\mathbf{a}$ is the activation from the hidden layer (Layer 2) can be written as:\n$$\n\\mathbf{a} = \\mathrm{ReLU}\\big((W^{(0)})^{\\top}\\mathbf{x} + \\mathbf{b}\\big),\n$$\nwhere the ReLU activation function is $\\mathrm{ReLU}(z) = \\max(z,0)$ and can be implemented in a vectorized fashion as follows."},{"metadata":{"trusted":true},"cell_type":"code","source":"# relu activation function\n# THE fastest vectorized implementation for ReLU\ndef relu(x):\n    x[x<0]=0\n    return x","execution_count":50,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Softmax activation, prediction, and the loss function\n\nFrom the hidden layer (Layer 2) to the output layer (layer 3), the weight matrix $W^{(1)}$ is of shape `(256,10)`, the form of which is as follows:\n$$\nW^{(1)} =\n\\begin{pmatrix}\n| & | & | & | \\\\\n\\boldsymbol{\\theta}_1 & \\boldsymbol{\\theta}_2 & \\cdots & \\boldsymbol{\\theta}_K \\\\\n| & | & | & |\n\\end{pmatrix},\n$$\nwhich maps the activation from Layer 2 to Layer 3 (output layer), and there is no bias because a constant can be freely added to the activation without changing the final output. \n\nAt the last layer, a softmax activation is used, which can be written as follows combining the weights matrix $W^{(1)}$ that maps the activation $\\mathbf{a}$ from the hidden layer to output layer:\n$$\nP\\big(y = k \\;| \\;\\mathbf{a}; W^{(1)}\\big) = \\sigma_k(\\mathbf{a}; W^{(1)}) := \\frac{\\exp\\big(\\boldsymbol{\\theta}^{\\top}_k \\mathbf{a} \\big)}\n{\\sum_{j=1}^K \\exp\\big(\\boldsymbol{\\theta}^{\\top}_j \\mathbf{a} \\big)}.\n$$\n$\\{P\\big(y = k \\;| \\;\\mathbf{a}; W^{(1)}\\big)\\}_{k=1}^K$ is the probability distribution of our model, which estimates the probability of the input $\\mathbf{x}$'s label $y$ is of class $k$. We denote this distribution by a vector \n$$\\boldsymbol{\\sigma}:= (\\sigma_1,\\dots, \\sigma_K)^{\\top}.$$\nWe hope that this estimate is as close as possible to the true probability: $1_{\\{y=k\\}}$, that is $1$ if the sample $\\mathbf{x}$ is in the $k$-th class and 0 otherwise. \n\nLastly, our prediction $\\hat{y}$ for sample $\\mathbf{x}$ can be made by choosing the class with the highest probability:\n$$\n\\hat{y} = \\operatorname{argmax}_{k=1,\\dots,K}  P\\big(y = k \\;| \\;\\mathbf{a}; W^{(1)}\\big). \\tag{$\\ast$}\n$$\n\nDenote the label of the $i$-th input as $y^{(i)}$, and then the sample-wise loss function is the cross entropy measuring the difference of the distribution of this model function above with the true one $1_{\\{y^{(i)}=k\\}}$: denote $W = (W^{(0)}, W^{(1)})$, $b = (\\mathbf{b})$, let $\\mathbf{a}^{(i)}$ be the activation for the $i$-th sample in the hidden layer (Layer 2),\n$$\nJ_i:= J(W,b;\\mathbf{x}^{(i)},y^{(i)}) := - \\sum_{k=1}^{K} \\left\\{  1_{\\left\\{y^{(i)} = k\\right\\} }\n\\log P\\big(y^{(i)} = k \\;| \\;\\mathbf{a}^{(i)}; W^{(1)}\\big)\\right\\}. \\tag{1}\n$$\n\nDenote the data sample matrix $X := (\\mathbf{x}^{(1)}, \\dots, \\mathbf{x}^{(N)})^{\\top}$, its label vector as $\\mathbf{y} := (y^{(1)}, \\dots, y^{(N)})$, and then the final loss has an extra $L^2$-regularization term for the weight matrices (not for bias): \n$$\nL(W,b; X, \\mathbf{y}) := \\frac{1}{N}\\sum_{i=1}^{N} J_i  + \\frac{\\alpha}{2} \\Big(\\|W^{(0)}\\|^2 + \\|W^{(1)}\\|^2\\Big),\n\\tag{2}\n$$\nwhere $\\alpha>0$ is a hyper-parameter determining the strength of the regularization, the bigger the $\\alpha$ is, the smaller the magnitudes of the weights will be after training."},{"metadata":{"trusted":true},"cell_type":"code","source":"def h(X,W,b):\n    '''\n    Hypothesis function: simple FNN with 1 hidden layer\n    Layer 1: input\n    Layer 2: hidden layer, with a size implied by the arguments W[0], b\n    Layer 3: output layer, with a size implied by the arguments W[1]\n    '''\n    # layer 1 = input layer\n    a1 = X\n    # layer 1 (input layer) -> layer 2 (hidden layer)\n    z1 = np.matmul(X, W[0]) + b[0]\n    \n    # add one more layer\n    \n    # layer 2 activation\n    a2 = relu(z1)\n    # layer 2 (hidden layer) -> layer 3 (output layer)\n    z2 = np.matmul(a2, W[1])\n    s = np.exp(z2)\n    total = np.sum(s, axis=1).reshape(-1,1)\n    sigma = s/total\n    # the output is a probability for each sample\n    return sigma","execution_count":51,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def softmax(X_in,weights):\n    '''\n    Un-used cell for demo\n    activation function for the last FC layer: softmax function \n    Output: K probabilities represent an estimate of P(y=k|X_in;weights) for k=1,...,K\n    the weights has shape (n, K)\n    n: the number of features X_in has\n    n = X_in.shape[1]\n    K: the number of classes\n    K = 10\n    '''\n    \n    s = np.exp(np.matmul(X_in,weights))\n    total = np.sum(s, axis=1).reshape(-1,1)\n    return s / total","execution_count":52,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def loss(y_pred,y_true):\n    '''\n    Loss function: cross entropy with an L^2 regularization\n    y_true: ground truth, of shape (N, )\n    y_pred: prediction made by the model, of shape (N, K) \n    N: number of samples in the batch\n    K: global variable, number of classes\n    '''\n    global K \n    K = 10\n    N = len(y_true)\n    # loss_sample stores the cross entropy for each sample in X\n    # convert y_true from labels to one-hot-vector encoding\n    y_true_one_hot_vec = (y_true[:,np.newaxis] == np.arange(K))\n    loss_sample = (np.log(y_pred) * y_true_one_hot_vec).sum(axis=1)\n    # loss_sample is a dimension (N,) array\n    # for the final loss, we need take the average\n    return -np.mean(loss_sample)","execution_count":53,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Backpropagation (Chain rule)\n\nThe derivative of the cross entropy $J$ in (1), for a single sample and its label $(\\mathbf{x}, y)$ , with respect to the weights and the bias is computed using the following procedure:\n> **Step 1**: Forward pass: computing the activations $\\mathbf{a} = (a_1,\\dots, a_{n_2})$ from the hidden layer (Layer 2), and $\\boldsymbol{\\sigma} = (\\sigma_1,\\dots, \\sigma_K)$ from the output layer (Layer 3). \n>\n> **Step 2**: Derivatives for $W^{(1)}$: recall that $W^{(1)} = (\\boldsymbol{\\theta}_1 ,\\cdots,  \\boldsymbol{\\theta}_K)$ and denote \n$$\\mathbf{z}^{(2)} = \\big(z^{(2)}_1, \\dots, z^{(2)}_K\\big)  = (W^{(1)})^{\\top}\\mathbf{a} =\n\\big(\\boldsymbol{\\theta}^{\\top}_1 \\mathbf{a} ,\\cdots,  \\boldsymbol{\\theta}^{\\top}_K \\mathbf{a}\\big),$$ \nfor the $k$-th output unit in the output layer (Layer 3), then\n$$\n\\delta^{(2)}_k\n:= \\frac{\\partial J}{\\partial z_k^{(2)}} = \\Big\\{  P\\big(y = k \\;| \\;\\mathbf{a}; W^{(1)}\\big)- 1_{\\{ y = k\\}} \\Big\\} = \\sigma_k - 1_{\\{ y = k\\}}\n$$\nand \n$$\n\\frac{\\partial J}{\\partial \\boldsymbol{\\theta}_k}= \\frac{\\partial J}{\\partial z_k^{(2)}}\\frac{\\partial z_k^{(2)}}{\\partial \\boldsymbol{\\theta}_k} = \\delta^{(2)}_k \\mathbf{a}.\n$$\n>\n> **Step 3**: Derivatives for $W^{(0)}$, $\\mathbf{b}$: recall that $W^{(0)} = (\\boldsymbol{w}_1 ,\\cdots,  \\boldsymbol{w}_{n_2})$, $\\mathbf{b} = (b_1,\\dots, b_{n_2})$, where $n_2$ is the number of units in the hidden layer (Layer 2), and denote \n$$\\mathbf{z}^{(1)} = (z_1^{(1)}, \\dots, z_{n_2}^{(1)})  = (W^{(0)})^{\\top}\\mathbf{x} + \\mathbf{b} =\n\\big(\\mathbf{w}^{\\top}_1 \\mathbf{x} +b_1 ,\\cdots,  \\mathbf{w}^{\\top}_{n_2} \\mathbf{x} + b_{n_2}\\big),$$ \nfor each node $i$ in the hidden layer (Layer $2$), $i=1,\\dots, n_2$, then\n$$\\delta^{(1)}_i : = \\frac{\\partial J}{\\partial z^{(1)}_i}  =\n\\frac{\\partial J}{\\partial a_i} \n\\frac{\\partial a_i}{\\partial z^{(1)}_i}=\n\\frac{\\partial J}{\\partial \\mathbf{z}^{(2)}}\n\\cdot\\left(\\frac{\\partial \\mathbf{z}^{(2)}}{\\partial a_i} \n\\frac{\\partial a_i}{\\partial z^{(1)}_i}\\right)\n\\\\\n=\\left( \\sum_{k=1}^{K} \\frac{\\partial J}{\\partial {z}^{(2)}_k}\n\\frac{\\partial {z}^{(2)}_k}{\\partial a_i}  \\right) f'(z^{(1)}_i) = \\left( \\sum_{k=1}^{K} w_{ki} \\delta^{(2)}_k \\right) 1_{\\{z^{(1)}_i\\; > 0\\}},\n$$\nwhere $1_{\\{z^{(1)}_i\\; > 0\\}}$ is ReLU activation $f$'s (weak) derivative, and the partial derivative of the $k$-th component (before activated by the softmax) in the output layer ${z}^{(2)}_k$ with respect to the $i$-th activation $a_i$ from the hidden layer is the weight $w^{(1)}_{ki}$. Thus\n>\n$$\n\\frac{\\partial J}{\\partial w_{ji}}  = x_j \\delta_i^{(1)} ,\\;\n\\frac{\\partial J}{\\partial b_{i}} = \\delta_i^{(1)}, \\;\\text{ and }\\;\n\\frac{\\partial J}{\\partial \\mathbf{w}_{i}}  = \\delta_i^{(1)}\\mathbf{x} ,\\;\n\\frac{\\partial J}{\\partial \\mathbf{b}} = \\boldsymbol{\\delta}^{(1)}.\n$$"},{"metadata":{"trusted":true},"cell_type":"code","source":"def backprop(W,b,X,y,alpha=1e-4):\n    '''\n    Step 1: explicit forward pass h(X;W,b)\n    Step 2: backpropagation for dW and db\n    '''\n    K = 10\n    N = X.shape[0]\n    \n    ### Step 1:\n    # layer 1 = input layer\n    a1 = X\n    # layer 1 (input layer) -> layer 2 (hidden layer)\n    z1 = np.matmul(X, W[0]) + b[0]\n    # layer 2 activation\n    a2 = relu(z1)\n    \n    # one more layer\n    \n    # layer 2 (hidden layer) -> layer 3 (output layer)\n    z2 = np.matmul(a2, W[1])\n    s = np.exp(z2)\n    total = np.sum(s, axis=1).reshape(-1,1)\n    sigma = s/total\n    \n    ### Step 2:\n    \n    # layer 2->layer 3 weights' derivative\n    # delta2 is \\partial L/partial z2, of shape (N,K)\n    y_one_hot_vec = (y[:,np.newaxis] == np.arange(K))\n    delta2 = (sigma - y_one_hot_vec)\n    grad_W1 = np.matmul(a2.T, delta2)\n    \n    # layer 1->layer 2 weights' derivative\n    # delta1 is \\partial a2/partial z1\n    # layer 2 activation's (weak) derivative is 1*(z1>0)\n    delta1 = np.matmul(delta2, W[1].T)*(z1>0)\n    grad_W0 = np.matmul(X.T, delta1)\n    \n    # Student project: extra layer of derivative\n    \n    # no derivative for layer 1\n    \n    # the alpha part is the derivative for the regularization\n    # regularization = 0.5*alpha*(np.sum(W[1]**2) + np.sum(W[0]**2))\n    \n    \n    dW = [grad_W0/N + alpha*W[0], grad_W1/N + alpha*W[1]]\n    db = [np.mean(delta1, axis=0)]\n    # dW[0] is W[0]'s derivative, and dW[1] is W[1]'s derivative; similar for db\n    return dW, db","execution_count":54,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Hyper-parameters and network initialization"},{"metadata":{"trusted":true},"cell_type":"code","source":"eta = 5e-1\nalpha = 1e-6 # regularization\ngamma = 0.99 # RMSprop\neps = 1e-3 # RMSprop\nnum_iter = 2000 # number of iterations of gradient descent\nn_H = 256 # number of neurons in the hidden layer\nn = X_train.shape[1] # number of pixels in an image\nK = 10","execution_count":55,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# initialization\nnp.random.seed(1127)\nW = [1e-1*np.random.randn(n, n_H), 1e-1*np.random.randn(n_H, K)]\nb = [np.random.randn(n_H)]","execution_count":56,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Gradient Descent: training of the network\n\nIn the training, we use a GD-variant of the RMSprop: for $\\mathbf{w}$ which stands for the parameter vector in our model\n> Choose $\\mathbf{w}_0$, $\\eta$, $\\gamma$, $\\epsilon$, and let $g_{-1} = 1$ <br><br>\n>    For $k=0,1,2, \\cdots, M$<br><br>\n>    &nbsp;&nbsp;&nbsp;&nbsp;  $g_{k} = \\gamma g_{k-1} + (1 - \\gamma)\\, \\left|\\partial_{\\mathbf{w}} L (\\mathbf{w}_k)\\right|^2$<br><br>\n>    &nbsp;&nbsp;&nbsp;&nbsp;    $\\displaystyle\\mathbf{w}_{k+1} =  \\mathbf{w}_k -  \\frac{\\eta} {\\sqrt{g_{k}+ \\epsilon}} \\partial_{\\mathbf{w}} L(\\mathbf{w}_k)$  \n\n### Remark: \nThe training takes a while since we use the gradient descent for all samples."},{"metadata":{"trusted":true,"_kg_hide-output":false},"cell_type":"code","source":"%%time\ngW0 = gW1 = gb0 = 1\n\nfor i in range(num_iter):\n    dW, db = backprop(W,b,X_train,y_train,alpha)\n    \n    gW0 = gamma*gW0 + (1-gamma)*np.sum(dW[0]**2)\n    etaW0 = eta/np.sqrt(gW0 + eps)\n    W[0] -= etaW0 * dW[0]\n    \n    gW1 = gamma*gW1 + (1-gamma)*np.sum(dW[1]**2)\n    etaW1 = eta/np.sqrt(gW1 + eps)\n    W[1] -= etaW1 * dW[1]\n    \n    gb0 = gamma*gb0 + (1-gamma)*np.sum(db[0]**2)\n    etab0 = eta/np.sqrt(gb0 + eps)\n    b[0] -= etab0 * db[0]\n    \n    if i % 500 == 0:\n        # sanity check 1\n        y_pred = h(X_train,W,b)\n        print(\"Cross-entropy loss after\", i+1, \"iterations is {:.8}\".format(\n              loss(y_pred,y_train)))\n        print(\"Training accuracy after\", i+1, \"iterations is {:.4%}\".format( \n              np.mean(np.argmax(y_pred, axis=1)== y_train)))\n        \n        # sanity check 2\n        print(\"gW0={:.4f} gW1={:.4f} gb0={:.4f}\\netaW0={:.4f} etaW1={:.4f} etab0={:.4f}\"\n              .format(gW0, gW1, gb0, etaW0, etaW1, etab0))\n        \n        # sanity check 3\n        print(\"|dW0|={:.5f} |dW1|={:.5f} |db0|={:.5f}\"\n             .format(np.linalg.norm(dW[0]), np.linalg.norm(dW[1]), np.linalg.norm(db[0])), \"\\n\")\n        \n        # reset RMSprop\n        gW0 = gW1 = gb0 = 1\n\ny_pred_final = h(X_train,W,b)\nprint(\"Final cross-entropy loss is {:.8}\".format(loss(y_pred_final,y_train)))\nprint(\"Final training accuracy is {:.4%}\".format(np.mean(np.argmax(y_pred_final, axis=1)== y_train)))","execution_count":57,"outputs":[{"output_type":"stream","text":"cross-entropy loss after 1 iterations is 7.3264114\ntraining accuracy after 1 iterations is 37.3524%\ngW0=1.0715 gW1=1.2174 gb0=0.9919\netaW0=0.4828 etaW1=0.4530 etab0=0.5018\n|dW0|=2.85527 |dW1|=4.76880 |db0|=0.43260 \n\n","name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n","\u001b[0;32m<ipython-input-54-c003a704ed53>\u001b[0m in \u001b[0;36mbackprop\u001b[0;34m(W, b, X, y, alpha)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;31m# delta1 is \\partial a2/partial z1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;31m# layer 2 activation's (weak) derivative is 1*(z1>0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0mdelta1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelta2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz1\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0mgrad_W0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"metadata":{},"cell_type":"markdown","source":"# Predictions for testing data\nThe prediction labels are generated by $(\\ast)$."},{"metadata":{"trusted":true},"cell_type":"code","source":"# predictions\ny_pred_test = np.argmax(h(X_test,W,b), axis=1)","execution_count":58,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Generating submission using pandas for grading\nsubmission = pd.DataFrame({'ImageId': range(1,len(X_test)+1) ,'Label': y_pred_test })\nsubmission.to_csv(\"simplemnist_result.csv\",index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}