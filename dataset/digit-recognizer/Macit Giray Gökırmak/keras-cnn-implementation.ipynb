{"cells":[{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.optimizers import Adadelta # I believe this is better optimizer for our case\nfrom keras.preprocessing.image import ImageDataGenerator # to augmenting our images for increasing accuracy\nfrom sklearn.model_selection import train_test_split # to split our train data into train and validation sets\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nnp.random.seed(13) # My lucky number","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"030b1ebf761aa450ce18b253813433ca5293c2fc"},"cell_type":"code","source":"num_classes = 10 # We have 10 digits to identify\nbatch_size = 128 # Handle 128 pictures at each round\nepochs = 10 # 10 Epoch is enough for %99.4 Accuracy!!!!\nimg_rows, img_cols = 28, 28 # Image dimensions 28 pixels in height&width\ninput_shape = (img_rows, img_cols,1) # We'll use this while building layers","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dc20a3a5e6d491e98be7d0ff3aaa6eec7bdc123b"},"cell_type":"code","source":"# Load some date to rock'n roll\ntrain = pd.read_csv(\"../input/train.csv\")\ntest = pd.read_csv(\"../input/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"23e65adaf0ef381576c5597aebb6ce5dcc466174"},"cell_type":"code","source":"# Drop the label from the data and move it to real label part\ny_train = train[\"label\"]\nx_train = train.drop(labels = [\"label\"],axis = 1 )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fc3b21df164c213db201b317ba453c8914dd2200"},"cell_type":"code","source":"# Normalize both sets\nx_train /= 255\ntest /= 255","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"922cb2c73388a6ec4b368b032f133b2c4c9783c6"},"cell_type":"code","source":"print(x_train.shape[0], 'train samples')\nprint(test.shape[0], 'test samples')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4ac625840bb33a39580405985955d9076c003d3c"},"cell_type":"code","source":"# Images should be in shape of height,width and color channel so it will be 28x28x1\nx_train = x_train.values.reshape(-1,img_rows,img_cols,1)\ntest = test.values.reshape(-1,img_rows,img_cols,1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"057065fd2bf1e11bdb9885b746c27452ee21505c"},"cell_type":"code","source":"# Class vectors needs to be binary so we use \"to_catogorical\" function of keras utilities for one-hot-encoding\ny_train = keras.utils.to_categorical(y_train, num_classes = num_classes)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d1965d067618cee1c84f6ef75461c25840b4b12c"},"cell_type":"code","source":"# Lets split our train set into train and validation test sets with my lucky number 13 :)\nx_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size = 0.1, random_state=13)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1e271e0d37535c7c401a873578ad3dd19b51b133"},"cell_type":"code","source":"model = Sequential()\n\n# Add convolutional layer consisting of 32 filters and shape of 5x5 with ReLU activation\n# We want to preserve more information for followin layers so we start using padding\n# 'Same' padding tries to pad evenly left and right, but if the amount of columns to be added is odd, it will add the extra column to the right\nmodel.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', activation ='relu', input_shape = input_shape))\nBatchNormalization(axis=-1)\n# Add convolutional layer consisting of 32 filters and shape of 5x5 with ReLU activation\nmodel.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', activation ='relu'))\n\n# Add Maxpool layer with the shape of 2x2\nmodel.add(MaxPooling2D(pool_size=(2,2)))\nBatchNormalization(axis=-1)\n# Dropping %25 of neurons\nmodel.add(Dropout(0.25))\n\n# Add convolutional layer consisting of 64 filters and shape of 3x3 with ReLU activation\nmodel.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', activation ='relu'))\nBatchNormalization(axis=-1)\n# Add convolutional layer consisting of 64 filters and shape of 3x3 with ReLU activation\nmodel.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', activation ='relu'))\n# Add convolutional layer consisting of 64 filters and shape of 3x3 with ReLU activation\nmodel.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', activation ='relu'))\n# Add Maxpool layer with the shape of 2x2 and strides for controlling convolutions over input volume\nmodel.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n# Dropping %25 of neurons\nmodel.add(Dropout(0.25))\n\n# To be able to merge into fully connected layer we have to flatten\nmodel.add(Flatten())\nBatchNormalization()\n# Adding fully connected layer with 256 ReLU activated neurons\nmodel.add(Dense(256, activation = \"relu\"))\nBatchNormalization()\n# Dropping %50 of neurons\nmodel.add(Dropout(0.5))\n\n# Lets add softmax activated neurons as much as number of classes\nmodel.add(Dense(num_classes, activation = \"softmax\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d66b221d286725527050b22d6942ab59693ed6fe"},"cell_type":"code","source":"# Adadelta (my favorite) inorder to get over %99 before 5th epoch\noptimizer = Adadelta()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7f35afd55620e744260b19f75f513fd5df90e4b9"},"cell_type":"code","source":"# Compile the model with loss and metrics\nmodel.compile(optimizer = optimizer , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3b5468b524407441d9731b87e97bb010b7cded69"},"cell_type":"code","source":"# Generate batches of tensor image data with real-time data augmentation more detail: https://keras.io/preprocessing/image/\ndatagen = ImageDataGenerator(rotation_range=10, zoom_range = 0.1, width_shift_range=0.1, height_shift_range=0.1)\ndatagen.fit(x_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8b7f141a76736640c752920eefa6062bcade357d"},"cell_type":"code","source":"# Start model training with the batch size\nhistory = model.fit_generator(datagen.flow(x_train,y_train, batch_size=batch_size),\n                              epochs = epochs, validation_data = (x_test,y_test),\n                              verbose = 2, steps_per_epoch=x_train.shape[0] // batch_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"64720957674b4dcd0d5305e48cc0a35e405b0d0a"},"cell_type":"code","source":"# Evaluate accuracy and loss over validation set\nscore = model.evaluate(x_test, y_test, verbose=0)\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}