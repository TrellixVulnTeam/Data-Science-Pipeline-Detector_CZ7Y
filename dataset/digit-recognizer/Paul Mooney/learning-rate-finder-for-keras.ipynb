{"cells":[{"metadata":{"_uuid":"7040ecb4e98f5db76b347204e765824a1d6e040b","_cell_guid":"21a599fa-3c08-458d-b85e-553c468b38ec"},"cell_type":"markdown","source":"**Learning Rate Finder for Keras**"},{"metadata":{"_uuid":"c765c25d7c4f70c8cc0582e8d8f5353fd5bfecd8","_cell_guid":"f0d175f4-f4e9-454f-a9e5-5b7421bec789"},"cell_type":"markdown","source":"Here I demonstrate a method for determining the optimal learning rate for your neural network.\n*  https://github.com/surmenok/keras_lr_finder\n*  https://arxiv.org/abs/1506.01186"},{"metadata":{"_uuid":"0b3e66f6be250dd737349268a26a5289a6ba8fe0","_cell_guid":"959634f0-0cc2-49a9-852f-791a26444a39"},"cell_type":"markdown","source":"*Step 1.1: Import Modules*"},{"metadata":{"_uuid":"f1a4c1cbc5dd6adf5377e968e04c59a80feb5466","_kg_hide-output":true,"_kg_hide-input":true,"_cell_guid":"08d5ae69-76e5-40fa-86c6-965a7188fe42","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\nimport itertools\nfrom PIL import Image\nimport sklearn as sklearn\nfrom sklearn.metrics import confusion_matrix\nfrom keras.callbacks import Callback, EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\nfrom sklearn import model_selection\nfrom sklearn.model_selection import train_test_split, learning_curve\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.neural_network import MLPClassifier as MLPC\nfrom sklearn import model_selection\nimport tensorflow as tf\nimport keras\nfrom keras import backend as K\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D\nfrom keras import initializers, layers, models\nfrom keras.utils import to_categorical\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras import callbacks\nfrom keras.utils.vis_utils import plot_model\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b9af084c1e13887bcfbd07f3a2153a5c47de101f","_cell_guid":"eff63fa3-0879-42b1-870d-b8868b0c4bfd"},"cell_type":"markdown","source":"*Step 1.2: Load Data*"},{"metadata":{"_uuid":"caaf6659b6f83c1690bc4ddbedfa8c4b2389c48f","_kg_hide-output":false,"_kg_hide-input":true,"_cell_guid":"ed48e3a0-787f-4898-8080-7927ec783f23","trusted":true,"collapsed":true},"cell_type":"code","source":"data = pd.read_csv('../input/train.csv')\ntestingData = pd.read_csv('../input/test.csv')\nX = data.drop(\"label\",axis=1).values\ny = data.label.values\n\ndef describeDataset(features,labels):\n    print(\"\\n'X' shape: %s.\"%(features.shape,))\n    print(\"\\n'y' shape: %s.\"%(labels.shape,))\n    print(\"\\nUnique elements in y: %s\"%(np.unique(y)))\ndescribeDataset(X,y)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0d710885f9d072258dc7a24d9ca9f8597c97f2d5","_cell_guid":"2a858973-1cc8-420e-be45-6b391b04366a"},"cell_type":"markdown","source":"*Step 1.3: Display Data*"},{"metadata":{"_uuid":"67180e31b36de1fbd9d33c0faa1ba179bacb35e8","_kg_hide-input":true,"_cell_guid":"b0632fe1-6a5f-442b-a530-5c1389924a61","trusted":true,"collapsed":true},"cell_type":"code","source":"def displayMNIST(flatData,labels):\n    \"\"\"Display MNIST data\"\"\"\n    flatData2 = data.drop(\"label\",axis=1).values\n    X2 = np.insert(flatData2,0,1,axis=1)\n    figure,image = plt.subplots(1,10, figsize=(10,10))\n    for i in range(10):\n        tenImages = np.random.choice(X2.shape[0], 10)\n        image[i].imshow(X2[tenImages,1:].reshape(-1,28))\n        image[i].axis('off')\ndisplayMNIST(X,y)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7dc216fe019489fae4471c6066d3311ba4ce07e2","_kg_hide-input":true,"_cell_guid":"d0da18b6-928d-4c7c-a226-361894dff2b8","trusted":true,"collapsed":true},"cell_type":"code","source":"def displayMNIST2(flatData,labels):\n    \"\"\" Displays 10 handwritten digis and 10 classification labels \"\"\"\n    figure,image = plt.subplots(1,10, figsize=(10,10))\n    for i in range(10):\n        image[i].imshow(flatData[i].reshape((28,28)))\n        image[i].axis('off')\n        image[i].set_title(labels[i])\ndisplayMNIST2(X,y)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ac821c7afafba2cd19fb6785228543d436fd55ec","_kg_hide-input":true,"_cell_guid":"cd50a825-9884-4b92-a1f7-cb5b9ca3a51b","trusted":true,"collapsed":true},"cell_type":"code","source":"X3 = X[1].reshape((28,28))\ndef pixelIntensities(flatData,labels):\n    \"\"\"Plot histogram of Pixel Intensities\"\"\"\n    plt.figure(figsize=(10,5))\n    plt.subplot(1,2,1)\n    plt.imshow(flatData)\n    plt.axis('off')\n    plt.title(labels[1])\n    histo = plt.subplot(1,2,2)\n    histo.set_ylabel('Count')\n    histo.set_xlabel('Pixel Intensity')\n    n_bins = 30\n    plt.hist(flatData[:,:].flatten(), bins= n_bins, lw = 0, color='r', alpha=0.5);\npixelIntensities(X3,y)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"eb363edc84a3812149b364977a9188ae26867aea","_cell_guid":"3f02dfd8-9610-46e5-9136-8fcb9cfa53ad"},"cell_type":"markdown","source":"*Step 2: Preprocess Data*"},{"metadata":{"_uuid":"ff52a3373831e4b4016c0d040fcde8dff82a8e7b","_kg_hide-input":true,"_cell_guid":"70680c25-f460-4e71-b06e-09c20c16cb19","trusted":true,"collapsed":true},"cell_type":"code","source":"X = data.iloc[:,1:] # everything but the first column  (pixel values)\ny = data.iloc[:,:1] # first column only  (label/answer)\n\nxValues = X\nyValues = y.values.ravel()\nx_train, x_test, y_train, y_test = train_test_split(xValues, yValues, test_size=0.2, random_state=23)\n\nx_train = x_train/ 256 # normalize values between zero and one\nx_test = x_test / 256\n\n# Reshape images from 1D flattened images to normal 2D images\nimg_rows, img_cols = 28, 28\nif K.image_data_format() == 'channels_first':\n    x_trainReshaped = x_train.values.reshape(x_train.shape[0], 1, img_rows, img_cols)\n    x_testReshaped = x_test.values.reshape(x_test.shape[0], 1, img_rows, img_cols)\n    input_shape = (1, img_rows, img_cols)\nelse:\n    x_trainReshaped = x_train.values.reshape(x_train.shape[0], img_rows, img_cols, 1)\n    x_testReshaped = x_test.values.reshape(x_test.shape[0], img_rows, img_cols, 1)\n    input_shape = (img_rows, img_cols, 1)\n\nx_trainReshaped = x_trainReshaped.astype('float32')\nx_testReshaped = x_testReshaped.astype('float32')\n\n# convert class vectors to binary class matrices\nnum_classes = 10\ny_trainReshaped = keras.utils.to_categorical(y_train, num_classes)\ny_testReshaped = keras.utils.to_categorical(y_test, num_classes)\n    \ndescribeDataset(x_trainReshaped,y_trainReshaped)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7bb52aaecef901eb70415d4ba4d9d1ba3e827e44","_cell_guid":"cd191d52-0a8c-4645-948e-aa3d1d0e97ad"},"cell_type":"markdown","source":"*Step 3: Define Helper Functions*"},{"metadata":{"_uuid":"0fe082333cd26b6a0052647574de5eb677862ac7","collapsed":true,"_kg_hide-output":true,"_kg_hide-input":true,"_cell_guid":"a852c1b7-b210-425a-99b7-533b5674ab82","trusted":true},"cell_type":"code","source":"class MetricsCheckpoint(Callback):\n    \"\"\"Callback that saves metrics after each epoch\"\"\"\n    def __init__(self, savepath):\n        super(MetricsCheckpoint, self).__init__()\n        self.savepath = savepath\n        self.history = {}\n    def on_epoch_end(self, epoch, logs=None):\n        for k, v in logs.items():\n            self.history.setdefault(k, []).append(v)\n        np.save(self.savepath, self.history)\n\ndef plotKerasLearningCurve():\n    plt.figure(figsize=(10,5))\n    metrics = np.load('logs.npy')[()]\n    filt = ['acc'] # try to add 'loss' to see the loss learning curve\n    for k in filter(lambda x : np.any([kk in x for kk in filt]), metrics.keys()):\n        l = np.array(metrics[k])\n        plt.plot(l, c= 'r' if 'val' not in k else 'b', label='val' if 'val' in k else 'train')\n        x = np.argmin(l) if 'loss' in k else np.argmax(l)\n        y = l[x]\n        plt.scatter(x,y, lw=0, alpha=0.25, s=100, c='r' if 'val' not in k else 'b')\n        plt.text(x, y, '{} = {:.4f}'.format(x,y), size='15', color= 'r' if 'val' not in k else 'b')   \n    plt.legend(loc=4)\n    plt.axis([0, None, None, None]);\n    plt.grid()\n    plt.xlabel('Number of epochs')\n\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    plt.figure(figsize = (5,5))\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=90)\n    plt.yticks(tick_marks, classes)\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n\ndict_characters = {0: '0', 1: '1', 2: '2', \n        3: '3', 4: '4', 5: '5', 6: '6', 7:'7',\n        8: '8', 9: '9'}\n\nfrom matplotlib import pyplot as plt\nimport math\nfrom keras.callbacks import LambdaCallback\nimport keras.backend as K\n\n\nclass LRFinder:\n    \"\"\"\n    Plots the change of the loss function of a Keras model when the learning rate is exponentially increasing.\n    See for details:\n    https://towardsdatascience.com/estimating-optimal-learning-rate-for-a-deep-neural-network-ce32f2556ce0\n    \"\"\"\n    def __init__(self, model):\n        self.model = model\n        self.losses = []\n        self.lrs = []\n        self.best_loss = 1e9\n\n    def on_batch_end(self, batch, logs):\n        # Log the learning rate\n        lr = K.get_value(self.model.optimizer.lr)\n        self.lrs.append(lr)\n\n        # Log the loss\n        loss = logs['loss']\n        self.losses.append(loss)\n\n        # Check whether the loss got too large or NaN\n        if math.isnan(loss) or loss > self.best_loss * 4:\n            self.model.stop_training = True\n            return\n\n        if loss < self.best_loss:\n            self.best_loss = loss\n\n        # Increase the learning rate for the next batch\n        lr *= self.lr_mult\n        K.set_value(self.model.optimizer.lr, lr)\n\n    def find(self, x_train, y_train, start_lr, end_lr, batch_size=64, epochs=1):\n        num_batches = epochs * x_train.shape[0] / batch_size\n        self.lr_mult = (end_lr / start_lr) ** (1 / num_batches)\n\n        # Save weights into a file\n        self.model.save_weights('tmp.h5')\n\n        # Remember the original learning rate\n        original_lr = K.get_value(self.model.optimizer.lr)\n\n        # Set the initial learning rate\n        K.set_value(self.model.optimizer.lr, start_lr)\n\n        callback = LambdaCallback(on_batch_end=lambda batch, logs: self.on_batch_end(batch, logs))\n\n        self.model.fit(x_train, y_train,\n                        batch_size=batch_size, epochs=epochs,\n                        callbacks=[callback])\n\n        # Restore the weights to the state before model fitting\n        self.model.load_weights('tmp.h5')\n\n        # Restore the original learning rate\n        K.set_value(self.model.optimizer.lr, original_lr)\n\n    def plot_loss(self, n_skip_beginning=10, n_skip_end=5):\n        \"\"\"\n        Plots the loss.\n        Parameters:\n            n_skip_beginning - number of batches to skip on the left.\n            n_skip_end - number of batches to skip on the right.\n        \"\"\"\n        plt.ylabel(\"loss\")\n        plt.xlabel(\"learning rate (log scale)\")\n        plt.plot(self.lrs[n_skip_beginning:-n_skip_end], self.losses[n_skip_beginning:-n_skip_end])\n        plt.xscale('log')\n\n    def plot_loss_change(self, sma=1, n_skip_beginning=10, n_skip_end=5, y_lim=(-0.01, 0.01)):\n        \"\"\"\n        Plots rate of change of the loss function.\n        Parameters:\n            sma - number of batches for simple moving average to smooth out the curve.\n            n_skip_beginning - number of batches to skip on the left.\n            n_skip_end - number of batches to skip on the right.\n            y_lim - limits for the y axis.\n        \"\"\"\n        assert sma >= 1\n        derivatives = [0] * sma\n        for i in range(sma, len(self.lrs)):\n            derivative = (self.losses[i] - self.losses[i - sma]) / sma\n            derivatives.append(derivative)\n\n        plt.ylabel(\"rate of loss change\")\n        plt.xlabel(\"learning rate (log scale)\")\n        plt.plot(self.lrs[n_skip_beginning:-n_skip_end], derivatives[n_skip_beginning:-n_skip_end])\n        plt.xscale('log')\n        plt.ylim(y_lim)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e1dc9c8b275638db6b155c57b92dbf367a52a7a1","_cell_guid":"35013357-4ed2-47a3-9890-8d2864d393cf"},"cell_type":"markdown","source":"*Step 6: Determine Optimal Learning Rate*"},{"metadata":{"_uuid":"76f13b25f6af176081880a7c1af3581ff733d600","_kg_hide-input":true,"_cell_guid":"ba80ac0e-3915-44b2-95da-71fcb1d529b9","trusted":true,"collapsed":true},"cell_type":"code","source":"def determineLearningRate(xtrain,ytrain,xtest,ytest):    \n    batch_size = 128\n    num_classes = 10\n    epochs = 5\n    model = Sequential()\n    model.add(Conv2D(32, kernel_size=(3, 3),\n                     activation='relu',\n                     input_shape=input_shape))\n    model.add(Conv2D(64, (3, 3), activation='relu'))\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(Dropout(0.25))\n    model.add(Flatten())\n    model.add(Dense(128, activation='relu'))\n    model.add(Dropout(0.5))\n    model.add(Dense(num_classes, activation='softmax'))\n    model.compile(loss=keras.losses.categorical_crossentropy,\n                  optimizer=keras.optimizers.SGD(),\n                  metrics=['accuracy'])\n    lr_finder = LRFinder(model)\n    lr_finder.find(xtrain,ytrain, start_lr=0.0000001, end_lr=100, batch_size=512, epochs=5)\n    lr_finder.plot_loss(n_skip_beginning=20, n_skip_end=5)\n    plt.show()\n    return model\ndetermineLearningRate(x_trainReshaped, y_trainReshaped,x_testReshaped,y_testReshaped)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0f1148c746ba5c5df69c8e310eb71dd97ba7a81a","_cell_guid":"2c8d26f1-125c-4e74-90e8-ff8e9b358f7b"},"cell_type":"markdown","source":"It looks like a learning rate of 0.1 is best for the SGD optimizer."},{"metadata":{"_uuid":"6f893202280b3e65c8c635a46295bc85789f49bd","_cell_guid":"57272190-5d08-4a04-841e-40e59c41538b"},"cell_type":"markdown","source":"*Step 7: Evaluate Model*"},{"metadata":{"_uuid":"f83e9373a00ef1109c299a8197067517f967da74","_kg_hide-input":true,"scrolled":true,"_cell_guid":"93e6ec92-525b-4b3c-8c9e-6ea5d2cbd57e","trusted":true,"collapsed":true},"cell_type":"code","source":"learningRate=0.1\ndef runKerasCNN(xtrain,ytrain,xtest,ytest):    \n    batch_size = 128\n    num_classes = 10\n    epochs = 5\n    model = Sequential()\n    model.add(Conv2D(32, kernel_size=(3, 3),\n                     activation='relu',\n                     input_shape=input_shape))\n    model.add(Conv2D(64, (3, 3), activation='relu'))\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(Dropout(0.25))\n    model.add(Flatten())\n    model.add(Dense(128, activation='relu'))\n    model.add(Dropout(0.5))\n    model.add(Dense(num_classes, activation='softmax'))\n    model.compile(loss=keras.losses.categorical_crossentropy,\n                  optimizer=keras.optimizers.SGD(lr=learningRate),\n                  metrics=['accuracy'])\n    model.fit(xtrain,ytrain,\n              batch_size=batch_size,\n              verbose=1,\n              epochs=epochs,\n              validation_data=(xtest,ytest),callbacks = [MetricsCheckpoint('logs')])\n    score = model.evaluate(xtest,ytest, verbose=0)\n    print('\\nKeras Convolutional Neural Network - accuracy:', score[1],\"\\n\")\n    Y_pred = model.predict(xtest)\n    print('\\n', sklearn.metrics.classification_report(np.where(ytest > 0)[1], np.argmax(Y_pred, axis=1), target_names=list(dict_characters.values())), sep='')    \n    Y_pred_classes = np.argmax(Y_pred,axis = 1) \n    Y_true = np.argmax(ytest,axis = 1) \n    confusion_mtx = confusion_matrix(Y_true, Y_pred_classes) \n    plot_confusion_matrix(confusion_mtx, classes = list(dict_characters.values())) \n    plt.show()\n    plotKerasLearningCurve()\n    plt.show()\n    return model\nrunKerasCNN(x_trainReshaped, y_trainReshaped,x_testReshaped, y_testReshaped)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2b95c0c0b3d08ed4fa7d89cb915b7d691071c454","collapsed":true,"_cell_guid":"e62cf72c-cd6a-47d3-beca-8329e4237b20","trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}