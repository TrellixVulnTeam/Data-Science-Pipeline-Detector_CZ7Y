{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-12-25T00:27:38.954852Z","iopub.execute_input":"2021-12-25T00:27:38.955126Z","iopub.status.idle":"2021-12-25T00:27:38.9872Z","shell.execute_reply.started":"2021-12-25T00:27:38.955096Z","shell.execute_reply":"2021-12-25T00:27:38.986517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**I am a student from HUST, [Huazhong University of Science and Technology](https://en.wikipedia.org/wiki/Huazhong_University_of_Science_and_Technology). This work will be my homework for the lesson，called AI introduction by Associate Professor Yan Jin.**","metadata":{}},{"cell_type":"markdown","source":"# source codes","metadata":{}},{"cell_type":"code","source":"!pwd","metadata":{"execution":{"iopub.status.busy":"2021-12-25T00:27:39.148431Z","iopub.execute_input":"2021-12-25T00:27:39.148997Z","iopub.status.idle":"2021-12-25T00:27:39.92817Z","shell.execute_reply.started":"2021-12-25T00:27:39.148962Z","shell.execute_reply":"2021-12-25T00:27:39.927475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cd ../input/mnist-dataset","metadata":{"execution":{"iopub.status.busy":"2021-12-25T00:27:39.930049Z","iopub.execute_input":"2021-12-25T00:27:39.930506Z","iopub.status.idle":"2021-12-25T00:27:39.936243Z","shell.execute_reply.started":"2021-12-25T00:27:39.930476Z","shell.execute_reply":"2021-12-25T00:27:39.935744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%matplotlib inline\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport struct\nimport os\nimport math","metadata":{"execution":{"iopub.status.busy":"2021-12-25T00:27:39.937584Z","iopub.execute_input":"2021-12-25T00:27:39.938168Z","iopub.status.idle":"2021-12-25T00:27:39.955597Z","shell.execute_reply.started":"2021-12-25T00:27:39.938127Z","shell.execute_reply":"2021-12-25T00:27:39.954689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 读取数据\n\n# 参考博客：https://blog.csdn.net/u013597931/article/details/80099243\ndef load_mnist_train(path, kind='train'):    \n    labels_path = os.path.join(path+'%s-labels-idx1-ubyte/'% kind,'%s-labels-idx1-ubyte'% kind)\n    images_path = os.path.join(path+'%s-images-idx3-ubyte/'% kind,'%s-images-idx3-ubyte'% kind)    # 训练集的路径读取\n    with open(labels_path, 'rb') as lbpath:          \n        magic, n = struct.unpack('>II',lbpath.read(8))\n        labels = np.fromfile(lbpath,dtype=np.uint8)\n    with open(images_path, 'rb') as imgpath:\n        magic, num, rows, cols = struct.unpack('>IIII',imgpath.read(16))\n        images = np.fromfile(imgpath,dtype=np.uint8).reshape(len(labels), 784)\n    return images, labels\ndef load_mnist_test(path, kind='t10k'):\n    labels_path = os.path.join(path+'%s-labels-idx1-ubyte/'% kind,'%s-labels-idx1-ubyte'% kind)\n    images_path = os.path.join(path+'%s-images-idx3-ubyte/'% kind,'%s-images-idx3-ubyte'% kind)\n    with open(labels_path, 'rb') as lbpath:\n        magic, n = struct.unpack('>II',lbpath.read(8))\n        labels = np.fromfile(lbpath,dtype=np.uint8)\n    with open(images_path, 'rb') as imgpath:\n        magic, num, rows, cols = struct.unpack('>IIII',imgpath.read(16))\n        images = np.fromfile(imgpath,dtype=np.uint8).reshape(len(labels), 784)\n    return images, labels   \n\npath='/kaggle/input/mnist-dataset/'\ntrain_images,train_labels=load_mnist_train(path)    # 训练数据\ntest_images,test_labels=load_mnist_test(path)      # 测试数据\n\nfig=plt.figure(figsize=(8,8))\nfig.subplots_adjust(left=0,right=1,bottom=0,top=1,hspace=0.05,wspace=0.05)\nfor i in range(30):\n    images = np.reshape(train_images[i], [28,28])\n    ax=fig.add_subplot(6,5,i+1,xticks=[],yticks=[])\n    ax.imshow(images,cmap=plt.cm.binary,interpolation='nearest')\n    ax.text(0,7,str(train_labels[i]))\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-25T00:27:39.957175Z","iopub.execute_input":"2021-12-25T00:27:39.957944Z","iopub.status.idle":"2021-12-25T00:27:41.633906Z","shell.execute_reply.started":"2021-12-25T00:27:39.957895Z","shell.execute_reply":"2021-12-25T00:27:41.633078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 数据规模\nprint(train_images.shape)\nprint(test_images.shape)\n\nprint(train_labels.shape)\nprint(test_labels.shape)","metadata":{"execution":{"iopub.status.busy":"2021-12-25T00:27:41.635638Z","iopub.execute_input":"2021-12-25T00:27:41.635933Z","iopub.status.idle":"2021-12-25T00:27:41.642186Z","shell.execute_reply.started":"2021-12-25T00:27:41.635894Z","shell.execute_reply":"2021-12-25T00:27:41.641395Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 数据归一化\ntrain_images = train_images / 255.0\ntest_images = test_images / 255.0","metadata":{"execution":{"iopub.status.busy":"2021-12-25T00:27:41.643355Z","iopub.execute_input":"2021-12-25T00:27:41.643586Z","iopub.status.idle":"2021-12-25T00:27:41.845233Z","shell.execute_reply.started":"2021-12-25T00:27:41.64354Z","shell.execute_reply":"2021-12-25T00:27:41.844445Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 抽取一个小批量\nBatch_size = 10000\nbatch_train = np.random.choice(np.arange(train_images.shape[0]), Batch_size)\nbatch_test = np.random.choice(np.arange(test_images.shape[0]), 2000)\ntrain_images0 = train_images[batch_train]\ntrain_labels0 = train_labels[batch_train]\ntest_images0 = test_images[batch_test]\ntest_labels0 = test_labels[batch_test]\nprint(train_images0.shape)","metadata":{"execution":{"iopub.status.busy":"2021-12-25T00:27:41.847027Z","iopub.execute_input":"2021-12-25T00:27:41.847343Z","iopub.status.idle":"2021-12-25T00:27:41.88815Z","shell.execute_reply.started":"2021-12-25T00:27:41.847312Z","shell.execute_reply":"2021-12-25T00:27:41.887314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" np.seterr(divide='ignore', invalid='ignore')  # 忽略掉除于0的元素","metadata":{"execution":{"iopub.status.busy":"2021-12-25T00:27:41.889385Z","iopub.execute_input":"2021-12-25T00:27:41.889595Z","iopub.status.idle":"2021-12-25T00:27:41.896278Z","shell.execute_reply.started":"2021-12-25T00:27:41.889556Z","shell.execute_reply":"2021-12-25T00:27:41.894864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 全连接层的初始化类\nclass FullyConnected():\n    def __init__(self, W, b):\n        '''\n        Parameter:\n        W: 权重矩阵，形状为(N, M), N为输入神经元的个数，M为输出神经元的个数\n        b: 偏移量矩阵 (M，)\n        '''\n        self.W = W    # 赋值\n        self.b = b    # 赋值\n\n        self.x = None      # 用来存储输入神经元的矩阵，为反向传播提供便利\n\n        self.dW = None   # 用来存储梯度，梯度下降时需要用来更新权重\n        self.db = None\n    \n    # 全连接层的前向传播\n    def forward(self, x):\n        '''\n        input:\n        x: 输入神经元的矩阵, 形状(B, N)，B为批量大小，N为输入神经元的个数\n        output:\n        y: 输出神经元的矩阵, 形状(B, M)， M为输出神经元的个数\n        '''\n        self.x = x            # 存储输入神经元的矩阵，便于反向传播计算更新权重\n        out = np.dot(self.x, self.W) + self.b   # 完成一次前向传播\n        return out    # 返回前向传播结果\n    \n    \n    # 全连接层的反向传播\n    def backward(self, dout):\n        '''\n        input:\n        dout: 损失函数相对于全连接层输出的梯度，形状为(B,M)，M是全连接层的输出神经元个数。\n        在前向传播时全连接层的输入记录在了self.x中，故由此我们可以利用dout和self.x得到W的梯度\n        output:\n        dx:  (B, N) 关于输入层的梯度，便于进一步反向传播\n        self.W和self.b的梯度分别存储在self.dW和self.db中\n        self.dW: (N, M) 与self.W形状相同，self.W的梯度\n        self.db: (M,)， self.b的梯度\n        将x的梯度返回。\n        '''\n        # 以下所有即为矩阵的求导方法，我们也可以根据形状输入输出求解\n        # 均依赖于公式 Y = X^T + W\n        self.db = np.sum(dout, axis=0)    # 需要将得到的所有dout延y轴相加，因为取loss是就除以了batch_size\n        self.dW = np.dot(self.x.T, dout)  # 在前向传播时全连接层的输入记录在了self.x中，这一项根据矩阵求导得到我们结果\n        dx = np.dot(dout, self.W.T)       # 由矩阵求导得出结果\n        return dx                       # 返回对输入层求导的结果，便于记录进一步反向传播","metadata":{"execution":{"iopub.status.busy":"2021-12-25T00:27:41.897726Z","iopub.execute_input":"2021-12-25T00:27:41.898193Z","iopub.status.idle":"2021-12-25T00:27:41.91415Z","shell.execute_reply.started":"2021-12-25T00:27:41.898135Z","shell.execute_reply":"2021-12-25T00:27:41.913016Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 卷积层初始化类\nclass Conv2d():\n    '''\n    Parameter:\n        in_channels: C_in from expected input shape (B, C_in, H_in, W_in).\n        channels: C_out from output shape (B, C_out, H_out, W_out).\n        kernel_size: default 3.\n        stride: default 1.\n        padding: default 0.\n    '''\n    \n    def __init__(self, in_channels: int, channels: int, kernel_size: int=3,\n                 stride: int=1, padding: int=0, bias: bool=False):\n        \"\"\"\n        二维卷积层\n        input:\n        - W: numpy.array, (C_out, C_in, K_h, K_w)\n        - b: numpy.array, (C_out)\n        - stride: int\n        - pad: int\n\n        \"\"\"\n        self.W = tensor(np.random.randn(channels, in_channels, kernel_size, kernel_size))\n        # self.b = b\n        self.stride = stride\n        self.pad = padding\n        self.kernel_size = kernel_size\n        self.x = None\n        self.col = None\n        self.col_W = None\n        # self.dW = None   self.W.grad\n        # self.db = None\n\n\n    def forward(self, x):\n        \"\"\"\n        input:\n            x: input of shape (B, C_in, H_in, W_in).\n        output:\n            out: output of shape (B, C_out, H_out, W_out).\n        \"\"\"\n        FN, C, FH, FW = self.W.shape\n        N, C, H, W = x.shape\n        out_h = 1 + int((H + 2*self.pad - FH) / self.stride)\n        out_w = 1 + int((W + 2*self.pad - FW) / self.stride)\n\n        col = Conv2d_im2col(x)\n        col_W = self.W.reshape(FN, -1).T\n\n        out = np.dot(col, col_W)\n        out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2)\n        \n        self.x = x\n        self.col = col\n        self.col_W = col_W\n        \n        return out\n        \n\n\n    def backward(self, dy):\n        \"\"\"\n        input:\n            dy: output delta of shape (B, C_out, H_out, W_out).\n        output:\n            dx: input delta of shape (B, C_in, H_in, W_in).\n        \"\"\"\n        def col2im(col, input_shape, filter_h, filter_w, stride=1, pad=0):\n            N, C, H, W = input_shape\n            out_h = (H + 2 * pad - filter_h) // stride + 1\n            out_w = (W + 2 * pad - filter_w) // stride + 1\n            col = col.reshape(N, out_h, out_w, C, filter_h, filter_w).transpose(0, 3, 4, 5, 1, 2)\n\n            img = np.zeros((N, C, H + 2 * pad + stride - 1, W + 2 * pad + stride - 1))\n            for y in range(filter_h):\n                y_max = y + stride * out_h\n                for x in range(filter_w):\n                    x_max = x + stride * out_w\n                    img[:, :, y:y_max:stride, x:x_max:stride] += col[:, :, y, x, :, :]\n\n            return img[:, :, pad:H + pad, pad:W + pad]\n        \n        FN, C, FH, FW = self.W.shape\n        dout = dy\n        dout = dout.transpose(0, 2, 3, 1).reshape(-1, FN)\n        # self.b.grad = np.sum(dout, axis=0)\n        self.W.grad = np.dot(self.col.T, dout)\n        self.W.grad = self.W.grad.transpose(1, 0).reshape(FN, C, FH, FW)\n        dcol = np.dot(dout, self.col_W.T)\n        dx = col2im(dcol, self.x.shape, FH, FW, self.stride, self.pad)\n        return dx\n\n\n\nclass Conv2d_im2col(Conv2d):\n    '''\n    贾杨清所给思想，利用将卷积层转化为二维大矩阵，\n    当然还有一个col2im是另外一个关于此将二位大矩阵转化为图片的卷积层，反向传播用。\n    '''\n\n    def forward(self, x):\n\n        # TODO Implement forward propogation of\n        # 2d convolution module using im2col method.\n        input_data = x\n        filter_h, filter_w = self.kernel_size, self.kernel_size\n        stride = self.stride\n        pad = self.pad\n        N, C, H, W = input_data.shape\n        out_h = (H + 2 * pad - filter_h) // stride + 1\n        out_w = (W + 2 * pad - filter_w) // stride + 1\n\n        img = np.pad(input_data, [(0, 0), (0, 0), (pad, pad), (pad, pad)], 'constant')\n        col = np.zeros((N, C, filter_h, filter_w, out_h, out_w))\n\n        for y in range(filter_h):\n            y_max = y + stride * out_h\n            for x in range(filter_w):\n                x_max = x + stride * out_w\n                col[:, :, y, x, :, :] = img[:, :, y:y_max:stride, x:x_max:stride]\n\n        col = col.transpose(0, 4, 5, 1, 2, 3).reshape(N * out_h * out_w, -1)\n        return col","metadata":{"execution":{"iopub.status.busy":"2021-12-25T00:27:41.917774Z","iopub.execute_input":"2021-12-25T00:27:41.918651Z","iopub.status.idle":"2021-12-25T00:27:41.941879Z","shell.execute_reply.started":"2021-12-25T00:27:41.918596Z","shell.execute_reply":"2021-12-25T00:27:41.940929Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 池化层的初始化类\nclass MaxPool:\n    \"\"\"MaxPooling层\n\n       Parameters:\n            kernel_size: default 2.\n            stride: default 2.\n            padding: default 0.\n        \"\"\"\n    def __init__(self, kernel_size: int=2,\n                 stride: int=2, padding: int=0):\n        '''\n        input:\n            kernel_size: default 2.\n            stride: default 2.\n            padding: default 0.\n        '''\n        self.pool_h = kernel_size\n        self.pool_w = kernel_size\n        self.stride = stride\n        self.pad = padding\n\n        \n    def forward(self, x):\n        \"\"\"\n        input:\n            x: input of shape (B, C, H_in, W_in).\n        output:\n            out: output of shape (B, C, H_out, W_out).\n        \"\"\"\n        \n        def im2col(input_data, filter_h, filter_w, stride=1, pad=0):\n            N, C, H, W = input_data.shape\n            out_h = (H + 2 * pad - filter_h) // stride + 1\n            out_w = (W + 2 * pad - filter_w) // stride + 1\n\n            img = np.pad(input_data, [(0, 0), (0, 0), (pad, pad), (pad, pad)], 'constant')\n            col = np.zeros((N, C, filter_h, filter_w, out_h, out_w))\n\n            for y in range(filter_h):\n                y_max = y + stride * out_h\n                for x in range(filter_w):\n                    x_max = x + stride * out_w\n                    col[:, :, y, x, :, :] = img[:, :, y:y_max:stride, x:x_max:stride]\n\n            col = col.transpose(0, 4, 5, 1, 2, 3).reshape(N * out_h * out_w, -1)\n            return col\n\n        \n        N, C, H, W = x.shape\n        FN, C, FH, FW = 1, C, self.pool_h, self.pool_w\n        out_h = (H + 2 * self.pad - FH) // self.stride + 1\n        out_w = (W + 2 * self.pad - FW) // self.stride + 1\n        col = im2col(x, FH, FW, self.stride, self.pad)\n        col = col.reshape((N*out_h*out_w*C, -1))\n        col = np.max(col, axis=-1)\n        col = col.reshape(N, out_h, out_w, C)\n        col = col.transpose(0, 3, 1, 2)\n        return col\n\n\n    def backward(self, dy):\n        \"\"\"\n        input:\n            dy: output delta of shape (B, C, H_out, W_out).\n        output:\n            out: input delta of shape (B, C, H_in, W_in).\n        \"\"\"\n        def col2im(col, input_shape, filter_h, filter_w, stride=1, pad=0):\n            N, C, H, W = input_shape\n            out_h = (H + 2 * pad - filter_h) // stride + 1\n            out_w = (W + 2 * pad - filter_w) // stride + 1\n            col = col.reshape(N, out_h, out_w, C, filter_h, filter_w).transpose(0, 3, 4, 5, 1, 2)\n\n            img = np.zeros((N, C, H + 2 * pad + stride - 1, W + 2 * pad + stride - 1))\n            for y in range(filter_h):\n                y_max = y + stride * out_h\n                for x in range(filter_w):\n                    x_max = x + stride * out_w\n                    img[:, :, y:y_max:stride, x:x_max:stride] += col[:, :, y, x, :, :]\n\n            return img[:, :, pad:H + pad, pad:W + pad]\n        \n        \n        dout = dy\n        dout = dout.transpose(0, 2, 3, 1)\n        pool_size = self.pool_h * self.pool_w\n        dmax = np.zeros((dout.size, pool_size))\n        dmax[np.arange(self.arg_max.size), self.arg_max.flatten()] = dout.flatten()\n        dmax = dmax.reshape(dout.shape + (pool_size,))\n        dcol = dmax.reshape(dmax.shape[0] * dmax.shape[1] * dmax.shape[2], -1)\n        dx = col2im(dcol, self.x.shape, self.pool_h, self.pool_w, self.stride, self.pad)\n        return dx\n\n\n","metadata":{"execution":{"iopub.status.busy":"2021-12-25T00:27:41.943745Z","iopub.execute_input":"2021-12-25T00:27:41.944733Z","iopub.status.idle":"2021-12-25T00:27:41.964694Z","shell.execute_reply.started":"2021-12-25T00:27:41.944692Z","shell.execute_reply":"2021-12-25T00:27:41.963973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class BatchNorm1d:\n    '''\n    BN层当时参考了一个外国人的博客，地址：https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html\n    后来的代码也是根据他的伪代码一步一步实现的，整体实现比较乱，因为只有这个代码不是我自己完全写的，如下所示：\n    '''\n    def __init__(self, length: int, momentum: float=0.9):\n        \"\"\"Module which applies batch normalization to input.\n\n        Args:\n            length: L from expected input shape (N, L).\n            momentum: default 0.9.\n        \"\"\"\n        super(BatchNorm1d, self).__init__()\n\n        L = length\n        self.gamma = tensor.ones(L) # initialize the parameters\n        self.beta = tensor.zeros(L)  # same as up\n        self.momentum = momentum\n        self.eps = 1e-5\n        self.bn_param = {}\n        self.bn_param['running_mean'] = tensor.zeros(L)\n        self.bn_param['running_var'] = tensor.ones(L)\n        self.x_hat = None\n        self.var = None\n        self.avg = None\n        self.vareps = None\n        ...\n\n        # End of todo\n\n    def forward(self, x):\n        \"\"\"Forward propagation of batch norm module.\n\n        Args:\n            x: input of shape (N, L).\n        Returns:\n            out: output of shape (N, L).\n        \"\"\"\n        # https://blog.csdn.net/weixin_39228381/article/details/107896863\n        # https://zhuanlan.zhihu.com/p/196277511\n        \n        running_mean = self.bn_param['running_mean']\n        running_var = self.bn_param['running_var']\n        \n        N, L = x.shape  # get the batch and the length of a sample\n        self.avg = np.sum(x, axis=0) / N # get the every sample's average\n        self.var = np.sum((x - np.tile(self.avg, (N, 1))) ** 2, axis=0) / N  # get the every sample's variance\n        self.xmu = x - self.avg\n        self.vareps = (self.var + self.eps) ** 0.5   # get the Denominators\n        self.x_hat = (x - np.tile(self.avg, (N, 1))) / np.tile(self.vareps, (N, 1))  # get the normalized sequence \n        \n        out = self.gamma * self.x_hat + self.beta\n        \n        running_mean = self.momentum * running_mean + (1 - self.momentum) * self.avg\n        running_var = self.momentum * running_var + (1 - self.momentum) * self.var\n        self.bn_param['running_mean'] = running_mean\n        self.bn_param['running_var'] = running_var\n        \n        return out\n        ...\n\n        # End of todo\n#     mu = self.avg\n#     xmu = x - mu\n#     sq = xmu ** 2 \n#     var = 1. / N * np.sum(sq, axis=0)\n#     sqrtvar = np.sqrt(var + eps)\n#     ivar = 1 / sqrtvar\n#     x_hat = xmu * ivar\n#     gammax = gamma * x_hat\n        \n    def backward(self, dy):\n        \"\"\"Backward propagation of batch norm module.\n\n        Args:\n            dy: output delta of shape (N, L).\n        Returns:\n            dx: input delta of shape (N, L).\n        \"\"\"\n\n        # TODO Implement backward propogation\n        # of 1d batchnorm module.\n#         N, L = dy.shape\n#         var_plus_eps = self.vareps\n#         self.gamma.grad = np.sum(self.x_hat * dy, axis=0)\n#         self.beta.grad = np.sum(dy, axis=0)\n        \n#         dx_hat = dy * self.gamma   # x_hat's grad\n#         x_hat = self.x_hat\n\n# #         dx = N * dx_hat - np.sum(dx_hat, axis=0) + (1.0/N) * np.sum(dx_hat, axis=0) * np.sum(dx_hat * x_hat, axis=0) - x_hat * np.sum(dx_hat * x_hat, axis=0) \n# #         dx *= (1 - 1.0/N) / var_plus_eps\n        \n#         dx = dx_hat * (1 - 1. / N) * (1. / var_plus_eps) * (1 - 1. / (N * self.var) * self.xmu ** 2)\n#         return dx\n\n        xhat,gamma,xmu,ivar,sqrtvar,var,eps = self.x_hat, self.gamma, self.xmu, self.vareps, 1 / self.vareps, self.var, self.eps\n\n        #get the dimensions of the input/output\n        N,D = dout.shape\n\n        #step9\n        dbeta = np.sum(dout, axis=0)\n        dgammax = dout #not necessary, but more understandable\n\n        #step8\n        dgamma = np.sum(dgammax*xhat, axis=0)\n        dxhat = dgammax * gamma\n\n        #step7\n        divar = np.sum(dxhat*xmu, axis=0)\n        dxmu1 = dxhat * ivar\n\n        #step6\n        dsqrtvar = -1. /(sqrtvar**2) * divar\n\n        #step5\n        dvar = 0.5 * 1. /np.sqrt(var+eps) * dsqrtvar\n\n        #step4\n        dsq = 1. /N * np.ones((N,D)) * dvar\n\n        #step3\n        dxmu2 = 2 * xmu * dsq\n\n        #step2\n        dx1 = (dxmu1 + dxmu2)\n        dmu = -1 * np.sum(dxmu1+dxmu2, axis=0)\n\n\n        #step1\n        dx2 = 1. /N * np.ones((N,D)) * dmu\n        \n        #step0\n        dx = dx1 + dx2\n\n        return dx","metadata":{"execution":{"iopub.status.busy":"2021-12-25T00:27:41.967628Z","iopub.execute_input":"2021-12-25T00:27:41.967892Z","iopub.status.idle":"2021-12-25T00:27:41.986441Z","shell.execute_reply.started":"2021-12-25T00:27:41.96786Z","shell.execute_reply":"2021-12-25T00:27:41.9858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 激活函数的初始化类\nclass Sigmoid():\n    '''\n    Parameter:\n    z:激活函数作用后得到的矩阵\n    '''\n    def __init__(self):\n        self.z = None\n    \n    \n    # Sigmoid激活函数的前向传播。\n    def forward(self, y):\n        '''\n        input: \n        y:全连接层前向传播得到矩阵，形状为(B, N)\n        output:\n        z:激活函数作用后得到矩阵，形状为(B, N)\n        '''\n        z = np.exp(y) / (1 + np.exp(y)) # 利用np.exp直接对矩阵运算\n        self.z = z   # 赋值\n        return z  # 返回矩阵\n    \n    # sigmoid的反向传播\n    def backward(self, dout):\n        '''\n        input: \n        dout：损失函数相对于sigmoid输出的梯度\n        output:\n        dz:相对于矩阵y得到的梯度\n        '''\n        dz = dout * self.z * (np.ones(self.z.shape) - self.z)\n        return dz\n\nclass Relu:\n    def __init__(self):\n        '''\n        Parameter:\n        z:激活函数作用后得到的矩阵\n        '''\n        self.mask = None\n    \n    # Relu激活函数的前向传播。\n    def forward(self, y):\n        '''\n        input: \n        y:全连接层前向传播得到矩阵，形状为(B, N)\n        output:\n        z:激活函数作用后得到矩阵，形状为(B, N)\n        '''\n        self.mask = (y <= 0)   # 得到关于y大于小于0的真值的矩阵\n        z = y.copy()       # 深度拷贝一个y矩阵\n        z[self.mask] = 0   # 将小于零的值赋为0\n        return z   # 返回矩阵\n\n    def backward(self, dout):\n        '''\n        input: \n        dout：损失函数相对于relu输出的梯度\n        output:\n        dz:相对于矩阵y得到的梯度\n        '''\n        dout[self.mask] = 0\n        dz = dout\n        return dz\n    \n\nclass Cos:\n    def __init__(self):\n        '''\n        Parameter:\n        z:激活函数作用后得到的矩阵\n        '''\n        self.z = None\n        \n    def forward(self, y):\n        '''\n        input: \n        y:全连接层前向传播得到矩阵，形状为(B, N)\n        output:\n        z:激活函数作用后得到矩阵，形状为(B, N)\n        '''\n        self.z = y\n        return np.cos(y)\n    \n    def backward(self, dout):\n        '''\n        input: \n        dout：损失函数相对于relu输出的梯度\n        output:\n        dz:相对于矩阵y得到的梯度\n        '''        \n        return dout * np.sin(-self.z)\n    \nclass Tanh:\n    \n    def __init__(self):\n        '''\n        Parameter:\n         x: 输入的矩阵\n        '''\n        self.x = None\n    \n    def forward(self, x):\n        '''\n        input: \n        x:全连接层前向传播得到矩阵，形状为(B, N)\n        output:\n        激活函数作用后得到矩阵，形状为(B, N)\n        '''\n        self.x = x\n        return np.tanh(x)\n\n    def backward(self, dout):\n        '''\n        input: \n        dout：损失函数相对于relu输出的梯度\n        output:\n        dz:相对于矩阵y得到的梯度\n        '''\n        return dout * (1 - np.tanh(self.x) ** 2)","metadata":{"execution":{"iopub.status.busy":"2021-12-25T01:18:25.152254Z","iopub.execute_input":"2021-12-25T01:18:25.15254Z","iopub.status.idle":"2021-12-25T01:18:25.165378Z","shell.execute_reply.started":"2021-12-25T01:18:25.15251Z","shell.execute_reply":"2021-12-25T01:18:25.164498Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 误差损失函数的类定义（Cross Entropy+softmax)\n# 激活函数softmax\ndef softmax(y):\n    '''\n    input:\n    y:最终得到的预测输出结果矩阵\n    output:\n    将其使用softmax归一化返回处理后的矩阵（利于计算损失函数）\n    '''\n    y = y - np.max(y, axis=1, keepdims=True)     # 防止产生exp溢出的危险，所以每一行都减去最大值，且由加减值性质易得不会对值产生影响\n    return np.exp(y) / np.sum(np.exp(y), axis=1, keepdims=True)   # 返回softmax处理后矩阵，利于进一步计算损失函数\n\n# 类定义\nclass SoftmaxWithLoss():\n    '''\n    Parameter:\n    y : 预测输出结果矩阵，需要进一步softmax处理并利用其求出误差损失，形状为(B, 10)\n    label: 真实标签矩阵，形状为 (B， 1)\n    '''\n    \n    def __init__(self):\n        self.loss = None\n        self.z = None\n        self.label = None\n    \n    # SoftMax + Cross Entropy的前向传播\n    def forward(self, y, label):\n        '''\n        input:\n        y : 预测输出结果矩阵，需要进一步softmax处理并利用其求出误差损失，形状为(B, 10)\n        label: 真实标签矩阵，形状为 (B， 1)\n        output:\n        loss: 交叉熵损失\n        '''\n        z = softmax(y)      # 使用激活函数将输出矩阵归一化\n        batch_size = z.shape[0]   # 得到batch_size\n        loss = -np.sum(np.log(z[np.arange(batch_size), label])) / batch_size    # 求出平均损失误差值，使用交叉熵，利用one-hot特性得到每组输入的log值\n#                 loss = -np.sum(np.log(z[np.arange(batch_size), t] + 1e-7)) / batch_size\n        self.loss = loss  # 记录损失值\n        self.z = z     \n        self.label = label    # 存储记录\n        return loss    # 返回误差损失\n    \n    # SoftMax + Cross Entropy的反向传播\n    def backward(self):\n        '''\n        output:\n        交叉熵+softmax梯度\n        '''\n        batch_size = self.z.shape[0]  # 得到batch_size\n        dz = np.copy(self.z)       # 深拷贝\n        for label_, z_ in zip(self.label, dz):   # 由求梯度+onehot编码推出仅需在真实值所在位置减1即得梯度\n            z_[label_] -= 1\n        dz /= batch_size   # 取平均\n        return dz   # 返回梯度","metadata":{"execution":{"iopub.status.busy":"2021-12-25T00:29:03.926111Z","iopub.execute_input":"2021-12-25T00:29:03.926642Z","iopub.status.idle":"2021-12-25T00:29:03.934832Z","shell.execute_reply.started":"2021-12-25T00:29:03.926608Z","shell.execute_reply":"2021-12-25T00:29:03.934085Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 神经网络实现\nclass Network:\n    # 初始化\n    def __init__(self, input_size, hidden_size1, hidden_size2, output_size=10, lr=0.1):\n        '''\n        Parameters:\n        input_size, hidden_size1, hidden_size2, output_size:\n        分别为输入层神经元个数、隐藏层神经元数、隐藏层神经元个数、输出层神经元个数数(手写数字识别默认为10), 学习率(默认为0.1)\n        output:None\n        '''\n        W1 = np.random.randn(input_size, hidden_size1)  # 随机初始化权重\n        W2 = np.random.randn(hidden_size1, hidden_size2)\n        W3 = np.random.randn(hidden_size2, output_size)\n        b1 = np.random.randn(hidden_size1)\n        b2 = np.random.randn(hidden_size2)\n        b3 = np.random.randn(output_size)\n        \n        \n        self.lr = lr     # 学习率\n        self.layer_1 = FullyConnected(W1, b1)\n        self.sigmoid_1 = Sigmoid()\n        self.layer_2 = FullyConnected(W2, b2)\n        self.sigmoid_2 = Sigmoid()\n        self.layer_last = FullyConnected(W3, b3)\n        self.loss = SoftmaxWithLoss()\n    \n    # 神经网络前向传播\n    def forward(self, x, label):\n        '''\n        input:\n        x: 形状为(B,N)，输入的原始数据， B为批量Batch_size\n        label:输入B个数据的分类类别，形状为(B, 1)\n        output:\n        最后输出的预测向量以及我们得到的误差\n        '''\n        y1 = self.layer_1.forward(x)  # 前向传播，一步步往后走\n        z1 = self.sigmoid_1.forward(y1)\n        y2 = self.layer_2.forward(z1)\n        z2 = self.sigmoid_2.forward(y2)\n        y3 = self.layer_last.forward(z2)\n        loss = self.loss.forward(y3, label)\n        \n        return y3, loss\n    \n    \n    # 神经网络反向传播\n    def backward(self):\n        '''\n        input:None\n        output:\n        各项参数的梯度\n        '''\n        d = self.loss.backward()     # 反向传播，一步步往前走，和前向完全相反\n        d = self.layer_last.backward(d)\n        d = self.sigmoid_2.backward(d)\n        d = self.layer_2.backward(d)\n        d = self.sigmoid_1.backward(d)\n        d = self.layer_1.backward(d)    # 至此，我们单次反向传播完成。\n        \n        return self.layer_1.dW, self.layer_1.db, self.layer_2.dW, self.layer_2.db, self.layer_last.dW, self.layer_last.db  # 将每层间权重的W 、偏移量b梯度返回\n        # 这一步只是为了方便检查展示，并没有很大的用途\n\n\n    # 神经网络更新权重\n    def refresh(self):\n        lr = self.lr\n        self.layer_1.W -= lr * self.layer_1.dW\n        self.layer_1.b -= lr * self.layer_1.db\n        self.layer_2.W -= lr * self.layer_2.dW\n        self.layer_2.b -= lr * self.layer_2.db\n        self.layer_last.W -= lr * self.layer_last.dW\n        self.layer_last.b -= lr * self.layer_last.db\n    \n    # 训练模型并判断正确率\n    def fit_pred(self, train_images, train_labels, test_images, test_labels, Epochs=5, batch_size=100, losses=None, accuracy=None):\n        '''\n        input：\n        train_images:训练集\n        test_labels:测试集\n        output:None\n        '''\n        samples_num = train_images.shape[0]   # 得到训练集数量\n        pred, pred_loss, right_rate = self.predict(test_images, test_labels)    # 计算测试集精度\n        print(\"Initial Test -- Average loss:{:.4f}, Accuracy:{:.3f}\\n\".format(pred_loss, right_rate))  # 第一次训练现在训练前看看准确率\n        if losses is not None and accuracy is not None:\n            losses.append(pred_loss)\n            accuracy.append(right_rate)\n        for epoch in range(1, Epochs + 1):   # 在训练集里面跑5次\n            i = 0\n            while i < samples_num:\n                self.forward(train_images[i:i+batch_size], train_labels[i:i+batch_size])  # 每次训练batch_size个样本\n                self.backward()         # 反向传播\n                self.refresh()          # 更新参数\n#                 print(\"Train Epoch: {}\\t batch_size_index:{} Loss:{:.6f}\".format(epoch, i+1, self.loss.loss))\n                i += batch_size\n                \n#             self.lr = (0.95 ** epoch) * self.lr     # 更新学习率，防止其因为学习率过大而导致无法有效下降。\n            print(\"Train Epoch: {}\\t Loss:{:.6f}\".format(epoch, self.loss.loss))\n            pred, pred_loss, right_rate = self.predict(test_images, test_labels)    # 计算测试集精度\n            print(\"Test -- Average loss:{:.4f}, Accuracy:{:.3f}\\n\".format(pred_loss, right_rate))\n            if losses is not None and accuracy is not None:\n                losses.append(pred_loss)\n                accuracy.append(right_rate)\n        return losses, accuracy\n    \n    # 预测模型\n    def predict(self, test_images, test_labels):\n        '''\n        '''\n        pred, loss = self.forward(test_images, test_labels)  # 预测值和损失\n        pred = np.argmax(pred, axis=1)   # 求出预测标签\n        return pred, loss, right_rate(pred, test_labels)   # 返回预测值向量和损失误差以及正确率。\n    def inference_(self, inference_images):\n        x = inference_images\n        y1 = self.layer_1.forward(x)  # 前向传播，一步步往后走\n        z1 = self.sigmoid_1.forward(y1)\n        y2 = self.layer_2.forward(z1)\n        z2 = self.sigmoid_2.forward(y2)\n        y3 = self.layer_last.forward(z2)\n        return y3","metadata":{"execution":{"iopub.status.busy":"2021-12-25T00:30:08.157147Z","iopub.execute_input":"2021-12-25T00:30:08.157438Z","iopub.status.idle":"2021-12-25T00:30:08.178335Z","shell.execute_reply.started":"2021-12-25T00:30:08.157407Z","shell.execute_reply":"2021-12-25T00:30:08.17744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 计算预测数据正确率\ndef right_rate(pred_label, label):\n    '''\n    input:\n    pred_label:预测的结果数组\n    label:实际的类别数组\n    output:\n    rate:正确率\n    '''\n    diff = pred_label - label\n    return diff.tolist().count(0) / len(diff)","metadata":{"execution":{"iopub.status.busy":"2021-12-25T00:30:08.804677Z","iopub.execute_input":"2021-12-25T00:30:08.805314Z","iopub.status.idle":"2021-12-25T00:30:08.809615Z","shell.execute_reply.started":"2021-12-25T00:30:08.805276Z","shell.execute_reply":"2021-12-25T00:30:08.808598Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Network(784, 200, 60, lr=0.3)","metadata":{"execution":{"iopub.status.busy":"2021-12-25T00:30:09.340912Z","iopub.execute_input":"2021-12-25T00:30:09.341437Z","iopub.status.idle":"2021-12-25T00:30:09.351764Z","shell.execute_reply.started":"2021-12-25T00:30:09.341397Z","shell.execute_reply":"2021-12-25T00:30:09.350642Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model.fit_pred(train_images,train_labels, test_images, test_labels, Epochs=30)\n# model.lr = 0.03\n# model.fit_pred(train_images,train_labels, test_images, test_labels, Epochs=10)\n# model.lr = 0.01\n# model.fit_pred(test_images,test_labels, test_images, test_labels, Epochs=20)","metadata":{"execution":{"iopub.status.busy":"2021-12-25T00:30:09.887416Z","iopub.execute_input":"2021-12-25T00:30:09.88772Z","iopub.status.idle":"2021-12-25T00:30:09.89142Z","shell.execute_reply.started":"2021-12-25T00:30:09.887691Z","shell.execute_reply":"2021-12-25T00:30:09.890645Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df_train = pd.read_csv('/kaggle/input/digit-recognizer/train.csv')\n# train_img = df_train.values[:, 1:] / 255.0\n# train_lab = df_train.values[:, 0]","metadata":{"execution":{"iopub.status.busy":"2021-12-25T00:30:10.303476Z","iopub.execute_input":"2021-12-25T00:30:10.304144Z","iopub.status.idle":"2021-12-25T00:30:10.307344Z","shell.execute_reply.started":"2021-12-25T00:30:10.304106Z","shell.execute_reply":"2021-12-25T00:30:10.306781Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_lab.shape, train_img.shape","metadata":{"execution":{"iopub.status.busy":"2021-12-25T00:30:10.542963Z","iopub.execute_input":"2021-12-25T00:30:10.54363Z","iopub.status.idle":"2021-12-25T00:30:10.54904Z","shell.execute_reply.started":"2021-12-25T00:30:10.543587Z","shell.execute_reply":"2021-12-25T00:30:10.548088Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model.lr = 0.05\n# model.fit_pred(train_img,train_lab, test_images, test_labels, Epochs=30)","metadata":{"execution":{"iopub.status.busy":"2021-12-25T00:30:10.726415Z","iopub.execute_input":"2021-12-25T00:30:10.727077Z","iopub.status.idle":"2021-12-25T00:30:10.730296Z","shell.execute_reply.started":"2021-12-25T00:30:10.727033Z","shell.execute_reply":"2021-12-25T00:30:10.729622Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df_test = pd.read_csv('/kaggle/input/digit-recognizer/test.csv')","metadata":{"execution":{"iopub.status.busy":"2021-12-25T00:30:10.923415Z","iopub.execute_input":"2021-12-25T00:30:10.923813Z","iopub.status.idle":"2021-12-25T00:30:10.926585Z","shell.execute_reply.started":"2021-12-25T00:30:10.923784Z","shell.execute_reply":"2021-12-25T00:30:10.926073Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# inference_images = df_test.values / 255.0","metadata":{"execution":{"iopub.status.busy":"2021-12-25T00:30:11.109416Z","iopub.execute_input":"2021-12-25T00:30:11.109936Z","iopub.status.idle":"2021-12-25T00:30:11.11516Z","shell.execute_reply.started":"2021-12-25T00:30:11.109902Z","shell.execute_reply":"2021-12-25T00:30:11.114417Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# inference_images.shape","metadata":{"execution":{"iopub.status.busy":"2021-12-25T00:30:11.382411Z","iopub.execute_input":"2021-12-25T00:30:11.383159Z","iopub.status.idle":"2021-12-25T00:30:11.386206Z","shell.execute_reply.started":"2021-12-25T00:30:11.383125Z","shell.execute_reply":"2021-12-25T00:30:11.385677Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pred = model.inference_(inference_images)\n# pred = np.argmax(pred, axis=1)   # 求出预测标签","metadata":{"execution":{"iopub.status.busy":"2021-12-25T00:30:11.596435Z","iopub.execute_input":"2021-12-25T00:30:11.597212Z","iopub.status.idle":"2021-12-25T00:30:11.60122Z","shell.execute_reply.started":"2021-12-25T00:30:11.597156Z","shell.execute_reply":"2021-12-25T00:30:11.600529Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from pandas import Series,DataFrame\n# data = {'ImageId':Series(list(range(1, 28001))),\n#        'Label':Series(pred)}\n# submit = DataFrame(data)\n# print(submit)","metadata":{"execution":{"iopub.status.busy":"2021-12-25T00:30:11.815611Z","iopub.execute_input":"2021-12-25T00:30:11.816121Z","iopub.status.idle":"2021-12-25T00:30:11.819339Z","shell.execute_reply.started":"2021-12-25T00:30:11.816089Z","shell.execute_reply":"2021-12-25T00:30:11.818442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 转化为csv文件\n# submission = pd.concat([submit['ImageId'], submit['Label']], axis=1)\n# submission.to_csv('/kaggle/working/submission_2.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-12-25T00:30:12.029238Z","iopub.execute_input":"2021-12-25T00:30:12.029823Z","iopub.status.idle":"2021-12-25T00:30:12.032956Z","shell.execute_reply.started":"2021-12-25T00:30:12.029783Z","shell.execute_reply":"2021-12-25T00:30:12.0324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# The model of every activation\n- 每个模型不同激活函数的loss曲线\n    * sigmoid\n    * relu\n    * Cos\n    * tanh\n- 每个模型不同的学习率的曲线（限定20个epoch）\n- SGD和Adam算法","metadata":{}},{"cell_type":"markdown","source":"### 每个模型不同激活函数","metadata":{}},{"cell_type":"code","source":"sig_loss = []\nsig_acc = []\nrelu_loss = []\nrelu_acc = []\ncos_loss = []\ncos_acc = []\ntanh_loss = []\ntanh_acc = []","metadata":{"execution":{"iopub.status.busy":"2021-12-25T01:08:25.222623Z","iopub.execute_input":"2021-12-25T01:08:25.222928Z","iopub.status.idle":"2021-12-25T01:08:25.228003Z","shell.execute_reply.started":"2021-12-25T01:08:25.222899Z","shell.execute_reply":"2021-12-25T01:08:25.227143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_loss = {}\naccuracy = {}","metadata":{"execution":{"iopub.status.busy":"2021-12-25T01:23:47.669396Z","iopub.execute_input":"2021-12-25T01:23:47.670101Z","iopub.status.idle":"2021-12-25T01:23:47.673673Z","shell.execute_reply.started":"2021-12-25T01:23:47.670061Z","shell.execute_reply":"2021-12-25T01:23:47.672837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### sigmoid","metadata":{}},{"cell_type":"code","source":"# 当激活函数是sigmoid时\nmodel = Network(784, 200, 60, lr=0.3)\nmodel.fit_pred(train_images,train_labels, test_images, test_labels, Epochs=30, losses=sig_loss, accuracy=sig_acc)","metadata":{"execution":{"iopub.status.busy":"2021-12-25T01:08:26.846263Z","iopub.execute_input":"2021-12-25T01:08:26.84667Z","iopub.status.idle":"2021-12-25T01:10:08.260344Z","shell.execute_reply.started":"2021-12-25T01:08:26.846639Z","shell.execute_reply":"2021-12-25T01:10:08.259553Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 画图部分\nall_loss['sigmoid'] = sig_loss\naccuracy['sigmoid'] = sig_acc","metadata":{"execution":{"iopub.status.busy":"2021-12-25T01:35:04.18722Z","iopub.execute_input":"2021-12-25T01:35:04.187512Z","iopub.status.idle":"2021-12-25T01:35:04.19182Z","shell.execute_reply.started":"2021-12-25T01:35:04.187481Z","shell.execute_reply":"2021-12-25T01:35:04.190999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### relu","metadata":{}},{"cell_type":"code","source":"# 当激活函数时relu时\nmodel = Network(784, 200, 60, lr=0.1)  # 这里尝试过，发现如果学习率调到0.3Relu模型不收敛,即使学习率调的比较低有时也不收敛，实际上是由于Relu的梯度太高所指\nmodel.sigmoid_1 = Relu()\nmodel.sigmoid_2 = Relu()\nmodel.fit_pred(train_images,train_labels, test_images, test_labels, Epochs=30, losses=relu_loss, accuracy=relu_acc)","metadata":{"execution":{"iopub.status.busy":"2021-12-25T01:10:14.855285Z","iopub.execute_input":"2021-12-25T01:10:14.856009Z","iopub.status.idle":"2021-12-25T01:11:34.895227Z","shell.execute_reply.started":"2021-12-25T01:10:14.85596Z","shell.execute_reply":"2021-12-25T01:11:34.894328Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 画图部分\nall_loss['relu'] = relu_loss\naccuracy['relu'] = relu_acc","metadata":{"execution":{"iopub.status.busy":"2021-12-25T01:35:15.590489Z","iopub.execute_input":"2021-12-25T01:35:15.590775Z","iopub.status.idle":"2021-12-25T01:35:15.59454Z","shell.execute_reply.started":"2021-12-25T01:35:15.590745Z","shell.execute_reply":"2021-12-25T01:35:15.593761Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Cos function","metadata":{}},{"cell_type":"code","source":"# 当激活函数时Cos时\nmodel = Network(784, 200, 60, lr=0.5)  # 尝试过各种学习率，都不行，可能是函数代码有问题\nmodel.sigmoid_1 = Cos()\nmodel.sigmoid_2 = Cos()\nmodel.fit_pred(train_images,train_labels, test_images, test_labels, Epochs=30, losses=cos_loss, accuracy=cos_acc)\n# model.fit_pred(train_images,train_labels, test_images, test_labels, Epochs=30)","metadata":{"execution":{"iopub.status.busy":"2021-12-25T01:27:39.365792Z","iopub.execute_input":"2021-12-25T01:27:39.366101Z","iopub.status.idle":"2021-12-25T01:29:49.728881Z","shell.execute_reply.started":"2021-12-25T01:27:39.366057Z","shell.execute_reply":"2021-12-25T01:29:49.727898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 画图部分\nall_loss['cos'] = cos_loss\naccuracy['cos'] = cos_acc","metadata":{"execution":{"iopub.status.busy":"2021-12-25T01:35:36.201242Z","iopub.execute_input":"2021-12-25T01:35:36.202318Z","iopub.status.idle":"2021-12-25T01:35:36.207191Z","shell.execute_reply.started":"2021-12-25T01:35:36.20226Z","shell.execute_reply":"2021-12-25T01:35:36.206265Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### tanh","metadata":{}},{"cell_type":"code","source":"# 当激活函数时Tanh时\nmodel = Network(784, 200, 60, lr=0.2) \nmodel.sigmoid_1 = Tanh()\nmodel.sigmoid_2 = Tanh()\nmodel.fit_pred(train_images,train_labels, test_images, test_labels, Epochs=30, losses=tanh_loss, accuracy=tanh_acc)\n# model.fit_pred(train_images,train_labels, test_images, test_labels, Epochs=30)","metadata":{"execution":{"iopub.status.busy":"2021-12-25T01:19:28.459196Z","iopub.execute_input":"2021-12-25T01:19:28.459448Z","iopub.status.idle":"2021-12-25T01:21:52.640884Z","shell.execute_reply.started":"2021-12-25T01:19:28.459422Z","shell.execute_reply":"2021-12-25T01:21:52.640101Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 画图部分\nall_loss['tanh'] = tanh_loss\naccuracy['tanh'] = tanh_acc","metadata":{"execution":{"iopub.status.busy":"2021-12-25T01:36:35.711166Z","iopub.execute_input":"2021-12-25T01:36:35.711443Z","iopub.status.idle":"2021-12-25T01:36:35.715267Z","shell.execute_reply.started":"2021-12-25T01:36:35.711415Z","shell.execute_reply":"2021-12-25T01:36:35.714624Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_loss, accuracy","metadata":{"execution":{"iopub.status.busy":"2021-12-25T01:37:31.134003Z","iopub.execute_input":"2021-12-25T01:37:31.134834Z","iopub.status.idle":"2021-12-25T01:37:31.144166Z","shell.execute_reply.started":"2021-12-25T01:37:31.134779Z","shell.execute_reply":"2021-12-25T01:37:31.143627Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Visualization","metadata":{}},{"cell_type":"markdown","source":"## loss图","metadata":{}},{"cell_type":"code","source":"# 设置颜色\ncolors = ['blue', 'yellow', 'green', 'red']\ncolors = dict(zip(all_loss.keys(), colors))\nprint(colors)","metadata":{"execution":{"iopub.status.busy":"2021-12-25T01:38:55.652356Z","iopub.execute_input":"2021-12-25T01:38:55.653091Z","iopub.status.idle":"2021-12-25T01:38:55.657723Z","shell.execute_reply.started":"2021-12-25T01:38:55.653046Z","shell.execute_reply":"2021-12-25T01:38:55.657013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epoch = np.arange(1, 32)\nplt.figure(figsize=(10, 7))\nplt.title('The loss of Every model with epoch')\nfor name, loss in all_loss.items():\n    # print(name, colors[name])\n    plt.plot(epoch, loss, color=colors[name], label=name, linewidth=1.5)\n\n    for x, y in zip(epoch, loss):\n        plt.text(x, y, '%3s'%round(y, 1), ha='center', va='bottom')\n\n\nplt.legend(loc='best')\nplt.savefig('/kaggle/working/all_loss.png')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-25T02:15:41.352123Z","iopub.execute_input":"2021-12-25T02:15:41.352402Z","iopub.status.idle":"2021-12-25T02:15:42.409663Z","shell.execute_reply.started":"2021-12-25T02:15:41.352375Z","shell.execute_reply":"2021-12-25T02:15:42.408835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epoch = np.arange(1, 32)\nplt.figure(figsize=(29, 20))\nfor index, data in enumerate(all_loss.items(), 1):\n    name, loss = data\n    plt.subplot(2, 2, index)\n    plt.title(f'The loss of model {name} with epoch')\n    plt.plot(epoch, loss, color=colors[name], label=name, linewidth=1.5)\n    plt.legend(loc='best')\n    \nplt.savefig('/kaggle/working/The loss of every model with epoch')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-25T01:49:42.13369Z","iopub.execute_input":"2021-12-25T01:49:42.134681Z","iopub.status.idle":"2021-12-25T01:49:43.33969Z","shell.execute_reply.started":"2021-12-25T01:49:42.134642Z","shell.execute_reply":"2021-12-25T01:49:43.338849Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## accuracy","metadata":{}},{"cell_type":"code","source":"epoch = np.arange(1, 32)\nplt.figure(figsize=(10, 7))\nplt.title('The Accuracy of Every model with epoch')\nfor name, rate in accuracy.items():\n    print(name, colors[name])\n    plt.plot(epoch, rate, color=colors[name], label=name, linewidth=1.5)\n\n    for x, y in zip(epoch, rate):\n        plt.text(x, y, '%3s'%round(y, 2), ha='center', va='bottom')\n\n\nplt.legend(loc='best', borderpad=3)\nplt.savefig('/kaggle/working/all_accuracy.png')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-25T01:50:08.871511Z","iopub.execute_input":"2021-12-25T01:50:08.871813Z","iopub.status.idle":"2021-12-25T01:50:09.73661Z","shell.execute_reply.started":"2021-12-25T01:50:08.871781Z","shell.execute_reply":"2021-12-25T01:50:09.735991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epoch = np.arange(1, 32)\nfor name, rate in accuracy.items():\n    plt.figure()\n    plt.title(f'The Accuracy of model {name} with epoch')\n    plt.plot(epoch, rate, color=colors[name], label=name)\n    plt.legend(loc='best')\n    plt.savefig('/kaggle/working/%s_accuracy.png'%name)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-25T01:50:32.368659Z","iopub.execute_input":"2021-12-25T01:50:32.368957Z","iopub.status.idle":"2021-12-25T01:50:33.346907Z","shell.execute_reply.started":"2021-12-25T01:50:32.368926Z","shell.execute_reply":"2021-12-25T01:50:33.345966Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**That' s all! Wish you good luck!**","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}