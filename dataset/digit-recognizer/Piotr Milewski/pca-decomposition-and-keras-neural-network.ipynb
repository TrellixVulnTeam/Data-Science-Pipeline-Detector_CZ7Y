{"metadata":{"language_info":{"version":"3.6.3","mimetype":"text/x-python","name":"python","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"}},"nbformat":4,"cells":[{"metadata":{"_cell_guid":"d49076d0-e056-487c-ba21-e04e7c360e02","_uuid":"6d6a33659185d29328951ffdbb8e2fe302114616"},"cell_type":"markdown","source":"# Introduction\nIn this short notebook I wanted to experiment with PCA and neural network implemented with Keras library.\nAlso I would like to ilustrate how PCA decomposition works on data from this dataset. Most important thing, I would like hopefully to get some feedback to improve this method or find completely different approach.\n\nThanks to other authors for publishing their notebooks, I reused some parts of the code when I was looking for fiding nice solutions for problems I had on a way. "},{"metadata":{"_cell_guid":"29c9a17a-3b2c-422a-895c-7c61a3b078c5","_uuid":"8f778f4743f01a4579116c7940e6520fcd301abe"},"cell_type":"markdown","source":"# Loading and normalizing data"},{"metadata":{"_cell_guid":"eb92874b-37cf-4980-b253-9a75d2b906d8","_uuid":"267eec2a8b717893e07b0eb6529a942b30a2c179"},"cell_type":"code","outputs":[],"source":"import numpy as np\nimport pandas as pd\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\n\nfrom keras.models import Sequential\nfrom keras.utils import np_utils\nfrom keras.layers import Dense, Dropout, GaussianNoise, Conv1D\nfrom keras.preprocessing.image import ImageDataGenerator\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","execution_count":null},{"metadata":{"_cell_guid":"119e0721-f6fa-45bc-b134-4d478fa2fd22","collapsed":true,"_uuid":"43dc1e91f1fd263b3e8cf8a7582733b2442d3abe"},"cell_type":"code","outputs":[],"source":"test = pd.read_csv('../input/test.csv')\ntrain = pd.read_csv('../input/train.csv')\n\nY_train = train['label'].values.astype('int32')\nY_train = np_utils.to_categorical(Y_train) \ntrain.drop(['label'], axis=1, inplace=True)\n\nX_train = (train.values).astype('float32')\nX_test = (test.values).astype('float32')","execution_count":null},{"metadata":{"_cell_guid":"4325e472-ed6a-4752-be45-e634cfc646ca","_uuid":"4f7cfc5e02ae6d2ec75f836c879c13d2dd748941"},"cell_type":"markdown","source":"After reading data from CSV I create Numpy arrays from Pandas dataframe. Separately X_train, X_test where is data about pixels, Y_train with labels. Labels' data, which was in form of numbers from 0 to 9 need to be converted to categorical vector, because of neural network construction I used later.\n\nIt looks like this now:"},{"metadata":{"scrolled":false,"_uuid":"ea6dcd7b4cd6194772ce9127891ee6c200e3f8d6","_cell_guid":"f39e45cc-10c4-4976-85c0-de6518f00e6c"},"cell_type":"code","outputs":[],"source":"print('Y_train value form: {}'.format(Y_train[1]))\nprint('Which is 0 (1 in [0] position of the vector).')\nplt.imshow(X_train[1].reshape(28,28))\nplt.show()","execution_count":null},{"metadata":{"_cell_guid":"7b27b3cf-c32a-4177-ab26-2e1b3b4f4342","_uuid":"8957f154b0074496bb0477dcb7291c900f2dbe72"},"cell_type":"markdown","source":"Image has 28x28 pixels, so data has 784 features. Now for computers such amount of data isn't that big, but there can be cases, when dimensional reduction can be important. It is better for further processing to have 100 features containing most of the data than 800 features.\n\nBefore I use PCA to reduce dimensionality of the data I will standardize it using sklearn StandartScaler. It is fitted to train data, because I assume that I know nothing about test data. Both datasets are transformed. "},{"metadata":{"_cell_guid":"763129c7-5ca2-4e26-bb44-90f3605375ea","collapsed":true,"_uuid":"8e69ae0b3734faa0fa43ef254431256287e322c5"},"cell_type":"code","outputs":[],"source":"scaler = StandardScaler()\nscaler.fit(X_train)\nX_sc_train = scaler.transform(X_train)\nX_sc_test = scaler.transform(X_test)","execution_count":null},{"metadata":{"_cell_guid":"ae1568cf-0b9a-456f-89dd-b78a3db1fe01","_uuid":"ed1bc1d7c1bc9a0f3c28d26a5b55fc1330efee10"},"cell_type":"markdown","source":"# PCA decomposition\n\nBefore I use data with reduced dimensionality, I would like to show what is this about.\n\nI arbitrary set number of components to 500. It depends on data, to do such visualization we should aim to set number of components close to the number of original feature number. Transformation take more time depending on number of components."},{"metadata":{"scrolled":false,"_uuid":"d9c2a13aed577acc8df54a0ef76c3932ea79cf8a","_cell_guid":"9c9ab78b-d045-41f9-b4cd-923748549c2e"},"cell_type":"code","outputs":[],"source":"pca = PCA(n_components=500)\npca.fit(X_train)\n\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlabel('Number of components')\nplt.ylabel('Cumulative explained variance')","execution_count":null},{"metadata":{"_cell_guid":"261069f0-9759-4659-a94a-2f12b1310ba6","_uuid":"7635d253685c806b39d966bd6342c389fc43955e"},"cell_type":"markdown","source":"In plot above we can see that cumulative explained variance is very high near 100 and then it increases very slowly. That means that data describing changes is mostly contained in 100 components. We need to evaluate trade-offs before we choose number of components we use further. I choose 100 to check how it will work as it seems to have most of the data."},{"metadata":{"scrolled":false,"_uuid":"5e7162fd7859262d3216a18f2def997d4075c365","_cell_guid":"207554ac-0161-4c1f-9662-5ca682e796d9"},"cell_type":"code","outputs":[],"source":"NCOMPONENTS = 100\n\npca = PCA(n_components=NCOMPONENTS)\nX_pca_train = pca.fit_transform(X_sc_train)\nX_pca_test = pca.transform(X_sc_test)\npca_std = np.std(X_pca_train)\n\nprint(X_sc_train.shape)\nprint(X_pca_train.shape)","execution_count":null},{"metadata":{"_cell_guid":"59430822-4322-4e2d-85df-66cbe6b91a68","_uuid":"91b68b8f0894c743d4aa92fa1e347cc1684ad264"},"cell_type":"markdown","source":"PCA decomposition can be inverted. Let's see how images look after reconstructing them from 100 components."},{"metadata":{"_cell_guid":"e9e933b6-2b8b-4870-b174-bee5d9b31bd9","collapsed":true,"_uuid":"1d02c57bf9dfcb1025eb3447b99245d2d94ae8b9"},"cell_type":"code","outputs":[],"source":"inv_pca = pca.inverse_transform(X_pca_train)\ninv_sc = scaler.inverse_transform(inv_pca)","execution_count":null},{"metadata":{"scrolled":false,"_uuid":"28572ec67b3203b97dabb602851744079538766f","_cell_guid":"5ad9bd0a-afa2-43db-8ae1-27490b59d119"},"cell_type":"code","outputs":[],"source":"def side_by_side(indexes):\n    org = X_train[indexes].reshape(28,28)\n    rec = inv_sc[indexes].reshape(28,28)\n    pair = np.concatenate((org, rec), axis=1)\n    plt.figure(figsize=(4,2))\n    plt.imshow(pair)\n    plt.show()\n    \nfor index in range(0,10):\n    side_by_side(index)","execution_count":null},{"metadata":{"_cell_guid":"0e638b49-8067-45e5-82c0-26c05802a910","_uuid":"66173038d307ba78f6b9e0faa41f7618fe1bc185"},"cell_type":"markdown","source":"After inverting PCA and scaler transform I printed images side by side. It looks like the quality of the reconstructed image decreased in comparison to original, but the numbers are clearly visible. It will depend on the number of components."},{"metadata":{"_cell_guid":"0dd191a7-1eba-44b0-9379-2a577daeebda","_uuid":"fa84fa65c5f63e9e2ab9cf41aa4c3b8a1366ffd4"},"cell_type":"markdown","source":"# Neural network with Keras\n\nI implemened simple model of multilayer perceptron (MLP) neural network using Keras and experimented with it.\n\nUsing library is simple. First you need to create model instance and then add layers using models.add() method. First layer need to be set up for proper input dimension. Output layer needs to have proper output dimension and activation function. In between hidden layers can be added.\n\nDuring compilation parameters of loss function, optimizer and metrics need to be set depanding on problem."},{"metadata":{"scrolled":true,"_uuid":"6c36000a4f09b345d36ff47e4a7416ab7226f6b0","_cell_guid":"333ec0ec-13ed-4a2c-8bb3-df3620cfb9eb"},"cell_type":"code","outputs":[],"source":"model = Sequential()\nlayers = 1\nunits = 128\n\nmodel.add(Dense(units, input_dim=NCOMPONENTS, activation='relu'))\nmodel.add(GaussianNoise(pca_std))\nfor i in range(layers):\n    model.add(Dense(units, activation='relu'))\n    model.add(GaussianNoise(pca_std))\n    model.add(Dropout(0.1))\nmodel.add(Dense(10, activation='softmax'))\n\nmodel.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['categorical_accuracy'])\n\nmodel.fit(X_pca_train, Y_train, epochs=100, batch_size=256, validation_split=0.15, verbose=2)","execution_count":null},{"metadata":{"_cell_guid":"8329e0da-00ac-4a9d-8856-a0a79df6d367","_uuid":"a35e8fc3128d4db03474d871f10f17e719786047"},"cell_type":"markdown","source":"I experimented with parameters of compilation (metrics and loss), fit (split, epochs, batch_size) and network construction (number of layers, units, layer construction). Time performance changed, but all in all the predictions I got seems to be the same in the end. Above code generates result of 0.97+ accuracy predictions."},{"metadata":{"_cell_guid":"6485ca28-caea-40b9-8084-d9f34e313d92","collapsed":true,"_uuid":"63bbd5a435a4f21ce8d218c2ea03de7b1e8b8792"},"cell_type":"code","outputs":[],"source":"predictions = model.predict_classes(X_pca_test, verbose=0)\n\ndef write_predictions(predictions, fname):\n    pd.DataFrame({\"ImageId\": list(range(1,len(predictions)+1)), \"Label\": predictions}).to_csv(fname, index=False, header=True)\n\nwrite_predictions(predictions, \"pca-keras-mlp.csv\")","execution_count":null},{"metadata":{"_cell_guid":"9e9678a3-d4d6-4e6d-a643-370bd6c74152","_uuid":"52e5548513608692d4112a5c876ed37811cb8915"},"cell_type":"markdown","source":"# Conclusion\n\nAfter experiments with parameters of the used models I came to the result of 0.97+ accuracy on the part of the test set, but I am unable to improve it.\n\nBecause dataset is build with images of handwritten digits getting bigger train set could help. Maybe I need to change approach, ignore PCA decomposition at all and use ImageDataGenerator to generate more images or use convolution layers. \n\nIf you know how above result can be (easily?) improved, please leave comment with suggestion. As data science newbie I would be grateful for any suggestions :)"}],"nbformat_minor":1}