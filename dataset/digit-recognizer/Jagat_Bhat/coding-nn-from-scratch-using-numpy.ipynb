{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-02-12T20:42:25.095826Z","iopub.execute_input":"2022-02-12T20:42:25.096312Z","iopub.status.idle":"2022-02-12T20:42:25.133258Z","shell.execute_reply.started":"2022-02-12T20:42:25.096216Z","shell.execute_reply":"2022-02-12T20:42:25.132475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nclass HyperParameters:\n    def __init__(self, learning_rate=0.01, epochs=10, mini_batch_size=None, beta=.9, layers=None, beta1=.9, beta2=.998, lambd=0):\n        if layers is None:\n            layers = [10, 20, 10]\n        self.layers = layers\n        self.no_l = len(layers)\n        self.learning_rate = learning_rate\n        self.epochs = epochs\n        self.batch_size = mini_batch_size\n        self.beta = beta\n        self.beta1 = beta1\n        self.beta2 = beta2\n        self.lambd = lambd\n\n'''        \nClass that is responsbile for initialising weights and biases. \n'''\nclass WeightAndBias: \n    def __init__(self, number_features, layers, initialisation_type=\"random\"):\n        \n        self.initialisation_type = initialisation_type\n        self.layers = [number_features] + layers\n        self.weights = [pd.DataFrame()] + [np.random.randn(self.layers[i+1], self.layers[i]) * 0.01 for i in range(len(self.layers)-1)]\n        self.biases = [pd.DataFrame()] + [np.zeros([self.layers[i+1], 1]) for i in range(len(self.layers)-1)]\n                \n    ''' \n    method to update learning parameters    \n    '''\n    def update_learning_parameters(self, no_l, hp_obj, dW, db, m_training) :\n        for l in range(1, no_l+1):\n            self.biases[l] =  self.biases[l] - hp_obj.learning_rate * db[l]\n            self.weights[l] = (1 - (hp_obj.lambd *  hp_obj.learning_rate)/m_training) * self.weights[l] - hp_obj.learning_rate * dW[l]\n\n'''            \nActivationFunctions that takes layers and list of activation functions to be used for each of the layers.\n'''\nclass ActivationFunctions:\n    def __init__(self, layers, activation_functions=None) :\n        if activation_functions is None: \n            activation_functions= ['tanh'] * (len(layers) - 1) + ['softmax']\n            \n        self.activation_functions = [None] + [eval(f'ActivationFunctions.{activation_function}') \n                                     for activation_function in activation_functions]\n        \n        self.derivative_functions = [None] + [eval(f'ActivationFunctions.{activation_function}_derivative') \n                                     for activation_function in activation_functions]\n        \n    @staticmethod\n    def sigmoid(z) :\n        return 1 / (1 + np.exp( -z ))\n    \n    @staticmethod\n    def relu(z) : \n        return np.where(z>0, z, 0.0001 * z )\n    \n    @staticmethod\n    def tanh(z) :\n        # return np.tanh(z\n        z = np.clip(z, -20, 20)\n        return (np.exp(z) - np.exp(-z))/ (np.exp(z) + np.exp(-z))\n    \n    @staticmethod\n    def softmax(z):\n        z = np.clip(z, -20, 20)\n        return np.exp(z) / np.sum(np.exp(z), axis=0) \n    \n    @staticmethod\n    def softmax_derivative(y, a) :\n        return a - y\n    \n    @staticmethod\n    def sigmoid_derivative(y, a) :\n        return a - y\n    \n    @staticmethod\n    def tanh_derivative(z) :\n        return (1 - np.tanh(z) ** 2)\n    \n    @staticmethod\n    def relu_derivative(z) :\n        return (z > 0) * 1\n    \n    @staticmethod\n    def calculate_loss(a, y, m, hp, lp) :\n            return (-1/m * np.sum(np.multiply(y, np.log(a))), \n                    -1/m * np.sum(np.multiply(y, np.log(a))) + hp.lambd/(2 *m ) * sum(np.sum(np.square(lp.weights[i]))\n                                                                                    for i in range(1, hp.no_l+1)))\n    \n'''\nNeuralNetwork class where the magic happens, Forward prop and Backprop happens.\n'''\nclass NeuralNetwork: \n    def __init__(self, X_train, y_train, HyperParameters, activation_functions=None) :\n        \n        self.X_train, self.y_train = X_train, y_train\n        self.n, self.m = X_train.shape\n        \n        print(f\"number of training examples: {self.m}\\nnumber of features: {self.n}\"\n              f\"\\nshape of y_train {self.y_train.shape}\")\n\n        #hp --> hyperparameters\n        self.hp = HyperParameters \n        self.layers = self.hp.layers\n        self.no_l = self.hp.no_l\n        \n        self.act_function_obj = ActivationFunctions(self.layers, activation_functions=activation_functions)\n \n        #lp --> learning parameters -> weights and biases\n        self.lp = WeightAndBias(self.n, self.layers) \n    \n        if self.hp.batch_size is None:\n            self.hp.batch_size = self.m\n            \n\n    def forward_propagation(self, X_batch) :\n        self.Z, self.A = [0] + [None] * self.no_l, [X_batch ] + [None] * self.no_l\n        activation_functions = self.act_function_obj.activation_functions\n\n        for l in range(1, self.no_l + 1):\n            self.Z[l] = np.dot(self.lp.weights[l], self.A[l-1]) + self.lp.biases[l]\n            self.A[l] = activation_functions[l](self.Z[l])      \n\n    def back_propagation(self, y_batch) :\n\n        derivative_functions = self.act_function_obj.derivative_functions\n        batch_size = y_batch.shape[1]\n        \n        self.dZ =[None] +  [None] * self.no_l\n        self.dW =[None] +  [None] * self.no_l\n        self.db =[None] +  [None] * self.no_l\n\n        self.dZ[self.no_l] = derivative_functions[self.no_l](y_batch, self.A[self.no_l])\n        self.dW[self.no_l] = 1/batch_size * np.dot(self.dZ[self.no_l] , self.A[self.no_l - 1].T)\n        self.db[self.no_l] = 1/batch_size * np.sum(self.dZ[self.no_l], axis=1, keepdims=True)\n\n        assert self.dZ[self.no_l].shape == self.Z[self.no_l].shape\n        assert self.db[self.no_l].shape == self.lp.biases[self.no_l].shape        \n        assert self.dW[self.no_l].shape == self.lp.weights[self.no_l].shape\n\n        for l in range(self.no_l - 1, 0, -1) : \n\n            self.dZ[l] = np.dot(self.lp.weights[l+1].T, self.dZ[l+1] )* derivative_functions[l](self.Z[l])\n            self.dW[l] = 1/batch_size * np.dot(self.dZ[l], self.A[l-1].T)\n            self.db[l] = 1/batch_size * np.sum(self.dZ[l], axis=1, keepdims=True)\n\n            assert self.dZ[l].shape == self.Z[l].shape\n            assert self.dW[l].shape == self.lp.weights[l].shape\n            assert self.db[l].shape == self.lp.biases[l].shape  \n            \n            \n    def train_nn(self, verbose=False, per_epoch_log=100) :\n        for epoch in range(self.hp.epochs): \n            for batch_s in range(0, self.m, self.hp.batch_size) :\n                \n                batch_e = min(batch_s + self.hp.batch_size, self.m)\n                \n                X_batch = self.X_train[:, batch_s: batch_e]\n                y_batch = self.y_train[:, batch_s: batch_e]\n                m_batch_size = batch_e - batch_s\n\n                self.forward_propagation(X_batch)\n                self.back_propagation(y_batch)\n                self.lp.update_learning_parameters(self.no_l, self.hp,  self.dW, self.db, m_batch_size)\n\n            if verbose and epoch % per_epoch_log == 0: \n                print(f\"epochs {epoch} loss: \",ActivationFunctions.calculate_loss(self.A[self.no_l], y_batch, m_batch_size, self.hp,  \n                                                                                  self.lp))\n\n    def predict(self, X_test):\n        self.forward_propagation(X_test)\n        preds=  self.A[self.no_l].T\n        return (preds == preds.max(axis=1)[:,None]).astype(int)\n\ndef one_hot_encoding_y(train_data) :\n    a = train_data.label\n    b = np.zeros((a.size, 10))\n    b[np.arange(a.size),a] = 1\n    return b","metadata":{"execution":{"iopub.status.busy":"2022-02-12T20:42:25.134895Z","iopub.execute_input":"2022-02-12T20:42:25.135234Z","iopub.status.idle":"2022-02-12T20:42:25.178811Z","shell.execute_reply.started":"2022-02-12T20:42:25.135196Z","shell.execute_reply":"2022-02-12T20:42:25.177971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = pd.read_csv('/kaggle/input/digit-recognizer/train.csv')","metadata":{"execution":{"iopub.status.busy":"2022-02-12T20:42:25.180281Z","iopub.execute_input":"2022-02-12T20:42:25.18087Z","iopub.status.idle":"2022-02-12T20:42:28.676476Z","shell.execute_reply.started":"2022-02-12T20:42:25.180828Z","shell.execute_reply":"2022-02-12T20:42:28.675636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"m = train_data.shape[0]\nX = train_data.drop('label', axis=1).iloc[0:m].to_numpy() / 255\ny  = one_hot_encoding_y(train_data)[:m]\ny = np.reshape(y, (m, 10))\n\nlayers=[256, 10]\nactivation_functions = ['relu'] * (len(layers) - 1) + ['softmax']\nhp = HyperParameters(layers=layers, learning_rate=.5, epochs=500, mini_batch_size=2048, lambd=.1)\n\nnn = NeuralNetwork(X.T, y.T, hp, activation_functions=activation_functions)\n\n# nn.train_nn( verbose=True, per_epoch_log=10)","metadata":{"execution":{"iopub.status.busy":"2022-02-12T20:42:28.678415Z","iopub.execute_input":"2022-02-12T20:42:28.67862Z","iopub.status.idle":"2022-02-12T20:42:28.886293Z","shell.execute_reply.started":"2022-02-12T20:42:28.678594Z","shell.execute_reply":"2022-02-12T20:42:28.885348Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prob_preds = lambda preds: (preds == preds.max(axis=1)[:,None]).astype(int)\npreds = nn.predict(X.T)\nr = np.sum(np.argmax(y, axis=1) == np.argmax(preds, axis=1))\nw = np.sum(np.argmax(y, axis=1) != np.argmax(preds, axis=1))\nprint(f\"total number of examples: {m}\\nnumber of right predictions: {r}\\nnumber of wrong predictions: {w}\\n\"\n          f\"accuracy on train: {r/m * 100}%\")","metadata":{"execution":{"iopub.status.busy":"2022-02-12T20:42:28.887763Z","iopub.execute_input":"2022-02-12T20:42:28.888095Z","iopub.status.idle":"2022-02-12T20:42:29.428568Z","shell.execute_reply.started":"2022-02-12T20:42:28.888056Z","shell.execute_reply":"2022-02-12T20:42:29.426736Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data = pd.read_csv('/kaggle/input/digit-recognizer/test.csv')\nX_test = test_data.to_numpy() / 255","metadata":{"execution":{"iopub.status.busy":"2022-02-12T20:42:29.429895Z","iopub.execute_input":"2022-02-12T20:42:29.430152Z","iopub.status.idle":"2022-02-12T20:42:31.366084Z","shell.execute_reply.started":"2022-02-12T20:42:29.430116Z","shell.execute_reply":"2022-02-12T20:42:31.365329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# preds = nn.predict(X_test.T)\n# preds = np.argmax(preds, axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-02-12T20:42:31.367642Z","iopub.execute_input":"2022-02-12T20:42:31.367902Z","iopub.status.idle":"2022-02-12T20:42:31.371371Z","shell.execute_reply.started":"2022-02-12T20:42:31.367867Z","shell.execute_reply":"2022-02-12T20:42:31.370513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sub_df = pd.DataFrame(preds, columns=['Label'])\n# sub_df.index.name= 'ImageId'\n# sub_df.index = sub_df.index + 1\n# sub_df.reset_index().to_csv('mnsit_submission.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2022-02-12T20:42:31.372823Z","iopub.execute_input":"2022-02-12T20:42:31.373261Z","iopub.status.idle":"2022-02-12T20:42:31.381759Z","shell.execute_reply.started":"2022-02-12T20:42:31.373224Z","shell.execute_reply":"2022-02-12T20:42:31.380956Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Using Tensorflow","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom keras.callbacks import EarlyStopping, ReduceLROnPlateau\nimport keras","metadata":{"execution":{"iopub.status.busy":"2022-02-12T20:42:31.383115Z","iopub.execute_input":"2022-02-12T20:42:31.383354Z","iopub.status.idle":"2022-02-12T20:42:36.099822Z","shell.execute_reply.started":"2022-02-12T20:42:31.38332Z","shell.execute_reply":"2022-02-12T20:42:36.099098Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"def normalize_img(image, label):\n  \"\"\"Normalizes images: `uint8` -> `float32`.\"\"\"\n  return tf.cast(image, tf.float32) / 255., label","metadata":{"execution":{"iopub.status.busy":"2022-02-12T20:42:36.104152Z","iopub.execute_input":"2022-02-12T20:42:36.104363Z","iopub.status.idle":"2022-02-12T20:42:36.109137Z","shell.execute_reply.started":"2022-02-12T20:42:36.104336Z","shell.execute_reply":"2022-02-12T20:42:36.108207Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X.T.shape","metadata":{"execution":{"iopub.status.busy":"2022-02-12T20:42:36.110587Z","iopub.execute_input":"2022-02-12T20:42:36.111178Z","iopub.status.idle":"2022-02-12T20:42:36.1235Z","shell.execute_reply.started":"2022-02-12T20:42:36.111094Z","shell.execute_reply":"2022-02-12T20:42:36.122098Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = tf.keras.models.Sequential([\ntf.keras.layers.InputLayer(input_shape = [784]),\n    keras.layers.Dense(784),\n    keras.layers.BatchNormalization(),\n    keras.layers.LeakyReLU(alpha = 0.4),\n    \n    keras.layers.Dense(576),\n    keras.layers.BatchNormalization(),\n    keras.layers.LeakyReLU(alpha = 0.4),\n    \n    keras.layers.Dense(320),\n    keras.layers.BatchNormalization(),\n    keras.layers.Activation('relu'),\n    \n    keras.layers.Dense(256),\n    keras.layers.BatchNormalization(),\n    keras.layers.Activation('relu'),\n    \n    keras.layers.Dense(160, kernel_regularizer = keras.regularizers.l2(0.0025)),\n    keras.layers.BatchNormalization(),\n    keras.layers.LeakyReLU(alpha = 0.4),\n    \n    keras.layers.Dense(84, kernel_regularizer = keras.regularizers.l2(0.0025)),\n    keras.layers.BatchNormalization(),\n    keras.layers.LeakyReLU(alpha = 0.25),\n    \n    keras.layers.Dense(10, activation = keras.activations.softmax),\n])\n\nmodel.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005), \n            loss = tf.keras.losses.sparse_categorical_crossentropy, \n            metrics = ['accuracy'])\n\ncb1 = EarlyStopping(patience = 3, restore_best_weights=True, monitor = 'val_acc')\ncb2 = ReduceLROnPlateau(patience = 4, min_lr=0.00001, factor = 0.4, monitor = 'val_acc')\n\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2022-02-12T20:42:36.124832Z","iopub.execute_input":"2022-02-12T20:42:36.125422Z","iopub.status.idle":"2022-02-12T20:42:38.703566Z","shell.execute_reply.started":"2022-02-12T20:42:36.125385Z","shell.execute_reply":"2022-02-12T20:42:38.701466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# X_train = tf.constant(np.array(X), dtype = tf.float32)\n# X_test = tf.constant(np.array(X_test), dtype = tf.float32)\n# y_train = tf.constant(train_data.label)\n\n# X_train.shape, X_test.shape, y_train.shape","metadata":{"execution":{"iopub.status.busy":"2022-02-12T20:42:38.707073Z","iopub.execute_input":"2022-02-12T20:42:38.707435Z","iopub.status.idle":"2022-02-12T20:42:38.716544Z","shell.execute_reply.started":"2022-02-12T20:42:38.707391Z","shell.execute_reply":"2022-02-12T20:42:38.715917Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# history = model.fit(X_train, y_train, validation_split=0.2, epochs = 50, batch_size = 256, callbacks = [cb1, cb2], verbose = 1)","metadata":{"execution":{"iopub.status.busy":"2022-02-12T20:42:38.72111Z","iopub.execute_input":"2022-02-12T20:42:38.723563Z","iopub.status.idle":"2022-02-12T20:42:38.729363Z","shell.execute_reply.started":"2022-02-12T20:42:38.723523Z","shell.execute_reply":"2022-02-12T20:42:38.72863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model.predict(X_train)","metadata":{"execution":{"iopub.status.busy":"2022-02-12T20:42:38.731272Z","iopub.execute_input":"2022-02-12T20:42:38.732437Z","iopub.status.idle":"2022-02-12T20:42:38.738506Z","shell.execute_reply.started":"2022-02-12T20:42:38.732395Z","shell.execute_reply":"2022-02-12T20:42:38.737264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Trying perf with Data Augmentation","metadata":{}},{"cell_type":"code","source":"X = train_data.drop('label', axis=1).to_numpy() #/ 255\ny  = train_data.label","metadata":{"execution":{"iopub.status.busy":"2022-02-12T20:42:38.739621Z","iopub.execute_input":"2022-02-12T20:42:38.740177Z","iopub.status.idle":"2022-02-12T20:42:38.831066Z","shell.execute_reply.started":"2022-02-12T20:42:38.740136Z","shell.execute_reply":"2022-02-12T20:42:38.830202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# X.iloc[0]","metadata":{"execution":{"iopub.status.busy":"2022-02-12T20:42:48.261249Z","iopub.execute_input":"2022-02-12T20:42:48.261725Z","iopub.status.idle":"2022-02-12T20:42:48.265684Z","shell.execute_reply.started":"2022-02-12T20:42:48.26168Z","shell.execute_reply":"2022-02-12T20:42:48.264957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from scipy.ndimage.interpolation import shift\n","metadata":{"execution":{"iopub.status.busy":"2022-02-12T20:42:50.141765Z","iopub.execute_input":"2022-02-12T20:42:50.142093Z","iopub.status.idle":"2022-02-12T20:42:50.145953Z","shell.execute_reply.started":"2022-02-12T20:42:50.142036Z","shell.execute_reply":"2022-02-12T20:42:50.14528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"get_image_shape = lambda i_r : np.reshape(i_r, (28, 28))\ndef create_data_aug_images(X, y) :\n    X_transposed = np.reshape(X, (42000, 28, 28))\n    X_transposed = np.array([X_transposed[i].T for i in range(len(X_transposed))])\n    y_copy = y.copy()\n\n    image = np.reshape(image_rows, (28, 28))\n    return image\n    ","metadata":{"execution":{"iopub.status.busy":"2022-02-12T20:42:53.172396Z","iopub.execute_input":"2022-02-12T20:42:53.173194Z","iopub.status.idle":"2022-02-12T20:42:53.179363Z","shell.execute_reply.started":"2022-02-12T20:42:53.173151Z","shell.execute_reply":"2022-02-12T20:42:53.17841Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from matplotlib import pyplot as plt\n# plt.imshow(X_aug, cmap=plt.get_cmap('gray'))\n# plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-12T20:42:54.215396Z","iopub.execute_input":"2022-02-12T20:42:54.215649Z","iopub.status.idle":"2022-02-12T20:42:54.219611Z","shell.execute_reply.started":"2022-02-12T20:42:54.21562Z","shell.execute_reply":"2022-02-12T20:42:54.218803Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Using Numpy Code","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_reshaped = np.reshape(X, (42000, 28, 28))\nX_transposed = np.array([X_reshaped[i].T for i in range(len(X_reshaped))])\nX_aug = np.concatenate((X_reshaped, X_transposed), axis=0)\n# y_aug = \n\n\nX_aug.shape\n# X_transpose","metadata":{"execution":{"iopub.status.busy":"2022-02-12T20:42:55.343093Z","iopub.execute_input":"2022-02-12T20:42:55.343754Z","iopub.status.idle":"2022-02-12T20:42:55.73432Z","shell.execute_reply.started":"2022-02-12T20:42:55.343715Z","shell.execute_reply":"2022-02-12T20:42:55.73355Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def one_hot_encoding_y(a) :\n#     a = train_data.label\n    b = np.zeros((a.size, 10))\n    b[np.arange(a.size),a] = 1\n    return b","metadata":{"execution":{"iopub.status.busy":"2022-02-12T20:42:56.085009Z","iopub.execute_input":"2022-02-12T20:42:56.085299Z","iopub.status.idle":"2022-02-12T20:42:56.089666Z","shell.execute_reply.started":"2022-02-12T20:42:56.085268Z","shell.execute_reply":"2022-02-12T20:42:56.088838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_aug = np.concatenate((np.array(train_data.label), np.array(train_data.label)))\ny_aug","metadata":{"execution":{"iopub.status.busy":"2022-02-12T20:42:56.751977Z","iopub.execute_input":"2022-02-12T20:42:56.752641Z","iopub.status.idle":"2022-02-12T20:42:56.759818Z","shell.execute_reply.started":"2022-02-12T20:42:56.752606Z","shell.execute_reply":"2022-02-12T20:42:56.75904Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"m = X_aug.shape[0]\nX_aug_train = X_aug.reshape(84000, 784) / 255.0\ny_train  = one_hot_encoding_y(y_aug)\ny_train = np.reshape(y_train, (m, 10))","metadata":{"execution":{"iopub.status.busy":"2022-02-12T20:42:57.290131Z","iopub.execute_input":"2022-02-12T20:42:57.290408Z","iopub.status.idle":"2022-02-12T20:42:57.502693Z","shell.execute_reply.started":"2022-02-12T20:42:57.290378Z","shell.execute_reply":"2022-02-12T20:42:57.501975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# layers=[256, 10]\n# activation_functions = ['relu'] * (len(layers) - 1) + ['softmax']\n# hp = HyperParameters(layers=layers, learning_rate=.5, epochs=500, mini_batch_size=2048, lambd=.1)\n\n# nn = NeuralNetwork(X_aug_train.T, y_train.T, hp, activation_functions=activation_functions)","metadata":{"execution":{"iopub.status.busy":"2022-02-12T20:42:58.045553Z","iopub.execute_input":"2022-02-12T20:42:58.045816Z","iopub.status.idle":"2022-02-12T20:42:58.052316Z","shell.execute_reply.started":"2022-02-12T20:42:58.045784Z","shell.execute_reply":"2022-02-12T20:42:58.051579Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# nn.train_nn( verbose=True, per_epoch_log=10)","metadata":{"execution":{"iopub.status.busy":"2022-02-12T20:42:58.637807Z","iopub.execute_input":"2022-02-12T20:42:58.638071Z","iopub.status.idle":"2022-02-12T20:42:58.642067Z","shell.execute_reply.started":"2022-02-12T20:42:58.638023Z","shell.execute_reply":"2022-02-12T20:42:58.641031Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Trying out with keras NN","metadata":{}},{"cell_type":"code","source":"X_train = tf.constant(np.array(X_aug_train), dtype = tf.float32)\nX_test = tf.constant(np.array(X_test), dtype = tf.float32)\ny_train = tf.constant(y_aug)\n\nX_train.shape, X_test.shape, y_train.shape","metadata":{"execution":{"iopub.status.busy":"2022-02-12T20:42:59.686617Z","iopub.execute_input":"2022-02-12T20:42:59.688237Z","iopub.status.idle":"2022-02-12T20:43:00.544905Z","shell.execute_reply.started":"2022-02-12T20:42:59.688184Z","shell.execute_reply":"2022-02-12T20:43:00.5441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = tf.keras.models.Sequential([\ntf.keras.layers.InputLayer(input_shape = [784]),\n    keras.layers.Dense(784),\n    keras.layers.BatchNormalization(),\n    keras.layers.LeakyReLU(alpha = 0.4),\n    \n    keras.layers.Dense(576),\n    keras.layers.BatchNormalization(),\n    keras.layers.LeakyReLU(alpha = 0.4),\n    \n    keras.layers.Dense(320),\n    keras.layers.BatchNormalization(),\n    keras.layers.Activation('relu'),\n    \n    keras.layers.Dense(256),\n    keras.layers.BatchNormalization(),\n    keras.layers.Activation('relu'),\n    \n    keras.layers.Dense(160, kernel_regularizer = keras.regularizers.l2(0.0025)),\n    keras.layers.BatchNormalization(),\n    keras.layers.LeakyReLU(alpha = 0.4),\n    \n    keras.layers.Dense(84, kernel_regularizer = keras.regularizers.l2(0.0025)),\n    keras.layers.BatchNormalization(),\n    keras.layers.LeakyReLU(alpha = 0.25),\n    \n    keras.layers.Dense(10, activation = keras.activations.softmax),\n])\n\nmodel.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005), \n            loss = tf.keras.losses.sparse_categorical_crossentropy, \n            metrics = ['accuracy'])\n\ncb1 = EarlyStopping(patience = 3, restore_best_weights=True, monitor = 'val_acc')\ncb2 = ReduceLROnPlateau(patience = 4, min_lr=0.00001, factor = 0.4, monitor = 'val_acc')\n\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2022-02-12T20:43:01.145864Z","iopub.execute_input":"2022-02-12T20:43:01.14614Z","iopub.status.idle":"2022-02-12T20:43:01.300412Z","shell.execute_reply.started":"2022-02-12T20:43:01.146106Z","shell.execute_reply":"2022-02-12T20:43:01.299622Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# history = model.fit(X_train, y_train, \n#                     validation_split=0.2, epochs = 50, batch_size = 128, \n#                     callbacks = [cb1, cb2], verbose = 1)","metadata":{"execution":{"iopub.status.busy":"2022-02-12T20:43:02.352966Z","iopub.execute_input":"2022-02-12T20:43:02.353756Z","iopub.status.idle":"2022-02-12T20:43:02.357944Z","shell.execute_reply.started":"2022-02-12T20:43:02.353722Z","shell.execute_reply":"2022-02-12T20:43:02.356993Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Trying out with CNN using Keras","metadata":{"execution":{"iopub.status.busy":"2022-02-12T20:43:02.781898Z","iopub.execute_input":"2022-02-12T20:43:02.782139Z","iopub.status.idle":"2022-02-12T20:43:02.786881Z","shell.execute_reply.started":"2022-02-12T20:43:02.782109Z","shell.execute_reply":"2022-02-12T20:43:02.785483Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nimport matplotlib.pyplot as plt\nfrom kaggle_datasets import KaggleDatasets\nimport pandas as pd\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator","metadata":{"execution":{"iopub.status.busy":"2022-02-12T20:43:03.14078Z","iopub.execute_input":"2022-02-12T20:43:03.141069Z","iopub.status.idle":"2022-02-12T20:43:03.149463Z","shell.execute_reply.started":"2022-02-12T20:43:03.141019Z","shell.execute_reply":"2022-02-12T20:43:03.148697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('../input/digit-recognizer/train.csv') \ntest = pd.read_csv('../input/digit-recognizer/test.csv')\n\nX_train = train.drop(['label'], axis = 1)\ny_train = train['label']\n\nX_test = test\n\n\n# Divide the labels in the training, testing and validation set\ny_train = tf.constant(y_train)\n\n\n# Divide the input in the training, testing and validation set\nX_train = tf.constant(np.array(X_train), dtype = tf.float64)\nX_test = tf.constant(np.array(X_test), dtype = tf.float32)","metadata":{"execution":{"iopub.status.busy":"2022-02-12T20:43:03.965378Z","iopub.execute_input":"2022-02-12T20:43:03.966063Z","iopub.status.idle":"2022-02-12T20:43:08.552922Z","shell.execute_reply.started":"2022-02-12T20:43:03.96601Z","shell.execute_reply":"2022-02-12T20:43:08.552168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize = (10, 10))\ncount = 1\nidd = 0\nfor i in range(5):\n    for j in range(5):\n        plt.subplot(5, 5, count)\n        plt.imshow(tf.constant(X_test[idd, :], shape = [28, 28]), cmap = 'gray')\n        idd += 1\n        count += 1","metadata":{"execution":{"iopub.status.busy":"2022-02-12T20:43:08.555234Z","iopub.execute_input":"2022-02-12T20:43:08.555684Z","iopub.status.idle":"2022-02-12T20:43:10.781138Z","shell.execute_reply.started":"2022-02-12T20:43:08.555641Z","shell.execute_reply":"2022-02-12T20:43:10.779106Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_s = (X_train - 127.5) / 127.5\nX_test_s = (X_test - 127.5) / 127.5","metadata":{"execution":{"iopub.status.busy":"2022-02-12T20:43:10.782394Z","iopub.execute_input":"2022-02-12T20:43:10.78271Z","iopub.status.idle":"2022-02-12T20:43:10.793904Z","shell.execute_reply.started":"2022-02-12T20:43:10.78267Z","shell.execute_reply":"2022-02-12T20:43:10.793065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_valid, y_train, y_valid = train_test_split(X_train_s.numpy(), y_train.numpy(), test_size = 0.2)\nX_train.shape, X_valid.shape","metadata":{"execution":{"iopub.status.busy":"2022-02-12T20:43:10.795969Z","iopub.execute_input":"2022-02-12T20:43:10.796295Z","iopub.status.idle":"2022-02-12T20:43:11.800398Z","shell.execute_reply.started":"2022-02-12T20:43:10.796252Z","shell.execute_reply":"2022-02-12T20:43:11.79971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = tf.reshape(X_train, shape = [-1, 28, 28, 1])\nX_valid = tf.reshape(X_valid, shape = [-1, 28, 28, 1])\nX_test = tf.reshape(X_test_s, shape = [-1, 28, 28, 1])\n\ny_train = keras.utils.to_categorical(y_train)\ny_valid = keras.utils.to_categorical(y_valid)","metadata":{"execution":{"iopub.status.busy":"2022-02-12T20:43:11.804374Z","iopub.execute_input":"2022-02-12T20:43:11.806802Z","iopub.status.idle":"2022-02-12T20:43:12.105238Z","shell.execute_reply.started":"2022-02-12T20:43:11.806747Z","shell.execute_reply":"2022-02-12T20:43:12.104377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a model with keras sequential class\nmod = keras.models.Sequential([\n\n    keras.layers.InputLayer(input_shape = X_train.shape[1:]), \n\n    keras.layers.Conv2D(filters = 32, kernel_size = (3, 3)), \n    keras.layers.LeakyReLU(alpha = 0.25),     # (26, 26)\n    keras.layers.BatchNormalization(),\n\n    keras.layers.Conv2D(filters = 48, kernel_size = (3, 3)), \n    keras.layers.LeakyReLU(alpha = 0),\n    keras.layers.BatchNormalization(),\n\n    keras.layers.Conv2D(filters = 64, kernel_size = (5, 5)),\n    keras.layers.LeakyReLU(alpha = 0.25),    \n    keras.layers.BatchNormalization(),        # (20, 20)\n    \n    keras.layers.Conv2D(filters = 96, kernel_size = (5, 5)),\n    keras.layers.LeakyReLU(alpha = 0),    \n    keras.layers.BatchNormalization(),       # (16, 16)\n    \n    keras.layers.Conv2D(filters = 128, kernel_size = (7, 7)),\n    keras.layers.LeakyReLU(alpha = 0.25),    # (10, 10)\n    keras.layers.BatchNormalization(),\n    keras.layers.Dropout(0.5),\n    \n    keras.layers.Flatten(),                    # (12800)\n  \n    keras.layers.Dense(112), \n    keras.layers.LeakyReLU(alpha = 0), \n    keras.layers.BatchNormalization(),\n    keras.layers.Dropout(0.5),\n    \n    keras.layers.Dense(64),\n    keras.layers.LeakyReLU(alpha = 0.25),\n    keras.layers.BatchNormalization(),\n    \n    keras.layers.Dense(32),\n    keras.layers.LeakyReLU(alpha = 0),\n    keras.layers.BatchNormalization(),\n\n    keras.layers.Dense(10),\n    keras.layers.BatchNormalization(),\n    keras.layers.Activation(keras.activations.softmax),\n\n])\n\n# Compile the model\nmod.compile(optimizer=keras.optimizers.Adam(learning_rate=0.1), \n            loss = keras.losses.categorical_crossentropy, \n            metrics = ['accuracy'])\n\n\n\n# summarize the model\nmod.summary()\n\n\n\n\n# Plot the model\nkeras.utils.plot_model(mod, show_shapes=True)","metadata":{"execution":{"iopub.status.busy":"2022-02-12T20:43:12.109238Z","iopub.execute_input":"2022-02-12T20:43:12.109457Z","iopub.status.idle":"2022-02-12T20:43:13.339403Z","shell.execute_reply.started":"2022-02-12T20:43:12.109428Z","shell.execute_reply":"2022-02-12T20:43:13.338443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cb1 = EarlyStopping(patience = 3, restore_best_weights=True, monitor = 'val_acc', verbose = 1)\ncb2 = ReduceLROnPlateau(patience = 3, min_lr=0.00001, factor = 0.15, monitor = 'val_acc', verbose = 1)","metadata":{"execution":{"iopub.status.busy":"2022-02-12T20:43:13.340701Z","iopub.execute_input":"2022-02-12T20:43:13.34104Z","iopub.status.idle":"2022-02-12T20:43:13.347189Z","shell.execute_reply.started":"2022-02-12T20:43:13.340991Z","shell.execute_reply":"2022-02-12T20:43:13.346017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_train = ImageDataGenerator(\n    rotation_range = 10, \n    width_shift_range = 4, \n    height_shift_range = 4, \n    zoom_range = 0.15,\n )\n\ndata_valid = ImageDataGenerator()\n\ntrain_gen = data_train.flow(X_train, y_train, batch_size = 168, shuffle = True)\nvalid_gen = data_valid.flow(X_valid, y_valid, batch_size = 56, shuffle = True)","metadata":{"execution":{"iopub.status.busy":"2022-02-12T20:43:13.349717Z","iopub.execute_input":"2022-02-12T20:43:13.35025Z","iopub.status.idle":"2022-02-12T20:43:13.607551Z","shell.execute_reply.started":"2022-02-12T20:43:13.350215Z","shell.execute_reply":"2022-02-12T20:43:13.60673Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# history = mod.fit(train_gen, epochs = 275, validation_data = valid_gen, callbacks = [cb1, cb2])\n# acc = mod.evaluate(X_valid, y_valid)","metadata":{"execution":{"iopub.status.busy":"2022-02-12T20:43:18.958956Z","iopub.execute_input":"2022-02-12T20:43:18.959537Z","iopub.status.idle":"2022-02-12T20:43:18.964132Z","shell.execute_reply.started":"2022-02-12T20:43:18.959497Z","shell.execute_reply":"2022-02-12T20:43:18.962302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pred = mod.predict(X_test)\n# pred = np.argmax(pred, axis = 1)\n# pred.shape","metadata":{"execution":{"iopub.status.busy":"2022-02-12T20:43:29.879724Z","iopub.execute_input":"2022-02-12T20:43:29.879986Z","iopub.status.idle":"2022-02-12T20:43:29.88448Z","shell.execute_reply.started":"2022-02-12T20:43:29.879949Z","shell.execute_reply":"2022-02-12T20:43:29.882912Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sub = pd.read_csv('../input/digit-recognizer/sample_submission.csv')\n# sub['Label'] = pred\n# sub.to_csv('submission.csv', index = False)\n# sub.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-12T20:43:29.886851Z","iopub.execute_input":"2022-02-12T20:43:29.887424Z","iopub.status.idle":"2022-02-12T20:43:29.896849Z","shell.execute_reply.started":"2022-02-12T20:43:29.887383Z","shell.execute_reply":"2022-02-12T20:43:29.896095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Trying out ther nets","metadata":{"execution":{"iopub.status.busy":"2022-02-12T20:43:29.900083Z","iopub.execute_input":"2022-02-12T20:43:29.900308Z","iopub.status.idle":"2022-02-12T20:43:29.905593Z","shell.execute_reply.started":"2022-02-12T20:43:29.900275Z","shell.execute_reply":"2022-02-12T20:43:29.9049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Import the necessary libs\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport seaborn as sns\n%matplotlib inline\n\nnp.random.seed(2)\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nimport itertools\n\nfrom keras.utils.np_utils import to_categorical # convert to one-hot-encoding\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, BatchNormalization\nfrom tensorflow.keras.optimizers import RMSprop\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ReduceLROnPlateau\nfrom keras.datasets import mnist\nimport tensorflow as tf\n\nsns.set(style='white', context='notebook', palette='deep')","metadata":{"execution":{"iopub.status.busy":"2022-02-12T20:43:29.907151Z","iopub.execute_input":"2022-02-12T20:43:29.907334Z","iopub.status.idle":"2022-02-12T20:43:30.031661Z","shell.execute_reply.started":"2022-02-12T20:43:29.907312Z","shell.execute_reply":"2022-02-12T20:43:30.030997Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('../input/digit-recognizer/train.csv')\ntest = pd.read_csv('../input/digit-recognizer/test.csv')\nsub = pd.read_csv('../input/digit-recognizer/sample_submission.csv')\n\nprint(\"Data are Ready!!\")\n\nY_train = train[\"label\"]\nX_train = train.drop(labels = [\"label\"], axis = 1) \n\n(x_train1, y_train1), (x_test1, y_test1) = mnist.load_data()\n\ntrain1 = np.concatenate([x_train1, x_test1], axis=0)\ny_train1 = np.concatenate([y_train1, y_test1], axis=0)\n\nY_train1 = y_train1\nX_train1 = train1.reshape(-1, 28*28)\n\nX_train = X_train / 255.0\ntest = test / 255.0\n\nX_train1 = X_train1 / 255.0\n\nX_train = np.concatenate((X_train.values, X_train1))\nY_train = np.concatenate((Y_train, Y_train1))\n\nX_train = X_train.reshape(-1,28,28,1)\ntest = test.values.reshape(-1,28,28,1)\n\nY_train = to_categorical(Y_train, num_classes = 10)\n\nX_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size = 0.1, random_state=2)\n\n","metadata":{"execution":{"iopub.status.busy":"2022-02-12T20:43:30.034118Z","iopub.execute_input":"2022-02-12T20:43:30.034622Z","iopub.status.idle":"2022-02-12T20:43:35.379138Z","shell.execute_reply.started":"2022-02-12T20:43:30.034581Z","shell.execute_reply":"2022-02-12T20:43:35.37824Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Sequential()\n\nmodel.add(Conv2D(filters = 64, kernel_size = (5,5),padding = 'Same', activation ='relu', input_shape = (28,28,1)))\nmodel.add(BatchNormalization())\n\nmodel.add(Conv2D(filters = 64, kernel_size = (5,5),padding = 'Same', activation ='relu'))\nmodel.add(BatchNormalization())\n\nmodel.add(MaxPool2D(pool_size=(2,2)))\nmodel.add(Dropout(0.25))\n\nmodel.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', activation ='relu'))\nmodel.add(BatchNormalization())\n\nmodel.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', activation ='relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\nmodel.add(Dropout(0.25))\n\nmodel.add(Conv2D(filters = 64, kernel_size = (3,3), padding = 'Same',  activation ='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.25))\n\nmodel.add(Flatten())\nmodel.add(Dense(256, activation = \"relu\"))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.25))\n\nmodel.add(Dense(10, activation = \"softmax\"))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optimizer = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)\nmodel.compile(optimizer = optimizer , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])\nlearning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', \n                                            patience=3, \n                                            verbose=1, \n                                            factor=0.5, \n                                            min_lr=0.00001)\nepochs = 50\nbatch_size = 128\n\n#Data Augmentation \ndatagen = ImageDataGenerator(\n        featurewise_center=False, # set input mean to 0 over the dataset\n        samplewise_center=False,  # set each sample mean to 0\n        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n        samplewise_std_normalization=False,  # divide each input by its std\n        zca_whitening=False,  # apply ZCA whitening\n        rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n        zoom_range = 0.1, # Randomly zoom image \n        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n        horizontal_flip=False,  # randomly flip images\n        vertical_flip=False)  # randomly flip images\n\n#datagen.fit(X_train)\ntrain_gen = datagen.flow(X_train,Y_train, batch_size=batch_size)\n\nhistory = model.fit(train_gen,\n                              epochs = epochs,validation_data = (X_val,Y_val),\n                              verbose = 1, steps_per_epoch=X_train.shape[0] // batch_size\n                              , callbacks=[learning_rate_reduction],\n                             validation_steps = X_val.shape[0] // batch_size)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Make predictions about test sets\nresults = model.predict(test)\n\n# Convert one-hot vector to number\nresults = np.argmax(results,axis = 1)\n\nresults = pd.Series(results,name=\"Label\")\n\nsubmission = pd.concat([pd.Series(range(1,28001),name = \"ImageId\"),results],axis = 1)\n\nsubmission.to_csv(\"submission.csv\",index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}