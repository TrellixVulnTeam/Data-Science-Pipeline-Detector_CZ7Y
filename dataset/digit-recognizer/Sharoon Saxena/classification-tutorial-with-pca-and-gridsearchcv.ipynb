{"cells":[{"metadata":{"_uuid":"855d1bd2aad044e8623f1067f1bf07fd90360221"},"cell_type":"markdown","source":"# Objective:\nTo demonstrate Logistic Regression on Mnist dataset with the help of PCA and GridSearchCV"},{"metadata":{"_uuid":"3906a23aaba4c24700f2990412adbe2d1f32ad2e"},"cell_type":"markdown","source":"##  1. Importing the necessary Libraries\n\n### numpy\n        NumPy is the fundamental package for scientific computing with Python. It contains among other things: a powerful N-dimensional array object; sophisticated (broadcasting) functions\n\n### pandas\n        In computer programming, pandas is a software library written for the Python programming language for data manipulation and analysis. In particular, it offers data structures and operations for manipulating numerical tables and time series.\n        \n### datetime\n        In Python, date, time and datetime classes provides a number of function to deal with dates, times and time intervals. Date and datetime are an object in Python, so when you manipulate them, you are actually manipulating objects and not string or timestamps.\n        \n### matplotlib\n        Matplotlib is a plotting library for the Python programming language and its numerical mathematics extension NumPy."},{"metadata":{"trusted":true,"_uuid":"a81bad9e0e9389bed312ec2186797aaeafba23db","collapsed":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport datetime\nimport matplotlib.pyplot as plt\n#import warnings; warnings.simplefilter('ignore')\n%env JOBLIB_TEMP_FOLDER=/tmp","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5201fe0c14195088fc4501472d7185d63c9265d5"},"cell_type":"markdown","source":"## 2. Importing the dataset\nThis is done in the following way, Using the Pandas Library.\n#### Syntax: \n#### pd.read_csv('File_Name')\n\n### Also\n\n### We need to saperate the Independent and the dependent variables, i.e. x and y respectively.\nHere the matrix of 784 features are the independednt variable and the target labels corresponding to each image are the dependent variables.\n\n## 3. Making test and the training set\nThis is done so that the model trained must have some unseen data to test upon. Therefore 10,000 images are kept aside to test the performance of our model upon.\n\nThe MNIST dataset is divided in to 4 parts:\n\n## x_train : \n        The dataset over which the model will be trained upon, it is a set of 60,000 greyscale images. Our Logistic Regression Algorithm will be trained over this data\n        \n## y_train :\n        This is the labelled dataset corresponding to the x_train, this contains the target labels corresponding to the Images of the dataset.\n        \n## x_test : \n        This is the dataset kept aside to test the Machine Learning model trained upon the Dataset x_train, so that out model will have unseen data to predict and test upon.\n        \n## y_test :\n        This is the labelled dataset corresponding to the x_train, this contains the target labels corresponding to the Images of the dataset."},{"metadata":{"trusted":true,"_uuid":"17cce7a77d36b280708d8dea771714d5ca60786c","collapsed":true},"cell_type":"code","source":"x_train = pd.read_csv('../input/train.csv')\ny_train = x_train['label'].values\nx_train = x_train.drop('label', axis = 1).values\ntest    = pd.read_csv('../input/test.csv').values\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"64356442a241f3b8bb7bb1486f4cfda0dcff0fd6"},"cell_type":"markdown","source":"## 4. Feature Scaling:\nFeature scaling is a method used to standardize the range of independent variables or features of data. In data processing, it is also known as data normalization and is generally performed during the data preprocessing step.\n\n### Standardization\nIn machine learning, we can handle various types of data, e.g. audio signals and pixel values for image data, and this data can include multiple dimensions. Feature standardization makes the values of each feature in the data have zero-mean (when subtracting the mean in the numerator) and unit-variance. This method is widely used for normalization in many machine learning algorithms (e.g., support vector machines, logistic regression, and neural networks. The general method of calculation is to determine the distribution mean and standard deviation for each feature. Next we subtract the mean from each feature. Then we divide the values (mean is already subtracted) of each feature by its standard deviation.\n\n## Standard Scaler\n\n##  $$x'= \\frac{x - mean(x)}{\\sigma}$$\n"},{"metadata":{"_uuid":"8bc65017213f7c471bb71483d87c6a1ad9f6414b"},"cell_type":"markdown","source":"## Fitting the Scaler to the x_train\nso that the scaler can reshape itself to the X_train Dimensions\n"},{"metadata":{"trusted":true,"_uuid":"45f89ef526fd615ae1908bc08799f967a4342e48","collapsed":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaler.fit(x_train)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7e924d6fff742c989a892d867fc3bb65aad54b1d"},"cell_type":"markdown","source":"### Here the warning issued is about the change of values from Interger to Float, which is OKAY for us."},{"metadata":{"_uuid":"bad054f756752c0551f34890bad57c2daf903080"},"cell_type":"markdown","source":"## Now *implementing* the scalar over the x_train and x_test and transforming them to x_train_scaler and x_test_scaler"},{"metadata":{"trusted":true,"_uuid":"016ae9faa345c756518fad41af47bd89314e0021","collapsed":true},"cell_type":"code","source":"x_train_scaler = scaler.transform(x_train)\ntest_scaler = scaler.transform(test)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6a98d72ff1408e8196bba6d71110fd305b6b3d24"},"cell_type":"markdown","source":"## 5. Dimentionality Reduction\nHaving too many features in the training set can make the training process extremely slow and makes it unlikely to get a good solution.\nIn statistics, machine learning, and information theory, dimensionality reduction or dimension reduction is the process of reducing the number of random variables under consideration by obtaining a set of principal variables. It can be divided into feature selection and feature extraction.\n\n## Principal component analysis (PCA)\nThe main linear technique for dimensionality reduction, principal component analysis, performs a linear mapping of the data to a lower-dimensional space in such a way that the variance of the data in the low-dimensional representation is maximized."},{"metadata":{"_uuid":"67125da826b5c868a345fa5caa7b149dd90aea35"},"cell_type":"markdown","source":"### Ploting the relation between the Variance and the Dimensions"},{"metadata":{"scrolled":false,"trusted":true,"_uuid":"de9a19bfecccb78ead8b6db087df0ba28776056f","collapsed":true},"cell_type":"code","source":"#Using elbow-plot variance/dimensions\nfrom sklearn.decomposition import PCA\npca = PCA()\npca.fit(x_train_scaler)\ncumsum = np.cumsum(pca.explained_variance_ratio_)*100\nd = [n for n in range(len(cumsum))]\nplt.figure(figsize=(10, 10))\nplt.plot(d,cumsum, color = 'red',label='cumulative explained variance')\nplt.title('Cumulative Explained Variance as a Function of the Number of Components')\nplt.ylabel('Cumulative Explained variance')\nplt.xlabel('Principal components')\nplt.axhline(y = 95, color='k', linestyle='--', label = '95% Explained Variance')\nplt.legend(loc='best')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e06321bc3c8904703695ae321bd4c15570d362c3"},"cell_type":"markdown","source":"### Applying PCA and Transforming x_train_scaler and x_test_scaler to x_train_pca and x_test_pca respectively"},{"metadata":{"trusted":true,"_uuid":"8f088c9911d5a36e515a3367bf752d3919e8dff4","collapsed":true},"cell_type":"code","source":"from sklearn.decomposition import PCA\npca = PCA(.95) \npca.fit(x_train_scaler)\n\nx_train_pca = pca.transform(x_train_scaler)\ntest_pca        = pca.transform(test_scaler)\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8c98449fe6327e9d0bc4ef0eeb11cff16416305d"},"cell_type":"markdown","source":"Here the the min Variance is set to 95%, therefore the the minimum dimentions are chosen keeping the stated Variance."},{"metadata":{"_uuid":"38dc98764eb6d6e333da26638dfd0d41fa339393"},"cell_type":"markdown","source":"## 6. Comparing the images before and after applying the PCA"},{"metadata":{"trusted":true,"_uuid":"7f37facd096080eeaa6545ca83d122229d09fafd","collapsed":true},"cell_type":"code","source":"sample = x_train[23]\nsample.shape = (28,28)\n\n\na = plt.subplot(1,2,1)\na.set_title('Original Image')\nplt.imshow(sample, cmap = plt.cm.gray_r)\n\nsample = pca.inverse_transform(x_train_pca[23])\nsample.shape = (28,28)\n\nb = plt.subplot(1,2,2)\nb.set_title(\"Reduced after PCA\")\nplt.imshow(sample, cmap = plt.cm.gray_r)\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a54305fad32abdaf04782d2577e5443530fee5ab"},"cell_type":"markdown","source":"## 7. Logistic Regression:\nlogistic regression is a regression model where the dependent variable (DV) is categorical, where the output can take only two values, \"0\" and \"1\", which represent outcomes such as pass/fail, win/lose, alive/dead or healthy/sick. Cases where the dependent variable has more than two outcome categories may be analysed in multinomial logistic regression.\n\n###### MNIST dataset in a Multivariate Classification problem with 10 classes ( 0, 1, 2, ... , 9)\n\n## Applying the Logistic Regression model:"},{"metadata":{"_uuid":"152e7aa03b0a79e04e1678623dd46f578e0a189a"},"cell_type":"markdown","source":"### GridSearch Cross Validation\nIn machine learning, two tasks are commonly done at the same time in data pipelines: cross validation and (hyper)parameter tuning. Cross validation is the process of training learners using one set of data and testing it using a different set. Parameter tuning is the process to selecting the values for a model’s parameters that maximize the accuracy of the model."},{"metadata":{"trusted":true,"_uuid":"75a5f767dbe698ba81f5ff81d2a15eb7aeb1b11f","scrolled":true,"collapsed":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\n\n# making skeletal model\nlogistic_regression = LogisticRegression(n_jobs = -1)\n\n# Set of parameters we want to try for out Model\nparameters = { 'C' : [1.1,1.25,1.5]}\n\n#Running the Model with above chosen parameter\ngrid_search = GridSearchCV(estimator = logistic_regression, param_grid = parameters , scoring = 'accuracy', cv = 3, n_jobs = -1 , verbose = 2)\ngrid_scores = grid_search.fit(x_train_pca , y_train)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f4bc97b09f6030ce19b5ba645658d2f336bb59b0","scrolled":true,"collapsed":true},"cell_type":"code","source":"print( grid_search.best_score_)\nprint(grid_search.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f800ccd5591ddb81d6644d76ddf3c3cbe09cd007"},"cell_type":"markdown","source":"#### As the best Hyper parameter turns out to be C = 0.1\n#### The final model should be made with the above mentioned Hyper-parameters\n         "},{"metadata":{"trusted":true,"_uuid":"8fa633da65b21ab53a4a732507746fe68327505a","collapsed":true},"cell_type":"code","source":"# Making the Final Classification model.\nlogistic_regression = LogisticRegression( C = 1.25, n_jobs = -1)\ntick =datetime.datetime.now()\nlogistic_regression.fit(x_train_pca, y_train)\ntock=datetime.datetime.now()\nlr_train_time = tock - tick\nprint(\"Time taken for training a Logistic Regression model = \" + str(lr_train_time))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"13e0259d0af556b7f3fb51ebea80eeea5f769834"},"cell_type":"markdown","source":"## 8. Predicting values on training set"},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"2678901236c73d1e9786d72770689c61e5a00cc2","collapsed":true},"cell_type":"code","source":"tick=datetime.datetime.now()\nlr_train_predict=logistic_regression.predict(x_train_pca)\ntock=datetime.datetime.now()\nlr_pred_train_time = tock - tick\nprint('Time taken to predict the data points in the Test set is : ' + str(lr_pred_train_time))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e30b41755f00a5526e985d164da478081ad73185"},"cell_type":"markdown","source":"## 9. Confusion Matrix (Training Set)\nIn the field of machine learning and specifically the problem of statistical classification, a confusion matrix, also known as an error matrix,is a specific table layout that allows visualization of the performance of an algorithm, typically a supervised learning one (in unsupervised learning it is usually called a matching matrix). Each row of the matrix represents the instances in a predicted class while each column represents the instances in an actual class (or vice versa).The name stems from the fact that it makes it easy to see if the system is confusing two classes (i.e. commonly mislabelling one as another).\n\nIt is a special kind of contingency table, with two dimensions (\"actual\" and \"predicted\"), and identical sets of \"classes\" in both dimensions (each combination of dimension and class is a variable in the contingency table)."},{"metadata":{"trusted":true,"_uuid":"517d2dbf61c5f0ba5ec330af046760598ad0b1c4","collapsed":true},"cell_type":"code","source":"#Making the confusion Matrix\n\nfrom sklearn.metrics import confusion_matrix\ncf= confusion_matrix(y_train, lr_train_predict)\n\n# Visualizing the Confusion Matrix`\n\nplt.matshow(cf , cmap = plt.cm.gray, )\nplt.show()\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c8de9f978bdd2c3238a8adb05899f6f0972db598"},"cell_type":"markdown","source":"### Here the rows represent the ACTUAL CLASS and the columns represent PREDICTED CLASS.\nthe whiteness across the diagonal shows the high values that Most of the class were predicted correctly."},{"metadata":{"_uuid":"b9684faf4a2da7464b0110e73d9554f5cb2ed740"},"cell_type":"markdown","source":"## 10. Error Analysis (Training Set)"},{"metadata":{"trusted":true,"_uuid":"33be985d5a2caf31430c3c29ca38e505ff14f671","collapsed":true},"cell_type":"code","source":"# Analyzing the Errors\nrow_sums = cf.sum(axis=1 , keepdims = True)\nnormal_cf = cf/row_sums\n\nnp.fill_diagonal(normal_cf,0)\nplt.matshow(normal_cf,cmap = plt.cm.gray)\n\nplt.title(\"error Analysis\")\nplt.show()\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"afc550607f2e58b56784cbceef818f42105d5c82"},"cell_type":"markdown","source":"### Looking at the error analysis:\n\n#### most prominent errors are:\n\n$4,9$ and $9,4$\n\n$9,7$ and $7,9$\n\n$8,5$ but not $5,8$\n\n$5,3$ and $3,5$\n\n\n\n##### That is these values were often misunderstood ,\n##### but it should also be noted that the most of the errors are not symmetrical."},{"metadata":{"_uuid":"67cd080772d69190b46536d158c08117d75ff2e4"},"cell_type":"markdown","source":"## 11. Accuracy on Training Set"},{"metadata":{"trusted":true,"_uuid":"7b5a2941e413650836cebc146e4cc6ee69bbc122","collapsed":true},"cell_type":"code","source":"k = logistic_regression.score(x_train_pca, y_train)\nprint('the Accuracy on the Training set come out to be : ' + str(k))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b8cd539f4366c00a88f70d0cd1238535bc399ed5"},"cell_type":"markdown","source":"##### Accuracy on the training set came out to be $92.51$% which is vey good indeed."},{"metadata":{"_uuid":"55ab16d2837d2fee75805c5fd3e9527fea47a96b"},"cell_type":"markdown","source":"## 12. Lets have a look at the classification report of prediction on training set"},{"metadata":{"_uuid":"eb785b6ed471bc8c08823e1974c15d387f1198f5"},"cell_type":"markdown","source":"### True Positives (TP) - \nThese are the correctly predicted positive values which means that the value of actual class is yes and the value of predicted class is also yes. E.g. if actual class value indicates that this passenger survived and predicted class tells you the same thing.\n\n### True Negatives (TN) - \nThese are the correctly predicted negative values which means that the value of actual class is no and value of predicted class is also no. E.g. if actual class says this passenger did not survive and predicted class tells you the same thing.\n\nFalse positives and false negatives, these values occur when your actual class contradicts with the predicted class.\n\n### False Positives (FP) –\nWhen actual class is no and predicted class is yes. E.g. if actual class says this passenger did not survive but predicted class tells you that this passenger will survive.\n\n### False Negatives (FN) –\nWhen actual class is yes but predicted class in no. E.g. if actual class value indicates that this passenger survived and predicted class tells you that passenger will die.\n\nOnce you understand these four parameters then we can calculate Accuracy, Precision, Recall and F1 score.\n\n### F1 Score\nIn statistical analysis of binary classification, the F1 score (also F-score or F-measure) is a measure of a test's accuracy. It considers both the precision p and the recall r of the test to compute the score: p is the number of correct positive results divided by the number of all positive results returned by the classifier, and r is the number of correct positive results divided by the number of all relevant samples (all samples that should have been identified as positive). The F1 score is the harmonic average of the precision and recall, where an F1 score reaches its best value at 1 (perfect precision and recall) and worst at 0.\n\n ### $$F1 =\\frac{2}{\\frac{1}{Precision} + \\frac{1} {Recall} }$$"},{"metadata":{"trusted":true,"_uuid":"cfbce6241bd6415ff580cc1f6e2b11ca3f95963b","collapsed":true},"cell_type":"code","source":"import sklearn.metrics as skm\nprint(skm.classification_report( y_train , lr_train_predict ))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a2c0422edd80e67a07328e074006acee9b90af05"},"cell_type":"markdown","source":"## The F1 Score comes out to be 0.92, which is reasonably good."},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"240924b7541129590f2bb35a5f80937cba0e894f"},"cell_type":"markdown","source":"# Running model on the test set."},{"metadata":{"trusted":true,"_uuid":"c848cb217341dd78bb888ab2e380622e9dc97a78","collapsed":true},"cell_type":"code","source":"predict = logistic_regression.predict(test_pca)\nprint(predict)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"41981d253aadbb0d3fcab52099eee599bc2660aa"},"cell_type":"markdown","source":"### Creating a submission CSV file"},{"metadata":{"trusted":true,"_uuid":"63d5a00240e9c584562c8ee64a7ebb624a517f95","collapsed":true},"cell_type":"code","source":"\nsubmission = pd.DataFrame({'ImageId': range(1,len(predict)+1),\n                           'Label': predict})\nprint(submission)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"444a17fded680d8e68f30a99fbaf42174d0cfecd","collapsed":true},"cell_type":"code","source":"submission.to_csv('out.csv', header=True, index = False)   # Generating output csv\nprint(submission)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"e34475e500ce9d4d32a8f2c46baa4d9eaecccf7c"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"a8b10f41d9dd7518f4a6ac83cce15246c2573dd8"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}