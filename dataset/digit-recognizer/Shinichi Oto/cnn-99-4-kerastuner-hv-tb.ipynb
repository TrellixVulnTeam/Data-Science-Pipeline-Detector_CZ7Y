{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import time\nimport itertools\nimport json\nimport panel as pn\nimport numpy as np\nimport pandas as pd\nimport holoviews as hv\nimport folium\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport datetime, os\n\nfrom panel.widgets import Tqdm\n\nfrom folium.plugins import HeatMap, HeatMapWithTime, MarkerCluster\n\nfrom bokeh import models, plotting, io\nfrom bokeh.layouts import column\nfrom bokeh.plotting import figure, show\nfrom bokeh.models import ColumnDataSource, RangeTool, BoxAnnotation\n\nfrom holoviews import opts\nfrom holoviews.streams import Pipe, Buffer\nfrom holoviews.operation.timeseries import rolling, rolling_outlier_std\n\nfrom scipy.linalg import LinAlgError\n\nimport statsmodels.api as sm\nfrom statsmodels.tsa.stattools import adfuller, acf, pacf\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nfrom statsmodels.graphics.gofplots import qqplot\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\n\nfrom sklearn.metrics import ConfusionMatrixDisplay\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import minmax_scale\nfrom sklearn.pipeline import Pipeline\n\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\nfrom tensorflow.keras.optimizers import RMSprop,Adam\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.callbacks import ModelCheckpoint\n\n\nimport keras_tuner as kt\nimport tensorflow as tf\nfrom tensorflow import keras\n\n\n\nhv.extension('bokeh', 'plotly')\nhv.renderer('bokeh').theme= 'dark_minimal'\n\n%matplotlib inline","metadata":{"_kg_hide-input":true,"jupyter":{"source_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style= \"background-color:#000000;font-family:Georgia;color:#FFFFFF;font-size:400%;text-align:center;border-radius:10px 10px;border-style:solid;border-width:3px;border-color:#000000;\"><b>CNN</b></p>\n\n# Please give me an UPVOTE if you can. Your UPVOTE will be a great encouragement to me!","metadata":{"execution":{"iopub.status.busy":"2022-02-24T12:41:13.570988Z","iopub.execute_input":"2022-02-24T12:41:13.571635Z","iopub.status.idle":"2022-02-24T12:41:13.585837Z","shell.execute_reply.started":"2022-02-24T12:41:13.571542Z","shell.execute_reply":"2022-02-24T12:41:13.585222Z"}}},{"cell_type":"code","source":"train_set = pd.read_csv('../input/digit-recognizer/train.csv')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_set.head(2)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_set.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_set = pd.read_csv('../input/digit-recognizer/test.csv')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_set.head(2)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_set.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_set.info(), test_set.info()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = train_set.drop(labels=['label'], axis=1)\nY_train = train_set['label']","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n---","metadata":{}},{"cell_type":"code","source":"y_count = pd.pivot_table(Y_train.reset_index(), values='index', index='label', aggfunc='count')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hv.Bars(y_count).opts(width=600, height=500, tools=['hover'], color='Green')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_1 = X_train.iloc[0].values\nimage_1 = image_1.reshape((28, 28))\n\nimage_2 = X_train.iloc[3].values\nimage_2 = image_2.reshape((28,28))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img_1 = hv.Image(image_1)\nimg_2 = hv.Image(image_2)\nimg_1.opts(width=600, height=500, tools=['hover'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img_2.opts(width=600, height=500, tools=['hover'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = X_train / 255.0\ntest = test_set / 255.0\nprint(\"x_train shape : {} \".format(X_train.shape))\nprint(\"test shape: {} \".format(test.shape))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Reshape\nX_train = X_train.values.reshape(-1,28,28,1)\ntest = test_set.values.reshape(-1,28,28,1)\nprint(\"x_train shape: {}\".format(X_train.shape))\nprint(\"test shape: {}\".format(test.shape))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Y_train = to_categorical(Y_train, num_classes=10)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Y_train.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.1, random_state=42)\nprint(\"x_train shape : {}, dtype : {}\".format(X_train.shape, X_train.dtype))\nprint(\"x_test shape : {}, dtype : {}\".format(X_val.shape, X_val.dtype))\nprint(\"y_train shape : {}, dtype : {}\".format(Y_train.shape, Y_train.dtype))\nprint(\"y_test shape : {}, dtype : {}\".format(Y_val.shape, Y_val.dtype))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hv.Image(X_train[2][:,:,0])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"1\"></a>\n### <p style=\"background-color:#000000;font-family:Georgia;color:#FFFFFF;font-size:250%;text-align:center;border-radius:10px 10px;\">Keras-Tuner</p>\n\n* Adjust the number of units in the first Dense layer. Just define an integer hyperparameter in hp.Int('units', min_value=32, max_value=512, step=32). Its range is from 32 to 512. The minimum step to walk the interval is 32 if sampling from it.\n\n* There are many other types of hyperparameters. In this function, you can define multiple hyperparameters. In the following code, we tune whether to use the dropout layer with hp.Boolean(), the activation function with hp.Choice(), and the learning rate of the optimizer with hp.Float().\n\n---\n* * The architecture of a CNN takes the form of several convolutional layers (Conv layers) stacked on top of each other, followed by pooling layers, convolutional layers, and pooling layers; as the CNN moves forward, the information passed to the input becomes smaller, but also deeper due to the effect of the convolutional layers and the generation of feature maps. At the top of the CNN stack is an additional neural network, which is usually an advancing neural network consisting of several fully coupled layers, with the last normal layer outputting predictions. If the normal layer outputs the probability of a class, use the softmax layer.\n\n* The caveat to using Conv layers with large kernels other than the initial Conv layer is that rather than using a Conv layer with a 5×5 kernel, stacking two 3×3 Conv layers will usually give better performance than using a larger kernel. The reason why the first Conv layer is an exception is that by using a 5×5 kernel with stride 2, it is possible to reduce the dimensionality of the space without losing too much information, and it also solves the cost problem.\n\n* Increasing the number of filters as we move towards the output layer. Although the number of low-level features is often small, the number of features created by combining them is large, so it makes sense to do it this way. The most common method is to double the number of filters each time it passes through the pooling layer.\n\n* In the full join layer, it is assumed that the features for each instance are in a 1D array, so the input has to be flattened before the full join layer.\n* The subsequent dropout layer is used to mitigate overtraining, and is conditionalized by parameter search.\n---\n* * For the first layer in the following function, if the input image is not very large, it is better to use a larger value and no stride as a filter.\n\n### <p style=\"background-color:#000000;font-family:Georgia;color:#FFFFFF;font-size:250%;text-align:center;border-radius:10px 10px;\">optimizer</p>\n\n* optimizer='sgd' : To train the model using a simple stochastic gradient descent method. Run a back-propagation algorithm (a combination of reverse-mode automatic differentiation and gradient descent).\n    - When using the sgd optimizer, it is important to adjust the learning rate. So, in the general syntax, use the following to adjust the learning rate. Default is lr=0.01\n        - optimizer=keras.optimizer.SGD(lr=n)\n        - Adam, Nadam...\n---\n* Efficient optimization\n    - Streamline the gradient descent part.\n    - Keep the automatic differentiation part.\n \n* The reason for using loss='sparse_categorical_crossentropy' is when the labels are sparse (only 0 to 9 target class indices for individual instances) and the classes are mutually exclusive.\n* loss='categorical_crossentropy' loss function is used when the target probability is calculated for each class in each instance (0, 0, 0, 1, ...) 0, 0, 0, ...) This loss function such as\n\n* In the case of binary classification (with one anomalous binary label), use \"sigmoid\" (logistic function) instead of \"softmax\" as the activation function for the output layer, so use loss=\"binary_crossentropy\".\n-----\n- The reason why we use loss='categorical_crossentropy' is because from tensorflow.keras.utils import to_categorical Y_train = to_categorical(Y_train, num_ classes=10) to implement one-hot encoding.\n\n---\n* metrics=\"accuracy\" Since this is a classification problem, we measure the percentage of correct answers during training evaluation.","metadata":{}},{"cell_type":"code","source":"def build_model(hp):\n    model = keras.Sequential()\n    model.add(Conv2D(32, kernel_size=(5,5),\n                     activation=\"relu\", kernel_initializer='he_normal',\n                     input_shape=(28,28,1)))\n    model.add(Conv2D(32, kernel_size=(3, 3), \n                     activation='relu',kernel_initializer='he_normal'))\n    model.add(MaxPool2D(pool_size=(2,2)))\n    \n    model.add(Dropout(hp.Choice('rate', [0.2, 0.25, 0.3, 0.4, 0.5]))) #0.25\n    \n    model.add(Conv2D(64, kernel_size=(3,3), padding='Same',\n                     activation=\"relu\"))\n    model.add(Conv2D(64, kernel_size=(3,3), padding='Same',\n                     activation=\"relu\"))\n    \n    model.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\n    model.add(Dropout(hp.Choice('rate', [0.2, 0.25, 0.3, 0.4, 0.5])))\n    \n    model.add(Flatten())\n    \n    model.add(Dense(256, activation=\"relu\"))\n    if hp.Boolean(\"dropout\"):\n        model.add(Dropout(hp.Choice('rate', [0.2, 0.25, 0.3, 0.4, 0.5])))\n    model.add(Dense(10, activation=\"softmax\"))\n\n    \n    learning_rate = hp.Float(\"learning_rate\", min_value=1e-4, max_value=1e-2, sampling=\"log\")\n    optimizer = Adam(lr=learning_rate, beta_1=0.9, beta_2=0.999)\n    model.compile(\n        optimizer=optimizer,\n        loss=\"categorical_crossentropy\",\n        metrics=[\"accuracy\"],\n    )\n    return model","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"build_model(kt.HyperParameters())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tuner = kt.RandomSearch(\n    build_model,\n    objective='val_accuracy', \n    max_trials=2) ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tuner.search(X_train, Y_train, epochs=2, validation_data=(X_val, Y_val))\nbest_model = tuner.get_best_models()[0]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_model.summary()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"keras.utils.plot_model(best_model, show_shapes=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epochs = 25\nbatch_size = 250","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <p style=\"background-color:#000000;font-family:Georgia;color:#FFFFFF;font-size:150%;text-align:center;border-radius:10px 10px;\">Data expansion</p>","metadata":{}},{"cell_type":"code","source":"datagen = ImageDataGenerator(\n        featurewise_center=False, samplewise_center=False,\n        featurewise_std_normalization=False, samplewise_std_normalization=False, \n        zca_whitening=False, rotation_range=12, zoom_range = 0.18,\n        width_shift_range=5, height_shift_range=5,  \n        horizontal_flip=False, vertical_flip=False)\n\ndatagen.fit(X_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <p style=\"background-color:#000000;font-family:Georgia;color:#FFFFFF;font-size:150%;text-align:center;border-radius:10px 10px;\">Early Stopping</p>\n\n* Abort training and optionally rollback to the best model if performance on the validation set does not improve by the number of epochs specified by the patience argument.\n* Use checkPoint to save model checkpoints (in case of PC crash) and terminate training early when performance fails (time and computational resources).\n* Maintain the best weights and restore them at the end of the training (basically, there is no need to save and restore the best model when using together).","metadata":{}},{"cell_type":"code","source":"early_stopping = EarlyStopping(\n                        monitor='val_loss',\n                        min_delta=0.0,\n                        patience=10, restore_best_weights=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <p style=\"background-color:#000000;font-family:Georgia;color:#FFFFFF;font-size:150%;text-align:center;border-radius:10px 10px;\">Scheduling by performance</p>\n\n\n- Multiply the learning rate by 0.5 if the best verification loss does not improve in 5 consecutive epochs","metadata":{}},{"cell_type":"code","source":"learning_rate_reduction = ReduceLROnPlateau(monitor='val_loss', \n                                            patience=3, \n                                            verbose=1, \n                                            factor=0.5, \n                                            min_lr=0.00001)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* If you use a validation set during training, you can set save_best_only=True when you create a checkpoint. In this case, you can save the model only when its performance against the validation set is the best.\n* Using this, you don't have to worry about overtraining the training set forever.\n    - If we simply restore the saved model after training, it will be the best performing model for the validation set. This is the model callback that corresponds to early stopping","metadata":{}},{"cell_type":"code","source":"checkpoint = ModelCheckpoint(\"keras_model.h5\", save_best_only=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Overlearning management, custom callbacks","metadata":{}},{"cell_type":"code","source":"class ValTrainRatioCustomCallback(keras.callbacks.Callback):\n    def on_epoch_end(self, epoch, logs):\n        print(\"\\nval/train: {:.2f}\".format(logs[\"val_loss\"] / logs[\"loss\"]))\n\ncustom_call = ValTrainRatioCustomCallback()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\ntensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* X_train: input features, Y_train: target class\n    - Number of training epochs: Unspecified, default is 1, but 1 is not enough to converge to a good solution, so it must be specified.\n    - validation_data=(X_val, Y_val) : By passing the validation set, it measures and displays the loss and other indicators for the validation set.\n        - If the performance on the training set is much better than the performance on the validation set, then the model is probably overtraining the training set. If the performance on the training set is much better than the performance on the validation set, then the model is probably overtraining the training set. Other possible bugs exist, such as data gaps between the training and validation sets.\n    - You can also use validation_split=n to specify how much of the training set to use as validation data, without passing the validation set in the validation_data argument. 0.1 will use 10% from the end of the training set.","metadata":{}},{"cell_type":"code","source":"history = best_model.fit(datagen.flow(X_train,Y_train, batch_size=batch_size),\n                              epochs = epochs, validation_data = (X_val,Y_val), steps_per_epoch=X_train.shape[0] // batch_size,\n                              callbacks=[learning_rate_reduction, early_stopping, checkpoint, custom_call, tensorboard_callback])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_h5 = keras.models.load_model('keras_model.h5')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <p style=\"background-color:#000000;font-family:Georgia;color:#FFFFFF;font-size:150%;text-align:center;border-radius:10px 10px;\">TensorBoard</p>","metadata":{}},{"cell_type":"code","source":"from tensorboard import notebook\nnotebook.list()\n#notebook.display(port=6006, height=800)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%reload_ext tensorboard","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%load_ext tensorboard","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%tensorboard --logdir logs","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss = hv.Curve(pd.DataFrame(history.history)['loss'], label='loss')\naccuracy = hv.Curve(pd.DataFrame(history.history)['accuracy'], label='accuracy')\nval_loss = hv.Curve(pd.DataFrame(history.history)['val_loss'], label='val_loss')\nval_accuracy = hv.Curve(pd.DataFrame(history.history)['val_accuracy'], label='val_accuracy')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"overlay_0 = (loss * val_loss).relabel('Loss').opts(ylabel='Loss', xlabel='Epochs')\noverlay_1 = (accuracy * val_accuracy).relabel('Accuracy').opts(ylabel='Accuracy', xlabel='Epochs')\n(overlay_0 + overlay_1).opts(opts.Curve(width=500, tools=['hover']), opts.Overlay(legend_position='right'))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hv.Curve(history.history['val_loss'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Y_pred = best_model.predict(X_val)\n \nY_pred_classes = np.argmax(Y_pred,axis=1) \n\nY_true = np.argmax(Y_val,axis=1) ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(Y_val, Y_pred.astype(int), zero_division=True))\ndisp = ConfusionMatrixDisplay(confusion_matrix=confusion_matrix(Y_true, Y_pred_classes))\nfig, ax = plt.subplots(figsize=(10,10))\ndisp.plot(cmap='Blues', ax=ax);","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_model.evaluate(test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results = best_model.predict(test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results = np.argmax(results, axis=1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub = pd.read_csv('../input/digit-recognizer/sample_submission.csv')\nsub['Label'] = results\nsub.to_csv('submission.csv', index=False)","metadata":{},"execution_count":null,"outputs":[]}]}