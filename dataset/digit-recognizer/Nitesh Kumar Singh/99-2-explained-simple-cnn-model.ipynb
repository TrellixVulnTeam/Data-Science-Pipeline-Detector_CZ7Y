{"cells":[{"metadata":{},"cell_type":"markdown","source":"**Lets Import our Tools**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport cv2\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport random\nfrom sklearn.model_selection import train_test_split\nfrom PIL import Image,ImageFilter","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**And our Data**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/digit-recognizer/train.csv\")\nX_test_main = pd.read_csv(\"/kaggle/input/digit-recognizer/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preprocessing","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**So We split the data into X and Y**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"Y = df.iloc[:,0]\nX = df.iloc[:,1:]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**And convert them to numpy for preprocessing**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X = X.to_numpy()\nY = Y.to_numpy()\nX_test_main = X_test_main.to_numpy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**So we will reshape it into proper image shapes as we will be using convolutional networks**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X = X.reshape(-1,28,28)\nX_test_main = X_test_main.reshape(-1,28,28)\nprint(X.shape)\nprint(X_test_main.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Lets have a look at our data**","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"w=14\nh=14\nfig=plt.figure(figsize=(w,h))\ncolumns = 4\nrows = 5\nfor i in range(1, rows*columns+1):\n    img1 = X[i+random.randrange(1,300)]\n    fig.add_subplot(rows, columns, i)\n    plt.imshow(img1)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = np.expand_dims(X,axis=-1)\nX_test_main = np.expand_dims(X_test_main,axis=-1)\nX_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size= 0.2,shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X_train.shape,Y_train.shape)\nprint(X_test.shape,Y_test.shape)\nprint(X_test_main.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. **Alright then lets normalize the images and go to the model**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = X_train/255.\nX_test = X_test/255.\nX_test_main = X_test_main/255.\nprint(X_train.shape)\nprint(X_test.shape)\nprint(X_test_main.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**SO 10 classes, One hot matrix encoding**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def one_hottie(labels,C):\n    \"\"\"\n    One hot Encoding is used in multi-class classification problems to encode every label as a vector of binary values\n        eg. if there are 3 class as 0,1,2\n            one hot vector for class 0 could be : [1,0,0]\n                           then class 1: [0,1,0]\n                           and class 2: [0,0,1]\n    We need this encoding in out labels for the model learns to predict in a similar way.\n    \n    Without it,if only integer values are used in labels,it could affect model in different ways,\n        such as predicting a class that does not exist.\n        \n    \"\"\"\n    One_hot_matrix = tf.one_hot(labels,C)\n    return tf.keras.backend.eval(One_hot_matrix)\n\nY_test_later = Y_test.copy()\nY_train = one_hottie(Y_train, 10)\nY_test = one_hottie(Y_test, 10)\nprint (\"Y shape: \" + str(Y_train.shape))\nprint (\"Y test shape: \" + str(Y_test.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# def res_net_block(input_data, filters=[128], conv_size=[3,5]):\n#     x = tf.keras.layers.Conv2D(filters[0], conv_size[0], activation='relu', padding='same')(input_data)\n#     x = tf.keras.layers.BatchNormalization()(x)\n#     x = tf.keras.layers.Conv2D(filters[0], conv_size[1], activation=None, padding='same')(x)\n#     x = tf.keras.layers.BatchNormalization()(x)\n#     x = tf.keras.layers.Add()([x, input_data])\n#     x = tf.keras.layers.Activation('relu')(x)\n#     return x","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# BUILDING THE MODEL","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Lets use a sequential for real fast assemble of layers**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"As the data is quite simple, we won't need any complex model, so here is just a simple good enough model","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"A conv2d block = \"**CONV2D(number of filters,size of filters) -> ReLU -> MAXPOOL2D**\"\n\n*INPUT DATA -> CONV2D(64,3) -> ReLU -> CONV2D BLOCK(128,3) -> CONV2D BLOCK(256,3) -> CONV2D BLOCK(256,3) -> \nFlatten out the ouput -> DENSE(100) -> DROPOUT(0.4) -> ReLU -> DENSE(50) -> ReLU -> DROPOUT(0.4) -> DENSE(10) -> OUTPUT DATA(that is kinda of likeliness of each particular class being correct) -> SOFTMAX FOR CLASSIFICATION\n\nBETTER TO CHECK THIS OUT AT MODEL SUMMARY","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model = tf.keras.Sequential([\n    tf.keras.layers.Conv2D(64, 3, activation='relu', input_shape=(28,28,1),padding=\"same\"),\n    tf.keras.layers.MaxPool2D(strides=2),\n    \n    \n    tf.keras.layers.Conv2D(128, 3, activation='relu',padding=\"same\"),\n    tf.keras.layers.MaxPool2D(strides=2),\n    \n    tf.keras.layers.Dropout(0.2),\n        \n    tf.keras.layers.Conv2D(256, 3, activation='relu',padding=\"same\"),\n    tf.keras.layers.MaxPool2D(strides=2),\n    \n    tf.keras.layers.Conv2D(256, 3, activation='relu',padding=\"same\"),\n    tf.keras.layers.MaxPool2D(strides=2),\n        \n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(100,kernel_regularizer=tf.keras.regularizers.l2(0.01), activation='relu'),\n    \n    tf.keras.layers.Dense(50,kernel_regularizer=tf.keras.regularizers.l2(0.01), activation='relu'),\n        \n    tf.keras.layers.Dense(10, kernel_regularizer=tf.keras.regularizers.l2(0.01) ,activation='softmax')\n])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## MODEL SUMMARY","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# initial_learning_rate = 0.001 #initial rate\n# # Rate decay with exponential decay\n# # new rate = initial_learning_rate * decay_rate ^ (step / decay_steps)\n\n# lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n#     initial_learning_rate,\n#     decay_steps=800,\n#     decay_rate=0.5,\n#     staircase=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**For simplicity , We won't be varying the learning rate with scheduler, instead we will just train it multiple times**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* For the 1st train, I'll boost the training with a 0.006 learning rate in just 20 epochs\n* Actually less than that were required, i just made sure it gets a little stable","execution_count":null},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"model.compile(optimizer=tf.keras.optimizers.Nadam(learning_rate=0.006),\n              loss=tf.keras.losses.CategoricalCrossentropy(),\n              metrics=['accuracy','Recall','Precision'])\n\nresult = model.fit(x=X_train,y=Y_train,batch_size=64,epochs=20,verbose=1,shuffle=False,initial_epoch=0,\n                   validation_split=0.1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So right now we already hit 98 but that is expected as the data is just too simple","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"LETS CHECK OUT THE PLOTS\n* As you can see, it hits the 96+ immediately and gets stable there\n* So now we will reduce the learning rate by a lot, and train the model for real","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plt.plot(result.history['acc'], label='train')\nplt.plot(result.history['val_acc'], label='valid')\nplt.legend(loc='upper left')\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.show()\nplt.plot(result.history['loss'], label='train')\nplt.plot(result.history['val_loss'], label='test')\nplt.legend(loc='upper right')\nplt.title('Model Cost')\nplt.ylabel('Cost')\nplt.xlabel('Epoch')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"See those spikes, that tells me that learning rate was a bit higher.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**2nd TRaining**\nNow we start form 99% but taking this to 100 would be hard with this model\n* Also we need to reduce the loss a lot. Being such a simple model the loss is still high","execution_count":null},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"model.compile(optimizer=tf.keras.optimizers.Nadam(learning_rate=0.0001),\n              loss=tf.keras.losses.CategoricalCrossentropy(),\n              metrics=['accuracy','Recall','Precision'])\n\nresult = model.fit(x=X_train,y=Y_train,batch_size=64,epochs=40,verbose=1,shuffle=False,initial_epoch=20,\n                   validation_split=0.1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**So we barely touched 99.1 here**\n* I saw a variance problem here so i actually reduced dropouts.","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plt.plot(result.history['acc'], label='train')\nplt.plot(result.history['val_acc'], label='valid')\nplt.legend(loc='upper left')\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.show()\nplt.plot(result.history['loss'], label='train')\nplt.plot(result.history['val_loss'], label='test')\nplt.legend(loc='upper right')\nplt.title('Model Cost')\nplt.ylabel('Cost')\nplt.xlabel('Epoch')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***3rd Training***\nWe reduce the learning rate further to check more performance can be extracted here","execution_count":null},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"model.compile(optimizer=tf.keras.optimizers.Nadam(learning_rate=0.00006),\n              loss=tf.keras.losses.CategoricalCrossentropy(),\n              metrics=['accuracy','Recall','Precision'])\n\nresult = model.fit(x=X_train,y=Y_train,batch_size=64,epochs=60,verbose=1,shuffle=False,initial_epoch=40,\n                   validation_split=0.1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n\n**And WHen you check out the training and validation set combined**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"check = model.evaluate(X_test,Y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**We have hit 99.3\n% now**\n* So lets see where things went wrong with a confusion matrix","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = model.predict_classes(X)\npreds.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# X = pd.read_csv(\"/kaggle/input/digit-recognizer/train.csv\")\n# Y_test = X.iloc[:,0]\n# Y_test = Y_test.to_numpy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"conf = tf.math.confusion_matrix(preds,Y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with tf.Session() as session:\n    print(conf.eval())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"One more training. This time with data Augmentation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_gen = tf.keras.preprocessing.image.ImageDataGenerator(rotation_range=20,\n                                                            zoom_range=0.20,\n                                                            width_shift_range=0.2,\n                                                            height_shift_range=0.2,\n                                                            shear_range=0.20,\n                                                            horizontal_flip=False,\n                                                            brightness_range=[0.1,1],\n                                                            rescale=1./255)\ntest_gen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As you can see I included rescale in the data generator, but the images are already normalized. So we denormalize it before passing it to generator","execution_count":null},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"model.compile(optimizer=tf.keras.optimizers.Nadam(learning_rate=0.00005),\n              loss=tf.keras.losses.CategoricalCrossentropy(),\n              metrics=['accuracy','Recall','Precision'])\n\nresult = model.fit(train_gen.flow(X_train*255,Y_train,batch_size=64),\n                   validation_data = test_gen.flow(X_test*255,Y_test,batch_size=16),\n                   epochs=70,\n                   verbose=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Make predictions","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = model.predict_classes(X_test_main)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"arr = [x for x in range(1,28001)]\nlabel = pd.DataFrame(arr,columns = [\"ImageId\"])\nlabel[\"Label\"] = pd.DataFrame(preds)\nlabel.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"label.to_csv('Y_test.csv',header=True,index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.save(\"saved_model\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}