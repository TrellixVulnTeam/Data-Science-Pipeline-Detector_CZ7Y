{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction\n\nWhile I've done many different machine learning projects, I've never actually done the original MNIST. I intend to use shufflenet, which is a odd approach to this problem since it is often used with much larger images. I will do neural surgery to make it work with the smaller images present in this dataset.  ","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd \nimport torch\nfrom torch import optim\nimport torch.nn as nn\nfrom PIL import Image\nfrom matplotlib import pyplot as plt\nimport os ","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-02T02:25:33.814947Z","iopub.execute_input":"2022-02-02T02:25:33.81633Z","iopub.status.idle":"2022-02-02T02:25:35.45261Z","shell.execute_reply.started":"2022-02-02T02:25:33.81617Z","shell.execute_reply":"2022-02-02T02:25:35.451433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create the Custom Dataset Class\n\nWe need this to be able to load the image and label into the model we will create. So we will create a custom dataset to handle this","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import Dataset\n\nclass CustomDataset(Dataset):\n  def __init__(self, X, y, BatchSize, transform):\n    super().__init__()\n    self.BatchSize = BatchSize\n    self.y = y\n    self.X = X\n    self.transform = transform\n    \n  def num_of_batches(self):\n    \"\"\"\n    Detect the total number of batches\n    \"\"\"\n    return math.floor(len(self.list_IDs) / self.BatchSize)\n\n  def __getitem__(self,idx):\n    class_id = self.y[idx]\n    img = self.X[idx].reshape(28,28)\n    img = Image.fromarray(np.uint8(img * 255)).convert('L')\n    img = self.transform(img)\n    return img, torch.tensor(int(class_id))\n\n  def __len__(self):\n    return len(self.X)","metadata":{"execution":{"iopub.status.busy":"2022-02-02T02:25:35.454704Z","iopub.execute_input":"2022-02-02T02:25:35.454939Z","iopub.status.idle":"2022-02-02T02:25:35.465031Z","shell.execute_reply.started":"2022-02-02T02:25:35.45491Z","shell.execute_reply":"2022-02-02T02:25:35.464202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Look at Images","metadata":{}},{"cell_type":"code","source":"# load data\ndf = pd.read_csv(r\"../input/digit-recognizer/train.csv\",dtype = np.float32)\n\n# Shuffle dataframe\ndf = df.sample(frac=1)\n\n# Split data into features X and labels y\nX = df.loc[:, df.columns != \"label\"].values / 255 \ny = df.label.values\n\nfig,ax = plt.subplots(2,5)\nfor i in range(10):\n    nparray = X[i].reshape(28,28)\n    image = Image.fromarray(nparray * 255)\n    ax[i%2][i//2].imshow(image)\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-02T02:25:35.466251Z","iopub.execute_input":"2022-02-02T02:25:35.466567Z","iopub.status.idle":"2022-02-02T02:25:40.097297Z","shell.execute_reply.started":"2022-02-02T02:25:35.46654Z","shell.execute_reply":"2022-02-02T02:25:40.095846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Instantiate the Datasets\n\nWe will form them into torch dataloaders to make the data easier to work with. We are also going to put in a minor amount of image augmentation in the train dataset.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms\n\n# Define Transforms\ntransform = transforms.Compose([\n                transforms.RandomRotation(10, fill=0),\n                transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.05),\n                transforms.ToTensor(),\n                transforms.RandomAffine(degrees=0, translate=(0.025, 0.025), fill=256),\n                transforms.Normalize([0.5], [0.5])\n            ])\n\ntest_transform = transforms.Compose([\n                transforms.ToTensor(),\n                transforms.Normalize((0.5,), (0.5,)),\n            ])\n\ntrain_ratio = 0.90\n\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=1 - train_ratio, stratify = y, random_state = 0)\n\ndataset_stages = ['train', 'val']\n\nbatch_size = 320\nimage_datasets = {'train' : CustomDataset(X_train, y_train, batch_size, transform), 'val' : CustomDataset(X_val, y_val, batch_size, test_transform)}\ndataloaders = {x: DataLoader(image_datasets[x], batch_size=image_datasets[x].BatchSize,\n                                            shuffle=True, num_workers=0)\n            for x in dataset_stages}\n\ndataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}","metadata":{"execution":{"iopub.status.busy":"2022-02-02T02:25:40.100749Z","iopub.execute_input":"2022-02-02T02:25:40.100951Z","iopub.status.idle":"2022-02-02T02:25:42.036105Z","shell.execute_reply.started":"2022-02-02T02:25:40.10093Z","shell.execute_reply":"2022-02-02T02:25:42.035277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Test Images from Dataset","metadata":{}},{"cell_type":"code","source":"fig,ax = plt.subplots(2,5)\nfor i in range(10):\n    nparray = image_datasets['train'][i][0].cpu().numpy() \n    image = transforms.ToPILImage()(image_datasets['train'][i][0].cpu()).convert(\"RGB\")\n    ax[i%2][i//2].imshow(image)\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-02T02:25:42.037737Z","iopub.execute_input":"2022-02-02T02:25:42.038014Z","iopub.status.idle":"2022-02-02T02:25:42.97365Z","shell.execute_reply.started":"2022-02-02T02:25:42.037976Z","shell.execute_reply":"2022-02-02T02:25:42.972478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create a Training Function","metadata":{}},{"cell_type":"code","source":"import time\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\ndef train_model(model, criterion, optimizer, scheduler, num_epochs=10):\n    since = time.time()\n    best_acc = 0.0\n\n    for epoch in range(num_epochs):\n        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n        print('-' * 10)\n        # Each epoch has a training and validation phase\n        for phase in ['train', 'val']:\n            if phase == 'train':\n                model.train()  # Set model to training mode\n            else:\n                model.eval()   # Set model to evaluate mode\n\n            running_loss = 0.0\n            running_corrects = 0\n            num_batches = 0\n            outputs = None\n            # Iterate over data.\n            for inputs, labels in dataloaders[phase]:\n                # Loading Bar\n                if (phase == 'train'):\n                    num_batches += 1\n                    percentage_complete = ((num_batches * batch_size) / (dataset_sizes[phase])) * 100\n                    percentage_complete = np.clip(percentage_complete, 0, 100)\n                    print(\"{:0.2f}\".format(percentage_complete), \"% complete\", end=\"\\r\")\n\n                inputs = inputs.to(device)\n                labels = labels.to(device)\n\n                # zero the parameter gradients\n                optimizer.zero_grad()\n                \n                # forward\n                # track history if only in train\n                with torch.set_grad_enabled(phase == 'train'):\n                    outputs = model(inputs)\n                    loss = criterion(outputs.float(), labels)\n                    # backward + optimize only if in training phase\n                    if phase == 'train':\n                        loss.backward()\n                        # TODO: try removal\n                        torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n                        optimizer.step()\n\n                # statistics\n                running_loss += loss.item() * inputs.size(0)\n                \n                predicted = torch.max(outputs.data, 1)[1] \n                running_correct = (predicted == labels).sum()\n                running_corrects += running_correct\n            if phase == 'train':\n                scheduler.step()\n\n            epoch_loss = running_loss / dataset_sizes[phase]\n            \n            epoch_acc = running_corrects / dataset_sizes[phase]\n            #epoch_acc = sum(epoch_acc) / len(epoch_acc)\n\n            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n                phase, epoch_loss, epoch_acc.item()))\n\n    time_elapsed = time.time() - since\n    print('Training complete in {:.0f}m {:.0f}s'.format(\n        time_elapsed // 60, time_elapsed % 60))\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-02-02T02:25:42.97495Z","iopub.execute_input":"2022-02-02T02:25:42.975624Z","iopub.status.idle":"2022-02-02T02:25:42.993677Z","shell.execute_reply.started":"2022-02-02T02:25:42.975593Z","shell.execute_reply":"2022-02-02T02:25:42.992642Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load up Shufflenet\n\nHere I will change the first layer to suit a smaller image and the classification layer to match the numbers found in MNIST. ","metadata":{}},{"cell_type":"code","source":"from torchvision import models\nfrom torch.optim import lr_scheduler\n\nshufflenet = models.shufflenet_v2_x1_0()\nshufflenet.conv1[0] = nn.Conv2d(1, 24, kernel_size=(2, 2), stride=(1, 1))\nshufflenet.fc = nn.Linear(in_features=1024, out_features=10, bias=True)\nmodel_ft = shufflenet","metadata":{"execution":{"iopub.status.busy":"2022-02-02T02:25:42.995114Z","iopub.execute_input":"2022-02-02T02:25:42.995382Z","iopub.status.idle":"2022-02-02T02:25:43.08161Z","shell.execute_reply.started":"2022-02-02T02:25:42.995335Z","shell.execute_reply":"2022-02-02T02:25:43.080908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss()\n\noptimizer_ft = optim.Adam(model_ft.parameters(), lr=0.01)\n\nexp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n\nshufflenet = train_model(shufflenet.to(device), criterion, optimizer_ft, exp_lr_scheduler, 35)","metadata":{"execution":{"iopub.status.busy":"2022-02-02T02:25:43.083036Z","iopub.execute_input":"2022-02-02T02:25:43.083304Z","iopub.status.idle":"2022-02-02T03:05:40.48306Z","shell.execute_reply.started":"2022-02-02T02:25:43.083269Z","shell.execute_reply":"2022-02-02T03:05:40.482265Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Run on Test Set","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score \n\naccuracy_scores = []\npredicted = []\n\nrunning_corrects = 0\noutputs = None\n\ntest_df = pd.read_csv(\"../input/digit-recognizer/test.csv\")\n\nfor input in test_df.values:\n    input = input.reshape(28,28)    \n    input = Image.fromarray(np.uint8(input)).convert('L')\n    inputs = test_transform(input)\n    model_ft.eval()\n    inputs = inputs.to(device)\n    outputs = model_ft(inputs.unsqueeze(0))\n    predicted.append(torch.max(outputs.data, 1)[1].cpu().item())    ","metadata":{"execution":{"iopub.status.busy":"2022-02-02T03:21:34.122871Z","iopub.execute_input":"2022-02-02T03:21:34.123188Z","iopub.status.idle":"2022-02-02T03:27:00.042749Z","shell.execute_reply.started":"2022-02-02T03:21:34.123158Z","shell.execute_reply":"2022-02-02T03:27:00.042245Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create Submission","metadata":{}},{"cell_type":"code","source":"submission = pd.read_csv(\"../input/digit-recognizer/sample_submission.csv\")\nsubmission[\"Label\"] = predicted\nsubmission.to_csv(\"submission.csv\", index=False)\nprint(submission.head())","metadata":{"execution":{"iopub.status.busy":"2022-02-02T03:27:10.128107Z","iopub.execute_input":"2022-02-02T03:27:10.129368Z","iopub.status.idle":"2022-02-02T03:27:10.208062Z","shell.execute_reply.started":"2022-02-02T03:27:10.129305Z","shell.execute_reply":"2022-02-02T03:27:10.207171Z"},"trusted":true},"execution_count":null,"outputs":[]}]}