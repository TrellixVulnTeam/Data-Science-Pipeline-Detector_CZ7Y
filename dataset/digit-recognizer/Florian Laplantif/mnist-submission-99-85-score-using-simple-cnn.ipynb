{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport multiprocessing\nimport torch\nfrom pytorch_lightning import LightningModule, Trainer\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom torch.utils.data import DataLoader, random_split\nfrom torchmetrics import Accuracy\nfrom torchvision import transforms\nfrom torchvision.datasets import MNIST\nimport pandas as pd\nimport numpy as np\n\nPATH_DATASETS = os.environ.get(\"PATH_DATASETS\", \".\")\nAVAIL_GPUS = min(1, torch.cuda.device_count())\nBATCH_SIZE = 256 if AVAIL_GPUS else 64\nMAX_EPOCHS = 10\nKAGGLE_FILE = \"../input/digit-recognizer/test.csv\"\n\n\nclass PermutationOperation(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, x):\n        # print('Shape before permutation', x.shape)\n        out = x.permute(0, 2, 3, 1)\n        # print('Shape after permutation', out.shape)\n        return out\n\n# Based on M7 from https://arxiv.org/pdf/2008.10400v2.pdf\nclass SimpleMNISTCNN(LightningModule):\n    def __init__(self, data_dir=PATH_DATASETS, learning_rate=2e-4):\n\n        super().__init__()\n\n        # Set our init args as class attributes\n        self.data_dir = data_dir\n        self.learning_rate = learning_rate\n\n        # Hardcode some dataset specific attributes\n        self.num_classes = 10\n        self.dims = (1, 28, 28)\n        channels, width, height = self.dims\n        self.transform = transforms.Compose(\n            [\n                transforms.ToTensor(),\n                transforms.Normalize((0.1307,), (0.3081,)),\n            ]\n        )\n\n        # Define PyTorch model\n        self.model = nn.Sequential(\n            nn.Conv2d(\n                in_channels=1,\n                out_channels=48,\n                kernel_size=7,\n                bias=False,\n            ),\n            nn.BatchNorm2d(48),\n            nn.ReLU(),\n            nn.Conv2d(\n                in_channels=48,\n                out_channels=96,\n                kernel_size=7,\n                bias=False,\n            ),\n            nn.BatchNorm2d(96),\n            nn.ReLU(),\n            nn.Conv2d(\n                in_channels=96,\n                out_channels=144,\n                kernel_size=7,\n                bias=False,\n            ),\n            nn.BatchNorm2d(144),\n            nn.ReLU(),\n            nn.Conv2d(\n                in_channels=144,\n                out_channels=192,\n                kernel_size=7,\n                bias=False,\n            ),\n            nn.BatchNorm2d(192),\n            nn.ReLU(),\n            PermutationOperation(),\n            nn.Flatten(),\n            nn.Linear(3072, self.num_classes, bias=False),\n            nn.BatchNorm1d(10)\n        )\n\n        self.accuracy = Accuracy(num_classes=10)\n\n    def forward(self, x):\n        x = self.model(x)\n        return F.log_softmax(x, dim=1)\n\n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = F.cross_entropy(logits, y)\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = F.cross_entropy(logits, y)\n        preds = torch.argmax(logits, dim=1)\n        self.accuracy(preds, y)\n\n        # Calling self.log will surface up scalars for you in TensorBoard\n        self.log(\"val_loss\", loss, prog_bar=True)\n        self.log(\"val_acc\", self.accuracy, prog_bar=True)\n        return loss\n\n    def test_step(self, batch, batch_idx):\n        # Here we just reuse the validation_step for testing\n        return self.validation_step(batch, batch_idx)\n\n    \n    def configure_optimizers(self):\n        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer)\n        return [optimizer], [{\"scheduler\": scheduler, \"monitor\": \"val_loss\"}]\n\n    ####################\n    # DATA RELATED HOOKS\n    ####################\n\n    def prepare_data(self):\n        # download\n        MNIST(self.data_dir, train=True, download=True)\n        MNIST(self.data_dir, train=False, download=True)\n\n    def setup(self, stage=None):\n\n        # Assign train/val datasets for use in dataloaders\n        if stage == \"fit\" or stage is None:\n            mnist_full = MNIST(self.data_dir, train=True,\n                               transform=self.transform)\n            self.mnist_train, self.mnist_val = random_split(\n                mnist_full, [55000, 5000])\n\n        # Assign test dataset for use in dataloader(s)\n        if stage == \"test\" or stage is None:\n            self.mnist_test = MNIST(\n                self.data_dir, train=False, transform=self.transform)\n\n    def train_dataloader(self):\n        return DataLoader(self.mnist_train, batch_size=BATCH_SIZE, num_workers=multiprocessing.cpu_count())\n\n    def val_dataloader(self):\n        return DataLoader(self.mnist_val, batch_size=BATCH_SIZE, num_workers=multiprocessing.cpu_count())\n\n    def test_dataloader(self):\n        return DataLoader(self.mnist_test, batch_size=BATCH_SIZE, num_workers=multiprocessing.cpu_count())\n\n\nif __name__ == '__main__':\n    model = SimpleMNISTCNN()\n    trainer = Trainer(\n        gpus=AVAIL_GPUS,\n        max_epochs=MAX_EPOCHS,\n        progress_bar_refresh_rate=20,\n    )\n    trainer.fit(model)\n    test = pd.read_csv(KAGGLE_FILE)\n    tensor = torch.from_numpy(np.array(test))\n    print(tensor.shape)\n    tensor = tensor.reshape((-1, 1, 28, 28)).type(dtype=torch.float32)\n    print(tensor.shape)\n    # print(test)\n    results = None\n    with torch.no_grad():\n        inputs_to_predict = torch.split(tensor, BATCH_SIZE)\n        for input_to_predict in inputs_to_predict:\n            pred = model(input_to_predict)\n            if results == None:\n                results = pred\n            else:\n                results = torch.cat((results, pred), 0)\n            print(results.shape)\n\n    print(results.shape)\n    np_pred = results.cpu().detach().numpy()\n    y_pred = np.argmax(np_pred, axis=1)\n    # creates de submission array, (28000,2)\n    # First column = ImageId, Second Column = Label\n    # ImageID follows the order of the Test File order\n    submission = pd.DataFrame(\n        {'ImageId': [i for i in range(1, len(np_pred)+1)], 'Label': y_pred})\n    # saves the submission file\n    submission.to_csv('submission.csv', index=False)\n    submission\n","metadata":{"execution":{"iopub.status.busy":"2022-06-08T06:22:58.840861Z","iopub.execute_input":"2022-06-08T06:22:58.841226Z","iopub.status.idle":"2022-06-08T06:25:21.194288Z","shell.execute_reply.started":"2022-06-08T06:22:58.841197Z","shell.execute_reply":"2022-06-08T06:25:21.191803Z"},"trusted":true},"execution_count":null,"outputs":[]}]}