{"cells":[{"metadata":{"_uuid":"a51479ece3c5358f27941ab8840f062b8964e763"},"cell_type":"markdown","source":"## Summary\n\nThis is a little experiment to check if we can have reproducible Keras models when using a GPU.  [Keras documentation](https://keras.io/getting-started/faq/#how-can-i-obtain-reproducible-results-using-keras-during-development) provides some recommendations that we have put in practice here. We have used the famous MNIST dataset and built a deep learning model for it. We have repeated several times the training of this model and observed the accuracy for each test. This experiment has been done for two cases: ussing a gpu or only cpu. It has not been possible to get full reproducibility in case of using a gpu, but setting a seed helps.\n\n*Note:* if you want to run this kernel in Kaggle you should attach a gpu to it in the configuration."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"code","source":"import warnings\nwarnings.simplefilter(\"ignore\", category=FutureWarning)\n\nimport os\nimport multiprocessing\nimport numpy as np \nimport pandas as pd \nimport tensorflow as tf\nimport random as rn\nfrom sklearn.model_selection import train_test_split\nfrom keras import backend as K\nfrom keras.utils import to_categorical\nfrom keras.layers import Dense, Input, Dropout, Flatten, Conv2D\nfrom keras.models import Model, Sequential","execution_count":1,"outputs":[]},{"metadata":{"_uuid":"4940dd9c2c1cb8108d6a33ca59784bb3bd8993f7"},"cell_type":"markdown","source":"## Data loading\nwe load the dataset and apply some transformations to use it in a deep learning model"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"data_train = pd.read_csv(\"../input/train.csv\")","execution_count":2,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"4e0e4d8275dfcd6a1b2a54b5041810a28aa6c5ce"},"cell_type":"code","source":"y = data_train['label'].astype('int32')\nX = data_train.drop('label', axis=1).astype('float32')","execution_count":3,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fb1ca34ac26116e34586ad5500742122afac43ca","collapsed":true},"cell_type":"code","source":"X = X.values.reshape(-1, 28, 28, 1)\ny = to_categorical(y)","execution_count":4,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"252ebe1186b5327bb7ed0b92ee0d6c42e6745769"},"cell_type":"code","source":"SEED = 1\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=SEED)\nX_train /= 255\nX_val /= 255","execution_count":5,"outputs":[]},{"metadata":{"_uuid":"52b5615c7383c46632c6de52cef96f59af94a358"},"cell_type":"markdown","source":"## Deep Learning model\nWe are going to build a model that we will use in the tests"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"7d2abeaaea9b98b98790dd9da1591c7dccb3983e"},"cell_type":"code","source":"def create_model():\n    model = Sequential()\n    model.add(Conv2D(30, kernel_size=(3, 3),\n                     strides=2,\n                     activation='relu',\n                     input_shape=(28, 28, 1)))\n    model.add(Dropout(0.5))\n    model.add(Conv2D(30, kernel_size=(3, 3), strides=2, activation='relu'))\n    model.add(Dropout(0.5))\n    model.add(Flatten())\n    model.add(Dense(128, activation='relu'))\n    model.add(Dense(10, activation='softmax'))\n\n    model.compile(loss='categorical_crossentropy',\n                  optimizer='adam',\n                  metrics=['accuracy'])\n    return model","execution_count":6,"outputs":[]},{"metadata":{"_uuid":"e348e89ee64c2035d37ae8d853861bcdea56437c"},"cell_type":"markdown","source":"## Test funcion\nThis function lets to test to test the deep learning model several times with the option of using a gpu or only a cpu"},{"metadata":{"trusted":true,"_uuid":"0749ea8202816ddad8ead34b87ea6faaa3b9cbaf","collapsed":true},"cell_type":"code","source":"def execute_test(mode='gpu', n_repeat=5, seed=1):\n    n_epochs = 2\n    batch_size = 128    \n    num_cores=1  \n    \n    if type(seed)==int:\n        seed_list = [seed]*n_repeat\n    else:\n        if (type(seed) in [list, tuple]) and (len(seed) >= n_repeat): \n            seed_list = seed\n        else:\n            raise ValueError('seed must be an integer or a list/tuple the lenght n_repeat')\n        \n    if mode=='gpu':\n        num_GPU = 1\n        num_CPU = 1\n        gpu_name = tf.test.gpu_device_name()\n        if (gpu_name != ''):\n            gpu_message = gpu_name  \n            print(\"Testing with GPU: {}\".format(gpu_message))\n        else:\n            gpu_message = \"ERROR <GPU NO AVAILABLE>\"\n            print(\"Testing with GPU: {}\".format(gpu_message))\n            return  \n    else:    \n        num_CPU = 1\n        num_GPU = 0\n        max_cores = multiprocessing.cpu_count()\n        print(\"Testing with CPU: using {} core ({} availables)\".format(num_cores, max_cores))\n\n    results = []    \n    for i in range(n_repeat):\n        os.environ['PYTHONHASHSEED'] = '0'                      \n        np.random.seed(seed_list[i])\n        rn.seed(seed_list[i])\n        tf.set_random_seed(seed_list[i])\n\n        session_conf = tf.ConfigProto(intra_op_parallelism_threads=num_cores,\n                                      inter_op_parallelism_threads=num_cores, \n                                      allow_soft_placement=True,\n                                      device_count = {'CPU' : num_CPU, 'GPU' : num_GPU})\n\n        sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n        K.set_session(sess)\n\n        model = create_model()\n\n        model.fit(X_train, y_train, batch_size = batch_size, epochs=n_epochs, verbose=0)\n        eval_acc = model.evaluate(x=X_val, y=y_val, batch_size=batch_size, verbose=0)[1]\n        results.append(eval_acc)\n        print(\"Accuracy Test {}: {}\".format(i, eval_acc))\n    K.clear_session()\n    return results","execution_count":47,"outputs":[]},{"metadata":{"_uuid":"ee9ca43a40a363d20f074631ba3908b5355ed816"},"cell_type":"markdown","source":"## Test\nLet's test now!"},{"metadata":{"_uuid":"917acd1d90dda35f3db1582c327e5cd2b50e07c4"},"cell_type":"markdown","source":"### CPU test"},{"metadata":{"trusted":true,"_uuid":"5115245deb1e2dd24ba9975a70b39fb49b8e205c","collapsed":true},"cell_type":"code","source":"res_cpu_same_seed = execute_test(mode='cpu', n_repeat=5, seed=SEED)","execution_count":55,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ea87d5ec7674af23b35388b4dc7848ef38ca12c8","collapsed":true},"cell_type":"code","source":"print(\"mean: {}\".format(np.mean(res_cpu_same_seed)))\nprint(\"std: {}\".format(np.std(res_cpu_same_seed)))    ","execution_count":56,"outputs":[]},{"metadata":{"_uuid":"e656ab7aa6682813114d583477466dcc447541f9"},"cell_type":"markdown","source":"In the case of using a cpu we can see that the results are always the same if we use the same seed. If we change the seed the result can change a bit:"},{"metadata":{"trusted":true,"_uuid":"8e5c32d65f296086e0bc43cbf8f7a408a344c581","collapsed":true},"cell_type":"code","source":"_ = execute_test(mode='cpu', n_repeat=1, seed=SEED*2)","execution_count":59,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"11db6a0355e10a0f4f0e42e7afc1bf5d43295fc1","collapsed":true},"cell_type":"code","source":"_ = execute_test(mode='cpu', n_repeat=1, seed=SEED*10)","execution_count":60,"outputs":[]},{"metadata":{"_uuid":"584d53cfab75ec69150db000dd43c94452bf1441"},"cell_type":"markdown","source":"### GPU test\nWe are going to check now if a gpu produces the same behaviour:"},{"metadata":{"trusted":true,"_uuid":"4444db661b8a3bed95c2fda689c3a378610bb0d0","collapsed":true},"cell_type":"code","source":"res_gpu_same_seed = execute_test(mode='gpu', n_repeat=10, seed=SEED)","execution_count":61,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b9b07409002f12862f3aa6b09ca062bcb13c9852","collapsed":true},"cell_type":"code","source":"print(\"mean: {}\".format(np.mean(res_gpu_same_seed)))\nprint(\"std: {}\".format(np.std(res_gpu_same_seed)))   ","execution_count":62,"outputs":[]},{"metadata":{"_uuid":"19864db61c51c47616cd69e2802ed1ab8ae02c2c"},"cell_type":"markdown","source":"We can see that with the same seed we have slightly different results. We are going to check now if these differences are bigger in case of using a different seed each time:"},{"metadata":{"trusted":true,"_uuid":"a650858e6e57e5c970e13277c5bcfbe651583de3","collapsed":true},"cell_type":"code","source":"res_gpu_diff_seed = execute_test(mode='gpu', n_repeat=10, seed=[i*10 for i in range(10)])","execution_count":63,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"99e03554c635239c21254b3fdd992c398067c42d","collapsed":true},"cell_type":"code","source":"print(\"mean: {}\".format(np.mean(res_gpu_diff_seed)))\nprint(\"std: {}\".format(np.std(res_gpu_diff_seed)))   ","execution_count":64,"outputs":[]},{"metadata":{"_uuid":"0a0f792fe87e636a5940a0763fc48e93d7f553d4"},"cell_type":"markdown","source":"We can see in this example that the standard deviation is bigger so, if we keep the same seed when training our neural networks using gpus, our results will be more reproducible. \nThis random behaviour affect only to the training process: when the weights of the neural network are obtained and fixed, all the predictions done with it produces the same result as we can see here:"},{"metadata":{"trusted":true,"_uuid":"8195bdfc8dfed6b8836efbe1d8c61a1c527262e7","collapsed":true},"cell_type":"code","source":"model = create_model()\nmodel.fit(X_train, y_train, batch_size = 128, epochs=2, verbose=0)\nfor i in range(5):\n    eval_acc = model.evaluate(x=X_val, y=y_val, batch_size=128, verbose=0)[1]\n    print(\"Accuracy Test: {}\".format(eval_acc))","execution_count":65,"outputs":[]},{"metadata":{"_uuid":"19dca09353dd3d2fa18acdf1105a9c76c745e44b"},"cell_type":"markdown","source":"## Conclusion\nWith the configuration recomended in the Keras documentation it has not been possible to obtain full reproducibility when using a gpu although it has been possible when using only a cpu. The gpus used at the moment in kaggle are Nvidia K80. It looks like that Nvidia and proabably other brands use some kind of non deterministic internal process so, slightly different results can happen each time. Although it is not possible to have full reproducibility when using a gpu, if a seed is fixed, the results will be more similar. It makes sense, when using gpus, to train the same model with different seeds as the performance can be a bit better for some of them. Once the wights of a model have been obtained, predictions done with that model are completely deterministic. \n "},{"metadata":{"_uuid":"a8ad2f1a93e1a10c8adf4ae48ece49b46a683598"},"cell_type":"markdown","source":"References:\n- https://keras.io/getting-started/faq/#how-can-i-obtain-reproducible-results-using-keras-during-development\n- https://stackoverflow.com/questions/46836857/results-not-reproducible-with-keras-and-tensorflow-in-python\n- https://www.twosigma.com/insights/article/a-workaround-for-non-determinism-in-tensorflow/\n- https://www.kaggle.com/dansbecker/dropout-and-strides-for-larger-models\n- https://machinelearningmastery.com/reproducible-results-neural-networks-keras/"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}