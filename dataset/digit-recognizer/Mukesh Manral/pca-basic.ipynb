{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"----\n----\n\n# My Other Notebook:\n* [Simple Linear Regression](https://www.kaggle.com/mukeshmanral/linear-regression-basic)\n* [Multiple Linear Regression](https://www.kaggle.com/mukeshmanral/multiple-linear-regression-basic)\n* [Polynomial Regression](https://www.kaggle.com/mukeshmanral/polynomial-regression-basic)\n* [Advanced Linear Regression](https://www.kaggle.com/mukeshmanral/advance-linear-regression-basic-gridsearchcv-hpt)\n\n----\n* [Feature Engineering 1](https://www.kaggle.com/mukeshmanral/feature-engineering-diff-dataset-1)\n* [Feature Engineering 2](https://www.kaggle.com/mukeshmanral/feature-engineering-diff-dataset-2)\n* [Feature Engineering 3](https://www.kaggle.com/mukeshmanral/feature-engineering-diff-dataset-3)\n* [Feature Engineering 4](https://www.kaggle.com/mukeshmanral/feature-engineering-diff-dataset-4)\n\n----\n\n* [How KNN-Algorith(1) Works (Basic)](https://www.kaggle.com/mukeshmanral/k-nn-algorithm-1-basic)\n* [How KNN-Algorith(2) Works (Basic)](https://www.kaggle.com/mukeshmanral/k-nn-algorithm-2-basic)\n\n____\n\n* [Ensemble-Bagging-Random Forest-Extra Tree Basic](https://www.kaggle.com/mukeshmanral/bagging-ensemble-concept-rf-extree-basic) \n\n____\n____","metadata":{}},{"cell_type":"markdown","source":"# `One can Understand Basic of PCA by this implementation`","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-10-25T12:06:30.139561Z","iopub.execute_input":"2021-10-25T12:06:30.140439Z","iopub.status.idle":"2021-10-25T12:06:30.149034Z","shell.execute_reply.started":"2021-10-25T12:06:30.140387Z","shell.execute_reply":"2021-10-25T12:06:30.148043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{"execution":{"iopub.status.busy":"2021-10-25T12:06:30.629395Z","iopub.execute_input":"2021-10-25T12:06:30.629679Z","iopub.status.idle":"2021-10-25T12:06:31.552231Z","shell.execute_reply.started":"2021-10-25T12:06:30.62965Z","shell.execute_reply":"2021-10-25T12:06:31.551404Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load MNIST Data (train.csv)\nd0 = pd.read_csv('/kaggle/input/digit-recognizer/train.csv')","metadata":{"execution":{"iopub.status.busy":"2021-10-13T14:40:05.12908Z","iopub.execute_input":"2021-10-13T14:40:05.129832Z","iopub.status.idle":"2021-10-13T14:40:08.205052Z","shell.execute_reply.started":"2021-10-13T14:40:05.129798Z","shell.execute_reply":"2021-10-13T14:40:08.20434Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print first five rows of d0\nd0.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-13T14:40:08.205987Z","iopub.execute_input":"2021-10-13T14:40:08.206741Z","iopub.status.idle":"2021-10-13T14:40:08.231502Z","shell.execute_reply.started":"2021-10-13T14:40:08.206703Z","shell.execute_reply":"2021-10-13T14:40:08.230724Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# save the labels into a variable l.\n\nl = d0.label","metadata":{"execution":{"iopub.status.busy":"2021-10-13T14:40:08.233118Z","iopub.execute_input":"2021-10-13T14:40:08.233448Z","iopub.status.idle":"2021-10-13T14:40:08.240349Z","shell.execute_reply.started":"2021-10-13T14:40:08.233418Z","shell.execute_reply":"2021-10-13T14:40:08.239365Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Drop the label feature from d0 and store the pixel data in d\nd = d0.drop('label',axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-10-13T14:40:08.241389Z","iopub.execute_input":"2021-10-13T14:40:08.241706Z","iopub.status.idle":"2021-10-13T14:40:08.35457Z","shell.execute_reply.started":"2021-10-13T14:40:08.241671Z","shell.execute_reply":"2021-10-13T14:40:08.353673Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#print shape of pixel and label data\nprint(l.shape)\nprint(d.shape)","metadata":{"execution":{"iopub.status.busy":"2021-10-13T14:40:08.355853Z","iopub.execute_input":"2021-10-13T14:40:08.356053Z","iopub.status.idle":"2021-10-13T14:40:08.36703Z","shell.execute_reply.started":"2021-10-13T14:40:08.35603Z","shell.execute_reply":"2021-10-13T14:40:08.366105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"idx = 1\n# print label value for index 1\nprint(l[idx])","metadata":{"execution":{"iopub.status.busy":"2021-10-13T14:40:08.368611Z","iopub.execute_input":"2021-10-13T14:40:08.368912Z","iopub.status.idle":"2021-10-13T14:40:08.378996Z","shell.execute_reply.started":"2021-10-13T14:40:08.368876Z","shell.execute_reply":"2021-10-13T14:40:08.378026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Display or Plot above label","metadata":{}},{"cell_type":"code","source":"d.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-13T14:40:08.379948Z","iopub.execute_input":"2021-10-13T14:40:08.38023Z","iopub.status.idle":"2021-10-13T14:40:08.402559Z","shell.execute_reply.started":"2021-10-13T14:40:08.380203Z","shell.execute_reply":"2021-10-13T14:40:08.401738Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(2,2))\n\n# reshape d from 1d to 2d pixel array for given idx ( prefer 28 X 28)\ngrid_data = d.loc[idx].values.reshape(28,28)\n\n#plot above grid image with cmap as gray and interpoltion as none\nplt.imshow(grid_data,interpolation='none',cmap='gray')\n\n#display plot\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-13T14:40:08.403584Z","iopub.execute_input":"2021-10-13T14:40:08.404396Z","iopub.status.idle":"2021-10-13T14:40:08.584455Z","shell.execute_reply.started":"2021-10-13T14:40:08.404354Z","shell.execute_reply":"2021-10-13T14:40:08.583875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2D Visualization using PCA","metadata":{}},{"cell_type":"code","source":"# Pick first 15K data-points to work on for time-effeciency\n#Excercise: Perform the same analysis on all of 42K data-points\n\nlabels = l.head(15000)#labels with 15k data points\ndata = d.head(15000)#data with 15k data points\n\nprint(\"the shape of sample data = \",data.shape)","metadata":{"execution":{"iopub.status.busy":"2021-10-13T14:40:08.586163Z","iopub.execute_input":"2021-10-13T14:40:08.586932Z","iopub.status.idle":"2021-10-13T14:40:08.591671Z","shell.execute_reply.started":"2021-10-13T14:40:08.5869Z","shell.execute_reply":"2021-10-13T14:40:08.591072Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"d.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-13T14:40:08.592541Z","iopub.execute_input":"2021-10-13T14:40:08.593117Z","iopub.status.idle":"2021-10-13T14:40:08.618033Z","shell.execute_reply.started":"2021-10-13T14:40:08.59308Z","shell.execute_reply":"2021-10-13T14:40:08.617436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"l.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-13T14:40:08.618984Z","iopub.execute_input":"2021-10-13T14:40:08.619413Z","iopub.status.idle":"2021-10-13T14:40:08.631849Z","shell.execute_reply.started":"2021-10-13T14:40:08.619382Z","shell.execute_reply":"2021-10-13T14:40:08.630968Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data-preprocessing\n### Standardizing data","metadata":{}},{"cell_type":"code","source":"# import standard scalar\nfrom sklearn.preprocessing import StandardScaler\n\n#fit transform data\nstandardized_data = StandardScaler().fit_transform(data)\n\n#print shape of standardized_data\ndata.shape","metadata":{"execution":{"iopub.status.busy":"2021-10-13T14:40:08.633233Z","iopub.execute_input":"2021-10-13T14:40:08.633517Z","iopub.status.idle":"2021-10-13T14:40:09.07463Z","shell.execute_reply.started":"2021-10-13T14:40:08.633489Z","shell.execute_reply":"2021-10-13T14:40:09.073645Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Find the co-variance matrix which is : \n                                                            A^T * A`\n* Find covariance matrix of dataset by multiplying the matrix of features by its transpose \n* It is a measure of how much each of the dimensions vary from the mean with respect to each other\n\n\nCovariance is measured between 2 dimensions to see if there is a relationship between the 2 dimensions, e.g., relationship between height and weight of students\n\n* Positive value of covariance indicates that both dimensions are directly proportional to each other, where if one dimension increases the other dimension increases accordingly\n\n* Negative value of covariance indicates that both dimensions are indirectly proportional to each other, where if one dimension increases then other dimension decreases accordingly\n\n* If in case covariance is zero, then the two dimensions are independent of each other","metadata":{}},{"cell_type":"code","source":"sample_data = standardized_data\n\n#use matrix multiplication on sample_data using numpy to find covariance matrix\ncovar_matrix = np.matmul(sample_data.T,sample_data)\n\n\n#print shape of covar_matrix\nprint ( \"The shape of variance matrix \\n\",covar_matrix.shape)","metadata":{"execution":{"iopub.status.busy":"2021-10-13T14:40:09.075768Z","iopub.execute_input":"2021-10-13T14:40:09.076005Z","iopub.status.idle":"2021-10-13T14:40:09.249333Z","shell.execute_reply.started":"2021-10-13T14:40:09.07598Z","shell.execute_reply":"2021-10-13T14:40:09.248401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"covar_matrix","metadata":{"execution":{"iopub.status.busy":"2021-10-13T14:40:09.251319Z","iopub.execute_input":"2021-10-13T14:40:09.252021Z","iopub.status.idle":"2021-10-13T14:40:09.260143Z","shell.execute_reply.started":"2021-10-13T14:40:09.251974Z","shell.execute_reply":"2021-10-13T14:40:09.259232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Computing Eigenvectors and Eigenvalues\nEigenvectors and eigenvalues of a covariance (or correlation) matrix represent the “core” of a PCA: \n* Eigenvectors (principal components) determine the directions of new feature space\n* Eigenvalues determine their magnitude <br>\nIn other words,`eigenvalues explain variance of data along new feature axes`\n\nEigenvectors and Eigenvalues of covariance matrix will give the principal components and a vector that we can use to project high-dimensional inputs to lower-dimensional subspace","metadata":{}},{"cell_type":"code","source":"# finding the top two eigen-values and corresponding eigen-vectors \n# for projecting onto a 2-Dim space\n\nfrom scipy.linalg import eigh \n\n# the parameter 'eigvals' is defined (low value to heigh value) \n# eigh function will return the eigen values in asending order\n\n# this code generates only the top 2 (782 and 783) eigenvalues\nvalues, vectors = eigh(covar_matrix,eigvals=(782,783))\n\nprint(\"Shape of eigen vectors = \",vectors.shape)\nprint(vectors)\n\n# converting the eigen vectors into (2,d) shape for easyness of further computations\nvectors = vectors.T\nprint(\"Updated shape of eigen vectors = \",vectors.shape)\nprint(vectors)\n# here the vectors[1] represent the eigen vector corresponding 1st principal eigen vector\n# here the vectors[0] represent the eigen vector corresponding 2nd principal eigen vector","metadata":{"execution":{"iopub.status.busy":"2021-10-13T14:40:09.26204Z","iopub.execute_input":"2021-10-13T14:40:09.262713Z","iopub.status.idle":"2021-10-13T14:40:09.34042Z","shell.execute_reply.started":"2021-10-13T14:40:09.262667Z","shell.execute_reply":"2021-10-13T14:40:09.339416Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**`projecting the original data sample on the plane formed by multiplication of two principal eigen vectors with transposed sample_data`**","metadata":{}},{"cell_type":"code","source":"# multiplication of two principal eigen vectors with transposed sample_data to get 2d projected data\nnew_coordinates = np.matmul(vectors,sample_data.T)\n\nprint (\" resultanat new data points' shape \",vectors.shape, \"X\",sample_data.T.shape,\" = \",new_coordinates.shape)\n","metadata":{"execution":{"iopub.status.busy":"2021-10-13T14:40:09.342507Z","iopub.execute_input":"2021-10-13T14:40:09.343269Z","iopub.status.idle":"2021-10-13T14:40:09.36463Z","shell.execute_reply.started":"2021-10-13T14:40:09.343205Z","shell.execute_reply":"2021-10-13T14:40:09.36365Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# appending label to the 2d projected data \nnew_coordinates = np.vstack((new_coordinates,labels)).T\n\n# creating a new data frame for ploting the labeled points.\ndataframe = pd.DataFrame(data=new_coordinates,columns=('1st_principal','2nd_principal','label'))\n\n#print dataframe head\ndataframe.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-13T14:40:09.366677Z","iopub.execute_input":"2021-10-13T14:40:09.367468Z","iopub.status.idle":"2021-10-13T14:40:09.390051Z","shell.execute_reply.started":"2021-10-13T14:40:09.367416Z","shell.execute_reply":"2021-10-13T14:40:09.388731Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**`ploting the 2d data points with seaborn`**","metadata":{}},{"cell_type":"code","source":"sns.FacetGrid(dataframe,hue='label',size=8).map(plt.scatter,'1st_principal','2nd_principal','label').add_legend()\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-13T14:40:09.392469Z","iopub.execute_input":"2021-10-13T14:40:09.393221Z","iopub.status.idle":"2021-10-13T14:40:10.470874Z","shell.execute_reply.started":"2021-10-13T14:40:09.393167Z","shell.execute_reply":"2021-10-13T14:40:10.469769Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# PCA using Scikit-Learn","metadata":{}},{"cell_type":"code","source":"# initializing the pca\nfrom sklearn import decomposition\n\n\npca = decomposition.PCA()","metadata":{"execution":{"iopub.status.busy":"2021-10-13T14:40:42.749986Z","iopub.execute_input":"2021-10-13T14:40:42.750777Z","iopub.status.idle":"2021-10-13T14:40:42.900894Z","shell.execute_reply.started":"2021-10-13T14:40:42.750737Z","shell.execute_reply":"2021-10-13T14:40:42.900178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### configuring the parameteres","metadata":{"execution":{"iopub.status.busy":"2021-10-13T14:40:49.914461Z","iopub.execute_input":"2021-10-13T14:40:49.914723Z","iopub.status.idle":"2021-10-13T14:40:49.921235Z","shell.execute_reply.started":"2021-10-13T14:40:49.914698Z","shell.execute_reply":"2021-10-13T14:40:49.92046Z"}}},{"cell_type":"code","source":"# the number of components = 2\npca.n_components = 2\n\n# fit transform sample data using pca \npca_data = pca.fit_transform(sample_data)\n\n# pca_reduced will contain the 2-d projects of simple data\nprint(\"shape of pca_reduced.shape = \",pca_data.shape)","metadata":{"execution":{"iopub.status.busy":"2021-10-13T14:51:59.878287Z","iopub.execute_input":"2021-10-13T14:51:59.878769Z","iopub.status.idle":"2021-10-13T14:52:00.560011Z","shell.execute_reply.started":"2021-10-13T14:51:59.878727Z","shell.execute_reply":"2021-10-13T14:52:00.559148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" pd.DataFrame(data=np.vstack((pca_data.T,labels)).T,columns=('1st_principal','2nd_principal','label'))","metadata":{"execution":{"iopub.status.busy":"2021-10-13T14:51:37.406393Z","iopub.execute_input":"2021-10-13T14:51:37.406825Z","iopub.status.idle":"2021-10-13T14:51:37.421685Z","shell.execute_reply.started":"2021-10-13T14:51:37.406783Z","shell.execute_reply":"2021-10-13T14:51:37.421078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# attaching the label for each 2-d data point (Hint: Use np.vstack)\npca_data = np.vstack((pca_data.T,labels)).T\n\n# creating a new data fram which help us in ploting the result data\npca_df = pd.DataFrame(data=pca_data,columns=('1st_principal','2nd_principal','label'))\n\n#plotting the 2d data points\nsns.FacetGrid(pca_df,hue='label',size=8).map(plt.scatter,'1st_principal','2nd_principal','label').add_legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-13T14:52:02.477675Z","iopub.execute_input":"2021-10-13T14:52:02.477935Z","iopub.status.idle":"2021-10-13T14:52:03.489223Z","shell.execute_reply.started":"2021-10-13T14:52:02.47791Z","shell.execute_reply":"2021-10-13T14:52:03.488274Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# PCA for dimensionality redcution (not for visualization)\nDistribution of explained variance for each principal component gives a sense of how much information will be represented and how much lost when the full, 64-dimensional input is reduced using a principal component model (i.e., a model that utilizes only the first N principal components).","metadata":{}},{"cell_type":"code","source":"# the number of components = 784\npca.n_components = 784\n\n\n# fit transform sample data using pca \npca_data = pca.fit_transform(sample_data)\n\n\n#calculating percentage of variance explained in the data\npercentage_var_explained = pca.explained_variance_ / np.sum(pca.explained_variance_)\n\n#cumulative sum of the percentage_var_explained\ncumulative_explained_variance = np.cumsum(percentage_var_explained)","metadata":{"execution":{"iopub.status.busy":"2021-10-13T14:59:40.346414Z","iopub.execute_input":"2021-10-13T14:59:40.346674Z","iopub.status.idle":"2021-10-13T14:59:42.335463Z","shell.execute_reply.started":"2021-10-13T14:59:40.346648Z","shell.execute_reply":"2021-10-13T14:59:42.334535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot the PCA spectrum\nplt.figure(figsize=(6,4))\nplt.plot(cumulative_explained_variance,linewidth=4)\nplt.grid()\n\nplt.xlabel('n_components')\nplt.ylabel('Cumulative_explained_variance')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-13T15:02:08.341885Z","iopub.execute_input":"2021-10-13T15:02:08.342177Z","iopub.status.idle":"2021-10-13T15:02:08.55398Z","shell.execute_reply.started":"2021-10-13T15:02:08.342149Z","shell.execute_reply":"2021-10-13T15:02:08.552842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From above you can see that if we take 200-dimensions, approx. 90% of variance is expalined.\n\nOur intention with princpal component analysis is to reduce the high-dimensional input to a low-dimensional input \n\nUltimately that low-dimensional input is intended for use in a model, since adding more components increases the cost and the accuracy","metadata":{}},{"cell_type":"markdown","source":"# PCA is a method that brings together:\n* A measure of how each variable is associated with one another. (Covariance matrix.)\n\n* The directions in which our data are dispersed. (Eigenvectors.)\n\n* The relative importance of these different directions. (Eigenvalues.)\n\n* PCA combines our predictors and allows us to drop the Eigenvectors that are relatively unimportant.\n\n","metadata":{}},{"cell_type":"markdown","source":"----\n----\n\n# My Other Notebook:\n* [Simple Linear Regression](https://www.kaggle.com/mukeshmanral/linear-regression-basic)\n* [Multiple Linear Regression](https://www.kaggle.com/mukeshmanral/multiple-linear-regression-basic)\n* [Polynomial Regression](https://www.kaggle.com/mukeshmanral/polynomial-regression-basic)\n* [Advanced Linear Regression](https://www.kaggle.com/mukeshmanral/advance-linear-regression-basic-gridsearchcv-hpt)\n\n----\n\n* [Feature Engineering 1](https://www.kaggle.com/mukeshmanral/feature-engineering-diff-dataset-1)\n* [Feature Engineering 2](https://www.kaggle.com/mukeshmanral/feature-engineering-diff-dataset-2)\n* [Feature Engineering 3](https://www.kaggle.com/mukeshmanral/feature-engineering-diff-dataset-3)\n* [Feature Engineering 4](https://www.kaggle.com/mukeshmanral/feature-engineering-diff-dataset-4)\n\n----\n\n* [How KNN-Algorith(1) Works (Basic)](https://www.kaggle.com/mukeshmanral/k-nn-algorithm-1-basic)\n* [How KNN-Algorith(2) Works (Basic)](https://www.kaggle.com/mukeshmanral/k-nn-algorithm-2-basic)\n\n____\n\n* [Ensemble-Bagging-Random Forest-Extra Tree Basic](https://www.kaggle.com/mukeshmanral/bagging-ensemble-concept-rf-extree-basic) \n\n____\n____","metadata":{}}]}