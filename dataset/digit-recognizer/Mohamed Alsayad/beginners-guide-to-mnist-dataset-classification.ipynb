{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-02-03T08:27:55.249589Z","iopub.execute_input":"2022-02-03T08:27:55.249866Z","iopub.status.idle":"2022-02-03T08:27:55.260691Z","shell.execute_reply.started":"2022-02-03T08:27:55.249838Z","shell.execute_reply":"2022-02-03T08:27:55.259702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In this notebook we will go through different classyfication algorithms that you may think of when you want to implement a model that can distinguish between different hand written and how you can enhance your model and the right way to perform different sequence of experiments on your dataset till you find the optimum point.","metadata":{}},{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, confusion_matrix\n\nfrom tensorflow.keras.layers import Dense, Conv2D, Input, Dropout, BatchNormalization, MaxPooling2D, Flatten\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.losses import SparseCategoricalCrossentropy\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport time","metadata":{"execution":{"iopub.status.busy":"2022-02-03T08:35:59.983578Z","iopub.execute_input":"2022-02-03T08:35:59.983862Z","iopub.status.idle":"2022-02-03T08:35:59.991042Z","shell.execute_reply.started":"2022-02-03T08:35:59.983832Z","shell.execute_reply":"2022-02-03T08:35:59.99043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Anlysis","metadata":{}},{"cell_type":"markdown","source":"Firstly let's get the MNIST training data which is represented in csv format where a row represents one image sample. and each row contains **785** column.\n\nThe first column is the **Label** column which contains the ground truth of this image sample, and the rest of the **784 columns** are the values of pixels of this image sample. \n\nLet's load it","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv(\"/kaggle/input/digit-recognizer/train.csv\")\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-03T08:27:57.423449Z","iopub.execute_input":"2022-02-03T08:27:57.424169Z","iopub.status.idle":"2022-02-03T08:28:01.509809Z","shell.execute_reply.started":"2022-02-03T08:27:57.424121Z","shell.execute_reply":"2022-02-03T08:28:01.508964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's now divide the dataset into **Features (X)** and their corespoinding **Labels (Y)**","metadata":{}},{"cell_type":"code","source":"X = train_df.drop(columns=[\"label\"]).values\nY = train_df[\"label\"].values\n\nprint(\"Shape of features:\", X.shape)\nprint(\"Shape of features:\", Y.shape)\n\nprint(\"Type of X:\", type(X))\nprint(\"Type of Y:\", type(Y))","metadata":{"execution":{"iopub.status.busy":"2022-02-03T08:28:04.33571Z","iopub.execute_input":"2022-02-03T08:28:04.336667Z","iopub.status.idle":"2022-02-03T08:28:04.44927Z","shell.execute_reply.started":"2022-02-03T08:28:04.336622Z","shell.execute_reply":"2022-02-03T08:28:04.447982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So basically now we have **42000 training samples** each one has **784 features** and **1 label** and they are represented in numpy arrays","metadata":{}},{"cell_type":"markdown","source":"Let's try to visualize the data to make sure that everything is good till now.<br>\nIn order to do this we need to reshape each sample from being one row of shape (784,1) to be a square matrix of shape (28,28)","metadata":{}},{"cell_type":"code","source":"X_square = X.reshape(-1, 28, 28)\n\n# Just a value to index both the features and the labels\n# Feel free to change this number to visualize the samples\nindex = 100\n\nplt.imshow(X_square[100])\nprint(f\"Label is {Y[100]}\")","metadata":{"execution":{"iopub.status.busy":"2022-02-03T08:28:10.376929Z","iopub.execute_input":"2022-02-03T08:28:10.377271Z","iopub.status.idle":"2022-02-03T08:28:10.616758Z","shell.execute_reply.started":"2022-02-03T08:28:10.377235Z","shell.execute_reply":"2022-02-03T08:28:10.61585Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The second step you need always to do whenever you want to analyse a dataset is to check the distribustion of your samples throughout the space of the targets. In simplified words, you need to see how many samples does this dataset has in each category. so basically we have 10 categories (the digits from 0 to 10) and we want to check how many samples we have for each digit.","metadata":{}},{"cell_type":"code","source":"uniques, count = np.unique(Y, return_counts=True)\nplt.bar(uniques, count)\nplt.xlabel(\"Digit Category\")\nplt.ylabel(\"Number of samples\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-03T08:28:12.910503Z","iopub.execute_input":"2022-02-03T08:28:12.910842Z","iopub.status.idle":"2022-02-03T08:28:13.058329Z","shell.execute_reply.started":"2022-02-03T08:28:12.910801Z","shell.execute_reply":"2022-02-03T08:28:13.057006Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So according to this bar chart, we have a good distribution of the samples for each category. Now we can safely state that this dataset is **Balanced**","metadata":{}},{"cell_type":"markdown","source":"Now if we gave a look to the features we will find that a single features may have an integer number in the range of (0:255), and that's a large range for some models that prefers to work with data of features varies between (0 to 1). Therefore, we need to normalize the features","metadata":{}},{"cell_type":"code","source":"X_square = X_square / 255.\nX = X / 255.","metadata":{"execution":{"iopub.status.busy":"2022-02-03T08:28:17.67738Z","iopub.execute_input":"2022-02-03T08:28:17.677707Z","iopub.status.idle":"2022-02-03T08:28:17.994521Z","shell.execute_reply.started":"2022-02-03T08:28:17.677672Z","shell.execute_reply":"2022-02-03T08:28:17.993484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prepare Train and Test sets","metadata":{}},{"cell_type":"markdown","source":"In order to have a good model, we need to prepare a test set that covers most of the cases in our dataset to have a proper evaluation for our models to compare between them.\n\nOther thing we also need to **unify** the test set for all of the following experiments","metadata":{}},{"cell_type":"code","source":"test_ratio = 0.1  # 10%\nx_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=test_ratio, random_state=42)\n\nprint(x_train.shape, y_train.shape)\nprint(x_test.shape, y_test.shape)\n\nuniques, count = np.unique(y_train, return_counts=True)\nplt.bar(uniques, count, label=\"train\")\nuniques, count = np.unique(y_test, return_counts=True)\nplt.bar(uniques, count, label=\"test\")\nplt.xlabel(\"Digit Category\")\nplt.ylabel(\"Number of samples\")\nplt.legend()\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2022-02-03T08:28:19.295266Z","iopub.execute_input":"2022-02-03T08:28:19.29552Z","iopub.status.idle":"2022-02-03T08:28:20.163178Z","shell.execute_reply.started":"2022-02-03T08:28:19.295493Z","shell.execute_reply":"2022-02-03T08:28:20.162236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Well now we can say that we have a **balanced** and **well-distributed** test set","metadata":{}},{"cell_type":"markdown","source":"**We are now ready to do some experiments !!**","metadata":{}},{"cell_type":"markdown","source":"# Experiments","metadata":{}},{"cell_type":"markdown","source":"One good fact, is that there's nothing called \"Best Classifier Algorithm Ever\". A classifier algorithm can fit well over a dataset which is linearlly separable - for example the SVM -, can't really fit very well over other types of data. \n\nTherefore it's always a good practice to test multiple algorithms on your dataset before choosing one, then you can fine tune that model till you get yourself the **best model** for this specific **dataset**","metadata":{}},{"cell_type":"markdown","source":"In this section we will dive into different types of experiments to try to find the best classifier that can fit over this dataset. \n\nLet's start !!","metadata":{}},{"cell_type":"markdown","source":"## 1. Decision Tree","metadata":{}},{"cell_type":"markdown","source":"# Decision Tree Classifier","metadata":{}},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\n\ndecision_tree_classifier = DecisionTreeClassifier(random_state=0)\ndecision_tree_classifier.fit(x_train, y_train)\nstart = time.time()\npredictions = decision_tree_classifier.predict(x_test)\nend = time.time()\n\naccuracy = accuracy_score(y_test, predictions)\nconf_matrix = confusion_matrix(y_test, predictions)\n\nprint(\"Decision Tree\")\nprint(f\"Accuracy: {accuracy}\")\nprint(\"Confusion Matrix\\n\", conf_matrix)\nprint(f\"Elapsed Time: {end - start: .4f} second\")","metadata":{"execution":{"iopub.status.busy":"2022-02-03T09:37:53.450454Z","iopub.execute_input":"2022-02-03T09:37:53.450807Z","iopub.status.idle":"2022-02-03T09:38:05.567194Z","shell.execute_reply.started":"2022-02-03T09:37:53.45077Z","shell.execute_reply":"2022-02-03T09:38:05.566472Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Random Forest Classifier","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nrandom_forest_classifier = RandomForestClassifier()\nrandom_forest_classifier.fit(x_train, y_train)\nstart = time.time()\npredictions = random_forest_classifier.predict(x_test)\nend = time.time()\n\naccuracy = accuracy_score(y_test, predictions)\nconf_matrix = confusion_matrix(y_test, predictions)\n\nprint(\"Random Forest\")\nprint(f\"Accuracy: {accuracy}\")\nprint(\"Confusion Matrix\\n\", conf_matrix)\nprint(f\"Elapsed Time: {end - start: .4f} second\")","metadata":{"execution":{"iopub.status.busy":"2022-02-03T09:38:05.56863Z","iopub.execute_input":"2022-02-03T09:38:05.568995Z","iopub.status.idle":"2022-02-03T09:38:31.124464Z","shell.execute_reply.started":"2022-02-03T09:38:05.568962Z","shell.execute_reply":"2022-02-03T09:38:31.123515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Support Vector Machine","metadata":{}},{"cell_type":"code","source":"from sklearn.svm import SVC\n\nsvm_classifier = SVC()\nsvm_classifier.fit(x_train, y_train)\nstart = time.time()\npredictions = svm_classifier.predict(x_test)\nend = time.time()\n\naccuracy = accuracy_score(y_test, predictions)\nconf_matrix = confusion_matrix(y_test, predictions)\n\nprint(\"Support Vector Machine\")\nprint(f\"Accuracy: {accuracy}\")\nprint(\"Confusion Matrix\\n\", conf_matrix)\nprint(f\"Elapsed Time: {end - start: .4f} second\")","metadata":{"execution":{"iopub.status.busy":"2022-02-03T09:38:31.125475Z","iopub.execute_input":"2022-02-03T09:38:31.125915Z","iopub.status.idle":"2022-02-03T09:43:45.528273Z","shell.execute_reply.started":"2022-02-03T09:38:31.125873Z","shell.execute_reply":"2022-02-03T09:43:45.52744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Neural Network","metadata":{}},{"cell_type":"markdown","source":"## Model 1 \"simple Fully connected Neural Network\"","metadata":{}},{"cell_type":"markdown","source":"In this model we're going to build a simple fully connected neural network with input shape = (784,) and output shape = (10,) activated with softmax so that we can have 10 probabilities for the input image.\n\nThe reason why we've used \"Sparse Categorical Cross Entropy\" is that this is a categorical classification problem, where the labels are integers ranged from 0 to 9. If we decided to change the labels to be one-hot encoded then we will need to change that loss function to be the normal \"Categorical Cross Entropy\"","metadata":{}},{"cell_type":"code","source":"input_layer = Input(shape=(X.shape[-1]))\n\nx = Dense(700, activation=\"relu\")(input_layer)\nx = Dense(350, activation=\"relu\")(x)\nx = Dense(175, activation=\"relu\")(x)\nx = Dense(100, activation=\"relu\")(x)\nx = Dense(50, activation=\"relu\")(x)\nx = Dense(25, activation=\"relu\")(x)\nx = Dense(15, activation=\"relu\")(x)\noutput_layer = Dense(10, activation=\"softmax\")(x)\n\nmodel1 = Model(inputs=input_layer, outputs=output_layer)\n\nmodel1.summary()\n\nloss = SparseCategoricalCrossentropy()\noptimizer = Adam(learning_rate=0.01)\n\nmodel1.compile(loss=loss, optimizer=optimizer, metrics=[\"accuracy\"])\nmodel1.fit(\n    x=x_train,\n    y=y_train,\n    epochs=10,\n    batch_size=64,\n    validation_split=0.1\n)","metadata":{"execution":{"iopub.status.busy":"2022-02-03T08:53:03.299549Z","iopub.execute_input":"2022-02-03T08:53:03.299834Z","iopub.status.idle":"2022-02-03T08:53:48.351976Z","shell.execute_reply.started":"2022-02-03T08:53:03.299806Z","shell.execute_reply":"2022-02-03T08:53:48.351177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame(model1.history.history).plot()","metadata":{"execution":{"iopub.status.busy":"2022-02-03T08:53:55.02998Z","iopub.execute_input":"2022-02-03T08:53:55.030262Z","iopub.status.idle":"2022-02-03T08:53:55.268469Z","shell.execute_reply.started":"2022-02-03T08:53:55.030226Z","shell.execute_reply":"2022-02-03T08:53:55.267949Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see the model overfitted because we over trained it on the data. So let's try to solve this problem by adding some Dropout layers and observe the difference.","metadata":{}},{"cell_type":"markdown","source":"## Model 2 \"Add Dropouts\"","metadata":{}},{"cell_type":"code","source":"input_layer = Input(shape=(X.shape[-1]))\n\nx = Dense(700, activation=\"relu\")(input_layer)\nx = Dense(350, activation=\"relu\")(x)\nx = Dropout(0.2)(x)\nx = Dense(175, activation=\"relu\")(x)\nx = Dense(100, activation=\"relu\")(x)\nx = Dropout(0.2)(x)\nx = Dense(50, activation=\"relu\")(x)\nx = Dense(25, activation=\"relu\")(x)\nx = Dropout(0.2)(x)\nx = Dense(15, activation=\"relu\")(x)\noutput_layer = Dense(10, activation=\"softmax\")(x)\n\nmodel2 = Model(inputs=input_layer, outputs=output_layer)\nmodel2.summary()\n\nloss = SparseCategoricalCrossentropy()\noptimizer = Adam(learning_rate=0.01)\n\nmodel2.compile(loss=loss, optimizer=optimizer, metrics=[\"accuracy\"])\nmodel2.fit(\n    x=x_train,\n    y=y_train,\n    epochs=10,\n    batch_size=64,\n    validation_split=0.1\n)","metadata":{"execution":{"iopub.status.busy":"2022-02-03T08:54:48.974767Z","iopub.execute_input":"2022-02-03T08:54:48.975114Z","iopub.status.idle":"2022-02-03T08:55:39.338787Z","shell.execute_reply.started":"2022-02-03T08:54:48.975078Z","shell.execute_reply":"2022-02-03T08:55:39.337359Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame(model2.history.history).plot()","metadata":{"execution":{"iopub.status.busy":"2022-02-03T08:56:04.118553Z","iopub.execute_input":"2022-02-03T08:56:04.119622Z","iopub.status.idle":"2022-02-03T08:56:04.357973Z","shell.execute_reply.started":"2022-02-03T08:56:04.11958Z","shell.execute_reply":"2022-02-03T08:56:04.357056Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model 3 \"Add Early Stopping\"","metadata":{}},{"cell_type":"markdown","source":"It looks like that the model was doing good till the third epoch then it overfitted. let's then try to add an early stopping call back that will stop the training and save the best weights whenever the model starts to overfit. and in order to decide that the model overfitted we will monitor the validation loss, if it started to increase for 2 epochs then it will stop","metadata":{}},{"cell_type":"code","source":"input_layer = Input(shape=(X.shape[-1]))\n\nx = Dense(700, activation=\"relu\")(input_layer)\nx = Dense(350, activation=\"relu\")(x)\nx = Dropout(0.2)(x)\nx = Dense(175, activation=\"relu\")(x)\nx = Dense(100, activation=\"relu\")(x)\nx = Dropout(0.2)(x)\nx = Dense(50, activation=\"relu\")(x)\nx = Dense(25, activation=\"relu\")(x)\nx = Dropout(0.2)(x)\nx = Dense(15, activation=\"relu\")(x)\noutput_layer = Dense(10, activation=\"softmax\")(x)\n\nmodel3 = Model(inputs=input_layer, outputs=output_layer)\nmodel3.summary()\n\nloss = SparseCategoricalCrossentropy()\noptimizer = Adam(learning_rate=0.01)\nearly_stopping = EarlyStopping(monitor=\"val_loss\", patience=2)\n\nmodel3.compile(loss=loss, optimizer=optimizer, metrics=[\"accuracy\"])\nmodel3.fit(\n    x=x_train,\n    y=y_train,\n    epochs=10,\n    batch_size=64,\n    validation_split=0.1,\n    callbacks=[early_stopping]\n)","metadata":{"execution":{"iopub.status.busy":"2022-02-03T09:00:58.697686Z","iopub.execute_input":"2022-02-03T09:00:58.698036Z","iopub.status.idle":"2022-02-03T09:01:20.117011Z","shell.execute_reply.started":"2022-02-03T09:00:58.697994Z","shell.execute_reply":"2022-02-03T09:01:20.1161Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame(model3.history.history).plot()","metadata":{"execution":{"iopub.status.busy":"2022-02-03T09:01:46.043008Z","iopub.execute_input":"2022-02-03T09:01:46.043361Z","iopub.status.idle":"2022-02-03T09:01:46.320295Z","shell.execute_reply.started":"2022-02-03T09:01:46.043325Z","shell.execute_reply":"2022-02-03T09:01:46.319273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model 4 \"Add Decaying Learning Rate\"","metadata":{}},{"cell_type":"markdown","source":"Well then, at least now we have a better model. but that's not enough. Why don't we try to reduce the learning rate to avoid falling in some local minimum. The idea is to star with some \"relatively\" big learning rate to move quickly from the initial point then whenever an overfitting is triggering, the learning rate will be reduced with some factor to avoid it. once we reach the minimum learning rate, and some overfitting started to happen then the early stopping will prevent it. now we can safely increase the number of epochs","metadata":{}},{"cell_type":"code","source":"# This function only to print out the learning rate\ndef get_lr_metric(optimizer):\n    def lr(y_true, y_pred):\n        return optimizer.lr\n    return lr\n\n\ninput_layer = Input(shape=(X.shape[-1]))\n\nx = Dense(700, activation=\"relu\")(input_layer)\nx = Dense(350, activation=\"relu\")(x)\nx = Dropout(0.2)(x)\nx = Dense(175, activation=\"relu\")(x)\nx = Dense(100, activation=\"relu\")(x)\nx = Dropout(0.2)(x)\nx = Dense(50, activation=\"relu\")(x)\nx = Dense(25, activation=\"relu\")(x)\nx = Dropout(0.2)(x)\nx = Dense(15, activation=\"relu\")(x)\noutput_layer = Dense(10, activation=\"softmax\")(x)\n\nmodel4 = Model(inputs=input_layer, outputs=output_layer)\nmodel4.summary()\n\nloss = SparseCategoricalCrossentropy()\noptimizer = Adam(learning_rate=0.01)\nearly_stopping = EarlyStopping(monitor=\"val_loss\", patience=3)\ndecaying_learning_rate = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=2, min_delta=0.1, min_lr=0.0001)\nlr_metric = get_lr_metric(optimizer)\n\nmodel4.compile(loss=loss, optimizer=optimizer, metrics=[\"accuracy\", lr_metric])\nmodel4.fit(\n    x=x_train,\n    y=y_train,\n    epochs=20,\n    batch_size=64,\n    validation_split=0.1,\n    callbacks=[early_stopping, decaying_learning_rate]\n)","metadata":{"execution":{"iopub.status.busy":"2022-02-03T09:09:54.323932Z","iopub.execute_input":"2022-02-03T09:09:54.324259Z","iopub.status.idle":"2022-02-03T09:11:32.820058Z","shell.execute_reply.started":"2022-02-03T09:09:54.324227Z","shell.execute_reply":"2022-02-03T09:11:32.819368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame(model4.history.history).plot()","metadata":{"execution":{"iopub.status.busy":"2022-02-03T09:11:48.633508Z","iopub.execute_input":"2022-02-03T09:11:48.634056Z","iopub.status.idle":"2022-02-03T09:11:48.934222Z","shell.execute_reply.started":"2022-02-03T09:11:48.63402Z","shell.execute_reply":"2022-02-03T09:11:48.933311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**It looks like we're going somewhere now :D**","metadata":{}},{"cell_type":"markdown","source":"## Model 5 \"Add Batch Normalization\"","metadata":{}},{"cell_type":"markdown","source":"One good practice in trainig a neural network activated with relu, is to always add batch normalization after the activation to avoid any overshooting in the weights.","metadata":{}},{"cell_type":"code","source":"# This function only to print out the learning rate\ndef get_lr_metric(optimizer):\n    def lr(y_true, y_pred):\n        return optimizer.lr\n    return lr\n\n\ninput_layer = Input(shape=(X.shape[-1]))\n\nx = Dense(700, activation=\"relu\")(input_layer)\nx = BatchNormalization()(x)\nx = Dense(350, activation=\"relu\")(x)\nx = BatchNormalization()(x)\nx = Dropout(0.2)(x)\nx = Dense(175, activation=\"relu\")(x)\nx = BatchNormalization()(x)\nx = Dense(100, activation=\"relu\")(x)\nx = BatchNormalization()(x)\nx = Dropout(0.2)(x)\nx = Dense(50, activation=\"relu\")(x)\nx = BatchNormalization()(x)\nx = Dense(25, activation=\"relu\")(x)\nx = BatchNormalization()(x)\nx = Dropout(0.2)(x)\nx = Dense(15, activation=\"relu\")(x)\nx = BatchNormalization()(x)\noutput_layer = Dense(10, activation=\"softmax\")(x)\n\nmodel5 = Model(inputs=input_layer, outputs=output_layer)\nmodel5.summary()\n\nloss = SparseCategoricalCrossentropy()\noptimizer = Adam(learning_rate=0.01)\nearly_stopping = EarlyStopping(monitor=\"val_loss\", patience=3)\ndecaying_learning_rate = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=2, min_delta=0.1, min_lr=0.0001)\nlr_metric = get_lr_metric(optimizer)\n\nmodel5.compile(loss=loss, optimizer=optimizer, metrics=[\"accuracy\", lr_metric])\nmodel5.fit(\n    x=x_train,\n    y=y_train,\n    epochs=20,\n    batch_size=64,\n    validation_split=0.1,\n    callbacks=[early_stopping, decaying_learning_rate]\n)","metadata":{"execution":{"iopub.status.busy":"2022-02-03T09:15:29.393032Z","iopub.execute_input":"2022-02-03T09:15:29.393353Z","iopub.status.idle":"2022-02-03T09:16:24.2195Z","shell.execute_reply.started":"2022-02-03T09:15:29.39332Z","shell.execute_reply":"2022-02-03T09:16:24.218699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame(model5.history.history).plot()","metadata":{"execution":{"iopub.status.busy":"2022-02-03T09:17:49.24537Z","iopub.execute_input":"2022-02-03T09:17:49.245728Z","iopub.status.idle":"2022-02-03T09:17:49.52618Z","shell.execute_reply.started":"2022-02-03T09:17:49.245685Z","shell.execute_reply":"2022-02-03T09:17:49.525416Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Okay, That's a good gain to be honest :D**\n\nLet's try this model over the test set","metadata":{}},{"cell_type":"code","source":"start = time.time()\npredictions = model5.predict(x_test)\nend = time.time()\n\n# Decode the one-hotted output\npredictions = [np.argmax(p) for p in predictions]\n\naccuracy = accuracy_score(y_test, predictions)\nconf_matrix = confusion_matrix(y_test, predictions)\n\nprint(\"Support Vector Machine\")\nprint(f\"Accuracy: {accuracy}\")\nprint(\"Confusion Matrix\\n\", conf_matrix)\nprint(f\"Elapsed Time: {end - start: .4f} second\")","metadata":{"execution":{"iopub.status.busy":"2022-02-03T09:21:22.764928Z","iopub.execute_input":"2022-02-03T09:21:22.765281Z","iopub.status.idle":"2022-02-03T09:21:23.291631Z","shell.execute_reply.started":"2022-02-03T09:21:22.765245Z","shell.execute_reply":"2022-02-03T09:21:23.290673Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Convolutional Neural Network","metadata":{}},{"cell_type":"markdown","source":"Well the Neural network did very well actually, let's try to improve it a little bit by chnging the input itself. Let's try to extract some features first before we feeding it to the network, and in order to do that, let's add some base line of convolutional and pooling layers before the network to get a set of features and feed it to our network ","metadata":{}},{"cell_type":"markdown","source":"but first we need to square our **x_train** and **x_test**","metadata":{}},{"cell_type":"code","source":"x_train_square = x_train.reshape(-1, 28, 28)\nx_test_square = x_test.reshape(-1, 28, 28)","metadata":{"execution":{"iopub.status.busy":"2022-02-03T09:25:03.288744Z","iopub.execute_input":"2022-02-03T09:25:03.289136Z","iopub.status.idle":"2022-02-03T09:25:03.293777Z","shell.execute_reply.started":"2022-02-03T09:25:03.289093Z","shell.execute_reply":"2022-02-03T09:25:03.293115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This function only to print out the learning rate\ndef get_lr_metric(optimizer):\n    def lr(y_true, y_pred):\n        return optimizer.lr\n    return lr\n\n\ninput_layer = Input(shape=(x_train_square.shape[1], x_train_square.shape[2], 1))\n\nx = Conv2D(16, (2, 2), padding=\"same\", activation=\"relu\")(input_layer)\nx = BatchNormalization()(x)\nx = MaxPooling2D(pool_size=(2, 2))(x)\n\nx = Conv2D(32, (2, 2), padding=\"same\", activation=\"relu\")(x)\nx = BatchNormalization()(x)\nx = MaxPooling2D(pool_size=(2, 2))(x)\n\nx = Conv2D(64, (2, 2), padding=\"same\", activation=\"relu\")(x)\nx = BatchNormalization()(x)\nx = MaxPooling2D(pool_size=(2, 2))(x)\n\nx = Conv2D(128, (2, 2), padding=\"same\", activation=\"relu\")(x)\nx = BatchNormalization()(x)\nx = MaxPooling2D(pool_size=(2, 2))(x)\n\nx = Flatten()(x)\n\nx = Dense(700, activation=\"relu\")(x)\nx = BatchNormalization()(x)\nx = Dense(350, activation=\"relu\")(x)\nx = BatchNormalization()(x)\nx = Dropout(0.2)(x)\nx = Dense(175, activation=\"relu\")(x)\nx = BatchNormalization()(x)\nx = Dense(100, activation=\"relu\")(x)\nx = BatchNormalization()(x)\nx = Dropout(0.2)(x)\nx = Dense(50, activation=\"relu\")(x)\nx = BatchNormalization()(x)\nx = Dense(25, activation=\"relu\")(x)\nx = BatchNormalization()(x)\nx = Dropout(0.2)(x)\nx = Dense(15, activation=\"relu\")(x)\nx = BatchNormalization()(x)\n\noutput_layer = Dense(10, activation=\"softmax\")(x)\n\nmodel6 = Model(inputs=input_layer, outputs=output_layer)\nmodel6.summary()\n\nloss = SparseCategoricalCrossentropy()\noptimizer = Adam(learning_rate=0.01)\nearly_stopping = EarlyStopping(monitor=\"val_loss\", patience=3)\ndecaying_learning_rate = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=2, min_delta=0.1, min_lr=0.0001)\nlr_metric = get_lr_metric(optimizer)\n\nmodel6.compile(loss=loss, optimizer=optimizer, metrics=[\"accuracy\", lr_metric])\nmodel6.fit(\n    x=x_train_square,\n    y=y_train,\n    epochs=20,\n    batch_size=64,\n    validation_split=0.1,\n    callbacks=[early_stopping, decaying_learning_rate]\n)","metadata":{"execution":{"iopub.status.busy":"2022-02-03T09:30:29.35849Z","iopub.execute_input":"2022-02-03T09:30:29.359029Z","iopub.status.idle":"2022-02-03T09:33:43.310711Z","shell.execute_reply.started":"2022-02-03T09:30:29.358976Z","shell.execute_reply":"2022-02-03T09:33:43.309832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame(model6.history.history).plot()","metadata":{"execution":{"iopub.status.busy":"2022-02-03T09:34:13.6141Z","iopub.execute_input":"2022-02-03T09:34:13.614423Z","iopub.status.idle":"2022-02-03T09:34:13.900686Z","shell.execute_reply.started":"2022-02-03T09:34:13.614387Z","shell.execute_reply":"2022-02-03T09:34:13.900002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**ALRIGHT that's a very optimistic result**, let's try it on the test set","metadata":{}},{"cell_type":"code","source":"start = time.time()\npredictions = model6.predict(x_test_square)\nend = time.time()\n\n# Decode the one-hotted output\npredictions = [np.argmax(p) for p in predictions]\n\naccuracy = accuracy_score(y_test, predictions)\nconf_matrix = confusion_matrix(y_test, predictions)\n\nprint(\"Support Vector Machine\")\nprint(f\"Accuracy: {accuracy}\")\nprint(\"Confusion Matrix\\n\", conf_matrix)\nprint(f\"Elapsed Time: {end - start: .4f} second\")","metadata":{"execution":{"iopub.status.busy":"2022-02-03T09:35:17.952408Z","iopub.execute_input":"2022-02-03T09:35:17.953403Z","iopub.status.idle":"2022-02-03T09:35:19.215506Z","shell.execute_reply.started":"2022-02-03T09:35:17.953337Z","shell.execute_reply":"2022-02-03T09:35:19.214647Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submit","metadata":{}},{"cell_type":"code","source":"df_test = pd.read_csv(\"/kaggle/input/digit-recognizer/test.csv\")\n\ntest_data = df_test.values\ntest_data = test_data / 255.\n\ntest_data_square = test_data.reshape(-1, 28, 28)\npredictions = model6.predict(test_data_square)\n\n# Decode the one-hotted output\npredictions = [np.argmax(p) for p in predictions]\n\nwith open(\"/kaggle/working/submission.csv\", \"w\") as out:\n    out.write(\"ImageId,Label\\n\")\n    for i, p in enumerate(predictions):\n        out.write(f\"{i+1},{p}\\n\")","metadata":{"execution":{"iopub.status.busy":"2022-02-03T09:43:45.530388Z","iopub.execute_input":"2022-02-03T09:43:45.530704Z","iopub.status.idle":"2022-02-03T09:43:53.203705Z","shell.execute_reply.started":"2022-02-03T09:43:45.530663Z","shell.execute_reply":"2022-02-03T09:43:53.202999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Conclusion","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"There are a lot of other experiments that you can do over this data set, try to increase the size of the network, reduce it, try different activations and different combinations of trainig techniques.","metadata":{}},{"cell_type":"markdown","source":"Hope that was helpful and useful, if you enjoyed it please **Upvote** :D","metadata":{}}]}