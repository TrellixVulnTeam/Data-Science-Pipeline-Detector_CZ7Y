{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directoryka\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/digit-recognizer/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/digit-recognizer/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape, test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Normalize the data\ntrain = train / 255.0\ntest = test / 255.0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = train[\"label\"]\ny = tf.keras.utils.to_categorical(y, num_classes=10)\nimage_id = list(test.index)\nimage_id = [i+1 for i in image_id]\n\ntrain = train.drop(\"label\", axis=1)\ntrain = train.values.reshape(-1, 28, 28, 1)\ntest = test.values.reshape(-1, 28, 28, 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_image(index):\n    plt.figure(0, figsize=(4,4))\n    plt.imshow(np.reshape(train[index],(28,28)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_image(300)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_mulitple_images(columns, lines, index):\n    plt.figure(0, figsize=(10,10))\n    for i in range(columns*lines):\n        plt.subplot(lines, columns, i+1)\n        plt.imshow(np.reshape(train[index+i], (28,28)))\n        plt.axis('off')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_mulitple_images(5, 6, 34)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import keras, os\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten\nfrom keras.layers import Conv2D, MaxPool2D, BatchNormalization, MaxPool2D\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xtrain, xtest, ytrain, ytest = train_test_split(train, y, test_size=0.2)\nxtrain, xval, ytrain, yval = train_test_split(xtrain, ytrain, test_size=0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xtrain.shape, xval.shape, xtest.shape","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"from matplotlib import pyplot as plt\nimport math\nfrom keras.callbacks import LambdaCallback\nimport keras.backend as K\n\n\nclass LRFinder:\n    \"\"\"\n    Plots the change of the loss function of a Keras model when the learning rate is exponentially increasing.\n    See for details:\n    https://towardsdatascience.com/estimating-optimal-learning-rate-for-a-deep-neural-network-ce32f2556ce0\n    \"\"\"\n    def __init__(self, model):\n        self.model = model\n        self.losses = []\n        self.lrs = []\n        self.best_loss = 1e9\n\n    def on_batch_end(self, batch, logs):\n        # Log the learning rate\n        lr = K.get_value(self.model.optimizer.lr)\n        self.lrs.append(lr)\n\n        # Log the loss\n        loss = logs['loss']\n        self.losses.append(loss)\n\n        # Check whether the loss got too large or NaN\n        if math.isnan(loss) or loss > self.best_loss * 4:\n            self.model.stop_training = True\n            return\n\n        if loss < self.best_loss:\n            self.best_loss = loss\n\n        # Increase the learning rate for the next batch\n        lr *= self.lr_mult\n        K.set_value(self.model.optimizer.lr, lr)\n\n    def find(self, x_train, y_train, start_lr, end_lr, batch_size=64, epochs=1):\n        num_batches = epochs * x_train.shape[0] / batch_size\n        self.lr_mult = (end_lr / start_lr) ** (1 / num_batches)\n\n        # Save weights into a file\n        self.model.save_weights('tmp.h5')\n\n        # Remember the original learning rate\n        original_lr = K.get_value(self.model.optimizer.lr)\n\n        # Set the initial learning rate\n        K.set_value(self.model.optimizer.lr, start_lr)\n\n        callback = LambdaCallback(on_batch_end=lambda batch, logs: self.on_batch_end(batch, logs))\n\n        self.model.fit(x_train, y_train,\n                        batch_size=batch_size, epochs=epochs,\n                        callbacks=[callback])\n\n        # Restore the weights to the state before model fitting\n        self.model.load_weights('tmp.h5')\n\n        # Restore the original learning rate\n        K.set_value(self.model.optimizer.lr, original_lr)\n\n    def plot_loss(self, n_skip_beginning=10, n_skip_end=5):\n        \"\"\"\n        Plots the loss.\n        Parameters:\n            n_skip_beginning - number of batches to skip on the left.\n            n_skip_end - number of batches to skip on the right.\n        \"\"\"\n        plt.ylabel(\"loss\")\n        plt.xlabel(\"learning rate (log scale)\")\n        plt.plot(self.lrs[n_skip_beginning:-n_skip_end], self.losses[n_skip_beginning:-n_skip_end])\n        plt.xscale('log')\n\n    def plot_loss_change(self, sma=1, n_skip_beginning=10, n_skip_end=5, y_lim=(-0.01, 0.01)):\n        \"\"\"\n        Plots rate of change of the loss function.\n        Parameters:\n            sma - number of batches for simple moving average to smooth out the curve.\n            n_skip_beginning - number of batches to skip on the left.\n            n_skip_end - number of batches to skip on the right.\n            y_lim - limits for the y axis.\n        \"\"\"\n        assert sma >= 1\n        derivatives = [0] * sma\n        for i in range(sma, len(self.lrs)):\n            derivative = (self.losses[i] - self.losses[i - sma]) / sma\n            derivatives.append(derivative)\n\n        plt.ylabel(\"rate of loss change\")\n        plt.xlabel(\"learning rate (log scale)\")\n        plt.plot(self.lrs[n_skip_beginning:-n_skip_end], derivatives[n_skip_beginning:-n_skip_end])\n        plt.xscale('log')\n        plt.ylim(y_lim)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def model():\n\n    model = Sequential()\n    model.add(Conv2D(32, kernel_size = 3, activation='relu', input_shape = (28, 28, 1)))\n    model.add(BatchNormalization())\n    model.add(Conv2D(32, kernel_size = 3, activation='relu'))\n    model.add(BatchNormalization())\n    model.add(Conv2D(32, kernel_size = 5, strides=2, padding='same', activation='relu'))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.4))\n\n    model.add(Conv2D(64, kernel_size = 3, activation='relu'))\n    model.add(BatchNormalization())\n    model.add(Conv2D(64, kernel_size = 3, activation='relu'))\n    model.add(BatchNormalization())\n    model.add(Conv2D(64, kernel_size = 5, strides=2, padding='same', activation='relu'))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.5))\n\n    model.add(Conv2D(128, kernel_size = 4, activation='relu'))\n    model.add(BatchNormalization())\n    model.add(Flatten())\n    model.add(Dropout(0.4))\n    model.add(Dense(10, activation='softmax'))\n\n    adam = tf.keras.optimizers.Adam()\n\n    model.compile(optimizer=adam, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n\n    early = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=50)\n\n    checkpoint_path = 'training_1/cp.ckpt'\n    checkpoint_dir = os.path.dirname(checkpoint_path)\n\n    # create checkpoint callback\n    cp_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_path,\n                                                save_weights_only=True,\n                                                verbose=1)\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef determineLearningRate(xtrain,ytrain,xtest,ytest):    \n    input_shape = (28,28,1)\n    num_classes = 10\n    epochs = 15\n    \n    lr_finder = LRFinder(model())\n    lr_finder.find(xtrain,ytrain, start_lr=0.000001, end_lr=100, batch_size=64, epochs=epochs)\n    lr_finder.plot_loss(n_skip_beginning=20, n_skip_end=5)\n    plt.show()\n    return model\ndetermineLearningRate(xtrain, ytrain, xtest, ytest)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def model():\n\n    model = Sequential()\n    model.add(Conv2D(32, kernel_size = 3, activation='relu', input_shape = (28, 28, 1)))\n    model.add(BatchNormalization())\n    model.add(Conv2D(32, kernel_size = 3, activation='relu'))\n    model.add(BatchNormalization())\n    model.add(Conv2D(32, kernel_size = 5, strides=2, padding='same', activation='relu'))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.4))\n\n    model.add(Conv2D(64, kernel_size = 3, activation='relu'))\n    model.add(BatchNormalization())\n    model.add(Conv2D(64, kernel_size = 3, activation='relu'))\n    model.add(BatchNormalization())\n    model.add(Conv2D(64, kernel_size = 5, strides=2, padding='same', activation='relu'))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.5))\n\n    model.add(Conv2D(128, kernel_size = 4, activation='relu'))\n    model.add(BatchNormalization())\n    model.add(Flatten())\n    model.add(Dropout(0.4))\n    model.add(Dense(10, activation='softmax'))\n\n    adam = tf.keras.optimizers.Adam(\n    learning_rate=0.001,\n    beta_1=0.9,\n    beta_2=0.999,\n    epsilon=1e-07,\n    amsgrad=False)\n\n    model.compile(optimizer=adam, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n\n    early = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=50)\n\n    checkpoint_path = 'training_1/cp.ckpt'\n    checkpoint_dir = os.path.dirname(checkpoint_path)\n\n    # create checkpoint callback\n    cp_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_path,\n                                                save_weights_only=True,\n                                                verbose=1)\n    print(model.summary())\n    history = model.fit(xtrain, ytrain, epochs=100, callbacks=[cp_callback, early], validation_data=(xval, yval))\n    prediction = model.predict(test)\n    prediction = np.argmax(prediction, axis=1)\n\n    return history, prediction","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history, prediction = model()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = {\"ImageId\": image_id, \"Label\":prediction}\nresults = pd.DataFrame(data)\nresults.to_csv(\"result.csv\",index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_mulitple_images_with_label(columns, lines, index):\n    plt.figure(0, figsize=(12,12))\n    for i in range(columns*lines):\n        plt.subplot(lines, columns, i+1)\n        plt.imshow(np.reshape(test[index+i], (28,28)))\n        plt.title(\"Predicted : \"+str(prediction[index+i]))\n        plt.axis('off')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_mulitple_images_with_label(5, 3, 27)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Using Image data Generator","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.image import ImageDataGenerator","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"datagen_train = datagen_valid = ImageDataGenerator(\n        featurewise_center = False,\n        samplewise_center = False,\n        featurewise_std_normalization = False, \n        samplewise_std_normalization = False,\n        zca_whitening = False,\n        horizontal_flip = False,\n        vertical_flip = False,\n        fill_mode = 'nearest',\n        rotation_range = 10,  \n        zoom_range = 0.1, \n        width_shift_range = 0.1, \n        height_shift_range = 0.1)\n        \n\ndatagen_train.fit(xtrain)\ntrain_gen = datagen_train.flow(xtrain, ytrain, batch_size=64)\ndatagen_valid.fit(xval)\nvalid_gen = datagen_valid.flow(xval, yval, batch_size=64)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential([\n        Conv2D(32, kernel_size=(3, 3), activation='relu', padding='same', input_shape = (28,28,1)),\n        Conv2D(32, kernel_size=(3, 3), activation='relu' ),\n        BatchNormalization(),\n        MaxPool2D(pool_size=(2, 2)),\n        Dropout(0.2),\n        \n        Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same'),\n        Conv2D(64, kernel_size=(3, 3), activation='relu' ),\n        BatchNormalization(),\n        MaxPool2D(pool_size=(2, 2)),\n        Dropout(0.2),\n        \n        Conv2D(128, kernel_size=(3, 3), activation='relu', padding='same' ),\n        Conv2D(128, kernel_size=(3, 3), activation='relu' ),\n        BatchNormalization(),\n        MaxPool2D(pool_size=(2, 2)),\n        Dropout(0.2),\n        \n        \n        Flatten(),\n        \n        Dense(512, activation='relu'),\n        Dropout(0.5),\n        \n        Dense(10, activation = \"softmax\")\n        \n    ])\nadam = tf.keras.optimizers.Adam(\n    learning_rate=0.001,\n    beta_1=0.9,\n    beta_2=0.999,\n    epsilon=1e-07,\n    amsgrad=False)\n\nmodel.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n\nearly = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=50)\n\ncheckpoint_path = 'training_1/cp.ckpt'\ncheckpoint_dir = os.path.dirname(checkpoint_path)\n\ncp_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_path,\n                                                save_weights_only=True,\n                                                verbose=1)\nprint(model.summary())\nhistory = model.fit((train_gen), epochs=100, callbacks=[cp_callback, early], validation_data=(valid_gen))\nprediction = model.predict(test)\nprediction = np.argmax(prediction, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = {\"ImageId\": image_id, \"Label\":prediction}\nresults = pd.DataFrame(data)\nresults.to_csv(\"result_data_generator.csv\",index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_mulitple_images_with_label(5, 3, 27)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}