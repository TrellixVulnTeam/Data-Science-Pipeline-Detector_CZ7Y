{"cells":[{"execution_count":null,"metadata":{"_uuid":"f9bc1cf47cc53e2f802c999913e8250e800bfb2b","_cell_guid":"0fce0229-4a77-40ea-bc6d-2b687dc181f4"},"cell_type":"markdown","source":"# Kaggle Competition: Digit recognition on MNIST data\n8/3/2017 Wei-Ying Wang\n\nThis is my tutorial about how to use Keras to construct a CNN model for digit recognition. The tutorial tried to be comprehensive about building CNN with Keras. Keras is designed to be easy to use and manipulate, however I found difficult to understand the structure I built when I first used it. I hope this tutorial can help smooth the learning curve of using Keras.\n\nMost of the information is on chapter 2 and 3. I will emphasize a lot on knowing the **number of parameters**, **inputs**, and **outputs**.\n\nI eventually got 99.21% correction rate (With 2 layer of CNN and one dense layer, i.e. normal neural network layer, and some filters of different sizes from here). Note that it is not surprising that one can get 100% on the test set provided by Kaggle, since it is not difficult to find all the MNIST data somewhere else. The real winner (correct me if I am wrong) so far is from [Dan CireÅŸan et. al. 2012](https://arxiv.org/abs/1202.2745), who got 99.77% correction rate, which achieved near human performance. He used CNN, too.\n\n## Table of content\n\n### 1. Import modules and preprocessing the data\n\n### 2. A Typical CNN structure: one CNN layer\n\n### 3. Stack more CNN layers\n\n### 4. Using the learned trained model to predict the test set\n\nYou can download this Ipython notebook here or at [My Github Website](https://github.com/wayinone/MNIST_Kaggle).\n","outputs":[]},{"execution_count":null,"metadata":{"_uuid":"bdf78dbba270e909f5b7df09d0a92cdb9f2a41a4","_cell_guid":"488c0986-36e4-4885-97aa-3b891c2ecee9"},"cell_type":"markdown","source":"## 1. Import modules and preprocessing the data","outputs":[]},{"outputs":[],"metadata":{"_uuid":"4de6a5d29b28e552819c1599d875f9d3529bd356","_execution_state":"idle","trusted":true,"_cell_guid":"0608ac4c-6ce3-45c7-ac26-fe65893cd1e1"},"cell_type":"code","source":"from importlib import reload\n#from __future__ import print_function\nimport keras\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout,Flatten,Conv2D, MaxPooling2D\nfrom keras.optimizers import RMSprop\nfrom keras.utils.np_utils import to_categorical\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split","execution_count":1},{"execution_count":null,"metadata":{"_uuid":"82e588181f2493bbc207b86c2dc033d19cc8188a","_cell_guid":"f1bf60b0-964b-42fb-8635-b874fee9b22f"},"cell_type":"markdown","source":"Import the data.","outputs":[]},{"outputs":[],"metadata":{"_uuid":"189f36fa9d303d0764cf0603856a3b74d39dfb4e","_execution_state":"idle","trusted":true,"_cell_guid":"c5359d99-d444-4866-9528-f19dc0550ee7"},"cell_type":"code","source":"train = pd.read_csv('../input/train.csv')\ntrain.head()\nprint('training data is (%d, %d).'% train.shape)","execution_count":3},{"execution_count":null,"metadata":{"_uuid":"ed68d622cf3e6f8d6d45be15105b3957d93fb3b0","_execution_state":"idle","_cell_guid":"d41f3ebe-28b8-42c8-92a3-aec7e508af3c"},"cell_type":"markdown","source":"Let's split the training set into 2, training set and validation set. This is done with `train_test_split` in `sklearn` module.","outputs":[]},{"outputs":[],"metadata":{"_uuid":"738686cc43ffa7df8a359ec0670144862b99e1bf","_execution_state":"idle","trusted":true,"collapsed":true,"_cell_guid":"a43ce5a4-dd71-4aba-b0e2-31fdd03bb0f5"},"cell_type":"code","source":"X_train_all = (train.iloc[:,1:].values).astype('float32')/255 # all pixel values, convert to value in [0,1]\ny_train_all = train.iloc[:,0].values.astype('int32') # only labels i.e targets digits\ny_train_all= to_categorical(y_train_all) # This convert y into onehot representation\nX_train, X_val, y_train, y_val = train_test_split(X_train_all, y_train_all, test_size=0.10, random_state=42)","execution_count":6},{"execution_count":null,"metadata":{"_uuid":"b8982a96ec0a8b40e3496c688e8a120aceb28347","_cell_guid":"8207dbc8-5d70-4109-95a1-0d50ebf88f51"},"cell_type":"markdown","source":"We have to map the image vectors (size 784) back to image. Specially, we have to convert to (28,28,1), since the convoluation layer of Kares only accept image of dimension 3 (the last dimension is the color channel).","outputs":[]},{"outputs":[],"metadata":{"_uuid":"23dd3c5f645a9c11d9bac5faffa9e85689cae213","_execution_state":"idle","trusted":true,"collapsed":true,"_cell_guid":"fe297e6a-2a73-447d-8139-22f805541669"},"cell_type":"code","source":"X_train_img=np.reshape(X_train,(X_train.shape[0],28,28,1))\nX_val_img = np.reshape(X_val,(X_val.shape[0],28,28,1))","execution_count":7},{"outputs":[],"metadata":{"_uuid":"ab843ef321822dcf656641ff5a468e8bbad9734e","_execution_state":"idle","trusted":false,"_cell_guid":"32b1cebd-0d58-4028-8dd9-a2d8a52bef2b"},"cell_type":"code","source":"plt.imshow(X_train_img[0,:,:,0],cmap = 'gray')\nplt.show()","execution_count":13},{"execution_count":null,"metadata":{"_uuid":"9beedc8de718175a1a659838a5f2cf2b132be36e","_cell_guid":"bc09721e-2e99-4c4c-a720-f08dfa4066e0"},"cell_type":"markdown","source":"## 2. A Typical CNN structure: one CNN layer.\n\nWe are going to setup the following CNN model. Basically: input layer --> CNN layer --> softmax layer\n\n### 1. Input layer: \nThe original input image is `28 x 28`, so the `input_shape=(28,28,1)`, where `1` indicates that number of color channels is 1. \n \n### 2. Convolution layer:\n In the following convolution layer, there are 32 filters, and each filters is `3 x 3`.\n  * You can set border differently by:\n       ```\n       border_mode='same', 'fall', or 'valid' (default)\n       ```\n  * With `valid` border, after convolution (with stride=0), the filted image (AKA **feature map**) size is `(28-2) x (28-2) = 26 x 26` . Note there are 32 feature maps.\n    \n  * Set **stride** 2 by `strides=(2,2)`. The stride action will skip one pixel (both vertically and horizontally) while applying convolution. So now the every feature map (the filtered and subsampled image) is `13 x 13`\n \n  * There are $$32\\cdot3\\cdot3+32 =320$$ parameters:\n     - Each \"pixel\" of a feature map is obtained by $$\\sum_{i=1}^9 w_i x_i +b,$$ where $$w_1,...,w_9,b\\in \\mathbb{R}^{10}$$ are parameters and $$(x_1,...,x_9)$$ are pixel values of a `3 x 3` image patches, AKA **receptive field**, in the original input image.\n   \n  * Using `relu` units by `activation='relu'`, then max Pooling by 2. So the output of this CNN layer will be 32 of `6 x  6` \"images\".\n   \n### 3. Flatten layer:\n one has to 'flatten' the output of the CNN layer before applying softmax layer. After flatten, the input of the next layer is of $$6\\cdot 6\\cdot 32=1152$$ values.\n \n### 4. Softmax layer:\nSo the last layer ('softmax') would require $$1152\\cdot 10+10=11530$$ parameters.\n","outputs":[]},{"outputs":[],"metadata":{"_uuid":"f84a4d313311daeed5dff0dc70ca10be153305ac","_execution_state":"idle","trusted":true,"_cell_guid":"bf2ed711-f2b1-43a9-89e9-f73bc9a8bf69"},"cell_type":"code","source":"model = Sequential()\nmodel.add(Conv2D(32, (3, 3),\n                 activation='relu',\n                 input_shape=(28,28,1),strides=(2,2)))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Flatten(name='flatten'))\nmodel.add(Dense(10, activation='softmax'))\nmodel.summary()","execution_count":8},{"execution_count":null,"metadata":{"_uuid":"e3ceb1a121fc83870b50178805c34ad7a46fd62b","_cell_guid":"8f04f142-761e-4aa4-a721-c931fc25015b"},"cell_type":"markdown","source":"Before fitting the parameters, we need to compile the model first.","outputs":[]},{"outputs":[],"metadata":{"_uuid":"f2fb8b8916c679ebfce08621a0bebd5467d137d1","_execution_state":"idle","trusted":true,"collapsed":true,"_cell_guid":"b2e4fb4d-88c4-466a-92ae-ab4ada308489"},"cell_type":"code","source":"model.compile(loss='categorical_crossentropy',\n              optimizer='RMSprop',\n              metrics=['accuracy'])","execution_count":9},{"execution_count":null,"metadata":{"_uuid":"1e7a7b470b5b3de0ac4976d6ec3d16f65af879f3","_cell_guid":"9380b41b-141c-4c03-98b0-3f0594007ee7"},"cell_type":"markdown","source":"We can now fit the parameters. Note that:\n\n * A better `batch_size` should be the exponent of 2, (e.g. 32, 64, 128, ..., etc) this is from the document of Keras.\n * Each epoch will scan through all the training data. Where each update of parameters uses `batch_size` number of data (It will be chosen randomly through each epoch, i.e. the data will be shuffled before entering the next epoch.)\n * `initial_epoch` allows you to continue your last parameter fitting.\n   - e.g. `initial_epoch`=10, will continue the parameter fitting from the 10th epoch.\n\n * `verbose` controls the information to be displayed. 0: no information displayed, 1: max information, 2: somewhat between.","outputs":[]},{"outputs":[],"metadata":{"_uuid":"21f1700e44c35ba9bab842952cb37220355c2b7c","_execution_state":"idle","trusted":true,"_cell_guid":"6756fcf6-0993-442b-98b4-27fd296a8be4"},"cell_type":"code","source":"batch_size = 64 \nnb_epochs = 5 \nhistory = model.fit(X_train_img, y_train,\n                    batch_size=batch_size,\n                    epochs=nb_epochs,\n                    verbose=2, \n                    validation_data=(X_val_img, y_val),\n                    initial_epoch=0)","execution_count":10},{"execution_count":null,"metadata":{"_uuid":"31a2bf07ef37bf110532be2292f79918ceb1d2f1","_cell_guid":"47ae5637-175f-474b-9dee-d658a45f5fb9"},"cell_type":"markdown","source":"We can see that after 20 epoch, the validation accuracy is not improved (around 97.6%). The training set accuracy is about 98.8%. We should first try more complicated model to see if the training set accuracy get higher.","outputs":[]},{"execution_count":null,"metadata":{"_uuid":"331601beca7541fc9ac6cc469809a49c4ed34353","_cell_guid":"3200030f-b296-426b-a1f1-53a9ba79de1d"},"cell_type":"markdown","source":"## 3. Stack more CNN layers\n\nIn the following model, we have:\n### 1. Input layer\n* The input is `28 x 28 x 1`.  Note that I can describe it as `28 x 28` like before. However, I describe this way (`28 x 28 x 1`) because it will be easier for you to understand if you want to stack another CNN layer later.\n\n### 2. First CNN layer:\n* Filter (kernel) size: `5 x 5 x 1`. With `valid` padding.\n* Number of filters: 64.\n* The output is 64 of `24 x 24` feature maps. i.e.` 24 x 24 x 64`\n* The number of parameters: $$25\\cdot 64+64=1664$$.\n    \n### 3. Second CNN layer:\n* The input is `24 x 24 x 64`\n* Filter (kernel) size: `3 x 3 x 64`. With `valid` padding.\n    * That is, we treat the input as a `24 x 24` image with **64 color channels**.\n* Number of filters: 32.\n* The output is 32 of `22 x 22` filtered images (or say, feature maps). i.e. `22 x 22 x 32`\n* The number of parameters: $$3\\cdot 3 \\cdot 64\\cdot 32+32 = 18464$$.\n    * Think this way: There are 64 nodes, and each node is `24 x 24`  feature maps.\n    \n### 4. MaxPool layer: \n* Set `pool_size = (2,2)`, this means a pixel of the new feature map is the max intensity of 4 of the pixels from old feature map.\n* The output is `11 x 11 x 32`, i.e. 32  feature maps.\n\n### 5. Dense layer: A normal neural network layer with 128 nodes.\n* Each node here will connect to 32 nodes of previous layer. (Each node in previous layer is represented as a `11 x 11` image.)\n* The output is `11 x 11 x 128`.\n* The number of parameters: $$32\\cdot 128+128=4224.$$\n* Apply dropout rate 0.2 to prevent overfitting.\n\n### 6.: Flatten the output of the previous layer:\n* The output is $$11\\cdot 11\\cdot 128=15488$$ nodes.\n\n### 7.  Softmax layer: 10 softmax units\n* The number of parameter: $$15488\\cdot10+10=154890.$$","outputs":[]},{"outputs":[],"metadata":{"_uuid":"eb0d55e3feefcf8e6ecc81ccbca5fe1bd5648b15","_execution_state":"idle","trusted":false,"_cell_guid":"8300c740-1c54-4b57-a89b-fcb43dee8f3f"},"cell_type":"code","source":"model = Sequential()\nmodel.add(Conv2D(64, (5, 5),\n                 activation='relu',\n                 input_shape=(28,28,1)))\nmodel.add(Conv2D(32, (3, 3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dropout(0.2))   \nmodel.add(Flatten(name='flatten'))\nmodel.add(Dense(10, activation='softmax'))\nmodel.summary()\nmodel.compile(loss='categorical_crossentropy',\n              optimizer='RMSprop',\n              metrics=['accuracy'])","execution_count":26},{"execution_count":null,"metadata":{"_uuid":"e526504a5bf2a7ba69e1f91474d0c4121f726aab","_execution_state":"idle","_cell_guid":"418fe739-8d5c-4f8b-9f93-efd2160a9f8d"},"cell_type":"markdown","source":"The following will be very slow. It will take several minute to finish one epoch.","outputs":[]},{"outputs":[],"metadata":{"_uuid":"7c5fd13c0b78fa789c5cd00a4bf629b1d7f6750b","_execution_state":"idle","trusted":false,"_cell_guid":"f70cc586-9066-4d9d-ada9-ba6b34b557f3"},"cell_type":"code","source":"batch_size = 64\nnb_epochs = 2\nhistory = model.fit(X_train_img, y_train,\n                    batch_size=batch_size,\n                    epochs=nb_epochs,\n                    verbose=2, # verbose controls the infromation to be displayed. 0: no information displayed\n                    validation_data=(X_val_img, y_val),\n                    initial_epoch=0)\n","execution_count":28},{"execution_count":null,"metadata":{"_uuid":"b44795b4eca6cbbf93d51010d0247f4f4080a220","_cell_guid":"fc5d2b80-985f-4318-ba06-c0b7a4ea8418"},"cell_type":"markdown","source":"The following function shows the wrongly predicted images. Many of them I can't even tell what it is...","outputs":[]},{"execution_count":null,"metadata":{"_uuid":"cd97d21a84731ba6f8bee5a5672f3e4c3d1326cd","_cell_guid":"139e5629-a822-4aac-ae41-63dabe6c16f4"},"cell_type":"markdown","source":"## 4. Using the learned model to predict the test set.","outputs":[]},{"outputs":[],"metadata":{"trusted":true,"collapsed":true,"_uuid":"25dfa42e01d93c77d51bd10cc6c6ee7bcd537b39"},"cell_type":"code","source":"test = pd.read_csv('../input/test.csv')\nX_test = (test.values).astype('float32')/255 # all pixel values\nX_test_img = np.reshape(X_test,(X_test.shape[0],28,28,1))","execution_count":11},{"outputs":[],"metadata":{"_uuid":"355fc986ab5c32dc51de4829cd8cad5f1aae9b50","_execution_state":"idle","trusted":true,"_cell_guid":"807eac83-7181-4b4c-8726-1da71ddd901b"},"cell_type":"code","source":"pred_classes = model.predict_classes(X_test_img,verbose=0)","execution_count":12},{"execution_count":null,"metadata":{"_uuid":"02a3ab74db0f1b38a5331c76110185c82cfda755","_execution_state":"idle","_cell_guid":"b266b5e8-ae8c-480d-add6-ed897d3e4153"},"cell_type":"markdown","source":"Take a look at what we predicted on a test sample.","outputs":[]},{"outputs":[],"metadata":{"_uuid":"7f8806c95b9c8a04f8def3a2006964e0a69825fc","_execution_state":"idle","trusted":true,"_cell_guid":"a9b2a01c-ce11-49dc-885c-1609c1986593"},"cell_type":"code","source":"i=10\nplt.imshow(X_test_img[i,:,:,0],cmap='gray')\nplt.title('prediction:%d'%pred_classes[i])\nplt.show()","execution_count":13},{"execution_count":null,"metadata":{"_uuid":"450ed451833a94e77006a6e9fab9dc13d951df8d","_cell_guid":"74de49cb-78f8-4fff-b776-c548442c9180"},"cell_type":"markdown","source":"## Appendix:\n\nThe following function `plot_difficult_samples` will plot difficult samples from training set.","outputs":[]},{"outputs":[],"metadata":{"_uuid":"f8d65f69aa7daa8f6271b041d0d27e51fc49a2a0","_execution_state":"idle","trusted":false,"collapsed":true,"_cell_guid":"d355712c-eaf3-46a9-9046-7d2669862870"},"cell_type":"code","source":"def plot_difficult_samples(model,x,y, verbose=True):\n    \"\"\"\n    model: trained model from keras\n    x: size(n,h,w,c)\n    y: is categorical, i.e. onehot, size(n,10)\n    \"\"\" \n    #%%\n    \n    pred_classes = model.predict_classes(x,verbose= 0)\n    y_val_classes = np.argmax(y, axis=1)\n    er_id = np.nonzero(pred_classes!=y_val_classes)[0]\n    #%%\n    K = np.ceil(np.sqrt(len(er_id)))\n    fig = plt.figure()\n    print('There are %d wrongly predicted images out of %d validation samples'%(len(er_id),x.shape[0]))\n    for i in range(len(er_id)):\n        ax = fig.add_subplot(K,K,i+1)\n        k = er_id[i]\n        ax.imshow(x[er_id[i],:,:,0])\n        ax.axis('off')\n        if verbose:\n            ax.set_title('%d as %d'%(y_val_classes[k],pred_classes[k]))","execution_count":36},{"outputs":[],"metadata":{"_uuid":"e636ad30e420bea03c15f8e3c353e6dc059d7e19","_execution_state":"idle","trusted":false,"_cell_guid":"12d1c608-ce5f-45e0-8c18-256c8bacc564"},"cell_type":"code","source":"plot_difficult_samples(model,X_val_img,y_val,verbose=False)\nplt.show()","execution_count":37}],"nbformat":4,"metadata":{"language_info":{"version":"3.6.1","mimetype":"text/x-python","file_extension":".py","nbconvert_exporter":"python","codemirror_mode":{"version":3,"name":"ipython"},"pygments_lexer":"ipython3","name":"python"},"kernelspec":{"display_name":"Python 3","name":"python3","language":"python"}},"nbformat_minor":2}