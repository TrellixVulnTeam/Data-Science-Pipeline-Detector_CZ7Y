{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# MNIST DATASET","metadata":{}},{"cell_type":"markdown","source":"The MNIST database (Modified National Institute of Standards and Technology database) is a large database of handwritten digits that is commonly used for training various image processing systems.The database is also widely used for training and testing in the field of machine learning.It was created by \"re-mixing\" the samples from NIST's original datasets.","metadata":{}},{"cell_type":"markdown","source":"# 1. Importing necessary libraries. ","metadata":{}},{"cell_type":"code","source":"import torch\nfrom tqdm import tqdm\nimport numpy as np\nfrom torchvision import transforms,models\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom torch.utils.data import TensorDataset, DataLoader, Dataset,ConcatDataset\nfrom torch import nn","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Using pandas library to read the datasets.","metadata":{}},{"cell_type":"code","source":"train=pd.read_csv(\"../input/digit-recognizer/train.csv\")\ntest=pd.read_csv(\"../input/digit-recognizer/test.csv\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Creating a custom class to load data.","metadata":{}},{"cell_type":"code","source":"class MnistDataset(Dataset):\n    \n    def __init__(self, dataframe, \n                 transform = transforms.Compose([transforms.ToTensor()])):\n        \n        df = dataframe\n        self.n_pixels = 784\n        \n        if len(df.columns) == self.n_pixels:\n            # validation data\n            self.X = df.values.reshape((-1,28,28)).astype(np.uint8)[:,:,:,None]\n            self.y = None\n        else:\n            # training data\n            self.X = df.iloc[:,1:].values.reshape((-1,28,28)).astype(np.uint8)[:,:,:,None]\n            self.y = torch.from_numpy(df.iloc[:,0].values)\n            \n        self.transform = transform\n    \n    def __len__(self):\n        return len(self.X)\n    \n    def __getitem__(self, idx):\n        if self.y is not None:\n            return self.transform(self.X[idx]), self.y[idx]\n        else:\n            return self.transform(self.X[idx])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Defining the transforms.","metadata":{}},{"cell_type":"code","source":" img_tform_1 = transforms.Compose([\n    transforms.ToPILImage(),transforms.ToTensor(),transforms.Normalize((0.5),(0.5))])\n\nimg_tform_2 = transforms.Compose([\n    transforms.ToPILImage(),transforms.RandomRotation(10),transforms.ToTensor(),transforms.Normalize((0.5),(0.5))])\n\nimg_tform_3 = transforms.Compose([\n    transforms.ToPILImage(),transforms.RandomRotation(20),transforms.ToTensor(),transforms.Normalize((0.5),(0.5))])\n\nimg_tform_4 = transforms.Compose([\n    transforms.ToPILImage(),transforms.RandomAffine(degrees=15, translate=(0.1,0.1), scale=(0.85,0.85)),\\\n    transforms.ToTensor(),transforms.Normalize((0.5),(0.5))])\n\nimg_tform_5 = transforms.Compose([\n    transforms.ToPILImage(),transforms.RandomAffine(0,shear=30,scale=[1.15,1.15]),\\\n    transforms.ToTensor(),transforms.Normalize((0.5),(0.5))])\n\nimg_tform_6 = transforms.Compose([\n    transforms.ToPILImage(),transforms.RandomAffine(0,shear=20,scale=[0.8,0.8]),\\\n    transforms.ToTensor(),transforms.Normalize((0.5),(0.5))])\n\nimg_tform_7 = transforms.Compose([\n    transforms.ToPILImage(),transforms.RandomAffine(degrees=30, scale=(1.2,1.2)),\\\n    transforms.ToTensor(),transforms.Normalize((0.5),(0.5))])\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5. Using the defined class and DataLoader to load data into pytorch datasets.","metadata":{}},{"cell_type":"markdown","source":"We will be using the batch-size as 64 and num_workers as 4. We use the customized \"MNISTDataset\" class to convert the data from csv files into a format which is loaded into the dataloader. This is done for both train and validation datasets.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nseed=42\ndef create_dataloaders(seed, test_size=0.1, df=train, batch_size=64):\n    # Create training set and validation set\n    train_df, val_df = train_test_split(df,test_size=test_size,random_state=seed)\n    \n    # Create Datasets\n    train_data_1 = MnistDataset(train_df)\n    train_data_2 = MnistDataset(train_df, img_tform_2)\n    train_data_3 = MnistDataset(train_df, img_tform_3)\n    train_data_4 = MnistDataset(train_df, img_tform_4)\n    train_data_5 = MnistDataset(train_df, img_tform_5)\n    train_data_6 = MnistDataset(train_df, img_tform_6)\n    train_data_7 = MnistDataset(train_df, img_tform_7)\n    train_final = ConcatDataset([train_data_1, train_data_2, train_data_3, train_data_4, train_data_5,\\\n                                   train_data_6,train_data_7])\n\n    val_data = MnistDataset(val_df)\n    \n    # Create Dataloaders\n    train_loader = torch.utils.data.DataLoader(train_final, batch_size=batch_size, shuffle=True)\n    valid_loader = torch.utils.data.DataLoader(val_data, batch_size=batch_size, shuffle=False)\n\n    return train_loader, valid_loader\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\nprint(device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will be defining the classes on which we will be predicting. We are having 10 classes consisting of numbers 0-9.","metadata":{}},{"cell_type":"code","source":"classes = [i for i in range(0,10)]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 6. Defining the pretrained model.","metadata":{}},{"cell_type":"markdown","source":"ResNet, which was proposed in 2015 by researchers at Microsoft Research introduced a new architecture called Residual Network.\n\nResidual Block:\nIn order to solve the problem of the vanishing/exploding gradient, this architecture introduced the concept called Residual Network. In this network we use a technique called skip connections . The skip connection skips training from a few layers and connects directly to the output.\n\nThis network uses a 34-layer plain network architecture inspired by VGG-19 in which then the shortcut connection is added. These shortcut connections then convert the architecture into residual network. ","metadata":{}},{"cell_type":"markdown","source":"![](https://media.geeksforgeeks.org/wp-content/uploads/20200424011138/ResNet.PNG)","metadata":{}},{"cell_type":"markdown","source":"We tune the number of channels in the first layer of the ResNet to suit the images present in the dataset.\nWe also add a fully connected layer at the end of the network architecture to make the prediction.","metadata":{}},{"cell_type":"code","source":"model = models.resnet34(pretrained=True)\nmodel.conv1 = torch.nn.Conv1d(1, 64, (3, 3), (1, 1), (1, 1), bias=False)\nnum_ftrs = model.fc.in_features\nmodel.fc = nn.Linear(num_ftrs, 10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 7. Defining the loss function and optimizer.","metadata":{}},{"cell_type":"markdown","source":"We are using a \"CrossEntropy\" loss function and an \"Adam\" optimizer while training the model.\nWe also use a scheduler which decays the learning rate of each parameter group by gamma every step_size epochs. Notice that such decay can happen simultaneously with other changes to the learning rate from outside this scheduler.","metadata":{}},{"cell_type":"code","source":"if torch.cuda.is_available():\n    model.cuda()\n    \nimport torch.optim as optim\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(),amsgrad=True)\nxp_lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1, verbose=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 8. Training the defined network.","metadata":{}},{"cell_type":"markdown","source":"We will be training our network on 10 epochs.","metadata":{}},{"cell_type":"code","source":"epochs= 20\nvalid_loss_min = np.Inf\ntrain_epoch=[]\ntrain_loss_vals=[]\ntrain_acc_vals=[]\nvalid_epoch=[]\nvalid_loss_vals=[]\nvalid_acc_vals=[]\ntest_loss_val=[]\ntest_epoch=[]\ntrain_loader, valid_loader= create_dataloaders(seed=seed)\nfor i in range(epochs):\n    model.train()\n    train_acc=0\n    valid_acc=0\n    total=0\n    with tqdm(train_loader, unit=\"batch\") as tepoch:\n        for data, target in tepoch:\n            if torch.cuda.is_available():\n                data,target = data.cuda(), target.cuda()\n            optimizer.zero_grad()\n            output=model(data)\n            _, predicted = torch.max(output.data, 1)\n            train_acc+=((predicted==target).sum().item())\n            total += target.size(0)\n            loss = criterion(output, target)\n            loss.backward()\n            train_epoch.append(loss.item())\n            optimizer.step()\n        \n    xp_lr_scheduler.step()\n    train_loss_vals.append(sum(train_epoch)/len(train_epoch))\n    train_acc_vals.append(100 * train_acc/ total)\n    model.eval()\n    total=0\n    with tqdm(valid_loader, unit=\"batch\") as tepoch:\n        for data, target in tepoch:\n            if torch.cuda.is_available():\n                data,target= data.cuda(),target.cuda()\n            output=model(data)\n            _, predicted = torch.max(output.data, 1)\n            valid_acc+=((predicted==target).sum().item())\n            total += target.size(0)\n            loss= criterion(output,target)\n            valid_epoch.append(loss.item())\n    valid_loss_vals.append(sum(valid_epoch)/len(valid_epoch))\n    valid_acc_vals.append(100 * valid_acc/ total)\n    \n    print(\"epoch:{}\\t  training_loss:{}\\t  validation_loss:{}\\t  train_accuracy:{}\\t  validation_accuracy:{}\"\n          .format(i,train_loss_vals[i],valid_loss_vals[i],train_acc_vals[i],valid_acc_vals[i]))\n    if valid_loss_vals[i] <= valid_loss_min:\n        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n        valid_loss_min,\n        valid_loss_vals[i]))\n        torch.save(model.state_dict(), 'model_cifar.pt')\n        valid_loss_min = valid_loss_vals[i]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 9. Saving the model.","metadata":{}},{"cell_type":"code","source":"model.load_state_dict(torch.load('model_cifar.pt'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 10. Plotting the train and validation accuracy curves.","metadata":{}},{"cell_type":"code","source":"plt.plot(np.linspace(1, epochs, epochs).astype(int), train_acc_vals,label='train_accuracy')\nplt.plot(np.linspace(1, epochs, epochs).astype(int), valid_acc_vals,label='valid_accuracy')\nplt.xlabel('epochs')\nplt.ylabel('accuracy')\nplt.legend()\nplt.title('Accuracy curve')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 11. Plotting the train and validation loss curves.","metadata":{}},{"cell_type":"code","source":"plt.plot(np.linspace(1, epochs, epochs).astype(int), train_loss_vals,label='train_loss')\nplt.plot(np.linspace(1, epochs, epochs).astype(int), valid_loss_vals,label='valid_loss')\nplt.xlabel('epochs')\nplt.ylabel('loss')\nplt.legend()\nplt.title('loss functions')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 12. Loading the test data and preprocessing the images.","metadata":{}},{"cell_type":"code","source":"\ntest_images = test.values.reshape((-1, 1, 28, 28)) / 255.0\nprint(test_images.shape)\n\ntest_image_tensor = torch.tensor(test_images, dtype=torch.float32)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 13. Making the predictions.","metadata":{}},{"cell_type":"code","source":"model.eval()\nresult = np.zeros(test_images.shape[0], dtype=np.int64)\n\nwith torch.no_grad():\n    for i in range(test_images.shape[0]):\n        image = test_image_tensor[i, 0, :, :].view(1, 1, 28, 28)\n        output=model(image.cuda())\n        _, pred = torch.max(output, 1) \n        result[i] = classes[pred.item()]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result[:10]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission=pd.read_csv('../input/digit-recognizer/sample_submission.csv')\nsample_submission['Label']=result","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 14. Converting predictions to CSV file and submitting.","metadata":{}},{"cell_type":"code","source":"sample_submission.to_csv('submission1.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### With this, we come to the end of the notebook.\n**Please upvote if you found it useful :)\nIt motivates me a lot to share more such stuff.**","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}