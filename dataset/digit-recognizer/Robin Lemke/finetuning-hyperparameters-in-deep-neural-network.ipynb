{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom functools import partial\n#import seaborn as sn\nfrom sklearn.model_selection import train_test_split\n\nfrom random import seed\nseed(1)\nseed = 43\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow import image\nfrom tensorflow import core\nfrom tensorflow.keras import layers\nprint(\"Tensorflow Version: \", tf.__version__)\nprint(\"Keras Version: \",keras.__version__)\n\n\nkaggle = 1 # Kaggle path active = 1\n\n# change your local path here\nif kaggle == 1 :\n    MNIST_PATH= '../input/digit-recognizer'\nelse:\n    MNIST_PATH= '../Digit_Recognition_with_a_Deep_Neural_Network/data/input/digit-recognizer'\n\n\n\nimport os\nfor dirname, _, filenames in os.walk(MNIST_PATH): \n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Introduction - MNIST Training Competition\nThis notebook is a fork of my previous developed notebook for digit recognition. Therefore you will find some parts that look common the the notebook <a href=\"https://www.kaggle.com/skiplik/digit-recognition-with-a-deep-neural-network\">Digit Recognition with a Deep Neural Network</a> and some parts that are completely different.\n\nWith this I want to take a deeper look in some parts of finetuning hyperparameters. The following list shows some of the finetuning parameters which I will take a look into, one or two ore more ... :\n- Dwindling / Exploding Gradients\n    - <b>Initializing the Weights</b>\n    - <b>Batchnormalization</b>\n    - <s>Gradient Clipping</s>\n    - <b>Saturated Activataion Functions</b>\n- Optimizers\n    - <s>Momentum Optimizers</s>\n    - <s>Nesterov</s>\n    - <s>AdaGrad</s>\n    - <s>RMSProp</s>\n    - <b>Adam - Optimizer</b>\n    - <s>Scheduling Learnrate</s>\n- Regulations\n    - <s>Drop-Outs</s>\n    - <b>l1 / l2 - Regulations</b>\n    - <s>Monte-Carlo Drop-out ???</s>\n    - <s>Max Norm Regulations ????</s>\n\nNot part of this notebook will be the use of pretrained neural networks (Transferlearning). I just want to list this here for the sake of completeness.\n\nLink to the data topic: https://www.kaggle.com/c/digit-recognizer/data\n\nAs in the previous notebooks I will use Tensorflow with Keras. I already mentioned in other notebooks, I will skip some explanations about the data set here. Moreover I will use the already discovered knowledge about the data and transform/prepare the data rightaway.\n\n## Notebook Versions with Different Hyperparameter Configurations\nAs described in the part above, I used/tested different hyperparameter settings to get a little bit closer to its effects on the neural network and the network's results. I know that there are parameters that effect other parameters when they have changed (and therefore should have been changed as well), however in these cases I just tried a little bit around. Sometimes I kept one or two parameters together, which should be together (e.g. kernel initializer \"lecun\" and activation function \"selu\") and sometimes not. The main purpose here was to use them and see the results.\n\nTherefore on Kaggle you can look in the different versions of this notebook if you are interested. In the following I will list some versions with the used hyperparameter config in it:\n- Version 7 and 6:\n    - Activation Function - \"relu\"\n    - Initializing Weights - \"He Normalization\"\n    - Batchnormalization\n- Version 9:\n    - Activation Function - \"selu\"\n    - Initializing Weights - \"LeCun Normal\"\n- Version 12 and 14:\n    - Regularisation with L1 and L2\n- Version 15:\n    - Activation Function - \"relu\"\n    - Initializing Weights - \"He Normalization\"\n    - Batchnormalization\n    - Optimizer - \"Adam\"\n- Version 18:\n    - Activation Function - \"relu\"\n    - Initializing Weights - \"He Normalization\"\n    - Batchnormalization\n    - Optimizer - \"Adam\"\n\nThe current best run was based on the Version 16 with an accuracy of 0.97714 on the kaggle competition \"Digit Recognzier\".\n\n## Tensorflow Data Api for Preprocessing\nSince notebook version 18 I use the Tensorflow Data Api for Preprocessing. It helps you a lot with batching and processing a big bunch of data really fast. More interesting here is the fact that the hole part of data processing can be added to the model itself to transport it on mobile devices  later\n\n\n## My other Projects\nIf you are interested in some more clearly analysis of the dataset take a look into my other notebooks about the MNIS-dataset:\n- Digit Recognition with a Deep Neural Network: https://www.kaggle.com/skiplik/digit-recognition-with-a-deep-neural-network\n- Another MNIST Try: https://www.kaggle.com/skiplik/another-mnist-try\n- First NN by Detecting Handwritten Characters: https://www.kaggle.com/skiplik/first-nn-by-detecting-handwritten-characters\n...\n\n\n","metadata":{}},{"cell_type":"markdown","source":"# Get Data","metadata":{}},{"cell_type":"code","source":"# Data path and file\nCSV_FILE_TRAIN='train.csv'\nCSV_FILE_TEST='test.csv'\n\ndef load_mnist_data(minist_path, csv_file):\n    csv_path = os.path.join(minist_path, csv_file)\n    return pd.read_csv(csv_path)\n\ndef load_mnist_data_manuel(minist_path, csv_file):\n    csv_path = os.path.join(minist_path, csv_file)\n    csv_file = open(csv_path, 'r')\n    csv_data = csv_file.readlines()\n    csv_file.close()\n    return csv_data\n\ndef split_train_val(data, val_ratio):\n    return \n    \n\ntrain = load_mnist_data(MNIST_PATH,CSV_FILE_TRAIN)\ntest = load_mnist_data(MNIST_PATH,CSV_FILE_TEST)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y = train['label'].copy()\nX = train.drop(['label'], axis=1)\n\n# competition dataset\nX_test = test.copy()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train / Val Split","metadata":{}},{"cell_type":"code","source":"print(\"Shape of the Features: \",X.shape)\nprint(\"Shape of the Labels: \", y.shape)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Label Value Count\nVisualizing the label distribution of the full train dataset.","metadata":{}},{"cell_type":"code","source":"train.value_counts('label')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_val, y_train, y_val = train_test_split(X, y, random_state=seed, test_size=0.20\n                                                  , stratify=y\n                                                 )","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Comparing the equally splitted train- and val-sets based on the given label y.","metadata":{}},{"cell_type":"code","source":"print(\"Train - Set Distribution\")\nprint(y_train.value_counts() / y_train.value_counts().sum() )\nprint('--------------------------------------------------------------')\nprint('--------------------------------------------------------------')\nprint('--------------------------------------------------------------')\nprint(\"Val - Set Distribution\")\nprint(y_val.value_counts() / y_val.value_counts().sum() )\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"X: \", X.shape)\nprint(\"X_train: \", X_train.shape)\nprint(\"X_val: \", X_val.shape)\n\nprint(\"y_train: \", y_train.shape)\nprint(\"y_val: \", y_val.shape)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Building Transforming Piplines","metadata":{}},{"cell_type":"code","source":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import Normalizer\nfrom sklearn.preprocessing import StandardScaler\n\npipeline = Pipeline([\n    #('normalizer', Normalizer())\n    ('std_scalar',StandardScaler())\n])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Preprocessing Data","metadata":{}},{"cell_type":"markdown","source":"### Data Augmentation with Tensorflow Data Api","metadata":{}},{"cell_type":"code","source":"def random_crop(image):\n    shape = tf.shape(image)\n    min_dim = tf.reduce_min([shape[0], shape[1]]) * 85 // 100       # croping to 90% of the initial picture \n    return tf.image.random_crop(image, [min_dim, min_dim, 1])\n\n\ndef crop_flip_resize(image, label, flipping = True):\n    if flipping == True:\n        cropped_image = random_crop(image)\n        cropped_image = tf.image.flip_left_right(cropped_image)\n    else:\n        cropped_image = random_crop(image)\n\n    ## final solution\n    resized_image = tf.image.resize(cropped_image, [28,28])\n    final_image = resized_image\n    #final_image = keras.applications.xception.preprocess_input(resized_image)\n    return final_image, label  \n    ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_val.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# converting dataframe format into tensorflow compatible format.\nX_train = X_train.values.reshape(X_train.shape[0], 28, 28, 1)\nX_val = X_val.values.reshape(X_val.shape[0], 28, 28, 1)\n\nX_train_crop = X_train.copy()\nX_val_crop = X_val.copy()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating tensorbased dataset \n\ntraining_dataset = (\n    tf.data.Dataset.from_tensor_slices(\n        (\n            tf.cast(X_train, tf.float32),\n            tf.cast(y_train, tf.int32)\n        )\n    )\n)\n\n\nval_dataset = (\n    tf.data.Dataset.from_tensor_slices(\n        (\n             tf.cast(X_val, tf.float32),\n             tf.cast(y_val, tf.int32)\n        )\n    )\n)\n\n\ntraining_crop_dataset = (\n    tf.data.Dataset.from_tensor_slices(\n        (\n            tf.cast(X_train_crop, tf.float32),\n            tf.cast(y_train, tf.int32)\n        )\n    )\n)\n\n\nval_crop_dataset = (\n    tf.data.Dataset.from_tensor_slices(\n        (\n             tf.cast(X_val_crop, tf.float32),\n             tf.cast(y_val, tf.int32)\n        )\n    )\n)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# resizing, croping images via self build function\ntraining_crop_dataset = training_crop_dataset.map(partial(crop_flip_resize, flipping=False))\nval_crop_dataset = val_crop_dataset.map(partial(crop_flip_resize, flipping=False))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualizing a croped, flipped, resized image from new dataset.\nfor X_values, y_values in training_crop_dataset.take(1):\n    for index in range(1):\n        plt.imshow(X_values)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# concate the two datasets\ntraining_dataset_all = training_dataset.concatenate(training_crop_dataset)\nval_dataset_all = val_dataset.concatenate(val_crop_dataset)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"training_dataset_all length: \", len(list(training_dataset_all)))\nprint(\"val_dataset_all length: \", len(list(val_dataset_all)))\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# shuffeling and batching data\ntf.random.set_seed(seed)\n\ntrain_ds = training_dataset_all.shuffle(10000).batch(32).prefetch(1)\nval_ds = val_dataset_all.shuffle(8000).batch(32).prefetch(1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Building a Deep Neural Network","metadata":{}},{"cell_type":"markdown","source":"## Preparing Model Visualization with Tensorboard (not for Kaggle)","metadata":{}},{"cell_type":"code","source":"root_logdir = \"../../tensorboard-logs\"\n\nprint(\"Relative root_logdir: \",root_logdir)\n\ndef get_run_logdir():\n    import time\n    run_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\")\n    return os.path.join(root_logdir,run_id)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"run_logdir = get_run_logdir()\nprint(\"Current run logdir for Tensorboard: \", run_logdir)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"run_logdir","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Keras Callbacks for Tensorboard\nWith Keras there is a way of using Callbacks for the Tensorboard to write log files for the board and visualize the different graphs (loss and val curve)\n","metadata":{}},{"cell_type":"code","source":"tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Building Model Architecture","metadata":{}},{"cell_type":"code","source":"from keras.layers import LeakyReLU\n\ninput_shape=[784]\ninput_shape_notFlattened=[28,28]\n\n\nlearning_rt = 1e-03 \nactivation_fn = \"relu\"\ninitializer = \"he_normal\"\nregularizer =  None #keras.regularizers.l2(0.01)\n\n# Model building\nmodel = keras.models.Sequential()\n\nmodel.add(keras.layers.Flatten(input_shape=input_shape_notFlattened))\nmodel.add(keras.layers.BatchNormalization())\nmodel.add(keras.layers.Dense(500, activation=activation_fn, kernel_initializer=initializer, kernel_regularizer= regularizer)) ## add  kernel_regularizer=keras.regularizers.l2(0.01)) ???\nmodel.add(keras.layers.BatchNormalization())\nmodel.add(keras.layers.Dense(500, activation=activation_fn, kernel_initializer=initializer, kernel_regularizer= regularizer))\nmodel.add(keras.layers.BatchNormalization())\nmodel.add(keras.layers.Dense(300, activation=activation_fn, kernel_initializer=initializer, kernel_regularizer= regularizer))\nmodel.add(keras.layers.BatchNormalization())\nmodel.add(keras.layers.Dense(300, activation=activation_fn, kernel_initializer=initializer, kernel_regularizer= regularizer))\nmodel.add(keras.layers.BatchNormalization())\nmodel.add(keras.layers.Dense(300, activation=activation_fn, kernel_initializer=initializer, kernel_regularizer= regularizer))\nmodel.add(keras.layers.BatchNormalization())\nmodel.add(keras.layers.Dense(300, activation=activation_fn, kernel_initializer=initializer, kernel_regularizer= regularizer))\n\nmodel.add(keras.layers.Dense(10, activation=\"softmax\", kernel_initializer=\"glorot_uniform\"))\n\n\noptimizer = keras.optimizers.Adam(learning_rate=learning_rt)\n\n\nmodel.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'] )\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Checkpoints","metadata":{}},{"cell_type":"code","source":"checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_train_model.h5\", save_best_only=True, save_weights_only=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Training","metadata":{}},{"cell_type":"code","source":"history = model.fit(train_ds, epochs=200, validation_data=(val_ds), callbacks=[checkpoint_cb, keras.callbacks.EarlyStopping(patience=20), tensorboard_cb])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Visualizing the Progress","metadata":{}},{"cell_type":"code","source":"plt.plot(pd.DataFrame(history.history))\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model Training with Full Dataset \nIn this part I will train the model with the full dataset. This time I will use the discovered hyperparameters from previous section.","metadata":{}},{"cell_type":"code","source":"\n# Model building\nmodel_full = keras.models.Sequential()\n\nmodel_full.add(keras.layers.Flatten(input_shape=input_shape_notFlattened))\nmodel_full.add(keras.layers.BatchNormalization())\nmodel_full.add(keras.layers.Dense(500, activation=activation_fn, kernel_initializer=initializer, kernel_regularizer= regularizer)) ## add  kernel_regularizer=keras.regularizers.l2(0.01)) ???\nmodel_full.add(keras.layers.BatchNormalization())\nmodel_full.add(keras.layers.Dense(500, activation=activation_fn, kernel_initializer=initializer, kernel_regularizer= regularizer))\nmodel_full.add(keras.layers.BatchNormalization())\nmodel_full.add(keras.layers.Dense(300, activation=activation_fn, kernel_initializer=initializer, kernel_regularizer= regularizer))\nmodel_full.add(keras.layers.BatchNormalization())\nmodel_full.add(keras.layers.Dense(300, activation=activation_fn, kernel_initializer=initializer, kernel_regularizer= regularizer))\nmodel_full.add(keras.layers.BatchNormalization())\nmodel_full.add(keras.layers.Dense(300, activation=activation_fn, kernel_initializer=initializer, kernel_regularizer= regularizer))\nmodel_full.add(keras.layers.BatchNormalization())\nmodel_full.add(keras.layers.Dense(300, activation=activation_fn, kernel_initializer=initializer, kernel_regularizer= regularizer))\n\nmodel_full.add(keras.layers.Dense(10, activation=\"softmax\", kernel_initializer=\"glorot_uniform\"))\n\n\noptimizer = keras.optimizers.SGD(learning_rate=learning_rt)\n\n\nmodel_full.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'] )","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_full.summary()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# creating a new log dir for tensorboard\ntensorboard_cb_f = keras.callbacks.TensorBoard(get_run_logdir())\ncheckpoint_cb_f = keras.callbacks.ModelCheckpoint(\"my_modell_full.h5\", save_best_only=False, save_weights_only=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# preparing full features set (X) for the tensorflow data api\n\ntraining_dataset_all = training_dataset.concatenate(training_crop_dataset)\nval_dataset_all = val_dataset.concatenate(val_crop_dataset)\n\ntraining_ds_all = training_dataset_all.concatenate(val_dataset_all)\n\ntraining_ds_all = training_ds_all.shuffle(20000).batch(32).prefetch(1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train the model again pleeeeease with all you got .... especially the new transformed data matrix X \nhistory_full = model_full.fit(training_ds_all, epochs=60, callbacks=[tensorboard_cb_f, checkpoint_cb_f])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(pd.DataFrame(history_full.history))\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Image Prediction of Unknown Data (Test Data)","metadata":{}},{"cell_type":"markdown","source":"## Peparing Test Data\nAs well as previously done, we need to create a TF dataset of the test set as well.","metadata":{}},{"cell_type":"code","source":"# converting dataframe format into tensorflow compatible format.\nX_test = X_test.values.reshape(X_test.shape[0], 28, 28, 1)\n\n\ntest_dataset = (\n    tf.data.Dataset.from_tensor_slices(\n        (\n            tf.cast(X_test, tf.float32)\n        )\n    )\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dataset","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_ds = test_dataset.batch(32).prefetch(1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Creating Competition File","metadata":{}},{"cell_type":"code","source":"mnist_competition_file = pd.DataFrame(columns=['ImageId','Label'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Prediction of Testdata","metadata":{}},{"cell_type":"code","source":"# Visualizing the image\nplt.figure(figsize=(12, 12))\nfor X_batch in test_ds.take(1):\n    for index in range(1):\n        plt.subplot(3, 3, index + 1)\n        plt.imshow(X_batch[index])\n\nplt.show()","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for element in test_ds.take(1):\n    print(\"Propability of all lables for given pixels: \", model_full.predict(test_ds.take(1))[0])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Predicted Digit: \",np.argmax(model_full.predict(test_ds.take(1))[0]))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = model_full.predict(test_ds)                                                                           # predict the probability\npredictions = np.argmax(predictions, axis=1)                                                                        # getting the predicted digit numbers based ont the probability of every np element \nmnist_competition_file = pd.DataFrame(predictions)                                                                  # converting into df\nmnist_competition_file.index += 1                                                                                   # index should start at 1\nmnist_competition_file.reset_index(level=0, inplace=True)                                                           # make the index a column \nmnist_competition_file = mnist_competition_file.rename(columns={\"index\": \"ImageId\", 0: \"Label\"}, errors=\"raise\")    # renamen them according to the competition requirements","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mnist_competition_file","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mnist_competition_file.ImageId = mnist_competition_file.ImageId.astype(int)\nmnist_competition_file.Label = mnist_competition_file.Label.astype(int)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mnist_competition_file.to_csv('mnist_submission.csv', index=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}