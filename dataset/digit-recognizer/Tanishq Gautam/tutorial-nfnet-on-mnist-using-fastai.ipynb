{"cells":[{"metadata":{},"cell_type":"markdown","source":"# NFNET Overview\n![ImageNet](https://miro.medium.com/max/1400/1*CjpipU_oChc899f_Esjpyg.png)\n \nNFNET's are Convolutional Residual Style Networks that have no batch normalization build in them. But without the batch normalization usually networks are not performing so well or cannot scale to larger batch sizes however NFNET builds networks that scale to large batch sizes and are more efficient than previous state-of-the-art methods. The training latency vs accuracy graph shows that NFnets are 8.7Ã— times faster than EffNet-B7 for the same top-1 accuracy score trained on ImageNet. "},{"metadata":{},"cell_type":"markdown","source":"# What is Fastai?\n> Fastai is a deep learning library which provides practitioners with high-level components that can quickly and easily provide state-of-the-art results in standard deep learning domains, and provides researchers with low-level components that can be mixed and matched to build new approaches. It aims to do both things without substantial compromises in ease of use, flexibility, or performance. This is possible thanks to a carefully layered architecture, which expresses common underlying patterns of many deep learning and data processing techniques in terms of decoupled abstractions. These abstractions can be expressed concisely and clearly by leveraging the dynamism of the underlying Python language and the flexibility of the PyTorch library."},{"metadata":{},"cell_type":"markdown","source":"# *Please upvote the kernel if you find it useful*"},{"metadata":{},"cell_type":"markdown","source":"# Importing the necessary libraries:"},{"metadata":{"trusted":true},"cell_type":"code","source":"import sys; \nsys.path.insert(0,'../input/timm-nfnet')\nimport timm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"! pip install fastai==1.0.61","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torch.utils.data import Dataset, DataLoader","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd \nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nfrom pathlib import Path\n\nfrom fastai.vision import *\nfrom fastai.metrics import error_rate\n\nfrom PIL import Image","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"inputs=Path(\"../input/digit-recognizer\")\nos.listdir(inputs)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Read the train and test datasets:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# load training data and explore the first three rows\ntrain=pd.read_csv(inputs/\"train.csv\")\ntrain.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# load test data and explore the first three rows\ntest=pd.read_csv(inputs/\"test.csv\")\ntest.head(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Image transforms"},{"metadata":{},"cell_type":"markdown","source":"**To get a set of transforms with default values that work pretty well in a wide range of tasks, it's often easiest to use get_transforms.**                                                                                                     \n* **tfms is just a parameter used later during training, which is initalized here.** \n* **tr and te are paths to be used.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# tfms can be passed directly to define a DataBunch object (see below) which is then associated with a model to begin training.\ntfms = get_transforms(do_flip=False) # if True the image is randomly flipped\ntr=Path(\"../train\")\nte=Path(\"../test\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**We have to try and get the dataset into a folder format, from the existing format, which will make it easier to use fastai's functions.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"for index in range(10):\n    try:\n        os.makedirs(tr/str(index))\n    except:\n        pass","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sorted(os.listdir(tr))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"try:\n    os.makedirs(te)\nexcept:\n    pass\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Prepare Data\n**Currently, it is not even an image, just a 0s and 1s, as seen from the training set. Using the functions below, we can convert them into images:**"},{"metadata":{"trusted":true},"cell_type":"code","source":"for index, row in train.iterrows():\n    \n    label,digit = row[0], row[1:]\n    \n    filepath = tr/str(label)\n    filename = f\"{index}.jpg\"\n    \n    digit = digit.values\n    digit = digit.reshape(28,28)\n    digit = digit.astype(np.uint8)\n    \n    img = Image.fromarray(digit)\n    img.save(filepath/filename)\n    \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for index, digit in test.iterrows():\n\n    filepath = te\n    filename = f\"{index}.jpg\"\n    \n    digit = digit.values\n    digit = digit.reshape(28,28)\n    digit = digit.astype(np.uint8)\n    \n    img = Image.fromarray(digit)\n    img.save(filepath/filename)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Display images "},{"metadata":{"trusted":true},"cell_type":"code","source":"def displayRandomImagesFromEveryFolder(directory=tr, samplesPerDigit=5):\n\n    fig = plt.figure(figsize=(5,10))\n    \n    for rowIndex in range(1, 10):\n        subdirectory = str(rowIndex)\n        path = directory/subdirectory\n        images = os.listdir(path)\n        for sampleIndex in range(1,samplesPerDigit+1):\n            randomNumber = random.randint(0, len(images)-1)\n            image = Image.open(path/images[randomNumber])\n            ax = fig.add_subplot(10, 5, samplesPerDigit*rowIndex + sampleIndex)\n            ax.axis(\"off\")\n            \n            plt.imshow(image, cmap='gray')\n            \n    \n    plt.show()\n    \ndisplayRandomImagesFromEveryFolder()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**The dataset has been converted into images!**\n\n**We can move on to getting the data from folders, and seperating them into training and validation sets. Also normalization is very important to make sure all values lie between 0 and 1.**\n\n**It turns out that the PosixPath is not iterated by ImageDataBunch in Kaggle.So we can change the path created by pathlib library which was a PosixPath object to just a string which specifies the path to the training and testing directories, so in our case train path = \"../train\" and test path = \"../test\"**"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = ImageDataBunch.from_folder(path=\"../train\",test=\"../test\",ds_tfms=tfms, valid_pct=0.2,bs=32,size=24).normalize(imagenet_stats)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.show_batch(rows=3 ,figsize=(5,5))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**The data has been successfully extracted from the folders.**\n\n**We can also check what classes exist:**"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(data.classes)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model \n\nNFNets are a family of modified ResNets that achieves competitive accuracies without batch normalization. To do so, it applies 3 different techniques:\n* Modified residual branches and convolutions with Scaled Weight Standardization\n* Adaptive Gradient Clipping\n* Architecture optimization for improved accuracy and training spee"},{"metadata":{"trusted":true},"cell_type":"code","source":"class NFNetModel(nn.Module):\n    \n    def __init__(self, num_classes=10, model_name='nfnet_f1', pretrained=False):\n        super(NFNetModel, self).__init__()\n        self.model = timm.create_model(model_name, pretrained=pretrained)\n        self.model.head.fc = nn.Linear(self.model.head.fc.in_features, num_classes)\n        \n    def forward(self, x):\n        x = self.model(x)\n        return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = NFNetModel()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn = Learner(data, model, metrics=accuracy)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Next, we can figure out what ideal learning rates are:**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# find optimal learning rate and plot the graph\nlearn.lr_find()\n# plot loss vs. learning rate\nlearn.recorder.plot()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**We can clearly see that the learning rate is most effective at 1e-03, but let's try without a predefined learning rate:**"},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.fit_one_cycle(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**99% accuracy, not bad at all.**\n\n**Saving this model:**"},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.save(\"501\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Now, let's try with the optimal learning rates:**"},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.unfreeze()\nlearn.fit_one_cycle(10,max_lr=1e-3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.save(\"502\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Results\n\n**We can interpret our results as well:**"},{"metadata":{"trusted":true},"cell_type":"code","source":"interp=ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix(figsize=(8,8))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"interp.most_confused(min_val=3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**From this, we can see which two numbers are confused most and the number of times.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"interp.plot_top_losses(9,figsize=(7,7))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**These are the images which had the highest loss, that is the biggest difference between the probability of being corect and actually being correct.**"},{"metadata":{},"cell_type":"markdown","source":"# Prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"class_score,y=learn.get_preds(DatasetType.Test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"probs= class_score[0].tolist()\n[f\"{index}: {probs[index]}\" for index in range(len(probs))]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**These are the probabilities that the image is any of these numbers. But we don't want that. We only want the highest probability:**"},{"metadata":{"trusted":true},"cell_type":"code","source":"class_score=np.argmax(class_score,axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class_score[0].item()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Submission\n\nNow, creating the submission file based on the example given (which should contain ImageId and Label):"},{"metadata":{"trusted":true},"cell_type":"code","source":"samplesub=pd.read_csv(inputs/\"sample_submission.csv\")\nsamplesub.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ImageId = [os.path.splitext(path)[0] for path in os.listdir(te)]\nImageId = [int(path) for path in ImageId]\nImageId = [ID+1 for ID in ImageId]\nImageId[:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"subs=pd.DataFrame({\"ImageId\":ImageId,\"Label\":class_score})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"subs.to_csv(\"submission.csv\",index=False)\nsubs.head(3)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}