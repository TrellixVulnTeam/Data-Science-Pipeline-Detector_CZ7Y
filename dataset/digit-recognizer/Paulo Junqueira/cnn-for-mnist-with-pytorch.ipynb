{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n#pytorch libraries\nimport torch\nfrom torch import nn, optim\nfrom torchvision.transforms import ToTensor\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset,ConcatDataset,DataLoader\n#Visualization \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n#Sklearn train teste split and report\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom torch.optim import lr_scheduler\nfrom torchvision import transforms \n#\nimport copy\nimport os\n\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nsns.set_style(\"darkgrid\", {\"grid.color\": \".6\", \"grid.linestyle\": \":\"})","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-02-23T20:10:06.917523Z","iopub.execute_input":"2022-02-23T20:10:06.918157Z","iopub.status.idle":"2022-02-23T20:10:09.442907Z","shell.execute_reply.started":"2022-02-23T20:10:06.918062Z","shell.execute_reply":"2022-02-23T20:10:09.442157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# CNN with Pytorch\n\nThe main objective of this notebook is to implement a Convolutional Neural Network (CNN) with the library pytorch. Therefore, learn the basics steps of this library  \nTo achieve this goal, the following steps are implemented:\n- Loading Data: Prepare the training and validation datasets\n- **Update: Added Datasets class**\n- Model: Generate a basic model using the architerure of pytorch\n- Run the Model: Execute and validate the model\n- Submission: Use the trained model to predict the test set","metadata":{}},{"cell_type":"code","source":"# General purpose functions\ndef checkData(data):\n    \"\"\"Function to check size of dataframe\"\"\"\n    numLabels = len(data['label'].unique())\n    size, dim = data.shape\n    \n    print(f'Number of labels: {numLabels}\\nDimension size: {dim - 1}\\nExamples: {size}')","metadata":{"execution":{"iopub.status.busy":"2022-02-23T20:10:09.444521Z","iopub.execute_input":"2022-02-23T20:10:09.444718Z","iopub.status.idle":"2022-02-23T20:10:09.450259Z","shell.execute_reply.started":"2022-02-23T20:10:09.444688Z","shell.execute_reply":"2022-02-23T20:10:09.44949Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Configuration File is a class that is used to improve quality of life while testing diferent parameters in the experiments.","metadata":{}},{"cell_type":"code","source":"# Class to define hyperparameters\nclass configFile():\n    \"\"\"Configuration file for easy parameters changes\"\"\"\n    # Size of the training batch\n    batch_size = 128\n    # Percentage of train and validation split\n    val_size = 0.1\n    # learning rate parameter of the optmizer\n    learning_rate = 1e-3\n    \n    \n# Instanciating a config file object\ncfg = configFile()","metadata":{"execution":{"iopub.status.busy":"2022-02-23T20:10:09.451966Z","iopub.execute_input":"2022-02-23T20:10:09.452529Z","iopub.status.idle":"2022-02-23T20:10:09.461356Z","shell.execute_reply.started":"2022-02-23T20:10:09.452489Z","shell.execute_reply":"2022-02-23T20:10:09.460478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading Data","metadata":{}},{"cell_type":"code","source":"class MNISTdataset(Dataset):\n    \"\"\"V2\"\"\"\n    def __init__(self, data, transform = None):\n        \n        self.X = (data.drop('label', axis =1).values/255).reshape((-1, 28, 28,1))\n        self.y = data['label'].values.reshape(-1,1)\n        self.n_samples = data.shape[0]\n        \n        self.transform = transform \n    \n    def __getitem__(self, index):\n        X,y = self.X[index], self.y[index]\n        \n        if self.transform:\n            X  = self.transform(X)\n            \n        return (X,y)\n        \n    def __len__(self):\n        return self.n_samples\n\n\n\n#Train and Validation sets    \ntrain_df = pd.read_csv('../input/digit-recognizer/train.csv',dtype=np.float32)\n#Apply a list of multiple transforms transforms.RandomHorizontalFlip(p=0.5)\ncompose_agu = transforms.Compose([transforms.ToTensor(),transforms.RandomRotation(degrees=(0, 45), fill=(0,))])\ncompose_original = transforms.Compose([transforms.ToTensor()])\n\ndataset_original = MNISTdataset(train_df,transform = compose_original)\ndataset_aug = MNISTdataset(train_df,transform = compose_agu)\n\nincreased_dataset = ConcatDataset([dataset_aug,dataset_original])\n\n#spliting Data\nX_train, X_val, = train_test_split(increased_dataset, test_size = cfg.val_size, \\\n                                                  random_state = 666)\n\n# Creating Datasets Loaders\ntrain_loader = DataLoader(X_train, batch_size  = cfg.batch_size,  shuffle = True)\nval_loader = DataLoader(X_val, batch_size  = cfg.batch_size,  shuffle = True)","metadata":{"execution":{"iopub.status.busy":"2022-02-23T20:10:09.464267Z","iopub.execute_input":"2022-02-23T20:10:09.464794Z","iopub.status.idle":"2022-02-23T20:10:30.976299Z","shell.execute_reply.started":"2022-02-23T20:10:09.464737Z","shell.execute_reply":"2022-02-23T20:10:30.975572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Visualizing some examples in the training dataset X\nfig, ax = plt.subplots(nrows = 2, ncols = 5,  figsize = (15,8))\nfor i in range(5):\n    image, y = dataset_aug.__getitem__(i)\n    ax[0,i].imshow(image.squeeze(), cmap=\"magma\") # Squeeze removes length (1,28,28) -> (28,28)\n    ax[0,i].set_title(int(y))\n    \n    image, y = dataset_original.__getitem__(i)\n    ax[1,i].imshow(image.squeeze(), cmap=\"viridis\") # Squeeze removes length (1,28,28) -> (28,28)\n    ax[1,i].set_title(int(y))\n\nax[0,0].set_ylabel('Augmented Images',fontdict= {'fontsize':16})\nax[1,0].set_ylabel('Original Images', fontdict= {'fontsize':16})","metadata":{"execution":{"iopub.status.busy":"2022-02-23T20:10:30.977535Z","iopub.execute_input":"2022-02-23T20:10:30.977777Z","iopub.status.idle":"2022-02-23T20:10:32.809568Z","shell.execute_reply.started":"2022-02-23T20:10:30.977744Z","shell.execute_reply":"2022-02-23T20:10:32.808879Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(device)","metadata":{"execution":{"iopub.status.busy":"2022-02-23T20:10:32.810923Z","iopub.execute_input":"2022-02-23T20:10:32.811162Z","iopub.status.idle":"2022-02-23T20:10:32.854831Z","shell.execute_reply.started":"2022-02-23T20:10:32.811128Z","shell.execute_reply":"2022-02-23T20:10:32.853939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ConvNet(nn.Module):\n    \"\"\"Class to build the Convolutional network\"\"\"\n    def __init__(self):\n        super(ConvNet,self).__init__()\n        #1 Conv layers\n        self.conv1 = nn.Conv2d(in_channels = 1,out_channels= 32,kernel_size = 5) \n        self.relu1 = nn.LeakyReLU()\n#         self.batchnorm1 = nn.BatchNorm2d(32)\n        self.pool1 = nn.MaxPool2d(kernel_size = 2)\n        #2 Conv layers\n        self.conv2 = nn.Conv2d(in_channels = 32,out_channels = 64,kernel_size = 5)\n        self.relu2 = nn.LeakyReLU()\n#         self.batchnorm2 = nn.BatchNorm2d(64)\n        self.pool2 = nn.MaxPool2d(kernel_size = 2)\n        \n        #Flatten Layer\n        self.flatten = nn.Flatten()\n        self.linear1 = nn.Linear(in_features = 1024,out_features = 160)\n        self.linear2 = nn.Linear(in_features = 160,out_features = 92)\n#         self.batchnorm3 = nn.BatchNorm1d(92)\n        self.linear3 = nn.Linear(in_features = 92, out_features = 10)\n        \n    def forward(self, x):\n        \"\"\"Forward Method\"\"\"\n        out = self.conv1(x)\n        out = self.relu1(out)\n#         out = self.batchnorm1(out)\n        out = self.pool1(out)\n        \n        out = self.conv2(out)\n        out = self.relu2(out)\n#         out = self.batchnorm2(out)\n        out = self.pool2(out)\n        \n        out = self.flatten(out)\n        out = self.linear1(out)\n        out = self.linear2(out)\n#         out = self.batchnorm3(out)\n        out = self.linear3(out)\n\n        \n        return out","metadata":{"execution":{"iopub.status.busy":"2022-02-23T20:10:32.856375Z","iopub.execute_input":"2022-02-23T20:10:32.856893Z","iopub.status.idle":"2022-02-23T20:10:32.869659Z","shell.execute_reply.started":"2022-02-23T20:10:32.856854Z","shell.execute_reply":"2022-02-23T20:10:32.86884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training Functions","metadata":{}},{"cell_type":"code","source":"#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n# Training Function\n#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\ndef train_(model, optmizer, loss_func, train_loader, device):\n    \"\"\"Function to train the model\"\"\"\n    train_loss = 0.0\n    train_correct = 0\n    size_sampler = len(train_loader.sampler)\n    \n    for i, (images, labels) in enumerate(train_loader):\n        \n        # Pushing to device (cuda or CPU)\n        images, labels = images.to(device), labels.to(device)\n        \n        #zeroing gradiants\n        optmizer.zero_grad()\n        \n        #feedfoard\n        y_hat = model(images)\n        \n        #Compute loss \n        loss = loss_func(y_hat, labels.long().squeeze())\n        \n        #Compute backpropagation\n        loss.backward()\n        \n        #updating weights\n        optmizer.step()\n        \n        # loss and correct values compute\n        train_loss +=loss.item() * images.size(0)\n        _ , pred = torch.max(y_hat.data, 1)\n        train_correct +=sum(pred == labels.long().squeeze()).sum().item()\n        \n    return np.round(train_loss/size_sampler,4), np.round(train_correct*100./size_sampler,3)\n\n#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n# Validation Function\n#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\ndef validation_(model, loss_func,val_loader, device):\n    val_loss = 0.0\n    val_correct = 0\n    size_sampler = len(val_loader.sampler)\n    with torch.no_grad():\n        for i, (images, labels) in enumerate(val_loader):\n\n            images, labels = images.to(device), labels.to(device)\n            y_hat = model(images)\n            loss = loss_func(y_hat, labels.long().squeeze())\n            \n            val_loss +=loss.item() * images.size(0)\n            _ , pred = torch.max(y_hat.data, 1)\n            val_correct +=(pred == labels.long().squeeze()).sum().item()\n        \n    return np.round(val_loss/size_sampler,4), np.round(val_correct*100./size_sampler,3)\n\n#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n# Main Training\n#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n\ndef train_model(model,optmizer, loss_func,scheduler, train_loader, val_loader, epochs, device, log = True):\n    \n    best_acc = 0\n    \n    \n    print('Initializing Training...')\n    \n    history = {'train_loss':[], 'val_loss':[], 'train_acc':[], 'val_acc':[]}\n    \n    for i in range(epochs):\n        \n        train_loss, train_acc=  train_(model, optmizer, loss_func, train_loader,device)\n        val_loss, val_acc = validation_(model, loss_func,val_loader, device)\n        \n        scheduler.step()\n        \n        if val_acc > best_acc:\n            print(f'>> Saving Best Model with Val Acc: Old: {best_acc} | New: {val_acc}')\n            best_model = copy.deepcopy(model)\n            best_acc = val_acc\n        \n        \n        if log and ((i+1)%2 == 0):\n            print(f'> Epochs: {i+1}/{epochs} - Train Loss: {train_loss} - Train Acc: {train_acc} - Val Loss: {val_loss} - Val Acc: {val_acc}')\n        \n        #Saving infos on a history dict\n        for key, value in zip(history, [train_loss,val_loss,train_acc,val_acc]):\n            history[key].append(value)\n    \n    \n    \n    print('...End Traing')\n            \n    return history,best_model\n\n#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n# Ploting graphics\n#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n\ndef plot_history(history):\n    \n    #Ploting the Loss and Accuracy Curves\n    fig, ax = plt.subplots(nrows = 1, ncols = 2, figsize = (16,6))\n    \n    #Loss\n    sns.lineplot(data = history['train_loss'], label = 'Training Loss', ax = ax[0])\n    sns.lineplot(data = history['val_loss'], label = 'Validation Loss', ax = ax[0])\n    ax[0].legend(loc = 'upper right')\n    ax[0].set_title('Loss')\n    \n    #Accuracy\n    sns.lineplot(data = history['train_acc'], label = 'Training Accuracy', ax = ax[1])\n    sns.lineplot(data = history['val_acc'], label = 'Validation Accuracy', ax = ax[1])\n    ax[1].legend(loc = 'lower right')\n    ax[1].set_title('Accuracy')","metadata":{"execution":{"iopub.status.busy":"2022-02-23T20:10:32.872636Z","iopub.execute_input":"2022-02-23T20:10:32.873189Z","iopub.status.idle":"2022-02-23T20:10:33.056617Z","shell.execute_reply.started":"2022-02-23T20:10:32.873144Z","shell.execute_reply":"2022-02-23T20:10:33.055854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n# Execute training\n#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\nmodel = ConvNet().to(device)\noptmizer = optim.AdamW(model.parameters(), lr = cfg.learning_rate)\nloss_func =  nn.CrossEntropyLoss()\nscheduler = lr_scheduler.StepLR(optmizer, step_size=9, gamma=0.05)\n\nhistory,best_model = train_model(model = model, \n                    optmizer = optmizer, \n                    loss_func = loss_func,\n                    scheduler = scheduler,\n                    train_loader = train_loader,\n                    val_loader = val_loader,\n                    epochs = 30,\n                    device = device)\n\n#ploting results\nplot_history(history)","metadata":{"execution":{"iopub.status.busy":"2022-02-23T20:10:33.057826Z","iopub.execute_input":"2022-02-23T20:10:33.058513Z","iopub.status.idle":"2022-02-23T20:12:11.681762Z","shell.execute_reply.started":"2022-02-23T20:10:33.058472Z","shell.execute_reply":"2022-02-23T20:12:11.681011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Test Data","metadata":{}},{"cell_type":"code","source":"class MNISTdataset_inference(Dataset):\n    \"\"\"Inference\"\"\"\n    def __init__(self, data, transform = None):\n        self.X = (data.values/255).reshape((-1, 28, 28,1))\n        self.n_samples = data.shape[0]\n        \n        self.transform = transform \n    \n    def __getitem__(self, index):\n        X = self.X[index]\n        \n        if self.transform:\n            X  = self.transform(X)\n            \n        return X\n        \n    def __len__(self):\n        return self.n_samples    \n\n\n#Test Sets\ntest_df = pd.read_csv('../input/digit-recognizer/test.csv',dtype=np.float32)\ntest_dataset = MNISTdataset_inference(test_df, transform = compose_original)\ntest_loader = DataLoader(test_dataset, batch_size  = len(test_dataset))","metadata":{"execution":{"iopub.status.busy":"2022-02-23T20:12:11.688138Z","iopub.execute_input":"2022-02-23T20:12:11.690086Z","iopub.status.idle":"2022-02-23T20:12:13.978111Z","shell.execute_reply.started":"2022-02-23T20:12:11.690042Z","shell.execute_reply":"2022-02-23T20:12:13.977374Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Checking Results","metadata":{}},{"cell_type":"code","source":"y_pred_list = []\nwith torch.no_grad():\n    for X_test_ld in (test_loader):\n        y_pred = best_model(X_test_ld.to(device))\n        _, y_pred_tags = torch.max(y_pred.data, dim = 1)\n#         y_pred_list.append((y_pred_tags.to('cpu')))\n\nresult = [int(i) for i in y_pred_tags.to('cpu')]\ndf = pd.Series(result,name = 'Label').reset_index()\ndf.rename(columns ={'index':'ImageId'},inplace = True )\ndf['ImageId'] = df['ImageId'] +1\n\ndf.to_csv('submission.csv', index=False)\n\nsub_stauts = os.path.exists (\"./submission.csv\")\n\nprint(f\"\\nSubmission check: {sub_stauts}\")","metadata":{"execution":{"iopub.status.busy":"2022-02-23T20:12:13.979621Z","iopub.execute_input":"2022-02-23T20:12:13.979888Z","iopub.status.idle":"2022-02-23T20:12:14.607799Z","shell.execute_reply.started":"2022-02-23T20:12:13.979852Z","shell.execute_reply":"2022-02-23T20:12:14.607011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}