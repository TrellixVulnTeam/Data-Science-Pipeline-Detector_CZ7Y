{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install timm","metadata":{"execution":{"iopub.status.busy":"2022-02-24T10:35:32.144094Z","iopub.execute_input":"2022-02-24T10:35:32.14444Z","iopub.status.idle":"2022-02-24T10:35:41.418038Z","shell.execute_reply.started":"2022-02-24T10:35:32.144358Z","shell.execute_reply":"2022-02-24T10:35:41.414851Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nfrom matplotlib import cm\n\nfrom torch.utils.data import Dataset,ConcatDataset,DataLoader\nfrom torch import nn, optim\nimport torch \nimport copy\nfrom sklearn.model_selection import train_test_split\n\nfrom torchvision import transforms\nimport timm \nimport timm.optim\nfrom timm.data.transforms_factory import create_transform\nfrom torch.optim import lr_scheduler\nimport os\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nsns.set_style(\"darkgrid\", {\"grid.color\": \".6\", \"grid.linestyle\": \":\"})","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-02-24T10:35:41.420273Z","iopub.execute_input":"2022-02-24T10:35:41.420597Z","iopub.status.idle":"2022-02-24T10:35:44.742989Z","shell.execute_reply.started":"2022-02-24T10:35:41.420553Z","shell.execute_reply":"2022-02-24T10:35:44.742157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# CNN with Pytorch and Transfer Learning (timm)\n\nThe main objective of this notebook is to use the pre-trained model with the library timm.Therefore, learn the basics steps of this library.\nTo achieve this goal, the following steps are implemented (some steps were better explained another previous notebook) :\n- Loading Data: Prepare the training and validation datasets\n- **Update: Added Dataset class and augmentation**\n- **Model: Call a pretrained model with timm** \n- Run the Model: Execute and validate the model\n- Submission: Use the trained model to predict the test set","metadata":{}},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(device)","metadata":{"execution":{"iopub.status.busy":"2022-02-24T10:35:44.744675Z","iopub.execute_input":"2022-02-24T10:35:44.74493Z","iopub.status.idle":"2022-02-24T10:35:44.791042Z","shell.execute_reply.started":"2022-02-24T10:35:44.744894Z","shell.execute_reply":"2022-02-24T10:35:44.78844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class confiFile():\n    \"\"\"Configuration class for easy parametrization\"\"\"\n    \n    #Pretrained model with timm\n    model = 'resnet50'\n    epochs = 30\n    \n    in_chans = 1\n    num_classes = 10\n    learning_rate = 1e-3\n    \n    val_size = 0.3\n    batch_size = 128\n    \nCFG = confiFile()","metadata":{"execution":{"iopub.status.busy":"2022-02-24T10:35:44.79396Z","iopub.execute_input":"2022-02-24T10:35:44.794926Z","iopub.status.idle":"2022-02-24T10:35:44.800664Z","shell.execute_reply.started":"2022-02-24T10:35:44.794885Z","shell.execute_reply":"2022-02-24T10:35:44.799903Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading Data ","metadata":{}},{"cell_type":"markdown","source":"# Dataset Class","metadata":{}},{"cell_type":"code","source":"class MNISTdataset(Dataset):\n    \"\"\"V2\"\"\"\n    def __init__(self, data, transform = None):\n        \n        self.X = (data.drop('label', axis =1).values/255).reshape((-1, 28, 28,1))\n        self.y = data['label'].values.reshape(-1,1)\n        self.n_samples = data.shape[0]\n        \n        self.transform = transform \n    \n    def __getitem__(self, index):\n        X,y = self.X[index], self.y[index]\n        \n        if self.transform:\n            X  = self.transform(X)\n            \n        return (X,y)\n        \n    def __len__(self):\n        return self.n_samples\n\n\n    \n\n#Train and Validation sets    \ntrain_df = pd.read_csv('../input/digit-recognizer/train.csv',dtype=np.float32)\n#Apply a list of multiple transforms\ncompose_agu_1 = transforms.Compose([transforms.ToTensor(), transforms.RandomRotation(degrees=(300,350), fill=(0,))])\ncompose_agu_2 = transforms.Compose([transforms.ToTensor(), transforms.RandomRotation(degrees=(0,45), fill=(0,))])\ncompose_original = transforms.Compose([transforms.ToTensor()])\n\ndataset_original = MNISTdataset(train_df,transform = compose_original)\ndataset_aug_1 = MNISTdataset(train_df,transform = compose_agu_1)\ndataset_aug_2 = MNISTdataset(train_df,transform = compose_agu_2)\nincreased_dataset = ConcatDataset([dataset_aug_1,dataset_aug_2,dataset_original])\n\n\n#Spliting Data\nX_train, X_val, = train_test_split(increased_dataset, test_size = CFG.val_size, \\\n                                                  random_state = 666)\n\n# Creating Datasets Loaders\ntrain_loader = DataLoader(X_train, batch_size  = CFG.batch_size,  shuffle = True)\nval_loader = DataLoader(X_val, batch_size  = CFG.batch_size,  shuffle = True)","metadata":{"execution":{"iopub.status.busy":"2022-02-24T11:09:32.38809Z","iopub.execute_input":"2022-02-24T11:09:32.388588Z","iopub.status.idle":"2022-02-24T11:10:18.807229Z","shell.execute_reply.started":"2022-02-24T11:09:32.388545Z","shell.execute_reply":"2022-02-24T11:10:18.806382Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Visualizing some examples in the training dataset X\nfig, ax = plt.subplots(nrows = 2, ncols = 5,  figsize = (15,8))\nfor i in range(5):\n    image, y = dataset_aug_1.__getitem__(i)\n    ax[0,i].imshow(image.squeeze(), cmap=\"magma\") # Squeeze removes length (1,28,28) -> (28,28)\n    ax[0,i].set_title(int(y))\n    \n    image, y = dataset_original.__getitem__(i)\n    ax[1,i].imshow(image.squeeze(), cmap=\"viridis\") # Squeeze removes length (1,28,28) -> (28,28)\n    ax[1,i].set_title(int(y))\n\nax[0,0].set_ylabel('Augmented Images',fontdict= {'fontsize':16})\nax[1,0].set_ylabel('Original Images', fontdict= {'fontsize':16})","metadata":{"execution":{"iopub.status.busy":"2022-02-24T11:10:32.959616Z","iopub.execute_input":"2022-02-24T11:10:32.959962Z","iopub.status.idle":"2022-02-24T11:10:34.843393Z","shell.execute_reply.started":"2022-02-24T11:10:32.959929Z","shell.execute_reply":"2022-02-24T11:10:34.842385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model\nImporting a pretrained model in timm","metadata":{}},{"cell_type":"code","source":"#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n# Training Function\n#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\ndef train_(model, optmizer, loss_func, train_loader, device):\n    \"\"\"Function to train the model\"\"\"\n    train_loss = 0.0\n    train_correct = 0\n    size_sampler = len(train_loader.sampler)\n    \n    for i, (images, labels) in enumerate(train_loader):\n        \n        # Pushing to device (cuda or CPU)\n        images, labels = images.to(device), labels.to(device)\n        \n        #zeroing gradiants\n        optmizer.zero_grad()\n        \n        #feedfoard\n        y_hat = model(images)\n        \n        #Compute loss \n        loss = loss_func(y_hat, labels.long().squeeze())\n        \n        #Compute backpropagation\n        loss.backward()\n        \n        #updating weights\n        optmizer.step()\n        \n        # loss and correct values compute\n        train_loss +=loss.item() * images.size(0)\n        _ , pred = torch.max(y_hat.data, 1)\n        train_correct +=sum(pred == labels.long().squeeze()).sum().item()\n        \n    return np.round(train_loss/size_sampler,4), np.round(train_correct*100./size_sampler,3)\n\n#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n# Validation Function\n#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\ndef validation_(model, loss_func,val_loader, device):\n    val_loss = 0.0\n    val_correct = 0\n    size_sampler = len(val_loader.sampler)\n    with torch.no_grad():\n        for i, (images, labels) in enumerate(val_loader):\n\n            images, labels = images.to(device), labels.to(device)\n            y_hat = model(images)\n            loss = loss_func(y_hat, labels.long().squeeze())\n            \n            val_loss +=loss.item() * images.size(0)\n            _ , pred = torch.max(y_hat.data, 1)\n            val_correct +=(pred == labels.long().squeeze()).sum().item()\n        \n    return np.round(val_loss/size_sampler,4), np.round(val_correct*100./size_sampler,3)\n\n#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n# Main Training\n#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n\ndef train_model(model,optmizer, loss_func,scheduler, train_loader, val_loader, epochs, device, log = True):\n    \n    best_acc = 0\n    \n    \n    print('Initializing Training...')\n    \n    history = {'train_loss':[], 'val_loss':[], 'train_acc':[], 'val_acc':[]}\n    \n    for i in range(epochs):\n        \n        train_loss, train_acc=  train_(model, optmizer, loss_func, train_loader,device)\n        val_loss, val_acc = validation_(model, loss_func,val_loader, device)\n        \n        scheduler.step()\n        \n        if val_acc > best_acc:\n            print(f'>> Saving Best Model with Val Acc: Old: {best_acc} | New: {val_acc}')\n            best_model = copy.deepcopy(model)\n            best_acc = val_acc\n        \n        \n        if log and ((i+1)%2 == 0):\n            print(f'> Epochs: {i+1}/{epochs} - Train Loss: {train_loss} - Train Acc: {train_acc} - Val Loss: {val_loss} - Val Acc: {val_acc}')\n        \n        #Saving infos on a history dict\n        for key, value in zip(history, [train_loss,val_loss,train_acc,val_acc]):\n            history[key].append(value)\n    \n    \n    \n    print('...End Traing')\n            \n    return history,best_model\n\n#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n# Ploting graphics\n#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n\ndef plot_history(history):\n    \n    #Ploting the Loss and Accuracy Curves\n    fig, ax = plt.subplots(nrows = 1, ncols = 2, figsize = (16,6))\n    \n    #Loss\n    sns.lineplot(data = history['train_loss'], label = 'Training Loss', ax = ax[0])\n    sns.lineplot(data = history['val_loss'], label = 'Validation Loss', ax = ax[0])\n    ax[0].legend(loc = 'upper right')\n    ax[0].set_title('Loss')\n    #Accuracy\n    sns.lineplot(data = history['train_acc'], label = 'Training Accuracy', ax = ax[1])\n    sns.lineplot(data = history['val_acc'], label = 'Validation Accuracy', ax = ax[1])\n    ax[1].legend(loc = 'lower right')\n    ax[1].set_title('Accuracy')\n        ","metadata":{"execution":{"iopub.status.busy":"2022-02-24T10:36:08.753879Z","iopub.execute_input":"2022-02-24T10:36:08.754262Z","iopub.status.idle":"2022-02-24T10:36:08.776022Z","shell.execute_reply.started":"2022-02-24T10:36:08.754228Z","shell.execute_reply":"2022-02-24T10:36:08.775415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n# Execute training\n#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n\nmodel = timm.create_model(CFG.model, \n                          pretrained = True,\n                          in_chans = CFG.in_chans, \n                          num_classes = CFG.num_classes)\n\nmodel = model.to(device)\n\n# # Same as using the model.fc (for resnet only) but easier as it can change in other models\n# print(model.get_classifier())\n\n# #checking the global pooling from timm\n# print(model.global_pool)\n\n\noptmizer = optim.Adam(model.parameters(), lr = CFG.learning_rate)\nloss_func =  nn.CrossEntropyLoss()\nscheduler = lr_scheduler.StepLR(optmizer, step_size=5, gamma=0.1)\n\nhistory,best_model = train_model(model = model, \n                    optmizer = optmizer, \n                    loss_func = loss_func,\n                    scheduler = scheduler,\n                    train_loader = train_loader,\n                    val_loader = val_loader,\n                    epochs = CFG.epochs,\n                    device = device)\n\n#ploting results\nplot_history(history)","metadata":{"execution":{"iopub.status.busy":"2022-02-24T10:36:08.777157Z","iopub.execute_input":"2022-02-24T10:36:08.777389Z","iopub.status.idle":"2022-02-24T10:50:48.958939Z","shell.execute_reply.started":"2022-02-24T10:36:08.777356Z","shell.execute_reply":"2022-02-24T10:50:48.958268Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Testing Data","metadata":{}},{"cell_type":"code","source":"class MNISTdataset_inference(Dataset):\n    \"\"\"Inference\"\"\"\n    def __init__(self, data, transform = None):\n        self.X = (data.values/255).reshape((-1, 28, 28,1))\n        self.n_samples = data.shape[0]\n        \n        self.transform = transform \n    \n    def __getitem__(self, index):\n        X = self.X[index]\n        \n        if self.transform:\n            X  = self.transform(X)\n            \n        return X\n        \n    def __len__(self):\n        return self.n_samples    \n\n\n#Test Sets\ntest_df = pd.read_csv('../input/digit-recognizer/test.csv',dtype=np.float32)\ntest_dataset = MNISTdataset_inference(test_df, transform = compose_original)\ntest_loader = DataLoader(test_dataset, batch_size  = len(test_dataset))","metadata":{"execution":{"iopub.status.busy":"2022-02-24T10:50:48.960382Z","iopub.execute_input":"2022-02-24T10:50:48.961206Z","iopub.status.idle":"2022-02-24T10:50:51.350629Z","shell.execute_reply.started":"2022-02-24T10:50:48.961165Z","shell.execute_reply":"2022-02-24T10:50:51.349869Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred_list = []\nwith torch.no_grad():\n    for X_test_ld in (test_loader):\n        y_pred = best_model(X_test_ld.to(device))\n        _, y_pred_tags = torch.max(y_pred.data, dim = 1)\n#         y_pred_list.append((y_pred_tags.to('cpu')))\n\nresult = [int(i) for i in y_pred_tags.to('cpu')]\ndf = pd.Series(result,name = 'Label').reset_index()\ndf.rename(columns ={'index':'ImageId'},inplace = True )\ndf['ImageId'] = df['ImageId'] +1\n\ndf.to_csv('submission.csv', index=False)\n\nsub_stauts = os.path.exists (\"./submission.csv\")\n\nprint(f\"\\nSubmission check: {sub_stauts}\")","metadata":{"execution":{"iopub.status.busy":"2022-02-24T10:50:51.353018Z","iopub.execute_input":"2022-02-24T10:50:51.353313Z","iopub.status.idle":"2022-02-24T10:50:54.966136Z","shell.execute_reply.started":"2022-02-24T10:50:51.353273Z","shell.execute_reply":"2022-02-24T10:50:54.964652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Image and Predicted Labels from Test","metadata":{}},{"cell_type":"code","source":"#Visualizing Image Results\nfig, ax = plt.subplots(nrows = 3, ncols = 5,  figsize = (15,8))\npic = 0\nfor r in range(3):\n    for i in range(5):\n        image = test_dataset.__getitem__(pic)\n        y = result[pic]\n        ax[r,i].imshow(image.squeeze(), cmap=\"plasma\") # Squeeze removes length (1,28,28) -> (28,28)\n        ax[r, i].set_title(int(y), fontdict= {'fontsize':14})\n        pic+=1\n\nfig.suptitle('Test Images and Predicted Labels', size = 18)\nplt.tight_layout()","metadata":{"execution":{"iopub.status.busy":"2022-02-24T11:00:27.560597Z","iopub.execute_input":"2022-02-24T11:00:27.561402Z","iopub.status.idle":"2022-02-24T11:00:30.967869Z","shell.execute_reply.started":"2022-02-24T11:00:27.561362Z","shell.execute_reply":"2022-02-24T11:00:30.966995Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}