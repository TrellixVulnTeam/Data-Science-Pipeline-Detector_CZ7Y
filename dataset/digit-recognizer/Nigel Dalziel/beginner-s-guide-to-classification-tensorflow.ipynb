{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"file_extension":".py","mimetype":"text/x-python","pygments_lexer":"ipython3","name":"python","codemirror_mode":{"name":"ipython","version":3},"version":"3.6.3","nbconvert_exporter":"python"}},"nbformat_minor":1,"nbformat":4,"cells":[{"metadata":{"_cell_guid":"f8d12425-a428-431f-b191-d0c470158db1","_uuid":"650d4c4af19573b348217e019bc18d3ae3ee4feb"},"source":"If you're looking for the simplest way to get started with image classification, Keras, provides an interface to TensorFlow that allows you to create a neural network with only a few lines of code. (This kernel will help you get started: [Beginner's Guide to Image Classification (Keras)](https://www.kaggle.com/ndalziel/beginner-s-guide-to-image-classification-keras)). However, if you want the greater flexibility that you get from using TensorFlow directly, please read on....\n\nTo keep things simple we'll use a 4-layer perceptron (or feed-forward) architecture. However, since the success of the [Hinton team in the 2012 ImageNet competition](http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf), deep Convolutional Neural Network (CNN) architectures are typically used to achieve the best results on image classification. \n\nThis notebook is split into five sections:\n1. Intro to TensorFlow\n2. Preprocessing the data\n3. Setting up the network\n4. Training the network, and\n5. Making predictions","cell_type":"markdown"},{"metadata":{"_cell_guid":"fe2a929c-e9c5-4093-87f5-c40fbc75c2d8","_uuid":"109a8a774e27ff1c27470f932470920314ccb96f"},"source":"## 1. Introduction to TensorFlow\n\n### What is a tensor? \nTensorFlow is a framework to run computations on tensors, which are  n-dimensional matrices. Although you can convert various Python objects to Tensor objects (using the tf.convert_to_tensor function), we'll see that you can pass in NumPy arrays of data.\n\n### How do you use TensorFlow?\n\nTensorFlow separates the definition of computation from the execution of the computation. In practice, this means that you need to:\n\n1. Create a computational [graph](https://upload.wikimedia.org/wikipedia/commons/thumb/4/4b/Directed_acyclic_graph.svg/1280px-Directed_acyclic_graph.svg.png)\n\n2. Run a session to execute the operations in the graph\n\nMore efficient computation is a major benefit of this approach. The graph structure facilitates computation across multiple CPUs or GPUs by allowing you to direct the computation of different parts of the graph to specific CPUs / GPUs. \n\nLet's take a look at a simple computation example...","cell_type":"markdown"},{"metadata":{"_cell_guid":"c3da15b5-1fac-48ad-b40b-8fcdce47a4f7","collapsed":true,"_uuid":"2d2c31ed2642fd0a9fc0fbaf63969ad4bf63f329"},"source":"import tensorflow as tf\n# STEP 1 - Create a computational graph\nop1 = tf.add(3,4)\nop2 = tf.multiply(op1,5)\n\n#STEP 2 - Run a session to execute the operations in the graph\nwith tf.Session() as sess:\n    print (sess.run(op2))","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"b4a15585-3a54-4d3b-adb3-9b8e911d0ffc","_uuid":"8c31db29b22235eda8664c63ee9bd00d79d451af"},"source":"In the example above, the constants have been stored in the definition of the graph. If we want to define the graph in more general terms and pass in variables, we need to create placeholders...","cell_type":"markdown"},{"metadata":{"_cell_guid":"c4f5f367-0743-47f2-82c3-ff5ebbc5b663","collapsed":true,"_uuid":"076e7612bdc6bb70a9029a0f6b429ffe22800b05"},"source":"# STEP 0 - Create placeholders\nX = tf.placeholder(tf.int32,name = \"X\")\nY = tf.placeholder(tf.int32,name = \"Y\")\nZ = tf.placeholder(tf.int32,name = \"Z\")\n\n# STEP 1 - Create a computational graph\nop1 = tf.add(X,Y)\nop2 = tf.multiply(op1,Z)\n\n#STEP 2 - Run a session to execute the operations in the graph\nvar1=3; var2 = 4; var3 = 5\nwith tf.Session() as sess:\n    x = sess.run(op2, feed_dict={X: var1, Y: var2, Z: var3}) \n    print(x)","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"4e0062f4-23be-4028-8862-04f58bbc0392","_uuid":"e610c75d658bd974d9b35962bfcc592e934fde2f"},"source":"Now that we understand how to do a simple operation in TensorFlow, let's turn to image classification...","cell_type":"markdown"},{"metadata":{"_cell_guid":"4209ea2e-41fa-4ef0-bce0-78679eee94dd","_uuid":"9af9e71f411ca94b96e80e3aecf632da81581088"},"source":"## 2. Preprocessing the data","cell_type":"markdown"},{"metadata":{"_cell_guid":"1171effd-a105-4cbf-8b56-08b48ed3bb8c","collapsed":true,"_uuid":"471339fb7cd19800e47a0eb9d7d8ded2863f369d"},"source":"import math\nimport numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow.python.framework import ops","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"5aaaec85-c8ef-4c3e-be5b-cf30ca522657","_uuid":"73d8a64be0289ddc518d225d20577341a28effec"},"source":"Let's begin by reading in the training data, and taking a look at some sample images. Note that the data has already been partially pre-processed - the 28x28 data matrix for each image has been flattened into a 784-column vector. So, to display the images, we need to reshape into a 28x28 matrix. ","cell_type":"markdown"},{"metadata":{"_cell_guid":"41154bea-24df-4802-a2de-bb1a0acb9e00","collapsed":true,"_uuid":"ac4c544eb0bb6b5d9d18fdadd8c8f29f37c7eeb3"},"source":"traindev = pd.read_csv('../input/train.csv')\nX_traindev = traindev.loc[:,'pixel0':'pixel783']\nY_traindev = traindev.loc[:,'label']\n\nfor n in range(1,10):\n    plt.subplot(1,10,n)\n    plt.imshow(X_traindev.iloc[n].values.reshape((28,28)),cmap='gray')\n    plt.title(Y_traindev.iloc[n])","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"7902240e-fb7e-485d-9143-90e08cd2de41","_uuid":"8f5a794324375eb12217a2d5b175754f29748cb9"},"source":"Next, we'll split the data into a training set and a cross-validation set. We'll use dummy variables to encode the labels - resulting in a 10-column label matrix (one for each digit). Note that we'll stack the training examples and labels in columns (following the convention in [Andrew Ng's Deep Learning course](https://www.coursera.org/learn/neural-networks-deep-learning)) by taking the transpose of X and Y - this will make the implementation easier later on..","cell_type":"markdown"},{"metadata":{"_cell_guid":"4591c86f-7d7a-4bc8-bafb-2919bea64452","collapsed":true,"_uuid":"011249dfb773736121fb0bcbedac1e40137a4c83"},"source":"# Create training data set\nX_train = X_traindev[:40000].T.values\nY_train = Y_traindev[:40000]\nY_train = pd.get_dummies(Y_train).T.values\n\n# Create cross-validation set\nX_dev = X_traindev[40000:42000].T.values\nY_dev = Y_traindev[40000:42000]\nY_dev = pd.get_dummies(Y_dev).T.values\n\n# read in test set\nX_test = pd.read_csv('../input/test.csv').T.values\n\nprint (\"number of training examples = \" + str(X_train.shape[1]))\nprint (\"number of cross-validation examples = \" + str(X_dev.shape[1]))\nprint (\"X_train shape: \" + str(X_train.shape))\nprint (\"Y_train shape: \" + str(Y_train.shape))\nprint (\"X_dev shape: \" + str(X_dev.shape))\nprint (\"Y_dev shape: \" + str(Y_dev.shape))\nprint (\"X_test shape: \" + str(X_test.shape))","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"9a347ba4-4953-4f0f-bfc2-9f299435fb42","_uuid":"1ec558a1960d029e971cc519bc13961c882a3a64"},"source":"## 3. Setting up the network\nWe need to start off by initializing variables, creating placeholders and implementing the computation graph. At thre core of the function below are the **feed-foward equations**:\n1.  Z(i) = W(i)*A(i-1) + b(i)\n2. A(i) = activation_function * Z(i)\n\nWe use the rectified linear unit (relu) function as the activation function for the hidden layers. We need 784 units (28*28) as the number of units for the input layer, and we need 10 units for the output layer to correspond to the number of digits. We've chosen to add a 2nd and 3rd layer with 32 and 16 units respectively.","cell_type":"markdown"},{"metadata":{"_cell_guid":"ffb38681-a881-4bb9-a449-f582a8a6cc78","collapsed":true,"_uuid":"9447c2feca9ba919db197529359df6ca73a69982"},"source":"def create_graph(X_train,Y_train):\n    #setup\n    ops.reset_default_graph()                         # reset computation graph\n\n    # initialize variables\n    (n_x, training_examples) = X_train.shape                          \n    n_y = Y_train.shape[0]                            \n    costs = []\n\n    # create placeholders\n    X = tf.placeholder(tf.float32, shape=(n_x, None),name = \"X\")\n    Y = tf.placeholder(tf.float32, shape=(n_y, None),name = \"Y\")\n    \n    # initialize weights\n    W1 = tf.get_variable(\"W1\", [32,784], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n    W2 = tf.get_variable(\"W2\", [16,32], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n    W3 = tf.get_variable(\"W3\", [10,16], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n    \n    # initialize biases\n    b1 = tf.get_variable(\"b1\", [32,1], initializer = tf.zeros_initializer())\n    b2 = tf.get_variable(\"b2\", [16,1], initializer = tf.zeros_initializer())\n    b3 = tf.get_variable(\"b3\", [10,1], initializer = tf.zeros_initializer())\n\n    # create the graph for forward propagation\n    Z1 = tf.add(tf.matmul(W1,X),b1)                                             \n    A1 = tf.nn.relu(Z1)                                                         \n    Z2 = tf.add(tf.matmul(W2,A1),b2)                                            \n    A2 = tf.nn.relu(Z2)                                                         \n    Z3 = tf.add(tf.matmul(W3,A2),b3)\n    return X, Y, Z3, training_examples","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"0e2bc76c-4a8c-41e0-8cba-8133ad7238cb","_uuid":"e81f2996c54c925c10339cd1124e15c38e0708eb"},"source":"Next, we'll specifiy how the model is optimized by choosing the **optimization algorithm** and the **cost (or loss) function.** The Adam optimization algorithm works well across a wide range of neural network architectures. (Adam essentially combined two other successful algorithms - gradient descent with momentum, and RMSProp.) For the loss function, 'softmax_cross_entropy_with_logits' is a good choice for multi-class classification. ","cell_type":"markdown"},{"metadata":{"_cell_guid":"d1f4febc-142c-433e-ab53-31e0f44dde3d","collapsed":true,"_uuid":"d7dbcd53af040899d008fa41302d6c3747c2f803"},"source":"def define_optimization(Z3,Y):\n    logits = tf.transpose(Z3)\n    labels = tf.transpose(Y)\n    cost = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels) )\n    optimizer = tf.train.AdamOptimizer(learning_rate = 0.001).minimize(cost)\n    \n    return optimizer, cost","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"84a0f1eb-81a4-4905-a131-752879edfd50","_uuid":"e610f26a1572038c65cdb25b8334b86b5b280d70"},"source":"Before we train the network, we need a function that creates randomized batches of training data so that we can implement mini-batch optimization (which will lead to faster optimization convergence)...","cell_type":"markdown"},{"metadata":{"_cell_guid":"7a241c9c-75cf-4af9-8f62-59ae584fe8a2","collapsed":true,"_uuid":"b18d39144ae673bf50fa76c15499602e2e88bcc0"},"source":"def random_mini_batches(X, Y, mini_batch_size = 64, seed = 0):\n    \n    m = X.shape[1]                  # number of training examples\n    mini_batches = []\n    np.random.seed(seed)\n    \n    # Step 1: Shuffle (X, Y)\n    permutation = list(np.random.permutation(m))\n    shuffled_X = X[:, permutation]\n    shuffled_Y = Y[:, permutation].reshape((Y.shape[0],m))\n\n    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n    num_complete_minibatches = math.floor(m/mini_batch_size) \n    for k in range(0, num_complete_minibatches):\n        mini_batch_X = shuffled_X[:, k * mini_batch_size : k * mini_batch_size + mini_batch_size]\n        mini_batch_Y = shuffled_Y[:, k * mini_batch_size : k * mini_batch_size + mini_batch_size]\n        mini_batch = (mini_batch_X, mini_batch_Y)\n        mini_batches.append(mini_batch)\n    \n    # Handling the end case (last mini-batch < mini_batch_size)\n    if m % mini_batch_size != 0:\n        mini_batch_X = shuffled_X[:, num_complete_minibatches * mini_batch_size : m]\n        mini_batch_Y = shuffled_Y[:, num_complete_minibatches * mini_batch_size : m]\n        mini_batch = (mini_batch_X, mini_batch_Y)\n        mini_batches.append(mini_batch)\n    \n    return mini_batches","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"26ccf5c7-33b1-4d2a-96c2-ac4f1b817a33","_uuid":"ffaa92e99a0f6f180dbc8951a2ed81736f6ded7a"},"source":"## 4. Training the network\nNow, we'll open the TensorFlow session and execute the computation graph. Note that 1 epoch (or iteration) corresponds to a complete pass through the training set. For each epoch, we execute forward-prop and back-prop on all of the mini-batches. We'll print out the train and dev (or corss-validation) accuracy for each epoch, so can diagnose how well the neural network is performing...","cell_type":"markdown"},{"metadata":{"_cell_guid":"ab6a615a-257f-4502-a9d0-427bd8b4c9df","collapsed":true,"_uuid":"c9b99442dac2d854d7a327298c2f015d00773ace"},"source":"def train_network(X_train,Y_train,X_dev, Y_dev, X_test, num_epochs,minibatch_size=64,print_n_epochs=1):\n    \n    tf.set_random_seed(1)                             \n    X,Y,Z_final,training_examples = create_graph(X_train,Y_train)\n    optimizer, cost = define_optimization(Z_final,Y)\n    init = tf.global_variables_initializer() # set up variable initialization\n    \n    with tf.Session() as sess:\n        sess.run(init) # initializes all the variables we've created\n        for epoch in range(num_epochs):\n\n            epoch_cost = 0.                       \n            num_minibatches = int(training_examples / minibatch_size) \n            minibatches = random_mini_batches(X_train, Y_train)\n\n            for minibatch in minibatches:\n                (minibatch_X, minibatch_Y) = minibatch\n                _ , minibatch_cost = sess.run([optimizer, cost], \n                                              feed_dict={X: minibatch_X, Y: minibatch_Y})                \n                epoch_cost += minibatch_cost / num_minibatches\n            \n            print (\"Cost after epoch %i: %.3f\" % (epoch+1, epoch_cost), end = \"\") \n            correct_prediction = tf.equal(tf.argmax(Z_final), tf.argmax(Y))\n            accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n            print (\"     Train Accuracy: %.3f\" % (accuracy.eval({X: X_train, Y: Y_train})), end = \"\")\n            print (\"     Dev Accuracy: %.3f\" % (accuracy.eval({X: X_dev, Y: Y_dev})))\n        \n        print (\"Network has been trained\")\n        predict = tf.argmax(Z_final).eval({X: X_test})\n        probs = tf.nn.softmax(Z_final).eval({X: X_test})\n        \n        return predict, probs\n    \nY_predict, Y_probs = train_network(X_train, Y_train, X_dev, Y_dev, X_test, num_epochs = 20)","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"2f48d9d4-af65-47e0-bd6b-a6a7773a95b5","_uuid":"9a03ed3ed29dae0c30b3be42b842426edaaa1704"},"source":"## 5. Making Predictions\nThe function above returns the predictions, so we just need to create the submission file. Note that, although we're not using it here, the function above also uses the softmax function to return the probabilities associated with each digit.","cell_type":"markdown"},{"metadata":{"_cell_guid":"fa36b1d1-4fbe-4758-aaad-03a61351231b","collapsed":true,"_uuid":"42d6c59a1ea3cff1972f634e2abf9f832434b8d5"},"source":"Y_predict = Y_predict.reshape(-1,1)\npredictions_df = pd.DataFrame (Y_predict,columns = ['Label'])\npredictions_df['ImageID'] = predictions_df.index + 1\nsubmission_df = predictions_df[predictions_df.columns[::-1]]\nsubmission_df.to_csv(\"submission.csv\", index=False, header=True)\nsubmission_df.head()","execution_count":null,"cell_type":"code","outputs":[]}]}