{"cells":[{"metadata":{},"cell_type":"markdown","source":"# **Basics of Image Denoising using Autoencoders**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Hi :) \nThis is my first notebook that I prepared to share and help others learn.\n\nI have recently done some courses and I am putting my knowledge to practical use !!!\n\nOpen to suggestions !!!\nHope you like it.\n\nPlease note that I have focused on denoising and not really on analysis of results etc. using confusion matrix and other techniques\n\nThis notebook is not aimed at getting the best results, more fine tuning of the networks will definitely improve the results :)\n\nThis notebook has been coded using TensorFlow 2.0 and Python 3","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Importing Libraries**","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import seaborn as sns #data visualization\n\nfrom sklearn.model_selection import train_test_split #data preprocessing to divide our data\n\nimport matplotlib.pyplot as plt #to plot the images of numbers\nfrom tensorflow.keras.models import Sequential, Model #for building neural network\nfrom tensorflow.keras.layers import Dense, Input #for layers of neural network\nfrom tensorflow.keras.utils import to_categorical \n#plot in the notebook itself\n%matplotlib inline \nsns.set(style = 'white', context = 'notebook', palette = 'deep')\nnp.random.seed(42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Loading Data into Train and Test**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#using pandas.read_csv we read the csv data into a pandas dataframe\n\ntrain = pd.read_csv(\"/kaggle/input/digit-recognizer/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/digit-recognizer/test.csv\")\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Some data analysis","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#train.head() shows us the first five columns of a dataframe\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. From above we observe that data is stored such a way that a row represents one image and the pixels of the image are stored from pixel0 to pixel783. Which means that there are 784 pixels i.e image size is of 28x28 pixels. \n2. The label column represents the class of the image if it is a one, two, three ... or so on. We have a total of 10 classes (0-9)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Understanding how to process data for our aim**\n1. Our aim is to predict the class of an image, therefore we are going to store the labels column from the datafram in y_train.\n2. To store x_train we will first drop the \"label\" column from the train dataframe and then proceed cast the dataframe to a new variable x_train\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\ny_train = train[\"label\"]\nx_train = train.drop(labels = [\"label\"], axis = 1)\n\n\n#sns.countplot will count the number of unique values in y_train and give us the distribution in the form of a graph\ngraph = sns.countplot(y_train)\n\ny_train.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Preprocessing","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(type(x_train))\nprint(type(y_train))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that x_train is in the form of a pandas Dataframe and y_train in the form of pandas series, We are going to convert it into numpy to introduce noise in it later.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train = x_train.to_numpy()\ny_train = y_train.to_numpy()\n\nprint(type(x_train))\nprint(type(y_train))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Data Normalization \nx_train = x_train.astype('float')/255.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Splitting our data into training and testing\nx_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size = 0.2, random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#Number of the samples we have\nx_train_size = len(x_train)\nx_val_size = len(x_val)\n\nprint(x_train_size)\nprint(x_val_size)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Adding Noise","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* We are going to artifically add noise to our data.\n* To do that we simpy add random data to our existing data.\n* Recall that we had earlier normalid out data to be  between 0 and 1 by diving it by 255. Therefore the noise added should also be between 0 and 1. To do that we simply multiply the random noise generated by 0.9\n* After adding the noise we clip the data to be between 0 and 1 again.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\nx_train_noisy = x_train + np.random.rand(x_train_size, 784) * 0.9\nx_val_noisy = x_val + np.random.rand(x_val_size, 784) * 0.9\n\n#clipping the noise under 0 and 1 incase something went above\nx_train_noisy = np.clip(x_train_noisy, 0., 1.)\nx_val_noisy = np.clip(x_val_noisy, 0., 1.)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Defining a function to plot the images of numbers**\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot(x, p , labels = False):\n    plt.figure(figsize = (20,2))\n    for i in range(10):\n        plt.subplot(1, 10, i+1)\n        plt.imshow(x[i].reshape(28,28), cmap = 'binary') #reshaping the to conver 784 to 28x28 for an image plotting\n        plt.xticks([])\n        plt.yticks([])\n        if labels:\n            plt.xlabel(np.argmax(p[i]))\n    plt.show()\n    return\nplot(x_train, None)\n\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"See the how the noise has been introduced to our dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plot(x_train_noisy, None)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Building and Training a Classifier","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Constructing a very simple neural network to classify our images.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"classifier = Sequential([\n    Dense(512, activation = 'relu', input_shape = (784, )),\n    Dense(512, activation = 'relu'),\n    Dense(256, activation = 'relu'),\n    Dense(10, activation = 'softmax')\n])\n\n#using spare_categorical_crossentropy because labels have not been one-hot encoded\nclassifier.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics = 'accuracy')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"classifier.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Proceeding to train the classifier","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"classifier.fit(x_train, y_train, epochs = 10, batch_size = 512)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loss, acc = classifier.evaluate(x_val, y_val)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The classifier acheives good accuracy on images with no noise. But what about images with noise in them? The follwoing cell will tell","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"loss, acc = classifier.evaluate(x_val_noisy, y_val)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* As you can see the accuracy of the model decreased manifold because of noise. The model is confused about making predictions because noise has rendered it unusable. Real world data is like that, there is lot of noise in images and if we want to make neural networks and deploy them in making predictions, then they need to be robust to noise too. \n* This is where autoencoders come into play.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# What are Autoencoders?","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* Autoencoders are unsupervised neural netowrks which learn how to encode data and then reconstruct the data from reduced encodings to a representation as close to the original data.\n* The consist of and **encoder**, **bottleneck**,** decoder** and **reconstruction loss**.\n* As the name suggests, first the encoder learns to encode the data into a *compressed representation*.\n* The bottleneck is basically the smallest encoded representation of the data.\n* The decoder network learns how to *reconstruct the data* from the encoded network.\n* The reconstruction loss is a measure of how well the image is being reconstructed and how close the output is to the original input.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"![](https://www.jeremyjordan.me/content/images/2018/03/Screen-Shot-2018-03-09-at-10.20.44-AM.png)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* To learn more about autoencoders you can visit [this](http://ufldl.stanford.edu/tutorial/unsupervised/Autoencoders/)\n* I would suggest going through the above website before proceeding further to get a better understanding of the code. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Building an Autoencoder","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Constructing a very simple encoder and decoder network.\n* You can see that our bottleneck is the Dense layer with 64 units.\n* In the decoder network, the last layer gives us an output of 784 units using sigmoid activation. Sigmoid activation outputs either 1 or 0.\n* Basically this means that the network will decide either to keep the noisy pixel or not. The output of 784 units are the pixels for out denoised image","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"input_image = Input(shape = (784, ) )\n\nencoded = Dense(512, activation = 'relu')(input_image)\nencoded = Dense(512, activation = 'relu')(encoded)\nencoded = Dense(256, activation = 'relu')(encoded)\nencoded = Dense(256, activation = 'relu')(encoded)\nencoded = Dense(64,  activation = 'relu')(encoded)\n\ndecoded = Dense(512, activation = 'relu')(encoded)\ndecoded = Dense(784, activation = 'sigmoid')(decoded)\n\nautoencoder = Model(input_image, decoded)\nautoencoder.compile(loss= 'binary_crossentropy' , optimizer = 'adam')\nautoencoder.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training Autoencoder","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Lambda callback willl print the val_loss after each epoch\nfrom tensorflow.keras.callbacks import LambdaCallback\n\nautoencoder.fit(x_train_noisy, x_train,\n               epochs = 100, batch_size = 512,\n               validation_split = 0.2, verbose = False,\n               callbacks=[LambdaCallback(on_epoch_end=lambda e,l: print('{:.3f}'.format(l['val_loss']), end=' _ '))]\n               )\n\nprint(\"Training has finished !\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Denoised Images","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Now we will use the autoencoder to produce denoised images from noisy ones present in x_val","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = autoencoder.predict(x_val_noisy)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot(x_val, None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot(x_val_noisy, None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plotting the denoised images\nplot(preds, None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loss, acc = classifier.evaluate(preds, y_val)\nprint(f\"Loss : {loss} \\nAccuracy : {acc}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Earlier the accuracy was around 0.2 on images with noise.\nAs you can see our classifier performs well on denoised images. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Combined Model","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Seperate models for noise reduction and classification are not very practical, hence we are going to combine them into single unit using the Model class.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"noisy_image = Input(shape = (784, ))\nx = autoencoder(noisy_image)\ny = classifier(x)\n \n#combined model\ndenoise_and_classify = Model(noisy_image, y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Making predictions using the combined network","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"final_preds = denoise_and_classify.predict(x_val_noisy)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plotting predicted class along with the images","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plot(x_val_noisy, final_preds , True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plotting actual class with images","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plot(x_val_noisy, to_categorical(y_val), True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Making predinctions on our test data. First we convert it into a numpy array","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Please note that data in test does not contain noise, for submitting and showing that our model works on test data too, we are making predictions using the complete model we have built","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"test = test.to_numpy()\n\nplot(test, None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_preds = denoise_and_classify.predict(test)\n\nresults = np.argmax(test_preds, axis = 1)\n\nresults = pd.Series(results, name = 'Label')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#The test images don't have noise but just for fun lets see what the autoencoder does to them\ndenoise_test = autoencoder.predict(test)\nplot(denoise_test, None)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"You can observe that autoencoder has somehow tried to *enhance* the images.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plotting test predictions\nplot(test,test_preds, True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.concat([pd.Series(range(1,28001),name = \"ImageId\"),results],axis = 1)\n\nsubmission.to_csv(\"cnn_mnist_datagen.csv\",index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}