{"cells":[{"metadata":{},"cell_type":"markdown","source":"In this notebbok we will see how to reduce dimension on MNIST dataset with two well known technics :\n* PCA \n* T-SNE\n\nThen we'll use the two first components as standalone new features and compare the obtained performances among:\n* original data\n* original data + 2 first pca components\n* original + 2 first t-sne vectors\n\nThe models that wee'll be using :\n* RandomForestClassifier\n* KNeighborsClassifier\n* GaussianNB\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nimport plotly.express as px\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.neighbors import KNeighborsClassifier \nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB \nfrom sklearn.model_selection import cross_val_score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## DataViz"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load the data\ntrain = pd.read_csv(\"../input/digit-recognizer/train.csv\")\ntest = pd.read_csv(\"../input/digit-recognizer/test.csv\")\nprint(\"train shape\", train.shape)\nprint(\"test shape\", test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets take a look to the repartition of the classes"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = train.drop('label', axis=1)\ny_train = train[\"label\"]\n\n# free some space\ndel train\n\ng = sns.countplot(y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"index = 7\nprint(\"real value : \", y_train[index])\ndigit = X_train.loc[index,:]\n\n#reshape to a squared shape\nreshape_size = int(np.sqrt(len(digit)))\ndigit = digit.values.reshape(reshape_size,reshape_size)\n#imshow to plot raw digit \nplt.imshow(digit,cmap='binary')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### PCA : Plot a projection on the 2 first principal axis:\n\nThe main idea behind PCA is to find out projection vectors called principal axes so that a **maximum ratio of original variance is preserved**.\nThe corresponding vectors minimize the mean squared distance between the original dataset and its projection onto these axes.\n\nGenral approach:\n#### 1. Find out the principal axes : \nBy means of SVD-Singular Value Decomposition technique that lets to decompose X_train into three matrices $U Σ V^T$, where V contains all the principal components that we are looking for. These vectors constitue a new orthonormal basis that will be used as the projection basis \n\n#### 2. Project training data to the principal components basis\nOnce you have identified all the principal components (r axes), the dimensionality reducing is performed by projecting the orginal dataset onto the hyperplane defined by the first r principal components, that will preserve as much variance as possible.\nMathematically, it consists of appling a **dot product** between the training set matrix $X-{train}$ and the matrix $V_r$ , defined as the matrix containing the first r principal components (of the matrix V). \n\nPlease note that the algorithm assumes that dataset is normalized. (BTW The only family of algorithms that are scale-insensitive are tree-based methods : RF, XGB, LGB..) Even though scikit learn implementation integrates this normalisation step\n\nLet's Compute PCA components using `fit_transform()` method of the `sklearn.decomposition.PCA` model"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# first scaling:\n# Feature Scaling\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\npca = PCA(n_components=2)\npca_proj = pca.fit_transform(X_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now let's plot in  2D the two first principal components"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,8))\nplt.scatter(pca_proj[:, 0], pca_proj[:, 1], \n            c=y_train, cmap=\"Paired\")\nplt.colorbar()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To chose the right reducing dimension number, we will compute the number of components that preserve rather than 100% but for example 90% which is a reasonable proportion. (for data viz we select only the 2 or 3 first ones)\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pca_test = PCA()\npca_test.fit(X_train)\n\ndef get_right_dimension(pca, threshold=0.9):\n    cumsum = np.cumsum(pca.explained_variance_ratio_)\n    d = np.argmax(cumsum >= threshold) + 1\n    print(f'The {d} first components are sufficient to preserve {threshold * 100}% of the variance')\n    return d","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"d = get_right_dimension(pca_test)\nprint(f'Original data dimension : {X_train.shape[-1]}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Use Plotly:\nWe will use [Plotly](https://plotly.com/)'s python library to make more interactive and high-quality graphs.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# construct the dataframe corresponding to the new pca_proj\ndf = pd.DataFrame(pca_proj, columns=['axis_{}'.format(i+1) for i in range(pca.n_components)])\ndf['value'] = y_train\n# we have to transform the labels into str so that we can use it for the color fi \ndf[\"value\"] = df[\"value\"].astype(str)\nfig = px.scatter(df, x=\"axis_1\", y=\"axis_2\", color=\"value\", width=800, height=600)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now lets persist the computed projections for future use"},{"metadata":{"trusted":true},"cell_type":"code","source":"# save results\ndf.to_pickle('pca_embedding.pkl')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Use Tsne \nThe t-distributed stochastic neighbor embedding [(t-SNE)](http://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf) algorithm is a dimension reduction technique for data visualization developed by Geoﬀrey Hinton and Laurens van der Maaten.\nThe t-SNE algorithm is based on a probabilistic interpretation of proximities. A probability distribution is deﬁned on the pairs of points in the original space\nso that points close to each other have a high probability of being chosen while distant points have a low probability to be selected.  The t-SNE algorithm consists of matching the two probability densities, minimizing the[ Kullback-Leibler divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence) between the two distributions with respect to the\nlocation of the points on the map.\n\nThe t-SNE non-linear ”feature extraction” algorithm constructs a new representation of the data so that the close data in the original space has a high probability of having close representations in the new space. in the other hand, data that are distant in the original space, have a low probability of having close representations in the new space. \nIn practice, the similarity between each pair of data, in both spaces, is measured by means of probabilistic calculations based on distribution hypotheses. And the new representations are constructed in such a way as to minimize the diﬀerence between the probability distributions measured in the original space and those of the new space. \n\nIn code block below applies the T-SNE algorithm on our digits, in order to project them into a 2-dimensional space (which are the two axes of the image). \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#%%time\nif False:\n    tsn_proj = (TSNE(n_components=2)\n                .fit_transform(X_train)\n               )\n    tsn_proj = pd.read_pickle('../tsne_2_components_embedding.pkl')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# it takes a loot of time \n# I ve already saved it on the first run  \ntsn_proj = pd.read_pickle(\"../input/digitrecognizertsne/tsne_embedding.pkl\")\ntsn_proj.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,8))\nplt.scatter(tsn_proj.loc[:, 'axis_1'], tsn_proj.loc[:, 'axis_2'], c=y_train, cmap=\"Paired\")\nplt.colorbar()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The image in the center corresponds to the projection of each image in this new space, each digit being associated with\na diﬀerent color (in the fig the orange color represents the number ”1”, the red color the number ”0”). \n\nAs can be seen, the representation provided by the t-SNE algorithm makes it possible to separate and form distinct groups for each of the\ndigits of the data set.\n\nLets use plotly"},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndf = pd.DataFrame(tsn_proj, columns=['axis_1', 'axis_2'])\ndf['value'] = y_train\n\ndf[\"value\"] = df[\"value\"].astype(str)\nfig = px.scatter(df, x=\"axis_1\", y=\"axis_2\", color=\"value\", \n                  width=800, height=600,\n                 title='T-SNE vizualisation of MNIST Data')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# save data\n#df.to_pickle('tsne_2_components_embedding.pkl')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{},"cell_type":"markdown","source":"Lets evaluate different try different algorithms on:\n* original data\n* original data + 2 first PCA\n* original data + 3 first t-sne vectors\n\nfor each case we will test out:\n* RandomForestClassifier\n* KNeighborsClassifier\n* GaussianNB\n\nTo evaluate performance: we will use **f1_weighted** loss function with 5-fold- cross validation strategy\n\nThe [F1_weighted](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score) allows to get the average of F1_scores (of each label) weighted by support (the number of true instances for each label)."},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's define our classifiers\nclassifiers = {\"RF\" : RandomForestClassifier(n_estimators=100, criterion = 'entropy', random_state = 42),\n              \"KNN\" : KNeighborsClassifier(n_neighbors = 7),\n              \"NB\" : GaussianNB() }","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1. Oiginal data"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\norigin_scores = {}\nfor name, clf in classifiers.items():\n    print(name, 'classifier')\n    origin_scores[name] = cross_val_score(clf, X_train, y_train,\n                             scoring=\"f1_weighted\",\n                             cv=5)\n    print(f'f1_score = {origin_scores[name]}')\n    print(name, \"overall score :\", \n          \"%.3f\"%(np.mean(origin_scores[name])),\n         \"+- %.3f\"%(np.std(origin_scores[name]))\n         )     \n    print(\"*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2. PCA transformed data\n\n#### 2.1. All components:\nnow we will apply the same models on `pca_proj`"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\npca_scores = {}\n\nfor name, clf in classifiers.items():\n    print(name, 'classifier')\n    pca_scores[name] = cross_val_score(clf, pca_proj, y_train,\n                             scoring=\"f1_weighted\",\n                             cv=5)\n    print(f'f1_score = {pca_scores[name]}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 2.1. Add the Two first components:\n\nIn order to  fairly compare with the tsne we'll add to the orginal data only the 2 first PCA"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ntwo_first_pca_scores = {}\n\n\nfor name, clf in classifiers.items():\n    print(name, 'classifier')\n    two_first_pca_scores[name] = cross_val_score(clf, \n                                                 np.c_[X_train, pca_proj[:, [0, 1]]], \n                                                 y_train,\n                                                 scoring=\"f1_weighted\",\n                                                 cv=5)\n    \n    print(f'CV f1 scores = {two_first_pca_scores[name]}')\n    print(name, \"overall score :\", \n          \"%.3f\"%(np.mean(two_first_pca_scores[name])),\n         \"+- %.3f\"%(np.std(two_first_pca_scores[name]))\n         )     \n    print(\"*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### 2. TSNE transformed data\nNow we will apply the same models on `tsn_proj`"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ntsne_scores = {}\n\nfrom sklearn.model_selection import cross_val_score\nfor name, clf in classifiers.items():\n    print(name, 'classifier')\n    tsne_scores[name] = cross_val_score(clf, np.c_[X_train, tsn_proj[['axis_1', 'axis_2']].values], y_train,\n                             scoring=\"f1_weighted\",\n                             cv=5)\n    print(f'f1_score = {tsne_scores[name]}')\n    print(name, \"overall score :\", \n          \"%.3f\"%(np.mean(tsne_scores[name])),\n         \"+- %.3f\"%(np.std(tsne_scores[name]))\n         ) \n    print(\"*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### Save results:"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nresults = pd.DataFrame(classifiers.keys(), columns=['Classifier'])\n\nfor colname, dic in zip([\"original_F1_Score\", \"original+2_first_pca_f1_score\", \"original+2_first_tsne_f1_score\"],\n                        [origin_scores, two_first_pca_scores, tsne_scores]):\n    d = {}\n    for k, v in dic.items():\n        d[k] = np.mean(v)\n    results[colname] = d.values()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we see the T-sne embedding definitely gives the better encoding for the classifiers"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}