{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# MNIST digits with PyTorch","metadata":{}},{"cell_type":"markdown","source":"In this notebook, I attempt to document what I have learned about PyTorch. While I have used PyTorch before, I gained a deeper understanding through [PyTorch's tutorials and examples](https://pytorch.org/tutorials/).\n\n\nWherever I could, I wrote code that I can reuse as much as possible. For example, while it is definitely possible to make predictions on the test data in one go, I decided to use a dataloader to make predictions in batches instead.","metadata":{}},{"cell_type":"markdown","source":"# Importing libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport time\nimport random\n\nfrom sklearn.model_selection import train_test_split\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import ExponentialLR\nfrom torch.utils.data import DataLoader, Dataset\n\n# For data augmentation\nfrom albumentations import Compose, ShiftScaleRotate, ToFloat\nfrom albumentations.pytorch import ToTensorV2\nimport cv2\n\nimport plotly.express as px\npx.defaults.template = 'plotly_white'\npx.defaults.color_discrete_sequence = ['steelblue']\n\nimport os\nfrom pathlib import Path\npath = Path('/kaggle/input/digit-recognizer')\nos.listdir(path)","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","execution":{"iopub.status.busy":"2022-06-22T17:05:05.274556Z","iopub.execute_input":"2022-06-22T17:05:05.275164Z","iopub.status.idle":"2022-06-22T17:05:05.286979Z","shell.execute_reply.started":"2022-06-22T17:05:05.275125Z","shell.execute_reply":"2022-06-22T17:05:05.285687Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we fix seeds for reproducibility. This means that we will get the same results each time we run the notebook.","metadata":{}},{"cell_type":"code","source":"def set_seed(seed=0):\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    \nset_seed(0)","metadata":{"execution":{"iopub.status.busy":"2022-06-22T17:05:05.291163Z","iopub.execute_input":"2022-06-22T17:05:05.292345Z","iopub.status.idle":"2022-06-22T17:05:05.30052Z","shell.execute_reply.started":"2022-06-22T17:05:05.292308Z","shell.execute_reply":"2022-06-22T17:05:05.299487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will use the variable `device` to indicate whether to use GPU or CPU.","metadata":{}},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","metadata":{"execution":{"iopub.status.busy":"2022-06-22T17:05:05.305155Z","iopub.execute_input":"2022-06-22T17:05:05.305424Z","iopub.status.idle":"2022-06-22T17:05:05.311589Z","shell.execute_reply.started":"2022-06-22T17:05:05.3054Z","shell.execute_reply":"2022-06-22T17:05:05.309446Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading the data","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv(path/'train.csv')\ntest = pd.read_csv(path/'test.csv')\nsample_sub = pd.read_csv(path/'sample_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2022-06-22T17:05:05.316693Z","iopub.execute_input":"2022-06-22T17:05:05.317675Z","iopub.status.idle":"2022-06-22T17:05:09.02909Z","shell.execute_reply.started":"2022-06-22T17:05:05.317638Z","shell.execute_reply":"2022-06-22T17:05:09.028133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-22T17:05:09.031008Z","iopub.execute_input":"2022-06-22T17:05:09.03147Z","iopub.status.idle":"2022-06-22T17:05:09.047776Z","shell.execute_reply.started":"2022-06-22T17:05:09.031432Z","shell.execute_reply":"2022-06-22T17:05:09.046695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-22T17:05:09.049487Z","iopub.execute_input":"2022-06-22T17:05:09.050096Z","iopub.status.idle":"2022-06-22T17:05:09.068051Z","shell.execute_reply.started":"2022-06-22T17:05:09.050005Z","shell.execute_reply":"2022-06-22T17:05:09.067111Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_sub.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-22T17:05:09.071447Z","iopub.execute_input":"2022-06-22T17:05:09.071684Z","iopub.status.idle":"2022-06-22T17:05:09.082666Z","shell.execute_reply.started":"2022-06-22T17:05:09.071662Z","shell.execute_reply":"2022-06-22T17:05:09.081627Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Distribution of labels","metadata":{}},{"cell_type":"code","source":"label_counts = (train.label\n                .value_counts()\n                .to_frame()\n                .reset_index()\n                .rename(columns={'index': 'Label', 'label': 'Count'})\n                .sort_values('Label')\n               )\nfig = px.bar(label_counts, x='Label', y='Count')\nfig.update_traces(textposition='outside',\n                  texttemplate='%{y}',\n                  cliponaxis=False,\n                  hovertemplate=\n                  'Label: <b>%{x}</b><br>'+\n                  'Count: <b>%{y}</b>'\n                 )\nfig.update_layout(title='Distribution of labels',\n                  yaxis_title='Count',\n                  xaxis_title='Label',\n                  xaxis_type='category',\n                  yaxis_tickformat=',',\n                  hoverlabel_bgcolor=\"white\",\n                  hoverlabel_font_size=14,\n                  hovermode=\"x\"\n                 )\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-22T17:05:09.084266Z","iopub.execute_input":"2022-06-22T17:05:09.084692Z","iopub.status.idle":"2022-06-22T17:05:09.151948Z","shell.execute_reply.started":"2022-06-22T17:05:09.084657Z","shell.execute_reply":"2022-06-22T17:05:09.150966Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Splitting the data","metadata":{}},{"cell_type":"markdown","source":"Here we split the train data to create a validation set. We'll use 20% of the train data as a validation set. Since the Albumentations library expects a channel dimension, we reshape each image to have the shape (28, 28, 1), where 1 is the channel dimension. Since our images are in grayscale, this dimension is 1. If we had colour images instead, this dimension would be 3 - one each for red, green, and blue (RGB).","metadata":{}},{"cell_type":"code","source":"X = train.drop(columns='label').values.reshape(-1, 28, 28, 1)\ny = train.label.values\n\ntrain_X, valid_X, train_y, valid_y = train_test_split(X,\n                                                      y,\n                                                      test_size=0.2\n                                                     )","metadata":{"execution":{"iopub.status.busy":"2022-06-22T17:05:09.153429Z","iopub.execute_input":"2022-06-22T17:05:09.153826Z","iopub.status.idle":"2022-06-22T17:05:09.599538Z","shell.execute_reply.started":"2022-06-22T17:05:09.153774Z","shell.execute_reply":"2022-06-22T17:05:09.598568Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset and augmentation","metadata":{}},{"cell_type":"markdown","source":"In PyTorch, the best way to feed data to the model is with a dataloader. In particular, `torch.utils.data.DataLoader` is an iterator which provides features such as batching, shuffling, and loading data in parallel. In order to use PyTorch's dataloader, we need to create a dataset first. The most flexible way to do this is by creating a custom dataset class that inherits from `torch.utils.data.Dataset`, which is an abstract class. The [PyTorch dataloader tutorial](https://pytorch.org/tutorials/recipes/recipes/custom_dataset_transforms_loader.html#create-a-dataset-class) tells us that we should override the following methods:\n\n> - `__len__` so that `len(dataset)` returns the size of the dataset.\n- `__getitem__` to support indexing such that `dataset[i]` can be used to get the $i$th sample\n\nNote that one of the parameters used to initialize the dataset is used to specify transformations to be applied to the image before returning it.","metadata":{}},{"cell_type":"code","source":"class MNISTDataset(Dataset):\n    def __init__(self, X, y=None, is_test=False, transforms=None):\n        self.X = X\n        self.y = y\n        self.is_test = is_test\n        self.transforms = transforms\n\n    def __len__(self):\n        return len(self.X)\n\n    def __getitem__(self, index):\n        image = self.X[index]\n        \n        if self.transforms:\n            image = self.transforms(image=image)['image']\n            \n        if self.is_test:\n            return image\n        else:\n            return image, self.y[index]","metadata":{"execution":{"iopub.status.busy":"2022-06-22T17:05:09.601202Z","iopub.execute_input":"2022-06-22T17:05:09.601595Z","iopub.status.idle":"2022-06-22T17:05:09.609433Z","shell.execute_reply.started":"2022-06-22T17:05:09.60156Z","shell.execute_reply":"2022-06-22T17:05:09.608213Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next, we define the transformations that we will use for the train and validation sets. We use the Compose function to chain together several transformations. Our pixel values are integers between 0 and 255 inclusive. \n- The ToFloat() transform divides the \"pixel values by max_value to get a float32 output array where all values lie in the range [0, 1.0]\". [[documentation]](https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.ToFloat)\n- We use the ShiftScaleRotate transform to randomly scale the input, randomly shift the input (horizontally and vertically), and randomly rotate the input slightly. We specify p=1 to indicate that we always want to apply ShiftScaleRotate. I will explain what the `border_mode` and `value` parameters are used for below. [[documentation]](https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.ShiftScaleRotate)\n- Finally, we transform the NumPy arrays to PyTorch Tensors with ToTensorV2(). This is necessary because our PyTorch model will expect Tensor inputs. We [use ToTensorV2() instead of ToTensor()](https://albumentations.ai/docs/faq/#which-transformation-should-i-use-to-convert-a-numpy-array-with-an-image-or-a-mask-to-a-pytorch-tensor-totensor-or-totensorv2) because \"ToTensor() is now deprecated and will be removed in future versions\". It is also important to note that an input shape of (28, 28, 1) will be converted to have a shape of (1, 28, 28) after applying ToTensorV2(). This again has to do with the input that PyTorch expects. Our model will need to have a Tensor input of shape nSamples x nChannels x Height x Width.\n","metadata":{}},{"cell_type":"code","source":"train_transforms = Compose([ToFloat(max_value=255),\n                            ShiftScaleRotate(shift_limit=0.1,\n                                             scale_limit=0.1,\n                                             rotate_limit=10,\n                                             border_mode=cv2.BORDER_CONSTANT,\n                                             value=0,\n                                             p=1),\n                            ToTensorV2()\n                           ])\nvalid_transforms = Compose([ToFloat(max_value=255),\n                            ToTensorV2()\n                           ])","metadata":{"execution":{"iopub.status.busy":"2022-06-22T17:05:09.611355Z","iopub.execute_input":"2022-06-22T17:05:09.612147Z","iopub.status.idle":"2022-06-22T17:05:09.618773Z","shell.execute_reply.started":"2022-06-22T17:05:09.612109Z","shell.execute_reply":"2022-06-22T17:05:09.61791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we display some images and their transformations. The top row consists of a random image from each class. The images in the second row are the original images with transformations applied. At first glance, the images look identical to the original images. However, notice that the 3 is shifted up higher than in the original image. The 8 is slightly rotated to the right and shifted up as well. It also looks like the 6 is shifted down.","metadata":{}},{"cell_type":"code","source":"def display_transformations(transforms):\n    fig = plt.figure(figsize=(12, 2))\n    for k in range(10):\n        idx = random.choice(train.label[train.label.eq(k)].index)\n        image, label = train.drop(columns='label').iloc[idx].values.reshape(28, 28), k\n\n        ax = plt.subplot(2, 10, k+1)\n        ax.title.set_text(label)\n        plt.axis('off')\n        plt.imshow(image, cmap='gray')\n\n        image = np.expand_dims(image, 2)\n        image = transforms(image=image)['image']\n        image = image.squeeze(0)\n        ax = plt.subplot(2, 10, 10+k+1)\n        plt.axis('off')\n        plt.imshow(image, cmap='gray')","metadata":{"execution":{"iopub.status.busy":"2022-06-22T17:05:09.620465Z","iopub.execute_input":"2022-06-22T17:05:09.620856Z","iopub.status.idle":"2022-06-22T17:05:09.631857Z","shell.execute_reply.started":"2022-06-22T17:05:09.620822Z","shell.execute_reply":"2022-06-22T17:05:09.630935Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display_transformations(train_transforms)","metadata":{"execution":{"iopub.status.busy":"2022-06-22T17:05:09.63742Z","iopub.execute_input":"2022-06-22T17:05:09.639035Z","iopub.status.idle":"2022-06-22T17:05:11.20686Z","shell.execute_reply.started":"2022-06-22T17:05:09.638996Z","shell.execute_reply":"2022-06-22T17:05:11.205754Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In the example below, I have used different arguments for the shift, scale, and rotate limits to make it easier to visualize what happens if we do not specify the `border_mode` and `value` arguments.","metadata":{}},{"cell_type":"code","source":"example_transforms = Compose([ToFloat(max_value=255),\n                            ShiftScaleRotate(shift_limit=0.3,\n                                             scale_limit=0.3,\n                                             rotate_limit=30,\n#                                              border_mode=cv2.BORDER_CONSTANT,\n#                                              value=0,\n                                             p=1),\n                            ToTensorV2()\n                           ])","metadata":{"execution":{"iopub.status.busy":"2022-06-22T17:05:11.208382Z","iopub.execute_input":"2022-06-22T17:05:11.209407Z","iopub.status.idle":"2022-06-22T17:05:11.21528Z","shell.execute_reply.started":"2022-06-22T17:05:11.209369Z","shell.execute_reply":"2022-06-22T17:05:11.214422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display_transformations(example_transforms)","metadata":{"execution":{"iopub.status.busy":"2022-06-22T17:05:11.21661Z","iopub.execute_input":"2022-06-22T17:05:11.217087Z","iopub.status.idle":"2022-06-22T17:05:12.5636Z","shell.execute_reply.started":"2022-06-22T17:05:11.217052Z","shell.execute_reply":"2022-06-22T17:05:12.562715Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The default argument for the `border_mode` parameter is `cv2.BORDER_REFLECT_101`. The above result is obviously not desirable and so we specify `border_mode=cv2.BORDER_CONSTANT` and `value=0` to make the borders black.","metadata":{}},{"cell_type":"markdown","source":"# Model architecture","metadata":{}},{"cell_type":"markdown","source":"We will see that this simple architecture will achieve an accuracy of over 99%. However, the purpose of this notebook is for me to document what I have learned, and not to achieve the highest accuracy possible (I don't use any ensembles either).\n\nSimilarly to how we created a custom dataset, to define a neural network, we inherit from nn.Module and simply implement the `forward()` method. Note that we define two different `nn.BatchNorm2d(48)` layers. Using the same batch normalization layer in multiple parts of the network would be a mistake because the `nn.BatchNorm2d()` class keeps track of the running mean and variance by default.","metadata":{}},{"cell_type":"code","source":"class CNN(nn.Module):\n    def __init__(self):\n        super(CNN, self).__init__()\n        self.dropout = nn.Dropout(0.5)\n        \n        self.bn1_1 = nn.BatchNorm2d(48)\n        self.bn1_2 = nn.BatchNorm2d(48)\n\n        self.bn2 = nn.BatchNorm1d(256)\n        \n        self.conv1 = nn.Conv2d(in_channels=1, out_channels=48, kernel_size=3, stride=1, padding=1)\n        self.conv2 = nn.Conv2d(in_channels=48, out_channels=48, kernel_size=3, stride=2, padding=1)\n        \n        self.fc1 = nn.Linear(in_features=48 * 14 * 14, out_features=256)\n        self.fc2 = nn.Linear(in_features=256, out_features=10)\n        \n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        x = self.bn1_1(x)\n        x = F.relu(self.conv2(x))\n        x = self.bn1_2(x)\n        x = self.dropout(x)\n        \n        x = torch.flatten(x, 1)\n        \n        x = F.relu(self.fc1(x))\n        x = self.bn2(x)\n        x = self.dropout(x)\n        \n        x = self.fc2(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2022-06-22T17:05:12.565145Z","iopub.execute_input":"2022-06-22T17:05:12.565559Z","iopub.status.idle":"2022-06-22T17:05:12.577097Z","shell.execute_reply.started":"2022-06-22T17:05:12.565521Z","shell.execute_reply":"2022-06-22T17:05:12.575842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Early stopping class","metadata":{}},{"cell_type":"markdown","source":"This early stopping implementation is based on the following three implementations that I found:\n\n- https://github.com/pytorch/ignite/blob/master/ignite/handlers/early_stopping.py\n- https://gist.github.com/stefanonardo/693d96ceb2f531fa05db530f3e21517d\n- https://github.com/Bjarten/early-stopping-pytorch/blob/master/pytorchtools.py\n\nEarly stopping allows us to terminate training if the validation loss stops improving for a certain number of epochs. This number is often referred to as 'patience'. Additionally, we use this class to save the model every time the validation loss improves. This allows us to load the model with the best validation score after training has stopped.","metadata":{}},{"cell_type":"code","source":"class EarlyStopping:\n    def __init__(self, mode, path, patience=3, delta=0):\n        if mode not in {'min', 'max'}:\n            raise ValueError(\"Argument mode must be one of 'min' or 'max'.\")\n        if patience <= 0:\n            raise ValueError(\"Argument patience must be a postive integer.\")\n        if delta < 0:\n            raise ValueError(\"Argument delta must not be a negative number.\")\n            \n        self.mode = mode\n        self.patience = patience\n        self.delta = delta\n        self.path = path\n        self.best_score = np.inf if mode == 'min' else -np.inf\n        self.counter = 0\n        \n    def _is_improvement(self, val_score):\n        \"\"\"Return True iff val_score is better than self.best_score.\"\"\"\n        if self.mode == 'max' and val_score > self.best_score + self.delta:\n            return True\n        elif self.mode == 'min' and val_score < self.best_score - self.delta:\n            return True\n        return False\n        \n    def __call__(self, val_score, model):\n        \"\"\"Return True iff self.counter >= self.patience.\n        \"\"\"\n        \n        if self._is_improvement(val_score):\n            self.best_score = val_score\n            self.counter = 0\n            torch.save(model.state_dict(), self.path)\n            print('Val loss improved. Saved model.')\n            return False\n        else:\n            self.counter += 1\n            print(f'Early stopping counter: {self.counter}/{self.patience}')\n            if self.counter >= self.patience:\n                print(f'Stopped early. Best val loss: {self.best_score:.4f}')\n                return True","metadata":{"execution":{"iopub.status.busy":"2022-06-22T17:05:12.578862Z","iopub.execute_input":"2022-06-22T17:05:12.579259Z","iopub.status.idle":"2022-06-22T17:05:12.591695Z","shell.execute_reply.started":"2022-06-22T17:05:12.579213Z","shell.execute_reply":"2022-06-22T17:05:12.590488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Functions","metadata":{}},{"cell_type":"markdown","source":"Instead of writing all of our code for training and validation in one cell, it can be helpful to break the different parts into functions.","metadata":{}},{"cell_type":"code","source":"def train_one_epoch(model, train_loader, optimizer, device, criterion):\n    \"\"\"Train model for one epoch and return the mean train_loss.\"\"\"\n    model.train()\n    running_loss_train = 0\n    for inputs, labels in train_loader:\n        inputs, labels = inputs.to(device), labels.to(device)\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        running_loss_train += loss.item()\n    train_loss = running_loss_train / len(train_loader.dataset)\n    return train_loss","metadata":{"execution":{"iopub.status.busy":"2022-06-22T17:05:12.593392Z","iopub.execute_input":"2022-06-22T17:05:12.593746Z","iopub.status.idle":"2022-06-22T17:05:12.603247Z","shell.execute_reply.started":"2022-06-22T17:05:12.593702Z","shell.execute_reply":"2022-06-22T17:05:12.60228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After every epoch of training, we compute the validation loss and the validation accuracy. Note that we do not need to compute gradients at this stage, so wrap the validation loop with `torch.no_grad()`.","metadata":{}},{"cell_type":"code","source":"def validate(model, valid_loader, device, criterion):\n    \"\"\"Validate model and return the accuracy and mean loss.\"\"\"\n    model.eval()\n    correct = 0\n    running_loss_val = 0\n    with torch.no_grad():\n        for inputs, labels in valid_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            pred = outputs.argmax(dim=1)\n            correct += pred.eq(labels).sum().item()\n            running_loss_val += loss.item()\n    val_acc = correct / len(valid_loader.dataset)\n    val_loss = running_loss_val / len(valid_loader.dataset)\n    return val_acc, val_loss","metadata":{"execution":{"iopub.status.busy":"2022-06-22T17:05:12.604626Z","iopub.execute_input":"2022-06-22T17:05:12.605652Z","iopub.status.idle":"2022-06-22T17:05:12.614696Z","shell.execute_reply.started":"2022-06-22T17:05:12.605594Z","shell.execute_reply":"2022-06-22T17:05:12.613737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We use the cross entropy loss and the Adam optimizer. We also decay our learning rate by multiplying the learning rate by 0.95 every epoch. If our validation loss does not improve after 5 consecutive epochs, we stop training.","metadata":{}},{"cell_type":"code","source":"def fit(model, train_loader, valid_loader, learning_rate, num_epochs):\n    criterion = nn.CrossEntropyLoss(reduction='sum')\n    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n    es = EarlyStopping(mode='min', path='model.pth', patience=5)\n    model = model.to(device)\n    scheduler = ExponentialLR(optimizer, gamma=0.9)\n\n    for epoch in range(1, num_epochs + 1):\n        train_loss = train_one_epoch(model, train_loader, optimizer, device, criterion)\n        val_acc, val_loss = validate(model, valid_loader, device, criterion)\n        scheduler.step()\n        print(f'Epoch {epoch:2}/{num_epochs}',\n              f'train loss: {train_loss:.4f}',\n              f'val loss: {val_loss:.4f}',\n              f'val acc: {val_acc:.2%}',\n              sep=' | '\n             )\n        if es(val_loss, model):\n            break","metadata":{"execution":{"iopub.status.busy":"2022-06-22T17:05:12.616264Z","iopub.execute_input":"2022-06-22T17:05:12.61705Z","iopub.status.idle":"2022-06-22T17:05:12.625985Z","shell.execute_reply.started":"2022-06-22T17:05:12.617013Z","shell.execute_reply":"2022-06-22T17:05:12.624961Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Fitting the model","metadata":{}},{"cell_type":"markdown","source":"Note that the validation batch size is larger than the train batch size. This is because unlike the train batch size, the validation batch size is not a hyperparameter. Therefore, our only concern is speed when it comes to validation. I found a validation batch size of 512 to be the fastest here. Note that since the images are small, we could actually just pass the entire validation set to the model at once. But, this turns out to be slower than using a batch size of 512 because the GPU has to wait for the CPU to load all of the images in the validation set into the GPU.\n\nNote that there is no need to shuffle the validation data, unlike the train data. Also, we use `drop_last=True` to indicate that we want to drop the last batch. The last batch is usually smaller than our specified batch size and because we are using batch normalization, our batches should not be too small. Since we are shuffling the training data, dropping the last batch does not have any significant impact on training.\n\nIt is always a good idea to tune the Dataloader's `num_workers` parameter. From the PyTorch [documentation](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader): \n> num_workers (int, optional) – how many subprocesses to use for data loading. 0 means that the data will be loaded in the main process. (default: 0)\n\nIn this case, I found the default of 0 `num_workers` to be best. The best way to set the `num_workers` parameter is by picking the fastest option by trial and error.","metadata":{}},{"cell_type":"code","source":"TRAIN_BATCH_SIZE = 64\nVALID_BATCH_SIZE = 512\nNUM_EPOCHS = 50\nLEARNING_RATE = 1e-3\nNUM_WORKERS = 0\nPIN_MEMORY = True\n\nmnist_train = MNISTDataset(train_X, train_y, is_test=False, transforms=train_transforms)\nmnist_valid = MNISTDataset(valid_X, valid_y, is_test=False, transforms=valid_transforms)\n\ntrain_loader = DataLoader(mnist_train, batch_size=TRAIN_BATCH_SIZE, shuffle=True, drop_last=True, num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\nvalid_loader = DataLoader(mnist_valid, batch_size=VALID_BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n\nmodel = CNN()\nstart = time.time()\nfit(model, train_loader, valid_loader, learning_rate=LEARNING_RATE, num_epochs=NUM_EPOCHS)\nprint(f'Total training time: {time.time() - start}')\nmodel.load_state_dict(torch.load('model.pth'))","metadata":{"execution":{"iopub.status.busy":"2022-06-22T17:05:12.627623Z","iopub.execute_input":"2022-06-22T17:05:12.628621Z","iopub.status.idle":"2022-06-22T17:06:04.713188Z","shell.execute_reply.started":"2022-06-22T17:05:12.628584Z","shell.execute_reply":"2022-06-22T17:06:04.711732Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Predict on test set and display predictions/images","metadata":{}},{"cell_type":"markdown","source":"Finally, we make predictions on the test data and create a CSV submission file.","metadata":{}},{"cell_type":"code","source":"TEST_BATCH_SIZE = 512\n\ntest_transforms = Compose([ToFloat(max_value=255),\n                           ToTensorV2()\n                          ])\n\ntest_X = test.values.reshape(-1, 28, 28, 1)\nmnist_test = MNISTDataset(test_X, is_test=True, transforms=test_transforms)\ntest_loader = DataLoader(mnist_test, batch_size=TEST_BATCH_SIZE, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2022-06-22T17:06:04.714261Z","iopub.status.idle":"2022-06-22T17:06:04.715253Z","shell.execute_reply.started":"2022-06-22T17:06:04.714996Z","shell.execute_reply":"2022-06-22T17:06:04.715021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict(model, test_loader, device):\n    \"\"\"Make predictions on the test data and return \n    the submission data frame.\n    \"\"\"\n    \n    model.eval()\n    predictions = sample_sub['Label'].values\n    with torch.no_grad():\n        for i, inputs in enumerate(test_loader):\n            inputs = inputs.to(device)\n            outputs = model(inputs)\n            pred = outputs.argmax(dim=1).to('cpu').numpy()\n            predictions[i*TEST_BATCH_SIZE:i*TEST_BATCH_SIZE+len(inputs)] = pred\n    \n    output = sample_sub.copy()\n    output['Label'] = predictions\n    output.to_csv('submission.csv', index=False)\n    return output\n\noutput = predict(model, test_loader, device)\noutput","metadata":{"execution":{"iopub.status.busy":"2022-06-22T17:06:04.716414Z","iopub.status.idle":"2022-06-22T17:06:04.717412Z","shell.execute_reply.started":"2022-06-22T17:06:04.717146Z","shell.execute_reply":"2022-06-22T17:06:04.717171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Just to make sure that our predictions make sense, we will display 10 different images from the test set for each of the 10 classes. For example, of all the images that the model assigned to class 0, we pick 10 images at random and display them in the first row. We do this for each class.","metadata":{}},{"cell_type":"code","source":"fig = plt.figure(figsize=(12, 10))\nfor k in range(10):\n    indices = output[output.Label.eq(k)].sample(10).index\n    for j, idx in enumerate(indices):\n        image, label = test_X[idx], output.loc[idx, 'Label'].item()\n        image = image.squeeze(2)\n        ax = plt.subplot(10, 10, 10*k+j+1)\n        ax.title.set_text(label)\n        plt.axis('off')\n        plt.imshow(image, cmap='gray')\n\nplt.subplots_adjust(left=0.125, bottom=0.1, right=0.9, top=0.9, wspace=0.2, hspace=0.6)","metadata":{"execution":{"iopub.status.busy":"2022-06-22T17:06:04.71866Z","iopub.status.idle":"2022-06-22T17:06:04.719628Z","shell.execute_reply.started":"2022-06-22T17:06:04.719362Z","shell.execute_reply":"2022-06-22T17:06:04.719388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We'll also plot the distribution of predictions.","metadata":{}},{"cell_type":"code","source":"prediction_counts = (output.Label\n                     .value_counts()\n                     .to_frame()\n                     .reset_index()\n                     .rename(columns={'index': 'Prediction', 'Label': 'Count'})\n                     .sort_values('Prediction')\n                    )\nfig = px.bar(prediction_counts, x='Prediction', y='Count')\nfig.update_traces(textposition='outside',\n                  texttemplate='%{y}',\n                  cliponaxis=False,\n                  hovertemplate=\n                  'Prediction: <b>%{x}</b><br>'+\n                  'Count: <b>%{y}</b>'\n                 )\nfig.update_layout(title='Distribution of predictions',\n                  yaxis_title='Count',\n                  xaxis_title='Prediction',\n                  xaxis_type='category',\n                  yaxis_tickformat=',',\n                  hoverlabel_bgcolor=\"white\",\n                  hoverlabel_font_size=14,\n                  hovermode=\"x\"\n                 )\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-22T17:06:04.720899Z","iopub.status.idle":"2022-06-22T17:06:04.721856Z","shell.execute_reply.started":"2022-06-22T17:06:04.721571Z","shell.execute_reply":"2022-06-22T17:06:04.721597Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This is currently a work in progress.","metadata":{}}]}