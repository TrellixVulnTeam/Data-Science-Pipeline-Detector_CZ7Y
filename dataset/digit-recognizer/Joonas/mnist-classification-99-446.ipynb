{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Dependencies\nimport PIL\nimport numpy as np\nimport pandas as pd\nimport random\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torchvision.transforms.functional import to_pil_image\nimport math\nimport shutil\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport os\nimport zipfile\nfrom copy import deepcopy\n\n# progress bar\nfrom tqdm import tqdm\nfrom tqdm.notebook import tqdm as tqdm_nb\n\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn import preprocessing\n\nimport pandas as pd","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-03-29T14:53:01.792961Z","iopub.execute_input":"2022-03-29T14:53:01.793316Z","iopub.status.idle":"2022-03-29T14:53:04.334538Z","shell.execute_reply.started":"2022-03-29T14:53:01.793227Z","shell.execute_reply":"2022-03-29T14:53:04.333777Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# CPU/GPU\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint('device:', device)","metadata":{"execution":{"iopub.status.busy":"2022-03-29T14:53:04.338059Z","iopub.execute_input":"2022-03-29T14:53:04.338255Z","iopub.status.idle":"2022-03-29T14:53:04.39478Z","shell.execute_reply.started":"2022-03-29T14:53:04.338231Z","shell.execute_reply":"2022-03-29T14:53:04.393694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv('../input/digit-recognizer/train.csv')\nprint(len(train_df))\ntrain_df","metadata":{"execution":{"iopub.status.busy":"2022-03-29T14:53:04.396251Z","iopub.execute_input":"2022-03-29T14:53:04.396586Z","iopub.status.idle":"2022-03-29T14:53:07.418104Z","shell.execute_reply.started":"2022-03-29T14:53:04.39655Z","shell.execute_reply":"2022-03-29T14:53:07.417448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Display image from csv\n\nColormap: https://matplotlib.org/stable/gallery/color/colormap_reference.html\n\nJust see images. any of transforming be not yet.","metadata":{}},{"cell_type":"code","source":"def array_to_image_tensor(array):\n    return np.array(array).reshape(28, 28, 1).astype(np.int32)","metadata":{"execution":{"iopub.status.busy":"2022-03-29T14:53:07.42014Z","iopub.execute_input":"2022-03-29T14:53:07.420396Z","iopub.status.idle":"2022-03-29T14:53:07.426517Z","shell.execute_reply.started":"2022-03-29T14:53:07.420362Z","shell.execute_reply":"2022-03-29T14:53:07.42588Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(3, 10, figsize=(10, 4))\nfig.suptitle('Labels / Images')\nax = ax.ravel()\nfor i in range(30):\n    raw = np.array(train_df.iloc[i])\n    label, img = raw[0], array_to_image_tensor(raw[1:])\n    ax[i].imshow(img, cmap='gist_gray')\n    ax[i].axis(\"off\")\n    ax[i].set_title(str(label))\nplt.subplots_adjust(hspace=0.2)","metadata":{"execution":{"iopub.status.busy":"2022-03-29T14:53:07.428063Z","iopub.execute_input":"2022-03-29T14:53:07.428688Z","iopub.status.idle":"2022-03-29T14:53:08.403097Z","shell.execute_reply.started":"2022-03-29T14:53:07.428644Z","shell.execute_reply":"2022-03-29T14:53:08.402449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mean = torch.tensor([0.485, 0.456, 0.406], dtype=torch.float32)\nstd = torch.tensor([0.229, 0.224, 0.225], dtype=torch.float32)\nnormalize = transforms.Normalize(mean.tolist(), std.tolist())\nunnormalize = transforms.Normalize((-mean / std).tolist(), (1.0 / std).tolist())","metadata":{"execution":{"iopub.status.busy":"2022-03-29T14:53:08.404373Z","iopub.execute_input":"2022-03-29T14:53:08.404715Z","iopub.status.idle":"2022-03-29T14:53:08.441449Z","shell.execute_reply.started":"2022-03-29T14:53:08.404678Z","shell.execute_reply":"2022-03-29T14:53:08.440843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Average image of Dataset for each numbers","metadata":{}},{"cell_type":"code","source":"numbers = []\nfor number in range(10):\n    pixels = train_df[train_df['label'] == number].mean()[1:]\n    numbers.append(array_to_image_tensor(pixels))\n\nfig, ax = plt.subplots(2, 5, figsize=(10, 4))\nfig.suptitle('Average of Images')\nax = ax.ravel()\nfor i in range(10):\n    ax[i].imshow(numbers[i], cmap='Blues')\n    ax[i].axis(\"off\")\n    ax[i].set_title(str(i))\nplt.subplots_adjust(hspace=0.2)","metadata":{"execution":{"iopub.status.busy":"2022-03-29T14:53:08.442706Z","iopub.execute_input":"2022-03-29T14:53:08.442946Z","iopub.status.idle":"2022-03-29T14:53:09.488349Z","shell.execute_reply.started":"2022-03-29T14:53:08.442914Z","shell.execute_reply":"2022-03-29T14:53:09.487629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Distribution of labels for digits","metadata":{}},{"cell_type":"code","source":"sns.countplot(x='label', data=train_df)","metadata":{"execution":{"iopub.status.busy":"2022-03-29T14:53:09.489592Z","iopub.execute_input":"2022-03-29T14:53:09.489922Z","iopub.status.idle":"2022-03-29T14:53:09.714427Z","shell.execute_reply.started":"2022-03-29T14:53:09.489884Z","shell.execute_reply":"2022-03-29T14:53:09.713705Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Put into Dataset and see it","metadata":{}},{"cell_type":"markdown","source":"Make custom Dataset for load binary pixels from dataframe, and convert it to RGB pixels with size (3, 28, 28)\n\nBecause of memory, we can not have all pixels after load from dataframe. So, just do transform only when get item","metadata":{}},{"cell_type":"code","source":"class DigitDataset(torch.utils.data.Dataset):\n    def __init__(self, df, transform=None):\n        self.df = df\n        self.transform = transform\n        self.targets = np.array(df['label']).reshape(-1, 1)\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        image = np.array(self.df.iloc[idx][1:]).reshape(28, 28) # (28, 28)\n        image = image.astype(np.float32)\n        label = self.df.iloc[idx]['label']\n        if self.transform:\n            image = self.transform(image)\n        return image, label\n    \n    def gettargets(self):\n        return self.targets","metadata":{"execution":{"iopub.status.busy":"2022-03-29T14:53:09.715688Z","iopub.execute_input":"2022-03-29T14:53:09.716448Z","iopub.status.idle":"2022-03-29T14:53:09.725469Z","shell.execute_reply.started":"2022-03-29T14:53:09.716408Z","shell.execute_reply":"2022-03-29T14:53:09.724679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In dataset, the size of image is `(28, 28)`. so we needs to transform it to resize to `(224, 224)` for VGG and also RGB channels.\n\n`bin_to_rgb(t)` makes binary image to RGB image with `(3, 28, 28)`. now it can be PIL Image.","metadata":{}},{"cell_type":"code","source":"def bin_to_rgb(t):\n    h, w = t.shape\n    img = np.array([np.array(t).reshape(1, h, w)] * 3).reshape(3, h, w).astype(np.float32)\n    return torch.tensor(img)\n\ntrain_set = DigitDataset(train_df, transform=transforms.Compose([\n    bin_to_rgb,\n    transforms.RandomSolarize(threshold=200, p=1),\n    normalize,\n]))\ntrain_loader = torch.utils.data.DataLoader(train_set, batch_size=64)\n\nimages, labels = next(iter(train_loader))\nto_pil_image(images[0]).resize((240, 240), PIL.Image.NEAREST)","metadata":{"execution":{"iopub.status.busy":"2022-03-29T14:53:09.729996Z","iopub.execute_input":"2022-03-29T14:53:09.730207Z","iopub.status.idle":"2022-03-29T14:53:09.8142Z","shell.execute_reply.started":"2022-03-29T14:53:09.730177Z","shell.execute_reply":"2022-03-29T14:53:09.813514Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rows = 4\ncols = 8\nimages, labels = next(iter(train_loader))\n\nfig, ax = plt.subplots(rows, cols, figsize=(12, 6))\nfig.suptitle('from dataset')\nax = ax.ravel()\nfor i in range(rows * cols):\n    image = images[i]\n    label = labels[i].numpy().item()\n    ax[i].imshow(np.asarray(to_pil_image(image)))\n    ax[i].axis(\"off\")\n    ax[i].set_title(label)\nplt.subplots_adjust(hspace=0.5)","metadata":{"execution":{"iopub.status.busy":"2022-03-29T14:53:09.815254Z","iopub.execute_input":"2022-03-29T14:53:09.815512Z","iopub.status.idle":"2022-03-29T14:53:10.925217Z","shell.execute_reply.started":"2022-03-29T14:53:09.815477Z","shell.execute_reply":"2022-03-29T14:53:10.924529Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Simple CNN\n\nonly 2 conv2d layers and 3 dense layers.","metadata":{}},{"cell_type":"code","source":"class MyConvNet(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layer = nn.Sequential(\n            nn.Conv2d(3, 64, kernel_size=(3, 3), padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(),\n            nn.Conv2d(64, 256, 3, padding=1),  # (256, 28, 28)\n            nn.BatchNorm2d(256),\n            nn.ReLU(),\n            nn.MaxPool2d(2, stride=2),         # (256, 14, 14)\n        )\n        self.classifier = nn.Sequential(\n            nn.Linear(256 * 14 * 14, 1024),\n            nn.ReLU(),\n            nn.Dropout(0.4),\n            nn.Linear(1024, 128),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(128, 10),\n        )\n\n    def forward(self, x):\n        x = self.layer(x)\n        x = x.view(x.size(0), -1)\n        x = self.classifier(x)\n        return x\n\nmodel = MyConvNet().to(device)\n\nprint(\"Params to learn:\")\nparams_to_update = []\nfor name, param in model.named_parameters():\n    if param.requires_grad == True:\n        params_to_update.append(param)\n        print(\"\\t\",name)\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(params_to_update, lr=1e-4)\n\nprint(model)","metadata":{"execution":{"iopub.status.busy":"2022-03-29T14:53:10.926423Z","iopub.execute_input":"2022-03-29T14:53:10.926882Z","iopub.status.idle":"2022-03-29T14:53:14.080956Z","shell.execute_reply.started":"2022-03-29T14:53:10.926843Z","shell.execute_reply":"2022-03-29T14:53:14.080211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datetime import datetime, timedelta\nstart_time = datetime.now()\nprint('start at:', start_time)","metadata":{"execution":{"iopub.status.busy":"2022-03-29T14:53:14.081919Z","iopub.execute_input":"2022-03-29T14:53:14.08252Z","iopub.status.idle":"2022-03-29T14:53:14.088196Z","shell.execute_reply.started":"2022-03-29T14:53:14.082481Z","shell.execute_reply":"2022-03-29T14:53:14.087337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Let's Train","metadata":{}},{"cell_type":"code","source":"EPOCHES = 45\nfold_count = 3\nbatch_size = 64\n\nkfold = StratifiedKFold(n_splits=fold_count, shuffle=True)\n\nprint(f'Epoches = {EPOCHES}, Fold = {fold_count}, batch size = {batch_size}')\n\ntrain_losses = []\nvalid_losses = []\ntrain_accs = []\nvalid_accs = []\n\nbest = {\n    'epoch': 0,\n    'train_loss': 1e9,\n    'valid_loss': 1e9,\n    'state': {},\n}\n\ndef train_valid_diff(epoch):\n    if epoch < 0: return math.inf\n    return abs(train_losses[epoch] - valid_losses[epoch])\n\nmodel.to(device)\n\n# Run apoch\nepoch = 0\nearly_stop = None\nwhile epoch < EPOCHES:\n    # K-Fold cross validation\n    splited_folds = kfold.split(train_set, train_set.targets)\n    for fold, (train_idx, valid_idx) in enumerate(splited_folds):\n        pbar = tqdm_nb(total=len(train_idx)+len(valid_idx)//2, desc=f'{epoch+1}/{EPOCHES} epoch')\n        # Split dataset and loader\n        train_subsampler = torch.utils.data.SubsetRandomSampler(train_idx)\n        valid_subsampler = torch.utils.data.SubsetRandomSampler(valid_idx)\n        # Use as train/valid set from train data set by k-fold\n        train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, sampler=train_subsampler)\n        valid_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, sampler=valid_subsampler)\n\n        running_loss = 0.0\n        running_acc = []\n\n        # Train\n        model.train()\n        for inputs, labels in train_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n\n            optimizer.zero_grad()\n\n            outputs = model.forward(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n            pred = torch.argmax(outputs.data, dim=1).to(device)\n\n            running_loss += loss.item()\n            running_acc.append(torch.sum(pred == labels).to('cpu') / len(labels))\n\n            pbar.update(len(labels))\n\n        # train loss (average)\n        train_loss = running_loss / len(train_loader)\n        train_acc = np.array(running_acc).mean()\n        train_losses.append(train_loss)\n        train_accs.append(train_acc)\n\n        model_state = deepcopy(model.state_dict())\n\n        # valid loss (just for check)\n        model.eval()\n        with torch.no_grad():\n            valid_loss_sum = 0\n            valid_acc = []\n            # Browse quickly, with not for all valid data set\n            v_cnt = len(valid_loader) / 2\n            for i, (images, labels) in enumerate(valid_loader):\n                if i >= v_cnt: break\n                images, labels = images.to(device), labels.to(device)\n                outputs = model(images).to(device)\n                pred = torch.argmax(outputs.data, dim=1)\n                pred = pred.to(device)\n                last_valid_result = (images, labels, pred)\n                valid_loss_sum += criterion(outputs, labels).item()\n                valid_acc.append(torch.sum(pred == labels).to('cpu') / len(labels))\n                pbar.update(len(labels))\n            valid_loss = valid_loss_sum / v_cnt\n            valid_acc_avg = np.array(valid_acc).mean()\n            valid_losses.append(valid_loss)\n            valid_accs.append(valid_acc_avg)\n\n        print('[%d] fold=%d, train loss: %.6f (%.3f %%), valid loss: %.6f (%.3f %%)' % \\\n              (epoch + 1, fold, train_loss, train_acc * 100, valid_loss, valid_acc_avg * 100))\n\n        # get best\n        if valid_loss < best['valid_loss']:\n            best = {\n                'epoch': epoch,\n                'train_loss': train_loss,\n                'valid_loss': valid_loss,\n                'state': model_state,\n            }\n\n        pbar.close()\n\n        epoch += 1\n        if epoch >= EPOCHES: break\n        if epoch < 3: continue\n\n        tl = np.array(train_losses[-3:]).mean()\n        vl = np.array(valid_losses[-3:]).mean()\n        minl, maxl = min(vl, tl), max(vl, tl)\n\n        # Early stop conditions\n        if minl > 0.5 and 2 * minl < maxl and 2 * train_valid_diff(epoch-1) < train_valid_diff(epoch):\n            early_stop = 'Train and valid have distance by overfitting'\n            break\n        if np.array([tl, vl]).mean() < 0.0005:\n            early_stop = 'Trained well enough'\n            break\n        if start_time + timedelta(hours=2) < datetime.now():\n            early_stop = 'Too much time'\n            break\n    if early_stop != None:\n        print('Early Stop -', early_stop)\n        break","metadata":{"execution":{"iopub.status.busy":"2022-03-29T15:24:30.820445Z","iopub.execute_input":"2022-03-29T15:24:30.820729Z","iopub.status.idle":"2022-03-29T15:26:25.747816Z","shell.execute_reply.started":"2022-03-29T15:24:30.820694Z","shell.execute_reply":"2022-03-29T15:26:25.746497Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Save model and show chart","metadata":{}},{"cell_type":"code","source":"# save model\nSAVE_BEST_PATH = './best_parameters.pth'\ntorch.save(best['state'], SAVE_BEST_PATH)\nbest_ = deepcopy(best)\nbest_.pop('state')\nprint(best_)","metadata":{"execution":{"iopub.status.busy":"2022-03-29T14:53:39.745013Z","iopub.execute_input":"2022-03-29T14:53:39.745283Z","iopub.status.idle":"2022-03-29T14:53:39.753512Z","shell.execute_reply.started":"2022-03-29T14:53:39.745254Z","shell.execute_reply":"2022-03-29T14:53:39.752829Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Draw chart\nplt.plot(train_losses, label=\"train loss\")\nplt.plot(valid_losses, label=\"valid loss\")\nplt.title(\"Loss\")\nplt.xlabel(\"epoch\")\nplt.axvline(best['epoch'], color='red', linestyle=':')\nplt.axhline(0, color='gray', linestyle=':')\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-29T14:53:39.849279Z","iopub.execute_input":"2022-03-29T14:53:39.850181Z","iopub.status.idle":"2022-03-29T14:53:40.039237Z","shell.execute_reply.started":"2022-03-29T14:53:39.850138Z","shell.execute_reply":"2022-03-29T14:53:40.037875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Draw chart\nplt.plot(train_accs, label=\"train acc\")\nplt.plot(valid_accs, label=\"valid acc\")\nplt.title(\"Accuracy\")\nplt.xlabel(\"epoch\")\nplt.axvline(best['epoch'], color='red', linestyle=':')\nplt.axhline(1.0, color='gray', linestyle=':')\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-29T14:53:40.040976Z","iopub.execute_input":"2022-03-29T14:53:40.041242Z","iopub.status.idle":"2022-03-29T14:53:40.225089Z","shell.execute_reply.started":"2022-03-29T14:53:40.041204Z","shell.execute_reply":"2022-03-29T14:53:40.224483Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(20, 6))\n\nplt.subplot(1, 2, 1)\nplt.plot(train_losses, label=\"train loss\")\nplt.title(\"Train Loss\")\nplt.xlabel(\"epoch\")\nplt.axvline(best['epoch'], color='red', linestyle=':')\nplt.axhline(0, color='gray', linestyle=':')\nplt.legend()\n\nplt.subplot(1, 2, 2)\nplt.plot(valid_losses, label=\"valid loss\")\nplt.title(\"Valid Loss\")\nplt.xlabel(\"epoch\")\nplt.axvline(best['epoch'], color='red', linestyle=':')\nplt.axhline(0, color='gray', linestyle=':')\nplt.legend()\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-29T14:53:40.226381Z","iopub.execute_input":"2022-03-29T14:53:40.226642Z","iopub.status.idle":"2022-03-29T14:53:40.538274Z","shell.execute_reply.started":"2022-03-29T14:53:40.226609Z","shell.execute_reply":"2022-03-29T14:53:40.537624Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"images, labels, preds = last_valid_result\nplt.figure(figsize=(15, 10))\nfor index in range(5 * 5):\n    plt.subplot(5, 5, index + 1)\n    # image = unnormalize(images[index])\n    image = images[index]\n    image = np.asarray(to_pil_image(image.to('cpu')))\n    label = labels[index].to('cpu').numpy().item()\n    guess = preds[index].to('cpu').numpy().item()\n    plt.title('{} [L:{}]'.format(guess, label))\n    plt.imshow(image)\n    plt.axis(\"off\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-29T14:53:40.540408Z","iopub.execute_input":"2022-03-29T14:53:40.540863Z","iopub.status.idle":"2022-03-29T14:53:40.560183Z","shell.execute_reply.started":"2022-03-29T14:53:40.540825Z","shell.execute_reply":"2022-03-29T14:53:40.55899Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Now, prepare to submit\n\nAccording document of competition, submission file format is:\n```\nImageId,Label\n1,0\n2,0\n3,0\n...\n```\n\nso, predict result by trained model and write it down.","metadata":{}},{"cell_type":"code","source":"test_df = pd.read_csv('../input/digit-recognizer/test.csv')\ntest_df","metadata":{"execution":{"iopub.status.busy":"2022-03-29T14:54:02.970179Z","iopub.execute_input":"2022-03-29T14:54:02.970444Z","iopub.status.idle":"2022-03-29T14:54:05.331878Z","shell.execute_reply.started":"2022-03-29T14:54:02.970414Z","shell.execute_reply":"2022-03-29T14:54:05.331146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class SubmitDataset(torch.utils.data.Dataset):\n    def __init__(self, df, transform=None):\n        self.df = df\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        image = np.array(self.df.iloc[idx]).reshape(28, 28) # (28, 28)\n        if self.transform:\n            image = self.transform(image)\n        return image, idx","metadata":{"execution":{"iopub.status.busy":"2022-03-29T14:54:05.333565Z","iopub.execute_input":"2022-03-29T14:54:05.333841Z","iopub.status.idle":"2022-03-29T14:54:05.339414Z","shell.execute_reply.started":"2022-03-29T14:54:05.333806Z","shell.execute_reply":"2022-03-29T14:54:05.338725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df = pd.read_csv('../input/digit-recognizer/test.csv')\ntest_set = SubmitDataset(test_df, transform=transforms.Compose([\n    bin_to_rgb,\n    transforms.RandomSolarize(threshold=200, p=1),\n    normalize,\n]))\ntest_loader = torch.utils.data.DataLoader(test_set, batch_size=64)","metadata":{"execution":{"iopub.status.busy":"2022-03-29T14:54:05.340783Z","iopub.execute_input":"2022-03-29T14:54:05.341287Z","iopub.status.idle":"2022-03-29T14:54:06.713317Z","shell.execute_reply.started":"2022-03-29T14:54:05.341251Z","shell.execute_reply":"2022-03-29T14:54:06.712591Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rows = 4\ncols = 8\nimages, indices = next(iter(test_loader))\n\nfig, ax = plt.subplots(rows, cols, figsize=(12, 6))\nfig.suptitle('test set')\nax = ax.ravel()\nfor i in range(rows * cols):\n    image = images[i]\n    index = indices[i]\n    ax[i].imshow(np.asarray(to_pil_image(image)))\n    ax[i].set_title(f'id={index}')\n    ax[i].axis(\"off\")\nplt.subplots_adjust(hspace=0.5)","metadata":{"execution":{"iopub.status.busy":"2022-03-29T14:54:06.716263Z","iopub.execute_input":"2022-03-29T14:54:06.716793Z","iopub.status.idle":"2022-03-29T14:54:07.864154Z","shell.execute_reply.started":"2022-03-29T14:54:06.71676Z","shell.execute_reply":"2022-03-29T14:54:07.863482Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Load model which getting the best during train according by accuracy.","metadata":{}},{"cell_type":"code","source":"best_model = MyConvNet().to(device)\nbest_model.load_state_dict(torch.load(SAVE_BEST_PATH))","metadata":{"execution":{"iopub.status.busy":"2022-03-29T14:54:07.865312Z","iopub.execute_input":"2022-03-29T14:54:07.865866Z","iopub.status.idle":"2022-03-29T14:54:08.360613Z","shell.execute_reply.started":"2022-03-29T14:54:07.865827Z","shell.execute_reply":"2022-03-29T14:54:08.359526Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result_df = pd.DataFrame(columns=['ImageId', 'Label'])\n\nmodel.eval()\nwith torch.no_grad():\n    for images, indices in tqdm_nb(test_loader):\n        images, indices = images.to(device), indices.to(device)\n        outputs = model(images).to(device)\n        pred = torch.argmax(outputs.data, dim=1)\n        last_test_result = (images, pred.to(device))\n        \n        indices = indices.to('cpu').numpy() + 1\n        pred = pred.to('cpu').numpy()\n        pred_df = pd.DataFrame({'ImageId': indices, 'Label': pred})\n        result_df = result_df.append(pred_df)\n\nresult_df.to_csv('submission.csv', index=False)\n\nresult_df","metadata":{"execution":{"iopub.status.busy":"2022-03-29T14:54:17.727818Z","iopub.execute_input":"2022-03-29T14:54:17.728552Z","iopub.status.idle":"2022-03-29T14:54:29.580136Z","shell.execute_reply.started":"2022-03-29T14:54:17.728514Z","shell.execute_reply":"2022-03-29T14:54:29.579485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"images, preds = last_test_result\nplt.figure(figsize=(15, 10))\nfor index in range(5 * 5):\n    plt.subplot(5, 5, index + 1)\n    image = unnormalize(images[index])\n    image = images[index]\n    image = np.asarray(to_pil_image(image.to('cpu')))\n    guess = preds[index].to('cpu').numpy().item()\n    plt.title(guess)\n    plt.imshow(image)\n    plt.axis(\"off\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-29T14:54:29.581797Z","iopub.execute_input":"2022-03-29T14:54:29.582044Z","iopub.status.idle":"2022-03-29T14:54:30.360386Z","shell.execute_reply.started":"2022-03-29T14:54:29.582011Z","shell.execute_reply":"2022-03-29T14:54:30.359722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# How about hard cases\n\nwhich images failed to predict?","metadata":{}},{"cell_type":"code","source":"hard_cases = [5, 365, 898, 1908, 1973, 2190, 2591, 2612, 2650, 3291, 3568, 4241, 4775, 7373, 7978, 9882, 10016, 12584, 12839, 15064, 15302, 15991, 18016, 20774, 21542, 26020, 26289, 26775, 26815, 27275]\nhard_cases = random.sample(hard_cases, 30)","metadata":{"execution":{"iopub.status.busy":"2022-03-29T15:34:22.809911Z","iopub.execute_input":"2022-03-29T15:34:22.810176Z","iopub.status.idle":"2022-03-29T15:34:22.815207Z","shell.execute_reply.started":"2022-03-29T15:34:22.81015Z","shell.execute_reply":"2022-03-29T15:34:22.814326Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(3, 10, figsize=(12, 5))\nax = ax.ravel()\nidx = 0\nfor i, row in result_df[result_df['ImageId'].isin(hard_cases)].iterrows():\n    image_id, guess = row['ImageId'], row['Label']\n    image_tensor, _ = test_set[image_id]\n    # print(image_id, image_tensor.shape, guess)\n    ax[idx].imshow(np.asarray(to_pil_image(image_tensor)))\n    ax[idx].set_title(f'id={image_id}\\n{guess}')\n    ax[idx].axis(\"off\")\n    idx += 1\nplt.subplots_adjust(hspace=0.8)","metadata":{"execution":{"iopub.status.busy":"2022-03-29T15:35:39.306779Z","iopub.execute_input":"2022-03-29T15:35:39.307324Z","iopub.status.idle":"2022-03-29T15:35:40.244034Z","shell.execute_reply.started":"2022-03-29T15:35:39.307284Z","shell.execute_reply":"2022-03-29T15:35:40.243327Z"},"trusted":true},"execution_count":null,"outputs":[]}]}