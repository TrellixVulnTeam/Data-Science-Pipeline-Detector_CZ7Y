{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# PCA","metadata":{}},{"cell_type":"markdown","source":"PCA or Pricipal component Analysis is a technique used for dimensionality reduction , It projects each data point onto only the first few principal components to obtain lower-dimensional data while preserving as much of the data's variation as possible. It does so by creating new uncorrelated variables that successively maximize variance. ","metadata":{}},{"cell_type":"markdown","source":"## what is dimenionality reduction ?","metadata":{}},{"cell_type":"markdown","source":"Dimensionality reduction, or dimension reduction, is the transformation of data from a high-dimensional space into a low-dimensional space so that the low-dimensional representation retains some meaningful properties of the original data","metadata":{}},{"cell_type":"markdown","source":"## why do we need to dimensionality reduction ?","metadata":{}},{"cell_type":"markdown","source":"- High Dimensional data with too many features is hard and longer to process\n- Most of the time many feature are co-related eg - humidity and rainfall , therefore processing them independently is redundant\n- Many Machine Learning simply breaks down when working with high dimentional data. This Phenomenon is commonly refered to as **Curse of dimensionality**","metadata":{}},{"cell_type":"markdown","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy.linalg import eigh\nfrom sklearn import decomposition\n\n\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls","metadata":{"execution":{"iopub.status.busy":"2021-10-29T09:54:06.489159Z","iopub.execute_input":"2021-10-29T09:54:06.489794Z","iopub.status.idle":"2021-10-29T09:54:06.49697Z","shell.execute_reply.started":"2021-10-29T09:54:06.489753Z","shell.execute_reply":"2021-10-29T09:54:06.496354Z"}}},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy.linalg import eigh\nfrom sklearn import decomposition","metadata":{"execution":{"iopub.status.busy":"2021-10-29T10:06:09.025967Z","iopub.execute_input":"2021-10-29T10:06:09.02721Z","iopub.status.idle":"2021-10-29T10:06:09.033134Z","shell.execute_reply.started":"2021-10-29T10:06:09.027151Z","shell.execute_reply":"2021-10-29T10:06:09.031899Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv(\"../input/digit-recognizer/train.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-10-29T10:06:09.035799Z","iopub.execute_input":"2021-10-29T10:06:09.036146Z","iopub.status.idle":"2021-10-29T10:06:12.73362Z","shell.execute_reply.started":"2021-10-29T10:06:09.036105Z","shell.execute_reply":"2021-10-29T10:06:12.732827Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-29T10:06:12.735281Z","iopub.execute_input":"2021-10-29T10:06:12.735583Z","iopub.status.idle":"2021-10-29T10:06:12.764601Z","shell.execute_reply.started":"2021-10-29T10:06:12.735544Z","shell.execute_reply":"2021-10-29T10:06:12.763808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"label = train_df.label\ntrain = train_df.drop('label', axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-10-29T10:06:12.766002Z","iopub.execute_input":"2021-10-29T10:06:12.766233Z","iopub.status.idle":"2021-10-29T10:06:12.87992Z","shell.execute_reply.started":"2021-10-29T10:06:12.766207Z","shell.execute_reply":"2021-10-29T10:06:12.879071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(label.shape)\nprint(train.shape)","metadata":{"execution":{"iopub.status.busy":"2021-10-29T10:06:12.882545Z","iopub.execute_input":"2021-10-29T10:06:12.882833Z","iopub.status.idle":"2021-10-29T10:06:12.894368Z","shell.execute_reply.started":"2021-10-29T10:06:12.882806Z","shell.execute_reply":"2021-10-29T10:06:12.893477Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"label[0]","metadata":{"execution":{"iopub.status.busy":"2021-10-29T10:06:12.89555Z","iopub.execute_input":"2021-10-29T10:06:12.895911Z","iopub.status.idle":"2021-10-29T10:06:12.906596Z","shell.execute_reply.started":"2021-10-29T10:06:12.895871Z","shell.execute_reply":"2021-10-29T10:06:12.905744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-29T10:06:12.907761Z","iopub.execute_input":"2021-10-29T10:06:12.908015Z","iopub.status.idle":"2021-10-29T10:06:12.931751Z","shell.execute_reply.started":"2021-10-29T10:06:12.90798Z","shell.execute_reply":"2021-10-29T10:06:12.930929Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(2,2))\n# reshape d from 1d to 2d pixel array for given idx ( prefer 28 X 28)\ngrid_data = train.loc[0].values.reshape(28,28)\n#plot above grid image with cmap as gray and interpoltion as none\nplt.imshow(grid_data,interpolation='none',cmap='gray')\n\n#display plot\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-29T10:06:12.933236Z","iopub.execute_input":"2021-10-29T10:06:12.933551Z","iopub.status.idle":"2021-10-29T10:06:13.127616Z","shell.execute_reply.started":"2021-10-29T10:06:12.933508Z","shell.execute_reply":"2021-10-29T10:06:13.126619Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Preprocessing ","metadata":{}},{"cell_type":"code","source":"standardized_data = StandardScaler().fit_transform(train)\nstandardized_data.shape","metadata":{"execution":{"iopub.status.busy":"2021-10-29T10:06:13.129139Z","iopub.execute_input":"2021-10-29T10:06:13.129458Z","iopub.status.idle":"2021-10-29T10:06:13.986822Z","shell.execute_reply.started":"2021-10-29T10:06:13.129417Z","shell.execute_reply":"2021-10-29T10:06:13.985796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# PCA Implementation ","metadata":{}},{"cell_type":"markdown","source":"to implement pca we need two things\n1. Co-variance Matrix\n2. Eigen Vectors and Eigen Values\n3. Projection onto 2D Plane","metadata":{}},{"cell_type":"markdown","source":"### 1. Co-variance Matrix\n\nCovariance Matrix basically describe the variance of the data","metadata":{"tags":[]}},{"cell_type":"markdown","source":"Equation : \n\n$\\mathbf{S} = \\mathbf{A}^\\intercal  \\mathbf{A}$\n","metadata":{}},{"cell_type":"code","source":"covar_matrix = np.matmul(standardized_data.T,standardized_data)\ncovar_matrix","metadata":{"execution":{"iopub.status.busy":"2021-10-29T10:06:13.988279Z","iopub.execute_input":"2021-10-29T10:06:13.98858Z","iopub.status.idle":"2021-10-29T10:06:14.490241Z","shell.execute_reply.started":"2021-10-29T10:06:13.988539Z","shell.execute_reply":"2021-10-29T10:06:14.489305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2. Eigen Vectors and Eigen Values \n   \n   The eigenvectors (principal components) determine the directions of the new feature space, and the eigenvalues determine their magnitude","metadata":{}},{"cell_type":"markdown","source":"Basic Equation is :               \n\n$\\mathbf{S}\\mu = \\lambda \\mu $\n\nwhere $\\lambda$ is eigen value, $\\mathbf{S}$ is Co-variance matrix and $\\mu $ is eigen vectors","metadata":{}},{"cell_type":"code","source":"# since we need to project (42000 X 784) to (42000 X 2). Therefore we need to select top 2 eigen values\nvalues, vectors = eigh(covar_matrix,eigvals=(782,783))\n\nprint(\"Shape of eigen vectors = \",vectors.shape)\nprint(vectors)\n\n# converting the eigen vectors into (2d) shape \nvectors = vectors.T\nprint(\"Updated shape of eigen vectors = \",vectors.shape)\nprint(vectors)\n# here the vectors[1] represent the eigen vector corresponding 1st principal eigen vector\n# here the vectors[0] represent the eigen vector corresponding 2nd principal eigen vector","metadata":{"execution":{"iopub.status.busy":"2021-10-29T10:06:14.491902Z","iopub.execute_input":"2021-10-29T10:06:14.492796Z","iopub.status.idle":"2021-10-29T10:06:14.569975Z","shell.execute_reply.started":"2021-10-29T10:06:14.492743Z","shell.execute_reply":"2021-10-29T10:06:14.569047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Projecting onto 2D Plane","metadata":{}},{"cell_type":"code","source":"new_coord =  np.matmul(vectors, standardized_data.T)\nprint(new_coord)\nprint(new_coord.shape)","metadata":{"execution":{"iopub.status.busy":"2021-10-29T10:06:14.576073Z","iopub.execute_input":"2021-10-29T10:06:14.579186Z","iopub.status.idle":"2021-10-29T10:06:14.626364Z","shell.execute_reply.started":"2021-10-29T10:06:14.579118Z","shell.execute_reply":"2021-10-29T10:06:14.625427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pca_data = pd.DataFrame({\"1st_principal\" : new_coord[1]\n                         ,\"2nd_principal\" : new_coord[0], \"label\" : label})","metadata":{"execution":{"iopub.status.busy":"2021-10-29T10:06:14.631911Z","iopub.execute_input":"2021-10-29T10:06:14.634636Z","iopub.status.idle":"2021-10-29T10:06:14.644132Z","shell.execute_reply.started":"2021-10-29T10:06:14.634578Z","shell.execute_reply":"2021-10-29T10:06:14.64312Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pca_data","metadata":{"execution":{"iopub.status.busy":"2021-10-29T10:06:14.653405Z","iopub.execute_input":"2021-10-29T10:06:14.656277Z","iopub.status.idle":"2021-10-29T10:06:14.682282Z","shell.execute_reply.started":"2021-10-29T10:06:14.656216Z","shell.execute_reply":"2021-10-29T10:06:14.681326Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.FacetGrid(pca_data, hue='label', height=8).map(plt.scatter, \"1st_principal\", \"2nd_principal\", 'label').add_legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-29T10:06:14.688021Z","iopub.execute_input":"2021-10-29T10:06:14.690771Z","iopub.status.idle":"2021-10-29T10:06:16.800816Z","shell.execute_reply.started":"2021-10-29T10:06:14.690711Z","shell.execute_reply":"2021-10-29T10:06:16.800027Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# PCA Implementation using Scikit-Learn","metadata":{}},{"cell_type":"code","source":"pca = decomposition.PCA()\npca.n_components = 2\npca_data_sci = pca.fit_transform(standardized_data)\npca_data_sci.shape","metadata":{"execution":{"iopub.status.busy":"2021-10-29T10:06:16.802235Z","iopub.execute_input":"2021-10-29T10:06:16.802766Z","iopub.status.idle":"2021-10-29T10:06:18.746005Z","shell.execute_reply.started":"2021-10-29T10:06:16.802724Z","shell.execute_reply":"2021-10-29T10:06:18.745181Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pca_data_sci_new = pd.DataFrame({\"1st_principal\" : pca_data_sci.T[0]\n                         , \"2nd_principal\" : pca_data_sci.T[1], \"label\" : label})","metadata":{"execution":{"iopub.status.busy":"2021-10-29T10:06:18.74815Z","iopub.execute_input":"2021-10-29T10:06:18.748495Z","iopub.status.idle":"2021-10-29T10:06:18.755326Z","shell.execute_reply.started":"2021-10-29T10:06:18.748443Z","shell.execute_reply":"2021-10-29T10:06:18.754476Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pca_data_sci_new","metadata":{"execution":{"iopub.status.busy":"2021-10-29T10:06:18.756863Z","iopub.execute_input":"2021-10-29T10:06:18.757434Z","iopub.status.idle":"2021-10-29T10:06:18.776712Z","shell.execute_reply.started":"2021-10-29T10:06:18.757385Z","shell.execute_reply":"2021-10-29T10:06:18.775718Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.FacetGrid(pca_data_sci_new, hue='label',height=8).map(plt.scatter, \"1st_principal\", \"2nd_principal\", 'label').add_legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-29T10:06:18.778027Z","iopub.execute_input":"2021-10-29T10:06:18.778304Z","iopub.status.idle":"2021-10-29T10:06:20.962794Z","shell.execute_reply.started":"2021-10-29T10:06:18.778272Z","shell.execute_reply":"2021-10-29T10:06:20.961912Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Variance Explained by PCA","metadata":{}},{"cell_type":"code","source":"pca.n_components = 784\n\npca_data = pca.fit_transform(standardized_data)\n\npercentage_var_explained = pca.explained_variance_ / np.sum(pca.explained_variance_)\n\n#cumulative sum of the percentage_var_explained\ncumulative_explained_variance = np.cumsum(percentage_var_explained)","metadata":{"execution":{"iopub.status.busy":"2021-10-29T10:06:20.963894Z","iopub.execute_input":"2021-10-29T10:06:20.964211Z","iopub.status.idle":"2021-10-29T10:06:26.378294Z","shell.execute_reply.started":"2021-10-29T10:06:20.964173Z","shell.execute_reply":"2021-10-29T10:06:26.377295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(6,4))\nplt.plot(cumulative_explained_variance,linewidth=3)\nplt.grid()\n\nplt.xlabel('n_components')\nplt.ylabel('Cumulative_explained_variance')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-29T10:06:26.379893Z","iopub.execute_input":"2021-10-29T10:06:26.380208Z","iopub.status.idle":"2021-10-29T10:06:26.593907Z","shell.execute_reply.started":"2021-10-29T10:06:26.380168Z","shell.execute_reply":"2021-10-29T10:06:26.593086Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"the above grpah shows that by choosing 200 n_components or principals we can get a variance of around 90% \nthus instead of working with all 784 dimensions we can work with around 200 dimensions without any major loss in information","metadata":{}}]}