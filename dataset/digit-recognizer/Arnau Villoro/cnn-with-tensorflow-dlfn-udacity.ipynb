{"metadata":{"kernelspec":{"language":"python","name":"python3","display_name":"Python 3"},"language_info":{"nbconvert_exporter":"python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","file_extension":".py","mimetype":"text/x-python","name":"python","version":"3.6.0"}},"nbformat":4,"cells":[{"metadata":{"_uuid":"e460b453c6c86f5841123378e8e925de983873a0"},"source":"# Digit Classification","cell_type":"markdown","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"96ef11e8dc0de1b6bd12ef6c2c1ffbc5f6ed298a"},"source":"import pandas as pd, numpy as np, tensorflow as tf, sys\n\nfrom sklearn.preprocessing import LabelBinarizer\n\nimport matplotlib.pyplot as plt, matplotlib.cm as cm\n%matplotlib inline\n\nimport plotly.offline as py, plotly.graph_objs as go\npy.init_notebook_mode()","cell_type":"code","outputs":[],"execution_count":1},{"metadata":{"collapsed":true,"_uuid":"2a85cfd7c60cff7aae2aae0cbaeb914a40513a7e"},"source":"## Basic exploration\n* Load train and test data\n* Find target column/columns and extract them from train data\n* Show num of features and num of entries in every dataset","cell_type":"markdown","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d9118905836c3d232f09f51a8fea28c76050fea6"},"source":"df_train = pd.read_csv('../input/train.csv') # Index is missing!\ndf_test = pd.read_csv('../input/test.csv')\n\ncolumn_target = [x for x in df_train.columns if x not in df_test.columns]\nlabels = df_train[column_target]\n\ndf_train.drop(column_target, axis=1, inplace=True)\n\nprint(\"Train shape: {}. Test shape: {}. Target shape {}\".format(df_train.shape, df_test.shape, labels.shape))\ndf_train.head()","cell_type":"code","outputs":[],"execution_count":2},{"metadata":{"_uuid":"0ff503674be6858eec298216d089533d7cd97263"},"source":"image_size = df_train.shape[1]\nprint(\"Number of pixels for each image: {}\".format(image_size))\nprint(\"Pixels ranges from {} to {}\".format(df_train.values.min(), df_train.values.max()))\n\nimage_width = image_height = np.ceil(np.sqrt(image_size)).astype(np.uint8)\nprint(\"Image size: {}x{}\".format(image_width, image_height))\n\nnum_colors = 1 # There are in black and white","cell_type":"code","outputs":[],"execution_count":3},{"metadata":{"_uuid":"74744ed792fe2488d35635a7f9e31be1611a6a46"},"source":"def display_image_with_label(index):\n    \"\"\" Show and image and it's label \"\"\"\n    \n    image = df_train.ix[index, :].values\n    label = labels.values[index]\n    \n    plt.axis('off')\n    plt.imshow(image.reshape(image_width, image_height), cmap=cm.binary)\n    print(\"It is a {}\".format(label))\n    \ndisplay_image_with_label(42)","cell_type":"code","outputs":[],"execution_count":4},{"metadata":{"_uuid":"02e89c52fac446a50e2b076af6203351602b2ad1"},"source":"## Preprocess \n#### Normalize\nFor deep learning is a good practice to normalize inputs. Usually we will substract the mean and divide by the standard deviation for each column. But for images it is more common to use global min/max\n#### One hot encode\nWe don't want the label itself, we want a matrix with all posible labels with all 0 except the correct one.\n\nFor example:\n* 0 => [1 0 0 0 0 0 0 0 0 0]\n* 1 => [0 1 0 0 0 0 0 0 0 0]\n* ...\n* 9 => [0 0 0 0 0 0 0 0 0 1]","cell_type":"markdown","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"aefe25c1f53271da857a4c759ec6822c3220a161"},"source":"def normalize(x):\n    \"\"\" Normalize a list of sample image data in the range of 0 to 1 \"\"\"\n    \n    return (x-x.values.min())/(x.values.max()-x.values.min())\n\ndef one_hot_encode(x):\n    \"\"\" One hot encode a list of sample labels. Return a one-hot encoded vector for each label. \"\"\"\n    \n    # check if encoder has been previously created, if not make a global var an initialize it\n    if 'encoder' not in globals():\n        global encoder\n        encoder = LabelBinarizer()\n        encoder.fit(range(10))\n    \n    return encoder.transform(x)\n\ndf_train = normalize(df_train)\ndf_test = normalize(df_test)\nlabels = one_hot_encode(labels)\n\nnum_labels = labels.shape[1]","cell_type":"code","outputs":[],"execution_count":6},{"metadata":{"_uuid":"29e4cb62ebf9f10e8e86185a4a6c9a90959f1473"},"source":"### Split dataset in train, test, val\n* Train --> for training the model\n* Val --> to check accuracy achived and prevent overfitting\n* Test --> final data where to perfome predictions\n\nWe will separate 10% of the training data to create the validation.","cell_type":"markdown","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5ca0db94a83f09ec034f93d92ec67b1eb725be8d"},"source":"cut_index = int(df_train.shape[0] * 0.1) # Since data is randomly distributed we can use that\n\nx_train, y_train = df_train.head(-cut_index), labels[:-cut_index]\nx_val, y_val = df_train.tail(cut_index), labels[-cut_index:]\n\nx_test = df_test\n\nprint(\"x_train: {}, y_train: {}.\".format(x_train.shape, y_train.shape))\nprint(\"x_val: {}, y_val: {}.\".format(x_val.shape, y_val.shape))\nprint(\"x_test: {}\".format(x_test.shape))","cell_type":"code","outputs":[],"execution_count":7},{"metadata":{"_uuid":"10a623d32996079cc2980d47268af9c7580d2715"},"source":"## Build the Neural Network\n\n### Auxiliar functions for the neural network\n#### Get_batch\nIt will split the whole train dataset in smaller groups of images. This increases the training performance. We use yield instead of return to minimize the preprocessing time\n\n#### Displayer class\nIt will be use to show how the neural network is learning","cell_type":"markdown","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"2920d44073bcebc8e600af16a0be2a5d3350f6b6"},"source":"def get_batch(x, y, batch_size):\n    \"\"\" Send smaller groups of images \"\"\"\n    \n    n_batches = x.shape[0]//batch_size\n    \n    for step in range(0, n_batches):\n        idx_low, idx_high = (step*batch_size, (step+1)*batch_size)\n        yield x[idx_low:idx_high], y[idx_low:idx_high]\n    \n    # If there is more data, yield the remaining\n    if (step + 1)*batch_size + 1 < x.shape[0]:\n        idx_low, idx_high = (idx_high, x.shape[0])\n        yield x[idx_low:idx_high], y[idx_low:idx_high]\n        \nclass Displayer():\n\n    def __init__(self):\n        self.accuracies = {'train':[], 'validation':[]}\n        \n    def show_stats(self, e, epochs, accuracy_train, accuracy_val):\n\n        sys.stdout.write(\"\\rProgress: {:.2f}%  \\tTrain accuracy: {:.4f}\\t\\tValidation accuracy: {:.4f}\\t   \".format(\n                                100 * e/float(epochs), accuracy_train, accuracy_val))\n        \n        # This will be useful when plotting for knowing in which epoch we are\n        self.accuracies['train'].append(accuracy_train)\n        self.accuracies['validation'].append(accuracy_val)","cell_type":"code","outputs":[],"execution_count":8},{"metadata":{"_uuid":"d0db94e7441539bda489d7bd66683a76f8fefb5b"},"source":"### Define NN architecture","cell_type":"markdown","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"4137b36937781be1d3d5c610cf83c832bf6952f1"},"source":"tf.reset_default_graph()\n\nalpha = 0.1\n\nx = tf.placeholder(tf.float32, [None, image_size], name='x')\ny = tf.placeholder(tf.float32, [None, num_labels], name='y')\nlearning_rate = tf.placeholder(tf.float32, name='learning_rate')\nkeep_prob = tf.placeholder(tf.float32, name='keep_prob')\n\nimages = tf.reshape(x, [-1, image_width, image_height, num_colors])\n\nnn = tf.layers.conv2d(images, 6, 5, padding='valid', kernel_initializer=tf.contrib.layers.xavier_initializer())\nnn = tf.nn.avg_pool(nn, [1,2,2,1], [1,2,2,1], padding='VALID')\n\nnn = tf.layers.conv2d(nn, 16, 5, padding='valid', kernel_initializer=tf.contrib.layers.xavier_initializer())\nnn = tf.nn.avg_pool(nn, [1,2,2,1], [1,2,2,1], padding='VALID')\n\nnn = tf.contrib.layers.flatten(nn)\n\nnn = tf.layers.dense(nn, 120, kernel_initializer=tf.contrib.layers.xavier_initializer())\nnn = tf.layers.batch_normalization(nn)\nnn = tf.maximum(nn, nn*alpha)\nnn = tf.nn.dropout(nn, keep_prob)\n\nnn = tf.layers.dense(nn, 84, kernel_initializer=tf.contrib.layers.xavier_initializer())\nnn = tf.layers.batch_normalization(nn)\nnn = tf.maximum(nn, nn*alpha)\nnn = tf.nn.dropout(nn, keep_prob)\n\nlogits = tf.layers.dense(nn, 10, kernel_initializer=tf.contrib.layers.xavier_initializer(), name=\"logits\")\n\ncost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\noptimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n\ncorrect_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\naccuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n\npred = tf.argmax(logits, 1)","cell_type":"code","outputs":[],"execution_count":9},{"metadata":{"_uuid":"e143167bea7a3e9b04d387b1d49a2829a54cfe7c"},"source":"## Train the Neural Network","cell_type":"markdown","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"7e28cbf1fd06328a1a29e9ff14f241b3ae773c42"},"source":"def train(epochs, batch_size, keep_probability, lr):\n    \n    disp = Displayer()\n\n    with tf.Session() as sess:\n        sess.run(tf.global_variables_initializer())\n\n        for e in range(epochs + 1):\n\n            for x_batch, y_batch in get_batch(df_train, labels, batch_size):  \n                sess.run(optimizer, feed_dict={x: x_batch, y: y_batch, keep_prob: keep_probability, learning_rate:lr})\n\n            accuracy_train = accuracy.eval({x: x_batch, y: y_batch, keep_prob: 1.0})\n            accuracy_val = accuracy.eval({x: x_val, y: y_val, keep_prob: 1.0})\n\n            disp.show_stats(e, epochs, accuracy_train, accuracy_val)\n            \n        output = pred.eval({x: x_test, keep_prob: 1.0})\n                     \n    data = [go.Scatter(y=disp.accuracies[c], name=c) for c in disp.accuracies]\n    py.iplot(data, show_link=False)\n    \n    return output","cell_type":"code","outputs":[],"execution_count":10},{"metadata":{"_uuid":"b240a034c43f781c47a3d925c0e0915e7aa15cfc"},"source":"epochs = 50\nbatch_size = 1024\nkeep_probability = 0.5\nlr = 0.001\n\npredictions = train(epochs, batch_size, keep_probability, lr)","cell_type":"code","outputs":[],"execution_count":11},{"metadata":{"_uuid":"e6a487ee6c05692370ad5e3447565b402de01a83"},"source":"## Export and view predictions for the test data","cell_type":"markdown","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1581eca8dc5a4c5943b0ae7bed9093c1daca6445"},"source":"df_out = pd.DataFrame(predictions)\ndf_out.index = [x + 1 for x in df_out.index]\ndf_out.columns = column_target\ndf_out.index.name = \"ImageId\"\n\ndf_out.to_csv(\"predictions.csv\")\ndf_out.head()","cell_type":"code","outputs":[],"execution_count":19},{"metadata":{"_uuid":"e295c7281b1f28c91fc14357beb4d2f3627de4cf"},"source":"for index in range(5):\n    plt.figure()\n    image = df_test.ix[index,:].values\n    plt.imshow(image.reshape(image_width, image_height), cmap=cm.binary)","cell_type":"code","outputs":[],"execution_count":14},{"metadata":{"collapsed":true,"_uuid":"af852920b027e917c06c69c91f0308d66c8d89ee"},"source":"","cell_type":"code","outputs":[],"execution_count":null}],"nbformat_minor":1}