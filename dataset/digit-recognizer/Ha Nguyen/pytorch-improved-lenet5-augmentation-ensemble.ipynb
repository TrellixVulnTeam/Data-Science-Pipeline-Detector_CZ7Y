{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Input data files are available in the \"../input/\" directory.\n# Any results you write to the current directory are saved as output.\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Import libraries\n%matplotlib inline\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nimport time\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset, ConcatDataset\nfrom torchvision import transforms\nimport torch.optim as optim","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Import Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Read csv files\ntrain_df = pd.read_csv('/kaggle/input/digit-recognizer/train.csv')\ntest_df = pd.read_csv('/kaggle/input/digit-recognizer/test.csv')\nsubmit_df = pd.read_csv('/kaggle/input/digit-recognizer/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Training data\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Number of pixels\nnum_pixel = len(train_df.columns) - 1\nnum_pixel","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Argumentation, Datasets and DataLoaders"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Transformers\ntransform_0 = transforms.Compose([\n    transforms.ToPILImage(),\n    transforms.ToTensor(),\n    transforms.Normalize([0.5], [0.5])\n])\n\ntransform_1 = transforms.Compose([\n    transforms.ToPILImage(),\n    transforms.RandomRotation(30),\n    transforms.ToTensor(),\n    transforms.Normalize([0.5], [0.5])\n])\n\ntransform_2 = transforms.Compose([\n    transforms.ToPILImage(),\n    transforms.RandomAffine(degrees=15, translate=(0.1,0.1), scale=(0.8,0.8)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.5], [0.5])\n])\n\ntransform_3 = transforms.Compose([\n    transforms.ToPILImage(),\n    transforms.RandomAffine(degrees=30, scale=(1.1,1.1)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.5], [0.5])\n])\n\ntransform_4 = transforms.Compose([\n    transforms.ToPILImage(),\n    transforms.RandomAffine(degrees=30, translate=(0.1,0.1)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.5], [0.5])\n])\n\ntransform_5 = transforms.Compose([\n    transforms.ToPILImage(),\n    transforms.RandomAffine(degrees=10, shear=45),\n    transforms.ToTensor(),\n    transforms.Normalize([0.5], [0.5])\n])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Write a class that transform a DataFrame to PyTorch Dataset\n# Your custom dataset should inherit Dataset and override the following methods:\n    # __len__ so that len(dataset) returns the size of the dataset.\n    # __getitem__ to support the indexing such that dataset[i] can be used to get i\n\nclass DataFrame_to_Dataset(Dataset):\n    \n    def __init__(self, df, transform=transform_0):\n\n        # Get features and labels\n        if len(df.columns) == num_pixel:\n            # Test dataset\n            self.features = df.values.reshape((-1,28,28)).astype(np.uint8) # .astype(np.uint8) for ToPILImage transformer\n            self.labels = None\n        else:\n            # Train dataset\n            self.features = df.iloc[:,1:].values.reshape((-1,28,28)).astype(np.uint8)\n            self.labels = torch.from_numpy(df.label.values)\n        \n        # Transformer\n        self.transform = transform\n    \n    def __len__(self):\n        return len(self.features)\n    \n    def __getitem__(self, index):\n            \n        if self.labels is not None:\n            return self.transform(self.features[index]), self.labels[index]\n        else:\n            return self.transform(self.features[index])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_dataloaders(seed, test_size=0.1, df=train_df, batch_size=32):\n    # Create training set and validation set\n    train_data, valid_data = train_test_split(df,\n                                              test_size=test_size,\n                                              random_state=seed)\n    \n    # Create Datasets\n    train_dataset_0 = DataFrame_to_Dataset(train_data)\n    train_dataset_1 = DataFrame_to_Dataset(train_data, transform_1)\n    train_dataset_2 = DataFrame_to_Dataset(train_data, transform_2)\n    train_dataset_3 = DataFrame_to_Dataset(train_data, transform_3)\n    train_dataset_4 = DataFrame_to_Dataset(train_data, transform_4)\n    train_dataset_5 = DataFrame_to_Dataset(train_data, transform_5)\n    train_dataset = ConcatDataset([train_dataset_0, train_dataset_1, train_dataset_2, train_dataset_3, train_dataset_4, train_dataset_5])\n\n    valid_dataset = DataFrame_to_Dataset(valid_data)\n    \n    # Create Dataloaders\n    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n\n    return train_loader, valid_loader","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# LeNet5 with Improvements\n\n- Two stacked 3x3 filters replace the single 5x5 filters.\n- A convolution with stride 2 replaces pooling layers. These become learnable pooling layers.\n- Batch normalization is added\n- Dropout is added\n- More feature maps (channels) are added\n\n![](https://pytorch.org/tutorials/_images/mnist.png)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a LeNet neural network\n\nclass Net(nn.Module):\n    def __init__(self):\n        # Super function. It inherits from nn.Module and we can access everythink in nn.Module\n        super().__init__()\n        \n        self.conv1 = nn.Sequential(\n            nn.Conv2d(1, 32, kernel_size=3),\n            nn.BatchNorm2d(32),\n            nn.ReLU(inplace=True), # inplace=True helps to save some memory\n            \n            nn.Conv2d(32, 32, kernel_size=3),\n            nn.BatchNorm2d(32),\n            nn.ReLU(inplace=True),\n            \n            nn.Conv2d(32, 32, kernel_size=5, stride=2, padding=14), # compute same padding by hand\n            nn.BatchNorm2d(32),\n            nn.ReLU(inplace=True),\n            \n            nn.MaxPool2d(2, 2),\n            nn.Dropout2d(0.25)\n        )\n        \n        self.conv2 = nn.Sequential(\n            nn.Conv2d(32, 64, kernel_size=3),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True), # inplace=True helps to save some memory\n            \n            nn.Conv2d(64, 64, kernel_size=3),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True),\n            \n            nn.Conv2d(64, 64, kernel_size=5, stride=2, padding=6), # compute same padding by hand\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True),\n            \n            nn.MaxPool2d(2, 2),\n            nn.Dropout2d(0.25)\n        )\n        \n        self.conv3 = nn.Sequential(\n            nn.Conv2d(64, 128, kernel_size=4),\n            nn.BatchNorm2d(128),\n            nn.ReLU(inplace=True),\n            nn.Dropout2d(0.25)\n        )\n        \n        self.fc = nn.Sequential(\n            nn.Linear(128*1*1, 10)\n        )\n        \n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.conv2(x)\n        x = self.conv3(x)\n        x = x.view(-1, 128*1*1)\n        x = self.fc(x)\n        \n        return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get the device\nuse_cuda = torch.cuda.is_available()\nprint(use_cuda)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train(seed, num_epochs):\n    \n    # Train and valid dataloaders\n    print('Creating new dataloaders...')\n    train_loader, valid_loader = create_dataloaders(seed=seed)\n\n    # Model\n    print('Creating a new model...')\n    net = Net()\n\n    # Loss function\n    criterion = nn.CrossEntropyLoss()\n\n    # Move to GPU\n    if use_cuda:\n        net.cuda()\n        criterion.cuda()\n\n    # Optimizer\n    optimizer = optim.Adam(net.parameters(), \n                           lr=0.003, betas=(0.9, 0.999), \n                           eps=1e-08, weight_decay=0, \n                           amsgrad=False)\n\n    # Sets the learning rate of each parameter group to the initial lr decayed by gamma every step_size epochs\n    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n\n    print('Training the model...')\n    for epoch in range(num_epochs):\n        \n        net.train() # Training mode -> Turn on dropout\n        t0 = time.time()\n        training_loss = 0.0\n        num_samples = 0\n        \n        for features, labels in train_loader:\n            \n            # Move data to GPU\n            if use_cuda:\n                features = features.cuda()\n                labels = labels.cuda()\n                \n            # Zero the parameter gradients\n            optimizer.zero_grad()\n            \n            # Forward + Backward + Optimize\n            outputs = net(features)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            \n            # Update loss\n            training_loss += loss.item()\n            num_samples += len(features)\n            \n        # Compute accuracy on validation set\n        net.eval() # Evaluation mode -> Turn off dropout\n        correct = 0\n        total = 0\n        with torch.no_grad():\n            for valid_features, valid_labels in valid_loader:\n                \n                # Move data to GPU\n                if use_cuda:\n                    valid_features = valid_features.cuda()\n                    valid_labels = valid_labels.cuda()\n\n                outputs = net(valid_features)\n                _, predicted = torch.max(outputs, 1)\n                total += valid_labels.size(0)\n                correct += (predicted == valid_labels).sum().item()\n        \n        scheduler.step()\n        \n        print('[model %d, epoch %d, time: %.3f seconds] train_loss: %.5f, val_acc: %4f %%' %\n              (seed + 1, epoch + 1, time.time() - t0, training_loss/num_samples, 100 * correct / total))\n    \n    # Prediction\n    net.eval() # Evaluation mode -> Turn off dropout\n    test_pred = torch.LongTensor()\n    \n    if use_cuda:\n        test_pred = test_pred.cuda()\n        \n    with torch.no_grad(): # Turn off gradients for prediction, saves memory and computations\n        for features in test_loader:\n\n            if use_cuda:\n                features = features.cuda()\n\n            # Get the softmax probabilities\n            outputs = net(features)\n            # Get the prediction of the batch\n            _, predicted = torch.max(outputs, 1)\n            # Concatenate the prediction\n            test_pred = torch.cat((test_pred, predicted), dim=0)\n    \n    model_name = 'model_' + str(seed + 1)\n    ensemble_df[model_name] = test_pred.cpu().numpy()\n    print('Prediction Saved! \\n')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Ensemble"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create test_loader\ntest_dataset = DataFrame_to_Dataset(test_df)\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)\n\nensemble_df = submit_df.copy()\n\nnum_models = 11\nnum_epochs = 6\n\nfor seed in range(num_models):\n    train(seed, num_epochs)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"ensemble_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Final prediction\nfinal_pred = ensemble_df.iloc[:,2:].mode(axis=1).iloc[:,0]\nsubmit_df.Label = final_pred.astype(int)\nsubmit_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a submission file\nsubmit_df.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}