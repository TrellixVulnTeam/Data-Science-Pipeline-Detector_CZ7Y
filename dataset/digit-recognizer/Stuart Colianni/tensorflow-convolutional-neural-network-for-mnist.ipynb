{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"a4498b9c-4772-29c3-678f-243e805150b0"},"source":"This notebook is meant to show off what I learned working through the tutorial titled [TensorFlow and deep earning, without a PhD](https://codelabs.developers.google.com/codelabs/cloud-tensorflow-mnist/#0) on Codelabs.\n\nMy goals in creating this notebook are as follows:\n\n1.  Demonstrate that I am learning the basics of TensorFlow and have created a functional project.\n2.  Show how to read training and test data for MNIST in from the provided Kaggle CSV files.\n    - I noticed that many people fraudulently import the data using imports from TensorFlow.  This is fraudulent as this does not allow one to learn how to import data and apply TensorFlow knowledge to other problems.  Additionally, there is more training data in the TensorFlow import.  This obviously would cause one's model to fraudulently be tested on some data on which it was trained.\n\nThings I learned in this tutorial:\n\n1.  Basics of TensorFlow, softmax, relu, preventing overfitting, etc.\n2.  Basics of Convolutional Neural Networks\n\nNote:  The code in this notebook is heavily borrowed from the code and exercises provided in the aforementioned Codelab tutorial."},{"cell_type":"markdown","metadata":{"_cell_guid":"9836396c-0cb4-a5f4-28f9-389b97526f91"},"source":"## Import Modules"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"35e8e5b5-4556-2de5-c74c-1679f620f00c"},"outputs":[],"source":"import tensorflow as tf\nimport numpy as np\nimport math\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom tensorflow.contrib.learn.python.learn.datasets.mnist import read_data_sets\ntf.set_random_seed(0)"},{"cell_type":"markdown","metadata":{"_cell_guid":"d88fc1f9-61af-f04a-6326-54c75c4bb401"},"source":"## Read in data from CSV files"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c40c83f7-7b57-02ae-0ef3-22beb96e4e9c"},"outputs":[],"source":"# Read in training data from train.csv\ndfTrain = pd.read_csv('../input/train.csv')\ndfTrainFeatureVectors = dfTrain.drop(['label'], axis=1)\ntrainFeatureVectors = dfTrainFeatureVectors.values.astype(dtype=np.float32)\ntrainFeatureVectorsConvoFormat = trainFeatureVectors.reshape(42000, 28, 28, 1)\n\ntrainLabelsList = dfTrain['label'].tolist()\nohTrainLabelsTensor = tf.one_hot(trainLabelsList, depth=10)\nohTrainLabelsNdarray = tf.Session().run(ohTrainLabelsTensor).astype(dtype=np.float64)\n\n# Read in testing data from test.csv\ndfTest = pd.read_csv('../input/test.csv')\ntestFeatureVectors = dfTest.values.astype(dtype=np.float32)\ntestFeatureVectorsConvoFormat = testFeatureVectors.reshape(28000, 28, 28, 1)"},{"cell_type":"markdown","metadata":{"_cell_guid":"d9bf3a80-0bcc-a30d-952e-0cda482fd933"},"source":"## Display image from CSV data\n\nThis lets us verify that we have read in and structured the data correctly."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"03945c82-84a8-5555-55ef-a8eba172896f"},"outputs":[],"source":"# Display an image read in from the CSV\n# testFeatureVectorsConvoFormat values are: [2, 0, 9, 0, 3, 7, ...]\npixels = testFeatureVectorsConvoFormat[0].reshape((28, 28))\nplt.imshow(pixels, cmap='gray')\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"42cec58c-8253-a54b-2cff-f9165ffdf837"},"source":"## Construct TensorFlow graph"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"28780b1e-93a1-414f-5bed-83bbf4f669d7"},"outputs":[],"source":"# Define Tensorflow graph\nX = tf.placeholder(tf.float32, [None, 28, 28, 1])\nY_ = tf.placeholder(tf.float32, [None, 10])\nlr = tf.placeholder(tf.float32)\npkeep = tf.placeholder(tf.float32)\n\nK = 6  # first convolutional layer output depth\nL = 12  # second convolutional layer output depth\nM = 24  # third convolutional layer\nN = 200  # fully connected layer (softmax)\n\nW1 = tf.Variable(tf.truncated_normal([6, 6, 1, K], stddev=0.1))  # 6x6 patch, 1 input channel, K output channels\nB1 = tf.Variable(tf.constant(0.1, tf.float32, [K]))\nW2 = tf.Variable(tf.truncated_normal([5, 5, K, L], stddev=0.1))\nB2 = tf.Variable(tf.constant(0.1, tf.float32, [L]))\nW3 = tf.Variable(tf.truncated_normal([4, 4, L, M], stddev=0.1))\nB3 = tf.Variable(tf.constant(0.1, tf.float32, [M]))\n\nW4 = tf.Variable(tf.truncated_normal([7 * 7 * M, N], stddev=0.1))\nB4 = tf.Variable(tf.constant(0.1, tf.float32, [N]))\n\nW5 = tf.Variable(tf.truncated_normal([N, 10], stddev=0.1))\nB5 = tf.Variable(tf.constant(0.1, tf.float32, [10]))\n\n# The model\nstride = 1  # output is 28x28\nY1 = tf.nn.relu(tf.nn.conv2d(X, W1, strides=[1, stride, stride, 1], padding='SAME') + B1)\nstride = 2  # output is 14x14\nY2 = tf.nn.relu(tf.nn.conv2d(Y1, W2, strides=[1, stride, stride, 1], padding='SAME') + B2)\nstride = 2  # output is 7x7\nY3 = tf.nn.relu(tf.nn.conv2d(Y2, W3, strides=[1, stride, stride, 1], padding='SAME') + B3)\n\n# reshape the output from the third convolution for the fully connected layer\nYY = tf.reshape(Y3, shape=[-1, 7 * 7 * M])\n\nY4 = tf.nn.relu(tf.matmul(YY, W4) + B4)\nYY4 = tf.nn.dropout(Y4, pkeep)\nYlogits = tf.matmul(YY4, W5) + B5\nY = tf.nn.softmax(Ylogits)"},{"cell_type":"markdown","metadata":{"_cell_guid":"aad7d38a-9e65-1be3-54d1-65d1b7c0c1b4"},"source":"## Define calculations we need from the Neural Network"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2aa9667d-2121-827e-b2ca-b179c8b9f14c"},"outputs":[],"source":"cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=Ylogits, labels=Y_)\ncross_entropy = tf.reduce_mean(cross_entropy)*100\n\n# accuracy of the trained model, between 0 (worst) and 1 (best)\ncorrect_prediction = tf.equal(tf.argmax(Y, 1), tf.argmax(Y_, 1))\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\npredictions = tf.argmax(Y, 1)\n\n# training step, the learning rate is a placeholder\ntrain_step = tf.train.AdamOptimizer(lr).minimize(cross_entropy)"},{"cell_type":"markdown","metadata":{"_cell_guid":"fa5623a3-6d22-e954-4ad7-82eed5d54bf8"},"source":"## Train the Neural Network"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1299c309-1f47-2eda-f834-fd25ea099da3"},"outputs":[],"source":"# init\ninit = tf.global_variables_initializer()\nsess = tf.Session()\nsess.run(init)\n\ndef getBatch(i, size, trainFeatures, trainLabels):\n    startIndex = (i * size) % 42000\n    endIndex = startIndex + size\n    batch_X = trainFeatures[startIndex : endIndex]\n    batch_Y = trainLabels[startIndex : endIndex]\n    return batch_X, batch_Y\n\n# You can call this function in a loop to train the model, 100 images at a time\ndef training_step(i):\n\n    # training on batches of 100 images with 100 labels\n    size = 100\n    batch_X, batch_Y = getBatch(i, size, trainFeatureVectorsConvoFormat, ohTrainLabelsNdarray)\n\n    # learning rate decay\n    max_learning_rate = 0.003\n    min_learning_rate = 0.0001\n    decay_speed = 2000.0\n    learning_rate = min_learning_rate + (max_learning_rate - min_learning_rate) * math.exp(-i/decay_speed)\n\n    # compute training values\n    if i % 100 == 0:\n        '''\n        When we sess.run here, we are calculating the accuracy and cross_entropy of the model on batch_X and batch_Y (ie. on 100 pieces of data)\n        '''\n        a, c = sess.run([accuracy, cross_entropy], {X: batch_X, Y_: batch_Y, pkeep: 1.0})\n        print(str(i) + \": training accuracy:\" + str(a) + \" training loss: \" + str(c) + \" (lr:\" + str(learning_rate) + \")\")\n\n    # compute test values\n    if i % 500 == 0:\n        '''\n        When we sess.run here, we are calculating the accuracy and cross_entropy of the model on all of the data\n        '''\n        a, c = sess.run([accuracy, cross_entropy], {X: trainFeatureVectorsConvoFormat[-10000:], Y_: ohTrainLabelsNdarray[-10000:], pkeep: 1.0})\n        print(str(i) + \": ********* test accuracy:\" + str(a) + \" test loss: \" + str(c))\n\n    # the backpropagation training step\n    sess.run(train_step, {X: batch_X, Y_: batch_Y, lr: learning_rate, pkeep: 0.75})\n\n# Run number of iterations training the NN    \nfor i in range(10000+1): \n    training_step(i)"},{"cell_type":"markdown","metadata":{"_cell_guid":"7a28174c-de94-98db-91a3-bf66b352572c"},"source":"## Make predictions"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ad3bb477-7db3-f721-f92c-c78eabbfd24b"},"outputs":[],"source":"# Print the test accurscy on some data that was held out\na, c = sess.run([accuracy, cross_entropy], {X: trainFeatureVectorsConvoFormat[-10000:], Y_: ohTrainLabelsNdarray[-10000:], pkeep: 1.0})\nprint(\"\\n ********* test accuracy:\" + str(a) + \" test loss: \" + str(c))\n\n# Get predictions on test data\np = sess.run([predictions], {X: testFeatureVectorsConvoFormat, pkeep: 1.0})\n\n# Write predictions to csv file\nresults = pd.DataFrame({'ImageId': pd.Series(range(1, len(p[0]) + 1)), 'Label': pd.Series(p[0])})\nresults.to_csv('results.csv', index=False)"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}