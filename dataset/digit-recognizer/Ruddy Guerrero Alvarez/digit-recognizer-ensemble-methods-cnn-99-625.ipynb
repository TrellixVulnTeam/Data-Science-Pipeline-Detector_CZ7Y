{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<div style=\"display:fill;border-radius:10px;\n            font-family:Nexa;background-color:#1B1918\">\n    <h1 id=\"head_1\" style=\"padding:10px;color:white;font-size:200%;text-align:center;\n              font-weight: bold;text-transform: uppercase;\">imports</h1>\n</div>","metadata":{}},{"cell_type":"code","source":"import os\nimport csv\nimport random\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom tensorflow import keras \nfrom keras import models\nfrom keras import applications\nfrom keras import regularizers\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.layers import *\nfrom keras.models import *\nfrom keras import backend as K\nfrom keras.callbacks import EarlyStopping\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.optimizers import Adam\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-04-28T07:40:15.364032Z","iopub.execute_input":"2022-04-28T07:40:15.36429Z","iopub.status.idle":"2022-04-28T07:40:15.373989Z","shell.execute_reply.started":"2022-04-28T07:40:15.364262Z","shell.execute_reply":"2022-04-28T07:40:15.373292Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<img src=\"https://github.com/Ruddy9501/Kaggle-competitions/blob/main/Digit%20Recognizer/img/numbers2.png?raw=true\" width=100% height=auto> ","metadata":{}},{"cell_type":"markdown","source":"<div style=\"display:fill;border-radius:10px;\n            font-family:Nexa;background-color:#1B1918\">\n    <h1 id=\"head_2\" style=\"padding:10px;color:white;font-size:200%;text-align:center;\n              font-weight: bold;text-transform: uppercase;\">data loading</h1>\n</div>\n\n<p style=\"font-size:150%;\">We read the training data, test data and sample submissions.</p>","metadata":{}},{"cell_type":"code","source":"train  = pd.read_csv('../input/digit-recognizer/train.csv')\ntest   = pd.read_csv('../input/digit-recognizer/test.csv')\nsample = pd.read_csv('../input/digit-recognizer/sample_submission.csv')\n\nX = np.array(train.iloc[:, 1:])\nY = np.array(train.iloc[:, 0])\ntest = test.to_numpy()\n\nn = X.shape[0]\nm = X.shape[1]\n\nprint(\"Number of training samples: \", n)\nprint (\"Number of features: \", m)","metadata":{"execution":{"iopub.status.busy":"2022-04-28T07:40:15.377787Z","iopub.execute_input":"2022-04-28T07:40:15.377994Z","iopub.status.idle":"2022-04-28T07:40:19.059317Z","shell.execute_reply.started":"2022-04-28T07:40:15.37797Z","shell.execute_reply":"2022-04-28T07:40:19.058568Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-size:150%;\">Now we check for missing values.</p>","metadata":{}},{"cell_type":"code","source":"np.sum(np.isnan(X))","metadata":{"execution":{"iopub.status.busy":"2022-04-28T07:40:19.061089Z","iopub.execute_input":"2022-04-28T07:40:19.061541Z","iopub.status.idle":"2022-04-28T07:40:19.103301Z","shell.execute_reply.started":"2022-04-28T07:40:19.061505Z","shell.execute_reply":"2022-04-28T07:40:19.102498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.sum(np.isnan(Y))","metadata":{"execution":{"iopub.status.busy":"2022-04-28T07:40:19.104749Z","iopub.execute_input":"2022-04-28T07:40:19.105Z","iopub.status.idle":"2022-04-28T07:40:19.113713Z","shell.execute_reply.started":"2022-04-28T07:40:19.104967Z","shell.execute_reply":"2022-04-28T07:40:19.112953Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.sum(np.isnan(test))","metadata":{"execution":{"iopub.status.busy":"2022-04-28T07:40:19.116015Z","iopub.execute_input":"2022-04-28T07:40:19.116593Z","iopub.status.idle":"2022-04-28T07:40:19.144912Z","shell.execute_reply.started":"2022-04-28T07:40:19.116556Z","shell.execute_reply":"2022-04-28T07:40:19.144187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"display:fill;border-radius:10px;\n            font-family:Nexa;background-color:#1B1918\">\n    <h1 id=\"head_3\" style=\"padding:10px;color:white;font-size:200%;text-align:center;\n              font-weight: bold;text-transform: uppercase;\">Data display</h1>\n</div>\n\n<p style=\"font-size:150%;\">Let's see some randomly selected images from the dataset.</p>","metadata":{}},{"cell_type":"code","source":"index = np.random.randint(0, n, 50)\nplt.figure(figsize = (20, 10))\nfor i, idx in enumerate(index):\n    plt.subplot(5, 10, i+1)\n    plt.imshow(np.resize(X[idx], (28, 28, 1)))\nprint(\"Random examples\".center(150))   \nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-28T07:40:19.148884Z","iopub.execute_input":"2022-04-28T07:40:19.150175Z","iopub.status.idle":"2022-04-28T07:40:23.437803Z","shell.execute_reply.started":"2022-04-28T07:40:19.150135Z","shell.execute_reply":"2022-04-28T07:40:23.437189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-size:150%;\">We show the number of examples of each label. As you can see the training data is balanced.</p>","metadata":{}},{"cell_type":"code","source":"x_labels = sorted(list(set(Y)))\ny_labels = [Y.tolist().count(i) for i in x_labels]\n\nplt.figure(figsize = (15,5))\nplt.ylabel('Number of labels',  fontsize=20)\nplt.xlabel('Labels',  fontsize=20)\nplt.xticks([i for i in range(len(x_labels))]) \nplt.bar(x_labels, y_labels)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-28T07:40:23.439139Z","iopub.execute_input":"2022-04-28T07:40:23.439589Z","iopub.status.idle":"2022-04-28T07:40:23.735893Z","shell.execute_reply.started":"2022-04-28T07:40:23.439551Z","shell.execute_reply":"2022-04-28T07:40:23.735183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"display:fill;border-radius:10px;\n            font-family:Nexa;background-color:#1B1918\">\n    <h1 id=\"head_4\" style=\"padding:10px;color:white;font-size:200%;text-align:center;\n              font-weight: bold;text-transform: uppercase;\">Data preprocessing</h1>\n</div>\n\n<p style=\"font-size:150%;\">\nIn order to train a neural network we perform the following normalizations and transformations:\n    <ul style=\"font-size:150%;\">\n    <li> Resize the images to $28 \\times 28 \\times 1$, which will be the input of our neural network.\n    <li> Transform the output $Y$ class following a one-hot encoding, becouse we are facing a classification problem with multiple classes.\n    <li> Normalize the training data dividing by the maximun value of the features ($255$).\n    </ul>\n</p>\n\n<div style=\"display:fill;border-radius:8px;\n            font-family:Nexa;background-color:#1B1918\">\n    <h2 id='head_4_1' style=\"padding:10px;color:white;font-size:150%;text-align:center;\n              font-weight: bold;text-transform: uppercase;\">Resize the images</h2>\n</div>","metadata":{}},{"cell_type":"code","source":"print('Dimensions of training data before resizing data: ', X.shape)\nprint('Dimensions of test data before resizing data: ', test.shape, '\\n')\n\nX = np.reshape(X, (X.shape[0], 28, 28, 1))\ntest = np.reshape(test, (test.shape[0], 28, 28, 1))\n\nprint('Dimensions of training data after resizing data: ', X.shape)\nprint('Dimensions of test data after resizing data: ', test.shape)","metadata":{"execution":{"iopub.status.busy":"2022-04-28T07:40:23.737207Z","iopub.execute_input":"2022-04-28T07:40:23.737453Z","iopub.status.idle":"2022-04-28T07:40:23.7448Z","shell.execute_reply.started":"2022-04-28T07:40:23.73742Z","shell.execute_reply":"2022-04-28T07:40:23.74343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"display:fill;border-radius:8px;\n            font-family:Nexa;background-color:#1B1918\">\n    <h2 id='head_4_2' style=\"padding:10px;color:white;font-size:150%;text-align:center;\n              font-weight: bold;text-transform: uppercase;\">One-hot encoding</h2>\n</div>","metadata":{}},{"cell_type":"code","source":"print('Example output before one-hot encoding: ', Y[0], '\\n')\n\nY = to_categorical(Y)\n\nprint('Example output after one-hot encoding: ', Y[0])","metadata":{"execution":{"iopub.status.busy":"2022-04-28T07:40:23.746008Z","iopub.execute_input":"2022-04-28T07:40:23.746648Z","iopub.status.idle":"2022-04-28T07:40:23.756881Z","shell.execute_reply.started":"2022-04-28T07:40:23.746615Z","shell.execute_reply":"2022-04-28T07:40:23.756066Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"display:fill;border-radius:8px;\n            font-family:Nexa;background-color:#1B1918\">\n    <h2 id='head_4_3' style=\"padding:10px;color:white;font-size:150%;text-align:center;\n              font-weight: bold;text-transform: uppercase;\">Normalize the training data</h2>\n</div>","metadata":{}},{"cell_type":"code","source":"print('Minimum value of the features before normalization: ', np.min(X))\nprint('Maximum value of features before normalization: ', np.max(X), '\\n')\n\nX = X / 255.0\ntest = test / 255.0\n\nprint('Minimum value of the features before normalization: ', np.min(X))\nprint('Maximum value of features before normalization: ', np.max(X))","metadata":{"execution":{"iopub.status.busy":"2022-04-28T07:40:23.758714Z","iopub.execute_input":"2022-04-28T07:40:23.758978Z","iopub.status.idle":"2022-04-28T07:40:24.046343Z","shell.execute_reply.started":"2022-04-28T07:40:23.758943Z","shell.execute_reply":"2022-04-28T07:40:24.045584Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"display:fill;border-radius:10px;\n            font-family:Nexa;background-color:#1B1918\">\n    <h1 id=\"head_5\" style=\"padding:10px;color:white;font-size:200%;text-align:center;\n              font-weight: bold;text-transform: uppercase;\">Division in train and test</h1>\n</div>\n\n<p style=\"font-size:150%;\">\nWe will split the train set into two new sets $p$  and $v$ . Basically, we will use the train set $p$ to train the model and the validation set $v$ to estimate the hyperparameters. The train set was divided into $80\\%$ for $p$ and $20\\%$ for $v$.\n</p>","metadata":{}},{"cell_type":"code","source":"indexs = np.arange(n)\nposicion = int(n * 0.8)\n\nX_train_p = X[indexs[:posicion]]\nX_train_v = X[indexs[posicion:]] \nY_train_p = Y[indexs[:posicion]]\nY_train_v = Y[indexs[posicion:]]\n\nprint(X_train_v.shape)\nprint(\"Number of examples of X_train_p: \", X_train_p.shape[0])\nprint(\"Number of examples of X_train_v: \", X_train_v.shape[0])","metadata":{"execution":{"iopub.status.busy":"2022-04-28T07:40:24.049281Z","iopub.execute_input":"2022-04-28T07:40:24.049732Z","iopub.status.idle":"2022-04-28T07:40:24.169567Z","shell.execute_reply.started":"2022-04-28T07:40:24.04969Z","shell.execute_reply":"2022-04-28T07:40:24.168804Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"display:fill;border-radius:10px;\n            font-family:Nexa;background-color:#1B1918\">\n    <h1 id=\"head_6\" style=\"padding:10px;color:white;font-size:200%;text-align:center;\n              font-weight: bold;text-transform: uppercase;\">Data aumentation</h1>\n</div>\n\n<p style=\"font-size:150%;\">\nWe apply <b>Data Augmentation</b>, with this set of techniques we will increase the amount of data by adding modified copies of existing data. Data augmentation makes machine learning more robust by creating variations in the model. For this we will perform some transformations on the images:\n</p>\n<ul style=\"font-size:150%;\">\n<li>Rotation.\n<li> Zoom.\n<li> Horizontal displacements.\n<li> Vertical displacements.\n</ul>\n","metadata":{}},{"cell_type":"code","source":"data_gen = ImageDataGenerator(validation_split = 0.2, rotation_range = 10, zoom_range = 0.1, width_shift_range = 0.1, height_shift_range = 0.1)","metadata":{"execution":{"iopub.status.busy":"2022-04-28T07:40:24.170877Z","iopub.execute_input":"2022-04-28T07:40:24.171222Z","iopub.status.idle":"2022-04-28T07:40:24.178135Z","shell.execute_reply.started":"2022-04-28T07:40:24.171182Z","shell.execute_reply":"2022-04-28T07:40:24.177355Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"display:fill;border-radius:10px;\n            font-family:Nexa;background-color:#1B1918\">\n    <h1 id=\"head_7\" style=\"padding:10px;color:white;font-size:200%;text-align:center;\n              font-weight: bold;text-transform: uppercase;\">Model training</h1>\n</div>\n\n<p style=\"font-size:150%;\">\nIn order to improve the classification rate we will use an <b>Ensemble Method</b>. This technique creates multiple models and combines them to produce improved results. Ensemble methods often produce more accurate solutions than single model solutions. Therefore we will use $7$ models. Each model will make its prediction and the top voted class will be the predicted one.\n</p>","metadata":{}},{"cell_type":"code","source":"# Create the model\nn_models = 7\nmodel = [0] * n_models\ninputs = Input(shape = (28, 28, 1))\nfor i in range(n_models):\n    model[i] = Conv2D(64, kernel_size  = (5, 5), padding = 'same', activation = 'relu')(inputs)\n    model[i] = BatchNormalization()(model[i])\n    model[i] = Conv2D(64, kernel_size  = (5, 5), padding = 'same',activation = 'relu')(model[i])\n    model[i] = BatchNormalization()(model[i])\n    model[i] = MaxPooling2D(pool_size  = (2, 2))(model[i])\n    model[i] = Dropout(0.5)(model[i])\n    model[i] = Conv2D(128, kernel_size = (5, 5), padding = 'same', activation = 'relu')(model[i])\n    model[i] = BatchNormalization()(model[i])\n    model[i] = Conv2D(128, kernel_size = (5, 5), padding = 'same', activation = 'relu')(model[i])\n    model[i] = BatchNormalization()(model[i])\n    model[i] = MaxPooling2D(pool_size  = (2, 2))(model[i])\n    \n    model[i] = Flatten()(model[i])\n\n    model[i] = Dropout(0.5)(model[i])\n    model[i] = Dense(256, activation = 'relu')(model[i])\n    model[i] = BatchNormalization()(model[i])\n    model[i] = Dropout(0.5)(model[i])\n    model[i] = Dense(10, activation   = 'softmax')(model[i])","metadata":{"execution":{"iopub.status.busy":"2022-04-28T07:40:24.179486Z","iopub.execute_input":"2022-04-28T07:40:24.180204Z","iopub.status.idle":"2022-04-28T07:40:24.793868Z","shell.execute_reply.started":"2022-04-28T07:40:24.180148Z","shell.execute_reply":"2022-04-28T07:40:24.793235Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-size:150%;\">\n    To mitigate overtraining we apply <b>Early Stopping</b>.\n<p style=\"font-size:150%;\">","metadata":{}},{"cell_type":"code","source":"for i in range(n_models):\n    model[i] = Model(inputs = inputs, outputs = model[i])\n    model[i].compile(optimizer = Adam(0.00001), loss = 'categorical_crossentropy', metrics = ['accuracy'])\n    \ncallbacks_list = [\n    keras.callbacks.ModelCheckpoint(\n        filepath='model.h5',\n        monitor='val_loss', save_best_only=True, verbose=1),\n    EarlyStopping(monitor='val_loss', patience=20,verbose=1)\n]","metadata":{"execution":{"iopub.status.busy":"2022-04-28T07:40:24.794822Z","iopub.execute_input":"2022-04-28T07:40:24.795066Z","iopub.status.idle":"2022-04-28T07:40:24.844513Z","shell.execute_reply.started":"2022-04-28T07:40:24.795033Z","shell.execute_reply":"2022-04-28T07:40:24.843897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-size:150%;\">\n    We show the summary of one of the models.\n</p>","metadata":{}},{"cell_type":"code","source":"model[0].summary()","metadata":{"execution":{"iopub.status.busy":"2022-04-28T07:40:24.845699Z","iopub.execute_input":"2022-04-28T07:40:24.845943Z","iopub.status.idle":"2022-04-28T07:40:24.859657Z","shell.execute_reply.started":"2022-04-28T07:40:24.84591Z","shell.execute_reply":"2022-04-28T07:40:24.85901Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-size:150%;\">\n    We start the training of the network. We specify the training sets and the number of iterations. We will save all the information in the history variable $h$ so that later we can plot the obtained result.\n</p>","metadata":{}},{"cell_type":"code","source":"h = [0] * n_models\nfor i in range(n_models): \n    data_train = data_gen.flow(X_train_p, Y_train_p, shuffle = True, subset = 'training', batch_size = 32)\n    h[i] = model[i].fit(data_train, validation_data = (X_train_v, Y_train_v), batch_size = 64, epochs = 200, callbacks = callbacks_list)","metadata":{"execution":{"iopub.status.busy":"2022-04-28T07:40:24.860678Z","iopub.execute_input":"2022-04-28T07:40:24.860986Z","iopub.status.idle":"2022-04-28T10:33:31.273768Z","shell.execute_reply.started":"2022-04-28T07:40:24.860951Z","shell.execute_reply":"2022-04-28T10:33:31.273019Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"display:fill;border-radius:8px;\n            font-family:Nexa;background-color:#1B1918\">\n    <h2 id='head_7_1' style=\"padding:10px;color:white;font-size:150%;text-align:center;\n              font-weight: bold;text-transform: uppercase;\">confusion matrix</h2>\n</div>\n\n<p style=\"font-size:150%;\">\nThe confusion matrix is a very useful tool for assessing how good a classification model based on machine learning is. In particular, it serves to show explicitly when one class is confused with another, which allows us to work separately with different types of errors. \n</p>","metadata":{}},{"cell_type":"code","source":"result = np.zeros((X.shape[0], 10))\nfor i in range(n_models):\n    result = result + model[i].predict(X)\n    \ny_pred = result.argmax(axis = 1)\ny_real = Y.argmax(axis = 1)\n\nplt.figure(figsize = (16,8))\nplt.title('Predicted digits', size=20)\nsns.heatmap(confusion_matrix(y_real, y_pred), annot=True, fmt = '.0f')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-28T10:33:31.275464Z","iopub.execute_input":"2022-04-28T10:33:31.275964Z","iopub.status.idle":"2022-04-28T10:33:52.242195Z","shell.execute_reply.started":"2022-04-28T10:33:31.275926Z","shell.execute_reply":"2022-04-28T10:33:52.241551Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"display:fill;border-radius:8px;\n            font-family:Nexa;background-color:#1B1918\">\n    <h2 id='head_7_2' style=\"padding:10px;color:white;font-size:150%;text-align:center;\n              font-weight: bold;text-transform: uppercase;\">history</h2>\n</div>\n\n<p style=\"font-size:150%;\">\n    We show the history of the behavior for the cost function in the training and validation sets for one model.\n</p>","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize = (20,6))\nplt.subplot(1, 2, 1)\nplt.plot(h[0].history['loss'], 'g', label = 'train loss')\nplt.plot(h[0].history['val_loss'], 'r', label = 'validation loss')\nplt.legend(['train loss', 'validation loss'], loc ='best')\n\nplt.subplot(1, 2, 2)\nplt.plot(h[0].history['accuracy'], 'g', label = 'training accuracy')\nplt.plot(h[0].history['val_accuracy'], 'r', label = 'validation accuracy')\nplt.legend(['training accuracy', 'validation accuracy'], loc ='best')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-28T10:33:52.243488Z","iopub.execute_input":"2022-04-28T10:33:52.243719Z","iopub.status.idle":"2022-04-28T10:33:52.567803Z","shell.execute_reply.started":"2022-04-28T10:33:52.243687Z","shell.execute_reply":"2022-04-28T10:33:52.567165Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"display:fill;border-radius:10px;\n            font-family:Nexa;background-color:#1B1918\">\n    <h1 id=\"head_8\" style=\"padding:10px;color:white;font-size:200%;text-align:center;\n              font-weight: bold;text-transform: uppercase;\">Classification of test examples</h1>\n</div>\n\n<p style=\"font-size:150%;\">\nThen we classify the test examples. We chose the class <b>most voted</b> by the $7$ models.\n</p>","metadata":{}},{"cell_type":"code","source":"result = np.zeros((test.shape[0], 10))\nfor i in range(n_models):\n    result = result + model[i].predict(test)\n\ny_pred = result.argmax(axis = 1)\nsample.iloc[:,1] = y_pred","metadata":{"execution":{"iopub.status.busy":"2022-04-28T10:33:52.569149Z","iopub.execute_input":"2022-04-28T10:33:52.569595Z","iopub.status.idle":"2022-04-28T10:34:10.381699Z","shell.execute_reply.started":"2022-04-28T10:33:52.569559Z","shell.execute_reply":"2022-04-28T10:34:10.380948Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-size:150%;\">\n    Finally, we save the classified examples.\n</p>","metadata":{}},{"cell_type":"code","source":"sample.to_csv('./submission.csv', index = False)","metadata":{"execution":{"iopub.status.busy":"2022-04-28T10:34:10.382962Z","iopub.execute_input":"2022-04-28T10:34:10.383226Z","iopub.status.idle":"2022-04-28T10:34:10.42882Z","shell.execute_reply.started":"2022-04-28T10:34:10.383191Z","shell.execute_reply":"2022-04-28T10:34:10.428187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-size:150%;\">\nAny <b>comments</b>, <b>advice</b> for improvement or <b>doubts</b> will be very helpful. Also, if you have found this work helpful, an <b>Upvotes</b> would be great! :)\n</p>\n","metadata":{}}]}