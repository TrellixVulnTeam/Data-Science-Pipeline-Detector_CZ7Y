{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Introduction\n### MNIST dataset\nHandwritten digits dataset <br> \nOur goal here is to correctly identify digits from handwritten digits"},{"metadata":{},"cell_type":"markdown","source":"# Importing the necessarry Libraries for Data Analysis\nWe'll be using the following libraries to perform data analysis "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing required libraries\n\nimport numpy as np\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\nimport itertools\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom keras.preprocessing.image import ImageDataGenerator\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Loading data and other preparations\nWe've loaded the data into training and testing datasets <br>\nFurther we've loaded the input and output (class) dataset as well"},{"metadata":{"trusted":true},"cell_type":"code","source":"mnist_train = pd.read_csv(\"/kaggle/input/digit-recognizer/train.csv\")\nmnist_test = pd.read_csv(\"/kaggle/input/digit-recognizer/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Print mnist training dataset\n\nmnist_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Print mnist test dataset\n\nmnist_test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Understanding the Dataset\nWe clearly see that there are **42,000 training examples** and **28,000 testing examples** with a total of **60,000 grayscale data** as a part of the dataset <br>\nAlso, the images are of **28x28 pixels** flattened out into **an array of size 784**"},{"metadata":{},"cell_type":"markdown","source":"# Design and Target\nWe'll now split the training and testing datasets into **design and target** <br>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# For x_train, we just need to drop the \"labels\" column\n# Axis is by default 0 in .drop meaning that by default row gets dropped\n# Axis = 1 here means that we're dropping a column\n\nx_train = mnist_train.drop(labels = \"label\", axis = 1)\nx_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# For y_train we just need the \"labels\" column\n\ny_train = mnist_train[\"label\"]\ny_train","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Normalization\nNormalization is required to bring out dataset to a **common scale**"},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train = x_train/255.0\nmnist_test = mnist_test/255.0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(x_train.shape)\nprint(y_train.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(mnist_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Resizing the 1D matrix into a 3D matrix\nHere, we are transforming the **1D matrix** of size **784** to a **3D matrix** of size **28x28x1**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# -1 means that we want to keep all the values, that is the initial size intact\n# 28x28x1 means 28 rows, 28 columns and 1 channel\n# The 1 channel will have values ranging from 0 to 1\n# For RGB, we would have had a 3 channel system \n\nx_train = x_train.values.reshape(-1,28,28,1)\nmnist_test = mnist_test.values.reshape(-1,28,28,1)\nx_train.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# One Hot Encoding\nHere, we'll convert our labels in y_train as a one-hot-encoded vector"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train = tf.keras.utils.to_categorical(y_train, num_classes=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(y_train[3])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Augmentation\nIt is performed in order to increase our dataset <br>\nCommonly used methods are, random flipping, rotation, cropping, etc of images"},{"metadata":{"trusted":true},"cell_type":"code","source":"datagen = ImageDataGenerator(rotation_range=12, zoom_range = 0.15, width_shift_range=0.15, height_shift_range=0.13) \n\ndatagen.fit(x_train)\ndatagen.fit(x_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Visualize the Dataset\nHere, we plot the first 20 entries of the dataset using matplotlib library"},{"metadata":{"trusted":true},"cell_type":"code","source":"num = 20\nimages = x_train[:num]\n\nnum_row = 5\nnum_col = 4\n\n# plot images\nfig, axes = plt.subplots(num_row, num_col, figsize=(1.5*num_col,2*num_row))\nfor i in range(num):\n    ax = axes[i//num_col, i%num_col]\n    ax.imshow(images[i].reshape(28,28), cmap='viridis')\n    ax.set_title('Label: {}'.format(y_train[i].tolist().index(1.0, 0, 10)))\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training and Validation set\nSplit the training data into training and validation sets"},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size = 0.15)\nprint(x_train.shape)\nprint(x_val.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Convolutional Neural Network using Keras\nHere, we build our learning model, which will be a convolutional neural network"},{"metadata":{"trusted":true},"cell_type":"code","source":"input_layer = tf.keras.layers.Input(shape = (28, 28, 1))\n\nhidden_layer_1 = tf.keras.layers.Conv2D(32, kernel_size=(3, 3), activation=tf.keras.activations.relu)(input_layer)\nnorm_1 = tf.keras.layers.BatchNormalization()(hidden_layer_1)\nmax_1 = tf.keras.layers.MaxPooling2D(pool_size=(3,3), strides=(2,2))(norm_1)\ndropout_layer_1 = tf.keras.layers.Dropout(0.25)(max_1)\n\n# hidden_layer_2 = tf.keras.layers.Conv2D(64, kernel_size=(3, 3), activation=tf.keras.activations.relu)(dropout_layer_1)\n# norm_2 = tf.keras.layers.BatchNormalization()(hidden_layer_2)\n# max_2 = tf.keras.layers.MaxPooling2D(pool_size=(3,3), strides=(2,2))(norm_2)\n\nflatten_layer = tf.keras.layers.Flatten()(dropout_layer_1)\n# dropout_layer_2 = tf.keras.layers.Dropout(0.25)(flatten_layer)\n\nhidden_layer_3 = tf.keras.layers.Dense(256, activation=tf.keras.activations.relu)(flatten_layer)\nhidden_layer_4 = tf.keras.layers.Dense(128, activation=tf.keras.activations.relu)(hidden_layer_3)\n\ndropout_layer_3 = tf.keras.layers.Dropout(0.5)(hidden_layer_4)\n\noutput_layer = tf.keras.layers.Dense(10, activation=tf.keras.activations.sigmoid)(dropout_layer_3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = tf.keras.Model(inputs = input_layer, outputs = output_layer)\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Plot of model graph"},{"metadata":{"trusted":true},"cell_type":"code","source":"tf.keras.utils.plot_model(model, \"digit_classifier_model.png\", show_shapes=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Loading the model\nHere, we load our CNN with optimizer, loss function and evaluation metric"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Here, we are using Adam optimiser\n# The loss function is categorical crossentropy function \n\nmodel.compile(optimizer = tf.keras.optimizers.Adam(), loss = tf.keras.losses.CategoricalCrossentropy(), metrics = ['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training our model\nHere, we are training our **convolutional neural network** on the testing and validation data"},{"metadata":{"trusted":true},"cell_type":"code","source":"his = model.fit(x_train, y_train, batch_size = 1000, epochs = 40, validation_data=(x_val, y_val))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Plotting model accuracy\nWe plot the model accuracies for both training and testing data"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(his.history['accuracy'])\nplt.plot(his.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='lower right')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Plotting model loss\nWe plot the model loss for both training and testing data"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(his.history['loss'])\nplt.plot(his.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper right')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Confusion Matrix\nA confusion matrix can very well describe the performance of a classification model <br>\nWe'll plot one for our CNN model here"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets plot the confusion matrix \n\ndef plot_confusion_matrix(cm, classes,title='Confusion matrix',cmap=plt.cm.viridis):\n    \n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"black\" if cm[i, j] > thresh else \"white\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n\n# Predict the values from the validation dataset\ny_pred = model.predict(x_val)\n# Convert predictions classes to one hot vectors \ny_pred_classes = np.argmax(y_pred,axis = 1) \n# Convert validation observations to one hot vectors\ny_true = np.argmax(y_val,axis = 1) \n# compute the confusion matrix\nconfusion_mtx = confusion_matrix(y_true, y_pred_classes) \n# plot the confusion matrix\nplot_confusion_matrix(confusion_mtx, classes = range(10)) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Displaying the error results and further analysis\nSo here, we are trying to display the digits which our model failed to recognize correctly <br>\nSome of these are very trivial mistakes while the later ones are quite genuine and any human can easily mistake them for other digits just like our model did here"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Display some error results \n\n# Errors are difference between predicted labels and true labels\nerrors = (y_pred_classes - y_true != 0)\n\ny_pred_classes_errors = y_pred_classes[errors]\ny_pred_errors = y_pred[errors]\ny_true_errors = y_true[errors]\nx_val_errors = x_val[errors]\n\ndef display_errors(errors_index,img_errors,pred_errors, obs_errors):\n    \n    n = 0\n    nrows = 4\n    ncols = 4\n    fig, ax = plt.subplots(nrows,ncols,sharex=True,sharey=True, figsize=(10,10))\n    plt.subplots_adjust(wspace=0.5, hspace=0.5)\n    for row in range(nrows):\n        for col in range(ncols):\n            error = errors_index[n]\n            ax[row,col].imshow((img_errors[error]).reshape((28,28)))\n            ax[row,col].set_title(\"Predicted label :{}\\nTrue label :{}\".format(pred_errors[error],obs_errors[error]))\n            n += 1\n\n# Probabilities of the wrong predicted numbers\ny_pred_errors_prob = np.max(y_pred_errors,axis = 1)\n\n# Predicted probabilities of the true values in the error set\ntrue_prob_errors = np.diagonal(np.take(y_pred_errors, y_true_errors, axis=1))\n\n# Difference between the probability of the predicted label and the true label\ndelta_pred_true_errors = y_pred_errors_prob - true_prob_errors\n\n# Sorted list of the delta prob errors\nsorted_dela_errors = np.argsort(delta_pred_true_errors)\n\n# Top 16 errors \nmost_important_errors = sorted_dela_errors[-16:]\n\n# Show the top 16 errors\ndisplay_errors(most_important_errors, x_val_errors, y_pred_classes_errors, y_true_errors)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Final Result\nSo here is the final result of our model :)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# predict results\nresults = model.predict(mnist_test)\nresults = np.argmax(results,axis = 1)\n\nresults = pd.Series(results,name=\"Label\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.concat([pd.Series(range(1,28001),name = \"ImageId\"),results],axis = 1)\n\nsubmission.to_csv(\"mnist_result.csv\",index=False)\nsubmission","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I have looked up the web for a lot of help regarding my code, especially the confusion matrix and error display part, so it may appear somewhat similar for that matter <br>\nNevertheless, I was surprised as to how much one can learn from a very basic dataset such as MNIST and it was surely a nice way to recapitulate the concepts I've studied"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}