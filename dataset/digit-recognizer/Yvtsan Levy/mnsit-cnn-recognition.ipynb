{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom keras.utils.np_utils import to_categorical\nfrom sklearn.model_selection import train_test_split\n\nimport tensorflow as tf\nprint(tf.__version__)\nfrom tensorflow.keras.layers import Input, Conv2D, Dense, Flatten, Dropout, GlobalMaxPooling2D, MaxPooling2D, BatchNormalization\nfrom tensorflow.keras.models import Model","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-12-19T11:47:36.218967Z","iopub.execute_input":"2021-12-19T11:47:36.21994Z","iopub.status.idle":"2021-12-19T11:47:43.445538Z","shell.execute_reply.started":"2021-12-19T11:47:36.219819Z","shell.execute_reply":"2021-12-19T11:47:43.444771Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train  = pd.read_csv('/kaggle/input/digit-recognizer/train.csv')\ntest = pd.read_csv('/kaggle/input/digit-recognizer/test.csv')","metadata":{"execution":{"iopub.status.busy":"2021-12-19T11:47:43.448112Z","iopub.execute_input":"2021-12-19T11:47:43.448968Z","iopub.status.idle":"2021-12-19T11:47:49.171656Z","shell.execute_reply.started":"2021-12-19T11:47:43.448916Z","shell.execute_reply":"2021-12-19T11:47:49.170953Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train","metadata":{"execution":{"iopub.status.busy":"2021-12-19T11:47:49.172928Z","iopub.execute_input":"2021-12-19T11:47:49.173669Z","iopub.status.idle":"2021-12-19T11:47:49.282826Z","shell.execute_reply.started":"2021-12-19T11:47:49.173633Z","shell.execute_reply":"2021-12-19T11:47:49.281828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create the X_train and y_train variables \nx_train = train.drop('label', axis=1)\ny_train = train.label\nprint(f'x_train = {x_train.shape}')\nprint(f'test = {test.shape}')","metadata":{"execution":{"iopub.status.busy":"2021-12-19T11:47:49.28419Z","iopub.execute_input":"2021-12-19T11:47:49.284403Z","iopub.status.idle":"2021-12-19T11:47:49.379025Z","shell.execute_reply.started":"2021-12-19T11:47:49.284377Z","shell.execute_reply":"2021-12-19T11:47:49.378026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# number of classes\nK = len(set(y_train))\nprint(\"number of classes:\", K)","metadata":{"execution":{"iopub.status.busy":"2021-12-19T11:47:49.38147Z","iopub.execute_input":"2021-12-19T11:47:49.381915Z","iopub.status.idle":"2021-12-19T11:47:49.394051Z","shell.execute_reply.started":"2021-12-19T11:47:49.381873Z","shell.execute_reply":"2021-12-19T11:47:49.393304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Normalize the data \nx_train = x_train / 255.0\ntest = test / 255.0\n\n# Reshape the data\nx_train = x_train.values.reshape(-1, 28, 28, 1)\ntest = test.values.reshape(-1, 28, 28, 1)\n\n# label encoding \ny_train = to_categorical(y_train, num_classes = 10)\n\n# Split the data\nx_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size=0.2, random_state=5)\n\nprint(f'x_train = {x_train.shape}')\nprint(f'test = {test.shape}')","metadata":{"execution":{"iopub.status.busy":"2021-12-19T11:47:49.395609Z","iopub.execute_input":"2021-12-19T11:47:49.396028Z","iopub.status.idle":"2021-12-19T11:47:49.910094Z","shell.execute_reply.started":"2021-12-19T11:47:49.395987Z","shell.execute_reply":"2021-12-19T11:47:49.909161Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train[0].shape","metadata":{"execution":{"iopub.status.busy":"2021-12-19T11:47:49.911817Z","iopub.execute_input":"2021-12-19T11:47:49.912145Z","iopub.status.idle":"2021-12-19T11:47:49.919643Z","shell.execute_reply.started":"2021-12-19T11:47:49.912086Z","shell.execute_reply":"2021-12-19T11:47:49.918641Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Build the model using the functional API\ni = Input(shape=x_train[0].shape)\n# x = Conv2D(32, (3, 3), strides=2, activation='relu')(i)\n# x = Conv2D(64, (3, 3), strides=2, activation='relu')(x)\n# x = Conv2D(128, (3, 3), strides=2, activation='relu')(x)\n\nx = Conv2D(32, (3, 3), activation='relu', padding='same')(i)\nx = BatchNormalization()(x)\nx = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\nx = BatchNormalization()(x)\nx = MaxPooling2D((2, 2))(x)\n# x = Dropout(0.2)(x)\nx = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\nx = BatchNormalization()(x)\nx = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\nx = BatchNormalization()(x)\nx = MaxPooling2D((2, 2))(x)\n# x = Dropout(0.2)(x)\nx = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\nx = BatchNormalization()(x)\nx = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\nx = BatchNormalization()(x)\nx = MaxPooling2D((2, 2))(x)\n# x = Dropout(0.2)(x)\n\n# x = GlobalMaxPooling2D()(x)\nx = Flatten()(x)\nx = Dropout(0.2)(x)\nx = Dense(1024, activation='relu')(x)\nx = Dropout(0.2)(x)\nx = Dense(K, activation='softmax')(x)\n\nmodel = Model(i, x)","metadata":{"execution":{"iopub.status.busy":"2021-12-19T11:47:49.921854Z","iopub.execute_input":"2021-12-19T11:47:49.922559Z","iopub.status.idle":"2021-12-19T11:47:50.392916Z","shell.execute_reply.started":"2021-12-19T11:47:49.922487Z","shell.execute_reply":"2021-12-19T11:47:50.392207Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Compile\n# Note: make sure you are using the GPU for this!\nmodel.compile(loss='categorical_crossentropy',\n              optimizer='rmsprop',\n              metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2021-12-19T11:47:50.393928Z","iopub.execute_input":"2021-12-19T11:47:50.39453Z","iopub.status.idle":"2021-12-19T11:47:50.408512Z","shell.execute_reply.started":"2021-12-19T11:47:50.394495Z","shell.execute_reply":"2021-12-19T11:47:50.407598Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Now that the model is so large, it's useful to summarize it\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2021-12-19T11:47:50.409959Z","iopub.execute_input":"2021-12-19T11:47:50.410245Z","iopub.status.idle":"2021-12-19T11:47:50.429811Z","shell.execute_reply.started":"2021-12-19T11:47:50.410211Z","shell.execute_reply":"2021-12-19T11:47:50.428831Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.preprocessing.image import ImageDataGenerator\n\ndatagen = ImageDataGenerator(\n        featurewise_center=False,  # set input mean to 0 over the dataset\n        samplewise_center=False,  # set each sample mean to 0\n        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n        samplewise_std_normalization=False,  # divide each input by its std\n        zca_whitening=False,  # dimesion reduction\n        rotation_range=5,  # randomly rotate images in the range 5 degrees\n        zoom_range = 0.1, # Randomly zoom image 10%\n        width_shift_range=0.1,  # randomly shift images horizontally 10%\n        height_shift_range=0.1,  # randomly shift images vertically 10%\n        horizontal_flip=False,  # randomly flip images\n        vertical_flip=False)  # randomly flip images\n\ndatagen.fit(x_train)","metadata":{"execution":{"iopub.status.busy":"2021-12-19T11:47:50.431103Z","iopub.execute_input":"2021-12-19T11:47:50.431451Z","iopub.status.idle":"2021-12-19T11:47:50.522302Z","shell.execute_reply.started":"2021-12-19T11:47:50.431418Z","shell.execute_reply":"2021-12-19T11:47:50.521312Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set a learning rate annealer\nfrom tensorflow.keras import callbacks\nfrom keras.callbacks import ReduceLROnPlateau\n\nlearning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', \n                                            patience=3,  \n                                            factor=0.5, \n                                            min_lr=0.00001)","metadata":{"execution":{"iopub.status.busy":"2021-12-19T11:47:50.523335Z","iopub.execute_input":"2021-12-19T11:47:50.523563Z","iopub.status.idle":"2021-12-19T11:47:50.529874Z","shell.execute_reply.started":"2021-12-19T11:47:50.523537Z","shell.execute_reply":"2021-12-19T11:47:50.528843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epochs = 75\nbatch_size = 90\n\nhistory = model.fit(datagen.flow(x_train,y_train, batch_size=batch_size),\n                              epochs = epochs, validation_data = (x_test,y_test),\n                              verbose = 1, steps_per_epoch=x_train.shape[0] // batch_size,\n                              callbacks=[learning_rate_reduction])","metadata":{"execution":{"iopub.status.busy":"2021-12-19T11:47:50.53137Z","iopub.execute_input":"2021-12-19T11:47:50.531728Z","iopub.status.idle":"2021-12-19T11:47:58.688065Z","shell.execute_reply.started":"2021-12-19T11:47:50.531684Z","shell.execute_reply":"2021-12-19T11:47:58.686628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot loss per iteration\nimport matplotlib.pyplot as plt\nplt.plot(history.history['loss'], label='loss')\nplt.plot(history.history['val_loss'], label='val_loss')\nplt.legend()","metadata":{"execution":{"iopub.status.busy":"2021-12-19T11:47:58.688974Z","iopub.status.idle":"2021-12-19T11:47:58.689313Z","shell.execute_reply.started":"2021-12-19T11:47:58.68914Z","shell.execute_reply":"2021-12-19T11:47:58.689158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot accuracy per iteration\nplt.plot(history.history['accuracy'], label='acc')\nplt.plot(history.history['val_accuracy'], label='val_acc')\nplt.legend()","metadata":{"execution":{"iopub.status.busy":"2021-12-19T11:47:58.690751Z","iopub.status.idle":"2021-12-19T11:47:58.691619Z","shell.execute_reply.started":"2021-12-19T11:47:58.6914Z","shell.execute_reply":"2021-12-19T11:47:58.691425Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"res = model.predict(test)\nres = np.argmax(res,axis = 1)\nres = pd.Series(res, name=\"Label\")\nsubmission = pd.concat([pd.Series(range(1 ,28001) ,name = \"ImageId\"),   res],axis = 1)\nsubmission.head(10)","metadata":{"execution":{"iopub.status.busy":"2021-12-19T11:47:58.692768Z","iopub.status.idle":"2021-12-19T11:47:58.693414Z","shell.execute_reply.started":"2021-12-19T11:47:58.693206Z","shell.execute_reply":"2021-12-19T11:47:58.693228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv('submission.csv',index=False)\n","metadata":{"execution":{"iopub.status.busy":"2021-12-19T11:47:58.694256Z","iopub.status.idle":"2021-12-19T11:47:58.694868Z","shell.execute_reply.started":"2021-12-19T11:47:58.694682Z","shell.execute_reply":"2021-12-19T11:47:58.694708Z"},"trusted":true},"execution_count":null,"outputs":[]}]}