{"cells":[{"metadata":{"_uuid":"e0fded589cf19f3b83100b88f80de6530bca882f"},"cell_type":"markdown","source":"# Understanding image filters\nIn this kernel, I will try to explain how convolution filters work in image analysis. Why such a subject ? Because it is the basis of every CNN models. To do that, I will first explain how a filter is applied on an image and then show some classic filters.\n\n1. Convolution filters\n2. Examples\n3. Learned filters visualisation"},{"metadata":{"_uuid":"cb8dfba170200cae8c2de6b3de0ddf9d4fae3615"},"cell_type":"markdown","source":"## Convolution filters\nA convolution filter is applied on an image to output another image. This is how it works\n\n* Superposition on the image (seen as a matrix of numbers).\n* Multiplication between filter elements and their corresponding image pixel\n* Sum of all the multiplication results\n* Creation of a new image with new pixel values\n![How to apply a convolution filter](http://machinelearninguru.com/_images/topics/computer_vision/basics/convolution/1.JPG)"},{"metadata":{"_uuid":"984951e68fdea6b5be1a1e079e138ac5ebd04eff"},"cell_type":"markdown","source":"## Examples\n### Utils"},{"metadata":{"trusted":true,"_uuid":"472c4a131ffb248b6fdd6e5014bb85faf307619e"},"cell_type":"code","source":"# Imports\nimport numpy as np\nimport pandas as pd\nimport os\nimport h5py\nimport matplotlib.pyplot as plt\nimport math\nimport random","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5c7c0a42d7d806f2b11ded177d28cda9eac581e4"},"cell_type":"code","source":"#function to load data\ndef load_dataset():\n    train_data = h5py.File('../input/happy-house-dataset/train_happy.h5', \"r\")\n    x_train = np.array(train_data[\"train_set_x\"][:])  \n    return x_train\n\n# Load the data\nX_train = load_dataset()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a7d69a3de0bd7a893d9bd303f39ee563a0dca304"},"cell_type":"code","source":"# Small function to create a dummy image, basically 4 different squares\ndef create_dummy_image(h=64, w=64):\n    img = np.zeros((h, w))\n    img[:h//2,:w//2] = 255\n    img[h//2:,:w//2] = 200\n    img[h//2:,w//2:] = 100\n    return img","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4b1a74f62fe5f6460d9b035360f07a2d5cead063"},"cell_type":"code","source":"# RGB to grayscale function --> filters can be applied channel per channel \n# but here we will focus only on grayscale images\ndef to_grayscale(img):\n    h = len(img)\n    w = len(img[0])\n    new_img = np.zeros((h, w))\n    for i in range(h):\n        for j in range(w):\n            new_img[i][j] = int(img[i][j][0] * 0.3 + img[i][j][1] * 0.59 + img[i][j][2] * 0.11)\n    return new_img","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8cb1e8cc2ef01cf39d95921bb7ded873e7b54119"},"cell_type":"code","source":"# Let's look at some images\nimg1 = to_grayscale(X_train[0])\nimg2 = to_grayscale(X_train[1])\nimg3 = to_grayscale(X_train[42])\ndummy_img = create_dummy_image()\n\nfig, axes = plt.subplots(nrows=1, ncols=4, figsize=(15, 3))\naxes[0].imshow(img1, cmap='Greys_r')\naxes[1].imshow(img2, cmap='Greys_r')\naxes[2].imshow(img3, cmap='Greys_r')\naxes[3].imshow(dummy_img, cmap='Greys_r')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"45b7c9a5c41f325690d888774014ae92c241b219"},"cell_type":"code","source":"# small function to apply a 2D filter on a 1-channel image\ndef apply_conv_filter(img, conv_filter):\n    h = len(img)\n    w = len(img[0])\n    hf = len(conv_filter)\n    wf = len(conv_filter[0])\n    h_off = hf // 2\n    w_off = wf // 2\n    new_img = np.zeros((h, w))\n    for i in range(h_off, h - h_off):\n        for j in range(w_off, w - w_off):\n            new_value = 0\n            for k in range(hf):\n                for l in range(wf):\n                    new_value += img[i - h_off + k][j - w_off + l] * conv_filter[k][l]\n            new_img[i][j] = abs(new_value)\n    \n    return new_img","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"98bae902f6299049eec4e30b4a016ec5be37fd7a"},"cell_type":"markdown","source":"### Edge detection filters\nHere, I will show some basic edge detection filters.\n\nFirst, what is an edge ? It's a border between two uniform zone. On a grayscale image, it's a border between a high value zone and a low value one. By saying that, we can get the intuition of how we can detect these edges : find the place where there is a big shift in pixel values."},{"metadata":{"trusted":true,"_uuid":"b847d45979b217eab2cb6070ec1f341fc0a17c6e"},"cell_type":"code","source":"# Let's create basic gradient filters\ngrad_v = [[-1, 0, 1]]\ngrad_h = [[-1],\n          [0],\n          [1]]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c2994a3f6b557948974d6e89d4ec1df1d52b4259"},"cell_type":"markdown","source":"Here, I have defined two filters. One is a line, the other one a column.\n\nThe first one, *grad_v* is a line that will compare the pixel to our left to the pixel to our right. It can be seen as a gradient, that will highlight vertical edges.\nThe second one, *grad_h* is a column. It will act as the first filter but for horizontal edges."},{"metadata":{"trusted":true,"_uuid":"2220ebbe1c93d7da8b3cdac0a8df3613d1f50730"},"cell_type":"code","source":"# Let's define a last one\ngrad = [[-1, -1, -1],\n        [-1, 8, -1],\n        [-1, -1, -1]]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5c51047e6ec51134b5755e098b8cd2d030e1f512"},"cell_type":"markdown","source":"This last filter acts the same way than the two previous one, it's a gradient. But it tries to get all the edges, and not only the ones along one axis.\n\nLet's look at their effect on some images."},{"metadata":{"trusted":true,"_uuid":"e78d3d2481d5824290811553ccfca6695443132d"},"cell_type":"code","source":"fig, axes = plt.subplots(nrows=4, ncols=4, figsize=(15, 10))\naxes[0][0].imshow(img1, cmap='Greys_r')\naxes[1][0].imshow(apply_conv_filter(img1, grad_v), cmap='Greys_r')\naxes[2][0].imshow(apply_conv_filter(img1, grad_h), cmap='Greys_r')\naxes[3][0].imshow(apply_conv_filter(img1, grad), cmap='Greys_r')\n\naxes[0][1].imshow(img2, cmap='Greys_r')\naxes[1][1].imshow(apply_conv_filter(img2, grad_v), cmap='Greys_r')\naxes[2][1].imshow(apply_conv_filter(img2, grad_h), cmap='Greys_r')\naxes[3][1].imshow(apply_conv_filter(img2, grad), cmap='Greys_r')\n\naxes[0][2].imshow(img3, cmap='Greys_r')\naxes[1][2].imshow(apply_conv_filter(img3, grad_v), cmap='Greys_r')\naxes[2][2].imshow(apply_conv_filter(img3, grad_h), cmap='Greys_r')\naxes[3][2].imshow(apply_conv_filter(img3, grad), cmap='Greys_r')\n\naxes[0][3].imshow(dummy_img, cmap='Greys_r')\naxes[1][3].imshow(apply_conv_filter(dummy_img, grad_v), cmap='Greys_r')\naxes[2][3].imshow(apply_conv_filter(dummy_img, grad_h), cmap='Greys_r')\naxes[3][3].imshow(apply_conv_filter(dummy_img, grad), cmap='Greys_r')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2d05c02a56d9019a8d79ddf641fb88b7829113f7"},"cell_type":"markdown","source":"First, look at the dummy image. The vertical gradient filter only detects vertical edges. Moreover, the magnitude of the border in the *edge image* depends on the difference of pixel values along this border. This filter does exactly what it is supposed to do !!\nWe can get the same type of conclusion for the two other filters."},{"metadata":{"_uuid":"ae134386963ac76036028385f0ec521ec26f9744"},"cell_type":"markdown","source":"### Blur filters\nHere we will look at the effect of two blur filters, the *box* one and the gaussian one. The purpose of using a blur filter is to, well..., blur an image. This can be done to reduce the noise of the image, to remove weird pixels, to smooth the image."},{"metadata":{"_uuid":"080a413041875ab9eafa8b93a928973b7bb4a36c"},"cell_type":"markdown","source":"The box filter aims at replacing the old pixel value by the mean of all the pixel of its neighbourhood. Here, it's a 3*3 square neighbourhood.\nThe idea behind this is to smooth the image by taking into account multiple pixel values to reduce the impact of singular extreme points"},{"metadata":{"trusted":true,"_uuid":"7dc50fbc62384bda671ab487ea9bd6af215e31a6"},"cell_type":"code","source":"# Box filter\nbox_filter = np.array([[1, 1, 1],\n                      [1, 1, 1],\n                      [1, 1, 1]])\n\n# Here, we want to normalize the filter so the pixel values of our output\n# image remain in a valid range\nbox_filter = (1/9) * box_filter","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"64537a857aa9997faedf3856c64573e9c706832e"},"cell_type":"markdown","source":"The gaussian blur works the sme way, except it uses a ponderation in the computation of the local mean around a pixel, to give more weights to closer pixels."},{"metadata":{"trusted":true,"_uuid":"5a0942d3b9724eb0b7fa26dd4c71db14e163031c"},"cell_type":"code","source":"# Gaussian filter\ngauss_filter = np.array([[1, 2, 1],\n                          [2, 4, 2],\n                          [1, 2, 1]])\n\n# Here, we want to normalize the filter so the pixel values of our output\n# image remain in a valid range\ngauss_filter = (1/16) * box_filter","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"05adc33538a9c6b6f1d809bed15010219b46170e"},"cell_type":"markdown","source":"Let's look at the effect of these two new filters."},{"metadata":{"trusted":true,"_uuid":"1e55778dbac769c216635cfc7e28e982489b47d9"},"cell_type":"code","source":"# First, let's add some noise to the top-right corner of the dummy image\nfor i in range(32):\n    for j in range(32):\n        dummy_img[i][32+j] = random.randint(0, 25)\n\n# and a singular point\ndummy_img[42][42] = 255","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c7744c2f69f993efb2f97d6100962e48172e327f"},"cell_type":"code","source":"fig, axes = plt.subplots(nrows=3, ncols=4, figsize=(15, 10))\naxes[0][0].imshow(img1, cmap='Greys_r')\naxes[1][0].imshow(apply_conv_filter(img1, box_filter), cmap='Greys_r')\naxes[2][0].imshow(apply_conv_filter(img1, gauss_filter), cmap='Greys_r')\n\naxes[0][1].imshow(img2, cmap='Greys_r')\naxes[1][1].imshow(apply_conv_filter(img2, box_filter), cmap='Greys_r')\naxes[2][1].imshow(apply_conv_filter(img2, gauss_filter), cmap='Greys_r')\n\naxes[0][2].imshow(img3, cmap='Greys_r')\naxes[1][2].imshow(apply_conv_filter(img3, box_filter), cmap='Greys_r')\naxes[2][2].imshow(apply_conv_filter(img3, gauss_filter), cmap='Greys_r')\n\naxes[0][3].imshow(dummy_img, cmap='Greys_r')\naxes[1][3].imshow(apply_conv_filter(dummy_img, box_filter), cmap='Greys_r')\naxes[2][3].imshow(apply_conv_filter(dummy_img, gauss_filter), cmap='Greys_r')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c43e517747cd020d7156e7700c76597e1827bb06"},"cell_type":"markdown","source":"We can see on the dummy image that almost all the noise of the top right corner has disappeared. Also, the singularity has been smoothed. But we also lost details on the edges. Usually, these filters are used before other filters, like the edge detection ones, to smooth the image and to avoid to detect noise as small edges."},{"metadata":{"_uuid":"69c879c8942814d65dbd600dd14fd0d231878c17"},"cell_type":"markdown","source":"## Learned filters visualisation\nNow, let's train a very small CNN on the MNIST dataset from which we will try to interpret the learned 2D filters."},{"metadata":{"_kg_hide-input":false,"trusted":true,"_uuid":"8995845ea3709631f94c58386424c4d977c821b5"},"cell_type":"code","source":"import tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nimport keras\nfrom keras.layers import Conv2D, MaxPooling2D\nfrom keras.layers import Dense, Dropout, Flatten, Activation\nfrom keras.models import Sequential, Model\nfrom keras.optimizers import Adam\ntf.set_random_seed(42)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"trusted":true,"_uuid":"20f44beb9fcd01df2358480fa290f7ceaa83f932"},"cell_type":"code","source":"# Settings\ntrain_path = os.path.join('..', 'input', 'digit-recognizer', 'train.csv')\nraw_train_df = pd.read_csv(train_path)\n\n# CNN model settings\nsize = 28\nlr = 0.002\nnum_classes = 10\n\n# Training settings\nepochs = 10\nbatch_size = 128","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"47ab9f6b8b67259eb37ed67ea5177ba6be559177"},"cell_type":"code","source":"# Utils\ndef parse_train_df(_train_df):\n    labels = _train_df.iloc[:,0].values\n    imgs = _train_df.iloc[:,1:].values\n    imgs_2d = np.array([[[[float(imgs[index][i*28 + j]) / 255] for j in range(28)] for i in range(28)] for index in range(len(imgs))])\n    processed_labels = [[0 for _ in range(10)] for i in range(len(labels))]\n    for i in range(len(labels)):\n        processed_labels[i][labels[i]] = 1\n    return np.array(processed_labels), imgs_2d","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"04134ed3f6add048909269d91716b76abc7abe8e"},"cell_type":"code","source":"# Data preprocessing\ny_train_set, x_train_set = parse_train_df(raw_train_df)\n\nx_train, x_val, y_train, y_val = train_test_split(x_train_set, y_train_set, test_size=0.20, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"87c6183c95508805cbe748eccda8925ced5e2d8b"},"cell_type":"markdown","source":"One important thing to do here : visualize some input images. 28*28 grayscale images."},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"2fca712f3524df56e944ef0aa439d06256bb6093"},"cell_type":"code","source":"# Image visualization\nn = 5\nfig, axs = plt.subplots(nrows=n, ncols=n, sharex=True, sharey=True, figsize=(12, 12))\nfor i in range(n**2):\n    ax = axs[i // n, i % n]\n    (-x_train[i]+1)/2\n    ax.imshow((-x_train[i, :, :, 0] + 1)/2, cmap=plt.cm.gray)\n    ax.axis('off')\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1fecd96fc43a1e9c98818eb9fc88dc5fba7d6268"},"cell_type":"markdown","source":"### CNN model definition\nHere, I will define and train a rather small CNN model. I decided to use 2 Conv2D layers and 1 Dense layer (before the output layer)."},{"metadata":{"trusted":true,"_uuid":"a4f8edead98f73e25fcf8c6d11737b9a4274230c"},"cell_type":"code","source":"# CNN model\nmodel = keras.Sequential()\n\nmodel.add(Conv2D(6, kernel_size=(3, 3), strides=(1, 1),\n                 activation='relu',\n                 input_shape=(size, size, 1),\n                 name='conv_1'\n                ))\nmodel.add(Conv2D(12, (3, 3), activation='relu', name='conv_2'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.3))\n\nmodel.add(Flatten())\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dropout(0.25))\nmodel.add(Dense(num_classes, activation='softmax'))\n\nmodel.compile(loss=keras.losses.categorical_crossentropy,\n              optimizer=Adam(lr),\n              metrics=['accuracy'])\n\n# Training\ntraining_history = model.fit(\n    x_train,\n    y_train,\n    epochs=epochs,\n    verbose=1,\n    validation_data=(x_val, y_val),\n)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ba4ac66d070a00b83957c767cc64804ab84ca6aa"},"cell_type":"markdown","source":"Let's get the weights of the first layer. But first, let's look at the model summary."},{"metadata":{"trusted":true,"_uuid":"abb6981be4373e6e39537ee0fb5eacd12e98f18a"},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"68d775450c97f0682e988ece988dcfc76310d29e"},"cell_type":"markdown","source":"In the first layer, we have 60 parameters. Why 60 ?\n\nThe first layer is composed of 6 filters with a (3, 3) shape. So, each filter being composed of 9 parameters, this gives us 6*9=54 weights. The remaining 6 weights are 6 biases, one for each filter output.\n\nIn the next cell, I will fetch the weights of the first layer filters.\n"},{"metadata":{"trusted":true,"_uuid":"58417162c11ad9fe97ea2be69034b60dec4b60f1"},"cell_type":"code","source":"w = model.layers[0].get_weights()\nfilters_1_raw = w[0]\nbiases_1_raw = w[1]\n\nfilters_1 = [np.zeros((3, 3)) for _ in range(6)]\nfor i in range(3):\n    for j in range(3):\n        for n in range(6):\n            filters_1[n][i][j] = filters_1_raw[i][j][0][n]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0287636db738320e20ffbeb2c772ecdda0133f7b"},"cell_type":"code","source":"# Let's look at these filters\nfig, axes = plt.subplots(nrows=3, ncols=2, figsize=(15, 10))\naxes[0][0].imshow(filters_1[0], cmap='Greys_r')\naxes[1][0].imshow(filters_1[1], cmap='Greys_r')\naxes[2][0].imshow(filters_1[2], cmap='Greys_r')\n\naxes[0][1].imshow(filters_1[3], cmap='Greys_r')\naxes[1][1].imshow(filters_1[4], cmap='Greys_r')\naxes[2][1].imshow(filters_1[5], cmap='Greys_r')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d746eb65a24ade96f7c0521e724efc02ad77f9f1"},"cell_type":"code","source":"# Here we select an image of the MNIST dataset\nmnist_img = x_train[42, :, :, 0]\nplt.imshow(mnist_img, cmap='Greys_r')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"03fe25f50ef3032f5af07a73bb96a5f3c7dbf906"},"cell_type":"markdown","source":"Let's apply the first layer filters to our selected image."},{"metadata":{"trusted":true,"_uuid":"514fc0a94aa3545c70bad47fdc24b28cd3bff5ec"},"cell_type":"code","source":"layer_1_imgs = []\nfig, axes = plt.subplots(nrows=1, ncols=6, figsize=(15, 10))\nfor i in range(6):\n    img = apply_conv_filter(mnist_img, filters_1[i])\n    layer_1_imgs += [img]\n    axes[i].imshow(img, cmap='Greys_r')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2ad3f466ec7acf0aab090cbe7138b4a353a7eafb"},"cell_type":"markdown","source":"The six images above are the output of our CNN first layer (without the effect of the biases). This is what our CNN *sees* after the first layer. It's hard to understand exactly what our model is doing here...\nWe just need to understand that we used only 54 parameters to get this visualization. After this, our CNN will create 12 different filters for each one of the above images."},{"metadata":{"_uuid":"b0a4c61580f65ff975bfea72542c80f16735b981"},"cell_type":"markdown","source":"Now, let's try to visualize some feature maps, for our convolutionnal layers."},{"metadata":{"trusted":true,"_uuid":"1db802573cf4fb97441e5e2f7e9411162eeb1722"},"cell_type":"code","source":"layer_to_visualize = ['conv_1', 'conv_2']\n\nlayer_outputs = [layer.output for layer in model.layers if layer.name in layer_to_visualize]\nactivation_model = Model(inputs=model.input, outputs=layer_outputs)\nintermediate_activations = activation_model.predict(np.expand_dims(x_train[42], axis=0))\n\nn_layer = len(layer_to_visualize)\nn_img_per_layer = 6\nlayer_cpt = 0\n\nfig, axes = plt.subplots(nrows=n_layer, ncols=n_img_per_layer, figsize=(15, 10))\n\nfor layer_name, layer_activation in zip(layer_to_visualize, intermediate_activations):\n    \n    for j in range(n_img_per_layer):\n        axes[layer_cpt][j].imshow(layer_activation[0, :, :, j], cmap='Greys_r')\n        axes[layer_cpt][j].set_title(\"{} - map {}\".format(layer_name, j))\n        \n    layer_cpt += 1","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6db1e4bbf7e1d04650361b222019f6f848f98498"},"cell_type":"markdown","source":"This gives us a visualization of 6 feature maps per layer. The more deep we are, the more the feature map will react to abstract features :\n\n* First layer : edges, ...\n* Second layer : vertical edges, corners, ..."},{"metadata":{"_uuid":"7be4d04bfdd051e14d47deb150e852f3180f3334"},"cell_type":"markdown","source":"Please feel free to ask anything, to comment or to upvote :)"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}