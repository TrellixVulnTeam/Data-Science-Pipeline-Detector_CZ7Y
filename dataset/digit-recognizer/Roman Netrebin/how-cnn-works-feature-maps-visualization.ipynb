{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Hello everybody!\n\nIn this kernel:\n* let's study the basic set for loading data from сsv\n* describe the LeNet model\n* train this model and plot the loss and accuracy graphs for several epochs\n* predict labels for the test dataset\n* make the first commit in the competition\n\nIn addition, i'll show how you can visualize the kernels (filters) of the convolutional network, activate (collapse) the image with them and see what happened.","metadata":{}},{"cell_type":"code","source":"# load modules\n\nimport os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\nfrom torch.utils.data import DataLoader, Dataset, TensorDataset\nimport torchvision\nimport torchvision.transforms as tfs\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-11-27T11:58:36.724855Z","iopub.execute_input":"2021-11-27T11:58:36.725393Z","iopub.status.idle":"2021-11-27T11:58:39.609158Z","shell.execute_reply.started":"2021-11-27T11:58:36.725268Z","shell.execute_reply":"2021-11-27T11:58:39.608081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# How the kernel works\n\nPreviously, kernels were made by hand, and various filters in Photoshop are based on them. People have come up with many different filters, let's take a look at some of them.","metadata":{}},{"cell_type":"code","source":"!wget --no-check-certificate http://1.bp.blogspot.com/-yXpsAHaepf8/Vh4rDB-S71I/AAAAAAAAAHE/FG59U5jSwJY/s320/Architecture%2BColleges.jpg -O sample_photo.jpg","metadata":{"execution":{"iopub.status.busy":"2021-11-27T11:58:39.615863Z","iopub.execute_input":"2021-11-27T11:58:39.616681Z","iopub.status.idle":"2021-11-27T11:58:40.555329Z","shell.execute_reply.started":"2021-11-27T11:58:39.616632Z","shell.execute_reply":"2021-11-27T11:58:40.554198Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import cv2\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsns.set(style=\"white\")\nimg = cv2.imread(\"sample_photo.jpg\")\nRGB_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\nplt.figure(figsize=(12, 8))\nplt.imshow(RGB_img)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-27T11:58:40.557387Z","iopub.execute_input":"2021-11-27T11:58:40.558634Z","iopub.status.idle":"2021-11-27T11:58:41.348849Z","shell.execute_reply.started":"2021-11-27T11:58:40.558581Z","shell.execute_reply":"2021-11-27T11:58:41.347612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kernels = dict()\nkernels['Identical transformation'] = torch.tensor([[0, 0, 0], [0, 1, 0], [0, 0, 0]]) # \nkernels['Selecting horizontal borders'] = torch.tensor([[0, 1, 0], [0, -2, 0], [0, 1, 0]])\nkernels['Selecting vertical borders'] = torch.tensor([[0, 0, 0], [1, -2, 1], [0, 0, 0]])\nkernels['Highlighting borders'] = torch.tensor([[0, 1, 0], [1, -4, 1], [0, 1, 0]])\nkernels['Sharpening'] = torch.tensor([[0, -1, 0], [-1, 5, -1], [0, -1, 0]])\nkernels['Blur'] = torch.tensor([[0.0625, 0.125, 0.0625], [0.125, 0.25, 0.125], [0.0625, 0.125, 0.0625]])\nkernels['Contrast'] = torch.tensor([[-1, -1, -1], [-1, 9, -1], [-1, -1, -1]])\nkernels","metadata":{"execution":{"iopub.status.busy":"2021-11-27T11:58:41.351948Z","iopub.execute_input":"2021-11-27T11:58:41.352367Z","iopub.status.idle":"2021-11-27T11:58:41.448978Z","shell.execute_reply.started":"2021-11-27T11:58:41.3523Z","shell.execute_reply":"2021-11-27T11:58:41.447986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('manual filters')\nfig, axes = plt.subplots(1, 7, figsize=(12, 6))\nfor ax, (desc, kernel) in zip(axes.flatten(), kernels.items()):\n    ax.imshow(kernel, cmap='binary')\n    ax.set_xticks([])\n    ax.set_title(desc.replace(' ','\\n'))\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-27T11:58:41.450678Z","iopub.execute_input":"2021-11-27T11:58:41.451032Z","iopub.status.idle":"2021-11-27T11:58:42.135352Z","shell.execute_reply.started":"2021-11-27T11:58:41.450975Z","shell.execute_reply":"2021-11-27T11:58:42.134412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Apply these convolutions to the image, we get activation maps.","metadata":{}},{"cell_type":"code","source":"img_t = torch.from_numpy(RGB_img).type(torch.float32).unsqueeze(0)\nimg_t = img_t.permute(3, 0, 1, 2)\nimg_t = nn.ReflectionPad2d(1)(img_t)\n\nfor letter, kernel in kernels.items():\n    kernel = kernel.reshape(1, 1, 3, 3).type(torch.float32)\n\n    div = kernel.sum().item()\n    if div == 0:\n        div = 1\n\n    result = F.conv2d(img_t, kernel)\n    result = (result.squeeze().permute(1, 2, 0).numpy() / div).astype(np.int)\n\n    plt.figure(figsize=(12, 8))\n    plt.imshow(result)\n    plt.xlabel(letter)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-27T11:58:42.137383Z","iopub.execute_input":"2021-11-27T11:58:42.137997Z","iopub.status.idle":"2021-11-27T11:58:45.576693Z","shell.execute_reply.started":"2021-11-27T11:58:42.137952Z","shell.execute_reply":"2021-11-27T11:58:45.57577Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Imagine the learning process in your head.\n\nFor example, we have a network of one convolution as a feature extractor and one neuron as a classifier. This convolution is called a feature extractor, followed by a fully linked layer classifier. It weighs the last activation card and makes a prediction based on the amount of data entered.\n\nWe need to distinguish images 0 from 1. Zero is so round, and one is a stick from top to bottom.\n\nThe filter contains some initial values ​​(for example, small random values).\nWe have a loss function like squared error or cross entropy. First, its significance is great because we are wrong.\n\nWe start to give examples for input, the network learns, changes convolution and neuron using backpropagation\n\nThe convolution gradually turns, for example, into a shape similar to the selection of vertical lines (or into some other shape necessary for the task - this is the beauty of the fact that we cannot interpret the filter strictly), and the classifier also learns by weighting the activation of the input values in this filter. The losses decrease until we start getting meaningful values.\n\nNow let's look at a real example.","metadata":{}},{"cell_type":"markdown","source":"# Data loading","metadata":{}},{"cell_type":"code","source":"# look at files\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"execution":{"iopub.status.busy":"2021-11-27T11:58:45.577978Z","iopub.execute_input":"2021-11-27T11:58:45.578439Z","iopub.status.idle":"2021-11-27T11:58:45.596171Z","shell.execute_reply.started":"2021-11-27T11:58:45.578398Z","shell.execute_reply":"2021-11-27T11:58:45.595184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# some constants\n\nDEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\nBATCH_SIZE = 128","metadata":{"execution":{"iopub.status.busy":"2021-11-27T11:58:45.599112Z","iopub.execute_input":"2021-11-27T11:58:45.599415Z","iopub.status.idle":"2021-11-27T11:58:45.655972Z","shell.execute_reply.started":"2021-11-27T11:58:45.599373Z","shell.execute_reply":"2021-11-27T11:58:45.654594Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load data to pandas dataframe, split into images and their classes\n\ndataset_train = pd.read_csv(\"../input/digit-recognizer/train.csv\")\ndataset_train_labels = dataset_train.label\ndataset_train_images = dataset_train.drop('label', axis=1)\n\n# split labeled dataset into train and validation parts\n\ndataset = TensorDataset(\n    torch.Tensor(dataset_train_images.values / 255),\n    torch.LongTensor(dataset_train_labels.values),\n)\ndataset_tr, dataset_val = train_test_split(dataset, train_size=0.8)\n\n# make loader - its really comfortable\n\ndataloader_train = DataLoader(dataset_tr, batch_size=BATCH_SIZE, shuffle=True)\ndataloader_valid = DataLoader(dataset_val, batch_size=BATCH_SIZE)","metadata":{"execution":{"iopub.status.busy":"2021-11-27T11:58:45.657922Z","iopub.execute_input":"2021-11-27T11:58:45.658735Z","iopub.status.idle":"2021-11-27T11:58:52.079221Z","shell.execute_reply.started":"2021-11-27T11:58:45.658606Z","shell.execute_reply":"2021-11-27T11:58:52.078177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# let's take some images and look at them \n\nimages, labels = next(iter(dataloader_train))\nimages = images.numpy().reshape(BATCH_SIZE, 28, 28)\nfig, axes = plt.subplots(4, 4, figsize=(8, 8))\nfor ax, img, img_cls in zip(axes.flatten(), images, labels.numpy()):\n    ax.imshow(img, cmap='binary')\n    ax.set_xticks([])\n    ax.set_xlabel(int(img_cls), fontsize=20)\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-27T11:58:52.08195Z","iopub.execute_input":"2021-11-27T11:58:52.082632Z","iopub.status.idle":"2021-11-27T11:58:53.670607Z","shell.execute_reply.started":"2021-11-27T11:58:52.082586Z","shell.execute_reply":"2021-11-27T11:58:53.669551Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# LeNet","metadata":{"execution":{"iopub.status.busy":"2021-11-12T08:15:23.374661Z","iopub.execute_input":"2021-11-12T08:15:23.375044Z","iopub.status.idle":"2021-11-12T08:15:23.380343Z","shell.execute_reply.started":"2021-11-12T08:15:23.375003Z","shell.execute_reply":"2021-11-12T08:15:23.379123Z"}}},{"cell_type":"markdown","source":"**LeNet** is a convolutional neural network structure proposed by Yann LeCun et al. in 1989. In general, LeNet refers to LeNet-5 and is a simple convolutional neural network. Convolutional neural networks are a kind of feed-forward neural network whose artificial neurons can respond to a part of the surrounding cells in the coverage range and perform well in large-scale image processing.\n![LeNet](https://cdn.analyticsvidhya.com/wp-content/uploads/2021/03/Screenshot-from-2021-03-18-12-52-17.png)","metadata":{"execution":{"iopub.status.busy":"2021-11-12T08:14:11.826547Z","iopub.execute_input":"2021-11-12T08:14:11.826961Z","iopub.status.idle":"2021-11-12T08:14:11.835517Z","shell.execute_reply.started":"2021-11-12T08:14:11.826915Z","shell.execute_reply":"2021-11-12T08:14:11.833891Z"}}},{"cell_type":"code","source":"# describe LeNet model\n\nclass LeNet(nn.Module):\n    def __init__(self):\n        super(LeNet, self).__init__()\n        # 1 input image channel, 6 output channels, 3x3 square conv kernel\n        self.conv1 = nn.Conv2d(1, 6, 3)\n        self.conv2 = nn.Conv2d(6, 16, 3)\n        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.flatten = nn.Flatten()\n        self.fc1 = nn.Linear(5 * 5 * 16, 120)  \n        self.fc2 = nn.Linear(120, 84)\n        self.fc3 = nn.Linear(84, 10)\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        #print(x.shape)\n        x = self.flatten(x)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2021-11-27T11:58:53.673371Z","iopub.execute_input":"2021-11-27T11:58:53.673941Z","iopub.status.idle":"2021-11-27T11:58:53.685026Z","shell.execute_reply.started":"2021-11-27T11:58:53.673896Z","shell.execute_reply":"2021-11-27T11:58:53.683907Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train model","metadata":{}},{"cell_type":"code","source":"# use CrossEntropyLoss for multiclass classification and 'Adam' optimizer as good default choise\n\nmodel = LeNet().to(DEVICE)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters())\nloaders = {\"train\": dataloader_train, \"valid\": dataloader_valid}","metadata":{"execution":{"iopub.status.busy":"2021-11-27T11:58:56.964482Z","iopub.execute_input":"2021-11-27T11:58:56.964842Z","iopub.status.idle":"2021-11-27T11:59:00.287151Z","shell.execute_reply.started":"2021-11-27T11:58:56.964809Z","shell.execute_reply":"2021-11-27T11:59:00.285692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train model\n\nmax_epochs = 10\n\nlosses = {\"train\": [], \"valid\": []}\naccuracies = {\"train\": [], \"valid\": []}\n\nfor epoch in range(max_epochs):\n    print(f\"Epoch: {epoch+1}\")\n    for phase, dataloader in loaders.items():\n        epoch_loss, epoch_correct, epoch_all = 0, 0, 0\n        for images, labels in dataloader:\n            images = images.reshape(images.shape[0], 1, 28, 28)\n            images = images.to(DEVICE)\n            labels = labels.to(DEVICE)\n            if phase == \"train\":\n                model.train()\n                optimizer.zero_grad()\n                output = model(images)\n                loss = criterion(output, labels)\n                loss.backward()\n                optimizer.step()\n            else:\n                model.eval()\n                with torch.no_grad():\n                    output = model(images)\n                    loss = criterion(output, labels)\n            preds = output.argmax(-1)\n            correct =  (preds == labels).sum()\n            epoch_loss += loss.item() * BATCH_SIZE\n            epoch_correct += correct.item()\n            epoch_all += BATCH_SIZE\n        \n        print('DataLoader: {}. Loss: {}. Accuracy: {}'.format(\n            phase, epoch_loss / epoch_all, epoch_correct / epoch_all\n        ))\n        losses[phase].append(epoch_loss / epoch_all)\n        accuracies[phase].append(epoch_correct / epoch_all)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n\n# look at loss function...\n\naxes[0].plot(losses['train'], label='train')\naxes[0].plot(losses['valid'], label='valid')\naxes[0].grid()\naxes[0].legend()\naxes[0].set_title('loss')\n\n# ...and accuracy.\n\naxes[1].plot(accuracies['train'], label='train')\naxes[1].plot(accuracies['valid'], label='valid')\naxes[1].grid()\naxes[1].legend()\naxes[1].set_title('accuracy')\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Predict","metadata":{"execution":{"iopub.status.busy":"2021-11-12T08:21:56.851271Z","iopub.execute_input":"2021-11-12T08:21:56.851725Z","iopub.status.idle":"2021-11-12T08:21:56.855503Z","shell.execute_reply.started":"2021-11-12T08:21:56.851692Z","shell.execute_reply":"2021-11-12T08:21:56.854907Z"}}},{"cell_type":"code","source":"# load test data\n\ndataset_test = pd.read_csv(\"../input/digit-recognizer/test.csv\")\ndataloader_test = DataLoader(torch.Tensor(dataset_test.values / 255), batch_size=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# predict classes for test dataset\n\npreds = []\nmodel.eval()\nfor image in dataloader_test:\n    image = image.reshape(image.shape[0], 1, 28, 28)\n    image = image.to(DEVICE)\n    output = model(image)\n    pred = output.argmax(-1)\n    preds.append(int(pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# form submission file\n\n# submission = pd.read_csv('/kaggle/input/digit-recognizer/sample_submission.csv', index_col='ImageId')\n# submission['Label'] = preds\n# submission.to_csv('./submission.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This simple convolutional network (one of the first) puts you right in the middle of the leaderboard. You can now use this baseline to reschedule your workout and easily climb much higher.\n\nBEST SCORE 0.98503 \n\nLook at my another notebooks, e.g. [Intro to PyTorch Transfer Learnining](https://www.kaggle.com/imcr00z/intro-to-pytorch-transfer-learnining). Then we will try to increase the score.","metadata":{}},{"cell_type":"markdown","source":"# Visualize Kernels","metadata":{}},{"cell_type":"markdown","source":"The convolutional network consists of an feature extractor and a classifier.\n\nThe kernels that the network uses and trains may not be understood by humans, but they are suitable for transforming an image into a set of features. These features are then fed to the input of the classifier, and already by training it, we get the probability of the class.\n\nThe darker the pixel, the higher the value. During training, the network forms 3x3 filters and after training they do not change. These filters highlight important features.","metadata":{}},{"cell_type":"code","source":"# look at the trained filters\n\nprint('convolution layer 1')\nfig, axes = plt.subplots(1, 6, figsize=(9, 2))\nfor ax, kernel in zip(axes.flatten(), model.conv1.weight):\n    ax.imshow(kernel[0, :, :].detach().cpu().numpy(), cmap='binary')\n    ax.set_xticks([])\nplt.tight_layout()\nplt.show()\n\n\nprint('convolution layer 2')\nfig, axes = plt.subplots(2, 8, figsize=(12, 3))\nfor ax, kernel in zip(axes.flatten(), model.conv2.weight):\n    ax.imshow(kernel[0, :, :].detach().cpu().numpy(), cmap='binary')\n    ax.set_xticks([])\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"During training, the network forms 3x3 filters and after training they do not change. These filters highlight important features.\n\nAt the first level, these are simple signs - straight lines, turns - the network itself learns what needs to be highlighted.","metadata":{}},{"cell_type":"markdown","source":"# Visualize Feature Maps","metadata":{}},{"cell_type":"markdown","source":"By applying trained filters to the image, we get level 1 activation maps. Based on the level 1 activation maps and the level 2 filters, we get more general characteristics. Having weighed these cards, the classifier can already understand what this figure is.\n\nThese are resizable maps with “highlighted” features. Try to play with the code, enter a different number and see.\n\nWe then send the maps to a second level of filters, where they highlight some of the more general characteristics specific to the dataset.\nYou should understand that we are not setting filters manually - they are generated.","metadata":{}},{"cell_type":"code","source":"# make hooks to intercept the activation map\n\nfeature_map = dict()\n\ndef get_feature_map_conv1():\n    def hook(model, input, output):\n        feature_map['conv1'] = output.detach()\n    return hook\n\ndef get_feature_map_conv2():\n    def hook(model, input, output):\n        feature_map['conv2'] = output.detach()\n    return hook","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# take the map when predicting (with forward distribution)\n\nmodel.conv1.register_forward_hook(get_feature_map_conv1())\nmodel.conv2.register_forward_hook(get_feature_map_conv2())\nmodel.eval()\nimage = next(iter(dataloader_test))\nimage = image.reshape(image.shape[0], 1, 28, 28)\nimage = image.to(DEVICE)\noutput = model(image)\n\n# show image\n\nplt.imshow(image.detach().cpu().numpy().reshape(28, 28), cmap='binary')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_map['conv1'].shape, feature_map['conv2'].shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# look at feature maps\n\nprint('convolution layer 1')\nfig, axes = plt.subplots(1, 6, figsize=(9, 2))\nfor ax, fmap in zip(axes.flatten(), feature_map['conv1'].squeeze()):\n    ax.imshow(fmap.detach().cpu().numpy(), cmap='binary')\n    ax.set_xticks([])\nplt.tight_layout()\nplt.show()\n\n\nprint('convolution layer 2')\nfig, axes = plt.subplots(2, 8, figsize=(12, 3))\nfor ax, fmap in zip(axes.flatten(), feature_map['conv2'].squeeze()):\n    ax.imshow(fmap.detach().cpu().numpy(), cmap='binary')\n    ax.set_xticks([])\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Each pixel of these maps is associated with a fully connected classifier layer (look at the model, yes, with 400 neurons of the first classifier layer). By weighing these pixels, the classifier on the second layer receives 120 values, then 84, then 10 class probabilities.","metadata":{}},{"cell_type":"markdown","source":"# High score\n\nSo, we need a high score to rank high in the competition. Let's take a network like in my notebook [Intro to PyTorch Transfer Learnining](https://www.kaggle.com/imcr00z/intro-to-pytorch-transfer-learnining).\n\nLet's take the alexnet architecture and slightly change it for our task:\n* resize the convolutions as our 28x28px image will collapse to 0 by default\n* let's change the classifier - we don't need such a powerful one, and the main number of weights is in the fully connected part","metadata":{}},{"cell_type":"code","source":"alexnet = torchvision.models.alexnet()\nalexnet","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Then I will change it a little, uncomment the first, second, third design in turn (I will leave the rest commented out). See how the introduction of additional layers changes the speed of learning, the quality of predictions, and how this affects overfit. Look at the charts.","metadata":{"execution":{"iopub.status.busy":"2021-11-24T08:19:55.735467Z","iopub.execute_input":"2021-11-24T08:19:55.736212Z","iopub.status.idle":"2021-11-24T08:19:55.744729Z","shell.execute_reply.started":"2021-11-24T08:19:55.736176Z","shell.execute_reply":"2021-11-24T08:19:55.743784Z"}}},{"cell_type":"code","source":"# alexnet = nn.Sequential(\n#     nn.Conv2d(1, 64, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1)),\n#     nn.ReLU(inplace=True),\n    \n#     nn.MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False),\n    \n#     nn.Conv2d(64, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n#     nn.ReLU(inplace=True),\n    \n#     nn.MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False),\n    \n#     nn.Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n#     nn.ReLU(inplace=True),\n    \n#     nn.Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n#     nn.ReLU(inplace=True),\n    \n#     nn.Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), padding=(1, 1)),\n#     nn.ReLU(inplace=True),\n    \n#     nn.MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False),\n    \n#     nn.Flatten(),\n#     nn.Linear(in_features=2304, out_features=10, bias=True)\n# )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here I put batch norms after the convolutions, this slightly improves the quality.","metadata":{}},{"cell_type":"code","source":"# alexnet = nn.Sequential(\n#     nn.Conv2d(1, 64, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1)),\n#     nn.BatchNorm2d(64),\n#     nn.ReLU(inplace=True),\n    \n#     nn.MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False),\n\n#     nn.Conv2d(64, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n#     nn.BatchNorm2d(192),\n#     nn.ReLU(inplace=True),\n\n#     nn.MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False),\n\n#     nn.Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n#     nn.BatchNorm2d(384),\n#     nn.ReLU(inplace=True),\n\n#     nn.Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n#     nn.BatchNorm2d(256),\n#     nn.ReLU(inplace=True),\n\n#     nn.Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), padding=(1, 1)),\n#     nn.BatchNorm2d(256),\n#     nn.ReLU(inplace=True),\n \n#     nn.MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False),\n    \n#     nn.Flatten(),\n#     nn.Linear(in_features=2304, out_features=10, bias=True)\n# )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Dropouts have been added here. This increases training time but improves quality.","metadata":{}},{"cell_type":"code","source":"alexnet = nn.Sequential(\n    nn.Conv2d(1, 64, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1)),\n    nn.BatchNorm2d(64),\n    nn.ReLU(inplace=True),\n\n    nn.MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False),\n    nn.Dropout(0.5),\n\n    nn.Conv2d(64, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n    nn.BatchNorm2d(192),\n    nn.ReLU(inplace=True),\n\n    nn.MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False),\n    nn.Dropout(0.5),\n\n    nn.Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n    nn.BatchNorm2d(384),\n    nn.ReLU(inplace=True),\n\n    nn.Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n    nn.BatchNorm2d(256),\n    nn.ReLU(inplace=True),\n\n    nn.Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), padding=(1, 1)),\n    nn.BatchNorm2d(256),\n    nn.ReLU(inplace=True),\n\n    nn.MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False),\n    nn.Dropout(0.5),\n\n    nn.Flatten(),\n    nn.Linear(in_features=2304, out_features=10, bias=True)\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"alexnet = alexnet.to(DEVICE)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install torchsummary\nfrom torchsummary import summary","metadata":{"execution":{"iopub.status.busy":"2021-11-27T11:59:17.067671Z","iopub.execute_input":"2021-11-27T11:59:17.067961Z","iopub.status.idle":"2021-11-27T11:59:28.826044Z","shell.execute_reply.started":"2021-11-27T11:59:17.067929Z","shell.execute_reply":"2021-11-27T11:59:28.824824Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"summary(alexnet, input_size=(1, 28, 28), batch_size=BATCH_SIZE)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(alexnet.parameters())\nloaders = {\"train\": dataloader_train, \"valid\": dataloader_valid}\n# train model\nmax_epochs = 20\nlosses = {\"train\": [], \"valid\": []}\naccuracies = {\"train\": [], \"valid\": []}\n\nfor epoch in range(max_epochs):\n    print(f\"Epoch: {epoch+1}\")\n    for phase, dataloader in loaders.items():\n        epoch_loss, epoch_correct, epoch_all = 0, 0, 0\n        for images, labels in dataloader:\n            images = images.reshape(images.shape[0], 1, 28, 28)\n            images = images.to(DEVICE)\n            labels = labels.to(DEVICE)\n            if phase == \"train\":\n                alexnet.train()\n                optimizer.zero_grad()\n                output = alexnet(images)\n                loss = criterion(output, labels)\n                loss.backward()\n                optimizer.step()\n            else:\n                alexnet.eval()\n                with torch.no_grad():\n                    output = alexnet(images)\n                    loss = criterion(output, labels)\n            preds = output.argmax(-1)\n            correct =  (preds == labels).sum()\n            epoch_loss += loss.item() * BATCH_SIZE\n            epoch_correct += correct.item()\n            epoch_all += BATCH_SIZE\n        \n        print('DataLoader: {}. Loss: {}. Accuracy: {}'.format(\n            phase, epoch_loss / epoch_all, epoch_correct / epoch_all\n        ))\n        losses[phase].append(epoch_loss / epoch_all)\n        accuracies[phase].append(epoch_correct / epoch_all)","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n\n# look at loss function...\n\naxes[0].plot(losses['train'], label='train')\naxes[0].plot(losses['valid'], label='valid')\naxes[0].grid()\naxes[0].legend()\naxes[0].set_title('loss')\n\n# ...and accuracy.\n\naxes[1].plot(accuracies['train'], label='train')\naxes[1].plot(accuracies['valid'], label='valid')\naxes[1].grid()\naxes[1].legend()\naxes[1].set_title('accuracy')\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# preds = []\n# alexnet.eval()\n# for image in dataloader_test:\n#     image = image.reshape(image.shape[0], 1, 28, 28)\n#     image = image.to(DEVICE)\n#     output = alexnet(image)\n#     pred = output.argmax(-1)\n#     preds.append(int(pred))\n# submission = pd.read_csv('/kaggle/input/digit-recognizer/sample_submission.csv', index_col='ImageId')\n# submission['Label'] = preds\n# submission.to_csv('./submission.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"BEST SCORE 0.99160\n\nMost likely, with the current data, it is unlikely that it will be possible to significantly improve the quality. You need augmentation to the dataset, try this.","metadata":{}},{"cell_type":"markdown","source":"# Augmentations","metadata":{}},{"cell_type":"code","source":"class CustomTensorDataset(Dataset):\n    \"\"\"TensorDataset with support of transforms.\n    \"\"\"\n    def __init__(self, tensors, transform=None):\n        assert all(tensors[0].size(0) == tensor.size(0) for tensor in tensors)\n        self.tensors = tensors\n        self.transform = transform\n\n    def __getitem__(self, index):\n        x = self.tensors[0][index]\n        if self.transform:\n            x = x.reshape(1, 28,28)\n            x = self.transform(x)\n            x = x.flatten()\n        y = self.tensors[1][index]\n        return x, y\n\n    def __len__(self):\n        return self.tensors[0].size(0)","metadata":{"execution":{"iopub.status.busy":"2021-11-27T11:59:28.829242Z","iopub.execute_input":"2021-11-27T11:59:28.829702Z","iopub.status.idle":"2021-11-27T11:59:28.840993Z","shell.execute_reply.started":"2021-11-27T11:59:28.829623Z","shell.execute_reply":"2021-11-27T11:59:28.839567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transform = tfs.Compose([\n    tfs.RandomAffine(degrees=10, translate=(0.1, 0.1), scale=(0.9, 1.1), fill=0),\n    tfs.Normalize(0.5, 0.5)\n])","metadata":{"execution":{"iopub.status.busy":"2021-11-27T11:59:28.842824Z","iopub.execute_input":"2021-11-27T11:59:28.843434Z","iopub.status.idle":"2021-11-27T11:59:28.854453Z","shell.execute_reply.started":"2021-11-27T11:59:28.843379Z","shell.execute_reply":"2021-11-27T11:59:28.853436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = CustomTensorDataset(tensors=(\n    torch.Tensor(dataset_train_images.values),\n    torch.LongTensor(dataset_train_labels.values)\n), transform=transform)\ndataset_tr, dataset_val = train_test_split(dataset, train_size=0.8)\ndataloader_train = DataLoader(dataset_tr, batch_size=BATCH_SIZE, shuffle=True)\ndataloader_valid = DataLoader(dataset_val, batch_size=BATCH_SIZE)","metadata":{"execution":{"iopub.status.busy":"2021-11-27T11:59:28.858674Z","iopub.execute_input":"2021-11-27T11:59:28.859471Z","iopub.status.idle":"2021-11-27T11:59:57.772614Z","shell.execute_reply.started":"2021-11-27T11:59:28.859392Z","shell.execute_reply":"2021-11-27T11:59:57.771601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = nn.Sequential(\n    nn.Conv2d(1, 64, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1)),\n    nn.BatchNorm2d(64),\n    nn.ReLU(inplace=True),\n\n    nn.Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1)),\n    nn.BatchNorm2d(64),\n    nn.ReLU(inplace=True),\n\n    nn.MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False),\n    nn.Dropout(0.5),\n\n    nn.Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n    nn.BatchNorm2d(64),\n    nn.ReLU(inplace=True),\n\n    nn.Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n    nn.BatchNorm2d(64),\n    nn.ReLU(inplace=True),\n\n    nn.MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False),\n    nn.Dropout(0.5),\n\n    nn.Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n    nn.BatchNorm2d(64),\n    nn.ReLU(inplace=True),\n\n    nn.Flatten(),\n    nn.Linear(in_features=1600, out_features=256, bias=True),\n    nn.Dropout(0.5),\n    nn.Linear(in_features=256, out_features=10, bias=True)\n)","metadata":{"execution":{"iopub.status.busy":"2021-11-27T11:59:57.774084Z","iopub.execute_input":"2021-11-27T11:59:57.774431Z","iopub.status.idle":"2021-11-27T11:59:57.800094Z","shell.execute_reply.started":"2021-11-27T11:59:57.774364Z","shell.execute_reply":"2021-11-27T11:59:57.799106Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = model.to(DEVICE)\nsummary(model, input_size=(1, 28, 28), batch_size=BATCH_SIZE)","metadata":{"execution":{"iopub.status.busy":"2021-11-27T12:00:22.51265Z","iopub.execute_input":"2021-11-27T12:00:22.513474Z","iopub.status.idle":"2021-11-27T12:00:28.573907Z","shell.execute_reply.started":"2021-11-27T12:00:22.513421Z","shell.execute_reply":"2021-11-27T12:00:28.572987Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = model.to(DEVICE)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters())\nloaders = {\"train\": dataloader_train, \"valid\": dataloader_valid}\n# train model\nmax_epochs = 50\nlosses = {\"train\": [], \"valid\": []}\naccuracies = {\"train\": [], \"valid\": []}\n\nfor epoch in range(max_epochs):\n    print(f\"Epoch: {epoch+1}\")\n    for phase, dataloader in loaders.items():\n        epoch_loss, epoch_correct, epoch_all = 0, 0, 0\n        for images, labels in dataloader:\n            images = images.reshape(images.shape[0], 1, 28, 28)\n            images = images.to(DEVICE)\n            labels = labels.to(DEVICE)\n            if phase == \"train\":\n                model.train()\n                optimizer.zero_grad()\n                output = model(images)\n                loss = criterion(output, labels)\n                loss.backward()\n                optimizer.step()\n            else:\n                model.eval()\n                with torch.no_grad():\n                    output = model(images)\n                    loss = criterion(output, labels)\n            preds = output.argmax(-1)\n            correct =  (preds == labels).sum()\n            epoch_loss += loss.item() * BATCH_SIZE\n            epoch_correct += correct.item()\n            epoch_all += BATCH_SIZE\n        \n        print('DataLoader: {}. Loss: {}. Accuracy: {}'.format(\n            phase, epoch_loss / epoch_all, epoch_correct / epoch_all\n        ))\n        losses[phase].append(epoch_loss / epoch_all)\n        accuracies[phase].append(epoch_correct / epoch_all)","metadata":{"execution":{"iopub.status.busy":"2021-11-27T12:00:31.556901Z","iopub.execute_input":"2021-11-27T12:00:31.557187Z","iopub.status.idle":"2021-11-27T12:02:26.760092Z","shell.execute_reply.started":"2021-11-27T12:00:31.557156Z","shell.execute_reply":"2021-11-27T12:02:26.758452Z"},"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n\n# look at loss function...\n\naxes[0].plot(losses['train'], label='train')\naxes[0].plot(losses['valid'], label='valid')\naxes[0].grid()\naxes[0].legend()\naxes[0].set_title('loss')\n\n# ...and accuracy.\n\naxes[1].plot(accuracies['train'], label='train')\naxes[1].plot(accuracies['valid'], label='valid')\naxes[1].grid()\naxes[1].legend()\naxes[1].set_title('accuracy')\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = []\nalexnet.eval()\nfor image in dataloader_test:\n    image = image.reshape(image.shape[0], 1, 28, 28)\n    image = image.to(DEVICE)\n    output = alexnet(image)\n    pred = output.argmax(-1)\n    preds.append(int(pred))\nsubmission = pd.read_csv('/kaggle/input/digit-recognizer/sample_submission.csv', index_col='ImageId')\nsubmission['Label'] = preds\nsubmission.to_csv('./submission.csv')","metadata":{},"execution_count":null,"outputs":[]}]}