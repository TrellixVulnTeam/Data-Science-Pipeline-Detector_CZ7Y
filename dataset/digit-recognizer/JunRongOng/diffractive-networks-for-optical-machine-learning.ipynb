{"cells":[{"metadata":{"_uuid":"f94fbc52f89fbf672968c2aaaa092db4388aef1f"},"cell_type":"markdown","source":"**Reproduction of results from \"All-Optical Machine Learning Using Diffractive Deep Neural Networks\"**\n\nhttps://arxiv.org/pdf/1810.01916.pdf\n\nJournal Paper Summary: The authors introduce a device they call a \"Diffractive Neural Network\" which consists of a series of 3D-printed plates which modulate the phase and amplitude of optical wavefronts. The plates have an array of indentations acting as nodes or \"neurons\" with learnable parameters like thickness and attenuation and are densely connected via optical diffraction. An input wavefront propagates through the plates and at the output the light is focused onto an arrangement of photo-detectors, each detector target representing the identification of a certain written digit e.g. \"0\" to \"9\". \n\nUsing the tensorflow framework, they model the series of plates and optimize or \"train\" the learnable parameters, demonstrating that their series of 3D-printed plates can perform classification of handwritted digits (MNIST). \n\nBelow is an attempt to reproduce the results from the above paper by Xing Lin et al. from UCLA. "},{"metadata":{"trusted":true,"_uuid":"53ea5ebe84a9eb0756ebd3e1d0586cf8d090b36a"},"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport math\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n#tf.enable_eager_execution() #eager execution","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d37ad1b0c3dfde4cbce6f1850ce5552fec33066c"},"cell_type":"markdown","source":"I have predefined a matrix which acts as a mask denoting the areas which are photo-sensitive i.e. photodetectors. The \"light\" which falls onto these areas will be collected and summed. "},{"metadata":{"trusted":true,"_uuid":"19e189ece78b37a8d7db67021517eac41dc71f8a"},"cell_type":"code","source":"#loading the dataset.......(Detector)\ndetector = pd.read_csv(\"../input/photodetector-spatial-layout/detector_template_28.txt\",delimiter=' ')\nprint(detector.shape)\ndetector.head()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"28b6249e1259a85a78288d09a884619f005aa4d9"},"cell_type":"code","source":"#loading the dataset.......(train,test)\ntrain = pd.read_csv(\"../input/digit-recognizer/train.csv\",delimiter=',',engine='c',na_filter=False,dtype=np.float32,low_memory=False)\ntest = pd.read_csv(\"../input/digit-recognizer/test.csv\",delimiter=',',engine='c',na_filter=False,dtype=np.float32,low_memory=False)\n\nm,n = train.shape\nmt,nt = test.shape\nprint(m,n)\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8b9d2a0f0d47b8b5e40c32a15c15af0deb388b55"},"cell_type":"code","source":"x_train = (train.iloc[:,1:].values).astype('float32') # all pixel values\ny_train = train.iloc[:,0].values.astype('int32') # only labels i.e targets digits\n\nx_test = (test.iloc[:,:].values).astype('float32') # all pixel values\n\nall_detectors = detector.iloc[0,:].values.astype('float32') # detector spatial layout\ndetector_template = detector.iloc[1:,].values.astype('float32') # individual detector positions\n\n#Plot Detector layout\nplt.figure()\nplt.imshow(all_detectors.reshape((28,28)))\nplt.title('Photodetector Layout')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f26aac1b4437959770e8859fb68a9165bda16fa3"},"cell_type":"code","source":"#Normalize the data and check data type \nx_train = x_train/255.0\nx_test = x_test/255.0\nall_detectors = all_detectors.reshape((1,n-1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"57622b609827dd875b181ec209df4b5556463184"},"cell_type":"code","source":"print('x_train shape:', x_train.shape)\nprint('x_train type:', x_train.dtype)\nprint('y_train shape:', y_train.shape)\nprint('y_train type:', y_train.dtype)\nprint('x_test shape:', x_test.shape)\nprint('x_test type:', x_test.dtype)\nprint('detector shape:', detector_template.shape)\nprint('detector type:', detector_template.dtype)\nprint('all detector shape:', all_detectors.shape)\nprint('all detector type:', all_detectors.dtype)\nprint(x_train.shape[0], 'train samples')\nprint(x_test.shape[0], 'test samples')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3cef2227d3941aa3fb2fbb8f4af9c81750a07a1d"},"cell_type":"markdown","source":"Since the distance between plates is the same for all the series of plates, I will pre-calculate the diffraction occuring at each plate and store the results as a numpy array to be used later. \n\nIn the journal paper, each plate has 200x200 nodes. Similarly, their input data should have 200x200 pixels. The kaggle dataset, as well as the original MNIST data, consists of images with 28x28 pixels. Hence, I have scaled down the physical system model in my simulation to fit the kaggle dataset, using their setup as a guide.\n\nFor the diffraction, I follow the paper and use the Rayleigh-Sommerfeld equation. "},{"metadata":{"trusted":true,"_uuid":"63b1d124e523fc09eace7154d245873956b5b779"},"cell_type":"code","source":"#Free-space propagation (diffraction)\n#Plate size in paper is 8cm x 8cm \n\nN = 28 #pixels per dimension\nz = 1.05e-2 #3 cm between plates\nweights = np.empty([N**2,N**2],dtype=complex)\nlam = 299792458/0.4e12 #0.4 THz\nprint(1e3*lam) #in mm\n\nD = N*(lam/2)\nx1 = np.linspace(0,D,N)\ny1 = np.linspace(0,D,N)\n\nx2 = np.linspace(0,D,N)\ny2 = np.linspace(0,D,N)\n\nxg1,yg1,xg2,yg2 = np.meshgrid(x1,y1,x2,y2)\n\nr = np.sqrt((xg1 - xg2)**2 + (yg1 - yg2)**2 + z**2)\ndA = (D/N)**2 #differential area\nw = (z/r**2) * (1/(2*np.pi*r) + 1/(1j*lam)) * np.exp(1j*2*np.pi*r/lam) * dA\n\nweights = w.reshape(N**2,N**2)\nprint(weights.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f6defa13aec914164eb4b5c129e64ebabd90c619"},"cell_type":"markdown","source":"For verification, I plot the diffraction pattern that is output from a single node and arrives at the next printed plate. "},{"metadata":{"trusted":true,"_uuid":"3013b5549bfb51a6d6ef6c7d9a62d561eca05dd6"},"cell_type":"code","source":"#Plot Weight Function\n\nplt.figure(figsize=(8,12))\nwe = weights[:,int(N*N/2+N/2)].reshape((N,N))\n\nplt.subplot(2,1,1)\nplt.imshow(np.abs(we)/np.max(np.abs(we)))\nplt.colorbar()\nplt.clim(0,1)\nplt.title('Normalized Amplitude')\n\nplt.subplot(2,1,2)\nplt.imshow(np.angle(we),cmap='jet')\nplt.colorbar()\nplt.clim(0,math.pi)\nplt.title('Phase Angle')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1d3794eb511f4b78c8f5d4f08f1ac92669b37896"},"cell_type":"markdown","source":"It will be interesting to see what happens to the handwritten digit image after it passes through the 3D-printed plate and undergoes diffraction. "},{"metadata":{"trusted":true,"_uuid":"a6589d52e912c385435bd0a5033850aab5476365"},"cell_type":"code","source":"#Visualize Diffraction\ntest_input = x_train[1,:]\nplt.imshow(test_input.reshape((N,N)))\nplt.title('Input Digit')\nplt.show()\n\ntest_output = np.matmul(test_input,np.transpose(weights))\nprint(test_output.shape)\nplt.imshow(np.abs(test_output.reshape((N,N),order='F')))\nplt.title('Digit After Diffraction')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"45048279246c55496d2ec97781defc41858c96c4"},"cell_type":"markdown","source":"Now we have the dataset and the physical model ready. We can start the training of the Diffractive Network. "},{"metadata":{"trusted":true,"_uuid":"32f0b4409a1736438f4d34981b434c7345a7f293"},"cell_type":"code","source":"#Training parameters\nn_epochs = 50\nbatch_size = 10\nn_batches = int(np.ceil(m / batch_size))\nlearning_rate = 0.001","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ea7ecdfeac7881696a0be0707189e3af6350cfed"},"cell_type":"code","source":"tf.reset_default_graph()\n\n#Construct Tensorflow Graph\ninputs = tf.placeholder(tf.complex64, shape=(batch_size,N*N),name='inputs') #input test cases\nlabels = tf.placeholder(tf.int32, shape=(batch_size,),name='labels') #labels for test cases\nw = tf.constant(weights, dtype=tf.complex64, name='w') #N**2 x N**2\nd = tf.constant(detector_template, dtype=tf.float32, name='detectors') #10 x N**2\nall_d = tf.constant(all_detectors, dtype=tf.float32, name='all_detectors') #1 x N**2\n\n#Input layer\ninputs1 = tf.matmul(inputs,tf.transpose(w))  \n\ndef diffraction_layer(X):\n    #Complex amplitude + phase of optical waves\n    a = tf.Variable(tf.constant(1,shape=[1,N*N],dtype=tf.float32),name='alpha')\n    t = tf.Variable(tf.constant(0.5,shape=[1,N*N],dtype=tf.float32),name='theta')\n    pi = tf.constant(math.pi,dtype=tf.float32)\n    theta = 2*pi*t #phase in exp(j*theta)\n    phase_mod = tf.complex(tf.cos(theta),tf.sin(theta),name='phase_mod')\n    a_mod = tf.nn.relu(a) / tf.reduce_max(tf.nn.relu(a))\n    amp_mod = tf.cast(a_mod,dtype=tf.complex64,name='amp_mod')\n    com_mod = tf.multiply(amp_mod,phase_mod,name='com_mod')\n\n    layer = X * tf.tile(com_mod,[batch_size,1]) #BS x N**2 \n    return tf.matmul(layer,tf.transpose(w)) \n\n#Five 3D printed plates\noutputs1 = diffraction_layer(inputs1)\noutputs2 = diffraction_layer(outputs1)\noutputs3 = diffraction_layer(outputs2)\noutputs4 = diffraction_layer(outputs3)\noutputsf = diffraction_layer(outputs4)\n\n#Photodetectors\noutputs_mask = tf.abs(outputsf) * tf.tile(all_d,[batch_size,1])\n\ndef normalize_output(A):\n    m0,n0 = A.shape\n    norm = tf.tile(tf.reshape(tf.reduce_max(A,axis=1),[m0,1]),[1,n0])\n    norm_A = tf.square(A/norm)\n    return norm_A\n\n#Cross Entropy\nout = tf.matmul(tf.abs(outputsf),tf.transpose(d)) #only sum over detectors\nout_norm = normalize_output(out)\nsm = tf.nn.softmax(logits=out_norm,name='soft_max')\nxent = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels,logits=out_norm)\nloss = tf.reduce_mean(xent,name='loss')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a3a2a84d2bbc75977c9acfc510a317526fa687c1"},"cell_type":"code","source":"#Optimizer\n#optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n#optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate,momentum=0.9)\noptimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n\ntraining_op = optimizer.minimize(loss)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true,"_uuid":"3ddacbf1d000f55e71fc9c1cb04d82dd4a09ce72"},"cell_type":"code","source":"#Mini-Batch Gradient Descent\ndef fetch_batch(epoch, batch_index, batch_size):\n    np.random.seed(epoch * n_batches + batch_index)  \n    indices = np.random.randint(m, size=batch_size)  \n    X_batch = x_train[indices,]\n    y_batch = y_train[indices,] \n    return X_batch, y_batch\n\ninit = tf.global_variables_initializer() \nsaver = tf.train.Saver()\n\n#Summary\nloss_summary = tf.summary.scalar('Loss', loss)\nfile_writer = tf.summary.FileWriter(\"tf_logs\", tf.get_default_graph())\n\n#Session\nwith tf.Session() as sess:\n    sess.run(init)\n    \n    for epoch in range(n_epochs):\n        epoch_loss = 0 #reset loss\n        for batch_index in range(n_batches):\n            X_batch, y_batch = fetch_batch(epoch, batch_index, batch_size)\n            \n            if batch_index % 100 == 0:\n                summary_str = loss_summary.eval(feed_dict={inputs: X_batch, labels: y_batch})\n                step = epoch * n_batches + batch_index\n                file_writer.add_summary(summary_str, step)\n                \n            _, batch_loss = sess.run([training_op,loss], feed_dict={inputs: X_batch, labels: y_batch})\n            epoch_loss += batch_loss\n            \n        print(\"Epoch\", epoch, \"Loss =\", epoch_loss/n_batches) #print out loss averaged over all batches              \n        save_path = saver.save(sess, \"tmp/my_model.ckpt\")\n    \n    save_path = saver.save(sess, \"tmp/my_model_final.ckpt\")  \n    \n    #Sample Results From Mini-batch\n    test_input = X_batch\n    test_labels = y_batch\n    test_softmax, test_output, test_mask, test_logits = sess.run([sm,outputsf,outputs_mask,out_norm],\n                                                                   feed_dict={inputs: X_batch, labels: y_batch})\n    \n    #Complex Modulation\n    c1, c2, c3, c4, c5 = sess.run([tf.get_default_graph().get_tensor_by_name(\"com_mod:0\"),\n                                   tf.get_default_graph().get_tensor_by_name(\"com_mod_1:0\"),\n                                   tf.get_default_graph().get_tensor_by_name(\"com_mod_2:0\"),\n                                   tf.get_default_graph().get_tensor_by_name(\"com_mod_3:0\"),\n                                   tf.get_default_graph().get_tensor_by_name(\"com_mod_4:0\")])\n     \nfile_writer.close()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e7044d2fe69c194275f6fb60bb333ae578c00d3b"},"cell_type":"markdown","source":"Let us visualize what the light pattern looks like on the the photodetector array. After propagation through the 3D plates, the light becomes focused onto certain spots corresponding to the \"predicted\" digit.  "},{"metadata":{"scrolled":false,"trusted":true,"_uuid":"e95756d851f3c459c7fe5927ac17e550e32a344b"},"cell_type":"code","source":"plt.figure(figsize=((8,180)))\nplt.subplot(1,3,1)\nplt.imshow(np.abs(test_input).reshape((batch_size*N,N)))\nplt.title('Input Digits')\nplt.subplot(1,3,2)\nplt.imshow(np.abs(test_output).reshape((batch_size*N,N)))\nplt.title('Output Pattern')\nplt.subplot(1,3,3)\nplt.imshow(np.abs(test_mask).reshape((batch_size*N,N)))\nplt.title('Photodetectors')\nplt.show()\n\nprint(test_labels)\nprint(np.argmax(test_softmax,axis=1))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f4c181d066c19931d91d7cda620ffb48aa959227"},"cell_type":"markdown","source":"Let us the visualize the amplitude and phase modulation of the plate arrays."},{"metadata":{"trusted":true,"_uuid":"093f268f487f4323a4109601afd2fe9b1e48891d"},"cell_type":"code","source":"def amp_phase(c):\n    return np.abs(c),np.angle(c)\n\nfig, ax = plt.subplots(5,2,figsize=((10,25)))\n\nfor ind,c in enumerate((c1,c2,c3,c4,c5)):\n    A,P = amp_phase(c)\n    im = ax[ind,0].imshow(A.reshape((N,N)))\n    fig.colorbar(im,ax=ax[ind,0])\n    im = ax[ind,1].imshow(P.reshape((N,N)))\n    fig.colorbar(im,ax=ax[ind,1])\n\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true,"_uuid":"3fe40a05f6851ecf55a7304b4313eb862f7a50e7"},"cell_type":"code","source":"#Predictions\ntest_softmax = np.empty((m,10))\n\nwith tf.Session() as sess:\n    sess.run(init)\n    saver.restore(sess, \"tmp/my_model_final.ckpt\")\n    \n    for batch_index in range(n_batches):\n        start_ind = batch_index*batch_size\n        end_ind = start_ind + batch_size\n        test_softmax[start_ind:end_ind,:] = sm.eval(feed_dict={inputs: x_train[start_ind:end_ind,:]})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5bfa2f143776186b0c8dd722608d0f5ce77b73a0"},"cell_type":"code","source":"#Naming Conventions\nY_pred = test_softmax\nY_pred_classes = np.argmax(Y_pred, axis = 1) \nprint(Y_pred_classes)\nY_true = y_train\nX_val = x_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"eb317c498d9107aeb4d1c19f5594e93dc0b5f95a"},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nimport itertools\n\n# Look at confusion matrix \n#Note, this code is taken straight from the SKLEARN website, an nice way of viewing confusion matrix.\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n\n# compute the confusion matrix\nconfusion_mtx = confusion_matrix(y_train, Y_pred_classes) \n# plot the confusion matrix\nplot_confusion_matrix(confusion_mtx, classes = range(10))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7ad2cec87d608350dfb863e3395bece533f172de"},"cell_type":"markdown","source":"The errors are reasonable, but the achieved accuracy is still a way off from the state of the art neural networks. "},{"metadata":{"trusted":true,"_uuid":"f340e54c2af1fdf4e74f695d7883a10a6afa5275"},"cell_type":"code","source":"# Errors are difference between predicted labels and true labels\nerrors = (Y_pred_classes - Y_true != 0)\n\nY_pred_classes_errors = Y_pred_classes[errors]\nY_pred_errors = Y_pred[errors]\nY_true_errors = Y_true[errors]\nX_val_errors = X_val[errors]\n\ntop_x_error = 16\n\ndef display_errors(errors_index,img_errors,pred_errors, obs_errors):\n    \"\"\" This function shows X images with their predicted and real labels\"\"\"\n    n = 0\n    nrows = int(np.sqrt(top_x_error))\n    ncols = int(np.sqrt(top_x_error))\n    fig, ax = plt.subplots(nrows,ncols,sharex=True,sharey=True,figsize=((14,14)))\n    for row in range(nrows):\n        for col in range(ncols):\n            error = errors_index[n]\n            ax[row,col].imshow((img_errors[error]).reshape((N,N)))\n            ax[row,col].set_title(\"Predicted label :{}\\nTrue label :{}\".format(pred_errors[error],obs_errors[error]))\n            n += 1\n\n# Probabilities of the wrong predicted numbers\nY_pred_errors_prob = np.max(Y_pred_errors,axis = 1)\n\n# Predicted probabilities of the true values in the error set\ntrue_prob_errors = np.diagonal(np.take(Y_pred_errors, Y_true_errors, axis=1))\n\n# Difference between the probability of the predicted label and the true label\ndelta_pred_true_errors = Y_pred_errors_prob - true_prob_errors\n\n# Sorted list of the delta prob errors\nsorted_dela_errors = np.argsort(delta_pred_true_errors)\n\n# Top X errors \nmost_important_errors = sorted_dela_errors[-top_x_error:]\n\n# Show the top X errors\ndisplay_errors(most_important_errors, X_val_errors, Y_pred_classes_errors, Y_true_errors)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"949e52f19944395ef75b9b4c96966d03735e8203"},"cell_type":"code","source":"acc = 1 - np.count_nonzero(errors)/m\n\nprint('Training Accuracy = {a:8.2f} %'.format(a=100*acc))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"51065bdeb909d7e07c12cd8e7eadfb762972bda3"},"cell_type":"code","source":"#Test parameters\nn_batches = int(np.ceil(mt / batch_size))\n\ntest_softmax = np.empty((mt,10))\n\nwith tf.Session() as sess:\n    sess.run(init)\n    saver.restore(sess, \"tmp/my_model_final.ckpt\")\n    \n    for batch_index in range(n_batches):\n        start_ind = batch_index*batch_size\n        end_ind = start_ind + batch_size\n        test_softmax[start_ind:end_ind,:] = sm.eval(feed_dict={inputs: x_test[start_ind:end_ind,:]})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"684b9141e8c369433fedb0a5ade5eb83c11dec7c"},"cell_type":"code","source":"#Naming Conventions\nY_pred = test_softmax\nY_pred_classes = np.argmax(Y_pred, axis = 1) \nprint(Y_pred_classes)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e00c19bf5691e05016b87592306f1b2c455c7512"},"cell_type":"code","source":"results = pd.Series(Y_pred_classes,name=\"Label\")\nsubmission = pd.concat([pd.Series(range(1,28001),name = \"ImageId\"),results],axis = 1)\nsubmission.to_csv(\"D2NN_submit.csv\",index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"041eae0edd763ff41db2cb72ee5870bab39f940c"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}