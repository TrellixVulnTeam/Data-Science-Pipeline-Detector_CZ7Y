{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# MNIST Regression\nWe often solve the MNIST problem by using different kinds of classification techniques. Of cause MNIST classifcation can achieve a better result, how about solving MNIST problem using Regression? I will try it in this notebook.\n## Importing Packages","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split","metadata":{"execution":{"iopub.status.busy":"2021-09-20T15:08:52.26825Z","iopub.execute_input":"2021-09-20T15:08:52.268593Z","iopub.status.idle":"2021-09-20T15:08:52.274744Z","shell.execute_reply.started":"2021-09-20T15:08:52.268563Z","shell.execute_reply":"2021-09-20T15:08:52.273763Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Loading the data","metadata":{}},{"cell_type":"code","source":"train_pd = pd.read_csv('/kaggle/input/digit-recognizer/train.csv')\ntest_pd = pd.read_csv('/kaggle/input/digit-recognizer/test.csv')\ntrain = np.array(train_pd)\ntest = np.array(test_pd)\ntrain_images = train[:, 1:].reshape((-1, 28, 28, 1)) / 255.0\ntrain_labels = train[:, 0].astype(np.uint8)\ntest_images = test.reshape((-1, 28, 28, 1)) / 255.0","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-09-20T15:08:57.41265Z","iopub.execute_input":"2021-09-20T15:08:57.412986Z","iopub.status.idle":"2021-09-20T15:09:02.751326Z","shell.execute_reply.started":"2021-09-20T15:08:57.412956Z","shell.execute_reply":"2021-09-20T15:09:02.750431Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Define a special RELU Layer with max value.","metadata":{}},{"cell_type":"code","source":"class RELUWithMaxValue(tf.keras.layers.Layer):\n    def __init__(self, max_value):\n        super(RELUWithMaxValue, self).__init__()\n        self.max_value = max_value\n    def call(self, inputs):\n        return tf.keras.backend.relu(inputs, max_value=self.max_value)","metadata":{"execution":{"iopub.status.busy":"2021-09-20T15:21:38.096826Z","iopub.execute_input":"2021-09-20T15:21:38.097145Z","iopub.status.idle":"2021-09-20T15:21:38.102182Z","shell.execute_reply.started":"2021-09-20T15:21:38.097114Z","shell.execute_reply":"2021-09-20T15:21:38.101219Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Development","metadata":{}},{"cell_type":"markdown","source":"Define a special version of accuracy metric that can calculate accuracy corretly.","metadata":{}},{"cell_type":"code","source":"def accuracy(y_true, y_pred):\n    return tf.reduce_mean(tf.cast(tf.round(y_pred) == y_true, dtype=tf.float32))","metadata":{"execution":{"iopub.status.busy":"2021-09-20T15:09:48.888735Z","iopub.execute_input":"2021-09-20T15:09:48.889066Z","iopub.status.idle":"2021-09-20T15:09:48.893005Z","shell.execute_reply.started":"2021-09-20T15:09:48.889037Z","shell.execute_reply":"2021-09-20T15:09:48.892179Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### CNN Model","metadata":{}},{"cell_type":"code","source":"def get_cnn_model():\n    model = Sequential([\n        Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n        MaxPooling2D(2, 2),\n        BatchNormalization(),\n        Conv2D(32, (3, 3), activation='relu', padding='same'),\n        MaxPooling2D(2, 2),\n        BatchNormalization(),\n        Conv2D(64, (3, 3), activation='relu', padding='same'),\n        MaxPooling2D(2, 2),\n        BatchNormalization(),\n        Flatten(),\n        Dense(32, activation='relu', kernel_regularizer=tf.keras.regularizers.l2()),\n        Dense(1),\n        RELUWithMaxValue(9.0)\n    ])\n    model.compile(\n        optimizer='adam',\n        loss='mean_squared_error',\n        metrics=[accuracy]\n    )\n    return model","metadata":{"execution":{"iopub.status.busy":"2021-09-20T15:21:44.371489Z","iopub.execute_input":"2021-09-20T15:21:44.371879Z","iopub.status.idle":"2021-09-20T15:21:44.379302Z","shell.execute_reply.started":"2021-09-20T15:21:44.371849Z","shell.execute_reply":"2021-09-20T15:21:44.378342Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cnn_model = get_cnn_model()\ncheckpoint_filepath = \"best_checkpoint\"\nmodel_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n    filepath=checkpoint_filepath,\n    save_weights_only=True,\n    save_best_only=True,\n    monitor=\"val_accuracy\",\n    mode=\"max\"\n)\nearly_stopping = tf.keras.callbacks.EarlyStopping(patience=10)\ntrain_images, val_images, train_labels, val_labels = train_test_split(train_images, train_labels)\nhistory = cnn_model.fit(train_images, train_labels, epochs=100, validation_data=(val_images, val_labels), callbacks=[model_checkpoint, early_stopping], verbose=1)\npd.DataFrame(history.history).plot()\nplt.show()\ncnn_model = get_cnn_model()\ncnn_model.load_weights(checkpoint_filepath)","metadata":{"execution":{"iopub.status.busy":"2021-09-20T15:21:46.533259Z","iopub.execute_input":"2021-09-20T15:21:46.533665Z","iopub.status.idle":"2021-09-20T15:24:15.645673Z","shell.execute_reply.started":"2021-09-20T15:21:46.533632Z","shell.execute_reply":"2021-09-20T15:24:15.644958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Submission\nAlthough I solve this problem using Regression techniques, calculating test scores still requires converting results to integers.","metadata":{}},{"cell_type":"code","source":"test_labels = np.array(np.round(cnn_model.predict(test_images)), dtype=int).reshape(-1)\nprint(test_labels.shape)","metadata":{"execution":{"iopub.status.busy":"2021-09-20T15:24:33.878169Z","iopub.execute_input":"2021-09-20T15:24:33.878513Z","iopub.status.idle":"2021-09-20T15:24:35.009341Z","shell.execute_reply.started":"2021-09-20T15:24:33.878482Z","shell.execute_reply":"2021-09-20T15:24:35.006914Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame(test_labels).hist()","metadata":{"execution":{"iopub.status.busy":"2021-09-20T15:24:36.785745Z","iopub.execute_input":"2021-09-20T15:24:36.786074Z","iopub.status.idle":"2021-09-20T15:24:36.925961Z","shell.execute_reply.started":"2021-09-20T15:24:36.786043Z","shell.execute_reply":"2021-09-20T15:24:36.925116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here is first 100 item of test_labels.","metadata":{}},{"cell_type":"code","source":"print(test_labels[:100])","metadata":{"execution":{"iopub.status.busy":"2021-09-20T15:24:41.63068Z","iopub.execute_input":"2021-09-20T15:24:41.630993Z","iopub.status.idle":"2021-09-20T15:24:41.636074Z","shell.execute_reply.started":"2021-09-20T15:24:41.630964Z","shell.execute_reply":"2021-09-20T15:24:41.634947Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_ids = np.arange(1, test_labels.shape[0]+1)\nresult = np.concatenate((image_ids.reshape(-1, 1), test_labels.reshape(-1, 1)), axis=1)\ndf = pd.DataFrame(result, columns=[\"ImageId\", \"Label\"], dtype='int')\ndf.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2021-09-20T15:24:44.576208Z","iopub.execute_input":"2021-09-20T15:24:44.576581Z","iopub.status.idle":"2021-09-20T15:24:44.790418Z","shell.execute_reply.started":"2021-09-20T15:24:44.576551Z","shell.execute_reply":"2021-09-20T15:24:44.789513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Conclusion","metadata":{}},{"cell_type":"markdown","source":"Currently MNIST Regression can achieve 92% test accuracy which is not bad, there must be more ways to improve it. Any suggesion? I am glad to hear from you.","metadata":{}}]}