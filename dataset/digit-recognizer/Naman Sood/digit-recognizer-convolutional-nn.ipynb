{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport seaborn as sns\n%matplotlib inline\nimport tensorflow as tf\nimport warnings\nwarnings.filterwarnings('ignore')\nnp.random.seed(0)\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nimport itertools\nfrom tensorflow import keras\nfrom keras.utils.np_utils import to_categorical # convert to one-hot-encoding\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\nfrom keras.optimizers import RMSprop\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ReduceLROnPlateau\nfrom keras.activations import selu","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Importing data"},{"metadata":{},"cell_type":"markdown","source":"<h2>Loading data</h2>"},{"metadata":{},"cell_type":"markdown","source":"We will first proceed to import the csv training and testing data into their respective dataframes using `read_csv`"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = pd.read_csv('../input/digit-recognizer/train.csv')\ndf_test = pd.read_csv('../input/digit-recognizer/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df_train.shape)\nprint(df_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have a total of 784 pixels (28 x 28) and the label column which corresponds to the digit the image signifies"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['label'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that the pixels are divided in the value range of 0-255 which is the norm for grayscale images."},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train = df_train['label']\nx_train = df_train.drop(['label'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"g = sns.countplot(x=y_train)\ng.set_ylabel('Frequency of occurence')\ng.set_xlabel('Number')\ng.set_title('Frequency of each digit in the training set')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that the dataset does not suffer from any form of imbalance. Each number has adequate training cases."},{"metadata":{},"cell_type":"markdown","source":"# NULL values"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Using the general format of testing for NULL values\nx_train.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that we have too many columns to comfortably display the NULL value sum in each column, therefore we will use list comprehensions"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking presence of NULL values in the training set:\nnan_cols = [i for i in x_train.columns if x_train[i].isnull().any()]\nnan_cols","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking presence of NULL values in the test set\nnan_test = [i for i in df_test.columns if df_test[i].isnull().any()]\nnan_test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Therefore, we see that none of the columns containing any NULL values, therefore we can continue accordingly."},{"metadata":{},"cell_type":"markdown","source":"# Normalizing\n\nWe can normalize the dataset to have values in the range (0-1) by diving all values by 255.0"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Divide by float to preserve fractional component\n\nx_train = x_train/255.0 \ndf_test = df_test/255.0","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Reshaping data"},{"metadata":{},"cell_type":"markdown","source":"We proceed to reshape the data in a correct format for our convolutional network to process: (Batch size, height, width, channels)\n\nHere the channel would be 1 as we are dealing with grayscale images, unlike RGB where we would have it as 3"},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train = x_train.values.reshape(-1,28,28,1)\ndf_test = df_test.values.reshape(-1,28,28,1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(x_train.shape)\nprint(df_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Viewing example images"},{"metadata":{},"cell_type":"markdown","source":"We can visualize these images using matplotlib"},{"metadata":{"trusted":true},"cell_type":"code","source":"n_rows = 2\nn_cols = 2 \nc = 0\nfig, ax = plt.subplots(n_rows,n_cols,sharex=True,sharey=True)\nfor i in range(0,n_rows):\n    for j in range (0,n_cols):\n        ax[i,j].imshow(x_train[c][:,:,0])\n        ax[i,j].set_title(y_train[c])\n        c=c+1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# One hot encoding "},{"metadata":{},"cell_type":"markdown","source":"We will encode the labels in the dataset to one-hot vectors"},{"metadata":{"trusted":true},"cell_type":"code","source":"#We have 10 classes (0-9)\ny_train = to_categorical(y_train,num_classes=10) #Will convert values like 1 to [0 1 0 0 0 0 0 0 0 0 ]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rand = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Split the training into training and validation dataset for our CNN\n\nx_train, x_val, y_train, y_val = train_test_split(x_train,y_train,test_size=0.1,random_state=rand)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\nmodel.add(Conv2D(filters=32,kernel_size=(5,5),padding='Same',activation='selu',kernel_initializer='lecun_normal'))\nmodel.add(Conv2D(filters=32,kernel_size=(5,5),padding='Same',activation='selu',kernel_initializer='lecun_normal'))\nmodel.add(MaxPool2D(pool_size=(2,2)))\nmodel.add(Conv2D(filters=64,kernel_size=(3,3),padding='Same',activation='selu',kernel_initializer='lecun_normal'))\nmodel.add(Conv2D(filters=64,kernel_size=(3,3),padding='Same',activation='selu',kernel_initializer='lecun_normal'))\nmodel.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\nmodel.add(Flatten())\nmodel.add(Dense(256, activation = \"relu\"))\nmodel.add(Dense(10, activation = \"softmax\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(optimizer = keras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0), loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We achieve a very high validation accuracy (0.9883), we can further improve this by introducing data augmentation as well as some learning rate scheduling."},{"metadata":{},"cell_type":"markdown","source":"We will introduce horizontal shifts, vertical shifts and zoomed training data in the dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"img_gen = ImageDataGenerator(\n    featurewise_center=False, \n    samplewise_center=False,\n    featurewise_std_normalization=False, \n    samplewise_std_normalization=False,\n    rotation_range=10, \n    width_shift_range=0.1,\n    height_shift_range=0.1, \n    zoom_range=0.1,\n    horizontal_flip=False,\n    vertical_flip=False,\n)\n\nimg_gen.fit(x_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We cannot incorporate horizontal and vertical flips as for cases with 6 and 9, it may hurt the dataset's training.\n\nThis above function will introduce the following operations:\n\n* Randomly rotate some images by 10 degrees\n* Randomly zoom 10% training images\n* Randomly shift training images 10% horizontally and vertically"},{"metadata":{"trusted":true},"cell_type":"code","source":"reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.3, patience=3, verbose=0, mode='auto',min_delta=0.0001, cooldown=0, min_lr=0.00001)\neas_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=0, mode='auto',baseline=None, restore_best_weights=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_epochs = 30\nhistory = model.fit_generator(img_gen.flow(x_train, y_train),epochs=n_epochs,validation_data=(x_val, y_val),callbacks=[reduce_lr])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Confusion matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Final optimisations of the model:\npd.DataFrame(history.history).plot(figsize=(10,10))\nplt.grid(True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"yp = model.predict(x_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"yp.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_class = np.argmax(yp,axis = 1) \ny = np.argmax(y_val,axis = 1) \nplt.figure(figsize=(10,10))\nconfusion_mtx = confusion_matrix(y, y_class) \nsns.heatmap(confusion_mtx,annot=True,cmap='Blues')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here, we see that we are able to get most of the predictions right except a few of them still left as wrongly labelled"},{"metadata":{},"cell_type":"markdown","source":"# Checking for errors"},{"metadata":{"trusted":true},"cell_type":"code","source":"error = (y_class - y)!=0\ny_class_error = y_class[error]\ny_error = y[error]\nx_val_error = x_val[error]\nn_rows = 2\nn_cols = 2\nc=0\nfig, ax = plt.subplots(n_rows,n_cols,sharex=True,sharey=True,figsize=(10,10))\nfor i in range(0,n_rows):\n    for j in range(0,n_cols):\n        ax[i,j].imshow((x_val_error[c]).reshape((28,28)))\n        ax[i,j].set_title((\"Predicted label :{}\\nTrue label :{}\".format(y_class_error[c],y_error[c])))\n        c=c+1\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# predict results\nresults = model.predict(df_test)\n\n# select the indix with the maximum probability\nresults = np.argmax(results,axis = 1)\n\nresults = pd.Series(results,name=\"Label\")\n\nsubmission = pd.concat([pd.Series(range(1,28001),name = \"ImageId\"),results],axis = 1)\n\nsubmission.to_csv(\"cnn_mnist_datagen.csv\",index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit_generator(img_gen.flow(x_train, y_train),epochs=10,validation_data=(x_val, y_val),callbacks=[reduce_lr])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}