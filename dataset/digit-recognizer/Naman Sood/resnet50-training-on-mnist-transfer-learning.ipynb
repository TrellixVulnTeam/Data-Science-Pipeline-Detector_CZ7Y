{"cells":[{"metadata":{},"cell_type":"markdown","source":"<h1>Resnet 50 tutorial with MNIST data - Transfer learning</h1>"},{"metadata":{},"cell_type":"markdown","source":"Transfer learning is the idea of overcoming the isolated learning paradigm and utilizing knowledge acquired for one task to solve related ones. . In Simpler terms, Transfer learning is the process of using `state-of-the-art` pretrained models such as VGG, Resnet, mobilenet, Inception for our own custom task. These models have already been pre-trained over millions of images and learnt a myriad of features from them. Adding our own dense neural network at the end results in the model fine-tuning for our task and producing much better predictions than if we had only worked with our own data. This method is helpful when we do not have a lot of data, or our task resembles other tasks that these models have been pre-trained for: \n\nThis notebooks serves as a basic rundown/guide of using transfer learning for tasks such as image recognition. We will be using the RESNET50 model which has been trained over the `imagenet` dataset.\n\nMore information about the RESNET50 model:\n\nhttps://arxiv.org/abs/1512.03385\n\nWe will be using the pre-trained model, the model weights will be imported as well in a `.h5` format:\n\nhttps://www.kaggle.com/gaborfodor/keras-pretrained-models"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom keras.utils import np_utils\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import OneHotEncoder\nimport matplotlib.pyplot as plt\nimport imageio\n\nfrom fastai import *\nfrom fastai.vision import *\nfrom fastai.vision.all import *","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2>File imports</h2>"},{"metadata":{},"cell_type":"markdown","source":"This given notebook can be used on different forms of single-channeled data, the notebook incoporates the clothing channel images and vanilla MNIST for digit recognition. The steps will be the same for any such dataset which consists of single-channel pixel values as rows of a CSV file"},{"metadata":{},"cell_type":"markdown","source":"<h3>The Clothing single channel CSV</h3>"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/clothing-single-channel/fashion-mnist_train.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>The Vanilla MNIST CSV</h3>"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/digit-recognizer/train.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Reference**: The corresponding categorical labels for the numeric labels in the clothing dataset:\n\n<ol>\n    <li>T-shirt/top</li>\n    <li>Trouser</li>\n    <li>Pullover</li>\n    <li>Dress</li>\n    <li>Coat</li>\n    <li>Sandal</li>\n    <li>Shirt</li>\n    <li>Sneaker</li>\n    <li>Bag</li>\n    <li>(0) Ankle boot</li>\n</ol>"},{"metadata":{},"cell_type":"markdown","source":"<h3>Sanity check for data import</h3>"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>Division of data and labels</h3>\n\nWe will proceed to separate out the data used for training as well as the labels, these will be separately used in our model creation phase"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_x = df.loc[:,'pixel0':'pixel783']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_y = df.loc[:,'label']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np_x = np.array(df_x)\nnp_x.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>Training data reshape</h3>\n\nAs our data is in the format of `28x28` images, we will proceed to reshape them accordingly. This will result in a 3-D array with dimensions:\n\n(Number of training samples, height, width)"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = np.array(np_x).reshape(-1,28, 28)\ny_train = np.array(df_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X_train.shape)\nprint(y_train.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>Channel stacking</h3>\n\nWe will proceed to stack the data along the last dimension. This will result in an artificially created 3-channeled image (RGB). As most of the pre-trained models work on RGB images, we need to convert ours in the same format as well"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = np.stack((X_train,)*3, axis=-1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X_train.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>Saving images</h3>\n\nWe will be using the keras method of `flow_from_directory`, this will require us to save our images in a directory. We will convert the numpy arrays of the data into `jpg` images and save them in the `/data/` directory."},{"metadata":{"trusted":true},"cell_type":"code","source":"def save_imgs(path:Path, data, labels):\n    for label in np.unique(labels):\n        (path/str(label)).mkdir(parents=True,exist_ok=True)\n    for i in range(len(data)):\n        if(len(labels)!=0):\n            imageio.imsave( str( path/str(labels[i])/(str(i)+'.jpg') ), data[i])\n        else:\n            imageio.imsave( str( path/(str(i)+'.jpg') ), data[i])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"save_imgs(Path('/data/digits'),X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>Sanity check</h3>\n\nWe will proceed to check the created directories to cross-verify whether our function has successfully worked:"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('total classes :', len(os.listdir('/data/digits')))\nprint('Images with label 1: ', len(os.listdir('/data/digits/1')))\n\nprint('Image names with label 1')\nprint(os.listdir('/data/digits/1')[:10])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>Flowing images into the Keras image generator</h3>"},{"metadata":{},"cell_type":"markdown","source":"Here, we use the `preprocess_input` function that processes an image into a form acceptable by the RESNET50 model. We also use the `class_mode=categorical` as our labels were numeric. This allows a conversion of those labels into sparse one-hot vectors that can be used in the output neurons of our model."},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.applications.resnet50 import preprocess_input\nimport tensorflow as tf\n\ntrain_datagen=tf.keras.preprocessing.image.ImageDataGenerator(preprocessing_function=preprocess_input, validation_split=0.15)\ntrain_generator = train_datagen.flow_from_directory('/data/digits', class_mode='categorical', subset='training')\nvalid_generator = train_datagen.flow_from_directory('/data/digits', class_mode='categorical', subset='validation')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>Shape of x and y</h3>\n\nWe iterate over the train generator and retrieve the first element. Here, the first element is the batch of 32 images. We can then proceed to iterate over the batch and display the images. The y dimensions correspond to the labels of the data in one-hot encoding. Each y label has a sparse single dimension vector with a value of 1 corresponding to the index of the label."},{"metadata":{"trusted":true},"cell_type":"code","source":"x, y = train_generator[0]\nprint(x.shape)\nprint(y.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our image here has pixels consisting of flat colors as we have artificially synthesized this single-channel image into a 3-channel one. Each layer of our image does not represent the `RGB` byte values but instead the single channel pixel values. This can result in the image showing varied characteristics after its conversion into RGB."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.imshow(x[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will define the path of our resnet pretrained model if we have the requirement to do so:"},{"metadata":{"trusted":true},"cell_type":"code","source":"resnet_weights_path = '../input/keras-pretrained-models/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>Model creation</h3>\n\nWe create our ResNet50 model through the `keras.applications` package. We will add the ResNet50 model and use `include_top=False` to remove the final output layer. As the ResNet model was trained for 1000 classes and we have only 10, we will replace this with our own 10 neuron layer.\n\nAs our weights for ResNet are pretrained, we will proceed to set its training capability to False. This will allow the model to only focus on the weights of the output neurons and give us a fast and efficient implementation."},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.applications.resnet50 import ResNet50\nmodel = Sequential()\n\nmodel.add(ResNet50(include_top = False, pooling = 'avg', weights = 'imagenet'))\n\n# 2nd layer as Dense for 2-class classification, i.e., dog or cat using SoftMax activation\nmodel.add(Dense(10, activation = 'softmax'))\n\n# Say not to train first layer (ResNet) model as it is already trained\nmodel.layers[0].trainable = False","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit_generator(generator=train_generator,epochs=10,validation_data=valid_generator)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}