{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"execution":{"iopub.status.busy":"2022-06-09T21:41:02.529397Z","iopub.execute_input":"2022-06-09T21:41:02.530079Z","iopub.status.idle":"2022-06-09T21:41:02.536248Z","shell.execute_reply.started":"2022-06-09T21:41:02.530047Z","shell.execute_reply":"2022-06-09T21:41:02.535615Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Business Problem:\n- To use a classification model to correctly classify the digits given in the mnist dataset.\n- We will use **SVM for classification** and **PCA,t-SNE for visualization and interpretability**.","metadata":{}},{"cell_type":"markdown","source":"### Importing required packages:","metadata":{}},{"cell_type":"code","source":"# Importing libraries\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom collections import Counter\nfrom sklearn import feature_extraction, model_selection, metrics, svm\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt, matplotlib.image as mpimg\nfrom sklearn import svm\n\nfrom sklearn.svm import SVC\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline\n","metadata":{"execution":{"iopub.status.busy":"2022-06-09T21:41:29.005355Z","iopub.execute_input":"2022-06-09T21:41:29.00573Z","iopub.status.idle":"2022-06-09T21:41:29.618168Z","shell.execute_reply.started":"2022-06-09T21:41:29.0057Z","shell.execute_reply":"2022-06-09T21:41:29.617451Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Loading train data into Dataframe:","metadata":{}},{"cell_type":"code","source":"mnist_train = pd.read_csv('/kaggle/input/digit-recognizer/train.csv')","metadata":{"execution":{"iopub.status.busy":"2022-06-09T21:43:49.707847Z","iopub.execute_input":"2022-06-09T21:43:49.708168Z","iopub.status.idle":"2022-06-09T21:43:53.022968Z","shell.execute_reply.started":"2022-06-09T21:43:49.708143Z","shell.execute_reply":"2022-06-09T21:43:53.022194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print first five rows of mnist_train.\n\nmnist_train.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mnist_train.shape","metadata":{"execution":{"iopub.status.busy":"2022-06-09T21:45:15.440373Z","iopub.execute_input":"2022-06-09T21:45:15.440715Z","iopub.status.idle":"2022-06-09T21:45:15.445588Z","shell.execute_reply.started":"2022-06-09T21:45:15.440687Z","shell.execute_reply":"2022-06-09T21:45:15.44492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# PCA implementations from first principles:","metadata":{}},{"cell_type":"code","source":"# save the labels into a variable l.\nl = mnist_train['label']\nl.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"l.value_counts().values.sum()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Drop the label feature and store the pixel data in d.\nd = mnist_train.drop(\"label\",axis=1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# display or plot a number.\nplt.figure(figsize=(5,5))\nidx = 100\ngrid_data = d.iloc[idx].to_numpy().reshape(28,28)  # reshape from 1d to 2d pixel array\nplt.imshow(grid_data,  cmap = \"gray\")\nplt.show()\nprint(l[idx])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#2D-Visualization\n# Pick first 15K data-points to work on for time-effeciency.\n\nlabels = l.head(15000)\ndata = d.head(15000)\nprint(\"the shape of sample data = \", data.shape)\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Data-preprocessing: Standardizing the data\nfrom sklearn.preprocessing import StandardScaler\nstandardized_data = StandardScaler().fit_transform(data)\nprint(standardized_data.shape)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#find the co-variance matrix which is : A^T * A\nsample_data = standardized_data\n# matrix multiplication using numpy\ncovar_matrix = np.matmul(sample_data.T , sample_data)\nprint ( \"The shape of variance matrix = \", covar_matrix.shape)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# finding the top two eigen-values and corresponding eigen-vectors \n# for projecting onto a 2-Dim space.\nfrom scipy.linalg import eigh \n# the parameter 'eigvals' is defined (low value to heigh value) \n# eigh function will return the eigen values in asending order\n# this code generates only the top 2 (782 and 783) eigenvalues. As largest two eigen values will be 782 and 783.\nvalues, vectors = eigh(covar_matrix, eigvals=(782,783))\nprint(\"Shape of eigen vectors = \",vectors.shape)\nprint(values)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#vectors[:,0] represents the eigen vector corresponding to the 2nd eigen value.(First col in vectors matrix)\n#vectors[:,1] represents the eigen vector correspondign to the 1st eigen value.(Second col in vectors matrix)\n#Note : Eigen values are arranged in ascending order so the Eigen vectors too.\n# converting the eigen vectors into (2,d) shape for ease of computation which we do ----> we will get matrix of (2 * 784)\nvector = vectors.T\nprint(\"Updated shape of eigen vectors = \",vector.shape)\n# Here, vectors[0] represent the eigen vector corresponding to the 2nd eigen value.\n# Here, vectors[1] represent the eigen vector corresponding to the 1st eigen value.","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Now, we need to swap the rows of the vector matrix \n#such that the first row corresponds to eigen vector with the largest eigen value\n#and second row corresponds to the eigen vector with second largest eigen value.\nvector[[0,1]]=vector[[1,0]]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# projecting the original data onto the eigen basis.\n# Basically, we form a matrix with the eigen vectors in row order. \n#Then, we do a matrix multiplication between updated vector and transose of sample data\nimport matplotlib.pyplot as plt\nnew_coordinates = np.matmul(vector, sample_data.T)\nprint (\" resultant new data points' shape \", vector.shape, \"X\", sample_data.T.shape, \" = \", new_coordinates.shape)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# appending label to the 2d projected data\nnew_coordinates = np.vstack((new_coordinates, labels)).T\n# creating a new data frame for ploting the labeled points.\ndataframe = pd.DataFrame(data=new_coordinates, columns=(\"1st_principal\", \"2nd_principal\", \"label\"))\nprint(dataframe.head())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ploting the 2d data points with seaborn\n# Note : This is 2d representation of 784 dimensions data.\nimport seaborn as sn\nsn.FacetGrid(dataframe, hue=\"label\", size=7).map(plt.scatter, '1st_principal', '2nd_principal').add_legend()\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# PCA using Scikit-Learn\n","metadata":{}},{"cell_type":"code","source":"# initializing the pca\nfrom sklearn import decomposition\npca = decomposition.PCA()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# configuring the parameteres\n# the number of components = 2\npca.n_components = 2\npca_data = pca.fit_transform(sample_data)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pca_reduced will contain the 2-d projects of simple data\nprint(\"shape of pca_reduced.shape = \", pca_data.shape)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# attaching the label for each 2-d data point \npca_data = np.vstack((pca_data.T, labels)).T","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# creating a new data fram which help us in ploting the result data\npca_df = pd.DataFrame(data=pca_data, columns=(\"1st_principal\", \"2nd_principal\", \"labels\"))\nsn.FacetGrid(pca_df, hue=\"labels\", size=6).map(plt.scatter, '1st_principal', '2nd_principal')\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Observation and Inferences from PCA: \n- Note : This is 2d representation of 784 dimensions data.\n- Imagine, we want to build a classifier in the 784 dimensions data. What can be the insights??:\n    - The dark blue points are grouped very nicely (0.0 label)on extreme right side.\n    - Likewise, the points with label 7 are groped at extreme left side.\n    - This we can easily build some sort of clusters/classification planes to classsify all likely points.\n- Since, 2-dimensions is an approximation. We are lossing so much information but still it gives me some understanding that probably in the high dimensions (784) which we can't even imagine/visualize, all our points with label 0.0 are close to each other, 9's are groped togesther. Which all the images corresponding to their class labels are grouped together.","metadata":{}},{"cell_type":"markdown","source":"# PCA for dimensionality reduction (not for visualization)\n","metadata":{}},{"cell_type":"code","source":"# PCA for dimensionality redcution (non-visualization)\npca.n_components = 784\npca_data = pca.fit_transform(sample_data)\npercentage_var_explained = pca.explained_variance_ / np.sum(pca.explained_variance_)\ncum_var_explained = np.cumsum(percentage_var_explained)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot the PCA spectrum\nplt.figure(1, figsize=(6, 4))\nplt.clf()\nplt.plot(cum_var_explained, linewidth=2)\nplt.axis('tight')\nplt.grid()\nplt.xlabel('n_components')\nplt.ylabel('Cumulative_explained_variance')\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Observation and Inferences from Variance explained by PCA: \n- As we can see we have 784 dimensions which cumulatively explains 100% of the data.\n- If we take first 100 dimensions, they explains ~75% of the data.\n- As we have plotted in 2 dimesnions for scatterplot. It's clearly visible that with 2-d we are able to explain 20% of the data.Still loosing 80% but we have some way to visualize (**Half Bread is better than No Bread :p**)  \n- But we can conclude that out of 784 features, only ~400 can explain almost all of my data, and around 300 can explain approx 90% of our data. which is great!!!","metadata":{}},{"cell_type":"markdown","source":"# t-Distributed Stochastic Neighbor Embedding (t-SNE)","metadata":{}},{"cell_type":"code","source":"# TSNE\nfrom sklearn.manifold import TSNE\n# Picking the top 1000 points as TSNE takes a lot of time for 15K points\ndata_1000 = standardized_data[0:1000,:]\nlabels_1000 = labels[0:1000]\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = TSNE(n_components=2, random_state=0)\n# configuring the parameteres\n# the number of components = 2\ntsne_data = model.fit_transform(data_1000)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# creating a new data frame which help us in ploting the result data\ntsne_data = np.vstack((tsne_data.T, labels_1000)).T\ntsne_df = pd.DataFrame(data=tsne_data, columns=(\"Dim_1\", \"Dim_2\", \"label\"))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Ploting the result of tsne\nsn.FacetGrid(tsne_df, hue=\"label\", size=6).map(plt.scatter, 'Dim_1', 'Dim_2').add_legend()\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Observation and Inferences from t-SNE:\n- Better visualization than PCA\n- More clarity in terms of interpretations ex: generally 4 and 9 are written similarly and hence we can see the points with label 4 (light blue) and 9(purple) are very closely grouped.\n- Now we have understood the data well, we can proceed with model building....","metadata":{}},{"cell_type":"markdown","source":"# Model Building with SVM","metadata":{}},{"cell_type":"code","source":"# We will choose first 5K data-points to work on for time-effeciency.\nfull_images = pd.read_csv('/kaggle/input/digit-recognizer/train.csv')\nimages = full_images.iloc[0:5000,1:]\nlabels = full_images.iloc[0:5000,:1]\nX_train, X_test,y_train, y_test = train_test_split(images, labels, train_size=0.8, random_state=0)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# display or plot a number.\nplt.figure(figsize=(5,5))\nidx = 100\ngrid_data = X_train.iloc[idx].to_numpy().reshape(28,28)  # reshape from 1d to 2d pixel array\nplt.imshow(grid_data,  cmap = \"gray\")\nplt.show()\nprint(l[idx])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Observations:\n- Note that these images aren't actually black and white (0,1). They are gray-scale (0-255).","metadata":{}},{"cell_type":"code","source":"# Visualizing data with histplot\n\nplt.hist(X_train.iloc[idx])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Observations:\n- Since this is a graysclae image, we are getting range of value from 0-255","metadata":{}},{"cell_type":"markdown","source":"# Model Training:\n- We will use svm's classfier called SVC for building a vector classifier","metadata":{}},{"cell_type":"markdown","source":"# Model -1 : Linear SVM Model","metadata":{}},{"cell_type":"code","source":"# SVC\n\nfrom sklearn.model_selection import GridSearchCV\n\nparams = {\n        'C': [1e-4,  0.001, 0.01, 0.1, 1,10] \n        }\nsvc = SVC(kernel='linear')\nclf = GridSearchCV(svc, params, scoring = \"accuracy\", cv=3)\n\nclf.fit(X_train, y_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"res = clf.cv_results_\n\nfor i in range(len(res[\"params\"])):\n    print(f\"Parameters:{res['params'][i]} Mean_score: {res['mean_test_score'][i]} Rank: {res['rank_test_score'][i]}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- As we have seen, change in hyperparam does not have any effect on the score and hence we will go with default C=1","metadata":{}},{"cell_type":"code","source":"clf = svm.SVC(kernel='linear')\nclf.fit(X_train, y_train)# learning / fitting from train data\nclf.score(X_test,y_test) # Evaluating score on test data","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Observations:\n- With Linear kernel, our accuracy is 0.91","metadata":{}},{"cell_type":"markdown","source":"# Model -2 : RBF SVM","metadata":{}},{"cell_type":"code","source":"# by defualt, svm.SVC() uses kernel == RBF, we will train our model on it.\n\nclf_1 = svm.SVC()\nclf_1.fit(X_train, y_train.values.ravel()) # learning / fitting from train data\nclf_1.score(X_test,y_test) # Evaluating score on test data","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Observations:\n- With RBF kernel, our accuracy increased to 0.942","metadata":{}},{"cell_type":"markdown","source":"# Model -3 : RBF SVM with binary cmap\n- Converting grayscale to binary with a simple logic:\n- To check if we have any improvement on our result.","metadata":{}},{"cell_type":"code","source":"X_test_copy = X_test.copy()\nX_train_copy = X_train.copy()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test_copy[X_test_copy>0]=1\nX_train_copy[X_train_copy>0]=1\n\n# img=train_images.iloc[idx].to_numpy().reshape((28,28))\n# plt.imshow(img,cmap='binary')\n# plt.title(train_labels.iloc[idx])\n\n# display or plot a number.\nplt.figure(figsize=(5,5))\nidx = 100\ngrid_data = X_train_copy.iloc[idx].to_numpy().reshape(28,28)  # reshape from 1d to 2d pixel array\nplt.imshow(grid_data,  cmap = \"binary\")\nplt.show()\nprint(l[idx])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualizing data with histplot\n\nplt.hist(X_train_copy.iloc[idx])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Observations:\n- Since this is a binary image, we are getting values 0 or 1 for our train dataset.","metadata":{}},{"cell_type":"code","source":"#Retraining our model\n\nclf_2 = svm.SVC()\nclf_2.fit(X_train_copy, y_train.values.ravel())\nclf_2.score(X_test_copy,y_test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Observations:\n- We are not getting any improvement in the scores and hence we will got with our second model (clf_1)","metadata":{}},{"cell_type":"markdown","source":"# Predictions on test data:","metadata":{}},{"cell_type":"code","source":"test_data=pd.read_csv('/kaggle/input/digit-recognizer/test.csv')","metadata":{"execution":{"iopub.status.busy":"2022-06-09T21:44:43.621999Z","iopub.execute_input":"2022-06-09T21:44:43.622336Z","iopub.status.idle":"2022-06-09T21:44:45.498402Z","shell.execute_reply.started":"2022-06-09T21:44:43.622312Z","shell.execute_reply":"2022-06-09T21:44:45.497638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data.shape","metadata":{"execution":{"iopub.status.busy":"2022-06-09T21:44:50.038584Z","iopub.execute_input":"2022-06-09T21:44:50.038894Z","iopub.status.idle":"2022-06-09T21:44:50.044977Z","shell.execute_reply.started":"2022-06-09T21:44:50.03887Z","shell.execute_reply":"2022-06-09T21:44:50.044506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_predictions=clf_1.predict(test_data)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_predictions","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.DataFrame(final_predictions)\ndf.index.name='ImageId'\ndf.index+=1\ndf.columns=['Label']\ndf.to_csv('final_predictions.csv', header=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Please Upvote if you like the Notebook Content !","metadata":{}}]}