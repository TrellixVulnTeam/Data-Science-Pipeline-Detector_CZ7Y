{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<h1>Deep Neural Networks and Data Augmentation</h1>\nIn this tutorial I will explain how to classify the MNIST dataset using DNNs (Deep Neural Network). The code will be written using python with the help of Keras library which is a high level library and i am going to use TensorFlow as it's backend.\n\n  Now let's import Keras and some other useful libraries that we are gonna use later and also we will load the data from keras databases for later use.","metadata":{"id":"eZsGESG14HEC","_uuid":"2aabf852e962b97322789830e4d8a33eed63579c"}},{"cell_type":"code","source":"from sklearn.utils import shuffle\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom keras.datasets import mnist\nimport numpy as np\nnp.random.seed(0)\n\nx_train = pd.read_csv('../input/train.csv')\nlabel = x_train['label']\nx_train.drop(['label'], inplace = True, axis = 1 )\n\nx_test = pd.read_csv('../input/test.csv')\nx_train = x_train.values\ny_train = label.values\nx_test = x_test.values\nx_train ,  y_train = shuffle(x_train, label , random_state=0)","metadata":{"_uuid":"c7c169a0c3671a76f3e8a333e096bc292856d6b6","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1>Data Exploration</h1>\nLet's explore the data we have as this will give us a hint on the algorithm we will use if we have to choose. Exploring data is also very important because it will tell you which accuracy metric you are going to use, if the data is balanced which means all the classes have fair contribution in the dataset regarding its numbers then we can easily use accuracy, But if the data is skewed then we won't be able to use accurace as it's results will be misleading and we may use F-beta score instead.","metadata":{"id":"RJcvQysl6xz2","_uuid":"9b6d56da1f704f4686f0da3e5fff9bf7082488a7"}},{"cell_type":"code","source":"print(x_train.shape)","metadata":{"_uuid":"00917d4e4deeddeaff882be7725b86dcc9dd7fd9","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"the number of training examples = %i\" % x_train.shape[0])\nprint(\"the number of classes = %i\" % len(np.unique(y_train)))\nprint(\"Flattened Image dimentions = %d x %d  \" % (x_train.shape[1], 1)  )\n\n#This line will allow us to know the number of occurrences of each specific class in the data\nprint(\"The number of occuranc of each class in the dataset = %s \" % label.value_counts(), \"\\n\" )\n\n\nX_train = x_train.reshape(-1, 28, 28).astype('float32')\nimages_and_labels = list(zip(X_train,  y_train))\nfor index, (image, label) in enumerate(images_and_labels[:12]):\n    plt.subplot(5, 4, index + 1)\n    plt.axis('off')\n    plt.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\n    plt.title('label: %i' % label)","metadata":{"id":"qISzguL479UT","outputId":"1f0ac568-3933-4e80-8961-3bc2a949d503","executionInfo":{"status":"ok","timestamp":1530806452580,"user_tz":-120,"elapsed":1944,"user":{"displayName":"Abd El Rhman ElMoghazy","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"112798608963931725749"}},"_uuid":"d4bc6a7b82fd34cd34218e4de032be22781b7f5a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"type(x_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the previous results we can see that the dataset consists of 60000 training example each is an image of dimention 28 * 28. We can see that the number of occurances of each class is almost balanced and based on that it is safe to use accuracy as our metric later.","metadata":{"id":"RSc6zzzPcI27","_uuid":"f29ddcf0cc3d87fea86d37f10e5c52006885cc1b"}},{"cell_type":"markdown","source":"<h1>Algorithm Choice</h1>\nIn this tutorial we will use the Deep Neural Networks Algorithm. Deep Neural Networks consist of levels. Each level of the  neural network consists of neurons. A neuron in the NN layer outputs discrete values for a classification task so it even fires or doesn't fire.\n![Deep Neural Network example](https://raw.githubusercontent.com/MoghazyCoder/Machine-Learning-Tutorials/master/assets/Deep.png)\n\nNeurons use the equation that determines whether or not it will fire. each neuron outputs the result from applying the function a(z) where a() is the activation function and z is the linear function WX + b and passes it to the next layer neurons. One of the mostly used activation function is the Relu function that is because it solves the problem of the exploding gradient, You can read more about that [here](https://machinelearningmastery.com/exploding-gradients-in-neural-networks/).","metadata":{"id":"_L47xC5pc20Z","_uuid":"ea9ff4a9d4734995a55095141ead9c27c988d550"}},{"cell_type":"markdown","source":"<h1>Parameter and Model Selection</h1>\nNow Let's fit the model. We will make a sequential model which is a stack of layers, each layer passes the output to the next layer. we must reshape the input data to make the image a 1d vector instead to be able to pass it to the Deep Neural Network.","metadata":{"id":"FoBmtygovCSD","_uuid":"aff3f396e48467028af15c6dec0cf92b644bc684"}},{"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense, Flatten\nfrom keras.layers import Dropout, Conv2D\nfrom keras import regularizers\n\nfrom keras.utils import np_utils\n\n#reshape the inputs\n# I will change the size of the training and testing sets to be able to use ImageDataGenerator wich accepts inputs in the following shape\nx_train = x_train.reshape(-1,28,28,1)\nx_test = x_test.reshape(-1,28,28,1)\n\nprint(x_train.shape )\nprint(x_train.shape )\n\n\n#Makine the outputs 1-hot vector of 10 elements\ny_train = np_utils.to_categorical(y_train)\n\nmodel = Sequential()\n# The first layer doesn't have significant importance in the code.\n# THe conv layer is used only to get the 3d images from the fit generator in the 2d format and flatten it using flatten layer\n# THe layer will not affect the layer since i am only using feature Pooling _ 1*1 convolution with only 1 feature map\nmodel.add(Conv2D(1, kernel_size=1, padding=\"same\",input_shape=(28, 28, 1), activation = 'relu'))\nmodel.add(Flatten())\n\n# model.add(Dense(units=800, activation='relu', input_dim= 784 ,  kernel_regularizer=regularizers.l2(0.001) ) )\n\nmodel.add(Dense(units=100, activation='relu'  ))\nmodel.add(Dropout(0.1))\nmodel.add(Dense(units=100, activation='relu'  ))\nmodel.add(Dropout(0.1))\nmodel.add(Dense(units=100, activation='relu'  ))\nmodel.add(Dropout(0.1))\n\n#and now the output layer which will have 10 units to\n#output a 1-hot vector to detect one of the 10 classes\nmodel.add(Dense(units=10, activation='softmax'))","metadata":{"id":"i6HEF4bXc2Wa","outputId":"0d26835c-8797-4ae3-e72a-12e9d7dd00c4","executionInfo":{"status":"ok","timestamp":1530806453178,"user_tz":-120,"elapsed":573,"user":{"displayName":"Abd El Rhman ElMoghazy","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"112798608963931725749"}},"_uuid":"9fb3e73b7b3d0a0edd38b7e71a8ba9022f7de5fd","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's configure the learning process and choose the suitable parameters.we will use [catagorical cross entropy as the loss finction](http://neuralnetworksanddeeplearning.com/chap3.html), adam optimizer which  is an efficient gradient descent algorithm that proved to work well and our performance metric will be the accuracy.","metadata":{"id":"6aODoDld8kEN","_uuid":"9f6a5c5c60d474efb143c4688b78a9626d4fd598"}},{"cell_type":"code","source":"from keras import optimizers\n\n# optimizer = optimizers.Adam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.001)\nmodel.compile(loss='categorical_crossentropy', optimizer='Nadam', metrics=['accuracy'])","metadata":{"id":"0kpTIGDQ8qAh","outputId":"b7c0f8cb-38a3-4e48-8b5f-a6ef1db7fc35","executionInfo":{"status":"ok","timestamp":1530806453746,"user_tz":-120,"elapsed":529,"user":{"displayName":"Abd El Rhman ElMoghazy","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"112798608963931725749"}},"_uuid":"3e1c28c3f990e45d77a5931196f1e357c0cb63fd","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Augmentation\nI will use ImageDataGenerator from keras to augment the images. Augmenting the images makes the model more robust and more generalizable when using newly unseen data like the data in the test set of the competition. There are many ways to augment the images like centering the images, normalization, rotation, shifting, and flipping and i will use some of them here .","metadata":{"_uuid":"54fd0230295e267437e44a8a795ec37d887f6bdb"}},{"cell_type":"code","source":"from tensorflow import keras\nfrom keras.preprocessing.image import ImageDataGenerator\nx_train2 = np.array(x_train, copy=True) \ny_train2 = np.array(y_train, copy=True) \n\ndatagen = ImageDataGenerator(\n    featurewise_center=True,\n    featurewise_std_normalization=True,\n    rotation_range=5,\n    fill_mode='nearest',\n    validation_split = 0.2\n    )\n\n\n# compute quantities required for featurewise normalization\n# (std, mean, and principal components if ZCA whitening is applied)\n\ndatagen.fit(x_train)\n\nprint(type(x_train))\n\nearlystopping = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=50, verbose=0, mode='min')\n\nvalidation_generator = datagen.flow(x_train2, y_train2, batch_size=60, subset='validation')\ntrain_generator = datagen.flow(x_train2, y_train2, batch_size=60, subset='training')\n\n\n# # fits the model on batches with real-time data augmentation:\nhistory = model.fit_generator(generator=train_generator,\n                    validation_data=validation_generator,\n                    use_multiprocessing=True,\n                    steps_per_epoch = len(train_generator) / 60,\n                    validation_steps = len(validation_generator) / 60,\n                    epochs = 300,\n                    workers=-1, callbacks = [earlystopping])","metadata":{"_uuid":"0efe80a7064c18d81fb44f59e468165118c624c6","_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nacc = history.history['acc']\nval_acc = history.history['val_acc']\n\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nplt.figure(figsize=(8, 8))\nplt.subplot(2, 1, 1)\nplt.plot(acc, label='Training Accuracy')\nplt.plot(val_acc, label='Validation Accuracy')\nplt.legend(loc='lower right')\nplt.ylabel('Accuracy')\nplt.ylim([-1,1])\nplt.title('Training and Validation Accuracy')\n\nplt.subplot(2, 1, 2)\nplt.plot(loss, label='Training Loss')\nplt.plot(val_loss, label='Validation Loss')\nplt.legend(loc='upper right')\nplt.ylabel('Cross Entropy')\nplt.ylim([-1,1.0])\nplt.title('Training and Validation Loss')\nplt.xlabel('epoch')\nplt.show()","metadata":{"_uuid":"ebbb6dd459ca7d20f104d3354883cc1690709a3d","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now it is time to train the Network. We will use an early stopping function to stp the training if the validation loss doesn't change with patience of 50 epochs","metadata":{"id":"GaXlA2t1lLKS","_uuid":"565b085a1dba154d865894584b6991f8d99cb295"}},{"cell_type":"markdown","source":"Now we should evaluate the model on the test set","metadata":{"id":"qlka9aQkD4Nl","_uuid":"c3f1f039f10bcf80089bb12798091fb2db84f679"}},{"cell_type":"code","source":"res = model.predict(x_test)\nres = np.argmax(res,axis = 1)\nres = pd.Series(res, name=\"Label\")\nsubmission = pd.concat([pd.Series(range(1 ,28001) ,name = \"ImageId\"),   res],axis = 1)\nsubmission.to_csv(\"solution.csv\",index=False)\nsubmission.head(10)","metadata":{"id":"vGjhPdAbD7wB","outputId":"78000d9f-fd82-4442-a3a1-faae0262d870","executionInfo":{"status":"ok","timestamp":1530806583605,"user_tz":-120,"elapsed":743,"user":{"displayName":"Abd El Rhman ElMoghazy","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"112798608963931725749"}},"_uuid":"439c3cd1ec8f70f6f58410027064ccfc0f57e966","trusted":true},"execution_count":null,"outputs":[]}]}