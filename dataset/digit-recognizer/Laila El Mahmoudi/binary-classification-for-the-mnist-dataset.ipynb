{"cells":[{"metadata":{},"cell_type":"markdown","source":"1. Summary\n2. Import the relevant libraries\n3. Loading the MNIST data\n4. Exploratory data analysis\n5. Preprocess the data set\n\n   5.1 Cleaning the data set\n   \n   5.2 Separate Features and Labels\n   \n6. Plotting the data set\n7. Data Splitting Process\n8. Training \n\n   8.1 Training a binary classifier\n\n         # STOCHASTIQUE GRADIENT DESCENT\n         # RANDOM FOREST ALGORITHM\n         # Comparing with a dump classifier\n         \n9. Performance Measures\n\n   9.1 Cross Validation\n   \n   9.2  Confusion Matrix\n   \n   9.3  Precision \n   \n   9.4 Recall \n   \n   9.5  F1 \n   \n   9.6 Precision/Recall Trade-off\n   \n10. The Test set","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 1. Summary","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"   \nThe goal of this notebook is to analyse a classification model with the MNIST data, so in this notebook, we will detect one number from MNIST data set using binary classifiers (A  classifier is an algorithm of machine learning that will determine the class to which the input data belongs to, based on a set of features). And then we will evaluate the measures of performance, and choose the model that have a great accuracy. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 2. Import the relevant libraries","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np  # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nimport seaborn as sns\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Data Splitting Process\n\nfrom sklearn.model_selection import train_test_split\n\n# Training Process\n\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Performance Measures \n\nfrom sklearn.base import BaseEstimator\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.metrics import precision_score, recall_score\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import roc_auc_score\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. Loading the MNIST data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"mnist_train = pd.read_csv(\"../input/digit-recognizer/train.csv\")\nmnist_test  = pd.read_csv(\"../input/digit-recognizer/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#Take copies of the master dataframes\n\ntrain = mnist_train.copy()\ntest = mnist_test.copy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4. Exploratory data analysis","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train.keys())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(test.keys())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5. Preprocess the data set","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### 5.1 Cleaning the data set","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train.isnull().any().any()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"the results means that the data is already clean, so we don't have any missing values","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### 5.2 Separate Features and Labels","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X, y = train.drop(labels = [\"label\"],axis = 1).to_numpy(), train[\"label\"]\nX.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 6. Plotting the data set","execution_count":null},{"metadata":{},"cell_type":"markdown","source":" feature X[20] contains '8' (image_pixel data) pixels 784 = 28*28\n y[20] contain 8 value","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"some_digit = X[20]\nsome_digit_show = plt.imshow(X[20].reshape(28,28), cmap=mpl.cm.binary)\ny[20]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = y.astype(np.uint8)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 7. Data Splitting Process","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 7. 1 Spliting Train and Test sets","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 8.Training Process","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 8.1 Training a binary classifier","execution_count":null},{"metadata":{"trusted":true},"cell_type":"markdown","source":"We were just training our model to predict 8.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_8 = (y_train == 8)\ny_test_8 = (y_test == 8)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### STOCHASTIQUE GRADIENT DESCENT","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sgd_clf = SGDClassifier(max_iter=1000,random_state = 42)\nsgd_clf.fit(X_train, y_train_8)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sgd_clf.predict([some_digit])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### RANDOM FOREST ALGORITHM","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\nrf_clf.fit(X_train, y_train_8)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_clf.predict([some_digit])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"so we can conclude that : some_digit X[20] == 8 is  True","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 9.1 Performance Measures","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"After we build our machine learning algorithm, we need to evaluate the performance for both models(SGD and Random Forest), there are many performance measures, in this notebook we will use the cross validation, Confusion Matrix, Precision/Recall/F1 score and ROC curve, and then we will analyze which model performs better.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### 9.1 Cross Validation","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Stochastique Gradien Descent","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"To evaluate the performance of a classifier model we can use the cross validation, but the accuracy is generally not the preferred performance measure for classifiers especially when some classes are more frequent than others.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cv_score_sgd = cross_val_score(sgd_clf, X_train, y_train_8, cv = 3, scoring = \"accuracy\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cv_score_sgd = np.mean(cv_score_sgd)\ncv_score_sgd","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Random Forest","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cv_score_rf = cross_val_score(rf_clf, X_train, y_train_8, cv= 3, scoring = \"accuracy\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cv_score_rf = np.mean(cv_score_rf)\ncv_score_rf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Comparing with a dump classifier","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"In general 92% accuracy seems good but we need to create a dumb \"Never8Classifier\", by extending Scikit-Learn's BaseEstimator","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class Never8Classifier(BaseEstimator):\n    def fit(sef, X, y=None):\n        pass\n    def predict(self, X):\n        return np.zeros((len(X), 1), dtype=bool)\n\nnever_8_clf = Never8Classifier()\n\ncross_val_score(never_8_clf, X_train, y_train_8, cv=3, scoring=\"accuracy\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We notice that only 10% of the images are 8s, so if we guess that an image is not a 8 , we will be right about 90% of the time.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 9.2 Confusion Matrix","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Stochastique Gradien Descent","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"A good way to measure the performance of a classifier is to look at the confusion matrix. The confusion matrix is the number of correct predictions and incorrect predictions are summarized with a count values and broke down by each class.\n\nTo calculate the confusion matrix we need a set of predictions, so that they can be compared to the actual targets.\n\nInstead, we can use the function of sklearn cross_val_predict().","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_pred = cross_val_predict(sgd_clf, X_train, y_train_8, cv= 3)\n\n\nconfusion_matrix(y_train_8, y_train_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Each row in the confusion matrix is an actual class, and each column represents a predicted class.\n\nThe first row of this matrix : 32,684 were correctly considred as non-8s (True Negatives)\nThe second row : 1,226, were wrongly classified as non-8s(there are called False Negative)\nThe first column : 1,456 we wrongly classifies as 8s (False Positive)\nThe second column : 2,434 we correcltly classifioed as 8s(True Positive)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 9.3 Precision","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"The confusion matrix gives a good results but sometimes we might use another metric more concise like the accuracy of the positive predictions, this called PRECISION of the classifier,\n\nPrecision is the ratio of correctly predicted positive observations, to the total predicted positive observations.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"precision_score(y_train_8, y_train_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 9.4 Recall","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Recall is the ratio of correctly predicted positive observations to the all observations in actual class, Recall is also called sensitivity or true positive rate (TPR).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"recall_score(y_train_8, y_train_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now the 8-detector does not look as the results of the accuracy, so when it claims an image represents a 8, it is correct only for 62.5%. More over, it only detects 66.5% of the 8s","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 9.5 F1 Score","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"F1 score is precision and recall combined into single metric. It's the harmonic mean of precision and recall","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"Score = f1_score(y_train_8, y_train_pred)\nprint(Score)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 9.6 Precision/Recall Trade-off","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"we can plot the precision and recall ratio by using the decision score, because sklearn does not give us the access to set the threshold. So using decision_function() we can get score values and decide whether it should be classified as 8 or not 8.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Stochastique Gradien Descent","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y_scores= cross_val_predict(sgd_clf, X_train, y_train_8, cv=3, method=\"decision_function\")\nprint(y_scores)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"precisions, recalls, thresholds = precision_recall_curve(y_train_8,y_scores)\n\n# here we use matplotlib to plot recall and precision as functions of the thresholds\n\ndef plot_precision_recall_vs_threshold(precisions, recalls, thresholds):\n    plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\")\n    plt.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\")\n    plt.xlabel(\"Threshold\")\n    plt.legend(loc=\"center left\")\n    plt.ylim([0, 1])\n    plt.title('Precision and recall versus the decision threshold')\n\n    \nplot_precision_recall_vs_threshold(precisions, recalls, thresholds)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_scores = sgd_clf.decision_function([X_train[0]])\nprint(\"Score for 1st digit: {0}\".format(y_scores[0]))\nprint(\"Was this digit a real 8? {0}\".format(y_train_8[0]))\n\ndigit_image = X_train[0].reshape(28,28)\nplt.imshow(digit_image, cmap= matplotlib.cm.binary, interpolation=\"nearest\")\nplt.axis(\"off\")\nplt.title(\"Digit image\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So here we set thresold to a very low value -250000, ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"threshold = -200000\ny_some_digit_pred = (y_scores > threshold)\ny_some_digit_pred","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Another way to select the best value of the threshold is to plot precision directly against recall ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def print_recalls_precision(recalls, precisions, title):\n    plt.figure(figsize=(8,6))\n    plt.plot(recalls, precisions, \"b-\", linewidth=2)\n    plt.xlabel(\"Recall\", fontsize=16)\n    plt.ylabel(\"Precision\", fontsize=16)\n    plt.title(\"Precision vs Recall plot - {0}\".format(title), fontsize=16)\n    plt.axis([0,1,0,1])\n    plt.show()\nprint_recalls_precision(recalls, precisions, \"stochastic gradient descend\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's use RandomForestClassifier and compare it with SGDClassifier","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Random Forest","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\ny_probas_forest = cross_val_predict(rf_clf, X_train, y_train_8, cv= 3, method= \"predict_proba\")\ny_scores_forest = y_probas_forest[:,1]\n\n# y_probas_forest contains 2 columns, one per class. Each row's sum of probabilities is equal to 1\n\nprecisions_forest, recalls_forest, thresholds = precision_recall_curve(y_train_8,y_scores_forest)\nprint_recalls_precision(recalls_forest, precisions_forest, \"Random Forest Classifier\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The graph results that the Random Forest Classifier performs clearly better than the SGD classifier.\n\nOtherwise we will plot the same graph for the dumb classifier, so that we can compare all the 3 classifiers; dumb classifier, Random Forest Classifier and SGD classifier.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### dumb classifier","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"never_8_predictions = cross_val_predict(never_8_clf, X_train, y_train_8, cv=3)\n\nprecisions_dumb, recalls_dumb, thresholds = precision_recall_curve(y_train_8, never_8_predictions)\n\nprint_recalls_precision(recalls_dumb, precisions_dumb, \"dumb classifier\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,6))\nplt.plot(precisions_forest, recalls_forest, \"-r\", label=\"Random Forest\")\nplt.plot(precisions,recalls, \"-g\",label=\"stochastic gradient descend\")\nplt.plot(precisions_dumb, recalls_dumb, \"-b\", label=\"dumb classifier\")\nplt.plot([0, 1], [1,0], \"k--\", label=\"Random guess\")\n\nplt.xlabel(\"Recall\", fontsize=16)\nplt.ylabel(\"precision\", fontsize=16)\n\n\nplt.title(\"Precision vs Recall - model comparison\", fontsize=16)\nplt.axis([0,1,0,1])\nplt.legend(loc=\"center left\")\nplt.ylim([0, 1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"F1 score for dumb classifier: {0}\".format(f1_score(y_train_8, never_8_predictions)))\nprint(\"F1 score for SGD classifier: {0}\".format(f1_score(y_train_8, y_train_pred)))\nprint(\"F1 score for Random Forest: {0}\".format(f1_score(y_train_8, y_scores_forest > 0.5)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"we can conclude that the random forest classifier performs better than the other classifiers","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 10. the test set","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions_sgd = sgd_clf.predict(X_test).astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Label = pd.Series(predictions_sgd,name = 'Label')\nImageId = pd.Series(range(1,28001),name = 'ImageId')\nsubmission = pd.concat([ImageId,Label],axis = 1)\nsubmission.to_csv('submission.csv',index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# clf = RandomForestClassifier(n_estimators=100, random_state=42)\n\n# clf.fit(X_train, y_train_8)\n\n\npredictions_forest = clf.predict(X_test).astype(int)\n\nLabel = pd.Series(predictions_forest,name = 'Label')\nImageId = pd.Series(range(1,28001),name = 'ImageId')\nsubmission = pd.concat([ImageId,Label],axis = 1)\nsubmission.to_csv('submission_forest.csv',index = False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}