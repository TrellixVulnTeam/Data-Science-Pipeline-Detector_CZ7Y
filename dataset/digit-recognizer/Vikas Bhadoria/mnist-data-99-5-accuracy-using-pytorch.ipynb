{"cells":[{"metadata":{},"cell_type":"markdown","source":"In this notebook I have created a CNN model built in PyTorch to predict MNIST dataset. \n**I was able to achieve 99.3% accuracy**. I also tested my model using a random image for internet and the prediction was perfect.\nPlease find my work below.","execution_count":null},{"metadata":{"id":"EhD5Io0VH5C2","trusted":true},"cell_type":"code","source":"import torch\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch.nn.functional as F\nfrom torch import nn\nfrom torchvision import datasets, transforms ## Hence torchvision is basically used as it has many datasets and also transform properities\n# that can be applied on these datasets","execution_count":null,"outputs":[]},{"metadata":{"id":"EYfm9Kx5JFs4"},"cell_type":"markdown","source":"Initializing **GPU** **Usage**","execution_count":null},{"metadata":{"id":"7ZFjoVVOIR4j","trusted":true},"cell_type":"code","source":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") # To use to cuba GPU","execution_count":null,"outputs":[]},{"metadata":{"id":"tcsmKEr2JMIX"},"cell_type":"markdown","source":"**Transformations**, this is really necessary as PyTorch works with tensors.","execution_count":null},{"metadata":{"id":"vD5N4B1gIekC","outputId":"716be132-6186-40ea-cd75-d6b98b7e8411","trusted":true},"cell_type":"code","source":"transform = transforms.Compose([transforms.Resize((28,28)),\n                               transforms.ToTensor(),\n                               transforms.Normalize((0.5,), (0.5,))\n                               ])\ntraining_dataset = datasets.MNIST(root='./data_1', train=True, download=True, transform=transform)\nvalidation_dataset = datasets.MNIST(root='./data_1', train=False, download=True, transform=transform)\n\ntraining_loader = torch.utils.data.DataLoader(training_dataset, batch_size=100, shuffle=True)\nvalidation_loader = torch.utils.data.DataLoader(validation_dataset, batch_size = 100, shuffle=False)","execution_count":null,"outputs":[]},{"metadata":{"id":"k2RW253wJSga"},"cell_type":"markdown","source":"**NumPy Image Conversion Function **, I did this to display my images using plt. As matplotlib.pyplot do not work with tensors.","execution_count":null},{"metadata":{"id":"aB6A4uZiI2iI","trusted":true},"cell_type":"code","source":"def im_convert(tensor):\n  image = tensor.cpu().clone().detach().numpy() # Just use cpu in this case as it is not compatible with gpu\n  image = image.transpose(1, 2, 0)\n  image = image * np.array((0.5, 0.5, 0.5)) + np.array((0.5, 0.5, 0.5))\n  image = image.clip(0, 1)\n  return image","execution_count":null,"outputs":[]},{"metadata":{"id":"TqJDnl66Jy7x","outputId":"a8874189-a37f-4b00-927c-fa386bc09274","trusted":true},"cell_type":"code","source":"\ndataiter = iter(training_loader)\nimages, labels = dataiter.next()\nfig = plt.figure(figsize=(25, 4))\n\nfor idx in np.arange(20):\n  ax = fig.add_subplot(2, 10, idx+1, xticks=[], yticks=[])\n  plt.imshow(im_convert(images[idx]))\n  ax.set_title([labels[idx].item()])","execution_count":null,"outputs":[]},{"metadata":{"id":"jW15TfMiJ2sk","trusted":true},"cell_type":"code","source":"class LeNet(nn.Module):\n    def __init__(self):\n      super().__init__()\n      self.conv1 = nn.Conv2d(1, 20, 5, 1) # Conv layer1\n      self.conv2 = nn.Conv2d(20, 50, 5, 1) # Conv layer2\n      self.fc1 = nn.Linear(4*4*50, 500)    # Fully connected layer1\n      self.dropout1 = nn.Dropout(0.5)   # We use dropout layer between these both FCL as they have the highest number of parameters b/t them\n      self.fc2 = nn.Linear(500, 10)   # Fully connected layer2\n    def forward(self, x):\n      x = F.relu(self.conv1(x))  # Apply ReLu to the feature maps produced after Conv 1 layer\n      x = F.max_pool2d(x, 2, 2)  # Pooling layer after Conv 1 layer\n      x = F.relu(self.conv2(x))  # Apply ReLu to the feature maps produced after Conv 2 layer\n      x = F.max_pool2d(x, 2, 2)  # Pooling layer after Conv 2 layer\n      x = x.view(-1, 4*4*50)     # Flattening the output of CNN to feed it into Fully connected layer\n      x = F.relu(self.fc1(x))   # Fully connected layer 1 with Relu\n      x = self.dropout1(x)     # We use dropout layer between these both FCL as they have the highest number of parameters b/t them\n      x = self.fc2(x)         # Fully connected layer 2 with no activation funct as we need raw output from CrossEntropyLoss\n      return x","execution_count":null,"outputs":[]},{"metadata":{"id":"KumHcA2gL8Po","outputId":"21bb9c62-c0e5-48b9-8a89-847f408e7b3f","trusted":true},"cell_type":"code","source":"model = LeNet().to(device)\nmodel","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* I will be using the most popular Adam optimizer.","execution_count":null},{"metadata":{"id":"nD0Tcsu9Oqkn","trusted":true},"cell_type":"code","source":"criterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr = 0.0001)","execution_count":null,"outputs":[]},{"metadata":{"id":"PCPw4kGMO3BK","outputId":"b1a1864f-b7c2-4b99-9e75-fb2caf50781c","trusted":true},"cell_type":"code","source":"epochs = 15\nrunning_loss_history = []\nrunning_corrects_history = []\nval_running_loss_history = []\nval_running_corrects_history = []\n\nfor e in range(epochs):\n  \n  running_loss = 0.0\n  running_corrects = 0.0\n  val_running_loss = 0.0\n  val_running_corrects = 0.0\n  \n  for inputs, labels in training_loader: # As our train_loader is batch size of 100 and had input images and corresponding labels\n    inputs = inputs.to(device)  # Put our inputs and labels in the device as our model is also in the device\n    labels = labels.to(device)\n    outputs = model(inputs)   # giving input to our model to get corresponding output\n    loss = criterion(outputs, labels) # comparing out model's output to original labels\n    \n    optimizer.zero_grad()  ##sets the initial gradient to zero\n    loss.backward()  ## The whole calculated loss is then back propogated to the model\n    optimizer.step()  ## Then the weights are updated by doing their derivative w.r.t the Loss\n    \n    _, preds = torch.max(outputs, 1) # Then we select the max value of raw output and consider it as our prediction. We select it from 10 o/ps\n    running_loss += loss.item()  # total loss of 1 epoch\n    running_corrects += torch.sum(preds == labels.data) #total accuracy of 1 epoch\n\n  else:\n    with torch.no_grad(): # This we done to set no gradient as we do not need it for val as our model is already trained.\n      for val_inputs, val_labels in validation_loader:\n        val_inputs = val_inputs.to(device)  # Put our val_inputs and labels in the device as our model is also in the device\n        val_labels = val_labels.to(device)\n        val_outputs = model(val_inputs)\n        val_loss = criterion(val_outputs, val_labels)\n        \n        _, val_preds = torch.max(val_outputs, 1)\n        val_running_loss += val_loss.item()\n        val_running_corrects += torch.sum(val_preds == val_labels.data)\n      \n    epoch_loss = running_loss/len(training_loader)\n    epoch_acc = running_corrects.float()/ len(training_loader)\n    running_loss_history.append(epoch_loss)\n    running_corrects_history.append(epoch_acc)\n    \n    val_epoch_loss = val_running_loss/len(validation_loader)\n    val_epoch_acc = val_running_corrects.float()/ len(validation_loader)\n    val_running_loss_history.append(val_epoch_loss)\n    val_running_corrects_history.append(val_epoch_acc)\n    print('epoch :', (e+1))\n    print('training loss: {:.4f}, acc {:.4f} '.format(epoch_loss, epoch_acc.item()))\n    print('validation loss: {:.4f}, validation acc {:.4f} '.format(val_epoch_loss, val_epoch_acc.item()))","execution_count":null,"outputs":[]},{"metadata":{"id":"Mu4RXFsIRo7j","outputId":"13a68ca6-28e0-4a95-b217-4999e1bf2159","trusted":true},"cell_type":"code","source":"plt.plot(running_loss_history, label='training loss')\nplt.plot(val_running_loss_history, label='validation loss')\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{"id":"XrWonqhvTFt6","outputId":"74d29907-e3af-4b4e-8a39-c8a247f410b8","trusted":true},"cell_type":"code","source":"plt.plot(running_corrects_history, label='training accuracy')\nplt.plot(val_running_corrects_history, label='validation accuracy')\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{"id":"3R8ri2c8TRiJ","trusted":true},"cell_type":"code","source":"import PIL.ImageOps","execution_count":null,"outputs":[]},{"metadata":{"id":"7qm7CAooTWIt","outputId":"ec06ecb8-84ce-4e6d-b0ef-0d921b2c316a","trusted":true},"cell_type":"code","source":"import requests\nfrom PIL import Image\n\nurl = 'https://images.homedepot-static.com/productImages/007164ea-d47e-4f66-8d8c-fd9f621984a2/svn/architectural-mailboxes-house-letters-numbers-3585b-5-64_1000.jpg'\nresponse = requests.get(url, stream = True)\nimg = Image.open(response.raw)\nplt.imshow(img)","execution_count":null,"outputs":[]},{"metadata":{"id":"gYjqL1WWTYzT","outputId":"303d374c-c0db-4e30-ad22-9f0dff6f2974","trusted":true},"cell_type":"code","source":"img = PIL.ImageOps.invert(img)  # we use Image operations from PIL to invert(i.e. make white black and vice versa)\nimg = img.convert('1') # we convert from RGB to Gray\nimg = transform(img) # Apply the transform funct we defined earlier to make our downloaded img same as what we trained on\nplt.imshow(im_convert(img))","execution_count":null,"outputs":[]},{"metadata":{"id":"CxSHTxFYTr4I","outputId":"fe3313b8-2e1b-4b95-c8d2-699bd6fd2d4d","trusted":true},"cell_type":"code","source":"images = img.to(device)  # As our model is in the device\nimage = images[0].unsqueeze(0).unsqueeze(0)\noutput = model(image)\n_, pred = torch.max(output, 1)\nprint(pred.item())","execution_count":null,"outputs":[]},{"metadata":{"id":"8UiHW9DmUB-x","outputId":"fd0eb2b7-db1b-49dc-c5ae-9eb43be38aeb","trusted":true},"cell_type":"code","source":"dataiter = iter(validation_loader)\nimages, labels = dataiter.next()\nimages = images.to(device)\nlabels = labels.to(device)\noutput = model(images)\n_, preds = torch.max(output, 1)\n\nfig = plt.figure(figsize=(25, 4))\n\nfor idx in np.arange(20):\n  ax = fig.add_subplot(2, 10, idx+1, xticks=[], yticks=[])\n  plt.imshow(im_convert(images[idx]))\n  ax.set_title(\"{} ({})\".format(str(preds[idx].item()), str(labels[idx].item())), color=(\"green\" if preds[idx]==labels[idx] else \"red\"))","execution_count":null,"outputs":[]},{"metadata":{"id":"zAiZXV6JUIe4"},"cell_type":"markdown","source":"**As seen I got really good accuracy and my model predicted all the new images correctly, which it has never seen before.**\n**As you can see it amount of coding that has to be done is much more when using PyTorch over Tensorflow**\n**I prefer working with Tensorflow & Keras because its simple to use.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}