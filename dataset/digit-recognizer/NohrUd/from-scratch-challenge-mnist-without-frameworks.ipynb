{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# ","metadata":{}},{"cell_type":"markdown","source":"## How to make a digit recognizer <font color=red>from scrach without frameworks</font>, Numpy-based solver.","metadata":{}},{"cell_type":"markdown","source":"### <font color=\"Blue\">1. Preparation before enjoying MNIST</font>","metadata":{}},{"cell_type":"code","source":"# import libraries\nimport os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split","metadata":{"execution":{"iopub.status.busy":"2021-07-22T06:01:10.119117Z","iopub.execute_input":"2021-07-22T06:01:10.119437Z","iopub.status.idle":"2021-07-22T06:01:10.829906Z","shell.execute_reply.started":"2021-07-22T06:01:10.119361Z","shell.execute_reply":"2021-07-22T06:01:10.829092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Follow Kaggle's way to load datasets.\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\ntrain_data = pd.read_csv(\"/kaggle/input/digit-recognizer/train.csv\")\nquiz_data = pd.read_csv(\"/kaggle/input/digit-recognizer/test.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-07-22T06:01:10.831349Z","iopub.execute_input":"2021-07-22T06:01:10.831723Z","iopub.status.idle":"2021-07-22T06:01:15.692792Z","shell.execute_reply.started":"2021-07-22T06:01:10.831683Z","shell.execute_reply":"2021-07-22T06:01:15.69197Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# X= 42,000 images of 28*28=784, Y= Correct labels \nX, T =train_data.iloc[0:,1:],train_data.iloc[0:,[0]]\nX=X.to_numpy()\nT=T.to_numpy()\n# Split data into training data and test data \nx_train, x_test, t_train, t_test = train_test_split(X, T, test_size=0.2)\nx_train = x_train.reshape(-1,1,28,28)\nx_test = x_test.reshape(-1,1,28,28)\n# Apply one-hot-vector to Taget data and change them into int type\nt_train = np.eye(10)[t_train.astype(\"int\")].reshape(-1,10)\nt_test = np.eye(10)[t_test.astype(\"int\")].reshape(-1,10)\n# Change quiz data into Numpy array\nquiz_x = quiz_data.iloc[0:,0:].to_numpy()\nquiz_x = quiz_x.reshape(-1,1,28,28)","metadata":{"execution":{"iopub.status.busy":"2021-07-22T06:01:15.694539Z","iopub.execute_input":"2021-07-22T06:01:15.694859Z","iopub.status.idle":"2021-07-22T06:01:16.071843Z","shell.execute_reply.started":"2021-07-22T06:01:15.694831Z","shell.execute_reply":"2021-07-22T06:01:16.071038Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <font color=\"Blue\">2. Let's check sample images.</font>","metadata":{}},{"cell_type":"code","source":"# Let's check sample images 28*28=784\n%matplotlib inline\nfor i in range(5):\n    plt.imshow(x_train[i][0],cmap='Greys')\n    plt.show()\n    print(\"label: \", t_train[i])","metadata":{"execution":{"iopub.status.busy":"2021-07-22T06:01:16.073441Z","iopub.execute_input":"2021-07-22T06:01:16.073787Z","iopub.status.idle":"2021-07-22T06:01:16.867286Z","shell.execute_reply.started":"2021-07-22T06:01:16.073749Z","shell.execute_reply":"2021-07-22T06:01:16.866296Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <font color=\"Blue\">3. Set classes and definitions like softmax,cross entropy,Relu.etc </font>","metadata":{}},{"cell_type":"code","source":"#This softmax function is applied an overflow countermajor.\ndef softmax(x):\n    if x.ndim == 2:\n        x = x.T\n        x = x - np.max(x, axis=0)   # x was trasposed on the previous line. That's why axis is 0.\n        y = np.exp(x) / np.sum(np.exp(x), axis=0)\n        return y.T \n    x = x - np.max(x) \n    return np.exp(x) / np.sum(np.exp(x))\n\ndef cross_entropy_error(y, t):\n    if y.ndim == 1:\n        t = t.reshape(1, t.size)\n        y = y.reshape(1, y.size)\n        \n    # argmax picked out the laragest array number in t. \n    if t.size == y.size:\n        t = t.argmax(axis=1)\n             \n    batch_size = y.shape[0]\n    delta = 1e-8\n    return -np.sum(np.log(y[np.arange(batch_size), t] + delta)) / batch_size\n","metadata":{"execution":{"iopub.status.busy":"2021-07-22T06:01:16.868613Z","iopub.execute_input":"2021-07-22T06:01:16.868957Z","iopub.status.idle":"2021-07-22T06:01:16.876923Z","shell.execute_reply.started":"2021-07-22T06:01:16.868922Z","shell.execute_reply":"2021-07-22T06:01:16.875938Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Definition of im2col, that is a useful image flatten transformation.\ndef im2col(input_data, filter_h, filter_w, stride=1, pad=0):\n    # Num = number of data , Channel = channel 、Height = height of image, W = Width\n    Num, Channel, Height, Width = input_data.shape\n    out_h = (Height + 2*pad - filter_h)//stride + 1\n    out_w = (Width + 2*pad - filter_w)//stride + 1\n    \n    img = np.pad(input_data, [(0,0), (0,0), (pad, pad), (pad, pad)] , 'constant') \n    col = np.zeros((Num, Channel, filter_h, filter_w, out_h, out_w))\n\n    for y in range(filter_h):\n        y_max = y + stride*out_h\n        for x in range(filter_w):\n            x_max = x + stride*out_w\n            #\n            col[:, :, y, x, :, :] = img[:, :, y:y_max:stride, x:x_max:stride]\n\n    col = col.transpose(0, 4, 5, 1, 2, 3).reshape(Num*out_h*out_w, -1)\n    return col","metadata":{"execution":{"iopub.status.busy":"2021-07-22T06:01:16.878435Z","iopub.execute_input":"2021-07-22T06:01:16.878815Z","iopub.status.idle":"2021-07-22T06:01:16.890336Z","shell.execute_reply.started":"2021-07-22T06:01:16.878775Z","shell.execute_reply":"2021-07-22T06:01:16.889571Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Definition of col2im, that is a backward transaction of im2col\ndef col2im(col, input_shape, filter_h, filter_w, stride=1, pad=0): \n    # Num = number of data , Channel = channel 、Height = height of image, W = Width\n    Num, Channel, Height, Width = input_shape\n    out_h = (Height + 2*pad - filter_h)//stride + 1\n    out_w = (Width + 2*pad - filter_w)//stride + 1\n    col = col.reshape(Num, out_h, out_w, Channel, filter_h, filter_w).transpose(0, 3, 4, 5, 1, 2)\n\n    img = np.zeros((Num, Channel, Height + 2*pad + stride - 1, Width + 2*pad + stride - 1))\n    for y in range(filter_h):\n        y_max = y + stride*out_h\n        for x in range(filter_w):\n            x_max = x + stride*out_w\n            img[:, :, y:y_max:stride, x:x_max:stride] += col[:, :, y, x, :, :]\n               \n\n    return img[:, :, pad:Height + pad, pad:Width + pad]","metadata":{"execution":{"iopub.status.busy":"2021-07-22T06:01:16.891539Z","iopub.execute_input":"2021-07-22T06:01:16.892074Z","iopub.status.idle":"2021-07-22T06:01:16.90085Z","shell.execute_reply.started":"2021-07-22T06:01:16.892036Z","shell.execute_reply":"2021-07-22T06:01:16.899766Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Set Relu function\nclass Relu:\n    def __init__(self):\n        self.mask = None\n\n    def forward(self, x):\n        self.mask = (x <= 0)\n        out = x.copy()\n        out[self.mask] = 0\n\n        return out\n\n    def backward(self, dout):\n        dout[self.mask] = 0\n        dx = dout\n\n        return dx\n","metadata":{"execution":{"iopub.status.busy":"2021-07-22T06:01:16.904673Z","iopub.execute_input":"2021-07-22T06:01:16.905174Z","iopub.status.idle":"2021-07-22T06:01:16.911944Z","shell.execute_reply.started":"2021-07-22T06:01:16.905135Z","shell.execute_reply":"2021-07-22T06:01:16.910714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set Affine layer\nclass Affine:\n    def __init__(self, input_size, output_size):\n        self.W = 0.01 * np.random.randn(input_size, output_size)\n        self.b = np.zeros(output_size)\n        self.x = None\n        self.original_x_shape = None\n        self.dW = None\n        self.db = None\n\n    def forward(self, x):\n        self.original_x_shape = x.shape\n        x = x.reshape(x.shape[0], -1)\n        self.x = x\n        out = np.dot(self.x, self.W) + self.b\n        return out\n\n    def backward(self, dout):\n        dx = np.dot(dout, self.W.T)\n        self.dW = np.dot(self.x.T, dout)\n        self.db = np.sum(dout, axis=0)\n        \n        #Decode data from reshaped one to iput shape\n        dx = dx.reshape(*self.original_x_shape) \n        return dx\n","metadata":{"execution":{"iopub.status.busy":"2021-07-22T06:01:16.913927Z","iopub.execute_input":"2021-07-22T06:01:16.914555Z","iopub.status.idle":"2021-07-22T06:01:16.923434Z","shell.execute_reply.started":"2021-07-22T06:01:16.914506Z","shell.execute_reply":"2021-07-22T06:01:16.922468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set Softmax with Loss Class\nclass SoftmaxWithLoss:\n    def __init__(self):\n        self.loss = None\n        self.y = None\n        self.t = None\n\n    def forward(self, x, t):\n        self.t = t\n        self.y = softmax(x)\n        self.loss = cross_entropy_error(self.y, self.t)\n        \n        return self.loss\n\n    def backward(self, dout=1):\n        batch_size = self.t.shape[0]\n        \n        #When Target data is one-hot-vector \n        if self.t.size == self.y.size:\n            dx = (self.y - self.t) / batch_size\n        else:\n            dx = self.y.copy()\n            dx[np.arange(batch_size), self.t] -= 1\n            dx = dx / batch_size\n        \n        return dx","metadata":{"execution":{"iopub.status.busy":"2021-07-22T06:01:16.924699Z","iopub.execute_input":"2021-07-22T06:01:16.925066Z","iopub.status.idle":"2021-07-22T06:01:16.933418Z","shell.execute_reply.started":"2021-07-22T06:01:16.92503Z","shell.execute_reply":"2021-07-22T06:01:16.932543Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Set convolution class\nclass Convolution:\n    def __init__(self, input_channel, output_channel, kernel_h=5, kernel_w=5, stride=1, pad=0):\n        self.W = 0.01 * np.random.randn(output_channel, input_channel, kernel_h, kernel_w)\n        self.b = np.zeros(output_channel)\n        self.stride = stride\n        self.pad = pad\n        self.x = None   \n        self.col = None\n        self.col_W = None\n        self.dW = None\n        self.db = None\n\n    def forward(self, x):\n        #Num_filters = Number of filters, Channel= Number of Channels, F_height=filter height, F_width= filter width\n        Num_filters, Channel, F_height, F_width = self.W.shape\n        Num, Channel, Height, Width = x.shape\n        out_h = 1 + int((Height + 2*self.pad - F_height) / self.stride)\n        out_w = 1 + int((Width + 2*self.pad - F_width) / self.stride)\n        col = im2col(x, F_height, F_width, self.stride, self.pad)\n        col_W = self.W.reshape(Num_filters, -1).T\n        out = np.dot(col, col_W) + self.b\n        out = out.reshape(Num, out_h, out_w, -1).transpose(0, 3, 1, 2)\n        self.x = x\n        self.col = col\n        self.col_W = col_W\n        return out\n    \n    def backward(self, dout):\n        #Num_filters = Number of filters, Channel= Number of Channels, F_height=filter height, F_width= filter width\n        Num_filters, Channel, F_height, F_width = self.W.shape\n        dout = dout.transpose(0,2,3,1).reshape(-1, Num_filters)\n        self.db = np.sum(dout, axis=0)\n        self.dW = np.dot(self.col.T, dout)\n        self.dW = self.dW.transpose(1, 0).reshape(Num_filters, Channel, F_height, F_width)\n        dcol = np.dot(dout, self.col_W.T)\n        dx = col2im(dcol, self.x.shape, F_height, F_width, self.stride, self.pad)\n        return dx","metadata":{"execution":{"iopub.status.busy":"2021-07-22T06:01:16.934705Z","iopub.execute_input":"2021-07-22T06:01:16.935109Z","iopub.status.idle":"2021-07-22T06:01:16.949955Z","shell.execute_reply.started":"2021-07-22T06:01:16.93507Z","shell.execute_reply":"2021-07-22T06:01:16.949135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Set Pooling Class\nclass Pooling:\n    def __init__(self, pool_h, pool_w, stride=1, pad=0):\n        self.pool_h = pool_h\n        self.pool_w = pool_w\n        self.stride = stride\n        self.pad = pad\n        \n        self.x = None\n        self.arg_max = None\n\n    def forward(self, x):\n        Num, Channel, Height, Width = x.shape\n        out_h = int(1 + (Height - self.pool_h) / self.stride)\n        out_w = int(1 + (Width - self.pool_w) / self.stride)\n        col = im2col(x, self.pool_h, self.pool_w, self.stride, self.pad)\n        col = col.reshape(-1, self.pool_h*self.pool_w)\n        arg_max = np.argmax(col, axis=1)\n        out = np.max(col, axis=1)\n        out = out.reshape(Num, out_h, out_w, Channel).transpose(0, 3, 1, 2)\n        self.x = x\n        self.arg_max = arg_max\n        return out\n\n    def backward(self, dout):\n        dout = dout.transpose(0, 2, 3, 1)\n        pool_size = self.pool_h * self.pool_w\n        dmax = np.zeros((dout.size, pool_size))\n        dmax[np.arange(self.arg_max.size), self.arg_max.flatten()] = dout.flatten()\n        dmax = dmax.reshape(dout.shape + (pool_size,))  \n        dcol = dmax.reshape(dmax.shape[0] * dmax.shape[1] * dmax.shape[2], -1)\n        dx = col2im(dcol, self.x.shape, self.pool_h, self.pool_w, self.stride, self.pad)\n        return dx","metadata":{"execution":{"iopub.status.busy":"2021-07-22T06:01:16.950896Z","iopub.execute_input":"2021-07-22T06:01:16.95125Z","iopub.status.idle":"2021-07-22T06:01:16.964623Z","shell.execute_reply.started":"2021-07-22T06:01:16.951214Z","shell.execute_reply":"2021-07-22T06:01:16.963836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Set Simple Convolution Network. x=input data, t=labels of target\nclass SimpleConvNet:\n    def __init__(self, input_dim=(1, 28, 28), output_size=10, weight_init_std=0.01):\n        self.layers = dict()\n        self.layers['Conv1'] = Convolution(1,10,5,5)\n        self.layers['Relu1'] = Relu()\n        self.layers['Pool1'] = Pooling(pool_h=2, pool_w=2, stride=2)\n        self.layers['Conv2'] =Convolution(10,10,5,5)\n        self.layers['Relu2'] = Relu()\n        self.layers['Affine'] = Affine(640, 10)\n        self.last_layer = SoftmaxWithLoss()\n        self.params = {}\n        self.params['W1'] = self.layers['Conv1'].W\n        self.params['b1'] = self.layers['Conv1'].b\n        self.params['W2'] = self.layers['Conv2'].W\n        self.params['b2'] = self.layers['Conv2'].b\n        self.params['W3'] = self.layers['Affine'].W\n        self.params['b3'] = self.layers['Affine'].b\n\n    def forward(self, x):\n        for layer in self.layers.values():\n            x = layer.forward(x)\n        return x\n\n    def loss(self, x, t):\n        y = self.forward(x) \n        return self.last_layer.forward(y, t)\n\n    def backward(self, x, t):\n        self.loss(x, t)\n        dout = 1\n        dout = self.last_layer.backward(dout)\n        layers = list(self.layers.values())\n        layers.reverse()\n        for layer in layers:\n            dout = layer.backward(dout)\n        grads = {}\n        grads['W1'], grads['b1'] = self.layers['Conv1'].dW, self.layers['Conv1'].db\n        grads['W2'], grads['b2'] = self.layers['Conv2'].dW, self.layers['Conv2'].db\n        grads['W3'], grads['b3'] = self.layers['Affine'].dW, self.layers['Affine'].db\n        return grads\n#Set NN\nnetwork = SimpleConvNet(input_dim=(1,28,28),  output_size=10, weight_init_std=0.01)\n#Set Static Gradient Descent\nclass SGD:\n    def __init__(self, lr=0.01):\n        self.lr = lr\n        \n    def update(self, params, grads):\n        for key in params.keys():\n            params[key] -= self.lr * grads[key] \n#Set learning rate .etc\nacc_list_SGD = []\nsgd = SGD(lr = 0.008)\nbatch_size = 256","metadata":{"execution":{"iopub.status.busy":"2021-07-22T06:01:16.966032Z","iopub.execute_input":"2021-07-22T06:01:16.966418Z","iopub.status.idle":"2021-07-22T06:01:16.983591Z","shell.execute_reply.started":"2021-07-22T06:01:16.966376Z","shell.execute_reply":"2021-07-22T06:01:16.982785Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Adam:\n\n    def __init__(self, lr, beta1, beta2): \n        self.lr = lr\n        self.beta1 = beta1\n        self.beta2 = beta2\n        self.iter = 0\n        self.m = None\n        self.v = None\n        \n    def update(self, params, grads):\n        if self.m is None:\n            self.m, self.v = {}, {}\n            for key, val in params.items():\n                self.m[key] = np.zeros_like(val)\n                self.v[key] = np.zeros_like(val)\n        \n        self.iter += 1\n        lr_t  = self.lr * np.sqrt(1.0 - self.beta2**self.iter) / (1.0 - self.beta1**self.iter)         \n        \n        for key in params.keys():\n            self.m[key] += (1 - self.beta1) * (grads[key] - self.m[key])\n            self.v[key] += (1 - self.beta2) * (grads[key]**2 - self.v[key])\n            \n            params[key] -= lr_t * self.m[key] / (np.sqrt(self.v[key]) + 1e-7)\n#Set learning rate .etc\nacc_list_adam = []\nadam = Adam(lr=0.0009, beta1=0.9, beta2=0.999)\nbatch_size = 256","metadata":{"execution":{"iopub.status.busy":"2021-07-22T06:01:16.985035Z","iopub.execute_input":"2021-07-22T06:01:16.985395Z","iopub.status.idle":"2021-07-22T06:01:16.996818Z","shell.execute_reply.started":"2021-07-22T06:01:16.98536Z","shell.execute_reply":"2021-07-22T06:01:16.995967Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <font color=\"Blue\">4. Let's start training with Static Gradient Descent. </font>","metadata":{}},{"cell_type":"code","source":"#Traing with Static Gradient Descent\nfor epoch in range(10):\n    perm = np.random.permutation(len(x_train))\n    for idx in np.arange(0, len(perm), batch_size):\n        x = x_train[perm[idx:idx+batch_size]]\n        t =  t_train[perm[idx:idx+batch_size]]\n        grads = network.backward(x, t)\n        sgd.update(network.params,grads)\n        \n    y_test = network.forward(x_test)\n    acc_list_SGD.append((y_test.argmax(axis=1) == t_test.argmax(axis=1)).mean())\n    print(f'epoch {epoch + 1} | accuracy {acc_list_SGD[-1]:.2%}')","metadata":{"execution":{"iopub.status.busy":"2021-07-22T06:04:14.329242Z","iopub.execute_input":"2021-07-22T06:04:14.329556Z","iopub.status.idle":"2021-07-22T06:05:45.234573Z","shell.execute_reply.started":"2021-07-22T06:04:14.329527Z","shell.execute_reply":"2021-07-22T06:05:45.233523Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <font color=\"Blue\">5. Let's start training with Adam. </font>","metadata":{}},{"cell_type":"code","source":"#Traing with Adam\nfor epoch in range(10):\n    perm = np.random.permutation(len(x_train))\n    for idx in np.arange(0, len(perm), batch_size):\n        x = x_train[perm[idx:idx+batch_size]]\n        t =  t_train[perm[idx:idx+batch_size]]\n        grads = network.backward(x, t)\n        adam.update(network.params,grads)\n        \n    y_test = network.forward(x_test)\n    acc_list_adam.append((y_test.argmax(axis=1) == t_test.argmax(axis=1)).mean())\n    print(f'epoch {epoch + 1} | accuracy {acc_list_adam[-1]:.2%}')","metadata":{"execution":{"iopub.status.busy":"2021-07-22T06:07:17.428431Z","iopub.execute_input":"2021-07-22T06:07:17.428767Z","iopub.status.idle":"2021-07-22T06:08:48.928542Z","shell.execute_reply.started":"2021-07-22T06:07:17.428737Z","shell.execute_reply":"2021-07-22T06:08:48.92753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(acc_list_SGD, label='Static Gradient Descent') \nplt.plot(acc_list_adam, label='Adam') \nplt.legend() ","metadata":{"execution":{"iopub.status.busy":"2021-07-22T06:09:29.157257Z","iopub.execute_input":"2021-07-22T06:09:29.157594Z","iopub.status.idle":"2021-07-22T06:09:29.32195Z","shell.execute_reply.started":"2021-07-22T06:09:29.157563Z","shell.execute_reply":"2021-07-22T06:09:29.320876Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <font color=\"Blue\">6. Let's predict by using test data! </font>","metadata":{}},{"cell_type":"markdown","source":"#### (1)Test sample","metadata":{}},{"cell_type":"code","source":"test_sample_num=2900\nplt.imshow(quiz_x[test_sample_num][0],cmap='Greys')","metadata":{"execution":{"iopub.status.busy":"2021-07-22T06:23:28.062649Z","iopub.execute_input":"2021-07-22T06:23:28.062981Z","iopub.status.idle":"2021-07-22T06:23:28.204715Z","shell.execute_reply.started":"2021-07-22T06:23:28.062947Z","shell.execute_reply":"2021-07-22T06:23:28.203776Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(network.forward(quiz_x[[test_sample_num]]))\ntest_answer=np.argmax(network.forward(quiz_x[[test_sample_num]]))\nprint(\"------------------------------------------------------\")\nprint(\"The prediction is\",test_answer,\".\",\"That's correct!\")\nprint(\"------------------------------------------------------\")","metadata":{"execution":{"iopub.status.busy":"2021-07-22T06:23:58.816063Z","iopub.execute_input":"2021-07-22T06:23:58.816396Z","iopub.status.idle":"2021-07-22T06:23:58.828316Z","shell.execute_reply.started":"2021-07-22T06:23:58.816368Z","shell.execute_reply":"2021-07-22T06:23:58.827107Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <font color=\"Blue\">7. Let's make submission data! </font>","metadata":{}},{"cell_type":"code","source":"list_predict=[]\nfor j in range(28000):\n    pj=np.argmax(network.forward(quiz_x[[j]]))\n    list_predict.append(pj)\nQuiz_TL = pd.Series(list_predict, name=\"Label\").astype(\"int32\")","metadata":{"execution":{"iopub.status.busy":"2021-07-22T06:24:14.357869Z","iopub.execute_input":"2021-07-22T06:24:14.358223Z","iopub.status.idle":"2021-07-22T06:24:38.573119Z","shell.execute_reply.started":"2021-07-22T06:24:14.358192Z","shell.execute_reply":"2021-07-22T06:24:38.572267Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"listIndex=[]\n[listIndex.append(i) for i in range(1,28001)]\nImageID = pd.Series(listIndex, name=\"ImageID\").astype(\"int32\")","metadata":{"execution":{"iopub.status.busy":"2021-07-22T06:24:38.57453Z","iopub.execute_input":"2021-07-22T06:24:38.574878Z","iopub.status.idle":"2021-07-22T06:24:38.594858Z","shell.execute_reply.started":"2021-07-22T06:24:38.574843Z","shell.execute_reply":"2021-07-22T06:24:38.594076Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.concat([ImageID,Quiz_TL],axis = 1)\nsubmission.tail()","metadata":{"execution":{"iopub.status.busy":"2021-07-22T06:27:12.542465Z","iopub.execute_input":"2021-07-22T06:27:12.542789Z","iopub.status.idle":"2021-07-22T06:27:12.552498Z","shell.execute_reply.started":"2021-07-22T06:27:12.542759Z","shell.execute_reply":"2021-07-22T06:27:12.551704Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv(\"prediction_from_scratch.csv\", index=False)\nprint(\"prediction_from_scratch.csv was saved.\")","metadata":{"execution":{"iopub.status.busy":"2021-07-22T06:26:36.899414Z","iopub.execute_input":"2021-07-22T06:26:36.899742Z","iopub.status.idle":"2021-07-22T06:26:36.917378Z","shell.execute_reply.started":"2021-07-22T06:26:36.899712Z","shell.execute_reply":"2021-07-22T06:26:36.916429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}