{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Digit recognizer using CNN, Tensorflow v2 and Keras"},{"metadata":{},"cell_type":"markdown","source":"# Intro\nWe have pretty simple looking task to classify pictures containing number 0-9. As dataset is pretty much clean, you will have no issues to get accuraccy above 95%, but when you want to cross 99.5% accuracy, things get more complicated. What I've learned here, that there is no \"best\" pattern and was more-less playing with composition of layers."},{"metadata":{},"cell_type":"markdown","source":"Initialy I've started simple CNN and then was adding and changing layers, but at the end I've added also image augmentation to make model more general. It slightly modify (rotate, flip, ...) images so it works better for unseen data. This notebook is as short as possible ;)"},{"metadata":{},"cell_type":"markdown","source":"# Load libraries and data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# first load libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout, BatchNormalization\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.optimizers import RMSprop\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# load data\ndf_train = pd.read_csv('/kaggle/input/digit-recognizer/train.csv')\ndf_test = pd.read_csv('/kaggle/input/digit-recognizer/test.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Extract our features and target"},{"metadata":{"trusted":true},"cell_type":"code","source":"# split our data into features & target\nX_train = df_train.drop('label', axis=1).values\ny_train = df_train['label'].values.reshape(-1,1)\n\nX_test = df_test.values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Rescale features\nDeep networks are sensitive on extreme values and is a must to scale features before running model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# rescale variables\nX_train = X_train.astype('float32')/255.0\nX_test = X_test.astype('float32')/255.0","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data check\nLet's show few images how it looks like in our dataset. Images are in grayscale (color channel dimension is equal to one) and is in range 0:255"},{"metadata":{"trusted":true},"cell_type":"code","source":"# check first few images\nplt.figure(figsize=(15,15))\nfor i in range(25):\n    plt.subplot(5,5,i+1)\n    plt.imshow(X_train[i].reshape(28,28), cmap='gray')\n    plt.title('Number:' + str(y_train[i][0]))\n    plt.axis('off')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Reshape data\nTensorflow require data in specific format (-1, image width, image height, number of channels), in our case it's (-1, 28, 28, 1)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# reshape features for tensorflow\nX_train = X_train.reshape(-1,28,28,1)\nX_test = X_test.reshape(-1,28,28,1)\n\n# one hot encode for target variable\ny_train = to_categorical(y_train)\ntarget_count = y_train.shape[1]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Image augmentation generator\nTo make our model robust and work well on unseen images, we will include also data augmentation step. This simply slightly modify our images in training process, each time in different random way. Our parameters are rotation in range of 10 degrees, width shift in range of 10%, height shift in range of 10%. Flip does not make sense too much in my opinion, it could i.e. in brain screening."},{"metadata":{},"cell_type":"markdown","source":"## Augmentation performance\nBased on my testing, augmentation of images did not bring much better performance on data validation dataset (was like 99.5%, but increased accuracy on unseen data from 99.4% to 99.6% that's really great."},{"metadata":{"trusted":true},"cell_type":"code","source":"# image augmentation \ndatagen = ImageDataGenerator(\n    featurewise_center=False,\n    samplewise_center=False,\n    featurewise_std_normalization=False,\n    samplewise_std_normalization=False,\n    zca_whitening=False,\n    rotation_range=10,\n    zoom_range = 0.1,\n    width_shift_range=0.1,\n    height_shift_range=0.1,\n    horizontal_flip=False,\n    vertical_flip=False)\n\n# fit generator on our train features\ndatagen.fit(X_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Split data into training & validation set\nThis might not be needed as tensorflow supports validation split, but we are using data image generator as well as want to check few of incorrectly classified images how good was our model"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Modelling\nModel structure:\n* 3 convolutional layers using 64 filters and kernel of size 3x3 with relu activation, each followed by batch normalization\n* max pooling with pool size 2x2, followed by droput (to avoid overfitting)\n* 3 convolutional layers using 64 filters and kernel of size 3x3 with relu activation, each followed by batch normalization\n* max pooling with pool size 2x2, followed by droput (to avoid overfitting)\n* flatten layer to make our data as 1 dimensional for dense layer\n* 2 dense layers (2nd is output one), batch normalization between dense layers\n* RMSprop algorithm as optimizer, using 0.001 learning rate and 0.99 rho\n* ReduceLROnPlateau function as callback to reduce learning rate when a metric has stopped improving."},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\n\nmodel.add(Conv2D(filters=64, kernel_size=(3,3), padding='valid', activation='relu', input_shape=(28, 28, 1)))\nmodel.add(BatchNormalization())\n\nmodel.add(Conv2D(filters=64, kernel_size=(3,3), padding='valid', activation='relu'))\nmodel.add(BatchNormalization())\n\nmodel.add(Conv2D(filters=64, kernel_size=(3,3), padding='valid', activation='relu'))\nmodel.add(BatchNormalization())\n\nmodel.add(MaxPooling2D(pool_size=(2,2)))\nmodel.add(Dropout(0.25))\nmodel.add(BatchNormalization())\n\nmodel.add(Conv2D(filters=128, kernel_size=(3,3), padding='valid', activation='relu'))\nmodel.add(BatchNormalization())\n\nmodel.add(Conv2D(filters=128, kernel_size=(3,3), padding='valid', activation='relu'))\nmodel.add(BatchNormalization())\n\nmodel.add(Conv2D(filters=128, kernel_size=(3,3), padding='valid', activation='relu'))\nmodel.add(BatchNormalization())\n\nmodel.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\nmodel.add(Dropout(0.25))\nmodel.add(BatchNormalization())\n\nmodel.add(Flatten())\n\nmodel.add(Dense(256, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(BatchNormalization())\n\nmodel.add(Dense(target_count, activation='softmax'))\n\n\noptimizer = RMSprop(learning_rate=0.001,rho=0.99)\nmodel.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.3, verbose=1,patience=2, min_lr=0.00000001)\n\ncallback = EarlyStopping(monitor='loss', patience=5)\nhistory = model.fit(datagen.flow(X_train,y_train, batch_size=64), epochs = 50, validation_data=(X_val, y_val), verbose = 1, callbacks=[reduce_lr, callback])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model evaluation\nWe could do confusion matrix, but not needed I think. Rather look on numbers those were incorrectly classified also discover for incorrectly classified, what were probabilities for other numbers. This will help us identify if our model is average, good, or excelent."},{"metadata":{"trusted":true},"cell_type":"code","source":"# prepare data for evaluation\ny_val_m = y_val.argmax(axis=1)\ny_val_hat_prob = model.predict(X_val)\ny_val_hat = y_val_hat_prob.argmax(axis=1)\nX_val_inc = X_val[y_val_m != y_val_hat, :, :, :]\ny_val_inc = y_val_m[y_val_m != y_val_hat]\ny_val_hat_inc = y_val_hat[y_val_m != y_val_hat]\ny_val_hat_prob_inc = y_val_hat_prob[y_val_m != y_val_hat]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Show incorrectly classified images\nWhat is expecatation? Simply to let human look on images and tell if it was possible to correctly classify them :) You will find some of them are almost impossible to classify to their actual value. Would you do it better than model?"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,15))\nfor i in range(16):\n    plt.subplot(4,4,i+1)\n    plt.imshow(X_val_inc[i, :, :, :].reshape(28,28), cmap='gray')\n    plt.axis('off')\n    plt.title('Actual: {}; Predicted: {}'.format(y_val_inc[i], y_val_hat_inc[i]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Probabilities of predicted vs actuals value\nJust look if in some cases model had high confidence in different value as well or was just keen on predicted value."},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(0,10):\n    act = y_val_inc[i]\n    pred = y_val_hat_inc[i]\n    print('Actual: {}; Confidence (act/pred): \\t{} - {:.0f}%  \\t{} - {:.0f}%'.format(act, act, y_val_hat_prob_inc[i][act]*100, pred, y_val_hat_prob_inc[i][pred]*100))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Submission\nLet's submit our data and hope in results at least 99.6% !"},{"metadata":{"trusted":true},"cell_type":"code","source":"# predict our test data\ny_test_hat = model.predict(X_test).argmax(axis=1)\n\ndf_submission = pd.read_csv('/kaggle/input/digit-recognizer/sample_submission.csv')\ndf_submission['Label'] = y_test_hat.astype('int32')\ndf_submission.to_csv('Submission.csv', index=False)\nprint('Submission saved!')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}