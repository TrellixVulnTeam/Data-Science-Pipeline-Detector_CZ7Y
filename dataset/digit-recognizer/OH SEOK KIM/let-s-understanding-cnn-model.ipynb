{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<hr style=\"border: solid 3px blue;\">\n\n# Introduction\n\n![](https://thumbs.gfycat.com/SecondhandSourGuineapig-max-1mb.gif)\n\nPicture Credit: https://thumbs.gfycat.com\n\nIn this notebook, we make a simple CNN model, and after training and testing, we want to check why our model makes such a decision using the MNIST dataset.","metadata":{}},{"cell_type":"code","source":"!pip install captum","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-12-11T05:28:39.902547Z","iopub.execute_input":"2021-12-11T05:28:39.902914Z","iopub.status.idle":"2021-12-11T05:28:48.784343Z","shell.execute_reply.started":"2021-12-11T05:28:39.902879Z","shell.execute_reply":"2021-12-11T05:28:48.783421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import libraries\nimport torch\nimport numpy as np\n\nimport torchvision\nimport torchvision.transforms as transforms\nimport torchvision.transforms.functional as TF\n\nfrom torchvision import models\n\nfrom captum.attr import IntegratedGradients\nfrom captum.attr import Saliency\nfrom captum.attr import DeepLift\nfrom captum.attr import NoiseTunnel\nfrom captum.attr import visualization as viz","metadata":{"execution":{"iopub.status.busy":"2021-12-11T05:28:48.786074Z","iopub.execute_input":"2021-12-11T05:28:48.786359Z","iopub.status.idle":"2021-12-11T05:28:48.793689Z","shell.execute_reply.started":"2021-12-11T05:28:48.786324Z","shell.execute_reply":"2021-12-11T05:28:48.792894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"--------------------------------------------------------------------\n# Loading Datasets\nDownload and executing the MNIST data set provided by Torchvision. batch_size can be selected according to your needs. Create a dataloader using the MNIST data set and use it as input data for the neural network.","metadata":{}},{"cell_type":"code","source":"from torchvision import datasets\nimport torchvision.transforms as transforms\nfrom torch.utils.data.sampler import SubsetRandomSampler\n\n# number of subprocesses to use for data loading\nnum_workers = 0\n# how many samples per batch to load\nbatch_size = 20\n# percentage of training set to use as validation\nvalid_size = 0.2\n\n# convert data to torch.FloatTensor\ntransform = transforms.ToTensor()\n\n# choose the training and test datasets\ntrain_data = datasets.MNIST(root='../data', train=True,\n                                   download=True, transform=transform)\ntest_data = datasets.MNIST(root='../data', train=False,\n                                  download=True, transform=transform)\n\n# obtain training indices that will be used for validation\nnum_train = len(train_data)\nindices = list(range(num_train))\nnp.random.shuffle(indices)\nsplit = int(np.floor(valid_size * num_train))\ntrain_idx, valid_idx = indices[split:], indices[:split]\n\n# define samplers for obtaining training and validation batches\ntrain_sampler = SubsetRandomSampler(train_idx)\nvalid_sampler = SubsetRandomSampler(valid_idx)\n\n# prepare data loaders\ntrain_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n    sampler=train_sampler, num_workers=num_workers)\nvalid_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, \n    sampler=valid_sampler, num_workers=num_workers)\ntest_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, \n    num_workers=num_workers)","metadata":{"execution":{"iopub.status.busy":"2021-12-11T05:28:48.795066Z","iopub.execute_input":"2021-12-11T05:28:48.795423Z","iopub.status.idle":"2021-12-11T05:28:48.866122Z","shell.execute_reply.started":"2021-12-11T05:28:48.795358Z","shell.execute_reply":"2021-12-11T05:28:48.865226Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"-------------------------------------------------------\n# Checking Dataset","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n%matplotlib inline\n    \n# obtain one batch of training images\ndataiter = iter(train_loader)\nimages, labels = dataiter.next()\nimages = images.numpy()\n\n# plot the images in the batch, along with the corresponding labels\nfig = plt.figure(figsize=(25, 4))\nfor idx in np.arange(10):\n    ax = fig.add_subplot(2, 10, idx+1, xticks=[], yticks=[])\n    ax.imshow(np.squeeze(images[idx]), cmap='gray')\n    ax.set_title(str(labels[idx].item()),fontsize=30)","metadata":{"execution":{"iopub.status.busy":"2021-12-11T05:28:48.868252Z","iopub.execute_input":"2021-12-11T05:28:48.868552Z","iopub.status.idle":"2021-12-11T05:28:49.555903Z","shell.execute_reply.started":"2021-12-11T05:28:48.868521Z","shell.execute_reply":"2021-12-11T05:28:49.554773Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img = np.squeeze(images[1])\n\nfig = plt.figure(figsize = (14,14)) \nax = fig.add_subplot(111)\nax.imshow(img, cmap='gray')\nwidth, height = img.shape\nthresh = img.max()/2.5\nfor x in range(width):\n    for y in range(height):\n        val = round(img[x][y],2) if img[x][y] !=0 else 0\n        ax.annotate(str(val), xy=(y,x),\n                    horizontalalignment='center',\n                    verticalalignment='center',\n                    color='white' if img[x][y]<thresh else 'black')","metadata":{"execution":{"iopub.status.busy":"2021-12-11T05:28:49.557334Z","iopub.execute_input":"2021-12-11T05:28:49.557638Z","iopub.status.idle":"2021-12-11T05:28:55.744913Z","shell.execute_reply.started":"2021-12-11T05:28:49.557607Z","shell.execute_reply":"2021-12-11T05:28:55.743925Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"--------------------------------------------\n# Defining Model\n\n* It consists of two Conv2d layers.\n* Use dropout to avoid overfitting.","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn\nimport torch.nn.functional as F\n\n# define the NN architecture\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n        self.conv2_drop = nn.Dropout2d()\n        self.fc1 = nn.Linear(320, 50)\n        self.fc2 = nn.Linear(50, 10)\n\n    def forward(self, x):\n        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n        x = x.view(-1, 320)\n        x = F.relu(self.fc1(x))\n        x = F.dropout(x, training=self.training)\n        x = self.fc2(x)\n        return x\n\n# initialize the NN\nmodel = Net()\nprint(model)","metadata":{"execution":{"iopub.status.busy":"2021-12-11T05:28:55.746006Z","iopub.execute_input":"2021-12-11T05:28:55.746246Z","iopub.status.idle":"2021-12-11T05:28:55.759644Z","shell.execute_reply.started":"2021-12-11T05:28:55.746207Z","shell.execute_reply":"2021-12-11T05:28:55.758664Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Defining Loss Function and Optimizer**\n\nFor classification, the loss function uses cross-entropy.","metadata":{}},{"cell_type":"code","source":"# specify loss function (categorical cross-entropy)\ncriterion = nn.CrossEntropyLoss()\n\n# specify optimizer (stochastic gradient descent) and learning rate = 0.01\noptimizer = torch.optim.SGD(model.parameters(), lr=0.01)","metadata":{"execution":{"iopub.status.busy":"2021-12-11T05:28:55.76099Z","iopub.execute_input":"2021-12-11T05:28:55.761387Z","iopub.status.idle":"2021-12-11T05:28:55.770306Z","shell.execute_reply.started":"2021-12-11T05:28:55.761332Z","shell.execute_reply":"2021-12-11T05:28:55.769328Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"-------------------------------------------\n# Training\n\nFor each batch, the training proceeds in the following order.\n\n* Clear the gradients of all optimized variables\n* Forward pass: compute predicted outputs by passing inputs to the model \n* Calculate the loss\n* Backward pass: compute gradient of the loss with respect to model parameters\n* Perform a single optimization step (parameter update)\n* Update average training loss\n\nTrain every number of epochs and save the model with the smallest valid loss","metadata":{}},{"cell_type":"code","source":"# number of epochs to train the model\nn_epochs = 10\n\n# initialize tracker for minimum validation loss\nvalid_loss_min = np.Inf # set initial \"min\" to infinity\n\nfor epoch in range(n_epochs):\n    # monitor training loss\n    train_loss = 0.0\n    valid_loss = 0.0\n    \n    ###################\n    # train the model #\n    ###################\n    model.train() # prep model for training\n    for data, target in train_loader:\n        # clear the gradients of all optimized variables\n        optimizer.zero_grad()\n        # forward pass: compute predicted outputs by passing inputs to the model\n        output = model(data)\n        # calculate the loss\n        loss = criterion(output, target)\n        # backward pass: compute gradient of the loss with respect to model parameters\n        loss.backward()\n        # perform a single optimization step (parameter update)\n        optimizer.step()\n        # update running training loss\n        train_loss += loss.item()*data.size(0)\n        \n    ######################    \n    # validate the model #\n    ######################\n    model.eval() # prep model for evaluation\n    for data, target in valid_loader:\n        # forward pass: compute predicted outputs by passing inputs to the model\n        output = model(data)\n        # calculate the loss\n        loss = criterion(output, target)\n        # update running validation loss \n        valid_loss += loss.item()*data.size(0)\n        \n    # print training/validation statistics \n    # calculate average loss over an epoch\n    train_loss = train_loss/len(train_loader.dataset)\n    valid_loss = valid_loss/len(valid_loader.dataset)\n    \n    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n        epoch+1, \n        train_loss,\n        valid_loss\n        ))\n    \n    # save model if validation loss has decreased\n    if valid_loss <= valid_loss_min:\n        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n        valid_loss_min,\n        valid_loss))\n        torch.save(model.state_dict(), 'model_cnn.pt')\n        valid_loss_min = valid_loss","metadata":{"execution":{"iopub.status.busy":"2021-12-11T05:28:55.772411Z","iopub.execute_input":"2021-12-11T05:28:55.772771Z","iopub.status.idle":"2021-12-11T05:33:17.7761Z","shell.execute_reply.started":"2021-12-11T05:28:55.772726Z","shell.execute_reply":"2021-12-11T05:33:17.775349Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Load the model with the minimum valid loss stored during training.","metadata":{}},{"cell_type":"code","source":"model.load_state_dict(torch.load('model_cnn.pt'))","metadata":{"execution":{"iopub.status.busy":"2021-12-11T05:33:17.777708Z","iopub.execute_input":"2021-12-11T05:33:17.777986Z","iopub.status.idle":"2021-12-11T05:33:17.786679Z","shell.execute_reply.started":"2021-12-11T05:33:17.777954Z","shell.execute_reply":"2021-12-11T05:33:17.785601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"-----------------------------------------------------\n# Testing\n\n* Finally, using the best model, the trained model is tested using the test data.\n* Evaluate the model using test dataset not used for training.\n* It can be evaluated by test loss and accuracy.","metadata":{}},{"cell_type":"code","source":"# initialize lists to monitor test loss and accuracy\ntest_loss = 0.0\nclass_correct = list(0. for i in range(10))\nclass_total = list(0. for i in range(10))\n\nmodel.eval() # prep model for evaluation\n\nfor data, target in test_loader:\n    # forward pass: compute predicted outputs by passing inputs to the model\n    output = model(data)\n    # calculate the loss\n    loss = criterion(output, target)\n    # update test loss \n    test_loss += loss.item()*data.size(0)\n    # convert output probabilities to predicted class\n    _, pred = torch.max(output, 1)\n    # compare predictions to true label\n    correct = np.squeeze(pred.eq(target.data.view_as(pred)))\n    # calculate test accuracy for each object class\n    for i in range(len(target)):\n        label = target.data[i]\n        class_correct[label] += correct[i].item()\n        class_total[label] += 1\n\n# calculate and print avg test loss\ntest_loss = test_loss/len(test_loader.dataset)\nprint('Test Loss: {:.6f}\\n'.format(test_loss))\n\nfor i in range(10):\n    if class_total[i] > 0:\n        print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (\n            str(i), 100 * class_correct[i] / class_total[i],\n            np.sum(class_correct[i]), np.sum(class_total[i])))\n    else:\n        print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))\n\nprint('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (\n    100. * np.sum(class_correct) / np.sum(class_total),\n    np.sum(class_correct), np.sum(class_total)))","metadata":{"execution":{"iopub.status.busy":"2021-12-11T05:33:17.789277Z","iopub.execute_input":"2021-12-11T05:33:17.789614Z","iopub.status.idle":"2021-12-11T05:33:20.688995Z","shell.execute_reply.started":"2021-12-11T05:33:17.789581Z","shell.execute_reply":"2021-12-11T05:33:20.688085Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---------------------------------------------------------\n# Interpreting Model\n\n![](https://i.pinimg.com/originals/ff/04/31/ff0431d11ff6b73e937280252f58f371.gif)\n\nPicture Credit: https://i.pinimg.com\n\nWe want to understand on what basis our CNN model judged the flower type. Therefore, we want to visually understand the judgment basis of our model through the following two methods.\n\n**Saliency detection**\n\nSaliency detection refers to separating an object of interest from a background that is not of interest, and the result is a binarized image as shown below. These detection methods help you spend less time and energy in determining the most relevant parts of an image. In other words, simplifying the representation of an image to make it more meaningful and easier to analyze.\n\n**IntegratedGradients**\n\nA gradient for each pixel of the generated images will appear, and integrating these gradients will show the effect on the overall pixel output. More details about integrated gradients can be found in the original paper: https://arxiv.org/abs/1703.01365","metadata":{}},{"cell_type":"code","source":"def imshow(img, transpose = True):\n    img = img / 2 + 0.5     # unnormalize\n    npimg = img.numpy()\n    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n    plt.show()\n\ndataiter = iter(valid_loader)\nimages, labels = dataiter.next()\n\n# print images\nimshow(torchvision.utils.make_grid(images))\n\n\noutputs = model(images)\n\n_, predicted = torch.max(outputs, 1)","metadata":{"execution":{"iopub.status.busy":"2021-12-11T05:33:20.690867Z","iopub.execute_input":"2021-12-11T05:33:20.691535Z","iopub.status.idle":"2021-12-11T05:33:20.947396Z","shell.execute_reply.started":"2021-12-11T05:33:20.691488Z","shell.execute_reply":"2021-12-11T05:33:20.94626Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def attribute_image_features(algorithm, input, **kwargs):\n    model.zero_grad()\n    tensor_attributions = algorithm.attribute(input,\n                                              target=labels[ind],\n                                              **kwargs\n                                             )\n    \n    return tensor_attributions","metadata":{"execution":{"iopub.status.busy":"2021-12-11T05:33:20.948849Z","iopub.execute_input":"2021-12-11T05:33:20.94944Z","iopub.status.idle":"2021-12-11T05:33:20.95574Z","shell.execute_reply.started":"2021-12-11T05:33:20.94939Z","shell.execute_reply":"2021-12-11T05:33:20.955026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for ind in range(5):\n    print('---'*20)\n    print(f'indicaton = {ind}')\n    input = images[ind].unsqueeze(0)\n    input.requires_grad = True\n    \n    saliency = Saliency(model)\n    grads = saliency.attribute(input, target=labels[ind].item())\n    squeeze_grads = grads.squeeze().cpu().detach()\n    squeeze_grads = torch.unsqueeze(squeeze_grads,0).numpy()\n    grads = np.transpose(squeeze_grads, (1, 2, 0))\n    \n    ig = IntegratedGradients(model)\n    attr_ig, delta = attribute_image_features(ig, input, baselines=input * 0, return_convergence_delta=True)\n    squeeze_ig = attr_ig.squeeze().cpu().detach()\n    squeeze_ig = torch.unsqueeze(squeeze_ig,0).numpy()\n    attr_ig = np.transpose(squeeze_ig, (1, 2, 0))\n    #print('Approximation delta: ', abs(delta))\n    \n    original_image = np.transpose((images[ind].cpu().detach().numpy() / 2) + 0.5, (1, 2, 0))\n\n    _ = viz.visualize_image_attr(None, original_image, \n                      method=\"original_image\", title=\"Original Image\")\n\n    _ = viz.visualize_image_attr(grads, original_image, method=\"blended_heat_map\", sign=\"absolute_value\",\n                          show_colorbar=True, title=\"Overlayed Gradient Magnitudes\")\n\n    _ = viz.visualize_image_attr(attr_ig, original_image, method=\"blended_heat_map\",sign=\"all\",\n                          show_colorbar=True, title=\"Overlayed Integrated Gradients\")","metadata":{"execution":{"iopub.status.busy":"2021-12-11T05:33:20.956986Z","iopub.execute_input":"2021-12-11T05:33:20.958004Z","iopub.status.idle":"2021-12-11T05:33:25.862264Z","shell.execute_reply.started":"2021-12-11T05:33:20.957964Z","shell.execute_reply":"2021-12-11T05:33:25.861258Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Looking at the pictures above, we can see the pixels that our CNN model has determined to be important. Visually, we can see that our model is looking at the appropriate pixels.","metadata":{}},{"cell_type":"markdown","source":"<hr style=\"border: solid 3px blue;\">","metadata":{}}]}