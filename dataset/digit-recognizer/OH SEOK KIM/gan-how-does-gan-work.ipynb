{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<hr style=\"border: solid 3px blue;\">\n\n# Introduction","metadata":{}},{"cell_type":"markdown","source":"DCGAN uses the image created by the creator. Therefore, after training, we discard the discriminator and use only the generator to create a plausible generative image.\nIn this notebook, we implement and train DCGAN using simple handwriting.\n\n![](https://miro.medium.com/max/920/1*fN-q2XG9CTii8S6Xh8SIyg.gif)\n\nPicture Credit: https://miro.medium.com","metadata":{}},{"cell_type":"markdown","source":"-------------------------------------------\n# Setting","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nimport pandas as pd\nfrom tensorflow.keras.datasets import mnist\nfrom tensorflow.keras.layers import Activation, BatchNormalization, Dense, Dropout, Flatten, Reshape\nfrom tensorflow.keras.layers import LeakyReLU\nfrom tensorflow.keras.layers import Conv2D, Conv2DTranspose\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.optimizers import Adam\n\nimport warnings , os\nwarnings.filterwarnings(action='ignore')\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n\nimport umap\nimport umap.plot","metadata":{"execution":{"iopub.status.busy":"2022-01-19T14:28:55.749596Z","iopub.execute_input":"2022-01-19T14:28:55.749899Z","iopub.status.idle":"2022-01-19T14:28:55.758471Z","shell.execute_reply.started":"2022-01-19T14:28:55.749859Z","shell.execute_reply":"2022-01-19T14:28:55.756764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img_rows = 28\nimg_cols = 28\nchannels = 1\n\n# input image dimension\nimg_shape = (img_rows, img_cols, channels)\n\n# size of noise vector to be used as generator input\nz_dim = 100","metadata":{"execution":{"iopub.status.busy":"2022-01-19T14:28:55.761069Z","iopub.execute_input":"2022-01-19T14:28:55.761505Z","iopub.status.idle":"2022-01-19T14:28:55.776514Z","shell.execute_reply.started":"2022-01-19T14:28:55.761382Z","shell.execute_reply":"2022-01-19T14:28:55.775194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"-----------------------------------\n# Modeling\n\n![](https://i.pinimg.com/originals/1d/6c/f3/1d6cf3dd8f447252b97475b7c6efe6d2.gif)\n\nPicture Credit: https://i.pinimg.com\n","metadata":{}},{"cell_type":"markdown","source":"> The generative network generates candidates while the discriminative network evaluates them. The contest operates in terms of data distributions. Typically, the generative network learns to map from a latent space to a data distribution of interest, while the discriminative network distinguishes candidates produced by the generator from the true data distribution. The generative network's training objective is to increase the error rate of the discriminative network (i.e., \"fool\" the discriminator network by producing novel candidates that the discriminator thinks are not synthesized (are part of the true data distribution)).\n\nRef: https://en.wikipedia.org/wiki/Generative_adversarial_network\n\n**Now, let's model the generator and discriminator so that they compete against each other.**","metadata":{}},{"cell_type":"markdown","source":"------------------------------------------------\n# Modeling Generator and Discriminator\n\n![](https://cdn-images-1.medium.com/max/880/1*EBbefSaB1VlrOqzAOSycWQ.gif)\n\nPicture Credit: https://cdn-images-1.medium.com","metadata":{}},{"cell_type":"markdown","source":"-------------------------------------\n## Modeling Generator\n\n![](https://pytorch.org/tutorials/_images/dcgan_generator.png)\n\nPicture credit: https://pytorch.org","metadata":{}},{"cell_type":"markdown","source":"The generator starts at the noise vector z. We use a fully connected layer to transform this vector into a 3D output with a small area and large length. Using transposed convolution, the output is gradually changed to decrease the depth and increase the area. When we reach the last layer, the size of the image will be the size we need 28 x 28 x 1.","metadata":{}},{"cell_type":"code","source":"def build_generator(z_dim):\n    model = Sequential()\n    model.add(Dense(256 * 7 * 7, input_dim=z_dim))\n    model.add(Reshape((7, 7, 256)))\n    model.add(Conv2DTranspose(128, kernel_size=3, strides=2, padding='same'))\n    model.add(BatchNormalization())\n    model.add(LeakyReLU(alpha=0.01))\n    model.add(Conv2DTranspose(64, kernel_size=3, strides=1, padding='same'))\n    model.add(BatchNormalization())\n    model.add(LeakyReLU(alpha=0.01))\n    model.add(Conv2DTranspose(1, kernel_size=3, strides=2, padding='same'))\n    model.add(Activation('tanh'))\n\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-01-19T14:28:55.778624Z","iopub.execute_input":"2022-01-19T14:28:55.779951Z","iopub.status.idle":"2022-01-19T14:28:55.792172Z","shell.execute_reply.started":"2022-01-19T14:28:55.779886Z","shell.execute_reply":"2022-01-19T14:28:55.79094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---------------------------------------------\n## Modeling Discriminator\n\n![](https://miro.medium.com/max/1600/1*UipjlvULzSCCr1szzZpKYQ.jpeg)\npicture credit: https://miro.medium.com","metadata":{}},{"cell_type":"markdown","source":"The discriminator is a ConvNet similar to a convolution network that takes an image and outputs a prediction vector. In this case, the binary classifier determines whether the input image is fake or real.","metadata":{}},{"cell_type":"code","source":"def build_discriminator(img_shape):\n    model = Sequential()\n    model.add(\n        Conv2D(32,\n               kernel_size=3,\n               strides=2,\n               input_shape=img_shape,\n               padding='same'))\n\n    model.add(LeakyReLU(alpha=0.01))\n\n    model.add(\n        Conv2D(64,\n               kernel_size=3,\n               strides=2,\n               padding='same'))\n\n    model.add(LeakyReLU(alpha=0.01))\n\n    model.add(\n        Conv2D(128,\n               kernel_size=3,\n               strides=2,\n               padding='same'))\n\n    model.add(LeakyReLU(alpha=0.01))\n\n    model.add(Flatten())\n    model.add(Dense(1, activation='sigmoid'))\n\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-01-19T14:28:55.794239Z","iopub.execute_input":"2022-01-19T14:28:55.795353Z","iopub.status.idle":"2022-01-19T14:28:55.812179Z","shell.execute_reply.started":"2022-01-19T14:28:55.795297Z","shell.execute_reply":"2022-01-19T14:28:55.810771Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"------------------------------------------------\n# Modeling GAN","metadata":{}},{"cell_type":"code","source":"def build_gan(generator, discriminator):\n    model = Sequential()\n\n    # Genearator -> Discriminator\n    model.add(generator)\n    model.add(discriminator)\n\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-01-19T14:28:55.814958Z","iopub.execute_input":"2022-01-19T14:28:55.815865Z","iopub.status.idle":"2022-01-19T14:28:55.829856Z","shell.execute_reply.started":"2022-01-19T14:28:55.815812Z","shell.execute_reply":"2022-01-19T14:28:55.82831Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Creating discriminator","metadata":{}},{"cell_type":"code","source":"# Creating and Compiling discriminator Models\ndiscriminator = build_discriminator(img_shape)\ndiscriminator.compile(loss='binary_crossentropy',\n                      optimizer=Adam(),\n                      metrics=['accuracy'])\n# Preserve the parameters of the discriminator while training the generator\ndiscriminator.trainable = False\n","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-01-19T14:28:55.832776Z","iopub.execute_input":"2022-01-19T14:28:55.833137Z","iopub.status.idle":"2022-01-19T14:28:55.910729Z","shell.execute_reply.started":"2022-01-19T14:28:55.833102Z","shell.execute_reply":"2022-01-19T14:28:55.909846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Creating generator","metadata":{}},{"cell_type":"code","source":"# Create a generator model\ngenerator = build_generator(z_dim)\n\n# Create and compile a GAN model with frozen discriminators to train the generator\ngan = build_gan(generator, discriminator)\ngan.compile(loss='binary_crossentropy', optimizer=Adam())","metadata":{"execution":{"iopub.status.busy":"2022-01-19T14:28:55.912482Z","iopub.execute_input":"2022-01-19T14:28:55.912825Z","iopub.status.idle":"2022-01-19T14:28:56.11895Z","shell.execute_reply.started":"2022-01-19T14:28:55.912781Z","shell.execute_reply":"2022-01-19T14:28:56.117753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Defining training function","metadata":{}},{"cell_type":"code","source":"losses = []\naccuracies = []\niteration_checkpoints = []\n\ndef train(iterations, batch_size, sample_interval):\n    # Load MNIST dataset\n    (x_train, y_train), (_, _) = mnist.load_data()\n\n    # [0, 255] scales black and white pixel values between [-1, 1]\n    X_train = x_train / 127.5 - 1.0\n    X_train = np.expand_dims(X_train, axis=3)\n\n    # Real Image Label: All 1\n    real = np.ones((batch_size, 1))\n\n    # Fake Image Labels: All 0\n    fake = np.zeros((batch_size, 1))\n\n    for iteration in range(iterations):\n        # -------------------------\n        #  Discriminant training\n        # -------------------------\n\n        # Get random batches from real images\n        idx = np.random.randint(0, X_train.shape[0], batch_size)\n        imgs = X_train[idx]\n\n        # Create fake image batches\n        z = np.random.normal(0, 1, (batch_size, 100))\n        gen_imgs = generator.predict(z)\n\n        # Training \n        d_loss_real = discriminator.train_on_batch(imgs, real)\n        d_loss_fake = discriminator.train_on_batch(gen_imgs, fake)\n        d_loss, accuracy = 0.5 * np.add(d_loss_real, d_loss_fake)\n\n        # ---------------------\n        #  Training generator\n        # ---------------------\n\n        # Create fake image batches\n        z = np.random.normal(0, 1, (batch_size, 100))\n        \n        if iteration == 0:\n            print(\"\\nRandom noise input image\")\n            sample_images(x_train[idx],y_train[idx],generator)            \n\n        g_loss = gan.train_on_batch(z, real)\n\n        if iteration < 200 and (iteration + 1) % 20 == 0:\n            # Save loss and accuracy to plot graphs after training\n            losses.append((d_loss, g_loss))\n            accuracies.append(100.0 * accuracy)\n            iteration_checkpoints.append(iteration + 1)\n\n            print(\"%d [D loss: %f, accuracy: %.2f%%] [G loss: %f]\" %\n                  (iteration + 1, d_loss, 100.0 * accuracy, g_loss))\n\n            # Generated image sample output\n            sample_images(x_train[idx],y_train[idx],generator)\n            \n        elif (iteration + 1) % sample_interval == 0:\n            # Save loss and accuracy to plot graphs after training\n            losses.append((d_loss, g_loss))\n            accuracies.append(100.0 * accuracy)\n            iteration_checkpoints.append(iteration + 1)\n\n            print(\"%d [D loss: %f, accuracy: %.2f%%] [G loss: %f]\" %\n                  (iteration + 1, d_loss, 100.0 * accuracy, g_loss))\n\n            # Generated image sample output\n            sample_images(x_train[idx],y_train[idx],generator)","metadata":{"execution":{"iopub.status.busy":"2022-01-19T14:28:56.120581Z","iopub.execute_input":"2022-01-19T14:28:56.120922Z","iopub.status.idle":"2022-01-19T14:28:56.140126Z","shell.execute_reply.started":"2022-01-19T14:28:56.120879Z","shell.execute_reply":"2022-01-19T14:28:56.139024Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"-------------------------------------------------------------------\n# Sampling and drawing images","metadata":{}},{"cell_type":"code","source":"image_grid_rows=4\nimage_grid_columns=4\n    \ndef plot_dim_reduction(imgs, gen_imgs, img_labels):\n    sns.set(style=\"ticks\", context=\"talk\",font_scale = 1)\n    plt.style.use(\"dark_background\")\n    markers=['o','v','^','<','>','8','s','P','*','X','+','*']    \n    plt.figure(figsize=(9,9))\n    reshape_real_imgs = imgs.reshape(imgs.shape[0], -1)\n    reshape_gen_imgs = gen_imgs.reshape(gen_imgs.shape[0], -1)*255\n    concat_imgs = np.concatenate([reshape_real_imgs,reshape_gen_imgs])\n    fake_label = np.empty((image_grid_rows * image_grid_columns))\n    fake_label[:] = 11\n    concat_label = np.concatenate([img_labels,fake_label])\n    mapper = umap.UMAP().fit_transform(concat_imgs)\n    for i,marker in enumerate(markers):\n        mask = concat_label == i\n        if i == 11:\n            plt.scatter(mapper[mask, 0], mapper[mask, 1], label=i, s=100, alpha=1,marker=marker, c ='yellow')\n        else:\n            plt.scatter(mapper[mask, 0], mapper[mask, 1], label=i, s=10, alpha=1,marker=marker)\n    plt.legend(bbox_to_anchor=(1.00, 1), loc='upper left',fontsize=15)     \n    plt.title(\"Plotting real and generated images after dimension reduction\",fontsize = 15)","metadata":{"execution":{"iopub.status.busy":"2022-01-19T14:28:56.143206Z","iopub.execute_input":"2022-01-19T14:28:56.144016Z","iopub.status.idle":"2022-01-19T14:28:56.160856Z","shell.execute_reply.started":"2022-01-19T14:28:56.143934Z","shell.execute_reply":"2022-01-19T14:28:56.158941Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_dist(gen_imgs):\n    sns.set(style=\"ticks\", context=\"talk\",font_scale = 1)\n    plt.style.use(\"dark_background\")\n    plt.figure(figsize=(10,6))\n    sns.distplot(gen_imgs,color='purple',kde=True)\n    plt.title(\"Generated Images Distribution\",fontsize = 20)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-19T14:28:56.162663Z","iopub.execute_input":"2022-01-19T14:28:56.163681Z","iopub.status.idle":"2022-01-19T14:28:56.175681Z","shell.execute_reply.started":"2022-01-19T14:28:56.163626Z","shell.execute_reply":"2022-01-19T14:28:56.174095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def sample_images(imgs, img_labels, generator):\n    # random noise sampling\n    z = np.random.normal(0, 1, (image_grid_rows * image_grid_columns, z_dim))\n    \n    # Generate Image from Random Noise\n    gen_imgs = generator.predict(z)  \n    \n    # Scale image pixel values between [0, 1]\n    gen_imgs = 0.5 * gen_imgs + 0.5\n\n    plot_dim_reduction(imgs, gen_imgs, img_labels)\n    plot_dist(gen_imgs)\n\n    fig, axs = plt.subplots(image_grid_rows,\n                            image_grid_columns,\n                            figsize=(11, 11),\n                            sharey=True,\n                            sharex=True)\n\n    cnt = 0\n    #plt.figure(figsize=(9,9))\n    for i in range(image_grid_rows):\n        for j in range(image_grid_columns):\n            axs[i, j].imshow(gen_imgs[cnt, :, :, 0], cmap='Greys_r')\n            axs[i, j].axis('off')\n            cnt += 1\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-19T14:29:50.374727Z","iopub.execute_input":"2022-01-19T14:29:50.375071Z","iopub.status.idle":"2022-01-19T14:29:50.384789Z","shell.execute_reply.started":"2022-01-19T14:29:50.375033Z","shell.execute_reply":"2022-01-19T14:29:50.383998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"--------------------------------------------------------------\n# Setting Hyperparameters","metadata":{}},{"cell_type":"code","source":"# Setting Hyperparameter\niterations = 3201\nbatch_size = 128\nsample_interval = 400","metadata":{"execution":{"iopub.status.busy":"2022-01-19T14:29:50.868469Z","iopub.execute_input":"2022-01-19T14:29:50.868907Z","iopub.status.idle":"2022-01-19T14:29:50.873544Z","shell.execute_reply.started":"2022-01-19T14:29:50.868872Z","shell.execute_reply":"2022-01-19T14:29:50.872573Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"------------------------------------------------------------------------------------\n# Training\n\n![](https://miro.medium.com/max/517/1*okBKjQcLOpeswTyZZfvZWA.png)\n\nPicture Credit: https://miro.medium.com\n\nNow, we will train using the designed GAN model. Since we do not know the distribution of the real dataset at first, we assume that the distribution of the real dataset is a normal distribution. The generator and discriminator of the GAN model will train in adversarial competition. That is, the generator will produce fakes that look like the real ones, and the discriminators will be trained to discriminate whether they are real or fake. Now let's visually see how the training is actually going.","metadata":{}},{"cell_type":"markdown","source":"To understand the operation of GAN, we would like to draw three plots as follows.\n* **UMAP plot**: After reducing the dimensionality of real images and generated images to two dimensions, I try drawing them in a specific iteration during training. Even though, it is a compression of the high dimension of $28 * 28$ into two dimensions, it seems sufficient to understand the operation of the GAN.\nAdditionally, GANs are unlabeled as Unsupervised Learning. Therefore, the label of the generated image was arbitrarily decided as 11, and it was expressed by a yellow star.\n* **Distribution of Generated Images**: It shows how GAN learns distributions of real images by changing the distribution of generated images.\n* **Generated images**: I drew the images generated by the generator.\n\n**Now, let's trust our model and go on a long journey, shall we?**","metadata":{}},{"cell_type":"code","source":"# DCGAN training for a specified number of iterations\ntrain(iterations, batch_size, sample_interval)","metadata":{"execution":{"iopub.status.busy":"2022-01-19T14:29:51.414449Z","iopub.execute_input":"2022-01-19T14:29:51.415103Z","iopub.status.idle":"2022-01-19T14:30:31.110059Z","shell.execute_reply.started":"2022-01-19T14:29:51.415056Z","shell.execute_reply":"2022-01-19T14:30:31.108345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"If you look at the above results, you can see the process that starts with a random image and gradually changes to handwritten numeric data. With more iterations you will get clearer handwriting.\n\n\nAlso, please pay attention to the change in the distribution of the generated image. It can be said that this is the core at which GAN works. In other words, it learns the distribution of real data from a random distribution and uses it to create a real fake.\n\n**Looking at the UMAP plot, it is very interesting. Our yellow stars are finding their place, as if they were in distant galaxies and returning to their original homes.**","metadata":{}},{"cell_type":"markdown","source":"---------------------------------------------------------------------------------\n# Checking Results","metadata":{}},{"cell_type":"code","source":"losses = np.array(losses)\n\n# Training loss graph of discriminant and generator\nplt.figure(figsize=(15, 5))\nplt.plot(iteration_checkpoints, losses.T[0], label=\"Discriminator loss\",linewidth=2)\nplt.plot(iteration_checkpoints, losses.T[1], label=\"Generator loss\",linewidth=2)\n\nplt.xticks(iteration_checkpoints, rotation=90)\n\nplt.title(\"Training Loss\",fontsize = 20)\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Loss\")\nplt.legend()","metadata":{"execution":{"iopub.status.busy":"2022-01-19T14:32:14.441267Z","iopub.execute_input":"2022-01-19T14:32:14.44155Z","iopub.status.idle":"2022-01-19T14:32:14.718066Z","shell.execute_reply.started":"2022-01-19T14:32:14.441521Z","shell.execute_reply":"2022-01-19T14:32:14.717038Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accuracies = np.array(accuracies)\n\n# Discriminator Accuracy Graph\nplt.figure(figsize=(15, 5))\nplt.plot(iteration_checkpoints, accuracies, label=\"Discriminator accuracy\",linewidth=2)\n\nplt.xticks(iteration_checkpoints, rotation=90)\nplt.yticks(range(0, 100, 10))\n\nplt.title(\"Discriminator Accuracy\",fontsize = 20)\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Accuracy (%)\")\nplt.legend()","metadata":{"execution":{"iopub.status.busy":"2022-01-19T14:32:42.799162Z","iopub.execute_input":"2022-01-19T14:32:42.799526Z","iopub.status.idle":"2022-01-19T14:32:43.019772Z","shell.execute_reply.started":"2022-01-19T14:32:42.799482Z","shell.execute_reply":"2022-01-19T14:32:43.018322Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"-----------------------------------------------\n# Conclusion\n\nMore training is needed to get better results. However, the purpose of this notebook is to understand how DCGAN creates plausible generated images from random noise images.\nIn addition, since training of GAN requires a lot of time, we will be satisfied with the results obtained now.","metadata":{}},{"cell_type":"markdown","source":"<hr style=\"border: solid 3px blue;\">","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}