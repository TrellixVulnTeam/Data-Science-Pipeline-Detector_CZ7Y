{"cells":[{"metadata":{"_uuid":"cab8763201dce7a236261e942039a47edb034ba6"},"cell_type":"markdown","source":"# Convolutional Neural Networks (CNNs / ConvNets)"},{"metadata":{"_uuid":"be116d779ea917b34a41980d08bcbf2b2fd0f544"},"cell_type":"markdown","source":"# Orhan SERTKAYA"},{"metadata":{"_uuid":"8d9afded365958d61260600601d026451569b28f"},"cell_type":"markdown","source":"Content:\n* [Introduction](#1):\n* [Loading the Data Set](#2):\n* [Normalization, Reshape and Label Encoding](#3):\n* [Train-Test Split](#4):\n* [Convolutional Neural Network(Implementing with Keras)](#5):\n* [Define Optimizer](#6):\n* [Compile Model](#7):\n* [Epochs and Batch Size](#8):\n* [Data Augmentation](#9):\n* [Fit the Model](#10):\n* [Evaluate the model](#11):\n* [Predict For Random Sample](#12):\n* [Wrong Predicted Digit Values](#13):\n* [Predict Test Data](#14):\n* [Conclusion](#15):"},{"metadata":{"_uuid":"69c6710c06d3931105e2509302734db2020e16b0"},"cell_type":"markdown","source":"<a id=\"1\"></a> <br>\n# INTRODUCTION\n* In this kernel, we will be working on Digit Recognizer Dataset (Implementing with Keras)."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# import warnings\nimport warnings\n# filter warnings\nwarnings.filterwarnings('ignore')\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"<a id=\"2\"></a> <br>\n## Loading the Data Set\n* In this part we load and visualize the data."},{"metadata":{"trusted":true,"_uuid":"0d9e28c614cb07c26943eeb4cd07d97afc5981d7"},"cell_type":"code","source":"# read train\ntrain = pd.read_csv(\"../input/train.csv\")\nprint(train.shape)\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3dbff7802da059b16aa4e9f54ef094ba2585899b"},"cell_type":"markdown","source":"* For example,let's look at first sample pixel values"},{"metadata":{"trusted":true,"_uuid":"13fffe4dd8c9168a5b4fe8565fd48e3d6489d2c4"},"cell_type":"code","source":"train.iloc[0].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7e477274bdcf20697dc7d7de409cd4b38c8cc375"},"cell_type":"code","source":"# read test\ntest = pd.read_csv(\"../input/test.csv\")\nprint(test.shape)\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1358269bc1a0d3563b2fd080e01ce1b97d5cdbba"},"cell_type":"code","source":"# put labels into y_train variable\nY_train = train[\"label\"]\n# Drop 'label' column\nX_train = train.drop(labels = [\"label\"], axis = 1)\nX_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"16fa8729dbf96420b3f6d37c0e954d18f3959de5"},"cell_type":"code","source":"# visualize number of digits classes\nplt.figure(figsize=(15,7))\ng = sns.countplot(Y_train, palette=\"icefire\")\nplt.title(\"Number of digit classes\")\nY_train.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"98911ac479a97ebf42cbb196e3a5dfda7d0404cb"},"cell_type":"code","source":"# plot some samples\n#as_matrix = Converting to matrix\nimg = X_train.iloc[0].as_matrix()\nimg = img.reshape((28,28))\nplt.imshow(img,cmap=\"gray\")\nplt.title(Y_train.iloc[0]) #or plt.title(train.iloc[0,0]) both of them are okay.\nplt.axis(\"off\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a8ccb7bf5af9f0c7a0524cd3121618a490b4d156"},"cell_type":"code","source":"# plot some samples\nimg = X_train.iloc[7].as_matrix()\nimg = img.reshape((28,28))\nplt.imshow(img,cmap='gray')\nplt.title(Y_train.iloc[7]) #or plt.title(train.iloc[7,0]) both of them are okay.\nplt.axis(\"off\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"52d15a777717b7906d80d35c88302c3c8a05542d"},"cell_type":"code","source":"# plot some samples\nimg = X_train.iloc[7223].as_matrix()\nimg = img.reshape((28,28))\nplt.imshow(img,cmap='gray')\nplt.title(Y_train.iloc[7223]) #or plt.title(train.iloc[7223,0]) both of them are okay.\nplt.axis(\"off\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"66cff55fb24161985085e07eee4ca71cd402fd9e"},"cell_type":"markdown","source":"<a id=\"3\"></a> <br>\n## Normalization, Reshape and Label Encoding \n* Normalization\n    * We perform a grayscale normalization to reduce the effect of illumination's differences.\n    * If we perform normalization, CNN works faster.\n* Reshape\n    * Train and test images (28 x 28) \n    * We reshape all data to 28x28x1 3D matrices.\n    * Keras needs an extra dimension in the end which correspond to channels. Our images are gray scaled so it use only one channel. \n* Label Encoding  \n    * Encode labels to one hot vectors \n        * 2 => [0,0,1,0,0,0,0,0,0,0]\n        * 4 => [0,0,0,0,1,0,0,0,0,0]"},{"metadata":{"trusted":true,"_uuid":"a17ff507830e02211b50cfb246248b24afee43b9"},"cell_type":"code","source":"# Normalize the data\nX_train = X_train / 255.0\ntest = test / 255.0\nprint(\"X_train shape: \",X_train.shape)\nprint(\"test shape: \",test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3fccd827cbc79cc830ba984e03fc3e17d0585b71"},"cell_type":"code","source":"# Reshape\nX_train = X_train.values.reshape(-1,28,28,1)\ntest = test.values.reshape(-1,28,28,1)\nprint(\"X_train shape: \",X_train.shape)\nprint(\"test shape: \",test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1fe7e2437090091bc5dbcd22ce71c8c4b4d3c5d5"},"cell_type":"code","source":"# Label Encoding \nfrom keras.utils.np_utils import to_categorical # convert to one-hot-encoding(one hot vectors)\nY_train = to_categorical(Y_train, num_classes = 10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"aae51a136370c137b9622acb1c597e7ca4696090"},"cell_type":"markdown","source":"<a id=\"4\"></a>\n## Train-Test Split\n* We split the data into train and test sets.\n* test size is 10%.\n* train size is 90%."},{"metadata":{"trusted":true,"_uuid":"3ad29b68741af3b806fcfa82884047792607cc62"},"cell_type":"code","source":"# Split the train and the validation set for the fitting\nfrom sklearn.model_selection import train_test_split\nx_train, x_val, y_train, y_val = train_test_split(X_train, Y_train, test_size = 0.1, random_state = 2)\nprint(\"x_train shape: \",x_train.shape)\nprint(\"x_val shape: \",x_val.shape)\nprint(\"y_train shape: \",y_train.shape)\nprint(\"y_val shape :\",y_val.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dd597c6b2085b6a7fd3a042a11d823a6315f1ecc"},"cell_type":"code","source":"# Some examples\nplt.imshow(x_train[4][:,:,0], cmap=\"gray\")\nplt.axis(\"off\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"77e50295585713612f4d60ddafa719dccb75f4b2"},"cell_type":"markdown","source":"<a id=\"5\"></a>\n## Convolutional Neural Network "},{"metadata":{"_uuid":"2b5cd9bbec4fcaa56d6e1c531292b69ce9edc0b3"},"cell_type":"markdown","source":"## Implementing with Keras"},{"metadata":{"trusted":true,"_uuid":"42c3f8843fe8cf95900268956c2b3930113d1683"},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.metrics import confusion_matrix\nimport itertools\n\nfrom keras.utils.np_utils import to_categorical # convert to one-hot-encoding\nfrom keras.models import Sequential # to create a cnn model\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, Activation\nfrom keras.optimizers import RMSprop,Adam\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ReduceLROnPlateau\n\nmodel = Sequential()\n\nmodel.add(Conv2D(filters = 32, kernel_size = (5,5), padding = 'Same', input_shape = (28,28,1)))\nmodel.add(Conv2D(filters = 32, kernel_size = (5,5), padding = 'Same'))\nmodel.add(BatchNormalization())\nmodel.add(Activation(\"relu\"))\nmodel.add(MaxPool2D(pool_size = (2,2)))\n\nmodel.add(Conv2D(filters = 64, kernel_size = (3,3), padding = 'Same'))\nmodel.add(Conv2D(filters = 64, kernel_size = (3,3), padding = 'Same'))\nmodel.add(BatchNormalization())\nmodel.add(Activation(\"relu\"))\nmodel.add(MaxPool2D(pool_size = (2,2)))\n\nmodel.add(Conv2D(filters = 128, kernel_size = (3,3), padding = 'Same'))\nmodel.add(Conv2D(filters = 128, kernel_size = (3,3), padding = 'Same'))\nmodel.add(BatchNormalization())\nmodel.add(Activation(\"relu\"))\nmodel.add(MaxPool2D(pool_size = (2,2)))\n\n# fully connected\nmodel.add(Flatten())\nmodel.add(Dense(256))\nmodel.add(BatchNormalization())\nmodel.add(Activation(\"relu\"))\nmodel.add(Dropout(0.25))\nmodel.add(Dense(10, activation = \"softmax\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"61e31621500dda85ffe64c56c5e59f4f4d466ee5","scrolled":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ca63bb735dccecb9209166fcbe358a5858f2aa25"},"cell_type":"markdown","source":"<a id=\"6\"></a>\n### Define Optimizer   \n* Adam optimizer: Change the learning rate"},{"metadata":{"trusted":true,"_uuid":"fc59a15ed90044131931d82ad7373ccc0e0d94b1"},"cell_type":"code","source":"# Define the optimizer\n#optimizer = Adam(lr = 0.001, beta_1 = 0.9, beta_2 = 0.999)\n\nfrom keras.optimizers import RMSprop,Adam,SGD,Adagrad,Adadelta,Adamax,Nadam\n#optimizer=SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)\noptimizer = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n#optimizer=Nadam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, schedule_decay=0.004)\n#optimizer=Adadelta(lr=0.001, rho=0.95, epsilon=0.1, decay=0.1)\n#optimizer=Adamax(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0)\n#optimizer = RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.0)\n\n#optimizer=Adagrad(lr=0.01, epsilon=None, decay=0.0)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f12854879b70ea4475b948c8afdcd3c565ff16e5"},"cell_type":"code","source":"# # Define the optimizer\n# optimizer = RMSprop(lr = 0.001, rho=0.9, epsilon=1e-08, decay=0.0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"acee3f2f18047bb55dccca968d5a3b1290484780"},"cell_type":"markdown","source":"<a id=\"7\"></a>\n### Compile Model\n* categorical crossentropy\n* We make binary cross entropy at previous parts and in machine learning tutorial\n* At this time we use categorical crossentropy. That means that we have multi class.\n* <a href=\"https://ibb.co/jm1bpp\"><img src=\"https://preview.ibb.co/nN3ZaU/cce.jpg\" alt=\"cce\" border=\"0\"></a>"},{"metadata":{"trusted":true,"_uuid":"00491764899ccd78ea4a894e0896b461518d2c73"},"cell_type":"code","source":"# Compile the model\nmodel.compile(optimizer = optimizer, loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"18a384a51b3baa749ee2fa1ee08df712356c8a72"},"cell_type":"code","source":"# Set a learning rate annealer\nlearning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', \n                                            patience=3, \n                                            verbose=1, \n                                            factor=0.5, \n                                            min_lr=0.00001)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"872ee63cc296838fe8d3d60e91b36915da61cc69"},"cell_type":"markdown","source":"<a id=\"8\"></a>\n### Epochs and Batch Size"},{"metadata":{"trusted":true,"_uuid":"2b4734f6e7093166f25184ec4b9a94b79ab6a6f0"},"cell_type":"code","source":"epochs = 25  # for better result increase the epochs\nbatch_size = 250","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"59773feb564a0ea03586a3d4d262df08c6dfaa0f"},"cell_type":"markdown","source":"<a id=\"9\"></a>\n### Data Augmentation"},{"metadata":{"trusted":true,"_uuid":"9c537fdfb7158364a96f8925439a9aafa77f361b"},"cell_type":"code","source":"# data augmentation\ndatagen = ImageDataGenerator(   \n        featurewise_center=False,  # set input mean to 0 over the dataset\n        samplewise_center=False,  # set each sample mean to 0\n        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n        samplewise_std_normalization=False,  # divide each input by its std\n        zca_whitening=False,  # dimesion reduction\n        rotation_range=10,  # randomly rotate images in the range 10 degrees\n        zoom_range = 0.1, # Randomly zoom image 10%\n        width_shift_range=0.1,  # randomly shift images horizontally 10%\n        height_shift_range=0.1,  # randomly shift images vertically 10%\n        horizontal_flip=False,  # randomly flip images\n        vertical_flip=False  # randomly flip images\n        )\ndatagen.fit(x_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b30c725c79a5681df42f274008703f5e800354c9"},"cell_type":"markdown","source":"<a id=\"10\"></a>\n### Fit the Model"},{"metadata":{"trusted":true,"_uuid":"d58df797fb2049f7ae94be50e4034eddb8b96cd1"},"cell_type":"code","source":"# # Fit the model\n# history = model.fit_generator(datagen.flow(x_train, y_train, batch_size = batch_size),\n#                    epochs = epochs, validation_data = (x_val,y_val), steps_per_epoch = x_train.shape[0] // batch_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3b4860c25b8903ef497010b2715118ac817df968"},"cell_type":"code","source":"history = model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n                              epochs=epochs, validation_data = (x_val, y_val),\n                              verbose = 2, steps_per_epoch=x_train.shape[0] // batch_size,\n                              callbacks=[learning_rate_reduction]) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"65a4acd1b1151498a4d42a47e1a642a805c953d6"},"cell_type":"markdown","source":"<a id=\"11\"></a>\n## Evaluate the model\n* Validation and Loss visualization\n* Confusion matrix"},{"metadata":{"trusted":true,"_uuid":"961c1834a19d86cd11c5891d8f0c7844226107b3"},"cell_type":"code","source":"# Plot the loss curve for training\nplt.plot(history.history['loss'], color='r', label=\"Train Loss\")\nplt.title(\"Train Loss\")\nplt.xlabel(\"Number of Epochs\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c44563bbaf9fa10ee05be3205f2a9d3089ed3fc0"},"cell_type":"code","source":"# Plot the accuracy curve for training\nplt.plot(history.history['acc'], color='g', label=\"Train Accuracy\")\nplt.title(\"Train Accuracy\")\nplt.xlabel(\"Number of Epochs\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"36d8673cd326d8d6e8769a89177fcd440cae3943"},"cell_type":"code","source":"# Plot the loss curve for validation \nplt.plot(history.history['val_loss'], color='r', label=\"Validation Loss\")\nplt.title(\"Validation Loss\")\nplt.xlabel(\"Number of Epochs\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"34cd0eaf5b397bd0e274cfd485a3c00ffb1a9aa6"},"cell_type":"code","source":"# Plot the accuracy curve for validation \nplt.plot(history.history['val_acc'], color='g', label=\"Validation Accuracy\")\nplt.title(\"Validation Accuracy\")\nplt.xlabel(\"Number of Epochs\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"77f8f162c16b9dd1069aaffb31e35ceb0765b90f"},"cell_type":"code","source":"print('Train accuracy of the model: ',history.history['acc'][-1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"022a5ccc88968931140a8fe476bf664e3843f95f"},"cell_type":"code","source":"print('Train loss of the model: ',history.history['loss'][-1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2b179ef94da2f93c34d4cc9518e9e83c8bcdfe32"},"cell_type":"code","source":"print('Validation accuracy of the model: ',history.history['val_acc'][-1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"929d0bc38505c3257e134481249a5d199cca22d1"},"cell_type":"code","source":"print('Validation loss of the model: ',history.history['val_loss'][-1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"12\"></a>\n### Predict For Random Sample"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(x_val.shape)\nplt.imshow(x_val[100].reshape(28,28),cmap=\"gray\")\nplt.axis(\"off\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trueY = y_val[100]\nimg = x_val[100]\ntest_img = img.reshape(1,28,28,1)\n\npreds = model.predict_classes(test_img)\nprob = model.predict_proba(test_img)\n\nprint(\"trueY: \",np.argmax(trueY))#show the max value in trueY values(trueY is one hot vector!)\nprint(\"Preds: \",preds)\nprint(\"Prob: \",prob)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"13\"></a>\n### Let's show the wrong predicted digit values"},{"metadata":{"trusted":true},"cell_type":"code","source":"d = {'pred': model.predict_classes(x_val), 'true': np.argmax(y_val,axis=1)} #axis=1!important!\ndf = pd.DataFrame(data=d)\n\n#looking at wrong predicted values(For 1! you can change it.)\narray1 = np.array(df[(df.pred != df.true) & (df.true==1)].index)\nprint(array1)\n\n# shows total mistakes\ndf2 = df[(df.pred != df.true)]\ndf2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (12,12))\n\nfor i in range(len(df2)):\n    plt.subplot(6, 4, i+1)\n    img = x_val[df2.index[i]]\n    img = img.reshape((28,28))\n    plt.imshow(img, cmap='gray')\n    plt.title(\"True Class: \" + str(df2[\"true\"].iloc[i])+\"    Pred Class: \" + str(df2[\"pred\"].iloc[i]))\n    plt.axis('off')\n    \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* **As you can see, some numbers are hard to understand!**"},{"metadata":{"trusted":true,"_uuid":"7fc1901a103cb3e2665e5b598646cbb254eb59ff","_kg_hide-input":false},"cell_type":"code","source":"# confusion matrix\nimport seaborn as sns\n# Predict the values from the validation dataset\nY_pred = model.predict(x_val)\n# Convert predictions classes to one hot vectors \nY_pred_classes = np.argmax(Y_pred,axis = 1) \n# Convert validation observations to one hot vectors\nY_true = np.argmax(y_val,axis = 1) \n# compute the confusion matrix\nconfusion_mtx = confusion_matrix(Y_true, Y_pred_classes) \n# plot the confusion matrix\nf,ax = plt.subplots(figsize=(12, 12))\nsns.heatmap(confusion_mtx, annot=True, linewidths=0.01,cmap = \"gist_yarg_r\", linecolor=\"black\", fmt= '.1f',ax=ax)\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"True Label\")\nplt.title(\"Confusion Matrix\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(len(confusion_mtx)):\n    print(\"Class:\",str(i))\n    print(\"Number of Wrong Prediction:\", str(sum(confusion_mtx[i])-confusion_mtx[i][i]), \"out of \"+str(sum(confusion_mtx[i])))\n    print(\"Percentage of True Prediction: {:.2f}%\".format(confusion_mtx[i][i] / (sum(confusion_mtx[i])/100) ))\n    print(\"***********************************************************\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e0fbd2faa6c0f799ec7df00a881be5618f35a27b"},"cell_type":"markdown","source":"<a id=\"14\"></a>\n## PREDICT TEST DATA"},{"metadata":{"trusted":true,"_uuid":"a2b430eed5bed78d6613a82f92ef591848081414"},"cell_type":"code","source":"test = pd.read_csv('../input/test.csv')\ntest = test.values.reshape(-1,28,28,1)\ntest.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8236fdd00fb101b4e6d27a7d7efee3728a561dda"},"cell_type":"code","source":"# predict results\nresults = model.predict(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"468f0c23b3dd2f8b99a77586aa766170004a837e"},"cell_type":"code","source":"# select the index with the maximum probability\nresults = np.argmax(results,axis = 1)\n\nresults = pd.Series(results,name=\"Label\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"35c07821e8a5ef9ec02f3a890beed792e95b468c"},"cell_type":"code","source":"submission = pd.concat([pd.Series(range(1,len(test)+1),name = \"ImageId\"),results],axis = 1)\n\nsubmission.to_csv(\"Digit_Recognizer_CNN_Result_2.csv\",index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0f08637bac677d86325398d8db221a4d80cb5060"},"cell_type":"code","source":"submission.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ef4ddfb0963bc2a32f30f911018f42c27aa24afa"},"cell_type":"markdown","source":"<a id=\"15\"></a>\n# Conclusion\n* If you like it, please upvote.\n* If you have any question, I will be appreciate to hear it."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}