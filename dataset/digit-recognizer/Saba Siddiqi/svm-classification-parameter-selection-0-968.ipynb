{"cells":[{"metadata":{"_uuid":"ff39e8888a0e9c6873925cd143d2f00ff36c8a63"},"cell_type":"markdown","source":"# Classification (MultiClass) using SVM & Optimal Parameter Selection"},{"metadata":{"_uuid":"06b1aae4fd8fd4fca77782f9c7b37669590c9a38"},"cell_type":"markdown","source":"This notebook is intended for beginners to provide them a guideline for where to start using Simple Classifier  **SVM**. \n\n**Level :** Beginner <br>\n\n**Task :** To classify digits from 0 to 9 using [MNIST Handdrawn Digits Image Datatset](https://www.kaggle.com/c/digit-recognizer).\n\n**About Data: **\nData contains images of hand-drawn digits, from zero through nine. \n\nImage Details : Gray Scale , 28x28 pixel ,pixel value - 0(lightest) to 255(darkest)\n\n### Notebook Content \n1. [Exploring and Preparing Data](#explore)\n2. [Support Vector Machine (SVM)](#svm)\n3. [Classifcation Using SVM](#clf_svm) \n *    [ Case1 - GrayScale Images](#case1)\n *    [Case2 - Binary Images](#case2)\n *    [Case3 - GrayScale + Reduced Dimensions(using PCA)](#case3)\n *    [Case4 - Binary + Reduced Dimensions(using PCA)](#case4)\n *    [Comparison of Four Cases](#cases) \n4.  [Training Data Size Vs Accuracy, Score & Fitting Time](#compare)\n5. [Parameter Selection for SVM Using GridSearchCV](#param)\n6. [Submission](#sub)\n"},{"metadata":{"_uuid":"c22b325f8725618f30d7bbf0e03b305c10f88c33"},"cell_type":"markdown","source":"Importing required libraries"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt, matplotlib.image as mpimg\nimport time \nimport warnings\nfrom sklearn import svm\nfrom sklearn.model_selection import GridSearchCV\n\n%matplotlib inline\nwarnings.filterwarnings('ignore')\n\nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"547be527a3369f1bd7e3859c4c4fe8ee64ddf65a"},"cell_type":"markdown","source":"  ## <a id=\"explore\">1. Exploring and Preparing Data</a>\n<br>\nReading data from \"train.csv\", which will later be divided into train(to train model) and test(to check accuracy).  "},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"data = pd.read_csv('../input/train.csv')\nprint(\"Train Data Shape is: \",data.shape)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"89ffcc977d62df960f8ef1273348baf9127856e0"},"cell_type":"markdown","source":"Extracting label from data"},{"metadata":{"trusted":true,"_uuid":"7028d6cb747772f8bbc74d1f6e08d22a675bf6b6"},"cell_type":"code","source":"label = data.label\ndata=data.drop('label',axis=1)\nprint(\"Data Shape: \",data.shape)\nprint(\"Label Shape: \",label.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3f62c246fde233f44face2d54ef9db189b9d102c"},"cell_type":"code","source":"data.columns","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"221b4798ee0510404479eb64121dd28474ed3fe8"},"cell_type":"markdown","source":"Converting 1D array to 2D 28x28 array using [**reshape**](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.reshape.html) , to plot and view grayscale images. "},{"metadata":{"trusted":true,"_uuid":"8649eda8660fba55e68655ed998acde377f05365"},"cell_type":"code","source":"for x in range(0,4):\n    train_0=data[label==x]\n    data_new=[]\n    for idx in train_0.index:\n        val=train_0.loc[idx].values.reshape(28,28)\n        data_new.append(val)\n    plt.figure(figsize=(25,25))   \n    for x in range(1,5):\n        ax1=plt.subplot(1, 20, x)\n        ax1.imshow(data_new[x],cmap='gray')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ea847f9c61876df1bfcac0f3212233c09675ca2e"},"cell_type":"markdown","source":"Having a look at pixel values frequency (0 to 255)<br>\nTo get a better idea, lets convert the plot to Log Scale.<br>\nBased on leading 0s and 255s we can try converting it to binary in the later steps to simplify the problem."},{"metadata":{"trusted":true,"_uuid":"d6d5f97c4be2c1f57796cd10a6d074c3627875aa","scrolled":false},"cell_type":"code","source":"y = pd.value_counts(data.values.ravel()).sort_index()\nN = len(y)\nx = range(N)\nwidth =0.9\nplt.figure(figsize=[20,8])\nax1=plt.subplot(1, 2,1)\nax1.bar(x, y, width, color=\"blue\")\nplt.title('Pixel Value Frequency For Gray Scale Images')\nplt.xlabel('Pixel Value (0-255)')\nplt.ylabel('Frequency')\n#ax1.imshow(data_new[x],cmap='gray')\nax2=plt.subplot(1, 2,2)\nax2.bar(x, y, width, color=\"blue\")\nplt.title('Pixel Value Frequency (Log Scale)')\nplt.yscale('log')\nplt.xlabel('Pixel Value (0-255)')\nplt.ylabel('Frequency')\n#plt.yscale('')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d91f0c5fff90fff0665e86ebf65dfb70f6733030"},"cell_type":"markdown","source":"### Splitting data into Train and Test Data and Labels\n<br>\n\nDividing Data randomly into **train** and **test**. Out of total **train.csv** Data, 80% is kept as train for training the model and 20% is kept as test to score the model. "},{"metadata":{"trusted":true,"_uuid":"8ae423aef72c9195efe9f57c449859fbf6ad1e7c"},"cell_type":"code","source":"train, test,train_labels, test_labels = train_test_split(data, label, train_size=0.8, random_state=42)\nprint(\"Train Data Shape: \",train.shape)\nprint(\"Train Label Shape: \",train_labels.shape)\nprint(\"Test Data Shape: \",test.shape)\nprint(\"Test Label Shape: \",test_labels.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0e15da89eec84fe2807990275b68400781d7dc6d"},"cell_type":"markdown","source":"## <a id=\"svm\"> 2. Support Vector Machine (SVM)<a>\n\n> *<br>Support Vector Machine (SVM) is a supervised machine learning algorithm which can be used for both classification or regression problems.\n> It performs classification by finding the hyperplane that maximizes the margin between the two classes.*\n\nTo understand better, lets look at the following cases:<br>\n\n**CASE 1**: <br> \nHere, we have three hyper-planes (A, B and C). Now, identify the right hyper-plane to classify star and circle<br>\nAs per SVM: “Select the hyper-plane which segregates the two classes better”.<br>\nHere better means which keeps almost equal margin/distance from both classes. <br>\n**Answer : B**\n\n![scenario1](https://github.com/SabaSiddiqi/Backup/blob/master/svm_scenario1.png?raw=true)\n\n\n**CASE 2: **<br>\nHere, we have three hyper-planes (A, B and C). To identify the right hyper-plane to classify star and circle,\nMaximizing the distances between nearest data point (either class) and hyper-plane will help us to decide the right hyper-plane. \nThis distance is called as Margin\n**Answer : C**\n![margin](https://github.com/SabaSiddiqi/Backup/blob/master/margin.png?raw=true)\n\n"},{"metadata":{"_uuid":"00cfc6bd5a7944a01a6fcec527669547b1a074c8"},"cell_type":"markdown","source":"## <a id=\"clf_svm\">3. Classification using SVM<a>"},{"metadata":{"_uuid":"5561524ac22defb2c1e7569189e58f493f7da6dc"},"cell_type":"markdown","source":"Using [SVM Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html) from sklearn library.<br>\nWe have 33600 training samples, fitting them is going to take alot of time. To keep it simple for now, lets select 5000 out of them.<br>\nYou can change the value i to use the desired number of samples."},{"metadata":{"trusted":true,"_uuid":"a4f19ecf1bbe50355ccad04d5b598f229673dc19"},"cell_type":"code","source":"i=5000;\nscore=[]\nfittime=[]\nscoretime=[]\nclf = svm.SVC(random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7ca5d404393ced9c647d0311d101809a18f19fb0"},"cell_type":"markdown","source":"We are not passing parameters in this step to keep it simple and will be using the default ones."},{"metadata":{"trusted":true,"_uuid":"8504c14b53acbf6ea693ef9d935201f21802e7f2"},"cell_type":"code","source":"print(\"Default Parameters are: \\n\",clf.get_params)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3111c4c82e727d0874a18773ab67bdafc4b16e0d"},"cell_type":"markdown","source":"### <a id=\"case1\">Case 1 - Gray Scale Images</a>\n"},{"metadata":{"_uuid":"ee8d91d27c526120a120237a6242e9e34e3adaf5"},"cell_type":"markdown","source":"Fitting train data and finding a score for test data to check model performance"},{"metadata":{"trusted":true,"_uuid":"69ba7a8b8a30292b1fc95f7bee67f0b81e5954ee"},"cell_type":"code","source":"start_time = time.time()\nclf.fit(train[:i], train_labels[:i].values.ravel())\nfittime = time.time() - start_time\nprint(\"Time consumed to fit model: \",time.strftime(\"%H:%M:%S\", time.gmtime(fittime)))\nstart_time = time.time()\nscore=clf.score(test,test_labels)\nprint(\"Accuracy for grayscale: \",score)\nscoretime = time.time() - start_time\nprint(\"Time consumed to score: \",time.strftime(\"%H:%M:%S\", time.gmtime(scoretime)))\ncase1=[score,fittime,scoretime]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"706bc87d757ef8c06a8f571b810102c51eed7306"},"cell_type":"markdown","source":"### <a id=\"case2\">Case 2 - Binary Images</a>\n<br>\nTo simply the problem, converting images to black and white from gray scale by replacing all values > 0 to 1. \n<br>And Converting 1D array to 2D 28x28 array using [**reshape**](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.reshape.html) , to plot and view **binary** images. "},{"metadata":{"trusted":true,"_uuid":"ed59a97b33f44c26c766ab255992790d81ee1e32"},"cell_type":"code","source":"test_b=test\ntrain_b=train\ntest_b[test_b>0]=1\ntrain_b[train_b>0]=1\nfor x in range(0,4):\n    train_0=train_b[train_labels==x]\n    data_new=[]\n    for idx in train_0.index:\n        val=train_0.loc[idx].values.reshape(28,28)\n        data_new.append(val)\n    plt.figure(figsize=(25,25))   \n    for x in range(1,5):\n        ax1=plt.subplot(1, 20, x)\n        ax1.imshow(data_new[x],cmap='binary')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"89bda3c31aca042b674eb1798d24972a9de0e167"},"cell_type":"code","source":"start_time = time.time()\nclf.fit(train_b[:i], train_labels[:i].values.ravel())\nfititme = time.time() - start_time\nprint(\"Time consumed to fit model: \",time.strftime(\"%H:%M:%S\", time.gmtime(fittime)))\nscore=clf.score(test_b,test_labels)\nstart_time = time.time()\nclf.fit(train_b[:i], train_labels[:i].values.ravel())\nprint(\"Accuracy for binary: \",score)\nscoretime = time.time() - start_time\nprint(\"Time consumed to score: \",time.strftime(\"%H:%M:%S\", time.gmtime(scoretime)))\ncase2=[score,fittime,scoretime]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1171433edf950ae881602647476a7da0e0c064cf"},"cell_type":"markdown","source":"It can be seen that for the chosen training samples, accuracy of Case 2 (~90%) is way higher than Case 1 (10%)."},{"metadata":{"_uuid":"75da213819cf451e23c4794883bf60c7c605049f"},"cell_type":"markdown","source":"However, the high dimensionaly of data is making computational time high. Lets reduce the dimensions using PCA (Principal Component Analysis)\n\n### <a id=\"case3\">Case 3 - GrayScale + Dimensionality Reduction - PCA</a> <br>\n\n It is a linear transformation technique used to identify strong patterns in data by finding out variable correlation. It maps the data to a lower dimensional subspace in a way that data variance is maximized while retaining most of the information.\n \n To understand how PCA works, this tutorial may help - [Principal Component Analysis Explained](https://www.kaggle.com/sabasiddiqi/principal-component-analysis-explained)\n\nWe are using [sklearnPCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) library here to perform PCA Dimensionality Reduction.\n\nHere, Data is standardized and PCA is performed on data with all the components. Then variance is plotted for all components to decide which components to remove. "},{"metadata":{"trusted":true,"_uuid":"8b5a683aa116f9fef6080922a311b9b33c95a72c"},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA as sklearnPCA\n\n#standardized data\nsc = StandardScaler().fit(train)\nX_std_train = sc.transform(train)\nX_std_test = sc.transform(test)\n\n#If n_components is not set then all components are stored \nsklearn_pca = sklearnPCA().fit(X_std_train)\ntrain_pca = sklearn_pca.transform(X_std_train)\ntest_pca = sklearn_pca.transform(X_std_test)\n\n#Percentage of variance explained by each of the selected components.\n#If n_components is not set then all components are stored and the sum of the ratios is equal to 1.0.\nvar_per = sklearn_pca.explained_variance_ratio_\ncum_var_per = sklearn_pca.explained_variance_ratio_.cumsum()\n\nplt.figure(figsize=(30,10))\nind = np.arange(len(var_per)) \nplt.bar(ind,var_per)\nplt.xlabel('n_components')\nplt.ylabel('Variance')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"942a0a27077b16381a6150444488fb7ab6795de6"},"cell_type":"markdown","source":"Keeping 90% of information by choosing components falling within 0.90 cumulative."},{"metadata":{"trusted":true,"_uuid":"689a6957bd4285ea52a6f886897a1e60e84470d3"},"cell_type":"code","source":"n_comp=len(cum_var_per[cum_var_per <= 0.90])\nprint(\"Keeping 90% Info with \",n_comp,\" components\")\nsklearn_pca = sklearnPCA(n_components=n_comp)\ntrain_pca = sklearn_pca.fit_transform(X_std_train)\ntest_pca = sklearn_pca.transform(X_std_test)\nprint(\"Shape before PCA for Train: \",X_std_train.shape)\nprint(\"Shape after PCA for Train: \",train_pca.shape)\nprint(\"Shape before PCA for Test: \",X_std_test.shape)\nprint(\"Shape after PCA for Test: \",test_pca.shape)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2856d55886ff926c977bfd9edc8b5bd701c4486a"},"cell_type":"markdown","source":"Lets find the score using reduced dimensions keeping the same amount of samples, to compare accuracy."},{"metadata":{"trusted":true,"_uuid":"83850b8d7414c50ecf51a0694ec66b8d132b4427"},"cell_type":"code","source":"start_time = time.time()\nclf.fit(train_pca[:i], train_labels[:i].values.ravel())\nfittime = time.time() - start_time\nprint(\"Time consumed to fit model: \",time.strftime(\"%H:%M:%S\", time.gmtime(fittime)))\nstart_time = time.time()\nscore=clf.score(test_pca,test_labels)\nprint(\"Accuracy for grayscale: \",score)\nscoretime = time.time() - start_time\nprint(\"Time consumed to score model: \",time.strftime(\"%H:%M:%S\", time.gmtime(scoretime)))\ncase3=[score,fittime,scoretime]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"115c2156d80de117f13498311f9d398eeb42f345"},"cell_type":"markdown","source":"It can be seen that for the chosen training samples, accuracy of Case1 after PCA is (~91%) which was previously (10%)."},{"metadata":{"_uuid":"7ed8b4e2bde47c868f516c3c3a31177d421bd052"},"cell_type":"markdown","source":"### <a id=\"case4\">Case 4 - Binary + Dimensionality Reduction - PCA</a> <br> \n<br>\nUsing the steps steps we used in Case 3."},{"metadata":{"trusted":true,"_uuid":"bb88942452fe5b274fc8d10c8a0bfb298a326018"},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA as sklearnPCA\n\n#standardized data\nsc = StandardScaler().fit(train_b)\nX_std_train = sc.transform(train_b)\nX_std_test = sc.transform(test_b)\n\n#If n_components is not set then all components are stored \nsklearn_pca = sklearnPCA().fit(X_std_train)\n#train_pca_b = sklearn_pca.transform(X_std_train)\n#test_pca_b = sklearn_pca.transform(X_std_test)\n\n#Percentage of variance explained by each of the selected components.\n#If n_components is not set then all components are stored and the sum of the ratios is equal to 1.0.\nvar_per = sklearn_pca.explained_variance_ratio_\ncum_var_per = sklearn_pca.explained_variance_ratio_.cumsum()\n\nplt.figure(figsize=(30,10))\nind = np.arange(len(var_per)) \nplt.bar(ind,var_per)\nplt.xlabel('n_components')\nplt.ylabel('Variance')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2cfaf6af596614b5d2e1fa29a94d35a294637d43"},"cell_type":"code","source":"n_comp=len(cum_var_per[cum_var_per <= 0.90])\nprint(\"Keeping 90% Info with \",n_comp,\" components\")\nsklearn_pca = sklearnPCA(n_components=n_comp)\ntrain_pca_b = sklearn_pca.fit_transform(X_std_train)\ntest_pca_b = sklearn_pca.transform(X_std_test)\nprint(\"Shape before PCA for Train: \",X_std_train.shape)\nprint(\"Shape after PCA for Train: \",train_pca_b.shape)\nprint(\"Shape before PCA for Test: \",X_std_test.shape)\nprint(\"Shape after PCA for Test: \",test_pca_b.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"61e09e1d8523959857f75b553029a3be9927710c"},"cell_type":"code","source":"start_time = time.time()\nclf.fit(train_pca_b[:i], train_labels[:i].values.ravel())\nfittime = time.time() - start_time\nprint(\"Time consumed to fit model: \",time.strftime(\"%H:%M:%S\", time.gmtime(fittime)))\nstart_time = time.time()\nscore=clf.score(test_pca_b,test_labels)\nprint(\"Accuracy for grayscale: \",score)\nscoretime = time.time() - start_time\nprint(\"Time consumed to score model: \",time.strftime(\"%H:%M:%S\", time.gmtime(scoretime)))\ncase4=[score,fittime,scoretime]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0fdb69535af0e98e1cbbde92dba5a9fbfad8d2c6"},"cell_type":"markdown","source":"### <a id=\"cases\">Comparison of 4 Cases</a>"},{"metadata":{"trusted":true,"_uuid":"155dc8965c12633bbd50cd76f430b664ca9166f1"},"cell_type":"code","source":"head =[\"Accuracy\",\"FittingTime\",\"ScoringTime\"]\nprint(\"\\t\\t case1 \\t\\t\\t case2 \\t\\t\\t case3 \\t\\t\\t case4\")\nfor h, c1, c2, c3, c4 in zip(head, case1, case2, case3, case4):\n    print(\"{}\\t{}\\t{}\\t{}\\t{}\".format(h, c1, c2, c3, c4))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cee49d831fa188b89e5608b10edf27efa8548df9"},"cell_type":"markdown","source":"**Observations:**\n*  By simplifying the problem in Case 2 (by converting images to binary), accuracy increases from ~10% to ~91% for the chosen number of samples.\n* By reducing dimensions in Case3 and Case4, Fitting Times Reduces Drastically from ~43sec to ~6sec for the chosen number of samples."},{"metadata":{"_uuid":"e988066a8fb67a0ffff18d3b7e5c76d93a8c20ee"},"cell_type":"markdown","source":" ## <a id=\"sizevsacc\">4. Training Data Size Vs Accuracy , Fitting & Score Times </a>\n\n\n<br>Understanding how training data size affects accuracy,"},{"metadata":{"trusted":true,"_uuid":"39eb60f9a4d27193ec9683f32fa235d65983fab3"},"cell_type":"code","source":"from tqdm import tqdm\n\nfit_time=[]\nscore=[]\nscore_time=[]\nfor j in tqdm(range(1000,31000,5000)):\n    start_time = time.time()\n    clf.fit(train_pca_b[:j], train_labels[:j].values.ravel())\n    fit_time.append(time.time() - start_time)\n    start_time = time.time()\n    score.append(clf.score(test_pca_b,test_labels))\n    score_time.append(time.time() - start_time)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b84c0f9dbd1ec565ec8ba52bad053b32fddbc510"},"cell_type":"code","source":"x=list(range(1000,31000,5000))\nplt.figure(figsize=[20,5]);\nax1=plt.subplot(1, 2,1)\nax1.plot(x,score,'-o');\nplt.xlabel('Number of Training Samples')\nplt.ylabel('Accuray')\nax2=plt.subplot(1, 2,2)\nax2.plot(x,score_time,'-o');\nax2.plot(x,fit_time,'-o');\nplt.xlabel('Number of Training Samples')\nplt.ylabel('Time to Compute Score/Fit (sec)')\nplt.legend(['score_time','fitting_time'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"625abebd124da7f270366ce14803849ae132babd"},"cell_type":"markdown","source":"## <a id=\"param\"> 5. Parameter Selection for SVM using GridSearchCV</a>"},{"metadata":{"_uuid":"be7a408d0ce7619baeda78be1f7076cb234fc85d"},"cell_type":"markdown","source":"Out of parameters below, we will be playing with **Gamma** and **C**, where<br> \n    <br>[Gamma](https://www.quora.com/What-are-C-and-gamma-with-regards-to-a-support-vector-machine) is the parameter of a Gaussian Kernel (to handle non-linear classification)<br>\n    and **C** the parameter for the soft margin cost function, also known as cost of misclassification. A large C gives you low bias and high variance and vice versa."},{"metadata":{"trusted":true,"_uuid":"7d159826b5900fa4c595e4636eefd907c5ccf7dd"},"cell_type":"code","source":"clf.get_params","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"36981340bddf18404df356644cdc1ffc66c864a9"},"cell_type":"markdown","source":"To find optimal combination of parameters to achieve maximum accuracy ,using **GridSearchCV** from **sklearn** library. [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) does exhaustive search over specified parameter values for an estimator. <br>\nStoring values of parameters to be passed to GridSearch in **parameters**, keeping cross-validation folds as **3** and passing SVM as estimator. "},{"metadata":{"trusted":true,"_uuid":"c94a4d6b8f4cf8b890e92a4d8174b74064a4bb1a"},"cell_type":"code","source":"parameters = {'gamma': [1, 0.1, 0.01, 0.001],\n             'C': [1000, 100, 10, 1]} \n\np = GridSearchCV(clf , param_grid=parameters, cv=3)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"df4787a31c0c1b73c8ad69bef0627ce4bd960a4b"},"cell_type":"markdown","source":"For Case 4 (Binary Images and PCA Reduction):"},{"metadata":{"trusted":true,"_uuid":"b8a719c451cbe4657db8729a955b589af5b516fa"},"cell_type":"code","source":"X=train_pca_b[:i]\ny=train_labels[:i].values.ravel()\nstart_time = time.time()\np.fit(X,y)\nelapsed_time = time.time() - start_time\nprint(\"Time consumed to fit model: \",time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2529088018fc6d4092f31c58c60fae5b4943dc06"},"cell_type":"code","source":"print(\"Scores for all Parameter Combination: \\n\",p.cv_results_['mean_test_score'])\nprint(\"\\nOptimal C and Gamma Combination: \",p.best_params_)\nprint(\"\\nMaximum Accuracy acheieved on LeftOut Data: \",p.best_score_)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ee4085fb36b5182f21dafbdee5b209107afeff3d"},"cell_type":"markdown","source":"To verify, lets pass the optimal parameters to Classifier and check the score."},{"metadata":{"trusted":true,"_uuid":"5d11a80f9ab2472229ba1fe4c672c68a18fcb34e"},"cell_type":"code","source":"C=p.best_params_['C']\ngamma=p.best_params_['gamma']\nclf=svm.SVC(C=C,gamma=gamma, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"67c5707a3396b388468b3a64f502a0a6ac620913"},"cell_type":"code","source":"start_time = time.time()\nclf.fit(train_pca_b[:i], train_labels[:i].values.ravel())\nelapsed_time = time.time() - start_time\nprint(\"Time consumed to fit model: \",time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time)))\nprint(\"Accuracy for binary: \",clf.score(test_pca_b,test_labels))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3b606d288efccd01639468da960149c021c64723"},"cell_type":"markdown","source":"It can be seen that for the chosen training samples, accuracy of Case 2 has increased from (~91%) to (94%) for the optimal parameters.<br>\nNow using all training samples"},{"metadata":{"trusted":true,"_uuid":"4044cb747bc9131eea61d04e8296717d407117a9"},"cell_type":"code","source":"start_time = time.time()\nclf.fit(train_pca_b, train_labels.values.ravel())\nelapsed_time = time.time() - start_time\nprint(\"Time consumed to fit model: \",time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time)))\nprint(\"Accuracy for binary: \",clf.score(test_pca_b,test_labels))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4995add9ebc3164098018052985ae5b558760ae9"},"cell_type":"markdown","source":"## <a id=\"sub\"> 6. Submission</a>"},{"metadata":{"_uuid":"75428a358a56a81fcda361c0875f15a6b5363332"},"cell_type":"markdown","source":"Using all data from train and test files for Submission.\nFollowing Case4 for submission, which involves conversion of images to binary followed by dimensionality reduction."},{"metadata":{"trusted":true,"_uuid":"95d861a010f9b3e8e713d7c83e17e24c0a2b136f"},"cell_type":"code","source":"train_data = data #read and label removed in initial steps\ntrain_label=label\ntest_data = pd.read_csv('../input/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"be7a342a362b43ffd6515fed48619f7e46354f31"},"cell_type":"markdown","source":"Converting Images to binary"},{"metadata":{"trusted":true,"_uuid":"cbd1ce2359d95dfb3a28f52f5c7b7628fd58c7da"},"cell_type":"code","source":"train_data[train_data>0]=1\ntest_data[test_data>0]=1","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e3bd0ee65d7f6d0f6b237b32d9350a2e3a62da7c"},"cell_type":"markdown","source":"Reducing Dimensions using PCA"},{"metadata":{"trusted":true,"_uuid":"002034f0cbb3a180a2d945f908e95fa6e4951aca"},"cell_type":"code","source":"#standardize data\nsc = StandardScaler().fit(train_data)\nX_std_train = sc.transform(train_data)\nX_std_test = sc.transform(test_data)\n\n#If n_components is not set then all components are stored \nsklearn_pca = sklearnPCA().fit(X_std_train)\n\n#Percentage of variance explained by each of the selected components.\n#If n_components is not set then all components are stored and the sum of the ratios is equal to 1.0.\nvar_per = sklearn_pca.explained_variance_ratio_\ncum_var_per = sklearn_pca.explained_variance_ratio_.cumsum()\n\nplt.figure(figsize=(30,10))\nind = np.arange(len(var_per)) \nplt.bar(ind,var_per)\nplt.xlabel('n_components')\nplt.ylabel('Variance')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1233fd8e79e1bdc621b9df191840819b281c0efc"},"cell_type":"code","source":"n_comp=len(cum_var_per[cum_var_per <= 0.90])\nprint(\"Keeping 90% Info with \",n_comp,\" components\")\nsklearn_pca = sklearnPCA(n_components=n_comp)\ntrain_pca_b = sklearn_pca.fit_transform(X_std_train)\ntest_pca_b = sklearn_pca.transform(X_std_test)\nprint(\"Shape before PCA for Train: \",X_std_train.shape)\nprint(\"Shape after PCA for Train: \",train_pca_b.shape)\nprint(\"Shape before PCA for Test: \",X_std_test.shape)\nprint(\"Shape after PCA for Test: \",test_pca_b.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f4030dc7a5183b452ab4aa85078249e25da8c442"},"cell_type":"markdown","source":"Fitting Training Data"},{"metadata":{"trusted":true,"_uuid":"3fa29b61721cf7a8ebd1bcc13fe0a9459afcaba5"},"cell_type":"code","source":"start_time = time.time()\nclf.fit(train_pca_b, label.values.ravel())\nfittime = time.time() - start_time\nprint(\"Time consumed to fit model: \",time.strftime(\"%H:%M:%S\", time.gmtime(fittime)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5a54342c26f4161e068df5f0609bb7142651fd94"},"cell_type":"markdown","source":"Predicting Test Data"},{"metadata":{"trusted":true,"_uuid":"df30621173bfd0b75f891e5bff41240d023cc64c"},"cell_type":"code","source":"start_time = time.time()\nresult=clf.predict(test_pca_b)\nprint(\"Accuracy for Binary(PCA): \",result)\nelapsed_time = time.time() - start_time\nprint(\"Time consumed to predict: \",time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"104176ac54d7383ad88a45e45d8744e8f416247a"},"cell_type":"markdown","source":"Saving result to Output File for submission"},{"metadata":{"trusted":true,"_uuid":"552c34aded6bcc262fef357bde08295079774fa7"},"cell_type":"code","source":"data_to_submit = pd.DataFrame({\n    'ImageId':test_data.index.values+1,\n    'Label':result\n})\ndata_to_submit.index=data_to_submit['ImageId'].values\n\ndata_to_submit.to_csv('result.csv', index = False)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b1fbe289b33fbb23bda24360ab1ae265b326be95"},"cell_type":"markdown","source":"I'm writng these notebooks as a part of my learning and to give back to community, would love to hear if I have missed something, or if you have any suggestions. Thanks. "}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}