{"cells":[{"metadata":{},"cell_type":"markdown","source":"## For the interested reader:\n\nEven though the current script is public, there is still much more work needed to fully complete it. I am personally aiming to add visualization to better understand autoencoders, and I want to add well developed summaries after each implemeneted autoencoder section. Furthermore, I aim to also go in more detail for latent space exploration on the denoising deep autoencoder, and I still need to start a walkthrough for the variation autoencoder. That being said, I did finish writing the script for a simple fully connected autoencoder and deep autoencoder along with a denoising deep autoencoder. Lastly, please leave comments on how should I add more information and context to improve this comprehensive walkthrough on autoencoders. ","execution_count":null},{"metadata":{"_uuid":"360d34ec-1008-438b-91df-3f12086f074e","_cell_guid":"b343aeec-8948-4f95-ba03-a250a7092234","trusted":true},"cell_type":"markdown","source":"# How do we implement Autoencoders? \n\n\nAutoencoders are unique because they are considered generative neural networks, and each autoencoder contains a latent space which potentially learns relevant features either in a lower or high representation than the input data. For our walkthrough, we will always focus on latent spaces which are lower dimensional than the input data, that way, we can work with lower-dimensional data amenable for further human analysis. The main takeways of an autoencoder is the neural networks map the input data back to itself. However, in most successful autoencoders, constraints are placed on top of the input data before mapping to the outputs. For example, denoising autoencoders constraints the mapping by adding noise to the input data and then generating the original data (before adding noise) as the output data. Adding constraints is important to have learn useful features in the latent space because these same constraints helps avoid f (neural network mapping) become an identity function. \n\nWe will walk through various autoencoders: \n\n    1.) Fully connected autoencoder\n    2.) Deep Convolutional Auotencoder\n    3.) Denoising Convolutional Autoencoder: Added noise to input image + convolutional operations\n    4.) Variational Autoencoder (to be completed ...): Assume and constraint the latent space as a Gaussian distribution\n    \n    \nNot only does autoencoder help lower dimensional of the input data and extract important features within the latent space, autoencoders can also generate completely new data by exploring the latent space and using the decoder end of the network to output data.","execution_count":null},{"metadata":{"_uuid":"c13dbcf3-7bc0-4461-9422-689354cdf240","_cell_guid":"b4cebff0-0695-4f6f-b8c1-2963e6515e53","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e1ee1d05-c872-427a-bc0f-1b8d82b83978","_cell_guid":"4bbea3d7-277c-49da-83dd-cd619fbcc9b3","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport os","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"98445b5e-d261-47fd-9457-7629a0f2d824","_cell_guid":"ed1a61b8-904a-4454-8a5f-1e8f9acecf63","trusted":true},"cell_type":"code","source":"import tensorflow as tf\ntf.__version__\ndevice_name = tf.test.gpu_device_name()\nif \"GPU\" not in device_name:\n    print(\"GPU device not found\")\nprint('Found GPU at: {}'.format(device_name))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d882c854-fd24-4b9b-932f-8cc2de799025","_cell_guid":"2178a3e9-2df0-460f-a03f-d18443063086","trusted":true},"cell_type":"code","source":"tf.test.is_gpu_available()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"df118ef2-3a90-45fc-a7c2-54c97058d9e5","_cell_guid":"a00721ae-389e-40ec-a70f-61312168c714","trusted":true},"cell_type":"code","source":"from tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.metrics import Precision, Recall, AUC\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Input, Flatten,BatchNormalization,Activation\nfrom tensorflow.keras.layers import GlobalMaxPooling2D, GlobalAveragePooling2D\nfrom tensorflow.keras.callbacks import ModelCheckpoint, Callback, EarlyStopping","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"154b02e5-166c-4dd1-bb12-d3116ba6e657","_cell_guid":"5fcc23c3-9d93-4766-a2a7-615979ab18f4","trusted":true},"cell_type":"code","source":"main_path = '../input/digit-recognizer/'\ntrain_df = pd.read_csv(main_path + 'train.csv')\ntest_df = pd.read_csv(main_path + 'test.csv')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d958a8c6-93a2-43ca-8907-205c70f9cb1c","_cell_guid":"0a2d1441-6761-410a-adef-251f64791dfe","trusted":true},"cell_type":"markdown","source":"### Prep the data:","execution_count":null},{"metadata":{"_uuid":"3d355086-6321-4326-ab47-6bade86994ce","_cell_guid":"e049ed4c-6713-4105-8aa9-d3dda1b0f604","trusted":true},"cell_type":"code","source":"i = 3*32\nimg_size = (28*28,)\nX, y = np.zeros((i,) + img_size, dtype = \"float32\"), np.zeros((i,) + img_size, dtype = \"float32\")\nfor sample in range(i):\n    img = train_df.iloc[sample,1:]\n    X[ sample,:] = img\n    y[ sample,:] = img","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6c340961-8e60-4bdc-8f3b-8a8f271af2cb","_cell_guid":"d297c3ae-5430-4bc8-b9fe-d50506ed8695","trusted":true},"cell_type":"code","source":"img.shape, X.shape, X.reshape(96,784,).shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0085bae6-b38a-4d75-9468-6a727f31c77f","_cell_guid":"dee83ef5-392f-4060-a8b2-28ecb1058109","trusted":true},"cell_type":"code","source":"# split the training and testing dataframes    \ndef split_train_DF(df, train_perc):\n    # train_perc --> the percentage in the training set\n    final_train_df = df.iloc[0:round(train_perc*len((df.label))),:]\n    val_df = df.iloc[round(train_perc*len((df.label))):,:]\n    \n    return final_train_df, val_df    \n\n\n# catch statement: checks and verifies if the split was correct and \n# information/data was lost    \ndef verify_traintest_split(dataset, train_set, test_set):\n    \n    Total, train_A, train_B = len(dataset), len(train_set), len(test_set)\n    if Total == (train_A + train_B): print('Splitting the dataset into testing and training is successful ...\\n')\n    else: print('Splitting the dataset into testing and training failed ...')\n    return","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c4fa99d7-6977-4f5b-a05b-611e73629f87","_cell_guid":"8beb7a6e-df1e-481b-a148-3efeeca77340","trusted":true},"cell_type":"code","source":"from sklearn.model_selection import KFold\nimport matplotlib.pyplot as plt\n\n\ntrain_perc = 0.8\nnew_train_df, val_df = split_train_DF(train_df, train_perc)\n# check if the dataframe was properly split ... \nverify_traintest_split(train_df, new_train_df, val_df)\ncolumn_names = train_df.columns","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0d41dc1f-6f9a-4691-bad3-848e9f41c179","_cell_guid":"34882161-73a7-44e6-af4b-825e38901ddd","trusted":true},"cell_type":"code","source":"from tensorflow.keras import layers\nfrom tensorflow.keras.models import Model\n\n\n\n\ndef get_model(latent_space = 32):\n\n    input_img = Input(shape = (28*28,))\n    encoded = Dense(latent_space, activation = 'relu')(input_img)\n    decoded = Dense(28*28, activation = 'sigmoid')(encoded)\n    return Model(input_img, decoded)\n\n\nperc_autoencoder = get_model()\nopt = tf.keras.optimizers.SGD(learning_rate=0.0100000231231)\nperc_autoencoder.compile(optimizer=opt, loss='binary_crossentropy')\nperc_autoencoder.summary()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8250f2cd-d73a-4c50-9a0b-44a18810e23a","_cell_guid":"f532531c-60bf-4cba-99ab-71db3b5d9b00","trusted":true},"cell_type":"code","source":"train_df.iloc[:,0]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6ae08145-c6ec-4329-bbda-5a4bd6721c34","_cell_guid":"56bdabab-1994-4bec-818b-0164160bb04e","trusted":true},"cell_type":"code","source":"from tqdm import tqdm \nfrom sklearn.model_selection import train_test_split\nimport numpy as np\n\n\nX = np.zeros((len(train_df), 28*28,), dtype='float32')\ny = np.zeros((len(train_df), 1), dtype='float32')\n\nfor sample in tqdm(range(len(train_df))):\n    X[sample,:] = train_df.iloc[sample,1:].values\n    X[sample,:] *= 1/255.0\n    y[sample,0] = train_df.iloc[sample, 0]\n    \nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.20, random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4f0ed8f6-1524-4fd5-b2b4-23dce22a2d05","_cell_guid":"c612cc62-1831-4e82-92fe-27ad7f0998bc","trusted":true},"cell_type":"code","source":"train_history = perc_autoencoder.fit(X_train,X_train,  validation_data = (X_val,X_val), \n                     epochs = 200, batch_size = 250, verbose = 2, shuffle = True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"38b5b6f3-c5d0-4072-96e3-9f581e6c5d1f","_cell_guid":"5b598a0d-e581-4633-a1ca-f75bae3bf0ed","trusted":true},"cell_type":"code","source":"plt.figure(figsize = (12,7))\nplt.plot(train_history.history['loss'], 'r', LineWidth = 3, alpha = 0.45, label = 'training loss')\nplt.plot(train_history.history['val_loss'], 'b', LineWidth = 3, alpha = 0.45, label = 'validation loss')\nplt.xlabel('Epochs', fontsize = 18)\nplt.ylabel('Loss', fontsize = 18)\nplt.title('Comparing  loss between training and validation datasets', fontsize = 18, fontweight = 'bold')\nplt.legend(fontsize = 18)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"19bc9709-06a4-4720-a73a-d15f95149fc8","_cell_guid":"e5dcdd09-4a8f-42be-91f2-5d5454f817a7","trusted":true},"cell_type":"markdown","source":"# Deep Convolutional Autoencoder: \n\nEarlier, we showed how to construct a shallow autoencoder and train it on MNIST image data curated as vector representations. We achieved a poor loss performance ~0.3 for the validation and training data. This poor learning is mainly due to the lack of depth for the autoencoder, and more importantly, the lack of implementing convoltuional layers within the architecture. These convolutional layers allow to learn higher spatial features, drastically improving the represetnations found in the latent space of the autoencoder. So within the next example, we will construct a deep convoltuional autoencoder with a latent space that is amenable for our exploration and feature extraction.","execution_count":null},{"metadata":{"_uuid":"ae9ca47d-dbb2-4c38-80f0-2e35884bd0bc","_cell_guid":"8691c157-d9bd-4318-b74a-707159f15fac","trusted":true},"cell_type":"markdown","source":"Before we continue with implementing a deep convolutional autoencoder, let's free up some memory within our RAM ...","execution_count":null},{"metadata":{"_uuid":"8b76d891-cbd6-4a15-86f3-afae2e302666","_cell_guid":"a6fb339c-6111-4fba-89e8-259ee75df547","trusted":true},"cell_type":"code","source":"import gc\nimport psutil\n\n# Free up space\nprocess = psutil.Process(os.getpid())\nprint('Before deleting training data:', process.memory_info().rss)\ndel X, X_train, X_val; gc.collect()\nprint('After deleting training data:', process.memory_info().rss)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"822cd809-71ba-4641-a8fd-8281e4f3aa16","_cell_guid":"56f15c3d-0d48-4675-8547-e5c52a62be94","trusted":true},"cell_type":"markdown","source":"Here we convert the dataframe data columns corresponding to the pixel values to tensors, representing 2D image data rather 1D vectors.","execution_count":null},{"metadata":{"_uuid":"ac08f66e-30a3-4a24-aa16-d3251310464f","_cell_guid":"35f7575c-9c03-494c-81b8-76256ba4a60f","trusted":true},"cell_type":"code","source":"X = np.zeros((len(train_df), 28, 28, 1))\ny = np.zeros((len(train_df), 1))\nfor sample in tqdm(range(len(train_df))):\n    X[sample,:,:,0] = train_df.iloc[sample,1:].values.reshape(28,28,)\n    y[sample,0] = train_df.iloc[sample,0]\nprint('Final shape:', X.shape, y.shape)\n\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.20, random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"19174e89-b673-4cb9-8d7e-cd81600064db","_cell_guid":"71382fd3-203e-4107-b900-216bafd9886d","trusted":true},"cell_type":"markdown","source":"Here we construct the model architecture","execution_count":null},{"metadata":{"_uuid":"b3bd3737-f434-47e6-9ee1-efdd3e007a8d","_cell_guid":"69b7cbdd-92c7-452c-b0b4-a102ad62f726","trusted":true},"cell_type":"code","source":"from tensorflow.keras import layers\nfrom tensorflow.keras.models import Model\n\n\ndef gen_DCNN_autoencoder():\n    input_layer = Input(shape = (28,28,1))\n\n    x = layers.Conv2D(8, (3,3), padding = 'same', activation = 'relu')(input_layer)\n    b = layers.BatchNormalization()(x)\n    x = layers.Conv2D(16, (3,3), padding = 'same', activation = 'relu')(b)\n    b = layers.BatchNormalization()(x)\n    mp = layers.MaxPooling2D((2,2), padding = 'same')(b)\n    x = layers.Conv2D(32, (3,3), padding = 'same', activation = 'relu')(mp)\n    b = layers.BatchNormalization()(x)\n    x = layers.Conv2D(32, (3,3), padding = 'same', activation = 'relu')(b)\n    b = layers.BatchNormalization()(x)    \n\n    encoder = layers.MaxPooling2D((2,2), padding = 'same', name = 'encoding_z-space')(b)\n\n\n    t = layers.Conv2DTranspose(32, (3,3), strides = (2,2),  padding = 'same', activation = 'relu')(encoder)\n    b = layers.BatchNormalization()(t)\n    t = layers.Conv2DTranspose(32, (3,3), padding = 'same', activation = 'relu')(b)\n    b = layers.BatchNormalization()(t)\n    t = layers.Conv2DTranspose(16, (3,3),  strides = (2,2), padding = 'same', activation = 'relu')(b)\n    b = layers.BatchNormalization()(t)\n    t = layers.Conv2DTranspose(8, (3,3), padding = 'same', activation = 'relu')(b)\n\n    decoded = layers.Conv2D(1, (3,3), padding = 'same', activation = 'sigmoid')(t)\n\n    autoencoder = Model(input_layer, decoded)\n    autoencoder.compile(optimizer = 'adam', loss = 'mean_squared_error')\n\n    return autoencoder","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"78debaa4-b893-4b79-8ec1-c41ed9d2e599","_cell_guid":"672b8fd0-a1ce-4623-99e7-f8f71fde4e38","trusted":true},"cell_type":"code","source":"# normalize the data such that pixels are binned between [0., 1.]\nX_train *= 1/255.0\nX_val *= 1/255.0","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"de6a56af-9e8d-42a4-99ee-3a0d8cfe82bc","_cell_guid":"2afddb07-5fe2-45af-9057-983e515da7f0","trusted":true},"cell_type":"code","source":"autoencoder = gen_DCNN_autoencoder()\nhistory = autoencoder.fit(X_train, X_train, validation_data = (X_val,X_val), epochs = 30, batch_size = 250, shuffle=True, verbose = 2)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4210c6f0-a8b0-4bc4-86f3-90f91fcb930b","_cell_guid":"813744d7-f13f-4d87-b53b-b593b991bc35","trusted":true},"cell_type":"markdown","source":"With out loss results on the training and validation datasets, we should be very impressed. However, in general for autoencoders, this is not a good sign because our neural network is potentially only acting as an identity function which maps the input image back to itself. This is an issue since we lose the opportunity to learn imperative feature representations in the latent space, and these imperative feature representations can be extracted so that we can find way to better construct classification models. To avoid these issues, we will need to add extra constraints to the autoencoder, e.g., we can always decrease the complexity of the model. While this a plausible approach, we will decrease the size of the latent space and also implement a denosing autoencoder in the section.","execution_count":null},{"metadata":{"_uuid":"87e702a0-8726-40a6-b019-77188a460546","_cell_guid":"d2bb7ed4-b05e-46d8-90ed-7ae345df7702","trusted":true},"cell_type":"code","source":"plt.figure(figsize = (25,5))\nplt.plot(history.history['loss'], 'r', LineWidth = 3, alpha = 0.45, label = 'training loss')\nplt.plot(history.history['val_loss'], 'b', LineWidth = 3, alpha = 0.45, label = 'validation loss')\nplt.xlabel('Epochs', fontsize = 18)\nplt.ylabel('Loss', fontsize = 18)\nplt.title('Comparing loss between training and validation datasets', fontsize = 18, fontweight = 'bold')\nplt.legend(fontsize = 18)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6018a395-0bd1-4f59-883c-60a8ce4f56d0","_cell_guid":"542ee35f-c74e-4704-bde9-b7a53e21862a","trusted":true},"cell_type":"markdown","source":"Below, we will juxtapose the original input image with the correponding generate image from the neural network. We can see that the autoencoder generate a lower intensity image comparative to the original image. However, overall, the original and generated image is completely identical in terms of the spatial features.","execution_count":null},{"metadata":{"_uuid":"f7c8f62d-f0b8-48c8-8849-2599c14886eb","_cell_guid":"5800bb0e-b3fd-4b1e-802f-0f02536bbb15","trusted":true},"cell_type":"code","source":"for sample in range(10):\n    plt.figure(figsize = (5,5))\n    plt.subplot(1,2,1)\n    plt.imshow(X_val[sample].reshape(28,28,), cmap = 'gray')\n    plt.title(\"Original image\", fontsize = 18, fontweight = 'bold')\n    plt.axis('off')\n\n    plt.subplot(1,2,2)\n    plt.imshow(autoencoder.predict(X_val[sample].reshape(1,28,28,1)).reshape(28,28,), cmap = 'gray')\n    plt.title(\"Model prediction\", fontsize = 18, fontweight = 'bold')\n    plt.axis('off')\n    plt.tight_layout()\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2697dca7-9503-4832-a69d-dcefbbdabb26","_cell_guid":"f6a31d7d-2b07-492c-83b5-81d9aaa445af","trusted":true},"cell_type":"markdown","source":"Below, we will generate an image that contains only pixels plucked from a uniform distribution between [0,1] and compare it with the autoencoder generate image. Even though the output is nonsense, it is interesting to see how the output drastically changes comparative to the input.","execution_count":null},{"metadata":{"_uuid":"fa7b0137-1a41-45a1-8608-0606f02c2b82","_cell_guid":"536e0384-2290-4806-8378-31f338905847","trusted":true},"cell_type":"code","source":"noise = np.random.uniform(0,1,(28,28,1))\nplt.figure(figsize=(12,5))\n\nplt.subplot(1,2,1)\nplt.imshow(noise.reshape(28,28,), cmap = 'gray')\nplt.title('Uniform distribution between [0,1]', fontsize = 18, fontweight = 'bold')\nplt.axis('off')\n\nplt.subplot(1,2,2)\nplt.imshow(autoencoder.predict(noise.reshape(1,28,28,1)).reshape(28,28,), cmap = 'gray')\nplt.title('Model predicted image', fontsize = 18, fontweight = 'bold')\nplt.axis('off')\n\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d0bf1289-b1dd-428c-bc71-ba43d409c09f","_cell_guid":"462c0c98-0c67-4732-9704-bbea1e020bad","trusted":true},"cell_type":"markdown","source":"# Denoising Deep Autoencoder: \n\nHere, we add constraints to the previous Deep Autoencoder in two ways: \n    1. Decrease the size of the latent space\n    2. Add gaussian noise to the input images and let the neural network generate the original image\n    \nThe benefite of decreasing the latent space is that we lower dimensional of the feature space, allowing for additional analysis that much more amenable to humans comparative to deep neural networks, e.g., a logisitic regression or SVM. Additionally, adding noise to an input image and attempting to create a mapping function that generates the original image help avoid the trivial solution, where the neural network acts as an identity function.","execution_count":null},{"metadata":{"_uuid":"a8547802-9a0f-421b-b828-b67dc5e915cf","_cell_guid":"aa79ca27-5e68-4072-9661-21e591d0024a","trusted":true},"cell_type":"code","source":"\ndef Denoise_autoencoder(z):\n    input_layer = Input(shape = (28,28,1))\n\n    x = layers.Conv2D(8, (3,3), padding = 'same', activation = 'relu')(input_layer)\n    b = layers.BatchNormalization()(x)\n    mp = layers.MaxPooling2D((2,2), padding = 'same')(b)\n    x = layers.Conv2D(16, (3,3), padding = 'same', activation = 'relu')(mp)\n    b = layers.BatchNormalization()(x)\n    mp = layers.MaxPooling2D((2,2), padding = 'same')(b)\n    x = layers.Conv2D(16, (3,3), padding = 'same', activation = 'relu')(b)\n    b = layers.BatchNormalization()(x)    \n    mp = layers.MaxPooling2D((2,2), padding = 'same')(b)\n    x = layers.Conv2D(32, (3,3), padding = 'same', activation = 'relu')(mp)\n    b = layers.BatchNormalization()(x)    \n    mp = layers.MaxPooling2D((2,2), padding = 'same')(b)\n    x = layers.Conv2D(64, (3,3), padding = 'same', activation = 'relu')(mp)\n    b = layers.BatchNormalization()(x)    \n    mp = layers.MaxPooling2D((2,2), padding = 'same')(b)\n    x = layers.Conv2D(z, (3,3), padding = 'same', activation = 'relu')(mp)\n    b = layers.BatchNormalization()(x)    \n    \n    \n    encoder = layers.MaxPooling2D((2,2), padding = 'same', name = 'encoding_z-space')(b)\n    \n\n    t = layers.Conv2DTranspose(64, (3,3), strides = (2,2),  padding = 'valid', activation = 'relu')(encoder)\n    b = layers.BatchNormalization()(t)\n    t = layers.Conv2DTranspose(32, (3,3),  strides = (2,2), padding = 'valid', activation = 'relu')(b)\n    b = layers.BatchNormalization()(t)\n    t = layers.Conv2DTranspose(16, (3,3),  strides = (2,2), padding = 'same', activation = 'relu')(b)\n    b = layers.BatchNormalization()(t)\n    t = layers.Conv2DTranspose(16, (3,3),  strides = (2,2), padding = 'same', activation = 'relu')(b)\n    b = layers.BatchNormalization()(t)\n    t = layers.Conv2DTranspose(8, (3,3), padding = 'same', activation = 'relu')(b)\n    b = layers.BatchNormalization()(t)\n\n    \n    \n    decoded = layers.Conv2D(1, (3,3), padding = 'same', activation = 'sigmoid')(b)\n\n    opt = tf.keras.optimizers.Adam(learning_rate=0.000100000231231)\n    autoencoder = Model(input_layer, decoded)\n    autoencoder.compile(optimizer = 'adam', loss = 'mean_squared_error')\n\n    return autoencoder\ndenoise_ae = Denoise_autoencoder(3)\ndenoise_ae.summary()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d256c0b4-3f35-4f22-88b7-936c94192d51","_cell_guid":"e993733a-c4cf-4a7d-896f-166931bb7608","trusted":true},"cell_type":"code","source":"import cv2 as cv\nimport random\n\nsample = random.randint(0, 3000)\nimg = X_train[sample,:,:,0].copy()\nnoise_uni = cv.randu(img,(0),(1))\n\n# Present the before and after adding noise ...\nplt.imshow(X_train[sample,:,:,0].reshape(28,28,), cmap='gray')\nplt.show()\nplt.imshow((X_train[sample,:,:,0] + noise_uni), cmap='gray')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f1071bb3-0617-4bb8-8d03-da1f9b27c33f","_cell_guid":"99f5bc10-32a1-490b-a2ad-dd4af2f312ee","trusted":true},"cell_type":"code","source":"X_train_noise, X_val_noise = np.zeros((len(X_train), 28,28,1)), np.zeros((len(X_val), 28, 28, 1))\n\nfor sample in tqdm(range(len(X_train))):\n    \n    img = X_train[sample,:,:,0].copy()\n    noise_img = cv.randu(img,(0),(0.35))\n    X_train_noise[sample,:,:,0] =  X_train[sample,:,:,0] + noise_img\n    \n    if sample < len(X_val):\n        img2 = X_val[sample,:,:,0].copy()\n        noise_img2 = cv.randu(img2,(0),(0.35))\n        X_val_noise[sample,:,:,0] =  X_val[sample,:,:,0] + noise_img2","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fdda3fd3-6de4-4afc-8cb4-bb37260b2ae8","_cell_guid":"c5381e1c-c319-4577-b019-e505ff4d9728","trusted":true},"cell_type":"code","source":"plt.imshow(X_val[0,:,:,:].reshape(28,28,))\nplt.show()\nplt.imshow(X_val_noise[0,:,:,:].reshape(28,28,))\nplt.show()\n\nplt.imshow(X_train[0,:,:,:].reshape(28,28,))\nplt.show()\nplt.imshow(X_train_noise[0,:,:,:].reshape(28,28,))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ecd683c8-d5d3-4b25-a9f7-de42e805dc65","_cell_guid":"eec02954-942f-4ac5-9e9b-e643b2331e5b","trusted":true},"cell_type":"code","source":"denoise_ae = Denoise_autoencoder(3)\ndenoise_ae_history = denoise_ae.fit(X_train_noise, X_train, validation_data = (X_val_noise,X_val), epochs = 30, batch_size = 32, shuffle=True, verbose = 2)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"abe250ed-abde-43a6-b512-a38cc9e7c0aa","_cell_guid":"c9c8f3a1-1907-4ab7-a34c-28470f220a79","trusted":true},"cell_type":"code","source":"alpha = random.randint(0, len(X_val))\n\nplt.figure(figsize = (15,5))\nplt.subplot(1,3,1)\nplt.imshow(X_train[alpha,:,:,:].reshape(28,28,))\nplt.title('original image')\nplt.axis('off')\nplt.subplot(1,3,2)\nplt.imshow(X_train_noise[alpha,:,:,:].reshape(28,28,))\nplt.title('+ noise')\nplt.axis('off')\nplt.subplot(1,3,3)\nplt.imshow(denoise_ae.predict(X_train_noise[alpha,:,:,:].reshape(1,28,28,1)).reshape(28,28,))\nplt.title('generated image')\nplt.axis('off')\nplt.show()\n\n\n\nplt.figure(figsize = (15,5))\nplt.subplot(1,3,1)\nplt.imshow(X_val[alpha,:,:,:].reshape(28,28,))\nplt.title('original image')\nplt.axis('off')\nplt.subplot(1,3,2)\nplt.imshow(X_val_noise[alpha,:,:,:].reshape(28,28,))\nplt.title('+ noise')\nplt.axis('off')\nplt.subplot(1,3,3)\nplt.imshow(denoise_ae.predict(X_val_noise[alpha,:,:,:].reshape(1,28,28,1)).reshape(28,28,))\nplt.title('generated image')\nplt.axis('off')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"259f7073-ddd0-4b20-98c3-c9f0301efc75","_cell_guid":"001507f6-1448-41c6-897c-849cfaea8687","trusted":true},"cell_type":"markdown","source":"With the additional constraints (lower dimensional latent space and added noise), our neural network still learns to manages to generate the majority of the digits. Changing the structure of the network and hyperparameters can help will improving the overall results, but here, we will move on and explore the latent space.","execution_count":null},{"metadata":{"_uuid":"741a0df9-e7a7-4c20-b9f6-dc1933ce1fa5","_cell_guid":"e6cfbeef-2a1a-41f4-b31e-d5ac69d63d46","trusted":true},"cell_type":"markdown","source":"Below we construct the encoder which take input images and output low dimensional vector representations called the latent space. We will also construct decoder model which inputs that same low dimensional lower dimensional vector representation and outputs images of the corresponding label. We can also explore this latent space by generating new vectors and seeing what the decoder generates on the other side.","execution_count":null},{"metadata":{"_uuid":"f908a9fb-04a2-4596-9654-4c0548853ed8","_cell_guid":"8b28c975-d22f-422d-9d10-7fc9520926ee","trusted":true},"cell_type":"code","source":"\nencoder = Model(denoise_ae.input, denoise_ae.layers[-12].output)\nencoder.summary()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c2318f88-879d-4830-a1af-6d15e19b6c3f","_cell_guid":"60746cb6-b567-431a-bdb4-8ddb87de37fb","trusted":true},"cell_type":"code","source":"\ntrain_latent_space = np.zeros((500, 3))\nval_latent_space = np.zeros((500, 3))\nfor sample in tqdm(range(500)):\n    train_latent_space[sample,:] = encoder.predict(X_train[sample,:,:,0].reshape(1,28,28,1))[0][0][0]\n    val_latent_space[sample,:] = encoder.predict(X_val[sample,:,:,0].reshape(1,28,28,1))[0][0][0]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2fb48285-f9f6-4752-ad67-5086a1a51086","_cell_guid":"55079b14-b169-4298-a83d-ff11daf65be9","trusted":true},"cell_type":"code","source":"\n\nfig = plt.figure(figsize = (10, 7)) \nax = plt.axes(projection =\"3d\") \n  \n# Creating plot \nax.scatter3D(train_latent_space[:,0], train_latent_space[:,1], train_latent_space[:,2], alpha = 0.4); \nax.scatter3D(val_latent_space[:,0], val_latent_space[:,1], val_latent_space[:,2], alpha = 0.4); \nplt.title(\"simple 3D scatter plot\") \n# show plot \nplt.show()\n\nplt.figure(figsize = (12,3))\nplt.subplot(1,3,1)\nplt.scatter(train_latent_space[:,0], train_latent_space[:,1], alpha = 0.4)\nplt.scatter(val_latent_space[:,0], val_latent_space[:,1], alpha = 0.4)\n#plt.scatter(train_latent_space_8[:,0], train_latent_space_8[:,1])\nplt.subplot(1,3,2)\nplt.scatter(train_latent_space[:,0], train_latent_space[:,2], alpha = 0.4)\nplt.scatter(val_latent_space[:,0], val_latent_space[:,2], alpha = 0.4)\n\n#plt.scatter(train_latent_space_8[:,0], train_latent_space_8[:,2])\nplt.subplot(1,3,3)\nplt.scatter(train_latent_space[:,1], train_latent_space[:,2], alpha = 0.4)\nplt.scatter(val_latent_space[:,1], val_latent_space[:,2], alpha = 0.4)\n\n#plt.scatter(train_latent_space_8[:,1], train_latent_space_8[:,2])\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}