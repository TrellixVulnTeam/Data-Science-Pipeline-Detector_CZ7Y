{"cells":[{"metadata":{"_uuid":"59735dccf987e13fb90b366cafea68da11956eda"},"cell_type":"markdown","source":"![](https://www.kdnuggets.com/wp-content/uploads/photo.jpg)"},{"metadata":{"trusted":true,"_uuid":"2a635985ec3f29eea3f8029478a49e274914e860"},"cell_type":"markdown","source":"When you take a minute to stop and look around, the technological advancements of today could be perceived as something out of a futuristic novel. Cars are learning to drive, hands-free devices can turn on your lights or toast your bread, and flying drones are circling the skies. This is 2018. While the manifestation of Artificial Intelligence (AI) and Machine Learning (ML) haven’t been realized, impressive progress has certainly been made."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"![](https://i.stack.imgur.com/mFBCV.png)\n![](http://news.mit.edu/sites/mit.edu.newsoffice/files/styles/news_article_image_top_slideshow/public/images/2018/MIT-Invisible-Vision_0.jpg?itok=DWJiIHwB)\n\n                         Reveals “invisible” objects in the dark"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"![](https://img-s3.onedio.com/id-5867000b4813bb8710ce7f1a/rev-0/w-635/f-jpg-webp/s-21846e142a4303dd1b398188d8c63b16b94de38f.webp)\n\n                                   Google Lip Reading outperforms humans"},{"metadata":{"_uuid":"9088011e203ce4031ace7683658f826fee0cb9c7"},"cell_type":"markdown","source":"![](https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2018/03/191003285_edd8d0cf58-300x225.jpg)\n\n                    A man wearing a black shirt and a little girl wearing an orange dress share a treat ."},{"metadata":{"_uuid":"75b40609938361b5ac511f4aff56dbd0f5f8c193"},"cell_type":"markdown","source":"![](https://www.meme-arsenal.com/memes/9d68ce11eedb5c3d61049b393eee7dac.jpg)"},{"metadata":{"_uuid":"495c62d77e8240b76567aa8e25baa40baf636456"},"cell_type":"markdown","source":"## What is that Commonality among all the pictures have? \n\n\n## Video Time"},{"metadata":{"_uuid":"fe067ed17ee98b3554fe7eee8c81fba6d736ca8d"},"cell_type":"markdown","source":"# CNN\n\n### What is convolution\n\nConvolutional Neural networks allow computers to see, in other words, Convnets are used to recognize images by transforming the original image through layers to a class scores. CNN was inspired by the visual cortex. Every time we see something, a series of layers of neurons gets activated, and each layer will detect a set of features such as lines, edges. The high level of layers will detect more complex features in order to recognize what we saw.\n\n![](https://cdn-images-1.medium.com/max/800/1*XbuW8WuRrAY5pC4t-9DZAQ.jpeg)"},{"metadata":{"trusted":true,"_uuid":"99f65eec59cd7bb199c4ef7db73e799b7386494c"},"cell_type":"markdown","source":"### Why CNN became popular?\n\nCNNs can be thought of automatic feature extractors from the image. While if I use a algorithm with pixel vector I lose a lot of spatial interaction between pixels"},{"metadata":{"_uuid":"620cf7b514ce884696b974b8c2097887161f4c36"},"cell_type":"markdown","source":"ConvNet has two parts: feature learning (Conv, Relu,and Pool) and classification(FC and softmax).\n\n![](https://cdn-images-1.medium.com/max/800/1*2SWb6CmxzbPZijmevFbe-g.jpeg)"},{"metadata":{"_uuid":"8e7f34ba9eb6c95614618979ca5ef6c1466f73c3"},"cell_type":"markdown","source":"## Input (the training data):\n\n\nThe input layer or input volume is an image that has the following dimensions: [width x height x depth].It is a matrix of pixel values.\n\n**Example: Input: [32x32x3]=>(width=32, height=32, depth=3)The depth here, represents R,G,B channels.**"},{"metadata":{"_uuid":"5980772b192bf5ff3bb2135e32c7adaf77b75b6e"},"cell_type":"markdown","source":"## CONV layer:\n    \nThe objective of a Conv layer is to extract features of the input volume.\n\n![](https://cdn-images-1.medium.com/max/800/1*_34EtrgYk6cQxlJ2br51HQ.gif)"},{"metadata":{"_uuid":"71ddefcc0ef2da9f2d17760bffe88f3670fb9c25"},"cell_type":"markdown","source":"The outcome of this operation is a single integer of the output volume (feature map). Then we slide the filter over the next receptive field of the same input image by a Stride and compute again the dot products between the new receptive field and the same filter. We repeat this process until we go through the entire input image. The output is going to be the input for the next layer."},{"metadata":{"_uuid":"d1687fdf3f8a29924f69d59985e5242cd8b514c1"},"cell_type":"markdown","source":"### Terminology\n\n* **Filter, Kernel, or Feature Detector** is a small matrix used for features detection.\n\n* **Convolved Feature, Activation Map or Feature Map** is the output volume formed by sliding the filter over the image and computing the dot product.\n\n* **Depth** is the number of filters.\n\n* **Stride** has the objective of producing smaller output volumes spatially. For example, if a stride=2, the filter will shift by the amount of 2 pixels as it convolves around the input volume.\n"},{"metadata":{"_uuid":"c4529c5ed01b6d8b6b96655434c7d55044d208bd"},"cell_type":"markdown","source":"### How to compute the output volume[W2xH2xD2]?\n\nAnswer:\n\n* W2=(W1−F+2P)/S+1\n\n* H2=(H1−F+2P)/S+1\n\n* D2=K"},{"metadata":{"_uuid":"627645220b0dc4e523ea71bed362a79beebd740e"},"cell_type":"markdown","source":"## ReLU layer :\nReLU Layer applies an elementwise activation function max(0,x), which turns negative values to zeros (thresholding at zero). This layer does not change the size of the volume and there are no hyperparameters."},{"metadata":{"_uuid":"bbbee3df1328ce5f3b1d4e67bb3ed50b7916c0f9"},"cell_type":"markdown","source":"## POOL layer:\n\nPool Layer performs a function to reduce the spatial dimensions of the input, and the computational complexity of our model. And it also controls overfitting. It operates independently on every depth slice of the input. There are different functions such as Max pooling, average pooling"},{"metadata":{"_uuid":"4d59ff63672892fe811ebf5fe0a4e7a242e13e58"},"cell_type":"markdown","source":"Example of a Max pooling with 2x2 filter and stride = 2. So, for each of the windows, max pooling takes the max value of the 4 pixels.\n\n![](https://cdn-images-1.medium.com/max/800/1*S86gKd43MIYquHIeR9m8JQ.png)"},{"metadata":{"_uuid":"d98214b5648b09ea078cfdd83ea608c23a6c6b9a"},"cell_type":"markdown","source":"It has two hyperparameters: Filter (F) and Stride (S). More generally, having the input W1×H1×D1, the pooling layer produces a volume of size W2×H2×D2\n\n* W2=(W1−F)/S+1\n* H2=(H1−F)/S+1\n* D2=D1"},{"metadata":{"_uuid":"c4bc24f92c009c09078532cfdb7663585e28a9d5"},"cell_type":"markdown","source":"## Flatten\n\nW2×H2×D2 tensor becomes vector of [W2×H2×D2]\n\n## Fully Connected Layer (FC):\n\nFully connected layers connect every neuron in one layer to every neuron in another layer. The last fully-connected layer uses a softmax activation function for classifying the generated features of the input image into various classes based on the training dataset.\n"},{"metadata":{"_uuid":"2701d04db14d02184be6c86ec84feb57c0fc0316"},"cell_type":"markdown","source":"## Example of a ConvNet architecture:\n\nCIFAR-10 classification [INPUT — CONV — RELU — POOL — FC]"},{"metadata":{"trusted":true,"_uuid":"c19eb741be3d8e4a1a01a2473b29f4b4dcf7b81e"},"cell_type":"markdown","source":"## Hands on CNN using Digit Recognizer"},{"metadata":{"_uuid":"a69099500ed09da62d1a0d63e4e329124a315f11"},"cell_type":"markdown","source":"This is a 5 layers Sequential Convolutional Neural Network for digits recognition trained on MNIST dataset. "},{"metadata":{"trusted":true,"_uuid":"67fd93c8742db3f64d07d494282270b41eeeca4b"},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport seaborn as sns\n%matplotlib inline\n\nnp.random.seed(2)\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nimport itertools\n\nfrom keras.utils.np_utils import to_categorical # convert to one-hot-encoding\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\nfrom keras.optimizers import RMSprop\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ReduceLROnPlateau","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"790459766fa6e717c9eed808fcbb044f574f1361"},"cell_type":"code","source":"# Load the data\ntrain = pd.read_csv(\"../input/train.csv\")\ntest = pd.read_csv(\"../input/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8e8e48d0ae9777a9f69c03e30882a459cde1509e"},"cell_type":"code","source":"Y_train = train[\"label\"]\n\n# Drop 'label' column\nX_train = train.drop(labels = [\"label\"],axis = 1) \n\n# free some space\ndel train ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"28873846e3df7b7c36f7a49184e5cd9a8682500e"},"cell_type":"code","source":"g = sns.countplot(Y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3c11ee406dbd6665363ca63c60b5d9ea886174c7"},"cell_type":"code","source":"# Normalize the data\nX_train = X_train / 255.0\ntest = test / 255.0\n# Reshape image in 3 dimensions (height = 28px, width = 28px , canal = 1)\nX_train = X_train.values.reshape(-1,28,28,1)\ntest = test.values.reshape(-1,28,28,1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a175b4fbb7c78e67b6061a315fbb1cbbea33513e"},"cell_type":"code","source":"# Encode labels to one hot vectors (ex : 2 -> [0,0,1,0,0,0,0,0,0,0])\nY_train = to_categorical(Y_train, num_classes = 10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"56e72626f3856c8bd9d918cb2caec5280300bcc7"},"cell_type":"code","source":"# Set the random seed\nrandom_seed = 2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a035c13bbd1b4300e12902a62446e3bec6d25125"},"cell_type":"code","source":"# Split the train and the validation set for the fitting\nX_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size = 0.1, random_state=random_seed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ee48b72a69692586dc4f4091c9791ab0a11dde16"},"cell_type":"code","source":"g = plt.imshow(X_train[0][:,:,0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b0ede6f508365f75d2f990463d1a840ad07d486f"},"cell_type":"code","source":"model = Sequential()\n\nmodel.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'valid', \n                 activation ='relu', input_shape = (28,28,1)))\nmodel.add(MaxPool2D(pool_size=(2,2)))\nmodel.add(Dropout(0.25))\n\n\nmodel.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n                 activation ='relu'))\n\nmodel.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\nmodel.add(Dropout(0.25))\n\n\nmodel.add(Flatten())\nmodel.add(Dense(256, activation = \"relu\"))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(10, activation = \"softmax\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f798716e970044b2ef695942b7a1e0902a7aa5bd"},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ce305f1428c9344c6574c4d2ab965fe7afb58a41"},"cell_type":"code","source":"# Define the optimizer\noptimizer = RMSprop(lr=0.001)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0c5236edcc0671b9020de8387ae73ee9fc5192de"},"cell_type":"code","source":"# Compile the model\nmodel.compile(optimizer = optimizer , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"28d7747362724129f18362f1e1831a4809611305"},"cell_type":"code","source":"epochs = 2 \nbatch_size = 64","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"148e5b3a123b32a9afdad881d5c4368f52cd5221"},"cell_type":"code","source":"model.fit(X_train,Y_train,epochs=epochs,batch_size=batch_size,validation_data=(X_val,Y_val))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b5ead0124a06d298efba56af0a0c93377b4d6d78"},"cell_type":"code","source":"# Predict the values from the validation dataset\nY_pred = model.predict(X_val)\n# Convert predictions classes to one hot vectors \nY_pred_classes = np.argmax(Y_pred,axis = 1) \n# Convert validation observations to one hot vectors\nY_true = np.argmax(Y_val,axis = 1) \n# compute the confusion matrix\nconfusion_mtx = confusion_matrix(Y_true, Y_pred_classes) \nconfusion_mtx","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"039ba7259b4af6e4e34cbda95398ae4243c35742"},"cell_type":"code","source":"# predict results\nresults = model.predict(test)\n\n# select the indix with the maximum probability\nresults = np.argmax(results,axis = 1)\n\nresults = pd.Series(results,name=\"Label\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1228296b0c9bd368a555748915b1ded256336026"},"cell_type":"code","source":"submission = pd.concat([pd.Series(range(1,28001),name = \"ImageId\"),results],axis = 1)\n\nsubmission.to_csv(\"submission.csv\",index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c62a9e6236f6238ccd5ba8a6c61583688c4dc81e"},"cell_type":"markdown","source":"## Key Take aways:\n\n* CNN helps in preserving the spatial information unlike NN.\n\n* Total no of parameter that are to be trained in CNN are minimal when compared to NN\n\n* Pooling avoids overfitting by down sampling the data.\n\n\n**Please upvote if you find this kernel helpful.**"},{"metadata":{"_uuid":"1f879a6dc07dfa1cc666929087e8d437c30a0f57"},"cell_type":"markdown","source":"## Spread Happiness and Kindness..."},{"metadata":{"trusted":true,"_uuid":"252846a01dc74ac41dc328bfc7cbbcf99043d012"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}