{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Importing Dataset","metadata":{}},{"cell_type":"code","source":"mnist_df = pd.read_csv(\"/kaggle/input/digit-recognizer/train.csv\")\nmnist_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mnist_df.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It is given that pixelx is located on row i and column j of a 28 x 28 matrix\nSo, we get 28 x 28 = 784 after excluding labels (actual number values)","metadata":{}},{"cell_type":"markdown","source":"# Constructing a digit from pixel values","metadata":{}},{"cell_type":"code","source":"import matplotlib as mpl\nimport matplotlib.pyplot as plt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for val in (10, 25, 1000, 2500):\n    sample_digit = mnist_df.iloc[val][1:] #taking all pixel values\n    sample_digit_mat = sample_digit.values.reshape(28,28)\n    plt.imshow(sample_digit_mat, cmap=\"binary\")\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# to confirm the values\nfor val in (10, 25, 1000, 2500):\n    print( mnist_df.iloc[val][0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"They are equal so everything seems almost fine and we are good to go with our EDA and learning models.","metadata":{}},{"cell_type":"markdown","source":"# EDA","metadata":{}},{"cell_type":"markdown","source":"Checking missing or null values","metadata":{}},{"cell_type":"code","source":"mnist_df.isnull().any().describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There is no null or missing value present in the training dataset","metadata":{}},{"cell_type":"code","source":"import seaborn as sns\nplt.ioff()\nsns.set_theme(style=\"darkgrid\")\nax = sns.countplot(data=mnist_df, x=\"label\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can visualize that dataset has similar count of values for all digits from 0 to 9","metadata":{}},{"cell_type":"markdown","source":"# Splitting of Training and Validation datasets","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = mnist_df.drop([\"label\"], axis=1)\ny = mnist_df[\"label\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Verify it with its shape\nprint(X.shape)\nprint(y.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Classification Models","metadata":{}},{"cell_type":"markdown","source":"1. SGD Classifier","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import SGDClassifier\n\nsgd_clf = SGDClassifier(random_state = 42, n_jobs=-1)\nsgd_clf_log = SGDClassifier(random_state = 42, n_jobs=-1, loss=\"log\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sgd_clf.fit(X_train, y_train)\nsgd_clf_log.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\n\ny_pred_sgd_clf = sgd_clf.predict(X_test)\ny_pred_sgd_clf_log = sgd_clf_log.predict(X_test)\n\nprint(accuracy_score(y_test, y_pred_sgd_clf))\nprint(accuracy_score(y_test, y_pred_sgd_clf_log))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Confusion Matrix to look into false positive and false negative predictions","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\n\ny_test_pred_sgd = sgd_clf.predict(X_test)\nconfusion_matrix(y_test, y_test_pred_sgd)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the confusion matrix we can see a lot of digits as false positives and negatives","metadata":{}},{"cell_type":"markdown","source":"Prediction and Recall scores","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import precision_score, recall_score, f1_score\n\nprint(precision_score(y_test, y_test_pred_sgd, average=\"weighted\"))\nprint(recall_score(y_test, y_test_pred_sgd, average=\"weighted\"))\nprint(f1_score(y_test, y_test_pred_sgd, average=\"weighted\"))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let us try with some better classification model with higher accuracy and f1 score","metadata":{}},{"cell_type":"markdown","source":"2. Random Forest Classifier","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nrfc = RandomForestClassifier(n_estimators=500, max_leaf_nodes=30, random_state=42, n_jobs=-1, verbose=1)\nrfc.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\n\ny_test_pred_rfc = rfc.predict(X_test)\naccuracy_score(y_test, y_test_pred_rfc)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"3. K-Nearest Neighbors Classifier","metadata":{}},{"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import GridSearchCV","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**GridSearchCV to try out KNN at different hyperparameters**","metadata":{}},{"cell_type":"code","source":"param_grid = [{'weights': [\"uniform\", \"distance\"], 'n_neighbors': [5, 8, 13]}]\n\nknn_clf = KNeighborsClassifier(n_jobs=-1)\n\ngrid_search = GridSearchCV(knn_clf, param_grid, cv=3, verbose=3)\ngrid_search.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(grid_search.best_params_)\nprint(grid_search.best_score_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let us try to increase accuracy of knn by hyperparameter tuning of k value","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\n\naccuracies = []\nfor k in np.arange(4,13):\n    knn = KNeighborsClassifier(n_neighbors=k)\n    scores = cross_val_score(knn, X_train, y_train, cv=3, n_jobs=-1, verbose=3)\n    accuracies.append(scores.mean())\n    \nprint(accuracies)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(np.arange(4,13), accuracies)\nplt.xlabel('k in kNN')\nplt.ylabel('Accuracy')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Visualizng accuracy on different k values in KNN, we can see it stays same from k 3 to 6 which is around 96.3% and then it drops sharply for higher k","metadata":{}},{"cell_type":"markdown","source":"4. SVM Classification","metadata":{}},{"cell_type":"code","source":"from sklearn.decomposition import PCA\n\npca = PCA()\npca.fit(X_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cumsum = np.cumsum(pca.explained_variance_ratio_)\n#print(cumsum)\nd = np.argmax(cumsum >= 0.95) + 1 # to maintain 95% variance\nprint(d)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#better way is to specify the variance we need to preserve\npca = PCA(n_components=0.95)\nX_train_reduced = pca.fit_transform(X_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_reduced.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.svm import SVC\n\nsvm_clf = SVC(kernel=\"rbf\", random_state=42, verbose=3)\nsvm_clf.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_test_pred_svm = svm_clf.predict(X_test)\naccuracy_score(y_test, y_test_pred_svm)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**GridSearchCV to try out SVC at different hyperparameters of C**","metadata":{}},{"cell_type":"code","source":"params_grid_svm = {'C':[0.1, 1, 10, 100]}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"grid_search_svm = GridSearchCV(SVC(kernel=\"rbf\", random_state=42), param_grid=params_grid_svm, verbose=3)\ngrid_search_svm.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(grid_search_svm.best_params_)\nprint(grid_search_svm.best_score_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Building an ensemble with the above classifiers","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import VotingClassifier\n\nsgd_clf = SGDClassifier(random_state = 42)\nrf_clf = RandomForestClassifier(n_estimators=500, random_state=42, n_jobs=-1, verbose=1)\nknn_clf = KNeighborsClassifier(n_neighbors=5, weights=\"distance\")\nsvm_clf = SVC(kernel=\"rbf\", C=10, probability=True, random_state=42)\n\nvoting_clf = VotingClassifier(estimators=[(\"sgd\", sgd_clf), (\"rfc\", rf_clf), (\"knn\", knn_clf), (\"svc\", svm_clf)])\nvoting_clf.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for clf in (sgd_clf, rf_clf, knn_clf, svm_clf, voting_clf):\n    clf.fit(X_train, y_train)\n    y_pred = clf.predict(X_test)\n    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"voting_clf = VotingClassifier(estimators=[(\"rfc\", rf_clf), (\"knn\", knn_clf), (\"svc\", svm_clf)], voting=\"soft\")\nvoting_clf.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred_votclf = voting_clf.predict(X_test)\naccuracy_score(y_test, y_pred_votclf)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.svm import SVC\n\nsvm_clf = SVC(kernel=\"rbf\", C=10, random_state=42, verbose=3)\nsvm_clf.fit(X_train_reduced, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"svm_clf.score(pca.transform(X_test), y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pca = PCA(n_components=0.95)\n#X_train_reduced = pca.fit_transform(X_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"svm_clf.fit(pca.fit_transform(X), y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val = pd.read_csv('/kaggle/input/digit-recognizer/test.csv')\npred = svm_clf.predict(pca.transform(val))\n# ImageId,Label\n\nval['Label'] = pd.Series(pred)\nval['ImageId'] = val.index +1\nsub = val[['ImageId','Label']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub.to_csv('submission_updated1.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Trying to go beyond 98.5% - 99% accuracy with CNN","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.preprocessing.image import ImageDataGenerator","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_scaled = X_train/255\nX_test_scaled = X_test/255","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#reshaping for data augmentation\nX_train_scaled = X_train_scaled.values.reshape(-1, 28, 28, 1)\nX_test_scaled = X_test_scaled.values.reshape(-1, 28, 28, 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"datagen = ImageDataGenerator(\n        rotation_range=10,\n        width_shift_range=0.1,\n        height_shift_range=0.1,\n        zoom_range=0.1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"datagen.fit(X_train_scaled) # we do data augmentation of training images, leaving the test images untouched","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_set = datagen.flow(X_train_scaled, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Conv2D, MaxPooling2D\nfrom keras.layers import Activation, Dropout, Flatten, Dense, Dropout\n\nmodel = Sequential()\n\nmodel.add(Conv2D(32, kernel_size=5, input_shape=(28, 28, 1), activation=\"relu\"))\nmodel.add(Conv2D(32, kernel_size=5, activation=\"relu\"))\nmodel.add(MaxPooling2D(pool_size=2))\nmodel.add(Dropout(0.4))\n\nmodel.add(Conv2D(64, kernel_size=3, activation=\"relu\"))\nmodel.add(Conv2D(64, kernel_size=3, activation=\"relu\"))\nmodel.add(MaxPooling2D(pool_size=2, strides=2))\nmodel.add(Dropout(0.4))\n\nmodel.add(Flatten())\nmodel.add(Dense(300, activation = \"relu\"))\nmodel.add(Dropout(0.4))\nmodel.add(Dense(10, activation = \"softmax\"))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# now compiling and training my CNN\n# we mention our sparse_categorical as it works on integers\nmodel.compile(optimizer=keras.optimizers.Adam(learning_rate = 0.001), loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#callbacks_es = keras.callbacks.EarlyStopping(patience=10)\ncallbacks_cp = keras.callbacks.ModelCheckpoint(filepath=\"/kaggle/working/mykeras_model.h5\", save_best_only=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit(training_set, validation_data = (X_test_scaled, y_test), epochs=30, verbose=2, callbacks=[callbacks_cp])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val = pd.read_csv('/kaggle/input/digit-recognizer/test.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_scaled = val/255\nval_scaled = val_scaled.values.reshape(-1, 28, 28, 1)\nval_scaled.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now pred has probability for each class, but we need to consider the max one","metadata":{}},{"cell_type":"code","source":"pred = model.predict(val_scaled)\n\nresult = np.argmax(pred, axis=1)\nresult1 = pd.Series(result, name=\"Label\")\n#result['ImageId'] = val.index +1\nsub = pd.concat([pd.Series(range(1, 28001), name=\"ImageId\"), result1], axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub.to_csv('submission_updated4.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# trying to tune more hyperparameters using keras tuner","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have used Keras Tuner to find out the best hyperparameters for our CNN. It found out 300 units in the Dense layer and learning rate of Adam optimizer = 0.001","metadata":{}},{"cell_type":"code","source":"!pip install -q -U keras-tuner","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import kerastuner as kt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def model_builder(hp):\n    model = keras.models.Sequential()\n\n    model.add(Conv2D(32, kernel_size=5, input_shape=(28, 28, 1), activation=\"relu\"))\n    model.add(Conv2D(32, kernel_size=5, activation=\"relu\"))\n    model.add(MaxPooling2D(pool_size=2))\n\n    model.add(Conv2D(64, kernel_size=3, activation=\"relu\"))\n    model.add(Conv2D(64, kernel_size=3, activation=\"relu\"))\n    model.add(MaxPooling2D(pool_size=2, strides=2))\n\n    model.add(Flatten())\n    hp_units = hp.Int('units', min_value=150, max_value=300, step=50)\n    model.add(Dense(units = hp_units, activation = \"relu\"))\n    model.add(Dense(10, activation = \"softmax\"))\n\n    hp_lr = hp.Choice('learning_rate', values=[1e-1, 1e-2, 1e-3])\n    my_optim = keras.optimizers.Adam(learning_rate = hp_lr)\n    model.compile(optimizer=my_optim, loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n    \n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tuner = kt.Hyperband(model_builder,\n                     objective='val_accuracy',\n                     max_epochs=10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\ntuner.search(training_set, validation_data = (X_test_scaled, y_test), epochs=30, verbose=2, callbacks=[stop_early])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(best_hps.get('learning_rate'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(best_hps.get('units'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}