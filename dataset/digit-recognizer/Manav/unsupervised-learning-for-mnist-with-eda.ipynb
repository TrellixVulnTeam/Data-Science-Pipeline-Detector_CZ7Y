{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Why this Dataset?\nLet's be frank up-front. MNIST is the most classical example of a machine learning datatset. One which has been explored quite a number of times by many people and We do not have any novelty to add to the solution. **Unless we try to use an etirely different approach to solve the problem.** Much of the existing work done on this dataset has been using some form of *Supervised Learning*, may it be Linear models or Decision Trees or GBTs or CNNs.  \nWhat we are going to do through this notebook is to find an unique approach to solve this problem through an ***Unsupervised Algorithm***. Sounds novel now? ðŸ˜Ž\n\n# Problem Statement\nOur goal is to correctly identify digits from a dataset of tens of thousands of handwritten images.  \nThe scoring metric for this competition is also a classical one for multi-class classification: **Accuracy**.  \nHow often do we encouter this nowaday! ðŸ˜†\n\n## Data Description:-\n1. The data files train.csv and test.csv contain gray-scale images of hand-drawn digits, from zero through nine.\n2. Each image is 28 pixels in height and 28 pixels in width, for a total of 784 pixels in total. Each pixel has a single pixel-value associated with it, indicating the lightness or darkness of that pixel, with higher numbers meaning darker. This pixel-value is an integer between 0 and 255, inclusive.\n\n## Expected Outcome:-\n* Correctly identify the digit form the pixel values of an grayscale image.\n\n## Problem Category:-\n* For the data and objective its is evident that this is a **multi-class classification problem** in the **Computer Vision** domain.\n\n# About this Notebook\n* This Notebook proposes a novel approach towards attempting the classical MNIST problem as an unsupervised learning problem.\n* Since MNIST is a beginner dataset, this notebook will also be a **beginner friendly** notebook.\n* This notebook assumes the reader has some basic idea regarding unsupervised learning methods and types.\n    * If you are not confident, I would highly recommend you to brush up on topics from [this Notebook](https://www.kaggle.com/manabendrarout/unsupervised-learning-for-beginners), I wrote some times back. It is very extensive and should cover all the basics.\n* This notebook also attempts to project other general use cases and inferences from Unsupervised learning models.\n\n# Imports\nKeeping these above points in mind, let's start by importing some basic libraries that we require though our journey of this notebook."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Asthetics\nimport warnings\nimport sklearn.exceptions\nwarnings.filterwarnings('ignore', category=DeprecationWarning)\nwarnings.filterwarnings('ignore', category=FutureWarning)\nwarnings.filterwarnings(\"ignore\", category=sklearn.exceptions.UndefinedMetricWarning)\n\n# General\nimport pandas as pd\npd.set_option('display.max_columns', None)\nimport numpy as np\nimport os\nimport time\nimport random\nfrom scipy.stats import mode\n\n# Visialisation\nimport matplotlib.pyplot as plt\nfrom matplotlib import offsetbox\n%matplotlib inline\nimport seaborn as sns\nsns.set(style=\"whitegrid\")\nfrom plotly import graph_objs as go\nimport plotly.express as px\nimport plotly.figure_factory as ff\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n\n# Machine Learning\n# Dimensionallity Reduction\nfrom sklearn.manifold import Isomap, TSNE\nfrom sklearn.decomposition import PCA\n# Clustering\nfrom sklearn.mixture import GaussianMixture\n#Metrics\nfrom sklearn import metrics\nfrom sklearn.metrics import confusion_matrix, classification_report","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now many models/libraries have a random initialization state which might differ from one run to another. Which might lead to difference in performance purely due to randomness and not due to any changes in code or algorithm. To account for such difference, let's fix the randomness by seeding the values to a fixed integer so that we have a much more predictable performance measure."},{"metadata":{"trusted":true},"cell_type":"code","source":"RANDOM_SEED = 42","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def seed_everything(seed=RANDOM_SEED):\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    random.seed(seed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"seed_everything()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we will get predictable performance everytime we run this code. Moving on, let's read the train and testset files..."},{"metadata":{"trusted":true},"cell_type":"code","source":"data_path = '../input/digit-recognizer'\n\ntrain_file_path = os.path.join(data_path, 'train.csv')\nsample_sub_path = os.path.join(data_path, 'sample_submission.csv')\ntest_file_path = os.path.join(data_path, 'test.csv')\n\nprint(f'Training File path: {train_file_path}')\nprint(f'Sample Submission File path: {sample_sub_path}')\nprint(f'Test Files path: {test_file_path}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv(train_file_path)\nsample_sub_df = pd.read_csv(sample_sub_path)\ntest_df = pd.read_csv(test_file_path)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# EDA\nLet's look at some example images..."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = train_df.drop(['label'], axis=1).values.reshape(-1,28,28,1)\ntrain_data = test_df.values.reshape(-1,28,28,1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_examples = 10\nplt.figure(figsize=(20,20))\nfor i in range(num_examples):\n    plt.subplot(1, num_examples, i+1)\n    plt.imshow(train_data[i], cmap='Greys')\n    plt.axis('off')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Great! Let's see what is the class balance in this particular ptoblem set..."},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = plt.subplots(figsize=(18, 6))\nsns.set_style(\"whitegrid\")\nsns.countplot(x='label', data=train_df);\nplt.ylabel(\"No. of Observations\", size=20);\nplt.xlabel(\"Class Name\", size=20);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Okay, the class looks pretty balanced. Now let's convert the images into 2D and 3D embeddings and see how closely they resemble one another..."},{"metadata":{"trusted":true},"cell_type":"code","source":"iso = Isomap(n_components=2)\n\n# Using only 1/10 of the data as full data takes a lot of time to run\niso.fit(train_df.drop(['label'], axis=1)[::10])\ndata_2d = iso.transform(train_df.drop(['label'], axis=1)[::10])\n\niso_df = pd.DataFrame(data_2d)\niso_df['label'] = train_df.label.values[::10]\niso_df.columns = ['x', 'y', 'label']\n# Converting label to string to get discrete colors in plot\niso_df['label'] = iso_df['label'].astype(str)\niso_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.scatter(iso_df, x='x', y='y', color='label',\n                 hover_data=['label'])\nfig.update_layout(title = 'MNIST ISOmap 2D')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"iso = Isomap(n_components=3)\n\n# Using only 1/10 of the data as full data takes a lot of time to run\niso.fit(train_df.drop(['label'], axis=1)[::10])\ndata_3d = iso.transform(train_df.drop(['label'], axis=1)[::10])\n\niso_df = pd.DataFrame(data_3d)\niso_df['label'] = train_df.label.values[::10]\niso_df.columns = ['x', 'y', 'z', 'label']\n# Converting label to string to get discrete colors in plot\niso_df['label'] = iso_df['label'].astype(str)\niso_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.scatter_3d(iso_df, x='x', y='y', z='z', color='label',\n                    hover_data=['label'])\nfig.update_layout(title = 'MNIST ISOmap 3D')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**NOTE:- The above plot is interactive. Feel free to rotate/zoom to clearly observe the clusters.**  \n\nAnother dimensionality reduction technique is PCA. Let's see how those clusters look..."},{"metadata":{"trusted":true},"cell_type":"code","source":"pca = PCA(n_components=2)\n\n# Using only 1/5 of the data as full data takes a lot of time to run\npca.fit(train_df.drop(['label'], axis=1)[::5])\ndata_2d = pca.transform(train_df.drop(['label'], axis=1)[::5])\n\npca_df = pd.DataFrame(data_2d)\npca_df['label'] = train_df.label.values[::5]\npca_df.columns = ['x', 'y', 'label']\n# Converting label to string to get discrete colors in plot\npca_df['label'] = pca_df['label'].astype(str)\npca_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.scatter(pca_df, x='x', y='y', color='label',\n                 hover_data=['label'])\nfig.update_layout(title = 'MNIST PCA 2D')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pca = PCA(n_components=3)\n\n# Using only 1/5 of the data as full data takes a lot of time to run\npca.fit(train_df.drop(['label'], axis=1)[::5])\ndata_3d = pca.transform(train_df.drop(['label'], axis=1)[::5])\n\npca_df = pd.DataFrame(data_3d)\npca_df['label'] = train_df.label.values[::5]\npca_df.columns = ['x', 'y', 'z', 'label']\n# Converting label to string to get discrete colors in plot\npca_df['label'] = pca_df['label'].astype(str)\npca_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.scatter_3d(pca_df, x='x', y='y', z='z', color='label',\n                    hover_data=['label'])\nfig.update_layout(title = 'MNIST PCA 3D')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**NOTE:- The above plot is interactive. Feel free to rotate/zoom to clearly observe the clusters.**  \n\nAnother interesting and powerful dimensionality reduction technique is tSNE. Let's see how we do on that algorithm..."},{"metadata":{"trusted":true},"cell_type":"code","source":"tsne = TSNE(n_components=2, random_state=RANDOM_SEED)\n\n# Using only 1/5 of the data as full data takes a lot of time to run\ndata_2d = tsne.fit_transform(train_df.drop(['label'], axis=1)[::5])\n\ntsne_df = pd.DataFrame(data_2d)\ntsne_df['label'] = train_df.label.values[::5]\ntsne_df.columns = ['x', 'y', 'label']\n# Converting label to string to get discrete colors in plot\ntsne_df['label'] = tsne_df['label'].astype(str)\ntsne_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.scatter(tsne_df, x='x', y='y', color='label',\n                 hover_data=['label'])\nfig.update_layout(title = 'MNIST tSNE 2D')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tsne = TSNE(n_components=3, random_state=RANDOM_SEED)\n\n# Using only 1/5 of the data as full data takes a lot of time to run\ndata_3d = tsne.fit_transform(train_df.drop(['label'], axis=1)[::5])\n\ntsne_df = pd.DataFrame(data_3d)\ntsne_df['label'] = train_df.label.values[::5]\ntsne_df.columns = ['x', 'y', 'z', 'label']\n# Converting label to string to get discrete colors in plot\ntsne_df['label'] = tsne_df['label'].astype(str)\ntsne_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.scatter_3d(tsne_df, x='x', y='y', z='z', color='label',\n                    hover_data=['label'])\nfig.update_layout(title = 'MNIST tSNE 3D')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**NOTE:- The above plot is interactive. Feel free to rotate/zoom to clearly observe the clusters.**  "},{"metadata":{},"cell_type":"markdown","source":"Now that we have seen various embeddings of MNIST clustered on 2 and 3 axes. You can observe some interesting things here:-\n* Digits like 6 and 0 which look very close to one another have a lot of overlap in the cluster space.\n* Digits like 4 and 0 which look very different are far apart in the projected plot.\n* One other interesting thing is the overlap between 1 and 7 cluster spaces. I am guessing that might be due to some people writing ones with hats on top which makes them look closer to 7.\n* Interestingly it also shows that some 4 and 7 are also closely related. I guess these are because some people write 7 with a horizontal line in between which makes it seem closer to 4.\n\nAs you can see we can derive similar interesting insights from the Data using unsupervised learning, dimensionality reduction and plotting.  \n\nAnother interesting aspect to see is to track what kind of variations are there within a single class... To visualize this better let's create an helpfer function hat will output image thumbnails at the locations of the projections."},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_components(data, model, images=None, ax=None, thumb_frac=0.05, cmap='gray'):\n    ax = ax or plt.gca()\n    proj = model.fit_transform(data)\n    ax.plot(proj[:, 0], proj[:, 1], '.k')\n    if images is not None:\n        shown_images = np.array([2 * proj.max(0)])\n        for i in range(data.shape[0]):\n            shown_images = np.vstack([shown_images, proj[i]])\n            imagebox = offsetbox.AnnotationBbox(\n                offsetbox.OffsetImage(images[i], cmap=cmap),\n                proj[i])\n            ax.add_artist(imagebox)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's plot all of the ones and see how varied are they from one another..."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(10, 10))\nmodel = Isomap(n_neighbors=5, n_components=2, eigen_solver='dense')\ndata = train_df[train_df.label == 1][::5]\ndata = data.drop(['label'], axis=1).values\nplot_components(data, model, images=data.reshape((-1, 28, 28)), ax=ax, thumb_frac=0.05, cmap='gray_r')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Fascinating!  \nAs you can see this gives un an idea of the variety of Ones we have in out datatset.\nSome observations will be:-\n1. Towards the left there are ones which are tilting left.\n2. Towards the middle there are straight ones.\n3. Towards the right there are ones which tilt right.\n4. Towards bottom there are ones which are thinner.\n5. Towards the top there are ones which are thicker.\n\nLet's try the same on Zeros and let's see what comes up..."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(10, 10))\nmodel = Isomap(n_neighbors=5, n_components=2, eigen_solver='dense')\ndata = train_df[train_df.label == 0][::5]\ndata = data.drop(['label'], axis=1).values\nplot_components(data, model, images=data.reshape((-1, 28, 28)), ax=ax, thumb_frac=0.05, cmap='gray_r')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we can observe that:-\n1. On the left side of the plot there are perfectly circular(ish) zeros.\n2. Towards the right hand side the zeros are slightly tiled towsrds the right.\n3. On bottom the zeros are marked with a thinner pen/pencil.\n4. Towards the top the Zeros are a lot thicker.\n\nSimilarly we can find out a lot of interesting and beautiful insights from data using Unsupervised learning. It is often seen that Unsupervised Learning is usually overlooked and people are more interested in supeervised learning. But as you can see, Unsupervised learning can be really powerful, insightful and cool. And the best thing is we do not need to do much and just have to infer charecteristics from the already segmented data.  \n\nNow moving onto the coolest bit, for which we all are here... Can we solve the MNIST problem through unsupervised learning?  \nLet's find out... *(SPOILER ALERT:- `YES`)*  \n\n# Model Creation\n\nBefore clustering, let's reduce the dimentionality of the digits using TSNE."},{"metadata":{"trusted":true},"cell_type":"code","source":"tsne = TSNE(n_components=3,\n            n_jobs=-1,\n            random_state=RANDOM_SEED)\ndigits_proj = tsne.fit_transform(train_df.drop(['label'], axis=1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now predicting the clusters using Gaussian Mixture Model."},{"metadata":{"trusted":true},"cell_type":"code","source":"gmm = GaussianMixture(\n    n_components=train_df.label.nunique(),\n    random_state=RANDOM_SEED)\nclusters = gmm.fit_predict(digits_proj)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Usually clustering also involves optimizing for number of clusters. But in this scenario we already have an idea regarding how manu clusters weneed the data to be split into, we will not be guessing/optimizing for the optimal number of clusters.  \n\nNow that we have out clusters, let's assign the labels to them. What we are going to do is we will take the mode value of the cluster and assign to all the members of that cluster."},{"metadata":{"trusted":true},"cell_type":"code","source":"labels = np.zeros_like(clusters)\nfor i in range(10):\n    mask = (clusters == i)\n    labels[mask] = mode(train_df.label[mask])[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's find the cluster number to actual number mapping so that we can apply this to test set..."},{"metadata":{"trusted":true},"cell_type":"code","source":"mapping_dict = {}\nfor i in range(10):\n    mask = (clusters == i)\n    mapping_dict[i] = mode(train_df.label[mask])[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(mapping_dict)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now let's check the classification metrics..."},{"metadata":{"trusted":true},"cell_type":"code","source":"acc = metrics.accuracy_score(train_df.label.values, labels)\nprint(f'Accuracy Score: {acc}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(confusion_matrix(train_df.label.values, labels), annot=True, fmt='g', cmap=\"YlGnBu\");","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(train_df.label.values, labels))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I know this score might not look impressive especially considering what other \"Supervised\" algorithms can do with this dataset.  \n\nBut let me put this in perpective...  \n\n**We are getting >85% accuracy WITHOUT EVEN LOOKING AT THE LABELS.**  \n\nHow does that sound? Not bad... Right?  \n\n**If you liked this notebook and use parts of it in you code, please upvote this kernel. It keeps me inspired to come-up with unique approaches like this one and share it with the community.**  \n\nThanks and happy kaggling!"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}