{"cells":[{"metadata":{},"cell_type":"markdown","source":"**What are we trying to do?**\n\nWe will be building a Fisher's Linear Discriminant from scratch, The accuracy score is not bad for a linear classifier.\nThis classifier works by trying to find the best decision boundry given that it would maximize separation between classes means while minimizing the within-class variance. \n\nThe image on the left shows a bad decision boundry while the one on the right shows a good one. Discriminant function performs dimensionality reduction, we are trying to find the line which if the data was projected on, would give us the maximum separation between classes and smallest within class variance.\n\n<img src=\"https://drive.google.com/uc?id=14RIvA5W5rE0rfJFapE37pM_xTH9VH5de\">\n\nWe will be using the One-versus-the-rest approach for class decisions."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom matplotlib import pyplot as plt\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Load the data**:\n\nJust separating the labels from the features, count the classes and features.\nStoring the indexes of every class for later use.\n"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# Load Training Images:\n\nim_train = np.loadtxt('../input/train.csv', delimiter=',', dtype=int, skiprows=1)\ntrain_labels = im_train[:, 0]\nim_train = im_train[:, 1:]\nnclasses = len(np.unique(train_labels))\nnfeatures = np.size(im_train, axis=1)\nclass_indexes = []\nfor i in range(nclasses):\n    class_indexes.append(np.argwhere(train_labels == i))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"im_train.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"SW, the within-class covariance matrix equation is as below (shouldn't be hard to understand if you break it down)\nxn are the data points, m1 is the mean of class 1 and m2 is the mean of class 2 (all other classes combined) \n<img src=\"https://drive.google.com/uc?id=1QrpVfz8g_i0aWIHSXbU9ssdyusdCxPUv\">"},{"metadata":{},"cell_type":"markdown","source":"The weights vector, w, which is orthogonal to the decision boundy and w0 (the bias) are equal to:\n\n<img src=\"https://drive.google.com/uc?id=1hJlV-v4nU0drdIA6VSmWga_qlBVkNmBd\">\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Initializing needed variables\nclass_means, other_class_means = np.empty((nclasses, nfeatures)), np.empty((nclasses, nfeatures))\nother_class = []\nSW_one, SW_two, SW = np.zeros((nclasses, nfeatures, nfeatures)), np.zeros((nclasses, nfeatures, nfeatures)), np.zeros((nclasses, nfeatures, nfeatures))\nW = np.zeros((nclasses, nfeatures, 1))\nW0 = np.zeros((nclasses))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using some vectorization and matrix properties we can avoid all the looping on the dataset to create SW_one and SW_two and end up with a model that does not actually take a long time to train."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculating SW, W & W0 #\nfor i in range(nclasses):\n    class_means[i] = np.mean(im_train[class_indexes[i]], axis=0)\n    other_class.append(np.delete(im_train, class_indexes[i], axis=0)) # one-versus-the-rest approach\n    other_class_means[i] = np.mean(other_class[i], axis=0)\n    between_class1 = np.subtract(im_train[class_indexes[i]].reshape(-1, nfeatures), \n                                 class_means[i])\n    SW_one[i] = between_class1.T.dot(between_class1)\n    between_class2 = np.subtract(other_class[i], other_class_means[i])\n    SW_two[i] = between_class2.T.dot(between_class2)\n    SW[i] = SW_one[i] + SW_two[i]\n    W[i] = np.dot(np.linalg.pinv(SW[i]), \n                  np.subtract(other_class_means[i], \n                              class_means[i]).reshape(-1, 1))\n    W0[i] = -0.5 * np.dot(W[i].T, (class_means[i] + other_class_means[i]))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(SW.shape)\nprint(W.shape)\nprint(W0.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Load the test data**:\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"im_test = np.loadtxt('../input/test.csv', delimiter=',', dtype=int, skiprows=1)\nim_test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Classification**:\n<img src=\"https://drive.google.com/uc?id=1kMuH5FEGLuGR41-OGxmvEmsff1gCqj0B\">\n\nWe would calculate Y for every image we want to classify, every Y is a 1D array having length = number of possible classes.\nThe prediction is simply the argmin of Y (index of smallest Y)"},{"metadata":{"trusted":true},"cell_type":"code","source":"Y = np.zeros((len(im_test), nclasses))\npredict = np.zeros((len(im_test)), dtype=int)\nfor j in range(len(im_test)):\n    for i in range(nclasses):\n        Y[j, i] = np.dot(W[i].T,  im_test[j]) + W0[i]\n    predict[j] = np.argmin(Y[j])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predict[:10]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Sounds good, let's try to plot the first 10 testing images too."},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(10):\n    plt.subplot(1, 10, i+1) # plot index can not be 0\n    plt.imshow(im_test[i].reshape(28, 28))\n    plt.axis('off')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So apparently it made 9 correct decision out of 10. This is very good for a linear discriminant."},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame({\"ImageId\": np.arange(1, len(im_test)+1), \"Label\": predict})\nsubmission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}