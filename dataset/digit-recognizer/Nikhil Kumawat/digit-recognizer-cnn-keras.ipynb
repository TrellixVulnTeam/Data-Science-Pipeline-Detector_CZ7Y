{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom keras.preprocessing.image import ImageDataGenerator \nfrom tensorflow.keras.layers import Input, Conv2D, Dense, Activation, Flatten, Dropout, GlobalMaxPooling2D, GlobalAveragePooling2D, MaxPooling2D, AveragePooling2D, BatchNormalization, LeakyReLU, Concatenate\nfrom tensorflow.keras.models import Sequential, Model, load_model\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras import regularizers, optimizers\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load the data\ntrain = pd.read_csv(\"../input/digit-recognizer/train.csv\")\ntest = pd.read_csv(\"../input/digit-recognizer/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Feature Scaling**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train = train[\"label\"]\ny_train = to_categorical(y_train, 10)\n\nX_train = train.drop(labels = [\"label\"], axis = 1) \n\nX_train = X_train / 255 #Dividing by maximum value\nX_test = test / 255\n\nX_train = X_train.values.reshape(-1,28,28,1)\nX_test = X_test.values.reshape(-1,28,28,1)\n\nprint(\"X_train.shape:\", X_train.shape)\nprint(\"y_train.shape\", y_train.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Splitting The given train dataset for train and validation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\nX_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size = 0.1, random_state=1337)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Building model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Fully connected CNN model\nmodel = Sequential(name = 'cnn_mnist')\n\nmodel.add(Conv2D(filters = 32, kernel_size = (3,3), padding = 'Same', activation ='relu', input_shape = (28,28,1)))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(filters = 32, kernel_size = (3,3), padding = 'Same', activation ='relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size=(2,2)))\nmodel.add(Dropout(0.2))\nmodel.add(Conv2D(filters = 64, kernel_size = (3,3), padding = 'Same', activation ='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(filters = 64, kernel_size = (3,3), padding = 'Same', activation ='relu'))\nmodel.add(MaxPooling2D(pool_size=(2,2)))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.3))\nmodel.add(Conv2D(filters = 128, kernel_size = (3,3), padding = 'Same', activation ='relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size=(2,2)))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.4))\nmodel.add(Flatten())\nmodel.add(Dense(128, activation = \"relu\"))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(10, activation = \"softmax\"))\n#opt = optimizers.Adam\nopt = optimizers.Adam()\nmodel.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Data augmentation**-to create more data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"datagen = ImageDataGenerator(\n                            rotation_range=15,\n                            width_shift_range=0.1,\n                            height_shift_range=0.1,\n                            zoom_range=0.1,\n                            horizontal_flip=False,\n                            vertical_flip=False\n                            )\ndatagen.fit(X_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rls = ReduceLROnPlateau(monitor='accuracy', mode = 'max', factor=0.5, min_lr=1e-7, verbose = 1, patience=5)\nes = EarlyStopping(monitor='accuracy', mode='max', verbose = 1, patience=50)\nmc = ModelCheckpoint('cnn_best_model.h5', monitor='accuracy', mode='max', verbose = 1, save_best_only=True)\n\ncallback_list = [rls, es, mc]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit_generator(datagen.flow(X_train, y_train, batch_size = 64),\n                                 validation_data = (X_valid, y_valid),\n                                 steps_per_epoch = X_train.shape[0] // 64, \n                                 epochs = 50, \n                                 verbose = 2,\n                                 callbacks = callback_list)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Lets Evaluate our model**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_model(history): \n    fig, axs = plt.subplots(1,2,figsize=(16,5)) \n    # summarize history for accuracy\n    axs[0].plot(history.history['accuracy'], 'c') \n    axs[0].plot(history.history['val_accuracy'],'m') \n    axs[0].set_title('Model Accuracy')\n    axs[0].set_ylabel('Accuracy') \n    axs[0].set_xlabel('Epoch')\n    axs[0].legend(['train', 'validate'], loc='upper left')\n    # summarize history for loss\n    axs[1].plot(history.history['loss'], 'c') \n    axs[1].plot(history.history['val_loss'], 'm') \n    axs[1].set_title('Model Loss')\n    axs[1].set_ylabel('Loss') \n    axs[1].set_xlabel('Epoch')\n    axs[1].legend(['train', 'validate'], loc='upper right')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_model(model.history)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"saved_model = load_model('cnn_best_model.h5')\n\ny_pred = saved_model.predict_classes(X_test, verbose=0)\n\nsub = pd.DataFrame({\"ImageId\": list(range(1, len(y_pred)+1)),\n                          \"Label\": y_pred})\n\nsub.to_csv(\"submission_digit_recognizer.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}