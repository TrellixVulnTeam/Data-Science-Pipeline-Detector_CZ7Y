{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Implementing Tensorflow from scratch on MNIST Dataset to get a clear understanding of how different optimizers, Activation functions behave.\n\n\n### Activation Functions used :\n- Sigmoid \n- ReLU\n\n\n#### Implementing methadologies like \n- Batch Normalization \n- Dropout \n\n\n#### 4 Models used throughout the kernel :\n- Model 1: input (784) - sigmoid(512) - sigmoid(128) - softmax(output 10)\n- Model 2: input (784) - ReLu(512) - ReLu(128) - sigmoid(output 10)\n- Model 3: Input - Sigmoid(BatchNormalization(512)) - Sigmoid(BatchNormalization(128))- Sigmoid(output)\n- Model 4: Input - ReLu(512) - Dropout - ReLu(128)- Dropout -Sigmoid(output)\n\n\n### Using ADAM optimizer and GradientDescent optimizer to illustrate the significant difference in convergance with respect to epochs. \n\n\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# import tensorflow as tf\nimport tensorflow.compat.v1 as tf\ntf.disable_v2_behavior() \nimport pandas as pd \nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Loading the dataset "},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"training_data = pd.read_csv(\"../input/digit-recognizer/train.csv\")\ntesting_data = pd.read_csv(\"../input/digit-recognizer/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"flat_labels = training_data.iloc[:,0].values\nprint(\" Image Labels :  \",flat_labels[0:20])\nimages = training_data.iloc[:,1:].values\nimages = images.astype(np.float32)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# this function is used to update the plots for each epoch and error \ndef plt_dynamic(x, y, y_1, ax,ticks, title, colors=['b']):\n    ax.plot(x, y, 'b', label=\"Train Loss\")\n    ax.plot(x, y_1, 'r', label=\"Test Loss\")\n    if len(x)==1:\n        plt.legend()\n        plt.title(title)\n    plt.yticks(ticks)\n    fig.canvas.draw()\n# the function updates loss dynamically \n\n# class Dataset(object):\n#     def __init__(self, data):\n#         self.rows = len(data.values)\n#         self.images = data.iloc[:,1:].values\n#         self.images = self.images.astype(np.float32)\n#         self.images = np.multiply(self.images, 1.0 / 255.0)\n#         self.labels = np.array([np.array([int(i == l) for i in range(10)]) for l in data.iloc[:,0].values]) #one-hot\n#         self.index_in_epoch = 0\n#         self.epoch = 0\n#     def next_batch(self, batch_size):\n#         start = self.index_in_epoch\n#         self.index_in_epoch += batch_size\n#         if self.index_in_epoch > self.rows:\n#             self.epoch += 1\n#             perm = np.arange(self.rows)\n#             np.random.shuffle(perm)\n#             self.images = self.images[perm]\n#             self.labels = self.labels[perm]\n#             #next epoch\n#             start = 0\n#             self.index_in_epoch = batch_size\n#         end = self.index_in_epoch\n#         return self.images[start:end] , self.labels[start:end]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"one_hot_encoder = OneHotEncoder(sparse = False)\nflat_labels = flat_labels.reshape(len(flat_labels), 1)\nlabels = one_hot_encoder.fit_transform(flat_labels)\nlabels = labels.astype(np.uint8)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print(flat_labels[3])\n# labels[3]\nx_train, x_test, y_train, y_test = train_test_split(images, labels, test_size = 0.2, random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Defining network Architecture:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Network Parameters\nn_hidden_1 = 512 # 1st layer number of neurons\nn_hidden_2 = 128 # 2nd layer number of neurons\nn_input = 784 # MNIST data input (img shape: 28*28)\nn_classes = 10 # MNIST total classes (0-9 digits)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = tf.placeholder(tf.float32, [None, 784])\ny_ = tf.placeholder(tf.float32, [None, 10])\n\n# keep_prob: we will be using these placeholders when we use dropouts, while testing model\nkeep_prob = tf.placeholder(tf.float32)\nkeep_prob_input = tf.placeholder(tf.float32)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Sigmoid with Xavier/Glorot Normal initialization.\n-  If we sample weights from a normal distribution N(0,σ) we satisfy this condition with σ=√(2/(ni+ni+1). \n- h1 =>  σ=√(2/(fan_in+fan_out+1) = 0.039  => N(0,σ) = N(0,0.039)\n- h2 =>  σ=√(2/(fan_in+fan_out+1) = 0.055  => N(0,σ) = N(0,0.055)\n- out =>  σ=√(2/(fan_in+fan_out+1) = 0.120  => N(0,σ) = N(0,0.120)\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nweights_sgd = {\n    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1],stddev=0.039, mean=0)),    #784x512 # sqrt(2/(784+512)) = 0.039\n    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2],stddev=0.055, mean=0)), #512x128 # sqrt(2/(512+128)) = 0.055\n    'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes],stddev=0.120, mean=0))  #128x10\n}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## ReLU with He Normal initialization\n- If we sample weights from a normal distribution N(0,σ) we satisfy this condition with σ=√(2/(ni). \n- h1 =>  σ=√(2/(fan_in+1) = 0.062  => N(0,σ) = N(0,0.062)\n- h2 =>  σ=√(2/(fan_in+1) = 0.125  => N(0,σ) = N(0,0.125)\n- out =>  σ=√(2/(fan_in+1) = 0.120  => N(0,σ) = N(0,0.120)\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"weights_relu = {\n    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1],stddev=0.062, mean=0)),    #784x512\n    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2],stddev=0.125, mean=0)), #512x128\n    'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes],stddev=0.120, mean=0))  #128x10\n}\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"biases = {\n    'b1': tf.Variable(tf.random_normal([n_hidden_1])),             #512x1\n    'b2': tf.Variable(tf.random_normal([n_hidden_2])),             #128x1\n    'out': tf.Variable(tf.random_normal([n_classes]))              #10x1\n}\n\n# Parameters\ntraining_epochs = 25\nlearning_rate = 0.001\nbatch_size = 250\ndisplay_step = 1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model 1: input (784) - sigmoid(512) - sigmoid(128) - softmax(output 10)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/multi_layer_perceptron_mnist.html\n# Create model\ndef multilayer_perceptron(x, weights, biases):\n    # Use tf.matmul instead of \"*\" because tf.matmul can change it's dimensions on the fly (broadcast)\n    print( 'x:', x.get_shape(), 'W[h1]:', weights['h1'].get_shape(), 'b[h1]:', biases['b1'].get_shape())        \n    \n    # Hidden layer with Sigmoid activation\n    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1']) #(x*weights['h1']) + biases['b1']\n    layer_1 = tf.nn.sigmoid(layer_1)\n    print( 'layer_1:', layer_1.get_shape(), 'W[h2]:', weights['h2'].get_shape(), 'b[h2]:', biases['b2'].get_shape())        \n    \n    # Hidden layer with Sigmoid activation\n    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2']) # (layer_1 * weights['h2']) + biases['b2'] \n    layer_2 = tf.nn.sigmoid(layer_2)\n    print( 'layer_2:', layer_2.get_shape(), 'W[out]:', weights['out'].get_shape(), 'b3:', biases['out'].get_shape())        \n    \n    # Output layer with Sigmoid activation\n    out_layer = tf.matmul(layer_2, weights['out']) + biases['out'] # (layer_2 * weights['out']) + biases['out']    \n    out_layer = tf.nn.sigmoid(out_layer)\n    print('out_layer:',out_layer.get_shape())\n\n    return out_layer","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model 1 with With Adam optimizer:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Since we are using sigmoid activations in hiden layers we will be using weights that are initalized as weights_sgd\ny_sgd = multilayer_perceptron(x, weights_sgd, biases)\n\n# https://www.tensorflow.org/api_docs/python/tf/nn/softmax_cross_entropy_with_logits\ncost_sgd = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = y_sgd, labels = y_))\n\n# https://github.com/amitmac/Question-Answering/issues/2\n# there are many optimizers available: https://www.tensorflow.org/versions/r1.2/api_guides/python/train#Optimizers \noptimizer_adam = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost_sgd)\noptimizer_sgdc = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost_sgd)\n\nwith tf.Session() as sess:\n    tf.global_variables_initializer().run()\n    fig,ax = plt.subplots(1,1)\n    ax.set_xlabel('epoch') ; ax.set_ylabel('Soft Max Cross Entropy loss')\n    xs, ytrs, ytes = [], [], []\n    for epoch in range(training_epochs):\n        train_avg_cost = 0.\n        test_avg_cost = 0.\n        total_batch = int(training_data.shape[0]/batch_size)\n        # Loop over all batches\n        for i in range(total_batch):\n            \n            batch_start = ( i * batch_size) % (x_train.shape[0] - batch_size)\n            batch_end = batch_start + batch_size \n            batch_xs = x_train[batch_start:batch_end]\n            batch_ys = y_train[batch_start:batch_end]\n            \n#             batch_xs, batch_ys = mnist.next_batch(batch_size)\n\n            # here c: corresponds to the parameter cost_sgd\n            # w : correspondse to the parameter weights_sgd\n            # c = sess.run() return the cost after every bath during train\n            # w = sess.run() return the weights that are modified after every batch through Back prop\n            # w is dict w = {'h1': updated h1 weight vector after the current batch,\n            #                'h2': updated h2 weight vector after the current batch, \n            #                'out': updated output weight vector after the current batch, \n            #                }\n            # you check these w matrix for every iteration, and check whats happening during back prop\n            #\n            # note: sess.run() returns parameter values based on the input parameters\n            # _, c, w = sess.run([optimizer_adam, cost_sgd,weights_sgd]) it returns three parameters\n            # _, c = sess.run([optimizer_adam, cost_sgd ]) it returns two parameters\n            # _ = sess.run([optimizer_adam]) it returns one paramter (for the input optimizer it return none)\n            # c = sess.run([cost_sgd]) it returns one paramter (for the input cost return error after the batch)\n\n            # feed_dict={x: batch_xs, y_: batch_ys} here x, y_ should be placeholders\n            # x, y_ are the input parameters on which the models gets trained\n\n            # here we use AdamOptimizer\n            _, c, w = sess.run([optimizer_adam, cost_sgd,weights_sgd], feed_dict={x: batch_xs, y_: batch_ys})\n            train_avg_cost += c / total_batch\n            c = sess.run(cost_sgd, feed_dict={x: x_test, y_: y_test})\n            test_avg_cost += c / total_batch\n\n        xs.append(epoch)\n        ytrs.append(train_avg_cost)\n        ytes.append(test_avg_cost)\n        plt_dynamic(xs, ytrs, ytes, ax, np.arange(1.3, 1.8, step=0.04), \"input-sigmoid(512)-sigmoid(128)-sigmoid(output)-AdamOptimizer\")\n\n        if epoch%display_step == 0:\n            print(\"Epoch:\", '%04d' % (epoch+1), \"train cost={:.9f}\".format(train_avg_cost), \"test cost={:.9f}\".format(test_avg_cost))\n    plt_dynamic(xs, ytrs, ytes, ax, np.arange(1.3, 1.8, step=0.04), \"input-sigmoid(512)-sigmoid(128)-sigmoid(output)-AdamOptimizer\")\n\n    # we are calculating the final accuracy on the test data\n    correct_prediction = tf.equal(tf.argmax(y_sgd,1), tf.argmax(y_,1))\n    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n    print(\"Accuracy:\", accuracy.eval({x: x_test, y_: y_test}))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Plotting violin plots of weights to performm a sanity check\n- cheking wheather weights are not being blown out of proprotion. "},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nh1_w = w['h1'].flatten().reshape(-1,1)\nh2_w = w['h2'].flatten().reshape(-1,1)\nout_w = w['out'].flatten().reshape(-1,1)\n\n\nfig = plt.figure()\nplt.subplot(1, 3, 1)\nplt.title(\"Weight matrix\")\nax = sns.violinplot(y=h1_w,color='b')\nplt.xlabel('Hidden Layer 1')\n\nplt.subplot(1, 3, 2)\nplt.title(\"Weight matrix \")\nax = sns.violinplot(y=h2_w, color='r')\nplt.xlabel('Hidden Layer 2 ')\n\nplt.subplot(1, 3, 3)\nplt.title(\"Weight matrix \")\nax = sns.violinplot(y=out_w,color='y')\nplt.xlabel('Output Layer ')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model 1 with GradientDescentOptimizer "},{"metadata":{"trusted":true},"cell_type":"code","source":"# We can now launch the model in an InteractiveSession\n\n# We first have to create an operation to initialize the variables we created:\n# https://github.com/amitmac/Question-Answering/issues/2\n\n# Note: make sure you initialize variables.\n\nwith tf.Session() as sess:\n    tf.global_variables_initializer().run()\n    fig,ax = plt.subplots(1,1)\n    ax.set_xlabel('epoch') ; ax.set_ylabel('Soft Max Cross Entropy loss')\n    xs, ytrs, ytes = [], [], []\n    for epoch in range(training_epochs):\n        train_avg_cost = 0.\n        test_avg_cost = 0.\n        total_batch = int(x_train.shape[0]/batch_size)\n\n        # Loop over all batches\n        for i in range(total_batch):\n            batch_start = ( i * batch_size) % (x_train.shape[0] - batch_size)\n            batch_end = batch_start + batch_size \n            batch_xs = x_train[batch_start:batch_end]\n            batch_ys = y_train[batch_start:batch_end]\n#             batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n\n            # here we use GradientDescentOptimizer\n            _, c, w = sess.run([optimizer_sgdc, cost_sgd, weights_sgd], feed_dict={x: batch_xs, y_: batch_ys})\n            train_avg_cost += c / total_batch\n            c = sess.run(cost_sgd, feed_dict={x: x_test, y_: y_test})\n            test_avg_cost += c / total_batch\n\n        xs.append(epoch)\n        ytrs.append(train_avg_cost)\n        ytes.append(test_avg_cost)\n        plt_dynamic(xs, ytrs, ytes, ax, np.arange(2, 2.6, step=0.05), \"input-sigmoid(512)-sigmoid(128)-sigmoid(output)-GradientDescentOptimizer\")\n\n        if epoch%display_step == 0:\n            print(\"Epoch:\", '%04d' % (epoch+1), \"train cost={:.9f}\".format(train_avg_cost), \"test cost={:.9f}\".format(test_avg_cost))\n    plt_dynamic(xs, ytrs, ytes, ax, np.arange(2, 2.6, step=0.05), \"input-sigmoid(512)-sigmoid(128)-sigmoid(output)-GradientDescentOptimizer\")\n\n    # we are calculating the final accuracy on the test data\n    correct_prediction = tf.equal(tf.argmax(y_sgd,1), tf.argmax(y_,1))\n    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n    print(\"Accuracy:\", accuracy.eval({x: x_test, y_: y_test}))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"h1_w = w['h1'].flatten().reshape(-1,1)\nh2_w = w['h2'].flatten().reshape(-1,1)\nout_w = w['out'].flatten().reshape(-1,1)\n\n\nfig = plt.figure()\nplt.subplot(1, 3, 1)\nplt.title(\"Weight matrix\")\nax = sns.violinplot(y=h1_w,color='b')\nplt.xlabel('Hidden Layer 1')\n\nplt.subplot(1, 3, 2)\nplt.title(\"Weight matrix \")\nax = sns.violinplot(y=h2_w, color='r')\nplt.xlabel('Hidden Layer 2 ')\n\nplt.subplot(1, 3, 3)\nplt.title(\"Weight matrix \")\nax = sns.violinplot(y=out_w,color='y')\nplt.xlabel('Output Layer ')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model 2: input (784) - ReLu(512) - ReLu(128) - sigmoid(output 10)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/multi_layer_perceptron_mnist.html\n# Create model\ndef multilayer_perceptron_relu(x, weights, biases):\n    # Use tf.matmul instead of \"*\" because tf.matmul can change it's dimensions on the fly (broadcast)\n    print( 'x:', x.get_shape(), 'W[h1]:', weights['h1'].get_shape(), 'b[h1]:', biases['b1'].get_shape())        \n    \n    # Hidden layer with ReLu activation\n    # https://en.wikipedia.org/wiki/Rectifier_(neural_networks)\n    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1']) #(x*weights['h1']) + biases['b1']\n    layer_1 = tf.nn.relu(layer_1)\n    print( 'layer_1:', layer_1.get_shape(), 'W[h2]:', weights['h2'].get_shape(), 'b[h2]:', biases['b2'].get_shape())        \n    \n    # Hidden layer with ReLu activation\n    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2']) # (layer_1 * weights['h2']) + biases['b2'] \n    layer_2 = tf.nn.relu(layer_2)\n    print( 'layer_2:', layer_2.get_shape(), 'W[out]:', weights['out'].get_shape(), 'b3:', biases['out'].get_shape())        \n    \n    # Output layer with Sigmoid activation\n    out_layer = tf.matmul(layer_2, weights['out']) + biases['out'] # (layer_2 * weights['out']) + biases['out']    \n    out_layer = tf.nn.sigmoid(out_layer)\n    print('out_layer:',out_layer.get_shape())\n\n    return out_layer","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model 2 with Adam optimizer:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Since we are using Relu activations in hiden layers we will be using weights that are initalized as weights_relu\nyrelu = multilayer_perceptron_relu(x, weights_relu, biases)\n\n# https://www.tensorflow.org/api_docs/python/tf/nn/softmax_cross_entropy_with_logits\ncost_relu = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = yrelu, labels = y_))\n# https://github.com/amitmac/Question-Answering/issues/2\n# there are many optimizers available: https://www.tensorflow.org/versions/r1.2/api_guides/python/train#Optimizers \noptimizer_relu_adam = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost_relu)\noptimizer_relu_sgdc = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost_relu)\nwith tf.Session() as sess:\n    tf.global_variables_initializer().run()\n    \n    fig,ax = plt.subplots(1,1)\n    ax.set_xlabel('epoch') ; ax.set_ylabel('Soft Max Cross Entropy loss')\n    xs, ytrs, ytes = [], [], []\n    for epoch in range(training_epochs):\n        train_avg_cost = 0.\n        test_avg_cost = 0.\n        total_batch = int(x_train.shape[0]/batch_size)\n\n        # Loop over all batches\n        for i in range(total_batch):\n            batch_start = ( i * batch_size) % (x_train.shape[0] - batch_size)\n            batch_end = batch_start + batch_size \n            batch_xs = x_train[batch_start:batch_end]\n            batch_ys = y_train[batch_start:batch_end]\n            \n\n            # here we use AdamOptimizer\n            _, c, w = sess.run([optimizer_relu_adam, cost_relu, weights_relu], feed_dict={x: batch_xs, y_: batch_ys})\n            train_avg_cost += c / total_batch\n            c = sess.run(cost_relu, feed_dict={x: x_test, y_: y_test})\n            test_avg_cost += c / total_batch\n\n        xs.append(epoch)\n        ytrs.append(train_avg_cost)\n        ytes.append(test_avg_cost)\n        plt_dynamic(xs, ytrs, ytes, ax, np.arange(1.3, 1.8, step=0.04), \"input-ReLu(512)-ReLu(128)-sigmoid(output)-AdamOptimizer\")\n\n        if epoch%display_step == 0:\n            print(\"Epoch:\", '%04d' % (epoch+1), \"train cost={:.9f}\".format(train_avg_cost), \"test cost={:.9f}\".format(test_avg_cost))\n\n    # plot final results\n    plt_dynamic(xs, ytrs, ytes, ax,np.arange(1.3, 1.8, step=0.04), \"input-ReLu(512)-ReLu(128)-sigmoid(output)-AdamOptimizer\")\n\n    # we are calculating the final accuracy on the test data\n    correct_prediction = tf.equal(tf.argmax(yrelu,1), tf.argmax(y_,1))\n    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n    print(\"Accuracy:\", accuracy.eval({x: x_test, y_: y_test}))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"h1_w = w['h1'].flatten().reshape(-1,1)\nh2_w = w['h2'].flatten().reshape(-1,1)\nout_w = w['out'].flatten().reshape(-1,1)\n\n\nfig = plt.figure()\nplt.subplot(1, 3, 1)\nplt.title(\"Weight matrix\")\nax = sns.violinplot(y=h1_w,color='b')\nplt.xlabel('Hidden Layer 1')\n\nplt.subplot(1, 3, 2)\nplt.title(\"Weight matrix \")\nax = sns.violinplot(y=h2_w, color='r')\nplt.xlabel('Hidden Layer 2 ')\n\nplt.subplot(1, 3, 3)\nplt.title(\"Weight matrix \")\nax = sns.violinplot(y=out_w,color='y')\nplt.xlabel('Output Layer ')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model 2 with GradientDescentOptimmizer "},{"metadata":{"trusted":true},"cell_type":"code","source":"# We can now launch the model in an InteractiveSession\n\n# We first have to create an operation to initialize the variables we created:\n# https://github.com/amitmac/Question-Answering/issues/2\n\n# Note: make sure you initialize variables after AdamOptimizer\n\nwith tf.Session() as sess:\n    tf.global_variables_initializer().run()\n    fig,ax = plt.subplots(1,1)\n    ax.set_xlabel('epoch') ; ax.set_ylabel('Soft Max Cross Entropy loss')\n    xs, ytrs, ytes = [], [], []\n    for epoch in range(training_epochs):\n        train_avg_cost = 0.\n        test_avg_cost = 0.\n        total_batch = int(x_train.shape[0]/batch_size)\n\n        # Loop over all batches\n        for i in range(total_batch):\n            batch_start = ( i * batch_size) % (x_train.shape[0] - batch_size)\n            batch_end = batch_start + batch_size \n            batch_xs = x_train[batch_start:batch_end]\n            batch_ys = y_train[batch_start:batch_end]\n\n            # here we use GradientDescentOptimizer\n            _, c, w = sess.run([optimizer_relu_sgdc, cost_relu, weights_relu], feed_dict={x: batch_xs, y_: batch_ys})\n            train_avg_cost += c / total_batch\n            c = sess.run(cost_relu, feed_dict={x: x_test, y_: y_test})\n            test_avg_cost += c / total_batch\n\n        xs.append(epoch)\n        ytrs.append(train_avg_cost)\n        ytes.append(test_avg_cost)\n        plt_dynamic(xs, ytrs, ytes, ax, np.arange(1.5, 2.4, step=0.05), \"input-ReLu(512)-ReLu(128)-sigmoid(output)-GradientDescentOptimizer\")\n\n        if epoch%display_step == 0:\n            print(\"Epoch:\", '%04d' % (epoch+1), \"train cost={:.9f}\".format(train_avg_cost), \"test cost={:.9f}\".format(test_avg_cost))\n\n    # plot final results\n    plt_dynamic(xs, ytrs, ytes, ax, np.arange(1.5, 2.4, step=0.05), \"input-ReLu(512)-ReLu(128)-sigmoid(output)-GradientDescentOptimizer\")\n\n    # we are calculating the final accuracy on the test data\n    correct_prediction = tf.equal(tf.argmax(yrelu,1), tf.argmax(y_,1))\n    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n    print(\"Accuracy:\", accuracy.eval({x: x_test, y_: y_test}))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"h1_w = w['h1'].flatten().reshape(-1,1)\nh2_w = w['h2'].flatten().reshape(-1,1)\nout_w = w['out'].flatten().reshape(-1,1)\n\n\nfig = plt.figure()\nplt.subplot(1, 3, 1)\nplt.title(\"Weight matrix\")\nax = sns.violinplot(y=h1_w,color='b')\nplt.xlabel('Hidden Layer 1')\n\nplt.subplot(1, 3, 2)\nplt.title(\"Weight matrix \")\nax = sns.violinplot(y=h2_w, color='r')\nplt.xlabel('Hidden Layer 2 ')\n\nplt.subplot(1, 3, 3)\nplt.title(\"Weight matrix \")\nax = sns.violinplot(y=out_w,color='y')\nplt.xlabel('Output Layer ')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model 3: Input - Sigmoid(BatchNormalization(512)) - Sigmoid(BatchNormalization(128))- Sigmoid(output)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# https://www.tensorflow.org/api_docs/python/tf/nn/batch_normalization\n# https://r2rt.com/implementing-batch-normalization-in-tensorflow.html\nepsilon = 1e-3\ndef multilayer_perceptron_batch(x, weights, biases):\n    # Use tf.matmul instead of \"*\" because tf.matmul can change it's dimensions on the fly (broadcast)\n    print( 'x:', x.get_shape(), 'W[h1]:', weights['h1'].get_shape(), 'b[h1]:', biases['b1'].get_shape())        \n    \n    ############################################################\n    # Hidden layer with Sigmoid activation and batch normalization\n    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1']) #(x*weights['h1']) + biases['b1']\n    \n    # https://www.tensorflow.org/api_docs/python/tf/nn/moments\n    # Calculate the mean and variance of x.\n    batch_mean_1, batch_var_1 = tf.nn.moments(layer_1,[0])\n    \n    scale_1 = tf.Variable(tf.ones([n_hidden_1]))\n    beta_1 = tf.Variable(tf.zeros([n_hidden_1]))\n    \n    # https://www.tensorflow.org/api_docs/python/tf/nn/batch_normalization\n    layer_1 = tf.nn.batch_normalization(layer_1, batch_mean_1, batch_var_1, beta_1, scale_1, epsilon)\n    layer_1 = tf.nn.sigmoid(layer_1)\n    \n    print( 'layer_1:', layer_1.get_shape(), 'W[h2]:', weights['h2'].get_shape(), 'b[h2]:', biases['b2'].get_shape())        \n    \n    #####################################################################################\n    \n    # Hidden layer with Sigmoid activation and batch normalization\n    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2']) # (layer_1 * weights['h2']) + biases['b2'] \n    \n    # https://www.tensorflow.org/api_docs/python/tf/nn/moments\n    # Calculate the mean and variance of x.\n    batch_mean_2, batch_var_2 = tf.nn.moments(layer_2, [0])\n    \n    scale_2 = tf.Variable(tf.ones([n_hidden_2]))\n    beta_2 = tf.Variable(tf.zeros([n_hidden_2]))\n    \n    layer_2 = tf.nn.batch_normalization(layer_2, batch_mean_2, batch_var_2, beta_2, scale_2, epsilon)\n    layer_2 = tf.nn.sigmoid(layer_2)\n    print( 'layer_2:', layer_2.get_shape(), 'W[out]:', weights['out'].get_shape(), 'b3:', biases['out'].get_shape())        \n    \n    ######################################################################################\n    \n    # output layer with Sigmoid activation \n    out_layer = tf.matmul(layer_2, weights['out']) + biases['out'] # (layer_2 * weights['out']) + biases['out']    \n    out_layer = tf.nn.sigmoid(out_layer)\n    print('out_layer:',out_layer.get_shape())\n\n    return out_layer","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model 3 with AdamOptimizer"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Since we are using sigmoid activations in hiden layers we will be using weights that are initalized as weights_sgd\nybatch = multilayer_perceptron_batch(x, weights_sgd, biases)\n\n# https://www.tensorflow.org/api_docs/python/tf/nn/softmax_cross_entropy_with_logits\ncost_batch = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = ybatch, labels = y_))\n# https://github.com/amitmac/Question-Answering/issues/2\n# there are many optimizers available: https://www.tensorflow.org/versions/r1.2/api_guides/python/train#Optimizers \noptimizer_batch_adam = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost_batch)\noptimizer_batch_sgdc = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost_batch)\nwith tf.Session() as sess:\n    tf.global_variables_initializer().run()\n    fig,ax = plt.subplots(1,1)\n    ax.set_xlabel('epoch') ; ax.set_ylabel('Soft Max Cross Entropy loss')\n    xs, ytrs, ytes = [], [], []\n    for epoch in range(training_epochs):\n        train_avg_cost = 0.\n        test_avg_cost = 0.\n        total_batch = int(x_train.shape[0]/batch_size)\n\n        # Loop over all batches\n        for i in range(total_batch):\n            batch_start = ( i * batch_size) % (x_train.shape[0] - batch_size)\n            batch_end = batch_start + batch_size \n            batch_xs = x_train[batch_start:batch_end]\n            batch_ys = y_train[batch_start:batch_end]\n\n            # here we use AdamOptimizer\n            _, c, w = sess.run([optimizer_batch_adam, cost_batch, weights_sgd], feed_dict={x: batch_xs, y_: batch_ys})\n            train_avg_cost += c / total_batch\n            c = sess.run(cost_batch, feed_dict={x: x_test, y_: y_test})\n            test_avg_cost += c / total_batch\n\n        xs.append(epoch)\n        ytrs.append(train_avg_cost)\n        ytes.append(test_avg_cost)\n        plt_dynamic(xs, ytrs, ytes, ax, np.arange(1.3, 1.8, step=0.04), \"input-Sigmoid(BN(512))-Sigmoid(BN(128))-Sigmoid(output)-AdamOptimizer\")\n\n        if epoch%display_step == 0:\n            print(\"Epoch:\", '%04d' % (epoch+1), \"train cost={:.9f}\".format(train_avg_cost), \"test cost={:.9f}\".format(test_avg_cost))\n\n    # plot final results\n    plt_dynamic(xs, ytrs, ytes, ax, np.arange(1.3, 1.8, step=0.04), \"input-Sigmoid(BN(512))-Sigmoid(BN(128))-Sigmoid(output)-AdamOptimizer\")\n\n    # we are calculating the final accuracy on the test data\n    correct_prediction = tf.equal(tf.argmax(ybatch,1), tf.argmax(y_,1))\n    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n    print(\"Accuracy:\", accuracy.eval({x: x_test, y_: y_test}))\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"h1_w = w['h1'].flatten().reshape(-1,1)\nh2_w = w['h2'].flatten().reshape(-1,1)\nout_w = w['out'].flatten().reshape(-1,1)\n\n\nfig = plt.figure()\nplt.subplot(1, 3, 1)\nplt.title(\"Weight matrix\")\nax = sns.violinplot(y=h1_w,color='b')\nplt.xlabel('Hidden Layer 1')\n\nplt.subplot(1, 3, 2)\nplt.title(\"Weight matrix \")\nax = sns.violinplot(y=h2_w, color='r')\nplt.xlabel('Hidden Layer 2 ')\n\nplt.subplot(1, 3, 3)\nplt.title(\"Weight matrix \")\nax = sns.violinplot(y=out_w,color='y')\nplt.xlabel('Output Layer ')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model 4: Input - ReLu(512) - Dropout - ReLu(128)- Dropout -Sigmoid(output)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/multi_layer_perceptron_mnist.html\n# Create model\ndef multilayer_perceptron_dropout(x, weights, biases):\n    # Use tf.matmul instead of \"*\" because tf.matmul can change it's dimensions on the fly (broadcast)\n    print( 'x:', x.get_shape(), 'W[h1]:', weights['h1'].get_shape(), 'b[h1]:', biases['b1'].get_shape())        \n    # we are adding a drop out layer after input layers with parameter keep_prob_input\n    \n    # Hidden layer with ReLu activation\n    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1']) #(x*weights['h1']) + biases['b1']\n    layer_1 = tf.nn.relu(layer_1)\n    # we are adding a drop out layer after the first hidden layer with parameter keep_prob\n    layer_1_drop = tf.nn.dropout(layer_1, keep_prob)\n    \n    print( 'layer_1:', layer_1.get_shape(), 'W[h2]:', weights['h2'].get_shape(), 'b[h2]:', biases['b2'].get_shape())        \n    \n    # Hidden layer with ReLu activation\n    layer_2 = tf.add(tf.matmul(layer_1_drop, weights['h2']), biases['b2']) # (layer_1 * weights['h2']) + biases['b2'] \n    layer_2 = tf.nn.relu(layer_2)\n    # we are adding a drop out layer after the first hidden layer with parameter keep_prob\n    layer_2_drop = tf.nn.dropout(layer_2, keep_prob)\n    print( 'layer_2:', layer_2.get_shape(), 'W[out]:', weights['out'].get_shape(), 'b3:', biases['out'].get_shape())        \n    \n    # Output layer with Sigmoid activation\n    out_layer = tf.matmul(layer_2_drop, weights['out']) + biases['out'] # (layer_2 * weights['out']) + biases['out']    \n    out_layer = tf.nn.sigmoid(out_layer)\n    print('out_layer:',out_layer.get_shape())\n\n    return out_layer","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model 4 with AdamOptimizer"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Since we are using Relu activations in hiden layers we will be using weights that are initalized as weights_relu\nydrop = multilayer_perceptron_dropout(x, weights_relu, biases)\n\n# https://www.tensorflow.org/api_docs/python/tf/nn/softmax_cross_entropy_with_logits\ncost_drop = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = ydrop, labels = y_))\n# https://github.com/amitmac/Question-Answering/issues/2\n# there are many optimizers available: https://www.tensorflow.org/versions/r1.2/api_guides/python/train#Optimizers \noptimizer_drop_adam = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost_drop)\noptimizer_drop_sgdc = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost_drop)\n\nwith tf.Session() as sess:\n    tf.global_variables_initializer().run()\n    fig,ax = plt.subplots(1,1)\n    ax.set_xlabel('epoch') ; ax.set_ylabel('Soft Max Cross Entropy loss')\n    xs, ytrs, ytes = [], [], []\n    for epoch in range(training_epochs):\n        train_avg_cost = 0.\n        test_avg_cost = 0.\n        total_batch = int(x_train.shape[0]/batch_size)\n\n        # Loop over all batches\n        for i in range(total_batch):\n            \n            batch_start = ( i * batch_size) % (x_train.shape[0] - batch_size)\n            batch_end = batch_start + batch_size \n            batch_xs = x_train[batch_start:batch_end]\n            batch_ys = y_train[batch_start:batch_end]\n\n            \n            # here we use AdamOptimizer\n            _, c, w = sess.run([optimizer_drop_adam, cost_drop, weights_relu], feed_dict={x: batch_xs, y_: batch_ys, keep_prob: 0.5})\n            train_avg_cost += c / total_batch\n            c = sess.run(cost_drop, feed_dict={x: x_test, y_: y_test,  keep_prob: 1.0})\n            test_avg_cost += c / total_batch\n\n        xs.append(epoch)\n        ytrs.append(train_avg_cost)\n        ytes.append(test_avg_cost)\n        plt_dynamic(xs, ytrs, ytes, ax,np.arange(1, 1.8, step=0.05), \"input-ReLu(512)-Dropout-ReLu(128)-Dropout-Sigmoid(output)-AdamOptimizer\")\n\n        if epoch%display_step == 0:\n            print(\"Epoch:\", '%04d' % (epoch+1), \"train cost={:.9f}\".format(train_avg_cost), \"test cost={:.9f}\".format(test_avg_cost))\n\n    # plot final results\n    plt_dynamic(xs, ytrs, ytes, ax,np.arange(1, 1.8, step=0.05), \"input-ReLu(512)-Dropout-ReLu(128)-Dropout-Sigmoid(output)-AdamOptimizer\")\n\n    # we are calculating the final accuracy on the test data\n    correct_prediction = tf.equal(tf.argmax(ydrop,1), tf.argmax(y_,1))\n    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n    print(\"Accuracy:\", accuracy.eval({x: x_test, y_: y_test, keep_prob: 1.0 }))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}