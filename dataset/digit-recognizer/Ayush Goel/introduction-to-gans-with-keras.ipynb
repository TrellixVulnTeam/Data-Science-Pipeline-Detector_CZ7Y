{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Updates\n1. Now added GPU. Improves total training time from 4 hrs to 13 mins!\n2. Visualising GAN output after every epoch of training to see how the model is doing over time.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Introduction\n\nMaking Classifiers on the MNIST Database is fun, and seeing your model recognize digits as well as you is quite cool. But even cooler is when your model can write! In this notebook, I will be making 2 different GANs: FCGAN (Fully Connected GAN), as well as a DCGAN (Deep convolution GAN). \n\nI hope you find this tutorial helpful. I have tried to give a step by step explanation for each line of code. Feel free to fork the notebook and change the database to see your model create other pictures\n\n* Also, this article was a real help for this: https://medium.com/analytics-vidhya/implementing-a-gan-in-keras-d6c36bc6ab5f, so make sure to show your appreciation there as well.\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# A Simple Understanding of GANs\n\nGAN stands for Generative Adversarial Network, and they are used for generating images. \n\n![gan_example.png](attachment:gan_example.png)\n\nAs seen form the picture, a GAN has essentially 2 parts:\n\n1. The Generator: This generator takes in a noisy vector as input (random numbers), and tries to generate a picture. Obviously, the intitial images will be noise, so how do we train this network? That's where the discriminator comes in.\n\n2. The Discriminator: The discriminator takes an image as input, and simply tries to identify whether it is fake (generated by the GAN) or real (from our dataset of images).\n\nNow we essentially set up a game between the two, where each tries to outperform the other.\n\nThe Discriminator tries to do its best job in catching GANs, so even small deformations in the image will be noticed by this.\nThe Generator tries to fool the discriminator, in turn making more and more realistic images.\n\nAs the two keep learning, both get better. Once we are done training, we just take the generator part of the model, and feed it a random noise vector, and it generates realistic images!\n\nI am using the MNIST (digit and fashion) database to keep the training simple. In practice, you could use other kinds of datasets as well.\n\nAlso, there are lots of different GANs for different kinds of tasks. Below are a few:\n\n\n1. FCGAN - This is just a simple GAN (described above), where you use Dense layers and standard sequential models for generation. These are not very powerful. I will cover this in this notebook.\n\n2. DCGAN - This is another simple GAN, which involves using convolutional layers in the GAN, and these are much more powerful at generating images. I will cover this as well.\n\n3. Cycle GAN - Cycle GANs are used when you want to take an image as input, and morph a particular part of it. For example, if we want to convert all apples in an image to oranges, we would use a cycle GAN. It is called a cycle GAN as the GAN has an extra part where it has to reconstruct the original image. This is done purely so that the GAN only changes the apples to oranges and not the entire background, etc.\n\n4. Sytle GAN - This was developed by NVIDIA to make super realistic faces (and cars and bedrooms). If you check their results, you would be suprised at how realistic they are!). You can even use their pre trained model to develop some results!\n\nAnd the list goes on... Lot's of interesting stuff can be done with GANs, and even dangerous things (such as deepfaking videos of political leaders). But I won't get into that. \n\nTo get a better understanding of the loss function involved and the backpropagation changes, have a look at this lecture: \nhttps://youtu.be/ANszao6YQuM?list=PLoROMvodv4rOABXSygHTsbvUz4G_YQhOb&t=2766\n\n","attachments":{"gan_example.png":{"image/png":"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"}},"execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Now that we understand GANs, let's move on. First, let's import some libraries (there are other imports in other code blocks), and also set up some constants. I will explain each line of code, step by step with comments.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"First let's see if the GPU is enabled.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nprint(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's make sure tensorflow is doign all of it's operations on the GPU, and not the CPU","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"tf.debugging.set_log_device_placement(True)\n\n# Create some tensors\na = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\nb = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\nc = tf.matmul(a, b)\n\nprint(c)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Great, so now the GPU is working, and should speed up our computations.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Initialising Hyper Parameters\n\n#Uncomment one of the two below lines. For some reason, on some computers the first one works, while on others the second.\nfrom tensorflow.keras.optimizers import Adam\n#from keras.optimizers import adam\nimport tensorflow as tf\n\nimport numpy as np\nimport keras\n\nnp.random.seed(10)  #for consistency of random numbers and our images\n\nnoise_dim = 100  # input dimension of random vector - the vector that goes into the generator\n\nbatch_size = 16   #How many images do we want to include in each batch\nsteps_per_epoch = 3750  #How many steps do we want to take per iteration of our training set (number of batches)\nepochs = 10      #How many iterations of our training set do we want to do.\n\n#change the below values to the dimensions of your image. The channels number refers to the number of colors\nimg_rows, img_cols, channels = 28, 28, 1\n\n#These are the recommended values for the optimizer\noptimizer = Adam(0.0002, 0.5)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#importing mnist dataset (dataset to mimick), also reshaping the data for preprocessing\n\n#You can import the mnist digit dataset, or the fashion MNIST dataset. For FCGAN, mnist digit works better because it is simpler than the fashion dataset.\nfrom keras.datasets import mnist\n#from keras.datasets import fashion_mnist\n\nimport os\n\n(x_train, y_train), (x_tet, y_test) = mnist.load_data()     #loading data\n\nx_train = (x_train.astype(np.float32) - 127.5) / 127.5      #Nomralizing the data so that it is from -1 to 1. NNs converge faster this way\n\nx_train = x_train.reshape(-1, img_rows*img_cols*channels)    #The data is in an image style format, so we need to reshape it into a vector.\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# FCGAN Implementation\n\nAn FCGAN is the same thing that I described above. First we will create a generator. Here I am just using 4 dense layers. The activation we will be using for the hidden layers will be LeakyReLU with a negative slope of 0.2 (The original paper recommended this activation).\n\nRemember that the output layer has to have a tanh activation and not sigmoid because pixel values are from -1 to 1 (we normalized the data), not 0 to 1.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Starting to create FCGAN (fully connected GAN)\n\n#creating generator portion of the GAN\n\n\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers.advanced_activations import LeakyReLU\n\ndef create_generator():\n    generator = Sequential()\n\n    generator.add(Dense(256, input_dim=noise_dim))\n    generator.add(LeakyReLU(0.2))\n    \n    generator.add(Dense(512))\n    generator.add(LeakyReLU(0.2))\n    \n    generator.add(Dense(1024))\n    generator.add(LeakyReLU(0.2))\n    \n    generator.add(Dense(img_rows*img_cols*channels, activation='tanh'))  #The output layer has the same number of neurons as pixels in the image, because each neuron produces the color value for each pixel\n    \n    generator.compile(loss='binary_crossentropy', optimizer=optimizer) # A standard cross entropy loss would work, because the feedback it is getting is whether it fooled the discriminator or not.\n    \n    return generator\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The discriminator will also use the same format, except the number of neurons will be different. This is not a very bulky generator just to keep the training quicker.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#creating the discriminator for the GAN\n\ndef create_discriminator():\n    discriminator = Sequential()\n    \n    discriminator.add(Dense(1024, input_dim=img_rows*img_cols*channels))\n    discriminator.add(LeakyReLU(0.2))\n    \n    discriminator.add(Dense(512))\n    discriminator.add(LeakyReLU(0.2))\n    \n    discriminator.add(Dense(256))\n    discriminator.add(LeakyReLU(0.2))\n    \n    discriminator.add(Dense(1, activation='sigmoid'))  #sigmoid activation as output is 0/1 fake/real\n    \n    discriminator.compile(loss='binary_crossentropy', optimizer = optimizer)  #Binary Cross entropy loss as the discriminator has to try and classify all images (real or fake) in the right category\n    \n    return discriminator","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In this step, we will combine the generator and the discriminator. The reason is that the loss (feedback on whether the generator was good) has to flow from the discriminator. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#combining the generator and discriminator to make a single large gan (loss and derivatives have to flow from discriminator to generator)\n\nfrom keras.layers import Input\nfrom keras.models import Model\n\ndiscriminator = create_discriminator()   #Creating the discriminator using the function\ngenerator = create_generator()           #Creating the generator using the function\n\ndiscriminator.trainable = False          #We will set this to false, so that when we train the entire GAN together, only the generator part will be trained. We will train the discriminator separately. See below code block for more details.\n\ngan_input = Input(shape=(noise_dim,))    #We set the input of the whole model as the noise vector that the generator takes in as input. This is because the generator first has to develop an image for training to start.\nfake_image = generator(gan_input)        #Image generated by GAN for this noise vector is stored in fake_image\n\ngan_output = discriminator(fake_image)   #The output of the GAN is the discriminator\n\ngan = Model(gan_input, gan_output)       #Finally putting the generator and discriminator together\ngan.compile(loss='binary_crossentropy', optimizer=optimizer)  # Using binary cross entropy loss (same as discriminator loss)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here is where we finally train the GAN. The first for loop is meant for the number of epochs (iterations over training set). The second for loop is to split the training set into the number of batches we chose. I have also printed the loss for the discriminator and generator after every epoch.\n\nNote that the below code block is quite bulky and takes a long time to run.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Code for plotting results\nimport matplotlib.pyplot as plt\n\ndef show_images(noise, size_fig):\n    generated_images = generator.predict(noise)   #Create the images from the GAN.\n    plt.figure(figsize=size_fig)\n    \n    for i, image in enumerate(generated_images):\n        plt.subplot(size_fig[0], size_fig[1], i+1)\n        if channels == 1:\n            plt.imshow(image.reshape((img_rows, img_cols)), cmap='gray')    #If the image is grayscale, as in our case, then we will reshape the output in the following way.\n                                                                            #Also, we set the coloring to grayscale so that it doesn't look like it came out of an infrared camera :)\n        else:\n            plt.imshow(image.reshape((img_rows, img_cols, channels)))\n        plt.axis('off')\n    \n    plt.tight_layout()   #Tight layout so that all of the generated images form a nice grid\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"for epoch in range(epochs):         #iterate the dataset for the number of epochs\n    for batch in range(steps_per_epoch):    #for the number of batches we wanted to create\n        noise = np.random.normal(0, 1, size=(batch_size, noise_dim))   #We generate a new noise vector to feed the generator before every training iteration\n        fake_x = generator.predict(noise)        #The image the generator develops for the noise vector we created above\n\n        real_x = x_train[np.random.randint(0, x_train.shape[0], size=batch_size)]  #We won't use all real images from our dataset at once, we will only select a random sample of images\n        \n        x = np.concatenate((real_x, fake_x))    #making the x dataset for the discriminator. This includes a mix of real and fake examples for the discriminator to correctly classify\n\n        disc_y = np.zeros(2*batch_size)\n        disc_y[:batch_size] = 0.9\n\n        d_loss = discriminator.train_on_batch(x, disc_y)   #We are training the discriminator separately. Remember, we set trainable = false when adding it to the GAN, so that when we train the GAN, we only train the generator. Hence this extra step\n\n        y_gen = np.ones(batch_size)\n        g_loss = gan.train_on_batch(noise, y_gen)       #Now we train the entire GAN. But since the discriminator can't be trained, only the generator is trained in this step.\n\n    print(f'Epoch: {epoch + 1} \\t Discriminator Loss: {d_loss} \\t\\t Generator Loss: {g_loss}')\n    noise = np.random.normal(0, 1, size=(25, noise_dim))\n    show_images(noise, (5, 5))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finally, we finished training the GAN. We can see that over time, the GAN is definitely getting better. Blurry digits are becoming rarer, and the rest of the images are also getting sharper. Great! Let's just plot the results once again but with more images.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"noise = np.random.normal(0, 1, size=(100, noise_dim))\nshow_images(noise)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Wow. The images do look really good. However, some of the aprts of numbers (like the 8) are slightly faded in some regions. This is partly because of how many epochs, but also because we are using an FCGAN. Using a DCGAN would give much better results... which is what we are going to do next!","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# DCGAN Implementation","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"DCGANs are extremely similar to FCGANs. The only difference is that we add convolutional layers into the generator and the discriminator. As we saw in these datasets already, convolutional neural networks can better understand images than standard NNs. We are going to use this same understanding here so that we can generate much better and more clear images. To show the full power of the DCGAN, I will be switching over to the fashion MNIST database, but you can use the MNIST digit recognizer database as well.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Choose from one of the below datasets\nfrom keras.datasets import fashion_mnist\n#from keras.datasets import mnist\n\nimport os\n\n(x_train, y_train), (x_tet, y_test) = fashion_mnist.load_data()   #Load the data\n\nx_train = (x_train.astype(np.float32) - 127.5) / 127.5       #Normalize the images again so that the pixel value is from -1 to 1\n\nx_train = x_train.reshape(-1, img_rows, img_cols, channels)  #Reshaping the data into a more NN friendly format","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ok, so now we are going to create the generator for the DCGAN. Instead of all of the dense layers, we will be using Conv2D and Conv2D Transpose layers. Also, you can't just add new Conv2D Transpose layers like you can for the FCGAN, since the dimensions would change and the output won't be the same resolution as the image, you would need to reshape the data accordingly.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.initializers import RandomNormal\nfrom keras.layers import Dense, Conv2D, Conv2DTranspose, Reshape\nfrom keras.layers.advanced_activations import LeakyReLU\nfrom keras.models import Sequential\n\n\ndef create_generator_cgan():\n    generator = Sequential()\n    \n    d = 7\n    generator.add(Dense(d*d*256, kernel_initializer=RandomNormal(0, 0.02), input_dim=noise_dim))\n    generator.add(LeakyReLU(0.2))     #We are going to use the same leaky relu activation function as the FCGAN.\n    \n    generator.add(Reshape((d, d, 256)))\n    \n    generator.add(Conv2DTranspose(128, (4, 4), strides=2, padding='same', kernel_initializer=RandomNormal(0, 0.02)))\n    generator.add(LeakyReLU(0.2))\n\n    generator.add(Conv2DTranspose(128, (4, 4), strides=2, padding='same', kernel_initializer=RandomNormal(0, 0.02)))\n    generator.add(LeakyReLU(0.2))\n    \n\n    \n    generator.add(Conv2D(channels, (3, 3), padding='same', activation='tanh', kernel_initializer=RandomNormal(0, 0.02)))  #Remember that the final activation has to be tanh, since pixel values go from -1 to 1\n    \n    generator.compile(loss='binary_crossentropy', optimizer=optimizer)    #The loss doesn't change when you use convolutional layers\n    return generator","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we are going to make a discriminator. Since we are using convolutional layers, this model is going to look very similar to a standard Convolutional NN. The difference is that instead of classifying the class of the image, we are going to be classifying whether an image is real or fake.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.initializers import RandomNormal\nfrom keras.layers import Dense, Conv2D, Flatten, Reshape, Dropout\nfrom keras.layers.advanced_activations import LeakyReLU\nfrom keras.models import Sequential\n\ndef create_discriminator_cgan():\n    discriminator = Sequential()\n    \n    discriminator.add(Conv2D(64, (3, 3), padding='same', kernel_initializer=RandomNormal(0, 0.02), input_shape=(img_cols, img_rows, channels)))\n    discriminator.add(LeakyReLU(0.2))\n    \n    discriminator.add(Conv2D(128, (3, 3), strides=2, padding='same', kernel_initializer=RandomNormal(0, 0.02)))\n    discriminator.add(LeakyReLU(0.2))\n    \n    discriminator.add(Conv2D(128, (3, 3), strides=2, padding='same', kernel_initializer=RandomNormal(0, 0.02)))\n    discriminator.add(LeakyReLU(0.2))\n    \n    discriminator.add(Conv2D(256, (3, 3), strides=2, padding='same', kernel_initializer=RandomNormal(0, 0.02)))\n    discriminator.add(LeakyReLU(0.2))\n    \n    discriminator.add(Flatten())\n    discriminator.add(Dropout(0.4))\n    discriminator.add(Dense(1, activation='sigmoid', input_shape=(img_cols, img_rows, channels)))\n    \n    discriminator.compile(loss='binary_crossentropy', optimizer=optimizer)  #Again, the loss doesn't change when creating a DCGAN.\n    return discriminator","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now that we have changed the generator and discriminator, let's compile them into a single model. This code block is similar to the one used for FCGAN, because they way you join these models (and let the losses flow back) is the same.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.layers import Input\nfrom keras.models import Model\n\ndiscriminator = create_discriminator_cgan()\ngenerator = create_generator_cgan()\n\ndiscriminator.trainable = False\n\ngan_input = Input(shape=(noise_dim,))\nfake_image = generator(gan_input)\n\ngan_output = discriminator(fake_image)\n\ngan = Model(gan_input, gan_output)\ngan.compile(loss='binary_crossentropy', optimizer=optimizer)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The fashion MNIST database is more complex than tha standard digit MNIST database. This is because a shoe in the database is very different from a shirt, and these all have complex shapes. Hence, for making the life of the model simpler, I will only be generating T-shirts. The next code block basically limits the training data to only shirts. This makes the model learn to generate images better in fewer epochs (including all classes generates relatively low quality images).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train = x_train[np.where(y_train == 0)[0]]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, let's just plot a shirt to see how it looks before we train the model and see it's outputs","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"image = x_train[29]\nplt.imshow(image.reshape((img_rows, img_cols)), cmap='gray')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Again, this code is very similar to the previous one for the FCGAN. This is because the training procedure is also the same. The only fundamental difference is the convolutional layers. This code block takes some time to run. If you use a GPU, it's much faster.\nAlso, remember that we have already defined the show_images function above. Go back to remind yourself if needed.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for epoch in range(epochs):\n    for batch in range(steps_per_epoch):\n        noise = np.random.normal(0, 1, size=(batch_size, noise_dim))\n        fake_x = generator.predict(noise)\n\n        real_x = x_train[np.random.randint(0, x_train.shape[0], size=batch_size)]\n        #print(real_x.shape)\n        #print(fake_x.shape)\n        x = np.concatenate((real_x, fake_x))\n\n        disc_y = np.zeros(2*batch_size)\n        disc_y[:batch_size] = 0.9\n\n        d_loss = discriminator.train_on_batch(x, disc_y)\n\n        y_gen = np.ones(batch_size)\n        g_loss = gan.train_on_batch(noise, y_gen)\n\n    print(f'Epoch: {epoch + 1} \\t Discriminator Loss: {d_loss} \\t\\t Generator Loss: {g_loss}')\n    noise = np.random.normal(0, 1, size=(25, noise_dim))\n    show_images(noise, (5, 5))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Again, we can see that the model is learning quite well over time. Now let's just generate a larger sample of images.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"noise = np.random.normal(0, 1, size=(100, noise_dim))\nshow_images(noise, (10, 10))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Wow, the results are amazing. Some of the shirts look just like the ones in the database. There are a few faults, however, like one sleeve being longer than the other, but that is because we trained it for a smaller duration.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Extensions\n\nThis is just the basic implementation of a GAN using keras. There are a lot of research papers out there on how to improve the performance of such GANs, which you should try and check out.\n\nAlso, try forking this notebook and running it on the CIFAR -10 dataset, and see if you can generate color images! It isn't very hard to implement: you just need to change the dataset, and the number of channels to 3.\n\nYou can also experiment with other GANs such as Cycle GANs and Style GANs. This is still a growing area of research but I find it quite interesting that my computer can already draw better shirts than me!\n\nAlso, I will try to update this notebook (based on the turnout).\n\nThanks for reading the entire notebook. If you liked it, please upvote :). And please leave some feedback in the comments. I will try to make this notebook better.","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}