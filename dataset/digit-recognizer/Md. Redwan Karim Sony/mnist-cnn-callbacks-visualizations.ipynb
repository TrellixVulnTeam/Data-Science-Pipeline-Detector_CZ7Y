{"cells":[{"metadata":{},"cell_type":"markdown","source":"![](http://petr-marek.com/wp-content/uploads/2017/07/mnist-900x506.png)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 1. Loading Libraries\nLets load libraries first. Then read the csv files. \nhttps://www.kaggle.com/mukutkhandelwal/digit-recgonizer-99-05-score","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Importing the Required Library\nimport pandas as  pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom tensorflow.keras.callbacks import LearningRateScheduler, ModelCheckpoint, ReduceLROnPlateau\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dropout,Dense,Activation,Conv2D,MaxPooling2D,Flatten, BatchNormalization, MaxPool2D\nfrom tensorflow.keras.metrics import AUC\nfrom kerastuner.tuners import RandomSearch\nfrom tensorflow.keras.optimizers import RMSprop\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. Loading Data","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/digit-recognizer/train.csv')\nx_test = pd.read_csv('../input/digit-recognizer/test.csv')\n\n# taking the independent feature and the dependent feature in xtrain and ytrain of the traning data\nx_train = train.drop(columns = 'label')\ny_train = train['label']\nx_train.shape\ncounter = 0","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. Reshaping & Normalization  \nLet's separate the data and labels. It turns out that training data is of shape **(42000, 784)**. So the test set has 42000 inidividual images which are square image but unwinded to arrays of length 784. Lets make it into the image size of **28x28** so that we can plot and see. The pixel values are in th range of **[0, 255]**. Let's normalize them in the range **[0, 1]** for the convinence of CNN layers. \n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train = x_train.values.reshape(-1, int(np.sqrt(784)), int(np.sqrt(784)), 1)/255.0\nx_test =  x_test.values.reshape(-1, int(np.sqrt(784)), int(np.sqrt(784)), 1)/255.0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rows = 5\ncols = 15\nfig = plt.figure(figsize=(15,7))\nfor i in range(1, rows*cols+1):\n    fig.add_subplot(rows, cols, i)\n    plt.imshow(np.squeeze(x_train[counter + i-1]), cmap='gray')\n    plt.title(y_train[counter + i-1], fontsize=16)\n    plt.axis(False)\n    fig.add_subplot\ncounter += rows*cols","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"The orginal dataset has the labels numbered all the way from 0 to 9. However, this is not convinent for the CNN target output. Therefore, for the ease of classification model,  let's convert them into one hot vectors for each labels. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train = to_categorical(y_train, num_classes=10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. Splitting Up Train and Validation Set\nOriginal dataset doesn't have the train validation set separate. Therefore, in order to fine tune the model, we separate **10%** of the test data as hold out cross validation set. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train_trim ,x_valid , y_train_trim, y_valid = train_test_split(x_train, y_train, test_size= 0.1 ,random_state = 1455)\nprint(f'Training Set size: {x_train_trim.shape[0]}')\nprint(f'Validation Set size: {x_valid.shape[0]}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4. Model Building & Compilation\n### 4.1 Creating Model\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## With this model, I was able to get upto **0.99042** on the leaderboard\n```Python\nmodel = Sequential([\n          Conv2D(28, kernel_size = 3, padding='same', input_shape = x_train_trim.shape[-3:], activation ='relu'),\n          MaxPool2D(pool_size = (2,2)),\n          Conv2D(28, kernel_size=(2,2), padding='valid', activation='relu'),\n          MaxPool2D(pool_size =(2,2)),\n          Conv2D(28, kernel_size=(2,2), padding='valid', activation='relu'),\n          Dropout(0.2),\n          Flatten(),\n          Dense(512, activation='relu'),\n          Dropout(0.5),\n          Dense(256, activation='relu'),\n          Dense(10, activation = 'softmax')\n  ])\nmodel.summary()\n\nmodel.compile(optimizer='adam', \n              loss = 'categorical_crossentropy',  \n              metrics = ['accuracy', AUC()])\n\ntrain_history =  model.fit(x_train_trim, y_train_trim, \n                           epochs = 24, \n                           callbacks = [lr_schedule, checkpoint],\n                           validation_data = (x_valid, y_valid), \n                           batch_size = 1024, \n                           verbose = 2 )\n```","execution_count":null},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## With this model, I was able to get upto **0.99042** on the leaderboard\n```Python\nmodel  = Sequential([\n    Conv2D(75 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu' , input_shape = x_train_trim.shape[-3:]),\n    BatchNormalization(),\n    MaxPool2D((2,2) , strides = 2 , padding = 'same'),\n    Conv2D(50 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu'),\n    Dropout(0.2),\n    BatchNormalization(),\n    MaxPool2D((2,2) , strides = 2 , padding = 'same'),\n    Conv2D(25 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu'),\n    BatchNormalization(),\n    MaxPool2D((2,2) , strides = 2 , padding = 'same'),\n    Flatten(),\n    Dense(units = 512 , activation = 'relu'),\n    Dropout(0.3),\n    Dense(10, activation = 'softmax')])\n\n\n\nmodel.summary()\n\nmodel.compile(optimizer='adam', \n              loss = 'categorical_crossentropy',  \n              metrics = ['accuracy', AUC()])\n\ntrain_history =  model.fit(x_train_trim, y_train_trim, \n                           epochs = 24, \n                           callbacks = [lr_schedule, checkpoint],\n                           validation_data = (x_valid, y_valid), \n                           batch_size = 1024, \n                           verbose = 2 )\n```","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\n\nmodel.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', activation ='relu', input_shape = (28,28,1)))\nmodel.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', activation ='relu'))\nmodel.add(MaxPool2D(pool_size=(2,2)))\nmodel.add(Dropout(0.25))\n\nmodel.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', activation ='relu'))\nmodel.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', activation ='relu'))\nmodel.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\nmodel.add(Dropout(0.25))\n\nmodel.add(Flatten())\nmodel.add(Dense(256, activation = \"relu\"))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(10, activation = \"softmax\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.2 Creating Callbacks\nThe learning rate is a hyperparameter that controls how much to change the model in response to the estimated error each time the model weights are updated. Choosing the learning rate is challenging as a value too small may result in a long training process that could get stuck, whereas a value too large may result in learning a sub-optimal set of weights too fast or an unstable training process. Therefore let's take a learning rate scheduler from built in library then incorporate a scheduler to vary learning rate over time. For an in depth understanding of the effects of learning rate scheduler, you can have a look [here](https://machinelearningmastery.com/understand-the-dynamics-of-learning-rate-on-deep-learning-neural-networks/)  \n\nAnd another thing is the saving the model after training. However, during the training process the accuarcy of the model can go down. But if we are able to save the best possible state if the network over time, then it is possible to get the maximum out of the model.  ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_lrfn(lr_start=1e-4, lr_max=1e-3, \n               lr_min=0, lr_rampup_epochs=16, \n               lr_sustain_epochs=0, lr_exp_decay=.8):\n\n    def lrfn(epoch):\n        if epoch < lr_rampup_epochs:\n            lr = (lr_max - lr_start) / lr_rampup_epochs * epoch + lr_start\n        elif epoch < lr_rampup_epochs + lr_sustain_epochs:\n            lr = lr_max\n        else:\n            lr = (lr_max - lr_min) *\\\n                 lr_exp_decay**(epoch - lr_rampup_epochs\\\n                                - lr_sustain_epochs) + lr_min\n        return lr\n    return lrfn\n\nlrfn = build_lrfn()\nlr_schedule = LearningRateScheduler(lrfn, verbose=True)\n\n\n\n#Usually monitor='val_accuracy' should be tracked here. Since the training set is smaller let keep it limited to accuracy\ncheckpoint = ModelCheckpoint(\n    filepath='best_weights.hdf5',\n    save_weights_only=True,\n    monitor='accuracy',\n    mode='max',\n    save_best_only=True)\n\noptimizer_rmsprop = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)\nlearning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', \n                                            patience=3, \n                                            verbose=1, \n                                            factor=0.5, \n                                            min_lr=0.00001)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# With data augmentation to prevent overfitting (accuracy 0.99286)\n\ndatagen = ImageDataGenerator(\n        featurewise_center=False,  # set input mean to 0 over the dataset\n        samplewise_center=False,  # set each sample mean to 0\n        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n        samplewise_std_normalization=False,  # divide each input by its std\n        zca_whitening=False,  # apply ZCA whitening\n        rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n        zoom_range = 0.1, # Randomly zoom image \n        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n        horizontal_flip=False,  # randomly flip images\n        vertical_flip=False)  # randomly flip images\ndatagen.fit(x_train_trim)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4.3 Compilation and Training","execution_count":null},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## Used it upto version 7\n```Python\nmodel.compile(optimizer=optimizer_rmsprop, \n              loss = 'categorical_crossentropy',  \n              metrics = ['accuracy', AUC()])\n\ntrain_history =  model.fit(x_train_trim, y_train_trim, \n                           epochs = 24, \n                           callbacks = [lr_schedule, checkpoint],\n                           validation_data = (x_valid, y_valid), \n                           batch_size = 1024, \n                           verbose = 2 )\n```","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(optimizer=optimizer_rmsprop, \n              loss = 'categorical_crossentropy',  \n              metrics = ['accuracy'])\n\nbatch_size = 512\nepochs = 40\n\n\"\"\" \nBatch Size Variation epoch = 40 \n        batch size --->> accuracy\n        86         --->> 99.442\n        128        --->> 99.489\n        256        --->> 99.525\n        512        --->>   ?\n\"\"\"\n\n\ntrain_history = model.fit_generator(datagen.flow(x_train_trim, y_train_trim, batch_size=batch_size),\n                              epochs = epochs, validation_data = (x_valid,y_valid),\n                              verbose = 2, steps_per_epoch=x_train_trim.shape[0] // batch_size\n                              , callbacks=[lr_schedule, checkpoint])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4.4 Observe Training History\nNow the training is done, let's have a look at the training history. It is very crucial to determine whether the model is overfitting or not. Have a look [here](https://machinelearningmastery.com/introduction-to-regularization-to-reduce-overfitting-and-improve-generalization-error/) to check if the model is overfitting. ","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def visualize_training(history, lw = 3):\n    plt.figure(figsize=(10,10))\n    plt.subplot(2,1,1)\n    plt.plot(history.history['accuracy'], label = 'training', marker = '*', linewidth = lw)\n    plt.plot(history.history['val_accuracy'], label = 'validation', marker = 'o', linewidth = lw)\n    plt.title('Accuracy Comparison')\n    plt.xlabel('Epochs')\n    plt.ylabel('Accuracy')\n    plt.grid(True)\n    plt.legend(fontsize = 'x-large')\n    \n\n    plt.subplot(2,1,2)\n    plt.plot(history.history['loss'], label = 'training', marker = '*', linewidth = lw)\n    plt.plot(history.history['val_loss'], label = 'validation', marker = 'o', linewidth = lw)\n    plt.title('Loss Comparison')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.legend(fontsize = 'x-large')\n    plt.grid(True)\n    plt.show()\n\n    plt.figure(figsize=(10,5))\n    plt.plot(history.history['lr'], label = 'lr', marker = '*',linewidth = lw)\n    plt.title('Learning Rate')\n    plt.xlabel('Epochs')\n    plt.ylabel('Learning Rate')\n    plt.grid(True)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"visualize_training(train_history) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5. Inference and Submission Generation\nLets generate the predictions from the test set and have a look at the performance. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model.load_weights('./best_weights.hdf5')\npredictions_probs  = model.predict(x_test)\npredictions = np.argmax(predictions_probs, axis = 1)\ncounter = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rows = 5\ncols = 15\nfig = plt.figure(figsize=(15,7))\nfor i in range(1, rows*cols+1):\n    fig.add_subplot(rows, cols, i)\n    plt.imshow(np.squeeze(x_test[counter + i-1]), cmap='gray')\n    plt.title(predictions[counter + i-1], fontsize=16)\n    plt.axis(False)\n    fig.add_subplot\ncounter += rows*cols","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ImageId = np.arange(1,x_test.shape[0] + 1)\noutput = pd.DataFrame({'ImageId':ImageId, 'Label':predictions})\noutput.to_csv('submission.csv', index=False)\nprint(output)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}