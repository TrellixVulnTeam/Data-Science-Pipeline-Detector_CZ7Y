{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"### Introduction to Convolutional Neural Network using MNIST\n\nThis kernel aims at creating a multiclass classifier to handwritten digits by training our model on MNIST dataset.\n\n- We will try to understand why we are using convnets for this task and why they are popular for computer vision\n- Understand how convolution works\n- How different layers, like Pooling, Flattening and Dropout work\n- Then we will build our model and use it to predict the test dataset","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### What is Convolution?\n\nA convolution is an operation that basically represents how the shape of one function is modified by the other. In CNN, we use kernels or feature detectors which act as a filter to our initial input. These filters are usually of 3x3 or 5x5 sizes and slide over our 3D input, stopping at every possible location and extracting features to produce the feature map. \n\n\n\n$$(f*g)(t)\\;\\triangleq\\int_{-\\infty}^{\\infty} f(\\tau)\\;g(t-\\tau) \\;d\\tau$$ \n\n\nThe convolution works by element wise multiplication of the kernel and the 3D input as it passes over it and sums it all to give 1 value of the output feature map, thus reducing the size of input matrix.\n\nWe pass a number of feature detectors to the convolution layer, each of them producing a different output by retaining information from different parts of the image. This is called the depth of the output feature map. \n\nUnlike the densely connected layer a convolution layer can learn feature irrespective of its position in the image. For instance, after learning a certain pattern at the lower right corner of the image, a covnet can recognise it anywhere. Where as a densely connected layer has to learn the same pattern at the new location in order to recognise it. \n\nThe reason covnets are so popular for computer vision is because they preserve the spatial hierarchies of patterns. The layers start learning from small patterns like edges, and then the next layer will learn something bigger based on the output of first layer.\n\nConvolution work on 3D tensor, width, height and depth of the image. A tensor is simple a multi dimensional array. A matrix can be classified as a order 2 tensor. Since we are using grayscale images hence the depth will be 1. In case of colour image, the depth will be 3 because of the rbg scale. In case you ever need to work with Videos, an additional dimension, frame rate would make it a 4D tensor.\n\nLet's see how a convolution filter works.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from PIL import Image, ImageFilter ## To read and filter the image\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import imshow\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"im = Image.open(\"/kaggle/input/test-image/shutterstock_aero.jpg\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,8))\nplt.axis('off')\nimshow(im);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"That's a cool picture. Let's see how convolution works by using a 3x3 filter on the image.\n\n\n\n$$\ng=\n\\begin{pmatrix}\n-2 & -1 & 0\\\\\n-1 & 1 & 1\\\\\n0 & 1 & 2\n\\end{pmatrix}\t$$\n\nThis filter will start from the upper left corner and move 1 stride each time transforming the Image till it reaches the end (Bottom right). Increasing the stride can enable a greater dimensionality reduction and make the process faster but at the same time there is chance of loosing on information.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"emboss = [-2,-1,0,-1,1,1,0,1,2] # This perticular matrix is called emboss\nkernel = ImageFilter.Kernel((3,3), emboss)\n \nim2 = im.filter(kernel)\nplt.figure(figsize=(10,8))\nplt.xticks=[]\nimshow(im2);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### See the difference? Now let's get started on building our classifier.\n\n\n#### Importing Libraries","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"- MaxPooling: The function of pooling layer is downsizing the feature maps. It works in a similar fashion as convolution. It just extracts windows from the input feature and outputs the maximum value of the elements under that window. You will see how it reduces the feature maps which in turn reduces the number of parameters.\n\n- Flattening: After we apply pooling, we are going to flatten the 3-dimensional pooled feature map to a 1D array. The reason for this is we want to input it to an artificial neural network/ dense layer for further processing.\n\n\n- Dropout layer: Dropout layer let the model randomly drop a fraction of the units during forward or backward pass. This prevents units from co-adapting too much. Although I've already added dropouts, but the percentage of units we drop after each layer can be changed to see what works best.\n     - Things to keep in mind is, dropping out too many units will take higher number of epochs for the model to learn.\n     - However, the time taken for each epoch will decrease as there are less units to process.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Conv2D\nfrom keras.layers import MaxPooling2D\nfrom keras.layers import Dense\nfrom keras.layers import Flatten\nfrom keras.layers import Dropout\nimport numpy as np\nimport pandas as pd\nfrom keras.utils import to_categorical # to convert our lables to catagories\nimport collections #to get count the number of each label\nfrom sklearn.model_selection import train_test_split\nfrom keras.preprocessing.image import ImageDataGenerator","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pylab\npylab.rc('figure', figsize=(10,7))\n\nSMALL_SIZE = 8\nMEDIUM_SIZE = 10\nBIGGER_SIZE = 12\n\nplt.rc('font', size=SMALL_SIZE)          # controls default text sizes\nplt.rc('axes', titlesize=MEDIUM_SIZE)     # fontsize of the axes title\nplt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\nplt.rc('xtick', labelsize=MEDIUM_SIZE)    # fontsize of the tick labels\nplt.rc('ytick', labelsize=MEDIUM_SIZE)    # fontsize of the tick labels\nplt.rc('legend', fontsize=BIGGER_SIZE)    # legend fontsize\nplt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Load and Visualise the data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"Train_data=pd.read_csv(\"/kaggle/input/digit-recognizer/train.csv\")\nTest_data=pd.read_csv(\"/kaggle/input/digit-recognizer/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Convert the data frames to numpy values","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y=Train_data['label'].values\nTrain=Train_data.drop('label',axis=1).values\nTest=Test_data.values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Let's pull some of the digits from our training set","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,10))\nfor i in range(16):\n    plt.subplot(4,4,i+1)\n    plt.title(f\"Label: {y[i]}\",fontdict={'size'   : 12})\n    plt.gca().set_xticks([])\n    #plt.xticks([])\n    #plt.yticks([])\n    plt.grid(False)\n    plt.axis('off')\n    plt.imshow(Train.reshape(Train.shape[0],28,28)[i], cmap='gray')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def sort_dic(dic):  # Sort the counter dictionaries \n    index=sorted(dic)\n    sort_list=[]\n    label=[]\n    for i in index:\n        sort_list.append(dic[i])\n        label.append(i)\n    return sort_list,label","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_count=collections.Counter(y)\ntrain_count,labels=sort_dic(train_count)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Check the frequency of each digit in our training data and how they are distributed.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"x = np.arange(len(labels))  # the label locations\nwidth = 0.35  # the width of the bars\n\nfig, ax = plt.subplots()\nrects1 = ax.bar(x - width/2, train_count, width, label='Train_label_Count')\n\n# Add some text for labels, title and custom x-axis tick labels, etc.\nax.set_ylabel('Frequency')\nax.set_xlabel('Labels')\nax.set_title('Digit Counts in Training data')\nax.set_xticks(x)\nax.set_xticklabels(labels)\n\n\ndef autolabel(rects):\n    \"\"\"Attach a text label above each bar in *rects*, displaying its height.\"\"\"\n    for rect in rects:\n        height = rect.get_height()\n        ax.annotate('{}'.format(height),\n                    xy=(rect.get_x() + rect.get_width() / 2, height),\n                    xytext=(0, 3),  # 3 points vertical offset\n                    textcoords=\"offset points\",\n                    ha='center', va='bottom')\n\n\nautolabel(rects1)\n\nfig.tight_layout()\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data Preprocessing\n\nWe have to first reshape, split the dataset to train and validation data and scale them.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X=Train.reshape(Train.shape[0],28,28,1)\nX_test=Test.reshape(Test.shape[0],28,28,1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_t, X_v, y_t, y_v = train_test_split( X, y, test_size=0.1, random_state=42) #Taking a small validation size so that we have more examples to train from.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now since we are doing multi-class classification, we have to `one hot encode` our labels to be able to pass it through our model.\nAlso, we are going to `normalise` the training data between 0 and 1. Since each pixel is represented by an 8-bit value that ranges from `0` to `255`, simply dividing the feature maps by 255 will get the job done. This is because NN prefer to deal with small input values and would converge faster.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train=to_categorical(y_t,10)\ny_val=to_categorical(y_v,10)\n\nX_train=X_t/255\nX_val=X_v/255","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Building Classifier\n\nI am going to use a network consisting of 2 Convolution layer, with 64 filters with 2x2 pooling patches. I am also going to drop some of the parameters to reduce the chances of overfitting. We will be sticking to `relu` activation functions for the all layers except for the output layer where we are going to use a non linear activation function `Softmax`","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"classifier=Sequential()\n\nclassifier.add(Conv2D(64,(3,3),input_shape=(28,28,1),activation='relu')) #by default the stride is 1\nclassifier.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\nclassifier.add(Dropout(0.2))\nclassifier.add(Conv2D(64,(3,3),activation='relu'))\nclassifier.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\nclassifier.add(Dropout(0.2))\nclassifier.add(Flatten())\n\nclassifier.add(Dense(units=256,activation='relu'))\nclassifier.add(Dropout(0.5))\nclassifier.add(Dense(units=10,activation='softmax'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now our architecture is ready, let's take a look at it.\n\nYou, see can see how MaxPooling reduces the dimension to almost half. From 26 to 13 and then 11 to 5.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"classifier.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I am going to compile my model with `adam` optimizer as i think it is working better than `RMSprop` for me.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"classifier.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Fit our model to training features and corresponding labels","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#classifier.fit(X_train,y_train,epochs=5,batch_size=128)\n#score=classifier.evaluate(X_val,y_val)\n#print(\"The accuracy of the model on test data is:\",score[1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### I got an accuracy of 99.16% with the above code earlier. That's not bad considering the siplicity of our model.\n#### I tried data augmentation keeping everything else same and it just took me 99.72% accuracy. That's a huge leap!\n\n- Using data augmentation: This is the process of generating more training data from the existing samples by using a number of random transformations. This will help the model to learn additional characteristics of data and generalise better to reduce overfitting.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"T=25\ndatagen = ImageDataGenerator(\n        rotation_range=10, \n        zoom_range = 0.1,  \n        width_shift_range=0.1,  \n        height_shift_range=0.1,  \n        )  \n\n\ndatagen.fit(X_train)\n\nhistory = classifier.fit_generator(datagen.flow(X_train,y_train, batch_size=128),\n                              epochs = T, validation_data = (X_val,y_val),\n                              verbose = 2, steps_per_epoch=X_train.shape[0] // 128\n                              )\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since the training accuracy was not too high on my local, I didn't mind running it 25 epochs and got an accuracy of 99.72%. However let's visualise how our accuracies and losses varry with number of epochs.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"## Plotting the accuracies\n\nplt.plot(range(1,T+1),history.history['accuracy'],label='Training accuracy',c='r')\nplt.plot(range(1,T+1),history.history['val_accuracy'],label='Validation accuracy',c='b')\nplt.legend()\nplt.xlabel(\"# epochs\")\nplt.ylabel(\"Accuracy\")\nplt.title(\"Training and Validation accuracy wint number of epochs\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Looks like 10 epochs should do the job fine!","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"## Plotting the losses\n\nplt.plot(range(1,T+1),history.history['loss'],label='Training loss',c='r')\nplt.plot(range(1,T+1),history.history['val_loss'],label='Validation loss',c='b')\nplt.legend()\nplt.xlabel(\"# epochs\")\nplt.ylabel(\"Loss\")\nplt.title(\"Training and Validation loss with number of epochs\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred=classifier.predict(X_val)\npred_label=np.argmax(y_pred,axis=1)  ##Convert the results to class labels\nactual_label=np.argmax(y_val,axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### How many of each digits mis classified?","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"error=pred_label!=y_v\nmissclassified_images=X_v[error]\nmissclassified_label=pred_label[error]\ncorrect_label=y_v[error]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_counts=collections.Counter(pred_label)  ##Getting the frequencies\nlabels_count=collections.Counter(y_v)\n\nprediction_count,labels=sort_dic(pred_counts) ##Sorting the dictionary\nactual_count,labels=sort_dic(labels_count)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = np.arange(len(labels))  # the label locations\nwidth = 0.35  # the width of the bars\n\nfig, ax = plt.subplots()\nrects1 = ax.bar(x - width/2, prediction_count, width, label='Prediction')\nrects2 = ax.bar(x + width/2, actual_count, width, label='Actual')\n\n# Add some text for labels, title and custom x-axis tick labels, etc.\nax.set_ylabel('Frequency')\nax.set_xlabel('Labels')\nax.set_title('Label counts (Actual vs Prediction )')\nax.set_xticks(x)\nax.set_xticklabels(labels)\nax.legend()\n\n\ndef autolabel(rects):\n    \"\"\"Attach a text label above each bar in *rects*, displaying its height.\"\"\"\n    for rect in rects:\n        height = rect.get_height()\n        ax.annotate('{}'.format(height),\n                    xy=(rect.get_x() + rect.get_width() / 2, height),\n                    xytext=(0, 3),  # 3 points vertical offset\n                    textcoords=\"offset points\",\n                    ha='center', va='bottom')\n\n\nautolabel(rects1)\nautolabel(rects2)\n\nfig.tight_layout()\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,10))\nfor i in range(9):\n    plt.subplot(3,3,i+1)\n    plt.title(f\"Correct Label {correct_label[i]}\\n Prediction {missclassified_label[i]}\",fontdict={'size'   : 12})\n    #plt.xticks([])\n    #plt.yticks([])\n    plt.axis('off')\n    plt.grid(False)\n    plt.imshow(missclassified_images.reshape(missclassified_images.shape[0],missclassified_images.shape[1],missclassified_images.shape[2])[i], cmap='gray')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As you can see not all the digits are as easy to comprehend even for human eyes.\n\nSome of the parameters can be tweaked to see if they give any better results. I'll still keep digging since I see a lot of people have achieved a 100% accuracy. Will update the kernel if I hit any better.\n\nLet me know in the comments any ideas that you might have to enhance the model.\n\n### Thanks","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Submission","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#test_pred=classifier.predict(X_test)\n#test_pred=np.argmax(test_pred,axis=1)\n#submission =  pd.DataFrame({\n#        \"ImageId\": Test_data.index+1,\n#        \"Label\": test_pred\n#    })\n#submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission=pd.read_csv(\"/kaggle/input/submission-mnist/submission.csv\")  ##uploading locally predicted file\nsubmission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}