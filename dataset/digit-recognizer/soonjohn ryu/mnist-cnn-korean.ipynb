{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 머신러닝 입문자들의 \"Hello World\" : Mnist\n\n머신러닝계의 헬로우 월드라고 불릴만큼 입문자이 가장 먼저 접하게 되는 MNIST\n이제 막 입문한 캐린이로써 다른 커널을 참고해가며 아주 간단한 MNIST 모델을 구현해 보려고 합니다.\n\n\n* 우선 데이터 먼저 LOAD (TRAIN, TEST)\n  데이터의 모양을 살펴보자\n","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/digit-recognizer/train.csv')\ntest = pd.read_csv('../input/digit-recognizer/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train.shape , test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* train은 42,000개, test는 28,000개 확인했다.\n* train.data의 모양을 확인했을때 라벨이 하나 있고 784열 즉, 28 * 28 모양의 글자 모양임을 알수 있다.\n\n## 그럼 이제 필요한 라이브러리를 로드하자.\n\n* 기본적으로 간단하게 케라스를 이용해서 CNN망을 구현해보고자 한다.\n* 그러기 위해서 필수적인 라이브러리들을 로드한다.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# LOAD LIBRARIES\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n#넘파이, 판다스, MATPLOT은 기본으로 항상\n\nfrom sklearn.model_selection import train_test_split\n# train.test 셋을 쉽게 분리하기 위해서\n\nfrom keras.utils.np_utils import to_categorical\n# cnn을 통해 최종적으로 결과를 받으면 라벨수만큼의 각각의 확률값으로 반환된다. 결과값을 받기 편하게 하기위한 함수\nfrom keras.models import Sequential\n# 케라스 모델구성기본 함수\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, BatchNormalization\n# 케라스에서 필요한 레이어들 간편하게 쓸수 있다.\nfrom keras.preprocessing.image import ImageDataGenerator\n# 이미지를 조금 변화해줌으로써 성능을 올릴수 있다. 그랜드 마스터 Chris Deotte 의 25 Million Images! [0.99757] MNIST 커널에서 참고했다.(그외에도 거의 많이 참고했다.)\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nfrom tqdm.keras import TqdmCallback\n# 콜벡 모델이 어떤 기준으로 돌다가 멈추고 저장하고 하는것들을 설정해줄수 있다.\nimport warnings\nwarnings.filterwarnings('ignore')\n# 지저분하게 워닝뜨는걸 막아준다.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* 라이브러리들을 호출했으면 이제 데이터 전처리를 해야한다.\n* MNIST는 결측치가 있는것도 아니고 아웃라이어 값들이 있을리도 없어서 CNN에 들어갈 구조에 맞게 간단하게 모양만 잡아준다.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# X의 라벨값을 CNN에 넣을수 없고 이따 예측비교시 쓸거니까 분리해준다.\nY_train = train[\"label\"]\nX_train = train.drop(labels = [\"label\"],axis = 1)\n\n# 0~255 사이의 픽셀명암의 숫자를 계산하기 편하기 255로 나눠 비율은 유지하고 숫자는 작게\nX_train = X_train / 255.0\nX_final = test / 255.0\n#이제 (28,28) 모양으로 RESHAPE\nX_train = X_train.values.reshape(-1,28,28,1)\nX_final = X_final.values.reshape(-1,28,28,1)\n#아까 뺴놓은 라벨값도 CNN결과값이랑 비교할수 있는 형태로 \nY_train = to_categorical(Y_train, num_classes = 10)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* 변환한 김에 X_train의 28,28 이미지를 시각화 해보면 아래와 같다.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# matplot으로 간단하게 시각화 해보면 라벨값에 맞는 숫자를 확인할수 있다.\n\nfig = plt.figure(figsize=(10,10))\n\nfor i in range(10):\n    i += 1\n    plt.subplot(2,5,i)\n    plt.title(train['label'][i])\n    plt.imshow(X_train[i].reshape(28,28))\n    plt.axis('off')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* 모델을 구성하기전에 모델의 성능을 올리기 위한 데이터 전처리로 ImageDataGenerator를 미리 선언해준다.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# 10도 정도 돌리고 10% 정도 줌하고, 왼쪽, 오른쪽 시프트를 해서 다양한 변화를 준 데이터를 추가해준다.(이따 케라스 모델 fit_generator 할때 사용예정)\ndatagen = ImageDataGenerator(\n        rotation_range=10,\n        zoom_range=0.1,\n        width_shift_range=0.1,\n        height_shift_range=0.1,)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 아까 사이킷런에서 가져온 split으로 train안에서 훈련분과 검증분을 나눈다.(보통 0.3 비중으로 검증사이즈를 잡지만 데이터가 충분해서 0.1로 한다)\nX_train, X_test, y_train, y_test = train_test_split(X_train, Y_train, test_size = 0.1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### keras를 이용해서 간단하게 모델을 구성해본다.\n \n * CNN 모델의 이론적인 내용들은 더 공부가 필요하지만 개략적으로 CONVOLUTION하고 배치노말라이제이션으로 정규화, DROP아웃을 통해 과적합을 막아주는 레이어를 차곡차곡 쌓는다.\n * 마지막 열에가서는 당연하게도 평평하게 펴서(FLATTEN) 10개의 라벨에 맞게 각 확률값이 계산되도록 소프트 맥스 함수를 이용하여 Dense Layer를 구성한다","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\n\nmodel.add(Conv2D(32,kernel_size=3,activation= 'relu', input_shape = (28,28,1) ))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(32,kernel_size=3,activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(32,kernel_size=5,activation = 'relu', padding='same',strides=2))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.4))\n\nmodel.add(Conv2D(64,kernel_size=3,activation= 'relu'))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(64,kernel_size=3,activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(64,kernel_size=5,activation = 'relu', padding='same',strides=2))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.4))\n\nmodel.add(Conv2D(128,kernel_size=4,activation= 'relu'))\nmodel.add(BatchNormalization())\nmodel.add(Flatten())\nmodel.add(Dropout(0.4))\nmodel.add(Dense(10,activation='softmax'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# MODEL확인 : 참 편하다\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* 모델을 구성했다면 컴파일을 어떻게 할지 설정해준다(옵티마이저는 멀로할지, loss는 어떤값기준으로, metric 등을 설정해준다. keras document참고하면 많다)\n  (잘모르겠으면 사람들 많이 쓰는 adam, cross엔트로피, accuracy을 쓴다)\n  \n* 모델 fit_generator에 쓸 콜벡도 미리 선언하자(earlystopping, 모델체크포인트(딱히 저장할필요가 없어서 안했다) 등등...)\n  ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(optimizer='adam', loss = \"categorical_crossentropy\", metrics=['accuracy'])\n\n# 콜벡은 이렇게 선언해서 callbacks에 담아놓자\nearlyStopping = EarlyStopping(patience=10, verbose=0)\nreduce_lr_loss = ReduceLROnPlateau(factor=0.1, patience=5, min_lr=0.000001, verbose=0)\ntqdm = TqdmCallback(verbose=0) #진행율 표시해준다.(없으면 답답하다)\ncallbacks = [earlyStopping, reduce_lr_loss, tqdm]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* 배치 단위로 생산한 데이터에 대해서 모델을 학습시키는 model.fit_generator를 이용해 학습을 진행한다.\n* 일단 아까 선언한 datagen을 flow시켜서 인풋 데이터를 다양하게 해주고,\n* epochs는 20회 정도로 잡았다. 배치사이즈를 64로 잡았으니까 step_per_epoch는 전체 train갯수/64로 나눠줬다.\n* callbacks = callbacks로 담아줘서 위에서 선언한 callback이 적용되도록 했다.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit_generator(datagen.flow(X_train,y_train, batch_size=64),\n                              epochs = 20,\n                              steps_per_epoch = X_train.shape[0]//64,\n                              validation_data = (X_test,y_test),\n                              callbacks=callbacks,\n                              verbose=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 결과를 확인해보자(학습대상의 정확도, 검증대상의 정확도)\nprint('train_acc:{0:.5f} , val_acc:{1:.5f}'.format(max(history.history['accuracy']),max(history.history['val_accuracy'])))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 그래프로 표시해보는 정확도 Accuracy, 훈련이 적당히 잘된거 같다. 더이상 훈련은 생략...\n\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('Accuracy', fontsize=14)\nplt.xlabel('Epoch', fontsize=14)\nplt.ylabel('Accuracy',fontsize=14)\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 모델 훈련이 완료되었으니 이제 test data를 넣어서 예측.\n* 아까 처음에 test data를 X_final에 담아서 똑같이 전치리 해줬었다.\n\n> X_final = test / 255.0\n\n> X_final = X_final.values.reshape(-1,28,28,1)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#결과값을 담을 results를 0으로 testsize, 라벨개수의 행렬로 선언하고, 거기다가 결과값을 더해서 담는다\nresults = np.zeros( (X_final.shape[0],10) ) \nresults = results+model.predict(X_final)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 이런식으로 처음 데이터의 대한 10개 라벨의 예측값이 담겨있다\nresults[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 각각의 확률값중에 가장높은값이 바로 예측값이니까 argmax를 이용해서 뽑아준다.\nresults = np.argmax(results, axis=1)\n# 하나 뽑힌값을 pd.Series를 이용해 1차원으로 만들어 준다.순서대로 각 데이터의 예측 라벨이된다.\nresults = pd.Series(results,name='label')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#submission 양식에 맞춰야 하니까 0부터 시작이 아닌 1부터 시작하는 형태로 데이터 프레임을 만든다.\nsubmission = pd.concat([pd.Series(range(1,28001), name='Imageid'),results],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 최종 데이터값 저장\nsubmission.to_csv(\"submission.csv\",index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 이제 막 시작한 캐린이로써 허접하지만 여기저기 참고하고 필사해 가면서 공부중이다.\n## 비슷한 길을 걷는 모든이들이 조금이라도 도움이 되었으면...","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}