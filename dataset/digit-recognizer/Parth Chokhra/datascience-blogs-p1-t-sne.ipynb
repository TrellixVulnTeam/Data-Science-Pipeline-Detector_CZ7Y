{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"# Introduction to t-SNE\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<img src=\"https://hypercompetent.github.io/post/2017-12-31-gganimate-tweenr-tsne-plot_files/figure-html/get_tsne-1.png\">","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### t-Distributed Stochastic Neighbor Embedding (t-SNE)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Dimensionality Reduction","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### If you have worked with a dataset before with a lot of features, you can fathom how difficult it is to understand or explore the relationships between the features. Not only it makes the EDA process difficult but also affects the machine learning model’s performance since the chances are that you might overfit your model or violate some of the assumptions of the algorithm, like the independence of features in linear regression. This is where dimensionality reduction comes in. In machine learning, dimensionality reduction is the process of reducing the number of random variables under consideration by obtaining a set of principal variables. By reducing the dimension of your feature space, you have fewer relationships between features to consider which can be explored and visualized easily and also you are less likely to overfit your model.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<img src = \"https://www.visiondummy.com/wp-content/uploads/2014/04/dimensionality_vs_performance.png\">","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Dimensionality reduction can be achieved in the following ways:\n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* Feature Elimination\n* Feature Selection\n* Feature Extraction","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Feature Extraction: You create new independent features, where each new independent feature is a combination of each of the old independent features. These techniques can further be divided into linear and non-linear dimensionality reduction techniques.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Linear dimensional reduction using PCA\n\n\nNon Linear dimensional reduction using t-SNE","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<img src=\"https://lh3.googleusercontent.com/proxy/49afPkB9IYGkacOkUhgAQYrO3OWPHvTgjzgtaFhnm9Rw83jBS--jrGw1r8TXOvcFBdxUMLlrCXj7fQKR1zywtzAyVIAgGMEME9haT1tbGgt-Kw\">","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## t-Distributed Stochastic Neighbor Embedding (t-SNE)\n### t-Distributed Stochastic Neighbor Embedding (t-SNE) is a non-linear technique for dimensionality reduction that is particularly well suited for the visualization of high-dimensional datasets. It is extensively applied in image processing, NLP, genomic data and speech processing. To keep things simple, here’s a brief overview of working of t-SNE:","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* #### The algorithms starts by calculating the probability of similarity of points in high-dimensional space and calculating the probability of similarity of points in the corresponding low-dimensional space. The similarity of points is calculated as the conditional probability that a point A would choose point B as its neighbor if neighbors were picked in proportion to their probability density under a Gaussian (normal distribution) centered at A.\n\n* #### It then tries to minimize the difference between these conditional probabilities (or similarities) in higher-dimensional and lower-dimensional space for a perfect representation of data points in lower-dimensional space.\n\n* #### To measure the minimization of the sum of difference of conditional probability t-SNE minimizes the sum of Kullback-Leibler divergence of overall data points using a gradient descent method.\n\n#### Note : Kullback-Leibler divergence or KL divergence is is a measure of how one probability distribution diverges from a second, expected probability distribution.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### *In simpler terms, t-Distributed stochastic neighbor embedding (t-SNE) minimizes the divergence between two distributions: a distribution that measures pairwise similarities of the input objects and a distribution that measures pairwise similarities of the corresponding low-dimensional points in the embedding.*\n\n### *In this way, t-SNE maps the multi-dimensional data to a lower dimensional space and attempts to find patterns in the data by identifying observed clusters based on similarity of data points with multiple features. However, after this process, the input features are no longer identifiable, and you cannot make any inference based only on the output of t-SNE. Hence it is mainly a data exploration and visualization technique.*","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### T-SNE : Lets code","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nprint(os.listdir(\"../input\"))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nmnist_df = pd.read_csv('../input/digit-recognizer/train.csv')\nmnist_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mnist_labels = mnist_df['label']\nmnist_pixels = mnist_df.drop('label', axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# standardize mnist dataset\n\nfrom sklearn.preprocessing import StandardScaler\nmnist_pixels_std_df = StandardScaler().fit_transform(mnist_pixels)\nprint(mnist_pixels_std_df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Clip 1000 datapoints\n\nmnist_pixels_tsne = mnist_pixels_std_df[0:5000,:]\nmnist_labels_tsne = mnist_labels[:5000]\nprint(\"MNIST labels : \", mnist_labels_tsne.shape)\nprint(\"MNIST features : \", mnist_pixels_tsne.shape)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true,"collapsed":true},"cell_type":"code","source":"# t-SNE\nfrom sklearn.manifold import TSNE\nimport timeit\n# code you want to evaluate\n# D-dash = 2\n# default perplexity = 30\n# default learning rate = 200\n# default no. of iteration for optimization = 1000\n\nstart_time = timeit.default_timer()\n\nmodel = TSNE(n_components = 2, random_state = 0,verbose = 2,n_iter = 2000)\ntsne_model = model.fit_transform(mnist_pixels_tsne).T\n\nelapsed = timeit.default_timer() - start_time","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# visualizing t-SNE\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ntsne_data = np.vstack((tsne_model, mnist_labels_tsne)).T\ntsne_df = pd.DataFrame(data = tsne_data, columns=('Dimention 1', 'Dimention 2', 'Label'))\nsns.FacetGrid(tsne_df, size = 8, hue = 'Label').map(plt.scatter, 'Dimention 1', 'Dimention 2').add_legend()\n\nprint('\\n\\nElapsed time for t-SNE visualization :', elapsed, 'seconds')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}