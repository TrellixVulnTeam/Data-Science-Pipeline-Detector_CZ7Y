{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# What's Cooking?\n\n#### before we start with the problem itself there are some questions we need to answer:\n1. What is the business question?\n2. What each row represent?\n3. What is the evaluation method?\n\n#### for this problem (and all kaggle problems) the answers to these questions is always in the problem's overview page.\n1. What is the category of a dish's cuisine given a list of its ingredients? (Supervised ML Problem)\n2. Each row represent a recipe.\n3. Submissions are evaluated on the categorization accuracy (the percent of dishes that you correctly classify).","metadata":{"_uuid":"9106fb9b-ee6e-40a5-a6e6-0a99f7210106","_cell_guid":"a41875b0-6fb3-4a8c-a8d6-125912a61755","trusted":true}},{"cell_type":"markdown","source":"# 1. Important imports\n### let's start by importing needed libraries.","metadata":{"_uuid":"b1f45ea1-605d-4d51-bd23-b366a87ab422","_cell_guid":"7118f04d-476a-4065-8e7a-8a637b84b0a5","trusted":true}},{"cell_type":"code","source":"# load data libraries\nimport numpy as np # linear algebra library\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport zipfile # to read zip files\nfrom sklearn.model_selection import train_test_split\n\n\n# data understanding libraries\nimport matplotlib.pyplot as plt # ploting library\n%matplotlib inline\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nfrom collections import Counter\nimport seaborn as sns\n\n\n\n# data preparation\nimport re\nfrom nltk.stem import PorterStemmer\n\n\n# ADS Creation\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.preprocessing import StandardScaler\n\n# Modeling\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import LinearSVC, SVC\nfrom sklearn.naive_bayes import MultinomialNB\n\n# Evaluation and Model Selection\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\nfrom sklearn import metrics\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.model_selection import GridSearchCV","metadata":{"_uuid":"47463bfc-8edc-4293-a1ee-9b76a20b9424","_cell_guid":"2606727a-627c-4851-8b03-d9f523509419","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.set_option('display.max_rows', 10000)\npd.set_option('display.max_columns', 500)\npd.set_option('display.precision',150)\npd.options.display.float_format = '{:,.3f}'.format","metadata":{"_uuid":"997195d9-7a81-424b-a3d2-2da8cd426fe4","_cell_guid":"20fd889f-664d-44f6-b7fb-8020ef1e86ce","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Load Data\n### Let's load the data and have a look on it.\n1. data is provieded in a zip file, so we need to unzip it first using zipfile library.\n2. the traning/ testing files available in json file format, to read it we use pd.read_json function.\n        we read the data into pandas dataframes which is a 2-dimensional labeled data structure with columns of\n        potentially different types. You can think of it like a spreadsheet or SQL table.\n3. to view some rows of the dataframe we use df_name.head() method which output the first 5 rows of the dataframe.","metadata":{"_uuid":"5f19d139-2da1-4401-afdd-355bd5fd28b6","_cell_guid":"0133ade8-9aab-4ca4-8701-da32ee2c1b12","trusted":true}},{"cell_type":"code","source":"#unzip the files\narchive_train = zipfile.ZipFile('/kaggle/input/whats-cooking/train.json.zip')\n\n#read training json file \ntrain = pd.read_json(archive_train.read('train.json'))\n\n#output the frist 5 rows\ntrain.head()","metadata":{"_uuid":"ed249b99-d69b-417d-bb45-4957c2d8e8a9","_cell_guid":"87f47a4e-d1bb-43d0-a81a-8a24a26a5b05","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> There are only 3 columns: id, cuisine and ingredients","metadata":{"_uuid":"9bcb0db9-a6f0-4e18-9eab-ac087d57ddd5","_cell_guid":"22e19aa8-f5ae-4bda-b076-4be31122cf59","trusted":true}},{"cell_type":"code","source":"train_data, test_data = train_test_split(train, test_size=0.4, random_state=1)\nval_data, test_data = train_test_split(test_data, test_size=0.5, random_state=1)\n\ntrain_data = train_data.reset_index(drop=True)\nval_data = val_data.reset_index(drop=True)\ntest_data = test_data.reset_index(drop=True)","metadata":{"_uuid":"57253301-b2bd-43ad-af2b-87181a13931e","_cell_guid":"d8ca1606-f9c7-4533-bd90-07bdd0f67727","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Train set size is \",len(train_data))\nprint(\"Val set size is \",len(val_data))\nprint(\"Test set size is \",len(test_data))","metadata":{"_uuid":"c23be4a4-324e-4c21-8bcd-89eea56709c8","_cell_guid":"40839e7c-dd75-44e9-abaa-115c13807625","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Data Preparation","metadata":{"_uuid":"5b6bab66-72b7-48ec-baf2-009d3640be04","_cell_guid":"18c68dcd-8c3b-43e6-b4a2-c44311d1855f","trusted":true}},{"cell_type":"code","source":"stopwords = set([\"Campbell's\",\"hellmann\",\"oz\",\"M&M\",\"Pasoâ„¢\",\"\"])\nporter = PorterStemmer()\n# lancaster=LancasterStemmer()\n\ndef ret_words(ingredients):\n    ingredients_text = ' '.join(ingredients)\n    ingredients_text = ingredients_text.lower()\n    ingredients_text = ingredients_text.replace('-', '')\n    ingredients_text = ingredients_text.replace(',', ' ')\n    words = []\n    for word in ingredients_text.split():\n        if re.findall('[0-9]', word): continue\n        if len(word) <= 2: continue\n        if '’' in word: continue\n        if '®' in word: continue\n        if word in stopwords: continue\n        if re.findall('[^a-zA-Z]',re.sub(r'[^\\w\\s]','',word)): continue\n        if len(word) > 0: words.append(porter.stem(re.sub(r'[^\\w\\s]','',word)))\n    return ' '.join(words)\n\ndef preprocess(df,flag):\n    # Remove recipes with only one Ingredient\n    df[\"ingredients_num\"]=df[\"ingredients\"].apply(len)\n    if flag == 0 :\n        df = df.drop(df[df[\"ingredients_num\"]<=1].index)\n    \n    # Convert list of ingredients to string\n    df['ingredients_txt'] = df[\"ingredients\"].apply(ret_words)\n    \n    return df","metadata":{"_uuid":"6dc6535b-8dd0-4396-888d-14e19a244e55","_cell_guid":"6f41fc49-923e-4960-8e37-61962bed5427","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_preprocessed = preprocess(train_data,0)\nval_preprocessed = preprocess(val_data,1)\ntest_preprocessed = preprocess(test_data,1)","metadata":{"_uuid":"ba6523f6-90f9-4603-b6dc-653a898eb3a0","_cell_guid":"d46c3812-5444-4258-a870-14804a7165de","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_preprocessed.head()","metadata":{"_uuid":"b8f8492f-c4e1-4cd3-a717-c13a2f5c5918","_cell_guid":"8ef0acfb-9ee7-4e42-8b3e-ca83a4ddcb94","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modeling","metadata":{"_uuid":"12991fc4-c506-4449-9f81-d8ff77e71e6c","_cell_guid":"223bbafc-5a7b-4464-add6-ce60375473b7","trusted":true}},{"cell_type":"code","source":"id_train, X_train, y_train = train_preprocessed['id'], train_preprocessed['ingredients_txt'], train_preprocessed['cuisine']\nid_val, X_val, y_val = val_preprocessed['id'], val_preprocessed['ingredients_txt'], val_preprocessed['cuisine']\nid_test, X_test, y_test = test_preprocessed['id'], test_preprocessed['ingredients_txt'], test_preprocessed['cuisine']","metadata":{"_uuid":"0d1cd8b7-28b9-45de-9e66-ae19622d1acf","_cell_guid":"cd53ee92-bee3-44bc-967f-1ea1f3f26512","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## BoW","metadata":{"_uuid":"3e788912-a182-4317-a25c-312dfffdeaef","_cell_guid":"ba9a974c-ae9b-43be-a166-82be2d70bddf","trusted":true}},{"cell_type":"code","source":"LR_clf_counts = Pipeline([\n    ('vect', CountVectorizer()),\n    ('clf', LogisticRegression(random_state=0, max_iter=2000))\n])\nLR_clf_counts.fit(X_train, y_train)\nLR_cnt_pred_tr = LR_clf_counts.predict(X_train)\n\nprint(accuracy_score(y_train, LR_cnt_pred_tr))\nprint(precision_score(y_train, LR_cnt_pred_tr, average='weighted'))","metadata":{"_uuid":"5795e1c3-4422-4557-983b-4d90cca63ab0","_cell_guid":"f23b4a45-7186-48fd-a51b-38a093f69206","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create CV training and test scores for various training set sizes\ntrain_sizes, train_scores, test_scores = learning_curve(LR_clf_counts, \n                                                        X_train, \n                                                        y_train,\n                                                        # Number of folds in cross-validation\n                                                        cv=3,\n                                                        # Evaluation metric\n                                                        scoring='precision_weighted',\n                                                        # Use all computer cores\n                                                        n_jobs=-1, \n                                                        # 50 different sizes of the training set\n                                                        train_sizes=np.linspace(0.01, 1.0, 10))\n\n# Create means and standard deviations of training set scores\ntrain_mean = np.mean(train_scores, axis=1)\ntrain_std = np.std(train_scores, axis=1)\n\n# Create means and standard deviations of test set scores\ntest_mean = np.mean(test_scores, axis=1)\ntest_std = np.std(test_scores, axis=1)\nfig, ax = plt.subplots(figsize=(15,10))\n# Draw lines\nplt.plot(train_sizes, train_mean, '--', color=\"#111111\",  label=\"Training score\")\nplt.plot(train_sizes, test_mean, color=\"#111111\", label=\"Cross-validation score\")\n\n# Draw bands\nplt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, color=\"#DDDDDD\")\nplt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, color=\"#DDDDDD\")\n\n# Create plot\nplt.title(\"Learning Curve\")\nplt.xlabel(\"Training Set Size\"), plt.ylabel(\"Weighted Precision Score\"), plt.legend(loc=\"best\")\nplt.tight_layout()\nplt.show()","metadata":{"_uuid":"83fac687-7a64-49e4-b64e-e41e8b5d7958","_cell_guid":"9e0ec81f-8a16-4f52-8f06-648d8cbb07b7","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SVM_clf_counts = Pipeline([\n    ('vect', CountVectorizer()),\n    ('clf', LinearSVC(max_iter=3000))\n])\nSVM_clf_counts.fit(X_train, y_train)\nSVM_cnt_pred_tr = SVM_clf_counts.predict(X_train)\n\nprint(accuracy_score(y_train, SVM_cnt_pred_tr))\nprint(precision_score(y_train, SVM_cnt_pred_tr, average='weighted'))","metadata":{"_uuid":"283b0de8-cf9d-4d04-bd0d-bfdc2c45f17b","_cell_guid":"ab5dd7c1-38d3-4da5-a02b-7f2a50a3a7d4","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create CV training and test scores for various training set sizes\ntrain_sizes, train_scores, test_scores = learning_curve(SVM_clf_counts, \n                                                        X_train, \n                                                        y_train,\n                                                        # Number of folds in cross-validation\n                                                        cv=3,\n                                                        # Evaluation metric\n                                                        scoring='precision_weighted',\n                                                        # Use all computer cores\n                                                        n_jobs=-1, \n                                                        # 50 different sizes of the training set\n                                                        train_sizes=np.linspace(0.01, 1.0, 10))\n\n# Create means and standard deviations of training set scores\ntrain_mean = np.mean(train_scores, axis=1)\ntrain_std = np.std(train_scores, axis=1)\n\n# Create means and standard deviations of test set scores\ntest_mean = np.mean(test_scores, axis=1)\ntest_std = np.std(test_scores, axis=1)\nfig, ax = plt.subplots(figsize=(15,10))\n# Draw lines\nplt.plot(train_sizes, train_mean, '--', color=\"#111111\",  label=\"Training score\")\nplt.plot(train_sizes, test_mean, color=\"#111111\", label=\"Cross-validation score\")\n\n# Draw bands\nplt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, color=\"#DDDDDD\")\nplt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, color=\"#DDDDDD\")\n\n# Create plot\nplt.title(\"Learning Curve\")\nplt.xlabel(\"Training Set Size\"), plt.ylabel(\"Weighted Precision Score\"), plt.legend(loc=\"best\")\nplt.tight_layout()\nplt.show()","metadata":{"_uuid":"fdedd2ef-08a8-4fd6-b6f6-0678cec4cac8","_cell_guid":"e590d7f4-1bbc-4f57-86d9-f27157e23a90","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"NB_clf_counts = Pipeline([\n    ('vect', CountVectorizer()),\n    ('clf', MultinomialNB())\n])\nNB_clf_counts.fit(X_train, y_train)\nNB_cnt_pred_tr = NB_clf_counts.predict(X_train)\n\nprint(accuracy_score(y_train, NB_cnt_pred_tr))\nprint(precision_score(y_train, NB_cnt_pred_tr, average='weighted'))","metadata":{"_uuid":"6866fe86-79be-4439-b9d9-366bb59b0bd3","_cell_guid":"4577058c-76fa-489e-8df8-9a8044cb784d","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create CV training and test scores for various training set sizes\ntrain_sizes, train_scores, test_scores = learning_curve(NB_clf_counts, \n                                                        X_train, \n                                                        y_train,\n                                                        # Number of folds in cross-validation\n                                                        cv=3,\n                                                        # Evaluation metric\n                                                        scoring='precision_weighted',\n                                                        # Use all computer cores\n                                                        n_jobs=-1, \n                                                        # 50 different sizes of the training set\n                                                        train_sizes=np.linspace(0.01, 1.0, 10))\n\n# Create means and standard deviations of training set scores\ntrain_mean = np.mean(train_scores, axis=1)\ntrain_std = np.std(train_scores, axis=1)\n\n# Create means and standard deviations of test set scores\ntest_mean = np.mean(test_scores, axis=1)\ntest_std = np.std(test_scores, axis=1)\nfig, ax = plt.subplots(figsize=(15,10))\n# Draw lines\nplt.plot(train_sizes, train_mean, '--', color=\"#111111\",  label=\"Training score\")\nplt.plot(train_sizes, test_mean, color=\"#111111\", label=\"Cross-validation score\")\n\n# Draw bands\nplt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, color=\"#DDDDDD\")\nplt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, color=\"#DDDDDD\")\n\n# Create plot\nplt.title(\"Learning Curve\")\nplt.xlabel(\"Training Set Size\"), plt.ylabel(\"Weighted Precision Score\"), plt.legend(loc=\"best\")\nplt.tight_layout()\nplt.show()","metadata":{"_uuid":"f034032f-4365-49df-ad6a-144af040ff30","_cell_guid":"e66d7679-14f4-43b2-bb82-287f428cf9fa","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## TFIDF","metadata":{"_uuid":"a7c32320-9d88-41f3-b2cb-ca46ae937357","_cell_guid":"d6882465-02bc-4129-9f9e-13e705863fca","trusted":true}},{"cell_type":"code","source":"LR_clf_tfidf = Pipeline([\n    ('tfidf', TfidfVectorizer(sublinear_tf=True, min_df=5, max_df=0.25, norm='l2', encoding='latin-1',ngram_range=(1, 2), stop_words='english')),\n    ('clf', LogisticRegression(random_state=0, max_iter=2000))\n])\nLR_clf_tfidf.fit(X_train, y_train)\nLR_tfidf_pred_tr = LR_clf_tfidf.predict(X_train)\n\nprint(accuracy_score(y_train, LR_tfidf_pred_tr))\nprint(precision_score(y_train, LR_tfidf_pred_tr, average='weighted'))","metadata":{"_uuid":"dd716690-0a3e-4e7d-9a3c-597483118e9e","_cell_guid":"ab44181b-db08-4bb3-b0ee-38f9165a5523","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create CV training and test scores for various training set sizes\ntrain_sizes, train_scores, test_scores = learning_curve(LR_clf_tfidf, \n                                                        X_train, \n                                                        y_train,\n                                                        # Number of folds in cross-validation\n                                                        cv=3,\n                                                        # Evaluation metric\n                                                        scoring='precision_weighted',\n                                                        # Use all computer cores\n                                                        n_jobs=-1, \n                                                        # 50 different sizes of the training set\n                                                        train_sizes=np.linspace(0.01, 1.0, 10))\n\n# Create means and standard deviations of training set scores\ntrain_mean = np.mean(train_scores, axis=1)\ntrain_std = np.std(train_scores, axis=1)\n\n# Create means and standard deviations of test set scores\ntest_mean = np.mean(test_scores, axis=1)\ntest_std = np.std(test_scores, axis=1)\nfig, ax = plt.subplots(figsize=(15,10))\n# Draw lines\nplt.plot(train_sizes, train_mean, '--', color=\"#111111\",  label=\"Training score\")\nplt.plot(train_sizes, test_mean, color=\"#111111\", label=\"Cross-validation score\")\n\n# Draw bands\nplt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, color=\"#DDDDDD\")\nplt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, color=\"#DDDDDD\")\n\n# Create plot\nplt.title(\"Learning Curve\")\nplt.xlabel(\"Training Set Size\"), plt.ylabel(\"Weighted Precision Score\"), plt.legend(loc=\"best\")\nplt.tight_layout()\nplt.show()","metadata":{"_uuid":"6145f379-3ebf-423d-8388-38b4c121940d","_cell_guid":"329e7e48-490f-4361-a047-2752c04d52ac","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SVM_clf_tfidf = Pipeline([\n    ('tfidf', TfidfVectorizer(sublinear_tf=True, min_df=5, max_df=0.25, norm='l2', encoding='latin-1', ngram_range=(1, 2), stop_words='english')),\n    ('clf', LinearSVC( max_iter=2000))\n])\nSVM_clf_tfidf.fit(X_train, y_train)\nSVM_tfidf_pred_tr = SVM_clf_tfidf.predict(X_train)\n\nprint(accuracy_score(y_train, SVM_tfidf_pred_tr))\nprint(precision_score(y_train, SVM_tfidf_pred_tr, average='weighted'))","metadata":{"_uuid":"12d555c3-b1fa-4fb2-8fe6-fba1130e238a","_cell_guid":"bb21f97f-b172-4634-8949-51a6e87255cb","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create CV training and test scores for various training set sizes\ntrain_sizes, train_scores, test_scores = learning_curve(SVM_clf_tfidf, \n                                                        X_train, \n                                                        y_train,\n                                                        # Number of folds in cross-validation\n                                                        cv=3,\n                                                        # Evaluation metric\n                                                        scoring='precision_weighted',\n                                                        # Use all computer cores\n                                                        n_jobs=-1, \n                                                        # 50 different sizes of the training set\n                                                        train_sizes=np.linspace(0.01, 1.0, 10))\n\n# Create means and standard deviations of training set scores\ntrain_mean = np.mean(train_scores, axis=1)\ntrain_std = np.std(train_scores, axis=1)\n\n# Create means and standard deviations of test set scores\ntest_mean = np.mean(test_scores, axis=1)\ntest_std = np.std(test_scores, axis=1)\nfig, ax = plt.subplots(figsize=(15,10))\n# Draw lines\nplt.plot(train_sizes, train_mean, '--', color=\"#111111\",  label=\"Training score\")\nplt.plot(train_sizes, test_mean, color=\"#111111\", label=\"Cross-validation score\")\n\n# Draw bands\nplt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, color=\"#DDDDDD\")\nplt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, color=\"#DDDDDD\")\n\n# Create plot\nplt.title(\"Learning Curve\")\nplt.xlabel(\"Training Set Size\"), plt.ylabel(\"Weighted Precision Score\"), plt.legend(loc=\"best\")\nplt.tight_layout()\nplt.show()","metadata":{"_uuid":"935ed48f-10ee-4a30-af50-ddfc4de8a85d","_cell_guid":"eb6f19e6-77a6-43e5-b8b5-780a05b0e072","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"NB_clf_tfidf = Pipeline([\n    ('tfidf', TfidfVectorizer(sublinear_tf=True, min_df=5, max_df=0.25, norm='l2', encoding='latin-1', ngram_range=(1, 2), stop_words='english')),\n    ('clf', MultinomialNB())\n])\nNB_clf_tfidf.fit(X_train, y_train)\nNB_tfidf_pred_tr = NB_clf_tfidf.predict(X_train)\n\nprint(accuracy_score(y_train, NB_tfidf_pred_tr))\nprint(precision_score(y_train, NB_tfidf_pred_tr, average='weighted'))","metadata":{"_uuid":"360c382c-6013-4ddf-b62c-fd01956a8a8a","_cell_guid":"7b103c15-1898-4207-a0be-c39aa6a8680d","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create CV training and test scores for various training set sizes\ntrain_sizes, train_scores, test_scores = learning_curve(NB_clf_tfidf, \n                                                        X_train, \n                                                        y_train,\n                                                        # Number of folds in cross-validation\n                                                        cv=3,\n                                                        # Evaluation metric\n                                                        scoring='precision_weighted',\n                                                        # Use all computer cores\n                                                        n_jobs=-1, \n                                                        # 50 different sizes of the training set\n                                                        train_sizes=np.linspace(0.01, 1.0, 10))\n\n# Create means and standard deviations of training set scores\ntrain_mean = np.mean(train_scores, axis=1)\ntrain_std = np.std(train_scores, axis=1)\n\n# Create means and standard deviations of test set scores\ntest_mean = np.mean(test_scores, axis=1)\ntest_std = np.std(test_scores, axis=1)\nfig, ax = plt.subplots(figsize=(15,10))\n# Draw lines\nplt.plot(train_sizes, train_mean, '--', color=\"#111111\",  label=\"Training score\")\nplt.plot(train_sizes, test_mean, color=\"#111111\", label=\"Cross-validation score\")\n\n# Draw bands\nplt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, color=\"#DDDDDD\")\nplt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, color=\"#DDDDDD\")\n\n# Create plot\nplt.title(\"Learning Curve\")\nplt.xlabel(\"Training Set Size\"), plt.ylabel(\"Weighted Precision Score\"), plt.legend(loc=\"best\")\nplt.tight_layout()\nplt.show()","metadata":{"_uuid":"fb1a41da-55da-4287-b359-5337bf2f07fc","_cell_guid":"e81c74b1-80e2-4ffd-92a5-686ec268fe98","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Hyperparameter tuning","metadata":{"_uuid":"cb3f65be-8505-4e66-9454-41dcec52289d","_cell_guid":"c857fbed-ffa6-46e0-a4c6-c3e55d3e073e","trusted":true}},{"cell_type":"code","source":"vect=  CountVectorizer()\nX_train_cnt = vect.fit_transform(X_train)","metadata":{"_uuid":"f81e1caf-3922-4361-a8f0-3b52f751e887","_cell_guid":"e7cef755-c676-4ce6-bf1d-c782c6dd1fab","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### SVM","metadata":{"_uuid":"305a53ed-4342-47e1-b66d-d442e3cda737","_cell_guid":"0115e815-5d46-43c3-b3d4-edfe9ae3951c","trusted":true}},{"cell_type":"code","source":"def svc_param_selection(X, y, nfolds, kernal):\n    Cs = [ 0.1, 1, 10]\n    gammas = [0.01, 0.1, 1]\n    degrees = [0, 1, 2, 3]\n    rbf_param_grid = {'C': Cs, 'gamma' : gammas}\n    linear_param_grid = {'C': Cs}\n    poly_param_grid = {'C': Cs, 'gamma' : gammas, 'degree':degrees}\n    if kernal == 'rbf':\n        grid_search = GridSearchCV(SVC(kernel=kernal), rbf_param_grid, cv=nfolds)\n    elif kernal == 'linear':\n        grid_search = GridSearchCV(SVC(kernel=kernal), linear_param_grid, cv=nfolds)\n    else:\n        grid_search = GridSearchCV(SVC(kernel=kernal), poly_param_grid, cv=nfolds)\n    grid_search.fit(X, y)\n    grid_search.best_params_\n    return grid_search.best_params_","metadata":{"_uuid":"933cd15b-b3ae-41f4-abe5-dbb1442e33dc","_cell_guid":"e474a678-b2dd-45ed-87a3-130fa6df639b","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"svc_param_selection( X_train_cnt,y_train,2, 'rbf')","metadata":{"_uuid":"4fcddb4e-0e40-4a55-892d-4598cb32bf9b","_cell_guid":"186625e9-0b1f-4d68-bc17-c076432b7717","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SVM_clf_counts_rbf = Pipeline([\n    ('vect', CountVectorizer()),\n    ('clf', SVC(C=10, gamma=0.01, kernel='rbf', max_iter=2000))\n])\nSVM_clf_counts_rbf.fit(X_train, y_train)\nSVM_cnt_pred_tr_rbf = SVM_clf_counts_rbf.predict(X_train)\nSVM_cnt_pred_val_rbf = SVM_clf_counts_rbf.predict(X_val)\n\nprint(\"accuracy on training: \",accuracy_score(y_train, SVM_cnt_pred_tr_rbf))\nprint(\"precision on training: \",precision_score(y_train, SVM_cnt_pred_tr_rbf, average='micro'))\nprint(\"precision on validation: \",precision_score(y_val, SVM_cnt_pred_val_rbf, average='micro'))","metadata":{"_uuid":"00bc23be-608d-4331-b340-b2e6a359400f","_cell_guid":"ce8d5fd7-8bd7-43b9-899f-acf8dc248f58","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"svc_param_selection( X_train_cnt,y_train,2, 'linear')","metadata":{"_uuid":"df9ba71c-ec8a-47e8-9f0f-fcfd830dd7d2","_cell_guid":"4a887437-7d87-44cc-9f15-7b1b117f4a47","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SVM_clf_counts_lin = Pipeline([\n    ('vect', CountVectorizer()),\n    ('clf', SVC(C=0.1, kernel='linear'))\n])\nSVM_clf_counts_lin.fit(X_train, y_train)\nSVM_cnt_pred_tr_lin = SVM_clf_counts_lin.predict(X_train)\nSVM_cnt_pred_val_lin = SVM_clf_counts_lin.predict(X_val)\n\nprint(\"accuracy on training: \",accuracy_score(y_train, SVM_cnt_pred_tr_lin))\nprint(\"precision on training: \",precision_score(y_train, SVM_cnt_pred_tr_lin, average='micro'))\nprint(\"precision on validation: \",precision_score(y_val, SVM_cnt_pred_val_lin, average='micro'))","metadata":{"_uuid":"93045a78-2d3a-4fb9-8ed1-30d334eaa1ec","_cell_guid":"0d31502e-5783-42e9-a460-832cd5478170","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Logistic Regression","metadata":{"_uuid":"dce021a9-7e49-467f-bf32-6e8311383cdc","_cell_guid":"c0f073aa-fcc9-42d9-9def-1b84108aa47b","trusted":true}},{"cell_type":"code","source":"def LR_param_selection(X, y, nfolds):\n    Cs = [0.01, 0.1, 1, 10]\n    param_grid = {'C': Cs}\n    grid_search = GridSearchCV(LogisticRegression(random_state=0,max_iter=2000), param_grid, cv=nfolds)\n    grid_search.fit(X, y)\n    grid_search.best_params_\n    return grid_search.best_params_","metadata":{"_uuid":"c8b224f8-d34a-4519-9f5a-ae4076e28fc3","_cell_guid":"f6a253f0-2835-4a59-961b-950241976899","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"LR_param_selection( X_train_cnt,y_train,2)","metadata":{"_uuid":"712d7e0c-f546-4c49-84df-cb2958c6d250","_cell_guid":"76d3c8bf-1a53-4487-89a1-f5ac484d985f","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"LR_clf_counts = Pipeline([('vect', CountVectorizer()),\n                   ('clf', LogisticRegression(C=1,random_state=0, max_iter=2000)),\n                  ])\nLR_clf_counts.fit(X_train, y_train)\nLR_cnt_pred_tr = LR_clf_counts.predict(X_train)\nLR_cnt_pred_val = LR_clf_counts.predict(X_val)\n\nprint(\"accuracy on training: \",accuracy_score(y_train, LR_cnt_pred_tr))\nprint(\"precision on training: \",precision_score(y_train, LR_cnt_pred_tr, average='micro'))\nprint(\"precision on validation: \",precision_score(y_val, LR_cnt_pred_val, average='micro'))","metadata":{"_uuid":"40374fc5-4f51-4249-b7bb-be959fa5226d","_cell_guid":"f349a1d7-07f5-4c30-bde4-7caa013be9a2","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Final Model","metadata":{"_uuid":"89fc3039-522b-48ff-a832-5859b1ebb11d","_cell_guid":"f5d47419-c499-417f-8fa3-0fcb18d16503","trusted":true}},{"cell_type":"code","source":"LR_clf_counts = Pipeline([('vect', CountVectorizer()),\n                   ('clf', LogisticRegression(C=1,random_state=0, max_iter=2000)),\n                  ])\nLR_clf_counts.fit(X_train, y_train)\nLR_cnt_pred_tr = LR_clf_counts.predict(X_train)\nLR_cnt_pred_val = LR_clf_counts.predict(X_val)\nLR_cnt_pred_tst = LR_clf_counts.predict(X_test)\n\n\nprint(\"precision on training: \",precision_score(y_train, LR_cnt_pred_tr, average='micro'))\nprint(\"precision on validation: \",precision_score(y_val, LR_cnt_pred_val, average='micro'))\nprint(\"precision on testing: \",precision_score(y_test, LR_cnt_pred_tst, average='micro'))","metadata":{"_uuid":"247f120e-a91b-4bb7-b91c-da806f712737","_cell_guid":"d2eefab3-f924-4ae4-9575-6187533ea1ad","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create CV training and test scores for various training set sizes\ntrain_sizes, train_scores, test_scores = learning_curve(LR_clf_counts, \n                                                        X_train, \n                                                        y_train,\n                                                        # Number of folds in cross-validation\n                                                        cv=3,\n                                                        # Evaluation metric\n                                                        scoring='precision_weighted',\n                                                        # Use all computer cores\n                                                        n_jobs=-1, \n                                                        # 50 different sizes of the training set\n                                                        train_sizes=np.linspace(0.01, 1.0, 10))\n\n# Create means and standard deviations of training set scores\ntrain_mean = np.mean(train_scores, axis=1)\ntrain_std = np.std(train_scores, axis=1)\n\n# Create means and standard deviations of test set scores\ntest_mean = np.mean(test_scores, axis=1)\ntest_std = np.std(test_scores, axis=1)\nfig, ax = plt.subplots(figsize=(15,10))\n# Draw lines\nplt.plot(train_sizes, train_mean, '--', color=\"#111111\",  label=\"Training score\")\nplt.plot(train_sizes, test_mean, color=\"#111111\", label=\"Cross-validation score\")\n\n# Draw bands\nplt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, color=\"#DDDDDD\")\nplt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, color=\"#DDDDDD\")\n\n# Create plot\nplt.title(\"Learning Curve\")\nplt.xlabel(\"Training Set Size\"), plt.ylabel(\"Weighted Precision Score\"), plt.legend(loc=\"best\")\nplt.tight_layout()\nplt.show()","metadata":{"_uuid":"8246c145-496d-4d59-9078-c68256b328c3","_cell_guid":"d1271514-4948-48a2-a95d-c526c57fa7a2","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels = train['cuisine'].unique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"conf_mat = pd.DataFrame(confusion_matrix(y_val, LR_cnt_pred_val, labels=labels))\nconf_mat_n = conf_mat.divide(conf_mat.sum(axis=1), axis=0)\nfig, ax = plt.subplots(figsize=(20,12))\nsns.heatmap(conf_mat_n, annot=True,xticklabels=labels,yticklabels=labels)\nplt.ylabel('Actual')\nplt.xlabel('Predicted')\nplt.show()","metadata":{"_uuid":"741bb98d-8eb2-4499-adac-3adc744c3b51","_cell_guid":"5be62e01-4749-4157-9417-2b26ba1c29dc","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Predicted_vals = pd.DataFrame({'id' : id_val, 'Ingredients': X_val, 'Actual': y_val, 'Preds': LR_cnt_pred_val})","metadata":{"_uuid":"4489f763-22ed-440a-ac90-ba81ce1294fd","_cell_guid":"0825d57a-126d-48d9-b8e5-ac3619a4eb46","trusted":true},"execution_count":null,"outputs":[]}]}