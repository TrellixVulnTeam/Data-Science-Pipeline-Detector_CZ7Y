{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# What's Cooking?\n\n#### before we start with the problem itself there are some questions we need to answer:\n1. What is the business question?\n2. What each row represent?\n3. What is the evaluation method?\n\n#### for this problem (and all kaggle problems) the answers to these questions is always in the problem's overview page.\n1. What is the category of a dish's cuisine given a list of its ingredients? (Supervised ML Problem)\n2. Each row represent a recipe.\n3. Submissions are evaluated on the categorization accuracy (the percent of dishes that you correctly classify).","metadata":{"_uuid":"94751e7e-c0ba-4c25-bb4f-da6d15d09893","_cell_guid":"dd505386-81f9-4085-877e-aebdacec3ecc","trusted":true}},{"cell_type":"markdown","source":"# 1. Important imports\n### let's start by importing needed libraries.","metadata":{"_uuid":"354028b5-dc2c-4e74-b981-866e837118b3","_cell_guid":"fb833841-c18a-492a-957d-233ececeb8ec","trusted":true}},{"cell_type":"code","source":"# load data libraries\nimport numpy as np # linear algebra library\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport zipfile # to read zip files\nfrom sklearn.model_selection import train_test_split\n\n\n# data understanding libraries\nimport matplotlib.pyplot as plt # ploting library\n%matplotlib inline\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nfrom collections import Counter\n\n\n# data preparation\nimport re\nfrom nltk.stem import PorterStemmer\n\n\n# ADS Creation\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.preprocessing import StandardScaler\n\n# Modeling\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import LinearSVC, SVC\nfrom sklearn.naive_bayes import MultinomialNB\n\n# Evaluation and Model Selection\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\nfrom sklearn import metrics\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.model_selection import GridSearchCV","metadata":{"_uuid":"2ad41d50-32d6-4161-bc19-b4d14ce268c8","_cell_guid":"99e177ed-95b5-42ef-8612-def83d99bee6","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.set_option('display.max_rows', 10000)\npd.set_option('display.max_columns', 500)\npd.set_option('display.max_colwidth', -1)  # or 199\npd.set_option('display.precision',150)\npd.options.display.float_format = '{:,.3f}'.format","metadata":{"_uuid":"f05d9e55-de5b-45a9-8141-1d456a31a73c","_cell_guid":"56c206e7-c3ac-4680-8496-222c6349e030","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Load Data\n### Let's load the data and have a look on it.\n1. data is provieded in a zip file, so we need to unzip it first using zipfile library.\n2. the traning/ testing files available in json file format, to read it we use pd.read_json function.\n        we read the data into pandas dataframes which is a 2-dimensional labeled data structure with columns of\n        potentially different types. You can think of it like a spreadsheet or SQL table.\n3. to view some rows of the dataframe we use df_name.head() method which output the first 5 rows of the dataframe.","metadata":{"_uuid":"2922188f-ffbd-43ac-a35c-f17f69569faf","_cell_guid":"5caf5954-5344-4e47-ac77-b3def01a26fa","trusted":true}},{"cell_type":"code","source":"#unzip the files\narchive_train = zipfile.ZipFile('/kaggle/input/whats-cooking/train.json.zip')\n\n#read training json file \ntrain = pd.read_json(archive_train.read('train.json'))\n\n#output the frist 5 rows\ntrain.head()","metadata":{"_uuid":"4e6a23ee-e8a9-46f3-b040-1aecc8587e9c","_cell_guid":"3f368df2-07a0-40dc-8cfa-40613961a7a8","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> There are only 3 columns: id, cuisine and ingredients","metadata":{"_uuid":"e02385ec-b57a-40c5-9535-ed6601d236d5","_cell_guid":"69d77712-96d4-492f-b086-db1567cb458e","trusted":true}},{"cell_type":"code","source":"train_data, test_data = train_test_split(train, test_size=0.4, random_state=1)\n\ntrain_data = train_data.reset_index(drop=True)\ntest_data = test_data.reset_index(drop=True)","metadata":{"_uuid":"a909f2da-bf31-442f-8043-e6b667067fb4","_cell_guid":"4d437085-3872-4921-b23b-33b1b5df42cc","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Train set size is \",len(train_data))\nprint(\"Test set size is \",len(test_data))","metadata":{"_uuid":"b5d5f40c-20a6-4130-b541-74c7db06e517","_cell_guid":"9cfbe2e6-5820-4ac7-9f0a-80a982d21647","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Data Preparation\n\n## 2.1 Data Cleansing\n### First let's have another look on the ingredients text.","metadata":{"_uuid":"9ab8deb7-5390-45ed-bd0f-ee480ff0ad9a","_cell_guid":"9a5d39af-716c-4a04-93eb-469145127189","trusted":true}},{"cell_type":"code","source":"train_data['ingredients_txt'] = pd.Series([' , '.join(row[\"ingredients\"]) for ind,row in train_data.iterrows()])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ingredients = pd.Series((','.join([','.join(row[\"ingredients\"]) for ind,row in train_data.iterrows()])).split(','))\nwords = pd.Series(' '.join(ingredients).split())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(set(words))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data['ingredients_txt'].sample(150)","metadata":{"_uuid":"5d3cd6e2-82e7-4b67-ad7c-a8602571e1b8","_cell_guid":"3def3afa-5f04-4576-84e6-3ee390735f1e","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ingredients = pd.Series((' '.join([','.join(row[\"ingredients\"]) for ind,row in train_data.iterrows()])).split(','))","metadata":{"_uuid":"b66685e5-faf9-4eea-afe4-1db6d0aff555","_cell_guid":"9241639a-965b-483c-b032-beb484d03073","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.Series([s for s in ingredients if \"-\" in s]).unique()","metadata":{"_uuid":"ba83cd2b-e976-49f0-a86b-95b5b5e5bae9","_cell_guid":"0e465492-ab21-4031-91a6-1747f2bb5ef1","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.Series([s for s in ingredients if any(char.isdigit() for char in s)]).unique()","metadata":{"_uuid":"fe978b5f-a4bb-4445-9e68-7c05dc13dbae","_cell_guid":"05d9b035-9d50-43c6-a4be-d887c346709b","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.Series([s for s in ingredients if \"®\" in s]).unique()","metadata":{"_uuid":"07adec9b-cec8-45e9-83d4-4747856a156f","_cell_guid":"cf09cd01-62b3-4734-8762-a917d30f0667","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.Series([s for s in ingredients if \"'\" in s]).unique()","metadata":{"_uuid":"1c67d15c-6403-4027-af4b-98b7948812cc","_cell_guid":"bce397a7-9f44-41f5-ae85-b7cef8880eeb","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.Series([s for s in words if re.findall('[^a-zA-Z]',re.sub(r'[^\\w\\s]','',s))]).unique()","metadata":{"_uuid":"c715fc57-7d11-484d-9246-d0f3a8f84a43","_cell_guid":"fa83e8cd-6d8c-4edd-9773-9dcc4874b60b","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.Series([s for s in ingredients if \" oz\" in s]).unique()","metadata":{"_uuid":"8dae3046-c45a-457c-a47c-8dc1b5d86716","_cell_guid":"7c392840-bede-44eb-8378-8c07b37c1269","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"What is need to be cleaned?\n- lower and upper case data.\n- punctuation\n- dashed data\n- numbers\n- non-english char","metadata":{"_uuid":"a0d4d1ae-ad37-4d5c-bc1d-b844d204efd7","_cell_guid":"1de791d2-8697-444f-9104-533521d62049","trusted":true}},{"cell_type":"code","source":"stopwords = set([\"Campbell's\",\"hellmann\",\"oz\",\"M&M\",\"Pasoâ„¢\",\"I Can't Believe It's Not Butter!®\"])\nporter = PorterStemmer()\n# lancaster=LancasterStemmer()\n\ndef ret_words(ingredients):\n    ingredients_text = ' '.join(ingredients)\n    ingredients_text = ingredients_text.lower()\n    ingredients_text = ingredients_text.replace('-', '')\n    ingredients_text = ingredients_text.replace(',', ' ')\n    ingredients_text = ingredients_text.replace('\\'', '')\n    words = []\n    for word in ingredients_text.split():\n        if re.findall('[0-9]', word): continue\n        if len(word) <= 2: continue\n        if '®' in word: continue\n        if word in stopwords: continue\n        if re.findall('[^a-zA-Z]',re.sub(r'[^\\w\\s]','',word)): continue\n        if len(word) > 0: words.append(porter.stem(re.sub(r'[^\\w\\s]','',word)))\n    return ' '.join(words)\n\ndef preprocess(df,flag):\n    # add column\n    df[\"ingredients_num\"]=df[\"ingredients\"].apply(len)\n    \n    # Remove recipes with only one Ingredient\n    if flag == 0 :\n        df = df.drop(df[df[\"ingredients_num\"]<=1].index)\n    \n    # Convert list of ingredients to string\n    df['ingredients_txt'] = df[\"ingredients\"].apply(ret_words)\n    \n    return df","metadata":{"_uuid":"629f449c-1f53-4266-92a7-e8b8b9be6bdd","_cell_guid":"bbae16ce-6ad2-4624-9060-fadfae6d625a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_preprocessed = preprocess(train_data,0)\ntest_preprocessed = preprocess(test_data,1)","metadata":{"_uuid":"1817839b-9491-4b1a-b897-18504a14c229","_cell_guid":"e65af9f9-0592-4ccb-91be-85202888e4c1","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_preprocessed.head(10)","metadata":{"_uuid":"40f1ec0b-cbce-4fa8-a57f-638eb717a5ed","_cell_guid":"39f587ec-85d1-4636-9c5f-f916d7f24de4","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(set(pd.Series(' '.join([row[\"ingredients_txt\"] for ind,row in train_preprocessed.iterrows()]).split(' '))))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Sperate the data","metadata":{}},{"cell_type":"code","source":"id_train, X_train, y_train = train_preprocessed['id'], train_preprocessed['ingredients_txt'], train_preprocessed['cuisine']\nid_test, X_test, y_test = test_preprocessed['id'], test_preprocessed['ingredients_txt'], test_preprocessed['cuisine']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## ADS Creation","metadata":{}},{"cell_type":"code","source":"# BoW\nBoW = CountVectorizer()\n\nBoW.fit(X_train)\nCount_data = BoW.transform(X_train)\n\nBoW_X_train = pd.DataFrame(Count_data.toarray(),columns=BoW.get_feature_names())\n\nBoW_X_train","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BoW.fit(X_train.head())\nCount_data = BoW.transform(X_train.head())\nBoW_X_train = pd.DataFrame(Count_data.toarray(),columns=BoW.get_feature_names())\nBoW_X_train","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# TFIDF\nTFIDF = TfidfVectorizer(sublinear_tf=True, min_df=5, max_df=0.25, norm='l2', encoding='latin-1',\\\n                ngram_range=(1, 2), stop_words='english')\n\nTFIDF.fit(X_train)\nCount_data = TFIDF.transform(X_train)\nTFIDF_X_train = pd.DataFrame(Count_data.toarray(),columns=TFIDF.get_feature_names())\n\n\nTFIDF_X_train","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TFIDF = TfidfVectorizer()\nTFIDF.fit(X_train.head(5))\nCount_data = TFIDF.transform(X_train.head(5))\nTFIDF_X_train = pd.DataFrame(Count_data.toarray(),columns=TFIDF.get_feature_names())\n\n\nTFIDF_X_train","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}