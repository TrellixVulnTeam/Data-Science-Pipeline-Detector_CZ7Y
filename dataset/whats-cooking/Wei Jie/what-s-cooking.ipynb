{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"import re\nimport cv2\nimport time\nimport warnings\nimport os\nimport numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport keras\nimport lightgbm as lgb\n\nfrom keras import initializers, regularizers, constraints, optimizers, layers, callbacks\nfrom keras import backend as K\nfrom keras.applications.xception import Xception\nfrom keras.applications.xception import preprocess_input as xception_preprocessor\nfrom keras.applications.inception_v3 import InceptionV3\nfrom keras.applications.inception_v3 import preprocess_input as inception_v3_preprocessor\nfrom keras.applications.inception_resnet_v2 import InceptionResNetV2\nfrom keras.applications.inception_resnet_v2 import preprocess_input as inception_resnet_v2_preprocessor\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, TensorBoard, Callback\nfrom keras.engine import InputSpec, Layer\nfrom keras.layers import Dense, Input, Flatten, Dropout, GlobalAveragePooling2D, Conv1D\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.layers import LSTM, Embedding, Activation, GRU, CuDNNGRU, CuDNNLSTM, BatchNormalization\nfrom keras.layers import Bidirectional, GlobalMaxPool1D, MaxPooling1D, Add, Flatten, Masking\nfrom keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, SpatialDropout1D\nfrom keras.models import Model, load_model\nfrom keras.optimizers import Adam\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.preprocessing.image import img_to_array\nfrom keras.preprocessing.image import load_img\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom os import makedirs\nfrom os.path import expanduser, exists, join\nfrom scipy import stats\nfrom scipy.sparse import hstack, csr_matrix\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom wordcloud import WordCloud\nfrom collections import Counter\nfrom nltk.corpus import stopwords\nfrom nltk.util import ngrams\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import LinearSVC\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn.utils import shuffle\nfrom gensim.models import Word2Vec\nfrom tqdm import tqdm\nfrom gensim.test.utils import datapath\nfrom gensim.models.word2vec import Text8Corpus\nfrom gensim.models.phrases import Phrases, Phraser\n\nwarnings.filterwarnings(\"ignore\")\npd.set_option('max_colwidth',400)\n\ntqdm.pandas()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ba6f14e588ebf11790287d4f11b8276ef05141f8"},"cell_type":"markdown","source":"# EDA & FE"},{"metadata":{"trusted":true,"_uuid":"ab0fa74be8858a2b3197cf2761c85cc6964f5600","_kg_hide-input":false},"cell_type":"code","source":"# Generate path to image\ntrain_set = pd.read_json('../input/whats-cooking/train.json', orient='columns')\ntest_set = pd.read_json('../input/whats-cooking/test.json', orient='columns')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ingredients_individual = Counter([ingredient for ingredient_list in train_set.ingredients for ingredient in ingredient_list])\ningredients_individual = pd.DataFrame.from_dict(ingredients_individual,orient='index').reset_index()\ningredients_individual = ingredients_individual.rename(columns={'index':'Ingredient', 0:'Count'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(train_set.cuisine.unique()))\nprint(len(ingredients_individual.Ingredient.unique()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Category list for current major category\ncat = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19]\nnum_class = len(cat)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"28c5b2320f5ef863df3d0b4e4d9175c59bd61ef0"},"cell_type":"markdown","source":"## One-Hot Encoding\nEnsure nn does not mistake a correlation between each cateogries. There may be problems as there is no ordinal relationship and allowing the representation to lean on any such relationship might be damaging to learning."},{"metadata":{"trusted":true,"_uuid":"0c7299c8895405ef00049595ead1ef89649ba71b","_kg_hide-output":true},"cell_type":"code","source":"lb = LabelEncoder()\nenc = OneHotEncoder(sparse=False)\nenc.fit(np.array(cat).reshape(-1, 1))\n\ny_cat = lb.fit_transform(train_set.cuisine)\ny_ohe = enc.transform(y_cat.reshape(-1, 1))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cdbe2595e31608b72cfbdc8d4bfc75840bfe3a0d"},"cell_type":"markdown","source":"## Prepare Text"},{"metadata":{},"cell_type":"markdown","source":"### Text Cleaning"},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocessing(titles_array):\n    \n    processed_array = []\n    \n    for title in tqdm(titles_array):\n        ingredients = []\n        \n        for ingredient in title:\n            # remove other non-alphabets symbols with space (i.e. keep only alphabets and whitespaces).\n            ingredient = ingredient.strip().lower()\n            ingredient = re.sub('[^a-zA-Z ]', '', ingredient)\n            ingredient = re.sub('   ',  ' ', ingredient)\n            ingredient = re.sub('  ', ' ', ingredient)\n            \n            words = ingredient.split()\n        \n            # keep words that have length of more than 1 (e.g. gb, bb), remove those with length 1.\n            ingredients.append('_'.join([word for word in words if len(word) > 1]))\n            \n        processed_array.append(' '.join(ingredients))\n    \n    return processed_array\n\ntrain_set['processed'] = preprocessing(train_set['ingredients'])\ntest_set['processed'] = preprocessing(test_set['ingredients'])\nprint(train_set['processed'][0])","execution_count":16,"outputs":[{"output_type":"stream","text":"100%|██████████| 39774/39774 [00:01<00:00, 26513.23it/s]\n100%|██████████| 9944/9944 [00:00<00:00, 26423.96it/s]\n","name":"stderr"},{"output_type":"execute_result","execution_count":16,"data":{"text/plain":"0                                                                                                             romaine_lettuce black_olives grape_tomatoes garlic pepper purple_onion seasoning garbanzo_beans feta_cheese_crumbles\n1                                                                                                        plain_flour ground_pepper salt tomatoes ground_black_pepper thyme eggs green_tomatoes yellow_corn_meal milk vegetable_oil\n2                                                                                          eggs pepper salt mayonaise cooking_oil green_chilies grilled_chicken_breasts garlic_powder yellow_onion soy_sauce butter chicken_livers\n3                                                                                                                                                                                                   water vegetable_oil wheat salt\n4    black_pepper shallots cornflour cayenne_pepper onions garlic_paste milk butter salt lemon_juice water chili_powder passata oil ground_cumin boneless_chicken_skinless_thigh garam_masala double_cream natural_yogurt bay_leaf\nName: processed, dtype: object"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"### Custom Word Vector with Word2Vec"},{"metadata":{"trusted":true},"cell_type":"code","source":"sentences = pd.concat([train_set['processed'], test_set['processed']],axis=0)\ntrain_sentences = list(sentences.progress_apply(str.split).values)\n\nmodel = Word2Vec(sentences=train_sentences, \n                 sg=1, \n                 min_count=1,\n                 size=300,  \n                 workers=4)\n\nmodel.wv.save_word2vec_format('custom_glove_300d.txt')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"16eb2d1f3fbf33fc70606ddc6de747e367a77287"},"cell_type":"code","source":"print('Max number of ingredient in train is {0:.0f}.'.format(np.max(train_set.ingredients.map(lambda l: len(l)))))\nprint('Max number of ingredient in test is {0:.0f}.'.format(np.max(test_set.ingredients.map(lambda l: len(l)))))\nprint('Vocab Size: ', len(model.wv.vocab.keys()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Weijie Countvectorizer"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Custom Embedding\n\n- Text data will have to be encoded for the NN.\n- Custom trained embedding is used in this case.\n- Embedding matrix for the model will also be created"},{"metadata":{"trusted":true,"_uuid":"afaa845d44d72b9997ce037ab547ab4010701311","_kg_hide-input":false},"cell_type":"code","source":"max_len = max(np.max(train_set.ingredients.map(lambda l: len(l))), np.max(test_set.ingredients.map(lambda l: len(l))))\n\nmax_features = len(model.wv.vocab.keys())\ntk = Tokenizer(lower = True, filters='', num_words=max_features)\ntk.fit_on_texts(train_sentences)\n\nembedding_path = \"custom_glove_300d.txt\"\nembed_size = 300\n\ntrain_set1 = tk.texts_to_sequences(train_set['processed'])\ntrain_set1 = pad_sequences(train_set1, maxlen = max_len)\n\ntest_set1 = tk.texts_to_sequences(test_set['processed'])\ntest_set1 = pad_sequences(test_set1, maxlen = max_len)\n\ndef get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\nembedding_index = dict(get_coefs(*o.strip().split(\" \")) for o in open(embedding_path))\n\nword_index = tk.word_index\nnb_words = min(max_features, len(word_index))\nembedding_matrix = np.zeros((nb_words+1, embed_size))\nfor word, i in word_index.items():\n    if i >= max_features: continue\n    embedding_vector = embedding_index.get(word)\n    if embedding_vector is not None:embedding_matrix[i] = embedding_vector\n\nmax_features = nb_words\nprint('No. of Features is {0:.0f}.'.format(max_features))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e3b48429e015146fa598257593003dd234116f19","_kg_hide-input":true},"cell_type":"markdown","source":"# Functions\n***\n*  Custom attention layer for text"},{"metadata":{"trusted":true,"_uuid":"7861669f72f36145c25911926a51bc688f51d473","_kg_hide-input":true},"cell_type":"code","source":"class Attention(Layer):\n    def __init__(self, step_dim,\n                 W_regularizer=None, b_regularizer=None,\n                 W_constraint=None, b_constraint=None,\n                 bias=True, **kwargs):\n        self.supports_masking = True\n        #self.init = initializations.get('glorot_uniform')\n        self.init = initializers.get('glorot_uniform')\n        self.W_regularizer = regularizers.get(W_regularizer)\n        self.b_regularizer = regularizers.get(b_regularizer)\n        self.W_constraint = constraints.get(W_constraint)\n        self.b_constraint = constraints.get(b_constraint)\n        self.bias = bias\n        self.step_dim = step_dim\n        self.features_dim = 0\n        super(Attention, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        assert len(input_shape) == 3\n\n        self.W = self.add_weight((input_shape[-1],),\n                                 initializer=self.init,\n                                 name='{}_W'.format(self.name),\n                                 regularizer=self.W_regularizer,\n                                 constraint=self.W_constraint)\n        self.features_dim = input_shape[-1]\n\n        if self.bias:\n            self.b = self.add_weight((input_shape[1],),\n                                     initializer='zero',\n                                     name='{}_b'.format(self.name),\n                                     regularizer=self.b_regularizer,\n                                     constraint=self.b_constraint)\n        else:\n            self.b = None\n\n        self.built = True\n\n    def compute_mask(self, input, input_mask=None):\n        # do not pass the mask to the next layers\n        return None\n\n    def call(self, x, mask=None):\n        features_dim = self.features_dim\n        step_dim = self.step_dim\n        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)), K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n\n        if self.bias:\n            eij += self.b\n\n        eij = K.tanh(eij)\n        a = K.exp(eij)\n        # apply mask after the exp. will be re-normalized next\n        if mask is not None:\n            # Cast the mask to floatX to avoid float64 upcasting in theano\n            a *= K.cast(mask, K.floatx())\n        # in some cases especially in the early stages of training the sum may be almost zero\n        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n        a = K.expand_dims(a)\n        weighted_input = x * a\n        \n        return K.sum(weighted_input, axis=1)\n\n    def compute_output_shape(self, input_shape):\n        #return input_shape[0], input_shape[-1]\n        return input_shape[0],  self.features_dim","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"58c29867982a695f802700c77ebb213a52e18659"},"cell_type":"markdown","source":"\nModel Architecture:\n- Input\n- Embedding1\n- GRU\n- Attention Layer\n- Dense Layer"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"19030bd2b6277b9e5dbc6933ca0abcf68fc7b3f9"},"cell_type":"code","source":"def textv1():\n    inp = Input(shape = (max_len,))\n    x = Embedding(max_features + 1, embed_size, weights = [embedding_matrix], trainable = True)(inp)\n    x = SpatialDropout1D(0.3)(x)\n    x = Bidirectional(CuDNNGRU(256, return_sequences = True))(x)\n    x = Attention(max_len)(x)\n    x = BatchNormalization()(x)\n    x = Dense(128, activation='relu') (x)\n    x = Dropout(0.5)(x)\n    x = Dense(num_class, activation = \"softmax\")(x)\n    \n    model = Model(inputs = inp, outputs = x)\n    \n    return model\n\nmodel1 = textv1()\nmodel1.summary()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4238c8ecc07a68e5d5d41633e13a8c216d361ef6","_kg_hide-input":false},"cell_type":"markdown","source":"### textv2\n\n#### MODEL HAVE REALLY LOW ACCURACY. NOT USING.\n\nModel Architecture:\n- Input\n- Embedding\n    - GRU\n        - Conv1D - Average\n        - Conv1D - Max\n    - LSTM\n        - Conv1D - Average\n        - Conv1D - Max\n- Concatenation Layer\n- Dense Layer"},{"metadata":{"trusted":true,"_uuid":"447f99ac9780537644de2e6f891259563c09807e","_kg_hide-input":false},"cell_type":"code","source":"def textv2():\n    inp = Input(shape = (max_len,))\n    x1 = Embedding(max_features + 1, embed_size, weights = [embedding_matrix], trainable = True)(inp)\n    x1 = SpatialDropout1D(0.3)(x1)\n    \n    x_gru = Bidirectional(CuDNNGRU(128, return_sequences = True))(x1)\n    \n    x_conv1 = Conv1D(32, kernel_size=4, padding='valid', kernel_initializer='he_uniform')(x_gru)\n    avg_pool1_gru = GlobalAveragePooling1D()(x_conv1)\n    max_pool1_gru = GlobalMaxPooling1D()(x_conv1)\n    \n    x_conv2 = Conv1D(32, kernel_size=3, padding='valid', kernel_initializer='he_uniform')(x_gru)\n    avg_pool2_gru = GlobalAveragePooling1D()(x_conv2)\n    max_pool2_gru = GlobalMaxPooling1D()(x_conv2)\n    \n    \n    x_lstm = Bidirectional(CuDNNLSTM(128, return_sequences = True))(x1)\n    \n    x_conv3 = Conv1D(32, kernel_size=4, padding='valid', kernel_initializer='he_uniform')(x_lstm)\n    avg_pool1_lstm = GlobalAveragePooling1D()(x_conv3)\n    max_pool1_lstm = GlobalMaxPooling1D()(x_conv3)\n    \n    x_conv4 = Conv1D(32, kernel_size=3, padding='valid', kernel_initializer='he_uniform')(x_lstm)\n    avg_pool2_lstm = GlobalAveragePooling1D()(x_conv4)\n    max_pool2_lstm = GlobalMaxPooling1D()(x_conv4)\n    \n    \n    x = concatenate([avg_pool1_gru, max_pool1_gru, avg_pool2_gru, max_pool2_gru,\n                     avg_pool1_lstm, max_pool1_lstm, avg_pool2_lstm, max_pool2_lstm])\n    x = BatchNormalization()(x)\n    x = Dense(128, activation='relu') (x)\n    x = Dropout(0.5)(x)\n    x = Dense(num_class, activation = \"softmax\")(x)\n    \n    model = Model(inputs = inp, outputs = x)\n    \n    return model\n\n#model2 = textv2()\n#model2.summary()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4280a91b5138033f93a4753857c3b8c2aea5dadc","_kg_hide-input":false},"cell_type":"markdown","source":"### textv3\n\n#### MODEL HAVE REALLY LOW ACCURACY. NOT USING.\n\nModel Architecture:\n- Input\n- Embedding1\n- Masking\n- LSTM\n- Dense Layer"},{"metadata":{"trusted":true,"_uuid":"25cf8eecc7330a92b1ad67de65342b6bea32f935","_kg_hide-input":false},"cell_type":"code","source":"def textv3():\n    inp = Input(shape = (max_len,))\n    x = Embedding(max_features + 1, embed_size, weights = [embedding_matrix], trainable = True)(inp)\n    \n    x = SpatialDropout1D(0.3)(x)\n    x = Masking()(x)\n    x = LSTM(256)(x)\n    \n    x = BatchNormalization()(x)\n    x = Dense(128, activation='relu') (x)\n    x = Dropout(0.5)(x)\n    x = Dense(num_class, activation = \"softmax\")(x)\n    \n    model = Model(inputs = inp, outputs = x)\n    \n    return model\n\n#model3 = textv3()\n#model3.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training\n\nTraining model on k-fold validation."},{"metadata":{"trusted":true,"_uuid":"3c5041e397d4bac60b849eb088827454cf1af294"},"cell_type":"code","source":"n_folds=20\nepochs=10\nverbose=2 # 1 for Debugging. 2 for Committing\nbatch_size = 64\n\nfor i in range(n_folds):\n    print(\"Training on Fold: \", i + 1)\n\n    x_train, x_valid, y_train, y_valid = train_test_split(train_set1, y_ohe, test_size = 0.1, random_state = i)\n    \n    # Model 1\n    model1 = textv1()\n    callbacks = [EarlyStopping(monitor='val_loss', patience=3, verbose=1, min_delta=1e-4),\n                 ModelCheckpoint(filepath='temp1.hdf5', verbose=1,save_best_only=True, mode='auto')]\n    model1.compile(loss = \"categorical_crossentropy\", optimizer='adam', metrics = [\"accuracy\"])\n    model1.fit(x_train, y_train, batch_size = batch_size, epochs = epochs, verbose = verbose,\n               validation_data = (x_valid, y_valid), callbacks = callbacks, shuffle=True)\n    \n    model1.load_weights('temp1.hdf5')\n    model1.compile(loss = \"categorical_crossentropy\", optimizer='adam', metrics = [\"accuracy\"])\n    \n    '''\n    # Model 2\n    model2 = textv2()\n    callbacks = [EarlyStopping(monitor='val_loss', patience=3, verbose=1, min_delta=1e-4),\n                 ModelCheckpoint(filepath='temp2.hdf5', verbose=1,save_best_only=True, mode='auto')]\n    model2.compile(loss = \"categorical_crossentropy\", optimizer='adam', metrics = [\"accuracy\"])\n    model2.fit(x_train, y_train, batch_size = batch_size, epochs = epochs, verbose = verbose,\n               validation_data = (x_valid, y_valid), callbacks = callbacks, shuffle=True)\n    \n    model2.load_weights('temp2.hdf5')\n    model2.compile(loss = \"categorical_crossentropy\", optimizer='adam', metrics = [\"accuracy\"])\n    '''\n    '''\n    # Model 3\n    model3 = textv3()\n    callbacks = [EarlyStopping(monitor='val_loss', patience=3, verbose=1, min_delta=1e-4),\n                 ModelCheckpoint(filepath='temp3.hdf5', verbose=1,save_best_only=True, mode='auto')]\n    model3.compile(loss = \"categorical_crossentropy\", optimizer='adam', metrics = [\"accuracy\"])\n    model3.fit(x_train, y_train, batch_size = batch_size, epochs = epochs, verbose = verbose,\n               validation_data = (x_valid, y_valid), callbacks = callbacks, shuffle=True)\n    \n    model3.load_weights('temp3.hdf5')\n    model3.compile(loss = \"categorical_crossentropy\", optimizer='adam', metrics = [\"accuracy\"])\n    '''\n    # Predict\n    if (i == 0):\n        train_pred = model1.predict(train_set1, verbose=2)\n        test_pred = model1.predict(test_set1, verbose=2)\n    else:\n        train_pred += model1.predict(train_set1, verbose=2)\n        test_pred += model1.predict(test_set1, verbose=2)\n    '''\n    train_pred += model2.predict(train_set1, verbose=2)\n    test_pred += model2.predict(test_set1, verbose=2)\n    train_pred += model3.predict(train_set1, verbose=2)\n    test_pred += model3.predict(test_set1, verbose=2)\n    '''\n    print(\"=======\"*12, end=\"\\n\\n\\n\")\n\ntrain_pred /= n_folds\ntest_pred /= n_folds","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Save Results"},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = np.round(np.argmax(test_pred, axis=1)).astype(int)\ntest_set.drop(['ingredients', 'processed'], axis=1)\n\nsub = pd.DataFrame({'id': test_set['id'], 'cuisine': lb.inverse_transform(predictions)}, columns=['id', 'cuisine'])\nsub.to_csv('predicitions.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Download Link\n\nKaggle does not save csv. Using download link as bypass to get file directly without having to commit."},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"from IPython.display import HTML\nimport pandas as pd\nimport numpy as np\nimport base64\n\ndef create_download_link(df, filename = \"submission.csv\", title = \"Download CSV file\"):  \n    csv = df.to_csv(index=False)\n    b64 = base64.b64encode(csv.encode())\n    payload = b64.decode()\n    html = '<a download=\"{filename}\" href=\"data:text/csv;base64,{payload}\" target=\"_blank\">{title}</a>'\n    html = html.format(payload=payload,title=title,filename=filename)\n    return HTML(html)\n\n# create a link to download the dataframe\ncreate_download_link(sub)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}