{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"91ff2bd7-1627-2db6-d406-5625cc0517fe"},"outputs":[],"source":"'''\nAuthor         : Oguzhan Gencoglu\nContact        : oguzhan.gencoglu@topdatascience.com, oguzhan.gencoglu@tut.fi\nDescription    : 17th Place out of 1388 teams (top 2%) Submission for Kaggle What's Cooking Competition\n'''\n\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nimport sys\nreload(sys)\nsys.setdefaultencoding('utf-8')\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder\nfrom keras.models import Sequential\nfrom keras.layers.core import Dense, Dropout\nfrom nltk.stem.wordnet import WordNetLemmatizer\nimport re\nimport itertools\nimport os.path\nimport json\nfrom datetime import datetime\n\ndef k_to_one_hot(k_hot_vector):\n    # This function converts k-hot target vector to one-hot target matrix\n    \n    classes = np.unique(k_hot_vector)\n    one_hot_matrix = []\n    \n    for i in np.arange(len(classes)):\n        row = (k_hot_vector == classes[i]).astype(int, copy = False)\n        if len(one_hot_matrix) == 0:\n            one_hot_matrix = row\n        else:\n            one_hot_matrix = np.vstack((one_hot_matrix, row))\n            \n    return classes, one_hot_matrix.conj().transpose()\n    \n    \ndef read_data(filename):\n    # read data into lists\n    \n    with open(filename) as data_file:    \n        data = json.load(data_file)\n        \n    ids, cuisines, ingredients = [], [], []\n    if 'cuisine' in data[0].keys():\n        for i in range(len(data)):\n            ids.append(data[i]['id'])\n            cuisines.append(data[i]['cuisine'])\n            ingredients.append(data[i]['ingredients'])\n    else:\n        for i in range(len(data)):\n            ids.append(data[i]['id'])\n            ingredients.append(data[i]['ingredients'])    \n                \n    return ids, cuisines, ingredients\n    \n    \ndef create_submission(test_ids, guess):\n    # create submission in proper format\n    \n    sub = np.transpose(np.vstack((test_ids, guess)))\n    sub = np.vstack((['id', 'cuisine'], sub))\n    sub_file_name = 'submission_' + str(datetime.now())[0:16] +'.csv'\n    sub_file_name = sub_file_name.replace(' ', '_')\n    sub_file_name = sub_file_name.replace(':', '-')\n    np.savetxt(sub_file_name, sub, delimiter=\",\", fmt=\"%s\")\n    \n    return None  \n    \n\ndef remove_numbers(ing):\n    # remove numbers from ingredients\n    \n    return [[re.sub(\"\\d+\", \"\", x) for x in y] for y in ing]\n\n    \ndef remove_special_chars(ing):\n    # remove certain special characters from ingredients\n   \n    ing = [[x.replace(\"-\", \" \") for x in y] for y in ing] \n    ing = [[x.replace(\"&\", \" \") for x in y] for y in ing] \n    ing = [[x.replace(\"'\", \" \") for x in y] for y in ing] \n    ing = [[x.replace(\"''\", \" \") for x in y] for y in ing] \n    ing = [[x.replace(\"%\", \" \") for x in y] for y in ing] \n    ing = [[x.replace(\"!\", \" \") for x in y] for y in ing] \n    ing = [[x.replace(\"(\", \" \") for x in y] for y in ing] \n    ing = [[x.replace(\")\", \" \") for x in y] for y in ing] \n    ing = [[x.replace(\"/\", \" \") for x in y] for y in ing] \n    ing = [[x.replace(\"/\", \" \") for x in y] for y in ing] \n    ing = [[x.replace(\",\", \" \") for x in y] for y in ing] \n    ing = [[x.replace(\".\", \" \") for x in y] for y in ing] \n    ing = [[x.replace(u\"\\u2122\", \" \") for x in y] for y in ing] \n    ing = [[x.replace(u\"\\u00AE\", \" \") for x in y] for y in ing] \n    ing = [[x.replace(u\"\\u2019\", \" \") for x in y] for y in ing] \n\n    return ing\n    \n    \ndef make_lowercase(ing):\n    # make all letters lowercase for all ingredients\n    \n    return [[x.lower() for x in y] for y in ing]\n    \n    \ndef remove_extra_whitespace(ing):\n    # removes extra whitespaces\n    \n    return [[re.sub( '\\s+', ' ', x).strip() for x in y] for y in ing] \n    \n    \ndef stem_words(ing):\n    # word stemming for ingredients\n    \n    lmtzr = WordNetLemmatizer()\n    \n    def word_by_word(strng):\n        \n        return \" \".join([\"\".join(lmtzr.lemmatize(w)) for w in strng.split()])\n    \n    return [[word_by_word(x) for x in y] for y in ing] \n    \n    \ndef remove_units(ing):\n    # remove certain words from ingredients\n    \n    remove_list = ['g', 'lb', 's', 'n']\n        \n    def check_word(strng):\n        \n        s = strng.split()\n        resw  = [word for word in s if word.lower() not in remove_list]\n        \n        return ' '.join(resw)\n\n    return [[check_word(x) for x in y] for y in ing] \n    \n\ndef extract_feats(ingredients, uniques):\n    # each ingredient + each word as feature\n    \n    feats_whole = np.zeros((len(ingredients), len(uniques)))\n    for i in range(len(ingredients)):\n        for j in ingredients[i]:\n            feats_whole[i, uniques.index(j)] = 1\n            \n    new_uniques = []\n    for m in uniques:\n        new_uniques.append(m.split())\n    new_uniques = list(set(list(itertools.chain.from_iterable(new_uniques))))\n    \n    feats_each = np.zeros((len(ingredients), len(new_uniques))).astype(np.uint8)\n    for i in range(len(ingredients)):\n        for j in ingredients[i]:\n            for k in j.split():\n                feats_each[i, new_uniques.index(k)] = 1\n            \n    return np.hstack((feats_whole, feats_each)).astype(bool)\n    \n    \ndef load_model():\n    # load neural net model architectiure\n    \n    mdl = Sequential()\n    mdl.add(Dense(512, init='glorot_uniform', activation='relu', \n                                        input_shape=(train_feats.shape[1],)))\n    mdl.add(Dropout(0.5))\n    mdl.add(Dense(128, init='glorot_uniform', activation='relu'))\n    mdl.add(Dropout(0.5))\n    mdl.add(Dense(20, activation='softmax'))\n    mdl.compile(loss='categorical_crossentropy', optimizer='adadelta')\n    \n    return mdl    \n\n    \nif __name__ == '__main__':\n    \n    # preprocess training set\n    print(\"\\nPreprocessing...\\n\")  \n    train_ids, train_cuisines, train_ingredients = read_data('train.json')\n    train_ingredients = make_lowercase(train_ingredients)\n    train_ingredients = remove_numbers(train_ingredients)\n    train_ingredients = remove_special_chars(train_ingredients)\n    train_ingredients = remove_extra_whitespace(train_ingredients)\n    train_ingredients = remove_units(train_ingredients)\n    train_ingredients = stem_words(train_ingredients)\n    \n    # preprocess test set\n    test_ids, test_cuisines, test_ingredients = read_data('test.json')\n    test_ingredients = make_lowercase(test_ingredients)\n    test_ingredients = remove_numbers(test_ingredients)\n    test_ingredients = remove_special_chars(test_ingredients)\n    test_ingredients = remove_extra_whitespace(test_ingredients)\n    test_ingredients = remove_units(test_ingredients)\n    test_ingredients = stem_words(test_ingredients)\n    \n    # encode   \n    print(\"Encoding...\\n\")  \n    le = LabelEncoder()\n    targets = le.fit_transform(train_cuisines)\n    classes, targets = k_to_one_hot(targets)\n    \n    # extract features\n    print(\"Feature extraction...\\n\") \n    uniques = list(set([item for sublist in train_ingredients + test_ingredients for item in sublist]))\n    train_feats = extract_feats(train_ingredients, uniques)\n    test_feats = extract_feats(test_ingredients, uniques)\n  \n    # train\n    n_ensemble = 10\n    for ens in range(n_ensemble):\n        print(\"\\n\\tTraining...\", ens)\n        model = load_model()\n        \n        # if model already exists, continue training\n        model_name = 'model' + str(ens) + '.hdf5'\n        if os.path.isfile(model_name):\n            model.load_weights(model_name)\n            \n        model.fit(train_feats, targets, nb_epoch=2500, batch_size=4096, \n                                                        show_accuracy = True)\n        model.save_weights(model_name, overwrite=True)\n\n    # create submission out of the ensemble\n    preds = []\n    for ens in range(n_ensemble):\n        print(\"\\nSubmission\", ens)\n        model = load_model()\n\n        model_name = 'model' + str(ens) + '.hdf5'\n        model.load_weights(model_name)            \n        preds.append(model.predict_proba(test_feats))\n\n    # final cuisine decision: argmax of sum of log probabilities  \n    print(\"\\nPredicting...\")      \n    preds = sum(np.log(preds))\n    guess = le.inverse_transform(np.argmax(preds, axis=1))\n    create_submission(test_ids, guess) "}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}