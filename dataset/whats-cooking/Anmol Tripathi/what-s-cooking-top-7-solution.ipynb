{"cells":[{"metadata":{},"cell_type":"markdown","source":"### Presenting a solution to get into top 7% of leaderboard using Support Vector Classifier with an accuracy score of 0.81063"},{"metadata":{},"cell_type":"markdown","source":"<center><img src=\"https://media3.s-nbcnews.com/j/newscms/2019_41/3044956/191009-cooking-vegetables-al-1422_ae181a762406ae9dce02dd0d5453d1ba.nbcnews-fp-1200-630.jpg\" alt=\"Cooking Image from Google\"></center>"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport sklearn\nimport os\nimport json\nimport re\nimport nltk\nimport zipfile\n\nfrom datetime import datetime\nfrom sklearn.preprocessing import LabelEncoder\nfrom nltk.stem import WordNetLemmatizer\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nfrom sklearn.feature_extraction.text import CountVectorizer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for t in ['train','test']:\n    with zipfile.ZipFile(\"../input/whats-cooking/{}.json.zip\".format(t),\"r\") as z:\n        z.extractall(\".\")\n    \nwith open('./train.json') as data_file:    \n    data = json.load(data_file)\n    \nwith open('./test.json') as test_file:\n    test = json.load(test_file)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.DataFrame(data)\ntest_df = pd.DataFrame(test)\n\ntest_ids = test_df['id']\n\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# EDA"},{"metadata":{"trusted":true},"cell_type":"code","source":"(df.isnull().sum() / len(df))*100 # No null values in train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(test_df.isnull().sum() / len(test_df))*100 # No null values in test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(10,10))\nper_vals = round(df[\"cuisine\"].value_counts(normalize=True)*100, 2)\nfor i, v in enumerate(per_vals):\n    ax.text(v + 3, i + .25, str(v)+\"%\", color='blue', fontweight='bold')\ndf[\"cuisine\"].value_counts().plot.barh(ax = ax)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(22,7))\nextensive_ing_list = []\nfor x in df['ingredients']:\n    for y in x:\n        extensive_ing_list.append(y)\n        \nextensive_ing_list = pd.Series(extensive_ing_list)\nextensive_ing_list.value_counts().sort_values(ascending=False).head(30).plot.bar(ax = ax)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Creating ingredients per cuisine"},{"metadata":{"trusted":true},"cell_type":"code","source":"cuisine = df[\"cuisine\"].unique()\n\nall_cus = dict()\nfor cs in cuisine:\n    i = []\n    for ing_list in df[df['cuisine']==cs]['ingredients']:\n        for ing in ing_list:\n            i.append(ing)\n    all_cus[cs] = i\n\nall_cus.keys()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 25 MOST USED INGREDIENTS- CUISINE WISE"},{"metadata":{"trusted":true},"cell_type":"code","source":"for key in all_cus.keys():\n    fig, ax = plt.subplots(figsize=(25,2))\n    pd.Series(all_cus[key]).value_counts().head(25).plot.bar(ax=ax, title=key)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 25 LEAST USED INGREDIENTS- CUISINE WISE"},{"metadata":{"trusted":true},"cell_type":"code","source":"# for key in all_cus.keys():\n#     fig, ax = plt.subplots(figsize=(25,2))\n#     pd.Series(all_cus[key]).value_counts().tail(25).plot.bar(ax=ax, title=key)\n#     plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preprocessing"},{"metadata":{},"cell_type":"markdown","source":"### String Preprocess"},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess_df(df):\n    \n    def process_string(x):\n        x = [\" \".join([WordNetLemmatizer().lemmatize(q) for q in p.split()]) for p in x] #Lemmatization\n        x = list(map(lambda x: re.sub(r'\\(.*oz.\\)|crushed|crumbles|ground|minced|powder|chopped|sliced','', x), x))\n        x = list(map(lambda x: re.sub(\"[^a-zA-Z]\", \" \", x), x))   # To remove everything except a-z and A-Z\n        x = \" \".join(x)                                 # To make list element a string element \n        x = x.lower()\n        return x\n    \n    df = df.drop('id',axis=1)\n    df['ingredients'] = df['ingredients'].apply(process_string)\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_cuisine_cumulated_ingredients(df):\n    cuisine_df = pd.DataFrame(columns=['ingredients'])\n\n    for cus in cuisine:\n        st = \"\"\n        for x in df[df.cuisine == cus]['ingredients']:\n            st += x\n            st += \" \"\n        cuisine_df.loc[cus,'ingredients'] = st\n\n    cuisine_df = cuisine_df.reset_index()\n    cuisine_df = cuisine_df.rename(columns ={'index':'cuisine'})\n    return cuisine_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = preprocess_df(df)\ntest_df = preprocess_df(test_df)\n\ncuisine_df = get_cuisine_cumulated_ingredients(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = df['ingredients']\ntarget = df['cuisine']\ntest = test_df['ingredients']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Count Vectorizer"},{"metadata":{"trusted":true},"cell_type":"code","source":"def count_vectorizer(train, test=None):\n    cv = CountVectorizer()\n    train = cv.fit_transform(train)\n    if test is not None:\n        test = cv.transform(test)\n        return train, test, cv\n    else:\n        return train, cv","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train_cv , test_cv, cv = count_vectorizer(train,test)\n# cuisine_data_cv, cuisine_cv = count_vectorizer(cuisine_df['ingredients'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## TFiDF Vectorizer"},{"metadata":{"trusted":true},"cell_type":"code","source":"def tfidf_vectorizer(train, test=None):\n    tfidf = TfidfVectorizer(stop_words='english',\n                             ngram_range = ( 1 , 1 ),analyzer=\"word\", \n                             max_df = .57 , binary=False , token_pattern=r'\\w+' , sublinear_tf=False)\n    train = tfidf.fit_transform(train)\n    if test is not None:\n        test = tfidf.transform(test)\n        return train, test, tfidf\n    else:\n        return train, tfidf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_tfidf, test_tfidf, tfidf = tfidf_vectorizer(train,test)\ncuisine_data_tfidf, cuisine_tfidf = tfidf_vectorizer(cuisine_df['ingredients'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Cluster as a parameter"},{"metadata":{},"cell_type":"markdown","source":"There are 20 different types of cuisine to classify. It gives an intuition that certain groups of cuisine may have much more similarity than others. We can try to find such groups as well"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cluster import KMeans\nfrom sklearn.decomposition import KernelPCA,PCA,TruncatedSVD\n\ndef get_kmeans_wcss(data, n_limit=15):\n    wcss = [] #Within cluster sum of squares (WCSS)\n    for i in range(1,n_limit):\n        km = KMeans(init='k-means++', n_clusters=i, n_init=10)\n        km.fit(data)\n        wcss.append(km.inertia_)\n    plt.title(\"Elbow Method\")\n    plt.plot(range(1, n_limit), wcss)\n    plt.xlabel(\"Number of clusters\")\n    plt.ylabel(\"WCSS\")\n    return wcss\n    \n    \ndef kmeans(data, n):\n    km = KMeans(init='k-means++', n_clusters=n, n_init=10)\n    km = km.fit(data)\n    return km.predict(data), km \n\n\ndef get_PCA(data, n_components=2):\n    pca = PCA(n_components = n_components)\n    reduced_data = pca.fit_transform(data)\n    explained_variance = pca.explained_variance_ratio_\n    print(explained_variance)\n    return reduced_data, pca, explained_variance\n\ndef get_kernel_PCA(data, n_components=2, kernel='rbf'):\n    kpca = KernelPCA(n_components = 2, kernel = kernel)\n    reduced_data = kpca.fit_transform(data)\n    explained_variance = kpca.explained_variance_ratio_\n    print(explained_variance)\n    return reduced_data, kpca, explained_variance\n\ndef get_TSVD(data, n_components=2, n_ittr=5, algorithm='randomized'):\n    tsvd = TruncatedSVD(n_components=n_components, n_iter=n_ittr, algorithm=algorithm)\n    reduced_data = tsvd.fit_transform(data)\n    explained_variance = tsvd.explained_variance_ratio_\n    print(explained_variance)\n    return reduced_data, tsvd, explained_variance\n\n\n\ndef create_pca_graph(cluster_pca, red_pca, n_clus):\n\n    c_mask = []\n    c_x = []\n    c_y = []\n    \n    for i in range(0,n_clus):\n        c_mask.append([x for x in cluster_pca==i])\n    \n    for i in range(0,n_clus):\n        c_x.append([a[0] for a, b in zip(red_pca, c_mask[i]) if b])\n        c_y.append([a[1] for a, b in zip(red_pca, c_mask[i]) if b])\n\n    colours = ['red','blue','green','orange','purple','cyan','black','magenta']\n    \n    for i in range(0,n_clus):\n        plt.scatter(c_x[i], c_y[i], s=30, c=colours[i], label='Cluster {}'.format(i))\n        \n        \n#     for i in range(0,20):\n#         label = label_list[i]\n#         plt.annotate(label, (c_x[i],c_y[i]), textcoords=\"offset points\", xytext=(0,10), # distance from text to points (x,y)\n#                      ha='center') # horizontal alignment can be left, right or center\n        \n     \n    plt.title(\"Clusters of PCA\")\n    plt.xlabel(\"PCA 1\")\n    plt.ylabel(\"PCA 2\")\n    plt.legend()\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In order to visualize clusters, let us reduce the data using PCA"},{"metadata":{"trusted":true},"cell_type":"code","source":"# red_tsvd, tsvd, var_tsvd = get_TSVD(train_cv,2)  #Used because train_cv is a sparse matrix. PCA won't work\n# red_pca, pca, var_pca = get_PCA((train_cv).toarray(),2)\n# red_pca, pca, var_pca = get_PCA((train_tfidf).toarray(),2)\n# red_tsvd, tsvd, var_tsvd = get_TSVD(train_tfidf,2)  #Used because train_tfidf is a sparse matrix. PCA won't work\n# red_kpca, kpca, var_kpca = get_kernel_PCA(train_cv,2)  #Uses excessive RAM","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"red_cuisine_pca, cus_pca, var_cus_pca = get_PCA((cuisine_data_tfidf).toarray(),2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nwcss_pca = get_kmeans_wcss(red_cuisine_pca,20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"WCSS for reduced cuisine dataset shows that number of clusters = 3 should be an apt choice (elbow point)"},{"metadata":{"trusted":true},"cell_type":"code","source":"cluster_cus_pca, km_cus_pca = kmeans(red_cuisine_pca,3)\ncluster_cus_pca","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"create_pca_graph(cluster_cus_pca, red_cuisine_pca, 3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can notice there are 3 clusters of cuisines"},{"metadata":{"trusted":true},"cell_type":"code","source":"# cuisine_df[cluster_cus_pca==0]['cuisine']\n# cuisine_df[cluster_cus_pca==1]['cuisine']\n# cuisine_df[cluster_cus_pca==2]['cuisine']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**CLUSTER 1: <br>**\n> GREEK<br>\n> SPANISH<br>\n> ITALIAN<br>\n> FRENCH<br>\n> MOROCCAN<br>\n> RUSSIAN<br>\n\n<br><br>\n**CLUSTER 2: <br>**\n> FILIPINO<br>\n> CHINESE<br>\n> THAI<br>\n> VIETNAMESE<br>\n> KOREAN<br>\n\n<br><br>\n**CLUSTER 3: <br>**\n> SOUTHERN US<br>\n> INDIAN<br>\n> JAMAICAN<br>\n> MEXICAN<br>\n> BRITISH<br>\n> CAJUN CREOLE<br>\n> BRAZILIAN<br>\n> JAPANESE<br>\n> IRISH<br>"},{"metadata":{},"cell_type":"markdown","source":"## Creating Actual Clusters "},{"metadata":{},"cell_type":"markdown","source":"### NOTE: Don't add cluster for best results. (Skip this section. Move to Model Development)"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nwcss = get_kmeans_wcss(train_tfidf,30)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"WCSS shows number of clusters = 19 can be an apt choice (elbow point)"},{"metadata":{"trusted":true},"cell_type":"code","source":"cluster, km = kmeans(train_tfidf,19) # train_cv or train_tfidf\ncluster_test = km.predict(test_tfidf)\ncluster","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\nenc = OneHotEncoder(handle_unknown='ignore')\nenc.fit(cluster.reshape(-1, 1))\ncluster_encoded = enc.transform(cluster.reshape(-1, 1)).toarray()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cluster_test_encoded = enc.transform(cluster_test.reshape(-1, 1)).toarray()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Adding cluster as a feature\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_tfidf_nonsparse = np.append((train_tfidf).toarray(), cluster_encoded, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_tfidf_nonsparse = np.append((test_tfidf).toarray(), cluster_test_encoded, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"TRAINING DATASET: Added cluster of shape {} to train_cv of shape {} as a column\".format(cluster_encoded.shape, train_tfidf.shape))\nprint(\"TESTING DATASET: Added cluster of shape {} to test_cv of shape {} as a column\".format(cluster_test_encoded.shape, test_tfidf.shape))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Development"},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy import sparse\n\n# train = sparse.csr_matrix(train_tfidf_nonsparse)\n# test = sparse.csr_matrix(test_tfidf_nonsparse)\n\ntrain = train_tfidf # USE THIS FOR BEST RESULTS (0.8106)\ntest = test_tfidf # USE THIS FOR BEST RESULTS (0.8106)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Linear SVC"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import LinearSVC, SVC\nfrom sklearn.metrics import f1_score\n\nparam_grid = {'C': [0.001, 0.1, 1, 10, 50, 100, 500, 1000, 5000],  \n              'penalty': ['l1','l2'],\n             'loss': ['hinge','squared hinge']} \n\ngrid = GridSearchCV(LinearSVC(), param_grid, refit = True, verbose = 3, n_jobs=-1, scoring='f1_micro')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ngrid.fit(train, target) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid.best_score_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import f1_score\nfrom sklearn.svm import LinearSVC, SVC\n\ndef evalfn(C, gamma):\n    s = SVC(C=float(C), gamma=float(gamma), kernel='rbf', class_weight='balanced')\n    f = cross_val_score(s, train, target, cv=5, scoring='f1_micro')\n    return f.max()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from bayes_opt import BayesianOptimization\nfrom sklearn.model_selection import cross_val_score\nnew_opt = BayesianOptimization(evalfn, {'C': (0.1, 1000),  \n              'gamma': (0.0001, 1)  })","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"############################################\n### OPTIMIZED PARAMETERS ARE SHOWN BELOW ###\n##  HYPER PARAMETER OPT IS TIME CONSUMING ##\n############################################\n\n# %%time\n# new_opt.maximize(n_iter=15, init_points=3)   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# new_opt.max","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# OPTIMIZED PARAMETERS\n# {'target': 0.7945391461758937,\n#  'params': {'C': 604.5300203551828, 'gamma': 0.9656489284085462}}\n\n# With cluster(n=19) as a parameter:\n# {'target': 0.7940917661847894,\n#  'params': {'C': 509.674609734803, 'gamma': 0.724238238886398}}\n\nC = 604.5300203551828\ngamma = 0.9656489284085462\n\nclf = SVC(C=float(C), gamma=float(gamma), kernel='rbf')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nclf.fit(train, target)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pickle\nfrom datetime import datetime\n\nnow = datetime.now()\nprint(\"MODEL SAVED AT {}\".format(now))\nmodel_name = \"SVC-whats-cooking-trial-final2-{}.pickle.dat\".format(now)\npickle.dump(clf, open(model_name, \"wb\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# clf = pickle.load(open(\"SVC-whats-cooking-trial-final2-{}.pickle.dat\", \"rb\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = clf.predict(test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"my_submission = pd.DataFrame({'id':test_ids})\nmy_submission['cuisine'] = y_pred\nnow = datetime.now()\nmy_submission.to_csv('submission_{}.csv'.format(now), index=False)\nprint('Saved file to disk as submission_{}.csv.'.format(now))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# FERTIG"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}