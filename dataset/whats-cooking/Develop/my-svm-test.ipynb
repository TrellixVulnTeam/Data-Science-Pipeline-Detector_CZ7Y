{"cells":[{"metadata":{"trusted":true,"_uuid":"d96abc737208de7a4cc86c8698346fe78517afa2"},"cell_type":"code","source":"import json\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n%matplotlib inline\n\nDEBUG = False","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0bb07acbf975abfcf52a27ac2e8054f96fa55001"},"cell_type":"code","source":"if (DEBUG):\n    train = pd.read_json('./train.json')\n    test = pd.read_json('./test.json')\nelse:\n    train = pd.read_json('../input/train.json')\n    test = pd.read_json('../input/test.json')\ntrain.ingredients = train.ingredients.apply(lambda l: \", \".join(l))\ntest.ingredients = test.ingredients.apply(lambda l: \", \".join(l))\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4d003f7e86c3b32b8d9a9d96f58b9a1f8ffe39cb"},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n# words - (\\w+?)(?:,\\s|\\s|$)    ingredients - (.+?)(?:,\\s|$)\nvectorizerIngr = CountVectorizer(analyzer = \"word\", tokenizer = None, preprocessor = None, stop_words = None,\\\n                             max_features = 50000, binary = False, token_pattern=r'(.+?)(?:,\\s|$)') \nvectorizerIngr.fit(train['ingredients'])\nbagOfWords = vectorizerIngr.transform(train['ingredients'])\nbagOfWordsTest = vectorizerIngr.transform(test['ingredients'])\n\nvectorizerWords = CountVectorizer(analyzer = \"word\", tokenizer = None, preprocessor = None, stop_words = None,\\\n                             max_features = 50000, binary = False, token_pattern=r'(\\w+?)(?:,\\s|\\s|$)') \nvectorizerWords.fit(train['ingredients'])\nbagOfWordsWords = vectorizerWords.transform(train['ingredients'])\nbagOfWordsTestWords = vectorizerWords.transform(test['ingredients'])\n\nvectorizerTfDf =  TfidfVectorizer(analyzer = \"word\", tokenizer = None, preprocessor = None, stop_words = None,\\\n                             max_features = 50000, binary = False, token_pattern=r'(\\w+?)(?:,\\s|\\s|$)')\n\nvectorizerTfDf.fit(train['ingredients'])\nbagOfWordsTfDf = vectorizerTfDf.transform(train['ingredients'])\nbagOfWordsTestTfDf = vectorizerTfDf.transform(test['ingredients'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6ed44c1bdbd9617a52a0eb4f99c1d272cf28ac2f"},"cell_type":"code","source":"import warnings, sklearn\nimport scipy.sparse as sparse\nfrom sklearn import *","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"318ef5ed67d2db0be258ec7daee612d3d41d647e"},"cell_type":"code","source":"bagOfWords = bagOfWords.astype('float')\nbagOfWordsWords = bagOfWordsWords.astype('float')\nfrom sklearn.cluster import KMeans\n#clusters = KMeans(n_clusters=2, random_state=0).fit_predict(data0)\n#clusterDummies = pd.get_dummies(clusters, prefix='cluster')\n\nkolvoWords = np.sum(bagOfWordsWords, axis=1)\nkolvoIngr = np.sum(bagOfWords, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4dd4c7b5eec6ddc3fd9f15f4103bbcac576e9d3c"},"cell_type":"code","source":"lambdaKolvoIngr =  0.0606\nlambdaKolvoWords = 0.05\nlambdaTfDf = 1\nlambdaBagOfW = 0.2121\nlambdaWords = 0.116\n#data1 = sparse.hstack((data0, clusterDummies*lambdaClusters, kolvo*lambdaKolvo))\ndata1 = sparse.hstack((lambdaTfDf * bagOfWordsTfDf, \\\n                       bagOfWords * lambdaBagOfW, \\\n                       bagOfWordsWords * lambdaWords, \\\n                       kolvoWords*lambdaKolvoWords, \\\n                       kolvoIngr*lambdaKolvoIngr))\n\nSVM = svm.LinearSVC(C=0.3)\nSVM.fit(data1, train['cuisine'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0d43fe936b6ae4071181a4718f665c25785a3dba"},"cell_type":"code","source":"bagOfWordsTest = bagOfWordsTest.astype('float')\nbagOfWordsTestWords = bagOfWordsTestWords.astype('float')\n\nkolvoWordsTest = np.sum(bagOfWordsTestWords, axis=1)\nkolvoIngrTest = np.sum(bagOfWordsTest, axis=1)\n\ndataTest = sparse.hstack((lambdaTfDf * bagOfWordsTestTfDf, \\\n                       bagOfWordsTest * lambdaBagOfW, \\\n                       bagOfWordsTestWords * lambdaWords, \\\n                       kolvoWordsTest*lambdaKolvoWords, \\\n                       kolvoIngrTest*lambdaKolvoIngr))\n#res = SVM.predict(dataTest)\"\"\"\nclass FuncCreator:\n    def __init__(self):\n        self.dict_cuisine = dict()\n        self.ind = 0\n    def __call__(self, it):\n        x = it.lower()\n        if not (x in self.dict_cuisine.keys()):\n            self.dict_cuisine[x] = self.ind\n            self.ind = self.ind + 1\n        return it\n    \n    def getNameFromId(self, id):\n        for key, val in self.dict_cuisine.items():\n            if (val == id):\n                return key\n        return \"UNKNOW\"\n    \nclass MySVM:\n    def __init__(self, C=0.3):\n        self.SVMS = dict()\n        self.rep = FuncCreator()\n        self.keys=[]\n        self.C=C\n        \n    def learn(self, data1, y, res, ar=[]):\n        self.rep = FuncCreator()\n        y.apply(self.rep)\n        self.keys=np.array(list(self.rep.dict_cuisine.keys()))\n        i = 0\n        for key in self.keys:\n            #print(key)\n            ndata=y==key\n            #print(ndata)\n            if (i >= len(ar)):\n                SVM = svm.LinearSVC(C=self.C)\n            else:\n                SVM = svm.LinearSVC(C=ar[i])\n            SVM.fit(data1, ndata)\n            self.SVMS[key] = SVM\n            i = i + 1\n            \n    def predict(self, data, ar=[]):\n        arr = np.zeros((data.shape[0],len(self.keys)))\n        i = 0\n        for key in self.keys:\n            if (len(ar) == 0 or ((len(ar)) < len(self.rep.dict_cuisine.keys()))):\n                arr[:,i]=self.SVMS[key].decision_function(data)\n            else:\n                arr[:,i]=ar[i] * self.SVMS[key].decision_function(data)\n            i = i+1\n        return self.keys[np.argmax(arr, axis=1)]\n    \nSVM = MySVM(C=0.29)\nSVM.learn(data1, train['cuisine'], 0, [0.25, 0.4, 0.25, 0.25, 0.1, 0.3, 0.3, 0.3, 0.2, 0.35, 0.35, 0.5, 0.25, 0.3, 0.3, 0.35, 0.6, 0.4, 0.55, 0.45])\nres = SVM.predict(dataTest)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"814180d16f117faf8274046e6cf70f1acadfa936"},"cell_type":"code","source":"f =open(\"svm_output.csv\", \"w\")\nf.write(\"id,cuisine\\n\")\ni = 0\nfor item in test[\"id\"]:\n    f.write(str(item))\n    f.write(\",\")\n    f.write(res[i])\n    f.write(\"\\n\")\n    i= i +1\n    \n    \nf.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"e3c84e24fb7185292fb01c709c986acb4fc56f40"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"de69260fcba6c0c0c3d4067319da320915a6b1e9"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"21c4e34ea0b56695f684828fa1a9060bb0fe25e1"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"c4adda153307c860e33dbf64b6745ee9d45a4b3a"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"86fd6750c0e17a980d6630e3a619ce37853c7504"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"fc2e8061516949b7f9f5bf14d596fa90ac49a5c7"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}