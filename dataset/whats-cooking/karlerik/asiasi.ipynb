{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\nimport inflection as infl\n\nimport re\nimport nltk\nfrom nltk.stem import WordNetLemmatizer\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import model_selection\n\nfrom sklearn.svm import LinearSVC\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"def custom_fixes(word):\n    \n    #large, all, purpose, non, fra, che, for, added, ready, not, the, \n    #and, fine, five, leav, ton, sec, msg, min, sum, tel\n    dropped_words = [\"all\", \"purpose\", \"the\", \"large\", \"fra\", \"che\", \"for\", \"added\", \"ready\", \n                     \"the\", \"and\", \"five\", \"ton\", \"sec\", \"msg\", \"min\", \"sum\", \"tel\"]\n    \n    if word == \"chilli\" or word == \"chily\" or word == \"chile\":\n        return \"chili\"\n    if word == \"leafe\" or word == \"leav\":\n        return \"leaf\"\n    if word == \"olife\":\n        return \"olive\"\n    if word == \"clofe\":\n        return \"clove\"\n    \n    if word in dropped_words or len(word) < 3:\n        return \"\"\n    \n    return word","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"25e80f5f7a08a353110856fbe9cc8d11c240a154"},"cell_type":"code","source":"train_data = pd.read_json(\"../input/train.json\")\ntest_data = pd.read_json(\"../input/test.json\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f6100a2de5c989c7285cb3daf199ff2e94852c1b"},"cell_type":"code","source":"infl.singularize(\"olives\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"13256158ca58fad66291adbfcc1a02072b6f99c4"},"cell_type":"code","source":"train_data['another_clean_string'] = [ ' '.join(\n    [ WordNetLemmatizer().lemmatize(re.sub('[^A-Za-z]', ' ', ingredient)) for ingredient in recipe ]\n).strip().lower() for recipe in train_data['ingredients'] ]\n\n\ntrain_data['clean_string'] = [ ' '.join(\n    [ custom_fixes(infl.singularize(word)) for word in string.split(\" \")]\n) for string in train_data['another_clean_string']]\n\ntrain_data.head(25)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"27863ae9df3860f62ac2ee8c0f7e141292718e07"},"cell_type":"code","source":"test_data['another_clean_string'] = [ ' '.join(\n    [ WordNetLemmatizer().lemmatize(re.sub('[^A-Za-z]', ' ', ingredient)) for ingredient in recipe ]\n).strip().lower() for recipe in test_data['ingredients'] ]\n\n\n\ntest_data['clean_string'] = [ ' '.join(\n    [ custom_fixes(infl.singularize(word)) for word in string.split(\" \")]\n) for string in test_data['another_clean_string']]\n\ntest_data.head(25)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"77e520d7cf6cff4b63738c9643ea2500e6301fe5"},"cell_type":"code","source":"testing = train_data['clean_string'].values\ntesting\n\narray = []\n\nfor recipe in testing:\n    recipe_ingredients = recipe.split(\" \")\n    for recipe_ingredient in recipe_ingredients:\n        array.append(recipe_ingredient)\n        \ndf = pd.DataFrame(array)\nvalues = df[0].value_counts()\nvalues\n\nprint(values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fec13fc79075996e83f53fcc45702cbb7561257f"},"cell_type":"code","source":"from PIL import Image\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n\nimport matplotlib.pyplot as plt\n% matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4a846fc79ff288aa6e9717c9a21a97f4c9ab0165","scrolled":false},"cell_type":"code","source":"string = [' '.join(word for word in df[0])]\n#print(string)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"deb0e989b898b99c5c669a0b1c9f585d039f1bfd"},"cell_type":"code","source":"wordcloud = WordCloud().generate(string[0])\n\n# Display the generated image:\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a91f6b9e47433cc6fe719a0002b866855aac8a28"},"cell_type":"code","source":"wordcloud = WordCloud(width=3000, height=1200, max_words=100, background_color=\"white\").generate(string[0])\nplt.figure()\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4329a7541adda9fd2764b3d56c4dd04b1a5c04ed"},"cell_type":"code","source":"wordcloud.to_file(\"words.png\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6810c29f7ab7ee387ac63236fa8feaca68137066"},"cell_type":"code","source":"train_data_corpus = train_data['clean_string']\ntrain_data_vectorizer = TfidfVectorizer(stop_words='english',\n                             ngram_range = ( 1 , 1 ),analyzer=\"word\", \n                             max_df = .57 , binary=False , token_pattern=r'(\\w+?)(?:,\\s|\\s|$)' , sublinear_tf=False)\n\n#train_data_vectorizer = TfidfVectorizer(stop_words='english',\n#                             ngram_range = ( 1 , 1 ),analyzer=\"word\", \n#                             max_df = .57 , binary=False , token_pattern=r'\\w+' , sublinear_tf=False)\n\n#train_data_vectorizer = TfidfVectorizer(stop_words='english',\n#                             ngram_range = ( 1 , 1 ),analyzer=\"word\", \n#                             max_df = .50 , binary=False , token_pattern=r'\\w+' , sublinear_tf=False)\n\n#train_data_vectorizer = TfidfVectorizer(stop_words='english',\n#                             ngram_range = ( 1 , 1 ),analyzer=\"word\", \n#                             max_df = .67 , binary=False , token_pattern=r'\\w+' , sublinear_tf=False)\n\n# This is only .25% accurate\n#train_data_vectorizer = TfidfVectorizer(ngram_range = ( 1 , 1 ),analyzer=\"char\", \n#                             max_df = .57 )\n\ntrain_data_tfidf = train_data_vectorizer.fit_transform(train_data_corpus).todense()\n\n#TFIDF stands for term frequency- inverse document frequency.\n#The TFIDF weight is used in text mining and IR. The weight is a measure used to evaluate how important a word is to a document in a collection of documents.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"80a6e6ed5fbc51ed0d6bc701649c991d2b4adc98"},"cell_type":"code","source":"test_data_corpus = test_data['clean_string']\ntest_data_vectorizer = TfidfVectorizer(stop_words='english')\ntest_data_tfidf = train_data_vectorizer.transform(test_data_corpus)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ce4a37f5e19fa1814bb16e986c69604eb33940c2"},"cell_type":"code","source":"train_data_predictors = train_data_tfidf\ntrain_data_targets = train_data['cuisine']\ntest_data_predictors = test_data_tfidf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a817c7149d58be859ddcc1b32b22815bb4ec2fc9"},"cell_type":"code","source":"# Logistic Regression\n#https://medium.com/@aneesha/svm-parameter-tuning-in-scikit-learn-using-gridsearchcv-2413c02125a0\n\nparameters = {'C':[1, 10]}\nclf = LogisticRegression()\nclassifier = model_selection.GridSearchCV(clf, parameters)\n\nclassifier = classifier.fit(train_data_predictors,train_data_targets)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"352c2db848fce5cb926c0f1a8f6daed41e5299d0"},"cell_type":"code","source":"# Linear SVC\n\nclf = LinearSVC()\nclassifier = LinearSVC(C=0.90, penalty=\"l2\", dual=False)\nclassifier = classifier.fit(train_data_predictors,train_data_targets)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fcb80b8e2f4ea5a5686f75432411983eba81562b"},"cell_type":"code","source":"predictions = classifier.predict(test_data_predictors)\ntest_data['cuisine'] = predictions\nprint(test_data[[ 'id', 'cuisine' ]].head(10))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d90fa686876225fc42cbc6e31c5487b70a3bd4fa"},"cell_type":"code","source":"test_data[[ 'id', 'cuisine' ]].to_csv(\"testsubmission.csv\", index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}