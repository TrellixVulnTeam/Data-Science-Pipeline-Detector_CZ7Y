{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"1d5192b3-fbdb-c242-8968-acf89b022311"},"source":"I had the idea to apply distributed word vectors ([word2vec]( https://en.wikipedia.org/wiki/Word2vec)) to this dataset. \n\nWord2vec, in a very high level, is an algorithm capable to learn the relationship between words using the context (neighbouring words), and encodes those relatinships in a vector. Using these vectors, we can cluster the words in or library, or even do operations. The classic example of the latter is; \"king - man + woman = queen.\"\n\nWord2vec uses recurrent neural networks to learn, then usually works better with huge datasets (billions of words), but we will see how it performs with the cooking dataset, where each receipt will be a sentence. One of the best features of this algorithm published by Google is the speed. Other recurrent neural networks had been proposed, however they were insanely CPU time consuming. If you want more detailed information about this, I strongly suggest you to read about [here]( https://www.google.ca/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&uact=8&ved=0ahUKEwiU5PmPoOPSAhVn5IMKHcIUDmIQFggaMAA&url=https%3A%2F%2Fpapers.nips.cc%2Fpaper%2F5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf&usg=AFQjCNFvn2t3S41dxIocYbx5EpeOwmjXVQ&sig2=IxYxjFBtWI_BkYLKymPAsw&bvm=bv.149760088,d.amc), [here]( https://www.quora.com/How-does-word2vec-work) and [here]( https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/) : . Now let’s tackle our dataset. \n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"aa9d6443-ca75-2369-24c4-7528032ccb2f"},"outputs":[],"source":"from __future__ import print_function\n\n# Handle data\nimport json\nimport operator\nimport collections\nimport re\n\n# Handle table-like data \nimport numpy as np\nimport pandas as pd\n\n# Model Algorithms\n# we could use also tensor flow, there are multiple implementations of word2vec\nfrom gensim.models import word2vec\n\n# Modelling Helpers, see above the description\nfrom sklearn.manifold import TSNE\n\n# Visualisation\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\nimport seaborn as sns\n%matplotlib inline"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4b88afb8-776f-b74e-ac20-7bab9ef95507"},"outputs":[],"source":"# Load the dataset\n# json format labels: cuisine, id number and ingredients (list)\ntrainrecipts = json.load(open('../input/train.json','r'))"},{"cell_type":"markdown","metadata":{"_cell_guid":"ce777f5f-27f7-b39a-dbf1-7310dc95a2e4"},"source":"## General Exploration of the Dataset\n\nAlthough this dataset is probably quite clean, do a general exploration of our data is really good habit, no matter what kind of model you will apply to them. Plot a few frequencies and means can provide a value information of all sorts about potential problems, bias, typos, etc."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c0d86594-f493-80b4-3704-904bbba0dc8a"},"outputs":[],"source":"# Quick&dirty code to extract info2list\nraw_ingredients = list()\n\nfor recipt in trainrecipts:\n    for ingredient in recipt[u'ingredients']:\n        raw_ingredients.append(ingredient.strip())\n        \n\nraw_cuisines = list()\nfor recipt in trainrecipts:\n    raw_cuisines.append(recipt[u'cuisine'].strip())\n\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"bba12074-ec79-ca9c-57ec-4a6d83685ba2"},"outputs":[],"source":"\n# use Counter to get frequencies \ncounts_ingr = collections.Counter(raw_ingredients)\ncounts_cuis = collections.Counter(raw_cuisines)"},{"cell_type":"markdown","metadata":{"_cell_guid":"dc0bde8c-e8be-4b96-ae7a-3abfb6bff874"},"source":"### Cuisines"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"46c91bf6-26a3-e470-b40e-f330522d3977"},"outputs":[],"source":"# this will help us to have an idea how our corpora of \n# ingredients looks like\nprint('Size Ingredients dataset (with repetition):  \\t{}'.format((len(raw_ingredients))))\nprint('Unique Ingredients dataset: \\t\\t\\t{}'.format((len(counts_ingr.values()))))\n\n# This will provide a distribution of cusines, indirect \n# info of the ingredients\nprint('Total # of recipts \\t\\t\\t\\t{}'.format(len(raw_cuisines)))\nprint('Total # of Cuisines \\t\\t\\t\\t{}'.format((len(counts_cuis.values()))))\n\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"47626cd7-006c-22a0-52bb-9ecfb6e24283"},"outputs":[],"source":"# top 10\ncounts_cuis.most_common(10)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"71abe6bc-29b8-236f-0db0-f8fcf197c898"},"outputs":[],"source":"# Distribution \n\nprint(np.mean(list(counts_cuis.values())))\nprint(np.std(list(counts_cuis.values())))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"56889cc6-3d6b-0c23-4eea-93539a5bbeb3"},"outputs":[],"source":"# lets plot this \n# sort\nx_cu = [cu for cu, frq in counts_cuis.most_common()]\ny_frq = [frq for cu, frq in counts_cuis.most_common()]\nfbar = sns.barplot(x = x_cu, y = y_frq)\n# xlabels\nfor item in fbar.get_xticklabels():\n    item.set_rotation(90)"},{"cell_type":"markdown","metadata":{"_cell_guid":"77bb241f-c28c-633c-e945-57df352dbcd7"},"source":"For instance, as we can see in the first plot, Italian and Mexican receipts represent more than a third of the entire dataset. So, it is probable that this will affect how our vectors form. It is good to keep this on mind for this or any other further model we apply to this dataset. Let’s check if there is a bias on the size of the receipts. \n\n"},{"cell_type":"markdown","metadata":{"_cell_guid":"b8890e07-2306-4cc6-c9ad-b8db6fbaa141"},"source":"### Ingredients\n\nOther interesting parameter is the size of the receipts, how long are they? there is any bias?"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9c23fed0-f1f7-4f54-446a-f8181a4ec10f"},"outputs":[],"source":"# init a dict with a empty list\nnum_ingredients = dict(zip(counts_cuis.keys(), [list() for x in counts_cuis.keys()]))\nfor recipt in trainrecipts:\n    # append the number in the list\n    num_ingredients[recipt['cuisine']].append(len(recipt['ingredients']))\n\nlen(num_ingredients)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a97967b0-6835-0400-da76-b7dbc31d5b07"},"outputs":[],"source":"for cu, frq in num_ingredients.items():\n\n    print('{}    \\t\\t{:.2f}'.format(cu, np.mean(frq)))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7df7b076-1cbb-e762-529e-f3c0e6a6be64"},"outputs":[],"source":"x_cu = [cu for cu, frq in num_ingredients.items()]\ny_frq = [np.mean(frq) for cu, frq in num_ingredients.items()]\nerr = [np.std(frq) for cu, frq in num_ingredients.items()]\nfbar = sns.barplot(x = x_cu, y = y_frq, yerr=err)\n# xlabels\nfor item in fbar.get_xticklabels():\n    item.set_rotation(90)"},{"cell_type":"markdown","metadata":{"_cell_guid":"024b4e34-2143-218a-03a3-d2497b347afd"},"source":"Well, on terms of size, all the receipts appear to be more similar. Then, let’s focus on the ingredients. As I mentioned above, I guess this dataset is really clean, or at least more than a real-world dataset and I do not expect any pre-processing. Also, full disclaimer, I did not check any of the models submitted to kaggel, and my intention is build the word2vec with as a proof of concept."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"870ed4fb-3362-ee5b-a443-2339beb57919"},"outputs":[],"source":"# Dispersion of the frequencies Ingredients\nprint(np.mean(list(counts_ingr.values())))\nprint(np.std(list(counts_ingr.values())))\n\n"},{"cell_type":"markdown","metadata":{"_cell_guid":"2d9ebea3-ca92-1bc1-5828-4195406c012c"},"source":"The frequency of the ingredients presents a similar scenario, a few ingredients are tremendously popular. Make sense, some ingredients as salt, or water are common in any recipes.  Half of the ingredients only appear 4 or less times in the dataset, that is wide less what I expected. Let's check the most popular."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"640ff33b-3657-6a2f-4808-1b1764869be8"},"outputs":[],"source":"# This is to big to plot, let's check the percentiles\nprint(np.median(list(counts_ingr.values())))\nprint(np.percentile(list(counts_ingr.values()), [25., 50., 75., 99.]))"},{"cell_type":"markdown","metadata":{"_cell_guid":"05e3f22a-c855-aabd-8132-b02b756a7c5c"},"source":"Half of the ingredients only appear 4 or less times in the dataset, that is wide less what I expected. Let's check the most populars."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ddd1a015-dacb-dd25-ae72-35580f081c32"},"outputs":[],"source":"# top 15\ncounts_ingr.most_common(15)"},{"cell_type":"markdown","metadata":{"_cell_guid":"75f9350a-0369-f9c0-52e5-c2e3c1b88bdb"},"source":"A few ingredients like Salt and water make a lot of sense that they are highly frequent, but the present of olive oil among these omnipresent ingredients make me think that is an artefact of the bias of the dataset to the Italian cooking."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c3fedb15-a7c3-2f4e-c57d-768c199d68f5"},"outputs":[],"source":"# Tail 50\ncounts_ingr.most_common()[-50:]"},{"cell_type":"markdown","metadata":{"_cell_guid":"30705d93-7aba-4fd4-c229-23aa9ce2cc0c"},"source":"There are some very specific ingredients... I expect that some of those are typos, or just versions of other ingredients. Also notice that in the dataset the same ingredient can present in different formats, garlic, and garlic cloves. First a quick search for parenthesis or similar symbols that rise a red flag to typos, or weird writing\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8e5a4b33-28f0-819c-9386-78751b3ce3e1"},"outputs":[],"source":"symbols = list()\n\nfor recipt in trainrecipts:\n\n    # I want ingredient remove \n    for ingredient in recipt['ingredients']:\n        if re.match(\"\\(|@|\\$\\?\", ingredient.lower()):\n            symbols.append(ingredient)\nlen(symbols)\ncounts_symbols = collections.Counter(symbols)\ncounts_symbols.most_common(20)"},{"cell_type":"markdown","metadata":{"_cell_guid":"a7dedf33-1eef-da49-5b1d-a4d84e1ed265"},"source":"Well, I guess some pre-processing could be good, but let's see how our model behave. Let's train the neural network with a raw version of the dataset."},{"cell_type":"markdown","metadata":{"_cell_guid":"16118276-098d-c42a-9aad-4b4d0dd068a3"},"source":"# Word2Vec"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"076e7cd7-3413-1611-2a3d-9b795f7e21a4"},"outputs":[],"source":"sentences = list()\n# one hot ingredients\n\n\nfor recipt in trainrecipts:\n    clean_recipt = list()\n    # I want ingredient remove \n    for ingredient in recipt['ingredients']:\n        # remove this description from the ingredients\n        # minimal preprocessing\n        ingredient =  re.sub(r'\\(.*oz.\\)|crushed|crumbles|ground|minced|powder|chopped|sliced',\n                             '', \n                             ingredient)\n        clean_recipt.append(ingredient.strip())\n    sentences.append(clean_recipt)\n        \nlen(sentences)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"726e736d-a9c9-b7af-18d9-fb9e8d47d7ec"},"outputs":[],"source":"# Set values for NN parameters\nnum_features = 300    # Word vector dimensionality                      \nmin_word_count = 3    # 50% of the corpus                    \nnum_workers = 4       # Number of CPUs\ncontext = 10          # Context window size; \n                      # let's use avg recipte size                                                                                  \ndownsampling = 1e-3   # threshold for configuring which \n                    # higher-frequency words are randomly downsampled\n\n# Initialize and train the model \nmodel = word2vec.Word2Vec(sentences, workers=num_workers, \\\n            size=num_features, min_count = min_word_count, \\\n            window = context, sample = downsampling)\n\n# If you don't plan to train the model any further, calling \n# init_sims will make the model much more memory-efficient.\nmodel.init_sims(replace=True)"},{"cell_type":"markdown","metadata":{"_cell_guid":"367d5cf5-fad2-a0ad-732d-7ca35ed23c8d"},"source":"Once the model is train we can ask him a few questions, for example what is similar to feta cheese. Looks like all the ingredients are stuff that you expect to find with feta cheese, and looks like they belong to Greek cuisine."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b8cb309b-b8ad-9130-bace-65db7d08af96"},"outputs":[],"source":"## Results"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d6b1353c-fb3d-74ed-c241-e0d011d74554"},"outputs":[],"source":"model.most_similar(u'feta cheese')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"49e9f676-10b9-3196-0e9a-031bd72ad1e0"},"outputs":[],"source":"model.similarity('broccoli', 'bacon')\n\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"615a1a93-0045-6358-f84a-aae703e3f984"},"outputs":[],"source":"model.similarity('broccoli', 'carrots')\n\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9e82dbc4-58bc-49d4-0587-27e02ea8106a"},"outputs":[],"source":"model.similarity('broccoli', u'mushrooms')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"16cae408-2390-ac5d-2156-ab0a80a68359"},"outputs":[],"source":"#['garlic', 'onion'], ['olive oil']\nx = 'basil'\nb= 'tomato sauce'\na = 'pasta'\npredicted = model.most_similar([x, b], [a])[0][0]\nprint(\" {} is to  {} as {} is to {} \".format(a, b, x, predicted))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"940f3b36-1123-f63c-3109-a7b3ce9b308a"},"outputs":[],"source":"#['garlic', 'onion'], ['olive oil']\nx = 'chicken'\nb= 'broccoli'\na = 'bacon'\npredicted = model.most_similar([x, b], [a])[0][0]\nprint(\" {} is to  {} as {} is to {} \".format(a, b, x, predicted))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c202d3c4-3893-002e-2eb6-4506393e405e"},"outputs":[],"source":"model.wv.most_similar_cosmul(positive=['chili', u'meat'], negative=['tomato sauce'])"},{"cell_type":"markdown","metadata":{"_cell_guid":"a0dbcfc6-1717-90fc-ec95-bdc83b8cc8fe"},"source":"# Viz with t-SNE"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"54570f9d-dc14-8e51-e019-25ced77774d4"},"outputs":[],"source":"corpus = sorted(model.wv.vocab.keys()) #not sure the exact api\nemb_tuple = tuple([model[v] for v in corpus])\nX = np.vstack(emb_tuple)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8ce71405-156a-2a6a-c741-30cd6f1048eb"},"outputs":[],"source":"tsne = TSNE(n_components=2)\nX_tsne = tsne.fit_transform(X)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c17e0bed-9cc2-3b25-a089-cb2320f3932c"},"outputs":[],"source":"plt.scatter(X_tsne[:, 0], X_tsne[:, 1])\n\n"},{"cell_type":"markdown","metadata":{"_cell_guid":"2185fde4-9ea2-1eb9-0a9b-2e3b8fe52c96"},"source":"Looks like there are some clusters of ingredients but is difficult to say anything else without adding labels or colors. Let's start easy and color each ingredient by the cuisine where it is more frequent. May be not the best way, but one of the fastest approaches to test if this is working. I will normalize the frequency."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"19707c2b-cda7-3a94-9a8e-ee8cb2596bf4"},"outputs":[],"source":"#\n# I will label a ingredient by frequency \ntrack_ingredients = dict(zip(counts_cuis.keys(), [list() for x in counts_cuis.keys()]))\nfor recipt in trainrecipts:\n    # append the number in the list\n    clean_recipt = list()\n    # I want ingredient remove \n    for ingredient in recipt['ingredients']:\n        # remove this description from the ingredients\n        # (10 oz.) \n        ingredient =  re.sub(r'crushed|crumbles|ground|minced|powder|chopped|sliced', '', ingredient)\n        clean_recipt.append(ingredient.strip())\n        \n    track_ingredients[recipt['cuisine']].extend(clean_recipt)\n\nfor label, tracking in track_ingredients.items():\n    track_ingredients[label] = collections.Counter(tracking)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e4bd54d2-2c93-66e5-99cc-59fb5b7bbec8"},"outputs":[],"source":"def return_most_popular(v):\n    cuisine = None\n    record = 0\n    for label, tracking in track_ingredients.items():\n        norm_freq = float(tracking[v]) / float(counts_cuis[label])\n        if norm_freq > record:\n            cuisine = label\n            record = norm_freq\n    return cuisine"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2b52fcf5-6a6c-6481-3b17-f4ffc5023b40"},"outputs":[],"source":"track_2color = {u'irish':\"#000000\", # blak\n                u'mexican':\"#FFFF00\", #yellow\n                u'chinese':\"#1CE6FF\", #cyan\n                u'filipino': \"#FF34FF\", #pink \n                u'vietnamese':\"#FF4A46\", #red\n                u'spanish':\"#FFC300\",  # green forest\n                u'japanese':\"#006FA6\", # blue ocean\n                u'moroccan':\"#A30059\",# purple\n                u'french':\"#FFDBE5\",  #light pink\n                u'greek': \"#7A4900\",  # gold or brown \n                u'indian':\"#0000A6\", # blue electric \n                u'jamaican':\"#63FFAC\", # green phospho\n                u'british': \"#B79762\", #brown\n                u'brazilian': \"#EEC3FF\", #  \n                u'russian':\"#8FB0FF\", # light blue \n                u'cajun_creole':\"#997D87\", #violet\n                u'thai':\"#5A0007\", \n                u'southern_us':\"#809693\", \n                u'korean':\"#FEFFE6\", #ligt yellow\n                u'italian':\"#1B4400\"}\n\ncolor_vector = list()\nfor v in corpus:\n    cuisine = return_most_popular(v)\n    color_vector.append(track_2color[cuisine])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8c9ae142-5f69-8d82-1501-0ba2f04678ad"},"outputs":[],"source":"# ensemble the legend\nlgend = list()\nfor l, c in track_2color.items():\n    lgend.append(mpatches.Patch(color=c, label=l))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0626c374-4bed-528a-f333-075dbd2dd1d4"},"outputs":[],"source":"sns.set_context(\"poster\")\nfig, ax = plt.subplots(figsize=(18,18))\nplt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=color_vector, alpha=.6, s=60)\nplt.legend(handles=lgend)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0c23b761-dbc1-49ed-427a-e0b393059c0d"},"outputs":[],"source":""}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}