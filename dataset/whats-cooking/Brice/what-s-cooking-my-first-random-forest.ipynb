{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"c234821d-3b0f-d3fc-ae6e-013cfe6bf752"},"source":""},{"cell_type":"markdown","metadata":{"_cell_guid":"0605afe6-1892-3e67-d019-a5b667e2ae61"},"source":"In this Kernel, I am exploring random forest classifications with this simple case study. I will try to figure out the impact of the model parameters on the model score as well as on the computational time."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"55de53ed-7ca5-c8ac-9fef-0ba21efc4c76"},"outputs":[],"source":"import pandas\nfrom sklearn.ensemble import RandomForestClassifier as RFC\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport random\nimport gc"},{"cell_type":"markdown","metadata":{"_cell_guid":"36aa8547-81bd-515d-4529-8084b615264e"},"source":"Data visualization & imbalance check\n============="},{"cell_type":"markdown","metadata":{"_cell_guid":"93e70482-9a46-5771-ac4c-9612aacdd426"},"source":"The train data, when converted to a dataframe, contains the following information: Each row represents a recipe, characterized by a cuisine type, an id and a list of ingredients.\n\nLet's visualize how imbalanced the cuisines are in this data set: "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ace8acc2-f0b8-a4bd-5225-9a9b2128d68e"},"outputs":[],"source":"train_df=pandas.read_json('../input/train.json')\nsubmission_df=pandas.read_json('../input/test.json')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"dbe98bfd-5373-541b-9245-af2d25316787"},"outputs":[],"source":"ax = train_df['cuisine'].value_counts().plot(kind='bar', title =\"Cuisine types\", figsize=(8, 3), legend=True, fontsize=12)\nax.set_ylabel(\"recipes\", fontsize=12)\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"b6ee08bb-6198-5aa6-2dd2-7e41d2ebc120"},"source":"It is very obvious that we have over-represented cuisines (Italian and Mexican for example). It is known that it could bring some fake accuracy on classification models because these will try to fit on the most represented cuisines."},{"cell_type":"markdown","metadata":{"_cell_guid":"d530a834-1d15-8ce3-5430-b7eb34fb222c"},"source":"Data pre-processing\n==================="},{"cell_type":"markdown","metadata":{"_cell_guid":"ca83a9cc-fcf3-1843-f00b-af1f3d095096"},"source":"The first thing to do is to separate a train set of data from a test set. The original \"Test\" set provided as an input file is only for submission, so we need to create our own. Let's make the following hypotheses:\nTrain/Test ratio: The literature suggest a ratio between 60/40 and 80/20, so let's choose 70/30...\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ddc7077c-0ad5-446c-3340-51dcfd2d5bf5"},"outputs":[],"source":"# We divide the original train data into 3 sets: \n# test_df will contain 30% recipes for each cuisine type\n# train_df will contain 70% recipes for each cuisine type\n# cut_df will contain the remaining data (could be used as another test set...) if exist\n\nnew_test_df=pandas.DataFrame()\nnew_train_df=pandas.DataFrame()\ncut_df=pandas.DataFrame()\ncut_percentage=0.01  \nfor cuisine in train_df['cuisine'].drop_duplicates().values :\n    temp=pandas.DataFrame()\n    temp=train_df[train_df['cuisine']==cuisine]\n    rows_test = random.sample(list(temp.index), round(0.3*(1-cut_percentage)*len(train_df[train_df['cuisine']==cuisine])))\n    new_test_df=new_test_df.append(temp.ix[rows_test])\n    rows_train= random.sample(list(temp.drop(rows_test).index), round(0.7*(1-cut_percentage)*len(train_df[train_df['cuisine']==cuisine])))\n    new_train_df=new_train_df.append(temp.ix[rows_train])\n    rows=rows_test+rows_train\n    cut_df=cut_df.append(temp.drop(rows))\n    del temp\n\nax=plt.subplot()\nCuisineCall = list(range(0,len(cut_df['cuisine'].value_counts().index)))\nLABELS=cut_df['cuisine'].value_counts().index\nax.bar(CuisineCall,cut_df['cuisine'].value_counts(),width=0.5,color='r',align='center',label='cut data')\nax.bar(CuisineCall,new_train_df['cuisine'].value_counts(),width=0.5,color='b',align='center', label='new train data')\nax.bar(CuisineCall,new_test_df['cuisine'].value_counts(),width=0.5,color='g',align='center',label='new test data')\nplt.xticks(CuisineCall, LABELS,rotation=85)\nax.autoscale(tight=True)\nplt.legend()\n\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"2f1841af-ea03-6b99-0dda-2fffb63368e1"},"source":"We now need to convert the last column containing the list of ingredients into n columns, n being the number of existing ingredients in this study. The goal is to have for each recipe, a combination of 0 and 1 telling us which ingredients are used."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6a01e435-b331-b984-f956-c043cc9aa556"},"outputs":[],"source":"try: \n    del train_df\nexcept:pass;gc.collect()\n\n# Get the ingredients column from the new train data and create the list of all existing ingredients\nnew_ingredients=new_train_df.ingredients\nrawlist=[item for sublist in new_ingredients.ravel() for item in sublist] #convert the ingredients list of lists into a list\ningredients=list(set(rawlist)) #remove duplicates\n\nfor ing in ingredients:\n    vector=[]\n    # loop for train data\n    for recipe in new_train_df.ingredients: \n        if ing in recipe:\n            vector.append(1)\n        else:\n            vector.append(0)\n    new_train_df[ing]=pandas.Series(vector,index=new_train_df.index) # Adds column containing 0 and 1's for this ingredient\n    \n    # loop for test data\n    vector=[]\n    for recipe in new_test_df.ingredients:\n        if ing in recipe:\n            vector.append(1)\n        else:\n            vector.append(0)\n    new_test_df[ing]=pandas.Series(vector,index=new_test_df.index) # Adds column containing 0 and 1's for this ingredient\n   \n    # loop for cut data\n    vector=[]\n    for recipe in cut_df.ingredients:\n        if ing in recipe:\n            vector.append(1)\n        else:\n            vector.append(0)\n    cut_df[ing]=pandas.Series(vector,index=cut_df.index) # Adds column containing 0 and 1's for this ingredient\n\n    # While we are here, let's build also the submission data\n    vector=[]\n    for recipe in submission_df.ingredients:\n        if ing in recipe:\n            vector.append(1)\n        else:\n            vector.append(0)\n    submission_df[ing]=pandas.Series(vector,index=submission_df.index) # Adds column containing 0 and 1's for this ingredient\n\n# useless columns removal\nnew_train_df=new_train_df.drop('ingredients',1)\nnew_train_df=new_train_df.drop('id',1)\n\nnew_test_df=new_test_df.drop('ingredients',1)\nnew_test_df=new_test_df.drop('id',1)\n\ncut_df=cut_df.drop('ingredients',1)\ncut_df=cut_df.drop('id',1)\n\nsubmission_df=submission_df.drop('ingredients',1)\n\nnew_train_df.head()"},{"cell_type":"markdown","metadata":{"_cell_guid":"4ca30d81-cb79-006d-b036-8cfd390ee505"},"source":"Learning\n========"},{"cell_type":"markdown","metadata":{"_cell_guid":"a1165c7a-073b-4b5d-a902-df54a8ae2a14"},"source":"Let's define train and test tables first and clear some variables"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"760ad44e-ee09-b980-6124-17c2ee0317e4"},"outputs":[],"source":"try: \n    X_train=new_train_df.drop('cuisine',axis=1)\n    Y_train=new_train_df['cuisine']\n    X_test=new_test_df.drop('cuisine',axis=1)\n    Y_test=new_test_df['cuisine']\n    X_cut=cut_df.drop('cuisine',axis=1)\n    Y_cut=cut_df['cuisine']\n    del new_train_df\n    del new_test_df\n    del new_ingredients\n    del rawlist\n    del ingredients\n    del vector\nexcept:pass;gc.collect()"},{"cell_type":"markdown","metadata":{"_cell_guid":"7a3c5cd5-f872-e679-a14a-dd153efba436"},"source":"train"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"31dafa2a-9734-b2f9-ee7f-a7c47a8e7a36"},"outputs":[],"source":"from sklearn import metrics\nforest=RFC(n_estimators=10,max_features=10)\nforest.fit(X_train,Y_train)\noutput=forest.predict(X_test)\nmetrics.accuracy_score(Y_test, output)"},{"cell_type":"markdown","metadata":{"_cell_guid":"07223c38-ecb8-4602-6cb3-5b761cc36d92"},"source":"Method optimization\n==================="},{"cell_type":"markdown","metadata":{"_cell_guid":"aa879eb1-0816-b456-4621-90ceb38632f3"},"source":"Now let's explore the importance of the number of trees, maximum number of features"},{"cell_type":"markdown","metadata":{"_cell_guid":"0c0e3a65-c02f-080d-f061-4d49cb5a78b6"},"source":"Number of trees\n--------------------"},{"cell_type":"markdown","metadata":{"_cell_guid":"1310d9ad-c664-dad8-fb5a-7904b244d6f4"},"source":"We are going to plot the fitting score VS the number of estimators for a given number of maximum features (5 only to reduce computation time)."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a2a8c42c-aa8d-935a-ece7-e14c4bca05af"},"outputs":[],"source":"opt_table_estimators=list()\nn_features=5\nn_estimators=50\nfor i in range(1,n_estimators):\n    forest=RFC(n_estimators=i,max_features=n_features)\n    forest.fit(X_train,Y_train)\n    output=forest.predict(X_test)\n    opt_table_estimators.append(metrics.accuracy_score(Y_test, output))\nplt.plot(range(1,n_estimators), opt_table_estimators)\nplt.xlabel('Number of trees')\nplt.ylabel('Random Forest Score')\nplt.title('Random Forest Score VS Number of trees (5 features)')\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"381ff8a9-c71a-bd42-85fe-ef64cdbfc0a4"},"source":"The plot above shows that in our case the importance of the number of trees is very important until 20. Then, the slope is getting weaker..."},{"cell_type":"markdown","metadata":{"_cell_guid":"46e73189-0543-511d-5ea6-09b2893eb55e"},"source":"Number of features\n------------------"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ea7ee80d-8f44-1b1b-9736-a20734089aef"},"outputs":[],"source":"opt_table_n_features=list()\nn_estimators=5\nn_features=50\nfor i in range(1,n_features):\n    forest=RFC(n_estimators=i,max_features=n_features)\n    forest.fit(X_train,Y_train)\n    output=forest.predict(X_test)\n    opt_table_n_features.append(metrics.accuracy_score(Y_test, output))\nplt.plot(range(1,n_features), opt_table_n_features)\nplt.xlabel('Number of features')\nplt.ylabel('Random Forest Score')\nplt.title('Random Forest Score VS Number of features (5 trees)')\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"814b9001-67b8-f0ed-8526-bb1dda9dbd64"},"source":"The importance of the number of features has a little less effect on the prediction than the number of trees. "},{"cell_type":"markdown","metadata":{"_cell_guid":"4af9993f-0ebe-6904-43d8-5b36b4edd61c"},"source":"First optimized model\n---------------------"},{"cell_type":"markdown","metadata":{"_cell_guid":"28f1d5b0-9c6f-ebb0-3bcd-06a12df6878d"},"source":"At this step, and thanks to the previous optimization study, we are sure to get better result with a large number of trees and with at least 5 features."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0daa0a80-23f4-aba2-8598-77b50291b848"},"outputs":[],"source":"forest=RFC(n_estimators=40,max_features=10)\nforest.fit(X_train,Y_train)\noutput=forest.predict(X_test)\nmetrics.accuracy_score(Y_test, output)"},{"cell_type":"markdown","metadata":{"_cell_guid":"753921f2-c9db-be67-1c82-d8596cb06ca0"},"source":"Most important ingredients\n========================================"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"279cd352-fec7-ae1c-2b1a-ff57bef37ebc"},"outputs":[],"source":"importance = forest.feature_importances_\nimportance = pandas.DataFrame(importance, index=X_train.columns, columns=[\"Importance\"])\nimportance_plot=importance.sort_values('Importance',ascending=False ).loc[importance['Importance']>0.004,:]\nx = np.arange(len(importance_plot.index.values))\ny = importance_plot.ix[:, 0]\nplt.bar(x, y,align='center')\nplt.xticks(x,importance_plot.index.values,rotation=85)\nplt.ylabel('Importance')\nplt.title('Main ingredients importance')\nplt.autoscale(tight=True)\nplt.show()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a3673db4-a878-3096-b5f1-dd85c4fa1332"},"outputs":[],"source":"#FOR SUBMISSION\nX_submission=submission_df.drop('id',1)\npred=forest.predict(X_submission)\nOutput=pandas.DataFrame(submission_df['id'],index=submission_df.index)\nOutput['cuisine']=pandas.Series(pred,index=submission_df.index)\nOutput.to_csv('output.csv',index=False)"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}