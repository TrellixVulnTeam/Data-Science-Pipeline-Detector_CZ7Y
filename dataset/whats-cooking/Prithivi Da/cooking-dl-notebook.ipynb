{"cells":[{"metadata":{"_uuid":"c679d702e87474d48613eaa8b06667b1ea9ad259"},"cell_type":"markdown","source":"This probably is the dataset that is closest to my heart. Because its about food :-), more precisely about the cuisines of the world and ingredients.\n\n\n![](https://media.giphy.com/media/MXWQKVEi2OM5W/giphy.gif)"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nimport numpy as np \nimport pandas as pd \nfrom pandas import read_csv\n\nprint(os.listdir(\"../input\"))\npd.set_option('display.max_colwidth', -1)\nnp.random.seed(7)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"df6a5a8d3b5c0dcb32cc2c362ac28fe3420a549f"},"cell_type":"markdown","source":"<h3> Data level Challenges </h3>"},{"metadata":{"_uuid":"813e694ce9a9a5ce1291fb4fa3d022e66bb1fa2c"},"cell_type":"markdown","source":"\nThe datasets are JSON files. The raw json object is nested and it looks like below.\nSo data level we have some challenge.  We need to flatten the nested json style ingredients and see this huge blob as a NLP feature to fit a model to the data. Let's see how do we do that.\n```json\n[  \n   {  \n      \"id\":10259,\n      \"cuisine\":\"greek\",\n      \"ingredients\":[  \n         \"romaine lettuce\",\n         \"black olives\",\n         \"grape tomatoes\",\n         \"garlic\",\n         \"pepper\",\n         \"purple onion\",\n         \"seasoning\",\n         \"garbanzo beans\",\n         \"feta cheese crumbles\"\n      ]\n   },\n   {  \n      \"id\":25693,\n      \"cuisine\":\"southern_us\",\n      \"ingredients\":[  \n         \"plain flour\",\n         \"ground pepper\",\n         \"salt\",\n         \"tomatoes\",\n         \"ground black pepper\",\n         \"thyme\",\n         \"eggs\",\n         \"green tomatoes\",\n         \"yellow corn meal\",\n         \"milk\",\n         \"vegetable oil\"\n      ]\n   }\n]\n```\n"},{"metadata":{"_uuid":"d86c1535fc98f66d93beb07001ec94f90452070a"},"cell_type":"markdown","source":"<h3> Problem domain Challenges </h3>"},{"metadata":{"_uuid":"1729cb2434ccaae5ded10acf7bfc49039251bdbb"},"cell_type":"markdown","source":"\n\nThe challenge is while we know different international cuisines share ingredients, given a set of ingredients how accuractely can you predict the cuisine ? The challenge has two parts, \n<ul>\n<li>There are lot of frequently occuring ingredients which may **not** be predictive like **salt, garlic, pepper, milk or tomatoes**. </li>\n<li>But there can be **pretty unique and predictive** ingredients like **black olives and feta cheese**. </li>\n </ul>\n \n In a NLP sense, salt and garlic are like the \"stop words\" of this corpus. Should we construct a special stop word list to remove the less predictive ingredients ?"},{"metadata":{"_uuid":"39c2e367ee2a72caebc3c50f704b1e47dc378a5b"},"cell_type":"markdown","source":"Ok, Let's start with flattening and exploring the data.  Also, we need to attend to a tiny little detail here, if we flatted all the ingredients to form blob say, like this \"romaine lettuce black olives grape tomatoes\" we lose the affinity between the word n-grams. What do I mean by that black olives is actually a single ingredient and our model should have some means to look at it as single word \"black_olives\"  (i.e as a bigram) and not as black olives. So can we as a part of flattening all the data make sure all the word n-grams are intact and not losing their  affinity ? Thats where 1D-CNN comes into play. They are a natural  **dynamic**  n-gram detector.  \n\n\nNext, do you think word count can be predictive of the cuisine ? Say Indian cuisine would use more ingredients than Italian or Greek hence bigger word count  ? I don't know, sounds like a decent numerical feature to test with :-)"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train_filename =  '../input/train.json'\ntest_filename =  '../input/test.json'\n\ndef flatten_json(input_file):\n    import json\n    import re\n    from pandas.io.json import json_normalize\n    corpus_file = open(input_file,\"r\")\n    corpus = corpus_file.read()\n    entries =  json.loads(corpus)\n    df =  json_normalize(entries)\n    df['flat_ingredients'] = df.apply(lambda row: ' '.join(ingredient for ingredient in row['ingredients']), axis=1)\n    df['word_count'] = df.apply(lambda row: len(row['flat_ingredients'].split(' ')), axis=1)\n    df.drop('ingredients', axis=1, inplace=True)   \n    df.sort_values(['word_count'], ascending=False, inplace=True)\n    return df                          \n        \ntrain_data_raw = flatten_json(train_filename)\ntest_data_raw = flatten_json(test_filename)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"030e11baf4329eae1b216e56e34cca6f3b0ab8dd"},"cell_type":"code","source":"train_data_raw.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"012a59e1988961660fdef84d7ba7c1f7ca989a24"},"cell_type":"markdown","source":"Word count could be predictive, But I was dead wrong about *Italian cuisine , the cuisine that topped with most ingredients is Italian with 65 ingredients and Brazilian seconds it ??. I wanna know what these dishes are !* . Here I continue shamelessly. "},{"metadata":{"_uuid":"1937efcde1dc0d8a6cdd1e1123f1f1a340c04b3f"},"cell_type":"markdown","source":"<h3> TF-IDF + Logistic Regression </h3>"},{"metadata":{"trusted":true,"_uuid":"93f81028f97685a83f24e6910ada0d513c496cce"},"cell_type":"code","source":"# from sklearn.linear_model import LogisticRegression\n# from sklearn.feature_extraction.text import TfidfVectorizer \n# from sklearn.model_selection import cross_val_score \n# from scipy.sparse import hstack\n\n# ingredient_vectorizer = TfidfVectorizer(\n#                         sublinear_tf=True,\n#                         strip_accents='unicode',\n#                         analyzer='word',\n#                         #token_pattern=r'\\w{1,}',\n#                         ngram_range=(1,4),\n#                         stop_words='english',\n#                         max_features=200000)\n\n# ingredient_char_vectorizer = TfidfVectorizer(\n#                     sublinear_tf=True,\n#                     strip_accents='unicode',\n#                     analyzer='char',\n#                     stop_words='english',\n#                     ngram_range=(5, 10),\n#                     max_features=400000)\n\n# all_ingredients = pd.concat([\n#      train_data_raw['flat_ingredients'],\n#      test_data_raw['flat_ingredients']]\n# )\n\n# ingredient_vectorizer.fit(all_ingredients)\n# ingredient_char_vectorizer.fit(all_ingredients)\n\n# # Create TF-IDF vectors for training features.\n# x_word_train = ingredient_vectorizer.transform(train_data_raw['flat_ingredients'])\n# x_char_train = ingredient_char_vectorizer.transform(train_data_raw['flat_ingredients'])\n# x_train = hstack([x_char_train, x_word_train])\n# # Create TF-IDF vectors for test features.\n# x_word_test = ingredient_vectorizer.transform(test_data_raw['flat_ingredients'])\n# x_char_test = ingredient_char_vectorizer.transform(test_data_raw['flat_ingredients'])\n# x_test = hstack([x_char_test, x_word_test])\n\n# y_train = pd.get_dummies(train_data_raw['cuisine'])\n# columns  = y_train.columns.tolist()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"93caa9d7bc1854a3b7d88da90735b2d3bba5ddc2"},"cell_type":"code","source":"# output = pd.DataFrame()\n# scores = []\n# for column in columns:\n#     classifier = LogisticRegression(C=0.1,  solver='sag')\n#     classifier.fit(x_train, y_train[column])\n#     output[column] = classifier.predict_proba(x_test)[:, 1]\n#     cv_score = np.mean(cross_val_score(classifier, x_train, y_train[column], cv=3, scoring='accuracy'))\n#     scores.append(cv_score)\n#     print('CV score for class {} is {}'.format(column, cv_score))\n\n# print('Total CV score is {}'.format(np.mean(scores)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6dc459c3fc842239dc29af4a774243593c39d71a"},"cell_type":"code","source":"# cuisine = output.idxmax(axis=1)\n# cuisine.shape\n# submission = pd.DataFrame.from_dict({'id': test_data_raw['id'],'cuisine': cuisine})\n# submission.to_csv('submission.csv',index=False)\n# submission.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"90d892955633aa961d2b2e664bdc5ef5351faf23"},"cell_type":"markdown","source":"<h3> biLSTM + CNN </h3>"},{"metadata":{"trusted":true,"_uuid":"f14a329f900ad06777511ab0aa2ab39779bef857","scrolled":true},"cell_type":"code","source":"from keras.preprocessing import text, sequence\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\nvocab_size =  50000            # based on words in the entire corpus\nmax_len = 64                   # based on word count in phrases\n\nall_corpus     = list(train_data_raw['flat_ingredients'].values) + list(test_data_raw['flat_ingredients'].values)\ntrain_phrases  = list(train_data_raw['flat_ingredients'].values) \ntest_phrases   = list(test_data_raw['flat_ingredients'].values)\nX_train_target_binary = pd.get_dummies(train_data_raw['cuisine'])\ncolumns = X_train_target_binary.columns.tolist()\n\n#Vocabulary-Indexing of the train and test flat_ingredients, make sure \"filters\" parm doesn't clean out punctuations\ntokenizer = Tokenizer(num_words=vocab_size)\ntokenizer.fit_on_texts(all_corpus)\nword_index = tokenizer.word_index\nprint(\"word_index\", len(word_index))\n\nencoded_train_phrases = tokenizer.texts_to_sequences(train_phrases)\nencoded_test_phrases = tokenizer.texts_to_sequences(test_phrases)\n\n#Watch for a POST padding, as opposed to the default PRE padding\nX_train_words = sequence.pad_sequences(encoded_train_phrases, maxlen=max_len,  padding='post')\nX_test_words = sequence.pad_sequences(encoded_test_phrases, maxlen=max_len,  padding='post')\nprint (X_train_words.shape)\nprint (X_test_words.shape)\nprint (X_train_target_binary.shape)\n\nX_train_num = train_data_raw['word_count']\nX_test_num = test_data_raw['word_count']\n\nprint ('Done Tokenizing and indexing phrases based on the vocabulary learned from the entire Train and Test corpus')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"da6d6dd9f59f29bf08253f3bce7bd3d482c6a103"},"cell_type":"code","source":"from keras.callbacks import EarlyStopping\nfrom keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Conv1D, CuDNNGRU, CuDNNLSTM, BatchNormalization\nfrom keras.layers import  GlobalMaxPool1D, SpatialDropout1D\nfrom keras.layers import Bidirectional\nfrom keras.models import Model\nfrom keras.layers.merge import concatenate\n\nearly_stop = EarlyStopping(monitor = \"val_loss\", mode=\"min\", patience = 3, verbose=1)\n\nprint(\"Building layers\")        \nnb_epoch = 10\nprint('starting to stitch and compile  model')\nfrom keras.initializers import he_normal\ninitializer = he_normal(seed=None)\n\ninput_numeric = Input((1, ))\ndense_num = Dense(32, activation='relu')(input_numeric)\n\n# Embedding layer for text inputs\ninput_words = Input((max_len, ))\nx_words = Embedding(vocab_size, 300, input_length=max_len)(input_words)\nx_words = Bidirectional(CuDNNLSTM(50, return_sequences=True))(x_words)\nx_words = Dropout(0.5)(x_words)\nx_words = Conv1D(64, 3,   activation='relu')(x_words)\nx_words = Conv1D(64, 1,   activation='relu')(x_words)\nx_words = GlobalMaxPool1D()(x_words)\nx_words = Dropout(0.5)(x_words)\n\n# merge\nmerged = concatenate([x_words, dense_num])\nx = Dense(64, activation=\"relu\")(merged)\npredictions = Dense(20, activation=\"softmax\")(x)\n\nmodel = Model(inputs=[input_words, input_numeric], outputs=predictions)\nmodel.compile(optimizer='nadam',loss='categorical_crossentropy', metrics=['accuracy'])\nprint(model.summary())\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"579598846077ccc19c30a1252fb1a7c09bb60fea"},"cell_type":"code","source":"#fit the model\nhistory = model.fit([X_train_words, X_train_num], X_train_target_binary, epochs=nb_epoch, verbose=1, batch_size = 128, callbacks=[early_stop], validation_split = 0.2, shuffle=True)\ntrain_loss = np.mean(history.history['loss'])\nval_loss = np.mean(history.history['val_loss'])\nprint('Train loss: %f' % (train_loss))\nprint('Validation loss: %f' % (val_loss))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3367b7c8fc030e8ed4da33e25a85c9c20c8a64a6"},"cell_type":"code","source":"pred = model.predict([X_test_words,X_test_num], batch_size=128, verbose = 1)\nprint (pred.shape) \nmax_pred = np.round(np.argmax(pred, axis=1)).astype(int)\ncuisines = [columns[m] for m in max_pred]\ndf =pd.DataFrame({'cuisines': cuisines}).reset_index()\ndf.groupby('cuisines').agg('count')\nsubmission = pd.DataFrame({'id':test_data_raw['id'],'cuisine': cuisines})\nsubmission.to_csv('submission.csv',index=False)\n\n    \n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}