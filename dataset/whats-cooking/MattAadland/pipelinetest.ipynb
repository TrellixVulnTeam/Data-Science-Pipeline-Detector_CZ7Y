{"cells":[{"metadata":{"_kg_hide-output":true,"_uuid":"25f7c6f106386ab466021ff2245f3273345f2dd3","_cell_guid":"99a6fc0b-fb51-408e-8286-84eca2e41d8e","collapsed":true},"source":"import nltk\nimport re\nimport json\nimport string\nfrom nltk import tokenize\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords, wordnet\nfrom nltk import pos_tag\nfrom pkg_resources import resource_filename as filename\nDEFAULT_STOPWORDS = set(stopwords.words(\"english\")) | set(string.ascii_letters)\nDEFAULT_LEMMATIZER = WordNetLemmatizer()\n\n\n#### Data Setup ####\nDEFAULT_STRING = \"@#$%^&*()+=,.:;'{}[]|<>`?“”\"\nEXCLUSION = {}\nfor character in DEFAULT_STRING:\n    EXCLUSION[character] = \"\"\nEXCLUSION['\"'] = \"\"\nEXCLUSION[\"\\\\\"] = \" \"\nEXCLUSION[\"/\"] = \" \"\nEXCLUSIONS_TABLE = EXCLUSION\n# negations map \nNEGATIONS = [\"not \", \"no \",]\nNEGATIONS_MAP = [\"not_\", \"no_\"]\nNEGATIONS_TABLE = {}\nfor negation, conversion in zip(NEGATIONS, NEGATIONS_MAP):\n    NEGATIONS_TABLE[\"\\\\b{0}\\\\b\".format(negation)] = conversion\nNEGATIONS = NEGATIONS_TABLE\n\nNEGATIONS_RE = re.compile(\"{0}\".format(\"|\").join(NEGATIONS.keys()),\n                          flags=re.IGNORECASE)\n\n#### Processing Functions ####\ndef replacement_gen(document, repl_dict=NEGATIONS, repl=NEGATIONS_RE):\n    \"\"\" Replaces specific phrases with a corresponding term\n        Args:\n            document(str): pre-tokenized string\n            repl_dict(dict): dict of words and their replacements\n            repl(SRE_Pattern): precompiled regrex pattern\n        Returns:\n            document(str): orginal document with the target words replaced\n    \"\"\"\n    def replace(match):\n        \"\"\" replaces the match key\n        \"\"\"\n        match_token = \"\\\\b{0}\\\\b\".format(match.group(0).lower())\n        return repl_dict[match_token]\n    return repl.sub(replace, document)\ndef token_gen(document):\n    \"\"\"Generates tokens using nltk.word_tokenize\n        Args:\n            document(str): higher level document primative\n        Returns:\n            tokens(list): list of word tokens\n    \"\"\"\n    return tokenize.word_tokenize(document)\n\n# def keep_gen(token, LETTERS=string.ascii_letters):\n#    return [token for token in tokens if set(tokens).intersection(LETTERS)]\n\ndef clean_gen(tokens, exclusion_table=EXCLUSIONS_TABLE, LETTERS=set(string.ascii_letters)):\n    \"\"\"Cleans a list of tokens\n        Args:\n            tokens(list): list of word tokens\n            exclusion_table(dict) characters to remove from a token\n                defaults to \"!@#$%^&*()+=,.:;'{}[]|<>`?“”\"\n        Returns:\n            clean_tokens(list): tokens with the offending characters removed\n    \"\"\"\n    exclusion = str.maketrans(exclusion_table)\n    return [token.translate(exclusion).lower() for token in tokens if set(token).intersection(LETTERS)]\n\n\ndef wordnet_get(tagged_tokens):\n    \"\"\"Helper function for normalizing wordnet labels\n    \"\"\"\n    out_tokens = []\n    for token in tagged_tokens:\n        if token[1].startswith(\"J\"):\n            out_token = (token[0], wordnet.ADJ)\n        elif token[1].startswith(\"V\"):\n            out_token = (token[0], wordnet.VERB)\n        elif token[1].startswith(\"R\"):\n            out_token = (token[0], wordnet.ADV)\n        else:\n            out_token = (token[0], wordnet.NOUN)\n        out_tokens.append(out_token)\n    return out_tokens\n\ndef pos_gen(tokens):\n    \"\"\"Generates parts of speech and normalizes them to wordnet labels\n    \"\"\"\n    tagged = wordnet_get(pos_tag(tokens))\n    return tagged \n\ndef lemma_gen(tokens, wnl=DEFAULT_LEMMATIZER, tag=False):\n    \"\"\"Lemmatizes words\n        Args:\n            tokens(list): list of word strings\n            wnl(WordNetLemmatizer): lemmatizer object\n            tag(bool): performs part of speech tagging if true defaults to False \n                for speed\n        Returns:\n            lemms(list):list of words that have been lemmatized\n    \"\"\"\n    if tag:\n        wnl_tokens = pos_gen(tokens)\n        lemmas = [wnl.lemmatize(token[0], pos=token[1]) for token in wnl_tokens]\n    else:\n        lemmas = [wnl.lemmatize(token) for token in tokens]\n    return lemmas\ndef stopword_gen(tokens, default=DEFAULT_STOPWORDS, custom=None):\n    \"\"\"Removes the stopwords\n        Args:\n            tokens(list): list of word tokens\n            default(set): set of the stopwords in nltk's stopword corpus\n            custom(set): custom stopwords\n        returns:\n            no_stops(list): lists of words not found in either set\n    \"\"\"\n    if custom is not None:\n        module_stopwords = default | custom\n    else:\n        module_stopwords = default\n    return [word for word in tokens if word not in module_stopwords]\ndef default_gen(documents):\n    \"\"\"Default pipeline for cleaning a text field\n        Args:\n            documents(list): list of strings with the top level document\n            Runs in the order of:\n                1. Tokenize\n                2. Character level cleaning (numbers, punctuation, etc.)\n                3. Part of Speech Tagging\n                4. Lemmatize the tokens\n                5. Generate phrases for negations\n                6. Remove stopwords\n        Yields:\n            finished_document(list): list of tokens with the text normalized\n    \"\"\"\n    for document in documents:\n        tokens = token_gen(document)\n        clean_tokens = clean_gen(tokens)\n        lemmas = lemma_gen(clean_tokens, tag=True)\n        # add the phrase model here\n        negations = replacement_gen(\" \".join(lemmas), NEGATIONS,\n                                    NEGATIONS_RE)\n        new_tokens = negations.split(\" \")\n        finished_document = stopword_gen(new_tokens)\n        yield finished_document\n","cell_type":"code","outputs":[],"execution_count":12},{"metadata":{"_uuid":"ab3801c44f884e861cc143d45badecbeb09e66e7","_cell_guid":"ef9477d1-6d4e-41be-9eef-2021c572da53","collapsed":true},"source":"def pre_phrases(documents):\n    \"\"\"Generate a pipeline before the phrases are built\n    \"\"\"\n    for document in documents:\n        doc_string = \" \".join(document)\n        raw_tokens = token_gen(doc_string)\n        clean_tokens = clean_gen(raw_tokens)\n        lemmas = lemma_gen(clean_tokens, tag=False)\n        yield lemmas","cell_type":"code","outputs":[],"execution_count":13},{"metadata":{"_uuid":"e2c27f0b8336466c11c35537252a2e94ccb8b03e","_cell_guid":"a403063e-ff0d-4895-9cc5-edc7b48c9863"},"source":"import pandas as pd\nfrom gensim.models import Phrases\nfrom gensim.models.phrases import Phraser\nfrom sklearn.model_selection import train_test_split as split\nfrom sklearn.metrics import classification_report\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom collections import Counter\nfrom random import shuffle\ndata = pd.read_json(\"../input/train.json\")\n","cell_type":"code","outputs":[],"execution_count":14},{"metadata":{"_uuid":"ddc66a51ec72c77c57aef18052bb14857595dd29","_cell_guid":"dc566919-f6c0-45d9-b240-8a66865d5a6e","collapsed":true},"source":"#### Function Pipeline for the Data processing ####\ndef ingredient_pipeline(recipe, bigram_model):\n    \"\"\"Operates on individual recipes\n    \"\"\"\n    new_recipe = []\n    for ingredient in recipe:\n        ingredient_tokens = token_gen(ingredient)\n        clean_tokens = clean_gen(ingredient_tokens)\n        lemmas = lemma_gen(clean_tokens)\n        bigrams = bigram_model[lemmas]\n        clean_ingredient = \" \".join(bigrams)\n        new_recipe.append(clean_ingredient)\n    return new_recipe\n\ndef all_recipes(recipes, bigram_model):\n    \"\"\"Operates on all the recipes\n    \"\"\"\n    for recipe in recipes:\n        yield ingredient_pipeline(recipe, bigram_model)\n\ndef subset_pairs(X_rows, y_rows, target_1, target_2):\n    \"\"\"Generates a unique classifier to distinguish between two cuisine types \n    \"\"\"\n    X_index, y_subset = [], []\n    for index, row in enumerate(y_rows):\n        if row == target_1 or row == target_2:\n            y_subset.append(row)\n            X_index.append(index)\n    X_subset = X_rows[X_index]\n    return X_subset, y_rows[y_subset]\n\ndef regroup(X_rows, y_rows, target_1, target_2):\n    \"\"\"Splits the testing pairs again\n    \"\"\"\n    X_index, y_subset = [], []\n    for index, row in enumerate(y_rows.index):\n        if y_rows[row] == target_1 or y_rows[row] == target_2:\n            y_subset.append(row)\n            X_index.append(index)\n    X_subset = X_rows[X_index]\n    return X_subset, y_rows[y_subset]\n\ndef pairwise_clf(X_predictions, y_predictions, target_1, target_2):\n    \"\"\"Generates a binary classifier between target_1 and target_2\n    \"\"\"\n    retrain_x, retrain_y = regroup(X_train, y_train, target_1, target_2)\n    retrain_test_x, retrain_test_y = regroup(X_test, y_predictions, target_1, target_2)\n    retrain_clf = SGDClassifier(shuffle=True).fit(retrain_x, retrain_y)\n    retrain_predictions = retrain_clf.predict(retrain_test_x)\n    retrain_predictions = pd.Series(retrain_predictions, index=retrain_test_x.index)\n    # print(classification_report(retrain_test_y, retrain_predictions))\n    return retrain_predictions\n\ndef reassign_groups(X_train, y_train, first_pred, y_test, targets):\n    \"\"\"Reassigns groups through an iterator\n    \"\"\"\n    reassigned_predictions = first_pred\n    for target in targets:\n        print(\"Converting {0} and {1}\".format(target[0], target[1]))\n        prediction_next = pairwise_clf(X_train, y_train, target[0], target[1])\n        reassigned_predictions[prediction_next.index] = prediction_next\n        #print(classification_report(y_test, reassigned_predictions))\n    return reassigned_predictions","cell_type":"code","outputs":[],"execution_count":40},{"metadata":{"_uuid":"7367286f706d9409f1f7fd39fcc39284e31aabd1","_cell_guid":"a672168b-0127-4445-a20c-d97211cbdeb0"},"source":"text_gen = pre_phrases(data[\"ingredients\"])\nbigram_model = Phraser(Phrases(text_gen))\nrecipe_generator = all_recipes(data[\"ingredients\"], bigram_model)\nrecipe_list = [x for x in recipe_generator]\ntfidf_vectorizer = TfidfVectorizer(tokenizer = lambda doc: doc, lowercase=False)\nmatrix = tfidf_vectorizer.fit_transform(data[\"ingredients\"])\nprint(matrix.shape)","cell_type":"code","outputs":[],"execution_count":16},{"metadata":{"_uuid":"89a960f1f6e2681abe2bd9a76595467ad6c2e05f","_cell_guid":"960e5952-f2d7-4fdb-869c-26ce3439ab54"},"source":"X_train, X_test, y_train, y_test = split(matrix, data[\"cuisine\"], test_size=.20)\nsgd_clf = SGDClassifier(shuffle=True).fit(X_train, y_train)\nsgd_predictions = sgd_clf.predict(X_test)\nsgd_series = pd.Series(sgd_predictions, index=y_test.index)\nsgd_report = classification_report(y_test, sgd_predictions)\nprint(sgd_report)","cell_type":"code","outputs":[],"execution_count":17},{"metadata":{"_uuid":"cee08fdeada54850d83a4285faf408a4ba5152c4","_cell_guid":"f8d43955-0aee-4474-8807-4630f17a6bc9"},"source":"pairs = [(\"vietnamese\", \"thai\"), (\"brazilian\", \"mexican\"), (\"cajun_creole\", \"southern_us\"),\n        ('french', 'italian'), ('greek', 'italian'), ('british', 'southern_us'), \n        ('southern_us', 'italian'), ('southern_us', 'mexican'), ('irish', 'southern_us'),\n        ('british', 'italian'), ('italian', 'mexican'), ('russian', 'french'), \n         ('russian', 'southern_us'),\n        ('spanish', 'italian'), ('filipino', 'chinese')]\nnew_predictions = reassign_groups(X_train=X_train, y_train=y_train, first_pred=sgd_series, y_test=y_test,\n                                 targets=pairs)","cell_type":"code","outputs":[],"execution_count":41},{"metadata":{},"source":"test_data = pd.read_json(\"../input/test.json\")\ntest_generator = all_recipes(test_data[\"ingredients\"], bigram_model)\ntest_list = [x for x in test_generator]\ntest_matrix = tfidf_vectorizer.transform(test_list)\n\nprint(test_matrix.shape)","cell_type":"code","outputs":[],"execution_count":33},{"metadata":{},"source":"inital_test = sgd_clf.predict(test_matrix)\ninital_series = pd.Series(inital_test, index=test_data[\"ingredients\"].index)\nX_test = test_matrix\nfinal_predictions = reassign_groups(X_train=X_train, y_train=y_train, first_pred=inital_series, y_test=y_test,\n                                 targets=pairs)","cell_type":"code","outputs":[],"execution_count":42},{"metadata":{"_kg_hide-output":true},"source":"id = inital_series.index\ncuisine = final_predictions\nsubmission = pd.DataFrame({\"id\":id, \"cuisine\":cuisine})\nprint(submission.head())\nprint(id[0:100])","cell_type":"code","outputs":[],"execution_count":21},{"metadata":{},"source":"import os\nprint(os.listdir())","cell_type":"code","outputs":[],"execution_count":22}],"metadata":{"language_info":{"version":"3.6.1","pygments_lexer":"ipython3","mimetype":"text/x-python","codemirror_mode":{"version":3,"name":"ipython"},"file_extension":".py","nbconvert_exporter":"python","name":"python"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat_minor":1,"nbformat":4}