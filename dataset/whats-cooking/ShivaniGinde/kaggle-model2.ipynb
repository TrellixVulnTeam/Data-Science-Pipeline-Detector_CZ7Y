{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"import os\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.ensemble import RandomForestClassifier\n#from KaggleWord2VecUtility import KaggleWOrd2VecUtility\nfrom pandas import Series, DataFrame\n#from corpora import Corpus\nimport pandas as pd\nimport numpy as np\n#import urllib2\nimport json\nimport matplotlib.pyplot as plt\nfrom nltk.stem import WordNetLemmatizer\nimport re\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import grid_search\nprint (0)"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"#importing the train and test data sets\ntrain_df = pd.read_json(\"../input/train.json\")\ntest_df = pd.read_json(\"../input/test.json\")\n\nprint (0)"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"#use the Natural Language Tool Kit word lemmatizer to stem the words in the ingredients lists\n#stemmer  = WordNetLemmatizer()\n#Do I need to download this package since I'm using Kaggle's site?\n#nltk.download('wordnet')\n#ingredients = train_df[\"ingredients\"]\n\n#corpus = stemmer.lemmatize(ingredients)\n#train_df['ingredients_clean'] = [' , '.join(z).strip() for z in train_df['ingredients']]\n\n#test_df['ingredients_clean'] = test_df['ingredients']\n\nprint (0)"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"train_df['ingredients_clean'] = [' '.join([WordNetLemmatizer().lemmatize(re.sub('[^A-Za-z]', '', line)) \n                                         for line in lists]).strip() for lists in train_df['ingredients']]\n\ntest_df['ingredients_clean'] = [' '.join([WordNetLemmatizer().lemmatize(re.sub('[^A-Za-z]', '', line)) \n                                         for line in lists]).strip() for lists in test_df['ingredients']]\n\nprint (0)"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"corpus_train = train_df['ingredients_clean']\ncorpus_test = test_df['ingredients_clean']\nprint (0)"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"vectorizer_train = TfidfVectorizer(stop_words = 'english', ngram_range = (1,1), analyzer = \"word\",\n                                    max_df = 1.0, token_pattern = r'\\w+')\nvectorizer_test = TfidfVectorizer(stop_words = 'english')\n\ntfidf_train = vectorizer_train.fit_transform(corpus_train)\n#tfidf2 = vectorizer1.fit_transform(corpus_test)\npredictor = tfidf_train\nx = predictor\nprint (0)"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"target = train_df['cuisine']\ny = target\nprint (0)"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"#Logistic Regression model\nparameter = {'C':[1, 80000]}\n\nLR = LogisticRegression()\n\nprint(0)"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"classifier = grid_search.GridSearchCV(LR, parameter)\nclassifier = classifier.fit(x, y)\n\nprint(0)"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"prediction = pd.DataFrame(classifier.predict(predictor))\ntest_df['cuisine'] = ''\ntest_df['cuisine'] = prediction\n\n#type(test_df['cuisine'])\n#print (prediction)\n#type(prediction)\n#prediction.__len__()\n#print (prediction)\n#x = df.columns.to_series().groupby(df.dtypes).groups\nprint (0)"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"submission_python = test_df[['id', 'cuisine']]\n\nFALSE = 0\nTRUE = 1\n\nsubmission_python.to_csv(\"LR_Submission.csv\", index = FALSE)\n\nprint (0)"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":0}