{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"#importing libraries\n\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport os\nfrom os import listdir\nimport seaborn as sns\nfrom operator import itemgetter \nimport matplotlib.image as mpimg\nimport random\nfrom PIL import Image\nimport collections as co\nimport cv2\nimport scipy as sp\nimport copy\nimport plotly.graph_objs as go\nimport plotly.offline as py","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Whales identification challenge\n\nWhales are group of aquatic marine mammals, whose closest relatives among land animals are hippopotamuses. After hundreds of years of relentless hunting, whales are now protected by international law. The North Atlantic right whales were close to extinction in the twentieth century, with a population of 450, and the North Pacific grey whale population is ranked Critically Endangered by the IUCN (according to Wiki). For the purpose of preservation of their population, it's important to count the population of whales and monitor their activity. The goal of this notebook is to propose the computer vision approach to identification of whale based on the photo of its humpback. \n\n\n## Files Statistics","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"First of all we will check how many image files we have for training and testing. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We set the path to the folders of train and test images together with the path to the file with labels.","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"trainDir = \"../input/whale-categorization-playground/train/train\"\ntestDir=\"../input/whale-categorization-playground/test/test/\"\nvaluesFile= \"../input/whale-categorization-playground/train.csv\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"On the next step we estimate the number of files in each dataset, and compare the number of images in training set with number of values in the file with labels","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\nlntrd=len(listdir(trainDir))\nlntsd=len(listdir(testDir))\n\nprint(\"number of train files: \"+ str(lntrd))\nprint(\"number of test files: \" + str(lntsd))\ntrainPD=pd.read_csv(valuesFile)\nif lntrd>0:\n    print(\"lengths of test set to train set: %6.2f\" % (lntsd/lntrd))\n    if trainPD.shape[0]==lntrd:\n        print(\"number of values and length of train set are consistent\")\n    else:\n        print(\"number of values and length of train set are inconsistent\")\nelse:\n    print(\"train set is empty\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have larger test set than train set, but this is not an issue if the images of train set are representative set of population.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Statistics of Labels","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Let's take a look at statistics of labels.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"First of all we calculate the frequency of occurence of ID in the files with values:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"FrameID = trainPD.groupby(\"Id\",as_index = False)[\"Image\"].count()\nsortedID_train = FrameID.sort_values(\"Image\",ascending = False)\nidnum=sortedID_train.shape[0]\nprint(idnum)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are 4251 unique labels. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sortedID_train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The frequency of the first class is over 20 times larger than the one of the second most frequent one. Let's plot the statistics:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(range(idnum),sortedID_train[\"Image\"])\n\nplt.xlabel(\"sorted index\")\nplt.ylabel(\"frequency of occurence\")\nplt.title(\"frequency of occurence of labels\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We are dealing with extemely imbalance dataset. Let's plot without the first class: ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(range(1,idnum),sortedID_train[\"Image\"][1:idnum])\n\nplt.xlabel(\"sorted index\")\nplt.ylabel(\"density\")\nplt.title(\"Density Plot for Labels\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Imbalance is still evident. More than half of labels have frequency of only 1:","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"The same graph in logarithmic scale:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(range(1,idnum+1),sortedID_train[\"Image\"])\nplt.yscale(\"log\")\nplt.xlabel(\"ID\")\nplt.ylabel(\"Frequency of occurence\")\nplt.title(\"Frequency of occurence: log scale\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Without the first element: ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(range(1,idnum),sortedID_train[\"Image\"][1:idnum])\nplt.yscale(\"log\")\n#plt.yscale(\"log\")\nplt.xlabel(\"ID\")\nplt.ylabel(\"Frequency of occurence\")\nplt.title(\"Frequency of occurence: log scale\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In such case of extreme distribution we would definetely require augmentation of train dataset.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Image Visualization","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Let's take a look at first 25 images: ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"imnum=25\nplt.rcParams[\"figure.figsize\"] = (70,70)\nfig, subplots = plt.subplots(5,5)\n\nfor i in range(imnum):\n    readImg=mpimg.imread(trainDir+\"/\"+(listdir(trainDir))[i])\n    subplots[i // 5,i % 5].imshow(readImg)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can notice the following issues:\n1. They are not consistent in terms of color spectrum: we can notice several images in black-and-white and most others in full color. \n2. They vary in size a lot. The model's pipeline would require substantial resizing.  \n3. Some of them also have the fields with labels unrelated to the ID. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"readImg=mpimg.imread(trainDir+\"/\"+(listdir(trainDir))[10])\nplt.rcParams[\"figure.figsize\"] = (10,10)\nplt.imshow(readImg)\nprint(listdir(trainDir)[10])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainPD[trainPD[\"Image\"] == \"47841f63.jpg\"]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that information on the label on yellow space is not consistent with ID. Most likely, this information is useless for identification.  ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Lets take a look at test set:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"imnum=25\nplt.rcParams[\"figure.figsize\"] = (70,70)\nfig, subplots = plt.subplots(5,5)\n\nfor i in range(imnum):\n    readImg=mpimg.imread(testDir+\"/\"+(listdir(testDir))[i])\n    subplots[i // 5,i % 5].imshow(readImg)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see the images of test set also differ in size a lot and are not consistent in terms of colour spectrum. In the next section we study how significant is this issue. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"\n### Distribution of sizes of images","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Let's count the frequency of occurence of different sizes using dictionary: ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sizedict_train=dict()\nfilelist=listdir(trainDir)\nfor filename in filelist:\n    size=(Image.open(trainDir+\"/\"+filename)).size\n    if size in sizedict_train:\n        sizedict_train[size]+=1\n    else:\n        sizedict_train[size]=1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's sort the dictionary by values in descending order","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sortpairs_train= sorted(sizedict_train.items(), key = itemgetter(1), reverse = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that three most common sizes are (1050,600), (1050,700) and (1050,450)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Normalization of arrays:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sortsized_train = [sortpairs_train[i][1] for i in range(len(sortpairs_train))]\nsortsized_train = sortsized_train/ np.sum(sortsized_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's plot the statistics:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"numsizes=len(sizedict_train)\nprint(numsizes)\nplt.rcParams[\"figure.figsize\"] = (5,5)\nplt.plot(sortsized_train)\n\nplt.xlabel(\"index\")\nplt.ylabel(\"probability\")\nplt.title(\"probability of size\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sortsized_train[0:10]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have 2587 different sizes of images. The first three most frequent sizes occur with frequency: 11.2%, 9.6%, 4.1 %. The plot in logarithm scale:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"numsizes=len(sizedict_train)\nprint(numsizes)\n\nplt.plot(sortsized_train)\nplt.yscale(\"log\")\nplt.xlabel(\"index\")\nplt.ylabel(\"probability\")\nplt.title(\"probability of size\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The situation with test set is similar: ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sizedict_test=dict()\nfilelist=listdir(testDir)\nfor filename in filelist:\n    size=(Image.open(testDir+\"/\"+filename)).size\n    if size in sizedict_test:\n        sizedict_test[size]+=1\n    else:\n        sizedict_test[size]=1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sortpairs_test= sorted(sizedict_test.items(), key = itemgetter(1), reverse = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sortpairs_test[0:10]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Again we see that two most common sizes are (1050,600),(1050,700) and (1050,450) ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sortsized_test = [sortpairs_test[i][1] for i in range(len(sortpairs_test))]\nsortsized_test = sortsized_test/ np.sum(sortsized_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"numsizes=len(sizedict_test)\nprint(numsizes)\n\nplt.plot(sortsized_test)\n\nplt.xlabel(\"index\")\nplt.ylabel(\"probability\")\nplt.title(\"probability of size\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"numsizes=len(sizedict_test)\nprint(numsizes)\n\nplt.plot(sortsized_test)\nplt.yscale(\"log\")\nplt.xlabel(\"sorted index\")\nplt.ylabel(\"density\")\nplt.title(\"Density Plot for Labels\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sortsized_test[0:10]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The test has 3527 different sizes of images. The probability of the first three: 13%, 8.3%, 4.34%. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Color scheme","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"In this section we estimate how many images are in grayscale format. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def checkrgb(rgb):\n    \n    if len(rgb.shape)==3:\n        return 0\n    else:\n        return 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lntd=len(listdir(trainDir))\ngrayscale=[checkrgb(mpimg.imread(trainDir+\"/\"+(listdir(trainDir))[i])) for i in range(lntd)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"share_grey_train=np.sum(grayscale)/len(grayscale)\nshare_grey_train","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Around 15 % of images in train set are in greyscale format. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"lntd=len(listdir(testDir))\ngrayscale=[checkrgb(mpimg.imread(testDir+\"/\"+(listdir(testDir))[i])) for i in range(lntd)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"share_grey_test=np.sum(grayscale)/len(grayscale)\nshare_grey_test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The test set has 22% of greyscale images. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Clustering of images","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We will now examine how similar are train set images to the images of test set. \n\nIn order to do this we will tranform all images to grayscale and resize them. We choose the size (100,100), which will result in loss of finer details on images.\n\nAfter transformation we select the subset of train and test images and use t-SNE - the machine learning algorithm for dimensionality reduction. The algorithm t-SNE maps high dimensional objects to two- or three-dimensional dots in the way that similar objects are modelled by nearby dots and dissimilar ones by distant dots. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Image transformation functions: ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def rgb2grey(rgb): \n    if len(rgb.shape)==3:\n        return np.dot(rgb[...,:3], [0.2989, 0.5870, 0.1140]) \n    else:\n        return rgb\n\n\ndef transform_image(img, rsc_dim):\n    resized = cv2.resize(img, (rsc_dim, rsc_dim), cv2.INTER_LINEAR)\n    \n    normalized = cv2.normalize(resized, None, 0.0, 1.0, cv2.NORM_MINMAX)\n                         \n    trans = normalized.reshape(1, np.prod(normalized.shape))\n\n    return trans/np.linalg.norm(trans)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainImg=[rgb2grey(mpimg.imread(trainDir+\"/\"+(listdir(trainDir))[i])) for i in range(400)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"testImg=[rgb2grey(mpimg.imread(testDir+\"/\"+(listdir(testDir))[i])) for i in range(400)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rsc_dim=100\ngray_all_images_train = [transform_image(img, rsc_dim) for img in trainImg]\ngray_all_images_test  = [transform_image(img, rsc_dim) for img in testImg]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gray_imgs_mat_train = np.array(gray_all_images_train).squeeze()\ngray_imgs_mat_test= np.array(gray_all_images_test).squeeze()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have prepared the array of transformed images for t-SNE procedure.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"inputtsne=np.concatenate([gray_imgs_mat_train, gray_imgs_mat_test])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.manifold import TSNE\ntsne = TSNE(\n    n_components=3,\n    init='random', # pca\n    random_state=101,\n    method='barnes_hut',\n    n_iter=500,\n    verbose=2\n).fit_transform(inputtsne)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nimport matplotlib.pyplot as plt\n\nplt.rcParams[\"figure.figsize\"] = (20,20)\n\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\n\nx =tsne[0:400,0]\ny =tsne[0:400,1]\nz =tsne[0:400,2]\n\nax.scatter3D(x, y, z, c='r', marker='o')\n\nx =tsne[400:800,0]\ny =tsne[400:800,1]\nz =tsne[400:800,2]\n\nax.scatter3D(x, y, z, c='b', marker='o')\n\n\nax.set_xlabel('X Label')\nax.set_ylabel('Y Label')\nax.set_zlabel('Z Label')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see there is a big heterogeneous cluster for both training and test sets and a few quite distant outliers. This means that training and test sets are quite similar after projection to low dimension. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Conclusion of exploration","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We make several conclusions on the basis of exploration:\n\n1) We have to augment our data because there are many classes that are underrepresented. \n\n2) There are thousand of different image sizes in the datasets. We need to recise the picture, and probably try different sizes for transformation.  \n\n3) We also see that there are different color schemes in datasets. We need to greyscale (or red scale) the images before applying the model.\n\n4) We have selected the subsets of training and test images and used projection to low dimensional space and found that projections from both datasets form large claster \nwith few outliers. That means that training and test are quit similar in terms of low diminsional patterns. ","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}