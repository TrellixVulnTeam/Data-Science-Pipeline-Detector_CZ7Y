{"nbformat_minor":1,"cells":[{"source":"# imports\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport keras.preprocessing.image as kim\nfrom keras.backend import tf as ktf\nimport os\nfrom PIL import Image\nfrom matplotlib import pyplot as plt\nimport copy\n\n#helpers\n\nsigLev = 3\npd.options.display.precision = sigLev\nfigWidth = figHeight = 9\ninputDir = \"../input\"\n","cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3084c7e6-81c2-4513-8000-85a624995e65","_uuid":"83fcf5edf1cdd4c86e546a91d98821c8c5076a17"},"outputs":[]},{"source":"# Motivation\n\nBased on my [previous analysis](https://www.kaggle.com/mmrosenb/whales-an-exploration), it's apparent that we have a severe imbalanced classes problem. Thus, I think we need to spend some time applying transformations to observations in the smaller classes in order to create a more balanced dataset for fitting.\n\n# Find small class\n\nLet's find an extremely small class to work with, perhaps one with just one image.","cell_type":"markdown","metadata":{}},{"source":"trainFrame = pd.read_csv(f'{inputDir}/train.csv')","cell_type":"code","execution_count":null,"metadata":{},"outputs":[]},{"source":"classCountFrame = trainFrame.groupby(\"Id\",as_index = False)[\"Image\"].count()\nclassCountFrame = classCountFrame.rename(columns = {\"Image\":\"count\"})\n#then order\nclassCountFrame = classCountFrame.sort_values(\"count\",ascending = True)\n#then just check the head\nclassCountFrame.head()","cell_type":"code","execution_count":null,"metadata":{},"outputs":[]},{"source":"We see that `w_ffdab7a` seems to be a relatively small class.Turns out from later analysis that this is a greyscaled image. Given that we are looking to augment our dataset with further greyscaling, I find this to not be an optimal choice for an example class. Thus, let's use this class `w_6384242` for our example transformations.","cell_type":"markdown","metadata":{}},{"source":"chosenClass = classCountFrame[\"Id\"].iloc[1]\nconsideredImageObs = trainFrame[trainFrame[\"Id\"] == chosenClass]\nconsideredImageFilename = consideredImageObs[\"Image\"].iloc[0]","cell_type":"code","execution_count":null,"metadata":{},"outputs":[]},{"source":"# Observe Class Image\n\nLet's just explore this particular class image.","cell_type":"markdown","metadata":{}},{"source":"fullImageFilename = f'{inputDir}/train/{consideredImageFilename}'\nchosenImage = Image.open(fullImageFilename)","cell_type":"code","execution_count":null,"metadata":{},"outputs":[]},{"source":"plt.imshow(chosenImage)\n#get rid of axes\ncur_axes = plt.gca()\ncur_axes.axes.get_xaxis().set_visible(False)\ncur_axes.axes.get_yaxis().set_visible(False)","cell_type":"code","execution_count":null,"metadata":{},"outputs":[]},{"source":"_Figure 1: Our chosen image for our preprocessing examples._","cell_type":"markdown","metadata":{}},{"source":"# Image Resizing","cell_type":"markdown","metadata":{}},{"source":"chosenImage.size","cell_type":"code","execution_count":null,"metadata":{},"outputs":[]},{"source":"We see that this image is currently $1050 \\times 525$ in size. Based on our [previous analysis](https://www.kaggle.com/mmrosenb/whales-an-exploration), the largest image size group is $1050 \\times 600$. Given that I prefer having the fewest image resizings as possible, I think $1050 \\times 600$ is a good choice for resizing all of the images. Thus, I will resize this to be slightly bigger than it currently is. We will use nearest neighbors to fill in the space.","cell_type":"markdown","metadata":{}},{"source":"idealWidth = 1050\nidealHeight = 600\nresizedChosenImage = chosenImage.resize((idealWidth,idealHeight),Image.NEAREST)","cell_type":"code","execution_count":null,"metadata":{},"outputs":[]},{"source":"plt.imshow(resizedChosenImage)\n#get rid of axes\ncur_axes = plt.gca()\ncur_axes.axes.get_xaxis().set_visible(False)\ncur_axes.axes.get_yaxis().set_visible(False)","cell_type":"code","execution_count":null,"metadata":{},"outputs":[]},{"source":"_Figure 2: The Resized Chosen Image._\n\nSeems like the resizing didn't affect that much, which is good!","cell_type":"markdown","metadata":{}},{"source":"# Greyscaling\n\nThe first thing we want to do is make an image into a greyscale. We can do this with `PIL`, thankfully.","cell_type":"markdown","metadata":{}},{"source":"greyChosenImage = resizedChosenImage.convert(\"LA\")","cell_type":"code","execution_count":null,"metadata":{"collapsed":true},"outputs":[]},{"source":"#then plot\nfig, axes = plt.subplots(1,2)\nfig.set_size_inches(figWidth,figHeight)\naxes[0].imshow(resizedChosenImage)\naxes[1].imshow(greyChosenImage)","cell_type":"code","execution_count":null,"metadata":{},"outputs":[]},{"source":"_Figure 3: Grey-Scaling of our Chosen Image._\n\nThis allows us to multiply the number of images in a class by at most $2$. Obviously, if a class is already greyscaled, then we can't multiply the number of images in this manner\n\n# Coloring\n\nI had an idea for how to potentially multiply the number of greyscaled images we have by coloring said images.\n\n1. Greyscale all color images.\n2. Using a Convolution-to-Convolution network, use the greyscaled images to predict the colored images. This gives us a pipeline between grey images to colored images.\n3. Then, map all greyscaled examples through this network to get colored versions of these images.\n\nGiven how important the color and patterns of a whale is to the actual identification of it, it might be important to have these example for predictive purposes. That being said, this kind of prediction task is out of the scope of this particular notebook. We might be interested in trying this later down the line.\n\n# Random Rotation\n\nFor the next sections, I will be using the parameters suggested by [this notebook](https://www.kaggle.com/lextoumbourou/humpback-whale-id-data-and-aug-exploration).\n\nLet's start with some random rotations.","cell_type":"markdown","metadata":{}},{"source":"#convert image to array\nimageArray = np.array(resizedChosenImage)","cell_type":"code","execution_count":null,"metadata":{},"outputs":[]},{"source":"numRotations = 4\nrotationSize = 30\n#make rotation\nrotatedImages = [\n    kim.random_rotation(imageArray,rotationSize, \n                        row_axis=0, col_axis=1, channel_axis=2, fill_mode='nearest')\n    for _ in range(numRotations)]","cell_type":"code","execution_count":null,"metadata":{},"outputs":[]},{"source":"#then plot each\nfig, givenSubplots = plt.subplots(2,2)\nfig.set_size_inches(figWidth,figHeight)\nfor i in range(len(rotatedImages)):\n    givenSubplots[int(i / 2),i % 2].imshow(rotatedImages[i])","cell_type":"code","execution_count":null,"metadata":{},"outputs":[]},{"source":"_Figure 4: Rotating our Image around  $30$ degrees._\n\nWe see that this image remains recognizable despite its slight rotation. I would say this is a strong reason to consider rotations for bumping up the image set.\n\n# Random Shift\n\nLet's try shifting the image some small amount.","cell_type":"markdown","metadata":{}},{"source":"numShifts = numRotations\nwidthRange = 0.1\nheightRange = 0.3\nshiftedImages = [\n    kim.random_shift(imageArray, wrg= widthRange, hrg= heightRange, \n                     row_axis=0, col_axis=1, channel_axis=2, fill_mode='nearest')\n    for _ in range(numShifts)]","cell_type":"code","execution_count":null,"metadata":{},"outputs":[]},{"source":"#then plot each\nfig, givenSubplots = plt.subplots(2,2)\nfig.set_size_inches(figWidth,figHeight)\nfor i in range(len(shiftedImages)):\n    givenSubplots[int(i / 2),i % 2].imshow(shiftedImages[i])","cell_type":"code","execution_count":null,"metadata":{},"outputs":[]},{"source":"_Figure 5: Some Image Shifts._\n\nSome of these shifts may be a bit too severe for our work. one thing we may want to do is lower the height shift range just a tad bit to be more in line with the width shift range.","cell_type":"markdown","metadata":{}},{"source":"# Random Shear\n\nI have no idea what a shear does, but if someone could provide reference on it, that would be great. All I know is that is does something to the images in transformation. Let's see what it does!","cell_type":"markdown","metadata":{}},{"source":"numShears = numRotations\ngivenIntensity = 0.4\nshearedImages = [\n    kim.random_shear(imageArray, intensity= givenIntensity, \n                 row_axis=0, col_axis=1, channel_axis=2, fill_mode='nearest')\n    for _ in range(numShears)]","cell_type":"code","execution_count":null,"metadata":{},"outputs":[]},{"source":"#then plot\nfig, givenSubplots = plt.subplots(2,2)\nfig.set_size_inches(figWidth,figHeight)\nfor i in range(len(shearedImages)):\n    givenSubplots[int(i / 2),i % 2].imshow(shearedImages[i])","cell_type":"code","execution_count":null,"metadata":{},"outputs":[]},{"source":"_Figure 6: Some sheared versiions of our image._\n\nStill can't tell with shearing is doing from these images. Anyone have suggestions on this?\n\n# Random Zoom\n\nGiven that these whales have flukes that come in all shapes and sizes, let's try to add some randomization via a zooming method.","cell_type":"markdown","metadata":{}},{"source":"numZooms = numRotations\nzoomRangeWidth = 1.5\nzoomRangeHeight = .7\nzoomedImages = [\n    kim.random_zoom(imageArray, zoom_range=(zoomRangeWidth,zoomRangeHeight),\n                row_axis=0, col_axis=1, channel_axis=2, fill_mode='nearest')\n    for _ in range(numZooms)]","cell_type":"code","execution_count":null,"metadata":{},"outputs":[]},{"source":"#then plot\nfig, givenSubplots = plt.subplots(2,2)\nfig.set_size_inches(figWidth,figHeight)\nfor i in range(len(zoomedImages)):\n    givenSubplots[int(i / 2),i % 2].imshow(zoomedImages[i])","cell_type":"code","execution_count":null,"metadata":{},"outputs":[]},{"source":"_Figure 7: Zoomed versions of our image._\n\nWe see that a lot of variation begins to show in this method. In my opinion, this will probably have the strongest impact on generalization, because not all cameras can get close enough to get the best picture of the fluke.","cell_type":"markdown","metadata":{}},{"source":"# Redscaling\n\nAs suggested by my [previous analysis](https://www.kaggle.com/mmrosenb/whales-an-exploration), there is some evidence of redscaling in the training and test dataset. Thus, we should have our pictures be robust to this scale. While I still haven't figured out a way to identify redscaling in a picture, it is pretty straightforward to transfer a current picture into redscale.","cell_type":"markdown","metadata":{}},{"source":"#collapse to redscale\nredImageArray = copy.deepcopy(imageArray)\nredImageArray[:,:,1] = 0\nredImageArray[:,:,2] = 0","cell_type":"code","execution_count":null,"metadata":{},"outputs":[]},{"source":"plt.imshow(redImageArray)","cell_type":"code","execution_count":null,"metadata":{},"outputs":[]},{"source":"_Figure 8: Redscaling of image._\n\nThis is not the kind of redscaling we are looking for. This is much too dark when compared to the redscaling present in [my previous analysis](https://www.kaggle.com/mmrosenb/whales-an-exploration). Thus, I think I'm going to leave redscaling out of the transformation set until I have this right.\n\n# Better Red-Scaling\n\nLet's try redscaling via [these instructions](http://nicolas.riousset.com/category/software-methodologies/basic-color-scale-conversion-algorithm/).","cell_type":"markdown","metadata":{}},{"source":"colorEnhancer = 30\naverageArray = np.mean(imageArray,axis = 2)\nredImageArray = copy.deepcopy(imageArray)\nredImageArray[:,:,1] = averageArray\nredImageArray[:,:,2] = averageArray\nredImageArray[:,:,0] += colorEnhancer","cell_type":"code","execution_count":null,"metadata":{},"outputs":[]},{"source":"plt.imshow(redImageArray)","cell_type":"code","execution_count":null,"metadata":{},"outputs":[]},{"source":"_Figure 9: New Redscaling of Image._\n\nLooks like that's not the kind of redscaling we are looking for either. We'll need to get back to this to get the redscaling we are looking for as in [this analysis](https://www.kaggle.com/mmrosenb/whales-an-exploration).","cell_type":"markdown","metadata":{}},{"source":"# Other transformation options\n\n* One other possible transformation option is the vertical flip, but as suggested by [this notebook](https://www.kaggle.com/lextoumbourou/humpback-whale-id-data-and-aug-exploration), since vertical alignment is crucial between Flukes, it might be better to leave this transformation out of consideration\n\n# Number of images post-transformation\n\nLet's do a quick count of the number of images you can create if you keep applying the five transformations above in a sequential manner.\n\n## Color Picture\n\nThis allows us to start with two pictures: the color version and the greyscale version. Say that we apply four randomized versions of each transformation sequentially.","cell_type":"markdown","metadata":{}},{"source":"initialNumImages = 2\nnumberOfTransformations = 4\nnumRandomIterationsOfTransformation = 4\nnumImages = initialNumImages\n#then apply sequentially\nfor i in range(numberOfTransformations):\n    numImages += (numImages * numRandomIterationsOfTransformation)\nnumImages","cell_type":"code","execution_count":null,"metadata":{},"outputs":[]},{"source":"If we keep applying these, this gets us $1250$ new images! Even bigger than the largest class (see [this analysis](https://www.kaggle.com/mmrosenb/whales-an-exploration)).\n\n# Grayscaled Picture\n\nSince it is already grayscaled, we only start with one image.","cell_type":"markdown","metadata":{}},{"source":"initialNumImages = 1\nnumberOfTransformations = 4\nnumRandomIterationsOfTransformation = 4\nnumImages = initialNumImages\n#then apply sequentially\nfor i in range(numberOfTransformations):\n    numImages += (numImages * numRandomIterationsOfTransformation)\nnumImages","cell_type":"code","execution_count":null,"metadata":{},"outputs":[]},{"source":"Only half, as expected. Still, this gets us close the largest class of $810$ images (see [this analysis](https://www.kaggle.com/mmrosenb/whales-an-exploration) for details).","cell_type":"markdown","metadata":{}},{"source":"# Conclusion\n\nWe can significantly increase the size of our small classes by sequentially applying various transformations to our images. Of course, we will have to make sure these transformations do not alter the availability of the fluke in the image, else this would defeat the purpose of making these transformations. Nonetheless, This should be a step in the right direction in balancing our classes.\n\nWe still need to figure out how to apply an appropriate redscaling algorithm. Perhaps instructions on the internet [like these](https://stackoverflow.com/questions/5976153/how-to-convert-an-image-to-redscale).","cell_type":"markdown","metadata":{}},{"source":"","cell_type":"code","execution_count":null,"metadata":{"collapsed":true},"outputs":[]}],"nbformat":4,"metadata":{"language_info":{"nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4","name":"python","codemirror_mode":{"name":"ipython","version":3},"mimetype":"text/x-python","file_extension":".py"},"kernelspec":{"name":"python3","language":"python","display_name":"Python 3"}}}