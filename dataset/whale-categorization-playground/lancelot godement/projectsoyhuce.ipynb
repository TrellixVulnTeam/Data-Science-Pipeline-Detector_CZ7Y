{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Project SoyHuCe\n\nL'objectif est ici d'identifier une baleine à partir d'une photo de sa queue.\n\nIl y a 4 250 noms de baleine différents inscrits dans les données d'entraînement, ainsi que 810 images qui sont étiquetées *new_whale*. Il y a en tout plus de 25 000 images.\n\n> Notebook réalisé par Lancelot Godement pour l'entreprise SoyHuCe.\n\n#### les imports"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport cv2\n\nfrom tqdm import tqdm\nimport os","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"On définit nos chemins d'environnement qui nous serviront pour tout le reste du projet."},{"metadata":{"trusted":true},"cell_type":"code","source":"projF  = \"../input/whale-categorization-playground/\"\ntestF  = \"test/\"\ntrainF = \"train/\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Entraînement\n\nOn récupère les données dont on a besoin : les flags associés aux images d'entraînement."},{"metadata":{"trusted":true},"cell_type":"code","source":"dataflag = pd.read_csv( projF + \"train.csv\" )\ndataflag.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1re idée : l'extraction d'objet au premier plan\n\nL'idée ici est d'isoler sur chaque image et dans la mesure du possible, l'objet au premier plan de celle-ci, c'est-à-dire la queue de l'animal. Dans le plan initial, nous souhaitions faire tourner l'algorithme d'apprentissage sur ces données de façon à se focaliser sur les données les plus importantes.\n\nCette idée a par la suite été abandonnée, car elle semble d'une complexité beaucoup trop élevé. Si cela ne pose pas de problème sur le petit nombre d'images utilisé pour l'exemple, dans un cas d'une base de données encore plus conséquente que celle utilisée dans le présent exemple, cela peut devenir une contrainte des plus bloquante."},{"metadata":{},"cell_type":"markdown","source":"Description des fonctions :\n* load : charge l'image sous la forme d'une matrice d'une taille définie, puis reconvertie l'image dans un nouvel espace de colorimétrie.\n* adaptive_hist : on améliore l'image en utilisant OpenCV CLAHE (Contrast Limited Adaptive Histogram Equalization). Pour éviter une augmentation trop forte du gain sur la totalité de l'image, on découpe cette dernière en grille avant d'appliquer l'égalisation d'histogramme sur chaque case.\n* k_means : améliore la qualité de l'image en utilisant l'algorithme d'apprentissage en créant une version \"compressée\" de celle-ci.\n* find_box : trace une boite autour de l'objet à cibler, via la technologie de OpenCV : [findContours](https://docs.opencv.org/master/d3/dc0/group__imgproc__shape.html#gadf1ad6a0b82947fa1fe3c3d497f260e0).\n* forgrd_ext : extrait l'objet au premier plan."},{"metadata":{"trusted":true},"cell_type":"code","source":"def load(path, size=128):\n    return cv2.cvtColor( cv2.resize( cv2.imread(path),(size,size) ), cv2.COLOR_BGR2RGB)\n\ndef adaptive_hist(img, clipLimit= 4.0):\n    window    = cv2.createCLAHE( clipLimit = clipLimit, tileGridSize = (8, 8) )\n    img_lab   = cv2.cvtColor( img, cv2.COLOR_BGR2Lab )\n    ch1, ch2, ch3 = cv2.split( img_lab )\n    img_l     = window.apply( ch1 )\n    img_clahe = cv2.merge( (img_l, ch2, ch3) )\n    return cv2.cvtColor( img_clahe, cv2.COLOR_Lab2BGR )\n\nfrom sklearn.cluster import KMeans\n\ndef k_means(img, n_colors = 4):\n    w, h, d  = original_shape = tuple(img.shape)\n    img      = img/255.0\n    image_array = np.reshape( img, (w * h, d) )\n    kmeans   = KMeans( n_clusters = n_colors, random_state = 0 ).fit( image_array )\n    labels   = kmeans.predict( image_array )\n    codebook = kmeans.cluster_centers_\n    d        = codebook.shape[1]\n    image    = np.zeros((w, h, d))\n    label_idx = 0\n    for i in range(w):\n        for j in range(h):\n            image[i][j] = codebook[labels[label_idx]]\n            label_idx += 1\n    return image\n\ndef find_box(edges): \n    co, hi  = cv2.findContours(edges, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n    con     = max(co, key=cv2.contourArea)\n    conv_hull = cv2.convexHull(con)\n    \n    top     = tuple(conv_hull[conv_hull[:,:,1].argmin()][0])\n    bottom  = tuple(conv_hull[conv_hull[:,:,1].argmax()][0])\n    left    = tuple(conv_hull[conv_hull[:,:,0].argmin()][0])\n    right   = tuple(conv_hull[conv_hull[:,:,0].argmax()][0])\n    \n    return top, bottom, left, right\n\ndef forgrd_ext(img, rec):\n    mask    = np.zeros(img.shape[:2], np.uint8)\n    bgmodel = np.zeros((1, 65), np.float64)\n    fgmodel = np.zeros((1, 65), np.float64)\n    \n    cv2.grabCut(img, mask, rec, bgmodel, fgmodel, 3, cv2.GC_INIT_WITH_RECT)\n    \n    mask2   = np.where((mask==2)|(mask==0), 0, 1).astype('uint8')\n    img     = img*mask2[:,:,np.newaxis]\n    img[np.where( (img == [0,0,0]).all(axis = 2) )] = [255.0, 255.0, 255.0]\n    return img","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def ext_frgd():\n    f, ax = plt.subplots(5, 5, figsize=(40,30))\n    for i in tqdm(range(25)):\n        \n        path   = os.path.join( projF + trainF, dataflag.Image[i])\n        img_id = dataflag.Id[i]\n        \n        img    = load(path, 300)\n        img    = adaptive_hist(img, clipLimit = 4.0)\n        org    = img.copy()\n        img    = k_means(img , n_colors= 10)\n        \n        img_gray = cv2.cvtColor(np.uint8(img*255), cv2.COLOR_RGB2GRAY)\n        img_gray = cv2.medianBlur(img_gray,7)\n        edges    = cv2.Canny(img_gray,100,200)\n        \n        kernel = cv2.getStructuringElement(cv2.MORPH_RECT,(15,15))\n        edges  = cv2.morphologyEx(edges, cv2.MORPH_CLOSE, kernel)\n        \n        top,bottom,left,right = find_box(edges)\n        rec = (left[0], top[1], right[0]-left[0], bottom[1]-top[1])\n        forground_img = forgrd_ext(org, rec)\n        \n        ax[i//5][i%5].imshow(forground_img, aspect='auto')\n        ax[i//5][i%5].set_title(img_id)\n        ax[i//5][i%5].set_xticks([]); ax[i//5][i%5].set_yticks([])\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ext_frgd()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---\n\n\n# 2me idée : faire un apprentissage plus classique\n\nPlus direct de conception, cette idée se base sur le projet [CNN with Keras for Humpback Whale ID](https://www.kaggle.com/anezka/cnn-with-keras-for-humpback-whale-id) dont nous avons tout de fois optimisé le code, forçant ainsi sa compréhension dans son intégralité. Il s'agit ici d'associer les matrices d'images à l'*Id* qui les décrits, puis d'utiliser l'apprentissage artificiel : un réseau neuronal convolutif (CNN), de façon à être capable de prédire à partir d'une image inconnue le nom ou les noms potentiels d'une baleine.\n\n#### Import"},{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib inline\n\nimport matplotlib.image as mplimg\nfrom matplotlib.pyplot import imshow\n\nimport gc\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\n\nfrom keras import layers\nfrom keras.preprocessing import image\nfrom keras.applications.imagenet_utils import preprocess_input\nfrom keras.layers import Input, Dense, Activation, BatchNormalization, Flatten, Conv2D\nfrom keras.layers import AveragePooling2D, MaxPooling2D, Dropout\nfrom keras.models import Model\n\nimport keras.backend as K\nfrom keras.models import Sequential\n\nimport warnings\nwarnings.simplefilter(\"ignore\", category=DeprecationWarning)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Préparation d'image\n\nOn définit les images de façon à ce qu'elles soient encodées sous la forme de matrices interprétables par notre système."},{"metadata":{"trusted":true},"cell_type":"code","source":"def prepareImages(pathD, data, m, dataset):\n    \n    X_train = np.zeros((m, 100, 100, 3))\n    \n    for i in tqdm(range(len(data['Image']))) :\n        img = image.load_img(pathD + data['Image'][i], target_size = (100, 100, 3)) # charge les images sous la forme d'une matrice de 100 * 100 * 3\n        x   = image.img_to_array(img) #                                               transforme l'image en array\n        x   = preprocess_input(x)     #                                               traite les données sous la forme d'un tableau numpy\n        X_train[i] = x\n        \n    X_train /= 255\n    return X_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = prepareImages(projF + trainF, dataflag, dataflag.shape[0], \"train\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Préparation des Labels\nOn encode les labels de façon à ce qu'ils puissent être représentés sous une forme booléenne.\nAinsi, les données sont représentées sous la forme :\n> 0, 0, 0, 0, 1, 0, 0, 0, 0, 0\n\nChaque colonne représentant un *Id* particulier."},{"metadata":{"trusted":true},"cell_type":"code","source":"def prepare_labels(y):\n    \n    values          = np.array(y)\n    label_encoder   = LabelEncoder()\n    integer_encoded = label_encoder.fit_transform(values) #              transforme les valeurs en entier\n    print(integer_encoded)\n    \n    onehot_encoder  = OneHotEncoder(sparse=False)\n    integer_encoded = integer_encoded.reshape(len(integer_encoded), 1) # retourne la matrice\n    onehot_encoded  = onehot_encoder.fit_transform(integer_encoded)    # met les données sous forme booléenne\n    print(onehot_encoded)\n    \n    y               = onehot_encoded\n    print(y.shape)\n    return y, label_encoder","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y, label_encoder = prepare_labels(dataflag['Id'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"X-train shape :\", X.shape)\nprint(\"y-train shape :\", y.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## On définit le modèle d'apprentissage\n\nL'apprentissage peut prendre un temps assez conséquent. Il est donc vivement conseillé d'utiliser un GPU pour cela."},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\n\nmodel.add(Conv2D(32, (7, 7), strides = (1, 1), name = 'conv0', input_shape = (100, 100, 3))) # Convolution ajoutant un biais aux données\n\nmodel.add(BatchNormalization(axis = 3, name = 'bn0')) #                                        Normalise les inputs tout en limitant les pertes d'informations\nmodel.add(Activation('relu')) #                                                                Applique la fonction d'activation basée sur la fonction Relu\n\nmodel.add(MaxPooling2D((2, 2), name='max_pool')) #                                             Couche de mise en commun maximum\nmodel.add(Conv2D(64, (3, 3), strides = (1,1), name=\"conv1\"))\nmodel.add(Activation('relu'))\nmodel.add(AveragePooling2D((3, 3), name='avg_pool')) #                                         Couche de mise en commun moyenne\n\nmodel.add(Flatten()) #                                                                         Applatit l'entrée\nmodel.add(Dense(500, activation=\"relu\", name='rl'))\nmodel.add(Dropout(0.8)) #                                                                      Met de côté les valeurs les moins représenté pour éviter le surajustement\nmodel.add(Dense(4251, activation='softmax', name='sm'))\n\nmodel.compile(loss='categorical_crossentropy', optimizer=\"adam\", metrics=['accuracy'])\nmodel.summary()\nprint(model.output_shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(X, y, epochs=100, batch_size=100, verbose=1)\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(history.history['accuracy'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# On passe à la pratique sur les données de test\nComme expliqué dans le projet *CNN with Keras for Humpback Whale ID*, nous divisons les données en plusieurs parties de façon à ne pas être bloqué par la taille de celle-ci."},{"metadata":{"trusted":true},"cell_type":"code","source":"test      = os.listdir( projF + testF )\ncol       = ['Image']\ntestData  = pd.DataFrame(test,             columns = col)\ntestData1 = pd.DataFrame(test[:3900],      columns = col)\ntestData2 = pd.DataFrame(test[3900:7800],  columns = col)\ntestData3 = pd.DataFrame(test[7800:11700], columns = col)\ntestData4 = pd.DataFrame(test[11700:],     columns = col)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"On prédit puis concatène toutes les prédictions."},{"metadata":{"trusted":true},"cell_type":"code","source":"testAllData = [testData1, testData2, testData3, testData4]\nprediction = []\nfor data in testAllData :\n    X = prepareImages(projF + testF, data, data.shape[0], \"test\")\n    prediction += [model.predict(np.array(X), verbose = 1)]\n    gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = np.concatenate(prediction, axis=0)\ngc.collect()\nprint(predictions.shape)\nprint(predictions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"copy_predB = np.copy(predictions)\nidx       = []\nfor i in tqdm(range(5)) :\n    idx += [np.argmax(copy_predB, axis=1)]\n    copy_predB[:,idx[i]] = 0","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"On retransforme les *Id* de leur forme numérique à leur forme textuelle.\nOn en profite pour appliquer un seuil de précision de façon à ne garder que les prédictions les plus probables."},{"metadata":{"trusted":true},"cell_type":"code","source":"results   = []\nthreshold = 0.05 # seuil de précision\n\nfor i in tqdm(range(0, predictions.shape[0])):\n    each = []\n    tags = []\n    for j in range(len(idx)):\n        each += [np.zeros((4251, 1))]\n    if((predictions[i, idx[4][i]] > threshold)):\n        tags = []\n        for j in range(len(idx)):\n            each[j][idx[j][i]] = 1\n            tags += [label_encoder.inverse_transform([np.argmax(each[j])])[0]]\n            \n    elif((predictions[i, idx[3][i]] > threshold)):\n        print(predictions[i, idx[3][i]])\n        for j in range(len(idx)-1):\n            each[j][idx[j][i]] = 1\n        tags = []\n        for j in range(len(idx)-1):\n            tags += [label_encoder.inverse_transform([np.argmax(each[j])])[0]]\n            \n    elif((predictions[i, idx[2][i]] > threshold)):\n        for j in range(len(idx)-2):\n            each[j][idx[j][i]] = 1\n        tags = []\n        for j in range(len(idx)-2):\n            tags += [label_encoder.inverse_transform([np.argmax(each[j])])[0]]\n            \n    elif((predictions[i, idx[1][i]] > threshold)):\n        for j in range(len(idx)-3):\n            each[j][idx[j][i]] = 1\n        tags = []\n        for j in range(len(idx)-3):\n            tags += [label_encoder.inverse_transform([np.argmax(each[j])])[0]]\n            \n    else:\n        each[0][idx[0][i]] = 1\n        tags = [label_encoder.inverse_transform([np.argmax(each[0])])[0]]\n    results += [tags]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Dernière étape : on enregistre nos résultats"},{"metadata":{"trusted":true},"cell_type":"code","source":"import csv\nmyfile = open('output.csv','w')\ncolumn = ['Image', 'Id']\n\nwrtr   = csv.writer(myfile, delimiter=',')\nwrtr.writerow(column)\n\nfor i in tqdm(range(testData.shape[0])):\n    pred = \"\"\n    for j in range(len(results[i])):\n        if j != 0:\n            pred += \" \"\n        pred += results[i][j]\n            \n    result = [testData['Image'][i], pred]\n    wrtr.writerow(result)\n    \nmyfile.close()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conclusion\n\nL'objectif de cet exercice était de parvenir à prédire le nom d'une baleine à partir de sa queue. De façon objective, nous parvenons à un résultat assez satisfaisant.\n\nD'un point de vue méthodologique, si je n'ai pas produit l'entièreté du code par moi-même, j'ai tenu à le comprendre et à l'optimiser de façon à ce qu'il soit le plus possible à appréhender, et cela, même pour quelqu'un qui n'est pas spécialiste du domaine.\n\nN'ayant pas encore beaucoup d'expérience dans le domaine, ainsi qu'un nombre d'applications réelles très limité, je souhaiterais apprendre un maximum de chose auprès des professionnels que vous êtes.\n\n> GODEMENT Lancelot"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}