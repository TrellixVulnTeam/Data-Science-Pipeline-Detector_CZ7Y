{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nimg = '/kaggle/input'\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After importing the images of the whales within the train/test set, for this step, the images will be extracted into pixel values for future computations. The feature of whale images will be classified using the basic l-layered logistic regression.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.image as mpimg","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Imaages either in RGB or Grayscale, hence to make every data consistent, one will attempt to convert every image to greyscale. Firstly, will use the library in importing the image plot from MATLAB to graph the image pixel values (greyscale). ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def rgb_to_grey(img):\n    if len(img.shape) == 2: #if there are only two scales then it is grayscale image\n        return(img)\n    greyImage = np.zeros(img.shape)\n    Red = img[:,:,0] * 0.299\n    Green = img[:,:,1] * 0.587\n    Blue = img[:,:,2] * 0.114\n    \n    greyImage = Red + Green + Blue\n    \n    return(greyImage)\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The next step is to standardize the image sizes, so all the images will have the consistent number of features. This is done by the following function. The constants that are multiplied for each images for each of the colors varied by the index number when converted to greyscaling. Reference: Stack Exchange","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def img_standardize(img, x_bin = 100, y_bin = 100):\n    x_splits = np.linspace(0,img.shape[1] - 1, x_bins + 1, dtype = int)\n    y_splits = np.linspace(0,img.shape[0] - 1, x_bins + 1, dtype = int)\n    compressed = np.zeros((y_bins,x_bins))\n    \n    for i in range(y_bins):\n        for j in range(x_bins):\n            temp = np.mean(img[y_splits[i]:y_splits[i+1], x_splits[j]:x_splits[j+1]])\n            if math.isnan(temp):\n                if y_splits[i] == y_splits[i+1]:\n                    compressed[i,j] = compressed[i-1,j]\n                else:\n                    compressed[i,j] = compressed[i,j-1]\n            else: \n                compressed[i,j] = int(temp)\n                \n    return(compressed)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This process converts the .jpeg images into array of pixel values. Reference: Stack Exchange","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_files_names = os.listdir(train_dir)[::5]\nimgs_train = [rgb_to_grey(mpimg.imread(train_dir + '/' + file, format = 'JGP')) for file in os.listdir(train_dir)]\n\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Remark: For the following step, I could not find a way to convert the image pixels into the train and test set yet, but the following will be the algorthm in making predictions and training the data set. For the **feature engineering** phase, I haven't have the knowledge to write the code, but for theoretical context, several procedures and techniques are appropriate for usage before the train/test set computation. \n\nThe first pre-processing that was implemented above was the recoloring of the picture, in which the function takes the image that is supposedly RGB to a greyscale color scheme. This is for the ease of analysis, as every picture will be stardardize to commonscale. Afterwards, resizing is implemented to make all the images scaled into the same sizes, so every matrix of the layers when analyze in the logistic regression would be consistent (crucial as the future calculation will be based on vectorization). These image pixel numbers will be the train_x and test_x data sets. The third preprocessing phase would be the division of all the data sets by 255. Because the RGB-converted greyscale ranges from 1 to 255, every values are normalize to between 0 to 1 for ease of analysis (stats and probability are also in this same range).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Reshape the training and test examples \ntrain_x_flatten = imgs_train.reshape(train_x_orig.shape[0], -1).T   # The \"-1\" makes reshape flatten the remaining dimensions\ntest_x_flatten = test_x_orig.reshape(test_x_orig.shape[0], -1).T\n\n# Standardize data to have feature values between 0 and 1.\ntrain_x = train_x_flatten/255.\ntest_x = test_x_flatten/255.\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is the first phase of computation process of training set and test set. As mentioned, the identifying of the whale will be done thorugh l-layered logistic regression. In the first will initialize initial test/train set through random gaussian variable; this is because the initial data set regulated to be nonzero for the performance of the computation. The input variables are the n_x,n_h,n_y are the dimensions for the row, hidden, and columns of the training set, respectively. The beta would be the scale value that should be in the order of 10^-4 to 10^-2.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def initialize_parameters(n_x, n_h, n_y, beta):\n    W1 = np.random.randn(n_h,n_x) * beta\n    b1 = np.zeros((n_h, 1))\n    W2 = np.random.randn(n_y,n_h) * beta\n    b2 = np.zeros((n_y, 1))\n   \n    parameters = {\"W1\": W1,\n                  \"b1\": b1,\n                  \"W2\": W2,\n                  \"b2\": b2}\n    \n    return parameters    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The above function would only randomize the variables for only for one single layer, hence this initialize_parameters_deep function would introduce the layer-dimensions into the data. The \"W\" and \"b\" of each layer-l is the weight and the bias for the training set. \n\nFor **feature engineering**: use the basis of regularization and dropout. To prevent high variance the regularization of L-2 norm using the lambda/(the number of data sets x) * weight for each  of the linear_forward and linear_backward functions. But the \"inverted dropout\" can also be implemented with using the \"keep_prob\" parameter which will keep that amount of portion of the data for analyze. By eliminating the rest will decrese chance of overfitting: (with number of layer i):\n\nd[i] = np.random.rand(a[i].shape[0], a[i].shape[1]) < keep_prob\na[i] = np.multiply(a3,d3)\na3 /= keep_prob #columns of d[i] that will be zero","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def initialize_parameters_deep(layer_dims):\n\n    np.random.seed(3)\n    parameters = {}\n    L = len(layer_dims)            # number of layers in the network\n\n    for l in range(1, L):\n        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) * 0.01\n        parameters['b' + str(l)] = np.zeros((layer_dims[l],1))\n  \n    return parameters","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Linear_forward function performs the forward propagation. The output Z and cache will be used for the activation function in each of the iterations of calculation.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def linear_forward(A, W, b):\n\n    Z = np.dot(W,A) + b\n   \n    return Z, cache","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The following function model will serve as an appendix for the computation sequences below. These functions are all positive definite function and will be used as an activation function for the forward process in the l-layered network.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def sigmoid(z):\n    s = 1/(1+np.exp(-z))\n    return s\n\ndef tanh(z):\n    s = (np.exp(2*x) - 1)/(np.exp(2*x) + 1)\n    return s\n\ndef relu(z):\n    s = np.maximum(0,z)\n    return s\n\ndef softmax(z):\n    expo = np.exp(z)\n    expo_sum = np.sum(np.exp(z))\n    return expo/expo_sum","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"linear_activation_forward takes in the iteration of the previous activation value A_prev, the weight, bias, and \"activation\" or certain activation function types. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef linear_activation_forward(A_prev, W, b, activation):\n    #\"\"\"\"\"\"\n   # Arguments:\n   # A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n   # W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n   # b -- bias vector, numpy array of shape (size of the current layer, 1)\n   # activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n\n   # Returns:\n   # A -- the output of the activation function, also called the post-activation value \n   # cache -- a python tuple containing \"linear_cache\" and \"activation_cache\";\n   #          stored for computing the backward pass efficiently\n    \n    if activation == \"sigmoid\":\n        Z, linear_cache = linear_forward(A_prev, W, b)\n        A, activation_cache = sigmoid(Z)\n        \n    elif activation == \"relu\":\n        Z, linear_cache = linear_forward(A_prev, W, b)\n        A, activation_cache = relu(Z)\n        \n    elif activation == \"hyptan\":\n        Z, linear_cache = linear_forward(A_prev, W, b)\n        A, activation_cache = tanh(Z)\n        \n    elif activation == \"softmax\":\n        Z, linear_cache = linear_forward(A_prev, W, b)\n        A, activation_cache = softmax(Z)\n    \n    return A, cache","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For the performance of the forward propagation, each of the functions in the appendix are utilized by each of the third of the data. The relu is best as the function increases the most at the beginning of the data. The hyperbolic tangent function will perform best when the number of data is not too large or too small. And the softmax is performed best near beginning and the end because of its own regression and the interpolation performances of the exponential functions.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def L_model_forward(X, parameters):\n\n    caches = []\n    A = X\n    L = len(parameters) // 2                  # number of layers in the neural network\n    \n        \n    for l in range(1, L//3):\n        A_prev = A \n        A, cache = linear_activation_forward(A_prev, parameters['W' + str(l)], parameters['b' + str(l)], activation = \"softmax\")\n        caches.append(cache)\n        \n    for l in range(L//3, 2*L//3):\n        A_prev = A \n        A, cache = linear_activation_forward(A_prev, parameters['W' + str(l)], parameters['b' + str(l)], activation = \"hyptan\")\n        caches.append(cache)\n        \n    for l in range(2*L//3, L):\n        A_prev = A \n        A, cache = linear_activation_forward(A_prev, parameters['W' + str(l)], parameters['b' + str(l)], activation = \"softmax\")\n        caches.append(cache)\n\n    AL, cache = linear_activation_forward(A, parameters['W' + str(L)], parameters['b' + str(L)], activation = \"sigmoid\")\n    caches.append(cache)\n   \n            \n    return AL, caches","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The compute_cost is the function that prepares the cost function to analyze the performance of the forward propagation. This is derived by the loss (Error) function from the sigmoid function. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def compute_cost(AL, Y):\n  \n    m = Y.shape[1]\n\n\n    logprobs = np.multiply(np.log(AL),Y) +  np.multiply(np.log(1-AL), (1-Y))\n    cost = -1/m*np.sum(logprobs)\n\n    cost = np.squeeze(cost)     \n    \n    return cost","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The linear_backward function calculates the gradient descent in which will iterate through the weight and bias parameters, introducing the dW and db, dZ. The three quantities are from the jacobian of the each W, b, Z function. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def linear_backward(dZ, cache):\n \n    A_prev, W, b = cache\n    m = A_prev.shape[1]\n\n    dW = 1./m*np.dot(dZ, A_prev.T)\n    db = 1./m*np.sum(dZ, axis = 1, keepdims=True)\n    dA_prev = np.dot(W.T, dZ)\n    \n\n    return dA_prev, dW, db","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Defines the activation function backwards, prepared using the four functions below: ReLU, Sigmoid, Hyperbolic Tangent, Softmax.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def linear_activation_backward(dA, cache, activation):\n\n    linear_cache, activation_cache = cache\n    \n    if activation == \"relu\":\n        dZ = relu_backward(dA, activation_cache)\n        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n    \n    elif activation == \"sigmoid\":\n        dZ = sigmoid_backward(dA, activation_cache)\n        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n        \n    elif activation == \"hyptan\":\n        dZ = hyptan_backward(dA, activation_cache)\n        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n        \n    elif activation == \"softmax\":\n        dZ = softmax_backward(dA, activation_cache)\n        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n    \n    return dA_prev, dW, db","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Use the L_model_backward to do a gradient descent for each hidden layers, utilizes the sigmoid and softmax function. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def L_model_backward(AL, Y, caches):\n\n    grads = {}\n    L = len(caches) # the number of layers\n    m = AL.shape[1]\n    Y = Y.reshape(AL.shape) \n    \n    # Initializing the backpropagation\n    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n    \n    # Lth layer (SIGMOID -> LINEAR) gradients. \n    current_cache = caches[L-1]\n    grads[\"dA\" + str(L)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache, activation = \"sigmoid\")\n \n    for l in reversed(range(L-1)):\n        # lth layer: (SOFTMAX -> LINEAR) gradients.\n    \n        current_cache = caches[l]\n        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l + 2)],  current_cache, activation = \"softmax\")\n        grads[\"dA\" + str(l + 1)] = dA_prev_temp\n        grads[\"dW\" + str(l + 1)] = dW_temp\n        grads[\"db\" + str(l + 1)] = db_temp\n\n    return grads","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is the final function, which includes the other hyperparameters: learning rate, number of iterations, layer dimension to calculate the accuracy of the image. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def L_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):\n    np.random.seed(1)\n    costs = []                         # keep track of cost\n    \n    # Parameters initialization. \n    parameters = initialize_parameters_deep(layers_dims)\n   \n    # Loop (gradient descent)\n    for i in range(0, num_iterations):\n\n        # Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.\n    \n        AL, caches = L_model_forward(X, parameters)\n      \n        # Compute cost.\n    \n        cost = compute_cost(AL, Y)\n       \n        # Backward propagation.\n        grads = L_model_backward(AL, Y, caches)\n   \n        # Update parameters.\n        parameters = update_parameters(parameters, grads, learning_rate)\n        \n        # Print the cost every 100 training example\n        if print_cost and i % 100 == 0:\n            print (\"Cost after iteration %i: %f\" %(i, cost))\n        if print_cost and i % 100 == 0:\n            costs.append(cost)\n            \n    # plot the cost\n    plt.plot(np.squeeze(costs))\n    plt.ylabel('cost')\n    plt.xlabel('iterations (per hundreds)')\n    plt.title(\"Learning rate =\" + str(learning_rate))\n    plt.show()\n    \n    return parameters","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Take the parameters and analyze the prediction for the function below.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict(parameters, X):\n    \n    # Computes probabilities using forward propagation, and classifies to 0/1 using 0.5 as the threshold.\n    A2, cache = forward_propagation(X,parameters)\n    predictions = (A2 > 0.5)\n \n    return predictions","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}