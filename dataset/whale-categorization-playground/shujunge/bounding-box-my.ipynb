{"cells":[{"metadata":{"trusted":true,"_uuid":"fcc80c91799f2019db62e49719a91308d150b602"},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os \nprint(os.listdir(\"../input/\"))\nbox_path=\"../input/humpback-whale-identification-fluke-location/cropping.txt\"\ntrain_dataset=\"../input/whale-categorization-playground/train/train/\"\ntest_dataset=\"../input/whale-categorization-playground/test/test/\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fae13a7619d68862cdff85b2ce577e921ea12a27"},"cell_type":"code","source":"boxs=[]\nwith open(box_path,\"r\") as f:\n    for line in f.readlines():\n        p,*coord = line.split(\",\")\n        line=(p,[(int(coord[i]),int(coord[i+1])) for i in range(0,len(coord),2)]) \n        boxs.append(line)\nprint(len(boxs))\nprint(boxs[:2])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cb1af5418d6b57a3e480cae82a3c49b3a1073246"},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport cv2\ndef pathfile(path):\n    from os.path import isfile\n    p=train_dataset+path\n    if isfile(p):\n        return p\n    p=test_dataset+path\n    if isfile(p):\n        return p\n    return p\n\ndef bounding_rectangle(list,src_size,size=128):\n    x0, y0 = list[0]\n    x1, y1 = x0, y0\n    for x,y in list[1:]:\n        x0 = min(x0, x)\n        y0 = min(y0, y)\n        x1 = max(x1, x)\n        y1 = max(y1, y)\n#     return x0,y0,x1,y1\n    return x0/src_size[1],y0/src_size[0],x1/src_size[1],y1/src_size[0]\n\nsize=128\nsrc=cv2.imread(pathfile(boxs[0][0]),3)\nprint(src.shape)\nplt.imshow(src)\nimg_box= bounding_rectangle(boxs[0][1],src.shape[:2],size)\nsrc=cv2.resize(src,(size,size))\nprint(img_box)\ncv2.rectangle(src,(int(size*img_box[0]),int(size*img_box[1])),(int(size*img_box[2]),int(size*img_box[3])),(0,255,0), 1)\nplt.imshow(src)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"346d433b38eb6b72c6f5ec6a79849b00b1681bd4"},"cell_type":"code","source":"import numpy as np\nfrom keras.models import *\nfrom keras.layers import *\nfrom keras.losses import *\nfrom keras.optimizers import *\nfrom keras.callbacks import *\nfrom keras.applications import *\nfrom keras.utils import *\nfrom keras.metrics import *\nimport keras.backend as K\nimport cv2\nimport pandas as pd","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b1ac2b4cde76b378b7ce31873624ac562b0047e1"},"cell_type":"markdown","source":"# Image preprocessing code\nImages are preprocessed by:\n1. Converting to black&white;\n1. Compressing horizontally by a factor of 2.15 (the mean aspect ratio);\n1. Apply a random image transformation (only for training)\n1. Resizing to 128x128;\n1. Normalizing to zero mean and unit variance.\n\nThese operation are performed by the following code that is later invoked when preparing the corpus."},{"metadata":{"trusted":true,"_uuid":"50ecaa520bf8edc5a4ccf126e4be7ccc80918e50"},"cell_type":"code","source":"anisotropy = 2.15\nimg_shape=(128,128,1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ca0291c0ef9f5a38668f5b3b504e3500238858a2"},"cell_type":"code","source":"import random\nimport numpy as np\nfrom scipy.ndimage import affine_transform\nfrom keras.preprocessing.image import img_to_array\nfrom numpy.linalg import inv as mat_inv\nfrom PIL import Image as pil_image\nfrom PIL.ImageDraw import Draw\nfrom os.path import isfile\n\n\ndef read_raw_image(p):\n    return pil_image.open(pathfile(p))\n\ndef boundingrectangle(list):\n    x0, y0 = list[0]\n    x1, y1 = x0, y0\n    for x,y in list[1:]:\n        x0 = min(x0, x)\n        y0 = min(y0, y)\n        x1 = max(x1, x)\n        y1 = max(y1, y)\n    return x0,y0,x1,y1\n\n# Read an image as black&white numpy array\ndef read_array(p):\n    img = read_raw_image(p).convert('L')\n    return img_to_array(img)\n\ndef build_transform(rotation, shear, height_zoom, width_zoom, height_shift, width_shift):\n    rotation        = np.deg2rad(rotation)\n    shear           = np.deg2rad(shear)\n    rotation_matrix = np.array([[np.cos(rotation), np.sin(rotation), 0], [-np.sin(rotation), np.cos(rotation), 0], [0, 0, 1]])\n    shift_matrix    = np.array([[1, 0, height_shift], [0, 1, width_shift], [0, 0, 1]])\n    shear_matrix    = np.array([[1, np.sin(shear), 0], [0, np.cos(shear), 0], [0, 0, 1]])\n    zoom_matrix     = np.array([[1.0/height_zoom, 0, 0], [0, 1.0/width_zoom, 0], [0, 0, 1]])\n    shift_matrix    = np.array([[1, 0, -height_shift], [0, 1, -width_shift], [0, 0, 1]])\n    return np.dot(np.dot(rotation_matrix, shear_matrix), np.dot(zoom_matrix, shift_matrix))\n\n# Compute the coordinate transformation required to center the pictures, padding as required.\ndef center_transform(affine, input_shape):\n    hi, wi = float(input_shape[0]), float(input_shape[1])\n    ho, wo = float(img_shape[0]), float(img_shape[1])\n    top, left, bottom, right = 0, 0, hi, wi\n    if wi/hi/anisotropy < wo/ho: # input image too narrow, extend width\n        w     = hi*wo/ho*anisotropy\n        left  = (wi-w)/2\n        right = left + w\n    else: # input image too wide, extend height\n        h      = wi*ho/wo/anisotropy\n        top    = (hi-h)/2\n        bottom = top + h\n    center_matrix   = np.array([[1, 0, -ho/2], [0, 1, -wo/2], [0, 0, 1]])\n    scale_matrix    = np.array([[(bottom - top)/ho, 0, 0], [0, (right - left)/wo, 0], [0, 0, 1]])\n    decenter_matrix = np.array([[1, 0, hi/2], [0, 1, wi/2], [0, 0, 1]])\n    return np.dot(np.dot(decenter_matrix, scale_matrix), np.dot(affine, center_matrix))\n\n# Apply an affine transformation to an image represented as a numpy array.\ndef transform_img(x, affine):\n    matrix   = affine[:2,:2]\n    offset   = affine[:2,2]\n    x        = np.moveaxis(x, -1, 0)\n    channels = [affine_transform(channel, matrix, offset, output_shape=img_shape[:-1], order=1,\n                                 mode='constant', cval=np.average(channel)) for channel in x]\n    return np.moveaxis(np.stack(channels, axis=0), 0, -1)\n\n# Read an image for validation, i.e. without data augmentation.\ndef read_for_validation(p):\n    x  = read_array(p)\n    t  = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n    t  = center_transform(t, x.shape)\n    x  = transform_img(x, t)\n    x -= np.mean(x, keepdims=True)\n    x /= np.std(x, keepdims=True) + K.epsilon()\n    return x,t \n\n# Read an image for training, i.e. including a random affine transformation\ndef read_for_training(p):\n    x  = read_array(p)\n    t  = build_transform(\n            random.uniform(-5, 5),\n            random.uniform(-5, 5),\n            random.uniform(0.9, 1.0),\n            random.uniform(0.9, 1.0),\n            random.uniform(-0.05*img_shape[0], 0.05*img_shape[0]),\n            random.uniform(-0.05*img_shape[1], 0.05*img_shape[1]))\n    t  = center_transform(t, x.shape)\n    x  = transform_img(x, t)\n    x -= np.mean(x, keepdims=True)\n    x /= np.std(x, keepdims=True) + K.epsilon()\n    return x,t   \n\n# Transform corrdinates according to the provided affine transformation\ndef coord_transform(list, trans):\n    result = []\n    for x,y in list:\n        y,x,_ = trans.dot([y,x,1]).astype(np.int)\n        result.append((x,y))\n    return result","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"abd830919f17ec657fb50f93e2afdbe951e92acd"},"cell_type":"code","source":"class DataGenerator(Sequence):\n    'Generates data for Keras'\n    def __init__(self, list_IDs,  batch_size=32, dim=(128,128), n_channels=3,\n                 n_classes=5005, shuffle=False,model=True):\n        self.dim = dim\n        self.batch_size = batch_size\n        self.list_IDs = list_IDs\n        self.n_channels = n_channels\n        self.n_classes = n_classes\n        self.shuffle = shuffle\n        self.model = model\n        self.on_epoch_end()\n\n    def __len__(self):\n        'Denotes the number of batches per epoch'\n        return int(np.floor(len(self.list_IDs) / self.batch_size))\n\n    def __getitem__(self, index):\n        'Generate one batch of data'\n        # Generate indexes of the batch\n        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n        # Find list of IDs\n        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n        # Generate data\n        X, y = self.__data_generation(list_IDs_temp)\n\n        return X, y\n\n    def on_epoch_end(self):\n        'Updates indexes after each epoch'\n        self.indexes = np.arange(len(self.list_IDs))\n        if self.shuffle == True:\n            np.random.shuffle(self.indexes)\n\n    def __data_generation(self, list_IDs_temp):\n        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n        # Initialization\n        X = np.empty((self.batch_size, *self.dim, self.n_channels),dtype=np.float32)\n        y = np.zeros((self.batch_size,self.n_classes))\n        # Generate data\n        if self.model:\n            for i, ID in enumerate(list_IDs_temp):\n                img,trans  = read_for_training(ID[0])\n                coords  = coord_transform(ID[1], mat_inv(trans))\n                X[i,]= img\n                y[i,]= boundingrectangle(coords)\n        else:\n            for i, ID in enumerate(list_IDs_temp):\n                img,trans  = read_for_validation(ID[0])\n                coords  = coord_transform(ID[1], mat_inv(trans))\n                X[i,]= img\n                y[i,]= boundingrectangle(coords)\n        return X, y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"82def9f582f53b7f7ccf7f834ba7ce4c54f3b0b3"},"cell_type":"code","source":"images_size=128\nparams = {'dim': (images_size,images_size),\n          'batch_size': 32,\n          'n_classes': 4,\n          'n_channels': 1,\n          'shuffle': True,\n         'model':True}\n\nfrom sklearn.model_selection import train_test_split\ntrain_txt, test_txt = train_test_split(boxs, test_size=200, random_state=1)\nprint(type(train_txt))\ntrain_txt += train_txt\ntrain_txt += train_txt\ntrain_txt += train_txt\ntrain_txt += train_txt\nprint(len(train_txt),len(test_txt))\n# Generators\ntraining_generator = DataGenerator(train_txt,  **params)\nparams = {'dim': (images_size,images_size),\n          'batch_size': 32,\n          'n_classes': 4,\n          'n_channels': 1,\n          'shuffle': True,\n         'model':False}\nvalidation_generator = DataGenerator(test_txt,  **params)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ee45fbcbbd72373b3070dcb429efbedf0bdb090b"},"cell_type":"code","source":"for image,label in training_generator:\n        print(image.shape,label.shape)\n        break","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c98955a5a1a3cea6e5cf2bb89299126b88407d39"},"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom tqdm import tqdm, tqdm_notebook\nfrom keras import backend as K\nfrom keras.preprocessing.image import array_to_img\nfrom numpy.linalg import inv as mat_inv\n\ndef show_whale(imgs, per_row=5):\n    n         = len(imgs)\n    rows      = (n + per_row - 1)//per_row\n    cols      = min(per_row, n)\n    fig, axes = plt.subplots(rows,cols, figsize=(24//per_row*cols,24//per_row*rows))\n    for ax in axes.flatten(): ax.axis('off')\n    for i,(img,ax) in enumerate(zip(imgs, axes.flatten())): ax.imshow(img.convert('RGB'))\n\nimages = []\nfor image,label in validation_generator:\n    for i in range(3):\n        a         = image[i:i+1]\n        rect1     = label[i]\n        img       = array_to_img(a[0]).convert('RGB')\n        draw      = Draw(img)\n        draw.rectangle(list(rect1), outline='red')\n        images.append(img)\n    break\n\nshow_whale(images)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2bed2237a023eab8160e87bcc13206177c35fb7c"},"cell_type":"code","source":"\nimg_shape  = (128,128,1)\ndef build_model(with_dropout=True):\n    kwargs     = {'activation':'relu', 'padding':'same'}\n    conv_drop  = 0.2\n    dense_drop = 0.5\n    inp        = Input(shape=img_shape)\n\n    x = inp\n\n    x = Conv2D(64, (9, 9), **kwargs)(x)\n    x = Conv2D(64, (3, 3), **kwargs)(x)\n    x = BatchNormalization()(x)\n    if with_dropout: x = Dropout(conv_drop, noise_shape=(None, 1, 1, int(x.shape[-1])))(x)\n\n    x = Conv2D(64, (2, 2), **kwargs, strides=2)(x)\n    x = Conv2D(64, (3, 3), **kwargs)(x)\n    x = Conv2D(64, (3, 3), **kwargs)(x)\n    x = BatchNormalization()(x)\n    if with_dropout: x = Dropout(conv_drop, noise_shape=(None, 1, 1, int(x.shape[-1])))(x)\n\n    x = Conv2D(64, (2, 2), **kwargs, strides=2)(x)\n    x = Conv2D(64, (3, 3), **kwargs)(x)\n    x = Conv2D(64, (3, 3), **kwargs)(x)\n    x = BatchNormalization()(x)\n    if with_dropout: x = Dropout(conv_drop, noise_shape=(None, 1, 1, int(x.shape[-1])))(x)\n\n    x = Conv2D(64, (2, 2), **kwargs, strides=2)(x)\n    x = Conv2D(64, (3, 3), **kwargs)(x)\n    x = Conv2D(64, (3, 3), **kwargs)(x)\n    x = BatchNormalization()(x)\n    if with_dropout: x = Dropout(conv_drop, noise_shape=(None, 1, 1, int(x.shape[-1])))(x)\n\n    x = Conv2D(64, (2, 2), **kwargs, strides=2)(x)\n    x = Conv2D(64, (3, 3), **kwargs)(x)\n    x = Conv2D(64, (3, 3), **kwargs)(x)\n    x = BatchNormalization()(x)\n    if with_dropout: x = Dropout(conv_drop, noise_shape=(None, 1, 1, int(x.shape[-1])))(x)\n\n    x = Conv2D(64, (2, 2), **kwargs, strides=2)(x)\n    x = Conv2D(64, (3, 3), **kwargs)(x)\n    x = Conv2D(64, (3, 3), **kwargs)(x)\n    x = BatchNormalization()(x)\n    if with_dropout: x = Dropout(conv_drop, noise_shape=(None, 1, 1, int(x.shape[-1])))(x)\n\n    h = MaxPooling2D(pool_size=(1, int(x.shape[2])))(x)\n    h = Flatten()(h)\n    if with_dropout: h = Dropout(dense_drop)(h)\n    h = Dense(16, activation='relu')(h)\n\n    v = MaxPooling2D(pool_size=(int(x.shape[1]), 1))(x)\n    v = Flatten()(v)\n    if with_dropout: v = Dropout(dense_drop)(v)\n    v = Dense(16, activation='relu')(v)\n\n    x = Concatenate()([h,v])\n    if with_dropout: x = Dropout(0.5)(x)\n    x = Dense(4, activation='linear')(x)\n    return Model(inp,x)\n\nmodel = build_model(with_dropout=True)\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c2ac55e66d57692eae76ef6b71c19dc0f8eb344b"},"cell_type":"code","source":"def calculate_iou(y_true, y_pred):\n  \"\"\"\n  Input:\n  Keras provides the input as numpy arrays with shape (batch_size, num_columns).\n\n  Arguments:\n  y_true -- first box, numpy array with format [x, y, width, height, conf_score]\n  y_pred -- second box, numpy array with format [x, y, width, height, conf_score]\n  x any y are the coordinates of the top left corner of each box.\n\n  Output: IoU of type float32. (This is a ratio. Max is 1. Min is 0.)\n\n  \"\"\"\n  import numpy as np\n  \n  results = []\n  \n  for i in range(0, y_true.shape[0]):\n    # set the types so we are sure what type we are using\n    y_true = y_true.astype(np.float32)\n    y_pred = y_pred.astype(np.float32)\n    # boxTrue\n    x_boxTrue_tleft = y_true[0, 0]  # numpy index selection\n    y_boxTrue_tleft = y_true[0, 1]\n    boxTrue_width = y_true[0, 2]\n    boxTrue_height = y_true[0, 3]\n    area_boxTrue = (boxTrue_width * boxTrue_height)\n    # boxPred\n    x_boxPred_tleft = y_pred[0, 0]\n    y_boxPred_tleft = y_pred[0, 1]\n    boxPred_width = y_pred[0, 2]\n    boxPred_height = y_pred[0, 3]\n    area_boxPred = (boxPred_width * boxPred_height)\n    # calculate the bottom right coordinates for boxTrue and boxPred\n    # boxTrue\n    x_boxTrue_br = x_boxTrue_tleft + boxTrue_width\n    y_boxTrue_br = y_boxTrue_tleft + boxTrue_height  # Version 2 revision\n    # boxPred\n    x_boxPred_br = x_boxPred_tleft + boxPred_width\n    y_boxPred_br = y_boxPred_tleft + boxPred_height  # Version 2 revision\n    \n    # calculate the top left and bottom right coordinates for the intersection box, boxInt\n    \n    # boxInt - top left coords\n    x_boxInt_tleft = np.max([x_boxTrue_tleft, x_boxPred_tleft])\n    y_boxInt_tleft = np.max([y_boxTrue_tleft, y_boxPred_tleft])  # Version 2 revision\n    \n    # boxInt - bottom right coords\n    x_boxInt_br = np.min([x_boxTrue_br, x_boxPred_br])\n    y_boxInt_br = np.min([y_boxTrue_br, y_boxPred_br])\n    \n    # Calculate the area of boxInt, i.e. the area of the intersection\n    # between boxTrue and boxPred.\n    # The np.max() function forces the intersection area to 0 if the boxes don't overlap.\n    \n    \n    # Version 2 revision\n    area_of_intersection = \\\n      np.max([0, (x_boxInt_br - x_boxInt_tleft)]) * np.max([0, (y_boxInt_br - y_boxInt_tleft)])\n    \n    iou = area_of_intersection / ((area_boxTrue + area_boxPred) - area_of_intersection)\n    \n    # This must match the type used in py_func\n    iou = iou.astype(np.float32)\n    \n    # append the result to a list at the end of each loop\n    results.append(iou)\n  # return the mean IoU score for the batch\n  return np.mean(results)\n\n\ndef bbox_IoU(y_true, y_pred):\n  # print(K.eval(bbox_IoU(np.array([[1, 2, 32, 33]], dtype=np.float32), np.array([[1, 2, 32, 33]], dtype=np.float32))))\n  import tensorflow as tf\n  iou = tf.py_func(calculate_iou, [y_true, y_pred], tf.float32)\n  \n  return iou\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"08ece4975d0c90102fd8ff05db0ed70351f5d543"},"cell_type":"code","source":"def top_5(y_true, y_pred):\n    return top_k_categorical_accuracy(y_true, y_pred, k=5)\n\nmodel.compile(loss='mse', optimizer=Adam(lr=0.032),metrics=[bbox_IoU])\n\ncheckpoint = ModelCheckpoint('weights.h5',  # model filename\n                             monitor='val_bbox_IoU', # quantity to monitor\n                             verbose=1, # verbosity - 0 or 1\n                             save_best_only= True, # The latest best model will not be overwritten\n                             mode='max') # The decision to overwrite model is m\nearly_stopping = EarlyStopping(monitor='val_bbox_IoU',mode='max', patience=20, verbose=1)\nbacks=[checkpoint,early_stopping]\nhistory = model.fit_generator(training_generator, epochs=100, validation_data=validation_generator,callbacks=backs,verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"86cd29460233d36ca0669aad9b1a2ac7bafe74f0"},"cell_type":"code","source":"model.load_weights(\"weights.h5\")\noptimizer = Adam(lr=0.002) \nmodel.compile(loss='mse', optimizer=optimizer,metrics=[bbox_IoU])\ncheckpoint = ModelCheckpoint('weights.h5',  # model filename\n                             monitor='val_bbox_IoU', # quantity to monitor\n                             verbose=1, # verbosity - 0 or 1\n                             save_best_only= True, # The latest best model will not be overwritten\n                             mode='max') # The decision to overwrite model is m\nearly_stopping = EarlyStopping(monitor='val_bbox_IoU',mode='max', patience=30, verbose=1)\nbacks=[checkpoint,early_stopping]\nhistory = model.fit_generator(training_generator, epochs=100, validation_data=validation_generator,callbacks=backs,verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3d512f775d7636ef4e69c52237e3b3dc7c466139"},"cell_type":"code","source":"model.load_weights(\"weights.h5\")\nprint(model.evaluate_generator(validation_generator))  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c9ae1c617fade80f35d3dc6f9d0f8b7a36f11efb"},"cell_type":"code","source":"model2 = build_model(with_dropout=False)\nmodel2.load_weights(\"weights.h5\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fe5b3c6cacba0051bf64d47557b1aee8505ba141"},"cell_type":"code","source":"for layer in model2.layers:\n    if not isinstance(layer, BatchNormalization):\n        layer.trainable = False\nmodel2.compile(Adam(lr=0.002), loss='mean_squared_error',metrics=[bbox_IoU])\nmodel2.fit_generator(training_generator, epochs=1, verbose=1, validation_data=validation_generator)\nfor layer in model2.layers:\n    if not isinstance(layer, BatchNormalization):\n        layer.trainable = True\nmodel2.compile(Adam(lr=0.002), loss='mean_squared_error')\nmodel2.save('cropping.model')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d2cc5ca16c801dccd95334f6bd67022be672b566"},"cell_type":"code","source":"print(model2.evaluate_generator(validation_generator))  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"483bea830e61c5aafc8233cf3c26b17cd8f59dc5"},"cell_type":"markdown","source":"## predict"},{"metadata":{"trusted":true,"_uuid":"ec26ea7605b0a2ced168e061984497af1685dd6d"},"cell_type":"code","source":"y_=model2.predict_generator(validation_generator)\nprint(y_.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a6a7d0605fa1b6829182ade27086d024c243a391"},"cell_type":"code","source":"y_test =y_\nprint(y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1904b652881db29581338d3ac621cb9c6d2d73ef"},"cell_type":"code","source":"y_true=np.array([label for image,label in validation_generator])\ny_true= y_true.reshape((-1,4))\nprint(y_true.shape) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c73a92f07a33b452a43ff45709179dc302f919fb"},"cell_type":"code","source":"def intersection(a,b):\n    #求重合区左上角坐标\n    x=max(a[0],b[0])\n    y=max(a[1],b[1])\n    #求出重合区右下角坐标，再求出重合区的宽度和高度\n    w=min(a[2],b[2]) - x\n    h=min(a[3],b[3]) - y\n    if w<=0 or h<=0:\n        return 0\n    return w*h\n\ndef union(a,b,intersection_area):\n    area1=(a[2]-a[0]) * (a[3]-a[1])\n    area2=(b[2]-b[0]) * (b[3]-b[1])\n\n    return area1+area2-intersection_area\n\ndef iou(a,b):\n    if a[0] >= a[2] or a[1] >= a[3] or b[0] >= b[2] or b[1] >= b[3]:\n            return 0.0\n\n    intersection_area=intersection(a,b)\n    return float(intersection_area)/union(a,b,intersection_area)\nall_ratio=[]\nfrom tqdm import tqdm\nfor index in tqdm(range(y_true.shape[0])) :\n    all_ratio.append(iou(y_true[index],y_test[index]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0e2b7b74b9cdb7275643a6087d59818ae2665aa9"},"cell_type":"code","source":"print(\"mean IOU:\",np.mean(all_ratio))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8a85eef7ba43b6c5f384ffd3b7589855c6c45164"},"cell_type":"code","source":"from PIL import Image as pil_image\nfrom PIL.ImageDraw import Draw\ndef show_whale(imgs, per_row=5):\n    n         = len(imgs)\n    rows      = (n + per_row - 1)//per_row\n    cols      = min(per_row, n)\n    fig, axes = plt.subplots(rows,cols, figsize=(24//per_row*cols,24//per_row*rows))\n    for ax in axes.flatten(): \n        ax.axis('off')\n    for i,(img,ax) in enumerate(zip(imgs, axes.flatten())): \n        ax.imshow(img.convert('RGB'))\n\n\nfrom keras.preprocessing.image import array_to_img\nimages = []\nfor image,label in validation_generator:\n    for i in range(3):\n        a         = image[i:i+1]\n        rect1     = label[i]\n        rect2     = model2.predict(a).squeeze()\n        img       = array_to_img(a[0]).convert('RGB')\n        draw      = Draw(img)\n        draw.rectangle(list(rect1), outline='red')\n        draw.rectangle(rect2, outline='yellow')\n        images.append(img)\n    break\n\nshow_whale(images)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"103afb390f076d24f1553589c398d6330c354f8c"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0b7c9a310cd104c3c9f83d1397ea525bbab88bff"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9322e672932b011d27cddbeb717221a724b23feb"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}