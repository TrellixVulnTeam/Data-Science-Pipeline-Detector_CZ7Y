{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport timeit\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.\n\n# Load the training data set\ndata = pd.read_csv(\"../input/train.csv\")\n\n#== DATA EXPLORATION\ndta = data.copy()\ndta['freq'] = dta.groupby('place_id')['x'].transform('count')\n\nprint(dta[:5])"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# import relevant general libraries\nimport math\nimport itertools\n\n# import libraries for data visualization\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom plotly import tools\n\n# import libraries for classification algorithms\nfrom sklearn import tree\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.multiclass import OneVsOneClassifier\nfrom sklearn.decomposition import PCA\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.svm import LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\n    \n# create grid and split data points assign cell index to each data point\ndef create_grid(a, N, datapoints, start_index=1):\n    n = N*N\n    modVal = N - start_index + 1 # modval is used to combine the x and y values of grid into a unique cell index\n    \n    celldata = np.zeros((n,), dtype=[('gridcell_index', 'i4'), ('row', 'i4'),('col', 'i4')])\n    gridcell = pd.DataFrame(celldata)\n\n    cellValuesCombo = np.asarray(list(itertools.product(range(1, int(N)+1),range(1, int(N)+1))))\n    gridcell['row'] = cellValuesCombo[:,0]\n    gridcell['col'] = cellValuesCombo[:,1]\n    gridcell['gridcell_index'] = gridcell['row'] + (modVal*gridcell['col'])\n    gridcell['gridcell_index'] = gridcell['gridcell_index'].astype(int)\n    \n    datapoints['gridcell_x'] = (datapoints['x']*N/a) + 1\n    datapoints['gridcell_x'] = datapoints['gridcell_x'].astype(int)\n    datapoints['gridcell_y'] = (datapoints['y']*N/a) + 1\n    datapoints['gridcell_y'] = datapoints['gridcell_y'].astype(int)\n    datapoints['gridcell_index'] = datapoints['gridcell_x'] + (modVal*datapoints['gridcell_y'])\n    datapoints['gridcell_index'] = datapoints['gridcell_index'].astype(int)\n    datapoints = datapoints.drop(['gridcell_x', 'gridcell_y'], axis=1, inplace=True)\n    \n    return gridcell\n\"\"\"\n# filter data points by grid cell index and frequency\ndef filter_data(x, y, datapoints, datapoints_full, gridcell):\n    filtered_data_index = datapoints[datapoints['gridcell_index'] == gridcell.iloc[x,y]]\n    filtered_data = datapoints_full[datapoints_full['row_id'].isin(filtered_data_index['row_id'])].copy()\n    if('place_id' in filtered_data.columns):\n        filtered_data['freq_grid'] = filtered_data.groupby('place_id')['x'].transform('count')\n        filtered_data = filtered_data[filtered_data['freq_grid'] > 3].copy()\n    \n    return filtered_data\n\"\"\"\ndef filter_data(index, datapoints, place_frequency, enablePrints=False):\n    t0 = timeit.default_timer()\n    filtered_data = datapoints[datapoints['gridcell_index'] == index].copy()\n    if(enablePrints): print( \"filtered_data_index time interval: \", (timeit.default_timer()-t0) )\n    \n    \"\"\"\n    t0 = timeit.default_timer()\n    filtered_data = datapoints_full[datapoints_full['row_id'].isin(filtered_data_index['row_id'])].copy()\n    print( \"filtered_data time interval: \", (timeit.default_timer()-t0) )\n    \"\"\"\n    \n    if('place_id' in filtered_data.columns):\n        t0 = timeit.default_timer()\n        filtered_data['freq_grid'] = filtered_data.groupby('place_id')['x'].transform('count')\n        if(enablePrints): print( \"filtered_data-freq_grid time interval: \", (timeit.default_timer()-t0) )\n    \n        t0 = timeit.default_timer()\n        filtered_data_used = filtered_data[filtered_data['freq_grid'] > place_frequency].copy()\n        if(enablePrints): print( \"filtered_data-filtered_data time interval: \", (timeit.default_timer()-t0) )\n    \n        t0 = timeit.default_timer()\n        filtered_data_unused = filtered_data[filtered_data['freq_grid'] <= place_frequency].copy()\n        if(enablePrints): print( \"filtered_data-filtered_data time interval: \", (timeit.default_timer()-t0) )\n    \n    return filtered_data_used, filtered_data_unused\n\n# split time value into smaller group sets\ndef split_time(datapoints):\n    datapoints['hour'] = (datapoints['time'] / 60) % 24\n    datapoints['hour'] = datapoints['hour'].astype(int)\n    datapoints['weekday'] = (datapoints['time'] / (60*24)) % 7\n    datapoints['weekday'] = datapoints['weekday'].astype(int)\n    datapoints['month'] = (datapoints['time'] / (60*24*30)) % 12\n    datapoints['month'] = datapoints['month'].astype(int)\n    datapoints['year'] = datapoints['time'] / (60*24*365)\n    datapoints['year'] = datapoints['year'].astype(int)\n    datapoints['day'] = (datapoints['time'] / (60*24)) % 365\n    datapoints['day'] = datapoints['day'].astype(int)\n\n# visualize data\ndef init_visualization_params(plot3d=False, size=1):\n    fig = None\n    axarr = None\n    halfSize = 0\n    \n    if plot3d:\n        fig = plt.figure(figsize=(16,10))\n    else:\n        halfSize = int(size / 2)\n        fig, axarr = plt.subplots(halfSize, halfSize) if (halfSize>1) else plt.subplots(1, 1)\n        fig.tight_layout()\n        \n    return fig, axarr, halfSize\n    \ndef visualize_data(datapoints, count, figure, axesInfo, plot3d=False, halfSize=0):\n    #-- plotting x against y with colors for each place\n    if plot3d:\n        projCount = 221 + count\n        ax = figure.add_subplot(projCount, projection='3d')\n        ax.scatter(datapoints['x'], datapoints['y'], datapoints['hour'], c=datapoints['place_id'], linewidth=0.0)\n    else:\n        datapoints_sub = datapoints[['x', 'y']].copy()\n\n        #-- reduce dimensionality of features\n        x_and_y = PCA(n_components=1).fit_transform(datapoints_sub)\n        datapoints['x_and_y'] = x_and_y\n        #tu_std = StandardScaler().fit_transform(tu)\n        #x_and_y_and_time = PCA(n_components=2).fit_transform(tu_std)\n\n        if (halfSize>0):\n            xIndex = (count % halfSize) #remainder\n            yIndex = (count / halfSize) #quotient\n            axesInfo[xIndex, yIndex].scatter(datapoints['x_and_y'], datapoints['time'], c=datapoints['place_id'], linewidth=0.0)#\n        else:\n            axesInfo.scatter(datapoints['x_and_y'], datapoints['hour'], c=datapoints['place_id'], linewidth=0.0)#\n        \n    plt.show()\n\ndef train_model(datapoints, datapoints_unused, model_type, valOnFull=False, enablePrints=False):\n    places = datapoints['place_id'].copy()\n    #features = datapoints[['x', 'y', 'accuracy', 'hour', 'day']].copy()\n    #features = datapoints[['x', 'y', 'hour']].copy()\n\n    features = datapoints[['x', 'y', 'hour', 'accuracy', 'day']].copy()\n    \"\"\"\n    features['x_squared'] = features['x']**2\n    features['y_squared'] = features['y']**2\n    features['hour_squared'] = features['hour']**2\n    \"\"\"\n\n    #\"\"\"\n    #features['accuracy'] = datapoints['accuracy']\n    #features['y_cubed'] = features['y']**3\n    #features['hour_cubed'] = features['hour']**3\n    #\"\"\"\n\n    X = features\n    y = places\n    X_unused = datapoints_unused[['x', 'y', 'hour', 'accuracy', 'day']].copy()\n    y_unused = datapoints_unused['place_id'].copy()\n\n    #split sample data into test and training sets\n    X_train, X_test, y_train, y_test = train_test_split(features, places, test_size=0.2, random_state=50)\n    \n    X_test_full = X_test.append(X_unused)\n    y_test_full = y_test.append(y_unused)\n\n    if(enablePrints): print( \"X_train Length: \", len(X_train) )\n    if(enablePrints): print( \"y_train Length: \", len(y_train) )\n    if(enablePrints): print( \"X_test Length: \", len(X_test) )\n    if(enablePrints): print( \"y_test Length: \", len(y_test) )\n    if(enablePrints): print( \"X_full Length: \", len(X_unused) )\n    if(enablePrints): print( \"y_full Length: \", len(y_unused) )\n\n    classifier_model = None\n    score_model = 0\n    useLogReg = True if (model_type == \"Logistic Regression\") else False\n    useDTClf = True if (model_type == \"Decision Tree\") else False\n    useRFClf = True if (model_type == \"Random Forest\") else False\n    if useLogReg:\n        logReg = LogisticRegression()\n        logReg.fit(X_train, y_train)\n        score_model = logReg.score(X_test_full, y_test_full) if (valOnFull) else logReg.score(X_test, y_test)\n        if(enablePrints): print (\"\\n\\nlog reg score: %.3f\", score_model)\n        \n        \"\"\"\n        OVR = OneVsRestClassifier(LogisticRegression()).fit(X_train, y_train)\n        if( displayPrint ):\n            print (\"OVR accuracy score: %.3f\", OVR.score(X_test, y_test))\n        \n        #X = features[:500]\n        #y = places[:500]\n        OVO = OneVsOneClassifier(LogisticRegression()).fit(X_train, y_train)\n        if( displayPrint ):\n            print (\"OVO accuracy score: %.3f\", OVO.score(X_test, y_test))\n        \"\"\"\n        \n        classifier_model = logReg\n        \n    elif useDTClf:\n        dtClf_model = tree.DecisionTreeClassifier()\n        dtClf_model.fit(X_train, y_train)\n        score_model = dtClf_model.score(X_test_full, y_test_full) if (valOnFull) else dtClf_model.score(X_test, y_test)\n        if(enablePrints): print (\"\\n\\ndecision tree classifier score: %.3f\", score_model)\n        \n        classifier_model = dtClf_model\n        \n    elif useRFClf:\n        rfClf_model = RandomForestClassifier(n_estimators=1000, # Number of trees\n                                             max_features=2,    # Num features considered\n                                             oob_score=True)    # Use OOB scoring*\n        rfClf_model.fit(X_train, y_train)\n        score_model = rfClf_model.score(X_test_full, y_test_full) if (valOnFull) else rfClf_model.score(X_test, y_test)\n        #score_model = rfClf_model.oob_score_\n        if(enablePrints): print (\"\\n\\nrandom forest classifier score: %.3f\", score_model)\n        if(enablePrints): print (\"\\n\\nrandom forest classifier oob score: %.3f\", rfClf_model.oob_score_)\n        \n        classifier_model = rfClf_model\n    \n    if(enablePrints): print (\"\\n\\n score on test val: %.3f\", classifier_model.score(X_test, y_test))\n    if(enablePrints): print (\"\\n\\n score on test_full val: %.3f\", classifier_model.score(X_test_full, y_test_full))\n    return classifier_model, score_model\n\ndef filter_gridcells(gridcell, gridcell_density, gridcell_distribution, enablePrints=False):\n    if('gc_density' in gridcell.columns):\n        t0 = timeit.default_timer()\n        gridcell_used = gridcell[gridcell['gc_density'] > gridcell_density].copy()\n        if(enablePrints): print( \"gc_density-gridcell_used time interval: \", (timeit.default_timer()-t0) )\n    \n        t0 = timeit.default_timer()\n        gridcell_unused = gridcell[gridcell['gc_density'] <= gridcell_density].copy()\n        if(enablePrints): print( \"gc_density-gridcell_unused time interval: \", (timeit.default_timer()-t0) )\n\t\t\n        if('gc_dist' in gridcell.columns):\n            t0 = timeit.default_timer()\n            gridcell_used_update = gridcell_used[gridcell_used['gc_dist'] > gridcell_distribution].copy()\n            if(enablePrints): print( \"gc_dist-gridcell_used time interval: \", (timeit.default_timer()-t0) )\n    \n            t0 = timeit.default_timer()\n            gridcell_unused_update = gridcell_used[gridcell_used['gc_dist'] <= gridcell_distribution].copy()\n            if(enablePrints): print( \"gc_dist-gridcell_unused time interval: \", (timeit.default_timer()-t0) )\n\t\t\n            gridcell_unused_update = gridcell_unused.append(gridcell_unused_update)\n    \n    return gridcell_used_update, gridcell_unused_update\n            \n\"\"\"\nda = dta[['row_id', 'x', 'y']].copy()\ngCell = create_grid(a=10, N=100, datapoints=da)\n#fData = filter_data(x=0, y=0, datapoints=da, datapoints_full=dta, gridcell=gCell)\nfData = filter_data(index=101, datapoints=da, datapoints_full=dta, gridcell=gCell)\nsplit_time(fData)\n\nprint()\nprint( fData[:5] )\n\n# DATA VISUALIZATION\nf, axInfo, half = init_visualization_params(plot3d=False, size=1)\nvisualize_data(datapoints=fData, count=0, plot3d=False, halfSize=half, figure=f, axesInfo=axInfo)\n\n# ALGORITHM\nclf_model = train_model(datapoints=fData, model_type=\"Logisitic Regression\")\n\"\"\""},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"print( dta[:5] )\n\nt0 = timeit.default_timer()\ngCell = create_grid(a=10, N=100, datapoints=dta)\nprint( \"time interval: \", (timeit.default_timer()-t0) )\n\nprint( dta[:5] )\nprint( \"dta length: \", len(dta) )"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"t0 = timeit.default_timer()\ndta['gc_density'] = dta.groupby('gridcell_index')['x'].transform('count')\nprint( \"gc_density time interval: \", (timeit.default_timer()-t0) )\n\nt0 = timeit.default_timer()\ndta['gc_dist'] = dta.groupby('gridcell_index')['place_id'].transform('nunique')\nprint( \"gc_dist time interval: \", (timeit.default_timer()-t0) )\n\nprint( dta[:5] )"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"print( gCell[:5] )\n\nt0 = timeit.default_timer()\ndta_pre_merge = dta[['gridcell_index', 'gc_density', 'gc_dist']].copy()\nprint( \"dta_pre_merge time interval: \", (timeit.default_timer()-t0) )\n\nt0 = timeit.default_timer()\ngCell = gCell.merge(dta_pre_merge, on='gridcell_index')\n#dta_pre_merge = dta.drop(['gridcell_x', 'gridcell_y'], axis=1, inplace=True)\nprint( \"gCell merge time interval: \", (timeit.default_timer()-t0) )\n\nprint( gCell[:5] )"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":""}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":0}