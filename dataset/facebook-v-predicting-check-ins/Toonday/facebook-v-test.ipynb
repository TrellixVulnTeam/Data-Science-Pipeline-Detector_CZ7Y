{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport timeit\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.\n\n# Load the training data set\ndata = pd.read_csv(\"../input/train.csv\")\nplaces = data['place_id']\nfeatures = data.drop(['place_id', 'row_id'], axis = 1)\n\n#== DATA EXPLORATION\ndta = data.copy()\ndta['freq'] = dta.groupby('place_id')['x'].transform('count')\n\nprint(dta[:5])"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"print( \"row_id\" )\n%timeit dta[dta['row_id'] == 0]"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"\nda = dta[['row_id', 'x', 'y']].copy()\n\n# DATA VISUALIZATION\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom plotly import tools\n\nplot3d = False\nif plot3d:\n    fig = plt.figure(figsize=(16,10))\nelse:\n    halfSize = int(len(range(100, 140, 50)) / 2)\n    f, axarr = plt.subplots(halfSize, halfSize) if (halfSize>1) else plt.subplots(1, 1)\n    f.tight_layout()\n\nimport math\nimport itertools\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.multiclass import OneVsOneClassifier\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn import tree\nfrom sklearn import preprocessing\nfrom sklearn.svm import LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\n\niCount = 0\nfor i in range(100, 140, 50):\n    print(\"i = \", i)\n    \n    # create grid\n    start_index = 1\n    a = 10\n    N = i#math.sqrt(n)\n    n = N*N#10000\n    modVal = N - start_index + 1\n    celldata = np.zeros((n,), dtype=[('index', 'i4'), ('row', 'i4'),('col', 'i4')])\n    gridcell = pd.DataFrame(celldata)\n\n    print( \"N: \",N )\n\n    cellValuesCombo = np.asarray(list(itertools.product(range(1, int(N)+1),range(1, int(N)+1))))\n    gridcell['row'] = cellValuesCombo[:,0]\n    gridcell['col'] = cellValuesCombo[:,1]\n    gridcell['index'] = gridcell['row'] + (modVal*gridcell['col'])\n    gridcell['index'] = gridcell['index'].astype(int)\n    \n    da['gridcell_x'] = (da['x']*N/a) + 1\n    da['gridcell_x'] = da['gridcell_x'].astype(int)\n    da['gridcell_y'] = (da['y']*N/a) + 1\n    da['gridcell_y'] = da['gridcell_y'].astype(int)\n    da['gridcell_index'] = da['gridcell_x'] + (modVal*da['gridcell_y'])\n    da['gridcell_index'] = da['gridcell_index'].astype(int)\n    da = da.drop(['gridcell_x', 'gridcell_y'], axis = 1)\n    \n    xy = da[da['gridcell_index'] == gridcell.iloc[0,0]]\n    vw = dta[dta['row_id'].isin(xy['row_id'])].copy()\n    vw['freq_grid'] = vw.groupby('place_id')['x'].transform('count')\n    vw = vw[vw['freq_grid'] > 3].copy()\n    \n    print( \"vw.shape: \",vw.shape )\n    print( \"vw unique places: \",len(vw['place_id'].unique()) )\n    print()\n    \n    vw['hour'] = (vw['time'] / 60) % 24\n    vw['hour'] = vw['hour'].astype(int)\n    vw['weekday'] = (vw['time'] / (60*24)) % 7\n    vw['weekday'] = vw['weekday'].astype(int)\n    vw['month'] = (vw['time'] / (60*24*30)) % 12\n    vw['month'] = vw['month'].astype(int)\n    vw['year'] = vw['time'] / (60*24*365)\n    vw['year'] = vw['year'].astype(int)\n    vw['day'] = (vw['time'] / (60*24)) % 365\n    vw['day'] = vw['day'].astype(int)\n    \n    \"\"\"\n    xy_sub = xy[['x', 'y', 'place_id', 'time', 'accuracy']]\n    vw = xy[['x', 'y']]\n    tu = xy[['x', 'y', 'time']]\n    print( \"xy: \",xy.shape )\n    \"\"\"\n\n\n    # DATA VISUALIZATION\n    #-- plotting x against y with colors for each place\n    if plot3d:\n        projCount = 221 + iCount\n        ax = fig.add_subplot(projCount, projection='3d')\n        ax.scatter(vw['x'], vw['y'], vw['hour'], c=vw['place_id'], linewidth=0.0)\n    else:\n        vw_sub = vw[['x', 'y']]\n\n        #-- reduce dimensionality of features\n        x_and_y = PCA(n_components=1).fit_transform(vw_sub)\n        vw['x_and_y'] = x_and_y\n        #tu_std = StandardScaler().fit_transform(tu)\n        #x_and_y_and_time = PCA(n_components=2).fit_transform(tu_std)\n    \n        if (halfSize>0) :\n            temp_x = (iCount % halfSize) #remainder\n            temp_y = (iCount / halfSize) #quotient\n            axarr[temp_x, temp_y].scatter(vw['x_and_y'], vw['time'], c=vw['place_id'], linewidth=0.0)#\n        else:\n            axarr.scatter(vw['x_and_y'], vw['hour'], c=vw['place_id'], linewidth=0.0)#\n        \n    iCount = iCount+1\n    \n    \n    # ALGORITHM\n    \n    places = vw['place_id'].copy()\n    #features = vw[['x', 'y', 'accuracy', 'hour', 'day']].copy()\n    #features = vw[['x', 'y', 'hour']].copy()\n    \n    #\"\"\"\n    features = vw[['x', 'y', 'hour']].copy()\n    features['x_squared'] = features['x']**2\n    features['y_squared'] = features['y']**2\n    features['hour_squared'] = features['hour']**2\n    #\"\"\"\n    \n    #\"\"\"\n    #features['accuracy'] = vw['accuracy']\n    #features['y_cubed'] = features['y']**3\n    #features['hour_cubed'] = features['hour']**3\n    #\"\"\"\n\n    \n    X = features\n    y = places\n    \n    X_train, X_test, y_train, y_test = train_test_split(features, places, test_size=0.2, random_state=50)\n\n    print( X_train[:5] )\n    print()\n    \n    print( X_test[:5] )\n    print()\n    \n    useLogReg = False\n    useDTClf = False\n    useRFClf = True\n    if useLogReg:\n        #\"\"\"\n        logReg = LogisticRegression()\n        logReg.fit(X_train, y_train)\n        print (\"\\n\\nlog reg score: %.3f\", logReg.score(X_test, y_test))\n        #\"\"\"\n    \n        #\"\"\"\n        OVR = OneVsRestClassifier(LogisticRegression()).fit(X_train, y_train)\n        print (\"OVR accuracy score: %.3f\", OVR.score(X_test, y_test))\n        #\"\"\"\n    \n        #\"\"\"\n        #X = features[:500]\n        #y = places[:500]\n        OVO = OneVsOneClassifier(LogisticRegression()).fit(X_train, y_train)\n        print (\"OVO accuracy score: %.3f\", OVO.score(X_test, y_test))\n        #\"\"\"\n    elif useDTClf:\n        #\"\"\"\n        dtClf_model = tree.DecisionTreeClassifier()\n        dtClf_model.fit(X_train, y_train)\n        print (\"\\n\\ndecision tree classifier score: %.3f\", dtClf_model.score(X_test, y_test))\n        #\"\"\"\n    elif useRFClf:\n        #\"\"\"\n        rfClf_model = RandomForestClassifier(n_estimators=1000, # Number of trees\n                                             max_features=2,    # Num features considered\n                                             oob_score=True)    # Use OOB scoring*\n        rfClf_model.fit(X_train, y_train)\n        print (\"\\n\\nrandom forest classifier score: %.3f\", rfClf_model.score(X_test, y_test))\n        print (\"\\n\\nrandom forest classifier oob score: %.3f\", rfClf_model.oob_score_)\n        #\"\"\"\n\nplt.show()"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":""},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"\nprint( dta[dta['row_id'] == 0])\n"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":0}