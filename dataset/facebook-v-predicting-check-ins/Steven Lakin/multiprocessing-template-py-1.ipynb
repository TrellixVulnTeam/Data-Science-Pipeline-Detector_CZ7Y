{"cells":[{"cell_type":"markdown","metadata":{},"source":"The cost incurred by using Python's multiprocessing module isn't always worth it, but for large test sets, this can improve the time from implementing a new idea to testing.  Here is a template that I've used in other applications, modified for this Kaggle competition.\n\nInput is stdin, output is stdout, messages to stderr.  Example for the test file with one header line running on 4 processes:\n\nzcat test.csv.gz | multiprocessing\\_template.py -s 1 -n 4\n\nThe user can set the chunksize (# of lines to read per block) to limit memory consumption if necessary.\n\nThe output looks like this:\n\n    row_id,place_id\n    0,first_prediction second_prediction third_prediction\n    1,first_prediction second_prediction third_prediction\n    2,first_prediction second_prediction third_prediction\n    333,first_prediction second_prediction third_prediction\n    3,first_prediction second_prediction third_prediction\n    334,first_prediction second_prediction third_prediction\n    4,first_prediction second_prediction third_prediction\n    335,first_prediction second_prediction third_prediction\n    5,first_prediction second_prediction third_prediction\n    ...\n"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"#!/usr/bin/env python3\n\n# Author: Steven Lakin\n# Kaggle Facebook V Challenge, May 2016\n\n#############\n## Imports ##\n#############\nimport argparse\nimport multiprocessing as mp\nimport sys\nimport logging\nimport resource\n\n\n##########\n## Vars ##\n##########\nchunksize = 20000000  # limit memory consumption by reading in blocks\noverall = 0  # counter for stderr writing\n\n\n###############\n## Functions ##\n###############\ndef input_parse():\n    \"\"\" Parses an input file.\n    This script only accepts stdin, so use cat file | script.py for correct functionality.\n    :return: generator yielding split observations\n    \"\"\"\n    for x in range(chunksize):\n        line = sys.stdin.readline()\n        if not line:\n            return  # stop iteration\n        yield line.rstrip().split(',')\n\n\ndef worker(chunk):\n    \"\"\" This code block is executed many times across the data block (each worker receives a chunk of that block).\n    The predictions are written to the logging cache (this is because writing to stdout produces thrashing).\n    The logging cache is then flushed on every iteration of the outer loop.\n    :param chunk: a chunk of reads divided amongst the pool of parallel workers\n    :return: void\n    \"\"\"\n    global my_global_object\n    for checkin in chunk:\n        # Do something\n        row_id = checkin[0]\n        p1 = 'first_prediction'\n        p2 = 'second_prediction'\n        p3 = 'third_prediction'\n        logging.info('{},{} {} {}'.format(row_id, p1, p2, p3))  # Output format\n\n\ndef split(a, n):\n    \"\"\" Splits an input list into n equal chunks; this works even if modulo > 0.\n    :param a: list of arbitrary length\n    :param n: number of groups to split into\n    :return: generator of chunks\n    \"\"\"\n    k, m = int(len(a) / n), len(a) % n\n    return (a[i * k + min(i, m):(i + 1) * k + min(i + 1, m)] for i in range(n))\n\n\ndef current_mem_usage():\n    \"\"\"\n    :return: current memory usage in MB\n    \"\"\"\n    return resource.getrusage(resource.RUSAGE_SELF).ru_maxrss / 1024\n\n\n\n##############\n## ArgParse ##\n##############\nparser = argparse.ArgumentParser('cat test_file.csv | multiprocessing_template.py')\nparser.add_argument('-n', '--num_process', type=int, default=1, help='Number of processes to run in parallel')\nparser.add_argument('-s', '--skip_lines', type=int, default=0, help='Number of header lines to skip in input file')\n\n\n##########\n## Main ##\n##########\nif __name__ == '__main__':\n    mp.freeze_support()\n    ## Parse the arguments using ArgParse\n    args = parser.parse_args()\n\n    ## Input must be on stdin; raise error if this is not the case\n    if sys.stdin.isatty():\n        raise IOError('Input must be on stdin.  Use stream redirect for correct functionality: cat file | script.py')\n    else:\n        for skip in range(args.skip_lines):\n            _ = sys.stdin.readline()  # Throw out header lines\n\n    ## Setup the logger for output of predictions to stdout.  This is necessary because writing directly to stdout\n    ## in parallel causes thrashing and variable results.  The logger caches observations passed to it on every loop\n    ## and flushes to stdout after the observations have been processed.\n    root = logging.getLogger()\n    root.setLevel(logging.DEBUG)\n    handler = logging.StreamHandler(stream=sys.stdout)\n    handler.setLevel(logging.DEBUG)\n    root.addHandler(handler)\n\n    logging.info('row_id,place_id')\n\n    ## Read in each file chunk, process, and output predictions.  Chunk size should be set such that\n    ## the block size doesn't overflow memory.\n    pool = mp.Pool(processes=args.num_process)  # create pool of workers for parallel processing\n    while True:\n        chunks = [z for z in split([x for x in input_parse()], args.num_process)]  # divide reads into chunks\n        sys.stderr.write('\\nMemory used: {}MB'.format(current_mem_usage()))\n        check = sum([len(x) for x in chunks])  # this is the break condition for the while loop (count of lines)\n        overall += check  # add to overall read count for reporting to stderr\n        sys.stderr.write('\\nTotal observations processed {}, screening...'.format(overall))\n        if check is 0:\n            pool.close()\n            pool.join()\n            pool.terminate()\n            del pool\n            break\n        res = pool.map(worker, chunks)  # All workers must finish before proceeding.\n        handler.flush()  # flush the logging cache to stdout\n\n        del chunks  # remove chunks from memory.  Otherwise memory usage will be doubled.\n        if check < chunksize:\n            pool.close()  # ask nicely\n            pool.join()  # sigterm\n            pool.terminate()  # sigkill\n            del pool  # make sure pool is cleared\n            break\n        del check\n        del res\n        sys.stderr.write('\\nFinished block.  Loading next chunk...\\n')\n    sys.stderr.write('\\nTotal observations processed {}'.format(overall))"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":0}