{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Libraries"},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"!pip install pydot graphviz","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nimport time\nfrom tqdm import tqdm\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.layers import Conv2D, Flatten, ReLU, BatchNormalization, Conv2DTranspose, Dense\n\nfrom IPython.display import clear_output\nfrom kaggle_datasets import KaggleDatasets\n# import matplotlib.animation as animation\n\nAUTOTUNE = tf.data.AUTOTUNE","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n    INPUT_PATH = KaggleDatasets().get_gcs_path()\nelse:\n    strategy = tf.distribute.get_strategy()\n    INPUT_PATH = \"../input/gan-getting-started\"\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)\nprint(\"Input Path:\", INPUT_PATH)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Dataset"},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"monet_tfrec_files = tf.io.gfile.glob(INPUT_PATH+\"/monet_tfrec/*.tfrec\")\nphoto_tfrec_files = tf.io.gfile.glob(INPUT_PATH+\"/photo_tfrec/*.tfrec\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"feature_description = {\n    'image': tf.io.FixedLenFeature([], tf.string),\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"def parse_tfrecord(record):\n    features = tf.io.parse_single_example(record, feature_description)\n    \n    image = features['image']\n    image = tf.io.decode_image(image)\n    image = tf.reshape(image, (256, 256, 3))\n    \n    return image","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"def normalize(image):\n    image = tf.cast(image, tf.float32)\n    image = (image / 127.5) - 1.0\n    \n    return image","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"def random_jitter(image):\n    image = tf.image.resize(image, [286, 286], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n    image = tf.image.random_crop(image, size=[256,256, 3])\n    image = tf.image.random_flip_left_right(image)\n    image = tf.image.random_saturation(image, 0.7, 1.2)\n    return image","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"BATCH_SIZE = 32\nBUFFER_SIZE = 1000\nprint(\"Batch Size:\", BATCH_SIZE)\nprint(\"Buffer Size:\", BUFFER_SIZE)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"!rm -r /kaggle/tmp\n!mkdir /kaggle/tmp\n!mkdir /kaggle/tmp/monet\n!mkdir /kaggle/tmp/photo\n!ls /kaggle/tmp","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" ## Monet Dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"monet_dataset = tf.data.TFRecordDataset(monet_tfrec_files)\nmonet_dataset = monet_dataset.map(parse_tfrecord, num_parallel_calls=AUTOTUNE)\n# monet_dataset = monet_dataset.cache(\"/kaggle/tmp/monet\")\nmonet_dataset = monet_dataset.map(random_jitter, num_parallel_calls=AUTOTUNE)\nmonet_dataset = monet_dataset.map(normalize, num_parallel_calls=AUTOTUNE)\nmonet_dataset = monet_dataset.repeat()\nmonet_dataset = monet_dataset.shuffle(BUFFER_SIZE)\nmonet_dataset = monet_dataset.batch(BATCH_SIZE)\nmonet_dataset = monet_dataset.prefetch(AUTOTUNE)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plt.figure(figsize=(20,10))\nfor images in monet_dataset.take(1):\n    for i in range(len(images)):\n        plt.subplot(4,8, i+1)\n        plt.imshow((images[i]+1)/2)\n        plt.xticks([])\n        plt.yticks([])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Photo Dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"photo_dataset = tf.data.TFRecordDataset(photo_tfrec_files)\nphoto_dataset = photo_dataset.map(parse_tfrecord, num_parallel_calls=AUTOTUNE)\n# photo_dataset = photo_dataset.cache(\"/kaggle/tmp/photo\")\nphoto_dataset = photo_dataset.map(random_jitter, num_parallel_calls=AUTOTUNE)\nphoto_dataset = photo_dataset.map(normalize, num_parallel_calls=AUTOTUNE)\nphoto_dataset = photo_dataset.shuffle(BUFFER_SIZE)\nphoto_dataset = photo_dataset.batch(BATCH_SIZE)\nphoto_dataset = photo_dataset.prefetch(AUTOTUNE)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plt.figure(figsize=(20,10))\nfor images in photo_dataset.take(1):\n    for i in range(len(images)):\n        plt.subplot(4,8, i+1)\n        plt.imshow((images[i]+1)/2)\n        plt.xticks([])\n        plt.yticks([])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Models"},{"metadata":{},"cell_type":"markdown","source":"## Definitions"},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"def downsample(filters, size, apply_batchnorm=True):\n    initializer = tf.random_normal_initializer(0., 0.02)\n    \n    result = tf.keras.Sequential()\n    result.add(tf.keras.layers.Conv2D(filters, size, strides=2, padding='same',kernel_initializer=initializer, use_bias=False))\n    \n    if apply_batchnorm:\n        result.add(tf.keras.layers.BatchNormalization())\n\n    result.add(tf.keras.layers.LeakyReLU())\n    return result","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"def upsample(filters, size, apply_dropout=False):\n    initializer = tf.random_normal_initializer(0., 0.02)\n    \n    result = tf.keras.Sequential()\n    result.add(\n        tf.keras.layers.Conv2DTranspose(filters, size, strides=2,\n                                    padding='same',\n                                    kernel_initializer=initializer,\n                                    use_bias=False))\n    \n    result.add(tf.keras.layers.BatchNormalization())\n    \n    if apply_dropout:\n        result.add(tf.keras.layers.Dropout(0.5))\n        \n    result.add(tf.keras.layers.ReLU())\n    \n    return result","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"def Discriminator():\n    initializer = tf.random_normal_initializer(0., 0.02)\n    \n    inp = tf.keras.layers.Input(shape=[256, 256, 3], name='input_image')\n    \n    down1 = downsample(64, 4, False)(inp) # (bs, 128, 128, 64)\n    down2 = downsample(128, 4)(down1) # (bs, 64, 64, 128)\n    down3 = downsample(256, 4)(down2) # (bs, 32, 32, 256)\n    \n    zero_pad1 = tf.keras.layers.ZeroPadding2D()(down3) # (bs, 34, 34, 256)\n    conv = tf.keras.layers.Conv2D(512, 4, strides=1,\n                                  kernel_initializer=initializer,\n                                  use_bias=False)(zero_pad1) # (bs, 31, 31, 512)\n    \n    batchnorm1 = tf.keras.layers.BatchNormalization()(conv)\n    \n    leaky_relu = tf.keras.layers.LeakyReLU()(batchnorm1)\n    \n    zero_pad2 = tf.keras.layers.ZeroPadding2D()(leaky_relu) # (bs, 33, 33, 512)\n    \n    last = tf.keras.layers.Conv2D(1, 4, strides=1,\n                                  kernel_initializer=initializer)(zero_pad2) # (bs, 30, 30, 1)\n    \n    return tf.keras.Model(inputs=[inp], outputs=last)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"def Generator():\n    inputs = tf.keras.layers.Input(shape=[256,256,3])\n    \n    down_stack = [\n        downsample(64, 4, apply_batchnorm=False), # (bs, 128, 128, 64)\n        downsample(128, 4), # (bs, 64, 64, 128)\n        downsample(256, 4), # (bs, 32, 32, 256)\n        downsample(512, 4), # (bs, 16, 16, 512)\n        downsample(512, 4), # (bs, 8, 8, 512)\n        downsample(512, 4), # (bs, 4, 4, 512)\n        downsample(512, 4), # (bs, 2, 2, 512)\n        downsample(512, 4), # (bs, 1, 1, 512)\n    ]\n    \n    up_stack = [\n        upsample(512, 4, apply_dropout=True), # (bs, 2, 2, 1024)\n        upsample(512, 4, apply_dropout=True), # (bs, 4, 4, 1024)\n        upsample(512, 4, apply_dropout=True), # (bs, 8, 8, 1024)\n        upsample(512, 4), # (bs, 16, 16, 1024)\n        upsample(256, 4), # (bs, 32, 32, 512)\n        upsample(128, 4), # (bs, 64, 64, 256)\n        upsample(64, 4), # (bs, 128, 128, 128)\n    ]\n    \n    initializer = tf.random_normal_initializer(0., 0.02)\n    last = tf.keras.layers.Conv2DTranspose(3, 4,\n                                         strides=2,\n                                         padding='same',\n                                         kernel_initializer=initializer,\n                                         activation='tanh') # (bs, 256, 256, 3)\n    \n    x = inputs\n    \n    # Downsampling through the model\n    skips = []\n    for down in down_stack:\n        x = down(x)\n        skips.append(x)\n        \n    skips = reversed(skips[:-1])\n    \n    # Upsampling and establishing the skip connections\n    for up, skip in zip(up_stack, skips):\n        x = up(x)\n        x = tf.keras.layers.Concatenate()([x, skip])\n        \n    x = last(x)\n    \n    return tf.keras.Model(inputs=inputs, outputs=x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with strategy.scope():\n    # Instantiate generators\n    G_PtoM = Generator()\n    G_MtoP = Generator()\n    # Instantiate discriminators\n    D_P = Discriminator()\n    D_M = Discriminator()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Downsample"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"tf.keras.utils.plot_model(G_PtoM.layers[1], dpi=64, to_file=\"downsample.png\", show_layer_names=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Upsample"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"tf.keras.utils.plot_model(G_PtoM.layers[-3], dpi=64, to_file=\"upsample.png\", show_layer_names=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Generator"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"tf.keras.utils.plot_model(G_PtoM, show_shapes=True, dpi=64, to_file=\"generator.png\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Discriminator"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"tf.keras.utils.plot_model(D_P, show_shapes=True, dpi=64, to_file=\"discriminator.png\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Losses"},{"metadata":{"trusted":true},"cell_type":"code","source":"LAMBDA = 10","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with strategy.scope():\n    loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.SUM)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def discriminator_loss(real, generated):\n    real_loss = loss_object(tf.ones_like(real), real)\n    generated_loss = loss_object(tf.zeros_like(generated), generated)\n    total_disc_loss = real_loss + generated_loss\n    total_disc_loss /= len(real)\n    \n    return total_disc_loss * 0.5","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def generator_loss(generated):\n    return loss_object(tf.ones_like(generated), generated)/len(generated)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def calc_cycle_loss(real_image, cycled_image): \n        return LAMBDA * tf.reduce_mean(tf.abs(real_image - cycled_image))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def identity_loss(real_image, same_image):\n    return LAMBDA * 0.5 * tf.reduce_mean(tf.abs(real_image - same_image))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Optimizers"},{"metadata":{"trusted":true},"cell_type":"code","source":"with strategy.scope():\n    G_MtoP_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n    G_PtoM_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n\n    D_M_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n    D_P_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def generate_images(model, model_r, test_input, figsize=(12,12)):\n    prediction = model(test_input)\n    reconstruction = model_r(prediction)\n    identity = model_r(test_input)\n    \n    display_list = [test_input[0], prediction[0], reconstruction[0], identity[0]]\n    plt.figure(figsize=figsize)\n    title = ['Input', 'Predicted', 'Reconstructed', 'Identity']\n\n    for i in range(4):\n        plt.subplot(1, 4, i+1)\n        plt.title(title[i])\n        # getting the pixel values between [0, 1] to plot it.\n        plt.imshow(display_list[i] * 0.5 + 0.5)\n        plt.axis('off')\n\n    plt.show()\n    \n    return display_list","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Performance Before Training"},{"metadata":{"trusted":true},"cell_type":"code","source":"for images in monet_dataset.take(1):\n    generate_images(G_MtoP, G_PtoM, images)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for images in photo_dataset.take(1):\n    generate_images(G_PtoM, G_MtoP, images)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training"},{"metadata":{"trusted":true},"cell_type":"code","source":"EPOCHS = 100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with strategy.scope():\n    G_MtoP_loss = tf.keras.metrics.Mean(name='G_MtoP_loss')\n    G_PtoM_loss = tf.keras.metrics.Mean(name='G_PtoM_loss')\n    D_M_loss = tf.keras.metrics.Mean(name='D_M_loss')\n    D_P_loss = tf.keras.metrics.Mean(name='D_P_loss')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_step(real_m, real_p):\n    \n    with tf.GradientTape(persistent=True) as tape:\n        \n        # G_PtoM translates P -> M\n        # G_MtoP translates M -> M\n        \n        fake_m = G_PtoM(real_p, training=True)\n        cycled_p = G_MtoP(fake_m, training=True)\n        \n        fake_p = G_MtoP(real_m, training=True)\n        cycled_m = G_PtoM(fake_p, training=True)\n        \n        # same_m and same_p for identity loss\n        same_m = G_PtoM(real_m, training=True)\n        same_p = G_MtoP(real_p, training=True)\n        \n        # disctiminator outputs\n        disc_real_m = D_M(real_m, training=True)\n        disc_real_p = D_P(real_p, training=True)\n        \n        disc_fake_m = D_M(fake_m, training=True)\n        disc_fake_p = D_P(fake_p, training=True)\n        \n        # Calculate Loss\n        gen_MtoP_loss = generator_loss(disc_fake_p)\n        gen_PtoM_loss = generator_loss(disc_fake_m)\n        \n        total_cycle_loss = calc_cycle_loss(real_m, cycled_m) + calc_cycle_loss(real_p, cycled_p)\n        \n        identity_loss_p = identity_loss(real_p, same_p)\n        identity_loss_m = identity_loss(real_m, same_m)\n        \n        # Total Loss\n        total_gen_MtoP_loss = gen_MtoP_loss + total_cycle_loss + identity_loss_p\n        total_gen_PtoM_loss = gen_PtoM_loss + total_cycle_loss + identity_loss_m\n        \n        disc_p_loss = discriminator_loss(disc_real_p, disc_fake_p)\n        disc_m_loss = discriminator_loss(disc_real_m, disc_fake_m)\n        \n    \n    # Calculate Gradients\n    gen_mtop_gradients = tape.gradient(total_gen_MtoP_loss, G_MtoP.trainable_variables)\n    gen_ptom_gradients = tape.gradient(total_gen_PtoM_loss, G_PtoM.trainable_variables)\n    disc_m_gradients = tape.gradient(disc_m_loss, D_M.trainable_variables)\n    disc_p_gradients= tape.gradient(disc_p_loss, D_P.trainable_variables)\n    \n    # Apply Gradients to optimizers\n    G_MtoP_optimizer.apply_gradients(zip(gen_mtop_gradients, G_MtoP.trainable_variables))\n    G_PtoM_optimizer.apply_gradients(zip(gen_ptom_gradients, G_PtoM.trainable_variables))\n    D_M_optimizer.apply_gradients(zip(disc_m_gradients, D_M.trainable_variables))\n    D_P_optimizer.apply_gradients(zip(disc_p_gradients, D_P.trainable_variables))\n    \n    # Update Running Loss\n    D_M_loss.update_state(disc_m_loss)\n    D_P_loss.update_state(disc_p_loss)\n    G_MtoP_loss.update_state(total_gen_MtoP_loss)\n    G_PtoM_loss.update_state(total_gen_PtoM_loss)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"@tf.function\ndef distributed_train_step(real_m, real_p):\n    strategy.run(train_step, args=(real_m, real_p))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# sample_m = next(iter(monet_dataset))\n# sample_p = next(iter(photo_dataset))\n\n# ptom_preds_images = []\n# mtop_reconstructions_images = []\n# ptop_identity_images = []\n\n# mtop_preds_images = []\n# ptom_reconstructions_images = []\n# mtom_identity_images = []\n\nfor epoch in range(EPOCHS):\n    \n#     clear_output()\n#     pmp_display_list = generate_images(G_PtoM, G_MtoP, sample_p)\n#     ptom_preds_images.append(pmp_display_list[1])\n#     mtop_reconstructions_images.append(pmp_display_list[2])\n#     ptop_identity_images.append(pmp_display_list[3])\n#     \n#     mpm_display_list = generate_images(G_MtoP, G_PtoM, sample_m)\n#     mtop_preds_images.append(mpm_display_list[1])\n#     ptom_reconstructions_images.append(mpm_display_list[2])\n#     mtom_identity_images.append(mpm_display_list[3])\n    \n    G_MtoP_loss.reset_states()\n    G_PtoM_loss.reset_states()\n    D_M_loss.reset_states()\n    D_P_loss.reset_states()\n    \n    ds = tf.data.Dataset.zip((photo_dataset, monet_dataset))\n    with tqdm(ds, desc=\"Epoch {}/{}\".format(epoch+1, EPOCHS)) as t:\n        for image_p, image_m in t:\n            distributed_train_step(image_m, image_p)\n            t.set_description(\n                f\"Epoch: {epoch+1}/{EPOCHS}, \"\n                f\"G_MtoP: {G_MtoP_loss.result():.4f}, \"\n                f\"G_PtoM: {G_PtoM_loss.result():.4f}, \"\n                f\"D_M: {D_M_loss.result():.4f}, \"\n                f\"D_P: {D_P_loss.result():.4f}\"\n            )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Training Visualisation"},{"metadata":{"trusted":true},"cell_type":"code","source":"# def make_animation(images, title):\n#     fig, ax = plt.subplots()\n#     plt.title(title)\n#     ims = []\n#     for image in images:\n#         im = ax.imshow(image, animated=True)\n#         ims.append([im])\n# #     ax.imshow(images[0])\n#     ani = animation.ArtistAnimation(fig, ims, repeat_delay=1000, blit=True)\n#     return ani ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plt.rcParams[\"animation.html\"] = \"html5\"\n# make_animation(ptom_preds_images, \"PtoM\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# make_animation(mtop_reconstructions_images, \"MtoP_Cycle\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# make_animation(ptop_identity_images, \"PtoP_Identity\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# make_animation(mtop_preds_images, \"MtoP\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# make_animation(ptom_reconstructions_images, \"PtoM_Cycle\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# make_animation(mtom_identity_images, \"MtoM_Identity\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_photo_dataset = tf.data.TFRecordDataset(photo_tfrec_files)\ntest_photo_dataset = test_photo_dataset.map(parse_tfrecord, num_parallel_calls=AUTOTUNE)\ntest_photo_dataset = test_photo_dataset.map(normalize, num_parallel_calls=AUTOTUNE)\ntest_photo_dataset = test_photo_dataset.batch(BATCH_SIZE)\n# test_photo_dataset = photo_dataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,20))\nfor images in test_photo_dataset.take(1):\n    predictions = G_PtoM(images, training=False)\n    for i in range(len(images)):\n        plt.subplot(8,8, 2*i+1)\n        plt.imshow((images[i]+1)/2)\n        plt.title(\"Photo\")\n        plt.axis('off')\n\n        plt.subplot(8,8, 2*i+2)\n        plt.imshow((predictions[i]+1)/2)\n        plt.title(\"To Monet\")\n        plt.axis('off')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_image(arr):\n    arr = (arr + 1) * 127.5\n    arr = tf.cast(arr, tf.int8)\n    return tf.keras.preprocessing.image.array_to_img(arr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from zipfile import ZipFile\n\nfile_index = 1\nwith ZipFile('images.zip', 'w') as submission_zip:\n    for images in tqdm(test_photo_dataset):\n        predictions = G_PtoM.predict(images)\n        for prediction in predictions:\n            filename = f'photo_to_monet_{file_index}.jpg'\n            img = get_image(prediction)\n            img.save(filename)\n            submission_zip.write(filename)\n            os.remove(filename)\n            file_index+=1\n        if file_index>=9950:\n            break\n    submission_zip.close()\n    \nprint(f\"Saved {file_index} files in images.zip\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}