{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom keras.preprocessing.image import ImageDataGenerator\nimport re\nfrom PIL import Image\nimport torch\nimport cv2\nfrom sklearn.cluster import MiniBatchKMeans\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nmonet_tfrec = '-60.tfrec'\nfor dirname, xx, filenames in os.walk('/kaggle/input'):\n   for filename in filenames:\n        fName = os.path.join(dirname, filename)\n        if monet_tfrec in fName: print(fName) \n        \n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-12T18:01:29.969452Z","iopub.execute_input":"2021-06-12T18:01:29.969847Z","iopub.status.idle":"2021-06-12T18:01:40.708901Z","shell.execute_reply.started":"2021-06-12T18:01:29.969807Z","shell.execute_reply":"2021-06-12T18:01:40.707528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Device:', tpu.master())\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nexcept:\n    strategy = tf.distribute.get_strategy()\n    #A state & compute distribution policy on a list of devices.\n\nprint('Number of replicas:', strategy.num_replicas_in_sync)\nAUTO = tf.data.experimental.AUTOTUNE\nprint(tf.__version__)","metadata":{"execution":{"iopub.status.busy":"2021-06-12T18:01:40.712906Z","iopub.execute_input":"2021-06-12T18:01:40.713293Z","iopub.status.idle":"2021-06-12T18:01:40.728334Z","shell.execute_reply.started":"2021-06-12T18:01:40.713254Z","shell.execute_reply":"2021-06-12T18:01:40.727411Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MONET_FILENAMES = tf.io.gfile.glob(str('/kaggle/input/gan-getting-started/monet_tfrec/*.tfrec'))\nPHOTO_FILENAMES = tf.io.gfile.glob(str('/kaggle/input/gan-getting-started/photo_tfrec/*.tfrec'))\nprint(\"Total MONET files\", len(MONET_FILENAMES), \"\\nTotal PHOTO files\", len(PHOTO_FILENAMES), \"\\n\")\nMONET_FILENAMES","metadata":{"execution":{"iopub.status.busy":"2021-06-12T18:01:40.730816Z","iopub.execute_input":"2021-06-12T18:01:40.731284Z","iopub.status.idle":"2021-06-12T18:01:40.748233Z","shell.execute_reply.started":"2021-06-12T18:01:40.731236Z","shell.execute_reply":"2021-06-12T18:01:40.747504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def count_data_items(filenames):\n    n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) for filename in filenames]\n    return np.sum(n)\n\nn_monet_samples = count_data_items(MONET_FILENAMES)\nn_photo_samples = count_data_items(PHOTO_FILENAMES)\n\nBATCH_SIZE =  4\nEPOCHS_NUM = 30\n\nprint(f'Monet TFRecord files: {len(MONET_FILENAMES)}')\nprint(f'Monet image files: {n_monet_samples}')\nprint(f'Photo TFRecord files: {len(PHOTO_FILENAMES)}')\nprint(f'Photo image files: {n_photo_samples}')\nprint(f\"Batch_size: {BATCH_SIZE}\")\nprint(f\"Epochs number: {EPOCHS_NUM}\")","metadata":{"execution":{"iopub.status.busy":"2021-06-12T18:01:40.749381Z","iopub.execute_input":"2021-06-12T18:01:40.749707Z","iopub.status.idle":"2021-06-12T18:01:40.761206Z","shell.execute_reply.started":"2021-06-12T18:01:40.749676Z","shell.execute_reply":"2021-06-12T18:01:40.760065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def data_augment(image):\n    print(\"data_augment\\n\")\n    p_rotate = tf.random.uniform([], 0, 1.0, dtype=tf.float32)\n    p_spatial = tf.random.uniform([], 0, 1.0, dtype=tf.float32)\n    p_crop = tf.random.uniform([], 0, 1.0, dtype=tf.float32)\n    if p_crop > .5:\n        image = tf.image.resize(image, [286, 286]) #resizing to 286 x 286 x 3\n        image = tf.image.random_crop(image, size=[256, 256, 3]) # randomly cropping to 256 x 256 x 3\n        if p_crop > .9:\n            image = tf.image.resize(image, [300, 300])\n            image = tf.image.random_crop(image, size=[256, 256, 3])\n    \n    if p_rotate > .9:\n        image = tf.image.rot90(image, k=3) # rotate 270ยบ\n    elif p_rotate > .7:\n        image = tf.image.rot90(image, k=2) # rotate 180ยบ\n    elif p_rotate > .5:\n        image = tf.image.rot90(image, k=1) # rotate 90ยบ\n        \n    ## random mirroring\n    if p_spatial > .6:\n        image = tf.image.random_flip_left_right(image)\n        image = tf.image.random_flip_up_down(image)\n        if p_spatial > .9:\n            image = tf.image.transpose(image)\n    \n    return image\n\ndef load_dataset(filenames):\n    print(\"load_dataset\\n\")\n    dataset = tf.data.TFRecordDataset(filenames)\n    dataset = dataset.map(read_tfrecord, num_parallel_calls=AUTO)\n    return dataset\n\ndef get_gan_dataset(monet_files, photo_files, augment=None, repeat=True, shuffle=True, batch_size=1):\n    print(\"get_gan_dataset\\n\")\n    monet_ds = load_dataset(monet_files)\n    photo_ds = load_dataset(photo_files)\n    \n    if augment:\n        monet_ds = monet_ds.map(augment, num_parallel_calls=AUTO)\n        photo_ds = photo_ds.map(augment, num_parallel_calls=AUTO)\n    \n    if repeat:\n        monet_ds = monet_ds.repeat()\n        photo_ds = photo_ds.repeat()\n    \n    if shuffle:\n        monet_ds = monet_ds.shuffle(2048)\n        photo_ds = photo_ds.shuffle(2048)\n        \n    monet_ds = monet_ds.batch(batch_size, drop_remainder=True)\n    photo_ds = photo_ds.batch(batch_size, drop_remainder=True)\n    monet_ds = monet_ds.cache()\n    photo_ds = photo_ds.cache()\n    monet_ds = monet_ds.prefetch(AUTO)\n    photo_ds = photo_ds.prefetch(AUTO)\n    \n    gan_ds = tf.data.Dataset.zip((monet_ds, photo_ds))\n    \n    return gan_ds","metadata":{"execution":{"iopub.status.busy":"2021-06-12T18:01:40.762498Z","iopub.execute_input":"2021-06-12T18:01:40.763101Z","iopub.status.idle":"2021-06-12T18:01:40.779434Z","shell.execute_reply.started":"2021-06-12T18:01:40.763055Z","shell.execute_reply":"2021-06-12T18:01:40.778584Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"IMAGE_SIZE = [256, 256]\ndef decode_image(image):\n    print(\"decode_image\\n\")\n    #Decode the JPEG-encoded images to uint8 tensors.\n    #channels = 3 for the output an RGB image.\n    #The attr channels indicates the desired number of color channels for the decoded image.\n    image = tf.image.decode_jpeg(image, channels=3)\n    #Cast the tensors to new type.\n    image = (tf.cast(image, tf.float32) / 127.5) - 1\n    image = tf.reshape(image, [*IMAGE_SIZE, 3])\n    return image\n\ndef read_tfrecord(example):\n    print(\"read_tfrecord\\n\")\n    tfrecord_format = {\n        \"image_name\": tf.io.FixedLenFeature([], tf.string),\n        \"image\": tf.io.FixedLenFeature([], tf.string),\n        \"target\": tf.io.FixedLenFeature([], tf.string)\n    }\n    example = tf.io.parse_single_example(example, tfrecord_format)\n    image = decode_image(example['image'])\n    return image","metadata":{"execution":{"iopub.status.busy":"2021-06-12T18:01:40.780758Z","iopub.execute_input":"2021-06-12T18:01:40.781288Z","iopub.status.idle":"2021-06-12T18:01:40.798632Z","shell.execute_reply.started":"2021-06-12T18:01:40.781255Z","shell.execute_reply":"2021-06-12T18:01:40.797262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"get_gan_dataset calls the following functions and prepares the image data:\\n\")\nfull_dataset = get_gan_dataset(MONET_FILENAMES, PHOTO_FILENAMES, augment=data_augment, repeat=True, shuffle=True, batch_size=BATCH_SIZE)","metadata":{"execution":{"iopub.status.busy":"2021-06-12T18:01:40.799979Z","iopub.execute_input":"2021-06-12T18:01:40.800504Z","iopub.status.idle":"2021-06-12T18:01:42.03975Z","shell.execute_reply.started":"2021-06-12T18:01:40.800464Z","shell.execute_reply":"2021-06-12T18:01:42.038442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"full_dataset?","metadata":{"execution":{"iopub.status.busy":"2021-06-12T16:50:28.915636Z","iopub.execute_input":"2021-06-12T16:50:28.915947Z","iopub.status.idle":"2021-06-12T16:50:28.922273Z","shell.execute_reply.started":"2021-06-12T16:50:28.915918Z","shell.execute_reply":"2021-06-12T16:50:28.921268Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"full_dataset.element_spec","metadata":{"execution":{"iopub.status.busy":"2021-06-12T16:50:29.502564Z","iopub.execute_input":"2021-06-12T16:50:29.502923Z","iopub.status.idle":"2021-06-12T16:50:29.509225Z","shell.execute_reply.started":"2021-06-12T16:50:29.502886Z","shell.execute_reply":"2021-06-12T16:50:29.508127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for images, labels in full_dataset.take(1):\n  print('images.shape: ', images.shape)\n  print('labels.shape: ', labels.shape)","metadata":{"execution":{"iopub.status.busy":"2021-06-12T16:50:39.975477Z","iopub.execute_input":"2021-06-12T16:50:39.975934Z","iopub.status.idle":"2021-06-12T16:50:44.428294Z","shell.execute_reply.started":"2021-06-12T16:50:39.975887Z","shell.execute_reply":"2021-06-12T16:50:44.427174Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"c = 0\nfor fn in MONET_FILENAMES:#PHOTO_FILENAMES,MONET_FILENAMES:\n  for record in tf.compat.v1.python_io.tf_record_iterator(fn):\n    c += 1\n  print(c, 'files in', fn)","metadata":{"execution":{"iopub.status.busy":"2021-06-12T16:50:44.430002Z","iopub.execute_input":"2021-06-12T16:50:44.430309Z","iopub.status.idle":"2021-06-12T16:50:44.447252Z","shell.execute_reply.started":"2021-06-12T16:50:44.430278Z","shell.execute_reply":"2021-06-12T16:50:44.446158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"example_monet ,example_photo = next(iter(full_dataset))\n# print(example_monet.numpy())\n# iter() contains an iterator\n# The next() gets the first iteration. Running next() again will get the second item of the iterator, and so-on.\nprint(\"example_monet: \", example_monet.shape, len(example_monet), len(example_monet[0]), len(example_monet[1]), len(example_monet[2]), len(example_monet[3]))\nprint(\"example_photo: \", example_photo.shape, len(example_photo), len(example_photo[0]), len(example_photo[1]), len(example_photo[2]), len(example_photo[3]))","metadata":{"execution":{"iopub.status.busy":"2021-06-12T16:50:48.942655Z","iopub.execute_input":"2021-06-12T16:50:48.942989Z","iopub.status.idle":"2021-06-12T16:50:48.955817Z","shell.execute_reply.started":"2021-06-12T16:50:48.942959Z","shell.execute_reply":"2021-06-12T16:50:48.953971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"example_monet?","metadata":{"execution":{"iopub.status.busy":"2021-06-12T16:50:48.957358Z","iopub.execute_input":"2021-06-12T16:50:48.957909Z","iopub.status.idle":"2021-06-12T16:50:48.974151Z","shell.execute_reply.started":"2021-06-12T16:50:48.957877Z","shell.execute_reply":"2021-06-12T16:50:48.973396Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"example_photo?","metadata":{"execution":{"iopub.status.busy":"2021-06-12T16:50:48.975322Z","iopub.execute_input":"2021-06-12T16:50:48.975791Z","iopub.status.idle":"2021-06-12T16:50:48.99114Z","shell.execute_reply.started":"2021-06-12T16:50:48.97576Z","shell.execute_reply":"2021-06-12T16:50:48.990178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nn = len(example_monet)\nfor ii in range(nn): \n    plt.subplot(1,nn,ii+1)\n    plt.imshow(example_monet[ii])#plt.imshow(example_monet[ii] * 0.5 + 0.5)\nexample_monet.shape[2]","metadata":{"execution":{"iopub.status.busy":"2021-06-12T16:50:58.466181Z","iopub.execute_input":"2021-06-12T16:50:58.466706Z","iopub.status.idle":"2021-06-12T16:50:58.898612Z","shell.execute_reply.started":"2021-06-12T16:50:58.466672Z","shell.execute_reply":"2021-06-12T16:50:58.897317Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xx = np.array(example_monet[2])#numpy.ndarray\nprint(xx.shape, type(xx))\nplt.imshow(xx)\nplt.savefig('test.png')","metadata":{"execution":{"iopub.status.busy":"2021-06-12T16:51:07.325822Z","iopub.execute_input":"2021-06-12T16:51:07.326389Z","iopub.status.idle":"2021-06-12T16:51:07.579197Z","shell.execute_reply.started":"2021-06-12T16:51:07.32634Z","shell.execute_reply":"2021-06-12T16:51:07.578007Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# zz = torch.from_numpy(xx)#torch.Tensor\n# yy = Image.fromarray((xx * 255).astype(np.uint8))#PIL.Image.Image\n# plt.imshow(zz)","metadata":{"execution":{"iopub.status.busy":"2021-06-12T18:02:09.69072Z","iopub.status.idle":"2021-06-12T18:02:09.691229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"total_monet_images, img_counter = 0, 0\nfull_dataset = get_gan_dataset(MONET_FILENAMES, PHOTO_FILENAMES, augment=data_augment, repeat=True, shuffle=True, batch_size=BATCH_SIZE)\nL_monet, L_monet_asTorch, L_photo, L_photo_asTorch = [], [], [], []\nfor jj in range(10000):\n    example_monet, example_photo = next(iter(full_dataset))\n    mm = len(example_monet)\n    total_monet_images += mm\n    print(mm, \"monet images in\", jj, 'Iteration and total_monet_images are', total_monet_images)\n    for ii in range(mm):\n        img_counter += 1\n        image = np.array(example_monet[ii])#numpy.ndarray\n        zz = torch.from_numpy(image)#torch.Tensor\n        L_monet.append(image)\n        L_monet_asTorch.append(zz)\n        fName = \"monet_proc\" + str(img_counter) +'.jpg';\n        print(\"Saving \", fName)\n        plt.imshow(image)\n        plt.savefig(fName)\n    nn = len(example_photo)\n    print(nn, \"photo images in\", jj, 'Iteration')\n    for ii in range(nn):\n        image = np.array(example_photo[ii])#numpy.ndarray\n        zz = torch.from_numpy(image)#torch.Tensor\n        L_photo.append(image)\n        L_photo_asTorch.append(zz)\n    ","metadata":{"execution":{"iopub.status.busy":"2021-06-12T18:04:16.47756Z","iopub.execute_input":"2021-06-12T18:04:16.478115Z","iopub.status.idle":"2021-06-12T18:39:04.419744Z","shell.execute_reply.started":"2021-06-12T18:04:16.47808Z","shell.execute_reply":"2021-06-12T18:39:04.416963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bPlot = 0\ncolor_hist = []\nnBins = 16\nbin_edges = np.linspace(-1, 1, nBins)\nxlim = np.min(image[-1]), np.max(image[-1])\nimage_counter = 0\nfor ii in L_monet:\n    if (image_counter % 50 == 0): print(\"Processing image#\", image_counter)\n    image_counter += 1\n    image = ii\n    mean_image = image.mean(axis=2).flatten()\n    bin_counts = plt.hist(mean_image, bins=bin_edges)\n    if bPlot: plt.show()#bins=bin_edges or bins=nBins\n    color_hist.append(bin_counts[0])\ncolor_hist = pd.DataFrame(color_hist)#rows are one image and columns are the quantized mean RGB image value for the processed images\ncolor_hist","metadata":{"execution":{"iopub.status.busy":"2021-06-12T19:07:25.750902Z","iopub.execute_input":"2021-06-12T19:07:25.751462Z","iopub.status.idle":"2021-06-12T19:07:45.387122Z","shell.execute_reply.started":"2021-06-12T19:07:25.751425Z","shell.execute_reply":"2021-06-12T19:07:45.386048Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Color Quantization with OpenCV\nfrom sklearn.cluster import MiniBatchKMeans\nimport cv2\nn_clusters = 16\n# construct the argument parser and parse the arguments\n# load the image and grab its width and height\n(h, w) = image.shape[:2]\n# convert the image from the RGB color space to the L*a*b*\n# color space -- since we will be clustering using k-means\n# which is based on the euclidean distance, we'll use the\n# L*a*b* color space where the euclidean distance implies\n# perceptual meaning\nimage = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n# reshape the image into a feature vector so that k-means\n# can be applied\nimage = image.reshape((image.shape[0] * image.shape[1], 3))\n# apply k-means using the specified number of clusters and\n# then create the quantized image based on the predictions\nclt = MiniBatchKMeans(n_clusters = n_clusters)\nlabels = clt.fit_predict(image)\nquant = clt.cluster_centers_.astype(\"uint8\")[labels]\n# reshape the feature vectors to images\nquant = quant.reshape((h, w, 3))\nimage = image.reshape((h, w, 3))\n# convert from L*a*b* to RGB\nquant = cv2.cvtColor(quant, cv2.COLOR_LAB2BGR)\nimage = cv2.cvtColor(image, cv2.COLOR_LAB2BGR)\n# display the images and wait for a keypress\ncv2.imshow(\"image\", np.hstack([image, quant]))\ncv2.waitKey(0)\n","metadata":{"execution":{"iopub.status.busy":"2021-06-12T17:35:06.295565Z","iopub.execute_input":"2021-06-12T17:35:06.295985Z","iopub.status.idle":"2021-06-12T17:35:06.352381Z","shell.execute_reply.started":"2021-06-12T17:35:06.295947Z","shell.execute_reply":"2021-06-12T17:35:06.350673Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# IGNORE THIS CELL\ntfrecord_filenames_queue = [MONET_FILENAMES]\n# tfrecord_filenames_queue : tfrecord filename queue\n#    String queue object from tf.train.string_input_producer()\n#reader = tf.TFRecordReader()\n#_, serialized_example = reader.read(tfrecord_filenames_queue)\nreader = tf.data.TFRecordDataset(tfrecord_filenames_queue)\n\ncount = 0\nfor serialized_example in reader.take(1):# read the first record.\n    count = count+1\n\n# Create a description of the features.\n# features={\n#         'height': tf.io.FixedLenFeature([], tf.int64),\n#         'width': tf.io.FixedLenFeature([], tf.int64),\n#         'image_raw': tf.io.FixedLenFeature([], tf.string),\n#         'mask_raw': tf.io.FixedLenFeature([], tf.string)\n#         }\n\nfeatures={\n        'image_name': tf.io.FixedLenFeature([], tf.string),\n        'image': tf.io.FixedLenFeature([], tf.string),\n        'target': tf.io.FixedLenFeature([], tf.string)\n        }\n            \nfeatures = tf.io.parse_single_example(serialized_example, features)\n# image = tf.io.decode_raw(features['image_raw'], tf.uint8)\n# annotation = tf.io.decode_raw(features['mask_raw'], tf.uint8)\n    \n# height = tf.cast(features['height'], tf.int32)\n# width = tf.cast(features['width'], tf.int32)\n    \n# image_shape = tf.pack([height, width, 3])#module 'tensorflow' has no attribute 'pack'\n# annotation_shape = tf.pack([height, width, 1])#module 'tensorflow' has no attribute 'pack'\n    \n# image = tf.reshape(image, image_shape)\n# notation = tf.reshape(annotation, annotation_shape)\n","metadata":{"execution":{"iopub.status.busy":"2021-06-12T10:24:43.563176Z","iopub.execute_input":"2021-06-12T10:24:43.56349Z","iopub.status.idle":"2021-06-12T10:24:43.588853Z","shell.execute_reply.started":"2021-06-12T10:24:43.563459Z","shell.execute_reply":"2021-06-12T10:24:43.587805Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# IGNORE THIS CELL\n# This will do preprocessing and realtime data augmentation:\ndatagen = ImageDataGenerator(\n        featurewise_center=False,  # set input mean to 0 over the dataset\n        samplewise_center=False,  # set each sample mean to 0\n        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n        samplewise_std_normalization=False,  # divide each input by its std\n        zca_whitening=False,  # apply ZCA whitening\n        rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n        horizontal_flip=True,  # randomly flip images\n        vertical_flip=False)  # randomly flip images","metadata":{"execution":{"iopub.status.busy":"2021-06-12T10:24:43.590069Z","iopub.execute_input":"2021-06-12T10:24:43.590358Z","iopub.status.idle":"2021-06-12T10:24:43.596514Z","shell.execute_reply.started":"2021-06-12T10:24:43.590322Z","shell.execute_reply":"2021-06-12T10:24:43.595514Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"datagen.flow(image, annotation, batch_size=32)","metadata":{"execution":{"iopub.status.busy":"2021-06-12T10:24:43.597675Z","iopub.execute_input":"2021-06-12T10:24:43.598049Z","iopub.status.idle":"2021-06-12T10:24:43.64246Z","shell.execute_reply.started":"2021-06-12T10:24:43.598016Z","shell.execute_reply":"2021-06-12T10:24:43.641033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}