{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install pydot graphviz","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-30T05:56:50.537149Z","iopub.execute_input":"2021-07-30T05:56:50.537536Z","iopub.status.idle":"2021-07-30T05:56:56.844919Z","shell.execute_reply.started":"2021-07-30T05:56:50.537505Z","shell.execute_reply":"2021-07-30T05:56:56.843793Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for loading/processing the images  \nfrom keras.preprocessing.image import load_img \nfrom keras.preprocessing.image import img_to_array \nfrom keras.applications.vgg16 import preprocess_input \n\n# models \nfrom keras.applications.vgg16 import VGG16 \nfrom keras.models import Model\n\n# clustering and dimension reduction\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\n\n# for everything else\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom random import randint\nimport pandas as pd\nimport pickle\nfrom kaggle_datasets import KaggleDatasets\n\n!pip install pydot graphviz","metadata":{"execution":{"iopub.status.busy":"2021-07-30T05:56:56.848345Z","iopub.execute_input":"2021-07-30T05:56:56.848744Z","iopub.status.idle":"2021-07-30T05:57:03.282168Z","shell.execute_reply.started":"2021-07-30T05:56:56.848701Z","shell.execute_reply":"2021-07-30T05:57:03.281242Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading Monet images","metadata":{}},{"cell_type":"code","source":"path = \"../input/gan-getting-started/monet_jpg/\"\n\n# this list holds all the image filename\nmonet_images = []\n\n# creates a ScandirIterator aliased as files\nwith os.scandir(path) as files:\n  # loops through each file in the directory\n    for file in files:\n        if file.name.endswith('.jpg'):\n          # adds only the image files to the monet_images list\n            monet_images.append(path + file.name)","metadata":{"execution":{"iopub.status.busy":"2021-07-30T05:57:03.284555Z","iopub.execute_input":"2021-07-30T05:57:03.284918Z","iopub.status.idle":"2021-07-30T05:57:03.293575Z","shell.execute_reply.started":"2021-07-30T05:57:03.284878Z","shell.execute_reply":"2021-07-30T05:57:03.29265Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"monet_images[0]","metadata":{"execution":{"iopub.status.busy":"2021-07-30T05:57:03.296331Z","iopub.execute_input":"2021-07-30T05:57:03.296653Z","iopub.status.idle":"2021-07-30T05:57:03.305657Z","shell.execute_reply.started":"2021-07-30T05:57:03.296627Z","shell.execute_reply":"2021-07-30T05:57:03.304749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load the image as a 224x224 array\nimg = load_img(monet_images[0], target_size=(224,224))\n# convert from 'PIL.Image.Image' to numpy array\nimg = np.array(img)\n\nprint(img.shape)","metadata":{"execution":{"iopub.status.busy":"2021-07-30T05:57:03.308796Z","iopub.execute_input":"2021-07-30T05:57:03.309049Z","iopub.status.idle":"2021-07-30T05:57:03.322052Z","shell.execute_reply.started":"2021-07-30T05:57:03.309026Z","shell.execute_reply":"2021-07-30T05:57:03.321086Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reshaped_img = img.reshape(1,224, 224, 3)\nprint(reshaped_img.shape)","metadata":{"execution":{"iopub.status.busy":"2021-07-30T05:57:03.323259Z","iopub.execute_input":"2021-07-30T05:57:03.323588Z","iopub.status.idle":"2021-07-30T05:57:03.328314Z","shell.execute_reply.started":"2021-07-30T05:57:03.323562Z","shell.execute_reply":"2021-07-30T05:57:03.32719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = preprocess_input(reshaped_img)","metadata":{"execution":{"iopub.status.busy":"2021-07-30T05:57:03.329922Z","iopub.execute_input":"2021-07-30T05:57:03.330599Z","iopub.status.idle":"2021-07-30T05:57:03.339072Z","shell.execute_reply.started":"2021-07-30T05:57:03.330549Z","shell.execute_reply":"2021-07-30T05:57:03.338133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nimport matplotlib.pyplot as plt\nimport matplotlib\nimport numpy as np\nimport time\nfrom PIL import Image\n%matplotlib inline","metadata":{"execution":{"iopub.status.busy":"2021-07-30T05:57:03.34274Z","iopub.execute_input":"2021-07-30T05:57:03.343106Z","iopub.status.idle":"2021-07-30T05:57:03.362868Z","shell.execute_reply.started":"2021-07-30T05:57:03.343071Z","shell.execute_reply":"2021-07-30T05:57:03.361999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Extracting features using trained model","metadata":{}},{"cell_type":"code","source":"# load model\nmodel = VGG16()\n# remove the output layer\nmodel = Model(inputs=model.inputs, outputs=model.layers[-2].output)","metadata":{"execution":{"iopub.status.busy":"2021-07-30T05:57:03.365078Z","iopub.execute_input":"2021-07-30T05:57:03.36553Z","iopub.status.idle":"2021-07-30T05:57:05.31745Z","shell.execute_reply.started":"2021-07-30T05:57:03.365489Z","shell.execute_reply":"2021-07-30T05:57:05.316552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features = model.predict(x)\nprint(features.shape)","metadata":{"execution":{"iopub.status.busy":"2021-07-30T05:57:05.318865Z","iopub.execute_input":"2021-07-30T05:57:05.319302Z","iopub.status.idle":"2021-07-30T05:57:05.593138Z","shell.execute_reply.started":"2021-07-30T05:57:05.319262Z","shell.execute_reply":"2021-07-30T05:57:05.592088Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load the model first and pass as an argument\nmodel = VGG16()\nmodel = Model(inputs = model.inputs, outputs = model.layers[-2].output)\n\ndef extract_features(file, model):\n    # load the image as a 224x224 array\n    img = load_img(file, target_size=(224,224))\n    # convert from 'PIL.Image.Image' to numpy array\n    img = np.array(img) \n    # reshape the data for the model reshape(num_of_samples, dim 1, dim 2, channels)\n    reshaped_img = img.reshape(1,224,224,3) \n    # prepare image for model\n    imgx = preprocess_input(reshaped_img)\n    # get the feature vector\n    features = model.predict(imgx, use_multiprocessing=True)\n    return features","metadata":{"execution":{"iopub.status.busy":"2021-07-30T05:57:05.594617Z","iopub.execute_input":"2021-07-30T05:57:05.595191Z","iopub.status.idle":"2021-07-30T05:57:07.468694Z","shell.execute_reply.started":"2021-07-30T05:57:05.595147Z","shell.execute_reply":"2021-07-30T05:57:07.46784Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"monet_images[0]","metadata":{"execution":{"iopub.status.busy":"2021-07-30T05:57:07.46997Z","iopub.execute_input":"2021-07-30T05:57:07.470377Z","iopub.status.idle":"2021-07-30T05:57:07.479449Z","shell.execute_reply.started":"2021-07-30T05:57:07.47034Z","shell.execute_reply":"2021-07-30T05:57:07.478621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = {}\np = \"monet_images.pkl\"\n\n# lop through each image in the dataset\nfor monet_image in monet_images:\n    # try to extract the features and update the dictionary\n    feat = extract_features(monet_image,model)\n    data[monet_image] = feat\n          \n \n# get a list of the filenames\nfilenames = np.array(list(data.keys()))\n\n# get a list of just the features\nfeat = np.array(list(data.values()))\nfeat.shape\n(210, 1, 4096)\n\n# reshape so that there are 210 samples of 4096 vectors\nfeat = feat.reshape(-1,4096)\nfeat.shape\n(210, 4096)\n\nunique_labels = list(set(list(range(30))))","metadata":{"execution":{"iopub.status.busy":"2021-07-30T05:57:07.480925Z","iopub.execute_input":"2021-07-30T05:57:07.481537Z","iopub.status.idle":"2021-07-30T05:57:52.764382Z","shell.execute_reply.started":"2021-07-30T05:57:07.481499Z","shell.execute_reply":"2021-07-30T05:57:52.763462Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Using PCA to lower dimentions","metadata":{}},{"cell_type":"code","source":"pca = PCA(n_components=100, random_state=22)\npca.fit(feat)\nx = pca.transform(feat)","metadata":{"execution":{"iopub.status.busy":"2021-07-30T05:57:52.768599Z","iopub.execute_input":"2021-07-30T05:57:52.77075Z","iopub.status.idle":"2021-07-30T05:57:53.203461Z","shell.execute_reply.started":"2021-07-30T05:57:52.770704Z","shell.execute_reply":"2021-07-30T05:57:53.194169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kmeans = KMeans(n_clusters=len(unique_labels),n_jobs=-1, random_state=22)\nkmeans.fit(x)","metadata":{"execution":{"iopub.status.busy":"2021-07-30T05:57:53.206281Z","iopub.execute_input":"2021-07-30T05:57:53.206635Z","iopub.status.idle":"2021-07-30T05:57:53.74602Z","shell.execute_reply.started":"2021-07-30T05:57:53.206602Z","shell.execute_reply":"2021-07-30T05:57:53.74517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# holds the cluster id and the images { id: [images] }\ngroups = {}\nfor file, cluster in zip(filenames,kmeans.labels_):\n    if cluster not in groups.keys():\n        groups[cluster] = []\n        groups[cluster].append(file)\n    else:\n        groups[cluster].append(file)","metadata":{"execution":{"iopub.status.busy":"2021-07-30T05:57:53.747491Z","iopub.execute_input":"2021-07-30T05:57:53.748125Z","iopub.status.idle":"2021-07-30T05:57:53.755548Z","shell.execute_reply.started":"2021-07-30T05:57:53.748083Z","shell.execute_reply":"2021-07-30T05:57:53.754693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"groups[0]","metadata":{"execution":{"iopub.status.busy":"2021-07-30T05:57:53.757299Z","iopub.execute_input":"2021-07-30T05:57:53.757992Z","iopub.status.idle":"2021-07-30T05:57:53.766352Z","shell.execute_reply.started":"2021-07-30T05:57:53.75795Z","shell.execute_reply":"2021-07-30T05:57:53.765644Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import cv2\n\nimgs = groups[0]\n\nplt.figure()\n\n#subplot(r,c) provide the no. of rows and columns\nf, axarr = plt.subplots(len(imgs),1, figsize = (len(imgs)*30,30))\n\nfor i, img_path in enumerate(imgs):\n    axarr[i].imshow(cv2.imread(imgs[i]))","metadata":{"execution":{"iopub.status.busy":"2021-07-30T05:57:53.767698Z","iopub.execute_input":"2021-07-30T05:57:53.768229Z","iopub.status.idle":"2021-07-30T05:57:54.811134Z","shell.execute_reply.started":"2021-07-30T05:57:53.768182Z","shell.execute_reply":"2021-07-30T05:57:54.810213Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Creating 30 Monet images dataset","metadata":{}},{"cell_type":"code","source":"# we choose image 0 in every cluster as a representitive\nmonet_dataset_paths = []\nfor i in range(len(groups)):\n    monet_dataset_paths.append(groups[i][0])\nprint(monet_dataset_paths)\nlen(monet_dataset_paths)","metadata":{"execution":{"iopub.status.busy":"2021-07-30T05:57:54.812768Z","iopub.execute_input":"2021-07-30T05:57:54.813208Z","iopub.status.idle":"2021-07-30T05:57:54.824988Z","shell.execute_reply.started":"2021-07-30T05:57:54.81316Z","shell.execute_reply":"2021-07-30T05:57:54.824066Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport time\nfrom tqdm import tqdm\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.layers import Conv2D, Flatten, ReLU, BatchNormalization, Conv2DTranspose, Dense\nimport albumentations as A\nimport cv2\nimport glob\nfrom functools import partial\n\nfrom IPython.display import clear_output\nfrom kaggle_datasets import KaggleDatasets\n# import matplotlib.animation as animation\n\nAUTOTUNE = tf.data.AUTOTUNE","metadata":{"execution":{"iopub.status.busy":"2021-07-30T05:57:54.826967Z","iopub.execute_input":"2021-07-30T05:57:54.82781Z","iopub.status.idle":"2021-07-30T05:57:54.835474Z","shell.execute_reply.started":"2021-07-30T05:57:54.82774Z","shell.execute_reply":"2021-07-30T05:57:54.834284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n    INPUT_PATH = KaggleDatasets().get_gcs_path()\nelse:\n    strategy = tf.distribute.get_strategy()\n    INPUT_PATH = \"../input/gan-getting-started\"\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)\nprint(\"Input Path:\", INPUT_PATH)","metadata":{"execution":{"iopub.status.busy":"2021-07-30T05:57:54.836919Z","iopub.execute_input":"2021-07-30T05:57:54.837334Z","iopub.status.idle":"2021-07-30T05:57:54.847575Z","shell.execute_reply.started":"2021-07-30T05:57:54.83729Z","shell.execute_reply":"2021-07-30T05:57:54.846596Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"photo_tfrec_files = tf.io.gfile.glob(INPUT_PATH+\"/photo_tfrec/*.tfrec\")","metadata":{"execution":{"iopub.status.busy":"2021-07-30T05:57:54.849184Z","iopub.execute_input":"2021-07-30T05:57:54.84982Z","iopub.status.idle":"2021-07-30T05:57:54.885913Z","shell.execute_reply.started":"2021-07-30T05:57:54.849783Z","shell.execute_reply":"2021-07-30T05:57:54.885072Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_description = {\n    'image': tf.io.FixedLenFeature([], tf.string),\n}","metadata":{"execution":{"iopub.status.busy":"2021-07-30T05:57:54.890813Z","iopub.execute_input":"2021-07-30T05:57:54.891068Z","iopub.status.idle":"2021-07-30T05:57:54.895528Z","shell.execute_reply.started":"2021-07-30T05:57:54.891042Z","shell.execute_reply":"2021-07-30T05:57:54.894526Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def parse_tfrecord(record):\n    features = tf.io.parse_single_example(record, feature_description)\n    \n    image = features['image']\n    image = tf.io.decode_image(image)\n    image = tf.reshape(image, (256, 256, 3))\n    \n    return image","metadata":{"execution":{"iopub.status.busy":"2021-07-30T05:57:54.898999Z","iopub.execute_input":"2021-07-30T05:57:54.899594Z","iopub.status.idle":"2021-07-30T05:57:54.906311Z","shell.execute_reply.started":"2021-07-30T05:57:54.899548Z","shell.execute_reply":"2021-07-30T05:57:54.904948Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def parse_img(image_name):\n    image = cv2.imread(image_name)\n    return image","metadata":{"execution":{"iopub.status.busy":"2021-07-30T05:57:54.908152Z","iopub.execute_input":"2021-07-30T05:57:54.908615Z","iopub.status.idle":"2021-07-30T05:57:54.915716Z","shell.execute_reply.started":"2021-07-30T05:57:54.908565Z","shell.execute_reply":"2021-07-30T05:57:54.914717Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"monet_dataset_paths","metadata":{"execution":{"iopub.status.busy":"2021-07-30T05:57:54.917268Z","iopub.execute_input":"2021-07-30T05:57:54.917805Z","iopub.status.idle":"2021-07-30T05:57:54.928122Z","shell.execute_reply.started":"2021-07-30T05:57:54.917763Z","shell.execute_reply":"2021-07-30T05:57:54.927158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_img = parse_img(monet_dataset_paths[0])\nprint(type(test_img))","metadata":{"execution":{"iopub.status.busy":"2021-07-30T05:57:54.92967Z","iopub.execute_input":"2021-07-30T05:57:54.930106Z","iopub.status.idle":"2021-07-30T05:57:54.940611Z","shell.execute_reply.started":"2021-07-30T05:57:54.930066Z","shell.execute_reply":"2021-07-30T05:57:54.939666Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"p_transformation = 0.1\ntransforms = A.Compose(\n    [\n#         A.Blur(p=p_transformation, blur_limit=(5, 5)),\n#         A.CLAHE(p=p_transformation, clip_limit=(10, 10), tile_grid_size=(3, 3)),\n#         #A.CenterCrop(p=p_transformation, height=100, width=150),\n#         A.ChannelDropout(p=p_transformation, channel_drop_range=(1, 2), fill_value=0),\n#         A.ChannelShuffle(p=p_transformation),\n        A.RandomCrop(p=p_transformation, height=150, width=150),\n        A.Cutout(p=p_transformation, num_holes=8, max_h_size=15, max_w_size=15),\n#         A.Downscale(p=p_transformation, scale_min=0.01, scale_max=0.20, interpolation=0),\n#         A.Equalize(p=p_transformation, mode='cv', by_channels=True),\n        A.HorizontalFlip(p=p_transformation),\n        A.VerticalFlip(p=p_transformation),\n        A.Flip(p=p_transformation),\n#         A.GaussNoise(p=p_transformation, var_limit=(500.0, 500.0)),\n#         A.GridDistortion( p=p_transformation, num_steps=15, distort_limit=(-2., 2.), interpolation=0, border_mode=0, value=(0, 0, 0), mask_value=None),\n#         A.HueSaturationValue(p=p_transformation, \n#             hue_shift_limit=(-100, 100), \n#             sat_shift_limit=(-100, 100), \n#             val_shift_limit=(-100, 100)),\n#         A.ISONoise(p=p_transformation, intensity=(0.0, 2.0), color_shift=(0.0, 1.0)),\n#         A.ImageCompression(p=p_transformation, quality_lower=0, quality_upper=10, compression_type=0),\n#         A.InvertImg(p=p_transformation),\n#         A.JpegCompression(p=p_transformation, quality_lower=0, quality_upper=10),\n#         A.MotionBlur(p=p_transformation, blur_limit=(3, 50)),\n#         A.MultiplicativeNoise(p=p_transformation, multiplier=(0.1, 5.0), per_channel=True, elementwise=False),\n    ]\n)","metadata":{"execution":{"iopub.status.busy":"2021-07-30T05:57:54.942891Z","iopub.execute_input":"2021-07-30T05:57:54.943145Z","iopub.status.idle":"2021-07-30T05:57:54.949709Z","shell.execute_reply.started":"2021-07-30T05:57:54.94312Z","shell.execute_reply":"2021-07-30T05:57:54.948413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def aug_fn(image, img_size):\n    data = {\"image\":image}\n    aug_data = transforms(**data)\n    aug_img = aug_data[\"image\"]\n    aug_img = tf.cast(aug_img, tf.float32)\n    aug_img = tf.image.resize(aug_img, size=[img_size, img_size], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n    return aug_img\n\ndef process_data(image, img_size):\n    aug_img = tf.numpy_function(func=aug_fn, inp=[image, img_size], Tout=tf.float32)\n    return aug_img\n\ndef random_jitter(image):\n    image = tf.image.resize(image, [286, 286], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n    image = tf.image.random_crop(image, size=[256,256, 3])\n    image = tf.image.random_flip_left_right(image)\n    image = tf.image.random_saturation(image, 0.7, 1.2)\n    return image\n\nBATCH_SIZE = 30\nBUFFER_SIZE = 1000\n\nprint(\"Batch Size:\", BATCH_SIZE)\nprint(\"Buffer Size:\", BUFFER_SIZE)\n\ndef normalize(image):\n    image = tf.image.resize(image, [256,256], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n    image = tf.cast(image, tf.float32)\n    image = (image / 127.5) - 1.0\n    \n    return image\n\ndef normalize_monet(image):\n    image = (image / 127.5) - 1.0\n    return image\n\nmonet_dataset = list(map(parse_img, monet_dataset_paths))\nmonet_dataset = tf.data.Dataset.from_tensor_slices(monet_dataset)\nmonet_dataset = monet_dataset.map(partial(process_data, img_size=256), num_parallel_calls=AUTOTUNE)\nmonet_dataset = monet_dataset.map(normalize_monet, num_parallel_calls=AUTOTUNE)\nmonet_dataset = monet_dataset.shuffle(BUFFER_SIZE)\nmonet_dataset = monet_dataset.batch(BATCH_SIZE)\nmonet_dataset = monet_dataset.prefetch(AUTOTUNE)\n\n# monet_dataset = map(parse_img, monet_jpg_files)\n# monet_dataset_list = list(map(random_jitter_monet, monet_dataset))\n\n# monet_dataset = tf.data.Dataset.from_tensor_slices(monet_dataset_list)\n# monet_dataset = monet_dataset.map(normalize, num_parallel_calls=AUTOTUNE)\n# monet_dataset = monet_dataset.repeat()\n# monet_dataset = monet_dataset.shuffle(BUFFER_SIZE)\n# monet_dataset = monet_dataset.batch(BATCH_SIZE)\n# monet_dataset = monet_dataset.prefetch(AUTOTUNE)","metadata":{"execution":{"iopub.status.busy":"2021-07-30T05:57:54.95148Z","iopub.execute_input":"2021-07-30T05:57:54.951894Z","iopub.status.idle":"2021-07-30T05:57:59.704186Z","shell.execute_reply.started":"2021-07-30T05:57:54.951853Z","shell.execute_reply":"2021-07-30T05:57:59.703388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"photo_dataset = tf.data.TFRecordDataset(photo_tfrec_files)\nphoto_dataset = photo_dataset.map(parse_tfrecord, num_parallel_calls=AUTOTUNE)\n# photo_dataset = photo_dataset.cache(\"/kaggle/tmp/photo\")\nphoto_dataset = photo_dataset.map(random_jitter, num_parallel_calls=AUTOTUNE)\nphoto_dataset = photo_dataset.map(normalize, num_parallel_calls=AUTOTUNE)\nphoto_dataset = photo_dataset.shuffle(BUFFER_SIZE)\nphoto_dataset = photo_dataset.batch(BATCH_SIZE)\nphoto_dataset = photo_dataset.prefetch(AUTOTUNE)","metadata":{"execution":{"iopub.status.busy":"2021-07-30T05:57:59.706042Z","iopub.execute_input":"2021-07-30T05:57:59.706509Z","iopub.status.idle":"2021-07-30T05:57:59.771672Z","shell.execute_reply.started":"2021-07-30T05:57:59.706473Z","shell.execute_reply":"2021-07-30T05:57:59.770918Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def downsample(filters, size, apply_batchnorm=True):\n    initializer = tf.random_normal_initializer(0., 0.02)\n    \n    result = tf.keras.Sequential()\n    result.add(tf.keras.layers.Conv2D(filters, size, strides=2, padding='same',kernel_initializer=initializer, use_bias=False))\n    \n    if apply_batchnorm:\n        result.add(tf.keras.layers.BatchNormalization())\n\n    result.add(tf.keras.layers.LeakyReLU())\n    return result","metadata":{"execution":{"iopub.status.busy":"2021-07-30T05:57:59.772935Z","iopub.execute_input":"2021-07-30T05:57:59.773286Z","iopub.status.idle":"2021-07-30T05:57:59.77919Z","shell.execute_reply.started":"2021-07-30T05:57:59.773249Z","shell.execute_reply":"2021-07-30T05:57:59.778268Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def upsample(filters, size, apply_dropout=False):\n    initializer = tf.random_normal_initializer(0., 0.02)\n    \n    result = tf.keras.Sequential()\n    result.add(\n        tf.keras.layers.Conv2DTranspose(filters, size, strides=2,\n                                    padding='same',\n                                    kernel_initializer=initializer,\n                                    use_bias=False))\n    \n    result.add(tf.keras.layers.BatchNormalization())\n    \n    if apply_dropout:\n        result.add(tf.keras.layers.Dropout(0.5))\n        \n    result.add(tf.keras.layers.ReLU())\n    \n    return result","metadata":{"execution":{"iopub.status.busy":"2021-07-30T05:57:59.780726Z","iopub.execute_input":"2021-07-30T05:57:59.781077Z","iopub.status.idle":"2021-07-30T05:57:59.7898Z","shell.execute_reply.started":"2021-07-30T05:57:59.781039Z","shell.execute_reply":"2021-07-30T05:57:59.788857Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def Discriminator():\n    initializer = tf.random_normal_initializer(0., 0.02)\n    \n    inp = tf.keras.layers.Input(shape=[256, 256, 3], name='input_image')\n    \n    down1 = downsample(64, 4, False)(inp) # (bs, 128, 128, 64)\n    down2 = downsample(128, 4)(down1) # (bs, 64, 64, 128)\n    down3 = downsample(256, 4)(down2) # (bs, 32, 32, 256)\n    \n    zero_pad1 = tf.keras.layers.ZeroPadding2D()(down3) # (bs, 34, 34, 256)\n    conv = tf.keras.layers.Conv2D(512, 4, strides=1,\n                                  kernel_initializer=initializer,\n                                  use_bias=False)(zero_pad1) # (bs, 31, 31, 512)\n    \n    batchnorm1 = tf.keras.layers.BatchNormalization()(conv)\n    \n    leaky_relu = tf.keras.layers.LeakyReLU()(batchnorm1)\n    \n    zero_pad2 = tf.keras.layers.ZeroPadding2D()(leaky_relu) # (bs, 33, 33, 512)\n    \n    last = tf.keras.layers.Conv2D(1, 4, strides=1,\n                                  kernel_initializer=initializer)(zero_pad2) # (bs, 30, 30, 1)\n    \n    return tf.keras.Model(inputs=[inp], outputs=last)","metadata":{"execution":{"iopub.status.busy":"2021-07-30T05:57:59.791573Z","iopub.execute_input":"2021-07-30T05:57:59.792346Z","iopub.status.idle":"2021-07-30T05:57:59.801727Z","shell.execute_reply.started":"2021-07-30T05:57:59.792303Z","shell.execute_reply":"2021-07-30T05:57:59.800907Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def Generator():\n    inputs = tf.keras.layers.Input(shape=[256,256,3])\n    \n    down_stack = [\n        downsample(64, 4, apply_batchnorm=False), # (bs, 128, 128, 64)\n        downsample(128, 4), # (bs, 64, 64, 128)\n        downsample(256, 4), # (bs, 32, 32, 256)\n        downsample(512, 4), # (bs, 16, 16, 512)\n        downsample(512, 4), # (bs, 8, 8, 512)\n        downsample(512, 4), # (bs, 4, 4, 512)\n        downsample(512, 4), # (bs, 2, 2, 512)\n        downsample(512, 4), # (bs, 1, 1, 512)\n    ]\n    \n    up_stack = [\n        upsample(512, 4, apply_dropout=True), # (bs, 2, 2, 1024)\n        upsample(512, 4, apply_dropout=True), # (bs, 4, 4, 1024)\n        upsample(512, 4, apply_dropout=True), # (bs, 8, 8, 1024)\n        upsample(512, 4), # (bs, 16, 16, 1024)\n        upsample(256, 4), # (bs, 32, 32, 512)\n        upsample(128, 4), # (bs, 64, 64, 256)\n        upsample(64, 4), # (bs, 128, 128, 128)\n    ]\n    \n    initializer = tf.random_normal_initializer(0., 0.02)\n    last = tf.keras.layers.Conv2DTranspose(3, 4,\n                                         strides=2,\n                                         padding='same',\n                                         kernel_initializer=initializer,\n                                         activation='tanh') # (bs, 256, 256, 3)\n    \n    x = inputs\n    \n    # Downsampling through the model\n    skips = []\n    for down in down_stack:\n        x = down(x)\n        skips.append(x)\n        \n    skips = reversed(skips[:-1])\n    \n    # Upsampling and establishing the skip connections\n    for up, skip in zip(up_stack, skips):\n        x = up(x)\n        x = tf.keras.layers.Concatenate()([x, skip])\n        \n    x = last(x)\n    \n    return tf.keras.Model(inputs=inputs, outputs=x)","metadata":{"execution":{"iopub.status.busy":"2021-07-30T05:57:59.80306Z","iopub.execute_input":"2021-07-30T05:57:59.803436Z","iopub.status.idle":"2021-07-30T05:57:59.815492Z","shell.execute_reply.started":"2021-07-30T05:57:59.8034Z","shell.execute_reply":"2021-07-30T05:57:59.81451Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with strategy.scope():\n    # Instantiate generators\n    G_PtoM = Generator()\n    G_MtoP = Generator()\n    # Instantiate discriminators\n    D_P = Discriminator()\n    D_M = Discriminator()","metadata":{"execution":{"iopub.status.busy":"2021-07-30T05:57:59.817191Z","iopub.execute_input":"2021-07-30T05:57:59.8176Z","iopub.status.idle":"2021-07-30T05:58:01.538477Z","shell.execute_reply.started":"2021-07-30T05:57:59.817562Z","shell.execute_reply":"2021-07-30T05:58:01.537548Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.keras.utils.plot_model(G_PtoM.layers[1], dpi=64, to_file=\"downsample.png\", show_layer_names=False)","metadata":{"execution":{"iopub.status.busy":"2021-07-30T05:58:01.539871Z","iopub.execute_input":"2021-07-30T05:58:01.540379Z","iopub.status.idle":"2021-07-30T05:58:01.736993Z","shell.execute_reply.started":"2021-07-30T05:58:01.540333Z","shell.execute_reply":"2021-07-30T05:58:01.73597Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.keras.utils.plot_model(G_PtoM.layers[-3], dpi=64, to_file=\"upsample.png\", show_layer_names=False)","metadata":{"execution":{"iopub.status.busy":"2021-07-30T05:58:01.738879Z","iopub.execute_input":"2021-07-30T05:58:01.73933Z","iopub.status.idle":"2021-07-30T05:58:01.928911Z","shell.execute_reply.started":"2021-07-30T05:58:01.739284Z","shell.execute_reply":"2021-07-30T05:58:01.928003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.keras.utils.plot_model(G_PtoM, show_shapes=True, dpi=64, to_file=\"generator.png\")","metadata":{"execution":{"iopub.status.busy":"2021-07-30T05:58:01.93063Z","iopub.execute_input":"2021-07-30T05:58:01.931033Z","iopub.status.idle":"2021-07-30T05:58:02.337064Z","shell.execute_reply.started":"2021-07-30T05:58:01.930991Z","shell.execute_reply":"2021-07-30T05:58:02.336129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.keras.utils.plot_model(D_P, show_shapes=True, dpi=64, to_file=\"discriminator.png\")","metadata":{"execution":{"iopub.status.busy":"2021-07-30T05:58:02.338793Z","iopub.execute_input":"2021-07-30T05:58:02.339245Z","iopub.status.idle":"2021-07-30T05:58:02.564706Z","shell.execute_reply.started":"2021-07-30T05:58:02.339195Z","shell.execute_reply":"2021-07-30T05:58:02.56386Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"LAMBDA = 10","metadata":{"execution":{"iopub.status.busy":"2021-07-30T05:58:02.566388Z","iopub.execute_input":"2021-07-30T05:58:02.566753Z","iopub.status.idle":"2021-07-30T05:58:02.573094Z","shell.execute_reply.started":"2021-07-30T05:58:02.566715Z","shell.execute_reply":"2021-07-30T05:58:02.572303Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with strategy.scope():\n    loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.SUM)","metadata":{"execution":{"iopub.status.busy":"2021-07-30T05:58:02.575979Z","iopub.execute_input":"2021-07-30T05:58:02.576281Z","iopub.status.idle":"2021-07-30T05:58:02.58293Z","shell.execute_reply.started":"2021-07-30T05:58:02.576251Z","shell.execute_reply":"2021-07-30T05:58:02.581974Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def discriminator_loss(real, generated):\n    real_loss = loss_object(tf.ones_like(real), real)\n    generated_loss = loss_object(tf.zeros_like(generated), generated)\n    total_disc_loss = real_loss + generated_loss\n    total_disc_loss /= len(real)\n    \n    return total_disc_loss * 0.5","metadata":{"execution":{"iopub.status.busy":"2021-07-30T05:58:02.584131Z","iopub.execute_input":"2021-07-30T05:58:02.584614Z","iopub.status.idle":"2021-07-30T05:58:02.594696Z","shell.execute_reply.started":"2021-07-30T05:58:02.584581Z","shell.execute_reply":"2021-07-30T05:58:02.593671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generator_loss(generated):\n    return loss_object(tf.ones_like(generated), generated)/len(generated)","metadata":{"execution":{"iopub.status.busy":"2021-07-30T05:58:02.596394Z","iopub.execute_input":"2021-07-30T05:58:02.597019Z","iopub.status.idle":"2021-07-30T05:58:02.604669Z","shell.execute_reply.started":"2021-07-30T05:58:02.596976Z","shell.execute_reply":"2021-07-30T05:58:02.603809Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def calc_cycle_loss(real_image, cycled_image): \n        return LAMBDA * tf.reduce_mean(tf.abs(real_image - cycled_image))","metadata":{"execution":{"iopub.status.busy":"2021-07-30T05:58:02.605918Z","iopub.execute_input":"2021-07-30T05:58:02.606705Z","iopub.status.idle":"2021-07-30T05:58:02.615186Z","shell.execute_reply.started":"2021-07-30T05:58:02.606183Z","shell.execute_reply":"2021-07-30T05:58:02.614316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def identity_loss(real_image, same_image):\n    return LAMBDA * 0.5 * tf.reduce_mean(tf.abs(real_image - same_image))","metadata":{"execution":{"iopub.status.busy":"2021-07-30T05:58:02.616395Z","iopub.execute_input":"2021-07-30T05:58:02.616721Z","iopub.status.idle":"2021-07-30T05:58:02.62551Z","shell.execute_reply.started":"2021-07-30T05:58:02.616665Z","shell.execute_reply":"2021-07-30T05:58:02.624565Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with strategy.scope():\n    G_MtoP_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n    G_PtoM_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n\n    D_M_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n    D_P_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)","metadata":{"execution":{"iopub.status.busy":"2021-07-30T05:58:02.626685Z","iopub.execute_input":"2021-07-30T05:58:02.626971Z","iopub.status.idle":"2021-07-30T05:58:02.648429Z","shell.execute_reply.started":"2021-07-30T05:58:02.626947Z","shell.execute_reply":"2021-07-30T05:58:02.647659Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_images(model, model_r, test_input, figsize=(12,12)):\n    prediction = model(test_input)\n    reconstruction = model_r(prediction)\n    identity = model_r(test_input)\n    \n    display_list = [test_input[0], prediction[0], reconstruction[0], identity[0]]\n    plt.figure(figsize=figsize)\n    title = ['Input', 'Predicted', 'Reconstructed', 'Identity']\n\n    for i in range(4):\n        plt.subplot(1, 4, i+1)\n        plt.title(title[i])\n        # getting the pixel values between [0, 1] to plot it.\n        plt.imshow(display_list[i] * 0.5 + 0.5)\n        plt.axis('off')\n\n    plt.show()\n    \n    return display_list","metadata":{"execution":{"iopub.status.busy":"2021-07-30T05:58:02.65046Z","iopub.execute_input":"2021-07-30T05:58:02.650791Z","iopub.status.idle":"2021-07-30T05:58:02.659478Z","shell.execute_reply.started":"2021-07-30T05:58:02.650759Z","shell.execute_reply":"2021-07-30T05:58:02.658647Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for images in monet_dataset.take(400):\n    generate_images(G_MtoP, G_PtoM, images)","metadata":{"execution":{"iopub.status.busy":"2021-07-30T05:58:02.661037Z","iopub.execute_input":"2021-07-30T05:58:02.661426Z","iopub.status.idle":"2021-07-30T05:58:03.247216Z","shell.execute_reply.started":"2021-07-30T05:58:02.661389Z","shell.execute_reply":"2021-07-30T05:58:03.246342Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with strategy.scope():\n    G_MtoP_loss = tf.keras.metrics.Mean(name='G_MtoP_loss')\n    G_PtoM_loss = tf.keras.metrics.Mean(name='G_PtoM_loss')\n    D_M_loss = tf.keras.metrics.Mean(name='D_M_loss')\n    D_P_loss = tf.keras.metrics.Mean(name='D_P_loss')","metadata":{"execution":{"iopub.status.busy":"2021-07-30T05:58:03.249089Z","iopub.execute_input":"2021-07-30T05:58:03.249737Z","iopub.status.idle":"2021-07-30T05:58:03.281551Z","shell.execute_reply.started":"2021-07-30T05:58:03.249691Z","shell.execute_reply":"2021-07-30T05:58:03.2808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_step(real_m, real_p):\n    \n    with tf.GradientTape(persistent=True) as tape:\n        \n        # G_PtoM translates P -> M\n        # G_MtoP translates M -> M\n        \n        fake_m = G_PtoM(real_p, training=True)\n        cycled_p = G_MtoP(fake_m, training=True)\n        \n        fake_p = G_MtoP(real_m, training=True)\n        cycled_m = G_PtoM(fake_p, training=True)\n        \n        # same_m and same_p for identity loss\n        same_m = G_PtoM(real_m, training=True)\n        same_p = G_MtoP(real_p, training=True)\n        \n        # disctiminator outputs\n        disc_real_m = D_M(real_m, training=True)\n        disc_real_p = D_P(real_p, training=True)\n        \n        disc_fake_m = D_M(fake_m, training=True)\n        disc_fake_p = D_P(fake_p, training=True)\n        \n        # Calculate Loss\n        gen_MtoP_loss = generator_loss(disc_fake_p)\n        gen_PtoM_loss = generator_loss(disc_fake_m)\n        \n        total_cycle_loss = calc_cycle_loss(real_m, cycled_m) + calc_cycle_loss(real_p, cycled_p)\n        \n        identity_loss_p = identity_loss(real_p, same_p)\n        identity_loss_m = identity_loss(real_m, same_m)\n        \n        # Total Loss\n        total_gen_MtoP_loss = gen_MtoP_loss + total_cycle_loss + identity_loss_p\n        total_gen_PtoM_loss = gen_PtoM_loss + total_cycle_loss + identity_loss_m\n        \n        disc_p_loss = discriminator_loss(disc_real_p, disc_fake_p)\n        disc_m_loss = discriminator_loss(disc_real_m, disc_fake_m)\n        \n    \n    # Calculate Gradients\n    gen_mtop_gradients = tape.gradient(total_gen_MtoP_loss, G_MtoP.trainable_variables)\n    gen_ptom_gradients = tape.gradient(total_gen_PtoM_loss, G_PtoM.trainable_variables)\n    disc_m_gradients = tape.gradient(disc_m_loss, D_M.trainable_variables)\n    disc_p_gradients= tape.gradient(disc_p_loss, D_P.trainable_variables)\n    \n    # Apply Gradients to optimizers\n    G_MtoP_optimizer.apply_gradients(zip(gen_mtop_gradients, G_MtoP.trainable_variables))\n    G_PtoM_optimizer.apply_gradients(zip(gen_ptom_gradients, G_PtoM.trainable_variables))\n    D_M_optimizer.apply_gradients(zip(disc_m_gradients, D_M.trainable_variables))\n    D_P_optimizer.apply_gradients(zip(disc_p_gradients, D_P.trainable_variables))\n    \n    # Update Running Loss\n    D_M_loss.update_state(disc_m_loss)\n    D_P_loss.update_state(disc_p_loss)\n    G_MtoP_loss.update_state(total_gen_MtoP_loss)\n    G_PtoM_loss.update_state(total_gen_PtoM_loss)","metadata":{"execution":{"iopub.status.busy":"2021-07-30T05:58:03.282845Z","iopub.execute_input":"2021-07-30T05:58:03.283209Z","iopub.status.idle":"2021-07-30T05:58:03.29473Z","shell.execute_reply.started":"2021-07-30T05:58:03.28317Z","shell.execute_reply":"2021-07-30T05:58:03.293683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@tf.function\ndef distributed_train_step(real_m, real_p):\n    strategy.run(train_step, args=(real_m, real_p))","metadata":{"execution":{"iopub.status.busy":"2021-07-30T05:58:03.296096Z","iopub.execute_input":"2021-07-30T05:58:03.296525Z","iopub.status.idle":"2021-07-30T05:58:03.571126Z","shell.execute_reply.started":"2021-07-30T05:58:03.29648Z","shell.execute_reply":"2021-07-30T05:58:03.570167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sample_m = next(iter(monet_dataset))\n# sample_p = next(iter(photo_dataset))\n\n# ptom_preds_images = []\n# mtop_reconstructions_images = []\n# ptop_identity_images = []\n\n# mtop_preds_images = []\n# ptom_reconstructions_images = []\n# mtom_identity_images = []\n\nEPOCHS = 500\n\nfor epoch in range(EPOCHS):\n    \n#     clear_output()\n#     pmp_display_list = generate_images(G_PtoM, G_MtoP, sample_p)\n#     ptom_preds_images.append(pmp_display_list[1])\n#     mtop_reconstructions_images.append(pmp_display_list[2])\n#     ptop_identity_images.append(pmp_display_list[3])\n#     \n#     mpm_display_list = generate_images(G_MtoP, G_PtoM, sample_m)\n#     mtop_preds_images.append(mpm_display_list[1])\n#     ptom_reconstructions_images.append(mpm_display_list[2])\n#     mtom_identity_images.append(mpm_display_list[3])\n    \n    G_MtoP_loss.reset_states()\n    G_PtoM_loss.reset_states()\n    D_M_loss.reset_states()\n    D_P_loss.reset_states()\n    \n    ds = tf.data.Dataset.zip((photo_dataset, monet_dataset))\n    with tqdm(ds, desc=\"Epoch {}/{}\".format(epoch+1, EPOCHS)) as t:\n        for image_p, image_m in t:\n            distributed_train_step(image_m, image_p)\n            t.set_description(\n                f\"Epoch: {epoch+1}/{EPOCHS}, \"\n                f\"G_MtoP: {G_MtoP_loss.result():.4f}, \"\n                f\"G_PtoM: {G_PtoM_loss.result():.4f}, \"\n                f\"D_M: {D_M_loss.result():.4f}, \"\n                f\"D_P: {D_P_loss.result():.4f}\"\n            )","metadata":{"execution":{"iopub.status.busy":"2021-07-30T05:58:03.576084Z","iopub.execute_input":"2021-07-30T05:58:03.576418Z","iopub.status.idle":"2021-07-30T07:05:41.685637Z","shell.execute_reply.started":"2021-07-30T05:58:03.576387Z","shell.execute_reply":"2021-07-30T07:05:41.68467Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_photo_dataset = tf.data.TFRecordDataset(photo_tfrec_files)\ntest_photo_dataset = test_photo_dataset.map(parse_tfrecord, num_parallel_calls=AUTOTUNE)\ntest_photo_dataset = test_photo_dataset.map(normalize, num_parallel_calls=AUTOTUNE)\ntest_photo_dataset = test_photo_dataset.batch(BATCH_SIZE)\n# test_photo_dataset = photo_dataset","metadata":{"execution":{"iopub.status.busy":"2021-07-30T07:05:41.69041Z","iopub.execute_input":"2021-07-30T07:05:41.692622Z","iopub.status.idle":"2021-07-30T07:05:41.908803Z","shell.execute_reply.started":"2021-07-30T07:05:41.692566Z","shell.execute_reply":"2021-07-30T07:05:41.907844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(20,20))\nfor images in test_photo_dataset.take(1):\n    predictions = G_PtoM(images, training=False)\n    for i in range(len(images)):\n        plt.subplot(8,8, 2*i+1)\n        plt.imshow((images[i]+1)/2)\n        plt.title(\"Photo\")\n        plt.axis('off')\n\n        plt.subplot(8,8, 2*i+2)\n        plt.imshow((predictions[i]+1)/2)\n        plt.title(\"To Monet\")\n        plt.axis('off')","metadata":{"execution":{"iopub.status.busy":"2021-07-30T07:05:41.913455Z","iopub.execute_input":"2021-07-30T07:05:41.915829Z","iopub.status.idle":"2021-07-30T07:05:46.315365Z","shell.execute_reply.started":"2021-07-30T07:05:41.915775Z","shell.execute_reply":"2021-07-30T07:05:46.314161Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_image(arr):\n    arr = (arr + 1) * 127.5\n    arr = tf.cast(arr, tf.int8)\n    return tf.keras.preprocessing.image.array_to_img(arr)","metadata":{"execution":{"iopub.status.busy":"2021-07-30T07:05:46.316793Z","iopub.execute_input":"2021-07-30T07:05:46.317347Z","iopub.status.idle":"2021-07-30T07:05:46.322233Z","shell.execute_reply.started":"2021-07-30T07:05:46.3173Z","shell.execute_reply":"2021-07-30T07:05:46.321157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from zipfile import ZipFile\n\nfile_index = 1\nwith ZipFile('images.zip', 'w') as submission_zip:\n    for images in tqdm(test_photo_dataset):\n        predictions = G_PtoM.predict(images)\n        for prediction in predictions:\n            filename = f'photo_to_monet_{file_index}.jpg'\n            img = get_image(prediction)\n            img.save(filename)\n            submission_zip.write(filename)\n            os.remove(filename)\n            file_index+=1\n        if file_index>=9950:\n            break\n    submission_zip.close()\n    \nprint(f\"Saved {file_index} files in images.zip\")","metadata":{"execution":{"iopub.status.busy":"2021-07-30T07:26:41.106559Z","iopub.execute_input":"2021-07-30T07:26:41.106931Z","iopub.status.idle":"2021-07-30T07:28:06.749554Z","shell.execute_reply.started":"2021-07-30T07:26:41.106896Z","shell.execute_reply":"2021-07-30T07:28:06.746347Z"},"trusted":true},"execution_count":null,"outputs":[]}]}