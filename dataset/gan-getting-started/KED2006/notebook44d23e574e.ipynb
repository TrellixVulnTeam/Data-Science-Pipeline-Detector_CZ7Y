{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Загрузка данных:","metadata":{"id":"ngdpeC1HqlBm"}},{"cell_type":"code","source":"#from google.colab import files\n#files.upload()\n\n#!mkdir -p ~/.kaggle\n#!cp kaggle.json ~/.kaggle/\n#!pip install kaggle\n#!chmod 600 /root/.kaggle/kaggle.json\n#!kaggle competitions download -c gan-getting-started\n#!unzip /content/gan-getting-started.zip","metadata":{"id":"7UladDi9qQIJ","outputId":"ca455de2-25d7-4cfe-c59f-78593814d1d6","execution":{"iopub.status.busy":"2022-05-01T09:46:11.151072Z","iopub.execute_input":"2022-05-01T09:46:11.151487Z","iopub.status.idle":"2022-05-01T09:46:11.155624Z","shell.execute_reply.started":"2022-05-01T09:46:11.151447Z","shell.execute_reply":"2022-05-01T09:46:11.154561Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms","metadata":{"id":"_1-MpZE2qqQm","execution":{"iopub.status.busy":"2022-05-01T09:46:11.157245Z","iopub.execute_input":"2022-05-01T09:46:11.158633Z","iopub.status.idle":"2022-05-01T09:46:11.166146Z","shell.execute_reply.started":"2022-05-01T09:46:11.158596Z","shell.execute_reply":"2022-05-01T09:46:11.165237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nfrom PIL import Image\n\nimport random\n\nclass PairImageDataset(Dataset):\n    def __init__(self, transforms):\n        super().__init__()\n        self.path_monet = '../input/gan-getting-started/monet_jpg'\n        self.path_photoes = '../input/gan-getting-started/photo_jpg'\n        self.transforms = transforms\n\n        filenames_monet = []\n        for root, dirs, filenames in os.walk(self.path_monet):\n            for file_name in filenames:\n              if file_name.endswith('.jpg'):\n                filenames_monet.append(os.path.join( self.path_monet, file_name))\n\n        filenames_photoes = []\n        for root, dirs, filenames in os.walk(self.path_photoes):\n            for file_name in filenames:\n              if file_name.endswith('.jpg'):\n                filenames_photoes.append(os.path.join(self.path_photoes, file_name))\n        \n        self.images_monet = []\n        self.images_photoes = []\n        for file_name in filenames_monet:\n            with Image.open(file_name) as img:\n                self.images_monet.append(img.copy())\n        for file_name in filenames_photoes:\n            with Image.open(file_name) as img:\n                self.images_photoes.append(img.copy())\n\n    def __len__(self):\n        return min(len(self.images_monet) , len(self.images_photoes))\n\n    def __getitem__(self, index):\n        index_photo = index\n        index_photo = random.randint(0, len(self.images_photoes) - 1)\n        index_monet = index\n        #print(self.images_photoes[index_photo])\n        #print(self.transforms)\n        return self.transforms(self.images_photoes[index_photo]), self.transforms(self.images_monet[index_monet])\n        ","metadata":{"id":"yyi7MNqEr7YP","execution":{"iopub.status.busy":"2022-05-01T09:46:11.172092Z","iopub.execute_input":"2022-05-01T09:46:11.172334Z","iopub.status.idle":"2022-05-01T09:46:11.187464Z","shell.execute_reply.started":"2022-05-01T09:46:11.172302Z","shell.execute_reply":"2022-05-01T09:46:11.186569Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transformer = transforms.Compose([\n    transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n])","metadata":{"id":"QB4eojdjw02E","execution":{"iopub.status.busy":"2022-05-01T09:46:11.18947Z","iopub.execute_input":"2022-05-01T09:46:11.190053Z","iopub.status.idle":"2022-05-01T09:46:11.195921Z","shell.execute_reply.started":"2022-05-01T09:46:11.190016Z","shell.execute_reply":"2022-05-01T09:46:11.195017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = PairImageDataset(transformer)","metadata":{"id":"vPiFsKRjxIcJ","execution":{"iopub.status.busy":"2022-05-01T09:46:11.197384Z","iopub.execute_input":"2022-05-01T09:46:11.197864Z","iopub.status.idle":"2022-05-01T09:46:23.583076Z","shell.execute_reply.started":"2022-05-01T09:46:11.197829Z","shell.execute_reply":"2022-05-01T09:46:23.582341Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nimage_photo, image_monet = dataset[0]\nplt.imshow(image_photo.permute(1, 2, 0))\nplt.show()\nplt.imshow(image_monet.permute(1, 2, 0))\nplt.show()","metadata":{"id":"9tbmChgBxQoS","outputId":"1754d6bc-be0c-42ef-ccc8-2fa5323c0d07","execution":{"iopub.status.busy":"2022-05-01T09:46:23.585628Z","iopub.execute_input":"2022-05-01T09:46:23.5859Z","iopub.status.idle":"2022-05-01T09:46:23.982989Z","shell.execute_reply.started":"2022-05-01T09:46:23.58586Z","shell.execute_reply":"2022-05-01T09:46:23.982349Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataloader = DataLoader(dataset, batch_size = 5, num_workers = 0)","metadata":{"id":"K23GevW9x9Cr","execution":{"iopub.status.busy":"2022-05-01T09:46:23.985532Z","iopub.execute_input":"2022-05-01T09:46:23.986293Z","iopub.status.idle":"2022-05-01T09:46:24.002642Z","shell.execute_reply.started":"2022-05-01T09:46:23.986252Z","shell.execute_reply":"2022-05-01T09:46:24.00187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')","metadata":{"id":"IBASUTD8mets","execution":{"iopub.status.busy":"2022-05-01T09:46:24.004223Z","iopub.execute_input":"2022-05-01T09:46:24.005112Z","iopub.status.idle":"2022-05-01T09:46:24.013193Z","shell.execute_reply.started":"2022-05-01T09:46:24.005069Z","shell.execute_reply":"2022-05-01T09:46:24.012293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(dataset.images_monet))\nprint(len(dataset.images_photoes))","metadata":{"id":"tlOnY8acKZr_","outputId":"ec09de15-19aa-446d-a608-3c2087792b48","execution":{"iopub.status.busy":"2022-05-01T09:46:24.014776Z","iopub.execute_input":"2022-05-01T09:46:24.015569Z","iopub.status.idle":"2022-05-01T09:46:24.023945Z","shell.execute_reply.started":"2022-05-01T09:46:24.015528Z","shell.execute_reply":"2022-05-01T09:46:24.022851Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import itertools\n\nclass CycleGAN:\n    def __init__(self, \n                 generator_monet2photo,\n                 generator_photo2monet,\n                 descriminator_photo,\n                 descriminator_monet):\n        self.generator_monet2photo = generator_monet2photo\n        self.generator_photo2monet = generator_photo2monet\n        self.descriminator_monet = descriminator_monet\n        self.descriminator_photo = descriminator_photo\n\n        self.descriminator_loss = nn.MSELoss()\n        self.image_loss = nn.L1Loss()\n\n        self.optimizer_generator_monet = torch.optim.Adam(\n            self.generator_monet2photo.parameters(), \n            lr = 1e-3, betas = (0.5, 0.996))\n        self.optimizer_generator_photo = torch.optim.Adam(\n            self.generator_photo2monet.parameters(), \n            lr = 1e-3, betas = (0.5, 0.996))\n\n        self.optimizer_descriminator_monet = torch.optim.Adam(self.descriminator_monet.parameters(), lr = 1e-3, betas = (0.5, 0.996))\n        self.optimizer_descriminator_photo = torch.optim.Adam(self.descriminator_photo.parameters(), lr = 1e-3, betas = (0.5, 0.996))\n\n    def fit_one_batch(self, photoes, monets, alpha):\n\n        self.optimizer_generator_photo.zero_grad()\n        self.optimizer_generator_monet.zero_grad()\n        #generator_photo2monet loss\n\n        ones = torch.ones((monets.shape[0], 1 * 16 * 16)).to(device)\n        zeros = torch.zeros((monets.shape[0], 1 * 16 * 16)).to(device)\n        \n        fake_monet = self.generator_photo2monet(photoes)\n        fake_photo = self.generator_monet2photo(monets)\n        \n        reconstructed_photo = self.generator_monet2photo(fake_monet)\n        reconstructed_monet = self.generator_photo2monet(fake_photo)\n        \n        loss_gen = 2 * (self.image_loss(fake_monet, photoes) + self.image_loss(fake_photo, monets)) / 2 \n        \n        loss_gen += (self.descriminator_loss(self.descriminator_monet(fake_monet), ones) + \\\n                     self.descriminator_loss(self.descriminator_photo(fake_photo), ones)) / 2\n        \n        \n        loss_gen += 9 * (self.image_loss(reconstructed_photo, photoes) +\\\n                     self.image_loss(reconstructed_monet, monets)) / 2 \n        \n        loss_gen.backward()\n        self.optimizer_generator_photo.step()\n        self.optimizer_generator_monet.step()\n\n        #descriminator_monet loss\n\n        self.optimizer_descriminator_monet.zero_grad()\n\n        real_monets_score = self.descriminator_monet(monets)\n        target_real_monets_score = torch.ones((monets.shape[0], 1 * 16 * 16)).to(device)\n\n        fake_monets_score = self.descriminator_monet(fake_monet.detach())\n        target_fake_monets_score = torch.zeros((fake_monet.shape[0], 1 * 16 * 16)).to(device)\n\n        loss_descriminator_monet = self.descriminator_loss(real_monets_score, target_real_monets_score) + \\\n                                   self.descriminator_loss(fake_monets_score, target_fake_monets_score)\n        loss_descriminator_monet /= 2\n        loss_descriminator_monet.backward()\n        self.optimizer_descriminator_monet.step()\n\n        #descriminator_photos loss\n        self.optimizer_descriminator_photo.zero_grad()\n\n        real_photos_score = self.descriminator_photo(photoes)\n        target_real_photos_score = torch.ones((photoes.shape[0], 1 * 16 * 16)).to(device)\n\n        fake_photos_score = self.descriminator_photo(fake_photo.detach())\n        target_fake_photos_score = torch.zeros((fake_photo.shape[0],1 * 16 * 16)).to(device)\n\n        loss_descriminator_photos = self.descriminator_loss(real_photos_score, target_real_photos_score) + \\\n                                    self.descriminator_loss(fake_photos_score, target_fake_photos_score)\n        loss_descriminator_photos /= 2\n\n        loss_descriminator_photos.backward()\n        self.optimizer_descriminator_photo.step()\n\n    def predict(self, photo):\n        return self.generator_photo2monet(photo)\n    \n    def predict_monet(self, photo):\n        return self.generator_monet2photo(photo)\n\n    def save(self, epoch):\n        torch.save(self.generator_photo2monet, f'generator_photo2monet_{epoch}.bin')\n        #torch.save(self.generator_monet2photo, f'generator_monet2photo_{epoch}.bin')\n        #torch.save(self.descriminator_monet, f'descriminator_monet_{epoch}.bin')\n        #torch.save(self.descriminator_photo, f'descriminator_photo_{epoch}.bin')\n        \n    def pretrain_generators(self, photoes, monets):\n        \n        self.pretrain_optimizer_p2m.zero_grad()\n        fake_photoes = self.generator_photo2monet(photoes)\n        loss_photoes = self.pretrain_loss(fake_photoes, photoes)\n        loss_photoes.backward()\n        self.pretrain_optimizer_p2m.step()\n\n        self.pretrain_optimizer_m2p.zero_grad()\n        fake_monets = self.generator_monet2photo(monets)\n        loss_monets = self.pretrain_loss(fake_monets, monets)\n        loss_monets.backward()\n        self.pretrain_optimizer_m2p.step()\n\n    def load(self, path):\n        self.generator_photo2monet = torch.load(path)\n        \n    def train(self):\n        self.generator_photo2monet.train()\n        self.generator_monet2photo.train()\n        self.descriminator_monet.train()\n        self.descriminator_photo.train()\n        \n    def eval(self):\n        self.generator_photo2monet.eval()\n        self.generator_monet2photo.eval()\n        self.descriminator_monet.eval()\n        self.descriminator_photo.eval()","metadata":{"id":"lPgQx-qhBLG3","execution":{"iopub.status.busy":"2022-05-01T09:50:04.389666Z","iopub.execute_input":"2022-05-01T09:50:04.389919Z","iopub.status.idle":"2022-05-01T09:50:04.425207Z","shell.execute_reply.started":"2022-05-01T09:50:04.389891Z","shell.execute_reply":"2022-05-01T09:50:04.424357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Block(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.module = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, 3, 1, 1),\n            nn.InstanceNorm2d(out_channels),\n            nn.LeakyReLU(0.01),\n        )\n\n    def forward(self, X):\n        return self.module(X)\n\n    \nclass BlockConv2(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.model = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, 4, 2, 1),\n            nn.InstanceNorm2d(out_channels),\n            nn.LeakyReLU(0.01),\n        )\n\n    def forward(self, X):\n        return self.model(X)","metadata":{"execution":{"iopub.status.busy":"2022-05-01T09:50:05.416817Z","iopub.execute_input":"2022-05-01T09:50:05.417066Z","iopub.status.idle":"2022-05-01T09:50:05.424597Z","shell.execute_reply.started":"2022-05-01T09:50:05.41704Z","shell.execute_reply":"2022-05-01T09:50:05.423829Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class UpNet(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.model = nn.Sequential(\n            nn.Upsample(scale_factor = 2, mode='nearest'),\n            Block(in_channels, out_channels),\n            Block(out_channels, out_channels),\n        )\n    def forward(self, X):\n        return self.model(X)","metadata":{"execution":{"iopub.status.busy":"2022-05-01T09:50:06.486414Z","iopub.execute_input":"2022-05-01T09:50:06.487067Z","iopub.status.idle":"2022-05-01T09:50:06.49213Z","shell.execute_reply.started":"2022-05-01T09:50:06.48703Z","shell.execute_reply":"2022-05-01T09:50:06.491364Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ResBlock(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.model = nn.Sequential(\n            Block(in_channels, out_channels),\n            Block(out_channels, out_channels)\n        )\n    def forward(self, X):\n        return self.model(X) + X\n    \nclass ResNet(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.model = nn.Sequential(\n            Block(3, 16),\n            BlockConv2(16, 64),\n            BlockConv2(64, 128),\n            #BlockConv2(128, 128),\n            \n            ResBlock(128, 128),\n            ResBlock(128, 128),\n            ResBlock(128, 128),\n            ResBlock(128, 128),\n            ResBlock(128, 128),\n            ResBlock(128, 128),\n            ResBlock(128, 128),\n            \n            #UpNet(128, 128),\n            UpNet(128, 64),\n            UpNet(64, 16),\n            #ResBlock(64, 64),\n            nn.Conv2d(16, 3, 3, 1, 1),\n            nn.Tanh()\n        )\n    def forward(self, X):\n        return self.model(X)","metadata":{"execution":{"iopub.status.busy":"2022-05-01T09:50:06.866169Z","iopub.execute_input":"2022-05-01T09:50:06.86661Z","iopub.status.idle":"2022-05-01T09:50:06.875385Z","shell.execute_reply.started":"2022-05-01T09:50:06.866575Z","shell.execute_reply":"2022-05-01T09:50:06.874446Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class DiscriminatorRes(nn.Module):\n    def __init__(self):\n        super(DiscriminatorRes, self).__init__()\n        \n        self.model = nn.Sequential(\n            Block(3, 16),\n            BlockConv2(16, 32),#128\n            Block(32, 64), \n            BlockConv2(64, 64),#64\n            Block(64, 128),\n            BlockConv2(128, 128), # 32\n            BlockConv2(128, 128), # 16\n            nn.Conv2d(128, 1, 3, 1, 1),\n            nn.Flatten()\n        )\n        \n        self.scale_factor = 16\n    \n    def forward(self, x):\n        return self.model(x)","metadata":{"execution":{"iopub.status.busy":"2022-05-01T09:50:07.404912Z","iopub.execute_input":"2022-05-01T09:50:07.405555Z","iopub.status.idle":"2022-05-01T09:50:07.412363Z","shell.execute_reply.started":"2022-05-01T09:50:07.405494Z","shell.execute_reply":"2022-05-01T09:50:07.411378Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = CycleGAN(ResNet().to(device), \n                 ResNet().to(device),\n                 DiscriminatorRes().to(device), \n                 DiscriminatorRes().to(device))\n\n\nmodel.load('../input/last-120/generator_photo2monet_50 (2).bin')","metadata":{"id":"AKZKF7bGgvxM","execution":{"iopub.status.busy":"2022-05-01T09:50:10.423732Z","iopub.execute_input":"2022-05-01T09:50:10.424013Z","iopub.status.idle":"2022-05-01T09:50:10.509078Z","shell.execute_reply.started":"2022-05-01T09:50:10.423985Z","shell.execute_reply":"2022-05-01T09:50:10.508342Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm.auto import tqdm\nfrom torchvision.utils import make_grid\n\ndef convert_image(x):\n    return make_grid(x, nrows = x.shape[0], normalize=True)\n\nnum_epochs = 0\ni = 0\n\nfor epoch in tqdm(range(num_epochs)):\n    for photo_batch, monet_batch in tqdm(dataloader):\n        i += 1\n        alpha = 10\n        if epoch > 10:\n            alpha = 4\n        model.fit_one_batch(photo_batch.to(device), monet_batch.to(device), alpha)\n        \n        if i % 150 == 1:\n \n            fig = plt.figure(figsize=(15, 15))\n            predicted_photo = model.predict(photo_batch.to(device))\n            predicted_monet = model.predict_monet(monet_batch.to(device))\n            fig.add_subplot(1, 4, 1)\n            \n            plt.imshow(convert_image(photo_batch[0]).permute(1, 2, 0).cpu())\n\n            fig.add_subplot(1, 4, 2)\n            plt.imshow(convert_image(monet_batch[0]).permute(1, 2, 0).cpu())\n\n            fig.add_subplot(1, 4, 3)\n            plt.imshow(convert_image(predicted_photo[0]).permute(1, 2, 0).cpu().detach().numpy())\n            \n            fig.add_subplot(1, 4, 4)\n            plt.imshow(convert_image(predicted_monet[0]).permute(1, 2, 0).cpu().detach().numpy())\n            \n            plt.title(f\"epoch {epoch + 1}\")\n            plt.show()\n    if epoch % 5 == 4:\n        model.save(epoch + 1)","metadata":{"id":"09ZdxIhcjag5","outputId":"096c564d-990b-4e88-a119-832ba154ceed","execution":{"iopub.status.busy":"2022-05-01T09:50:12.056267Z","iopub.execute_input":"2022-05-01T09:50:12.056878Z","iopub.status.idle":"2022-05-01T09:55:02.990585Z","shell.execute_reply.started":"2022-05-01T09:50:12.056841Z","shell.execute_reply":"2022-05-01T09:55:02.988423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nfrom PIL import Image\n\nclass ImageDataset(Dataset):\n    def __init__(self, transforms):\n        super().__init__()\n        self.path_monet = '../input/gan-getting-started/monet_jpg'\n        self.path_photoes = '../input/gan-getting-started/photo_jpg'\n        self.transforms = transforms\n\n        filenames_monet = []\n        for root, dirs, filenames in os.walk(self.path_monet):\n            for file_name in filenames:\n                if file_name.endswith('.jpg'):\n                    filenames_monet.append(os.path.join( self.path_monet, file_name))\n\n        filenames_photoes = []\n        for root, dirs, filenames in os.walk(self.path_photoes):\n            for file_name in filenames:\n                if file_name.endswith('.jpg'):\n                    filenames_photoes.append(os.path.join(self.path_photoes, file_name))\n        \n        self.images_monet = []\n        self.images_photoes = []\n        for file_name in filenames_monet:\n            with Image.open(file_name) as img:\n                self.images_monet.append(img.copy())\n        for file_name in filenames_photoes:\n            with Image.open(file_name) as img:\n                self.images_photoes.append(img.copy())\n\n    def __len__(self):\n        return len(self.images_photoes)\n\n    def __getitem__(self, index):\n        index_monet = index \n        index_photo = index\n        #print(self.images_photoes[index_photo])\n        #print(self.transforms)\n        return self.transforms(self.images_photoes[index_photo])\n        ","metadata":{"execution":{"iopub.status.busy":"2022-05-01T09:46:24.151338Z","iopub.status.idle":"2022-05-01T09:46:24.151898Z","shell.execute_reply.started":"2022-05-01T09:46:24.151657Z","shell.execute_reply":"2022-05-01T09:46:24.151684Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transformer_ans = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n])","metadata":{"execution":{"iopub.status.busy":"2022-05-01T09:46:24.152966Z","iopub.status.idle":"2022-05-01T09:46:24.153524Z","shell.execute_reply.started":"2022-05-01T09:46:24.153283Z","shell.execute_reply":"2022-05-01T09:46:24.153309Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset_for_ans = ImageDataset(transformer_ans)","metadata":{"execution":{"iopub.status.busy":"2022-05-01T09:46:24.154579Z","iopub.status.idle":"2022-05-01T09:46:24.155127Z","shell.execute_reply.started":"2022-05-01T09:46:24.154898Z","shell.execute_reply":"2022-05-01T09:46:24.154923Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!mkdir ../images","metadata":{"execution":{"iopub.status.busy":"2022-05-01T09:46:24.156172Z","iopub.status.idle":"2022-05-01T09:46:24.156729Z","shell.execute_reply.started":"2022-05-01T09:46:24.156485Z","shell.execute_reply":"2022-05-01T09:46:24.156523Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ph_dl = DataLoader(dataset_for_ans, batch_size=1, pin_memory=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-01T09:46:24.157799Z","iopub.status.idle":"2022-05-01T09:46:24.15836Z","shell.execute_reply.started":"2022-05-01T09:46:24.158129Z","shell.execute_reply":"2022-05-01T09:46:24.158156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm.auto import tqdm\n\ntrans = transforms.ToPILImage()\n\n\nt = tqdm(ph_dl, leave=False, total=ph_dl.__len__())\nfor i, photo in enumerate(t):\n    with torch.no_grad():\n        output = model.predict(photo.to(device))\n        output = convert_image(output)\n        pred_monet = output.reshape(3, 256, 256)\n        pred_monet = pred_monet.cpu().detach()\n        if i % 400 == 0:\n            fig = plt.figure(figsize=(15, 15))\n            plt.imshow(pred_monet.permute(1,2,0))\n            plt.show()\n    #print(pred_monet.shape)\n    img = trans(pred_monet).convert(\"RGB\")\n    img.save(\"../images/\" + str(i+1) + \".jpg\")","metadata":{"execution":{"iopub.status.busy":"2022-05-01T09:46:24.159429Z","iopub.status.idle":"2022-05-01T09:46:24.159997Z","shell.execute_reply.started":"2022-05-01T09:46:24.159758Z","shell.execute_reply":"2022-05-01T09:46:24.159781Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import shutil\n\nshutil.make_archive(\"/kaggle/working/images\", 'zip', \"/kaggle/images\")\n","metadata":{"execution":{"iopub.status.busy":"2022-05-01T09:46:24.161065Z","iopub.status.idle":"2022-05-01T09:46:24.161621Z","shell.execute_reply.started":"2022-05-01T09:46:24.161372Z","shell.execute_reply":"2022-05-01T09:46:24.161395Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}