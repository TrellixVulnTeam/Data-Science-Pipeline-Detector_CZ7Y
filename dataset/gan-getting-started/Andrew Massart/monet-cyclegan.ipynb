{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Imports","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport re\nimport PIL\nimport os\nimport shutil\n\nfrom kaggle_datasets import KaggleDatasets\nfrom random import random\nfrom tensorflow import keras\nfrom tensorflow.keras import layers, Model, losses, optimizers\nfrom tensorflow.keras.initializers import RandomNormal\nfrom tensorflow.keras.layers import Activation, Concatenate\nfrom tensorflow_addons.layers import InstanceNormalization","metadata":{"execution":{"iopub.status.busy":"2022-03-15T04:57:13.091317Z","iopub.execute_input":"2022-03-15T04:57:13.091642Z","iopub.status.idle":"2022-03-15T04:57:14.916083Z","shell.execute_reply.started":"2022-03-15T04:57:13.091586Z","shell.execute_reply":"2022-03-15T04:57:14.91524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Constants\n\n---\n","metadata":{}},{"cell_type":"code","source":"HEIGHT = 256\nWIDTH = 256\nBATCH_SIZE = 1\nCHANNELS = 3\nLAMBDA = 10\nEPOCHS = 250\n\nGCS_PATH = KaggleDatasets().get_gcs_path('gan-getting-started')","metadata":{"execution":{"iopub.status.busy":"2022-03-15T04:57:14.922075Z","iopub.execute_input":"2022-03-15T04:57:14.92234Z","iopub.status.idle":"2022-03-15T04:57:15.484866Z","shell.execute_reply.started":"2022-03-15T04:57:14.9223Z","shell.execute_reply":"2022-03-15T04:57:15.484134Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Optimize for TPU usage","metadata":{}},{"cell_type":"code","source":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print(f'Running on TPU {tpu.master()}')\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy()\n    \nREPLICAS = strategy.num_replicas_in_sync\nprint(f'REPLICAS: {REPLICAS}')\n\nAUTOTUNE = tf.data.AUTOTUNE\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-03-15T04:57:15.486398Z","iopub.execute_input":"2022-03-15T04:57:15.486699Z","iopub.status.idle":"2022-03-15T04:57:15.494278Z","shell.execute_reply.started":"2022-03-15T04:57:15.48666Z","shell.execute_reply":"2022-03-15T04:57:15.493516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load data\n\n---\n\n","metadata":{}},{"cell_type":"code","source":"MONET_FILENAMES = tf.io.gfile.glob(str(GCS_PATH + '/monet_tfrec/*.tfrec'))\nPHOTO_FILENAMES = tf.io.gfile.glob(str(GCS_PATH + '/photo_tfrec/*.tfrec'))\n\ndef count_data_items(filenames):\n    n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) for filename in filenames]\n    return np.sum(n)\n\nn_monet_samples = count_data_items(MONET_FILENAMES)\nn_photo_samples = count_data_items(PHOTO_FILENAMES)\n\nprint(f'Monet TFRecord files: {len(MONET_FILENAMES)}')\nprint(f'Monet image files: {n_monet_samples}')\nprint(f'Photo TFRecord files: {len(PHOTO_FILENAMES)}')\nprint(f'Photo image files: {n_photo_samples}')","metadata":{"execution":{"iopub.status.busy":"2022-03-15T04:57:15.497443Z","iopub.execute_input":"2022-03-15T04:57:15.497701Z","iopub.status.idle":"2022-03-15T04:57:16.060263Z","shell.execute_reply.started":"2022-03-15T04:57:15.497665Z","shell.execute_reply":"2022-03-15T04:57:16.059499Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Decode the image and rescale pizels to [-1 - 1] to use tanh activation function","metadata":{}},{"cell_type":"code","source":"def decode_image(image):\n    image = tf.image.decode_jpeg(image, channels=CHANNELS)\n    image = (tf.cast(image, tf.float32) / 127.5) - 1\n    image = tf.reshape(image, [HEIGHT, WIDTH, CHANNELS])\n    \n    return image ","metadata":{"execution":{"iopub.status.busy":"2022-03-15T04:57:16.061581Z","iopub.execute_input":"2022-03-15T04:57:16.061974Z","iopub.status.idle":"2022-03-15T04:57:16.067662Z","shell.execute_reply.started":"2022-03-15T04:57:16.061939Z","shell.execute_reply":"2022-03-15T04:57:16.066921Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Read and return the images from the TFRecord files","metadata":{}},{"cell_type":"code","source":"def read_tfrecord(example):\n    tfrecord_format = {\n        'image_name': tf.io.FixedLenFeature([], tf.string),\n        'image':      tf.io.FixedLenFeature([], tf.string),\n        'target':     tf.io.FixedLenFeature([], tf.string)\n    }\n    example = tf.io.parse_single_example(example, tfrecord_format)\n    image = decode_image(example['image'])\n    \n    return image","metadata":{"execution":{"iopub.status.busy":"2022-03-15T04:57:16.0688Z","iopub.execute_input":"2022-03-15T04:57:16.069038Z","iopub.status.idle":"2022-03-15T04:57:16.077795Z","shell.execute_reply.started":"2022-03-15T04:57:16.069003Z","shell.execute_reply":"2022-03-15T04:57:16.077001Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Extract the images from the tfrecs","metadata":{}},{"cell_type":"code","source":"def load_dataset(filenames, labeled=True, ordered=False):\n    dataset = tf.data.TFRecordDataset(filenames)\n    dataset = dataset.map(read_tfrecord, num_parallel_calls=AUTOTUNE)\n    \n    return dataset","metadata":{"execution":{"iopub.status.busy":"2022-03-15T04:57:16.079048Z","iopub.execute_input":"2022-03-15T04:57:16.079493Z","iopub.status.idle":"2022-03-15T04:57:16.086373Z","shell.execute_reply.started":"2022-03-15T04:57:16.079456Z","shell.execute_reply":"2022-03-15T04:57:16.085638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Random augmentaions to the data - a left or right flip, zoom or rotation","metadata":{}},{"cell_type":"code","source":"data_augmentation = tf.keras.Sequential([\n    layers.RandomFlip(\"horizontal\"),\n    layers.RandomZoom(0.1)\n])","metadata":{"execution":{"iopub.status.busy":"2022-03-15T04:57:16.087522Z","iopub.execute_input":"2022-03-15T04:57:16.087935Z","iopub.status.idle":"2022-03-15T04:57:17.094137Z","shell.execute_reply.started":"2022-03-15T04:57:16.087901Z","shell.execute_reply":"2022-03-15T04:57:17.092756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def gan_dataset(monet_files, photo_files, augment=None, repeat=True, shuffle=True, batch_size=1, buffer_size=2048):\n    \n    monet_dataset = load_dataset(monet_files)\n    photo_dataset = load_dataset(photo_files)\n    \n    if augment:\n        monet_dataset = monet_dataset.map(augment, AUTOTUNE)\n        photo_dataset = photo_dataset.map(augment, AUTOTUNE)\n        \n    monet_dataset = monet_dataset.cache()\n    photo_dataset = photo_dataset.cache()\n    \n    if shuffle:\n        monet_dataset = monet_dataset.shuffle(buffer_size)\n        photo_dataset = photo_dataset.shuffle(buffer_size)\n        \n    if repeat:\n        monet_dataset = monet_dataset.repeat()\n        photo_dataset = photo_dataset.repeat()\n        \n    monet_dataset = monet_dataset.batch(batch_size, drop_remainder=True)\n    photo_dataset = photo_dataset.batch(batch_size, drop_remainder=True)\n\n    monet_dataset = monet_dataset.prefetch(AUTOTUNE)\n    photo_dataset = photo_dataset.prefetch(AUTOTUNE)\n    \n    gan_dataset = tf.data.Dataset.zip((monet_dataset, photo_dataset))\n    \n    return gan_dataset\n\n# Load dataset\ndata = gan_dataset(MONET_FILENAMES, PHOTO_FILENAMES, augment=data_augmentation)","metadata":{"execution":{"iopub.status.busy":"2022-03-15T04:57:17.095624Z","iopub.execute_input":"2022-03-15T04:57:17.095891Z","iopub.status.idle":"2022-03-15T04:57:17.521231Z","shell.execute_reply.started":"2022-03-15T04:57:17.095855Z","shell.execute_reply":"2022-03-15T04:57:17.518926Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Check some photos from the dataset","metadata":{}},{"cell_type":"code","source":"def display_samples(dataset, row, col):\n    dataset_iter = iter(dataset)\n    plt.figure(figsize=(15, int(15*row/col)))\n    \n    for j in range(row*col):\n        example_sample = next(dataset_iter)\n        plt.subplot(row, col, j+1)\n        plt.axis('off')\n        plt.imshow(example_sample[0] * 0.5 + 0.5)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-15T04:57:17.523001Z","iopub.execute_input":"2022-03-15T04:57:17.523249Z","iopub.status.idle":"2022-03-15T04:57:17.537409Z","shell.execute_reply.started":"2022-03-15T04:57:17.523213Z","shell.execute_reply":"2022-03-15T04:57:17.536728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Show examples\ndisplay_samples(load_dataset(MONET_FILENAMES).shuffle(30).batch(1), 4, 4)","metadata":{"execution":{"iopub.status.busy":"2022-03-15T04:57:17.538687Z","iopub.execute_input":"2022-03-15T04:57:17.539265Z","iopub.status.idle":"2022-03-15T04:57:20.125184Z","shell.execute_reply.started":"2022-03-15T04:57:17.539228Z","shell.execute_reply":"2022-03-15T04:57:20.124303Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Show photos\ndisplay_samples(load_dataset(PHOTO_FILENAMES).shuffle(30).batch(1), 4, 4)","metadata":{"execution":{"iopub.status.busy":"2022-03-15T04:57:20.126955Z","iopub.execute_input":"2022-03-15T04:57:20.127239Z","iopub.status.idle":"2022-03-15T04:57:21.902404Z","shell.execute_reply.started":"2022-03-15T04:57:20.127201Z","shell.execute_reply":"2022-03-15T04:57:21.901763Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def display_generated_samples(dataset, model, num_samples):\n    dataset_iter = iter(dataset)\n    \n    for _ in range(num_samples):\n        example_sample = next(dataset_iter)\n        generated_sample = model.predict(example_sample)\n        \n        plt.subplot(121)\n        plt.title(\"input image\")\n        plt.imshow(example_sample[0] * 0.5 + 0.5)\n        plt.axis('off')\n        \n        plt.subplot(122)\n        plt.title(\"Generated image\")\n        plt.imshow(generated_sample[0] * 0.5 + 0.5)\n        plt.axis('off')\n        plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-15T04:57:21.906313Z","iopub.execute_input":"2022-03-15T04:57:21.906773Z","iopub.status.idle":"2022-03-15T04:57:21.915872Z","shell.execute_reply.started":"2022-03-15T04:57:21.906722Z","shell.execute_reply":"2022-03-15T04:57:21.914981Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict_and_save(input_dataset, generator_model, output_path):\n    i = 1\n    for image in input_dataset:\n        prediction = generator_model(image, training=False)[0].numpy()\n        # Re-scale\n        prediction = (prediction * 127.5 + 127.5).astype(np.uint8)   \n        im = PIL.Image.fromarray(prediction)\n        im.save(f'{output_path}{str(i)}.jpg')\n        i += 1","metadata":{"execution":{"iopub.status.busy":"2022-03-15T04:57:21.916981Z","iopub.execute_input":"2022-03-15T04:57:21.91729Z","iopub.status.idle":"2022-03-15T04:57:21.928047Z","shell.execute_reply.started":"2022-03-15T04:57:21.917255Z","shell.execute_reply":"2022-03-15T04:57:21.92721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Build Generator Model\n\n---\n\n#### Resnet blocks- [Link to the paper](https://arxiv.org/abs/1707.04881) (note section 2.2. ResGAN model)\n* Two 256 filter 3×3 Convolutional 2D layers\n* ` InstanceNormalization(axis = -1)` ensures features are normalized per feature map\n* ReLU activation function for first layer","metadata":{}},{"cell_type":"code","source":"def resnet_block(input_layer):\n    initializer = RandomNormal(stddev=0.02)\n    \n    # Layer 1\n    out = layers.Conv2D(256, 3, padding='same', kernel_initializer=initializer)(input_layer)\n    out = InstanceNormalization(axis=-1)(out)\n    out = layers.LeakyReLU(alpha=0.2)(out)\n    \n    # Layer 2\n    out = layers.Conv2D(256, 3, padding='same', kernel_initializer=initializer)(out)\n    out = InstanceNormalization(axis=-1)(out)\n    \n    # Merge with input layer and return\n    out = Concatenate()([out, input_layer])\n    \n    return out","metadata":{"execution":{"iopub.status.busy":"2022-03-15T04:57:21.929265Z","iopub.execute_input":"2022-03-15T04:57:21.929763Z","iopub.status.idle":"2022-03-15T04:57:21.939372Z","shell.execute_reply.started":"2022-03-15T04:57:21.929704Z","shell.execute_reply":"2022-03-15T04:57:21.938691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Build Generator Model\n\nUnpaired Image-to-Image Translation [Link to paper](https://arxiv.org/abs/1703.10593) (Note section 2 and figure 3)\n\n* Begin with preprocessing we only have 300 images in the training Moet set\n* The generator contains 9 residual blocks c7s1-64, d128, d256, R256, R256, R256, R256, R256, R256, R256, R256, R256, u128, u64, c7s1-3\n* **c7s1-k** - 7×7 Convolution-InstanceNorm-ReLU layer with k filters and stride 1. \n* **dk** - 3 × 3 Convolution-InstanceNorm-ReLU layer with k filters and stride 2.\n* **Rk** - resnet_block (from previous cell)\n* **uk** - 3 × 3 fractional-strided-Convolution InstanceNorm-ReLU layer with k filters\n\n    > Fractional striding is achieved by Conv2dTranspose [Here is a nice article on the topic from Beeren Sahu](https://beerensahu.wordpress.com/2018/04/10/pytorch-a-fractionally-strided-convolution-or-a-deconvolution/)\n","metadata":{}},{"cell_type":"code","source":"def generator():\n    inputs = layers.Input(shape=[HEIGHT, WIDTH, CHANNELS])\n    init = RandomNormal(mean=0.0, stddev=0.02)\n    \n    # c7s1-64\n    out = layers.Conv2D(64, 7, padding='same', kernel_initializer=init)(inputs)\n    out = InstanceNormalization()(out)\n    out = Activation('relu')(out)\n    \n    # d128\n    out = layers.Conv2D(128, 3, strides=2, padding='same', kernel_initializer=init)(out)\n    out = InstanceNormalization()(out)\n    out = Activation('relu')(out)\n    \n    # d256\n    out = layers.Conv2D(256, 3, strides=2, padding='same', kernel_initializer=init)(out)\n    out = InstanceNormalization()(out)\n    out = layers.Dropout(0.5)(out)\n    out = Activation('relu')(out)\n    \n    # R256 (9)\n    for _ in range(9):\n        out = resnet_block(out)\n    \n    # u128\n    out = layers.Conv2DTranspose(128, 3, strides=2, padding='same', kernel_initializer=init)(out)\n    out = InstanceNormalization()(out)\n    out = Activation('relu')(out)\n    \n    # u64\n    out = layers.Conv2DTranspose(64, 3, strides=2, padding='same', kernel_initializer=init)(out)\n    out = InstanceNormalization()(out)\n    out = Activation('relu')(out)\n    \n    # c7s1-3\n    out = layers.Conv2D(3, 7, padding='same', kernel_initializer=init)(inputs)\n    out = InstanceNormalization()(out)\n    out = Activation('tanh')(out)\n\n    return Model(inputs, out)","metadata":{"execution":{"iopub.status.busy":"2022-03-15T04:57:21.940854Z","iopub.execute_input":"2022-03-15T04:57:21.941341Z","iopub.status.idle":"2022-03-15T04:57:21.955592Z","shell.execute_reply.started":"2022-03-15T04:57:21.941304Z","shell.execute_reply":"2022-03-15T04:57:21.954911Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Build Discriminator Model\n\n---\n\n#### 70 x 70 patchGAN - [Link to the paper](https://arxiv.org/abs/1703.10593) (note section 7.2. Network architectures - Discriminator architectures)\n\n* The discriminator contains 4 blocks C64 - C128 - C256 - C512\n* `InstanceNormalization(axis = -1)` ensures features are normalized per feature map","metadata":{}},{"cell_type":"code","source":"def discriminator():\n    init = RandomNormal(mean=0.0, stddev=0.02)\n    inputs = layers.Input(shape=[HEIGHT, WIDTH, CHANNELS])\n    \n    # C64\n    out = layers.Conv2D(64, 4, strides=2, padding='same', kernel_initializer=init)(inputs)\n    out = layers.LeakyReLU(alpha=0.2)(out)\n    \n    # C128\n    out = layers.Conv2D(128, 4, strides=2, padding='same', kernel_initializer=init)(out)\n    out = InstanceNormalization()(out)\n    out = layers.LeakyReLU(alpha=0.2)(out)\n    \n    # C256\n    out = layers.Conv2D(256, 4, strides=2, padding='same', kernel_initializer=init)(out)\n    out = InstanceNormalization()(out)\n    out = layers.LeakyReLU(alpha=0.2)(out)\n    \n    # C512, 1x1 stride\n    out = layers.Conv2D(256, 4, padding='same', kernel_initializer=init)(out)\n    out = InstanceNormalization()(out)\n    out = layers.LeakyReLU(alpha=0.2)(out)\n    \n    # final step 1 filter\n    outputs = layers.Conv2D(1, 4, padding='same', kernel_initializer=init)(out)\n    \n    return Model(inputs, outputs)","metadata":{"execution":{"iopub.status.busy":"2022-03-15T04:57:21.957025Z","iopub.execute_input":"2022-03-15T04:57:21.957522Z","iopub.status.idle":"2022-03-15T04:57:21.968235Z","shell.execute_reply.started":"2022-03-15T04:57:21.957485Z","shell.execute_reply":"2022-03-15T04:57:21.967544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Loss Functions\n\n---\n\n* Discriminator loss {0: fake, 1: real} (The discriminator loss outputs the average of the real and generated loss)\n* Cycle consistency loss (measures if original photo and the twice transformed photo to be similar to one another)\n* Identity loss (compares the image with its generator (i.e. photo with photo generator))","metadata":{}},{"cell_type":"code","source":"with strategy.scope():\n    \n    def discriminator_loss(real, generated):\n        real_loss = losses.BinaryCrossentropy(from_logits=True, reduction=losses.Reduction.NONE)(tf.ones_like(real), real)\n        generated_loss = losses.BinaryCrossentropy(from_logits=True, reduction=losses.Reduction.NONE)(tf.zeros_like(generated), generated)\n        total_disc_loss = real_loss + generated_loss\n\n        return total_disc_loss * 0.5\n  ","metadata":{"execution":{"iopub.status.busy":"2022-03-15T04:57:21.969145Z","iopub.execute_input":"2022-03-15T04:57:21.971094Z","iopub.status.idle":"2022-03-15T04:57:21.979757Z","shell.execute_reply.started":"2022-03-15T04:57:21.971063Z","shell.execute_reply":"2022-03-15T04:57:21.979069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with strategy.scope():\n    \n    def generator_loss(generated):\n        return losses.BinaryCrossentropy(from_logits=True, reduction=losses.Reduction.NONE)(tf.ones_like(generated), generated)","metadata":{"execution":{"iopub.status.busy":"2022-03-15T04:57:21.980952Z","iopub.execute_input":"2022-03-15T04:57:21.981563Z","iopub.status.idle":"2022-03-15T04:57:21.988099Z","shell.execute_reply.started":"2022-03-15T04:57:21.981526Z","shell.execute_reply":"2022-03-15T04:57:21.98737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with strategy.scope():\n    \n    def cycle_loss(real_image, cycled_image, LAMBDA):\n        loss1 = tf.reduce_mean(tf.abs(real_image - cycled_image))\n\n        return LAMBDA * loss1","metadata":{"execution":{"iopub.status.busy":"2022-03-15T04:57:21.989476Z","iopub.execute_input":"2022-03-15T04:57:21.989759Z","iopub.status.idle":"2022-03-15T04:57:21.996392Z","shell.execute_reply.started":"2022-03-15T04:57:21.989723Z","shell.execute_reply":"2022-03-15T04:57:21.995625Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with strategy.scope():\n    \n    def identity_loss(real_image, same_image, LAMBDA):\n        loss = tf.reduce_mean(tf.abs(real_image - same_image))\n        \n        return LAMBDA * 0.5 * loss","metadata":{"execution":{"iopub.status.busy":"2022-03-15T04:57:21.997577Z","iopub.execute_input":"2022-03-15T04:57:21.99787Z","iopub.status.idle":"2022-03-15T04:57:22.004875Z","shell.execute_reply.started":"2022-03-15T04:57:21.997824Z","shell.execute_reply":"2022-03-15T04:57:22.004226Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Initialize Optimizers","metadata":{}},{"cell_type":"code","source":"with strategy.scope(): \n    \n    monet_generator_optimizer = optimizers.Adam(2e-4, beta_1=0.5)\n    photo_generator_optimizer = optimizers.Adam(2e-4, beta_1=0.5)\n\n    monet_discriminator_optimizer = optimizers.Adam(1e-4, beta_1=0.5, epsilon=0.1, amsgrad=True)\n    photo_discriminator_optimizer = optimizers.Adam(1e-4, beta_1=0.5, epsilon=0.1, amsgrad=True)","metadata":{"execution":{"iopub.status.busy":"2022-03-15T05:02:41.16318Z","iopub.execute_input":"2022-03-15T05:02:41.16347Z","iopub.status.idle":"2022-03-15T05:02:41.169538Z","shell.execute_reply.started":"2022-03-15T05:02:41.163439Z","shell.execute_reply":"2022-03-15T05:02:41.168647Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Build Composite Model\n\n---\n","metadata":{}},{"cell_type":"code","source":"with strategy.scope():\n    monet_generator = generator()\n    photo_generator = generator()\n\n    monet_discriminator = discriminator()\n    photo_discriminator = discriminator()\n\nclass CycleGan(Model):\n    def __init__(\n        self,\n        monet_generator,\n        photo_generator,\n        monet_discriminator,\n        photo_discriminator,\n        lambda_cycle=10,\n    ):\n        super(CycleGan, self).__init__()\n        self.monet_generator = monet_generator\n        self.photo_generator = photo_generator\n        self.monet_discriminator = monet_discriminator\n        self.photo_discriminator = photo_discriminator\n        self.lambda_cycle = lambda_cycle\n        \n    def compile(\n        self,\n        monet_generator_optimizer,\n        photo_generator_optimizer,\n        monet_discriminator_optimizer,\n        photo_discriminator_optimizer,\n        gen_loss_fn,\n        disc_loss_fn,\n        cycle_loss_fn,\n        identity_loss_fn\n    ):\n        super(CycleGan, self).compile()\n        self.monet_generator_optimizer = monet_generator_optimizer\n        self.photo_generator_optimizer = photo_generator_optimizer\n        self.monet_discriminator_optimizer = monet_discriminator_optimizer\n        self.photo_discriminator_optimizer = photo_discriminator_optimizer\n        self.gen_loss_fn = gen_loss_fn\n        self.disc_loss_fn = disc_loss_fn\n        self.cycle_loss_fn = cycle_loss_fn\n        self.identity_loss_fn = identity_loss_fn\n    \n    @tf.function\n    def train_step(self, batch_data):\n        real_monet, real_photo = batch_data\n        \n        with tf.GradientTape(persistent=True) as tape:\n            # photo to monet back to photo\n            fake_monet = self.monet_generator(real_photo, training=True)\n            cycled_photo = self.photo_generator(fake_monet, training=True)\n\n            # monet to photo back to monet\n            fake_photo = self.photo_generator(real_monet, training=True)\n            cycled_monet = self.monet_generator(fake_photo, training=True)\n\n            # generating itself\n            same_monet = self.monet_generator(real_monet, training=True)\n            same_photo = self.photo_generator(real_photo, training=True)\n\n            # discriminator used to check, inputing real images\n            disc_real_monet = self.monet_discriminator(real_monet, training=True)\n            disc_real_photo = self.photo_discriminator(real_photo, training=True)\n\n            # discriminator used to check, inputing fake images\n            disc_fake_monet = self.monet_discriminator(fake_monet, training=True)\n            disc_fake_photo = self.photo_discriminator(fake_photo, training=True)\n\n            # evaluates generator loss\n            monet_gen_loss = self.gen_loss_fn(disc_fake_monet)\n            photo_gen_loss = self.gen_loss_fn(disc_fake_photo)\n\n            # evaluates total cycle consistency loss\n            total_cycle_loss = self.cycle_loss_fn(real_monet, cycled_monet, self.lambda_cycle) + self.cycle_loss_fn(real_photo, cycled_photo, self.lambda_cycle)\n\n            # evaluates total generator loss\n            total_monet_gen_loss = monet_gen_loss + total_cycle_loss + self.identity_loss_fn(real_monet, same_monet, self.lambda_cycle)\n            total_photo_gen_loss = photo_gen_loss + total_cycle_loss + self.identity_loss_fn(real_photo, same_photo, self.lambda_cycle)\n\n            # evaluates discriminator loss\n            monet_disc_loss = self.disc_loss_fn(disc_real_monet, disc_fake_monet)\n            photo_disc_loss = self.disc_loss_fn(disc_real_photo, disc_fake_photo)\n\n        # Calculate the gradients for generator and discriminator\n        monet_generator_gradients = tape.gradient(total_monet_gen_loss, self.monet_generator.trainable_variables)\n        photo_generator_gradients = tape.gradient(total_photo_gen_loss, self.photo_generator.trainable_variables)\n\n        monet_discriminator_gradients = tape.gradient(monet_disc_loss, self.monet_discriminator.trainable_variables)\n        photo_discriminator_gradients = tape.gradient(photo_disc_loss, self.photo_discriminator.trainable_variables)\n\n        # Apply the gradients to the optimizer\n        self.monet_generator_optimizer.apply_gradients(zip(monet_generator_gradients, self.monet_generator.trainable_variables))\n        self.photo_generator_optimizer.apply_gradients(zip(photo_generator_gradients, self.photo_generator.trainable_variables))\n        self.monet_discriminator_optimizer.apply_gradients(zip(monet_discriminator_gradients, self.monet_discriminator.trainable_variables))\n        self.photo_discriminator_optimizer.apply_gradients(zip(photo_discriminator_gradients, self.photo_discriminator.trainable_variables))\n        \n        return {\n            'Monet generator loss': total_monet_gen_loss,\n            'Photo generator loss': total_photo_gen_loss,\n            'Monet discriminator loss': monet_disc_loss,\n            'Photo discriminator loss': photo_disc_loss\n        }","metadata":{"execution":{"iopub.status.busy":"2022-03-15T04:57:22.015617Z","iopub.execute_input":"2022-03-15T04:57:22.015952Z","iopub.status.idle":"2022-03-15T04:57:23.939149Z","shell.execute_reply.started":"2022-03-15T04:57:22.015852Z","shell.execute_reply":"2022-03-15T04:57:23.938352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Helper Functions\n\n---\n","metadata":{}},{"cell_type":"code","source":"def predict_and_save(input_dataset, generator_model, output_path):\n    i = 1\n    for image in input_dataset:\n        prediction = generator_model(image, training=False)[0].numpy()\n        prediction = (prediction * 127.5 + 127.5).astype(np.uint8)\n        im = PIL.Image.fromarray(prediction)\n        im.save(f'{output_path}{str(i)}.jpg')\n        i += 1","metadata":{"execution":{"iopub.status.busy":"2022-03-15T04:57:23.940521Z","iopub.execute_input":"2022-03-15T04:57:23.940789Z","iopub.status.idle":"2022-03-15T04:57:23.947335Z","shell.execute_reply.started":"2022-03-15T04:57:23.940753Z","shell.execute_reply":"2022-03-15T04:57:23.946552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Updating:\n- image buffer for fake images reduces model oscillation\n- discriminators using a history of generated images rather than the ones produced by the latest generators\n\n[Link to the paper](https://arxiv.org/abs/1703.10593) (note section 4. Implementation - Training details)","metadata":{}},{"cell_type":"code","source":"# def update_image_buffer(buffer_images, images, buffer_max =50):\n    \n#     output_images = list()\n#     for image in images:\n        \n#         # Fill the buffer\n#         if len(buffer) < buffer_max:\n            \n#             buffer.append(image)\n#             output_images.append(image)\n            \n#         # Use image, don't add to buffer\n#         elif random() < 0.5:\n#             output_images.append(image)\n            \n#         # Replace an existing image and use replaced image\n#         else:\n#             index = randint(0, len(buffer))\n#             output_images.append(buffer[index])\n#             buffer[index] = image\n            \n#     return tf.stack(selected, axis=0)","metadata":{"execution":{"iopub.status.busy":"2022-03-15T04:57:23.948927Z","iopub.execute_input":"2022-03-15T04:57:23.9494Z","iopub.status.idle":"2022-03-15T04:57:23.955828Z","shell.execute_reply.started":"2022-03-15T04:57:23.949294Z","shell.execute_reply":"2022-03-15T04:57:23.955192Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Compile GAN\n\n---\n","metadata":{}},{"cell_type":"code","source":"with strategy.scope():\n    \n    gan_model = CycleGan(monet_generator, photo_generator, monet_discriminator, photo_discriminator)\n\n    gan_model.compile(\n        monet_generator_optimizer = monet_generator_optimizer,\n        photo_generator_optimizer = photo_generator_optimizer,\n        monet_discriminator_optimizer = monet_discriminator_optimizer,\n        photo_discriminator_optimizer = photo_discriminator_optimizer,\n        gen_loss_fn = generator_loss,\n        disc_loss_fn = discriminator_loss,\n        cycle_loss_fn = cycle_loss,\n        identity_loss_fn = identity_loss,\n    )","metadata":{"execution":{"iopub.status.busy":"2022-03-15T04:57:23.957413Z","iopub.execute_input":"2022-03-15T04:57:23.957954Z","iopub.status.idle":"2022-03-15T04:57:23.977367Z","shell.execute_reply.started":"2022-03-15T04:57:23.957919Z","shell.execute_reply":"2022-03-15T04:57:23.976759Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train GAN\n\n---\n","metadata":{}},{"cell_type":"code","source":"gan_model.fit(data, epochs=EPOCHS, steps_per_epoch=(max(n_monet_samples , n_photo_samples )//5), verbose=2)\n\ngan_model.save('./cyclegan_model')","metadata":{"execution":{"iopub.status.busy":"2022-03-15T04:57:23.97977Z","iopub.execute_input":"2022-03-15T04:57:23.980176Z","iopub.status.idle":"2022-03-15T05:00:44.344071Z","shell.execute_reply.started":"2022-03-15T04:57:23.980148Z","shell.execute_reply":"2022-03-15T05:00:44.342963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Visualize predictions","metadata":{}},{"cell_type":"code","source":"display_generated_samples(load_dataset(PHOTO_FILENAMES).batch(1), monet_generator, 4)","metadata":{"execution":{"iopub.status.busy":"2022-03-15T05:00:44.345102Z","iopub.status.idle":"2022-03-15T05:00:44.347093Z","shell.execute_reply.started":"2022-03-15T05:00:44.34683Z","shell.execute_reply":"2022-03-15T05:00:44.34686Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot the image, its translation, and the reconstruction\ndef show_plot(imagesX, imagesY1, imagesY2):\n    \n    images = vstack((imagesX, imagesY1, imagesY2))\n    titles = ['Real', 'Generated', 'Reconstructed']\n\n    # Rescale from [-1,1] to [0,1]\n    images = (images + 1) / 2.0\n    \n    # plot images row by row\n    for i in range(len(images)):\n        pyplot.subplot(1, len(images), 1 + i)\n        pyplot.axis('off')\n        pyplot.imshow(images[i])\n        pyplot.title(titles[i])\n        \n    pyplot.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-15T05:00:44.348449Z","iopub.status.idle":"2022-03-15T05:00:44.349089Z","shell.execute_reply.started":"2022-03-15T05:00:44.348855Z","shell.execute_reply":"2022-03-15T05:00:44.348881Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Make predictions","metadata":{}},{"cell_type":"code","source":"# Create folder to save generated images\nos.makedirs('../images/')\n\nwith strategy.scope():\n    predict_and_save(load_dataset(PHOTO_FILENAMES).batch(1), monet_generator, '../images/')","metadata":{"execution":{"iopub.status.busy":"2022-03-15T05:00:44.350277Z","iopub.status.idle":"2022-03-15T05:00:44.350917Z","shell.execute_reply.started":"2022-03-15T05:00:44.350667Z","shell.execute_reply":"2022-03-15T05:00:44.350691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Submission file","metadata":{}},{"cell_type":"code","source":"shutil.make_archive('/kaggle/working/images/', 'zip', '../images')\n\nprint(f\"Generated samples: {len([name for name in os.listdir('../images/') if os.path.isfile(os.path.join('../images/', name))])}\")","metadata":{"execution":{"iopub.status.busy":"2022-03-15T05:00:44.352123Z","iopub.status.idle":"2022-03-15T05:00:44.352773Z","shell.execute_reply.started":"2022-03-15T05:00:44.352518Z","shell.execute_reply":"2022-03-15T05:00:44.352544Z"},"trusted":true},"execution_count":null,"outputs":[]}]}