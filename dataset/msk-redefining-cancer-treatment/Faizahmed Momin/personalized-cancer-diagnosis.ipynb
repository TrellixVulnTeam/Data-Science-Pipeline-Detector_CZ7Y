{"cells":[{"metadata":{},"cell_type":"markdown","source":"PERSONALIZED CANCER DIAGNOSIS\n\n**Problem Statement :**\nClassify the given genetic variations/mutations based on evidence from text-based clinical literature.\n\n**Objective:**\nPredict the probability of each data-point belonging to each of the nine classes.\n\n**Constraints:**\n1. Interpretability \n2. Class probabilities are needed. \n3. Penalize the errors in class probabilites => Metric is Log-loss.\n4. No Latency constraints.","execution_count":null},{"metadata":{"id":"BvqUZXM8EZtw","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport time\nimport warnings\nimport numpy as np\nwarnings.filterwarnings(\"ignore\")\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn import metrics\nfrom sklearn.metrics import roc_curve, auc\nfrom nltk.stem.porter import PorterStemmer\nimport re\nimport string\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom gensim.models import Word2Vec\nfrom gensim.models import KeyedVectors\nimport pickle\nfrom tqdm import tqdm\nimport os","execution_count":null,"outputs":[]},{"metadata":{"id":"UIIZrrVkNC_o"},"cell_type":"markdown","source":"## READ THE TRAINING VARIANTS DATA","execution_count":null},{"metadata":{"id":"YuK6sk6hKfIE","outputId":"1e58422e-e487-44fb-9344-b090ebdcb395","executionInfo":{"status":"ok","timestamp":1591643757293,"user_tz":-330,"elapsed":1672,"user":{"displayName":"Faizahmed Momin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjIpwU1PcmQEow-LMt2f-aSLgkwcuid0iQCqGiYLoc=s64","userId":"01335550890493547686"}},"trusted":true},"cell_type":"code","source":"data = pd.read_csv('../input/cancer-diagonsis/training_variants')\nprint('Number of data points : ', data.shape[0])\nprint('Number of features : ', data.shape[1])\nprint('Features : ', data.columns.values)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"ytSL4FnmMncj"},"cell_type":"markdown","source":"training/training_variants is a comma separated file containing the description of the genetic mutations used for training.\nFields are\n\n\n\n1.   ID : the id of the row used to link the mutation to the clinical evidence\n2.   Gene : the gene where this genetic mutation is located\n3.   Variation : the aminoacid change for this mutations\n4.   Class : 1-9 the class this genetic mutation has been classified on\n\n\n\n","execution_count":null},{"metadata":{"id":"NtW615PsNI-Z"},"cell_type":"markdown","source":"## READ THE TEXT DATA","execution_count":null},{"metadata":{"id":"HQVKNUNwMooA","outputId":"765268f3-fd1c-41f4-c770-122e3a44ea46","executionInfo":{"status":"ok","timestamp":1591643762777,"user_tz":-330,"elapsed":4218,"user":{"displayName":"Faizahmed Momin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjIpwU1PcmQEow-LMt2f-aSLgkwcuid0iQCqGiYLoc=s64","userId":"01335550890493547686"}},"trusted":true},"cell_type":"code","source":"data_text =pd.read_csv(\"../input/cancer-diagonsis/training_text\",sep=\"\\|\\|\",engine=\"python\",names=[\"ID\",\"TEXT\"],skiprows=1)\nprint('Number of data points : ', data_text.shape[0])\nprint('Number of features : ', data_text.shape[1])\nprint('Features : ', data_text.columns.values)\ndata_text.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"Euuw7w-xPGHM","outputId":"e9a4c7c0-17c9-4ccf-b931-d4b00b4d0776","executionInfo":{"status":"ok","timestamp":1591643762779,"user_tz":-330,"elapsed":1615,"user":{"displayName":"Faizahmed Momin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjIpwU1PcmQEow-LMt2f-aSLgkwcuid0iQCqGiYLoc=s64","userId":"01335550890493547686"}},"trusted":true},"cell_type":"code","source":"import nltk\nnltk.download('stopwords')","execution_count":null,"outputs":[]},{"metadata":{"id":"0-tq-8MbPA2I","trusted":true},"cell_type":"code","source":"stop_words = set(stopwords.words('english'))","execution_count":null,"outputs":[]},{"metadata":{"id":"WDZbIbi0PWKy"},"cell_type":"markdown","source":"## APPLY NLP PREPROCESSING TASK ","execution_count":null},{"metadata":{"id":"QqzAsEx_PZWa","trusted":true},"cell_type":"code","source":"def nlp_preprocessing(total_text,index,column):  \n  if type(total_text) is not int:\n    string = \"\"\n\n    # REPLACE EVERY SPECIAL CHARACTER WITH THE SPACE\n    total_text=re.sub('[^a-zA-Z0-9\\n]',' ',total_text)  \n\n    # REPLACE MULTIPLE SPACES WITH SINGLE SPACE\n    total_text=re.sub('\\s+',' ', total_text)\n\n    # CONVERT ALL THE CHARACTER TO LOWER CASE\n    total_text=total_text.lower()\n\n    for word in total_text.split() :\n      if not word in stop_words:    # IF THE WORD IS NOT STOP WORD THEN RETAIN THAT WORD AND ASSINGN IN STRING VARIABLE\n        string+=word + \" \"\n    data_text[column][index] = string","execution_count":null,"outputs":[]},{"metadata":{"id":"gVE08vNlRX5J","outputId":"88a2eff0-38a8-4ad4-a0a0-c02b4ffd5d88","executionInfo":{"status":"ok","timestamp":1591643816407,"user_tz":-330,"elapsed":30229,"user":{"displayName":"Faizahmed Momin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjIpwU1PcmQEow-LMt2f-aSLgkwcuid0iQCqGiYLoc=s64","userId":"01335550890493547686"}},"trusted":true},"cell_type":"code","source":"start_time = time.clock()\n\nfor index,row in data_text.iterrows():\n  if type(row['TEXT']) is str:\n    nlp_preprocessing(row['TEXT'],index,'TEXT')\n  else:\n    print('There is no text description for id :',index)\nprint('Time : ',time.clock() - start_time,\"seconds\")","execution_count":null,"outputs":[]},{"metadata":{"id":"-qvCScskUU7q","outputId":"231cf359-a11c-4482-b22f-47ccd972904f","executionInfo":{"status":"ok","timestamp":1591643817594,"user_tz":-330,"elapsed":1169,"user":{"displayName":"Faizahmed Momin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjIpwU1PcmQEow-LMt2f-aSLgkwcuid0iQCqGiYLoc=s64","userId":"01335550890493547686"}},"trusted":true},"cell_type":"code","source":"# MERGE THE DATA (GENE AND VARIATIONS) & TEXT DATA BASED ON THE ID\nfinal_data = pd.merge(data,data_text,on='ID',how='left')\n\nfinal_data.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"tc90uC4QWJKC","outputId":"55fca797-e1b7-45ab-8652-a202e06094d0","executionInfo":{"status":"ok","timestamp":1591643817598,"user_tz":-330,"elapsed":1148,"user":{"displayName":"Faizahmed Momin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjIpwU1PcmQEow-LMt2f-aSLgkwcuid0iQCqGiYLoc=s64","userId":"01335550890493547686"}},"trusted":true},"cell_type":"code","source":"final_data[final_data.isnull().any(axis=1)]","execution_count":null,"outputs":[]},{"metadata":{"id":"eP0wyzTEW9U7","trusted":true},"cell_type":"code","source":"final_data.loc[final_data['TEXT'].isnull(),'TEXT'] = final_data['Gene'] +' '+final_data['Variation']","execution_count":null,"outputs":[]},{"metadata":{"id":"A7a5pSXWXPOs","trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import cross_val_score\nfrom collections import Counter\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\n\n\nY_Class=final_data['Class'].values\nfinal_data.Gene=final_data.Gene.str.replace('\\s+','_')\nfinal_data.Variation=final_data.Variation.str.replace('\\s+','_')\n\n# SPLIT THE DATA INTO TEST,TRAIN AND CV\nX_1,X_Test,Y_1,Y_Test=train_test_split(final_data,Y_Class,stratify=Y_Class,test_size=0.2)\nX_Train,X_CV,Y_Train,Y_CV=train_test_split(X_1,Y_1,test_size=0.2)","execution_count":null,"outputs":[]},{"metadata":{"id":"1zP2uFdEarpv","outputId":"b5fa54d5-ed62-403e-a0ad-9ae4e4670e49","executionInfo":{"status":"ok","timestamp":1591590263484,"user_tz":-330,"elapsed":820,"user":{"displayName":"Faizahmed Momin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjIpwU1PcmQEow-LMt2f-aSLgkwcuid0iQCqGiYLoc=s64","userId":"01335550890493547686"}},"trusted":true},"cell_type":"code","source":"print('Number of data points in train data:', X_Train.shape[0])\nprint('Number of data points in test data:', X_Train.shape[0])\nprint('Number of data points in cross validation data:', X_CV.shape[0])","execution_count":null,"outputs":[]},{"metadata":{"id":"tXZWaekd2yNS","trusted":true},"cell_type":"code","source":"def plot_confusion_matrix(test_y,predict_y):\n  C = confusion_matrix(test_y,predict_y)\n\n  A=(((C.T)/(C.sum(axis=1))).T)\n\n  B = (C/C.sum(axis=0))\n\n  labels = [1,2,3,4,5,6,7,8,9]\n\n  print(\"-\"*20,\"Confusion Matrix\",\"-\"*20)\n  plt.figure(figsize = (20,7))\n\n  sns.heatmap(C, annot=True, cmap=\"YlGnBu\", fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n  plt.xlabel('Predicted Class')\n  plt.ylabel('Original Class')\n  plt.show()\n\n  print(\"-\"*20, \"Precision matrix (Columm Sum=1)\", \"-\"*20)\n  plt.figure(figsize=(20,7))\n  sns.heatmap(B, annot=True, cmap=\"YlGnBu\", fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n  plt.xlabel('Predicted Class')\n  plt.ylabel('Original Class')\n  plt.show()\n\n  print(\"-\"*20, \"Recall matrix (Row sum=1)\", \"-\"*20)\n  plt.figure(figsize=(20,7))\n  sns.heatmap(A, annot=True, cmap=\"YlGnBu\", fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n  plt.xlabel('Predicted Class')\n  plt.ylabel('Original Class')\n  plt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"PD1TcC7f97Cr","outputId":"4782f166-7458-48ed-a031-60b21b4fc464","executionInfo":{"status":"ok","timestamp":1591382781409,"user_tz":-330,"elapsed":60186,"user":{"displayName":"Faizahmed Momin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjIpwU1PcmQEow-LMt2f-aSLgkwcuid0iQCqGiYLoc=s64","userId":"01335550890493547686"}},"trusted":true},"cell_type":"code","source":"from sklearn.metrics import log_loss\nimport seaborn as sns\nX_Test_len = X_Test.shape[0]\nX_CV_len = X_CV.shape[0]\n\nY_CV_Predicted = np.zeros((X_CV_len,9))\n\nfor i in range(X_CV_len):\n  rand_probs=np.random.rand(1,9)\n  Y_CV_Predicted[i] =((rand_probs/sum(sum(rand_probs)))[0])\nprint(\"Log Loss on CV using Random Model\",log_loss(Y_CV, Y_CV_Predicted,eps=1e-15))\n\n\nY_Test_Predicted = np.zeros((X_Test_len,9))\n\nfor i in range(X_Test_len ):\n  rand_probs=np.random.rand(1,9)\n  Y_Test_Predicted[i] =((rand_probs/sum(sum(rand_probs)))[0])\nprint(\"Log Loss on CV using Random Model\",log_loss(Y_Test, Y_Test_Predicted,eps=1e-15))\n\npredicted_y =np.argmax(Y_Test_Predicted, axis=1)\nplot_confusion_matrix(Y_Test, predicted_y+1)","execution_count":null,"outputs":[]},{"metadata":{"id":"lnb-2uw_Nn5y"},"cell_type":"markdown","source":"#UNIVARIATE ANALYSIS","execution_count":null},{"metadata":{"id":"4U4VDNg6EAXw","trusted":true},"cell_type":"code","source":"def get_gene_variation_feature_dic(alpha,feature,df):\n  \n  value_count=df[feature].value_counts()\n  print(\"Value Count :\", value_count)\n  gene_var = dict()\n\n  for i,denominator in value_count.items():\n    vec=[]\n\n    for k in range(1,10):\n      class_count=df.loc[(df['Class'] == k) & (df[feature] == i)]\n      vec.append((class_count.shape[0] + alpha*10)/ (denominator + 90*alpha))   #Laplace Smoothing\n    gene_var[i]=vec\n  return gene_var","execution_count":null,"outputs":[]},{"metadata":{"id":"RL6VzTROQC1C","trusted":true},"cell_type":"code","source":"def get_gene_variation_features(alpha,feature,df):\n\n  gv_dict=get_gene_variation_feature_dic(alpha,feature,df)\n\n  value_count=df[feature].value_counts()\n\n  gv_fea=[]\n  print(\"DF Iteration_rows\", df.iterrows())\n  for index,row in df.iterrows():\n    if row[feature] in dict(value_count).keys():\n      gv_fea.append(gv_dict[row[feature]])\n    else:\n      gv_fea.append([1/9,1/9,1/9,1/9,1/9,1/9,1/9,1/9,1/9])\n  return gv_fea","execution_count":null,"outputs":[]},{"metadata":{"id":"6yeLXK3tk2kW"},"cell_type":"markdown","source":"## Univariate Analysis on Gene Features","execution_count":null},{"metadata":{"id":"44eI8NIck6OP","outputId":"af18b2f9-0892-46ff-d4de-31d4814e74ed","executionInfo":{"status":"ok","timestamp":1591382781416,"user_tz":-330,"elapsed":60143,"user":{"displayName":"Faizahmed Momin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjIpwU1PcmQEow-LMt2f-aSLgkwcuid0iQCqGiYLoc=s64","userId":"01335550890493547686"}},"trusted":true},"cell_type":"code","source":"unique_genes=X_Train['Gene'].value_counts()\n\nprint(\"Number of Unique Genes :\",unique_genes.shape[0])\n\nprint(unique_genes.head(10))","execution_count":null,"outputs":[]},{"metadata":{"id":"y5rCOhkQl-o5"},"cell_type":"markdown","source":"### Looking at the count , looks like there are 236 different categories of Gene thats are in Training Data","execution_count":null},{"metadata":{"id":"ysnLfyzDmPnt"},"cell_type":"markdown","source":"### Distribution are as follows","execution_count":null},{"metadata":{"id":"UHPbGWGzl-Gh","outputId":"527ad392-cb3e-48ce-9231-63bfbba6375e","executionInfo":{"status":"ok","timestamp":1591382781417,"user_tz":-330,"elapsed":60114,"user":{"displayName":"Faizahmed Momin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjIpwU1PcmQEow-LMt2f-aSLgkwcuid0iQCqGiYLoc=s64","userId":"01335550890493547686"}},"trusted":true},"cell_type":"code","source":"s = sum(unique_genes.values);\nh = unique_genes.values/s;\nplt.plot(h, label=\"Histrogram of Genes\")\nplt.xlabel('Index of a Gene')\nplt.ylabel('Number of Occurances')\nplt.legend()\nplt.grid()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"N4e0-cRPljP2","outputId":"5d0ef386-2214-4c7e-982a-d85089bec54a","executionInfo":{"status":"ok","timestamp":1591382782069,"user_tz":-330,"elapsed":60741,"user":{"displayName":"Faizahmed Momin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjIpwU1PcmQEow-LMt2f-aSLgkwcuid0iQCqGiYLoc=s64","userId":"01335550890493547686"}},"trusted":true},"cell_type":"code","source":"c = np.cumsum(h)\nplt.plot(c,label='Cumulative distribution of Genes')\nplt.grid()\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"qmYoKKfBn4ys"},"cell_type":"markdown","source":"#### There are 2 ways we can featurize this variable.\n\n\n\n1.   One Hot Encoding \n2.   Response Coding\n\nWe will choose the appropriate featurization based on the ML model we use.\n\n","execution_count":null},{"metadata":{"id":"3ueoDILIzcTc"},"cell_type":"markdown","source":"# BAG OF WORDS VECTORIZATION TECHNIQUE ","execution_count":null},{"metadata":{"id":"bn_D7QYJwuKQ"},"cell_type":"markdown","source":"### Response Coding Method on Gene Feature","execution_count":null},{"metadata":{"id":"8B5AgpT3oQSv","outputId":"a2fa03e5-09bc-4213-8ee1-28b890ea86ec","executionInfo":{"status":"ok","timestamp":1591382792941,"user_tz":-330,"elapsed":71588,"user":{"displayName":"Faizahmed Momin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjIpwU1PcmQEow-LMt2f-aSLgkwcuid0iQCqGiYLoc=s64","userId":"01335550890493547686"}},"trusted":true},"cell_type":"code","source":"alpha = 1 \n\nX_Train_gene_Feature_responsecoding = np.array(get_gene_variation_features(alpha,\"Gene\",X_Train))\nprint(\"Train Gene Feature :\",X_Train_gene_Feature_responsecoding.shape)\n\nprint(\"=\"*100)\n\nX_Test_gen_Feature_responsecoding = np.array(get_gene_variation_features(alpha,\"Gene\",X_Test))\nprint(\"Test Gene Feature :\",X_Test_gen_Feature_responsecoding.shape)\n\nprint(\"=\"*100)\n\nX_CV_gene_Feature_responsecoding = np.array(get_gene_variation_features(alpha,\"Gene\",X_CV))\nprint(\"CV Gene Feature :\",X_CV_gene_Feature_responsecoding.shape)\n\nprint(\"=\"*100)","execution_count":null,"outputs":[]},{"metadata":{"id":"8rO151zbwyEe"},"cell_type":"markdown","source":"### One Hot Encoding Method on Gene feature","execution_count":null},{"metadata":{"id":"Be1cS_jgssNj","outputId":"3bcf5571-0ff5-4f4f-aa99-afc6e789236b","executionInfo":{"status":"ok","timestamp":1591382792946,"user_tz":-330,"elapsed":71558,"user":{"displayName":"Faizahmed Momin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjIpwU1PcmQEow-LMt2f-aSLgkwcuid0iQCqGiYLoc=s64","userId":"01335550890493547686"}},"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\ngene_vectorizer=CountVectorizer()\n\nX_Train_gene_Feature_onehotEncoding=gene_vectorizer.fit_transform(X_Train[\"Gene\"])\nprint(\" Train Gene Feature :\" ,X_Train_gene_Feature_onehotEncoding.shape)\n\nprint(\"=\"*100)\n\nX_Test_gene_Feature_onehotencoding=gene_vectorizer.transform(X_Test[\"Gene\"])\nprint(\" Test Gene Feature :\" ,X_Test_gene_Feature_onehotencoding.shape)\n\nprint(\"=\"*100)\n\n\nX_CV_gene_Feature_onehotencoding=gene_vectorizer.transform(X_CV[\"Gene\"])\nprint(\" CV Gene Feature :\" ,X_CV_gene_Feature_onehotencoding.shape)\n\nprint(\"=\"*100)","execution_count":null,"outputs":[]},{"metadata":{"id":"FZIUza-OIy_F"},"cell_type":"markdown","source":"### APPLY SVM -->  SGD CLASSIFIER TO FIND THE BEST HYPERPARAMETER","execution_count":null},{"metadata":{"id":"rsuR7-_D2Hkz","outputId":"50768b6d-af49-4598-d5a0-1e7d85643725","executionInfo":{"status":"ok","timestamp":1591382795917,"user_tz":-330,"elapsed":74495,"user":{"displayName":"Faizahmed Momin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjIpwU1PcmQEow-LMt2f-aSLgkwcuid0iQCqGiYLoc=s64","userId":"01335550890493547686"}},"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import SGDClassifier\nfrom sklearn.calibration import CalibratedClassifierCV\nalpha = [10 ** x for x in range(-5, 1)]\n\ncv_log_error=[]\n\nfor i in alpha :\n  clf=SGDClassifier(alpha=i, penalty='l2', loss='log', random_state=42)\n  clf.fit(X_Train_gene_Feature_onehotEncoding,Y_Train)\n  sig_clf=CalibratedClassifierCV(clf,method=\"sigmoid\")\n  sig_clf.fit(X_Train_gene_Feature_onehotEncoding,Y_Train)\n  Predicted_Y=sig_clf.predict_proba(X_CV_gene_Feature_onehotencoding)\n  cv_log_error.append(log_loss(Y_CV,Predicted_Y,labels=clf.classes_,eps=1e-15))\n  print('For values of alpha = ', i, \"The log loss is:\",log_loss(Y_CV, Predicted_Y, labels=clf.classes_, eps=1e-15))\n\nfig,ax = plt.subplots()\nax.plot(alpha,cv_log_error,c='g')\n\nfor i , txt in enumerate(np.round(cv_log_error,3)):\n  ax.annotate((alpha[i],np.round(txt,3)), (alpha[i],cv_log_error[i]))\nplt.grid()\nplt.title(\"Cross Validation Error for each alpha\")\nplt.xlabel(\"Alpha i's\")\nplt.ylabel(\"Error Measure\")\nplt.show()\n\nbest_alpha = np.argmin(cv_log_error)\nclf = SGDClassifier(alpha=alpha[best_alpha], penalty='l2', loss='log', random_state=42)\nclf.fit(X_Train_gene_Feature_onehotEncoding, Y_Train)\nsig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\nsig_clf.fit(X_Train_gene_Feature_onehotEncoding, Y_Train)\n\npredict_y = sig_clf.predict_proba(X_Train_gene_Feature_onehotEncoding)\nprint('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(Y_Train, predict_y, labels=clf.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(X_CV_gene_Feature_onehotencoding)\nprint('For values of best alpha = ', alpha[best_alpha], \"The cross validation log loss is:\",log_loss(Y_CV, predict_y, labels=clf.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(X_Test_gene_Feature_onehotencoding)\nprint('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(Y_Test, predict_y, labels=clf.classes_, eps=1e-15))","execution_count":null,"outputs":[]},{"metadata":{"id":"vw2hhv3OZjmp","outputId":"42480628-9caf-4e00-bcf2-58e9ca169931","executionInfo":{"status":"ok","timestamp":1591382795918,"user_tz":-330,"elapsed":74474,"user":{"displayName":"Faizahmed Momin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjIpwU1PcmQEow-LMt2f-aSLgkwcuid0iQCqGiYLoc=s64","userId":"01335550890493547686"}},"trusted":true},"cell_type":"code","source":"print(\"Q6. How many data points in Test and CV datasets are covered by the \", unique_genes.shape[0], \" genes in train dataset?\")\n\ntest_coverage=X_Test[X_Test['Gene'].isin(list(set(X_Train['Gene'])))].shape[0]\ncv_coverage=X_CV[X_CV['Gene'].isin(list(set(X_Train['Gene'])))].shape[0]\n\nprint('Ans\\n1. In test data',test_coverage, 'out of',X_Test.shape[0], \":\",(test_coverage/X_Test.shape[0])*100)\nprint('2. In cross validation data',cv_coverage, 'out of ',X_CV.shape[0],\":\" ,(cv_coverage/X_CV.shape[0])*100)","execution_count":null,"outputs":[]},{"metadata":{"id":"feTX8ud3bF3V"},"cell_type":"markdown","source":"## Univariate Analysis on Variation Features","execution_count":null},{"metadata":{"id":"oeO6_GslbKL1","outputId":"9135afce-5f13-45f0-d114-a5d38a66e3e1","executionInfo":{"status":"ok","timestamp":1591382795920,"user_tz":-330,"elapsed":74442,"user":{"displayName":"Faizahmed Momin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjIpwU1PcmQEow-LMt2f-aSLgkwcuid0iQCqGiYLoc=s64","userId":"01335550890493547686"}},"trusted":true},"cell_type":"code","source":"unique_variations=X_Train['Variation'].value_counts()\n\nprint(\"Number of Unique Genes :\",unique_variations.shape[0])\n\nprint(unique_variations.head(10))","execution_count":null,"outputs":[]},{"metadata":{"id":"8gHsAg58bpIa","outputId":"d3990edb-d734-4b9a-b429-08dfb7e2e2ba","executionInfo":{"status":"ok","timestamp":1591382796524,"user_tz":-330,"elapsed":74983,"user":{"displayName":"Faizahmed Momin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjIpwU1PcmQEow-LMt2f-aSLgkwcuid0iQCqGiYLoc=s64","userId":"01335550890493547686"}},"trusted":true},"cell_type":"code","source":"s = sum(unique_variations.values);\nh = unique_variations.values/s;\nplt.plot(h, label=\"Histrogram of Variations\")\nplt.xlabel('Index of a Vriations')\nplt.ylabel('Number of Occurances')\nplt.legend()\nplt.grid()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"yWQhCuU6bzDX","outputId":"bf676fd9-fe0e-4d61-eb9c-359bfd838ae0","executionInfo":{"status":"ok","timestamp":1591382796527,"user_tz":-330,"elapsed":74952,"user":{"displayName":"Faizahmed Momin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjIpwU1PcmQEow-LMt2f-aSLgkwcuid0iQCqGiYLoc=s64","userId":"01335550890493547686"}},"trusted":true},"cell_type":"code","source":"c = np.cumsum(h)\nprint(c)\nplt.plot(c,label='Cumulative distribution of Variations')\nplt.grid()\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"Ydbgf1GqcEDg"},"cell_type":"markdown","source":"#### There are 2 ways we can featurize this variable.\n\n\n\n1.   One Hot Encoding \n2.   Response Coding\n\nWe will choose the appropriate featurization based on the ML model we use.","execution_count":null},{"metadata":{"id":"OW70tqM5cQdQ"},"cell_type":"markdown","source":"### Response Coding Method on Variation Features","execution_count":null},{"metadata":{"id":"QP2Qmrb7cE8a","outputId":"4823a920-1398-4d62-de1f-e853f8928e7d","executionInfo":{"status":"ok","timestamp":1591382874557,"user_tz":-330,"elapsed":152950,"user":{"displayName":"Faizahmed Momin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjIpwU1PcmQEow-LMt2f-aSLgkwcuid0iQCqGiYLoc=s64","userId":"01335550890493547686"}},"trusted":true},"cell_type":"code","source":"alpha = 1 \n\nX_Train_variation_Feature_responsecoding = np.array(get_gene_variation_features(alpha,\"Variation\",X_Train))\nprint(\"Train Variation Feature :\",X_Train_variation_Feature_responsecoding.shape)\n\nprint(\"=\"*100)\n\nX_Test_variation_Feature_responsecoding = np.array(get_gene_variation_features(alpha,\"Variation\",X_Test))\nprint(\"Test Variation Feature :\",X_Test_variation_Feature_responsecoding.shape)\n\nprint(\"=\"*100)\n\nX_CV_variation_Feature_responsecoding = np.array(get_gene_variation_features(alpha,\"Variation\",X_CV))\nprint(\"CV Variation Feature :\",X_CV_variation_Feature_responsecoding.shape)\n\nprint(\"=\"*100)","execution_count":null,"outputs":[]},{"metadata":{"id":"Exb_SANVeWRA"},"cell_type":"markdown","source":"### One Hot Encoding on Variation Feature","execution_count":null},{"metadata":{"id":"yeqr8IxteXzf","outputId":"01160873-5f0a-475a-e7d1-afe09a122dc6","executionInfo":{"status":"ok","timestamp":1591382874559,"user_tz":-330,"elapsed":152923,"user":{"displayName":"Faizahmed Momin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjIpwU1PcmQEow-LMt2f-aSLgkwcuid0iQCqGiYLoc=s64","userId":"01335550890493547686"}},"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nvariation_vectorizer=CountVectorizer()\n\nX_Train_variation_Feature_onehotEncoding=variation_vectorizer.fit_transform(X_Train[\"Variation\"])\nprint(\" Train Variation Feature :\" ,X_Train_variation_Feature_onehotEncoding.shape)\n\nprint(\"=\"*100)\n\nX_Test_variation_Feature_onehotencoding=variation_vectorizer.transform(X_Test[\"Variation\"])\nprint(\" Test Variation Feature :\" ,X_Test_variation_Feature_onehotencoding.shape)\n\nprint(\"=\"*100)\n\n\nX_CV_variation_Feature_onehotencoding=variation_vectorizer.transform(X_CV[\"Variation\"])\nprint(\" CV Variation Feature :\" ,X_CV_variation_Feature_onehotencoding.shape)\n\nprint(\"=\"*100)","execution_count":null,"outputs":[]},{"metadata":{"id":"JYWoB-aDffxS"},"cell_type":"markdown","source":"### APPLY SVM -->  SGD CLASSIFIER TO FIND THE BEST HYPERPARAMETER","execution_count":null},{"metadata":{"id":"6OTS_l-YffIf","outputId":"e4b5e018-394b-410e-fc32-ee4310bc9b38","executionInfo":{"status":"ok","timestamp":1591382876259,"user_tz":-330,"elapsed":154589,"user":{"displayName":"Faizahmed Momin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjIpwU1PcmQEow-LMt2f-aSLgkwcuid0iQCqGiYLoc=s64","userId":"01335550890493547686"}},"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import SGDClassifier\nfrom sklearn.calibration import CalibratedClassifierCV\nalpha = [10 ** x for x in range(-5, 1)]\n\ncv_log_error=[]\n\nfor i in alpha :\n  clf=SGDClassifier(alpha=i, penalty='l2', loss='log', random_state=42)\n  clf.fit(X_Train_variation_Feature_onehotEncoding,Y_Train)\n  sig_clf=CalibratedClassifierCV(clf,method=\"sigmoid\")\n  sig_clf.fit(X_Train_variation_Feature_onehotEncoding,Y_Train)\n  Predicted_Y=sig_clf.predict_proba(X_CV_variation_Feature_onehotencoding)\n  cv_log_error.append(log_loss(Y_CV,Predicted_Y,labels=clf.classes_,eps=1e-15))\n  print('For values of alpha = ', i, \"The log loss is:\",log_loss(Y_CV, Predicted_Y, labels=clf.classes_, eps=1e-15))\n\nfig,ax = plt.subplots()\nax.plot(alpha,cv_log_error,c='g')\n\nfor i , txt in enumerate(np.round(cv_log_error,3)):\n  ax.annotate((alpha[i],np.round(txt,3)), (alpha[i],cv_log_error[i]))\nplt.grid()\nplt.title(\"Cross Validation Error for each alpha\")\nplt.xlabel(\"Alpha i's\")\nplt.ylabel(\"Error Measure\")\nplt.show()\n\nbest_alpha = np.argmin(cv_log_error)\nclf = SGDClassifier(alpha=alpha[best_alpha], penalty='l2', loss='log', random_state=42)\nclf.fit(X_Train_variation_Feature_onehotEncoding, Y_Train)\nsig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\nsig_clf.fit(X_Train_variation_Feature_onehotEncoding, Y_Train)\n\npredict_y = sig_clf.predict_proba(X_Train_variation_Feature_onehotEncoding)\nprint('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(Y_Train, predict_y, labels=clf.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(X_CV_variation_Feature_onehotencoding)\nprint('For values of best alpha = ', alpha[best_alpha], \"The cross validation log loss is:\",log_loss(Y_CV, predict_y, labels=clf.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(X_Test_variation_Feature_onehotencoding)\nprint('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(Y_Test, predict_y, labels=clf.classes_, eps=1e-15))","execution_count":null,"outputs":[]},{"metadata":{"id":"qShDuXnwf3vg","outputId":"138c4d0e-d9f2-457b-dcbe-4c5ead2017d5","executionInfo":{"status":"ok","timestamp":1591382876264,"user_tz":-330,"elapsed":154574,"user":{"displayName":"Faizahmed Momin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjIpwU1PcmQEow-LMt2f-aSLgkwcuid0iQCqGiYLoc=s64","userId":"01335550890493547686"}},"trusted":true},"cell_type":"code","source":"test_coverage=X_Test[X_Test['Variation'].isin(list(set(X_Train['Variation'])))].shape[0]\ncv_coverage=X_CV[X_CV['Variation'].isin(list(set(X_Train['Variation'])))].shape[0]\n\nprint('1. In test data',test_coverage, 'out of',X_Test.shape[0], \":\",(test_coverage/X_Test.shape[0])*100)\nprint('2. In cross validation data',cv_coverage, 'out of ',X_CV.shape[0],\":\" ,(cv_coverage/X_CV.shape[0])*100)","execution_count":null,"outputs":[]},{"metadata":{"id":"-5AHFlp-gR7k"},"cell_type":"markdown","source":"## Univariate Analysis on Text Features","execution_count":null},{"metadata":{"id":"rgB8lZU1gVN-","trusted":true},"cell_type":"code","source":"def extract_dictionary_paddle(cls_text):\n    dictionary = defaultdict(int)\n    for index, row in cls_text.iterrows():\n        for word in row['TEXT'].split():\n            dictionary[word] +=1\n    return dictionary","execution_count":null,"outputs":[]},{"metadata":{"id":"9n9qWXt-meQP","trusted":true},"cell_type":"code","source":"import math\n\ndef get_text_responsecoding(df):\n    text_feature_responseCoding = np.zeros((df.shape[0],9))\n    for i in range(0,9):\n        row_index = 0\n        for index, row in df.iterrows():\n            sum_prob = 0\n            for word in row['TEXT'].split():\n                sum_prob += math.log(((dict_list[i].get(word,0)+10 )/(total_dict.get(word,0)+90)))\n            text_feature_responseCoding[row_index][i] = math.exp(sum_prob/len(row['TEXT'].split()))\n            row_index += 1\n    return text_feature_responseCoding","execution_count":null,"outputs":[]},{"metadata":{"id":"9gTk-RKKrj1T"},"cell_type":"markdown","source":"### One Hot Encoding on Text Feature","execution_count":null},{"metadata":{"id":"301KDDDqm8MU","outputId":"e65696ca-c863-477b-94d3-814b1f5fa00e","executionInfo":{"status":"ok","timestamp":1591382885611,"user_tz":-330,"elapsed":163885,"user":{"displayName":"Faizahmed Momin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjIpwU1PcmQEow-LMt2f-aSLgkwcuid0iQCqGiYLoc=s64","userId":"01335550890493547686"}},"trusted":true},"cell_type":"code","source":"text_vectorizer = CountVectorizer(min_df=3)\nX_Train_feature_onehotCoding = text_vectorizer.fit_transform(X_Train['TEXT'])\n# getting all the feature names (words)\nX_Train_text_features= text_vectorizer.get_feature_names()\n\n# train_text_feature_onehotCoding.sum(axis=0).A1 will sum every row and returns (1*number of features) vector\ntrain_text_fea_counts = X_Train_feature_onehotCoding.sum(axis=0).A1\n\n# zip(list(text_features),text_fea_counts) will zip a word with its number of times it occured\ntext_fea_dict = dict(zip(list(X_Train_text_features),train_text_fea_counts))\n\n\nprint(\"Total number of unique words in train data :\", len(X_Train_text_features))","execution_count":null,"outputs":[]},{"metadata":{"id":"f3VkVXvlnk2x","trusted":true},"cell_type":"code","source":"from collections import defaultdict\ndict_list = []\n# dict_list =[] contains 9 dictoinaries each corresponds to a class\nfor i in range(1,10):\n    cls_text = X_Train[X_Train['Class']==i]\n    # build a word dict based on the words in that class\n    dict_list.append(extract_dictionary_paddle(cls_text))\n    # append it to dict_list\n\n\ntotal_dict = extract_dictionary_paddle(X_Train)\n\n\nconfuse_array = []\nfor i in X_Train_text_features:\n    ratios = []\n    max_val = -1\n    for j in range(0,9):\n        ratios.append((dict_list[j][i]+10 )/(total_dict[i]+90))\n    confuse_array.append(ratios)\nconfuse_array = np.array(confuse_array)","execution_count":null,"outputs":[]},{"metadata":{"id":"UwFuqR7zroEV"},"cell_type":"markdown","source":"Response Coding on Text Feature","execution_count":null},{"metadata":{"id":"etMjAhZDrrW5","outputId":"3d9b77a1-b2cb-4dc9-c643-ec56cdc1f3b7","executionInfo":{"status":"ok","timestamp":1591383062458,"user_tz":-330,"elapsed":340702,"user":{"displayName":"Faizahmed Momin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjIpwU1PcmQEow-LMt2f-aSLgkwcuid0iQCqGiYLoc=s64","userId":"01335550890493547686"}},"trusted":true},"cell_type":"code","source":"X_Train_text_feature_responseCoding  = get_text_responsecoding(X_Train)\nprint(\"Train Text Feature :\", X_Train_text_feature_responseCoding.shape)\n\nprint(\"=\"*100)\n\nX_Test_text_feature_responseCoding  = get_text_responsecoding(X_Test)\nprint(\"Test Text Feature :\", X_Test_text_feature_responseCoding.shape)\n\nprint(\"=\"*100)\nX_CV_text_feature_responseCoding  = get_text_responsecoding(X_CV)\nprint(\"CV Text Feature :\", X_CV_text_feature_responseCoding.shape)\n\nprint(\"=\"*100)","execution_count":null,"outputs":[]},{"metadata":{"id":"Yvsur4RztkIN","trusted":true},"cell_type":"code","source":"X_Train_text_feature_responseCoding = (X_Train_text_feature_responseCoding.T/X_Train_text_feature_responseCoding.sum(axis=1)).T\nX_Test_text_feature_responseCoding = (X_Test_text_feature_responseCoding.T/X_Test_text_feature_responseCoding.sum(axis=1)).T\nX_CV_text_feature_responseCoding = (X_CV_text_feature_responseCoding.T/X_CV_text_feature_responseCoding.sum(axis=1)).T","execution_count":null,"outputs":[]},{"metadata":{"id":"ZGhn4WoZt1_w"},"cell_type":"markdown","source":"### Normalize the features","execution_count":null},{"metadata":{"id":"1sDO1yoLt1iC","outputId":"1fda0967-8c6e-4cc7-b7b7-d9545b5117b4","executionInfo":{"status":"ok","timestamp":1591383067925,"user_tz":-330,"elapsed":346087,"user":{"displayName":"Faizahmed Momin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjIpwU1PcmQEow-LMt2f-aSLgkwcuid0iQCqGiYLoc=s64","userId":"01335550890493547686"}},"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import normalize\n\nX_Train_feature_onehotCoding = normalize(X_Train_feature_onehotCoding, axis=0)\nprint(\"Train Text Feature :\", X_Train_feature_onehotCoding.shape)\n\nprint(\"=\"*100)\n\nX_Test_text_feature_onehotCoding = text_vectorizer.transform(X_Test['TEXT'])\nprint(\"Test Text Feature :\", X_Test_text_feature_onehotCoding.shape)\n\nprint(\"=\"*100)\n\nX_Test_text_feature_onehotCoding = normalize(X_Test_text_feature_onehotCoding, axis=0)\n\nX_CV_text_feature_onehotCoding = text_vectorizer.transform(X_CV['TEXT'])\nprint(\"CV Text Feature :\", X_CV_text_feature_onehotCoding.shape)\n\nprint(\"=\"*100)\n\nX_CV_text_feature_onehotCoding = normalize(X_CV_text_feature_onehotCoding, axis=0)","execution_count":null,"outputs":[]},{"metadata":{"id":"BSrbyUpmu7FH","trusted":true},"cell_type":"code","source":"sorted_text_fea_dict = dict(sorted(text_fea_dict.items(), key=lambda x: x[1] , reverse=True))\nsorted_text_occur = np.array(list(sorted_text_fea_dict.values())) ","execution_count":null,"outputs":[]},{"metadata":{"id":"ZVJVuDrKxhMF"},"cell_type":"markdown","source":"### APPLY SVM -->  SGD CLASSIFIER TO FIND THE BEST HYPERPARAMETER","execution_count":null},{"metadata":{"id":"NjAU7HCJxhxe","outputId":"4dc68df1-d507-49d3-deda-287f382d1983","executionInfo":{"status":"ok","timestamp":1591383160557,"user_tz":-330,"elapsed":438670,"user":{"displayName":"Faizahmed Momin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjIpwU1PcmQEow-LMt2f-aSLgkwcuid0iQCqGiYLoc=s64","userId":"01335550890493547686"}},"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import SGDClassifier\nfrom sklearn.calibration import CalibratedClassifierCV\nalpha = [10 ** x for x in range(-5, 1)]\n\ncv_log_error=[]\n\nfor i in alpha :\n  clf=SGDClassifier(alpha=i, penalty='l2', loss='log', random_state=42)\n  clf.fit(X_Train_feature_onehotCoding,Y_Train)\n  sig_clf=CalibratedClassifierCV(clf,method=\"sigmoid\")\n  sig_clf.fit(X_Train_feature_onehotCoding,Y_Train)\n  Predicted_Y=sig_clf.predict_proba(X_CV_text_feature_onehotCoding)\n  cv_log_error.append(log_loss(Y_CV,Predicted_Y,labels=clf.classes_,eps=1e-15))\n  print('For values of alpha = ', i, \"The log loss is:\",log_loss(Y_CV, Predicted_Y, labels=clf.classes_, eps=1e-15))\n\nfig,ax = plt.subplots()\nax.plot(alpha,cv_log_error,c='g')\n\nfor i , txt in enumerate(np.round(cv_log_error,3)):\n  ax.annotate((alpha[i],np.round(txt,3)), (alpha[i],cv_log_error[i]))\nplt.grid()\nplt.title(\"Cross Validation Error for each alpha\")\nplt.xlabel(\"Alpha i's\")\nplt.ylabel(\"Error Measure\")\nplt.show()\n\nbest_alpha = np.argmin(cv_log_error)\nclf = SGDClassifier(alpha=alpha[best_alpha], penalty='l2', loss='log', random_state=42)\nclf.fit(X_Train_feature_onehotCoding, Y_Train)\nsig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\nsig_clf.fit(X_Train_feature_onehotCoding, Y_Train)\n\npredict_y = sig_clf.predict_proba(X_Train_feature_onehotCoding)\nprint('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(Y_Train, predict_y, labels=clf.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(X_CV_text_feature_onehotCoding)\nprint('For values of best alpha = ', alpha[best_alpha], \"The cross validation log loss is:\",log_loss(Y_CV, predict_y, labels=clf.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(X_Test_text_feature_onehotCoding)\nprint('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(Y_Test, predict_y, labels=clf.classes_, eps=1e-15))","execution_count":null,"outputs":[]},{"metadata":{"id":"TcYkDbR7rlxU"},"cell_type":"markdown","source":"## BUILDING A MACHINE LEARNING MODELS","execution_count":null},{"metadata":{"id":"vQ2vgSZHrpAU","trusted":true},"cell_type":"code","source":"def predict_and_plot_confusionmatrix(X_train,Y_train,X_test,Y_test,clf):\n  clf.fit(X_train,Y_train)\n  sig_clf=CalibratedClassifierCV(clf,method=\"sigmoid\")\n  sig_clf.fit(X_train,Y_train)\n  pred_y=sig_clf.predict(X_test)\n\n\n  print(\"Log Loss : \",log_loss(Y_test,sig_clf.predict_proba(X_test)))\n  print(\"Number of Misclassified Points :\",np.count_nonzero((pred_y - Y_test))/Y_test.shape[0])\n\n  plot_confusion_matrix(Y_test,pred_y)","execution_count":null,"outputs":[]},{"metadata":{"id":"tb8udj6it2Yu","trusted":true},"cell_type":"code","source":"def report_log_loss(X_train,Y_train,X_test,Y_test,clf):\n  clf.fit(X_Train,Y_train)\n  sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n  sig_clf.fit(X_train,Y_train)\n  sig_clf_probs=sig_clf.predict_proba(X_Test)\n  return log_loss(Y_Test, sig_clf_probs, eps=1e-15)","execution_count":null,"outputs":[]},{"metadata":{"id":"gafbHZBlG6tL"},"cell_type":"markdown","source":"## Get Feature Names ","execution_count":null},{"metadata":{"id":"L7jS6dEWvnJj","trusted":true},"cell_type":"code","source":"def get_impfeature_names_tfidf(indices, text, gene, var, no_features):\n    gene_count_vec = TfidfVectorizer()\n    var_count_vec = TfidfVectorizer()\n    text_count_vec = TfidfVectorizer(min_df=3)\n    print (\"Hello\")\n    gene_vec = gene_count_vec.fit(X_Train['Gene'])\n    var_vec  = var_count_vec.fit(X_Train['Variation'])\n    text_vec = text_count_vec.fit(X_Train['TEXT'])\n    \n    fea1_len = len(gene_vec.get_feature_names())\n    fea2_len = len(var_count_vec.get_feature_names())\n    \n    word_present = 0\n    for i,v in enumerate(indices):\n        if (v < fea1_len):\n            word = gene_vec.get_feature_names()[v]\n            yes_no = True if word == gene else False\n            if yes_no:\n                word_present += 1\n                print(i, \"Gene feature [{}] present in test data point [{}]\".format(word,yes_no))\n        elif (v < fea1_len+fea2_len):\n            word = var_vec.get_feature_names()[v-(fea1_len)]\n            yes_no = True if word == var else False\n            if yes_no:\n                word_present += 1\n                print(i, \"variation feature [{}] present in test data point [{}]\".format(word,yes_no))\n        else:\n            word = text_vec.get_feature_names()[v-(fea1_len+fea2_len)]\n            yes_no = True if word in text.split() else False\n            if yes_no:\n                word_present += 1\n                print(i, \"Text feature [{}] present in test data point [{}]\".format(word,yes_no))\n\n    print(\"Out of the top \",no_features,\" features \", word_present, \"are present in query point\")","execution_count":null,"outputs":[]},{"metadata":{"id":"dW0cCzovxhip"},"cell_type":"markdown","source":"## STACKING THE THREE TYPES OF FEATURES","execution_count":null},{"metadata":{"id":"LBO6nz8lxmUs","trusted":true},"cell_type":"code","source":"from imblearn.over_sampling import SMOTE\nfrom collections import Counter\nfrom scipy.sparse import hstack\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.svm import SVC\nfrom collections import Counter, defaultdict\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nimport math\nfrom sklearn.metrics import normalized_mutual_info_score\nfrom sklearn.ensemble import RandomForestClassifier\nwarnings.filterwarnings(\"ignore\")\n\nfrom mlxtend.classifier import StackingClassifier\n\nfrom sklearn import model_selection\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.preprocessing import normalize\nfrom collections import defaultdict\nimport math\nfrom sklearn.metrics import log_loss\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"id":"i2z4maWoyxMR","outputId":"bc3cecf2-10c1-41c8-ce49-02a0f0a59e76","executionInfo":{"status":"ok","timestamp":1591383160926,"user_tz":-330,"elapsed":438978,"user":{"displayName":"Faizahmed Momin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjIpwU1PcmQEow-LMt2f-aSLgkwcuid0iQCqGiYLoc=s64","userId":"01335550890493547686"}},"trusted":true},"cell_type":"code","source":"X_Train_gene_var_onehotcoding = hstack((X_Train_gene_Feature_onehotEncoding,X_Train_variation_Feature_onehotEncoding))\nX_Test_gene_var_onehotcoding =  hstack((X_Test_gene_Feature_onehotencoding,X_Test_variation_Feature_onehotencoding))\nX_CV_gene_var_onehotcoding = hstack((X_CV_gene_Feature_onehotencoding,X_CV_variation_Feature_onehotencoding))\n\nX_Train_onehotCoding = hstack((X_Train_gene_var_onehotcoding,X_Train_feature_onehotCoding)).tocsr()\nprint(\"Train One Hot Encoding  :\", X_Train_onehotCoding.shape)\nprint(\"=\"*100)\n\nX_Test_onehotcoding = hstack((X_Test_gene_var_onehotcoding,X_Test_text_feature_onehotCoding))\nprint(\"Test One Hot Encoding  :\", X_Test_onehotcoding.shape)\nprint(\"=\"*100)\n\nX_CV_onehotcoding = hstack((X_CV_gene_var_onehotcoding,X_CV_text_feature_onehotCoding))\nprint(\"CV One Hot Encoding  :\", X_CV_onehotcoding.shape)\nprint(\"=\"*100)\n\n\nX_Train_gene_var_responseCoding = np.hstack((X_Train_gene_Feature_responsecoding,X_Train_variation_Feature_responsecoding))\nX_Test_gene_var_responseCoding = np.hstack((X_Test_gen_Feature_responsecoding,X_Test_variation_Feature_responsecoding))\nX_CV_gene_var_responseCoding = np.hstack((X_CV_gene_Feature_responsecoding,X_CV_variation_Feature_responsecoding))\n\nX_Train_responseCoding = np.hstack((X_Train_gene_var_responseCoding, X_Train_text_feature_responseCoding))\nprint(\"Train Response Coding  :\", X_Train_responseCoding.shape)\nprint(\"=\"*100)\n\nX_Test_responseCoding = np.hstack((X_Test_gene_var_responseCoding, X_Test_text_feature_responseCoding))\nprint(\"Test Response Coding  :\", X_Test_responseCoding.shape)\nprint(\"=\"*100)\nX_CV_responseCoding = np.hstack((X_CV_gene_var_responseCoding, X_CV_text_feature_responseCoding))\nprint(\"CV Response Coding  :\", X_CV_responseCoding.shape)\nprint(\"=\"*100)","execution_count":null,"outputs":[]},{"metadata":{"id":"jwvXVBauYIl2"},"cell_type":"markdown","source":"# BASE LINE MODEL","execution_count":null},{"metadata":{"id":"8nSAxb2XYQ9Y"},"cell_type":"markdown","source":"## NAIVE BAYES ALGORITHM","execution_count":null},{"metadata":{"id":"Ok14l5a_YODe","trusted":true},"cell_type":"code","source":"def Naive_Bayes_Algo(X_Train,Y_Train,X_CV,Y_CV,X_Test,Y_Test):\n\n  alpha = [0.00001, 0.0001, 0.001, 0.1, 1, 10, 100,1000]\n  cv_log_error=[]\n\n  for i in alpha:\n    print(\"for alpha =\", i)\n    clf = MultinomialNB(alpha=i)\n    clf.fit(X_Train_onehotCoding,Y_Train)\n    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n    sig_clf.fit(X_Train_onehotCoding, Y_Train)\n    sig_clf_probs = sig_clf.predict_proba(X_CV_onehotcoding)\n    cv_log_error.append(log_loss(Y_CV, sig_clf_probs, labels=clf.classes_, eps=1e-15))\n    #print(\"Log Loss :\",log_loss(Y_CV, sig_clf_probs))\n\n  fig, ax = plt.subplots()\n  ax.plot(np.log10(alpha), cv_log_error,c='g')\n  for i , txt in enumerate(np.round(cv_log_error,3)):\n    ax.annotate((alpha[i],str(txt)), (np.log10(alpha[i]),cv_log_error[i]))\n  plt.grid()\n  plt.xticks(np.log10(alpha))\n  plt.title(\"Cross Validation Error for each alpha\")\n  plt.xlabel(\"Alpha i's\")\n  plt.ylabel(\"Error measure\")\n  plt.show()\n\n  best_alpha = np.argmin(cv_log_error)\n  clf = MultinomialNB(alpha=alpha[best_alpha])\n  clf.fit(X_Train_onehotCoding, Y_Train)\n  sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n  sig_clf.fit(X_Train_onehotCoding, Y_Train)\n\n  predict_y = sig_clf.predict_proba(X_Train_onehotCoding)\n  print('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(Y_Train, predict_y, labels=clf.classes_, eps=1e-15))\n  \n  predict_y = sig_clf.predict_proba(X_CV_onehotcoding)\n  print('For values of best alpha = ', alpha[best_alpha], \"The cross validation log loss is:\",log_loss(Y_CV, predict_y, labels=clf.classes_, eps=1e-15))\n  \n  predict_y = sig_clf.predict_proba(X_Test_onehotcoding)\n  print('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(Y_Test, predict_y, labels=clf.classes_, eps=1e-15))\n\n  return alpha,best_alpha","execution_count":null,"outputs":[]},{"metadata":{"id":"T0jf72Kaeyli","outputId":"369fb92d-b23f-4633-9fe0-7fa215eacf39","executionInfo":{"status":"ok","timestamp":1591383167922,"user_tz":-330,"elapsed":445938,"user":{"displayName":"Faizahmed Momin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjIpwU1PcmQEow-LMt2f-aSLgkwcuid0iQCqGiYLoc=s64","userId":"01335550890493547686"}},"trusted":true},"cell_type":"code","source":"alpha,best_alpha = Naive_Bayes_Algo(X_Train_onehotCoding,Y_Train,X_CV_onehotcoding,Y_CV,X_Test_onehotcoding,Y_Test)","execution_count":null,"outputs":[]},{"metadata":{"id":"9ilABfgle4Lf","outputId":"18cc9317-e3e6-4bf4-94c3-78c34b739aa6","executionInfo":{"status":"ok","timestamp":1591383171407,"user_tz":-330,"elapsed":449396,"user":{"displayName":"Faizahmed Momin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjIpwU1PcmQEow-LMt2f-aSLgkwcuid0iQCqGiYLoc=s64","userId":"01335550890493547686"}},"trusted":true},"cell_type":"code","source":"clf = MultinomialNB(alpha=alpha[best_alpha])\nclf.fit(X_Train_onehotCoding, Y_Train)\nsig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\nsig_clf.fit(X_Train_onehotCoding, Y_Train)\nsig_clf_probs = sig_clf.predict_proba(X_CV_onehotcoding)\n\nprint(\"Log Loss :\",log_loss(Y_CV, sig_clf_probs))\n\nprint(\"Number of missclassified point :\", np.count_nonzero((sig_clf.predict(X_CV_onehotcoding)- Y_CV))/Y_CV.shape[0])\nplot_confusion_matrix(Y_CV, sig_clf.predict(X_CV_onehotcoding.toarray()))","execution_count":null,"outputs":[]},{"metadata":{"id":"6285UYGbl5PB"},"cell_type":"markdown","source":"## K-NEAREST NEIGHBORS ALGORITHM ","execution_count":null},{"metadata":{"id":"xCPXjKvNanvc"},"cell_type":"markdown","source":"## LOGISTIC REGRESSION (WITH CLASS BALANCING)","execution_count":null},{"metadata":{"id":"s-b3wVYeatBi","trusted":true},"cell_type":"code","source":"def Logistic_Regression_Algo(X_Train,Y_Train,X_CV,Y_CV,X_Test,Y_Test):\n\n  alpha = [10 ** x for x in range(-6, 3)]\n  cv_log_error=[]\n\n  for i in alpha:\n    print(\"for alpha =\", i)\n    clf = SGDClassifier(class_weight='balanced', alpha=i, penalty='l2', loss='log', random_state=42)\n    clf.fit(X_Train_onehotCoding,Y_Train)\n    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n    sig_clf.fit(X_Train_onehotCoding, Y_Train)\n    sig_clf_probs = sig_clf.predict_proba(X_CV_onehotcoding)\n    cv_log_error.append(log_loss(Y_CV, sig_clf_probs, labels=clf.classes_, eps=1e-15))\n    print(\"Log Loss :\",log_loss(Y_CV, sig_clf_probs))\n\n  fig, ax = plt.subplots()\n  ax.plot(np.log10(alpha), cv_log_error,c='g')\n  for i , txt in enumerate(np.round(cv_log_error,3)):\n    ax.annotate((alpha[i],str(txt)), (np.log10(alpha[i]),cv_log_error[i]))\n  plt.grid()\n  plt.xticks(np.log10(alpha))\n  plt.title(\"Cross Validation Error for each alpha\")\n  plt.xlabel(\"Alpha i's\")\n  plt.ylabel(\"Error measure\")\n  plt.show()\n\n  best_alpha = np.argmin(cv_log_error)\n  clf = SGDClassifier(class_weight='balanced', alpha=alpha[best_alpha], penalty='l2', loss='log', random_state=42)\n  clf.fit(X_Train_onehotCoding, Y_Train)\n  sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n  sig_clf.fit(X_Train_onehotCoding, Y_Train)\n\n  predict_y = sig_clf.predict_proba(X_Train_onehotCoding)\n  print('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(Y_Train, predict_y, labels=clf.classes_, eps=1e-15))\n  \n  predict_y = sig_clf.predict_proba(X_CV_onehotcoding)\n  print('For values of best alpha = ', alpha[best_alpha], \"The cross validation log loss is:\",log_loss(Y_CV, predict_y, labels=clf.classes_, eps=1e-15))\n  \n  predict_y = sig_clf.predict_proba(X_Test_onehotcoding)\n  print('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(Y_Test, predict_y, labels=clf.classes_, eps=1e-15))\n\n  return alpha,best_alpha","execution_count":null,"outputs":[]},{"metadata":{"id":"9_FXRyPdb_fL","outputId":"5ce7849f-02b8-43f7-9f7f-5894de9bd09d","executionInfo":{"status":"ok","timestamp":1591383280504,"user_tz":-330,"elapsed":558380,"user":{"displayName":"Faizahmed Momin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjIpwU1PcmQEow-LMt2f-aSLgkwcuid0iQCqGiYLoc=s64","userId":"01335550890493547686"}},"trusted":true},"cell_type":"code","source":"alpha,best_alpha = Logistic_Regression_Algo(X_Train,Y_Train,X_CV,Y_CV,X_Test,Y_Test)","execution_count":null,"outputs":[]},{"metadata":{"id":"8Qy2vWU6j0YD","outputId":"8c235018-3693-483c-d8cb-c53cc9b8de09","executionInfo":{"status":"ok","timestamp":1591383294740,"user_tz":-330,"elapsed":572592,"user":{"displayName":"Faizahmed Momin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjIpwU1PcmQEow-LMt2f-aSLgkwcuid0iQCqGiYLoc=s64","userId":"01335550890493547686"}},"trusted":true},"cell_type":"code","source":"clf = SGDClassifier(class_weight='balanced', alpha=alpha[best_alpha], penalty='l2', loss='log', random_state=42)\npredict_and_plot_confusionmatrix(X_Train_onehotCoding, Y_Train, X_CV_onehotcoding, Y_CV, clf)","execution_count":null,"outputs":[]},{"metadata":{"id":"13ucsPBEi1EA"},"cell_type":"markdown","source":"## LOGISTIC REGRESSION WITHOUT CLASS BALANCING ","execution_count":null},{"metadata":{"id":"eouj2EUki5-d","trusted":true},"cell_type":"code","source":"def Logistic_Regression_Algo_WithoutClassBalancing(X_Train,Y_Train,X_CV,Y_CV,X_Test,Y_Test):\n\n  alpha = [10 ** x for x in range(-6, 3)]\n  cv_log_error=[]\n\n  for i in alpha:\n    print(\"for alpha =\", i)\n    clf = SGDClassifier(alpha=i, penalty='l2', loss='log', random_state=42)\n    clf.fit(X_Train_onehotCoding,Y_Train)\n    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n    sig_clf.fit(X_Train_onehotCoding, Y_Train)\n    sig_clf_probs = sig_clf.predict_proba(X_CV_onehotcoding)\n    cv_log_error.append(log_loss(Y_CV, sig_clf_probs, labels=clf.classes_, eps=1e-15))\n    print(\"Log Loss :\",log_loss(Y_CV, sig_clf_probs))\n\n  fig, ax = plt.subplots()\n  ax.plot(np.log10(alpha), cv_log_error,c='g')\n  for i , txt in enumerate(np.round(cv_log_error,3)):\n    ax.annotate((alpha[i],str(txt)), (np.log10(alpha[i]),cv_log_error[i]))\n  plt.grid()\n  plt.xticks(np.log10(alpha))\n  plt.title(\"Cross Validation Error for each alpha\")\n  plt.xlabel(\"Alpha i's\")\n  plt.ylabel(\"Error measure\")\n  plt.show()\n\n  best_alpha = np.argmin(cv_log_error)\n  clf = SGDClassifier(alpha=alpha[best_alpha], penalty='l2', loss='log', random_state=42)\n  clf.fit(X_Train_onehotCoding, Y_Train)\n  sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n  sig_clf.fit(X_Train_onehotCoding, Y_Train)\n\n  predict_y = sig_clf.predict_proba(X_Train_onehotCoding)\n  print('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(Y_Train, predict_y, labels=clf.classes_, eps=1e-15))\n  \n  predict_y = sig_clf.predict_proba(X_CV_onehotcoding)\n  print('For values of best alpha = ', alpha[best_alpha], \"The cross validation log loss is:\",log_loss(Y_CV, predict_y, labels=clf.classes_, eps=1e-15))\n  \n  predict_y = sig_clf.predict_proba(X_Test_onehotcoding)\n  print('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(Y_Test, predict_y, labels=clf.classes_, eps=1e-15))\n\n  return alpha,best_alpha","execution_count":null,"outputs":[]},{"metadata":{"id":"7_G_07MRjdPh","outputId":"326f91e8-ecf9-4cae-a7fb-355156759d87","executionInfo":{"status":"ok","timestamp":1591383397760,"user_tz":-330,"elapsed":675594,"user":{"displayName":"Faizahmed Momin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjIpwU1PcmQEow-LMt2f-aSLgkwcuid0iQCqGiYLoc=s64","userId":"01335550890493547686"}},"trusted":true},"cell_type":"code","source":"alpha,best_alpha = Logistic_Regression_Algo_WithoutClassBalancing(X_Train,Y_Train,X_CV,Y_CV,X_Test,Y_Test)","execution_count":null,"outputs":[]},{"metadata":{"id":"XfDT_WznkP0g","outputId":"a18006a9-8323-4192-8541-a38fb5fa2fb4","executionInfo":{"status":"ok","timestamp":1591383410376,"user_tz":-330,"elapsed":688195,"user":{"displayName":"Faizahmed Momin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjIpwU1PcmQEow-LMt2f-aSLgkwcuid0iQCqGiYLoc=s64","userId":"01335550890493547686"}},"trusted":true},"cell_type":"code","source":"clf = SGDClassifier( alpha=alpha[best_alpha], penalty='l2', loss='log', random_state=42)\npredict_and_plot_confusionmatrix(X_Train_onehotCoding, Y_Train, X_CV_onehotcoding, Y_CV, clf)","execution_count":null,"outputs":[]},{"metadata":{"id":"uoIeeY9yke6P"},"cell_type":"markdown","source":"## LINEAR SUPPORT VECTOR MACHINE","execution_count":null},{"metadata":{"id":"-Vf8XooxkhwP","trusted":true},"cell_type":"code","source":"def LinearSVM_Algo(X_Train,Y_Train,X_CV,Y_CV,X_Test,Y_Test):\n\n  alpha = [10 ** x for x in range(-6, 3)]\n  cv_log_error=[]\n\n  for i in alpha:\n    print(\"for alpha =\", i)\n    clf = SGDClassifier(class_weight='balanced', alpha=i, penalty='l2', loss='hinge', random_state=42)\n    clf.fit(X_Train_onehotCoding,Y_Train)\n    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n    sig_clf.fit(X_Train_onehotCoding, Y_Train)\n    sig_clf_probs = sig_clf.predict_proba(X_CV_onehotcoding)\n    cv_log_error.append(log_loss(Y_CV, sig_clf_probs, labels=clf.classes_, eps=1e-15))\n    print(\"Log Loss :\",log_loss(Y_CV, sig_clf_probs))\n\n  fig, ax = plt.subplots()\n  ax.plot(np.log10(alpha), cv_log_error,c='g')\n  for i , txt in enumerate(np.round(cv_log_error,3)):\n    ax.annotate((alpha[i],str(txt)), (np.log10(alpha[i]),cv_log_error[i]))\n  plt.grid()\n  plt.xticks(np.log10(alpha))\n  plt.title(\"Cross Validation Error for each alpha\")\n  plt.xlabel(\"Alpha i's\")\n  plt.ylabel(\"Error measure\")\n  plt.show()\n\n  best_alpha = np.argmin(cv_log_error)\n  clf = SGDClassifier(class_weight='balanced', alpha=alpha[best_alpha], penalty='l2', loss='hinge', random_state=42)\n  clf.fit(X_Train_onehotCoding, Y_Train)\n  sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n  sig_clf.fit(X_Train_onehotCoding, Y_Train)\n\n  predict_y = sig_clf.predict_proba(X_Train_onehotCoding)\n  print('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(Y_Train, predict_y, labels=clf.classes_, eps=1e-15))\n  \n  predict_y = sig_clf.predict_proba(X_CV_onehotcoding)\n  print('For values of best alpha = ', alpha[best_alpha], \"The cross validation log loss is:\",log_loss(Y_CV, predict_y, labels=clf.classes_, eps=1e-15))\n  \n  predict_y = sig_clf.predict_proba(X_Test_onehotcoding)\n  print('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(Y_Test, predict_y, labels=clf.classes_, eps=1e-15))\n\n  return alpha,best_alpha","execution_count":null,"outputs":[]},{"metadata":{"id":"vlQsZ7JGk345","outputId":"edd7896f-50be-47b8-86ee-ec77153f308c","executionInfo":{"status":"ok","timestamp":1591383493658,"user_tz":-330,"elapsed":771461,"user":{"displayName":"Faizahmed Momin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjIpwU1PcmQEow-LMt2f-aSLgkwcuid0iQCqGiYLoc=s64","userId":"01335550890493547686"}},"trusted":true},"cell_type":"code","source":"alpha,best_alpha = LinearSVM_Algo(X_Train,Y_Train,X_CV,Y_CV,X_Test,Y_Test)","execution_count":null,"outputs":[]},{"metadata":{"id":"eNyxACHvpDx9"},"cell_type":"markdown","source":"## RANDOM FOREST CLASSIFIER","execution_count":null},{"metadata":{"id":"dECfaEPdpNP2","trusted":true},"cell_type":"code","source":"def Random_Forest_Algo(X_Train,Y_Train,X_CV,Y_CV,X_Test,Y_Test):\n  alpha = [100,200,500,1000,2000]\n  max_depth = [5, 10]\n  cv_log_error = []\n  for i in alpha:\n      for j in max_depth:\n          print(\"for n_estimators =\", i,\"and max depth = \", j)\n          clf = RandomForestClassifier(n_estimators=i, criterion='gini', max_depth=j, random_state=42, n_jobs=-1)\n          clf.fit(X_Train_onehotCoding, Y_Train)\n          sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n          sig_clf.fit(X_Train_onehotCoding, Y_Train)\n          sig_clf_probs = sig_clf.predict_proba(X_CV_onehotcoding)\n          cv_log_error.append(log_loss(Y_CV, sig_clf_probs, labels=clf.classes_, eps=1e-15))\n          print(\"Log Loss :\",log_loss(Y_CV, sig_clf_probs))   \n  best_alpha = np.argmin(cv_log_error)\n  clf = RandomForestClassifier(n_estimators=alpha[int(best_alpha/2)], criterion='gini', max_depth=max_depth[int(best_alpha%2)], random_state=42, n_jobs=-1)\n  clf.fit(X_Train_onehotCoding, Y_Train)\n  sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n  sig_clf.fit(X_Train_onehotCoding, Y_Train)\n\n  predict_y = sig_clf.predict_proba(X_Train_onehotCoding)\n  print('For values of best estimator = ', alpha[int(best_alpha/2)], \"The train log loss is:\",log_loss(Y_Train, predict_y, labels=clf.classes_, eps=1e-15))\n  predict_y = sig_clf.predict_proba(X_CV_onehotcoding)\n  print('For values of best estimator = ', alpha[int(best_alpha/2)], \"The cross validation log loss is:\",log_loss(Y_CV, predict_y, labels=clf.classes_, eps=1e-15))\n  predict_y = sig_clf.predict_proba(X_Test_onehotcoding)\n  print('For values of best estimator = ', alpha[int(best_alpha/2)], \"The test log loss is:\",log_loss(Y_Test, predict_y, labels=clf.classes_, eps=1e-15))\n  return alpha ,best_alpha,max_depth","execution_count":null,"outputs":[]},{"metadata":{"id":"06nMutxOgb_u","outputId":"2da2366f-571e-4ee1-9444-418d750011b9","executionInfo":{"status":"ok","timestamp":1591385322878,"user_tz":-330,"elapsed":2600666,"user":{"displayName":"Faizahmed Momin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjIpwU1PcmQEow-LMt2f-aSLgkwcuid0iQCqGiYLoc=s64","userId":"01335550890493547686"}},"trusted":true},"cell_type":"code","source":"alpha,best_alpha,max_depth = Random_Forest_Algo(X_Train,Y_Train,X_CV,Y_CV,X_Test,Y_Test)","execution_count":null,"outputs":[]},{"metadata":{"id":"AIEcs4fNhJxx","outputId":"c5bf292c-fd4b-4c69-a5d6-dd54b09b2f27","executionInfo":{"status":"ok","timestamp":1591385874011,"user_tz":-330,"elapsed":3151786,"user":{"displayName":"Faizahmed Momin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjIpwU1PcmQEow-LMt2f-aSLgkwcuid0iQCqGiYLoc=s64","userId":"01335550890493547686"}},"trusted":true},"cell_type":"code","source":"clf = RandomForestClassifier(n_estimators=alpha[int(best_alpha/2)], criterion='gini', max_depth=max_depth[int(best_alpha%2)], random_state=42, n_jobs=-1)\npredict_and_plot_confusionmatrix(X_Train_onehotCoding, Y_Train, X_CV_onehotcoding, Y_CV, clf)","execution_count":null,"outputs":[]},{"metadata":{"id":"tTsx-ly2zkAA"},"cell_type":"markdown","source":"# TF-IDF FEATURIZATION TECHNIQUE ","execution_count":null},{"metadata":{"id":"Kz9N1iuDz5RV"},"cell_type":"markdown","source":"## TF-IDF on Gene Features ","execution_count":null},{"metadata":{"id":"HUNipimnzntZ","outputId":"f6250007-ef23-4c4e-e081-215ffdb83d80","executionInfo":{"status":"ok","timestamp":1591643896018,"user_tz":-330,"elapsed":1583,"user":{"displayName":"Faizahmed Momin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjIpwU1PcmQEow-LMt2f-aSLgkwcuid0iQCqGiYLoc=s64","userId":"01335550890493547686"}},"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\ngene_vectorizer=TfidfVectorizer()\n\nX_Train_gene_Feature_onehotencoding_tfidf=gene_vectorizer.fit_transform(X_Train[\"Gene\"])\nprint(\" Train Gene Feature :\" ,X_Train_gene_Feature_onehotencoding_tfidf.shape)\n\nprint(\"=\"*100)\n\nX_Test_gene_Feature_onehotencoding_tfidf=gene_vectorizer.transform(X_Test[\"Gene\"])\nprint(\" Test Gene Feature :\" ,X_Test_gene_Feature_onehotencoding_tfidf.shape)\n\nprint(\"=\"*100)\n\n\nX_CV_gene_Feature_onehotencoding_tfidf=gene_vectorizer.transform(X_CV[\"Gene\"])\nprint(\" CV Gene Feature :\" ,X_CV_gene_Feature_onehotencoding_tfidf.shape)\n\nprint(\"=\"*100)","execution_count":null,"outputs":[]},{"metadata":{"id":"IXb_QI0J5hew"},"cell_type":"markdown","source":"## TF-IDF on Variation Features","execution_count":null},{"metadata":{"id":"XFHOXg5A5glX","outputId":"4cf3043b-37ee-481f-a8bb-898e7939170e","executionInfo":{"status":"ok","timestamp":1591643900870,"user_tz":-330,"elapsed":1300,"user":{"displayName":"Faizahmed Momin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjIpwU1PcmQEow-LMt2f-aSLgkwcuid0iQCqGiYLoc=s64","userId":"01335550890493547686"}},"trusted":true},"cell_type":"code","source":"variation_vectorizer=TfidfVectorizer()\n\nX_Train_variation_Feature_onehotencoding_tfidf=variation_vectorizer.fit_transform(X_Train[\"Variation\"])\nprint(\" Train Variation Feature :\" ,X_Train_variation_Feature_onehotencoding_tfidf.shape)\n\nprint(\"=\"*100)\n\nX_Test_variation_Feature_onehotencoding_tfidf=variation_vectorizer.transform(X_Test[\"Variation\"])\nprint(\" Test Variation Feature :\" ,X_Test_variation_Feature_onehotencoding_tfidf.shape)\n\nprint(\"=\"*100)\n\n\nX_CV_variation_Feature_onehotencoding_tfidf=variation_vectorizer.transform(X_CV[\"Variation\"])\nprint(\" CV Variation Feature :\" ,X_CV_variation_Feature_onehotencoding_tfidf.shape)\n\nprint(\"=\"*100)","execution_count":null,"outputs":[]},{"metadata":{"id":"CEmoQg8e6BI9"},"cell_type":"markdown","source":"## TF-IDF on Text Features ","execution_count":null},{"metadata":{"id":"9RzuNt1i6FOT","trusted":true},"cell_type":"code","source":"text_vectorizer = TfidfVectorizer(min_df=5,ngram_range=(1,4),max_features=3000)\nX_Train_feature_onehotencoding_tfidf = text_vectorizer.fit_transform(X_Train['TEXT'])\n# getting all the feature names (words)","execution_count":null,"outputs":[]},{"metadata":{"id":"7OVdbyjN6-he","outputId":"c965b1f4-9814-41b1-dfce-eac091c7d657","executionInfo":{"status":"ok","timestamp":1591646009742,"user_tz":-330,"elapsed":26668,"user":{"displayName":"Faizahmed Momin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjIpwU1PcmQEow-LMt2f-aSLgkwcuid0iQCqGiYLoc=s64","userId":"01335550890493547686"}},"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import normalize\n\nX_Train_feature_onehotencoding_tfidf = normalize(X_Train_feature_onehotencoding_tfidf, axis=0)\nprint(\"Train Text Feature :\", X_Train_feature_onehotencoding_tfidf.shape)\n\nprint(\"=\"*100)\n\nX_Test_text_feature_onehotencoding_tfidf = text_vectorizer.transform(X_Test['TEXT'])\nprint(\"Test Text Feature :\", X_Test_text_feature_onehotencoding_tfidf.shape)\n\nprint(\"=\"*100)\n\nX_Test_text_feature_onehotencoding_tfidf = normalize(X_Test_text_feature_onehotencoding_tfidf, axis=0)\n\nX_CV_text_feature_onehotencoding_tfidf = text_vectorizer.transform(X_CV['TEXT'])\nprint(\"CV Text Feature :\", X_CV_text_feature_onehotencoding_tfidf.shape)\n\nprint(\"=\"*100)\n\nX_CV_text_feature_onehotencoding_tfidf = normalize(X_CV_text_feature_onehotencoding_tfidf, axis=0)","execution_count":null,"outputs":[]},{"metadata":{"id":"cpyWjzN18T4y"},"cell_type":"markdown","source":"## STACKING THE THREE TYPES OF FEATURES ","execution_count":null},{"metadata":{"id":"VBTBeLfV8XiH","outputId":"7d772dc1-893a-40a8-98b7-e2a25d599e88","executionInfo":{"status":"ok","timestamp":1591646016168,"user_tz":-330,"elapsed":965,"user":{"displayName":"Faizahmed Momin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjIpwU1PcmQEow-LMt2f-aSLgkwcuid0iQCqGiYLoc=s64","userId":"01335550890493547686"}},"trusted":true},"cell_type":"code","source":"X_Train_gene_var_onehotencoding_tfidf = hstack((X_Train_gene_Feature_onehotencoding_tfidf,X_Train_variation_Feature_onehotencoding_tfidf))\nX_Test_gene_var_onehotencoding_tfidf =  hstack((X_Test_gene_Feature_onehotencoding_tfidf,X_Test_variation_Feature_onehotencoding_tfidf))\nX_CV_gene_var_onehotencoding_tfidf = hstack((X_CV_gene_Feature_onehotencoding_tfidf,X_CV_variation_Feature_onehotencoding_tfidf))\n  \nX_Train_onehotcoding_tfidf = hstack((X_Train_gene_var_onehotencoding_tfidf,X_Train_feature_onehotencoding_tfidf)).tocsr()\nprint(\"Train One Hot Encoding  :\", X_Train_onehotcoding_tfidf.shape)\nprint(\"=\"*100)\n\nX_Test_onehotcoding_tfidf = hstack((X_Test_gene_var_onehotencoding_tfidf,X_Test_text_feature_onehotencoding_tfidf)).tocsr()\nprint(\"Test One Hot Encoding  :\", X_Test_onehotcoding_tfidf.shape)\nprint(\"=\"*100)\n\nX_CV_onehotcoding_tfidf = hstack((X_CV_gene_var_onehotencoding_tfidf,X_CV_text_feature_onehotencoding_tfidf)).tocsr()\nprint(\"CV One Hot Encoding  :\", X_CV_onehotcoding_tfidf.shape)\nprint(\"=\"*100)","execution_count":null,"outputs":[]},{"metadata":{"id":"4iXV7TNFkZF5"},"cell_type":"markdown","source":"## NAIVE BAYES ALGORITHM","execution_count":null},{"metadata":{"id":"o9urIKFMkWSk","outputId":"fb83dec5-c8f1-4136-e24e-a5388f53a792","executionInfo":{"status":"ok","timestamp":1591433072184,"user_tz":-330,"elapsed":229422,"user":{"displayName":"Faizahmed Momin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjIpwU1PcmQEow-LMt2f-aSLgkwcuid0iQCqGiYLoc=s64","userId":"01335550890493547686"}},"trusted":true},"cell_type":"code","source":"alpha = [0.00001, 0.0001, 0.001, 0.1, 1, 10, 100,1000]\ncv_log_error_array = []\nfor i in alpha:\n    print(\"for alpha =\", i)\n    clf = MultinomialNB(alpha=i)\n    clf.fit(X_Train_onehotcoding_tfidf, Y_Train)\n    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n    sig_clf.fit(X_Train_onehotcoding_tfidf, Y_Train)\n    sig_clf_probs = sig_clf.predict_proba(X_CV_onehotcoding_tfidf)\n    cv_log_error_array.append(log_loss(Y_CV, sig_clf_probs, labels=clf.classes_, eps=1e-15))\n    # to avoid rounding error while multiplying probabilites we use log-probability estimates\n    print(\"Log Loss :\",log_loss(Y_CV, sig_clf_probs)) \n\nfig, ax = plt.subplots()\nax.plot(np.log10(alpha), cv_log_error_array,c='g')\nfor i, txt in enumerate(np.round(cv_log_error_array,3)):\n    ax.annotate((alpha[i],str(txt)), (np.log10(alpha[i]),cv_log_error_array[i]))\nplt.grid()\nplt.xticks(np.log10(alpha))\nplt.title(\"Cross Validation Error for each alpha\")\nplt.xlabel(\"Alpha i's\")\nplt.ylabel(\"Error measure\")\nplt.show()\n\n\nbest_alpha = np.argmin(cv_log_error_array)\nclf = MultinomialNB(alpha=alpha[best_alpha])\nclf.fit(X_Train_onehotcoding_tfidf, Y_Train)\nsig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\nsig_clf.fit(X_Train_onehotcoding_tfidf, Y_Train)\n\n\npredict_y = sig_clf.predict_proba(X_Train_onehotcoding_tfidf)\nprint('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(Y_Train, predict_y, labels=clf.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(X_CV_onehotcoding_tfidf)\nprint('For values of best alpha = ', alpha[best_alpha], \"The cross validation log loss is:\",log_loss(Y_CV, predict_y, labels=clf.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(X_Test_onehotcoding_tfidf)\nprint('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(Y_Test, predict_y, labels=clf.classes_, eps=1e-15))","execution_count":null,"outputs":[]},{"metadata":{"id":"V3UJ12qBmM6J","outputId":"f0eb1002-c31e-4958-c643-59429f85a397","executionInfo":{"status":"ok","timestamp":1591392158804,"user_tz":-330,"elapsed":17395,"user":{"displayName":"Faizahmed Momin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjIpwU1PcmQEow-LMt2f-aSLgkwcuid0iQCqGiYLoc=s64","userId":"01335550890493547686"}},"trusted":true},"cell_type":"code","source":"clf = MultinomialNB(alpha=alpha[best_alpha])\nclf.fit(X_Train_onehotcoding_tfidf, Y_Train)\nsig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\nsig_clf.fit(X_Train_onehotcoding_tfidf, Y_Train)\nsig_clf_probs = sig_clf.predict_proba(X_CV_onehotcoding_tfidf)\n\nprint(\"Log Loss :\",log_loss(Y_CV, sig_clf_probs))\n\nprint(\"Number of missclassified point :\", np.count_nonzero((sig_clf.predict(X_CV_onehotcoding_tfidf)- Y_CV))/Y_CV.shape[0])\nplot_confusion_matrix(Y_CV, sig_clf.predict(X_CV_onehotcoding_tfidf.toarray()))","execution_count":null,"outputs":[]},{"metadata":{"id":"1ARtath4IB8F"},"cell_type":"markdown","source":"## K-NEAREST NEIGHBORS ALGORITHM","execution_count":null},{"metadata":{"id":"XLs8xETPIHNN","outputId":"934bdcd6-a537-4dbf-e31b-ae9c2af4341a","executionInfo":{"status":"ok","timestamp":1591433793008,"user_tz":-330,"elapsed":540950,"user":{"displayName":"Faizahmed Momin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjIpwU1PcmQEow-LMt2f-aSLgkwcuid0iQCqGiYLoc=s64","userId":"01335550890493547686"}},"trusted":true},"cell_type":"code","source":"alpha = [5, 11, 15, 21, 31, 41, 51, 99]\ncv_log_error_array = []\nfor i in alpha:\n    print(\"for alpha =\", i)\n    clf = KNeighborsClassifier(n_neighbors=i)\n    clf.fit(X_Train_onehotcoding_tfidf, Y_Train)\n    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n    sig_clf.fit(X_Train_onehotcoding_tfidf, Y_Train)\n    sig_clf_probs = sig_clf.predict_proba(X_CV_onehotcoding_tfidf)\n    cv_log_error_array.append(log_loss(Y_CV, sig_clf_probs, labels=clf.classes_, eps=1e-15))\n    # to avoid rounding error while multiplying probabilites we use log-probability estimates\n    print(\"Log Loss :\",log_loss(Y_CV, sig_clf_probs)) \n\nfig, ax = plt.subplots()\nax.plot(alpha, cv_log_error_array,c='g')\nfor i, txt in enumerate(np.round(cv_log_error_array,3)):\n    ax.annotate((alpha[i],str(txt)), (alpha[i],cv_log_error_array[i]))\nplt.grid()\nplt.title(\"Cross Validation Error for each alpha\")\nplt.xlabel(\"Alpha i's\")\nplt.ylabel(\"Error measure\")\nplt.show()\n\n\nbest_alpha = np.argmin(cv_log_error_array)\nclf = KNeighborsClassifier(n_neighbors=alpha[best_alpha])\nclf.fit(X_Train_onehotcoding_tfidf, Y_Train)\nsig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\nsig_clf.fit(X_Train_onehotcoding_tfidf, Y_Train)\n\npredict_y = sig_clf.predict_proba(X_Train_onehotcoding_tfidf)\nprint('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(Y_Train, predict_y, labels=clf.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(X_CV_onehotcoding_tfidf)\nprint('For values of best alpha = ', alpha[best_alpha], \"The cross validation log loss is:\",log_loss(Y_CV, predict_y, labels=clf.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(X_Test_onehotcoding_tfidf)\nprint('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(Y_Test, predict_y, labels=clf.classes_, eps=1e-15))","execution_count":null,"outputs":[]},{"metadata":{"id":"2kd4Mjg32o2k","outputId":"b7c75418-98d2-49a7-e890-a2185c70d86f","executionInfo":{"status":"ok","timestamp":1591434405472,"user_tz":-330,"elapsed":77463,"user":{"displayName":"Faizahmed Momin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjIpwU1PcmQEow-LMt2f-aSLgkwcuid0iQCqGiYLoc=s64","userId":"01335550890493547686"}},"trusted":true},"cell_type":"code","source":"clf = KNeighborsClassifier(n_neighbors=alpha[best_alpha])\npredict_and_plot_confusionmatrix(X_Train_onehotcoding_tfidf, Y_Train, X_CV_onehotcoding_tfidf, Y_CV, clf)","execution_count":null,"outputs":[]},{"metadata":{"id":"513LDDKS5VjF","outputId":"6ea9b456-f045-4438-f39e-cc52bd885c8b","executionInfo":{"status":"ok","timestamp":1591434849722,"user_tz":-330,"elapsed":29188,"user":{"displayName":"Faizahmed Momin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjIpwU1PcmQEow-LMt2f-aSLgkwcuid0iQCqGiYLoc=s64","userId":"01335550890493547686"}},"trusted":true},"cell_type":"code","source":"clf = KNeighborsClassifier(n_neighbors=alpha[best_alpha])\nclf.fit(X_Train_onehotcoding_tfidf, Y_Train)\nsig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\nsig_clf.fit(X_Train_onehotcoding_tfidf, Y_Train)\n\ntest_point_index = 1\npredicted_cls = sig_clf.predict(X_Test_onehotcoding_tfidf[0].reshape(1,-1))\nprint(\"Predicted Class :\", predicted_cls[0])\nprint(\"Actual Class :\", Y_Test[test_point_index])\nneighbors = clf.kneighbors(X_Test_onehotcoding_tfidf[test_point_index].reshape(1, -1), alpha[best_alpha])\nprint(\"The \",alpha[best_alpha],\" nearest neighbours of the test points belongs to classes\",Y_Train[neighbors[1][0]])\nprint(\"Fequency of nearest points :\",Counter(Y_Train[neighbors[1][0]]))","execution_count":null,"outputs":[]},{"metadata":{"id":"5S816Fx86TpR"},"cell_type":"markdown","source":"## LOGISTIC REGRESSION WITH CLASS BALANCING ","execution_count":null},{"metadata":{"id":"VDXoatXk6Xj_","outputId":"501b708c-954a-4d2d-809b-7bdd75edbb08","executionInfo":{"status":"ok","timestamp":1591646061215,"user_tz":-330,"elapsed":37892,"user":{"displayName":"Faizahmed Momin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjIpwU1PcmQEow-LMt2f-aSLgkwcuid0iQCqGiYLoc=s64","userId":"01335550890493547686"}},"trusted":true},"cell_type":"code","source":"alpha = [10 ** x for x in range(-6, 3)]\ncv_log_error_array = []\nfor i in alpha:\n    print(\"for alpha =\", i)\n    clf = SGDClassifier(class_weight='balanced', alpha=i, penalty='l2', loss='log', random_state=42)\n    clf.fit(X_Train_onehotcoding_tfidf, Y_Train)\n    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n    sig_clf.fit(X_Train_onehotcoding_tfidf, Y_Train)\n    sig_clf_probs = sig_clf.predict_proba(X_CV_onehotcoding_tfidf)\n    cv_log_error_array.append(log_loss(Y_CV, sig_clf_probs, labels=clf.classes_, eps=1e-15))\n    # to avoid rounding error while multiplying probabilites we use log-probability estimates\n    print(\"Log Loss :\",log_loss(Y_CV, sig_clf_probs)) \n\nfig, ax = plt.subplots()\nax.plot(alpha, cv_log_error_array,c='g')\nfor i, txt in enumerate(np.round(cv_log_error_array,3)):\n    ax.annotate((alpha[i],str(txt)), (alpha[i],cv_log_error_array[i]))\nplt.grid()\nplt.title(\"Cross Validation Error for each alpha\")\nplt.xlabel(\"Alpha i's\")\nplt.ylabel(\"Error measure\")\nplt.show()\n\n\nbest_alpha = np.argmin(cv_log_error_array)\nclf = SGDClassifier(class_weight='balanced', alpha=alpha[best_alpha], penalty='l2', loss='log', random_state=42)\nclf.fit(X_Train_onehotcoding_tfidf, Y_Train)\nsig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\nsig_clf.fit(X_Train_onehotcoding_tfidf, Y_Train)\n\npredict_y = sig_clf.predict_proba(X_Train_onehotcoding_tfidf)\nprint('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(Y_Train, predict_y, labels=clf.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(X_CV_onehotcoding_tfidf)\nprint('For values of best alpha = ', alpha[best_alpha], \"The cross validation log loss is:\",log_loss(Y_CV, predict_y, labels=clf.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(X_Test_onehotcoding_tfidf)\nprint('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(Y_Test, predict_y, labels=clf.classes_, eps=1e-15))","execution_count":null,"outputs":[]},{"metadata":{"id":"iwrWaN7cL40f","outputId":"5d5f71e7-fba2-4678-ca60-f6c2779bcf6a","executionInfo":{"status":"ok","timestamp":1591646508789,"user_tz":-330,"elapsed":7972,"user":{"displayName":"Faizahmed Momin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjIpwU1PcmQEow-LMt2f-aSLgkwcuid0iQCqGiYLoc=s64","userId":"01335550890493547686"}},"trusted":true},"cell_type":"code","source":"clf = SGDClassifier(class_weight='balanced', alpha=alpha[best_alpha], penalty='l2', loss='log', random_state=42)\npredict_and_plot_confusionmatrix(X_Train_onehotcoding_tfidf, Y_Train, X_CV_onehotcoding_tfidf, Y_CV, clf)","execution_count":null,"outputs":[]},{"metadata":{"id":"87Z6FhLJUnUy"},"cell_type":"markdown","source":"## LOGISTIC REGRESSION WITHOUT CLASS BALANCING ","execution_count":null},{"metadata":{"id":"Q7Do8yKtUuE4","outputId":"fd2643a7-327e-48d4-f7a7-702f809ad6c6","executionInfo":{"status":"ok","timestamp":1591646549274,"user_tz":-330,"elapsed":35415,"user":{"displayName":"Faizahmed Momin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjIpwU1PcmQEow-LMt2f-aSLgkwcuid0iQCqGiYLoc=s64","userId":"01335550890493547686"}},"trusted":true},"cell_type":"code","source":"alpha = [10 ** x for x in range(-6, 3)]\ncv_log_error_array = []\nfor i in alpha:\n    print(\"for alpha =\", i)\n    clf = SGDClassifier(alpha=i, penalty='l2', loss='log', random_state=42)\n    clf.fit(X_Train_onehotcoding_tfidf, Y_Train)\n    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n    sig_clf.fit(X_Train_onehotcoding_tfidf, Y_Train)\n    sig_clf_probs = sig_clf.predict_proba(X_CV_onehotcoding_tfidf)\n    cv_log_error_array.append(log_loss(Y_CV, sig_clf_probs, labels=clf.classes_, eps=1e-15))\n    # to avoid rounding error while multiplying probabilites we use log-probability estimates\n    print(\"Log Loss :\",log_loss(Y_CV, sig_clf_probs)) \n\nfig, ax = plt.subplots()\nax.plot(alpha, cv_log_error_array,c='g')\nfor i, txt in enumerate(np.round(cv_log_error_array,3)):\n    ax.annotate((alpha[i],str(txt)), (alpha[i],cv_log_error_array[i]))\nplt.grid()\nplt.title(\"Cross Validation Error for each alpha\")\nplt.xlabel(\"Alpha i's\")\nplt.ylabel(\"Error measure\")\nplt.show()\n\n\nbest_alpha = np.argmin(cv_log_error_array)\nclf = SGDClassifier(alpha=alpha[best_alpha], penalty='l2', loss='log', random_state=42)\nclf.fit(X_Train_onehotcoding_tfidf, Y_Train)\nsig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\nsig_clf.fit(X_Train_onehotcoding_tfidf, Y_Train)\n\npredict_y = sig_clf.predict_proba(X_Train_onehotcoding_tfidf)\nprint('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(Y_Train, predict_y, labels=clf.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(X_CV_onehotcoding_tfidf)\nprint('For values of best alpha = ', alpha[best_alpha], \"The cross validation log loss is:\",log_loss(Y_CV, predict_y, labels=clf.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(X_Test_onehotcoding_tfidf)\nprint('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(Y_Test, predict_y, labels=clf.classes_, eps=1e-15))","execution_count":null,"outputs":[]},{"metadata":{"id":"-AxR7HHDmDk_","outputId":"d522f41c-191d-42d1-af95-c93620c6c9bd","executionInfo":{"status":"ok","timestamp":1591646647464,"user_tz":-330,"elapsed":6403,"user":{"displayName":"Faizahmed Momin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjIpwU1PcmQEow-LMt2f-aSLgkwcuid0iQCqGiYLoc=s64","userId":"01335550890493547686"}},"trusted":true},"cell_type":"code","source":"clf = SGDClassifier(alpha=alpha[best_alpha], penalty='l2', loss='log', random_state=42)\npredict_and_plot_confusionmatrix(X_Train_onehotcoding_tfidf, Y_Train, X_CV_onehotcoding_tfidf, Y_CV, clf)","execution_count":null,"outputs":[]},{"metadata":{"id":"UwTs_EyApaMM"},"cell_type":"markdown","source":"## LINEAR SUPPORT VECTOR MACHINE ","execution_count":null},{"metadata":{"id":"ZK2ii74wpfMy","outputId":"83ccf37f-9f90-45a1-8477-d6fab2e8b5d8","executionInfo":{"status":"ok","timestamp":1591450565927,"user_tz":-330,"elapsed":3183033,"user":{"displayName":"Faizahmed Momin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjIpwU1PcmQEow-LMt2f-aSLgkwcuid0iQCqGiYLoc=s64","userId":"01335550890493547686"}},"trusted":true},"cell_type":"code","source":"alpha = [10 ** x for x in range(-6, 3)]\ncv_log_error_array = []\nfor i in alpha:\n    print(\"for alpha =\", i)\n    clf = SGDClassifier(class_weight='balanced',alpha=i, penalty='l2', loss='hinge', random_state=42)\n    clf.fit(X_Train_onehotcoding_tfidf, Y_Train)\n    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n    sig_clf.fit(X_Train_onehotcoding_tfidf, Y_Train)\n    sig_clf_probs = sig_clf.predict_proba(X_CV_onehotcoding_tfidf)\n    cv_log_error_array.append(log_loss(Y_CV, sig_clf_probs, labels=clf.classes_, eps=1e-15))\n    # to avoid rounding error while multiplying probabilites we use log-probability estimates\n    print(\"Log Loss :\",log_loss(Y_CV, sig_clf_probs)) \n\nfig, ax = plt.subplots()\nax.plot(alpha, cv_log_error_array,c='g')\nfor i, txt in enumerate(np.round(cv_log_error_array,3)):\n    ax.annotate((alpha[i],str(txt)), (alpha[i],cv_log_error_array[i]))\nplt.grid()\nplt.title(\"Cross Validation Error for each alpha\")\nplt.xlabel(\"Alpha i's\")\nplt.ylabel(\"Error measure\")\nplt.show()\n\n\nbest_alpha = np.argmin(cv_log_error_array)\nclf = SGDClassifier(class_weight='balanced',alpha=alpha[best_alpha], penalty='l2', loss='hinge', random_state=42)\nclf.fit(X_Train_onehotcoding_tfidf, Y_Train)\nsig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\nsig_clf.fit(X_Train_onehotcoding_tfidf, Y_Train)\n\npredict_y = sig_clf.predict_proba(X_Train_onehotcoding_tfidf)\nprint('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(Y_Train, predict_y, labels=clf.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(X_CV_onehotcoding_tfidf)\nprint('For values of best alpha = ', alpha[best_alpha], \"The cross validation log loss is:\",log_loss(Y_CV, predict_y, labels=clf.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(X_Test_onehotcoding_tfidf)\nprint('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(Y_Test, predict_y, labels=clf.classes_, eps=1e-15))","execution_count":null,"outputs":[]},{"metadata":{"id":"6Autw3_1DD6O","outputId":"c3f76354-61e0-4f03-e324-6b7f1ffdfd1e","executionInfo":{"status":"ok","timestamp":1591454353689,"user_tz":-330,"elapsed":266899,"user":{"displayName":"Faizahmed Momin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjIpwU1PcmQEow-LMt2f-aSLgkwcuid0iQCqGiYLoc=s64","userId":"01335550890493547686"}},"trusted":true},"cell_type":"code","source":"clf = SGDClassifier(alpha=alpha[best_alpha], penalty='l2', loss='hinge', random_state=42,class_weight='balanced')\npredict_and_plot_confusionmatrix(X_Train_onehotcoding_tfidf, Y_Train,X_CV_onehotcoding_tfidf,Y_CV, clf)","execution_count":null,"outputs":[]},{"metadata":{"id":"xNq7dfC7FpIz"},"cell_type":"markdown","source":"## RANDOM FOREST CLASSIFIER ","execution_count":null},{"metadata":{"id":"KUSfkydMFtqi","outputId":"59c3289a-d46e-4223-d05d-9bdac17f0648","trusted":true},"cell_type":"code","source":"alpha = [100,200,500,1000,2000]\nmax_depth = [5, 10]\ncv_log_error_array = []\nfor i in alpha:\n    for j in max_depth:\n        print(\"for n_estimators =\", i,\"and max depth = \", j)\n        clf = RandomForestClassifier(n_estimators=i, criterion='gini', max_depth=j, random_state=42, n_jobs=-1)\n        clf.fit(X_Train_onehotcoding_tfidf, Y_Train)\n        sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n        sig_clf.fit(X_Train_onehotcoding_tfidf, Y_Train)\n        sig_clf_probs = sig_clf.predict_proba(X_CV_onehotcoding_tfidf)\n        cv_log_error_array.append(log_loss(Y_CV, sig_clf_probs, labels=clf.classes_, eps=1e-15))\n        print(\"Log Loss :\",log_loss(Y_CV, sig_clf_probs)) \n\n'''fig, ax = plt.subplots()\nfeatures = np.dot(np.array(alpha)[:,None],np.array(max_depth)[None]).ravel()\nax.plot(features, cv_log_error_array,c='g')\nfor i, txt in enumerate(np.round(cv_log_error_array,3)):\n    ax.annotate((alpha[int(i/2)],max_depth[int(i%2)],str(txt)), (features[i],cv_log_error_array[i]))\nplt.grid()\nplt.title(\"Cross Validation Error for each alpha\")\nplt.xlabel(\"Alpha i's\")\nplt.ylabel(\"Error measure\")\nplt.show()\n'''\n\nbest_alpha = np.argmin(cv_log_error_array)\nclf = RandomForestClassifier(n_estimators=alpha[int(best_alpha/2)], criterion='gini', max_depth=max_depth[int(best_alpha%2)], random_state=42, n_jobs=-1)\nclf.fit(X_Train_onehotcoding_tfidf, Y_Train)\nsig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\nsig_clf.fit(X_Train_onehotcoding_tfidf, Y_Train)\n\npredict_y = sig_clf.predict_proba(X_Train_onehotcoding_tfidf)\nprint('For values of best estimator = ', alpha[int(best_alpha/2)], \"The train log loss is:\",log_loss(Y_Train, predict_y, labels=clf.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(X_CV_onehotcoding_tfidf)\nprint('For values of best estimator = ', alpha[int(best_alpha/2)], \"The cross validation log loss is:\",log_loss(Y_CV, predict_y, labels=clf.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(X_Test_onehotcoding_tfidf)\nprint('For values of best estimator = ', alpha[int(best_alpha/2)], \"The test log loss is:\",log_loss(Y_Test, predict_y, labels=clf.classes_, eps=1e-15))","execution_count":null,"outputs":[]},{"metadata":{"id":"I0QtzbkbYCJ5"},"cell_type":"markdown","source":"## STACK THE MODELS ","execution_count":null},{"metadata":{"id":"Y9B9VgkWYIp0","trusted":true},"cell_type":"code","source":"clf1 = SGDClassifier(alpha=0.001, penalty='l2', loss='log', class_weight='balanced', random_state=0)\nclf1.fit(X_Train_onehotcoding_tfidf, Y_Train)\nsig_clf1 = CalibratedClassifierCV(clf1, method=\"sigmoid\")","execution_count":null,"outputs":[]},{"metadata":{"id":"Jjsv_GvFRg10","trusted":true},"cell_type":"code","source":"clf2 = SGDClassifier(alpha=1, penalty='l2', loss='hinge', class_weight='balanced', random_state=0)\nclf2.fit(X_Train_onehotcoding_tfidf, Y_Train)\nsig_clf2 = CalibratedClassifierCV(clf2, method=\"sigmoid\")","execution_count":null,"outputs":[]},{"metadata":{"id":"D9glX6J3RjMu","trusted":true},"cell_type":"code","source":"clf3 = MultinomialNB(alpha=0.001)\nclf3.fit(X_Train_onehotcoding_tfidf, Y_Train)\nsig_clf3 = CalibratedClassifierCV(clf3, method=\"sigmoid\")","execution_count":null,"outputs":[]},{"metadata":{"id":"HBepfIFaRmuS","outputId":"78917aa1-ebc2-4dc0-9861-b3642aa52823","executionInfo":{"status":"ok","timestamp":1591559665725,"user_tz":-330,"elapsed":527644,"user":{"displayName":"Faizahmed Momin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjIpwU1PcmQEow-LMt2f-aSLgkwcuid0iQCqGiYLoc=s64","userId":"01335550890493547686"}},"trusted":true},"cell_type":"code","source":"sig_clf1.fit(X_Train_onehotcoding_tfidf, Y_Train)\nprint(\"Logistic Regression :  Log Loss: %0.2f\" % (log_loss(Y_CV, sig_clf1.predict_proba(X_CV_onehotcoding_tfidf))))\nsig_clf2.fit(X_Train_onehotcoding_tfidf, Y_Train)\nprint(\"Support vector machines : Log Loss: %0.2f\" % (log_loss(Y_CV, sig_clf2.predict_proba(X_CV_onehotcoding_tfidf))))","execution_count":null,"outputs":[]},{"metadata":{"id":"t1hO_waTUkZI","outputId":"b5427b67-980e-4c76-9e2b-f7457c6a2902","executionInfo":{"status":"ok","timestamp":1591560143357,"user_tz":-330,"elapsed":24771,"user":{"displayName":"Faizahmed Momin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjIpwU1PcmQEow-LMt2f-aSLgkwcuid0iQCqGiYLoc=s64","userId":"01335550890493547686"}},"trusted":true},"cell_type":"code","source":"sig_clf3.fit(X_Train_onehotcoding_tfidf, Y_Train)\nprint(\"Naive Bayes : Log Loss: %0.2f\" % (log_loss(Y_CV, sig_clf3.predict_proba(X_CV_onehotcoding_tfidf))))\nprint(\"-\"*50)","execution_count":null,"outputs":[]},{"metadata":{"id":"91Xc05gHRu-U","outputId":"d835eabe-89a9-446b-fc1b-0e08e9296658","executionInfo":{"status":"ok","timestamp":1591565594832,"user_tz":-330,"elapsed":4174913,"user":{"displayName":"Faizahmed Momin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjIpwU1PcmQEow-LMt2f-aSLgkwcuid0iQCqGiYLoc=s64","userId":"01335550890493547686"}},"trusted":true},"cell_type":"code","source":"alpha = [0.0001,0.001,0.01,0.1,1,10] \nbest_alpha = 999\nfor i in alpha:\n    lr = LogisticRegression(C=i)\n    sclf = StackingClassifier(classifiers=[sig_clf1, sig_clf2, sig_clf3], meta_classifier=lr, use_probas=True)\n    sclf.fit(X_Train_onehotcoding_tfidf, Y_Train)\n    print(\"Stacking Classifer : for the value of alpha: %f Log Loss: %0.3f\" % (i, log_loss(Y_CV, sclf.predict_proba(X_CV_onehotcoding_tfidf))))\n    log_error =log_loss(Y_CV , sclf.predict_proba(X_CV_onehotcoding_tfidf))\n    if best_alpha > log_error:\n        best_alpha = log_error","execution_count":null,"outputs":[]},{"metadata":{"id":"UMkEjBvZtRGK","outputId":"a91800c1-d661-4370-be17-5a7afedab171","executionInfo":{"status":"ok","timestamp":1591567599318,"user_tz":-330,"elapsed":738132,"user":{"displayName":"Faizahmed Momin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjIpwU1PcmQEow-LMt2f-aSLgkwcuid0iQCqGiYLoc=s64","userId":"01335550890493547686"}},"trusted":true},"cell_type":"code","source":"lr = LogisticRegression(C=0.1)\nsclf = StackingClassifier(classifiers=[sig_clf1, sig_clf2, sig_clf3], meta_classifier=lr, use_probas=True)\nsclf.fit(X_Train_onehotcoding_tfidf, Y_Train)\n\nlog_error = log_loss(Y_Train, sclf.predict_proba(X_Train_onehotcoding_tfidf))\nprint(\"Log loss (train) on the stacking classifier :\",log_error)\n\nlog_error = log_loss(Y_CV, sclf.predict_proba(X_CV_onehotcoding_tfidf))\nprint(\"Log loss (CV) on the stacking classifier :\",log_error)\n\nlog_error = log_loss(Y_Test, sclf.predict_proba(X_Test_onehotcoding_tfidf))\nprint(\"Log loss (test) on the stacking classifier :\",log_error)\n\nprint(\"Number of missclassified point :\", np.count_nonzero((sclf.predict(X_Test_onehotcoding_tfidf)- Y_Test))/Y_Test.shape[0])\nplot_confusion_matrix(test_y=Y_Test, predict_y=sclf.predict(X_Test_onehotcoding_tfidf))","execution_count":null,"outputs":[]},{"metadata":{"id":"IgQztkBVyZp2"},"cell_type":"markdown","source":"# LOGISTIC REGRESSION WITH COUNTVECTORIZER USING BIGRAM ","execution_count":null},{"metadata":{"id":"QAs-s7k3ML9Z"},"cell_type":"markdown","source":"## CountVectorizer on Gene Features","execution_count":null},{"metadata":{"id":"fRrAfawAyo1l","outputId":"60cd33d3-4aac-4040-a3e9-4ae2f2f1f4c7","executionInfo":{"status":"ok","timestamp":1591591450603,"user_tz":-330,"elapsed":1434,"user":{"displayName":"Faizahmed Momin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjIpwU1PcmQEow-LMt2f-aSLgkwcuid0iQCqGiYLoc=s64","userId":"01335550890493547686"}},"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\n\ncount_vectorizer_bigram=CountVectorizer(ngram_range=(1,2))\n\nX_Train_gene_Feature_onehotencoding_bigram=count_vectorizer_bigram.fit_transform(X_Train[\"Gene\"])\nprint(\" Train Gene Feature :\" ,X_Train_gene_Feature_onehotencoding_bigram.shape)\n\nprint(\"=\"*100)\n\nX_Test_gene_Feature_onehotencoding_bigram=count_vectorizer_bigram.transform(X_Test[\"Gene\"])\nprint(\" Test Gene Feature :\" ,X_Test_gene_Feature_onehotencoding_bigram.shape)\n\nprint(\"=\"*100)\n\n\nX_CV_gene_Feature_onehotencoding_bigram=count_vectorizer_bigram.transform(X_CV[\"Gene\"])\nprint(\" CV Gene Feature :\" ,X_CV_gene_Feature_onehotencoding_bigram.shape)\n\nprint(\"=\"*100)","execution_count":null,"outputs":[]},{"metadata":{"id":"QWhVIoG-NU0w"},"cell_type":"markdown","source":"## CountVectorizer on Variation Features","execution_count":null},{"metadata":{"id":"Gdl_087hNY0i","outputId":"c6973555-c728-434c-b569-b40d4e45b71b","executionInfo":{"status":"ok","timestamp":1591591455198,"user_tz":-330,"elapsed":1183,"user":{"displayName":"Faizahmed Momin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjIpwU1PcmQEow-LMt2f-aSLgkwcuid0iQCqGiYLoc=s64","userId":"01335550890493547686"}},"trusted":true},"cell_type":"code","source":"variation_vectorizer_bigram=CountVectorizer(ngram_range=(1,2))\n\nX_Train_variation_Feature_onehotencoding_bigram=variation_vectorizer_bigram.fit_transform(X_Train[\"Variation\"])\nprint(\" Train Variation Feature :\" ,X_Train_variation_Feature_onehotencoding_bigram.shape)\n\nprint(\"=\"*100)\n\nX_Test_variation_Feature_onehotencoding_bigram=variation_vectorizer_bigram.transform(X_Test[\"Variation\"])\nprint(\" Test Variation Feature :\" ,X_Test_variation_Feature_onehotencoding_bigram.shape)\n\nprint(\"=\"*100)\n\n\nX_CV_variation_Feature_onehotencoding_bigram=variation_vectorizer_bigram.transform(X_CV[\"Variation\"])\nprint(\" CV Variation Feature :\" ,X_CV_variation_Feature_onehotencoding_bigram.shape)\n\nprint(\"=\"*100)","execution_count":null,"outputs":[]},{"metadata":{"id":"r7jIPc1MNvFE"},"cell_type":"markdown","source":"## CountVectorizer on Text Features","execution_count":null},{"metadata":{"id":"1s5yyt5KNzT8","outputId":"88ad7658-ce37-49b5-8227-654559304783","executionInfo":{"status":"ok","timestamp":1591591764405,"user_tz":-330,"elapsed":59070,"user":{"displayName":"Faizahmed Momin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjIpwU1PcmQEow-LMt2f-aSLgkwcuid0iQCqGiYLoc=s64","userId":"01335550890493547686"}},"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import normalize\n\ntext_vectorizer_bigram = CountVectorizer(min_df=3,ngram_range=(1,2))\nX_Train_feature_onehotencoding_bigram = text_vectorizer_bigram.fit_transform(X_Train['TEXT'])\n\n\nX_Train_feature_onehotencoding_bigram = normalize(X_Train_feature_onehotencoding_bigram, axis=0)\nprint(\"Train Text Feature :\", X_Train_feature_onehotencoding_bigram.shape)\n\nprint(\"=\"*100)\n\nX_Test_text_feature_onehotencoding_bigram = text_vectorizer_bigram.transform(X_Test['TEXT'])\nprint(\"Test Text Feature :\", X_Test_text_feature_onehotencoding_bigram.shape)\n\nprint(\"=\"*100)\n\nX_Test_text_feature_onehotencoding_bigram = normalize(X_Test_text_feature_onehotencoding_bigram, axis=0)\n\nX_CV_text_feature_onehotencoding_bigram = text_vectorizer_bigram.transform(X_CV['TEXT'])\nprint(\"CV Text Feature :\", X_CV_text_feature_onehotencoding_bigram.shape)\n\nprint(\"=\"*100)\n\nX_CV_text_feature_onehotencoding_bigram = normalize(X_CV_text_feature_onehotencoding_bigram, axis=0)","execution_count":null,"outputs":[]},{"metadata":{"id":"8eYfZ0G3PCfM"},"cell_type":"markdown","source":"## STACKING THE 3 TYPES OF FEATURES","execution_count":null},{"metadata":{"id":"ryuROqYxPGtn","outputId":"2712383b-36e5-48cb-e062-ad76dd4ac961","executionInfo":{"status":"ok","timestamp":1591591779298,"user_tz":-330,"elapsed":1635,"user":{"displayName":"Faizahmed Momin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjIpwU1PcmQEow-LMt2f-aSLgkwcuid0iQCqGiYLoc=s64","userId":"01335550890493547686"}},"trusted":true},"cell_type":"code","source":"X_Train_gene_var_onehotencoding_bigram = hstack((X_Train_gene_Feature_onehotencoding_bigram,X_Train_variation_Feature_onehotencoding_bigram))\nX_Test_gene_var_onehotencoding_bigram =  hstack((X_Test_gene_Feature_onehotencoding_bigram,X_Test_variation_Feature_onehotencoding_bigram))\nX_CV_gene_var_onehotencoding_bigram = hstack((X_CV_gene_Feature_onehotencoding_bigram,X_CV_variation_Feature_onehotencoding_bigram))\n  \nX_Train_onehotcoding_bigram = hstack((X_Train_gene_var_onehotencoding_bigram,X_Train_feature_onehotencoding_bigram)).tocsr()\nprint(\"Train One Hot Encoding  :\", X_Train_onehotcoding_bigram.shape)\nprint(\"=\"*100)\n\nX_Test_onehotcoding_bigram = hstack((X_Test_gene_var_onehotencoding_bigram,X_Test_text_feature_onehotencoding_bigram)).tocsr()\nprint(\"Test One Hot Encoding  :\", X_Test_onehotcoding_bigram.shape)\nprint(\"=\"*100)\n\nX_CV_onehotcoding_bigram = hstack((X_CV_gene_var_onehotencoding_bigram,X_CV_text_feature_onehotencoding_bigram)).tocsr()\nprint(\"CV One Hot Encoding  :\", X_CV_onehotcoding_bigram.shape)\nprint(\"=\"*100)","execution_count":null,"outputs":[]},{"metadata":{"id":"vUF5h1mqRTPO"},"cell_type":"markdown","source":"## LOGISTIC REGRESSION WITH CLASS BALANCING","execution_count":null},{"metadata":{"id":"GMGY1n6fRWsK","outputId":"40702e88-440c-4b29-de54-bd8a040c6378","executionInfo":{"status":"ok","timestamp":1591593021062,"user_tz":-330,"elapsed":865834,"user":{"displayName":"Faizahmed Momin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjIpwU1PcmQEow-LMt2f-aSLgkwcuid0iQCqGiYLoc=s64","userId":"01335550890493547686"}},"trusted":true},"cell_type":"code","source":"alpha = [10 ** x for x in range(-6, 3)]\ncv_log_error_array = []\nfor i in alpha:\n    print(\"for alpha =\", i)\n    clf = SGDClassifier(class_weight='balanced', alpha=i, penalty='l2', loss='log', random_state=42)\n    clf.fit(X_Train_onehotcoding_bigram, Y_Train)\n    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n    sig_clf.fit(X_Train_onehotcoding_bigram, Y_Train)\n    sig_clf_probs = sig_clf.predict_proba(X_CV_onehotcoding_bigram)\n    cv_log_error_array.append(log_loss(Y_CV, sig_clf_probs, labels=clf.classes_, eps=1e-15))\n    # to avoid rounding error while multiplying probabilites we use log-probability estimates\n    print(\"Log Loss :\",log_loss(Y_CV, sig_clf_probs)) \n\nfig, ax = plt.subplots()\nax.plot(alpha, cv_log_error_array,c='g')\nfor i, txt in enumerate(np.round(cv_log_error_array,3)):\n    ax.annotate((alpha[i],str(txt)), (alpha[i],cv_log_error_array[i]))\nplt.grid()\nplt.title(\"Cross Validation Error for each alpha\")\nplt.xlabel(\"Alpha i's\")\nplt.ylabel(\"Error measure\")\nplt.show()\n\n\nbest_alpha = np.argmin(cv_log_error_array)\nclf = SGDClassifier(class_weight='balanced', alpha=alpha[best_alpha], penalty='l2', loss='log', random_state=42)\nclf.fit(X_Train_onehotcoding_bigram, Y_Train)\nsig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\nsig_clf.fit(X_Train_onehotcoding_bigram, Y_Train)\n\npredict_y = sig_clf.predict_proba(X_Train_onehotcoding_bigram)\nprint('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(Y_Train, predict_y, labels=clf.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(X_CV_onehotcoding_bigram)\nprint('For values of best alpha = ', alpha[best_alpha], \"The cross validation log loss is:\",log_loss(Y_CV, predict_y, labels=clf.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(X_Test_onehotcoding_bigram)\nprint('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(Y_Test, predict_y, labels=clf.classes_, eps=1e-15))","execution_count":null,"outputs":[]},{"metadata":{"id":"H61xkyYzV2aZ","outputId":"bc765bd9-c232-45d5-eb87-7cca52f946ce","executionInfo":{"status":"ok","timestamp":1591593357764,"user_tz":-330,"elapsed":151583,"user":{"displayName":"Faizahmed Momin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjIpwU1PcmQEow-LMt2f-aSLgkwcuid0iQCqGiYLoc=s64","userId":"01335550890493547686"}},"trusted":true},"cell_type":"code","source":"clf = SGDClassifier(class_weight='balanced', alpha=alpha[best_alpha], penalty='l2', loss='log', random_state=42)\npredict_and_plot_confusionmatrix(X_Train_onehotcoding_bigram, Y_Train, X_CV_onehotcoding_bigram, Y_CV, clf)","execution_count":null,"outputs":[]},{"metadata":{"id":"hvMLMSBJW6Xv"},"cell_type":"markdown","source":"## LOGISTIC REGRESSION WITHOUT CLASS BALANCING","execution_count":null},{"metadata":{"id":"N_nEfw7SW9F4","outputId":"9c9ae962-1dbd-46c9-b35f-2147ead1753c","executionInfo":{"status":"ok","timestamp":1591594188788,"user_tz":-330,"elapsed":763923,"user":{"displayName":"Faizahmed Momin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjIpwU1PcmQEow-LMt2f-aSLgkwcuid0iQCqGiYLoc=s64","userId":"01335550890493547686"}},"trusted":true},"cell_type":"code","source":"alpha = [10 ** x for x in range(-6, 3)]\ncv_log_error_array = []\nfor i in alpha:\n    print(\"for alpha =\", i)\n    clf = SGDClassifier(alpha=i, penalty='l2', loss='log', random_state=42)\n    clf.fit(X_Train_onehotcoding_bigram, Y_Train)\n    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n    sig_clf.fit(X_Train_onehotcoding_bigram, Y_Train)\n    sig_clf_probs = sig_clf.predict_proba(X_CV_onehotcoding_bigram)\n    cv_log_error_array.append(log_loss(Y_CV, sig_clf_probs, labels=clf.classes_, eps=1e-15))\n    # to avoid rounding error while multiplying probabilites we use log-probability estimates\n    print(\"Log Loss :\",log_loss(Y_CV, sig_clf_probs)) \n\nfig, ax = plt.subplots()\nax.plot(alpha, cv_log_error_array,c='g')\nfor i, txt in enumerate(np.round(cv_log_error_array,3)):\n    ax.annotate((alpha[i],str(txt)), (alpha[i],cv_log_error_array[i]))\nplt.grid()\nplt.title(\"Cross Validation Error for each alpha\")\nplt.xlabel(\"Alpha i's\")\nplt.ylabel(\"Error measure\")\nplt.show()\n\n\nbest_alpha = np.argmin(cv_log_error_array)\nclf = SGDClassifier(alpha=alpha[best_alpha], penalty='l2', loss='log', random_state=42)\nclf.fit(X_Train_onehotcoding_bigram, Y_Train)\nsig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\nsig_clf.fit(X_Train_onehotcoding_bigram, Y_Train)\n\npredict_y = sig_clf.predict_proba(X_Train_onehotcoding_bigram)\nprint('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(Y_Train, predict_y, labels=clf.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(X_CV_onehotcoding_bigram)\nprint('For values of best alpha = ', alpha[best_alpha], \"The cross validation log loss is:\",log_loss(Y_CV, predict_y, labels=clf.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(X_Test_onehotcoding_bigram)\nprint('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(Y_Test, predict_y, labels=clf.classes_, eps=1e-15))","execution_count":null,"outputs":[]},{"metadata":{"id":"3hd4ZsXjaKr7","outputId":"aaae1e72-39e4-462b-b1d7-922077270052","executionInfo":{"status":"ok","timestamp":1591594313771,"user_tz":-330,"elapsed":73475,"user":{"displayName":"Faizahmed Momin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjIpwU1PcmQEow-LMt2f-aSLgkwcuid0iQCqGiYLoc=s64","userId":"01335550890493547686"}},"trusted":true},"cell_type":"code","source":"clf = SGDClassifier( alpha=alpha[best_alpha], penalty='l2', loss='log', random_state=42)\npredict_and_plot_confusionmatrix(X_Train_onehotcoding_bigram, Y_Train, X_CV_onehotcoding_bigram, Y_CV, clf)","execution_count":null,"outputs":[]},{"metadata":{"id":"CyHLtN_ua6Pl"},"cell_type":"markdown","source":"# CONCLUSION","execution_count":null},{"metadata":{"id":"GJIYTsqra9Jj","outputId":"b0610bd8-e3c5-4877-98c9-3026242f9dd2","executionInfo":{"status":"ok","timestamp":1591595414720,"user_tz":-330,"elapsed":7047,"user":{"displayName":"Faizahmed Momin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjIpwU1PcmQEow-LMt2f-aSLgkwcuid0iQCqGiYLoc=s64","userId":"01335550890493547686"}},"trusted":true},"cell_type":"code","source":"pip install -U PTable","execution_count":null,"outputs":[]},{"metadata":{"id":"mjBztCmLet94","outputId":"02ce904c-37c2-4664-b4ae-81b3bed7639a","executionInfo":{"status":"ok","timestamp":1591646901579,"user_tz":-330,"elapsed":1389,"user":{"displayName":"Faizahmed Momin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjIpwU1PcmQEow-LMt2f-aSLgkwcuid0iQCqGiYLoc=s64","userId":"01335550890493547686"}},"trusted":true},"cell_type":"code","source":"from prettytable import PrettyTable\nx= PrettyTable()\nx.title = \"TF-IDF Featurization\"\nx.field_names = [\"Algorithm\" , \"Train loss\", \"CV(Log Loss)\", \"Test loss\",\"%age of MissClassification\"]\nx.add_row([\"Naive Bayes\",0.81,1.24,1.21,\"42.00%\"])\nx.add_row([\"K-NN\",0.97,1.26,1.28,\"43.23%\"])\nx.add_row([\"Logistic Regression(With Class Balancing)\",0.36,1.05,1.05,\"34.39%\"])\nx.add_row([\"Logistic Regression(Without Class Balancing)\",0.35,1.08,1.06,\"35.33%\"])\nx.add_row([\"Linear SVM\",0.67,1.20,1.22,\"40.03%\"])\nx.add_row([\"Stacking Model\",0.58,1.19,1.23,\"39.84%\"])\nprint(x)\ny= PrettyTable()\ny.title = \"CountVectorizer BIGRAM\"\ny.field_names = [\"Algorithm\" , \"Train loss\", \"CV(Log Loss)\", \"Test loss\",\"%age of MissClassification\"]\ny.add_row([\"Linear Regression(With Class Balancing)\",0.68,1.16,1.19,\"35.33%\"])\ny.add_row([\"Linear Regression(Without Class Balancing)\",0.63,1.19,1.19,\"35.33%\"])\nprint(y)","execution_count":null,"outputs":[]},{"metadata":{"id":"gAXCD1OYlJqT"},"cell_type":"markdown","source":"### BY LOOKING AT THE PREETY TABLE , LOOKS LIKE TF-IDF FEATURIZATION USING LOGISTIC REGRESSION(WITHOUT CLASS BALANCING) WORKS WELL HAVING 0.36 TRAINING LOSS AND 34.39% MISS CLASSIFICATION POINTS. \n### TEST LOSS = 1.05 ","execution_count":null},{"metadata":{"id":"C682JPmAi2mM","trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}