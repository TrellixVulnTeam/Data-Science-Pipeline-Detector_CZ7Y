{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Loading all required packages\n# If any of it fails, do not panic. Just install it using \"pip3 install <package_name>\" or by using conda install package_name\n\nimport math\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nimport time\nimport warnings\n\nfrom nltk.corpus import stopwords\nfrom imblearn.over_sampling import SMOTE\nfrom collections import Counter\nfrom collections import Counter, defaultdict\nfrom scipy.sparse import hstack\nfrom mlxtend.classifier import StackingClassifier\n\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.preprocessing import normalize\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import normalized_mutual_info_score\nfrom sklearn.metrics.classification import accuracy_score, log_loss\nfrom sklearn.manifold import TSNE\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn import model_selection\nfrom sklearn.model_selection import StratifiedKFold \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\n\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# First, look at everything.\nfrom subprocess import check_output\nprint(check_output(['ls', '../input/msk-redefining-cancer-treatment/']).decode(\"utf8\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#  Pick a Dataset you might be interested in.\n#  Say, all airline-safety files...\nimport zipfile\n\ndataset = 'training_variants'\ndataset2 = 'training_text'\n\n# Will unzip the files so that you can see them..\nwith zipfile.ZipFile('../input/msk-redefining-cancer-treatment/'+dataset+'.zip','r') as z:\n    z.extractall(\".\")\n\n# Will unzip the files so that you can see them..\nwith zipfile.ZipFile('../input/msk-redefining-cancer-treatment/'+dataset2+'.zip','r') as y:\n    y.extractall('.')\n    \nfrom subprocess import check_output\nprint(check_output(['ls', 'training_variants']).decode(\"utf8\"))\nprint(check_output(['ls', 'training_text']).decode(\"utf8\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Select and read the files.\ndata_variants = pd.read_csv(dataset)\ndata_text = pd.read_csv(dataset2, sep=\"\\|\\|\", engine='python', names=['ID','TEXT'], skiprows=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_variants.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p>\n    Let's understand above data. There are 4 fields above: <br>\n    <ul>\n        <li><b>ID : </b>row id used to link the mutation to the clinical evidence</li>\n        <li><b>Gene : </b>the gene where this genetic mutation is located </li>\n        <li><b>Variation : </b>the aminoacid change for this mutations </li>\n        <li><b>Class :</b> class value 1-9, this genetic mutation has been classified on</li>\n    </ul>\n    \nKeep doing more analysis  on above data."},{"metadata":{"trusted":true},"cell_type":"code","source":"data_variants.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_variants.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_variants.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_text.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> So above dataset have 2 columns. ID and Text column. We can also observe column ID which is common in both the dataset. Lets keep exploring it."},{"metadata":{"trusted":true},"cell_type":"code","source":"data_text.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_text.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_text.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_text.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So, in short my datasets looks like this\n * data_variants (ID, Gene, Variations, Class)\n * data_text(ID, text)"},{"metadata":{},"cell_type":"markdown","source":"Ok, now we understood the dataset. Lets try to understand the same problem from Machine Learning point of view\n\nWe want to predict about class of cancer. Now question is what kind of data is present in class column. "},{"metadata":{"trusted":true},"cell_type":"code","source":"data_variants.Class.unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is descrete data so it is ***classification*** problem and since there are multiple descrete output possible so we can call it ***Multi class*** classification problem\n\n\n***Important note*** : This is medical related problem so correct results are very important. Error can be really costly here so we would like to have result  for each class in terms of Probablity. We might not be much bothered about time taken by ML algorithm as far as it is reasonable. \n\nWe also want our model to be highly interpritable because a medical practitionar want to also give proper reasonining on why ML algorithm is predicting any class. \n\nWe will evaluate our model using Confution matrix and Multi class log-loss"},{"metadata":{"trusted":true},"cell_type":"code","source":"# We would like to remove all stop words like a, is, an, the...\n# so we are collecting all of them from nltk library\nstop_words = set(stopwords.words('english'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def data_text_preprocess(total_text, ind, col):\n    # Remove int values from text data as that might not be important\n    if type(total_text) is not int:\n        string = ''\n        # replacing all special char with space\n        total_text = re.sub('[^a-zA-Z0-9\\n]', ' ', str(total_text))\n        # replacing multiple spaces with single space\n        total_text = re.sub('\\s', ' ', str(total_text))\n        # bring whole text to same lower-case scale\n        total_text = total_text.lower()\n        \n        for word in total_text.split():\n            # if word is not a stop word then retain that word from text\n            if not word in stop_words:\n                string += word + ''\n                \n        data_text[col][ind] = string","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Below code will take some time because it's huge text, so run it and have a cup of coffee\nfor index, row  in data_text.iterrows():\n    if type(row['TEXT']) is str:\n        data_text_preprocess(row['TEXT'], index, 'TEXT')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's merge both the dataset. Remember that ID was common column. So lets use it to merge."},{"metadata":{"trusted":true},"cell_type":"code","source":"# merging both gene_variatiions and text data based on ID\nresult = pd.merge(data_variants, data_text, on='ID', how='left')\nresult.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It's very important to look for missing values. Else they create problem in final analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"result[result.isnull().any(axis=1)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see many rows with missing data. Now the question is what to do with this missing value. One way could be that we can drop these rows having missing values or we can do some imputation in it. Let's go with imputation only. But question is what to impute here :\n\nHow about merging Gene and Variation column. Let's do it:"},{"metadata":{"trusted":true},"cell_type":"code","source":"result.loc[result.TEXT.isnull(),'TEXT'] = result['Gene']+ ' '+result['Variation']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's cross check it once again if there is any missing values"},{"metadata":{"trusted":true},"cell_type":"code","source":"result[result.isnull().any(axis=1)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Awesome, so all missing values are gone now."},{"metadata":{},"cell_type":"markdown","source":"## Creating Training, Test and Validation data\n\nBefore we split the data into taining, test and validation data set. We want to ensure that all spaces in Gene and Variation column to be replaced by _."},{"metadata":{"trusted":true},"cell_type":"code","source":"y_true = result['Class'].values\nresult.Gene = result.Gene.str.replace('\\s+', '_')\nresult.Variation = result.Variation.str.replace('\\s+', '_')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ok, so we can now start our split process in train, test and validation data set."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Splitting the data into train and test set\nX_train, test_df, y_train, y_test = train_test_split(result, y_true, stratify=y_true, test_size=0.2)\n# Split the train data now into train validation and cross validation\ntrain_df, cv_df, y_train, y_cv = train_test_split(X_train, y_train, stratify=y_train, test_size=0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Number of data points in train data:', train_df.shape[0])\nprint('Number of data points in test data:', test_df.shape[0])\nprint('Number of data points in cross validation data:', cv_df.shape[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's look at the distribution of data in train, test and validation set."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_class_distribution = train_df['Class'].value_counts().sort_index()\ntest_class_distribution = test_df['Class'].value_counts().sort_index()\ncv_class_distribution = cv_df['Class'].value_counts().sort_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_class_distribution","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So, what does above variable suggest us. This means in my train dataset we have class 1 values with count of 363, class 2 values having count of 289 and so on. It will be better idea to visualise it in graph format.\n\n*** Visualizing for train class distrubution***"},{"metadata":{"trusted":true},"cell_type":"code","source":"my_colors = 'rgbkymc'\ntrain_class_distribution.plot(kind='bar')\nplt.xlabel('Class')\nplt.ylabel('Number of Data Points per Class')\nplt.title('Distribution of yi in Train Data')\nplt.grid()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's look at distribution in form of percentage"},{"metadata":{"trusted":true},"cell_type":"code","source":"sorted_yi = np.argsort(-train_class_distribution.values)\nfor i in sorted_yi:\n    print('Number of data points in class', i+1, ':',train_class_distribution.values[i], '(', np.round((train_class_distribution.values[i]/train_df.shape[0]*100), 3), '%)')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now question is because we need log-loss as final evaluation metrics how do we say that model we are going to build will be good model. For doing this we will build a random model and will evaluate log loss. Our model should return lower log loss value than this. So, what are you waiting for. Always have a big smile while solving Machine learning problems :). That helps!!\n\n## Building a Random model"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data_len = test_df.shape[0]\ncv_data_len = cv_df.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We create an output array that has exactly same size as the CV data\ncv_predicted_y = np.zeros((cv_data_len, 9))\nfor i in range(cv_data_len):\n    rand_probs = np. random.rand(1,9)\n    cv_predicted_y[i] = ((rand_probs/sum(sum(rand_probs)))[0])\nprint('Log loss on Cross Validation Data using Random Model', log_loss(y_cv, cv_predicted_y, eps=1e-15))    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Test-Set error\n# We create an output array that has exactly same size as test data\ntest_predicted_y = np.zeros((test_data_len, 9))\nfor i in range(test_data_len):\n    rand_probs = np. random.rand(1,9)\n    test_predicted_y[i] = ((rand_probs/sum(sum(rand_probs)))[0])\nprint('Log loss on Test Data using Random Model', log_loss(y_test, test_predicted_y, eps=1e-15))    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's get the index of max probability\npredicted_y = np.argmax(test_predicted_y, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's see the output. These will be 665 values present in test_dataset\npredicted_y","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So you can see the index value ranging from 0 to 8. So, lets make it as 1 to 9 we will increase this value by 1."},{"metadata":{"trusted":true},"cell_type":"code","source":"predicted_y = predicted_y + 1\npredicted_y","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Confusion Matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"C = confusion_matrix(y_test, predicted_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels = [1,2,3,4,5,6,7,8,9]\nplt.figure(figsize=(20,7))\nsns.heatmap(C, annot=True, cmap=\"YlGnBu\", fmt=\".3f\", xticklabels=labels, yticklabels=labels)\nplt.xlabel('Predicted Class')\nplt.ylabel('Original Class')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Precision matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"B =(C/C.sum(axis=0))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,7))\nsns.heatmap(B, annot=True, cmap=\"YlGnBu\", fmt=\".3f\", xticklabels=labels, yticklabels=labels)\nplt.xlabel('Predicted Class')\nplt.ylabel('Original Class')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Recall matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"A =(((C.T)/(C.sum(axis=1))).T)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,7))\nsns.heatmap(A, annot=True, cmap=\"YlGnBu\", fmt=\".3f\", xticklabels=labels, yticklabels=labels)\nplt.xlabel('Predicted Class')\nplt.ylabel('Original Class')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Evaluating Gene Column\n\nNow we will look at each independent column to make sure its relavent for my target variable but the question is, how? Let's understand with our first column Gene which is categorial in nature.\n\nSo, lets explore column ***Gene*** and lets look at its distribution. "},{"metadata":{"trusted":true},"cell_type":"code","source":"unique_genes = train_df['Gene'].value_counts()\nprint('Number of Unique Genes:', unique_genes.shape[0])\n# the top 10 genes that occured most\nprint(unique_genes.head(10))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets see the number of unique values present in gene"},{"metadata":{"trusted":true},"cell_type":"code","source":"unique_genes.shape[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets look at the comulative distribution of unique Genes values"},{"metadata":{"trusted":true},"cell_type":"code","source":"s = sum(unique_genes.values)\nh = unique_genes.values / s\nc = np.cumsum(h)\nplt.plot(c, label='Cumulative Distribution of Genes')\nplt.grid()\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So, now we need to convert these categorical variable to appropirate format which my machine learning algorithm will be able to take as an input.\n\nSo we have 2 techniques to deal with it. \n\n<ol><li>\n     ***One-hot encoding*** </li>\n    <li> ***Response Encoding*** (Mean imputation) </li>\n</ol>\n\nLet's use both of them to see which one work the best. So lets start encoding using one hot encoder"},{"metadata":{"trusted":true},"cell_type":"code","source":"# One-hot encoding of Gene feature\ngene_vectorizer = CountVectorizer()\ntrain_gene_feature_onehotCoding = gene_vectorizer.fit_transform(train_df['Gene'])\ntest_gene_feature_onehotCoding = gene_vectorizer.fit_transform(test_df['Gene'])\ncv_gene_feature_onehotCoding = gene_vectorizer.fit_transform(cv_df['Gene'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's check the number of column generated after one hot encoding. One hot encoding will always return higher number of column."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_gene_feature_onehotCoding.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, lets also create Response encoding columns for Gene column"},{"metadata":{"trusted":true},"cell_type":"code","source":"# code for response coding with Laplace smoothing.\n# alpha : used for laplace smoothing\n# feature: ['gene', 'variation']\n# df: ['train_df', 'test_df', 'cv_df']\n# algorithm\n# ----------\n# Consider all unique values and the number of occurances of given feature in train data dataframe\n# build a vector (1*9) , the first element = (number of times it occured in class1 + 10*alpha / number of time it occurred in total data+90*alpha)\n# gv_dict is like a look up table, for every gene it store a (1*9) representation of it\n# for a value of feature in df:\n# if it is in train data:\n# we add the vector that was stored in 'gv_dict' look up table to 'gv_fea'\n# if it is not there is train:\n# we add [1/9, 1/9, 1/9, 1/9,1/9, 1/9, 1/9, 1/9, 1/9] to 'gv_fea'\n# return 'gv_fea'\n# ----------------------\n\n# get_gv_fea_dict: Get Gene varaition Feature Dict\ndef get_gv_fea_dict(alpha, feature, df):\n    # value_count: it contains a dict like\n    # print(train_df['Gene'].value_counts())\n    # output:\n    #        {BRCA1      174\n    #         TP53       106\n    #         EGFR        86\n    #         BRCA2       75\n    #         PTEN        69\n    #         KIT         61\n    #         BRAF        60\n    #         ERBB2       47\n    #         PDGFRA      46\n    #         ...}\n    # print(train_df['Variation'].value_counts())\n    # output:\n    # {\n    # Truncating_Mutations                     63\n    # Deletion                                 43\n    # Amplification                            43\n    # Fusions                                  22\n    # Overexpression                            3\n    # E17K                                      3\n    # Q61L                                      3\n    # S222D                                     2\n    # P130S                                     2\n    # ...\n    # }\n    value_count = train_df[feature].value_counts()\n    \n    # gv_dict : Gene Variation Dict, which contains the probability array for each gene/variation\n    gv_dict = dict()\n    \n    # denominator will contain the number of time that particular feature occured in whole data\n    for i, denominator in value_count.items():\n        # vec will contain (p(yi==1/Gi) probability of gene/variation belongs to perticular class\n        # vec is 9 diamensional vector\n        vec = []\n        for k in range(1,10):\n            # print(train_df.loc[(train_df['Class']==1) & (train_df['Gene']=='BRCA1')])\n            #         ID   Gene             Variation  Class  \n            # 2470  2470  BRCA1                S1715C      1   \n            # 2486  2486  BRCA1                S1841R      1   \n            # 2614  2614  BRCA1                   M1R      1   \n            # 2432  2432  BRCA1                L1657P      1   \n            # 2567  2567  BRCA1                T1685A      1   \n            # 2583  2583  BRCA1                E1660G      1   \n            # 2634  2634  BRCA1                W1718L      1   \n            # cls_cnt.shape[0] will return the number of rows\n\n            cls_cnt = train_df.loc[(train_df['Class']==k) & (train_df[feature]==i)]\n            \n            # cls_cnt.shape[0](numerator) will contain the number of time that particular feature occured in whole data\n            vec.append((cls_cnt.shape[0] + alpha*10)/ (denominator + 90*alpha))\n\n        # we are adding the gene/variation to the dict as key and vec as value\n        gv_dict[i]=vec\n    return gv_dict\n\n# Get Gene variation feature\ndef get_gv_feature(alpha, feature, df):\n    # print(gv_dict)\n    #     {'BRCA1': [0.20075757575757575, 0.03787878787878788, 0.068181818181818177, 0.13636363636363635, 0.25, 0.19318181818181818, 0.03787878787878788, 0.03787878787878788, 0.03787878787878788], \n    #      'TP53': [0.32142857142857145, 0.061224489795918366, 0.061224489795918366, 0.27040816326530615, 0.061224489795918366, 0.066326530612244902, 0.051020408163265307, 0.051020408163265307, 0.056122448979591837], \n    #      'EGFR': [0.056818181818181816, 0.21590909090909091, 0.0625, 0.068181818181818177, 0.068181818181818177, 0.0625, 0.34659090909090912, 0.0625, 0.056818181818181816], \n    #      'BRCA2': [0.13333333333333333, 0.060606060606060608, 0.060606060606060608, 0.078787878787878782, 0.1393939393939394, 0.34545454545454546, 0.060606060606060608, 0.060606060606060608, 0.060606060606060608], \n    #      'PTEN': [0.069182389937106917, 0.062893081761006289, 0.069182389937106917, 0.46540880503144655, 0.075471698113207544, 0.062893081761006289, 0.069182389937106917, 0.062893081761006289, 0.062893081761006289], \n    #      'KIT': [0.066225165562913912, 0.25165562913907286, 0.072847682119205295, 0.072847682119205295, 0.066225165562913912, 0.066225165562913912, 0.27152317880794702, 0.066225165562913912, 0.066225165562913912], \n    #      'BRAF': [0.066666666666666666, 0.17999999999999999, 0.073333333333333334, 0.073333333333333334, 0.093333333333333338, 0.080000000000000002, 0.29999999999999999, 0.066666666666666666, 0.066666666666666666],\n    #      ...\n    #     }\n    gv_dict = get_gv_fea_dict(alpha, feature, df)\n    # value_count is similar in get_gv_fea_dict\n    value_count = train_df[feature].value_counts()\n    \n    # gv_fea: Gene_variation feature, it will contain the feature for each feature value in the data\n    gv_fea = []\n    # for every feature values in the given data frame we will check if it is there in the train data then we will add the feature to gv_fea\n    # if not we will add [1/9,1/9,1/9,1/9,1/9,1/9,1/9,1/9,1/9] to gv_fea\n    for index, row in df.iterrows():\n        if row[feature] in dict(value_count).keys():\n            gv_fea.append(gv_dict[row[feature]])\n        else:\n            gv_fea.append([1/9,1/9,1/9,1/9,1/9,1/9,1/9,1/9,1/9])\n#             gv_fea.append([-1,-1,-1,-1,-1,-1,-1,-1,-1])\n    return gv_fea","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#response-coding of the Gene feature\n# alpha is used for laplace smoothing\nalpha = 1\n# train gene feature\ntrain_gene_feature_responseCoding = np.array(get_gv_feature(alpha, \"Gene\", train_df))\n# test gene feature\ntest_gene_feature_responseCoding = np.array(get_gv_feature(alpha, \"Gene\", test_df))\n# cross validation gene feature\ncv_gene_feature_responseCoding = np.array(get_gv_feature(alpha, \"Gene\", cv_df))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's look at columns after applying response encoding. We must be having 9 columns for Gene column after response encoding."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_gene_feature_responseCoding.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, question is how good is Gene column feature to predict my 9 classes. One idea could be that we will build model having only gene column with one hot encoder with simple model like Logistic regression. If log loss with only one column Gene comes out to be better than random model, than this feature is important."},{"metadata":{"trusted":true},"cell_type":"code","source":"# We need a hyperparameter for SGD classifier\nalpha = [10 ** x for x in range(-5, 1)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We will be using SGD classifier\n# http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html\n# We will also be using Calibrated Classifier to get the result into probablity format t be used for log loss\ncv_log_error_array=[]\n\nfor i in alpha:\n    clf = SGDClassifier(alpha=i, penalty='l2', loss='log', random_state=42)\n    clf.fit(train_gene_feature_onehotCoding, y_train)\n    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n    sig_clf.fit(train_gene_feature_onehotCoding, y_train)\n    predict_y = sig_clf.predict_proba(cv_gene_feature_onehotCoding)\n    cv_log_error_array.append(log_loss(y_cv, predict_y, labels=clf.classes_, eps=1e-15))\n    print('For values of alpha = ', i, \"The log loss is:\",log_loss(y_cv, predict_y, labels=clf.classes_, eps=1e-15))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's plot the same to check the best Alpha value\nfig, ax = plt.subplots()\nax.plot(alpha, cv_log_error_array,c='g')\nfor i, txt in enumerate(np.round(cv_log_error_array,3)):\n    ax.annotate((alpha[i],np.round(txt,3)), (alpha[i],cv_log_error_array[i]))\nplt.grid()\nplt.title(\"Cross Validation Error for each alpha\")\nplt.xlabel(\"Alpha i's\")\nplt.ylabel(\"Error measure\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}