{"cells":[{"metadata":{"id":"4cbgwZWWfWpp"},"cell_type":"markdown","source":"# New Section"},{"metadata":{"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"id":"D2n6SkYlMdf4","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport re\nimport time\nimport warnings\nimport numpy as np\nfrom nltk.corpus import stopwords\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.preprocessing import normalize\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.manifold import TSNE\nimport seaborn as sns\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics.classification import accuracy_score, log_loss\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import SGDClassifier\nfrom imblearn.over_sampling import SMOTE\nfrom collections import Counter\nfrom scipy.sparse import hstack\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import StratifiedKFold \nfrom collections import Counter, defaultdict\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nimport math\nfrom sklearn.metrics import normalized_mutual_info_score\nfrom sklearn.ensemble import RandomForestClassifier\nwarnings.filterwarnings(\"ignore\")\n\nfrom mlxtend.classifier import StackingClassifier\n\nfrom sklearn import model_selection\nfrom sklearn.linear_model import LogisticRegression\n","execution_count":null,"outputs":[]},{"metadata":{"id":"jOX2UjwsPrzm","trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"id":"asS6JFYrRCRh"},"cell_type":"markdown","source":"## **A.<U>Business constraints/requirements</U>:** <br> 1. No low latency requirements<br>2.Interpretability is very important<br>3.Very low error or no error<br>4.Probabilistic prediction for interpretablity"},{"metadata":{"id":"gRrbjkukTLiJ"},"cell_type":"markdown","source":"# <h1>B.Data Overview</h1>\n<h2><h6>training_variants</h6>\n<hr>\nID,Gene,Variation,Class<br>\n0,FAM58A,Truncating Mutations,1 <br>\n1,CBL,W802*,2 <br>\n2,CBL,Q249E,2 <br>\n...\n\n<h6> training_text</h6>\n<hr>\nID,Text <br>\n0||Cyclin-dependent kinases (CDKs) regulate a variety of fundamental cellular processes. CDK10 stands out as one of the last orphan CDKs for which no activating cyclin has been identified and no kinase activity revealed. Previous work has shown that CDK10 silencing increases ETS2 (v-ets erythroblastosis virus E26 oncogene homolog 2)-driven activation of the MAPK pathway, which confers tamoxifen resistance to breast cancer cells. The precise mechanisms by which CDK10 modulates ETS2 activity, and more generally the functions of CDK10, remain elusive. Here we demonstrate that CDK10 is a cyclin-dependent kinase by identifying cyclin M as an activating cyclin. Cyclin M, an orphan cyclin, is the product of FAM58A, whose mutations cause STAR syndrome, a human developmental anomaly whose features include toe syndactyly, telecanthus, and anogenital and renal malformations. We show that STAR syndrome-associated cyclin M mutants are unable to interact with CDK10. Cyclin M silencing phenocopies CDK10 silencing in increasing c-Raf and in conferring tamoxifen resistance to breast cancer cells. CDK10/cyclin M phosphorylates ETS2 in vitro, and in cells it positively controls ETS2 degradation by the proteasome. ETS2 protein levels are increased in cells derived from a STAR patient, and this increase is attributable to decreased cyclin M levels. Altogether, our results reveal an additional regulatory mechanism for ETS2, which plays key roles in cancer and development. They also shed light on the molecular mechanisms underlying STAR syndrome.Cyclin-dependent kinases (CDKs) play a pivotal role in the control of a number of fundamental cellular processes (1). The human genome contains 21 genes encoding proteins that can be considered as members of the CDK family owing to their sequence similarity with bona fide CDKs, those known to be activated by cyclins (2). Although discovered almost 20 y ago (3, 4), CDK10 remains one of the two CDKs without an identified cyclin partner. This knowledge gap has largely impeded the exploration of its biological functions. CDK10 can act as a positive cell cycle regulator in some cells (5, 6) or as a tumor suppressor in others (7, 8). CDK10 interacts with the ETS2 (v-ets erythroblastosis virus E26 oncogene homolog 2) transcription factor and inhibits its transcriptional activity through an unknown mechanism (9). CDK10 knockdown derepresses ETS2, which increases the expression of the c-Raf protein kinase, activates the MAPK pathway, and induces resistance of MCF7 cells to tamoxifen (6). ... </h2>"},{"metadata":{"id":"9WKFtleIUS46"},"cell_type":"markdown","source":"# **C. Mapping to ML problem**\n<br>1.Problem type: Multi class classification\n<br>2.Metric: Multi Log-loss(9 classes)\n<br>3.Metric: Multi class Confusion Matrix(With negligible FNR)-->shape=(9,9)"},{"metadata":{"id":"DYTyGgleZcRd"},"cell_type":"markdown","source":"# **D. Exploratory Data Analysis**"},{"metadata":{"id":"Utmfto8bRnI0","outputId":"42505d1b-aeba-46de-ee4e-8273dde3afa3","trusted":true},"cell_type":"code","source":"data_variants_tr = pd.read_csv('../input/cancer-data/training_variants')\nprint('shape of variants data:',data_variants_tr.shape)\ndata_variants_tr.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"6wCW0Vs9auH0","outputId":"bb920eda-bb0c-4ba8-fc17-0f96f1d0a15e","trusted":true},"cell_type":"code","source":"# note the seprator in this file\ndata_text =pd.read_csv(\"../input/cancer-data/training_text\",sep=\"\\|\\|\",engine=\"python\",names=[\"ID\",\"TEXT\"],skiprows=1)\nprint('Number of data points : ', data_text.shape[0])\nprint('Number of features : ', data_text.shape[1])\nprint('Features : ', data_text.columns.values)\ndata_text.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"MY-feGtSdG2M","outputId":"c94f70d8-d649-4745-9135-648fba302ff1","trusted":true},"cell_type":"code","source":"data_text['TEXT'][1]","execution_count":null,"outputs":[]},{"metadata":{"id":"VZN1OA_Vco0K"},"cell_type":"markdown","source":"#  **1.Text preprocessing: Here help has been taken from internet.**"},{"metadata":{"id":"FF1Tv7g3axcI","outputId":"b7f256ac-c8e6-4e59-8631-4bc4586e7bf3","trusted":true},"cell_type":"code","source":"import nltk\nnltk.download('stopwords')\n# loading stop words from nltk library\nstop_words = set(stopwords.words('english'))\n\n\ndef nlp_preprocessing(total_text, index, column):\n    if type(total_text) is not int:\n        string = \"\"\n        # replace every special char with space\n        total_text = re.sub('[^a-zA-Z0-9\\n]', ' ', total_text)\n        # replace multiple spaces with single space\n        total_text = re.sub('\\s+',' ', total_text)\n        # converting all the chars into lower-case.\n        total_text = total_text.lower()\n        \n        for word in total_text.split():\n        # if the word is a not a stop word then retain that word from the data\n            if not word in stop_words:\n                string += word + \" \"\n        \n        data_text[column][index] = string","execution_count":null,"outputs":[]},{"metadata":{"id":"ilpyJa3il1Oc","outputId":"a9b25a1a-da26-444e-bd11-13a03f13855d","trusted":true},"cell_type":"code","source":"data_text['TEXT']","execution_count":null,"outputs":[]},{"metadata":{"id":"qYI6bAAYhJv8","outputId":"891060c2-6cb8-409c-d34a-ca6b79600e04","trusted":true},"cell_type":"code","source":"#text processing stage.\nstart_time = time.clock()\nfor index, row in data_text.iterrows():\n    if type(row['TEXT']) is str:\n        nlp_preprocessing(row['TEXT'], index, 'TEXT')\n    else:\n        print(\"there is no text description for id:\",index)\nprint('Time took for preprocessing the text :',time.clock() - start_time, \"seconds\")","execution_count":null,"outputs":[]},{"metadata":{"id":"cChHmDyLrchn","trusted":true},"cell_type":"code","source":"result = pd.merge(data_variants_tr,data_text,on='ID',how='left')","execution_count":null,"outputs":[]},{"metadata":{"id":"Mc-cQ7LlsATW","outputId":"5d2c1701-c52d-487a-928f-38f599a1134d","trusted":true},"cell_type":"code","source":"result.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"lYjbOpE0swqk","trusted":true},"cell_type":"code","source":"result.drop('ID',axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"id":"x6liU1ZSTjHT","outputId":"29dbf376-f6da-4c31-f665-527107473621","trusted":true},"cell_type":"code","source":"result.shape","execution_count":null,"outputs":[]},{"metadata":{"id":"0dPACTsgUrjB","outputId":"a1d8e694-5ea0-463f-9b36-314e8af9bbdd","trusted":true},"cell_type":"code","source":"result[result.isnull().any(axis=1)]","execution_count":null,"outputs":[]},{"metadata":{"id":"WbsrOt4aJC4I","outputId":"a5f8c71a-1796-4a07-89fd-0e8ecdcbc137","trusted":true},"cell_type":"code","source":"#result = pd.merge(data_variants_tr,data_text,on='ID',how='left')\nresult.loc[result['TEXT'].isnull(),'TEXT'] = result['Gene'] +' '+result['Variation']\nresult[result['Gene']=='FANCA']","execution_count":null,"outputs":[]},{"metadata":{"id":"gPOWBf3nJCbi"},"cell_type":"markdown","source":""},{"metadata":{"id":"tidO0-1TsYBr"},"cell_type":"markdown","source":"# **2.Splitting Data into train,test and CV**"},{"metadata":{"id":"94jhqmaEsDHy","outputId":"aa44f379-322d-45f1-e1bf-ff582a1114da","trusted":true},"cell_type":"code","source":"\nresult.Gene      = result.Gene.str.replace('\\s+', '_')\nresult.Variation = result.Variation.str.replace('\\s+', '_')\nresult_x = result.drop('Class',axis=1)\nresult_y = result['Class']\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,Y_train,Y_test = train_test_split(result,result_y,stratify=result_y,test_size=0.2)\nX_tr,X_cv,Y_tr,Y_cv = train_test_split(X_train,Y_train,stratify=Y_train,test_size=0.2)\n\nprint('Train data shape: ',X_tr.shape)\nprint('Train class shape: ',Y_tr.shape)\nprint('Test data shape: ',X_test.shape)\nprint('Test class shape: ',Y_test.shape)\nprint('CV data shape: ',X_cv.shape)\nprint('CV class shape: ',Y_cv.shape)","execution_count":null,"outputs":[]},{"metadata":{"id":"R1xLne2VH0pY","outputId":"453af8c8-2c07-47c8-89f2-556b815be062","trusted":true},"cell_type":"code","source":"len(X_cv['Gene'].unique())","execution_count":null,"outputs":[]},{"metadata":{"id":"auwf9RKqtsrc"},"cell_type":"markdown","source":"# **3.Checking for imbalance in data across train,test and CV**"},{"metadata":{"id":"1DzlPTuMtnO4","outputId":"472d68e6-7855-403a-cfe9-664baed32b5f","trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,10))\nsns.countplot(Y_tr)\nplt.title('Class distribution of train data',fontsize=30)\nplt.xlabel('Class',fontsize=30)\nplt.ylabel('count',fontsize=30)\nplt.show()\nfor i in range(1,10):\n print('share of class {0} is {1} with {2} datapoints.'.format(i,100*(Y_tr[Y_tr==i].count())/len(Y_tr),Y_tr[Y_tr==i].count()))","execution_count":null,"outputs":[]},{"metadata":{"id":"YgIAt9-RuK3j","outputId":"3a81e6b8-de5b-473e-9219-72eb7759f871","trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,10))\nsns.countplot(Y_test)\nplt.title('Class distribution of test data',fontsize=30)\nplt.xlabel('Class',fontsize=30)\nplt.ylabel('count',fontsize=30)\nplt.show()\nfor i in range(1,10):\n print('share of class {0} is {1} with {2} datapoints.'.format(i,100*(Y_test[Y_test==i].count())/len(Y_test),Y_test[Y_test==i].count()))","execution_count":null,"outputs":[]},{"metadata":{"id":"bBlT0HIRuUvG","outputId":"7fd15598-3e54-4a2c-f160-de0f3387fa80","trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,10))\nsns.countplot(Y_cv)\nplt.title('Class distribution of CV data',fontsize=30)\nplt.xlabel('Class',fontsize=30)\nplt.ylabel('count',fontsize=30)\nplt.show()\nfor i in range(1,10):\n print('share of class {0} is {1} with {2} datapoints.'.format(i,100*(Y_cv[Y_cv==i].count())/len(Y_cv),Y_cv[Y_cv==i].count()))","execution_count":null,"outputs":[]},{"metadata":{"id":"kFMQTsEHxa_E"},"cell_type":"markdown","source":"# *Conclusion: <br>1.We have Imbalanced datasets among Class.<br>2. Our distribution of class across train, test and CV are almost same.*"},{"metadata":{"id":"FCv4nA4w4fp8"},"cell_type":"markdown","source":"# **4. Testing a dummy model and getting its log-loss score**<h5><br>Why we need dummy model?<br>Ans: Becuase the business requirement is to have multilog loss and we know that log-loss=[0,inf]. So if we have some log-loss let say 2.1 then we don't have reference to compare if this model is good or bad. So we need a dummy model  and compare our model with log-loss of dummy model to know where we are."},{"metadata":{"id":"st0fZHjNxWdz","outputId":"252a633f-e607-436d-f429-9ef446987f14","trusted":true},"cell_type":"code","source":"# This function plots the confusion matrices given y_i, y_i_hat.\ndef plot_confusion_matrix(test_y, predict_y):\n    C = confusion_matrix(test_y, predict_y)\n    # C = 9,9 matrix, each cell (i,j) represents number of points of class i are predicted class j\n    \n    A =(((C.T)/(C.sum(axis=1))).T)\n    #divid each element of the confusion matrix with the sum of elements in that column\n    \n    # C = [[1, 2],\n    #     [3, 4]]\n    # C.T = [[1, 3],\n    #        [2, 4]]\n    # C.sum(axis = 1)  axis=0 corresonds to columns and axis=1 corresponds to rows in two diamensional array\n    # C.sum(axix =1) = [[3, 7]]\n    # ((C.T)/(C.sum(axis=1))) = [[1/3, 3/7]\n    #                           [2/3, 4/7]]\n\n    # ((C.T)/(C.sum(axis=1))).T = [[1/3, 2/3]\n    #                           [3/7, 4/7]]\n    # sum of row elements = 1\n    \n    B =(C/C.sum(axis=0))\n    #divid each element of the confusion matrix with the sum of elements in that row\n    # C = [[1, 2],\n    #     [3, 4]]\n    # C.sum(axis = 0)  axis=0 corresonds to columns and axis=1 corresponds to rows in two diamensional array\n    # C.sum(axix =0) = [[4, 6]]\n    # (C/C.sum(axis=0)) = [[1/4, 2/6],\n    #                      [3/4, 4/6]] \n    \n    labels = [1,2,3,4,5,6,7,8,9]\n    # representing A in heatmap format\n    print(\"-\"*20, \"Confusion matrix\", \"-\"*20)\n    plt.figure(figsize=(20,7))\n    sns.heatmap(C, annot=True, cmap=\"YlGnBu\", fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n    plt.xlabel('Predicted Class')\n    plt.ylabel('Original Class')\n    plt.show()\n\n    print(\"-\"*20, \"Precision matrix (Columm Sum=1)\", \"-\"*20)\n    plt.figure(figsize=(20,7))\n    sns.heatmap(B, annot=True, cmap=\"YlGnBu\", fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n    plt.xlabel('Predicted Class')\n    plt.ylabel('Original Class')\n    plt.show()\n    \n    # representing B in heatmap format\n    print(\"-\"*20, \"Recall matrix (Row sum=1)\", \"-\"*20)\n    plt.figure(figsize=(20,7))\n    sns.heatmap(A, annot=True, cmap=\"YlGnBu\", fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n    plt.xlabel('Predicted Class')\n    plt.ylabel('Original Class')\n    plt.show()\n\n\n\n    # we need to generate 9 numbers and the sum of numbers should be 1\n# one solution is to genarate 9 numbers and divide each of the numbers by their sum\n# ref: https://stackoverflow.com/a/18662466/4084039\ntest_data_len = X_test.shape[0]\ncv_data_len = X_cv.shape[0]\n\n# we create a output array that has exactly same size as the CV data\ncv_predicted_y = np.zeros((cv_data_len,9))\nfor i in range(cv_data_len):\n    rand_probs = np.random.rand(1,9)\n    cv_predicted_y[i] = ((rand_probs/sum(sum(rand_probs)))[0])\nprint(\"Log loss on Cross Validation Data using Random Model\",log_loss(Y_cv,cv_predicted_y, eps=1e-15))\n\n\n# Test-Set error.\n#we create a output array that has exactly same as the test data\ntest_predicted_y = np.zeros((test_data_len,9))\nfor i in range(test_data_len):\n    rand_probs = np.random.rand(1,9)\n    test_predicted_y[i] = ((rand_probs/sum(sum(rand_probs)))[0])\nprint(\"Log loss on Test Data using Random Model\",log_loss(Y_test,test_predicted_y, eps=1e-15))\n\npredicted_y =np.argmax(test_predicted_y, axis=1)\nplot_confusion_matrix(Y_test, predicted_y+1)","execution_count":null,"outputs":[]},{"metadata":{"id":"jiKo6862Zf-h"},"cell_type":"markdown","source":"# <I>Text Encoding block<br>1.Response Encoding with laplace smoothing<br>2. One Hot Encoding<br>3. TFIDF Encoding"},{"metadata":{"id":"jt1Jnig1Za25","trusted":true},"cell_type":"code","source":"def res_encoding(cat_feature,tar_class):\n  genes = list(set(cat_feature))\n  vec_list = []\n  #print(genes)\n  df = pd.DataFrame({'cat_feature':cat_feature,'tar_class':tar_class})\n  #print('line 2')\n  #print(df)\n  dic_feature_prob_given_target = {}\n  for i in genes:\n    dic_feature_prob_given_target[i]=[]\n    for j in range(1,10):\n         #print('inside 2nd for loop') \n         #print(df[(df.cat_feature==i) & (df.tar_class==j)].count() / (df[df.cat_feature==i].count()))\n         dic_feature_prob_given_target[i].append(((df['cat_feature'][(df['cat_feature']==i) & (df['tar_class']==j)].count())+10) / ((df['cat_feature'][df['cat_feature']==i].count())+90))\n         #print('below dict')\n  for i in cat_feature:\n    vec_list.append(dic_feature_prob_given_target[i])       \n  return np.array(vec_list)\n\n\n\n\ndef one_hot_encoding(feature_tr,feature):\n    from sklearn.feature_extraction.text import CountVectorizer\n    from sklearn.preprocessing import StandardScaler\n    scaler=StandardScaler()\n    one_hot_encoder = CountVectorizer()\n    one_hot_encoder.fit(feature_tr)\n    one_hot_vec = one_hot_encoder.transform(feature).toarray()\n    scaler.fit(one_hot_vec)\n    return scaler.transform(one_hot_vec)\n\n\ndef tfidf_vectorizer(feature_tr,feature):\n    from sklearn.feature_extraction.text import TfidfVectorizer \n    tfidf = TfidfVectorizer()\n    tfidf.fit_transform(feature_tr)\n    vec= tfidf.transform(feature).toarray()\n    #print(tfidf.get_feature_names())\n    return vec","execution_count":null,"outputs":[]},{"metadata":{"id":"pT_WrfPrVpTz"},"cell_type":"markdown","source":"# **5.Univariate Analysis**<br><h5>To check the feature importance when we have <b><I>less number of dimensions</I></b> in our original dataset is to build a simple sensible linear model on top of each feature and watch the performance of the model. If its performing considerably better than the dummy model score then this feature is important for our objective. Otherwise we have perform some other advanced testing on features."},{"metadata":{"id":"78EGlmlUXIQj"},"cell_type":"markdown","source":"# *5.1: Feature--> Gene*"},{"metadata":{"id":"bKb23hepXjzL"},"cell_type":"markdown","source":"# <h5>1. What kind of feature(data type) is this?<br>2.What are the different categories and how many are they?<br>3.How to featurize it?<br>4.How well this feature is performing?<br>5.6.Is this feature stable across all dataset(train,test and CV)?</h5><br><br>We will try to answer all these questions with good amount of reasoning."},{"metadata":{"id":"z4kOjNZoYvKu"},"cell_type":"markdown","source":"# <h5>Ans(1): This feature is categorical type feature"},{"metadata":{"id":"K0OaByTD5qTR","outputId":"0755ee5c-7d09-4052-e383-6abe365726c5","trusted":true},"cell_type":"code","source":"len(result['Gene'].unique())","execution_count":null,"outputs":[]},{"metadata":{"id":"1kFUOcFBZMW_"},"cell_type":"markdown","source":"# <h5>Ans(2): We have <b>262</b> different types of genes.<br>Ans(3): Response, One Hot, TFIDF"},{"metadata":{"id":"7pwL23nw44Tx"},"cell_type":"markdown","source":"# *5.2:Vectorizing Gene*"},{"metadata":{"id":"M-rt_NkgZJ_A","trusted":true},"cell_type":"code","source":"gene_one_hot_tr = one_hot_encoding(X_tr['Gene'],X_tr['Gene'])\ngene_response_tr = res_encoding(X_tr['Gene'],Y_tr)\ngene_tfidf_tr = tfidf_vectorizer(X_tr['Gene'],X_tr['Gene'])\n\ngene_one_hot_cv = one_hot_encoding(X_tr['Gene'],X_cv['Gene'])\ngene_response_cv = res_encoding(X_cv['Gene'],Y_cv)\ngene_tfidf_cv = tfidf_vectorizer(X_tr['Gene'],X_cv['Gene'])\n\ngene_one_hot_test = one_hot_encoding(X_tr['Gene'],X_test['Gene'])\ngene_response_test = res_encoding(X_test['Gene'],Y_test)\ngene_tfidf_test = tfidf_vectorizer(X_tr['Gene'],X_test['Gene'])","execution_count":null,"outputs":[]},{"metadata":{"id":"XjrfIkopUTpe","outputId":"b2374d90-c258-433e-9e8c-c8d566fdba09","trusted":true},"cell_type":"code","source":"gene_response_test.shape","execution_count":null,"outputs":[]},{"metadata":{"id":"QOYq3Qvx6aPk"},"cell_type":"markdown","source":"# *5.3: Applying simple logistic regression on \"Gene\"  feature*"},{"metadata":{"id":"xv1kyMPd5fUz","outputId":"3e1dc33d-84a9-4c8c-8fce-696b95f20d99","trusted":true},"cell_type":"code","source":"uniq_val = result['Gene'].value_counts()\nplt.figure(figsize=(15,10))\nplt.plot(uniq_val.values / uniq_val.sum(),color='blue',linewidth=3)\nplt.title('percentage share of genes',fontsize=30)\nplt.grid(b=True)\nplt.xlabel('indices of genes',fontsize=20)\nplt.ylabel('% share',fontsize=20)\nplt.annotate('total unique genes= '+str(len(uniq_val)),xy=(180,0.075),fontsize=20,color='red' )\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"id":"FazFaj8F93gT"},"cell_type":"markdown","source":"# *The curve drops very fast which means that very few of the gens are dominating in numbers.*"},{"metadata":{"id":"3CJNbi5u7vfi","outputId":"2875fb23-3bda-4758-cb03-eb2bfa05d2cd","trusted":true},"cell_type":"code","source":"cs = np.cumsum(uniq_val.values / uniq_val.sum())\nplt.figure(figsize=(15,10))\nplt.plot(cs,color='blue',linewidth=3)\nplt.title('cummulative sum',fontsize=30)\nplt.grid(b=True)\nplt.xlabel('indices of genes',fontsize=20)\nplt.ylabel('% share',fontsize=20)\nplt.annotate('total unique genes= '+str(len(uniq_val)),xy=(180,0.075),fontsize=20,color='red' )\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"8RNexydz-yFK"},"cell_type":"markdown","source":"# *We can see that only close to 60 genes comprise of 80% of the total genes. Or in other words we can say that 60/262 = 23 % of the total unique genes account for 80% of total genes.*"},{"metadata":{"id":"LaF16bK2AKPa"},"cell_type":"markdown","source":"# *5.4: Checking performance of gene on LR model*"},{"metadata":{"id":"ikW8BS4X-hek","trusted":true},"cell_type":"code","source":"def performance(vector_tr,vector_test,vector_cv,Y_tr,Y_test,Y_cv,alpha):\n  error_cv=[]\n  error_tr=[]\n  error_test = []\n  for i in alpha:\n      clf = SGDClassifier(alpha=i, penalty='l2', loss='log', random_state=42)\n      clf.fit(vector_tr, Y_tr)\n      sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\") # we want our predicted value to be a probability for the interpretability hence we are using CalibratedClassifierCV\n      sig_clf.fit(vector_tr,Y_tr)\n      predict_y_tr = sig_clf.predict_proba(vector_tr)\n      predict_y = sig_clf.predict_proba(vector_cv)\n      predict_y_test = sig_clf.predict_proba(vector_test)\n      error_cv.append(log_loss(Y_cv, predict_y, labels=clf.classes_, eps=1e-15))\n      error_tr.append(log_loss(Y_tr, predict_y_tr, labels=clf.classes_, eps=1e-15))\n      error_test.append(log_loss(Y_test, predict_y_test, labels=clf.classes_, eps=1e-15))  \n  plt.figure(figsize=(15,10))\n  plt.plot(error_cv,color='blue',linewidth=3)\n  plt.plot(error_tr,color='grey',linewidth=3)\n  plt.plot(error_test,color='red',linewidth=3)\n  plt.title('performance checker',fontsize=30)\n  plt.grid(b=True)\n  plt.legend(['CV','Train','test'])\n  plt.xlabel('Hyperparameter value',fontsize=20)\n  plt.ylabel('log-loss',fontsize=20)\n  #for i,j in zip(alpha,error_cv):\n      #plt.annotate(str(round(j,2)),xy=(i,j),fontsize=20,color='grey' )\n  plt.show()\n  for i in (range(len(alpha))):\n    print('Log loss is train =  {0}, test = {1} and cv = {2} for alpha value {3}'.format(error_tr[i],error_test[i],error_cv[i],alpha[i] )) \n \n\n\n","execution_count":null,"outputs":[]},{"metadata":{"id":"B-4QDwV4Eeqe","outputId":"56904566-f55f-4d68-8f31-22825cf0260d","trusted":true},"cell_type":"code","source":"print('*'*50,'One Hot Encoding','*'*50)\nalpha = [10 ** x for x in range(-5, 1)]\nperformance(gene_one_hot_tr,gene_one_hot_test,gene_one_hot_cv,Y_tr,Y_test,Y_cv,alpha) \nprint('*'*50,'Response Encoding','*'*50)\nperformance(gene_response_tr,gene_response_test,gene_response_cv,Y_tr,Y_test,Y_cv,alpha) \nprint('*'*50,'TFIDF Encoding','*'*50)\nperformance(gene_tfidf_tr,gene_tfidf_test,gene_tfidf_cv,Y_tr,Y_test,Y_cv,alpha) ","execution_count":null,"outputs":[]},{"metadata":{"id":"yDW6CQKfX-J5"},"cell_type":"markdown","source":"# ***Conclusions: As we can see the plots One Hot Encoding with Logistic Regression ic clear winner here. which gives <br>Log loss:<br>train =  0.9852677424374561 <br>test = 1.232340278209871 <br>cv = 1.1964023839802274 <br>for alpha value 0.01***<br>From this we van conclude that feature GENE is a very important feature when compared to dummy model which had logloss=2.5."},{"metadata":{"id":"QMkX2iFVTpcl","trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"id":"7nLq41xhcHw1"},"cell_type":"markdown","source":"# **6: Feature: Variation**<br> we will do similar analysis  for this feature too as done for Gene."},{"metadata":{"id":"ZQeucng5cdY7"},"cell_type":"markdown","source":"# *6.1: Vectorization*"},{"metadata":{"id":"0-HSv76UcWa-","trusted":true},"cell_type":"code","source":"var_one_hot_tr = one_hot_encoding(X_tr['Variation'],X_tr['Variation'])\nvar_response_tr = res_encoding(X_tr['Variation'],Y_tr)\nvar_tfidf_tr = tfidf_vectorizer(X_tr['Variation'],X_tr['Variation'])\n\nvar_one_hot_cv = one_hot_encoding(X_tr['Variation'],X_cv['Variation'])\nvar_response_cv = res_encoding(X_cv['Variation'],Y_cv)\nvar_tfidf_cv = tfidf_vectorizer(X_tr['Variation'],X_cv['Variation'])\n\nvar_one_hot_test = one_hot_encoding(X_tr['Variation'],X_test['Variation'])\nvar_response_test = res_encoding(X_test['Variation'],Y_test)\nvar_tfidf_test = tfidf_vectorizer(X_tr['Variation'],X_test['Variation'])","execution_count":null,"outputs":[]},{"metadata":{"id":"uvADFeh1dFLG","outputId":"01d39fab-47d6-4805-e5f6-9a1a110c975d","trusted":true},"cell_type":"code","source":"uniq_val_var = result['Variation'].value_counts()\nplt.figure(figsize=(15,10))\nplt.plot(uniq_val_var.values / uniq_val_var.sum(),color='blue',linewidth=3)\nplt.title('percentage share of Variation',fontsize=30)\nplt.grid(b=True)\nplt.xlabel('indices of Variation',fontsize=20)\nplt.ylabel('% share',fontsize=20)\nplt.annotate('total unique variation= '+str(len(uniq_val_var)),xy=(2000,0.025),fontsize=20,color='red' )\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"YckrF34ldk8q","outputId":"7739c75a-ea08-4638-ef17-638c9d364327","trusted":true},"cell_type":"code","source":"cs = np.cumsum(uniq_val_var.values / uniq_val_var.sum())\nplt.figure(figsize=(15,10))\nplt.plot(cs,color='blue',linewidth=3)\nplt.title('cummulative sum',fontsize=30)\nplt.grid(b=True)\nplt.xlabel('indices of variation',fontsize=20)\nplt.ylabel('% share',fontsize=20)\nplt.annotate('total unique variation= '+str(len(uniq_val_var)),xy=(180,0.075),fontsize=20,color='red' )\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"vnSPCjdbd7LM","outputId":"14dc2a74-2066-43bd-875c-9df6bf40e99c","trusted":true},"cell_type":"code","source":"print('*'*50,'One Hot Encoding','*'*50)\nalpha = [10 ** x for x in range(-5, 1)]\nperformance(var_one_hot_tr,var_one_hot_test,var_one_hot_cv,Y_tr,Y_test,Y_cv,alpha) \nprint('*'*50,'Response Encoding','*'*50)\nperformance(var_response_tr,var_response_test,var_response_cv,Y_tr,Y_test,Y_cv,alpha) \nprint('*'*50,'TFIDF Encoding','*'*50)\nperformance(var_tfidf_tr,var_tfidf_test,var_tfidf_cv,Y_tr,Y_test,Y_cv,alpha) ","execution_count":null,"outputs":[]},{"metadata":{"id":"-3-nsqrZe6nx"},"cell_type":"markdown","source":"# *Conclusion: Best Parameters <br>LOG LOSS:<br>1.Train: Log loss is train = 0.5366492240360342, <br>2.test = 0.5413111923895613 <br>3.cv =  0.5427505443070724 <br>4alpha = 0.01*<br>5. Vectorizer: Response Encoding<br> Variation feature is not that strongly relatedto target value than GENE was. And we can also see that train loss and cv_loss and test_loss are having huge variance in terms of tfidf and one hot encoding. This tells us that this feature is trying to overfit the data. <br> But nonetheless it is performing way better than the dummy model and GENE feature. So we should definitely keep this feature. "},{"metadata":{"id":"9nOIUP56e7da"},"cell_type":"markdown","source":"# *7: Feature: TEXT*"},{"metadata":{"id":"Xp0uXbUAfMsp","trusted":true},"cell_type":"code","source":"text_one_hot_tr = one_hot_encoding(X_tr['TEXT'],X_tr['TEXT'])\ntext_response_tr = res_encoding(X_tr['TEXT'],Y_tr)\ntext_tfidf_tr = tfidf_vectorizer(X_tr['TEXT'],X_tr['TEXT'])\n\ntext_one_hot_cv = one_hot_encoding(X_tr['TEXT'],X_cv['TEXT'])\ntext_response_cv = res_encoding(X_cv['TEXT'],Y_cv)\ntext_tfidf_cv = tfidf_vectorizer(X_tr['TEXT'],X_cv['TEXT'])\n\ntext_one_hot_test = one_hot_encoding(X_tr['TEXT'],X_test['TEXT'])\ntext_response_test = res_encoding(X_test['TEXT'],Y_test)\ntext_tfidf_test = tfidf_vectorizer(X_tr['TEXT'],X_test['TEXT'])","execution_count":null,"outputs":[]},{"metadata":{"id":"1L2_thGyic5v","outputId":"f56d8ea2-c4a2-47c8-dca5-3d45a42a21d9","trusted":true},"cell_type":"code","source":"uniq_val_text = result['TEXT'].value_counts()\nplt.figure(figsize=(15,10))\nplt.plot(uniq_val_text.values / uniq_val_text.sum(),color='blue',linewidth=3)\nplt.title('percentage share of TEXT',fontsize=30)\nplt.grid(b=True)\nplt.xlabel('indices of TEXT',fontsize=20)\nplt.ylabel('% share',fontsize=20)\nplt.annotate('total unique TEXT= '+str(len(uniq_val_text)),xy=(1200,0.015),fontsize=20,color='red' )\nplt.show()\n\ncs = np.cumsum(uniq_val_text.values / uniq_val_text.sum())\nplt.figure(figsize=(15,10))\nplt.plot(cs,color='blue',linewidth=3)\nplt.title('cummulative sum',fontsize=30)\nplt.grid(b=True)\nplt.xlabel('indices of TEXT',fontsize=20)\nplt.ylabel('% share',fontsize=20)\nplt.annotate('total unique TEXT= '+str(len(uniq_val_text  )),xy=(1200,0.015),fontsize=20,color='red' )\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"CCnO_39ekDGG","outputId":"230d5584-702a-468b-d775-1da782aea66f","trusted":true},"cell_type":"code","source":"print('*'*50,'One Hot Encoding','*'*50)\nalpha = [10 ** x for x in range(-5, 1)]\nperformance(text_one_hot_tr,text_one_hot_test,text_one_hot_cv,Y_tr,Y_test,Y_cv,alpha) \nprint('*'*50,'Response Encoding','*'*50)\nperformance(text_response_tr,text_response_test,text_response_cv,Y_tr,Y_test,Y_cv,alpha) \nprint('*'*50,'TFIDF Encoding','*'*50)\nperformance(text_tfidf_tr,text_tfidf_test,text_tfidf_cv,Y_tr,Y_test,Y_cv,alpha) ","execution_count":null,"outputs":[]},{"metadata":{"id":"OwfEYqX0SSRr"},"cell_type":"markdown","source":"# *# *Conclusion: Best Parameters <br><h5>LOG LOSS:<br>1.Train: Log loss is train = 0.5883048303066178, <br>2.test = 0.6232510119525043 <br>3.cv = 0.631641928096624  <br>4alpha = 1e-05<br>5. Vectorizer: Response Encoding*<br> <h5>TEXT feature is  strongest related to target value than any other was. And we can also see that train loss and cv_loss and test_loss are having very less variance in terms of response encoding. This tells us that this feature is stable across datasets and maintains a good balance between underfitting and overfitting. <br>  So we should definitely keep this feature. *"},{"metadata":{"id":"1522QTfKUFCD"},"cell_type":"markdown","source":"# *Decision Making: <h5>So from above Univariate analysis of the features we will be doing:<br> 1.GENE: One Hot Encoding<br>2.Variation: Response Encoding<br>3. TEXT: Response Encoding*"},{"metadata":{"id":"BSQn1O6gV67c"},"cell_type":"markdown","source":"# **D. Model Selection**\n<h5> Since by now we know that how vectorization on each features has to be done.<br> But we don't know which model to apply. So we will try various model on our vectorized data.\n\n\n*   Logistic Regression\n*   Linear SVM\n*   KNN\n*   RandomForest\n\nNOTE: As mentioned in the business requirements, we need interpretability of the model, hence we are notconsidering Ensembles as they don't have much of the interpretability. But Logistic and LinearSVM have high level of interpretability So we are testing these two models only. Just to compare we are taking RandomForest also.<br> And to make decision on choosing of models we will use confusion metrics, recall and precision metrics.\n\n\n\n"},{"metadata":{"id":"7sIG9RDDX988"},"cell_type":"markdown","source":"# *D.1: Data Preparation*"},{"metadata":{"id":"hO6UqHigYO_Y","outputId":"f69adae2-126e-4cfa-e3fd-f8113a330c6a","trusted":true},"cell_type":"code","source":"data_tr = np.hstack((gene_one_hot_tr,var_response_tr,text_response_tr ))\ndata_test = np.hstack((gene_one_hot_test,var_response_test,text_response_test ))\ndata_cv = np.hstack((gene_one_hot_cv,var_response_cv,text_response_cv ))\nprint('train shape:',data_tr.shape)\nprint('test shape:',data_test.shape)\nprint('cv shape:',data_cv.shape)","execution_count":null,"outputs":[]},{"metadata":{"id":"sWETr-kHZGHA"},"cell_type":"markdown","source":"# *D.2: Hyperparameter Tuning*"},{"metadata":{"id":"Zkyw_SRDcSHp"},"cell_type":"markdown","source":"# *Logistic Regression*"},{"metadata":{"id":"LZ1MN_48Yd9u","trusted":true},"cell_type":"code","source":"alpha = [10 ** x for x in range(-6, 3)] # this is the  lambda value we need to find out by cross validation \ndef performance_model(vector_tr,vector_cv,Y_tr,Y_cv,alpha):\n  error_cv=[]\n  error_tr=[]\n  for i in alpha:\n      clf = SGDClassifier(alpha=i, penalty='l2', loss='log', random_state=42)\n      clf.fit(vector_tr, Y_tr)\n      sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\") # we want our predicted value to be a probability for the interpretability hence we are using CalibratedClassifierCV\n      sig_clf.fit(vector_tr,Y_tr)\n      predict_y_tr = sig_clf.predict_proba(vector_tr)\n      predict_y = sig_clf.predict_proba(vector_cv)\n      error_cv.append(log_loss(Y_cv, predict_y, labels=clf.classes_, eps=1e-15))\n      error_tr.append(log_loss(Y_tr, predict_y_tr, labels=clf.classes_, eps=1e-15)) \n  plt.figure(figsize=(15,10))\n  plt.plot(error_cv,color='blue',linewidth=3)\n  plt.plot(error_tr,color='grey',linewidth=3)\n  plt.title('performance checker',fontsize=30)\n  plt.grid(b=True)\n  plt.legend(['CV','Train'])\n  plt.xlabel('Hyperparameter value',fontsize=20)\n  plt.ylabel('log-loss',fontsize=20)\n  #for i,j in zip(alpha,error_cv):\n      #plt.annotate(str(round(j,2)),xy=(i,j),fontsize=20,color='grey' )\n  plt.show()\n  for i in (range(len(alpha))):\n    print('Log loss is train =  {0} and cv = {1} for alpha value {2}'.format(error_tr[i],error_cv[i],alpha[i] )) \n ","execution_count":null,"outputs":[]},{"metadata":{"id":"20Ud4APGaSkc","outputId":"08dc55e0-7e27-4a83-8a24-713b3a3ad998","trusted":true},"cell_type":"code","source":"performance_model(data_tr,data_cv,Y_tr,Y_cv,alpha)","execution_count":null,"outputs":[]},{"metadata":{"id":"7sA-ced2haZO"},"cell_type":"markdown","source":"Best model = Log loss is train =  1.004839020557088 and cv = 1.2158399073823039 for alpha value 0.001"},{"metadata":{"id":"hg-cXIbHglUq","trusted":true},"cell_type":"code","source":"\n# Now that we know our best model on whole dataset, we will train our model with best alpha value\n\ndef log_loss_and_confusion_matrix(data_tr, Y_tr,data_test, Y_test, model):\n    model.fit(data_tr, Y_tr)\n    sig_clf = CalibratedClassifierCV(model, method=\"sigmoid\")\n    sig_clf.fit(data_tr, Y_tr)\n    pred_y = sig_clf.predict(data_test)\n\n    # for calculating log_loss we willl provide the array of probabilities belongs to each class\n    print(\"Log loss :\",log_loss(Y_test, sig_clf.predict_proba(data_test)))\n    # calculating the number of data points that are misclassified\n    print(\"Number of mis-classified points :\", np.count_nonzero((pred_y- Y_test))/Y_test.shape[0])\n    plot_confusion_matrix(Y_test, pred_y)\n    return pred_y ","execution_count":null,"outputs":[]},{"metadata":{"id":"W_1MS8eyiRCQ","outputId":"1ac37033-dd32-4712-bf6f-de12bc9ebe2b","trusted":true},"cell_type":"code","source":"from sklearn.metrics import log_loss\nmodel = SGDClassifier( class_weight='balanced',loss='log',penalty='l2',alpha=0.001)\nlog_loss_and_confusion_matrix(data_tr, Y_tr,data_test, Y_test, model)","execution_count":null,"outputs":[]},{"metadata":{"id":"FZIh8IUFcNK2"},"cell_type":"markdown","source":"# *LinearSVM*"},{"metadata":{"id":"fgBSVHxiasKv","trusted":true},"cell_type":"code","source":"def performance_model_hinge(vector_tr,vector_cv,Y_tr,Y_cv,alpha):\n  error_cv=[]\n  error_tr=[]\n  for i in alpha:\n      clf = SGDClassifier(alpha=i, penalty='l2', loss='hinge', random_state=42)\n      clf.fit(vector_tr, Y_tr)\n      sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\") # we want our predicted value to be a probability for the interpretability hence we are using CalibratedClassifierCV\n      sig_clf.fit(vector_tr,Y_tr)\n      predict_y_tr = sig_clf.predict_proba(vector_tr)\n      predict_y = sig_clf.predict_proba(vector_cv)\n      error_cv.append(log_loss(Y_cv, predict_y, labels=clf.classes_, eps=1e-15))\n      error_tr.append(log_loss(Y_tr, predict_y_tr, labels=clf.classes_, eps=1e-15)) \n  plt.figure(figsize=(15,10))\n  plt.plot(error_cv,color='blue',linewidth=3)\n  plt.plot(error_tr,color='grey',linewidth=3)\n  plt.title('performance checker',fontsize=30)\n  plt.grid(b=True)\n  plt.legend(['CV','Train'])\n  plt.xlabel('Hyperparameter value',fontsize=20)\n  plt.ylabel('log-loss',fontsize=20)\n  #for i,j in zip(alpha,error_cv):\n      #plt.annotate(str(round(j,2)),xy=(i,j),fontsize=20,color='grey' )\n  plt.show()\n  for i in (range(len(alpha))):\n    print('Log loss is train =  {0} and cv = {1} for alpha value {2}'.format(error_tr[i],error_cv[i],alpha[i] )) \n ","execution_count":null,"outputs":[]},{"metadata":{"id":"47qwsEn7caLW","outputId":"0c7b2a77-280b-4ccc-dd7d-a9fd898c7c3e","trusted":true},"cell_type":"code","source":"performance_model_hinge(data_tr,data_cv,Y_tr,Y_cv,alpha)","execution_count":null,"outputs":[]},{"metadata":{"id":"bwDE9pRKj-Rg"},"cell_type":"markdown","source":"Best Model()"},{"metadata":{"id":"2CcMQamAkBsb","outputId":"24c135dc-939c-4a24-b9d2-a80d7bd30b2f","trusted":true},"cell_type":"code","source":"model = SGDClassifier(loss='hinge',penalty='l2',alpha=100)\nlog_loss_and_confusion_matrix(data_tr, Y_tr,data_test, Y_test, model)","execution_count":null,"outputs":[]},{"metadata":{"id":"v-HwM14PdSFU"},"cell_type":"markdown","source":"# *D.3: KNN*"},{"metadata":{"id":"J7yMWcoRcgTC","trusted":true},"cell_type":"code","source":"k = [1,3,5,7,9,11,13,15,17]\ndef performance_knn(vector_tr,vector_cv,Y_tr,Y_cv,alpha):\n  from sklearn.neighbors import KNeighborsClassifier\n  error_cv=[]\n  error_tr=[]\n  for i in alpha:\n      clf = KNeighborsClassifier(n_neighbors=i)\n      clf.fit(vector_tr, Y_tr)\n      sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\") # we want our predicted value to be a probability for the interpretability hence we are using CalibratedClassifierCV\n      sig_clf.fit(vector_tr,Y_tr)\n      predict_y_tr = sig_clf.predict_proba(vector_tr)\n      predict_y = sig_clf.predict_proba(vector_cv)\n      error_cv.append(log_loss(Y_cv, predict_y, labels=clf.classes_, eps=1e-15))\n      error_tr.append(log_loss(Y_tr, predict_y_tr, labels=clf.classes_, eps=1e-15)) \n  plt.figure(figsize=(15,10))\n  plt.plot(error_cv,color='blue',linewidth=3)\n  plt.plot(error_tr,color='grey',linewidth=3)\n  plt.title('performance checker',fontsize=30)\n  plt.grid(b=True)\n  plt.legend(['CV','Train'])\n  plt.xlabel('Hyperparameter value',fontsize=20)\n  plt.ylabel('log-loss',fontsize=20)\n  #for i,j in zip(alpha,error_cv):\n      #plt.annotate(str(round(j,2)),xy=(i,j),fontsize=20,color='grey' )\n  plt.show()\n  for i in (range(len(alpha))):\n    print('Log loss is train =  {0} and cv = {1} for alpha value {2}'.format(error_tr[i],error_cv[i],alpha[i] )) ","execution_count":null,"outputs":[]},{"metadata":{"id":"G_w6ZtGbebTr","outputId":"df88b083-5143-406d-d08b-66ac624270c3","trusted":true},"cell_type":"code","source":"performance_knn(data_tr,data_cv,Y_tr,Y_cv,k)","execution_count":null,"outputs":[]},{"metadata":{"id":"uY5pHSZdegP8","outputId":"b53d518a-5f3f-4108-e086-ce1334ec2904","trusted":true},"cell_type":"code","source":"model = KNeighborsClassifier(n_neighbors=17)\nlog_loss_and_confusion_matrix(data_tr, Y_tr,data_test, Y_test, model)","execution_count":null,"outputs":[]},{"metadata":{"id":"tvZcTgk9mywl"},"cell_type":"markdown","source":"# *Conclusions: Surprisingly KNN is performing alot better than LinearSVM and Logistic Regression. The difference of missclassification is almost 20 %. So for now our model is KNN with n_neighbors=17, Why?, because less than 17 we will overfit the data.*"},{"metadata":{"id":"D28u9E2dnyf6"},"cell_type":"markdown","source":"# *D.4: RandomForest*"},{"metadata":{"id":"tnPkdZKgkzLJ","trusted":true},"cell_type":"code","source":"def performance_rf(vector_tr,vector_cv,Y_tr,Y_cv,alpha):\n  from sklearn.ensemble import RandomForestClassifier\n  for l in [5,10]:\n    error_cv=[]\n    error_tr=[]\n    for i in alpha:\n        clf = RandomForestClassifier(n_estimators=i,criterion='gini',max_depth=l)\n        clf.fit(vector_tr, Y_tr)\n        sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\") # we want our predicted value to be a probability for the interpretability hence we are using CalibratedClassifierCV\n        sig_clf.fit(vector_tr,Y_tr)\n        predict_y_tr = sig_clf.predict_proba(vector_tr)\n        predict_y = sig_clf.predict_proba(vector_cv)\n        error_cv.append(log_loss(Y_cv, predict_y, labels=clf.classes_, eps=1e-15))\n        error_tr.append(log_loss(Y_tr, predict_y_tr, labels=clf.classes_, eps=1e-15)) \n    plt.figure(figsize=(15,10))\n    plt.plot(error_cv,color='blue',linewidth=3)\n    plt.plot(error_tr,color='grey',linewidth=3)\n    plt.title('performance checker max_depth='+str(l),fontsize=30)\n    plt.grid(b=True)\n    plt.legend(['CV','Train'])\n    plt.xlabel('Hyperparameter value',fontsize=20)\n    plt.ylabel('log-loss',fontsize=20)\n    #for i,j in zip(alpha,error_cv):\n        #plt.annotate(str(round(j,2)),xy=(i,j),fontsize=20,color='grey' )\n    plt.show()\n    for i in (range(len(alpha))):\n      print('Log loss is train =  {0} and cv = {1} for alpha value {2}'.format(error_tr[i],error_cv[i],alpha[i] )) ","execution_count":null,"outputs":[]},{"metadata":{"id":"_8dNNgMfpnu5","outputId":"60e0969f-adc3-4540-c7a4-086519876711","trusted":true},"cell_type":"code","source":"alpha = [100,200,500,1000,2000]\nperformance_rf(data_tr,data_cv,Y_tr,Y_cv,alpha)","execution_count":null,"outputs":[]},{"metadata":{"id":"URvdF-XBqjrF"},"cell_type":"markdown","source":"# *New Champion: Log loss is train =  0.03967625782118917 and cv = 0.04036323580651017 for alpha value 200 and max_depth=5*"},{"metadata":{"id":"AhiZt28np3M_","outputId":"1e05227f-4a41-4bc2-d481-87277a2c7e18","trusted":true},"cell_type":"code","source":"model = RandomForestClassifier(n_estimators=200,criterion='gini',max_depth=5)\nprediction_test=log_loss_and_confusion_matrix(data_tr, Y_tr,data_test, Y_test, model)","execution_count":null,"outputs":[]},{"metadata":{"id":"VaRGr-IJrWZ2"},"cell_type":"markdown","source":"# *Conclusion: RandomForest is working like magical. But the problem with RandomForest is it is not interpretable. So we can't use this.*"},{"metadata":{"id":"uEDkgZlu1try","trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}