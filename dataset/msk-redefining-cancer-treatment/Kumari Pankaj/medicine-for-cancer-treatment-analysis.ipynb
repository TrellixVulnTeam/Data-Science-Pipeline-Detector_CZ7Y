{"cells":[{"metadata":{},"cell_type":"markdown","source":"```Use Case Details:```\n\nA lot has been said during the past several years about how precision medicine and, more concretely, how genetic testing is going to disrupt the way diseases like cancer are treated.\n\nBut this is only partially happening due to the huge amount of manual work still required. Memorial Sloan Kettering Cancer Center (MSKCC) launched this competition, accepted by the NIPS 2017 Competition Track,  because we need your help to take personalized medicine to its full potential.\n\n\n\nOnce sequenced, a cancer tumor can have thousands of genetic mutations. But the challenge is distinguishing the mutations that contribute to tumor growth (drivers) from the neutral mutations (passengers). \n\nCurrently this interpretation of genetic mutations is being done manually. This is a very time-consuming task where a clinical pathologist has to manually review and classify every single genetic mutation based on evidence from text-based clinical literature.\n\nFor this competition MSKCC is making available an expert-annotated knowledge base where world-class researchers and oncologists have manually annotated thousands of mutations.\n\nWe need your help to develop a Machine Learning algorithm that, using this knowledge base as a baseline, automatically classifies genetic variations.\n\n\nKaggle is excited to partner with research groups to push forward the frontier of machine learning. Research competitions make use of Kaggle's platform and experience, but are largely organized by the research group's data science team. Any questions or concerns regarding the competition data, quality, or topic will be addressed by them."},{"metadata":{},"cell_type":"markdown","source":"## Importing and Loading Datasets"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# loading required libraries for cancer treament analysis\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n# To ignore warinings\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's check in which directory our data is available so that it will be easy to pull from specific source location\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# loading datasets\ntrain_variants = pd.read_csv('../input/msk-redefining-cancer-treatment/training_variants')\ntest_variants = pd.read_csv('../input/msk-redefining-cancer-treatment/test_variants')\ntrain_text = pd.read_csv('../input/msk-redefining-cancer-treatment/training_text', sep=\"\\|\\|\", engine='python', header=None, skiprows=1, names=[\"ID\",\"Text\"])\ntest_text = pd.read_csv('../input/msk-redefining-cancer-treatment/test_text', sep=\"\\|\\|\", engine='python', header=None, skiprows=1, names=[\"ID\",\"Text\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Let's pull the top 5 rows value from train variants"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_variants.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Similarlly let's pull top 5 rows value fromtrain text data"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_text.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Merging Train variants and Train text into one trai dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_merge = pd.merge(train_variants,train_text,how='left',on='ID')\n# let's pull train merge dataset and do the analysis on this\ntrain_merge.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's understand the type of values present in each column of our dataframe 'train_merge' dataframe.\ntrain_merge.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Now let's draw a histogram/count plot to see how the classes are distributed"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Histogram : To check class distribution\nplt.figure(figsize=(12,8))\nsns.countplot(x='Class',data=train_variants)\nplt.ylabel('Frequency-Counts', fontsize=15)\nplt.xlabel('Class',fontsize=13)\nplt.xticks(rotation='vertical')\nplt.title('Class Counts',fontsize=15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Now Let's explore the text column and see the text distribution\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_merge[\"Text_num_words\"] = train_merge[\"Text\"].apply(lambda x: len(str(x).split()) )\ntrain_merge[\"Text_num_chars\"] = train_merge[\"Text\"].apply(lambda x: len(str(x)) )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**- Let's look at the distribution of number of words in the text column.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12, 8))\nsns.distplot(train_merge.Text_num_words.values, bins=50, kde=False, color='red')\nplt.xlabel('Number of words in text', fontsize=12)\nplt.ylabel('Count', fontsize=12)\nplt.title(\"Frequency of number of words\", fontsize=15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"``` The peak is around 4000 words.``` \n\n- Now let us look at character level."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12, 8))\nsns.distplot(train_merge.Text_num_chars.values, bins=50, kde=False, color='brown')\nplt.xlabel('Number of characters in text', fontsize=12)\nplt.ylabel('log of Count', fontsize=12)\nplt.title(\"Frequency of Number of characters\", fontsize=15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- check if we could use the number of words in the text has predictive power."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,8))\nsns.boxplot(x='Class', y='Text_num_words', data=train_merge)\nplt.xlabel('Class', fontsize=12)\nplt.ylabel('Text - Number of words', fontsize=12)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Let's check whether the data is balance or not"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_merge.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Looks like data is pretty balanced since we didn't see any random pick value. We are good to go for further analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"# putting respon variable to y\n#y = train_merge['Class'].values\n#train_merge = train_merge.drop('Class',axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_merge.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#y","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Now let's merge the test datasets(variants & text) together to one dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_merge = pd.merge(test_variants,test_text,how='left',on='ID')\ntest_merge.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pid = test_merge['ID'].values\npid","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Let's have a quick look whether we have balance data or not in our test datasets"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_merge.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Awesome! Our test dataset is looks fine. "},{"metadata":{},"cell_type":"markdown","source":"## Missing Value Analysis\n\n- Check for missing values in both training and testing data columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"# check total number of null/missing value present in whole datasets\ntrain_merge.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"    - Ohh Ok, We have only 5 missing values present in text feature\n    - Let's remove those since it's only 5 missing in number if we see the percentage of missing values it's like only 0.1% . Since it's very less number we can remove those."},{"metadata":{"trusted":true},"cell_type":"code","source":"# find out percentage of \"?\" value present across the dataset\npercent_missing = train_merge.isnull().sum() * 100 / len(train_merge)\npercent_missing","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# droping missing values\ntrain_merge.dropna(inplace=True)\n\n# let's check again whether we have any further missing values\ntrain_merge.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Awesome our data is cleaned! Good to go for model building"},{"metadata":{},"cell_type":"markdown","source":"### Check test data is clean or not"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_merge.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Okay, We have only 1 missing value in text data. let's remove it."},{"metadata":{"trusted":true},"cell_type":"code","source":"# dropping missing values\ntest_merge.dropna(inplace=True)\n\n# check if our data is clean or not\ntest_merge.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Awesome we are good to model builing. let's do this."},{"metadata":{},"cell_type":"markdown","source":"\n# Splitting Datasets into Training and Testing sets\n**- Splitting Datasets into Training & Testing sets by using scikit learn library**\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ntrain,test = train_test_split(train_merge,test_size=0.2)\nnp.random.seed(0)\ntrain","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = train['Text'].values\nX_test = test['Text'].values\ny_train = train['Class'].values\ny_test = test['Class'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn import svm","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Set pipeline to build a complete text processing model with Vectorizer, Transformer and LinearSVC"},{"metadata":{"trusted":true},"cell_type":"code","source":"text_classifier = Pipeline([('vect', CountVectorizer()),\n                     ('tfidf', TfidfTransformer()),\n                     ('clf', svm.LinearSVC())\n])\ntext_classifier = text_classifier.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test_predicted = text_classifier.predict(X_test)\nnp.mean(y_test_predicted == y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Predicting values for test data"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test_final = test_merge['Text'].values\n#X_test_final","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predicted_class = text_classifier.predict(X_test_final)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Appended the predicted class values to the testing data"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_merge['predicted_class'] = predicted_class","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_merge.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Onehot encoding to get the predicted class values as columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"onehot = pd.get_dummies(test_merge['predicted_class'])\ntest_merge = test_merge.join(onehot)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_merge.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Preparing submission data"},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_df = test_merge[[\"ID\",1,2,3,4,5,6,7,8,9]]\nsubmission_df.columns = ['ID', 'class1','class2','class3','class4','class5','class6','class7','class8','class9']\nsubmission_df.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_df.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Work in Progress....."},{"metadata":{},"cell_type":"markdown","source":"**More to come. Stay tuned.!**"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## **If you found this notebook helpful or you just liked it , some upvotes would be very much appreciated - That's will keep me motivated :)**"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}