{"nbformat":4,"nbformat_minor":1,"metadata":{"language_info":{"name":"python","codemirror_mode":{"version":3,"name":"ipython"},"file_extension":".py","version":"3.6.1","mimetype":"text/x-python","pygments_lexer":"ipython3","nbconvert_exporter":"python"},"kernelspec":{"name":"python3","language":"python","display_name":"Python 3"}},"cells":[{"metadata":{"_cell_guid":"79e16cd8-aa8a-466d-aef0-5cd415ccae71","_uuid":"5b6ee3740a35177df75730c3fd93c8d1058fd1d7"},"cell_type":"markdown","source":"# CNN for text classification\nI'm late to this party (as always), but I will share this with you in case if you find it useful.  \nThis is my first kernel, I'm new to ML and text mining, I've started python two months ago (expect some unusual code) and I will never close it back :)  \n  \nThis notebook is set to run on kaggle, but with large sacrifice on model performance and with overfit as training data is truncated. You should run this on your local env and reset all variables as suggested.  You should reach ~0.8 log loss with ease on this fast training model.\n  \nGood luck!"},{"metadata":{"_cell_guid":"4d56cfbf-0315-4253-a155-61b09935be18","_uuid":"11d23c04081a092dc6c1a45d8aa36a20cd3d16af"},"cell_type":"markdown","source":"### Common includes"},{"metadata":{"_cell_guid":"face5577-4513-4d2f-8caf-e9190e043ed8","collapsed":true,"_uuid":"f6df16a6121c5b646f75f7e5b917c4a656b39a7d"},"cell_type":"code","execution_count":null,"outputs":[],"source":"import numpy as np\nimport pandas as pd\nfrom IPython.display import display"},{"metadata":{"_cell_guid":"12221acf-7a28-4aec-9c0d-f4ba3f46fc10","_uuid":"8f3e7563b47a660198c86c212114a878cbda2b79"},"cell_type":"markdown","source":"### Prepare data\ntraining_variants+training_text+test_variants[stage1_solution]+test_text[stage1_solution]"},{"metadata":{"_kg_hide-output":false,"_cell_guid":"94a89b6d-1055-4685-9070-73a74bf47364","_kg_hide-input":false,"_uuid":"5c9cd3e5fe706df91333b97a05d667731ebad887"},"cell_type":"code","execution_count":null,"outputs":[],"source":"# load training variants\ntrain = pd.read_csv('../input/training_variants')\n# load training text\ntrain_txt_ = pd.read_csv('../input/training_text', sep=\"\\|\\|\", engine='python', header=None, skiprows=1, names=[\"ID\",\"Text\"])\n# merge text & variants\ntrain = pd.merge(train, train_txt_, how='left', on='ID').fillna('')\n# clean up\ndel train_txt_\n# print train data info\ndisplay(train.info())\n\n# load test variants from stage 1\ntestold_var_ = pd.read_csv('../input/test_variants')\n# load test text from stage 1\ntestold_txt_ = pd.read_csv('../input/test_text', sep='\\|\\|', engine='python', header=None, skiprows=1, names=[\"ID\",\"Text\"])\n# merge text & variants\ntestold_ = pd.merge(testold_var_, testold_txt_, how='left', on='ID').fillna('')\n# clean up\ndel testold_var_\ndel testold_txt_\n\n# load stage1 solutions\nstage1sol_ = pd.read_csv('../input/stage1_solution_filtered.csv')\n# get class\nstage1sol_['Class'] = pd.to_numeric(stage1sol_.drop('ID', axis=1).idxmax(axis=1).str[5:]).fillna(0).astype(np.int64)\n# drop records from testold_ if they are not in stage1sol_\ntestold_ = testold_[testold_.index.isin(stage1sol_['ID'])]\n# merge class to testold_ from stage1sol_\nnewtraindata_ = testold_.merge(stage1sol_[['ID', 'Class']], on='ID', how='left')\n# reindex columns\nnewtraindata_ = newtraindata_.reindex_axis(['ID','Gene','Variation','Class','Text'], axis=1)\n# clean up\ndel stage1sol_\ndel testold_\n\n# append new train data\ntrain = train.append(newtraindata_)\n# clean up\ndel newtraindata_\n\n# print train data info\ndisplay(train.info())"},{"metadata":{"_cell_guid":"cac68d4f-f92c-4015-bbf0-d417dc9ab091","_uuid":"548cae4dc15de777fabed4598cd5b6bb92444b83"},"cell_type":"markdown","source":"### Load word2vec\nWhen running on local env download word2vec binary file from [bio.nlplab.org](http://bio.nlplab.org/).  \n**Word2vec can drastically improve model performance.**"},{"metadata":{"_cell_guid":"ec70fd13-cb3d-41c9-9dea-0621080ae54b","_uuid":"0eb26447b002fe423511a8088f586daaa651098e"},"cell_type":"code","execution_count":null,"outputs":[],"source":"print('Indexing word vectors.')\nimport os\nfrom gensim.models import KeyedVectors\nword2vec = None\n# make sure you load this on your local env and uncomment the line\n#word2vec = KeyedVectors.load_word2vec_format('PubMed-and-PMC-w2v.bin', binary=True)\nif (word2vec == None):\n    print(\"word2vec not loaded!\")\nelse:\n    print(\"Found {} word vectors of word2vec\".format(len(word2vec.vocab)))"},{"metadata":{"_cell_guid":"969120dd-8a2d-44ee-b72f-ec7862d8298b","_uuid":"cab5ec97be51c28e7e6977772353b13e741e7ad3"},"cell_type":"markdown","source":"### Truncate dataset for kaggle limit (or faster model hyperparameter search)\nSplit is done by truncating data equally by classes and corrects original dataset class distrubution."},{"metadata":{"_cell_guid":"7ef2fb06-50e2-4e56-b078-43ea6fee2909","_uuid":"eff43d8e87be0ac4bd74460bc846d6352a37fba7"},"cell_type":"code","execution_count":null,"outputs":[],"source":"# due kaggle limit we will truncate train database, remove this block if running on local env\n# debug msg\nprint('Split dataset.')\n# set to max value of orig dataset\nmaxsize = len(train)\n# check class distrubution and find min sample size\nfor c in range(1,10):\n    _ = len(train[train['Class']==c])\n    if (_ < maxsize):\n        maxsize = _\n# debug msg\nprint('max size', maxsize)\n# create new dataframe\ntrain_ = pd.DataFrame(columns=train.columns)\nfor c in range(1,10):\n    # append samples from train of length maxsize\n    train_ = train_.append(train[train['Class']==c][:maxsize], ignore_index=True)\n# display truncated data\ndisplay(train_.head())\n# debug msg\nprint('Train dataset old size {} new size {}'.format(len(train),len(train_)))\n# overwrite train with truncated train data\ntrain = train_\n# debug msg\nprint('Split dataset done')"},{"metadata":{"_cell_guid":"7934df5f-f277-4870-8ca5-c1638d23013a","_uuid":"8b079953fc90db70559cb48d691b440d5e39ba1b"},"cell_type":"markdown","source":"### Expand dataset by seperation of sentences in batches\nI found this increases model performance, keeps the model small and helps with small dataset."},{"metadata":{"_cell_guid":"6e715cd0-7fc1-4208-87fb-30debc86bfe6","_uuid":"a923fa4f347f37427dfd25019dfff5c49f3e3161"},"cell_type":"code","execution_count":null,"outputs":[],"source":"import nltk\n\n# Create a function called \"chunks\" with two arguments, l and n:\ndef chunks(l, n):\n    # For item i in a range that is a length of l,\n    for i in range(0, len(l), n):\n        # Create an index range for l of n items:\n        yield l[i:i+n]\n\nprint('Expand records to sentences.')\n# increase maxnumberofsentecs on local env to 400\nmaxnumberofsentences = 200\n# increase splitbysenteces on local env to 10\nsplitbysentences = 2\n# temp dict for new train set\ntmpdf_ = {'Text': [], 'Class': [], 'ID': [], 'Gene': [], 'Variation': []}\nfor index, row in train.iterrows():\n    # get sentences nltk\n    sent_tokenize_list = nltk.sent_tokenize(row['Text'])\n    # truncate sentences to last maxnumberofsentences (most important informations are at the end of text)\n    if (len(sent_tokenize_list) > maxnumberofsentences):\n        sent_tokenize_list = sent_tokenize_list[len(sent_tokenize_list)-maxnumberofsentences:]\n    # split sentences to batch\n    sent_chunk = list(chunks(sent_tokenize_list, splitbysentences))\n    for chunk in sent_chunk:\n        # join sentences in text\n        tmpdf_['Text'].append(\" \".join(chunk))\n        # assign class\n        tmpdf_['Class'].append(row['Class'])\n        # assign ID\n        tmpdf_['ID'].append(row['ID'])\n        # assign Gene\n        tmpdf_['Gene'].append(row['Gene'])\n        # assign Variation\n        tmpdf_['Variation'].append(row['Variation'])\n# create new train set from temp dict\norigtrainlen = len(train)\ntrain = pd.DataFrame(tmpdf_)\n# clean up\ndel tmpdf_\n# display head\ndisplay(train.head())\n# display \nprint('expanded from {} to {}'.format(origtrainlen,len(train)))"},{"metadata":{"_cell_guid":"0407e2fd-6f60-4a81-8ae4-515adc1ba446","_uuid":"7676af3fa2f381434303cc658541f393be2a35e8"},"cell_type":"markdown","source":"### Tokenizer and embedding\nWhen no word2vec is loaded model will try to learn weights by itself."},{"metadata":{"_cell_guid":"ed1343bd-8d82-4d82-ad27-a864f044dae2","_uuid":"5092e93cd3033d134ff4aa4f2dd3b9797ab9873a"},"cell_type":"code","execution_count":null,"outputs":[],"source":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\n# max top words, increase on local env to 100000\nnum_words = 5000\n# max sequence length, increase on local env to 500\nsequencelength = 200\n# init tokenizer\ntokenizer = Tokenizer(num_words=num_words)\n# fit tokenizer\ntokenizer.fit_on_texts(train['Text'])\n# get sequences\nX = tokenizer.texts_to_sequences(train['Text'])\n# unique words in text\nword_index = tokenizer.word_index\nprint(\"Found {} unique tokens.\".format(len(word_index)))\n# pad sequences\nX = pad_sequences(X, maxlen=sequencelength)\n\nembedding_matrix = None\nif (word2vec != None):\n    # out of vocabulary words > use this to do text analysis\n    oov_words = []\n    # prepare embedding matrix\n    embedding_matrix = np.zeros((num_words+1, 200)) #200 = word2vec dim\n    for word, i in word_index.items():\n        if i >= num_words:\n            continue\n        if word in word2vec.vocab:\n            # embedd from word2vec\n            embedding_matrix[i] = word2vec.word_vec(word)\n        else:\n            # add to out of vocabulary\n            oov_words.append(word)\n    print('Preparing embedding matrix done. out-of-vocabulary rate (OOV): {} ({})'.format(len(oov_words)/float(len(word_index)),len(oov_words)))\n    "},{"metadata":{"_cell_guid":"9823cf62-450e-4676-a91a-ea01a95f74dd","_uuid":"89984b763e3ebe61ec5fad74cfe8349110a0b803"},"cell_type":"markdown","source":"### CNN model\nData class distribution is uneven, so we will use class weights for correction.  \nOn local env you should train longer and with smaller lr. Model should reach ~0.8 log loss."},{"metadata":{"_cell_guid":"e0f84946-bdb9-415b-b336-66b4e39720ab","_uuid":"e2c59c0ede0cb81b9de53ebbc10eceab8ec73b02"},"cell_type":"code","execution_count":null,"outputs":[],"source":"import keras\nfrom sklearn.utils import class_weight\n\nembed_dim = 200 #same as word2vec dim\n\nmodel_filename = 'model'\n\n# prepare Y values\nY = train['Class'].values-1\n# get weights for unevenly distributed dataset \nclass_weight = class_weight.compute_class_weight('balanced', np.unique(Y), Y)\n# one hot\nY = keras.utils.to_categorical(Y)\n# batch size increase on local env\nbatch_size = 20\n# epochs increase on local env\nepochs = 3\n# Model saving callback\nckpt_callback = keras.callbacks.ModelCheckpoint(model_filename, monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\n\n# input layer\ninput1 = keras.layers.Input(shape=(sequencelength,))\n# embedding layer\nif (embedding_matrix == None):\n    # word2vec was not loaded. use fallback method\n    embedding = keras.layers.Embedding(num_words+1, embed_dim, trainable=True)(input1)\nelse:\n    # word2vec was loaded, load weights and set to untrainable\n    embedding = keras.layers.Embedding(num_words+1, embed_dim, weights=[embedding_matrix], trainable=False)(input1)\n \n# conv layers\nconvs = []\nfilter_sizes = [2,3,4]\nfor fsz in filter_sizes:\n    l_conv = keras.layers.Conv1D(filters=100,kernel_size=fsz,activation='relu')(embedding)\n    l_pool = keras.layers.MaxPooling1D(sequencelength-100+1)(l_conv)\n    l_pool = keras.layers.Flatten()(l_pool)\n    convs.append(l_pool)\n# merge conv layers\nl_merge = keras.layers.concatenate(convs, axis=1)\n# drop out regulation\nl_out = keras.layers.Dropout(0.5)(l_merge)\n# output layer\noutput = keras.layers.Dense(units=9, activation='softmax')(l_out)\n# model\nmodel = keras.models.Model(input1, output)\n# compile model\nmodel.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics = ['categorical_crossentropy'])\n# train model\nmodel.fit(X, Y, epochs=epochs, batch_size=batch_size, validation_split=0.2, verbose=1, class_weight=class_weight, callbacks=[ckpt_callback])"},{"metadata":{"_cell_guid":"dbe62309-dec4-4514-8d48-7ef6d0d523f2","_uuid":"919e027a05a87a2c9e294c495213ab8e3800817e"},"cell_type":"markdown","source":"### Prepare test data"},{"metadata":{"_cell_guid":"a8ffdd57-4eaa-413b-8787-1f2b6dba3e51","_uuid":"cf1d41547a8a944a8aea5de29de1fa5050e9b335"},"cell_type":"code","execution_count":null,"outputs":[],"source":"# load test dataset\ntest = pd.read_csv('../input/stage2_test_variants.csv')\n# load test text dataset\ntest_txt_ = pd.read_csv('../input/stage2_test_text.csv', sep='\\|\\|', engine='python', header=None, skiprows=1, names=[\"ID\",\"Text\"])\n# merge text & variants\ntest = pd.merge(test, test_txt_, how='left', on='ID')\n# clean up\ndel test_txt_"},{"metadata":{"_cell_guid":"b487c26f-d4ad-42bc-908d-4881c5ea53aa","_uuid":"3bf07b49849229fc76fc4a432f614b49d171b804"},"cell_type":"markdown","source":"### Expand dataset by seperation of sentences in batches\nSame as we did with training data."},{"metadata":{"_cell_guid":"3b350995-f156-46a4-882b-c0a4fe6ad0cf","_uuid":"f78f8bf0aaac90b458fd20da657a07d5ed02ea08"},"cell_type":"code","execution_count":null,"outputs":[],"source":"print('Expand records to sentences.')\n# temp dict for new train set\ntmpdf_ = {'Text': [], 'ID': [], 'Gene': [], 'Variation': []}\nfor index, row in test.iterrows():\n    # get sentences nltk\n    sent_tokenize_list = nltk.sent_tokenize(row['Text'])\n    # truncate sentences to last maxnumberofsentences (most important informations are at the end of text)\n    if (len(sent_tokenize_list) > maxnumberofsentences):\n        sent_tokenize_list = sent_tokenize_list[len(sent_tokenize_list)-maxnumberofsentences:]\n    # split sentences to batch\n    sent_chunk = list(chunks(sent_tokenize_list, splitbysentences))\n    for chunk in sent_chunk:\n        # join sentences in text\n        tmpdf_['Text'].append(\" \".join(chunk))\n        # assign ID\n        tmpdf_['ID'].append(row['ID'])\n        # assign Gene\n        tmpdf_['Gene'].append(row['Gene'])\n        # assign Variation\n        tmpdf_['Variation'].append(row['Variation'])\n# create new train set from temp dict\norigtestlen = len(test)\ntest = pd.DataFrame(tmpdf_)\n# clean up\ndel tmpdf_\n# display head\ndisplay(test.head())\n# display \nprint('expanded from {} to {}'.format(origtestlen,len(test)))"},{"metadata":{"_cell_guid":"bdd99716-e8ff-4439-b19d-99516e4cb5d8","_uuid":"bcebcdf6c2b76c118d860953f5e6fff688fc7eb7"},"cell_type":"markdown","source":"### Predict on test set and save to submission file"},{"metadata":{"_cell_guid":"4e7845f3-0a02-4d88-8057-e3d944e76e4b","_uuid":"549759160265e33ea699b0f25f96c17e306496a6"},"cell_type":"code","execution_count":null,"outputs":[],"source":"# load best model\nmodel = keras.models.load_model(model_filename)\n# get sequences\nXtest = tokenizer.texts_to_sequences(test['Text'])\n# pad sequences\nXtest = pad_sequences(Xtest, maxlen=sequencelength)\n# predict\nprobas = model.predict(Xtest, verbose=1)\n# prepare data for submission\nsubmission_df = pd.DataFrame(probas, columns=['class'+str(c+1) for c in range(9)])\n# insert IDs\nsubmission_df.insert(loc=0, column='ID', value=test['ID'].values)\n# average grouped data\nsubmission_df = submission_df.groupby(['ID'], as_index=False).mean()\n# save to csv\nsubmission_df.to_csv('submission.csv', index=False)\n# debug\nprint(\"\\n----------------------\\n\")\nprint(\"Done\")"}]}