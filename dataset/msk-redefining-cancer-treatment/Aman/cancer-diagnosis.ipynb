{"cells":[{"metadata":{},"cell_type":"markdown","source":"Here basically, we have to find probablities of each class(1-9) for each id(0-3320) of gene and class having \nmaximum probablity for a particular gene will be declared as the class of that gene.\n\nFor test\n\ninput---->id,gene,variation,text documentation\\\noutput--->class"},{"metadata":{"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"variant = pd.read_csv(r\"/kaggle/input/msk-redefining-cancer-treatment/training_variants.zip\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# as this is non comma seperated values so some changes in it\n\ntext_data = pd.read_csv(r\"/kaggle/input/msk-redefining-cancer-treatment/training_text.zip\" ,sep=\"\\|\\|\",engine=\"python\",names=[\"ID\",\"TEXT\"],skiprows=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text_data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Pre Processing text data"},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.corpus import stopwords\nstop_words = set(stopwords.words('english'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(stop_words)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\n\ndef cleaning(text, index, column):\n    if type(text) is not int:\n        \n        string = \"\"\n        # replace every special char with space\n        text = re.sub('[^a-zA-Z0-9\\n]', ' ', text)\n        \n        # replace multiple spaces with single space\n        text = re.sub('\\s+',' ', text)\n        \n        # converting all the chars into lower-case.\n        text = text.lower()\n        \n        for word in text.split():\n            if word not in stop_words:\n                string += word + \" \"\n        \n        text_data[column][index] = string\n                \n                \n                ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for index, row in text_data.iterrows():\n    if type(row['TEXT'] ) is str:\n        cleaning(row['TEXT'], index , 'TEXT')\n    else:\n        print(\"there is no text description for id:\",index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#merging both gene_variations and text data based on ID\n\nresult = pd.merge(variant, text_data,on='ID', how='left')\nresult.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result[result.isnull().any(axis = 1)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result.loc[result[\"TEXT\"].isnull(),'TEXT'] = result[\"Gene\"] + \"\" + result[\"Variation\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result[result[\"ID\"] == 1109]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Splitting data"},{"metadata":{"trusted":true},"cell_type":"code","source":"result","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_true = result['Class'].values\nresult.Gene      = result.Gene.str.replace('\\s+', '_')\nresult.Variation = result.Variation.str.replace('\\s+', '_')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = result.drop(\"Class\" ,axis = 1)\ny = result.iloc[: ,3:4]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nx_tr, x_test, y_tr, y_test = train_test_split(x, y, test_size=0.2 ,random_state = 0)\nx_train, x_cv, y_train, y_cv = train_test_split(x_tr, y_tr,test_size=0.2 ,random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(x_tr.shape)\nprint(x_test.shape)\nprint(y_tr.shape)\nprint(y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(x_train.shape)\nprint(y_train.shape)\nprint(x_test.shape)\nprint(y_test.shape)\nprint(x_cv.shape)\nprint(y_cv.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"total training data = \" , x_train.shape[0])\nprint(\"total cross validating data = \" , x_cv.shape[0])\nprint(\"total testing data = \" , x_test.shape[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Some visualization"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_class_distribution = y_train['Class'].value_counts()\ntrain_class_distribution.plot(kind = 'bar')\nplt.grid()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_class_distribution = y_test['Class'].value_counts()\ntest_class_distribution.plot(kind = 'bar')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"By above both plots we can understand that data is highly unbalanced and also both train\\\nand test are in sync(according to classes)"},{"metadata":{},"cell_type":"markdown","source":"### Prediction using a 'Random' Model</h2>"},{"metadata":{"trusted":true},"cell_type":"code","source":"'''in this what we are doing as it is told that we have to use log loss for comparing and the thing is while \nusing auc we know that for random numbers auc is 0.5 so we have to make our model with accuracy more than 0.5\nsame thing here as we are using log loss so first we will determine log loss for random numbers with this datasets\nand then we will try to built our model with log loss less than that random log loss'''\n\n'''we will take probablities of each class for all data points(for which we will divide class by total classes) \nIn a 'Random' Model, we generate the NINE class probabilites randomly such that they sum to 1.'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for cross validation data\n\nfrom sklearn.metrics.classification import accuracy_score, log_loss\n\ncross_validation_size = y_cv.shape[0]\n\nrandom_cross_validation = np.zeros((cross_validation_size,9)) # initializing with 0\n\nfor i in range(cross_validation_size):\n    random_prob = np.random.rand(1,9)\n    random_cross_validation[i] = ((random_prob/sum(sum(random_prob)))[0])\nprint(\"log loss for random cross validation is\", log_loss(y_cv,random_cross_validation))    \n  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for test data\n\nfrom sklearn.metrics.classification import accuracy_score, log_loss\n\ntest_size = y_test.shape[0]\n\nrandom_test_data = np.zeros((test_size,9)) # initializing with 0\n\nfor i in range(test_size):\n    random_prob = np.random.rand(1,9)\n    random_test_data[i] = ((random_prob/sum(sum(random_prob)))[0])\nprint(\"log loss for random test data is\", log_loss(y_test,random_test_data))   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''by  both obervation we can say that thershold is approximately 2.5 which means we have to build our model \nhaving log loss less than 2.5'''","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Univariate analysis"},{"metadata":{},"cell_type":"markdown","source":"### Gene analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"now we will do analysis of each feature one by one so lets start with gene\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"unique_gene = result[\"Gene\"].value_counts()\nunique_gene.head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"s = sum(unique_gene.values)\nh = unique_gene/s\nh","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(figsize=(10,6))\nplt.plot(h, label=\"Histrogram of Genes\")\nplt.xlabel('Index of a Gene')\nplt.ylabel('Number of Occurances')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"c = np.cumsum(h)\nplt.plot(c,label='Cumulative distribution of Genes')\nplt.grid()\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"by both plots we can understand that data is unequally distributed among genes"},{"metadata":{},"cell_type":"raw","source":"Now as we saw that log loss for our random model is 2.5(worst scenerio),lets make a model with only gene as a feature\nso that we can understand how much this feature is important and related to class(dependent feature)"},{"metadata":{},"cell_type":"raw","source":"As gene is a categorical feature so first we will convert it to numerical feature by one hot encoding"},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train['Gene'].shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_test['Gene'].shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_cv['Gene'].shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# converting categorical data into numerisal data , we will use two menthods\n# 1.One hot encoding\n# 2.Response coding\n\n# 1.One hot encoding\n\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ngene_vectorizer = CountVectorizer()\n\ntrain_gene_feature_onehotCoding = gene_vectorizer.fit_transform(x_train['Gene'])\ntest_gene_feature_onehotCoding = gene_vectorizer.transform(x_test['Gene'])\ncv_gene_feature_onehotCoding = gene_vectorizer.transform(x_cv['Gene'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_gene_feature_onehotCoding.shape)\nprint(test_gene_feature_onehotCoding.shape)\nprint(cv_gene_feature_onehotCoding.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_gene_feature_onehotCoding.toarray())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"now we will apply algorithm to it to find log loss"},{"metadata":{"trusted":true},"cell_type":"code","source":"# apply SGD algorithm\n\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.calibration import CalibratedClassifierCV\n\nalpha = [10**i for i in range(-5,1)]   # [1e-05, 0.0001, 0.001, 0.01, 0.1, 1]\n\ncv_log_error_array=[]\n\nfor i in alpha:\n    clf = SGDClassifier(loss='log' , alpha = i ,random_state = 42)\n    clf.fit(train_gene_feature_onehotCoding , y_train)\n    calibration = CalibratedClassifierCV(clf, method='sigmoid')\n    calibration.fit(train_gene_feature_onehotCoding , y_train)\n    y_cv_predict = calibration.predict_proba(cv_gene_feature_onehotCoding)\n    cv_log_error_array.append(log_loss(y_cv, y_cv_predict))\n    print('For values of alpha = ', i, \"The log loss is:\",log_loss(y_cv, y_cv_predict)) \n      ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"As we can see the minimum log loss is at alpha 0.0001 so we will take that value further"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(cv_log_error_array)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nmin_log_loss_position = np.argmin(cv_log_error_array)  # provide position of minimum value \n\n\nclf = SGDClassifier(loss='log' , alpha = alpha[min_log_loss_position],random_state = 42)\n\nclf.fit(train_gene_feature_onehotCoding , y_train)\ncalibration = CalibratedClassifierCV(clf, method='sigmoid')\n\ncalibration.fit(train_gene_feature_onehotCoding , y_train)\n\ny_train_predict = calibration.predict_proba(train_gene_feature_onehotCoding)\ny_cv_predict = calibration.predict_proba(cv_gene_feature_onehotCoding)\ny_test_predict = calibration.predict_proba(test_gene_feature_onehotCoding)\n\nprint('For values of alpha as',alpha[min_log_loss_position], \"The log loss for train data is:\",log_loss(y_train, y_train_predict)) \nprint('For values of alpha as',alpha[min_log_loss_position], \"The log loss for cv data is:\",log_loss(y_cv, y_cv_predict)) \nprint('For values of alpha as',alpha[min_log_loss_position], \"The log loss for test data is:\",log_loss(y_test, y_test_predict)) \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''now by looking at all the three log losses we can say that we have to keep gene feature as log loss is less by keeping\nonly this feature as compared to random log loss'''","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Variation analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"unique_variant = result[\"Variation\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"unique_variant","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"h = unique_variant/sum(unique_variant)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(figsize=(10,6))\nplt.plot(h, label=\"Histrogram of Variation\")\nplt.xlabel('Index of a Variation')\nplt.ylabel('Number of Occurances')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# cdf\n\ncdf = np.cumsum(h)\nf, ax = plt.subplots(figsize=(10,6))\nplt.plot(cdf, label=\"cdf of Variation\")\nplt.xlabel('Index of a Variation')\nplt.ylabel('Number of Occurances')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"by both plots we can understand that data is unequally distributed among variation."},{"metadata":{},"cell_type":"raw","source":"Now as we saw that log loss for our random model is 2.5(worst scenerio),lets make a model with only variation as a feature\nso that we can understand how much this feature is important and related to class(dependent feature)."},{"metadata":{},"cell_type":"raw","source":"As variation is a categorical feature so first we will convert it to numerical feature by one hot encoding"},{"metadata":{"trusted":true},"cell_type":"code","source":"# 1.One hot coding\n\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nvariation_vectorizer = CountVectorizer()\n\ntrain_variation_feature_onehotCoding = variation_vectorizer.fit_transform(x_train['Variation'])\ntest_variation_feature_onehotCoding = variation_vectorizer.transform(x_test['Variation'])\ncv_variation_feature_onehotCoding = variation_vectorizer.transform(x_cv['Variation'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_variation_feature_onehotCoding)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"now we will apply algorithm to it to find log loss"},{"metadata":{"trusted":true},"cell_type":"code","source":"# apply SGD algorithm\n\nfrom sklearn.calibration import CalibratedClassifierCV\n\nalpha = [10**i for i in range(-5,1)]   # [1e-05, 0.0001, 0.001, 0.01, 0.1, 1]\n\ncv_log_error_array=[]\n\nfor i in alpha:\n    clf = SGDClassifier(loss='log' , alpha = i, random_state=42)\n    clf.fit(train_variation_feature_onehotCoding , y_train)\n    calibration = CalibratedClassifierCV(clf, method='sigmoid')\n    calibration.fit(train_variation_feature_onehotCoding , y_train)\n    y_cv_predict = calibration.predict_proba(cv_variation_feature_onehotCoding)\n    cv_log_error_array.append(log_loss(y_cv, y_cv_predict))\n    print('For values of alpha = ', i, \"The log loss is:\",log_loss(y_cv, y_cv_predict)) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"As we can see the minimum log loss is at alpha 0.0001 so we will take that value further"},{"metadata":{"trusted":true},"cell_type":"code","source":"min_log_loss_position = np.argmin(cv_log_error_array)  # provide position of minimum value \n\n\nclf = SGDClassifier(loss='log' , alpha = alpha[min_log_loss_position], random_state=42)\n\nclf.fit(train_variation_feature_onehotCoding , y_train)\ncalibration = CalibratedClassifierCV(clf, method='sigmoid')\ncalibration.fit(train_variation_feature_onehotCoding , y_train)\n\ny_train_predict = calibration.predict_proba(train_variation_feature_onehotCoding)\ny_cv_predict = calibration.predict_proba(cv_variation_feature_onehotCoding)\ny_test_predict = calibration.predict_proba(test_variation_feature_onehotCoding)\n\nprint('For values of alpha as',alpha[min_log_loss_position], \"The log loss for train data is:\",log_loss(y_train, y_train_predict)) \nprint('For values of alpha as',alpha[min_log_loss_position], \"The log loss for cv data is:\",log_loss(y_cv, y_cv_predict)) \nprint('For values of alpha as',alpha[min_log_loss_position], \"The log loss for test data is:\",log_loss(y_test, y_test_predict)) \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''now by looking at all the three log losseses we can say that we have to keep variation feature as log loss \nis less by keeping only this feature as compared to random log loss'''","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Text analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"# 1.One hot coding\n\n\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# building a CountVectorizer with all the words that occured minimum 3 times in train data\ntext_vectorizer = CountVectorizer(min_df=3)\n\ntrain_text_feature_onehotCoding = text_vectorizer.fit_transform(x_train['TEXT'])\n\n# getting all the feature names (words)\ntrain_text_features= text_vectorizer.get_feature_names()\n\n# train_text_feature_onehotCoding.sum(axis=0).A1 will sum every row and returns (1*number of features) vector\ntrain_text_fea_counts = train_text_feature_onehotCoding.sum(axis=0).A1\n\n# zip(list(text_features),text_fea_counts) will zip a word with its number of times it occured\ntext_fea_dict = dict(zip(list(train_text_features),train_text_fea_counts))\n\n\nprint(\"Total number of unique words in train data :\", len(train_text_features))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"train_text_fea_counts"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import normalize\n\n# don't forget to normalize every feature\ntrain_text_feature_onehotCoding = normalize(train_text_feature_onehotCoding, axis=0)\n\n# we use the same vectorizer that was trained on train data\ntest_text_feature_onehotCoding = text_vectorizer.transform(x_test['TEXT'])\n\n# don't forget to normalize every feature\ntest_text_feature_onehotCoding = normalize(test_text_feature_onehotCoding, axis=0)\n\n# we use the same vectorizer that was trained on train data\ncv_text_feature_onehotCoding = text_vectorizer.transform(x_cv['TEXT'])\n\n# don't forget to normalize every feature\ncv_text_feature_onehotCoding = normalize(cv_text_feature_onehotCoding, axis=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"now we will apply algorithm to it to find log loss"},{"metadata":{"trusted":true},"cell_type":"code","source":"# apply SGD algorithm\n\nfrom sklearn.calibration import CalibratedClassifierCV\n\nalpha = [10**i for i in range(-5,1)]   # [1e-05, 0.0001, 0.001, 0.01, 0.1, 1]\n\ncv_log_error_array=[]\n\nfor i in alpha:\n    clf = SGDClassifier(loss='log' , alpha = i, random_state=42)\n    clf.fit(train_text_feature_onehotCoding , y_train)\n    calibration = CalibratedClassifierCV(clf, method='sigmoid')\n    calibration.fit(train_text_feature_onehotCoding , y_train)\n    y_cv_predict = calibration.predict_proba(cv_text_feature_onehotCoding)\n    cv_log_error_array.append(log_loss(y_cv, y_cv_predict))\n    print('For values of alpha = ', i, \"The log loss is:\",log_loss(y_cv, y_cv_predict))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"As we can see the minimum log loss is at alpha 0.001 so we will take that value further"},{"metadata":{"trusted":true},"cell_type":"code","source":"min_log_loss_position = np.argmin(cv_log_error_array)  # provide position of minimum value \n\n\nclf = SGDClassifier(loss='log' , alpha = alpha[min_log_loss_position], random_state=42)\n\nclf.fit(train_text_feature_onehotCoding , y_train)\ncalibration = CalibratedClassifierCV(clf, method='sigmoid')\ncalibration.fit(train_text_feature_onehotCoding , y_train)\n\ny_train_predict = calibration.predict_proba(train_text_feature_onehotCoding)\ny_cv_predict = calibration.predict_proba(cv_text_feature_onehotCoding)\ny_test_predict = calibration.predict_proba(test_text_feature_onehotCoding)\n\nprint('For values of alpha as',alpha[min_log_loss_position], \"The log loss for train data is:\",log_loss(y_train, y_train_predict)) \nprint('For values of alpha as',alpha[min_log_loss_position], \"The log loss for cv data is:\",log_loss(y_cv, y_cv_predict)) \nprint('For values of alpha as',alpha[min_log_loss_position], \"The log loss for test data is:\",log_loss(y_test, y_test_predict)) \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''now by looking at all the three log losseses we can say that we have to keep variation feature as log loss \nis less by keeping only this feature as compared to random log loss'''","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Applying Machine Learning Models"},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets make a function for plotting confusion matrix(C), precision(B) and recall(A) matrices\n\nfrom sklearn.metrics import confusion_matrix\n\ndef plot_mpr_matrices_heatmaps(y_actual,y_predicted):\n    \n    C = confusion_matrix(y_actual, y_predicted)\n    # C = 9,9 matrix, each cell (i,j) represents number of points of class i are predicted class j\n    \n    A =(((C.T)/(C.sum(axis=1))).T)\n    #divid each element of the confusion matrix with the sum of elements in that column\n    \n    # C = [[1, 2],\n    #     [3, 4]]\n    # C.T = [[1, 3],\n    #        [2, 4]]\n    # C.sum(axis = 1)  axis=0 corresonds to columns and axis=1 corresponds to rows in two diamensional array\n    # C.sum(axix =1) = [[3, 7]]\n    # ((C.T)/(C.sum(axis=1))) = [[1/3, 3/7]\n    #                           [2/3, 4/7]]\n\n    # ((C.T)/(C.sum(axis=1))).T = [[1/3, 2/3]\n    #                           [3/7, 4/7]]\n    # sum of row elements = 1\n    \n    B =(C/C.sum(axis=0))\n    #divid each element of the confusion matrix with the sum of elements in that row\n    # C = [[1, 2],\n    #     [3, 4]]\n    # C.sum(axis = 0)  axis=0 corresonds to columns and axis=1 corresponds to rows in two diamensional array\n    # C.sum(axix =0) = [[4, 6]]\n    # (C/C.sum(axis=0)) = [[1/4, 2/6],\n    #                      [3/4, 4/6]] \n    \n    labels = [1,2,3,4,5,6,7,8,9]\n    \n    print(\"-\"*20, \"Confusion matrix\", \"-\"*20)\n    \n    plt.figure(figsize=(20,7))\n    sns.heatmap(C, annot=True, cmap=\"YlGnBu\", fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n    plt.xlabel('Predicted Class')\n    plt.ylabel('Original Class')\n    plt.show()\n\n    print(\"-\"*20, \"Precision matrix (Columm Sum=1)\", \"-\"*20)\n    \n    plt.figure(figsize=(20,7))\n    sns.heatmap(B, annot=True, cmap=\"YlGnBu\", fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n    plt.xlabel('Predicted Class')\n    plt.ylabel('Original Class')\n    plt.show()\n    \n    print(\"-\"*20, \"Recall matrix (Row sum=1)\", \"-\"*20)\n    \n    plt.figure(figsize=(20,7))\n    sns.heatmap(A, annot=True, cmap=\"YlGnBu\", fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n    plt.xlabel('Predicted Class')\n    plt.ylabel('Original Class')\n    plt.show()\n    \n    \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets make a function for determining words cause of which our model is giving corresponding class value,\n# we will use this fuction at the end of each algorithms.\n\n#example:-Suppose our model give class label 6 for some test query, than what words our model found in that test query\n#    which make it giving class label 6 for that \n\ndef get_impfeature_names(indices, text, gene, var, no_features):\n    gene_count_vec = CountVectorizer()\n    var_count_vec = CountVectorizer()\n    text_count_vec = CountVectorizer(min_df=3)\n    \n    gene_fea_count_vec = gene_count_vec.fit(x_train[\"Gene\"])\n    var_fea_count_vec = var_count_vec.fit(x_train[\"Variation\"])\n    text_fea_count_vec = text_count_vec.fit(x_train[\"TEXT\"])\n    \n    len_unique_gene = len(gene_fea_count_vec.get_feature_names())  # 230\n    len_unique_var = len(var_fea_count_vec.get_feature_names())   #1967\n    \n    word_count = 0\n    for i , index in enumerate(indices):\n        if index < len_unique_gene:\n            word = gene_fea_count_vec.get_feature_names()[index]\n            yes_no = True if word == gene else False\n            if yes_no:\n                word_count = word_count+1\n                print(i, \"Gene feature [{}] present in test data point [{}]\".format(word,yes_no))   \n            \n        elif index < (len_unique_gene + len_unique_var):\n            word = var_fea_count_vec.get_feature_names()[index - len_unique_gene]\n            yes_no = True if word == var else False\n            if yes_no:\n                word_count = word_count+1\n                print(i, \"Var feature [{}] present in test data point [{}]\".format(word,yes_no))  \n                \n        else:\n            word = text_fea_count_vec.get_feature_names()[index - (len_unique_gene + len_unique_var)]\n            yes_no = True if word in text.split() else False\n            if yes_no:\n                word_count = word_count+1\n                print(i, \"Text feature [{}] present in test data point [{}]\".format(word,yes_no))   \n                \n    print(\"out of a total of\",no_features,\"feature\",word_count,\"are present in query point\")         \n                \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"    gene_count_vec = CountVectorizer()\n    var_count_vec = CountVectorizer()\n    text_count_vec = CountVectorizer(min_df=3)\n    \n    gene_fea_count_vec = gene_count_vec.fit(x_train[\"Gene\"])\n    var_fea_count_vec = var_count_vec.fit(x_train[\"Variation\"])\n    text_fea_count_vec = text_count_vec.fit(x_train[\"TEXT\"])\n\n\nlen(var_fea_count_vec.get_feature_names())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#after encoding all the 3 features we have to stack them together so as to make our complete datasets[D{xi,yi}]\n\nimport numpy as np\nfrom scipy.sparse import hstack\n\n\n# final datasets with one hot encoding\n\n\n#stacking gene and variation\ntrain_gene_var_onehotCoding = hstack((train_gene_feature_onehotCoding,train_variation_feature_onehotCoding))\ntest_gene_var_onehotCoding = hstack((test_gene_feature_onehotCoding,test_variation_feature_onehotCoding))\ncv_gene_var_onehotCoding = hstack((cv_gene_feature_onehotCoding,cv_variation_feature_onehotCoding))\n\n\n# train data\ntrain_x_onehotCoding = hstack((train_gene_var_onehotCoding, train_text_feature_onehotCoding)).tocsr()\ntrain_y = np.array(list(y_train['Class']))\n\n# test data\ntest_x_onehotCoding = hstack((test_gene_var_onehotCoding, test_text_feature_onehotCoding)).tocsr()\ntest_y = np.array(list(y_test['Class']))\n\n#cv data\ncv_x_onehotCoding = hstack((cv_gene_var_onehotCoding, cv_text_feature_onehotCoding)).tocsr()\ncv_y = np.array(list(y_cv['Class']))\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"One hot encoding features :\\n\")\n\nprint(\"(number of data points * number of features) in train data = \", train_x_onehotCoding.shape)\nprint(\"(number of data points * number of features) in test data = \", test_x_onehotCoding.shape)\nprint(\"(number of data points * number of features) in cross validation data =\", cv_x_onehotCoding.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1. Naive Bayes"},{"metadata":{"trusted":true},"cell_type":"code","source":"# As we are doing multi classification problem so we have to use multinomial naive bayes here .\n# here hyper parameter is alpha which is laplace smoothing\n\nfrom sklearn.naive_bayes import MultinomialNB\n\n# finding best hyper parameter\n\nalpha = [10**x for x in range(-5,3)]\n\ncv_log_error_array = []\n\nfor i in alpha:\n    clf = MultinomialNB(alpha = i)\n    clf.fit(train_x_onehotCoding , train_y)\n    calibration = CalibratedClassifierCV(clf , method='sigmoid')\n    calibration.fit(train_x_onehotCoding , train_y)\n    y_pred = calibration.predict_proba(cv_x_onehotCoding)\n    cv_log_error_array.append(log_loss(cv_y , y_pred))\n    print(\"log_loss for alpha\",i,\"is\",log_loss(cv_y , y_pred))\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plotting between alpha and log loss to see at what alpha log loss is minimum (same thing as above in graph)\n\nf, ax = plt.subplots(1,figsize=(10,5))\nax.plot(np.log10(alpha), cv_log_error_array)\nfor i, txt in enumerate(np.round(cv_log_error_array,3)):\n    ax.annotate((alpha[i],str(txt)), (np.log10(alpha[i]),cv_log_error_array[i]))\nplt.grid()\nplt.xticks(np.log10(alpha))\nplt.title(\"Cross Validation Error for each alpha\")\nplt.xlabel(\"Alpha i's\")\nplt.ylabel(\"Error\")\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# using best hyper parameter for our model\n\nbest_alpha = np.argmin(cv_log_error_array)\nclf = MultinomialNB(alpha=alpha[best_alpha])\nclf.fit(train_x_onehotCoding, train_y)\nsig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\nsig_clf.fit(train_x_onehotCoding, train_y)\n\n\npredict_y_train = sig_clf.predict_proba(train_x_onehotCoding)\npredict_y_cv = sig_clf.predict_proba(cv_x_onehotCoding)\npredict_y_test = sig_clf.predict_proba(test_x_onehotCoding)\n\n\nprint('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(train_y, predict_y_train))\npredict_y = sig_clf.predict_proba(cv_x_onehotCoding)\nprint('For values of best alpha = ', alpha[best_alpha], \"The cross validation log loss is:\",log_loss(cv_y, predict_y_cv))\npredict_y = sig_clf.predict_proba(test_x_onehotCoding)\nprint('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(test_y, predict_y_test,))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" <h5>As we understand log losses for all data, now we will make confusion matrix,precicion matrix,recall matrix for cv data to\nunderstand better that how misclassification happening between classes.</h5>"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nclf = MultinomialNB(alpha=alpha[best_alpha])\nclf.fit(train_x_onehotCoding, train_y)\nsig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\nsig_clf.fit(train_x_onehotCoding, train_y)\n\nsig_clf_predict_probs = sig_clf.predict_proba(cv_x_onehotCoding)\n\nsig_clf_predict = sig_clf.predict(cv_x_onehotCoding)\n\nprint(\"Log Loss cv data:\",log_loss(cv_y, sig_clf_predict_probs))\n\nprint(\"Number of missclassified point is\", np.count_nonzero((sig_clf_predict - cv_y)),\"out of total\",cv_y.shape[0],\"points\")\n\n# we are using the function that we declared earlier for plotting mpr matrices\n\nplot_mpr_matrices_heatmaps(cv_y,sig_clf_predict)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"How to see above matrices (will tell only for one top left box)\n\nconfusion --->There are a total of 47 points whose actual class is 1 and predicted class is also 1\nPrecision---->There are 53.4 % of point predicted to class 1 and also belong to class 1 only.\nRecall------->Out of total points having class label as 1 actually ,only 58.8% of them predicted correctly as 1."},{"metadata":{},"cell_type":"markdown","source":"### Last and main thing"},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets see how our model works for a query test data\n\n# we will see for first(first index) test data and lets see whether predicted and actual same or not and also find array of \n# probablities to it as it is our business requirement.\n\ntest_point_index = 6\nno_feature = 100\n\n\nquery_data = test_x_onehotCoding[test_point_index]\n\npredicted_class = sig_clf.predict(query_data)\n\npredicted_probablity = sig_clf.predict_proba(query_data)\n\nprint(\"Our model predicted class of this query data is =\" ,predicted_class[0])\n\nprint(\"Our model predicted probablities for all classes for this query data is =\" ,np.round(predicted_probablity,3))\n\nprint(\"Actual class of this query data is =\" , test_y[test_point_index])\n\n\n# now the main thing is, for interpretability we will find words cause of which corresponding class predicted \n# feature importance\n\n\n# calculating indices of datas having class same as our predicted class\nindices = np.argsort(-1*abs(clf.coef_))[predicted_class-1][:,:no_feature]\n\nprint(\"-\"*100)\n\nget_impfeature_names(indices[0], x_test['TEXT'].iloc[test_point_index],x_test['Gene'].iloc[test_point_index],\n                     x_test['Variation'].iloc[test_point_index], no_feature)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. K-Nearest Neighbour"},{"metadata":{"trusted":true},"cell_type":"code","source":"# as we know that knn wont work for high dimension data cause of curse of dimensionality ,but still lets see.\n\nfrom sklearn.neighbors import KNeighborsClassifier\n\nk = [x for x in range(1,20,2)]\n\ncv_log_loss_array = []\n\nfor i in k:\n    \n    clf = KNeighborsClassifier(n_neighbors = i)\n    clf.fit(train_x_onehotCoding , train_y)\n    calibration = CalibratedClassifierCV( clf , method='sigmoid')\n    calibration.fit(train_x_onehotCoding , train_y)\n    pred_probablity = calibration.predict_proba(cv_x_onehotCoding)\n    cv_log_loss_array.append(log_loss(cv_y , pred_probablity))\n    print(\"log_loss for k = \",i,\"is\",log_loss(cv_y , pred_probablity))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plotting for k vs error\n\nf,ax = plt.subplots(1 , figsize = (15,5))\nax.plot(k , cv_log_loss_array)\nfor i,text in enumerate (np.round(cv_log_loss_array , 3)) :\n    ax.annotate((k[i] , str(text)) , (k[i] , cv_log_loss_array[i] ))\n    \nplt.grid()\nplt.title(\"Cross Validation Error for each alpha\")\nplt.xlabel(\"K's\")\nplt.ylabel(\"Error measure\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# finding log loss for the best hyperparameter\n\nbest_k_position = np.argmin(cv_log_loss_array)\n\n\nclf = KNeighborsClassifier(n_neighbors = k[best_k_position])\nclf.fit(train_x_onehotCoding , train_y)\ncalibration = CalibratedClassifierCV( clf , method='sigmoid')\ncalibration.fit(train_x_onehotCoding , train_y)\nclf_predict = calibration.predict(cv_x_onehotCoding)\n\n\npredict_y_train = calibration.predict_proba(train_x_onehotCoding)\npredict_y_cv = calibration.predict_proba(cv_x_onehotCoding)\npredict_y_test = calibration.predict_proba(test_x_onehotCoding)\n\n\nprint('For values of best k = ', k[best_k_position], \"The train log loss is:\",log_loss(train_y, predict_y_train))\n\nprint('For values of best k = ', k[best_k_position], \"The cross validation log loss is:\",log_loss(cv_y, predict_y_cv))\n\nprint('For values of best k = ', k[best_k_position], \"The test log loss is:\",log_loss(test_y, predict_y_test,))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# we are using the function that we declared earlier for plotting mpr matrices and also we will find total misclassified \n# points ,both thing for our cv data.\n\ntotal_misclassified_point = np.count_nonzero(clf_predict-cv_y)\nprint(\"There are\" , total_misclassified_point,\"misclassified points out of a total of\", cv_y.shape[0],\"points\\n\")\n\nplot_mpr_matrices_heatmaps(cv_y,clf_predict)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Last and main thing"},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets see how our model works for a query test data\n\n# we will see for first(first index) test data and lets see whether predicted and actual same or not and also find array of \n# probablities to it as it is our business requirement.\n# we have to reshape here at last as we are using response coding\n\nfrom sklearn import neighbors\n\nclf = KNeighborsClassifier(n_neighbors = k[best_k_position])\nclf.fit(train_x_onehotCoding , train_y)\ncalibration = CalibratedClassifierCV( clf , method='sigmoid')\ncalibration.fit(train_x_onehotCoding , train_y)\n\ntest_point_index = 5\nno_feature = 100\n\n\n\nquery_data = test_x_onehotCoding[test_point_index]\n\npredicted_class = calibration.predict(test_x_onehotCoding[test_point_index].reshape(1,-1))\n\npredicted_class_probablity = calibration.predict_proba(test_x_onehotCoding[test_point_index].reshape(1,-1))\n\nprint(\"Our model predicted class of this query data is =\" , predicted_class[0])\n\nprint(\"Our model predicted probablities for all classes for this query data is =\" ,np.round(predicted_class_probablity,3))\n\nprint(\"Actual class of this query data is =\" , test_y[test_point_index])\n\nneighbors = clf.kneighbors(test_x_onehotCoding[test_point_index].reshape(1, -1), k[best_k_position])\n\nprint(\"The \",k[best_k_position],\" nearest neighbours of the query point belongs to classes\",train_y[neighbors[1][0]])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"# now we will make model by logistic regression with two types of data ,first one is with balanced data and the other\n# one is imbalanced data.\n# we will use logistiv=c regression by selecting sgd classfier and in argumnent selecting loss as log","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Logistic Regression with balanced data"},{"metadata":{"trusted":true},"cell_type":"code","source":"alpha = [10**x for x in range(-5,3)]        # hyperparameter\n\ncv_log_loss_array = []\n\nfor i in alpha:\n    \n    clf = SGDClassifier(loss='log',class_weight = 'balanced',alpha = i , random_state=42)   # balancing weight\n    clf.fit(train_x_onehotCoding , train_y)\n    calibration = CalibratedClassifierCV(base_estimator = clf)\n    calibration.fit(train_x_onehotCoding , train_y)\n    pred_probality = calibration.predict_proba(cv_x_onehotCoding)\n    cv_log_loss_array.append(log_loss(cv_y , pred_probality))\n    print(\"log loss for alpha value\",i,\"is\",log_loss(cv_y , pred_probality))\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plotting error vs hyperparamter\n\nfig,ax = plt.subplots(figsize = (10,8))\n\nax.plot(alpha , cv_log_loss_array )\n\nfor i,text in enumerate(np.round(cv_log_loss_array,3)):\n    ax.annotate((alpha[i] , text) ,(alpha[i] ,cv_log_loss_array[i]))\n    \nplt.grid()\nplt.title(\"Cross Validation Error for each alpha\")\nplt.xlabel(\"Alpha i's\")\nplt.ylabel(\"Error measure\")\nplt.show()    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# finding log loss for best hyperparmeter\n\nbest_alpha_position = np.argmin(cv_log_loss_array)\n\n\nclf = SGDClassifier(loss='log',class_weight = 'balanced', alpha = alpha[best_alpha_position] , random_state=42) \nclf.fit(train_x_onehotCoding , train_y)\ncalibration = CalibratedClassifierCV(base_estimator = clf)\ncalibration.fit(train_x_onehotCoding , train_y)\n\npred_cv = calibration.predict(cv_x_onehotCoding)\n\n\npred_probality_train = calibration.predict_proba(train_x_onehotCoding)\npred_probality_cv = calibration.predict_proba(cv_x_onehotCoding)\npred_probality_test = calibration.predict_proba(test_x_onehotCoding)\n\n\nprint(\"train data log loss for alpha =\",alpha[best_alpha_position],\"is\",log_loss(train_y , pred_probality_train))\nprint(\"cv data log loss for alpha =\",alpha[best_alpha_position],\"is\",log_loss(cv_y , pred_probality_cv))\nprint(\"test data log loss for alpha =\",alpha[best_alpha_position],\"is\",log_loss(test_y , pred_probality_test))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# misclassfied points and plotting matrices\n\ntotal_misclassified_point = np.count_nonzero(cv_y - pred_cv)\n\nprint(\"total misclassfied point is\",total_misclassified_point,\"out of a total of\",cv_y.shape[0],\"data points\")\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_mpr_matrices_heatmaps(cv_y,pred_cv)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# now testing our model with a query and we will also use a function to calculate its feature importance also.\n\nclf = SGDClassifier(alpha=alpha[best_alpha], penalty='l2', loss='log', random_state=42)\nclf.fit(train_x_onehotCoding,train_y)\ntest_point_index = 0\nno_feature = 500\n\npredicted_cls = calibration.predict(test_x_onehotCoding[test_point_index])\n\nprint(\"Predicted Class :\", predicted_cls[test_point_index])\nprint(\"Predicted Class Probabilities:\", np.round(sig_clf.predict_proba(test_x_onehotCoding[test_point_index]),4))\nprint(\"Actual Class :\", test_y[test_point_index])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# to calculate feature importance\n\n# calculating indices of datas having class same as our predicted class\nindices = np.argsort(-1*abs(clf.coef_))[predicted_cls-1][:,:no_feature]\n\nget_impfeature_names(indices[0], x_test['TEXT'].iloc[test_point_index],x_test['Gene'].iloc[test_point_index],\n                     x_test['Variation'].iloc[test_point_index], no_feature)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Logistic Regression with imbalanced data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# now for class imbalanced dataset for logistice regression we just have to change in argument of weight balancing\n# rest everything will be same .","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4. Linear SVM"},{"metadata":{"trusted":true},"cell_type":"code","source":"# we will use linear svm as it is same as logistice regression and also works very good on high dimension data\n# we are not using rbf svm as we donot know the correct kernel for it ,also it is not interpretable.\n\nalpha = [10**x for x in range(-5,4)]  # hyperparameter\n\ncv_log_error = []\n\nfor i in alpha:\n    clf = SGDClassifier(loss='hinge', penalty='l2', alpha = i , class_weight = 'balanced',random_state=42)\n    clf.fit(train_x_onehotCoding , train_y)\n    calibration_svm = CalibratedClassifierCV(base_estimator = clf ,  method='sigmoid')\n    calibration_svm.fit(train_x_onehotCoding , train_y)\n    pred_probality_cv = calibration_svm.predict_proba(cv_x_onehotCoding)\n    cv_log_error.append(log_loss(cv_y , pred_probality_cv))\n    print(\"log loss for cv data for svm model with alpha as\",i,\"is\",log_loss(cv_y , pred_probality_cv))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plotting error vs hyperparameter\n\nfig,ax = plt.subplots(figsize = (20,15))\n\nax.plot(alpha , cv_log_error )\n\nfor i,text in enumerate(np.round(cv_log_error,3)):\n    ax.annotate((alpha[i] , text) ,(alpha[i] ,cv_log_error[i]))\n    \nplt.grid()\nplt.title(\"Cross Validation Error for each alpha\")\nplt.xlabel(\"Alpha i's\")\nplt.ylabel(\"Error measure\")\nplt.show()  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# now lets start for best hyper parameter\n\nbest_alpha_position = np.argmin(cv_log_error)\n\nclf = SGDClassifier(loss='hinge', penalty='l2', alpha = alpha[best_alpha_position] , class_weight = 'balanced',random_state=42)\nclf.fit(train_x_onehotCoding , train_y)\ncalibration_svm = CalibratedClassifierCV(base_estimator = clf ,  method='sigmoid')\ncalibration_svm.fit(train_x_onehotCoding , train_y)\n\npred_test_svm = calibration_svm.predict(test_x_onehotCoding)\n\n\npred_probality_train = calibration_svm.predict_proba(train_x_onehotCoding)\npred_probality_cv = calibration_svm.predict_proba(cv_x_onehotCoding)\npred_probality_test = calibration_svm.predict_proba(test_x_onehotCoding)\n\n\n\nprint(\"log loss for train data for svm model is\",log_loss(train_y , pred_probality_train))\nprint(\"log loss for cv data for svm model is\",log_loss(cv_y , pred_probality_cv))\nprint(\"log loss for test data for svm model is\",log_loss(test_y , pred_probality_test))\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# total misclassified points by svm and its confusion matrices\n\npred_cv_svm = calibration_svm.predict(cv_x_onehotCoding)\ntotal_misclassfied_svm = np.count_nonzero(cv_y - pred_cv_svm )\n\nprint(\"total misclassfied poinnts for svm model is\",total_misclassfied_svm,\"out of\",cv_y.shape[0],\"data points\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_mpr_matrices_heatmaps(cv_y,pred_cv_svm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# now testing our model with query test point and find feature importance\n\n\ntest_point_index = 0\nno_of_feature  = 1000\n\nprint(\"predicted value for given  test data is \",pred_test_svm[test_point_index])\nprint(\"actual value for given test data is \",test_y[test_point_index])\nprint(\"predicted probablistic value for given test data is \",np.round(pred_probality_test[test_point_index],3))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# feature importance\n\nindices = np.argsort(-1*abs(clf.coef_))[pred_test_svm-1][:,:no_of_feature]\n\nget_impfeature_names(indices[0], x_test['TEXT'].iloc[test_point_index],x_test['Gene'].iloc[test_point_index],\n                     x_test['Variation'].iloc[test_point_index], no_of_feature)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5. Random Forest Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"# as rf depends on decision trees so less dimension data set will be better but still we will use both response coded \n# and one hot encoded data set to check the model.\n\n# In rf ,there are two hyperparmeters first one is total trees(n_estimators) and second is max depth of each tree. ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Random forest (one hot encoded)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nmax_depth = [5,10,15]\nmax_trees = [x for x in range(100,1000,200)]\n\n\ncv_log_rf_error = []\nfor i in max_trees:\n    for j in max_depth:\n        clf = RandomForestClassifier(n_estimators = i , criterion='gini', max_depth = j ,random_state = 42)\n        clf.fit(train_x_onehotCoding , train_y)\n        calibration_rf = CalibratedClassifierCV(base_estimator = clf)\n        calibration_rf.fit(train_x_onehotCoding , train_y)\n        predict_cv_class = calibration_rf.predict_proba(cv_x_onehotCoding)\n        cv_log_rf_error.append(log_loss(cv_y , predict_cv_class))\n        print(\"log loss for max_trees as\",i,\"and max_depth as\",j,\"is\",log_loss(cv_y , predict_cv_class))\n        \n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"min_log_loss_position = np.argmin(cv_log_rf_error)\nmin_log_loss_position","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"min_log_loss_position = np.argmin(cv_log_rf_error)\nmin_log_loss_position\n# now for best hyperparameter calculating log loss in all data\n\nclf = RandomForestClassifier(n_estimators = 900 , criterion='gini', max_depth = 10 ,random_state = 42)\nclf.fit(train_x_onehotCoding , train_y)\ncalibration_rf = CalibratedClassifierCV(base_estimator = clf)\ncalibration_rf.fit(train_x_onehotCoding , train_y)\n\npredict_cv_class = calibration_rf.predict(cv_x_onehotCoding)\n\npredict_prob__train_class = calibration_rf.predict_proba(train_x_onehotCoding)\npredict_prob_cv_class = calibration_rf.predict_proba(cv_x_onehotCoding)\npredict_prob_test_class = calibration_rf.predict_proba(test_x_onehotCoding)\n\n\nprint(\"log loss for best hyperparameter for train data is\",log_loss(train_y , predict_prob__train_class))\nprint(\"log loss for best hyperparameter for cv data is\",log_loss(cv_y , predict_prob_cv_class))\nprint(\"log loss for best hyperparameter for test data is\",log_loss(test_y , predict_prob_test_class))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# calculating misclassified points and plotting metrices\n\ntotal_misclassified_rf = np.count_nonzero(cv_y - predict_cv_class)\n\nprint(\"total misclassified point is\",total_misclassified_rf,\"out of a total of\",cv_y.shape[0],\"points\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_mpr_matrices_heatmaps(cv_y,predict_cv_class)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# testing model with a test data and calculating feature importance\n\ntest_point_index = 0\nno_of_feature  = 100\n\npred_test_rf = calibration_rf.predict(test_x_onehotCoding)\n\nprint(\"predicted value for given  test data is \",pred_test_rf[test_point_index])\nprint(\"actual value for given test data is \",test_y[test_point_index])\nprint(\"predicted probablistic value for given test data is \",np.round(predict_prob_test_class[test_point_index],3))\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 6. Stacking model"},{"metadata":{},"cell_type":"markdown","source":"For this we will use two types of classfier\\\na) Stacking classfier\\\nb) Voting classifier\n\nProblem with stacking model is\n\n1.Interpretability lost(no feature importance).\\\n2.Data separted between different classfier so individual accuracy decreases.\\\n3.Stacking model is good when we have millions of data ."},{"metadata":{},"cell_type":"markdown","source":"## Stacking Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"# we will use three different classifier and then merge it to the single classifier(default = logistic regression) \n\n# logistic regression\nclf_1 = SGDClassifier(alpha=0.001, penalty='l2', loss='log', class_weight='balanced', random_state=0)\nclf_1.fit(train_x_onehotCoding, train_y)\nsig_clf_1 = CalibratedClassifierCV(clf_1, method=\"sigmoid\")\n\n# svm\nclf_2 = SGDClassifier(alpha=1, penalty='l2', loss='hinge', class_weight='balanced', random_state=0)\nclf_2.fit(train_x_onehotCoding, train_y)\nsig_clf_2 = CalibratedClassifierCV(clf_2, method=\"sigmoid\")\n\n# Naive Bayes\n\nclf_3 = MultinomialNB(alpha=0.001)\nclf_3.fit(train_x_onehotCoding, train_y)\nsig_clf_3 = CalibratedClassifierCV(clf_3, method=\"sigmoid\")\n\n\n# log loss individual\nsig_clf_1.fit(train_x_onehotCoding, train_y)\nprint(\"log loss for logistic regression is\",log_loss(cv_y , sig_clf_1.predict_proba(cv_x_onehotCoding)))\n\nsig_clf_2.fit(train_x_onehotCoding, train_y)\nprint(\"log loss for svm is\",log_loss(cv_y , sig_clf_2.predict_proba(cv_x_onehotCoding)))\n\nsig_clf_3.fit(train_x_onehotCoding, train_y)\nprint(\"log loss for naive bayes is\",log_loss(cv_y , sig_clf_3.predict_proba(cv_x_onehotCoding)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import StackingClassifier\nfrom sklearn.linear_model import LogisticRegression\n\nalpha = [10**x for x in range(-3,2)]\n\nfor i in alpha:\n    final_classifier = LogisticRegression(C=i)\n    sclf = StackingClassifier(estimators=[('lr', sig_clf_1), ('svc', sig_clf_2), ('nb', sig_clf_3)], final_estimator = final_classifier)\n    sclf.fit(train_x_onehotCoding, train_y)\n    predicted_prob_value = sclf.predict_proba(cv_x_onehotCoding)\n    predicted_value = sclf.predict(cv_x_onehotCoding)\n    \n    print(\"log loss for the stacked classifier for alpha as\",i,\"is\",log_loss(cv_y , predicted_prob_value))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# testing model with best hyperparameter\n\nfinal_classifier = LogisticRegression(C=1)\nsclf = StackingClassifier(estimators=[('lr', sig_clf_1), ('svc', sig_clf_2), ('nb', sig_clf_3)], final_estimator = final_classifier)\nsclf.fit(train_x_onehotCoding, train_y)\n\nlog_error = log_loss(train_y, sclf.predict_proba(train_x_onehotCoding))\nprint(\"Log loss (train) on the stacking classifier :\",log_error)\n\nlog_error = log_loss(cv_y, sclf.predict_proba(cv_x_onehotCoding))\nprint(\"Log loss (CV) on the stacking classifier :\",log_error)\n\nlog_error = log_loss(test_y, sclf.predict_proba(test_x_onehotCoding))\nprint(\"Log loss (test) on the stacking classifier :\",log_error)\n\nprint(\"Number of missclassified point :\", np.count_nonzero((sclf.predict(test_x_onehotCoding)- test_y))/test_y.shape[0])\nplot_mpr_matrices_heatmaps(cv_y , predicted_value)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Maximun Voting classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import VotingClassifier\nvclf = VotingClassifier(estimators=[('lr', sig_clf_1), ('svc', sig_clf_2), ('nb', sig_clf_3)], voting='soft')\nvclf.fit(train_x_onehotCoding, train_y)\n\npredicted_value = vclf.predict(cv_x_onehotCoding)\nprint(\"Log loss (train) on the VotingClassifier :\", log_loss(train_y, vclf.predict_proba(train_x_onehotCoding)))\nprint(\"Log loss (CV) on the VotingClassifier :\", log_loss(cv_y, vclf.predict_proba(cv_x_onehotCoding)))\nprint(\"Log loss (test) on the VotingClassifier :\", log_loss(test_y, vclf.predict_proba(test_x_onehotCoding)))\nprint(\"Number of missclassified point :\", np.count_nonzero((vclf.predict(test_x_onehotCoding)- test_y))/test_y.shape[0])\nplot_mpr_matrices_heatmaps(cv_y , predicted_value)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}