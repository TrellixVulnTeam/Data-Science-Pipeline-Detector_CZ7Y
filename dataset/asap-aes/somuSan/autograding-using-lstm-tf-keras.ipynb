{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n        \n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport pandas as pd\n\ndf = pd.read_csv(\"../input/asap-aes/training_set_rel3.tsv\", sep='\\t', encoding='ISO-8859-1')\ndf = df.dropna(axis=1)\ndf = df.drop(columns=['rater1_domain1', 'rater2_domain1'])\ndf = df.drop(columns=['essay_id', 'essay_set'])\ndf.head()","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['essay'][1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")                     #Ignoring unnecessory warnings\n\nimport numpy as np                                  #for large and multi-dimensional arrays\nimport pandas as pd                                 #for data manipulation and analysis\nimport nltk                                         #Natural language processing tool-kit\n\nfrom nltk.corpus import stopwords                   #Stopwords corpus\nfrom nltk.stem import PorterStemmer                 # Stemmer\n\nfrom sklearn.feature_extraction.text import CountVectorizer          #For Bag of words\nfrom sklearn.feature_extraction.text import TfidfVectorizer          #For TF-IDF\nfrom gensim.models import Word2Vec                                   #For Word2Vec\n\nfrom tensorflow.keras.layers import Embedding\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.preprocessing.text import one_hot\nfrom tensorflow.keras.layers import LSTM\nfrom tensorflow.keras.layers import Dropout\nfrom tensorflow.keras.layers import Dense","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"list_1 = []\nlist_2 = []\nlist_3 = []\nlist_4 = []\nlist_5 = []\nlist_6 = []\nlist_7 = []\nlist_8 = []\nlist_9 = []\nlist_10 = []","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = []\nnum = [1,2,3,4,5,6,7,8,9,10]\ncount = 0\nfor i,j in zip(range(len(df)),df[\"essay\"]):\n    if df[\"domain1_score\"][i] == 1:\n        list_1.append(j)\n    if df[\"domain1_score\"][i] == 2:\n        list_2.append(j)\n    if df[\"domain1_score\"][i] == 3:\n        list_3.append(j)\n    if df[\"domain1_score\"][i] == 4:\n        list_4.append(j)\n    if df[\"domain1_score\"][i] == 5:\n        list_5.append(j)\n    if df[\"domain1_score\"][i] == 6:\n        list_6.append(j)\n    if df[\"domain1_score\"][i] == 7:\n        list_7.append(j)\n    if df[\"domain1_score\"][i] == 8:\n        list_8.append(j)\n    if df[\"domain1_score\"][i] == 9:\n        list_9.append(j)\n    if df[\"domain1_score\"][i] == 10:\n        list_10.append(j)\n   ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import itertools\ndata = list(itertools.chain(list_1,list_2,list_3,list_4,list_5,list_6,list_7,list_8,list_9,list_10))\nlen(data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def gen_num(num,length):\n    num_list = [num]*length\n    return num_list","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"score_1 = gen_num(1,len(list_1))\nscore_2 = gen_num(2,len(list_2))\nscore_3 = gen_num(3,len(list_3))\nscore_4 = gen_num(4,len(list_4))\nscore_5 = gen_num(5,len(list_5))\nscore_6 = gen_num(6,len(list_6))\nscore_7 = gen_num(7,len(list_7))\nscore_8 = gen_num(8,len(list_8))\nscore_9 = gen_num(9,len(list_9))\nscore_10 = gen_num(10,len(list_10))\n\nscore = list(itertools.chain(score_1,score_2,score_3,score_4,score_5,score_6,score_7,score_8,score_9,score_10))\nlen(score)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# dictionary of lists \ndictnary = {'essay': data, 'score': score}     \ndf = pd.DataFrame(dictnary) \ndf ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nltk.download('stopwords')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stop_words = set(stopwords.words('english'))\nlen(stop_words) #finding stop words","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nsnow = nltk.stem.SnowballStemmer('english')\n\ncorpus = []\nfor i in range(0, len(df)):\n    review = re.sub('[^a-zA-Z]', ' ', df['essay'][i])\n    review = review.lower()\n    review = review.split()\n    \n    review = [snow.stem(word) for word in review if not word in stopwords.words('english')]\n    review = ' '.join(review)\n    corpus.append(review)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"voc_size=5000\nonehot_repr=[one_hot(words,voc_size)for words in corpus] \ntype(onehot_repr)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sent_length=400\nembedded_docs=pad_sequences(onehot_repr,padding='pre',maxlen=sent_length)\nprint(embedded_docs)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport nltk\nimport re\nfrom nltk.corpus import stopwords\nfrom gensim.models import Word2Vec\n\ndef essay_to_wordlist(essay_v, remove_stopwords):\n    \"\"\"Remove the tagged labels and word tokenize the sentence.\"\"\"\n    essay_v = re.sub(\"[^a-zA-Z]\", \" \", essay_v)\n    words = essay_v.lower().split()\n    if remove_stopwords:\n        stops = set(stopwords.words(\"english\"))\n        words = [w for w in words if not w in stops]\n    return (words)\n\ndef essay_to_sentences(essay_v, remove_stopwords):\n    \"\"\"Sentence tokenize the essay and call essay_to_wordlist() for word tokenization.\"\"\"\n    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n    raw_sentences = tokenizer.tokenize(essay_v.strip())\n    sentences = []\n    for raw_sentence in raw_sentences:\n        if len(raw_sentence) > 0:\n            sentences.append(essay_to_wordlist(raw_sentence, remove_stopwords))\n    return sentences\n\ndef makeFeatureVec(words, model, num_features):\n    \"\"\"Make Feature Vector from the words list of an Essay.\"\"\"\n    featureVec = np.zeros((num_features,),dtype=\"float32\")\n    num_words = 0.\n    index2word_set = set(model.wv.index2word)\n    for word in words:\n        if word in index2word_set:\n            num_words += 1\n            featureVec = np.add(featureVec,model[word])        \n    featureVec = np.divide(featureVec,num_words)\n    return featureVec\n\ndef getAvgFeatureVecs(essays, model, num_features):\n    \"\"\"Main function to generate the word vectors for word2vec model.\"\"\"\n    counter = 0\n    essayFeatureVecs = np.zeros((len(essays),num_features),dtype=\"float32\")\n    for essay in essays:\n        essayFeatureVecs[counter] = makeFeatureVec(essay, model, num_features)\n        counter = counter + 1\n    return essayFeatureVecs","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import nltk\n\nnltk.download('stopwords')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.layers import Embedding, LSTM, Dense, Dropout, Lambda, Flatten\nfrom keras.models import Sequential, load_model, model_from_config\nimport keras.backend as K\n\ndef get_model():\n    \"\"\"Define the model.\"\"\"\n    model = Sequential()\n    model.add(LSTM(300, dropout=0.4, recurrent_dropout=0.4, input_shape=[1, 300], return_sequences=True))\n    model.add(LSTM(64, recurrent_dropout=0.4))\n    model.add(Dropout(0.5))\n    model.add(Dense(1, activation='relu'))\n\n    model.compile(loss='mean_squared_error', optimizer='rmsprop', metrics=['accuracy','mae'])\n    model.summary()\n\n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X=df\ny = X['score']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import KFold\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import cohen_kappa_score\n\ncv = KFold(n_splits = 5, shuffle = True)\nresults = []\ny_pred_list = []\n\ncount = 1\nfor traincv, testcv in cv.split(X):\n    print(\"\\n--------Fold {}--------\\n\".format(count))\n    X_test, X_train, y_test, y_train = X.iloc[testcv], X.iloc[traincv], y.iloc[testcv], y.iloc[traincv]\n    \n    train_essays = X_train['essay']\n    test_essays = X_test['essay']\n    \n    sentences = []\n    \n    for essay in train_essays:\n            # Obtaining all sentences from the training essays.\n            sentences += essay_to_sentences(essay, remove_stopwords = True)\n            \n    # Initializing variables for word2vec model.\n    num_features = 300 \n    min_word_count = 40\n    num_workers = 4\n    context = 10\n    downsampling = 1e-3\n\n    print(\"Training Word2Vec Model...\")\n    model = Word2Vec(sentences, workers=num_workers, size=num_features, min_count = min_word_count, window = context, sample = downsampling)\n\n    model.init_sims(replace=True)\n    model.wv.save_word2vec_format('word2vecmodel.bin', binary=True)\n\n    clean_train_essays = []\n    \n    # Generate training and testing data word vectors.\n    for essay_v in train_essays:\n        clean_train_essays.append(essay_to_wordlist(essay_v, remove_stopwords=True))\n    trainDataVecs = getAvgFeatureVecs(clean_train_essays, model, num_features)\n    \n    clean_test_essays = []\n    for essay_v in test_essays:\n        clean_test_essays.append(essay_to_wordlist( essay_v, remove_stopwords=True ))\n    testDataVecs = getAvgFeatureVecs( clean_test_essays, model, num_features )\n    \n    trainDataVecs = np.array(trainDataVecs)\n    testDataVecs = np.array(testDataVecs)\n    # Reshaping train and test vectors to 3 dimensions. (1 represnts one timestep)\n    trainDataVecs = np.reshape(trainDataVecs, (trainDataVecs.shape[0], 1, trainDataVecs.shape[1]))\n    testDataVecs = np.reshape(testDataVecs, (testDataVecs.shape[0], 1, testDataVecs.shape[1]))\n    \n    lstm_model = get_model()\n    lstm_model.fit(trainDataVecs, y_train, batch_size=64, epochs=50)\n    #lstm_model.load_weights('./model_weights/final_lstm.h5')\n    y_pred = lstm_model.predict(testDataVecs)\n    \n    # Save any one of the 5 models.\n    if count == 5:\n         lstm_model.save('./final_lstm.h5')\n    \n    # Round y_pred to the nearest integer.\n    y_pred = np.around(y_pred)\n    \n    # Evaluate the model on the evaluation metric. \"Quadratic mean averaged Kappa\"\n    result = cohen_kappa_score(y_test.values,y_pred,weights='quadratic')\n    from sklearn.metrics import accuracy_score\n    acc = accuracy_score(y_test.values,y_pred)\n    print(\"acc Score: {}\".format(acc))\n    print(\"Kappa Score: {}\".format(result))\n    results.append(result)\n\n    count += 1\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Average Kappa score after a 5-fold cross validation: \",np.around(np.array(results).mean(),decimals=4))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"demo = [\"Dear@CAPS1 @CAPS2, I believe that using computers will benefit us in many ways like talking and becoming friends will others through websites like facebook and mysace. Using computers can help us find coordibates, locations, and able ourselfs to millions of information. Also computers will benefit us by helping with jobs as in planning a house plan and typing a @NUM1 page report for one of our jobs in less than writing it. Now lets go into the wonder world of technology. Using a computer will help us in life by talking or making friends on line. Many people have myspace, facebooks, aim, these all benefit us by having conversations with one another. Many people believe computers are bad but how can you make friends if you can never talk to them? I am very fortunate for having a computer that can help with not only school work but my social life and how I make friends. Computers help us with finding our locations, coordibates and millions of information online. If we didn't go on the internet a lot we wouldn't know how to go onto websites that @MONTH1 help us with locations and coordinates like @LOCATION1. Would you rather use a computer or be in @LOCATION3. When your supposed to be vacationing in @LOCATION2. Million of information is found on the internet. You can as almost every question and a computer will have it. Would you rather easily draw up a house plan on the computers or take @NUM1 hours doing one by hand with ugly erazer marks all over it, you are garrenteed that to find a job with a drawing like that. Also when appling for a job many workers must write very long papers like a @NUM3 word essay on why this job fits you the most, and many people I know don't like writing @NUM3 words non-stopp for hours when it could take them I hav an a computer. That is why computers we needed a lot now adays. I hope this essay has impacted your descion on computers because they are great machines to work with. The other day I showed my mom how to use a computer and she said it was the greatest invention sense sliced bread! Now go out and buy a computer to help you chat online with friends, find locations and millions of information on one click of the button and help your self with getting a job with neat, prepared, printed work that your boss will love.\"]\ndemo_df = pd.DataFrame(demo,columns=['essay'])\ndemo_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"type(demo_df['essay'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"content = \"Dear@CAPS1 @CAPS2, I believe that using computers will benefit us in many ways like talking and becoming friends will others through websites like facebook and mysace. Using computers can help us find coordibates, locations, and able ourselfs to millions of information. Also computers will benefit us by helping with jobs as in planning a house plan and typing a @NUM1 page report for one of our jobs in less than writing it. Now lets go into the wonder world of technology. Using a computer will help us in life by talking or making friends on line. Many people have myspace, facebooks, aim, these all benefit us by having conversations with one another. Many people believe computers are bad but how can you make friends if you can never talk to them? I am very fortunate for having a computer that can help with not only school work but my social life and how I make friends. Computers help us with finding our locations, coordibates and millions of information online. If we didn't go on the internet a lot we wouldn't know how to go onto websites that @MONTH1 help us with locations and coordinates like @LOCATION1. Would you rather use a computer or be in @LOCATION3. When your supposed to be vacationing in @LOCATION2. Million of information is found on the internet. You can as almost every question and a computer will have it. Would you rather easily draw up a house plan on the computers or take @NUM1 hours doing one by hand with ugly erazer marks all over it, you are garrenteed that to find a job with a drawing like that. Also when appling for a job many workers must write very long papers like a @NUM3 word essay on why this job fits you the most, and many people I know don't like writing @NUM3 words non-stopp for hours when it could take them I hav an a computer. That is why computers we needed a lot now adays. I hope this essay has impacted your descion on computers because they are great machines to work with. The other day I showed my mom how to use a computer and she said it was the greatest invention sense sliced bread! Now go out and buy a computer to help you chat online with friends, find locations and millions of information on one click of the button and help your self with getting a job with neat, prepared, printed work that your boss will love.\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from gensim.models import Word2Vec\nfrom gensim.models import KeyedVectors\nnum_features = 300\n      \nmodel = KeyedVectors.load_word2vec_format( \"./word2vecmodel.bin\", binary=True)\nclean_test_essays = []\nclean_test_essays.append(essay_to_wordlist( content, remove_stopwords=True ))\ntestDataVecs = getAvgFeatureVecs( clean_test_essays, model, num_features )\ntestDataVecs = np.array(testDataVecs)\ntestDataVecs = np.reshape(testDataVecs, (testDataVecs.shape[0], 1, testDataVecs.shape[1]))\n\n# lstm_model = get_model()\nlstm_model.load_weights(\"./final_lstm.h5\")\npreds = lstm_model.predict(testDataVecs)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"int(np.around(preds))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# val_essays = demo_df['essay']\n# sentences = []\n\n# for essay in val_essays:\n#         sentences += essay_to_sentences(essay, remove_stopwords = True)\n        \n\n# num_features = 300 \n# min_word_count = 40\n# num_workers = 4\n# context = 10\n# downsampling = 1e-3\n\n\n# model = Word2Vec(sentences, workers=num_workers, size=num_features, min_count = min_word_count, window = context, sample = downsampling)\n\n# model.init_sims(replace=True)\n\n# clean_train_essays = []\n\n# # Generate training and testing data word vectors.\n# for essay_v in val_essays:\n#     clean_train_essays.append(essay_to_wordlist(essay_v, remove_stopwords=True))\n# trainDataVecs = getAvgFeatureVecs(clean_train_essays, model, num_features)\n\n# trainDataVecs = np.array(trainDataVecs)\n# # Reshaping train and test vectors to 3 dimensions. (1 represnts one timestep)\n# trainDataVecs = np.reshape(trainDataVecs, (trainDataVecs.shape[0], 1, trainDataVecs.shape[1]))\n\n\n# y_pred = lstm_model.predict(trainDataVecs)\n\n\n\n# y_pred = np.around(y_pred)\n\n\n# # result = cohen_kappa_score(y_test.values,y_pred,weights='quadratic')\n# # print(\"Kappa Score: {}\".format(result))\n# print(y_pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_test.T.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred1 = y_pred.T.reshape(2595,)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_test1 = y_test.T.reshape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save a palette to a variable:\npalette = sns.color_palette(\"bright\")\n \n# Use palplot and pass in the variable:\nsns.palplot(palette)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\nimport pandas as pd\nimport matplotlib.pyplot as plt\narray =cm[0:10,0:10]\ndf_cm = pd.DataFrame(array)\nplt.figure(figsize = (20,20))\nsns.heatmap(df_cm,cmap = 'Blues',square=True, annot=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cm[0:10,0:10]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"arr = np.array([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]])\nprint(arr.shape)\nprint(arr[1:4, 4:1])\n# print(arr)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cm.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corr = cm\n\nmask = np.triu(np.ones_like(corr, dtype=np.bool))\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(11, 9))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\nplt.title('Fig:1',size=15)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import f1_score\nf1_score(y_test, y_pred1, average='macro')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report\ncr = classification_report(y_test, y_pred1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cr","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\nimport numpy as np\nfrom sklearn.metrics import precision_recall_fscore_support\nimport matplotlib.pyplot as plt\n\ndef plot_classification_report(y_tru, y_prd, figsize=(10, 10), ax=None):\n\n    plt.figure(figsize=figsize)\n\n    xticks = ['precision', 'recall', 'f1-score', 'support']\n    yticks = list(np.unique(y_tru))\n    yticks += ['avg']\n\n    rep = np.array(precision_recall_fscore_support(y_tru, y_prd)).T\n    avg = np.mean(rep, axis=0)\n    avg[-1] = np.sum(rep[:, -1])\n    rep = np.insert(rep, rep.shape[0], avg, axis=0)\n\n    sns.heatmap(rep,\n                annot=True, \n                cbar=False, \n                xticklabels=xticks, \n                yticklabels=yticks,\n                ax=ax)\n\nplot_classification_report(y_test, y_pred1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nimport itertools\n\n\ndef plot_classification_report(classificationReport,\n                               title='Classification report',\n                               cmap='RdBu'):\n\n    classificationReport = classificationReport.replace('\\n\\n', '\\n')\n    classificationReport = classificationReport.replace(' / ', '/')\n    lines = classificationReport.split('\\n')\n\n    classes, plotMat, support, class_names = [], [], [], []\n    for line in lines[1:]:  # if you don't want avg/total result, then change [1:] into [1:-1]\n        t = line.strip().split()\n        if len(t) < 2:\n            continue\n        classes.append(t[0])\n        v = [float(x) for x in t[1: len(t) - 1]]\n        support.append(int(t[-1]))\n        class_names.append(t[0])\n        plotMat.append(v)\n\n    plotMat = np.array(plotMat)\n    xticklabels = ['Precision', 'Recall', 'F1-score']\n    yticklabels = ['{0} ({1})'.format(class_names[idx], sup)\n                   for idx, sup in enumerate(support)]\n\n    plt.imshow(plotMat, interpolation='nearest', cmap=cmap, aspect='auto')\n    plt.title(title)\n    plt.colorbar()\n    plt.xticks(np.arange(3), xticklabels, rotation=45)\n    plt.yticks(np.arange(len(classes)), yticklabels)\n\n    upper_thresh = plotMat.min() + (plotMat.max() - plotMat.min()) / 10 * 8\n    lower_thresh = plotMat.min() + (plotMat.max() - plotMat.min()) / 10 * 2\n    for i, j in itertools.product(range(plotMat.shape[0]), range(plotMat.shape[1])):\n        plt.text(j, i, format(plotMat[i, j], '.2f'),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if (plotMat[i, j] > upper_thresh or plotMat[i, j] < lower_thresh) else \"black\")\n\n    plt.ylabel('Metrics')\n    plt.xlabel('Classes')\n    plt.tight_layout()\n\n\ndef main():\n\n    sampleClassificationReport = cr\n    plot_classification_report(sampleClassificationReport)\n    plt.show()\n    plt.close()\n\n\nif __name__ == '__main__':\n    main()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score,plot_confusion_matrix","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"classifiers = {\n    \"LogisiticRegression\": LogisticRegression(),\n    \"KNearest\": KNeighborsClassifier(n_neighbors=1),\n    \"Support Vector Classifier\": SVC(),\n    \"DecisionTreeClassifier\": DecisionTreeClassifier(),\n    \"MultinimialNB\": MultinomialNB()\n}\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainDataVecs.shape[2]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainDataVecs1 = np.reshape(trainDataVecs,trainDataVecs.shape[0],trainDataVecs.shape[2])\ntrainDataVecs1.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\n\nclassifier = KNeighborsClassifier()\n\nclassifier.fit(trainDataVecs, y_train)\ntraining_score = cross_val_score(classifier, train_vectors, df_train[\"domain1_score\"], cv=5)\nprint(\"Classifiers: \", classifier.__class__.__name__, \"Has a training score of\", round(training_score.mean(), 2) * 100, \"% accuracy score\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.unique(df['domain1_score'],return_counts=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.utils import class_weight\nclass_weights = class_weight.compute_class_weight('balanced',\n                                                 np.unique(df['domain1_score']),\n                                                 train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}