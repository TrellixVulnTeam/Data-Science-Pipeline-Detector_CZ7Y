{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session//","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#for pre-processing\nimport nltk\nimport re\nfrom nltk.corpus import stopwords\nfrom gensim.models import Word2Vec\n\nnltk.download('stopwords')\n\n#for model training\n\nfrom keras.layers import Embedding, LSTM, Dense, Dropout, Lambda, Flatten\nfrom keras.models import Sequential, load_model, model_from_config\nimport keras.backend as K\n\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import cohen_kappa_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data = pd.read_csv(\"/kaggle/input/asap-aes/test_set.tsv\",sep='\\t', encoding='ISO-8859-1')\ntraining_data = pd.read_csv(\"/kaggle/input/asap-aes/training_set_rel3.tsv\",sep='\\t', encoding='ISO-8859-1',\n                            usecols = ['essay_id', 'essay_set', 'essay','domain1_score']).dropna(axis=1)\nvalid_data = pd.read_csv(\"/kaggle/input/asap-aes/valid_set.tsv\",sep='\\t', encoding='ISO-8859-1')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data.dropna(axis=1,inplace=True)\nvalid_data.dropna(axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training_data\ny = training_data['domain1_score']\nX = training_data.copy()\nX,y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def essay_to_wordlist(essay_v, remove_stopwords):\n    #Remove the tagged labels and word tokenize the sentence.\n    essay_v = re.sub(\"[^a-zA-Z]\", \" \", essay_v)\n    words = essay_v.lower().split()\n    if remove_stopwords:\n        stops = set(stopwords.words(\"english\"))\n        words = [w for w in words if not w in stops]\n    return (words)\n\ndef essay_to_sentences(essay_v, remove_stopwords):\n    \"\"\"Sentence tokenize the essay and call essay_to_wordlist() for word tokenization.\"\"\"\n    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n    raw_sentences = tokenizer.tokenize(essay_v.strip())\n    sentences = []\n    for raw_sentence in raw_sentences:\n        if len(raw_sentence) > 0:\n            sentences.append(essay_to_wordlist(raw_sentence, remove_stopwords))\n    return sentences\n\ndef makeFeatureVec(words, model, num_features):\n    \"\"\"Make ar from the words list of an Essay.\"\"\"\n    featureVec = np.zeros((num_features,),dtype=\"float32\")\n    num_words = 0.\n    index2word_set = set(model.wv.index2word)\n    for word in words:\n        if word in index2word_set:\n            num_words += 1\n            featureVec = np.add(featureVec,model[word])        \n    featureVec = np.divide(featureVec,num_words)\n    return featureVec\n\ndef getAvgFeatureVecs(essays, model, num_features):\n    \"\"\"Main function to generate the word vectors for word2vec model.\"\"\"\n    counter = 0\n    essayFeatureVecs = np.zeros((len(essays),num_features),dtype=\"float32\")\n    for essay in essays:\n        essayFeatureVecs[counter] = makeFeatureVec(essay, model, num_features)\n        counter = counter + 1\n    return essayFeatureVecs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.layers import Embedding, LSTM, Dense, Dropout, Lambda, Flatten\nfrom keras.models import Sequential, load_model, model_from_config\nimport keras.backend as K\n\ndef get_model():\n    \"\"\"Define the model.\"\"\"\n    model = Sequential()\n    model.add(LSTM(300, dropout=0.4, recurrent_dropout=0.4, input_shape=[1, 300], return_sequences=True))\n    model.add(LSTM(64, recurrent_dropout=0.4))\n    model.add(Dropout(0.5))\n    model.add(Dense(1, activation='relu'))\n\n    model.compile(loss='mean_squared_error', optimizer='rmsprop', metrics=['mae'])\n    model.summary()\n\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cv = KFold(n_splits = 5, shuffle = True)\nresults = []\ny_pred_list = []\n\ncount = 1\nfor traincv, testcv in cv.split(X):\n    print(\"\\n--------Fold {}--------\\n\".format(count))\n    X_test, X_train, y_test, y_train = X.iloc[testcv], X.iloc[traincv], y.iloc[testcv], y.iloc[traincv]\n    \n    train_essays = X_train['essay']\n    test_essays = X_test['essay']\n    \n    sentences = []\n    \n    for essay in train_essays:\n            # Obtaining all sentences from the training essays.\n            sentences += essay_to_sentences(essay, remove_stopwords = True)\n            \n    # Initializing variables for word2vec model.\n    num_features = 300\n    min_word_count = 40\n    num_workers = 8\n    context = 10\n    downsampling = 1e-3\n\n    print(\"Training Word2Vec Model...\")\n    model = Word2Vec(sentences, workers=num_workers, size=num_features, min_count = min_word_count, window = context, sample = downsampling)\n\n    model.init_sims(replace=True)\n    model.wv.save_word2vec_format('word2vecmodel.bin', binary=True)\n\n    clean_train_essays = []\n    \n    # Generate training and testing data word vectors.\n    for essay_v in train_essays:\n        clean_train_essays.append(essay_to_wordlist(essay_v, remove_stopwords=True))\n    trainDataVecs = getAvgFeatureVecs(clean_train_essays, model, num_features)\n    \n    clean_test_essays = []\n    for essay_v in test_essays:\n        clean_test_essays.append(essay_to_wordlist( essay_v, remove_stopwords=True ))\n    testDataVecs = getAvgFeatureVecs( clean_test_essays, model, num_features )\n    \n    trainDataVecs = np.array(trainDataVecs)\n    testDataVecs = np.array(testDataVecs)\n    # Reshaping train and test vectors to 3 dimensions. (1 represnts one timestep)\n    trainDataVecs = np.reshape(trainDataVecs, (trainDataVecs.shape[0], 1, trainDataVecs.shape[1]))\n    testDataVecs = np.reshape(testDataVecs, (testDataVecs.shape[0], 1, testDataVecs.shape[1]))\n    \n    lstm_model = get_model()\n    lstm_model.fit(trainDataVecs, y_train, batch_size=64, epochs=2)\n    #lstm_model.load_weights('./model_weights/final_lstm.h5')\n    y_pred = lstm_model.predict(testDataVecs)\n    \n    # Save any one of the 5 models.\n    if count == 5:\n         lstm_model.save_weights('final_lstm.h5')\n    \n    # Round y_pred to the nearest integer.\n    y_pred = np.around(y_pred)\n    \n    # Evaluate the model on the evaluation metric. \"Quadratic mean averaged Kappa\"\n    result = cohen_kappa_score(y_test.values,y_pred,weights='quadratic')\n    print(\"Kappa Score: {}\".format(result))\n    results.append(result)\n\n    count += 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Average Kappa score after a 5-fold cross validation: \",np.around(np.array(results).mean(),decimals=4))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}