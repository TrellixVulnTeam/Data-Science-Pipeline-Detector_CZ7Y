{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"In this sample notebook it is demonstrated how to apply Kalman Filter to improve the baseline slightly\n\nThe notebook is based on https://www.kaggle.com/jpmiller/baseline-from-host-data","metadata":{}},{"cell_type":"markdown","source":"## ensure you have everything you need","metadata":{}},{"cell_type":"code","source":"!pip install simdkalman","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-05-26T07:07:10.223364Z","iopub.execute_input":"2021-05-26T07:07:10.223732Z","iopub.status.idle":"2021-05-26T07:07:19.252048Z","shell.execute_reply.started":"2021-05-26T07:07:10.223695Z","shell.execute_reply":"2021-05-26T07:07:19.250902Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Please read the documentation if you would like to learn more about this implementation of kf: https://simdkalman.readthedocs.io/en/latest/","metadata":{}},{"cell_type":"code","source":"from pathlib import Path\nimport numpy as np\nimport pandas as pd\nimport simdkalman\nfrom tqdm.notebook import tqdm","metadata":{"execution":{"iopub.status.busy":"2021-05-26T07:07:19.254551Z","iopub.execute_input":"2021-05-26T07:07:19.255048Z","iopub.status.idle":"2021-05-26T07:07:19.268206Z","shell.execute_reply.started":"2021-05-26T07:07:19.254996Z","shell.execute_reply":"2021-05-26T07:07:19.267073Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## define kf model\n","metadata":{}},{"cell_type":"code","source":"T = 1.0\nstate_transition = np.array([[1, 0, T, 0, 0.5 * T ** 2, 0], [0, 1, 0, T, 0, 0.5 * T ** 2], [0, 0, 1, 0, T, 0],\n                             [0, 0, 0, 1, 0, T], [0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 1]])\nprocess_noise = np.diag([1e-5, 1e-5, 5e-6, 5e-6, 1e-6, 1e-6]) + np.ones((6, 6)) * 1e-9\nobservation_model = np.array([[1, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0]])\nobservation_noise = np.diag([5e-5, 5e-5]) + np.ones((2, 2)) * 1e-9\n\nkf = simdkalman.KalmanFilter(\n        state_transition = state_transition,\n        process_noise = process_noise,\n        observation_model = observation_model,\n        observation_noise = observation_noise)","metadata":{"execution":{"iopub.status.busy":"2021-05-26T07:07:19.270503Z","iopub.execute_input":"2021-05-26T07:07:19.271011Z","iopub.status.idle":"2021-05-26T07:07:19.285177Z","shell.execute_reply.started":"2021-05-26T07:07:19.270977Z","shell.execute_reply":"2021-05-26T07:07:19.283723Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def apply_kf_smoothing(df, kf_=kf):\n    unique_paths = df[['collectionName', 'phoneName']].drop_duplicates().to_numpy()\n    for collection, phone in tqdm(unique_paths):\n        cond = np.logical_and(df['collectionName'] == collection, df['phoneName'] == phone)\n        data = df[cond][['latDeg', 'lngDeg']].to_numpy()\n        data = data.reshape(1, len(data), 2)\n        smoothed = kf_.smooth(data)\n        df.loc[cond, 'latDeg'] = smoothed.states.mean[0, :, 0]\n        df.loc[cond, 'lngDeg'] = smoothed.states.mean[0, :, 1]\n    return df","metadata":{"execution":{"iopub.status.busy":"2021-05-26T07:07:19.287016Z","iopub.execute_input":"2021-05-26T07:07:19.287345Z","iopub.status.idle":"2021-05-26T07:07:19.297126Z","shell.execute_reply.started":"2021-05-26T07:07:19.287315Z","shell.execute_reply":"2021-05-26T07:07:19.296116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## evaluate train error","metadata":{}},{"cell_type":"code","source":"data_path = Path(\"../input/google-smartphone-decimeter-challenge\")\n\ntruths = (data_path / 'train').rglob('ground_truth.csv')\n    # returns a generator\n\ndf_list = []\ncols = ['collectionName', 'phoneName', 'millisSinceGpsEpoch', 'latDeg', 'lngDeg']\n\nfor t in tqdm(truths, total=73):\n    df_phone = pd.read_csv(t, usecols=cols)  \n    df_list.append(df_phone)\ndf_truth = pd.concat(df_list, ignore_index=True)\n\n#df_basepreds_kf = apply_kf_smoothing(pd.read_csv('../input/google-smartphone-decimeter-challenge/baseline_locations_train.csv', usecols=cols))\n#df_all = df_truth.merge(df_basepreds_kf, how='inner', on=cols[:3], suffixes=('_truth', '_basepred'))","metadata":{"execution":{"iopub.status.busy":"2021-05-26T07:07:19.298178Z","iopub.execute_input":"2021-05-26T07:07:19.298523Z","iopub.status.idle":"2021-05-26T07:08:11.229535Z","shell.execute_reply.started":"2021-05-26T07:07:19.298491Z","shell.execute_reply":"2021-05-26T07:08:11.2284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# simplified haversine distance\ndef calc_haversine(lat1, lon1, lat2, lon2):\n    \"\"\"Calculates the great circle distance between two points\n    on the earth. Inputs are array-like and specified in decimal degrees.\n    \"\"\"\n    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n    dlat = lat2 - lat1\n    dlon = lon2 - lon1\n    a = np.sin(dlat/2.0)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2.0)**2\n\n    c = 2 * np.arcsin(a**0.5)\n    dist = 6_367_000 * c\n    return dist","metadata":{"execution":{"iopub.status.busy":"2021-05-26T07:08:11.230887Z","iopub.execute_input":"2021-05-26T07:08:11.231223Z","iopub.status.idle":"2021-05-26T07:08:11.238883Z","shell.execute_reply.started":"2021-05-26T07:08:11.231189Z","shell.execute_reply":"2021-05-26T07:08:11.237539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_all['dist'] = calc_haversine(df_all.latDeg_truth, df_all.lngDeg_truth, \n    df_all.latDeg_basepred, df_all.lngDeg_basepred)\n\nprint(f'mean error on train dataset: {df_all.dist.mean():.3f}m - slightly better than the baseline, but still a lot of improvements are needed')\n","metadata":{"execution":{"iopub.status.busy":"2021-05-26T07:08:11.240357Z","iopub.execute_input":"2021-05-26T07:08:11.240845Z","iopub.status.idle":"2021-05-26T07:08:11.311042Z","shell.execute_reply.started":"2021-05-26T07:08:11.240801Z","shell.execute_reply":"2021-05-26T07:08:11.309976Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"res = df_all.dist.quantile([0.5, 0.95]).to_numpy()\nprint(f'score on train dataset: {0.5*(res[0] + res[1])}, 0.5: {res[0]}, 0.95: {res[1]}')\n#print(f'score on train dataset: {df_all.dist.mean():.3f}m - slightly better than the baseline, but still a lot of improvements are needed')\n","metadata":{"execution":{"iopub.status.busy":"2021-05-26T07:08:41.992121Z","iopub.execute_input":"2021-05-26T07:08:41.992512Z","iopub.status.idle":"2021-05-26T07:08:42.008398Z","shell.execute_reply.started":"2021-05-26T07:08:41.992474Z","shell.execute_reply":"2021-05-26T07:08:42.007217Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\ndef apply_kf_smoothing(df, i, kf_=kf):\n    unique_paths = df[['collectionName', 'phoneName']].drop_duplicates().to_numpy()\n    for collection, phone in tqdm(unique_paths):\n        cond = np.logical_and(df['collectionName'] == collection, df['phoneName'] == phone)\n        data = df[cond][['latDeg', 'lngDeg']].to_numpy()\n        data = data.reshape(1, len(data), 2)\n        smoothed = kf_.smooth(data)\n        df.loc[cond, 'latDeg'] = smoothed.states.mean[0, :, 0] - smoothed.states.mean[0, :, 2] * i / 100\n        df.loc[cond, 'lngDeg'] = smoothed.states.mean[0, :, 1] - smoothed.states.mean[0, :, 3] * i / 100\n    return df\n\nbest_score = 9999\n\nfor i in np.arange(100):\n    df_basepreds_kf = apply_kf_smoothing(pd.read_csv('../input/google-smartphone-decimeter-challenge/baseline_locations_train.csv', usecols=cols),\n                                         i=i)\n    df_all = df_truth.merge(df_basepreds_kf, how='inner', on=cols[:3], suffixes=('_truth', '_basepred'))\n    df_all['dist'] = calc_haversine(df_all.latDeg_truth, df_all.lngDeg_truth, \n        df_all.latDeg_basepred, df_all.lngDeg_basepred)\n    res = df_all.dist.quantile([0.5, 0.95]).to_numpy()\n    score = 0.5*(res[0] + res[1])\n    if score < best_score:\n        print(f'i: {i} score on train dataset: {score}, 0.5: {res[0]}, 0.95: {res[1]}')\n    #print(f'score on train dataset: {df_all.dist.mean():.3f}m - slightly better than the baseline, but still a lot of improvements are needed')\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2021-05-26T07:11:29.680282Z","iopub.execute_input":"2021-05-26T07:11:29.680713Z","iopub.status.idle":"2021-05-26T07:25:00.090473Z","shell.execute_reply.started":"2021-05-26T07:11:29.680676Z","shell.execute_reply":"2021-05-26T07:25:00.088238Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\ndef apply_kf_smoothing(df, i, kf_=kf):\n    unique_paths = df[['collectionName', 'phoneName']].drop_duplicates().to_numpy()\n    for collection, phone in tqdm(unique_paths):\n        cond = np.logical_and(df['collectionName'] == collection, df['phoneName'] == phone)\n        data = df[cond][['latDeg', 'lngDeg']].to_numpy()\n        data = data.reshape(1, len(data), 2)\n        smoothed = kf_.smooth(data)\n        df.loc[cond, 'latDeg'] = smoothed.states.mean[0, :, 0] - smoothed.states.mean[0, :, 2] * (0.01 + 0.2 * i / 100)\n        df.loc[cond, 'lngDeg'] = smoothed.states.mean[0, :, 1] - smoothed.states.mean[0, :, 3] * (0.01 + 0.2 * i / 100)\n    return df\n\nbest_score = 9999\n\nfor i in np.arange(100):\n    df_basepreds_kf = apply_kf_smoothing(pd.read_csv('../input/google-smartphone-decimeter-challenge/baseline_locations_train.csv', usecols=cols),\n                                         i=i)\n    df_all = df_truth.merge(df_basepreds_kf, how='inner', on=cols[:3], suffixes=('_truth', '_basepred'))\n    df_all['dist'] = calc_haversine(df_all.latDeg_truth, df_all.lngDeg_truth, \n        df_all.latDeg_basepred, df_all.lngDeg_basepred)\n    res = df_all.dist.quantile([0.5, 0.95]).to_numpy()\n    score = 0.5*(res[0] + res[1])\n    if score < best_score:\n        print(f'i: {i} score on train dataset: {score}, 0.5: {res[0]}, 0.95: {res[1]}')\n    #print(f'score on train dataset: {df_all.dist.mean():.3f}m - slightly better than the baseline, but still a lot of improvements are needed')\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2021-05-26T07:25:54.675084Z","iopub.execute_input":"2021-05-26T07:25:54.675449Z","iopub.status.idle":"2021-05-26T07:30:07.115363Z","shell.execute_reply.started":"2021-05-26T07:25:54.675419Z","shell.execute_reply":"2021-05-26T07:30:07.112394Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\ndef apply_kf_smoothing(df, i, kf_=kf):\n    unique_paths = df[['collectionName', 'phoneName']].drop_duplicates().to_numpy()\n    for collection, phone in tqdm(unique_paths):\n        cond = np.logical_and(df['collectionName'] == collection, df['phoneName'] == phone)\n        data = df[cond][['latDeg', 'lngDeg']].to_numpy()\n        data = data.reshape(1, len(data), 2)\n        smoothed = kf_.smooth(data)\n        df.loc[cond, 'latDeg'] = smoothed.states.mean[0, :, 0] - smoothed.states.mean[0, :, 2] * (0.012 + 0.05 * i / 100)\n        df.loc[cond, 'lngDeg'] = smoothed.states.mean[0, :, 1] - smoothed.states.mean[0, :, 3] * (0.012 + 0.05 * i / 100)\n    return df\n\nbest_score = 9999\n\nfor i in np.arange(100):\n    df_basepreds_kf = apply_kf_smoothing(pd.read_csv('../input/google-smartphone-decimeter-challenge/baseline_locations_train.csv', usecols=cols),\n                                         i=i)\n    df_all = df_truth.merge(df_basepreds_kf, how='inner', on=cols[:3], suffixes=('_truth', '_basepred'))\n    df_all['dist'] = calc_haversine(df_all.latDeg_truth, df_all.lngDeg_truth, \n        df_all.latDeg_basepred, df_all.lngDeg_basepred)\n    res = df_all.dist.quantile([0.5, 0.95]).to_numpy()\n    score = 0.5*(res[0] + res[1])\n    if score < best_score:\n        print(f'i: {i} score on train dataset: {score}, 0.5: {res[0]}, 0.95: {res[1]}')\n    #print(f'score on train dataset: {df_all.dist.mean():.3f}m - slightly better than the baseline, but still a lot of improvements are needed')\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2021-05-26T07:31:27.747467Z","iopub.execute_input":"2021-05-26T07:31:27.747829Z","iopub.status.idle":"2021-05-26T07:37:57.383113Z","shell.execute_reply.started":"2021-05-26T07:31:27.747798Z","shell.execute_reply":"2021-05-26T07:37:57.380378Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\n#best_score = 9999\ndef apply_kf_smoothing(df, i=0, kf_=kf):\n    d01={'Mi8': 1,\n  'Pixel4XL': 1,\n  'Pixel4': 1,\n  'Pixel4XLModded': 2,\n  'Pixel5': 3,\n    'SamsungS20Ultra': 8,\n    'Pixel4Modded': 0}\n    d02={'Mi8': 18,\n  'Pixel4XL': 21,\n  'Pixel4': 27,\n  'Pixel4XLModded': 26,\n  'Pixel5': 9,\n    'SamsungS20Ultra': 6,\n    'Pixel4Modded': 18}\n    unique_paths = df[['collectionName', 'phoneName']].drop_duplicates().to_numpy()\n    for collection, phone in tqdm(unique_paths):\n        cond = np.logical_and(df['collectionName'] == collection, df['phoneName'] == phone)\n        data = df[cond][['latDeg', 'lngDeg']].to_numpy()\n        data = data.reshape(1, len(data), 2)\n        smoothed = kf_.smooth(data)\n        df.loc[cond, 'latDeg'] = smoothed.states.mean[0, :, 0] - smoothed.states.mean[0, :, 2] * (d01[phone] / 100 - 0.01 + d02[phone]/2000)\n        df.loc[cond, 'lngDeg'] = smoothed.states.mean[0, :, 1] - smoothed.states.mean[0, :, 3] * (d01[phone] / 100 - 0.01 + d02[phone]/2000)\n    return df\n\nbest_score={'Mi8': 9991,\n'Pixel4XL': 9991,\n'Pixel4': 9991,\n'Pixel4XLModded': 9992,\n'Pixel5': 9993,\n'SamsungS20Ultra': 9998,\n'Pixel4Modded': 9990}\nfor i in [0]:#np.arange(40):\n    df_basepreds_kf = apply_kf_smoothing(pd.read_csv('../input/google-smartphone-decimeter-challenge/baseline_locations_train.csv', usecols=cols),\n                                         i=i)\n    df_all = df_truth.merge(df_basepreds_kf, how='inner', on=cols[:3], suffixes=('_truth', '_basepred'))\n    df_all['dist'] = calc_haversine(df_all.latDeg_truth, df_all.lngDeg_truth, \n        df_all.latDeg_basepred, df_all.lngDeg_basepred)\n    for phone in df_all['phoneName'].unique():\n        df_phone = df_all[df_all['phoneName'] == phone]\n        res = df_phone.dist.quantile([0.5, 0.95]).to_numpy()\n        score = 0.5*(res[0] + res[1])\n        if score < best_score[phone]:\n            best_score[phone] = score\n            print(f'i: {i} score on phone {phone} train sub-dataset: {score}, 0.5: {res[0]}, 0.95: {res[1]}')\n        #print(f'score on train dataset: {df_all.dist.mean():.3f}m - slightly better than the baseline, but still a lot of improvements are needed')\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2021-05-26T08:31:22.148618Z","iopub.execute_input":"2021-05-26T08:31:22.148989Z","iopub.status.idle":"2021-05-26T09:04:34.583535Z","shell.execute_reply.started":"2021-05-26T08:31:22.148958Z","shell.execute_reply":"2021-05-26T09:04:34.582698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"#best_score = 9999\ndef apply_kf_smoothing(df, i, kf_=kf):\n    unique_paths = df[['collectionName', 'phoneName']].drop_duplicates().to_numpy()\n    for collection, phone in tqdm(unique_paths):\n        cond = np.logical_and(df['collectionName'] == collection, df['phoneName'] == phone)\n        data = df[cond][['latDeg', 'lngDeg']].to_numpy()\n        data = data.reshape(1, len(data), 2)\n        smoothed = kf_.smooth(data)\n        df.loc[cond, 'latDeg'] = smoothed.states.mean[0, :, 0] - smoothed.states.mean[0, :, 2] * (i / 100)\n        df.loc[cond, 'lngDeg'] = smoothed.states.mean[0, :, 1] - smoothed.states.mean[0, :, 3] * (i / 100)\n    return df\n\nfor i in np.arange(100):\n    df_basepreds_kf = apply_kf_smoothing(pd.read_csv('../input/google-smartphone-decimeter-challenge/baseline_locations_train.csv', usecols=cols),\n                                         i=i)\n    df_all = df_truth.merge(df_basepreds_kf, how='inner', on=cols[:3], suffixes=('_truth', '_basepred'))\n    df_all['dist'] = calc_haversine(df_all.latDeg_truth, df_all.lngDeg_truth, \n        df_all.latDeg_basepred, df_all.lngDeg_basepred)\n    for collection in df_all['collectionName'].unique():\n        df_collection = df_all[df_all['collectionName'] == collection]\n        res = df_collection.dist.quantile([0.5, 0.95]).to_numpy()\n        score = 0.5*(res[0] + res[1])\n        #if score < best_score:\n        print(f'i: {i} score on collection {collection} train sub-dataset: {score}, 0.5: {res[0]}, 0.95: {res[1]}')\n        #print(f'score on train dataset: {df_all.dist.mean():.3f}m - slightly better than the baseline, but still a lot of improvements are needed')\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2021-05-26T08:00:21.75885Z","iopub.execute_input":"2021-05-26T08:00:21.75939Z","iopub.status.idle":"2021-05-26T08:14:15.477035Z","shell.execute_reply.started":"2021-05-26T08:00:21.759345Z","shell.execute_reply":"2021-05-26T08:14:15.473847Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_all.head()","metadata":{"execution":{"iopub.status.busy":"2021-05-26T07:59:43.737298Z","iopub.execute_input":"2021-05-26T07:59:43.737627Z","iopub.status.idle":"2021-05-26T07:59:43.754475Z","shell.execute_reply.started":"2021-05-26T07:59:43.737599Z","shell.execute_reply":"2021-05-26T07:59:43.753397Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## prepare a submission based on the sample submission","metadata":{}},{"cell_type":"code","source":"\n#best_score = 9999\ndef apply_kf_smoothing(df, i=0, kf_=kf):\n    d01={'Mi8': 1,\n  'Pixel4XL': 1,\n  'Pixel4': 1,\n  'Pixel4XLModded': 2,\n  'Pixel5': 3,\n    'SamsungS20Ultra': 8,\n    'Pixel4Modded': 0}\n    d02={'Mi8': 18,\n  'Pixel4XL': 21,\n  'Pixel4': 27,\n  'Pixel4XLModded': 26,\n  'Pixel5': 9,\n    'SamsungS20Ultra': 6,\n    'Pixel4Modded': 18}\n    unique_paths = df[['collectionName', 'phoneName']].drop_duplicates().to_numpy()\n    for collection, phone in tqdm(unique_paths):\n        cond = np.logical_and(df['collectionName'] == collection, df['phoneName'] == phone)\n        data = df[cond][['latDeg', 'lngDeg']].to_numpy()\n        data = data.reshape(1, len(data), 2)\n        smoothed = kf_.smooth(data)\n        df.loc[cond, 'latDeg'] = smoothed.states.mean[0, :, 0] - smoothed.states.mean[0, :, 2] * (d01[phone] / 100 - 0.01 + d02[phone]/2000)\n        df.loc[cond, 'lngDeg'] = smoothed.states.mean[0, :, 1] - smoothed.states.mean[0, :, 3] * (d01[phone] / 100 - 0.01 + d02[phone]/2000)\n    return df\n\nbest_score={'Mi8': 9991,\n'Pixel4XL': 9991,\n'Pixel4': 9991,\n'Pixel4XLModded': 9992,\n'Pixel5': 9993,\n'SamsungS20Ultra': 9998,\n'Pixel4Modded': 9990}\nfor i in [0]:#np.arange(40):\n    df_basepreds_kf = apply_kf_smoothing(pd.read_csv('../input/google-smartphone-decimeter-challenge/baseline_locations_train.csv', usecols=cols),\n                                         i=i)\n    df_all = df_truth.merge(df_basepreds_kf, how='inner', on=cols[:3], suffixes=('_truth', '_basepred'))\n    df_all['dist'] = calc_haversine(df_all.latDeg_truth, df_all.lngDeg_truth, \n        df_all.latDeg_basepred, df_all.lngDeg_basepred)\n    for phone in df_all['phoneName'].unique():\n        df_phone = df_all[df_all['phoneName'] == phone]\n        res = df_phone.dist.quantile([0.5, 0.95]).to_numpy()\n        score = 0.5*(res[0] + res[1])\n        if score < best_score[phone]:\n            best_score[phone] = score\n            print(f'i: {i} score on phone {phone} train sub-dataset: {score}, 0.5: {res[0]}, 0.95: {res[1]}')\n        #print(f'score on train dataset: {df_all.dist.mean():.3f}m - slightly better than the baseline, but still a lot of improvements are needed')\n","metadata":{"execution":{"iopub.status.busy":"2021-05-26T09:17:40.30496Z","iopub.execute_input":"2021-05-26T09:17:40.305633Z","iopub.status.idle":"2021-05-26T09:18:29.881993Z","shell.execute_reply.started":"2021-05-26T09:17:40.305582Z","shell.execute_reply":"2021-05-26T09:18:29.880936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"res = df_all.dist.quantile([0.5, 0.95]).to_numpy()\nscore = 0.5*(res[0] + res[1])\nprint(f'i: {i} score on the train dataset: {score}, 0.5: {res[0]}, 0.95: {res[1]}')","metadata":{"execution":{"iopub.status.busy":"2021-05-26T09:19:04.136604Z","iopub.execute_input":"2021-05-26T09:19:04.137015Z","iopub.status.idle":"2021-05-26T09:19:04.146812Z","shell.execute_reply.started":"2021-05-26T09:19:04.136977Z","shell.execute_reply":"2021-05-26T09:19:04.145579Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_base = pd.read_csv(\n    '../input/google-smartphone-decimeter-challenge/baseline_locations_test.csv')\n\nsub = pd.read_csv('../input/google-smartphone-decimeter-challenge/sample_submission.csv')\n\nkf_smoothed_baseline = apply_kf_smoothing(test_base)\nsub = sub.assign(\n    latDeg = kf_smoothed_baseline.latDeg,\n    lngDeg = kf_smoothed_baseline.lngDeg\n)\nsub.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-05-26T09:19:08.677203Z","iopub.execute_input":"2021-05-26T09:19:08.677659Z","iopub.status.idle":"2021-05-26T09:19:43.467545Z","shell.execute_reply.started":"2021-05-26T09:19:08.677622Z","shell.execute_reply":"2021-05-26T09:19:43.466513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}