{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n#!pip install tensorflow_datasets\n\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# import tensorflow as tf\n# import tensorflow_datasets as tfds\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n\nimport re # regular expressions\nfrom nltk.corpus import stopwords","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data_train_file = \"../input/random-acts-of-pizza/train.json\"\ndata_test_file = \"../input/random-acts-of-pizza/test.json\"\n\ndf_train = pd.read_json(data_train_file)\ndf_test = pd.read_json(data_test_file)\n\n# Number of rows of train and test datasets\ntrain_rows = df_train.shape[0]\ntest_rows = df_test.shape[0]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pizza_distributed = 0\nfor i in range(df_train['requester_received_pizza'].count()):\n    if df_train['requester_received_pizza'][i] == True:\n        pizza_distributed += 1\n\nbaseline_val = pizza_distributed/ train_rows\n\nprint(baseline_val)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using 0.25 as the predicted probability for the baseline (reflecting what we know about the original distribution of classes in our training dataset)."},{"metadata":{},"cell_type":"markdown","source":"1. Baseline\n2. Log loss\n3. Train"},{"metadata":{"trusted":true},"cell_type":"code","source":"def log_loss(Y_true, Y_pred):\n  \"\"\"Returns the binary log loss for a list of labels and predictions.\n  \n  Args:\n    Y_true: A list of (true) labels (0 or 1)\n    Y_pred: A list of corresponding predicted probabilities\n\n  Returns:\n    Binary log loss\n  \"\"\"\n  return -(Y_true * (np.log(Y_pred)) + (1 - Y_true) * (np.log(1 - Y_pred))).mean()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np_train_val = df_train['requester_received_pizza'].astype(int).to_numpy()\n\nbaseline_val_train = np.full((train_rows),baseline_val)\n\ntrain_log_loss = log_loss(np_train_val, baseline_val_train)\n\nprint(train_log_loss)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_submission = df_test[['request_id']].copy()\n\nbaseline_val_test = np.full((test_rows),baseline_val)\ndf_submission['requester_received_pizza'] = baseline_val_test\n\ndf_submission.to_csv('/kaggle/working/submission1.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Embedding the text to vectors"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_train_file = \"../input/random-acts-of-pizza/train.json\"\ndata_test_file = \"../input/random-acts-of-pizza/test.json\"\n\ndf_train = pd.read_json(data_train_file)\ndf_test = pd.read_json(data_test_file)\n\n# Number of rows of train and test datasets\ntrain_rows = df_train.shape[0]\ntest_rows = df_test.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def review_wordlist(review, remove_stopwords=False):\n    # 2. Removing non-letter.\n    review_text = re.sub(\"[^a-zA-Z]\",\" \",review_text)\n    # 3. Converting to lower case and splitting\n    words = review_text.lower().split()\n    # 4. Optionally remove stopwords\n    if remove_stopwords:\n        stops = set(stopwords.words(\"english\"))     \n        words = [w for w in words if not w in stops]\n    \n    return(words)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# word2vec expects a list of lists.\n# Using punkt tokenizer for better splitting of a paragraph into sentences.\n\nimport nltk.data\n#nltk.download('popular')\n\ntokenizer = nltk.data.load('tokenizers/punkt/english.pickle')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}