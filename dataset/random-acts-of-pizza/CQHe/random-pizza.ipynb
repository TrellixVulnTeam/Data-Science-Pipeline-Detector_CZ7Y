{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1ff2f397-90df-a1e3-f404-40875820d3aa"},"outputs":[],"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport pandas as pd\nimport numpy as np\n\n\n# SK-learn libraries for learning.\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.grid_search import GridSearchCV\n\n# SK-learn libraries for evaluation.\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn import metrics\nfrom sklearn.metrics import classification_report\n\n# SK-learn libraries for feature extraction from text.\nfrom sklearn.feature_extraction.text import *\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\npd_train = pd.read_json('../input/train.json', orient='columns')\npd_test = pd.read_json('../input/test.json', orient='columns')\n\n\nnp_test = np.array(pd_test)\nnp_train = np.array(pd_train)\n\nprint(np_train.shape)\n\n\nX = np_train[:,6]\nY = np_train[:,22]\nshuffle = np.random.permutation(np.arange(X.shape[0]))\nX, Y = X[shuffle], Y[shuffle]\n\nprint('data shape: ', X.shape)\nprint('label shape:', Y.shape)\n\nl=len(X)\ntrain_data, train_labels = X[:l/2], Y[:l/2]\ndev_data, dev_labels = X[l/2:(3*l)/4], Y[l/2:(3*l)/4]\ntest_data, test_labels = X[(3*l)/4:], Y[(3*l)/4:]\n\n# Any results you write to the current directory are saved as output."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"146c47ca-74dd-fe3f-363e-57c9de035657"},"outputs":[],"source":"#Run initial vectorizer and fit_transform on train_data and find vocab size from shape attribute.\nvect=CountVectorizer()\ndata=vect.fit_transform(train_data).toarray()\ndevdata=vect.transform(dev_data).toarray()\n\n\n#Use np.where to binarize train and dev set where values above and below 0.5.\nb=train_labels\ntrainlabels=np.where(b==True, 1, 0)\n\nbl=dev_labels\ndevlabels=np.where(bl==True, 1, 0)\n\nb2=test_labels\ntestlabels=np.where(b2==True, 1, 0)\n\ncategories = ['Got Pizza', 'Didn\\'t get pizza']\n\nprint('Baseline Scores...')\n#Run MultinomialNB Classifier\n# mnb_clf = Pipeline([('vect', CountVectorizer()), ('mnclf',MultinomialNB(alpha=0.01))])\n# mnb_clf = mnb_clf.fit(train_data, trainlabels)\n# pred = mnb_clf.predict(dev_data)\n# score1=metrics.accuracy_score(devlabels,pred)\n# print 'Naive Bayes Score:',score1\nbest_nb = []\nalphas = [0.0, 0.0001, 0.001, 0.01, 0.1, 0.5, 1.0, 2.0, 10.0]\nfor k in range(len(alphas)):\n    mnb_clf = Pipeline([('vect', CountVectorizer()), ('mnclf', MultinomialNB(alpha=alphas[k]))])\n    mnb_clf = mnb_clf.fit(train_data, trainlabels)\n    pred = mnb_clf.predict(dev_data)\n    metrics.accuracy_score(devlabels,pred)\n    best_nb.append(metrics.accuracy_score(devlabels,pred))\nbestAlphaAccuracy = max(best_nb)\nbestAlphaValue = alphas[best_nb.index(bestAlphaAccuracy)]\nprint('Naive Bayes Baseline:')\nprint('Best Alpha =', bestAlphaValue, ' accuracy:', bestAlphaAccuracy)\nprint('')\n\n\n\n#Run Logistic Regression classifier\nlog_clf = Pipeline([('vect', CountVectorizer()),('lgclf', LogisticRegression(C=0.5))])\nlog_clf = log_clf.fit(train_data, trainlabels) \npred = log_clf.predict(dev_data)        \nscore2= metrics.accuracy_score(devlabels,pred)\n#print 'Logistic Regression Score:',score2\nbest_logit = []\nC = [0.0001, 0.001, 0.01, 0.1, 0.5, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\nfor k in range(len(C)):\n    log_clf = Pipeline([('vect', CountVectorizer()),\n                     ('lgclf', LogisticRegression(C=C[k]))]);\n    log_clf = log_clf.fit(train_data, trainlabels)\n    pred = log_clf.predict(dev_data)\n    metrics.accuracy_score(devlabels,pred)\n    best_logit.append(metrics.accuracy_score(devlabels,pred))\n    weights = log_clf.named_steps['lgclf'].coef_\nbestCAccuracy = max(best_logit)\nbestCValue = C[best_logit.index(bestCAccuracy)]\nprint('Logistic Regression Baseline:')\nprint('Best C =', bestCValue, ' accuracy:', bestCAccuracy)\nprint('')"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}