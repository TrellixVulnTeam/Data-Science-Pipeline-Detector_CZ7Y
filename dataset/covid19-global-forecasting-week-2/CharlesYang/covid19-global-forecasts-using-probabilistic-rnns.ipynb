{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# install gluonts package\n!pip install gluonts\n!pip install sentence_transformers","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Preprocessing\n\n>1. take difference of data\n>2. fill 'NaN' Province/State values with Country/Region values\n>3. apply log transformation to target values"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# load and clean data\nimport pandas as pd\nimport numpy as np\n\ntrain_all = pd.read_csv(\"../input/covid19-global-forecasting-week-2/train.csv\")\nLOG_TRANSFORM = False\n\ndef preprocess(\n    df: pd.DataFrame,\n    log_transform: bool = LOG_TRANSFORM\n):\n    \n    # set index\n    df = df.set_index('Date')\n\n    # fill 'NaN' Province/State values with Country/Region values\n    df['Province_State'] = df['Province_State'].fillna(df['Country_Region'])\n\n    # take difference of fatalities and cases\n    df[['ConfirmedCases', 'Fatalities']] = df[['ConfirmedCases', 'Fatalities']].diff()\n    df = df.fillna(0)\n    \n    df._get_numeric_data()[df._get_numeric_data() < 0] = 0\n    assert df.isnull().sum().all() == 0\n    \n    # convert target values to log scale\n    if log_transform:\n        df[['ConfirmedCases', 'Fatalities']] = np.log1p(\n            df[['ConfirmedCases', 'Fatalities']].values\n    )\n    \n    return df\n\ndef split(\n    df: pd.DataFrame, \n    date: str = '2020-03-19', \n):\n\n    train = df.loc[df.index < date] \n    test = df.loc[df.index >= date]\n    return train, test\n\ntrain_all = preprocess(train_all)\n# drop early data -> bias the dataset more toward recent trajectories\n_, train_all = split(train_all, date = '2020-02-01') \ntrain, test = split(train_all)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Exploration"},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot confirmed cases and fatalities in train\nimport matplotlib.pyplot as plt\nfrom gluonts.dataset.util import to_pandas\nfrom gluonts.dataset.common import ListDataset\n\ndef plot_observations(\n    target: str = 'ConfirmedCases',\n    cumulative: bool = False,\n    log_transform: bool = LOG_TRANSFORM\n):\n    fig = plt.figure(figsize=(15, 6.1), facecolor=\"white\",  edgecolor='k')\n    \n    local_train = train.copy()\n    local_test = test.copy()\n    if log_transform:\n        local_train[['ConfirmedCases', 'Fatalities']] = np.expm1(\n            local_train[['ConfirmedCases', 'Fatalities']].values\n        )\n        local_test[['ConfirmedCases', 'Fatalities']] = np.expm1(\n            local_test[['ConfirmedCases', 'Fatalities']].values\n        )\n    \n    if cumulative:\n        cum_train = local_train.groupby(['Province_State', 'Country_Region'])[['ConfirmedCases', 'Fatalities']].cumsum()\n        cum_train = cum_train.groupby('Date').sum()\n        cum_test = local_test.groupby(['Province_State', 'Country_Region'])[['ConfirmedCases', 'Fatalities']].cumsum()\n        cum_test = cum_test.groupby('Date').sum() + cum_train.tail(1).values\n    else:\n        cum_train = local_train.groupby('Date').sum()\n        cum_test = local_test.groupby('Date').sum()\n\n    train_ds = ListDataset(\n        [{\"start\": cum_train.index[0], \"target\": cum_train[target].values}],\n        freq = \"D\",\n    )\n    test_ds = ListDataset(\n        [{\"start\": cum_test.index[0], \"target\": cum_test[target].values}],\n        freq = \"D\",\n    )\n    \n    for tr, te in zip(train_ds, test_ds):\n        tr = to_pandas(tr)\n        te = to_pandas(te)\n        tr.plot(linewidth=2, label = f'train {target}')\n        tr[-1:].append(te).plot(linewidth=2, label = f'test {target}')\n    \n    plt.axvline(cum_train.index[-1], color='purple') # end of train dataset\n    type_string = 'Cumulative' if cumulative else 'Daily'\n    plt.title(f'{type_string} number of {target} globally', fontsize=16)\n    plt.legend(fontsize=16)\n    plt.grid(which=\"both\")\n    plt.show()\n    \nplot_observations('ConfirmedCases')\nplot_observations('Fatalities')\nplot_observations('ConfirmedCases', cumulative = True)\nplot_observations('Fatalities', cumulative = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Augmentation\n\nCollecting complete datasets for all regions in this dataset is difficult and time-consuming. The complexity of dealing with missing values and incomplete data entries makes merging multiple datasets for data augmentation difficult for this competition. To encode some knowledge about the world to our model, we use [BERT](https://arxiv.org/abs/1810.04805), a natural language processing model. Specifically, we use the [sentence embeddings](https://github.com/UKPLab/sentence-transformers) obtained from this model as a continuous-valued latent vector that represent the world. This has the advantage of encoding a very large corpus of textual data about the world, with no need for model training and with continuous features for all regions."},{"metadata":{"trusted":true},"cell_type":"code","source":"places = []\nfor idx,row in train.iterrows():\n    if row['Province_State']!=row['Country_Region']:\n        places.append(row['Province_State']+\", \"+row[\"Country_Region\"])\n    else:\n        places.append(row['Country_Region'])\nplaces = np.unique(places)\nprint(len(np.unique(places)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sentence_transformers import SentenceTransformer\nmodel = SentenceTransformer('bert-base-nli-mean-tokens')\ncountry_embeddings = model.encode(list(places))\nembedding_dim = len(country_embeddings[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"province_state = [p.split(',')[0] if p!=\"Korea, South\" else p for p in places] #error catching\ncountry_region = [p.split(',')[1][1:] if (len(p.split(','))==2 and p!='Korea, South') else p for p in places]\nembed_df = pd.DataFrame(np.concatenate([np.array(province_state).reshape(-1,1),np.array(country_region).reshape(-1,1),country_embeddings],axis=1))\nembed_df.columns=['Province_State','Country_Region']+list(range(embedding_dim))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualize with sklearn t-sne\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\ntsne = TSNE(n_components=2)\ntsne_embed = tsne.fit_transform(country_embeddings)\nfor i in range(len(places))[:50]:\n    plt.scatter(tsne_embed[i,0],tsne_embed[i,1],label=country_region[i])\nplt.legend();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import plotly.graph_objects as go\ntsne = TSNE(n_components=3)\ntsne_embed = tsne.fit_transform(country_embeddings)\nfig = go.Figure(data=[go.Scatter3d(\n    x=tsne_embed[:,0],\n    y=tsne_embed[:,1],\n    z=tsne_embed[:,2],\n    mode='markers',\n    marker=dict(\n        size=10,\n        opacity=0.9\n    )\n)])\n\n# tight layout\nfig.update_layout(margin=dict(l=0, r=0, b=0, t=0))\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import OrdinalEncoder\n\n\ndef join(\n    df: pd.DataFrame,\n    embed_df: pd.DataFrame\n):\n    \n    # join, delete merge columns\n    new_df = df.reset_index().merge(\n        embed_df,\n        left_on = [\"Province_State\",'Country_Region'],\n        right_on = [\"Province_State\",'Country_Region'],\n        how = 'left'\n    ).set_index('Date')\n    \n    # replace columns that weren't matched in join with mean\n    #new_df = new_df.fillna(new_df.mean())\n    #make sure no NaN in dataframe\n    assert new_df.isnull().sum().sum()==0\n    return new_df\n\ndef encode(\n    df: pd.DataFrame\n):\n    \"\"\" encode 'Province_State' and 'Country_Region' categorical variables as numerical ordinals\"\"\"\n    \n    enc = OrdinalEncoder()\n    df[['Province_State', 'Country_Region']] = enc.fit_transform(\n        df[['Province_State', 'Country_Region']].values\n    )\n    return df, enc\n\njoin_df = join(train_all, embed_df)\nall_df, enc = encode(join_df)\ntrain_df, test_df = split(all_df)\n_, val_df = split(all_df, date = '2020-02-28')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from gluonts.dataset.common import ListDataset\nfrom gluonts.dataset.field_names import FieldName\nimport typing\n\nREAL_VARS = list(range(embedding_dim))\n\ndef build_dataset(\n    frame: pd.DataFrame,\n    target: str = 'Fatalities',\n    cat_vars: typing.List[str] = ['Province_State', 'Country_Region'],\n    real_vars: typing.List[int] = REAL_VARS\n):\n    return ListDataset(\n        [\n            {\n                FieldName.START: df.index[0], \n                FieldName.TARGET: df[target].values,\n                #FieldName.FEAT_STATIC_CAT: df[cat_vars].values[0],\n                FieldName.FEAT_STATIC_REAL: df[real_vars].values[0]\n            }\n            for g, df in frame.groupby(by=['Province_State', 'Country_Region'])\n        ],\n        freq = \"D\",\n    )\n\ntraining_data_fatalities = build_dataset(train_df)\ntraining_data_cases = build_dataset(train_df, target = 'ConfirmedCases')\ntraining_data_fatalities_all = build_dataset(all_df)\ntraining_data_cases_all = build_dataset(all_df, target = 'ConfirmedCases')\nval_data_fatalities = build_dataset(val_df)\nval_data_cases = build_dataset(val_df, target = 'ConfirmedCases')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Fit DeepAR Model Estimates\n\nThe DeepAR model was proposed by David Salinas, Valentin Flunkert, and Jan Gasthaus in \"DeepAR: Probabilistic Forecasting with Autoregressive Recurrent Networks\" (https://arxiv.org/abs/1704.04110). The approach trains an autoregressive RNN to produces time-variant parameters of a specified distribution on a large collection of related time series. The learned distribution can then be used to produce probabilistic forecasts. Here we use the authors' *GluonTS* implementation (https://gluon-ts.mxnet.io/index.html).\n\nWe believe the probabilistic nature of the DeepAR forecasts is a feature that differentiates our approach from others we have seen so far. Specifically, the ability to provide both confidence intervals and point estimates allows one to better understand the range of possible trajectories, from the worst-case scenario, to the best-case scenario, to the expected scenario. "},{"metadata":{"trusted":true},"cell_type":"code","source":"from gluonts.model.deepar import DeepAREstimator\nfrom gluonts.trainer import Trainer\nfrom gluonts.distribution import NegativeBinomialOutput\nimport mxnet as mx\nimport numpy as np\n\n# set random seeds for reproducibility\nmx.random.seed(0)\nnp.random.seed(0)\n\ndef fit(\n    training_data: ListDataset,\n    validation_data: ListDataset = None,\n    pred_length: int = 12,\n    epochs: int = 15,\n    weight_decay: float = 5e-5,\n    log_preds: bool = LOG_TRANSFORM,\n):\n    estimator = DeepAREstimator(\n        freq=\"D\", \n        prediction_length=pred_length,\n        context_length=pred_length//2,\n        use_feat_static_cat = False,\n        use_feat_static_real = True,\n        #cardinality = [train['Province_State'].nunique(), train['Country_Region'].nunique()],\n        distr_output=NegativeBinomialOutput(),\n        trainer=Trainer(\n            epochs=epochs,\n            learning_rate=0.001, \n            batch_size=64,\n            weight_decay=weight_decay\n        ),\n    )\n    _, trained_net, predictor = estimator.train_model(\n        training_data = training_data, \n        validation_data = validation_data\n    )\n    \n    return predictor, trained_net\n\npredictor_fatalities, net = fit(training_data_fatalities, val_data_fatalities)\npredictor_cases, case_net = fit(training_data_cases, val_data_cases)\npredictor_fatalities_all, all_net = fit(training_data_fatalities_all, pred_length=31)\npredictor_cases_all, all_case_net = fit(training_data_cases_all, pred_length=31)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Plot predictions from fit model parameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"from gluonts.dataset.util import to_pandas\nimport matplotlib.pyplot as plt\nfrom typing import List\n\n## make it run sorted_samples code again!\ndef plot_forecast(\n    predictor,\n    train_df: pd.DataFrame,\n    location: List[str] = ['Italy', 'Italy'],\n    target: str = 'Fatalities',\n    #cat_vars: typing.List[str] = ['Province_State', 'Country_Region'],\n    real_vars: typing.List[int] = REAL_VARS,\n    cumulative: bool = True,\n    log_preds: bool = LOG_TRANSFORM,\n    show_gt: bool = True,\n    start_offset: int = 0, \n    fontsize: int = 16,\n    save: bool = False\n):\n    fig = plt.figure(figsize=(15, 6.1), facecolor=\"white\",  edgecolor='k')\n\n    # plot train observations, true observations from public test set, and forecasts\n    location_tr = enc.transform(np.array(location).reshape(1,-1))\n    tr_df = train_df[np.all((train_df[['Province_State', 'Country_Region']].values == location_tr), axis=1)]\n\n    train_obs = ListDataset(\n        [{\n            FieldName.START: tr_df.index[0], \n            FieldName.TARGET: tr_df[target].values,\n            FieldName.FEAT_STATIC_REAL: real_vars,\n            #FieldName.FEAT_STATIC_CAT: tr_df[cat_vars].values[0],\n        }],\n        freq = \"D\",\n    )\n    te_df = test_df[np.all((test_df[['Province_State', 'Country_Region']].values == location_tr), axis=1)]\n\n    test_gt = ListDataset(\n        [{\"start\": te_df.index[0], \"target\": te_df[target].values}],\n        freq = \"D\",\n    )\n\n    for train_series, gt, forecast in zip(train_obs, test_gt, predictor.predict(train_obs)):\n        \n        train_series = to_pandas(train_series)\n        gt = to_pandas(gt)\n        \n        if start_offset:\n            train_series = train_series[start_offset:]\n        \n        # connect train series visually (either to GT or to forecast)\n        if show_gt:\n            train_series[train_series.index[-1] + pd.DateOffset(1)] = gt.iloc[0]\n        else:\n            train_series[train_series.index[-1] + pd.DateOffset(1)] = forecast.median[:1][0]\n            \n        # log and/or cumulative transforms\n        if log_preds:\n            train_series = np.expm1(train_series)\n            gt = np.expm1(gt)\n            forecast.samples = np.expm1(forecast.samples) \n            forecast._sorted_samples_value = None\n        if cumulative:\n            train_series = train_series.cumsum()\n            gt = gt.cumsum() + train_series.iloc[-2]\n            forecast.samples = np.cumsum(forecast.samples, axis=1) + train_series.iloc[-2]\n            forecast._sorted_samples_value = None\n\n        # plot\n        train_series.plot(linewidth=2, label = 'train series')\n        if show_gt:\n            gt.plot(linewidth=2, label = 'test ground truth')\n            \n        # plot layout\n        type_string = 'Cumulative' if cumulative else 'Daily'\n        plt.title(\n            f'{len(forecast.median)} day forecast: {type_string} number of {target} in {location[0]}', \n            fontsize=fontsize\n        )\n        plt.legend(fontsize = fontsize)\n        plt.grid(which='both')\n        if save:\n            forecast.plot(\n                color='g', \n                prediction_intervals=[50.0, 90.0], \n                show_mean = True,\n                output_file = f'{len(forecast.median)} day forecast {type_string} number of {target} in {location[0]}'\n            )\n        else:\n            forecast.plot(color='g', prediction_intervals=[50.0, 90.0], show_mean = True)\n        forecast._sorted_samples_value = None\n        plt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Plot 12 day forecasts (March 19th - March 30th)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot public leaderboard forecasts\nplot_forecast(predictor_fatalities, train_df, ['Italy', 'Italy'])\nplot_forecast(predictor_fatalities, train_df, ['Iran', 'Iran'])\nplot_forecast(predictor_fatalities, train_df, ['Spain', 'Spain'])\nplot_forecast(predictor_fatalities, train_df, ['Washington', 'US'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Plot 31 day forecasts (March 31st - April 30th)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot private leaderboard forecasts\nplot_forecast(predictor_fatalities_all, all_df, ['Italy', 'Italy'], show_gt = False)\nplot_forecast(predictor_fatalities_all, all_df, ['Iran', 'Iran'], show_gt = False)\nplot_forecast(predictor_fatalities_all, all_df, ['Washington', 'US'], show_gt = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Calculate metrics on public test"},{"metadata":{"trusted":true},"cell_type":"code","source":"from gluonts.evaluation.backtest import make_evaluation_predictions\nfrom gluonts.model.forecast import Forecast\nfrom gluonts.gluonts_tqdm import tqdm\nfrom gluonts.dataset.util import to_pandas\nimport json\nfrom typing import Dict, Union, Tuple\n\n# copied from https://github.com/awslabs/gluon-ts/blob/master/src/gluonts/evaluation/_base.py\ndef extract_pred_target(\n    time_series: Union[pd.Series, pd.DataFrame], forecast: Forecast\n) -> np.ndarray:\n    \n    assert forecast.index.intersection(time_series.index).equals(\n        forecast.index\n    ), (\n        \"Cannot extract prediction target since the index of forecast is outside the index of target\\n\"\n        f\"Index of forecast: {forecast.index}\\n Index of target: {time_series.index}\"\n    )\n\n    # cut the time series using the dates of the forecast object\n    return np.atleast_1d(\n        np.squeeze(time_series.loc[forecast.index].transpose())\n    )\n\ndef msle(target, forecast):\n    return np.mean(np.square(np.log1p(forecast) - np.log1p(target)))\n\n# bootstrapped and edited from https://github.com/awslabs/gluon-ts/blob/master/src/gluonts/evaluation/_base.py\ndef get_metrics_per_ts(\n    time_series: Union[pd.Series, pd.DataFrame], forecast: Forecast\n) -> Dict[str, Union[float, str, None]]:\n    pred_target = np.array(extract_pred_target(time_series, forecast))\n    \n    try:\n        mean_fcst = forecast.mean\n    except:\n        mean_fcst = None\n    median_fcst = forecast.quantile(0.5)\n\n    metrics = {\n        \"item_id\": forecast.item_id,\n        \"MSLE_on_mean\": msle(pred_target, mean_fcst)\n        if mean_fcst is not None\n        else None,\n        \"MSLE_on_median\": msle(pred_target, median_fcst)\n    }\n\n    return metrics\n\n# bootstrapped and edited from https://github.com/awslabs/gluon-ts/blob/master/src/gluonts/evaluation/_base.py\ndef get_aggregate_metrics(\n    metric_per_ts: pd.DataFrame\n) -> Tuple[Dict[str, float], pd.DataFrame]:\n    agg_funs = {\n        \"MSLE_on_mean\": \"mean\",\n        \"MSLE_on_median\": \"mean\",\n    }\n\n    assert (\n        set(metric_per_ts.columns) >= agg_funs.keys()\n    ), \"The some of the requested item metrics are missing.\"\n\n    totals = {\n        key: metric_per_ts[key].agg(agg) for key, agg in agg_funs.items()\n    }\n    totals[\"RMSLE_on_mean\"] = np.sqrt(totals[\"MSLE_on_mean\"])\n    totals[\"RMSLE_on_median\"] = np.sqrt(totals[\"MSLE_on_median\"])\n\n    return totals, metric_per_ts\n\ndef evaluate(\n    data_df: pd.DataFrame, \n    predictor_fatalities,\n    predictor_cases,\n    num_samples: int = 100,\n    log_preds: bool = LOG_TRANSFORM,\n):\n    \n    all_data_fat = build_dataset(all_df)\n    all_data_case = build_dataset(all_df, target = 'ConfirmedCases')\n\n    rows = []\n    with tqdm(\n        zip(training_data_fatalities, all_data_fat, predictor_fatalities.predict(training_data_fatalities)),\n        total=len(training_data_fatalities),\n        desc=\"Evaluating Fatalities Predictor\",\n    ) as it, np.errstate(invalid=\"ignore\"):\n        for train, ts, f in it:\n            \n            train = to_pandas(train)\n            ts = to_pandas(ts)\n            \n            # undo log\n            if log_preds:\n                train = np.expm1(train)\n                ts = np.expm1(ts)\n                f.samples = np.expm1(f.samples) \n                \n            f.samples = np.cumsum(f.samples, axis=1) + train.cumsum().iloc[-1]\n            rows.append(get_metrics_per_ts(ts.cumsum(), f))\n            \n    metrics_per_ts = pd.DataFrame(rows, dtype=np.float64)\n    agg_metrics, metrics_per_ts = get_aggregate_metrics(metrics_per_ts)\n    print(json.dumps(agg_metrics, indent=4))\n\n    rows = []\n    with tqdm(\n        zip(training_data_cases, all_data_case, predictor_cases.predict(training_data_cases)),\n        total=len(all_data_case),\n        desc=\"Evaluating Cases Predictor\",\n    ) as it, np.errstate(invalid=\"ignore\"):\n        for train, ts, f in it:\n            \n            train = to_pandas(train)\n            ts = to_pandas(ts)\n            \n            # undo log\n            if log_preds:\n                train = np.expm1(train)\n                ts = np.expm1(ts)\n                f.samples = np.expm1(f.samples) \n                 \n            f.samples = np.cumsum(f.samples, axis=1) + train.cumsum().iloc[-1]\n            rows.append(get_metrics_per_ts(ts.cumsum(), f))\n            \n    metrics_per_ts = pd.DataFrame(rows, dtype=np.float64)\n    agg_metrics, metrics_per_ts = get_aggregate_metrics(metrics_per_ts)\n    print(json.dumps(agg_metrics, indent=4))\n    \nevaluate(all_df, predictor_fatalities, predictor_cases)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# generate submission csv\n\ndef aggregate(\n    all_data: ListDataset,\n    train_data: ListDataset, \n    train_data_all: ListDataset, \n    predictor,\n    predictor_all,\n    log_preds: bool = LOG_TRANSFORM,\n    mean: bool = False,\n):\n    \n    aggregates = []\n    for train, train_all, public_forecast, private_forecast in zip(\n        train_data,\n        train_data_all,\n        predictor.predict(train_data),\n        predictor_all.predict(train_data_all)\n    ):\n        \n        train = to_pandas(train)\n        train_all = to_pandas(train_all)\n        \n        # undo log\n        if log_preds:\n            train = np.expm1(train)\n            train_all = np.expm1(train_all)\n            public_forecast.samples = np.expm1(public_forecast.samples) \n            private_forecast.samples = np.expm1(private_forecast.samples) \n            \n        # accumulate\n        ts = train.cumsum()\n        ts_all = train_all.cumsum()\n        public_forecast.samples = np.cumsum(public_forecast.samples, axis=1) + ts.iloc[-1]\n        private_forecast.samples = np.cumsum(private_forecast.samples, axis=1) + ts_all.iloc[-1]\n    \n        # concatenate\n        public_f = public_forecast.mean if mean else public_forecast.median\n        private_f = private_forecast.mean if mean else private_forecast.median\n        aggregates.append(np.concatenate((public_f, private_f)))  \n    \n    return aggregates\n\ndef submit(\n    filename: str,\n    mean: bool = False,\n):\n    \n    # aggregate fatalities\n    fatalities = aggregate(\n        build_dataset(all_df), \n        training_data_fatalities,\n        training_data_fatalities_all,\n        predictor_fatalities,\n        predictor_fatalities_all,\n        mean = mean\n    )\n\n    # aggregate cases\n    cases = aggregate(\n        build_dataset(all_df, target = 'ConfirmedCases'), \n        training_data_cases,\n        training_data_cases_all,\n        predictor_cases,\n        predictor_cases_all,\n        mean = mean\n    )\n\n    # load test csv \n    sub_df = pd.read_csv(\"../input/covid19-global-forecasting-week-2/test.csv\")\n\n    # fill 'NaN' Province/State values with Country/Region values\n    sub_df['Province_State'] = sub_df['Province_State'].fillna(sub_df['Country_Region'])\n\n    # get forecast ids\n    ids = []\n    for _, df in sub_df.groupby(by=['Province_State', 'Country_Region']):\n        ids.append(df['ForecastId'].values)\n\n    # create submission df\n    submission = pd.DataFrame(\n        list(zip(\n            np.array(ids).flatten(),\n            np.array(cases).flatten(),\n            np.array(fatalities).flatten()\n        )), \n        columns = ['ForecastId', 'ConfirmedCases', 'Fatalities']\n    )\n    submission.to_csv(filename, index=False)\n\nsubmission = submit('submission.csv', mean = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}