{"cells":[{"metadata":{},"cell_type":"markdown","source":"# COVID19 Model\nKaggle competition - Forecasting Covid 19\n\n## Data\nThe input data, *X*, currently has 5 columns:\n* ForecastId\n* Province_State\n* Country_Region\n* Date\n* ConfirmedCases\n* Fatalities\n\nThe output submission data, *Y**, has 3 columns:\n* ForecastId\n* ConfirmedCases\n* Fatalities\n\nNote that it only predicts one day in the future.\n\n## RNN Plan\nI think an RNN is a good idea. Here's how I picture it:\n<img src=\"figs/RNN.bmp\">\n\n## Data Augmentation\nI have a lot of ideas for how we can augment the input data *X*. I see *X* having 2 types of elements: those that change daily and those that are constant.\n\n### *X* Elements that change daily:\n* Date (have)\n* Confirmed cases (have)\n* Fatalities (have)\n* Day of week (add) - is spread higher on weekends?\n* Weather (add) - does the average temperature change people's behaviour?\n* Mitigation in place (add) - can we attempt to quantize the mitigation measures a country has put in place, say by assigning a rating of 0-1? For example:\n    * 0 = no measures would \n    * 0.2 = localized voluntary measures\n    * 0.4 = localized lockdowns\n    * 0.6 = voluntary region wide measures\n    * 0.8 = mandatory region wide measures\n    * 1.0 = state of emergency, all borders closed, complete lockdown\n    * -1.0 = unknown\n* Attitude (add) - it would be neat if we could gauge the local attitude towards COVID, eg by using most commonly tweeted words in the region that day. Maybe word encodings could be used for translation and sentiment analysis. This would require a pretty involved NLP model, but ideally we could include a sentiment rating from 0-1 reflecing whether the public is ignoring the virus completely = 0, versus taking the virus very seriously =1.\n\n### *X* Elements that are (relatively) constant:\n* Province/State (have)\n* Country/Region (have)\n* GDP (add)\n* Population of region (add) - surely population matters?\n* Area of region (add) - surely population density matters?\n* Region's average coldest annual temperature (add) - may play into population behaviours\n* Region's average warmest annual temperature (add) - may play into population behaviours\n* Region's national governance (add) - as some measure of freedom? eg:\n    * 0 = Democracy\n    * 0.5 = Other\n    * 1.0 = Autocracy (dictatorship, absolute monarchy)\n* Region's top N trading partners (add) - maybe the virus gets imported via trade?\n\n## To Do\n* Set up RNN\n* Augment input data *X*"},{"metadata":{},"cell_type":"markdown","source":"# NN Models\nLet's make some RNN and 1D CNN options. For now I'll just modify crap I already have, then frig around with the architectures more later.\n## Load Packages"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Start by loading packages.\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import imshow\n\nimport imageio #let's you read images in as numpy arrays\nimport argparse\nimport random\nimport pydot\n#from pydub import AudioSegment\nfrom IPython.display import SVG\nimport cv2\nimport h5py\nimport random\nimport sys\nimport io\nimport os\nimport glob\nimport IPython\nimport PIL\nfrom PIL import Image #Python Imaging Library image module\nimport time\nimport math\nimport numpy as np\nfrom numpy import genfromtxt\nimport pandas as pd\nimport geopandas as gpd\nimport tensorflow as tf\n\nfrom sklearn.metrics import confusion_matrix\nimport scipy.io as sio\nfrom scipy.io import wavfile\nfrom scipy import ndimage\nfrom scipy import misc\n\n#from pydub import AudioSegment\nfrom numpy import genfromtxt\n\nfrom datetime import datetime\nfrom datetime import date\n\n%matplotlib inline\n%load_ext autoreload\n%autoreload 2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check which version of tensorflow you're using. I'm using TF2\nprint(tf.__version__) #should be 1.14.0 for OLD tensorflow or 2.1.0 for NEW\nprint(sys.version) #see which python version, should be 3.7.7","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#For TF1\n'''from keras import backend as K\nfrom keras.models import Model, load_model, Sequential\nfrom keras.layers import merge, Conv2D, ZeroPadding2D, Dense, Dropout, Reshape, Lambda, RepeatVector\nfrom keras.layers import Dense, Activation, Dropout, Masking, TimeDistributed, LSTM, Conv1D\nfrom keras.layers import GRU, Bidirectional, BatchNormalization, Reshape\nfrom keras.layers import Conv2D, ZeroPadding2D, Activation, Input, Concatenate, concatenate\nfrom keras.layers import Conv1D, ZeroPadding1D, MaxPooling1D\nfrom keras.layers import Input, Add, LeakyReLU\nfrom keras.layers.pooling import MaxPooling2D, AveragePooling2D, GlobalMaxPooling2D\nfrom keras.layers.core import Lambda, Flatten, Dense\nfrom keras.layers.embeddings import Embedding\nfrom keras.preprocessing import sequence\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.optimizers import Adam, SGD\nfrom keras.initializers import glorot_uniform\nfrom keras.engine.topology import Layer\nfrom keras.utils.vis_utils import model_to_dot\nfrom keras.utils import plot_model\n#K.set_image_data_format('channels_first')'''\n\n#For TF2\nfrom tensorflow.keras.models import Model, load_model, Sequential\nfrom tensorflow.keras.layers import Conv2D, ZeroPadding2D, Dense, Dropout, Reshape, Lambda, RepeatVector\nfrom tensorflow.keras.layers import Dense, Activation, Dropout, Masking, TimeDistributed, LSTM, Conv1D\nfrom tensorflow.keras.layers import GRU, Bidirectional, BatchNormalization, Reshape\nfrom tensorflow.keras.layers import Conv2D, ZeroPadding2D, Activation, Input, Concatenate, concatenate\nfrom tensorflow.keras.layers import Conv1D, ZeroPadding1D, MaxPooling1D\nfrom tensorflow.keras.layers import Input, Add, LeakyReLU\nfrom tensorflow.keras.layers import MaxPooling2D, AveragePooling2D, GlobalMaxPooling2D\nfrom tensorflow.keras.layers import Lambda, Flatten, Dense\nfrom tensorflow.keras.layers import Embedding\nfrom tensorflow.keras.preprocessing import sequence\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.optimizers import Adam, SGD\nfrom tensorflow.keras.initializers import glorot_uniform\nfrom tensorflow.keras.layers import Layer\nfrom tensorflow.keras.utils import model_to_dot\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.models import load_model\n#K.set_image_data_format('channels_first')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Define Models\n### LSTM"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Edit this up later\ndef LSTMmodel(input_shape):\n    \"\"\"\n    Function creating the LSTM model's graph in Keras.\n    \n    Arguments:\n    input_shape -- shape of each training example\n\n    Returns:\n    model -- a Model() instance in Keras\n    \"\"\"    \n    \n    #########################################################\n    #1. Start with inputs\n    #########################################################\n    \n    # Define the input placeholder as a tensor with shape input_shape. \n    # Think of this as your input dataset (the input card feature vectors) being fed to the graph.\n    # This is supplied when you actually call this function later\n    X_input = Input(shape=input_shape) \n    \n    #########################################################\n    #2. Assemble the model layers\n    #########################################################\n    \n    # Propagate the embeddings through an LSTM layer with 128-dimensional hidden state\n    # Be careful, the returned output should be a batch of sequences to pass on \n    # the batch of sequences state = c is the hidden memory cell, and activation/output = a\n    X = LSTM(128,return_sequences=True)(X_input)\n    # Add dropout with a probability of 0.5 (to improve the robustness of training)\n    X = Dropout(0.2)(X)\n    \n    # Propagate X through another LSTM layer with 128-dimensional hidden state\n    # Be careful, the returned output should be a single hidden state, not a batch of sequences.\n    X = LSTM(128)(X)\n    # Add dropout with a probability of 0.5\n    X = Dropout(0.5)(X)\n    \n    # Propagate X through a Dense layer with softmax activation to get back a batch of 2-dimensional vectors.\n    X = Dense(2, activation='softmax')(X)\n    # Add a softmax activation\n    X = Activation('softmax')(X) #output the desired Y=0/1 probabilities at the output layer\n    #NOTE: LEAVE THESE AS SOFTMAXES FOR NOW\n    \n    #########################################################\n    #3. Create model instance with the correct \"inputs\" and \"outputs\"\n    #########################################################\n    \n    # The model takes as input an array X_inputs of feature vectors of shape (m, Ncards, Flen) defined by input_shape. \n    # It should output a softmax probability vector (the final X) of shape (m, C = 2).\n    # This step creates your Keras model instance, which will be used to train/test the model.\n    model = Model(inputs = X_input, outputs = X, name='LSTMmodel')\n    \n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### GRU"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Edit this up later\ndef GRUmodel(input_shape):\n    \"\"\"\n    Function creating the GRU model's graph in Keras.\n    \n    Argument:\n    input_shape -- shape of the model's input data (using Keras conventions)\n\n    Returns:\n    model -- Keras model instance\n    \"\"\"\n    \n    #########################################################\n    #1. Start with inputs\n    #########################################################\n    \n    # Define the input placeholder as a tensor with shape input_shape. \n    # Think of this as your input dataset (the input card feature vectors) being fed to the graph.\n    # This is supplied when you actually call this function later\n    X_input = Input(shape=input_shape) \n    \n    #########################################################\n    #2. Assemble the model layers\n    #########################################################\n\n    # Step 1: First GRU Layer\n    X = GRU(units = 128, return_sequences = True)(X_input) # GRU or could use LSTM\n    #return_sequences = True ensures that all the GRU's hidden states are fed to the next layer\n    X = Dropout(0.2)(X) \n    X = BatchNormalization()(X) \n    \n    # Step 2: Second GRU Layer\n    X = GRU(units = 128)(X) # GRU or could use LSTM\n    #Again, return_sequences = True means all units give output. This is many-to-many\n    #If you want many-to-one instead, just use\n    #X = GRU(128)(X) #many-to-one\n    X = Dropout(0.2)(X) \n    X = BatchNormalization()(X)\n    \n    # Step 3:  Dense layer\n    # Propagate X through a Dense layer with softmax activation to get back a batch of 128-dimensional vectors.\n    #X = TimeDistributed(Dense(64, activation = \"sigmoid\"))(X) #many-to-many\n    X = Dense(128, activation='relu')(X) #many-to-one let's just see how this goes with relu\n     \n    #Optional: Add another Dropout + dense\n    # Add dropout with a probability of 0.1\n    X = Dropout(0.1)(X)\n    # Propagate X through a Dense layer with softmax activation to get back a batch of 128-dimensional vectors.\n    #X = Dense(64, activation='sigmoid')(X)\n    X = Dense(2)(X) #2 output elements between 0 and 1\n\n    #########################################################\n    #3. Create model instance with the correct \"inputs\" and \"outputs\"\n    #########################################################\n    \n    # The model takes as input an array X_inputs of feature vectors of shape (m, Ncards, Flen) defined by input_shape. \n    # It should output a softmax probability vector (the final X) of shape (m, C = 2).\n    # This step creates your Keras model instance, which will be used to train/test the model.\n    model = Model(inputs = X_input, outputs = X, name='GRUmodel')\n    \n    return model  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1D CNN Inception Net"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Edit this up later\ndef inceptionModel(input_shape):\n    '''\n    1D inceptionCNN base network for time domain A(t) processing.\n    Takes input shape (m, t, n_aspectrows) = (m, 512, 4)\n    '''\n    \n    # Tweak channel numbers easily\n    ch00 = 8 #32 #16 #8 #12 #10 #16\n    ch0 = 16 #64 #32 #16 #24 #20 #32\n    ch1 = 16 #128 #64 #34 #48 #40 #64\n    ch2 = 32 #256 #128 #64 #96 #80 #128\n    ch3 = 64\n    droprate = 0.1 #fraction of input units to use for dropout\n    \n    \n    #########################################################\n    #1. Start with inputs\n    #########################################################\n    \n    # Define the input placeholder as a tensor with shape input_shape. \n    # Think of this as your input dataset (the input card feature vectors) being fed to the graph.\n    # This is supplied when you actually call this function later\n    X_inputs = Input(shape=input_shape) \n    #This will be (m,f,t,n_aspectrows)\n    \n    #########################################################\n    #2. Assemble the model layers\n    #########################################################\n    \n    # First Layer = the stem, bringing data into the network\n    #keras.layers.Conv1D(filters, kernel_size, strides=1, padding='valid', \n    #                    data_format='channels_last', dilation_rate=1, activation=None, \n    #                    use_bias=True, kernel_initializer='glorot_uniform', \n    #                    bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, \n    #                    activity_regularizer=None, kernel_constraint=None, bias_constraint=None)\n    #X = Conv1D(ch00, 1, strides=1, padding = 'valid', name='conv0')(X_inputs)\n    #X = Dropout(droprate)(X)\n    #X = BatchNormalization(epsilon=0.00001, name='bn0')(X)\n    #X = LeakyReLU(alpha=0.1, name = 'relu0')(X)\n    \n    '''X = concatenate([X1_1, X1_3, X1_5, X1_pool], axis=-1)\n    \n    # Second Layer - might make this an inception block!\n    X = Conv1D(ch1, 5, strides=2, padding = 'valid', name='conv2')(X) #switched same to valid\n    X = Dropout(droprate)(X)\n    X = BatchNormalization(epsilon=0.00001, name='bn2')(X)\n    X = LeakyReLU(alpha=0.1, name = 'relu2')(X)\n    \n    # Third Layer\n    X = Conv1D(ch2, 5, strides = 2, padding = 'valid', name='conv3')(X) #switched same to valid\n    X = Dropout(droprate)(X)\n    X = BatchNormalization(epsilon=0.00001, name='bn3')(X)\n    X = LeakyReLU(alpha=0.1, name = 'relu3')(X)'''\n    \n    # Inception 1:\n    ######################################################################\n    #The 1x1 convolution maintains the incoming pixels but changes channels\n    X1_1 = Conv1D(ch1, 1, name='inception_1_1_conv')(X_inputs)\n    X1_1 = BatchNormalization(axis=-1, epsilon=0.00001, name='inception_1_1_bn')(X1_1)\n    X1_1 = Activation('relu')(X1_1)\n    \n    #The 3x1 convolves 3x3 filters over the data pixels\n    X1_3 = Conv1D(ch1, 1, name ='inception_1_3_conv1')(X_inputs)\n    X1_3 = BatchNormalization(axis=-1, epsilon=0.00001, name = 'inception_1_3_bn1')(X1_3)\n    X1_3 = Activation('relu')(X1_3)\n    X1_3 = ZeroPadding1D(padding=1)(X1_3)\n    X1_3 = Conv1D(ch1, 3, name='inception_1_3_conv2')(X1_3)\n    X1_3 = BatchNormalization(axis=-1, epsilon=0.00001, name='inception_1_3_bn2')(X1_3)\n    X1_3 = Activation('relu')(X1_3)\n    \n    #The 5x1 convolves 5x1 filters over the data pixels \n    X1_5 = Conv1D(ch1, 1, name='inception_1_5_conv1')(X_inputs)\n    X1_5 = BatchNormalization(axis=-1, epsilon=0.00001, name='inception_1_5_bn1')(X1_5)\n    X1_5 = Activation('relu')(X1_5)\n    X1_5 = ZeroPadding1D(padding=2)(X1_5)\n    X1_5 = Conv1D(ch1, 5, name='inception_1_5_conv2')(X1_5)\n    X1_5 = BatchNormalization(axis=-1, epsilon=0.00001, name='inception_1_5_bn2')(X1_5)\n    X1_5 = Activation('relu')(X1_5)\n    \n    #The 7x1 convolves 7x1 filters over the data pixels \n    X1_7 = Conv1D(ch1, 1, name='inception_1_7_conv1')(X_inputs)\n    X1_7 = BatchNormalization(axis=-1, epsilon=0.00001, name='inception_1_7_bn1')(X1_7)\n    X1_7 = Activation('relu')(X1_7)\n    X1_7 = ZeroPadding1D(padding=3)(X1_7)\n    X1_7 = Conv1D(ch1, 7, name='inception_1_7_conv2')(X1_7)\n    X1_7 = BatchNormalization(axis=-1, epsilon=0.00001, name='inception_1_7_bn2')(X1_7)\n    X1_7 = Activation('relu')(X1_7)\n\n    #The MaxPooling layer is probably not so helpful but you never know\n    X1_pool = MaxPooling1D(pool_size=3, strides=1)(X_inputs)\n    X1_pool = Conv1D(ch1, 1, name='inception_1_pool_conv')(X1_pool)\n    X1_pool = BatchNormalization(axis=-1, epsilon=0.00001, name='inception_1_pool_bn')(X1_pool)\n    X1_pool = Activation('relu')(X1_pool)\n    X1_pool = ZeroPadding1D(1)(X1_pool)\n        \n    # CONCATENATE them all together along the channel axis\n    X = concatenate([X1_1, X1_3, X1_5, X1_7, X1_pool], axis=-1)\n    # DOWNSAMPLE\n    #X = MaxPooling1D(pool_size=3, strides=2)(X)\n    ######################################################################\n    \n    # Inception 2:\n    ######################################################################\n    #The 1x1 convolution maintains the incoming pixels but changes channels\n    X2_1 = Conv1D(ch2, 1, name='inception_2_1_conv')(X)\n    X2_1 = BatchNormalization(axis=-1, epsilon=0.00001, name='inception_2_1_bn')(X2_1)\n    X2_1 = Activation('relu')(X2_1)\n    \n    #The 3x1 convolves 3x3 filters over the data pixels\n    X2_3 = Conv1D(ch2, 1, name ='inception_2_3_conv1')(X)\n    X2_3 = BatchNormalization(axis=-1, epsilon=0.00001, name = 'inception_2_3_bn1')(X2_3)\n    X2_3 = Activation('relu')(X2_3)\n    X2_3 = ZeroPadding1D(padding=1)(X2_3)\n    X2_3 = Conv1D(ch2, 3, name='inception_2_3_conv2')(X2_3)\n    X2_3 = BatchNormalization(axis=-1, epsilon=0.00001, name='inception_2_3_bn2')(X2_3)\n    X2_3 = Activation('relu')(X2_3)\n    \n    #The 5x1 convolves 5x1 filters over the data pixels \n    X2_5 = Conv1D(ch2, 1, name='inception_2_5_conv1')(X)\n    X2_5 = BatchNormalization(axis=-1, epsilon=0.00001, name='inception_2_5_bn1')(X2_5)\n    X2_5 = Activation('relu')(X2_5)\n    X2_5 = ZeroPadding1D(padding=2)(X2_5)\n    X2_5 = Conv1D(ch2, 5, name='inception_2_5_conv2')(X2_5)\n    X2_5 = BatchNormalization(axis=-1, epsilon=0.00001, name='inception_2_5_bn2')(X2_5)\n    X2_5 = Activation('relu')(X2_5)\n    \n    #The 7x1 convolves 7x1 filters over the data pixels \n    X2_7 = Conv1D(ch1, 1, name='inception_2_7_conv1')(X)\n    X2_7 = BatchNormalization(axis=-1, epsilon=0.00001, name='inception_2_7_bn1')(X2_7)\n    X2_7 = Activation('relu')(X2_7)\n    X2_7 = ZeroPadding1D(padding=3)(X2_7)\n    X2_7 = Conv1D(ch1, 7, name='inception_2_7_conv2')(X2_7)\n    X2_7 = BatchNormalization(axis=-1, epsilon=0.00001, name='inception_2_7_bn2')(X2_7)\n    X2_7 = Activation('relu')(X2_7)\n\n    #The MaxPooling layer is probably not so helpful but you never know\n    X2_pool = MaxPooling1D(pool_size=3, strides=1)(X)\n    X2_pool = Conv1D(ch2, 1, name='inception_2_pool_conv')(X2_pool)\n    X2_pool = BatchNormalization(axis=-1, epsilon=0.00001, name='inception_2_pool_bn')(X2_pool)\n    X2_pool = Activation('relu')(X2_pool)\n    X2_pool = ZeroPadding1D(1)(X2_pool)\n        \n    # CONCATENATE them all together along the channel axis\n    X = concatenate([X2_1, X2_3, X2_5, X2_7, X2_pool], axis=-1)\n    #X = MaxPooling1D(pool_size=3, strides=2)(X)\n    ######################################################################\n\n    # Inception 3:\n    ######################################################################\n    #The 1x1 convolution maintains the incoming pixels but changes channels\n    X3_1 = Conv1D(ch2, 1, name='inception_3_1_conv')(X)\n    X3_1 = BatchNormalization(axis=-1, epsilon=0.00001, name='inception_3_1_bn')(X3_1)\n    X3_1 = Activation('relu')(X3_1)\n    \n    #The 3x1 convolves 3x3 filters over the data pixels\n    X3_3 = Conv1D(ch2, 1, name ='inception_3_3_conv1')(X)\n    X3_3 = BatchNormalization(axis=-1, epsilon=0.00001, name = 'inception_3_3_bn1')(X3_3)\n    X3_3 = Activation('relu')(X3_3)\n    X3_3 = ZeroPadding1D(padding=1)(X3_3)\n    X3_3 = Conv1D(ch2, 3, name='inception_3_3_conv2')(X3_3)\n    X3_3 = BatchNormalization(axis=-1, epsilon=0.00001, name='inception_3_3_bn2')(X3_3)\n    X3_3 = Activation('relu')(X3_3)\n    \n    #The 5x1 convolves 5x1 filters over the data pixels \n    X3_5 = Conv1D(ch2, 1, name='inception_3_5_conv1')(X)\n    X3_5 = BatchNormalization(axis=-1, epsilon=0.00001, name='inception_3_5_bn1')(X3_5)\n    X3_5 = Activation('relu')(X3_5)\n    X3_5 = ZeroPadding1D(padding=2)(X3_5)\n    X3_5 = Conv1D(ch2, 5, name='inception_3_5_conv2')(X3_5)\n    X3_5 = BatchNormalization(axis=-1, epsilon=0.00001, name='inception_3_5_bn2')(X3_5)\n    X3_5 = Activation('relu')(X3_5)\n    \n    #The 7x1 convolves 7x1 filters over the data pixels \n    X3_7 = Conv1D(ch1, 1, name='inception_3_7_conv1')(X)\n    X3_7 = BatchNormalization(axis=-1, epsilon=0.00001, name='inception_3_7_bn1')(X3_7)\n    X3_7 = Activation('relu')(X3_7)\n    X3_7 = ZeroPadding1D(padding=3)(X3_7)\n    X3_7 = Conv1D(ch1, 7, name='inception_3_7_conv2')(X3_7)\n    X3_7 = BatchNormalization(axis=-1, epsilon=0.00001, name='inception_3_7_bn2')(X3_7)\n    X3_7 = Activation('relu')(X3_7)\n\n    #The MaxPooling layer is probably not so helpful but you never know\n    X3_pool = MaxPooling1D(pool_size=3, strides=1)(X)\n    X3_pool = Conv1D(ch2, 1, name='inception_3_pool_conv')(X3_pool)\n    X3_pool = BatchNormalization(axis=-1, epsilon=0.00001, name='inception_3_pool_bn')(X3_pool)\n    X3_pool = Activation('relu')(X3_pool)\n    X3_pool = ZeroPadding1D(1)(X3_pool)\n        \n    # CONCATENATE them all together along the channel axis\n    X = concatenate([X3_1, X3_3, X3_5, X3_7, X3_pool], axis=-1)\n    #X = MaxPooling1D(pool_size=3, strides=2)(X)\n    ######################################################################\n    \n    # Inception 4:\n    ######################################################################\n    #The 1x1 convolution maintains the incoming pixels but changes channels\n    X4_1 = Conv1D(ch2, 1, name='inception_4_1_conv')(X)\n    X4_1 = BatchNormalization(axis=-1, epsilon=0.00001, name='inception_4_1_bn')(X4_1)\n    X4_1 = Activation('relu')(X4_1)\n    \n    #The 3x1 convolves 3x3 filters over the data pixels\n    X4_3 = Conv1D(ch2, 1, name ='inception_4_3_conv1')(X)\n    X4_3 = BatchNormalization(axis=-1, epsilon=0.00001, name = 'inception_4_3_bn1')(X4_3)\n    X4_3 = Activation('relu')(X4_3)\n    X4_3 = ZeroPadding1D(padding=1)(X4_3)\n    X4_3 = Conv1D(ch2, 3, name='inception_4_3_conv2')(X4_3)\n    X4_3 = BatchNormalization(axis=-1, epsilon=0.00001, name='inception_4_3_bn2')(X4_3)\n    X4_3 = Activation('relu')(X4_3)\n    \n    #The 5x1 convolves 5x1 filters over the data pixels \n    X4_5 = Conv1D(ch2, 1, name='inception_4_5_conv1')(X)\n    X4_5 = BatchNormalization(axis=-1, epsilon=0.00001, name='inception_4_5_bn1')(X4_5)\n    X4_5 = Activation('relu')(X4_5)\n    X4_5 = ZeroPadding1D(padding=2)(X4_5)\n    X4_5 = Conv1D(ch2, 5, name='inception_4_5_conv2')(X4_5)\n    X4_5 = BatchNormalization(axis=-1, epsilon=0.00001, name='inception_4_5_bn2')(X4_5)\n    X4_5 = Activation('relu')(X4_5)\n    \n    #The 7x1 convolves 7x1 filters over the data pixels \n    X4_7 = Conv1D(ch1, 1, name='inception_4_7_conv1')(X)\n    X4_7 = BatchNormalization(axis=-1, epsilon=0.00001, name='inception_4_7_bn1')(X4_7)\n    X4_7 = Activation('relu')(X4_7)\n    X4_7 = ZeroPadding1D(padding=3)(X4_7)\n    X4_7 = Conv1D(ch1, 7, name='inception_4_7_conv2')(X4_7)\n    X4_7 = BatchNormalization(axis=-1, epsilon=0.00001, name='inception_4_7_bn2')(X4_7)\n    X4_7 = Activation('relu')(X4_7)\n\n    #The MaxPooling layer is probably not so helpful but you never know\n    X4_pool = MaxPooling1D(pool_size=3, strides=1)(X)\n    X4_pool = Conv1D(ch2, 1, name='inception_4_pool_conv')(X4_pool)\n    X4_pool = BatchNormalization(axis=-1, epsilon=0.00001, name='inception_4_pool_bn')(X4_pool)\n    X4_pool = Activation('relu')(X4_pool)\n    X4_pool = ZeroPadding1D(1)(X4_pool)\n        \n    # CONCATENATE them all together along the channel axis\n    X = concatenate([X4_1, X4_3, X4_5, X4_7, X4_pool], axis=-1)\n    #X = MaxPooling1D(pool_size=3, strides=2)(X)\n    ######################################################################\n    \n    # Last Conv layer\n    X = Conv1D(ch2, 5, strides = 2, padding = 'valid', name='convX')(X) #switched same to valid\n    X = Dropout(droprate)(X)\n    X = BatchNormalization(epsilon=0.00001, name='bnX')(X)\n    X = LeakyReLU(alpha=0.1, name = 'reluX')(X)\n    \n    # FLATTEN X (means convert it to a vector) + FULLYCONNECTED\n    X = Flatten()(X)\n    \n    # OUTPUT LAYER - Fully Connected\n    # Propagate X through a Dense layer with sigmoid activation to get back a batch of 128-dimensional vectors.\n    X = Dense(ch2, activation='sigmoid', name='fc_1')(X)\n    # Then relu output\n    #X = Dense(2, activation='relu', name='fc_2')(X)\n    X = Dense(2, name='fc_2')(X) #Remove relu activation since my trick for log 0 makes 0's neg!\n    # Add dropout with a probability of 0.1\n    #X = Dropout(0.1)(X)\n    # Propagate X through a Dense layer with softmax activation to get back a batch of 128-dimensional vectors.\n    #X = Dense(ch2, name='fc_2')(X)\n    #Optional: Add another Dropout + dense\n    \n    #########################################################\n    #3. Create model instance with the correct \"inputs\" and \"outputs\"\n    #########################################################\n    \n    # This step creates your Keras model instance, which will be used to train/test the model.\n    model = Model(inputs = X_inputs, outputs = X, name='Jess1DInceptionNet')\n    \n    #Same as return Model(input, x)\n    return model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Define Loss and accuracy\nActually, I think I'll just use MSE loss for now during training since I take the log of the data that I'm using anyways.\nKeras has mean_squared_logarithmic_error too which will be good for later"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Define RMS Log Error","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data\nGet the data sorted out. For now we'll start by just loading the data in from the competition. Later, we'll probably have more crap to add.\n## Load Data\nEither from home computer or from Kaggle folder on kernel"},{"metadata":{"trusted":true},"cell_type":"code","source":"# The data is in csv files\n# Read in data to a pandas dataframe\n\n#At home:\n#df_train = pd.read_csv('data/train.csv') #NOTE: later, use more data!\n#df_test = pd.read_csv('data/test.csv')\n#df_submission = pd.read_csv('data/submission.csv')\n\n#On kernel:\ndf_train = gpd.read_file(\"/kaggle/input/covid19-global-forecasting-week-2/train.csv\") #NOTE: later, use more data!\ndf_test = gpd.read_file(\"/kaggle/input/covid19-global-forecasting-week-2/test.csv\")\ndf_submission = gpd.read_file(\"/kaggle/input/covid19-global-forecasting-week-2/submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Look at train data\ndf_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Look at Canada\n(df_train[df_train[\"Country_Region\"]=='Canada'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Look at test data\nlen(df_train[df_train[\"Country_Region\"]=='Afghanistan']) #how many days in this one eg","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dates_train = df_train.Date.unique()\nprint(\"Train dates: \")\nprint(\"From \" + str(min(dates_train)) + \" to \" + str(max(dates_train)))\nprint(\"Number of days: \" + str(len(dates_train)))\n\ndates_test = df_test.Date.unique()\nprint(\"Test dates: \")\nprint(\"From \" + str(min(dates_test)) + \" to \" + str(max(dates_test)))\nprint(\"Number of days: \" + str(len(dates_test)))\n\n\n#dates_submission = df_submission.Date.unique()\n#print(dates_submission)\nprint(\"Submission\")\nprint(\"Number of entries: \" + str(len(df_submission)))\nprint(\"I think this should match the number of entries in test.csv: \" + str(len(df_test)))\n\ndf_submission","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Pre-process data\nNow decide how to format the input data *X*. \n* Chop it up by country_Region\n* Decide how to put the dates in. Probably do something like 100-150 days and zero pad if there isn't data."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's make a new dataframe with just one row per country\ncountries = df_train.Country_Region.unique()\n#print(countries)\n\n#Now get all the unique places\nplaces = []\nfor country in countries:\n    #n_entries.append(len(df_train[df_train[\"Country_Region\"]==country]))\n    #Check and see if there are entries in Province_State or not\n    \n    #Stupid Denmark has both nans and states. Fix this crap.\n    if len(df_train[df_train['Province_State'].notnull() &\n                    (df_train[\"Country_Region\"]==country)]) == 0:\n        #This means it's just the whole country with no provinces or states\n        places.append([country,np.nan])\n        if len(df_train[df_train[\"Country_Region\"]==country]) != len(dates_train):\n            print(\"Weirdo country: \" + country)\n            print(\"Days: \" + str(len(df_train[df_train[\"Country_Region\"]==country])))\n    else:\n        #There means there are are provinces or states\n        #Get a list of them\n        provstates = df_train[df_train[\"Country_Region\"]==country].Province_State.unique()\n        for ps in provstates:\n            if type(ps)!=str:\n                #print('Its nan')\n                places.append([country,np.nan])\n            else:\n                #print(\"It's not nan\")\n                places.append([country,ps])\n            df_region = df_train[(df_train['Province_State']==ps)& (df_train['Country_Region'] == country)]\nprint(\"Total number of countries: \" + str(len(countries)))\nprint(\"Total number of places: \" + str(len(places)))\nprint(\"Total number of entries = ndays*places: \" + str(len(dates_train)*len(places)))\nprint(\"Expected number of entries: \" + str(len(df_train)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data generator\nMake a data generator that does some optional augmenting, eg by shifting the dates and multiplying by random noise.\n### Function version"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Start with a function version\ndef generate_data_fn(m, n_days, places, df_daydata, df_constdata, A_noise):\n    '''Function version of data generator for making X and Y\n    INPUTS\n    m: number of training examples\n    n_days: number of consecutive days to use = number of input elements\n    places: list of all unique places to generate predictions for\n    df_daydata: dataframe containing daily elements for all places\n    df_constdata: dataframe containing constant elements for all places\n    A_noise: amplitude of noise to add to X cases and fatalities\n    '''\n    IDs = [] # Id\n    X = [] # Data examples\n    Y = [] # Labels\n    \n    for i in range(m):\n        # 1. Randomly select which place to use\n        country,ps = random.choice(places)\n        #rint(country)\n        #print(ps)\n        #Now see if a province or state is given\n        #type(var) == str\n        if type(ps)!=str:\n            #print('Its nan')\n            df_country = df_train[(df_train['Province_State'].isnull()) &\n                                  (df_train[\"Country_Region\"]==country)]\n        else:\n            #print('not nan')\n            df_country = df_train[(df_train['Province_State']==ps) &\n                        (df_train[\"Country_Region\"]==country)]\n        Data = df_country[['Id','ConfirmedCases','Fatalities']].to_numpy()\n        Data = Data.astype(np.float) #Convert these to float since apparently they're strings '0.0'\n        \n        # 2. Randomly select n_days consecutive values, making sure they all fit\n        day0range = len(Data) - n_days\n        day0 = np.random.randint(day0range)\n        ids = Data[day0:(day0+n_days),0]\n        #x = Data[day0:(day0+n_days-1),1:]\n        #y = Data[(day0+n_days),1:]\n        xy = Data[day0:(day0+n_days),1:]\n        \n        # 3. Add a little noise to augment the x and y data, if desired\n        # Uniform random noise about zero\n        #x_noise = 2*np.random.rand(*x.shape)-1\n        #y_noise = 2*np.random.rand()-1\n        xy_noise = 2*np.random.rand(*xy.shape)-1\n        #x = x*(1+A_noise*x_noise)\n        #y = y*(1+A_noise*y_noise)\n        #print(\"xy type is: \" + str(type(xy)))\n        #print(\"xy_noise type is: \" + str(type(xy_noise)))\n        #print(\"A_noise type is: \" + str(type(A_noise)))\n        #print(\"xy is: \" + str(xy))\n        #print(\"xy_noise is: \" + str(xy_noise))\n        #print(\"A_noise is: \" + str(A_noise))\n        \n        xy = xy*(1+A_noise*xy_noise) #THIS GIVES AN ERROR IN KERNEL VERSION BUT NOT HOME VERSION\n        #TypeError: can't multiply sequence by non-int of type 'float'\n        \n        #Make sure it's still monotonically increasing though since it's cumulative!\n        #Use np.maximum.accumulate along the column axis\n        xy = np.maximum.accumulate(xy,axis=0)\n        #Then make them all back into integer numbers? Meh don't bother since logging\n        \n        # 4. Scale x and y. I think log is a good idea. Lets assume normalize by log(10^6)=6\n        #Should give us a good number between 0 and 1 (unless things really get out of hand)\n        #x = np.log10(x+0.5)/6 #The 0.5 prevents log(0) = -inf\n        #y = np.log10(y+0.5)/6 #The 0.5 prevents log(0) = -inf\n        xy = np.log10(xy+0.5)/6 #The 0.5 prevents log(0) = -inf\n        \n        #Now chop x and y apart\n        x = xy[:-1,:]\n        y = xy[-1,:]\n        \n        IDs.append(ids)\n        X.append(x)\n        Y.append(y)\n    #Now make those lists into arrays\n    IDs = np.array(IDs)\n    Y = np.array(Y)\n    X = np.array(X)\n    # Force these babies to be float32 instead of uint8 and float64\n    IDs = IDs.astype(np.float32, copy=False)\n    Y = Y.astype(np.float32, copy=False)\n    X = X.astype(np.float32, copy=False)\n        \n    return IDs, X, Y","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Generator version"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Next, a generator version that can be used endlessly! Mwahahaha\ndef generate_data(m, n_days, places, df_daydata, df_constdata, A_noise):\n    '''Function version of data generator for making X and Y\n    INPUTS\n    m: number of training examples\n    n_days: number of consecutive days to use = number of input elements\n    places: list of all unique places to generate predictions for\n    df_daydata: dataframe containing daily elements for all places\n    df_constdata: dataframe containing constant elements for all places\n    A_noise: amplitude of noise to add to X cases and fatalities\n    '''\n    \n    #Now, m-sized make batches of training data\n    #note: \"while True:\" is an infinite loop to keep generator going during model.fit\n    while True:\n        \n        IDs = [] # Id\n        X = [] # Data examples\n        Y = [] # Labels\n\n        for i in range(m):\n            # 1. Randomly select which place to use\n            country,ps = random.choice(places)\n            #rint(country)\n            #print(ps)\n            #Now see if a province or state is given\n            #type(var) == str\n            if type(ps)!=str:\n                #print('Its nan')\n                df_country = df_train[(df_train['Province_State'].isnull()) &\n                                      (df_train[\"Country_Region\"]==country)]\n            else:\n                #print('not nan')\n                df_country = df_train[(df_train['Province_State']==ps) &\n                            (df_train[\"Country_Region\"]==country)]\n            Data = df_country[['Id','ConfirmedCases','Fatalities']].to_numpy()\n            Data = Data.astype(np.float) #Convert these to float since apparently they're strings '0.0'\n\n            # 2. Randomly select n_days consecutive values, making sure they all fit\n            day0range = len(Data) - n_days\n            day0 = np.random.randint(day0range)\n            ids = Data[day0:(day0+n_days),0]\n            #x = Data[day0:(day0+n_days-1),1:]\n            #y = Data[(day0+n_days),1:]\n            xy = Data[day0:(day0+n_days),1:]\n\n            # 3. Add a little noise to augment the x and y data, if desired\n            # Uniform random noise about zero\n            #x_noise = 2*np.random.rand(*x.shape)-1\n            #y_noise = 2*np.random.rand()-1\n            xy_noise = 2*np.random.rand(*xy.shape)-1\n            #x = x*(1+A_noise*x_noise)\n            #y = y*(1+A_noise*y_noise)\n            xy = xy*(1+A_noise*xy_noise)\n            #Make sure it's still monotonically increasing though since it's cumulative!\n            #Use np.maximum.accumulate along the column axis\n            xy = np.maximum.accumulate(xy,axis=0)\n            #Then make them all back into integer numbers? Meh don't bother since logging\n\n            # 4. Scale x and y. I think log is a good idea. Lets assume normalize by log(10^6)=6\n            #Should give us a good number between 0 and 1 (unless things really get out of hand)\n            #x = np.log10(x+0.5)/6 #The 0.5 prevents log(0) = -inf\n            #y = np.log10(y+0.5)/6 #The 0.5 prevents log(0) = -inf\n            xy = np.log10(xy+0.5)/6 #The 0.5 prevents log(0) = -inf\n\n            #Now chop x and y apart\n            x = xy[:-1,:]\n            y = xy[-1,:]\n\n            IDs.append(ids)\n            X.append(x)\n            Y.append(y)\n        #Now make those lists into arrays\n        IDs = np.array(IDs)\n        Y = np.array(Y)\n        X = np.array(X)\n        # Force these babies to be float32 instead of uint8 and float64\n        IDs = IDs.astype(np.float32, copy=False)\n        Y = Y.astype(np.float32, copy=False)\n        X = X.astype(np.float32, copy=False)\n        \n        #NOTE: I took IDs out of the generator!\n        yield X, Y","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Create Data\nIf we just want to do it with a function instead of a function generator, use this:"},{"metadata":{"trusted":true},"cell_type":"code","source":"#TRAIN DATA\nm=1000\nn_days = 41\nA_noise = 0.1\nIDs_train, X_train, Y_train = generate_data_fn(m,n_days, places, df_train, df_train, A_noise)\nprint(\"IDs shape: \" + str(IDs_train.shape))\nprint(\"X shape: \" + str(X_train.shape))\nprint(\"Y shape: \" + str(Y_train.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Check train data to make sure it looks reasonable\nk=0\nprint(\"The kth place's last values of X are: \" + str(X_train[k,-1,:]))\nprint(\"The successive values of Y are: \" + str(Y_train[k,:]))\n\n# summarize history for loss\nplt.rcParams['figure.facecolor'] = 'w'\nplt.plot(X_train[k,:,0])# confirmed casescases\nplt.plot(X_train[k,:,1])# fatalities\nplt.scatter(41,Y_train[k,0])# confirmed casescases\nplt.scatter(41,Y_train[k,1])# fatalities\nplt.legend(['Xcc', 'Xf', 'Ycc', 'Yf'])#, loc='upper right') #toggle\nplt.title('Log Scale cases/fatalities')\nplt.ylabel('n')\nplt.xlabel('day')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#DUMMY TEST DATA - later import the real test sets\nm_test=20\nn_days = 41\nA_testnoise = 0\nIDs_test, X_test, Y_test = generate_data_fn(m_test,n_days, places, df_train, df_train, A_testnoise)\nprint(\"IDs shape: \" + str(IDs_test.shape))\nprint(\"X shape: \" + str(X_test.shape))\nprint(\"Y shape: \" + str(Y_test.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Check test data to make sure it looks reasonable\nk=9\nprint(\"The kth place's last values of X are: \" + str(X_test[k,-1,:]))\nprint(\"The successive values of Y are: \" + str(Y_test[k,:]))\n\n# summarize history for loss\nplt.rcParams['figure.facecolor'] = 'w'\nplt.plot(X_test[k,:,0])# confirmed casescases\nplt.plot(X_test[k,:,1])# fatalities\nplt.scatter(41,Y_test[k,0])# confirmed casescases\nplt.scatter(41,Y_test[k,1])# fatalities\nplt.legend(['Xcc', 'Xf', 'Ycc', 'Yf'])#, loc='upper right') #toggle\nplt.title('Log Scale cases/fatalities')\nplt.ylabel('n')\nplt.xlabel('day')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Run Model\n## Instantiate Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Step 1: create the model\n\n#del model #if you need to get rid of an old one\n\n############### MAKE NEW MODEL ################\n#input shape = shape of the data training/test examples\nmodel = inceptionModel(input_shape = (X_train.shape[1], X_train.shape[2]))\n#MODEL OPTIONS: inceptionModel #GRUmodel #LSTMmodel\n\n############### LOAD MODEL ################\n#Or load a previously trained, saved version of your model. To do so:\n#model = load_model('models/InceptionModelv1_2020-03-31.h5') #or whatever it is","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#look at it\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Step 2: compile the model to configure the learning process. \n\"\"\"Choose the 3 arguments of compile() wisely.\"\"\"\n\n#Choose an optimizer\nlr = 0.00005\ndecay = 0.00001\nopt = Adam(lr=lr, beta_1=0.9, beta_2=0.999, decay=decay)\n#opt = SGD(lr=0.01) # More computationally efficient but slower down the gradient than Adam\n\n# compile the model using the accuracy defined above\nmodel.compile(loss='mean_squared_error', optimizer=opt, metrics=[\"accuracy\"])\n#or try LSTM...\n#LSTMmodel.compile(loss='binary_crossentropy', optimizer=opt, metrics=[\"accuracy\"])\n#use loss='binary_crossentropy' when you are using a binary classifier (2 classes)\n#use loss='categorical_crossentropy' for multiclass classification (3+ classes)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train Model"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# Step 3: train the model!!! Choose the number of epochs and the batch size.\n\"\"\" Batch size is usually a power of 2 ranging from around 8 to 128\n    Epochs can be however many you want. Start with a few and see how fast it learns\n    Note: if you run fit() again, the model will continue to train with the parameters \n    it has already learned instead of reinitializing them \"\"\"\n\n#Starting crap\n#model.fit(X_train, Y_train, batch_size = 5, epochs=4) \n#LSTMmodel.fit(X_train, Y_train, batch_size = 5, epochs=1) \n#later increase the number of epochs and start with a higher learning rate, eg lr = 0.005\n\n#################################\n#FUNCTION VERSION\n#################################\n#This uses X_train and Y_train data that you already created with the generate_data_fn function\n#IDs_train, X_train, Y_train = generate_data_fn(m,n_days, places, df_train, df_train, A_noise)\n'''history = model.fit(X_train, Y_train,\n          batch_size=5,\n          epochs=20,\n          #validation_data=([te_pairs[:, 0], te_pairs[:, 1]], te_y)\n         )'''\n\n#################################\n#TRAIN MODEL USING DATA GENERATOR\n#################################\n# Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated\n#Use Model.fit instead now, which supports generators.\nbatch_size = 40 #this is the batch size for the generator, use this instead of m_train\nn_days = 40 #This is the number of days that will be input to the model, 1 less than other fn\nA_noise = 0.08\nsteps_per_epoch = 10\nepochs = 200\ndf_daydata = df_train\ndf_constdata = df_train #just as a placeholder for now\n#generate_data(m, n_days, places, df_daydata, df_constdata, A_noise)\ndataGen = generate_data(batch_size, n_days, places, df_daydata, df_constdata, A_noise)\n\n#Could set up a validation set the same way if desired\n#validation_data=([x1, x2], y)\n\nhistory = model.fit(dataGen,\n                    steps_per_epoch=steps_per_epoch, #how many batches to use per epoch\n                    #validation_data=valGen,\n                    #validation_steps=4\n                    epochs=epochs)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"LSTM Model:\n* Min loss so far: 0.1843... fiddling with various parameters. Should probably get rid of sigmoids in model.\n\nGRU Model:\n* Min loss so far: \n\nInception Model:\n* Min loss is 0.0018\n* Yay!"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plot the LEARNING CURVE\n\n#matplotlib.use('QT4Agg')\n#plt.get_backend()\n#with plt.xkcd():\nplt.rcParams[\"figure.figsize\"]=8,8\nplt.rcParams['figure.facecolor'] = 'w'\nplt.rcParams.update({'font.size': 20})\nplt.plot(history.history['loss'])\n#plt.plot(history.history['val_loss']) #toggle\n#opt = Adam(lr=0.01, beta_1=0.9, beta_2=0.999, decay=0.01) batch_size, steps_per_epoch\nplt.title('lr='+str(lr) +\n          ', decay='+str(decay)#+ \n          #', batch_sz='+str(batch_size)+\n          #', steps/epoch='+str(steps_per_epoch)\n         )\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\n#plt.legend(['Training Dataset', 'Validataion Dataset'])#, loc='upper right') #toggle\n#plt.annotate('OOH LOOK AT THIS,\\nPYTHON HAS A COOL\\nXKCD PLOT PACKAGE',\n#             xy=(3, 25), arrowprops=dict(arrowstyle='->'), xytext=(12, 12))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Evaluate model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Step 4: test/evaluate the model using the dev/test dataset \n\"\"\" Ideally it hasn't seen this data before or been trained on it \"\"\"\n\npreds = model.evaluate(x = X_test, y = Y_test)#or try LSTM...\n#preds = LSTMmodel.evaluate(x = X_test, y = Y_test)\nprint (\"Loss = \" + str(preds[0]))\nprint (\"Test Accuracy = \" + str(preds[1]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Predict with Model\n### Dummy Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Step 5: Use your model to generate predictions on dummy test set first\ntestpred = model.predict(X_test)\nprint(\"Here are three predictions: \" + str(testpred))\nprint(\"Here are the actual values: \" + str(Y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Check data to make sure it looks reasonable\nk=7\nprint(\"The kth place's last values of X are: \" + str(X_test[k,-1,:]))\nprint(\"The successive values of Y are: \" + str(Y_test[k,:]))\nprint(\"The model's predicted values of Yhat are: \" + str(testpred[k,:]))\n\n# summarize history for loss\nplt.rcParams['figure.facecolor'] = 'w'\nplt.plot(X_test[k,:,0])# confirmed casescases\nplt.plot(X_test[k,:,1])# fatalities\nplt.scatter(41,Y_test[k,0])# confirmed casescases\nplt.scatter(41,Y_test[k,1])# fatalities\nplt.scatter(41,testpred[k,0],marker=\"v\")# confirmed casescases\nplt.scatter(41,testpred[k,1],marker=\"v\")# fatalities\nplt.legend(['Xcc', 'Xf', 'Ycc', 'Yf','Tcc', 'Tf'])#, loc='upper right') #toggle\nplt.title('Log Scale cases/fatalities')\nplt.ylabel('n')\nplt.xlabel('day')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Now actually generate predictions for entire 28 day forecast window\nTo do this:\n* predict one day at a time\n* assume prediction is correct \n* make another prediction assuming first prediction is correct, n times for each location. Figure out how to map those IDs in here from the test stuff. I guess make a test data maker function.\n\nOnly use data prior to 2020-03-19 for predictions on the public leaderboard period.\n* Train dates: 2020-01-22 to 2020-03-26 = 65 days\n* Test dates: 2020-03-19 to 2020-04-30 = 43 days\n* Submission: 12642 entries = the number of entries in test.csv\n "},{"metadata":{},"cell_type":"markdown","source":"WARNING: THIS WILL TAKE A WHILE TO RUN"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"#Create X input for each date in test\n\n#We already have a list of all the places\n#print(\"Total number of places: \" + str(len(places)))\n\n#Go through each place, and see how many training examples we need for each\nlastday = '2020-03-19'\nlastday = datetime.strptime(lastday, \"%Y-%m-%d\") #string to date\n\npredictions = [] #store predictions in a list for now\n\nfor place in places:\n    country,ps = place\n    print(\"Now processing: \" + str(place))\n    #First, get the data for that place\n    if type(ps)!=str:\n        #print('Its nan')\n        df_place_train = df_train[(df_train['Province_State'].isnull()) &\n                                  (df_train[\"Country_Region\"]==country)]\n        df_place_test = df_test[(df_test['Province_State'].isnull()) &\n                                  (df_test[\"Country_Region\"]==country)]\n    else:\n        #print('not nan')\n        #First, get that place's training data for the n_days preceeding the forecast period\n        df_place_train = df_train[(df_train['Province_State']==ps) &\n                            (df_train[\"Country_Region\"]==country)]\n        #Also, get the true data to make the ground label (and get the forecast ID)\n        df_place_test = df_test[(df_test['Province_State']==ps) &\n                            (df_test[\"Country_Region\"]==country)]\n    #print(\"len of df_place_test: \" + str(len(df_place_test)))\n    ids_test= df_place_test['ForecastId'].to_numpy()\n    \n    #Now slice the desired date range to make x0,\n    #the first input data sample for that place\n    #Specify end and periods, the number of periods (days).\n    daterange0 = pd.date_range(end=lastday, periods=n_days) \n    #print(daterange0)\n    #Force Date into the right format\n    #df['date'] = pd.to_datetime(df['date']) \n    df_data0 = df_place_train[pd.to_datetime(df_place_train['Date']).isin(daterange0)]\n    #print(\"len of df_data0: \" + str(len(df_data0)))\n    #Now make an array of the columns we care about\n    data0 = df_data0[['Id','ConfirmedCases','Fatalities']].to_numpy()\n    data0 = data0.astype(np.float) #Convert these to float since apparently they're strings '0.0'\n    #Slice it into 2 arrrays\n    ids_train = data0[:,0] #just in case we need it later\n    x0 = data0[:,1:] #This is the first data sample to run through the network\n    # 4. Log scale x and normalize by log(10^6)=6\n    #Should give us a good number between 0 and 1 (unless things really get out of hand)\n    x0 = np.log10(x0+0.5)/6 #The 0.5 prevents log(0) = -inf\n    #It expects some value for m. Put in a 1\n    x = np.expand_dims(x0, axis=0) #x will have shape (1, 40, 2) = (m,day,elements)\n    #print(\"First x shape: \" + str(x.shape))\n    \n    for id in ids_test:\n        #Now, run that through the model to generate a prediction.\n        pred = model.predict(x) #pred will have shape (1,2)\n        #Make sure that prediction is greater than or equal to the preceeding value\n        if pred[0,0]<x[0,-1,0]:\n            pred[0,0] = x[0,-1,0]\n        if pred[0,1]<x[0,-1,1]:\n            pred[0,1] = x[0,-1,1]\n        #And since I trained it to generate log scale stuff, will already be log scaled\n        predictions.append([id,pred[0][0], pred[0][1]])\n        newrow = np.expand_dims(pred, axis=0) #x will have shape (1, 1, 2) = (m,day,elements)\n        #print(\"Newrow shape: \" + str(newrow.shape))\n        #Now chop the first day out of our previous value of x and tack newrow on the end\n        #np.concatenate((a, b), axis=0)\n        x = np.concatenate((x[:,1:,:],newrow), axis=1)\n        #print(\"Next x shape: \" + str(x.shape))\n        \n#Now make those lists into arrays\npredictions = np.array(predictions)\n# Force these babies to be float32 instead of uint8 and float64\n#predictions = predictions.astype(np.float32, copy=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Look at Predictions\nPlot predictions with the preceeding training data, showing the week of overlap so we can gauge how good or bad the predictions are."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Predictions shape:\" + str(predictions.shape))\nprint(\"Expected number of pedictions:\" + str(len(df_submission)))\nprint(\"First prediction ID:\" + str(predictions[0,0]))\nprint(\"Last prediction ID:\" + str(predictions[-1,0]))\nprint(\"First test ID:\" + str(df_test.loc[0 ,['ForecastId'] ]))\nprint(\"Last test ID:\" + str(df_test.loc[len(predictions)-1 ,['ForecastId'] ]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Post process predictions back from weirdo log scale stuff\n#pred = np.log10(x+0.5)/6\npredictions = predictions.astype(np.float) #Convert these to float since apparently they're strings '0.0'\nlinearpreds = np.copy(predictions) #Copy predictions over and then un-log the values\nlinearpreds[:,1:] = 10**(6*linearpreds[:,1:]) - 0.5 #This is screwy\n#Now round them all off to the nearest integer\nlinearpreds = np.around(linearpreds, decimals=1) # np.rint(linearpreds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Look at the log predictions along with the log inputs to see how the trends look\n\n#First, get all the data for each country from the training set\nallTrainData = df_train[['Id','ConfirmedCases','Fatalities']].to_numpy()\nallTrainData = allTrainData.astype(np.float) #Convert these to float since apparently they're strings '0.0'\nallTrainLogData = np.log10(allTrainData+0.5)/6 #The 0.5 prevents log(0) = -inf\nallTrainLogData[:,0] = allTrainData[:,0]\n\ntrainperplace = len(dates_train)\npredsperplace = len(dates_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"i=4 #Start at 1\n\n# Plot the log stuff\nplt.rcParams['figure.facecolor'] = 'w'\n#Remember predictions only overlap the training data by 1 week, \n#so shift them into the negative before preds with 1 week overlap\nxtr = np.arange(trainperplace)\nxpr = np.arange(predsperplace) + trainperplace - 7\n#days to plot\nt0 = ((i-1)*trainperplace)\nt1 = (i*trainperplace)\np0 = ((i-1)*predsperplace)\np1 = (i*predsperplace)\n#Training data\nplt.plot(xtr, allTrainLogData[t0:t1,1])# confirmed casescases\nplt.plot(xtr, allTrainLogData[t0:t1,2])# fatalities\n#Predictions\nplt.plot(xpr, predictions[p0:p1,1])# confirmed casescases\nplt.plot(xpr,predictions[p0:p1,2])# fatalities\nplt.axvline(x=trainperplace - 7,color='y', linestyle='--')\nplt.legend(['Actual cases', 'Actual fatalitites', \n            'Predicted cases', 'Predicted fatalities'], loc='upper left') #toggle\nplt.title('Log Scale cases/fatalities')\nplt.ylabel('n')\nplt.xlabel('day')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot the linear stuff\nplt.rcParams['figure.facecolor'] = 'w'\n\n#Training data\nplt.plot(xtr, allTrainData[t0:t1,1])# confirmed casescases\nplt.plot(xtr, allTrainData[t0:t1,2])# fatalities\n\n#Predictions\nplt.plot(xpr, linearpreds[p0:p1,1])# confirmed casescases\nplt.plot(xpr,linearpreds[p0:p1,2])# fatalities\nplt.axvline(x=trainperplace - 7,color='y', linestyle='--')\nplt.legend(['Actual cases', 'Actual fatalitites', \n            'Predicted cases', 'Predicted fatalities'], loc='upper left') #toggle\nplt.title('Linear Scale cases/fatalities')\nplt.ylabel('n')\nplt.xlabel('day')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I guess this model is optimistic! Good enough for a first try, anyway."},{"metadata":{},"cell_type":"markdown","source":"### Save Predictions\nIn a csv file"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Generate submission\nlinearpreds = linearpreds.astype(np.int) #Convert these to integers %10.5f\n#np.savetxt(\"submission.csv\", linearpreds, fmt='%d', delimiter=\",\")\n#Whoops need header, ugh just use pandas again and hope for the right format\ndf = pd.DataFrame({'ForecastId': linearpreds[:, 0], \n                   'ConfirmedCases': linearpreds[:, 1], \n                   'Fatalities': linearpreds[:, 2]})\ndf.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Save Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"############## TO SAVE ##############\nmodel.save('InceptionModelv0_' + str(date.today()) + '.h5')  # creates a HDF5 file\n\n############## TO LOAD ##############\n#model = load_model('models/InceptionModelv0_29032020_.h5')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Still need to add in lots of cool other data and then see what effect it has"},{"metadata":{},"cell_type":"markdown","source":""}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.7"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{"height":"calc(100% - 180px)","left":"10px","top":"150px","width":"336.875px"},"toc_section_display":true,"toc_window_display":true}},"nbformat":4,"nbformat_minor":4}