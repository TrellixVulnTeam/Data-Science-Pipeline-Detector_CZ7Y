{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy.optimize import curve_fit\nfrom datetime import timedelta \nfrom tqdm import tqdm_notebook as tqdm\nimport os\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"path = '/kaggle/input/covid19-global-forecasting-week-2'\n\ntrain = pd.read_csv(os.path.join(path, 'train.csv'))\ntest = pd.read_csv(os.path.join(path, 'test.csv'))\nsubm = pd.read_csv(os.path.join(path, 'submission.csv'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"valid_date = pd.to_datetime('2020-04-01')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['area'] = train['Country_Region'].astype(str) + '_' + train['Province_State'].astype(str)\ntest['area'] = test['Country_Region'].astype(str) + '_' + test['Province_State'].astype(str)\ntrain['Date'] = pd.to_datetime(train['Date'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_train_piece(area, valid_date):\n    data = train[(train.area == area) & (train.Date < valid_date)].reset_index()\n    data = data[data['ConfirmedCases'] > 0].reset_index(drop = True)\n    return data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_log_error","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['ratio'] = train['Fatalities'] / train['ConfirmedCases']\ngg = train.drop_duplicates('area', keep = 'last')\nmean_fat = gg[gg['ConfirmedCases'] > 1000].ratio.mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"version = 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dict_kakaha = {'US_Puerto Rico':(30000, 80000), 'US_Idaho':(30000, 80000)}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_df = pd.DataFrame()\n\nfor pred_area in tqdm(test.area.unique()):\n    train_df = get_train_piece(pred_area, valid_date)\n    len_train = train_df.shape[0]\n    \n    test_df = test[test.area == pred_area].reset_index(drop = True)\n    len_test = test_df.shape[0]\n    \n    ans = pd.DataFrame()\n    ans['ForecastId'] = test_df['ForecastId'].values\n    \n    if pred_area in dict_kakaha:\n        def log_curve(x, x0, k):\n            return dict_kakaha[pred_area][version] / (1 + np.exp(-k*(x-x0)))\n        popt, pcov = curve_fit(log_curve, list(train_df.index), train_df['ConfirmedCases'].values, \n                               bounds=([0,0],np.inf), \n                               p0=[10,0.3], maxfev=1000000)\n        pred = []\n        pred_fat = []\n\n        cur_fat = train_df['ratio'].values[-1]\n        cur_rat = (cur_fat * train_df['ConfirmedCases'].values[-1] + 10 * mean_fat) / (train_df['ConfirmedCases'].values[-1] + 10)\n\n        for x in range(len_train, len_train + len_test):\n            pred += [log_curve(x, popt[0], popt[1])]\n            pred_fat += [max(pred[-1] * cur_rat, train_df['Fatalities'].values[-1])]\n        ans['ConfirmedCases'] = pred\n        ans['Fatalities'] = pred_fat\n    else:\n        def log_curve(x, x0, k, ymax):\n            return ymax / (1 + np.exp(-k*(x-x0)))\n        popt, pcov = curve_fit(log_curve, list(train_df.index), train_df['ConfirmedCases'].values, \n                               bounds=([0,0, 0],[np.inf, np.inf, 150000]), \n                               p0=[10,0.3,10000], maxfev=1000000)\n        pred = []\n        pred_fat = []\n\n        cur_fat = train_df['ratio'].values[-1]\n        cur_rat = (cur_fat * train_df['ConfirmedCases'].values[-1] + 10 * mean_fat) / (train_df['ConfirmedCases'].values[-1] + 10)\n\n        for x in range(len_train, len_train + len_test):\n            pred += [log_curve(x, popt[0], popt[1], popt[2])]\n            pred_fat += [max(pred[-1] * cur_rat, train_df['Fatalities'].values[-1])]\n        ans['ConfirmedCases'] = pred\n        ans['Fatalities'] = pred_fat\n    \n    pred_df = pd.concat([pred_df, ans], axis = 0).reset_index(drop = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_df.tail(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":" pred_df.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}