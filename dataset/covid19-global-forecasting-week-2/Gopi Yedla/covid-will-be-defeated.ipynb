{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nfrom fbprophet import Prophet\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"path = '/kaggle/input/covid19-global-forecasting-week-2/'\npd.set_option('display.max_rows', None)\ntrain_df = pd.read_csv(path+'train.csv')\ntest_df = pd.read_csv(path+'test.csv')\nsubmission_df = pd.read_csv(path+'submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df['Date']= pd.to_datetime(test_df['Date']) \ntrain_df['Date']= pd.to_datetime(train_df['Date']) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"null_vals = pd.DataFrame(round ( 100 * (train_df.isnull().sum() / len(train_df.index)), 3))\nnull_vals.columns = ['% null values']\nnull_vals","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"null_vals = pd.DataFrame(round ( 100 * (test_df.isnull().sum() / len(test_df.index)), 3))\nnull_vals.columns = ['% null values']\nnull_vals","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['Province_State'].fillna('NO_STATE', inplace=True)\ntest_df['Province_State'].fillna('NO_STATE', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_grouped_df = train_df.groupby(['Country_Region', 'Province_State'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cluster0 = ['Botswana', 'Fiji', 'Gabon', 'Libya', 'Seychelles', 'Suriname', 'Mongolia', 'El Salvador', 'Barbados', 'Jamaica', 'Paraguay', 'Trinidad and Tobago','Montenegro', 'Georgia', 'Mauritius', 'Venezuela', 'Belarus', 'Malta', 'Oman', 'Albania', 'Cyprus', 'Kuwait', 'Jordan', 'Azerbaijan','Kazakhstan', 'Uruguay', 'Costa Rica', 'Bulgaria', 'Tunisia', 'Bosnia and Herzegovina', 'Latvia', 'Lebanon', 'Hungary', 'Armenia','Lithuania', 'Bahrain', 'Ukraine', 'Algeria', 'United Arab Emirates', 'Qatar', 'Estonia', 'Slovenia', 'Serbia', 'Colombia', 'Argentina','Dominican Republic', 'Peru', 'Mexico', 'Panama', 'Greece', 'South Africa', 'Indonesia', 'Saudi Arabia', 'Thailand', 'Ecuador', 'Poland','Romania', 'Malaysia', 'Brazil', 'Israel', 'Portugal', 'Turkey']\ncluster0_confirmed_cap = 1500\ncluster0_fatality_cap = 750\n\ncluster1 = ['Italy', 'Spain']\ncluster1_confirmed_cap = 50\ncluster1_fatality_cap = 25\n\ncluster2 = ['Iceland', 'Luxembourg', 'Switzerland']\ncluster2_confirmed_cap = 500\ncluster2_fatality_cap = 250\n\ncluster3 = ['Angola', 'Bangladesh', 'Belize', 'Benin', 'Bhutan', 'Burkina Faso', 'Cambodia', 'Cameroon', 'Chad', 'Egypt', 'Ethiopia', 'Ghana', 'Guatemala', 'Guinea', 'Guyana', 'Haiti', 'Honduras', 'Iraq', 'Kenya', 'Liberia', 'Madagascar', 'Mali', 'Mauritania', 'Morocco', 'Mozambique', 'Namibia', 'Nepal','Nicaragua', 'Niger', 'Nigeria', 'Pakistan', 'Papua New Guinea', 'Philippines', 'Rwanda', 'Senegal', 'Sri Lanka', 'Sudan', 'Togo', 'Uganda','Zambia', 'Zimbabwe']\ncluster3_confirmed_cap = 300\ncluster3_fatality_cap = 150\n\ncluster4 = ['China']\ncluster4_confirmed_cap = 1.5\ncluster4_fatality_cap = 1.35\n\ncluster5 = ['Hong Kong', 'Singapore']\ncluster5_confirmed_cap = 40\ncluster5_fatality_cap = 28\n\ncluster6 = ['United States']\ncluster6_confirmed_cap = 200\ncluster6_fatality_cap = 140\n\ncluster6 = ['United States']\ncluster6_confirmed_cap = 500\ncluster6_fatality_cap = 350\n\ncluster7 = ['Australia', 'Austria', 'Belgium', 'Canada', 'Chile', 'Denmark', 'Finland', 'Ireland', 'Japan', 'Netherlands', 'New Zealand', 'Norway', 'South Korea','Sweden', 'United Kingdom']\ncluster7_confirmed_cap = 100\ncluster7_fatality_cap = 70\n\ncluster8 = ['France', 'Germany', 'Iran']\ncluster8_confirmed_cap = 50\ncluster8_fatality_cap = 35\n\ncluster9 = ['India']\ncluster9_confirmed_cap = 30\ncluster9_fatality_cap = 21\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions_dataset =  pd.DataFrame([])\n\nfor key, state_df in train_grouped_df:\n    print('Key:', key)\n    #create the dataset and do the prediction\n    train_start_date = state_df['Date'].iloc[0]\n    state_df['days_elapsed'] = (state_df['Date'] - train_start_date).dt.days\n    train_state_df = state_df.drop(['Id' , 'Country_Region', 'Province_State'], axis = 1)\n    #print(train_state_df.shape)\n\n    y_train_case_count =  train_state_df['ConfirmedCases']\n    y_train_fatalities =  train_state_df['Fatalities']\n\n    x_train_state_case_df = train_state_df.drop(['Fatalities'], axis = 1)\n    x_train_state_fatalities_df = train_state_df.drop('ConfirmedCases', axis = 1)\n\n    test_state_df = pd.DataFrame([])\n    test_state_df = test_df[ (test_df['Country_Region'] == key[0]) & (test_df['Province_State'] == key[1])]\n    #test_state_df_copy = test_state_df.copy()\n    test_state_df['days_elapsed'] = (test_state_df['Date'] - train_start_date).dt.days\n    #print('test_state_df----', test_state_df)\n    x_pred_state_df = test_state_df.drop(['ForecastId' ,'Province_State', 'Country_Region'], axis = 1)\n    \n    \n    prophet_conf = Prophet(growth='logistic')\n    prophet_fatalities = Prophet(growth='logistic')\n    train_dataset =  pd.DataFrame()\n    test_dataset =  pd.DataFrame()\n    train_dataset['y'] = x_train_state_case_df['ConfirmedCases']\n    train_dataset['ds'] = x_train_state_case_df['Date'] \n    #train_dataset['cap'] = 2.0\n    #test_dataset['cap'] = 2.0\n    test_dataset['ds'] = test_state_df['Date']\n    \n    train_current_fatalities = train_state_df['Fatalities'].max()\n    train_current_cases = train_state_df['ConfirmedCases'].max()\n    train_min_fatalities = train_state_df['Fatalities'].min()\n    train_min_cases = train_state_df['ConfirmedCases'].min()\n\n    train_dataset['cap'] =  train_current_cases * 1000\n    test_dataset['cap'] =  train_current_cases * 1000\n\n    \n    train_dataset['floor'] =  0\n    test_dataset['floor'] =  0\n    \n    \n    #print(test_dataset)\n    #test_dataset['']\n    \n    train_cases_cap = train_current_cases\n    if train_current_fatalities == 0 :\n        train_current_fatalities = 1\n    if train_current_cases == 0 :\n        train_current_cases = 1\n\n    test_dataset['cap'] =  train_current_cases * 1000\n    \n    if train_current_cases > 50000 :\n        train_dataset['cap'] =  train_current_cases * 50\n        test_dataset['cap'] =  train_current_cases * 50\n\n    if (train_current_cases > 20000 and train_current_cases <= 50000):\n        train_dataset['cap'] =  train_current_cases * 100\n        test_dataset['cap'] =  train_current_cases * 100\n\n    if (train_current_cases > 10000 and train_current_cases <= 20000):\n        train_dataset['cap'] =  train_current_cases * 200\n        test_dataset['cap'] =  train_current_cases * 200\n\n    if (train_current_cases > 5000 and train_current_cases <= 10000):\n        train_dataset['cap'] =  train_current_cases * 400\n        test_dataset['cap'] =  train_current_cases * 400\n\n    if (train_current_cases > 1000 and train_current_cases <= 5000):\n        train_dataset['cap'] =  train_current_cases * 600\n        test_dataset['cap'] =  train_current_cases * 600\n\n    if (train_current_cases > 100 and train_current_cases <= 1000):\n        train_dataset['cap'] =  train_current_cases * 800\n        test_dataset['cap'] =  train_current_cases * 800\n\n    if (train_current_cases > 0 and train_current_cases <= 100):\n        train_dataset['cap'] =  train_current_cases * 1000\n        test_dataset['cap'] =  train_current_cases * 1000\n        \n\n    if key[0] in cluster0 :\n        train_dataset['cap'] =  train_current_cases * cluster0_fatality_cap\n        test_dataset['cap'] =  train_current_cases * cluster0_fatality_cap\n\n    if key[0] in cluster1 :\n        train_dataset['cap'] =  train_current_cases * cluster1_fatality_cap\n        test_dataset['cap'] =  train_current_cases * cluster1_fatality_cap\n\n    if key[0] in cluster2 :\n        train_dataset['cap'] =  train_current_cases * cluster2_fatality_cap\n        test_dataset['cap'] =  train_current_cases * cluster2_fatality_cap\n        \n    if key[0] in cluster3 :\n        train_dataset['cap'] =  train_current_cases * cluster3_fatality_cap\n        test_dataset['cap'] =  train_current_cases * cluster3_fatality_cap\n\n    if key[0] in cluster4 :\n        train_dataset['cap'] =  train_current_cases * cluster4_fatality_cap\n        test_dataset['cap'] =  train_current_cases * cluster4_fatality_cap\n\n    if key[0] in cluster5 :\n        train_dataset['cap'] =  train_current_cases * cluster5_fatality_cap\n        test_dataset['cap'] =  train_current_cases * cluster5_fatality_cap\n\n#    if key[0] in cluster6 :\n#        train_dataset['cap'] =  train_current_fatalities * cluster6_fatality_cap\n#        test_dataset['cap'] =  train_current_fatalities * cluster6_fatality_cap\n\n    if key[0] in cluster7 :\n        train_dataset['cap'] =  train_current_cases * cluster7_fatality_cap\n        test_dataset['cap'] =  train_current_cases * cluster7_fatality_cap\n\n    if key[0] in cluster8 :\n        train_dataset['cap'] =  train_current_cases * cluster8_fatality_cap\n        test_dataset['cap'] =  train_current_cases * cluster8_fatality_cap\n\n    if key[0] in cluster9 :\n        train_dataset['cap'] =  train_current_cases * cluster9_fatality_cap\n        test_dataset['cap'] =  train_current_cases * cluster9_fatality_cap\n        \n        \n    prophet_conf.fit(train_dataset)\n        \n        \n\n    y_case_predict = prophet_conf.predict(test_dataset)\n    #merged_test_state_df = pd.concat([test_state_df, y_case_predict], axis = 1)\n    #test_state_df['ConfirmedCases'] = y_case_predict\n    y_case_predict.rename(columns = {'trend':'ConfirmedCases'}, inplace = True) \n    test_state_df = pd.merge(test_state_df,  y_case_predict[['ConfirmedCases', 'ds']], left_on='Date', right_on = 'ds', how='left'  )\n\n    #test_state_df ['ConfirmedCases'] =   y_case_predict['yhat_upper']\n    \n    train_dataset['y'] = train_state_df['Fatalities']\n    test_dataset['cap'] =  train_current_fatalities * 1000\n\n    test_dataset['floor'] =  0\n    \n    train_dataset['cap'] =  train_current_fatalities * 1000\n    train_dataset['floor'] =  0\n\n    if train_current_cases > 50000 :\n        train_dataset['cap'] =  train_current_fatalities * 50\n        test_dataset['cap'] =  train_current_fatalities * 50\n\n    if (train_current_cases > 20000 and train_current_cases <= 50000):\n        train_dataset['cap'] =  train_current_fatalities * 70\n        test_dataset['cap'] =  train_current_fatalities * 70\n\n    if (train_current_cases > 10000 and train_current_cases <= 20000):\n        train_dataset['cap'] =  train_current_fatalities * 140\n        test_dataset['cap'] =  train_current_fatalities * 140\n\n    if (train_current_cases > 5000 and train_current_cases <= 10000):\n        train_dataset['cap'] =  train_current_fatalities * 280\n        test_dataset['cap'] =  train_current_fatalities * 280\n\n    if (train_current_cases > 1000 and train_current_cases <= 5000):\n        train_dataset['cap'] =  train_current_fatalities * 420\n        test_dataset['cap'] =  train_current_fatalities * 420\n\n    if (train_current_cases > 100 and train_current_cases <= 1000):\n        train_dataset['cap'] =  train_current_fatalities * 560\n        test_dataset['cap'] =  train_current_fatalities * 560\n\n        \n    if (train_current_cases > 0 and train_current_cases <= 100):\n        train_dataset['cap'] =  train_current_fatalities * 700\n        test_dataset['cap'] =  train_current_fatalities * 700\n        \n    if key[0] in cluster0 :\n        train_dataset['cap'] =  train_current_fatalities * cluster0_fatality_cap\n        test_dataset['cap'] =  train_current_fatalities * cluster0_fatality_cap\n        #print('0------', cluster0_fatality_cap)\n\n    if key[0] in cluster1 :\n        train_dataset['cap'] =  train_current_fatalities * cluster1_fatality_cap\n        test_dataset['cap'] =  train_current_fatalities * cluster1_fatality_cap\n        #print('1------', cluster1_fatality_cap)\n\n    if key[0] in cluster2 :\n        train_dataset['cap'] =  train_current_fatalities * cluster2_fatality_cap\n        test_dataset['cap'] =  train_current_fatalities * cluster2_fatality_cap\n        #print('2------', cluster2_fatality_cap)\n        \n    if key[0] in cluster3 :\n        train_dataset['cap'] =  train_current_fatalities * cluster3_fatality_cap\n        test_dataset['cap'] =  train_current_fatalities * cluster3_fatality_cap\n        #print('3------', cluster3_fatality_cap)\n\n    if key[0] in cluster4 :\n        train_dataset['cap'] =  train_current_fatalities * cluster4_fatality_cap\n        test_dataset['cap'] =  train_current_fatalities * cluster4_fatality_cap\n        #print('4------', cluster4_fatality_cap)\n\n    if key[0] in cluster5 :\n        train_dataset['cap'] =  train_current_fatalities * cluster5_fatality_cap\n        test_dataset['cap'] =  train_current_fatalities * cluster5_fatality_cap\n\n#    if key[0] in cluster6 :\n#        train_dataset['cap'] =  train_current_fatalities * cluster6_fatality_cap\n#        test_dataset['cap'] =  train_current_fatalities * cluster6_fatality_cap\n\n    if key[0] in cluster7 :\n        train_dataset['cap'] =  train_current_fatalities * cluster7_fatality_cap\n        test_dataset['cap'] =  train_current_fatalities * cluster7_fatality_cap\n\n    if key[0] in cluster8 :\n        train_dataset['cap'] =  train_current_fatalities * cluster8_fatality_cap\n        test_dataset['cap'] =  train_current_fatalities * cluster8_fatality_cap\n\n    if key[0] in cluster9 :\n        train_dataset['cap'] =  train_current_fatalities * cluster9_fatality_cap\n        test_dataset['cap'] =  train_current_fatalities * cluster9_fatality_cap\n\n    \n    prophet_fatalities.fit(train_dataset)\n    y_fatalities_predict = prophet_fatalities.predict(test_dataset)\n    #y_fatalities_predict.info()\n#    test_state_df ['Fatalities'] =  y_fatalities_predict['yhat_upper']\n    y_fatalities_predict.rename(columns = {'trend':'Fatalities'}, inplace = True) \n    test_state_df = pd.merge(test_state_df,  y_fatalities_predict[['Fatalities', 'ds']], left_on='Date', right_on = 'ds', how='left'  )\n    \n    predictions_dataset = predictions_dataset.append(test_state_df, ignore_index=True)\n    \n    #y_predict['Date'] = y_case_predict['ds']\n    #y_predict['Country/Region'] = key[0]  \n    #y_predict['Province/State'] = key[1]\n    #test_df = pd.merge(test_df, y_predict, on=['Country/Region', 'Province/State', 'Date'])\n    \n    #print('------------------------------------------------------', key)\n    #print( test_state_df)\n    #print('------------------------------------------------------')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions_dataset.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submissions = predictions_dataset.drop(['Country_Region', 'Province_State', 'Date', 'days_elapsed', 'ds_x', 'ds_y'], axis = 1)\n\nsubmissions['ConfirmedCases']  = submissions['ConfirmedCases'].astype(int)\nsubmissions['Fatalities']  = submissions['Fatalities'].astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submissions.to_csv ('submission.csv', index=False)\nprint('--Done--')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* "}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}