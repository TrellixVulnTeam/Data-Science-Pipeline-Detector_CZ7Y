{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_squared_log_error\n\nfrom keras.models import Sequential\nfrom keras.layers.core import Dense, Activation\nfrom keras.layers.recurrent import GRU\nfrom keras.optimizers import Adagrad\nfrom keras.callbacks import EarlyStopping\nfrom keras import backend as K\n\nimport datetime\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\nfont = {'family' : 'meiryo'}\nplt.rc('font', **font)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv(\"../input/covid19-global-forecasting-week-2/train.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = train_df[train_df[\"Date\"] < \"2020-03-19\"]\ntrain_df = train_df.fillna(\"No State\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_rate = 0.05\nmaxlen = 20\ntrain_date_count = len(set(train_df[\"Date\"]))\n\nX, Y = [],[]\n\nscaler = StandardScaler()\ntrain_df[\"ConfirmedCases_std\"] = scaler.fit_transform(train_df[\"ConfirmedCases\"].values.reshape(len(train_df[\"ConfirmedCases\"].values),1))\n\n#時系列モデル用に学習データを整形する\nfor state,country in train_df.groupby([\"Province_State\",\"Country_Region\"]).sum().index:\n    df = train_df[(train_df[\"Country_Region\"] == country) & (train_df[\"Province_State\"] == state)]\n    \n    #患者が0人の地域は予想不可⇒人為的に0で予想する\n    if df[\"ConfirmedCases\"].sum() != 0:\n        for i in range(len(df) - maxlen):\n            \n            #時系列データの患者が0人の場合は除外\n            if df[['ConfirmedCases']].iloc[i+maxlen].values != 0:\n                X.append(df[['ConfirmedCases_std']].iloc[i:(i+maxlen)].values)\n                Y.append(df[['ConfirmedCases_std']].iloc[i+maxlen].values)\n\nX=np.array(X)\nY=np.array(Y)\n    \nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_rate, shuffle = True ,random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"confirmedCases_std_min = train_df[\"ConfirmedCases_std\"].min()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\n\ndef huber_loss(y_true, y_pred, clip_delta=1.0):\n  error = y_true - y_pred\n  cond  = tf.keras.backend.abs(error) < clip_delta\n\n  squared_loss = 0.5 * tf.keras.backend.square(error)\n  linear_loss  = clip_delta * (tf.keras.backend.abs(error) - 0.5 * clip_delta)\n\n  return tf.where(cond, squared_loss, linear_loss)\n\ndef huber_loss_mean(y_true, y_pred, clip_delta=1.0):\n  return tf.keras.backend.mean(huber_loss(y_true, y_pred, clip_delta))\n\ndef rmsle(y, y_pred):\n    assert len(y) == len(y_pred)\n    terms_to_sum = [(math.log(y_pred[i] + 1) - math.log(y[i] + 1)) ** 2.0 for i,pred in enumerate(y_pred)]\n    return (sum(terms_to_sum) * (1.0/len(y))) ** 0.5","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"epochs_num = 20\nn_hidden = 300\nn_in = 1\n    \nmodel = Sequential()\nmodel.add(GRU(n_hidden,\n               batch_input_shape=(None, maxlen, n_in),\n               kernel_initializer='random_uniform',\n               return_sequences=False))\nmodel.add(Dense(n_in, kernel_initializer='random_uniform'))\nmodel.add(Activation(\"linear\"))\n\nopt = Adagrad(lr=0.01, epsilon=1e-08, decay=1e-4)\n#model.compile(loss = \"mean_squared_error\", optimizer=opt)\nmodel.compile(loss = huber_loss_mean, optimizer=opt)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"early_stopping = EarlyStopping(monitor='loss', patience=5, verbose=1)\n\nhist = model.fit(X_train, Y_train, batch_size=10, epochs=epochs_num,\n                 callbacks=[early_stopping],shuffle=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predicted_std = model.predict(X_test)\nresult_std= pd.DataFrame(predicted_std)\nresult_std.columns = ['predict']\nresult_std['actual'] = Y_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result_std.plot(figsize=(25,6))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loss = hist.history['loss']\nepochs = len(loss)\nfig = plt.figure()\nplt.plot(range(epochs), loss, marker='.', label='loss(training data)')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predicted = scaler.inverse_transform(predicted_std)\nY_test = scaler.inverse_transform(Y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#np.sqrt(mean_squared_log_error(predicted, Y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result= pd.DataFrame(predicted)\nresult.columns = ['predict']\nresult['actual'] = Y_test\nresult.plot(figsize=(25,6))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df = pd.read_csv(\"../input/covid19-global-forecasting-week-2/test.csv\")\ntest_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_c = pd.read_csv(\"../input/covid19-global-forecasting-week-2/submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"temp = (datetime.datetime.strptime(\"2020-03-18\", '%Y-%m-%d') - datetime.timedelta(days=maxlen)).strftime('%Y-%m-%d')\ntest_df = train_df[train_df[\"Date\"] > temp]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"check_df = pd.read_csv(\"../input/covid19-global-forecasting-week-2/train.csv\").query(\"Date>'2020-03-18'and Date<='2020-03-31'\")\ncheck_df[\"ConfirmedCases_std\"] = scaler.transform(check_df[\"ConfirmedCases\"].values.reshape(len(check_df[\"ConfirmedCases\"].values),1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"confirmedCases_pred = []\nfor i in range(0,294*maxlen,maxlen):\n    temp_array = np.array(test_df[\"ConfirmedCases_std\"][i:i+maxlen])\n    for j in range(43):\n        if j<13:\n            temp_array = np.append(temp_array,np.array(check_df[\"ConfirmedCases_std\"])[int(i*13/maxlen)+j])\n        elif np.array(test_df[\"ConfirmedCases\"][i:i+maxlen]).sum() == 0:\n            temp_array = np.append(temp_array,temp_array[-1])\n        else:\n            temp_array = np.append(temp_array,model.predict(temp_array[-maxlen:].reshape(1,maxlen,1)))\n    confirmedCases_pred.append(temp_array[-43:])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_c[\"ConfirmedCases\"] = np.abs(scaler.inverse_transform(np.array(confirmedCases_pred).reshape(294*43)))\nsubmission_c[\"ConfirmedCases_std\"] = np.array(confirmedCases_pred).reshape(294*43)\nsubmission_c","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_c.to_csv('./submission_c.csv')\nsubmission_c.to_csv('..\\output\\kaggle\\working\\submission_c.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_rate = 0.05\nmaxlen = 20\ntrain_date_count = len(set(train_df[\"Date\"]))\n\nX, Y = [],[]\n\nscaler = StandardScaler()\ntrain_df[\"Fatalities_std\"] = scaler.fit_transform(train_df[\"Fatalities\"].values.reshape(len(train_df[\"Fatalities\"].values),1))\n\nss = StandardScaler()\ntrain_df[\"ConfirmedCases_std\"] = ss.fit_transform(train_df[\"ConfirmedCases\"].values.reshape(len(train_df[\"ConfirmedCases\"].values),1))\n\n#時系列モデル用に学習データを整形する\nfor state,country in train_df.groupby([\"Province_State\",\"Country_Region\"]).sum().index:\n    df = train_df[(train_df[\"Country_Region\"] == country) & (train_df[\"Province_State\"] == state)]\n    \n    #患者と重傷者が0人の地域は予想不可\n    if df[\"Fatalities\"].sum() != 0 or df[\"ConfirmedCases\"].sum() != 0:\n        for i in range(len(df) - maxlen):\n            \n            #時系列データの患者と重傷者が0人の場合は除外\n            if (df[['ConfirmedCases']].iloc[i+maxlen].values != 0 or df[['Fatalities']].iloc[i+maxlen].values != 0):\n                X.append(df[['Fatalities_std','ConfirmedCases_std']].iloc[i:(i+maxlen)].values)\n                Y.append(df[['Fatalities_std']].iloc[i+maxlen].values)\n\nX=np.array(X)\nY=np.array(Y)\n    \nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_rate, shuffle = True ,random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fatalities_std_min = train_df[\"Fatalities_std\"].min()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"epochs_num = 25\nn_hidden = 300\nn_in = 2\n\nmodel = Sequential()\nmodel.add(GRU(n_hidden,\n               batch_input_shape=(None, maxlen, n_in),\n               kernel_initializer='random_uniform',\n               return_sequences=False))\nmodel.add(Dense(1, kernel_initializer='random_uniform'))\nmodel.add(Activation(\"linear\"))\n\nopt = Adagrad(lr=0.01, epsilon=1e-08, decay=1e-4)\n#model.compile(loss = \"mean_squared_error\", optimizer=opt)\nmodel.compile(loss = huber_loss_mean, optimizer=opt)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"early_stopping = EarlyStopping(monitor='loss', patience=5, verbose=1)\n\nhist = model.fit(X_train, Y_train, batch_size=8, epochs=epochs_num,\n                 callbacks=[early_stopping],shuffle=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The expected result may be negative because the loss function is MSE.\n\nBecause,restore by taking an absolute value."},{"metadata":{"trusted":true},"cell_type":"code","source":"predicted_std = model.predict(X_test)\nresult_std= pd.DataFrame(predicted_std)\nresult_std.columns = ['predict']\nresult_std['actual'] = Y_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result_std.plot(figsize=(25,6))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loss = hist.history['loss']\nepochs = len(loss)\nfig = plt.figure()\nplt.plot(range(epochs), loss, marker='.', label='loss(training data)')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predicted = scaler.inverse_transform(predicted_std)\nY_test = scaler.inverse_transform(Y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predicted","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test_ = scaler.inverse_transform(X_test)\nX_test_[9]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_test[9]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predicted[9]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#np.sqrt(mean_squared_log_error(predicted, Y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_df = submission_c","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"temp = (datetime.datetime.strptime(\"2020-03-18\", '%Y-%m-%d') - datetime.timedelta(days=maxlen)).strftime('%Y-%m-%d')\ntest_df = train_df[train_df[\"Date\"] > temp]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"check_df[\"Fatalities_std\"] = scaler.transform(check_df[\"Fatalities\"].values.reshape(len(check_df[\"Fatalities\"].values),1))\ncheck_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fatalities_pred = []\nfor i in range(0,294*maxlen,maxlen):\n    temp_array = np.array(test_df[[\"Fatalities_std\",\"ConfirmedCases_std\"]][i:i+maxlen])\n    for j in range(43):\n        if j<13:\n            temp_array = np.append(temp_array,np.append(np.array(check_df[\"Fatalities_std\"])[int(i*13/maxlen)+j],np.array(check_df[\"ConfirmedCases_std\"])[int(i*13/maxlen)+j]).reshape(1,2),axis=0)\n        elif np.array(test_df[[\"Fatalities\",\"ConfirmedCases\"]][i:i+maxlen]).sum() == 0:\n            temp_array = np.append(temp_array,np.array(temp_array[-1]).reshape(1,2),axis=0)\n        else:\n            temp_array = np.append(temp_array,np.append(model.predict(temp_array[-maxlen:].reshape(1,maxlen,2)),submission_df[\"ConfirmedCases_std\"][i/maxlen*43+j]).reshape(1,2),axis=0)\n    fatalities_pred.append(temp_array[-43:])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_df[\"Fatalities\"] = np.abs(scaler.inverse_transform([i[0] for i in np.array(fatalities_pred).reshape(294*43,2)]))\nsubmission_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_df[[\"ConfirmedCases\",\"Fatalities\"]] = submission_df[[\"ConfirmedCases\",\"Fatalities\"]].round().astype(int)\nsubmission_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_df = submission_df.drop(\"ConfirmedCases_std\",axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_df = submission_df.set_index('ForecastId')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_df.to_csv('submission.csv')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"}},"nbformat":4,"nbformat_minor":4}