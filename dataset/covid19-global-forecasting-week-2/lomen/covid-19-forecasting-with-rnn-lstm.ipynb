{"cells":[{"metadata":{},"cell_type":"markdown","source":"# 0, Introduction\n* The goal of this notebook is to forecast the number of Covid 19 confirmedCases and fatalities in the future.\n* The model uses an RNN (LSTM).\n* The only data set used is the Covid 19 global-forecast dataset.\n* The explanatory variable is the number of confirmedCases and fatalities over 20 days, and the objective variable is the number of confirmedCases and fatalities on day 21."},{"metadata":{},"cell_type":"markdown","source":"# 1, Import Library"},{"metadata":{"trusted":true},"cell_type":"code","source":"import datetime\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport matplotlib.dates as mdates\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_log_error\n\nfrom keras.models import Sequential\nfrom keras.layers.core import Dense, Activation\nfrom keras.layers.recurrent import LSTM\nfrom keras.optimizers import Adagrad\nfrom keras.callbacks import EarlyStopping","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2, Preparing the training data"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv(\"../input/covid19-global-forecasting-week-2/train.csv\")\ntrain_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df[\"Province_State\"] = train_df[\"Province_State\"].fillna(\"No State\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(25,6))\nplt.title(\"ConfirmedCases\")\nplt.plot(range(len(train_df[\"ConfirmedCases\"].values)),train_df[\"ConfirmedCases\"].values)\nplt.show()\n\nplt.figure(figsize=(25,6))\nplt.title(\"Fatalities\")\nplt.plot(range(len(train_df[\"Fatalities\"].values)),train_df[\"Fatalities\"].values)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels = list(set(train_df[\"Date\"].values))\nticks = 5\n\nplt.figure(figsize=(25,6))\nplt.title(\"ConfirmedCases(Japan)\")\nplt.plot(train_df[\"Date\"][train_df[\"Country_Region\"] == \"Japan\"].values,train_df[[\"ConfirmedCases\",\"Fatalities\"]][train_df[\"Country_Region\"] == \"Japan\"].values)\nplt.xticks(range(0, len(labels), ticks), labels[::ticks])\nplt.show()\n\nplt.figure(figsize=(25,6))\nplt.title(\"Fatalities(Japan)\")\nplt.plot(train_df[\"Date\"][train_df[\"Country_Region\"] == \"Japan\"].values,train_df[\"Fatalities\"][train_df[\"Country_Region\"] == \"Japan\"].values)\nplt.xticks(range(0, len(labels), ticks), labels[::ticks])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Ratio of test data\ntest_rate = 0.1\n\n#Length of time series data\ntime_series_len = 20\n\n#Number of dates that can be used as training data(1/22ï½ž3/18 = 57!)\ntrain_data_date_count = len(set(train_df[\"Date\"]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Preprocessing\nss_c = StandardScaler()\ntrain_df[\"ConfirmedCases_std\"] = ss_c.fit_transform(train_df[\"ConfirmedCases\"].values.reshape(len(train_df[\"ConfirmedCases\"].values),1))\n\nss_f = StandardScaler()\ntrain_df[\"Fatalities_std\"] = ss_f.fit_transform(train_df[\"Fatalities\"].values.reshape(len(train_df[\"Fatalities\"].values),1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In areas where all the ConfirmedCases and Fatalities in the training data are 0, it is not possible to predict when an outbreak will occur.\nIn that case, we exclude them from machine learning predictions and artificially predict them with ConfirmedCases = 0 and Fatalities = 0.\n\n\n* X =\n\n[[ConfirmedCases(n),Fatalities(n)],\n [ConfirmedCases(n+1),Fatalities(n+1)],\n...\n [ConfirmedCases(n+20),Fatalities(n+20)]]\n\n* Y_c =\n\nConfirmedCases(n+21)\n\n* Y_f =\n\nFatalities(n+21)"},{"metadata":{"trusted":true},"cell_type":"code","source":"X, Y_c, Y_f = [],[],[]\n\nfor state,country in train_df.groupby([\"Province_State\",\"Country_Region\"]).sum().index:\n    df = train_df[(train_df[\"Country_Region\"] == country) & (train_df[\"Province_State\"] == state)]\n    \n    if df[\"ConfirmedCases\"].sum() != 0 or df[\"Fatalities\"].sum() != 0:\n        \n        for i in range(len(df) - time_series_len):\n            \n            if (df[['ConfirmedCases']].iloc[i+time_series_len-1].values != 0 or df[['Fatalities']].iloc[i+time_series_len-1].values != 0):\n                X.append(df[['ConfirmedCases_std','Fatalities_std']].iloc[i:(i+time_series_len)].values)\n                Y_c.append(df[['ConfirmedCases_std']].iloc[i+time_series_len].values)\n                Y_f.append(df[['Fatalities_std']].iloc[i+time_series_len].values)\n\nX=np.array(X)\nY_f=np.array(Y_f)\nY_c=np.array(Y_c)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X.shape)\nprint(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(Y_c.shape)\nprint(Y_c)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(Y_f.shape)\nprint(Y_f)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#split\nX_train, X_test, Y_c_train, Y_c_test = train_test_split(X, Y_c, test_size=test_rate, shuffle = True ,random_state = 0)\nX_train, X_test, Y_f_train, Y_f_test = train_test_split(X, Y_f, test_size=test_rate, shuffle = True ,random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Stores the minimum value after standardization, i.e., the value with 0 standardized.\nconfirmedCases_std_min = train_df[\"ConfirmedCases_std\"].min()\nfatalities_std_min = train_df[\"Fatalities_std\"].min()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3, Build the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"#loss function\ndef huber_loss(y_true, y_pred, clip_delta=1.0):\n  error = y_true - y_pred\n  cond  = tf.keras.backend.abs(error) < clip_delta\n\n  squared_loss = 0.5 * tf.keras.backend.square(error)\n  linear_loss  = clip_delta * (tf.keras.backend.abs(error) - 0.5 * clip_delta)\n\n  return tf.where(cond, squared_loss, linear_loss)\n\ndef huber_loss_mean(y_true, y_pred, clip_delta=1.0):\n  return tf.keras.backend.mean(huber_loss(y_true, y_pred, clip_delta))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"epochs_num = 20\nbatch_size_num = 10\nn_hidden = 300\nn_in = 2\n\nmodel_c = Sequential()\nmodel_c.add(LSTM(n_hidden,\n               batch_input_shape=(None, time_series_len, n_in),\n               kernel_initializer='random_uniform',\n               return_sequences=False))\nmodel_c.add(Dense(1, kernel_initializer='random_uniform'))\nmodel_c.add(Activation(\"linear\"))\nopt = Adagrad(lr=0.01, epsilon=1e-08, decay=1e-4)\nmodel_c.compile(loss = huber_loss_mean, optimizer=opt)\n\nmodel_f = Sequential()\nmodel_f.add(LSTM(n_hidden,\n               batch_input_shape=(None, time_series_len, n_in),\n               kernel_initializer='random_uniform',\n               return_sequences=False))\nmodel_f.add(Dense(1, kernel_initializer='random_uniform'))\nmodel_f.add(Activation(\"linear\"))\nopt = Adagrad(lr=0.01, epsilon=1e-08, decay=1e-4)\nmodel_f.compile(loss = huber_loss_mean, optimizer=opt)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"early_stopping = EarlyStopping(monitor='loss', patience=5, verbose=1)\n\nhist_c = model_c.fit(X_train, Y_c_train, batch_size=batch_size_num, epochs=epochs_num,\n                 callbacks=[early_stopping],shuffle=False)\n\nhist_f = model_f.fit(X_train, Y_f_train, batch_size=batch_size_num, epochs=epochs_num,\n                 callbacks=[early_stopping],shuffle=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4, Predict and Confirm"},{"metadata":{"trusted":true},"cell_type":"code","source":"#predict\npredicted_c_std = model_c.predict(X_test)\nresult_c_std= pd.DataFrame(predicted_c_std)\nresult_c_std.columns = ['predict']\nresult_c_std['actual'] = Y_c_test\n\npredicted_f_std = model_f.predict(X_test)\nresult_f_std= pd.DataFrame(predicted_f_std)\nresult_f_std.columns = ['predict']\nresult_f_std['actual'] = Y_f_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loss_c = hist_c.history[\"loss\"]\nepochs = len(loss_c)\nplt.figure()\nplt.title(\"loss(ConfirmedCases)\")\nplt.plot(range(epochs), loss_c, marker=\".\")\nplt.show()\n\nloss_f = hist_f.history[\"loss\"]\nepochs = len(loss_f)\nplt.figure()\nplt.title(\"loss(Fatalities)\")\nplt.plot(range(epochs), loss_f, marker=\".\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Confirming the expected results in the standardized state\nplt.figure()\nresult_f_std[:30].plot.bar(title = \"ConfirmedCases_std\")\nplt.show()\n\nplt.figure()\nresult_c_std[:30].plot.bar(title = \"Fatalities_std\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Evaluate it back from standardization.\npredicted_c = ss_c.inverse_transform(predicted_c_std)\nY_c_inv_test = ss_c.inverse_transform(Y_c_test)\n\npredicted_f = ss_f.inverse_transform(predicted_f_std)\nY_f_inv_test = ss_f.inverse_transform(Y_f_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Check the forecast results\nresult= pd.DataFrame(predicted_c)\nresult.columns = ['predict']\nresult['actual'] = Y_c_inv_test\nresult[:30].plot.bar(title = \"ConfirmedCases\")\nplt.show()\n\nresult= pd.DataFrame(predicted_f)\nresult.columns = ['predict']\nresult['actual'] = Y_f_inv_test\nresult[:30].plot.bar(title = \"Fatalities\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv(\"../input/covid19-global-forecasting-week-2/submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df = pd.read_csv(\"../input/covid19-global-forecasting-week-2/test.csv\")\ntest_date_count = len(set(test_df[\"Date\"]))\n\nregion_count = len(train_df.groupby([\"Province_State\",\"Country_Region\"]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create the time series data needed to predict a date after 3/19\ntemp = (datetime.datetime.strptime(\"2020-03-18\", '%Y-%m-%d') - datetime.timedelta(days=time_series_len)).strftime('%Y-%m-%d')\ntest_df = train_df[train_df[\"Date\"] > temp]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f_pred = []\nc_pred = []\n\nfor i in range(0,region_count*time_series_len,time_series_len):\n    temp_array = np.array(test_df[[\"ConfirmedCases_std\",\"Fatalities_std\"]][i:i+time_series_len])\n    for j in range(test_date_count):\n        if np.array(test_df[[\"ConfirmedCases\",\"Fatalities\"]][i:i+time_series_len]).sum() == 0:\n            temp_array = np.append(temp_array,np.append(confirmedCases_std_min,fatalities_std_min).reshape(1,2),axis=0)\n        else:\n            temp_array = np.append(temp_array,np.append(model_c.predict(temp_array[-time_series_len:].reshape(1,time_series_len,2)),\n                                                        model_f.predict(temp_array[-time_series_len:].reshape(1,time_series_len,2))).reshape(1,2),axis=0)\n    c_pred.append([i[0] for i in temp_array[-test_date_count:]])\n    f_pred.append([i[1] for i in temp_array[-test_date_count:]])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission[\"ConfirmedCases\"] = np.abs(ss_c.inverse_transform(np.array(c_pred).reshape(region_count*test_date_count)))\nsubmission[\"ConfirmedCases_std\"] = np.array(c_pred).reshape(region_count*test_date_count)\n\nsubmission[\"Fatalities\"] = np.abs(ss_f.inverse_transform(np.array(f_pred).reshape(region_count*test_date_count)))\nsubmission[\"Fatalities_std\"] = np.array(f_pred).reshape(region_count*test_date_count)\n\nsubmission","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission[[\"ConfirmedCases\",\"Fatalities\"]] = submission[[\"ConfirmedCases\",\"Fatalities\"]].round().astype(int)\nsubmission = submission.drop([\"ConfirmedCases_std\",\"Fatalities_std\"],axis=1)\nsubmission = submission.set_index('ForecastId')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv('submission.csv')\nsubmission.to_csv('..\\output\\kaggle\\working\\submission_c.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"submission","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"}},"nbformat":4,"nbformat_minor":4}