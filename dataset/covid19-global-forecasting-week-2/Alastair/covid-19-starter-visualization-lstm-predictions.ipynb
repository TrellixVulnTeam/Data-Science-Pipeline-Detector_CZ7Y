{"cells":[{"metadata":{},"cell_type":"markdown","source":"This notebook does some quick data visualization by country, then attempts to use an LSTM model to do fitting and predictions (currently single-feature, using confirmed COVID-19 cases by country)."},{"metadata":{"trusted":true},"cell_type":"code","source":"## Import required packages\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sn\nimport pandas as pd\nimport tensorflow as tf\nimport geopandas as geopd\nimport datetime as dt\n\nfrom mpl_toolkits.axes_grid1 import make_axes_locatable\n\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Dense, Dropout, LSTM, TimeDistributed, Conv1D, MaxPooling1D, Flatten, Input, SimpleRNN, GRU\nfrom sklearn.model_selection import train_test_split\nfrom keras.models import load_model\nfrom keras.callbacks import ModelCheckpoint\nimport tensorflow as tf ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1. Load Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Load data\ntrain=pd.read_csv(\"../input/covid19-global-forecasting-week-2/train.csv\")\ntest=pd.read_csv(\"../input/covid19-global-forecasting-week-2/test.csv\")\ntrain['Date']=pd.to_datetime(train['Date'])\ntest['Date']=pd.to_datetime(test['Date'])\ntrain['Days']=((train['Date'] - dt.datetime(2020,1,22)).dt.total_seconds()/(24*60*60)).apply(int)\ntest['Days']=((test['Date'] - dt.datetime(2020,1,22)).dt.total_seconds()/(24*60*60)).apply(int)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2. Plot data on world map"},{"metadata":{},"cell_type":"markdown","source":"Given the exponential nature of the pandemic, the data is probably better expressed in a logarithmic scale. Plot on a world map to give a quick visualization. Some countries are missing on the map.\n\nIn decibel 10log10 scale, 0 dB = 1 case, 10 dB = 10 cases, 20 dB = 100 cases, 30 dB = 1,000 cases, 40 dB = 10,000 cases, 50 dB = 100,000 cases."},{"metadata":{"trusted":true},"cell_type":"code","source":"## Transform to dB scale, base 10\ntrain['ConfirmedCases_dB']=10*np.log10(train['ConfirmedCases'])\ntrain['Fatalities_dB']=10*np.log10(train['Fatalities'])\ntrain.loc[np.where(train.loc[:, 'ConfirmedCases_dB']==-np.inf)[0],'ConfirmedCases_dB']=np.nan # remove -inf\ntrain.loc[np.where(train.loc[:, 'Fatalities_dB']==-np.inf)[0],'Fatalities_dB']=np.nan # remove -inf\n\n## Get unique countries and dates\ncountryUnique=np.unique(train['Country_Region'])\ndateUnique=np.unique(train['Date'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Populate a geopandas world map with CC and F data\nworld = geopd.read_file(geopd.datasets.get_path('naturalearth_lowres'))\n\nworld['ConfirmedCases_dB']=0\nworld['Fatalities_dB']=0\nfor world_country_idx in range(0,len(world)):\n#for world_country_idx in range(1,2):\n    world_country_name=world.iloc[world_country_idx]['name']\n    country_name=[]\n    country_idx=np.where(world_country_name==countryUnique)[0]\n    if country_idx.shape[0]>0:\n        #print(np.max(train.loc[np.where(train.loc[:, 'Country/Region']==world_country_name)[0],'ConfirmedCases_dB']))\n        world.loc[world_country_idx,'ConfirmedCases_dB']=np.max(train.loc[np.where(train.loc[:, 'Country_Region']==world_country_name)[0],'ConfirmedCases_dB'])\n        world.loc[world_country_idx,'Fatalities_dB']=np.max(train.loc[np.where(train.loc[:, 'Country_Region']==world_country_name)[0],'Fatalities_dB'])\n    else:\n        if world_country_name=='United States of America':country_name='US'\n        elif world_country_name=='Dem. Rep. Congo':country_name='Congo (Kinshasa)'\n        elif world_country_name=='Congo':country_name='Congo (Brazzaville)'\n        elif world_country_name=='Dominican Rep.':country_name='Dominican Republic'\n        elif world_country_name=='CÃ´te d\\'Ivoire':country_name='Cote d\\'Ivoire'\n        elif world_country_name=='Central African Rep.':country_name='Central African Republic'\n        elif world_country_name=='Eq. Guinea':country_name='Equatorial Guinea'\n        elif world_country_name=='Gambia':country_name='Gambia, The'\n        elif world_country_name=='South Korea':country_name='Korea, South'\n        elif world_country_name=='Taiwan':country_name='Taiwan*'\n        elif world_country_name=='Bosnia and Herz.':country_name='Bosnia and Herzegovina'\n        if country_name!=[]:\n            world.loc[world_country_idx,'ConfirmedCases_dB']=np.max(train.loc[np.where(train.loc[:, 'Country_Region']==country_name)[0],'ConfirmedCases_dB'])\n            world.loc[world_country_idx,'Fatalities_dB']=np.max(train.loc[np.where(train.loc[:, 'Country_Region']==country_name)[0],'Fatalities_dB'])\n        #else:\n            #print(world_country_name)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Plot a world map with the most recent data\n\nfig, ax = plt.subplots(2,1,figsize=(20,10))\ndivider = make_axes_locatable(ax[0])\ncax = divider.append_axes(\"right\", size=\"5%\", pad=0.1)\n\nworld.plot(column='ConfirmedCases_dB', cmap='jet',ax=ax[0],legend=True, cax=cax, vmin=0, vmax=55);\nax[0].set_title('Confirmed Cases (dB)')\ndivider = make_axes_locatable(ax[1])\ncax = divider.append_axes(\"right\", size=\"5%\", pad=0.1)\nworld.plot(column='Fatalities_dB', cmap='jet',ax=ax[1],legend=True, cax=cax, vmin=0, vmax=55);\nax[1].set_title('Fatalities (dB)')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3. Plot data by country as time-series"},{"metadata":{},"cell_type":"markdown","source":"Let each combination of Province_State and Country_Region be a separate time-series.\n\nFirst, plot as cumulative confirmed cases and fatalities (the format that the data is given in)."},{"metadata":{"trusted":true},"cell_type":"code","source":"train['Province_State']=train['Province_State'].fillna('None')\ntrain_grouped=train.groupby(['Country_Region','Province_State'])\n\n## Plot confirmed cases and fatalities by unique combination of province/state and country/region\nnumRows, numCols = 37, 8\nfig, ax = plt.subplots(numRows,numCols,figsize=(20,55))\nfig.tight_layout(pad=2.5)\n\nfor idx in range(0,len(train_grouped)):\n    row, col = np.divmod(idx,numCols)\n    \n    days=train_grouped['Days'].apply(np.array)[idx]\n    cc=train_grouped['ConfirmedCases_dB'].apply(np.array)[idx]\n    f=train_grouped['Fatalities_dB'].apply(np.array)[idx]\n    if train_grouped['Province_State'].apply(list)[idx][0] == 'None':\n        title=train_grouped['Country_Region'].apply(list)[idx][0]\n    else:\n        title=train_grouped['Country_Region'].apply(list)[idx][0]+'\\n'+train_grouped['Province_State'].apply(list)[idx][0]\n    \n    # Plotting\n    sn.scatterplot(x=days,y=cc,ax=ax[row,col])\n    sn.scatterplot(x=days,y=f,ax=ax[row,col])\n    ax[row,col].set_title(title)\n    ax[row,col].set_ylim([-5,55])\n    ax[row,col].set_xlim([np.min(days),np.max(days)])\n    ax[row,col].set_ylabel('CC / F (dB)')\n    ax[row,col].set_xlabel('',visible=False)\n    ax[row,col].grid(1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Second, plot as the number of new confirmed cases and fatalities each day (the difference between consecutive days)."},{"metadata":{"trusted":true},"cell_type":"code","source":"train['Province_State']=train['Province_State'].fillna('None')\ntrain_grouped=train.groupby(['Country_Region','Province_State'])\n\n## Plot confirmed cases and fatalities by unique combination of province/state and country/region\nnumRows, numCols = 37, 8\nfig, ax = plt.subplots(numRows,numCols,figsize=(20,55))\nfig.tight_layout(pad=2.5)\n\nfor idx in range(0,len(train_grouped)):\n    row, col = np.divmod(idx,numCols)\n    \n    days=train_grouped['Days'].apply(np.array)[idx][1:len(train_grouped['Days'].apply(np.array)[idx])]\n    cc=10*np.log10(np.diff(train_grouped['ConfirmedCases'].apply(np.array)[idx]))\n    f=10*np.log10(np.diff(train_grouped['Fatalities'].apply(np.array)[idx]))\n    if train_grouped['Province_State'].apply(list)[idx][0] == 'None':\n        title=train_grouped['Country_Region'].apply(list)[idx][0]\n    else:\n        title=train_grouped['Country_Region'].apply(list)[idx][0]+'\\n'+train_grouped['Province_State'].apply(list)[idx][0]\n    \n    # Plotting\n    sn.scatterplot(x=days,y=cc,ax=ax[row,col])\n    sn.scatterplot(x=days,y=f,ax=ax[row,col])\n    ax[row,col].set_title(title)\n    ax[row,col].set_ylim([-5,45])\n    ax[row,col].set_xlim([np.min(days),np.max(days)])\n    ax[row,col].set_ylabel('diff CC / F (dB)')\n    ax[row,col].set_xlabel('',visible=False)\n    ax[row,col].grid(1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the example of China in the second plot, we see that if COVID-19 is able to be brought under control, the daily new confirmed cases and fatalities increase to a maxima, then decrease again.\n\nOther countries are on the upward slope of this trend."},{"metadata":{},"cell_type":"markdown","source":"### 4. Use LSTM to fit and predict confirmed cases/fatalities, without using predictions as input"},{"metadata":{},"cell_type":"markdown","source":"We can try to use a neural network time-series method (LSTM) to fit and predict on windows of the data.\n\nTraining x: windows of 2 weeks across all the past data up to 57 days after day 0.\n\nTraining y: the day after the 2 week period.\n\nTesting x: windows of 2 weeks ending in day 57 or later.\n\nTesting y: the day after the 2 week period.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Data windowing\n\nday_limit=57\nwindow_size=14\n\ntrain_x_cc=[]\ntrain_y_cc=[]\ntrain_x_f=[]\ntrain_y_f=[]\ntest_x_cc=[]\ntest_y_cc=[]\ntest_x_f=[]\ntest_y_f=[]\n\ntrain_x_days=[]\ntrain_y_days=[]\ntest_x_days=[]\ntest_y_days=[]\n\ntrain_idx=[]\ntest_idx=[]\n\n#for idx in range(0,1):\nfor idx in range(0,len(train_grouped)):\n    \n    days=train_grouped['Days'].apply(np.array)[idx]\n    cc=10*np.log10(np.diff(train_grouped['ConfirmedCases'].apply(np.array)[idx]))\n    f=10*np.log10(np.diff(train_grouped['Fatalities'].apply(np.array)[idx]))\n    \n    ## Make up for the missing difference day\n    cc=np.insert(cc,0,-np.inf)\n    f=np.insert(f,0,-np.inf)\n    \n    for window_start in range(0,day_limit-window_size):\n        window_end=window_start+window_size\n        days_window=days[window_start:window_end]\n        cc_window=cc[window_start:window_end]\n        f_window=f[window_start:window_end]\n        \n        ## Median replace\n        #train_cc_median=np.median(cc_window[cc_window>=0])\n        #train_f_median=np.median(f_window[f_window>=0])    \n        #cc_window[cc_window<0]=train_cc_median\n        #f_window[f_window<0]=train_f_median\n        #    \n        #f_window[(np.isnan(f_window))|(f_window==-np.inf)]=0\n        #f_window_end=f[window_end]\n        #if (f_window_end==-np.inf):\n        #    f_window_end=0\n\n        train_x_cc.append(cc_window)\n        train_x_f.append(f_window)\n        train_x_days.append(days_window)\n        train_y_cc.append(cc[window_end])\n        #train_y_f.append(f_window_end)\n        train_y_f.append(f[window_end])\n        train_y_days.append(days[window_end])\n        train_idx.append(idx)\n            \n            \n    for window_start in range(day_limit-window_size,len(days)-window_size):\n        window_end=window_start+window_size\n        days_window=days[window_start:window_end]\n        cc_window=cc[window_start:window_end]\n        f_window=f[window_start:window_end]\n        \n        ## Median replace\n        #test_cc_median=np.median(cc_window[cc_window>=0])\n        #test_f_median=np.median(f_window[f_window>=0])\n        #cc_window[cc_window<0]=test_cc_median\n        #f_window[f_window<0]=test_f_median\n        #    \n        #f_window[(np.isnan(f_window))|(f_window==-np.inf)]=0\n        #f_window_end=f[window_end]\n        #if (f_window_end==-np.inf):\n        #    f_window_end=0\n\n        test_x_cc.append(cc_window)\n        test_x_f.append(f_window)\n        test_x_days.append(days_window)\n        test_y_cc.append(cc[window_end])\n        #test_y_f.append(f_window_end)\n        test_y_f.append(f[window_end])\n        test_y_days.append(days[window_end])\n        test_idx.append(idx)\n        \n    \ntrain_x_cc = np.asarray(train_x_cc)\ntrain_y_cc = np.asarray(train_y_cc)\ntrain_x_f = np.asarray(train_x_f)\ntrain_y_f = np.asarray(train_y_f)\n\ntest_x_cc = np.asarray(test_x_cc)\ntest_y_cc = np.asarray(test_y_cc)\ntest_x_f = np.asarray(test_x_f)\ntest_y_f = np.asarray(test_y_f)\n\ntrain_x_days = np.asarray(train_x_days)\ntrain_y_days = np.asarray(train_y_days)\ntest_x_days = np.asarray(test_x_days)\ntest_y_days = np.asarray(test_y_days)\n\ntrain_idx = np.asarray(train_idx)\ntest_idx = np.asarray(test_idx)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Fit simple LSTM model."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Preprocessing\nscale_factor=60\n\n## Remove all with training target as 0, but yet have training vector mean >1\ntrain_keep=(train_y_cc!=-np.inf) & (np.ma.masked_invalid(train_x_cc).mean(axis=1).filled(0)>1)\ntrain_x_cc=train_x_cc[train_keep]\ntrain_y_cc=train_y_cc[train_keep]\ntrain_x_f=train_x_f[train_keep]\ntrain_y_f=train_y_f[train_keep]\n\n\ntrain_x_cc[np.isnan(train_x_cc)]=0\ntrain_y_cc[np.isnan(train_y_cc)]=0\ntrain_x_f[np.isnan(train_x_f)]=0\ntrain_y_f[np.isnan(train_y_f)]=0\n\ntest_x_cc[np.isnan(test_x_cc)]=0\ntest_y_cc[np.isnan(test_y_cc)]=0\ntest_x_f[np.isnan(test_x_f)]=0\ntest_y_f[np.isnan(test_y_f)]=0\n\ntrain_x_cc[train_x_cc==-np.inf]=0\ntrain_y_cc[train_y_cc==-np.inf]=0\ntrain_x_f[train_x_f==-np.inf]=0\ntrain_y_f[train_y_f==-np.inf]=0\n\ntest_x_cc[test_x_cc==-np.inf]=0\ntest_y_cc[test_y_cc==-np.inf]=0\ntest_x_f[test_x_f==-np.inf]=0\ntest_y_f[test_y_f==-np.inf]=0\n\n#train_X=train_x_cc.reshape(train_x_cc.shape[0],train_x_cc.shape[1],1)/scale_factor\n#train_y=train_y_cc.reshape(train_y_cc.shape[0],1)/scale_factor\n#test_X=test_x_cc.reshape(test_x_cc.shape[0],test_x_cc.shape[1],1)/scale_factor\n#test_y=test_y_cc.reshape(test_y_cc.shape[0],1)/scale_factor\n\ntrain_X=np.concatenate((train_x_cc.reshape(train_x_cc.shape[0],train_x_cc.shape[1],1)/scale_factor,\n                        train_x_f.reshape(train_x_f.shape[0],train_x_f.shape[1],1)/scale_factor),axis=2)\ntrain_y=np.concatenate((train_y_cc.reshape(train_y_cc.shape[0],1)/scale_factor,\n                        train_y_f.reshape(train_y_f.shape[0],1)/scale_factor),axis=1)\ntest_X=np.concatenate((test_x_cc.reshape(test_x_cc.shape[0],test_x_cc.shape[1],1)/scale_factor,\n                       test_x_f.reshape(test_x_f.shape[0],test_x_f.shape[1],1)/scale_factor),axis=2)\ntest_y=np.concatenate((test_y_cc.reshape(test_y_cc.shape[0],1)/scale_factor,\n                       test_y_f.reshape(test_y_f.shape[0],1)/scale_factor),axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\nmodel.add(LSTM(128, input_shape=(train_X.shape[1], train_X.shape[2])))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(train_y.shape[1]))\n\nmodel.compile(loss='mean_squared_error', optimizer='adam')\ncallback=ModelCheckpoint('20200331_model_cc_v2.h5', monitor='val_loss', save_best_only=True)\n\nmodel.fit(train_X,train_y,epochs=5,batch_size=16,validation_split=0.4,callbacks=[callback],verbose=1)\nmodel=tf.keras.models.load_model('20200331_model_cc_v2.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_y_predictions=model.predict(test_X)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plot prediction results against the original data."},{"metadata":{"trusted":true},"cell_type":"code","source":"## Plot confirmed cases and fatalities by unique combination of province/state and country/region\nnumRows, numCols = 37, 8\nfig, ax = plt.subplots(numRows,numCols,figsize=(20,55))\nfig.tight_layout(pad=2.5)\n\nfor idx in range(0,len(train_grouped)):\n    row, col = np.divmod(idx,numCols)\n    \n    days=train_grouped['Days'].apply(np.array)[idx][1:len(train_grouped['Days'].apply(np.array)[idx])]\n    cc=10*np.log10(np.diff(train_grouped['ConfirmedCases'].apply(np.array)[idx]))\n    f=10*np.log10(np.diff(train_grouped['Fatalities'].apply(np.array)[idx]))\n    if train_grouped['Province_State'].apply(list)[idx][0] == 'None':\n        title=train_grouped['Country_Region'].apply(list)[idx][0]\n    else:\n        title=train_grouped['Country_Region'].apply(list)[idx][0]+'\\n'+train_grouped['Province_State'].apply(list)[idx][0]\n    \n    # Plotting\n    sn.scatterplot(x=days,y=cc,ax=ax[row,col])\n    sn.scatterplot(x=days,y=f,ax=ax[row,col])\n    \n    sn.scatterplot(x=test_y_days[np.where(test_idx==idx)[0]],\n                   y=np.squeeze(test_y_predictions[np.where(test_idx==idx)[0]])[:,0]*scale_factor,ax=ax[row,col])\n    sn.scatterplot(x=test_y_days[np.where(test_idx==idx)[0]],\n                   y=np.squeeze(test_y_predictions[np.where(test_idx==idx)[0]])[:,1]*scale_factor,ax=ax[row,col])\n    \n    sn.lineplot(x=[day_limit, day_limit],y=[-5,45],color='r',ax=ax[row,col])\n    \n    ax[row,col].set_title(title)\n    ax[row,col].set_ylim([-5,45])\n    ax[row,col].set_xlim([np.min(days),np.max(days)])\n    ax[row,col].set_ylabel('diff CC / F (dB)')\n    ax[row,col].set_xlabel('',visible=False)\n    ax[row,col].grid(1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Observations:\n    \n1) Model does decently, but has some odd discontinuities for predictions.\n\n2) Model currently doesn't use predictions as input data; for long-term predictions will require this. Model may currently be overfitting.\n\n3) Many errors in code to correct"},{"metadata":{"trusted":true},"cell_type":"code","source":"#for idx in range(0,len(train_grouped)):\n#    \n#idx=509\n#test_y_predictions=model.predict(test_X)\n#plt.plot(test_x_days[idx],test_X[idx]*45)\n#plt.plot(test_y_days[idx],test_y[idx]*45,'bx')\n#plt.plot(test_y_days[idx],test_y_predictions[idx]*45,'ro')\n#plt.ylim([-5,45])\n#plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5. Use trained LSTM to predict confirmed cases, using predicted values to extrapolate to longer-term"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Data windowing\n\ntest_x_cc_extrp=[]\ntest_x_f_extrp=[]\n\ntest_x_days_extrp=[]\ntest_idx_extrp=[]\n\n#for idx in range(0,1):\nfor idx in range(0,len(train_grouped)):\n    \n    days=train_grouped['Days'].apply(np.array)[idx][1:len(train_grouped['Days'].apply(np.array)[idx])]\n    cc=10*np.log10(np.diff(train_grouped['ConfirmedCases'].apply(np.array)[idx]))\n    f=10*np.log10(np.diff(train_grouped['Fatalities'].apply(np.array)[idx]))\n    \n    for window_start in range(day_limit-window_size,day_limit-window_size+1):\n    #for window_start in range(day_limit-window_size+10,day_limit-window_size+1+10):\n        window_end=window_start+window_size\n        days_window=days[window_start:window_end]\n        cc_window=cc[window_start:window_end]\n        f_window=f[window_start:window_end]\n        train_cc_median=np.median(cc_window[cc_window>=0])\n        train_f_median=np.median(f_window[f_window>=0])\n\n        cc_window[cc_window<0]=train_cc_median\n        f_window[f_window<0]=train_f_median\n\n        test_x_cc_extrp.append(cc_window)\n        test_x_f_extrp.append(f_window)\n        test_x_days_extrp.append(days_window)\n        test_idx_extrp.append(idx)\n\ntest_x_cc_extrp = np.asarray(test_x_cc_extrp)\ntest_x_f_extrp = np.asarray(test_x_f_extrp)\n\ntest_x_days_extrp = np.asarray(test_x_days_extrp)\ntest_idx_extrp = np.asarray(test_idx_extrp)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Preprocessing\n\ntest_x_cc_extrp[np.isnan(test_x_cc_extrp)]=0\ntest_x_f_extrp[np.isnan(test_x_f_extrp)]=0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"days_to_predict_until=100\n\n#test_X=test_x_cc_extrp.reshape(test_x_cc_extrp.shape[0],test_x_cc_extrp.shape[1],1)/scale_factor\ntest_X=np.concatenate((test_x_cc_extrp.reshape(test_x_cc_extrp.shape[0],test_x_cc_extrp.shape[1],1)/scale_factor,\n                       test_x_f_extrp.reshape(test_x_f_extrp.shape[0],test_x_f_extrp.shape[1],1)/scale_factor),axis=2)\n\ntest_X_extrp=[]\ntest_X_extrp_days=[]\ntest_extrp_idx=[]\ntest_y_extrp_days=[]\ntest_y_extrp=[]\nfor idx in range(0,test_X.shape[0]):\n#for idx in range(0,1):\n#for idx in range(48,49):\n    test_X_extrp.append(test_X[idx])\n    test_X_extrp_days.append(test_x_days_extrp[idx])\n    while np.max(test_X_extrp_days[len(test_X_extrp_days)-1])<= days_to_predict_until-1:\n        new_days=test_X_extrp_days[len(test_X_extrp_days)-1]+1\n        \n        features_to_predict=test_X_extrp[len(test_X_extrp_days)-1].reshape(1,test_X_extrp[len(test_X_extrp_days)-1].shape[0],\n                                                                     test_X_extrp[len(test_X_extrp_days)-1].shape[1])\n        end_features=model.predict(features_to_predict)\n        end_features[end_features<0]=0\n        new_features=features_to_predict.reshape(features_to_predict.shape[1],features_to_predict.shape[2])[1:window_size+1]\n        new_features=np.concatenate((new_features,end_features),axis=0)\n        \n        test_X_extrp.append(new_features)\n        test_X_extrp_days.append(new_days)\n        test_extrp_idx.append(idx)\n        \n        test_y_extrp_days.append(np.max(new_days))\n        test_y_extrp.append(end_features)\n        \ntest_X_extrp=np.asarray(test_X_extrp)\ntest_X_extrp_days=np.asarray(test_X_extrp_days)\ntest_extrp_idx=np.asarray(test_extrp_idx)\n\ntest_y_extrp_days=np.asarray(test_y_extrp_days)\ntest_y_extrp=np.asarray(test_y_extrp)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Plot confirmed cases and fatalities by unique combination of province/state and country/region\nnumRows, numCols = 37, 8\nfig, ax = plt.subplots(numRows,numCols,figsize=(20,55))\nfig.tight_layout(pad=2.5)\n\nfor idx in range(0,len(train_grouped)):\n    row, col = np.divmod(idx,numCols)\n    \n    days=train_grouped['Days'].apply(np.array)[idx][1:len(train_grouped['Days'].apply(np.array)[idx])]\n    cc=10*np.log10(np.diff(train_grouped['ConfirmedCases'].apply(np.array)[idx]))\n    f=10*np.log10(np.diff(train_grouped['Fatalities'].apply(np.array)[idx]))\n    if train_grouped['Province_State'].apply(list)[idx][0] == 'None':\n        title=train_grouped['Country_Region'].apply(list)[idx][0]\n    else:\n        title=train_grouped['Country_Region'].apply(list)[idx][0]+'\\n'+train_grouped['Province_State'].apply(list)[idx][0]\n    \n    # Plotting\n    sn.scatterplot(x=days,y=cc,ax=ax[row,col])\n    sn.scatterplot(x=days,y=f,ax=ax[row,col])\n    \n    sn.scatterplot(x=test_y_extrp_days[np.where(test_extrp_idx==idx)[0]],\n                   y=test_y_extrp[np.where(test_extrp_idx==idx)[0],0,0]*scale_factor,ax=ax[row,col])\n    sn.scatterplot(x=test_y_extrp_days[np.where(test_extrp_idx==idx)[0]],\n                   y=test_y_extrp[np.where(test_extrp_idx==idx)[0],0,1]*scale_factor,ax=ax[row,col])\n    \n    \n    #sn.scatterplot(x=np.max(test_X_extrp_days[np.where(test_extrp_idx==idx)[0]],axis=1)+1,\n    #               y=np.squeeze(model.predict(test_X_extrp[np.where(test_extrp_idx==idx)[0]]))*scale_factor,ax=ax[row,col])\n    sn.lineplot(x=[day_limit, day_limit],y=[-5,45],color='g',ax=ax[row,col])\n    \n    ax[row,col].set_title(title)\n    ax[row,col].set_ylim([-5,45])\n    #ax[row,col].set_xlim([np.min(days),np.max(days)])\n    ax[row,col].set_xlim([np.min(days),110])\n    ax[row,col].set_ylabel('diff CC / F (dB)')\n    ax[row,col].set_xlabel('',visible=False)\n    ax[row,col].grid(1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Observations:\n    \n1) Model still seems ok for the short run, but is unconstrained for the long run and doesn't follow an expected curve.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Submission\nlast_timestamp=train['Date'].loc[day_limit-1]\nlast_timestamp_idx=np.squeeze(np.asarray(np.where(train['Date']==last_timestamp)))\nlast_cumulative_cc=np.asarray(train['ConfirmedCases'].iloc[last_timestamp_idx])\nlast_cumulative_f=np.asarray(train['Fatalities'].iloc[last_timestamp_idx])\n\nresult=[]\nfor idx in range(0,len(last_timestamp_idx)):\n    cumulative=[last_cumulative_cc[idx],last_cumulative_f[idx]]\n    for gg in np.where(test_extrp_idx==idx)[0]:\n        cumulative=cumulative+10**(test_y_extrp[gg][0,:]/10)\n        result.append(cumulative)\n\nresult=np.round(result)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Kaggle submission\nsubmission=pd.DataFrame({'ForecastId':np.arange(0,test_y_extrp.shape[0])+1,\n                     'ConfirmedCases':result[:,0],'Fatalities':result[:,1]})\nsubmission= submission.astype(int)\nsubmission.to_csv('Submission_1.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.7"}},"nbformat":4,"nbformat_minor":4}