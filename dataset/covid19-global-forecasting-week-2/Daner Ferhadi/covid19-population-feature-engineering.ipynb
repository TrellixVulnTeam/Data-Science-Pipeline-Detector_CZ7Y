{"cells":[{"metadata":{},"cell_type":"markdown","source":"## This notebook scrapes population data for the countries in the covid-19 dataset and combines datasets to make one set containing covid-19 data as well as population data.","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Import Data","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Create dataframes and country/province lists.","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/covid19-global-forecasting-week-2/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/covid19-global-forecasting-week-2/test.csv\")\nsubmission = pd.read_csv(\"/kaggle/input/covid19-global-forecasting-week-2/submission.csv\")\n\nsubmission","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Format date\ntrain[\"Date2\"] = train[\"Date\"].apply(lambda x: x.replace(\"-\",\"\"))\ntrain[\"Date2\"]  = train[\"Date2\"].astype(int)\n\n# drop nan's\n\n#train = train.dropna()\n#train.isnull().sum()\n\n# Do same to Test data\ntest[\"Date2\"] = test[\"Date\"].apply(lambda x: x.replace(\"-\",\"\"))\ntest[\"Date2\"]  = test[\"Date2\"].astype(int)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.sample(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training Set Pre-Processing and Feature Engineering","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# some pre-processing, training set\ntrain['Date'] = (pd.to_datetime(train['Date']))\ntest['Date'] = (pd.to_datetime(test['Date']))\nsLength = len(train['Id'])\ntLength = len(test['ForecastId'])\ntrain['daysFirstCase'] = pd.Series(np.zeros(sLength,dtype=int), index=train.index)\ntrain['daysFirstDeath'] = pd.Series(np.zeros(sLength,dtype=int), index=train.index)\ntrain['CountryID'] = pd.Series(np.zeros(sLength,dtype=int), index=train.index)\ntest['daysFirstCase'] = pd.Series(np.zeros(tLength,dtype=int), index=test.index)\ntest['daysFirstDeath'] = pd.Series(np.zeros(tLength,dtype=int), index=test.index)\ntest['CountryID'] = pd.Series(np.zeros(tLength,dtype=int), index=test.index)\nprovince_list = train['Province_State'].unique()[1:] # first entry is nan so skip it\ncountry_list = train['Country_Region'].unique()\nif len(test['Country_Region'].unique()) != len(country_list):\n    raise Exception('ERROR: Different number of unique countries in train/test sets!')\n\n# Create days since first case column.****************************************************\n\nprovinceFirstCaseList = []\nfor province in province_list:\n    loop_date = (train[train['Province_State']==province].tail(1)['Date'].values)[0]\n    for index, row in train[train['Province_State']==province].iterrows():\n        if row['ConfirmedCases'] > 0:\n            loop_date = row['Date']\n            break\n    provinceFirstCaseList.append(loop_date)\n            \n            \nfor index, row in train.iterrows():\n    loop_province = row['Province_State']\n    if type(loop_province) == float:\n        continue\n    loop_date = provinceFirstCaseList[np.where(province_list == loop_province)[0][0]]\n    train.loc[index,'daysFirstCase'] = (row['Date']-loop_date).days\n\ncountryFirstCaseList = []\nfor country in country_list:\n    loop_date = (train[train['Country_Region']==country].tail(1)['Date'].values)[0]\n    for index, row in train[train['Country_Region']==country].iterrows():\n        if row['ConfirmedCases'] > 0:\n            loop_date = row['Date']            \n            break\n    countryFirstCaseList.append(loop_date)\n\nfor index, row in train.iterrows():\n    loop_province = row['Province_State']\n    if type(loop_province) != float:\n        continue\n    loop_country = row['Country_Region']\n    loop_date = countryFirstCaseList[np.where(country_list == loop_country)[0][0]]\n    train.loc[index,'daysFirstCase'] = (row['Date']-loop_date).days\n\n# Create days since first death column. **************************************************\n\nprovinceFirstDeathList = []\nfor province in province_list:\n    loop_date = (train[train['Province_State']==province].tail(1)['Date'].values)[0]\n    for index, row in train[train['Province_State']==province].iterrows():\n        if row['Fatalities'] > 0:\n            loop_date = row['Date']\n            break    \n    provinceFirstDeathList.append(loop_date)\n       \nfor index, row in train.iterrows():\n    loop_province = row['Province_State']\n    if type(loop_province) == float:\n        continue\n    loop_date = provinceFirstDeathList[np.where(province_list == loop_province)[0][0]]\n    train.loc[index,'daysFirstDeath'] = (row['Date']-loop_date).days\n\ncountryFirstDeathList = []\nfor country in country_list:\n    loop_date = (train[train['Country_Region']==country].tail(1)['Date'].values)[0]\n    for index, row in train[train['Country_Region']==country].iterrows():\n        if row['Fatalities'] > 0:\n            loop_date = row['Date']\n            break\n    countryFirstDeathList.append(loop_date)\n\nfor index, row in train.iterrows():\n    loop_province = row['Province_State']\n    if type(loop_province) != float:\n        continue\n    loop_country = row['Country_Region']\n    loop_date = countryFirstDeathList[np.where(country_list == loop_country)[0][0]]\n    train.loc[index,'daysFirstDeath'] = (row['Date']-loop_date).days\n\n# Import world population data for migration metric\nworldpop = pd.read_csv(\"/kaggle/input/worldpop-utf8/world_population_countryID0.csv\")#, encoding = \"ISO-8859-1\", engine='python')\nprint(len(worldpop['CountryPop']))\nprint(len(country_list))\nworldpop.sample(10)\n\nfor index, row in train.iterrows():\n    loop_country = row['Country_Region']\n    loop_countryID = np.where(country_list == loop_country)[0][0]\n    train.loc[index,'CountryID'] = loop_countryID\n\n#Attempt some merging of world pop\nmerged = train.join(worldpop.set_index('CountryID'), on='CountryID', how='left')\nmerged['MigPerc'] = 100*merged['CountryMigration']/merged['CountryPop']\n\n# Normalize CountryPop and CountryMigration\nmerged['CountryPop'] = merged['CountryPop']/merged['CountryPop'].max()\nmerged['CountryMigration'] = merged['CountryMigration']/merged['CountryMigration'].max()\nmerged['Lag'] = merged['daysFirstCase']-merged['daysFirstDeath']\nmerged.sample(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Test Set Pre-Processing and Feature Engineering\n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Notice, when you look at a sample of the test data below, that in order to test our model, we need to test on the same features we train on. So we need to make the table blow have the same columns as the table above excluding the ConfirmedCases and Fatalities as these are the test targets.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for index, row in test.iterrows():\n    loop_province = row['Province_State']\n    if type(loop_province) != float:\n        continue\n    loop_country = row['Country_Region']\n    loop_date = countryFirstCaseList[np.where(country_list == loop_country)[0][0]]\n    test.loc[index,'daysFirstCase'] = (row['Date']-loop_date).days\n\nfor index, row in test.iterrows():\n    loop_province = row['Province_State']\n    if type(loop_province) != float:\n        continue\n    loop_country = row['Country_Region']\n    loop_date = countryFirstDeathList[np.where(country_list == loop_country)[0][0]]\n    test.loc[index,'daysFirstDeath'] = (row['Date']-loop_date).days\n    \nfor index, row in test.iterrows():\n    loop_country = row['Country_Region']\n    loop_countryID = np.where(country_list == loop_country)[0][0]\n    test.loc[index,'CountryID'] = loop_countryID\n\n    \n    \ntest.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make merged_test\n#Attempt some merging of world pop\nmerged_test = test.join(worldpop.set_index('CountryID'), on='CountryID', how='left')\nmerged_test['MigPerc'] = 100*merged_test['CountryMigration']/merged_test['CountryPop']\n\n# Normalize CountryPop and CountryMigration\nmerged_test['CountryPop'] = merged_test['CountryPop']/merged_test['CountryPop'].max()\nmerged_test['CountryMigration'] = merged_test['CountryMigration']/merged_test['CountryMigration'].max()\nmerged_test['Lag'] = merged_test['daysFirstCase'] - merged_test['daysFirstDeath']\nmerged_test.sample(7)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}