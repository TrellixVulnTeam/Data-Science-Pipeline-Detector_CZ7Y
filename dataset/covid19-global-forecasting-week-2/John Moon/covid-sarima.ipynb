{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\nfrom statsmodels.tsa.arima_model import ARIMA\n\ntrain = pd.read_csv('/kaggle/input/covid19-global-forecasting-week-2/train.csv')\ntest = pd.read_csv('/kaggle/input/covid19-global-forecasting-week-2/test.csv')\nsubmission = pd.read_csv('/kaggle/input/covid19-global-forecasting-week-2/submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install pmdarima\nimport pmdarima as pm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.fillna(' ', inplace=True)\ntest.fillna(' ', inplace=True)\ntrain.columns = train.columns.str.lower()\ntest.columns = test.columns.str.lower()\ntrain_id = train.pop('id')\n#test_id = test.pop('forecastid')\ntrain['c_p'] = train['country_region'] + train['province_state']\ntest['c_p'] = test['country_region'] + test['province_state']\ntrain.drop(['country_region','province_state'], axis=1, inplace=True)\ntest.drop(['country_region','province_state'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"countries_list = train.c_p.unique()\ntrain_new = []\nfor i in countries_list:\n    train_new.append(train[train['c_p']==i])\n\nplt.subplots(figsize =(12,8))\nfor i in train_new:\n    data = i.confirmedcases.astype('int64').tolist()\n    plt.plot(i.date, data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.subplots(figsize =(12,8))\nfor i in train_new:\n    f_data = i.fatalities.astype('int64').tolist()\n    plt.plot(i.date, f_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#example to find the best sarima parameter for the first country\ndata = train_new[0].confirmedcases.astype('int64').tolist()\nscmodel = pm.auto_arima(data,star_p=1,start_q=1, test='adf', max_p=3, max_q=3, m=12, start_P=0, seasonal=True,\n                        D=1, trace=True, error_action='ignore', suppress_warnings=True, stepwise=True) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#example to find the best sarima parameter for the first country\ndata = train_new[0].fatalities.astype('int64').tolist()\nsfmodel = pm.auto_arima(data,star_p=1,start_q=1, test='adf', max_p=3, max_q=3, m=12, start_P=0, seasonal=True,\n                        D=1, trace=True, error_action='ignore', suppress_warnings=True, stepwise=True) \nsfmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nsubmit_confirmed=[]\nsubmit_fatal = []\n\nfor i in train_new:\n    # confired cases predict\n    data = i.confirmedcases.astype('int64').tolist()\n    try :\n        model_c = SARIMAX(data, order=(1,1,0), seasonal_order=(0,1,1,12), measurement_error=True)\n        model_c_fit = model_c.fit(disp=False)\n        predicted = model_c_fit.predict(len(data), len(data)+34)\n        new = np.concatenate((np.array(data), np.array([int(num) for num in predicted])), axis=0)\n        submit_confirmed.extend(list(new[-43:]))\n    except:\n        submit_confirmed.extend(list(data[-10:-1]))\n        for j in range(34):\n            submit_confirmed.append(data[-1]*2)\n            \n    # Fatalities predict\n    \n    data = i.fatalities.astype('int64').tolist()\n    try :\n        model_f = SARIMAX(data, order = (1,1,0), seasonal_order=(0,1,0,12), measurement_error=True)\n        model_f_fit = model_f.fit(disp=False)\n        predicted = model_f_fit.predict(len(data), len(data)+34)\n        new = np.concatenate((np.array(data), np.array([int(num) for num in predicted])), axis = 0)\n        submit_fatal.extend(list(new[-43:]))\n            \n    except :\n        submit_fatal.extend(list(data[-10:-1]))\n        for j in range(34):\n            submit_fatal.append(data[-1]*2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result_submit = pd.concat([pd.Series(np.arange(1,1+len(submit_confirmed))), pd.Series(submit_confirmed), pd.Series(submit_fatal)], axis=1)\nresult_submit.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result_submit.rename(columns ={0:'ForecastId',1:'ConfirmedCases',2:'Fatalities'}, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#result_submit","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#result_submit.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Try LSTM method\n\nimport pandas as pd\ntrain = pd.read_csv('/kaggle/input/covid19-global-forecasting-week-2/train.csv', parse_dates =['Date'])\ntest = pd.read_csv('/kaggle/input/covid19-global-forecasting-week-2/test.csv',parse_dates=['Date'])\nsubmission = pd.read_csv('/kaggle/input/covid19-global-forecasting-week-2/submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.fillna(' ', inplace=True)\ntest.fillna(' ', inplace=True)\ntrain.columns = train.columns.str.lower()\ntest.columns = test.columns.str.lower()\ntrain_id = train.pop('id')\ntest_id = test.pop('forecastid')\ntrain['c_p'] = train['country_region'] + train['province_state']\ntest['c_p'] = test['country_region'] + test['province_state']\ntrain.drop(['country_region','province_state'], axis=1, inplace=True)\ntest.drop(['country_region','province_state'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ntrain['c_p_le'] = le.fit_transform(train['c_p'])\ntest['c_p_le'] = le.transform(test['c_p'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new = pd.DataFrame()\ndef create_time_features(data):\n    new['hour'] = data['date'].dt.hour\n    new['day'] = data['date'].dt.day\n    new['dayofweek'] = data['date'].dt.dayofweek\n    new['dayofyear'] = data['date'].dt.dayofyear\n    new['quarter'] = data['date'].dt.quarter\n    new['weekofyear'] = data['date'].dt.weekofyear\n    new['month'] = data['date'].dt.month\n    new['year'] = data['date'].dt.year\n    new_feature = new[['hour','day','dayofweek','dayofyear','quarter','weekofyear','month','year']]\n    \n    return new_feature\n\nadd_train = create_time_features(train) \nadd_test = create_time_features(test)\ntrain_tot = pd.concat([train, add_train], axis=1)\ntest_tot = pd.concat([test, add_test], axis=1)\n\ndef create_add_trend(data, a, b):\n    for d in data['date'].drop_duplicates():\n        for i in data['c_p_le'].drop_duplicates():\n            org_mask = (data['date']==d) & (data['c_p_le']==i)\n            for l in range(1,8):\n                mask_loc = (data['date']==(d-pd.Timedelta(days=l))) & (data['c_p_le']==i)\n                            \n                try:\n                    data.loc[org_mask, 'cf_'+ str(l)] = data.loc[mask_loc,a].values\n                    data.loc[org_mask, 'ft_'+ str(l)] = data.loc[mask_loc,b].values\n                except:\n                    data.loc[org_mask, 'cf_'+ str(l)] = 0.0\n                    data.loc[org_mask, 'ft_'+ str(l)] = 0.0\n\ncreate_add_trend(train_tot,'confirmedcases', 'fatalities')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_tot","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train_date = train_tot.pop('date')\n#test_date = test_new.pop('date')\n\ntrain_tot.drop('c_p', axis=1, inplace=True)\ntest_tot.drop('c_p', axis=1, inplace=True)\ntrain_tot.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import math\nimport numpy as np\nimport keras.backend as K\n\ndef RMSLE(predict, true):\n    assert predict.shape[0]==true.shape[0]\n    return K.sqrt(K.mean(K.square(K.log(predict+1) - K.log(true+1))))\n#print ('My RMSLE: ' + str(RMSLE(predict,true)) )\n\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom sklearn.model_selection import train_test_split\nc_features=['c_p_le', 'dayofyear', 'quarter', 'weekofyear', 'month',\n            'cf_1', 'cf_2', 'cf_3', 'cf_4', 'cf_5', 'cf_6','cf_7']\nf_features=['c_p_le', 'dayofyear', 'quarter', 'weekofyear', 'month',\n            'ft_1', 'ft_2', 'ft_3', 'ft_4', 'ft_5', 'ft_6','ft_7']\ntrain_x_c = train_tot[c_features].copy()\ntrain_y_c = train_tot['confirmedcases'].copy()\ntrain_x_f = train_tot[f_features].copy()\ntrain_y_f = train_tot['fatalities'].copy()\n\n\ntrain_X, val_X, train_Y, val_Y = train_test_split(train_x_c, train_y_c, test_size=0.1, random_state=0)\ntrain_X_f, val_X_f, train_Y_f, val_Y_f = train_test_split(train_x_f, train_y_f, test_size=0.1, random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfrom sklearn.preprocessing import MinMaxScaler\n\n## for the confirmedcases\n\nx_scale = MinMaxScaler()\ny_scale = MinMaxScaler()\n\n\ntrain_X_np=train_X.values\nval_X_np = val_X.values\ntrain_Y_np = train_Y.values\ntrain_Y_np_reshape = train_Y_np.reshape(-1,1)\nval_Y_np = val_Y.values\nval_Y_np_reshape = val_Y_np.reshape(-1,1)\n\n#train_X_np_s = x_scale.fit_transform(train_X_np)\n#val_X_np_s = x_scale.transform(val_X_np)\n#train_Y_np_s = y_scale.fit_transform(train_Y_np)\n#val_Y_np_s = y_scale.transform(val_Y_np)\n\ntrain_X_np_reshape=train_X_np.reshape((train_X_np.shape[0],1,train_X_np.shape[1]))\nval_X_np_reshape = val_X_np.reshape((val_X_np.shape[0],1,val_X_np.shape[1]))\n\n\n\n## for the fatalities\n\nx_scale_f = MinMaxScaler()\ny_scale_f = MinMaxScaler()\n\n\ntrain_X_f_np=train_X_f.values\nval_X_f_np = val_X_f.values\ntrain_Y_f_np = train_Y_f.values\ntrain_Y_f_np_reshape = train_Y_f_np.reshape(-1,1)\nval_Y_f_np = val_Y_f.values\nval_Y_f_np_reshape = val_Y_f_np.reshape(-1,1)\n\n#train_X_f_np_s = train_X_f_np #x_scale_f.fit_transform(train_X_f_np)\n#val_X_f_np_s = val_X_f_np#x_scale_f.transform(val_X_f_np)\n#train_Y_f_np_s = train_Y_f_np#y_scale_f.fit_transform(train_Y_f_np)\n#val_Y_f_np_s = val_Y_f_np#y_scale_f.transform(val_Y_f_np)\n\ntrain_X_f_np_reshape=train_X_f_np.reshape((train_X_f_np.shape[0],1,train_X_f_np.shape[1]))\nval_X_f_np_reshape = val_X_f_np.reshape((val_X_f_np.shape[0],1,val_X_f_np.shape[1]))\n\nprint(train_X_f_np_reshape.shape, train_Y_f_np_reshape.shape, val_X_f_np_reshape.shape, val_Y_f_np_reshape.shape)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.layers import LSTM\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\n\n\n\nes = EarlyStopping(monitor = 'val_loss', verbose=0, min_delta=0, patience=5, mode='auto')\nmc = ModelCheckpoint('model_cf.h5',monitor='val_loss',verbose=1,save_best_only=True)\nmc_f = ModelCheckpoint('model_ft.h5',monitor='val_loss',verbose=1,save_best_only=True)\ndef lstm(hidden_nodes,second_dim,third_dim):\n    model = Sequential([LSTM(hidden_nodes, input_shape=(second_dim, third_dim),activation='relu'),\n                        Dense(64, activation ='relu'),\n                        Dense(32, activation = 'relu'),\n                        Dense(1, activation='relu')])\n\n    model.compile(loss=RMSLE, optimizer='adam')\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 1. learning for confirmedcases \n\nmodel_cf = lstm(10, train_X_np_reshape.shape[1], train_X_np_reshape.shape[2])\n\nhistory_cf = model_cf.fit(train_X_np_reshape, train_Y_np_reshape, epochs=250, batch_size=512, validation_data=(val_X_np_reshape, val_Y_np_reshape), callbacks=[es,mc])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nplt.figure(figsize=(8,5))\nplt.plot(history_cf.history['loss'])\nplt.plot(history_cf.history['val_loss'])\nplt.title('CF Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 2. learning for fatalities\n\nmodel_ft = lstm(10, train_X_f_np_reshape.shape[1], train_X_f_np_reshape.shape[2])\n\nhistory_ft=model_ft.fit(train_X_f_np_reshape, train_Y_f_np_reshape, epochs=250, batch_size=512, validation_data=(val_X_f_np_reshape, val_Y_f_np_reshape), callbacks=[es,mc_f])\n\n#_Y_f_np_reshape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,5))\nplt.plot(history_ft.history['loss'])\nplt.plot(history_ft.history['val_loss'])\nplt.title('FT Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# test data also need to be re featuring\nfrom sklearn.preprocessing import MinMaxScaler\ntest_cf_scaler = MinMaxScaler()\ntest_ft_scaler = MinMaxScaler()\nimport pandas as pd\nfeatures = ['confirmedcases','fatalities','cf_1', 'ft_1', 'cf_2', 'ft_2', 'cf_3', 'ft_3', 'cf_4', 'ft_4', 'cf_5',\n       'ft_5', 'cf_6', 'ft_6', 'cf_7', 'ft_7']\nc_feat = ['c_p_le','dayofyear','quarter','weekofyear','month',\n          'cf_1', 'cf_2', 'cf_3', 'cf_4', 'cf_5', 'cf_6', 'cf_7']\ntrend_features =['c_p_le','dayofyear','quarter','weekofyear','month',\n                 'cf_1', 'ft_1', 'cf_2', 'ft_2', 'cf_3', 'ft_3', 'cf_4', 'ft_4', 'cf_5',\n                 'ft_5', 'cf_6', 'ft_6', 'cf_7', 'ft_7']\nf_feat = ['c_p_le','dayofyear','quarter','weekofyear','month',\n          'ft_1','ft_2', 'ft_3', 'ft_4', 'ft_5', 'ft_6', 'ft_7']\n\ntest_tot.dropna(inplace=True)\ntest_new =test_tot.copy().join(pd.DataFrame(columns=features))\ntest_new.head()\ntest_mask = (test_tot['date']<=train_tot['date'].max())\ntrain_mask = (train_tot['date']>= test_tot['date'].min())\ntest_new.loc[test_mask,features]=train_tot.loc[train_mask,features].values\nfuture_dt = pd.date_range(start=train_tot['date'].max()+pd.Timedelta(days=1), end=test_tot['date'].max(), freq='1D')\ndef create_add_trend_predict(data,a,b):\n    for d in future_dt:\n        for i in data['c_p_le'].drop_duplicates():\n            org_mask = (data['date']==d) & (data['c_p_le']==i)\n            for l in range(1,8):\n                mask_loc = (data['date']==(d-pd.Timedelta(days=l))) & (data['c_p_le']==i)\n                            \n                try:\n                    data.loc[org_mask, 'cf_'+ str(l)] = data.loc[mask_loc,a].values\n                    data.loc[org_mask, 'ft_'+ str(l)] = data.loc[mask_loc,b].values\n                \n                except:\n                    data.loc[org_mask, 'cf_'+ str(l)] = 0.0\n                    data.loc[org_mask, 'ft_'+ str(l)] = 0.0\n                \n                    \n                #try:\n                \n                #    data.loc[org_mask, 'ft_'+ str(l)] = data.loc[mask_loc,b].values\n                #except:\n                \n                #    data.loc[org_mask, 'ft_'+ str(l)] = 0.0\n                    \n            \n            test_X = data.loc[org_mask, trend_features]\n            \n            test_X_cc = test_X[c_feat]\n            test_X_cc = test_X_cc.to_numpy().reshape(1,-1)\n            test_cc_sc = test_X_cc#x_scale.transform(test_X_cc)\n            test_cc = test_cc_sc.reshape(test_cc_sc.shape[0],1,test_cc_sc.shape[1])\n            \n            test_X_ft = test_X[f_feat]\n            test_X_ft = test_X_ft.to_numpy().reshape(1,-1)\n            test_ft_sc = test_X_ft#x_scale_f.transform(test_X_ft)\n            test_ft = test_ft_sc.reshape(test_ft_sc.shape[0],1,test_ft_sc.shape[1])\n            \n            next_cc = model_cf.predict(test_cc)\n            next_ft = model_ft.predict(test_ft)\n            data.loc[org_mask, 'confirmedcases']=next_cc\n            data.loc[org_mask, 'fatalities']=next_ft\n            \n                   \ncreate_add_trend_predict(test_new,'confirmedcases','fatalities')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#from datetime import datetime\n#date_str = '04-01-2020'\n#d = datetime.strptime(date_str, '%m-%d-%Y').date()\n#mask_loc = (test_new['date']==(d-pd.Timedelta(days=1))) & (test_new['c_p_le']==0)\n#mask_lll = (test_new['date']==d) & (test_new['c_p_le']==0)\n\n#mask_loc.head(15)\n#aa =test_new.loc[mask_loc, 'confirmedcases'].values\n#mask_lll.head(15)\n#test_new.loc[13,'cf_1']=174\n\n#test_new.loc[mask_lll, 'ft_'+ str(1)]=test_new.loc[mask_loc, 'fatalities'].values\n\n#test_new.loc[mask_lll,'ft_'+ str(1)]\n#aa=test_new.loc[13,f_feat].values\n#aa=aa.reshape(1,-1)\n#aa=val_X_f_np[35]\n#aa=x_scale.transform(aa)\n#aa=aa.reshape(aa.shape[0],1,aa.shape[1])\n#test_new.loc[:,'confirmedcases':'ft_7'].head(50)\n#result=model_f.predict(aa)\n#result\n#val_Y_f_np[2]\n#test_new","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_new.loc[:,'confirmedcases':'fatalities'][1:50]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nresult = pd.DataFrame({'ForecastId':test_id,'ConfirmedCases':test_new['confirmedcases'], 'Fatalities': test_new['fatalities']})\nresult.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}