{"cells":[{"metadata":{},"cell_type":"markdown","source":"# COVID Global Forecast: \n\n## ARIMA\n\n* Not working with it right now as we want to add the population into consideration\n\n## Linear Model\n\n* working progress\n\n\n## LSTM\n\n\n* Long short-term memory (LSTM) is one type of recurrent neural network (RNN) architecture that uses special units in addition to standard units. LSTM units include a 'memory cell' that can maintain information in memory for long periods of time. A set of gates is used to control when information enters the memory, when it's output, and when it's forgotten. This architecture lets them learn longer-term dependencies\n\n* Unlike standard feedforward neural networks, LSTM has feedback connections. A common LSTM unit is composed of a cell, an input gate, an output gate and a forget gate. The cell remembers values over arbitrary time intervals and the three gates regulate the flow of information into and out of the cell."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import preprocessing\nimport time\nfrom datetime import datetime\nfrom scipy import integrate, optimize\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# ML libraries\nimport tensorflow as tf\nprint(tf.__version__)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1. Data Preprocessing <a id=\"section1\"></a>\n* The training dataset covers 163 countries and almost 2 full months from 2020"},{"metadata":{"_cell_guid":"","_uuid":"","trusted":true},"cell_type":"code","source":"submission_example = pd.read_csv(\"../input/covid19-global-forecasting-week-2/submission.csv\")\ntest = pd.read_csv(\"../input/covid19-global-forecasting-week-2/test.csv\")\ntrain = pd.read_csv(\"../input/covid19-global-forecasting-week-2/train.csv\")\nworld_population = pd.read_csv(\"/kaggle/input/population-by-country-2020/population_by_country_2020.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_example","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.Country_Region.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Baseline: \n\n1. **Features**. Select features e.g. Country/Region, date information \n2. **Dates**. Prepare the training from 2020-03-01 to 2020-03-18\n2. **Log transformation**. Apply log transformation to ConfirmedCases and Fatalities\n3. **Infinites**. Replace infinites from the logarithm with 0. Given the asymptotic behavior of the logarithm for log(0),this implies that when applying the inverse transformation (exponential) a 1 will be returned instead of a 0. This problem does not impact many countries, but still needs to be tackled sooner or later in order to obtain a clean solution.\n4. **Train/test split**. Split into train/valid/test\n5. **Prediction**. Linear Regression, training country by country and joining data\n6. **Submit**. Submit results in the correct format, and applying exponential to reverse log transformation "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Merge train and test, exclude overlap\ndates_overlap = ['2020-03-19','2020-03-20','2020-03-21','2020-03-22','2020-03-23', '2020-03-24', '2020-03-25', '2020-03-26', '2020-03-27', '2020-03-28', '2020-03-29', '2020-03-30']\ntrain2 = train.loc[~train['Date'].isin(dates_overlap)]\ndf = pd.concat([train2, test], axis = 0, sort=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Date'] = pd.to_datetime(df.Date)\ndf.sort_values(by='Date', inplace = True)\ndf.reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"world_population","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature engineering"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Day'] = df['Date'].dt.day\ndf['Month'] = df['Date'].dt.month\ndf['Dayofweek'] = df['Date'].dt.dayofweek\ndf['Is_weekend'] = np.where(df['Dayofweek'].isin([6,7]),1,0)\ndf['Quarter'] = df['Date'].dt.quarter\ndf['Year'] = df['Date'].dt.year\n\n# Fill null values \ndf['Province_State'].fillna(\"None\", inplace=True)\ndf['ConfirmedCases'].fillna(0, inplace=True)\ndf['Fatalities'].fillna(0, inplace=True)\ndf['Id'].fillna(-1, inplace=True)\ndf['ForecastId'].fillna(-1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Log transformation"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Apply log transformation to all ConfirmedCases and Fatalities columns, except for trends\ndf[['ConfirmedCases', 'Fatalities']] = df[['ConfirmedCases', 'Fatalities']].astype('float64')\ndf[['ConfirmedCases', 'Fatalities']] = df[['ConfirmedCases', 'Fatalities']].apply(lambda x: np.log(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Replace infinites\ndf.replace([np.inf, -np.inf], 0, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Cleaning the world population"},{"metadata":{"trusted":true},"cell_type":"code","source":"world_population = world_population[['Country (or dependency)', 'Population (2020)', 'Density (P/Km²)', 'Land Area (Km²)', 'Med. Age', 'Urban Pop %']]\nworld_population.columns = ['Country (or dependency)', 'Population (2020)', 'Density', 'Land Area', 'Med Age', 'Urban Pop']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"world_population['Urban Pop']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Replace United States by US\nworld_population.loc[world_population['Country (or dependency)']=='United States', 'Country (or dependency)'] = 'US'\n\n# Remove the % character from Urban Pop values\nworld_population['Urban Pop'] = world_population['Urban Pop'].str.rstrip('%')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Replace Urban Pop and Med Age \"N.A\" by their respective modes, then transform to int\nworld_population.loc[world_population['Urban Pop']=='N.A.', 'Urban Pop'] = int(world_population.loc[world_population['Urban Pop']!='N.A.', 'Urban Pop'].mode()[0])\nworld_population['Urban Pop'] = world_population['Urban Pop'].astype('int16')\nworld_population.loc[world_population['Med Age']=='N.A.', 'Med Age'] = int(world_population.loc[world_population['Med Age']!='N.A.', 'Med Age'].mode()[0])\nworld_population['Med Age'] = world_population['Med Age'].astype('int16')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"world_population","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# join the dataset, fill na\ndf = df.merge(world_population, left_on='Country_Region', right_on='Country (or dependency)', how='left')\ndf[['Population (2020)', 'Density', 'Land Area', 'Med Age', 'Urban Pop']] = df[['Population (2020)', 'Density', 'Land Area', 'Med Age', 'Urban Pop']].fillna(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Label Encoding"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import preprocessing\nle = preprocessing.LabelEncoder()\n\n\n# Label encode the day, countries and provinces. \n\ndf['Day_num'] = le.fit_transform(df.Date)\n\n#df.drop('Country (or dependency)', inplace=True, axis=1)\ndf['Country_Region'] = le.fit_transform(df['Country_Region'])\nnumber_c = df['Country_Region']\ncountries = le.inverse_transform(df['Country_Region'])\n# Save dictionary for exploration purposes\ncountry_dict = dict(zip(countries, number_c)) \n\n\ndf['Province_State'] = le.fit_transform(df['Province_State'])\nnumber_p = df['Province_State']\nprovince = le.inverse_transform(df['Province_State'])\nprovince_dict = dict(zip(province, number_p)) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.to_csv('features.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Train-test split"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Split data into train/test\ndef split_data(data):\n    \n    # Train set\n    x_train = data[data.ForecastId == -1].drop(['ConfirmedCases', 'Fatalities'], axis=1)\n    y_train_1 = data[data.ForecastId == -1]['ConfirmedCases']\n    y_train_2 = data[data.ForecastId == -1]['Fatalities']\n\n    # Test set\n    x_test = data[data.ForecastId != -1].drop(['ConfirmedCases', 'Fatalities'], axis=1)\n\n    # Clean Id columns and keep ForecastId as index\n    x_train.drop('Id', inplace=True, errors='ignore', axis=1)\n    x_train.drop('ForecastId', inplace=True, errors='ignore', axis=1)\n    x_test.drop('Id', inplace=True, errors='ignore', axis=1)\n    x_test.drop('ForecastId', inplace=True, errors='ignore', axis=1)\n    \n    return x_train, y_train_1, y_train_2, x_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_train_confirmed","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Linear Regression <a id=\"section1\"></a> "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Linear regression model\nfrom sklearn.linear_model import LinearRegression\n\ndef lin_reg(X_train, Y_train, X_test):\n    lr_model = LinearRegression()\n\n    # Train the model\n    lr_model.fit(X_train, Y_train)\n\n    # Make predictions using the testing set\n    y_pred = lr_model.predict(X_test)\n    \n    w = lr_model.coef_ \n    b = lr_model.intercept_ \n    \n    print(\"w: \", w)\n    print(\"b: \", b)\n    \n    return lr_model, y_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def lin_regression(X_train, Y_train, X_test):\n    lr_model = LinearRegression()\n\n    # Train the model\n    lr_model.fit(X_train, Y_train)\n\n    # Make predictions using the testing set\n    y_pred = lr_model.predict(X_test)\n    \n    w = lr_model.coef_ \n    b = lr_model.intercept_ \n    \n    print(\"w: \", w)\n    print(\"b: \", b)\n    \n    return lr_model, y_pred, w, b","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Submission function\ndef prepare_submissionFormat(df, target1, target2):\n    \n    prediction_1 = df[target1]\n    prediction_2 = df[target2]\n\n    # Submit predictions\n    prediction_1 = [int(item) for item in list(map(round, prediction_1))]\n    prediction_2 = [int(item) for item in list(map(round, prediction_2))]\n    \n    submission = pd.DataFrame({\n        \"ForecastId\": df['ForecastId'].astype('int32'), \n        \"ConfirmedCases\": prediction_1, \n        \"Fatalities\": prediction_2\n    })\n    submission.to_csv('test_submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ts = time.time()\n\nday_start = 52\ndf2 = df.loc[df.Day_num >= day_start]\n\n# Set the dfframe where we will update the predictions\ndf_pred = df[df.ForecastId != -1][['Country_Region', 'Province_State', 'Day_num', 'ForecastId']]\ndf_pred = df_pred.loc[df_pred['Day_num']>=day_start]\ndf_pred['Predicted_ConfirmedCases'] = [0]*len(df_pred)\ndf_pred['Predicted_Fatalities'] = [0]*len(df_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2['Country_Region'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"    data2 = data.loc[data.Day_num >= day_start]\n\n    # Set the dataframe where we will update the predictions\n    data_pred = data[data.ForecastId != -1][['Country_Region', 'Province_State', 'Day_num', 'ForecastId']]\n    data_pred = data_pred.loc[data_pred['Day_num']>=day_start]\n    data_pred['Predicted_ConfirmedCases'] = [0]*len(data_pred)\n    data_pred['Predicted_Fatalities'] = [0]*len(data_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# For every countries, run the linear regression\nfor c in df2['Country_Region'].unique():\n    \n    # List of provinces\n    provinces_list = df2[df2['Country_Region']==c]['Province_State'].unique()\n        \n    # If the country has several Province/State informed\n    if len(provinces_list)>1:\n        for p in provinces_list:\n            df_cp = df2[(df2['Country_Region']==c) & (df2['Province_State']==p)]\n            X_train1, Y_train_1, Y_train_2, X_test1 = split_data(df_cp)\n            model_1, pred_1, w1, b1 = lin_regression(X_train1, Y_train_1, X_test1)\n            model_2, pred_2, w2, b2 = lin_regression(X_train1, Y_train_2, X_test1)\n            df_pred.loc[((df_pred['Country_Region']==c) & (df2['Province_State']==p)), 'Pred_ConfirmedCases'] = pred_1\n            df_pred.loc[((df_pred['Country_Region']==c) & (df2['Province_State']==p)), 'Pred_Fatalities'] = pred_2\n\n    # Predict only the country\n    else:\n        df_c = df2[(df2['Country_Region']==c)]\n        X_train1, Y_train_1, Y_train_2, X_test1 = split_data(df_c)\n        model_1, pred_1, w1, b1 = lin_regression(X_train1, Y_train_1, X_test1)\n        model_2, pred_2, w2, b2 = lin_regression(X_train1, Y_train_2, X_test1)\n        df_pred.loc[(df_pred['Country_Region']==c), 'Pred_ConfirmedCases'] = pred_1\n        df_pred.loc[(df_pred['Country_Region']==c), 'Pred_Fatalities'] = pred_2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def linreg_basic_all_countries(data, day_start):\n    \n    data2 = data.loc[data.Day_num >= day_start]\n\n    # Set the dataframe where we will update the predictions\n    data_pred = data[data.ForecastId != -1][['Country_Region', 'Province_State', 'Day_num', 'ForecastId']]\n    data_pred = data_pred.loc[data_pred['Day_num']>=day_start]\n    data_pred['Predicted_ConfirmedCases'] = [0]*len(data_pred)\n    data_pred['Predicted_Fatalities'] = [0]*len(data_pred)\n\n    print(\"Currently running Logistic Regression for all countries\")\n\n    # Main loop for countries\n    for c in data2['Country_Region'].unique():\n\n        # List of provinces\n        provinces_list = data2[data2['Country_Region']==c]['Province_State'].unique()\n\n        # If the country has several Province/State informed\n        if len(provinces_list)>1:\n            for p in provinces_list:\n                data_cp = data2[(data2['Country_Region']==c) & (data2['Province_State']==p)]\n                X_train, Y_train_1, Y_train_2, X_test = split_data(data_cp)\n                model_1, pred_1 = lin_reg(X_train, Y_train_1, X_test)\n                model_2, pred_2 = lin_reg(X_train, Y_train_2, X_test)\n                data_pred.loc[((data_pred['Country_Region']==c) & (data2['Province_State']==p)), 'Predicted_ConfirmedCases'] = pred_1\n                data_pred.loc[((data_pred['Country_Region']==c) & (data2['Province_State']==p)), 'Predicted_Fatalities'] = pred_2\n\n        # No Province/State informed\n        else:\n            data_c = data2[(data2['Country_Region']==c)]\n            X_train, Y_train_1, Y_train_2, X_test = split_data(data_c)\n            model_1, pred_1 = lin_reg(X_train, Y_train_1, X_test)\n            model_2, pred_2 = lin_reg(X_train, Y_train_2, X_test)\n            data_pred.loc[(data_pred['Country_Region']==c), 'Predicted_ConfirmedCases'] = pred_1\n            data_pred.loc[(data_pred['Country_Region']==c), 'Predicted_Fatalities'] = pred_2\n\n    # Apply exponential transf. and clean potential infinites due to final numerical precision\n    data_pred[['Predicted_ConfirmedCases', 'Predicted_Fatalities']] = data_pred[['Predicted_ConfirmedCases', 'Predicted_Fatalities']].apply(lambda x: np.expm1(x))\n    data_pred.replace([np.inf, -np.inf], 0, inplace=True) \n    \n    return data_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ts = time.time()\nday_start = 52\ndf_pred = linreg_basic_all_countries(df, day_start)\nget_submission(df_pred, 'Predicted_ConfirmedCases', 'Predicted_Fatalities')\n\nprint(\"Process finished in \", round(time.time() - ts, 2), \" seconds\")\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. LSTM <a id=\"section1\"></a> "},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_train_data(scaled_x_data, scaled_y_data, divide_train_valid_index, time_step):\n    train_x, train_y = [], []\n    normalized_train_feature = scaled_x_data[0: -divide_train_valid_index]\n    normalized_train_label = scaled_y_data[0: -divide_train_valid_index]\n    for i in range(len(normalized_train_feature) - time_step + 1):\n        train_x.append(normalized_train_feature[i:i + time_step].tolist())\n        train_y.append(normalized_train_label[i:i + time_step].tolist())\n    return train_x, train_y\n\ndef get_train_fit_data(scaled_x_data, scaled_y_data, divide_train_valid_index, time_step):\n    train_fit_x, train_fit_y = [], []\n    normalized_train_feature = scaled_x_data[0: -divide_train_valid_index]\n    normalized_train_label = scaled_y_data[0: -divide_train_valid_index]\n    train_fit_remain = len(normalized_train_label) % time_step\n    train_fit_num = int((len(normalized_train_label) - train_fit_remain) / time_step)\n    temp = []\n    for i in range(train_fit_num):\n        train_fit_x.append(normalized_train_feature[i * time_step:(i + 1) * time_step].tolist())\n        temp.extend(normalized_train_label[i * time_step:(i + 1) * time_step].tolist())\n    if train_fit_remain > 0:\n        train_fit_x.append(normalized_train_feature[-time_step:].tolist())\n        temp.extend(normalized_train_label[-train_fit_remain:].tolist())\n    for i in temp:\n        train_fit_y.append(i[0])\n    return train_fit_x, train_fit_y, train_fit_remain\n\ndef get_valid_data(scaled_x_data, scaled_y_data, divide_train_valid_index, divide_valid_test_index, time_step):\n    valid_x, valid_y = [], []\n    normalized_valid_feature = scaled_x_data[-divide_train_valid_index: -divide_valid_test_index]\n    normalized_valid_label = scaled_y_data[-divide_train_valid_index: -divide_valid_test_index]\n    valid_remain = len(normalized_valid_label) % time_step\n    valid_num = int((len(normalized_valid_label) - valid_remain) / time_step)\n    temp = []\n    for i in range(valid_num):\n        valid_x.append(normalized_valid_feature[i * time_step:(i + 1) * time_step].tolist())\n        temp.extend(normalized_valid_label[i * time_step:(i + 1) * time_step].tolist())\n    if valid_remain > 0:\n        valid_x.append(normalized_valid_feature[-time_step:].tolist())\n        temp.extend(normalized_valid_label[-valid_remain:].tolist())\n    for i in temp:\n        valid_y.append(i[0])\n    return valid_x, valid_y, valid_remain\n\n\ndef get_test_data(scaled_x_data, scaled_y_data, divide_valid_test_index, time_step):\n    test_x, test_y = [], []\n    normalized_test_feature = scaled_x_data[-divide_valid_test_index:]\n    normalized_test_label = scaled_y_data[-divide_valid_test_index:]\n    test_remain = len(normalized_test_label) % time_step\n    test_num = int((len(normalized_test_label) - test_remain) / time_step)\n    temp = []\n    for i in range(test_num):\n        test_x.append(normalized_test_feature[i * time_step:(i + 1) * time_step].tolist())\n        temp.extend(normalized_test_label[i * time_step:(i + 1) * time_step].tolist())\n    if test_remain > 0:\n        test_x.append(scaled_x_data[-time_step:].tolist())\n        temp.extend(normalized_test_label[-test_remain:].tolist())\n    for i in temp:\n        test_y.append(i[0])\n    return test_x, test_y, test_remain","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nlr = 1e-3  \nbatch_size = 5  # minibatch \nrnn_unit = 30  # LSTM hiden unit number\ninput_size = 1  # \noutput_size = 1  # \ntime_step = 15  # \nepochs = 1000  # \ngradient_threshold = 15  # \nstop_loss = np.float32(0.04)  # \ntrain_keep_prob = [1.0, 0.5, 1.0] ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"divide_train_valid_index = 39\ndivide_valid_test_index = 5","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def lstm(X, keep_prob):\n    batch_size = tf.shape(X)[0]  # minibatch \n\n    # reshape the input for LSTM, start weights with truncated normal distribution \n    weights = tf.Variable(tf.truncated_normal(shape=[input_size, rnn_unit]))\n    biases = tf.Variable(tf.constant(0.1, shape=[rnn_unit, ]))\n    input = tf.reshape(X, [-1, input_size])\n\n    input_layer = tf.nn.tanh(tf.matmul(input, weights) + biases)\n    input_rnn = tf.nn.dropout(input_layer, keep_prob[0])\n    input_rnn = tf.reshape(input_rnn, [-1, time_step, rnn_unit])\n\n    # two layer LSTM，activation function by tanh, suggest to switch to relu if the network's deeper\n    initializer = tf.truncated_normal_initializer()\n    cell_1 = tf.nn.rnn_cell.LSTMCell(forget_bias=1.0, num_units=rnn_unit, use_peepholes=True, num_proj=None, initializer=initializer, name='lstm_cell_1')\n    cell_1_drop = tf.nn.rnn_cell.DropoutWrapper(cell=cell_1, output_keep_prob=keep_prob[1])\n\n    cell_2 = tf.nn.rnn_cell.LSTMCell(forget_bias=1.0, num_units=rnn_unit, use_peepholes=True, num_proj=output_size, initializer=initializer, name='lstm_cell_2')\n    cell_2_drop = tf.nn.rnn_cell.DropoutWrapper(cell=cell_2, output_keep_prob=keep_prob[2])\n\n    mutilstm_cell = tf.nn.rnn_cell.MultiRNNCell(cells=[cell_1_drop, cell_2_drop], state_is_tuple=True)\n    init_state = mutilstm_cell.zero_state(batch_size, dtype=tf.float32)\n\n    with tf.variable_scope('lstm', reuse=tf.AUTO_REUSE):\n        output, state = tf.nn.dynamic_rnn(cell=mutilstm_cell, inputs=input_rnn, initial_state=init_state, dtype=tf.float32)\n\n    return output, state","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_fit_seq(x, remain, sess, output, X, keep_prob, scaler, inverse):\n    fit_seq = []\n    if inverse:\n        # used the scaler for input data before, inverse it back to the original measurement\n        temp = []\n        for i in range(len(x)):\n            next_seq = sess.run(output, feed_dict={X: [x[i]], keep_prob: [1.0, 1.0, 1.0]})\n            if i == len(x) - 1:\n                temp.extend(scaler.inverse_transform(next_seq[0].reshape(-1, 1))[-remain:])\n            else:\n                temp.extend(scaler.inverse_transform(next_seq[0].reshape(-1, 1)))\n        for i in temp:\n            fit_seq.append(i[0])\n    else:\n        for i in range(len(x)):\n            next_seq = sess.run(output,\n                                feed_dict={X: [x[i]], keep_prob: [1.0, 1.0, 1.0]})\n            if i == len(x) - 1:\n                fit_seq.extend(next_seq[0].reshape(1, -1).tolist()[0][-remain:])\n            else:\n                fit_seq.extend(next_seq[0].reshape(1, -1).tolist()[0])\n\n    return fit_seq","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_lstm():\n    X = tf.placeholder(tf.float32, [10, time_step, input_size])\n    Y = tf.placeholder(tf.float32, [1, time_step, output_size])\n\n    keep_prob = tf.placeholder(tf.float32, [None])\n    output, state = lstm(X, keep_prob)\n    loss = tf.losses.mean_squared_error(tf.reshape(output, [-1]), tf.reshape(Y, [-1]))\n\n    # gradients \n    optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n    grads, variables = zip(*optimizer.compute_gradients(loss))\n    grads, global_norm = tf.clip_by_global_norm(grads, gradient_threshold)\n    train_op = optimizer.apply_gradients(zip(grads, variables))\n\n    X_train_fit = tf.placeholder(tf.float32, [None])\n    Y_train_fit = tf.placeholder(tf.float32, [None])\n    train_fit_loss = tf.losses.mean_squared_error(tf.reshape(X_train_fit, [-1]), tf.reshape(Y_train_fit, [-1]))\n\n    X_valid = tf.placeholder(tf.float32, [None])\n    Y_valid = tf.placeholder(tf.float32, [None])\n    valid_fit_loss = tf.losses.mean_squared_error(tf.reshape(X_valid, [-1]), tf.reshape(Y_valid, [-1]))\n\n    with tf.Session() as sess:\n        sess.run(tf.global_variables_initializer())\n        fit_loss_seq = []\n        valid_loss_seq = []\n\n        for epoch in range(epochs):\n            for index in range(len(train_x) - batch_size + 1):\n               sess.run(train_op, feed_dict={X: train_x[index: index + batch_size], Y: train_y[index: index + batch_size], keep_prob: train_keep_prob})\n\n            # fit the training sequence and validation sequence\n            train_fit_seq = get_fit_seq(train_fit_x, train_fit_remain, sess, output, X, keep_prob, scaler_y, False)\n            train_loss = sess.run(train_fit_loss, {X_train_fit: train_fit_seq, Y_train_fit: train_fit_y})\n            fit_loss_seq.append(train_loss)\n\n            valid_seq = get_fit_seq(valid_x, valid_remain, sess, output, X, keep_prob, scaler_y, False)\n            valid_loss = sess.run(valid_fit_loss, {X_valid: valid_seq, Y_valid: valid_y})\n            valid_loss_seq.append(valid_loss)\n\n            print('epoch:', epoch + 1, 'fit loss:', train_loss, 'valid loss:', valid_loss)\n\n            # earily stop \n            # stop_loss needs to be tried multiple times\n            if train_loss + valid_loss <= stop_loss:\n                train_fit_seq = get_fit_seq(train_fit_x, train_fit_remain, sess, output, X, keep_prob, scaler_y, True)\n                valid_fit_seq = get_fit_seq(valid_x, valid_remain, sess, output, X, keep_prob, scaler_y, True)\n                test_fit_seq = get_fit_seq(test_x, test_remain, sess, output, X, keep_prob, scaler_y, True)\n                print('best epoch: ', epoch + 1)\n                break\n\n    return fit_loss_seq, valid_loss_seq, train_fit_seq, valid_fit_seq, test_fit_seq\n\n#fit_loss_seq, valid_loss_seq, train_fit_seq, valid_fit_seq, test_fit_seq = train_lstm()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}