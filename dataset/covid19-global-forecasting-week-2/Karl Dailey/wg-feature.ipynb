{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"###############################################################################\n## Imports\n###############################################################################\nfrom datetime import timedelta\nimport numpy as np\nimport pandas as pd\nimport xgboost as xgb\nfrom xgboost import plot_importance, plot_tree\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom google.cloud import bigquery\nfrom sklearn.model_selection import KFold\nfrom scipy.spatial.distance import cdist\n\nfrom scipy.optimize import minimize\nfrom statsmodels.tsa.arima_model import ARIMA\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\nfrom statsmodels.tsa.api import ExponentialSmoothing, SimpleExpSmoothing, Holt\nfrom math import sin, cos, sqrt, atan2, radians\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import mean_squared_log_error\nfrom scipy.optimize import curve_fit\nfrom datetime import datetime\n###############################################################################\n## Functions\n###############################################################################\ndef create_time_features(df):\n    \"\"\"\n    Creates time series features from datetime index\n    \"\"\"\n    df['date'] = pd.to_datetime(df['Date']).values\n    df['hour'] = df['date'].dt.hour\n    df['dayofweek'] = df['date'].dt.dayofweek\n    df['quarter'] = df['date'].dt.quarter\n    df['month'] = df['date'].dt.month\n    df['year'] = df['date'].dt.year\n    df['dayofyear'] = df['date'].dt.dayofyear\n    df['dayofmonth'] = df['date'].dt.day\n    df['weekofyear'] = df['date'].dt.weekofyear\n    \n    return df\n\ndef mean_absolute_percentage_error(y_true, y_pred): \n    y_true, y_pred = np.array(y_true), np.array(y_pred)\n    return np.mean(np.abs((y_true - y_pred) / y_true)) * 10\n\n\ndef wg_func(params):\n    a = params[0]\n    r = params[1]\n    c = params[2]\n    hcnb = params[3]\n    \n    incr = [cdata[i] if i == 0 else cdata[i] - cdata[i-1] for i,item in enumerate(cdata)]\n    ptlt = [(1-(a/(a+item**c))**r)*(1-hcnb) for item in ts]\n    ptwt = [ptlt[i] if i == 0 else ptlt[i] - ptlt[i-1] for i,item in enumerate(ptlt)]\n    return (np.sum([j * np.log(i) for i,j in zip(ptwt,incr)]) \n            + (pop*(1-hcnb) - np.max(cdata)) * np.log(1-np.max(ptlt))\n            + np.log(hcnb)*pop*hcnb)*-1\n\ndef wg_func2(params):\n    a = params[0]\n    r = params[1]\n    c = params[2]\n    hcnb = .98\n    \n    incr = [cdata[i] if i == 0 else cdata[i] - cdata[i-1] for i,item in enumerate(cdata)]\n    ptlt = [(1-(a/(a+item**c))**r)*(1-hcnb) for item in ts]\n    ptwt = [ptlt[i] if i == 0 else ptlt[i] - ptlt[i-1] for i,item in enumerate(ptlt)]\n    return (np.sum([j * np.log(i) for i,j in zip(ptwt,incr)]) \n            + (pop*(1-hcnb) - np.max(cdata)) * np.log(1-np.max(ptlt))\n            + np.log(hcnb)*pop*hcnb)*-1\n\n\ndef constraint1(inputs):\n    return inputs[0]\n\n\ncons = ({'type': 'ineq', \"fun\": constraint1})\n\ndef calc_distance(lat1, lng1, lat2, lng2):\n    # approximate radius of earth in km\n    R = 6373.0\n    \n    #Python, all the trig functions use radians, not degrees\n    lat1 = radians(lat1)\n    lng1 = radians(lng1)\n    lat2 = radians(lat2)\n    lng2 = radians(lng2)\n    \n    dlon = lng2 - lng1\n    dlat = lat2 - lat1\n    \n    a = sin(dlat / 2)**2 + cos(lat1) * cos(lat2) * sin(dlon / 2)**2\n    c = 2 * atan2(sqrt(a), sqrt(1 - a))\n    \n    return R * c\n\ndef fill_missing_coords(df):\n    ## East is postiive\n    ## west is negative\n    ## north is possitive\n    ## south is negative\n    print(' Filling in missing lat,lng')\n    df.loc[df.Country_Region=='Zimbabwe', 'Lat'] = 19.0154\n    df.loc[df.Country_Region=='Zimbabwe', 'Long']= 29.1549\n    \n    df.loc[(df.Country_Region=='Angola') & (df.Province_State==''), 'Lat'] = -11.2027\n    df.loc[(df.Country_Region=='Angola') & (df.Province_State==''), 'Long']= 17.8739\n    \n    df.loc[(df.Country_Region=='Bahamas') & (df.Province_State==''), 'Lat'] = 25.0343\n    df.loc[(df.Country_Region=='Bahamas') & (df.Province_State==''), 'Long']= -77.3963\n    \n    df.loc[(df.Country_Region=='Belize') & (df.Province_State==''), 'Lat'] = 17.1899\n    df.loc[(df.Country_Region=='Belize') & (df.Province_State==''), 'Long']= -88.4976\n    \n    df.loc[(df.Country_Region=='United Kingdom') & (df.Province_State==''), 'Lat'] = 55.3781\n    df.loc[(df.Country_Region=='United Kingdom') & (df.Province_State==''), 'Long']= -3.4360\n    \n    df.loc[(df.Country_Region=='United Kingdom') & (df.Province_State=='Isle of Man'), 'Lat'] = 54.2361\n    df.loc[(df.Country_Region=='United Kingdom') & (df.Province_State=='Isle of Man'), 'Long']= -4.5481\n    \n    df.loc[(df.Country_Region=='Cabo Verde') & (df.Province_State==''), 'Lat'] = 16.5388\n    df.loc[(df.Country_Region=='Cabo Verde') & (df.Province_State==''), 'Long']= -23.0418\n    \n    df.loc[(df.Country_Region=='United Kingdom') & (df.Province_State=='Bermuda'), 'Lat'] = 32.3078\n    df.loc[(df.Country_Region=='United Kingdom') & (df.Province_State=='Bermuda'), 'Long']= -64.7505\n    \n    df.loc[(df.Country_Region=='Chad') & (df.Province_State==''), 'Lat'] = 15.4542\n    df.loc[(df.Country_Region=='Chad') & (df.Province_State==''), 'Long']= 18.7322\n    \n    df.loc[(df.Country_Region=='Uganda') & (df.Province_State==''), 'Lat'] = 1.3733\n    df.loc[(df.Country_Region=='Uganda') & (df.Province_State==''), 'Long']= 32.2903\n    \n    df.loc[(df.Country_Region=='Denmark') & (df.Province_State=='Greenland'), 'Lat'] = 71.7069\n    df.loc[(df.Country_Region=='Denmark') & (df.Province_State=='Greenland'), 'Long']= -42.6043\n    \n    df.loc[(df.Country_Region=='Denmark') & (df.Province_State==''), 'Lat'] = 56.2639\n    df.loc[(df.Country_Region=='Denmark') & (df.Province_State==''), 'Long']= 9.5018\n    \n    df.loc[(df.Country_Region=='Timor-Leste') & (df.Province_State==''), 'Lat'] = -8.8742\n    df.loc[(df.Country_Region=='Timor-Leste') & (df.Province_State==''), 'Long']= 125.7275\n    \n    df.loc[(df.Country_Region=='Syria') & (df.Province_State==''), 'Lat'] = 34.8021\n    df.loc[(df.Country_Region=='Syria') & (df.Province_State==''), 'Long']= 38.9968\n    \n    df.loc[(df.Country_Region=='Saint Kitts and Nevis') & (df.Province_State==''), 'Lat'] = 17.3578\n    df.loc[(df.Country_Region=='Saint Kitts and Nevis') & (df.Province_State==''), 'Long']= -62.7830\n    \n    df.loc[(df.Country_Region=='Papua New Guinea') & (df.Province_State==''), 'Lat'] = -6.3150\n    df.loc[(df.Country_Region=='Papua New Guinea') & (df.Province_State==''), 'Long']= 143.9555\n    \n    df.loc[(df.Country_Region=='Niger') & (df.Province_State==''), 'Lat'] = 17.6078\n    df.loc[(df.Country_Region=='Niger') & (df.Province_State==''), 'Long']= 8.0817\n    \n    df.loc[(df.Country_Region=='El Salvador') & (df.Province_State==''), 'Lat'] = 13.7942\n    df.loc[(df.Country_Region=='El Salvador') & (df.Province_State==''), 'Long']= -88.8965\n    \n    df.loc[(df.Country_Region=='Gambia') & (df.Province_State==''), 'Lat'] = 13.4432\n    df.loc[(df.Country_Region=='Gambia') & (df.Province_State==''), 'Long']= -15.3101\n    \n    df.loc[(df.Country_Region=='Libya') & (df.Province_State==''), 'Lat'] = 26.3351\n    df.loc[(df.Country_Region=='Libya') & (df.Province_State==''), 'Long']= 17.2283\n    \n    df.loc[(df.Country_Region=='Mali') & (df.Province_State==''), 'Lat'] = 17.5707\n    df.loc[(df.Country_Region=='Mali') & (df.Province_State==''), 'Long']= -3.9962\n    \n    df.loc[(df.Country_Region=='Grenada') & (df.Province_State==''), 'Lat'] = 12.1165\n    df.loc[(df.Country_Region=='Grenada') & (df.Province_State==''), 'Long']= -61.6790\n    \n    df.loc[(df.Country_Region=='Laos') & (df.Province_State==''), 'Lat'] = 19.8563\n    df.loc[(df.Country_Region=='Laos') & (df.Province_State==''), 'Long']= 102.4955\n    \n    df.loc[(df.Country_Region=='Madagascar') & (df.Province_State==''), 'Lat'] = -18.7669\n    df.loc[(df.Country_Region=='Madagascar') & (df.Province_State==''), 'Long']= 46.8691\n    \n    df.loc[(df.Country_Region=='Guinea-Bissau') & (df.Province_State==''), 'Lat'] = 11.8037\n    df.loc[(df.Country_Region=='Guinea-Bissau') & (df.Province_State==''), 'Long']= -15.1804\n    \n    df.loc[(df.Country_Region=='Fiji') & (df.Province_State==''), 'Lat'] = -17.7134\n    df.loc[(df.Country_Region=='Fiji') & (df.Province_State==''), 'Long']= 178.0650\n    \n    df.loc[(df.Country_Region=='Nicaragua') & (df.Province_State==''), 'Lat'] = 12.8654\n    df.loc[(df.Country_Region=='Nicaragua') & (df.Province_State==''), 'Long']= -85.2072\n    \n    df.loc[(df.Country_Region=='Eritrea') & (df.Province_State==''), 'Lat'] = 15.1794\n    df.loc[(df.Country_Region=='Eritrea') & (df.Province_State==''), 'Long']= 39.7823\n    \n    df.loc[(df.Country_Region=='Haiti') & (df.Province_State==''), 'Lat'] = 18.9712\n    df.loc[(df.Country_Region=='Haiti') & (df.Province_State==''), 'Long']= -72.2852\n    \n    df.loc[(df.Country_Region=='Dominica') & (df.Province_State==''), 'Lat'] = 15.4150\n    df.loc[(df.Country_Region=='Dominica') & (df.Province_State==''), 'Long']= -61.3710\n    \n    df.loc[(df.Country_Region=='Mozambique') & (df.Province_State==''), 'Lat'] = -18.6657\n    df.loc[(df.Country_Region=='Mozambique') & (df.Province_State==''), 'Long']= 35.5296\n    \n    df.loc[(df.Country_Region=='Netherlands') & (df.Province_State==''), 'Lat'] = 52.1326\n    df.loc[(df.Country_Region=='Netherlands') & (df.Province_State==''), 'Long']= 5.2913\n    \n    df.loc[(df.Country_Region=='Netherlands') & (df.Province_State=='Sint Maarten'), 'Lat'] = 18.0425\n    df.loc[(df.Country_Region=='Netherlands') & (df.Province_State=='Sint Maarten'), 'Long']= -63.0548\n    \n    df.loc[(df.Country_Region=='France') & (df.Province_State==''), 'Lat'] = 46.2276\n    df.loc[(df.Country_Region=='France') & (df.Province_State==''), 'Long']= 2.2137\n    \n    df.loc[(df.Country_Region=='France') & (df.Province_State=='New Caledonia'), 'Lat'] = -20.9043\n    df.loc[(df.Country_Region=='France') & (df.Province_State=='New Caledonia'), 'Long']= 165.6180\n    \n    df.loc[(df.Country_Region=='France') & (df.Province_State=='Martinique'), 'Lat'] = 14.6415\n    df.loc[(df.Country_Region=='France') & (df.Province_State=='Martinique'), 'Long']= -61.0242\n\nprint('done', datetime.now())\n\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"###############################################################################\n## Read Data\n###############################################################################\nPATH = '/kaggle/input/covid19-global-forecasting-week-2/'\ntrain  = pd.read_csv(PATH + 'train.csv')\ntest  = pd.read_csv(PATH + 'test.csv')\n\n#df_geo  = pd.read_csv('/kaggle/input/df-geo/' + 'df_geo.csv')\ndf_geo  = pd.read_csv('../input/df-geo/df_geo2.csv')\ndf_geo.Province_State.fillna('', inplace=True)\n\n\ndfp  = pd.read_csv('/kaggle/input/population3/' + 'population.csv')\ndfp.columns = ['Province_State', 'Country_Region', 'pop']\nprint(train.shape, dfp.shape)\n\n\n###############################################################################\n## Clean Data\n###############################################################################\nprint('Cleaning Data')\n## Fill in those missing states\ntrain.loc[train['Province_State'].isnull(), 'Province_State'] = ''\ntest.loc[test['Province_State'].isnull(), 'Province_State']   = ''\n\n\ndfp.loc[dfp['Province_State'].isnull(), 'Province_State']     = ''\n\n\n###############################################################################\n## Joining Data\n###############################################################################\nprint('Joining Data')\n\nprint(train.shape, test.shape)\nn = train.shape[0]\ntrain = pd.merge(train, dfp, on=['Country_Region','Province_State'], how='left')\nassert train.shape[0] == n\n\nn = test.shape[0]\ntest = pd.merge(test, dfp, on=['Country_Region','Province_State'], how='left')\nassert test.shape[0] == n\n\ntest.reset_index(drop=True, inplace=True)\ntrain.reset_index(drop=True, inplace=True)\n\nn = train.shape[0]\ntrain = pd.merge(train, df_geo, on=['Country_Region','Province_State'], how='left')\nassert train.shape[0] == n\n\nn = test.shape[0]\ntest = pd.merge(test, df_geo, on=['Country_Region','Province_State'], how='left')\nassert test.shape[0] == n\n\ntest.reset_index(drop=True, inplace=True)\ntrain.reset_index(drop=True, inplace=True)\n\ntrain.loc[train['pop'].isnull(),'pop'] = 0\ntest.loc[test['pop'].isnull(),'pop']   = 0\n\nprint('\\nNumber of countries with missing Lat/Lng: ',\n      train[train.Lat.isnull()]['Country_Region'].value_counts().shape[0])\n\nfill_missing_coords(train)\nfill_missing_coords(test)\n\nprint('\\nNumber of countries with missing Lat/Lng after fixing: ',\n      train[train.Lat.isnull()]['Country_Region'].value_counts().shape[0])\n\nprint(train.shape, test.shape)\n\n\n###############################################################################\n## Enrich Data\n###############################################################################\nprint('\\nEnriching Data')\n\n## Date Stuffs\nmo = train['Date'].apply(lambda x: x[5:7])\nda = train['Date'].apply(lambda x: x[8:10])\ntrain['day_from_jan_first'] = (da.apply(int)\n                               + 31*(mo=='02') \n                               + 60*(mo=='03')\n                               + 91*(mo=='04')  \n                              )\n\nmo = test['Date'].apply(lambda x: x[5:7])\nda = test['Date'].apply(lambda x: x[8:10])\ntest['day_from_jan_first'] = (da.apply(int)\n                               + 31*(mo=='02') \n                               + 60*(mo=='03')\n                               + 91*(mo=='04')  \n                              )\n\n\n#Create time features\ncreate_time_features(train)\ncreate_time_features(test)\n\nprint('done', datetime.now())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Geographic stuffs\n\ncountry1 = 'Luxembourg'\nlat1     = df_geo[df_geo.Country_Region==country1]['Lat'].item()\nlng1     = df_geo[df_geo.Country_Region==country1]['Long'].item()\n\ncountry2 = 'Singapore'\nlat2     = df_geo[df_geo.Country_Region==country2]['Lat'].item()\nlng2     = df_geo[df_geo.Country_Region==country2]['Long'].item()\n\n# This should be 10,436km\nprint('\\nDistance between ' + country1, country2, calc_distance(lat1, lng1, lat2, lng2))\nprint('This should be 10,436km')\n\nprint(' Label Encoding the geographic features...')\n\ndf1 = train[['Country_Region','Province_State','Lat','Long']].copy()\ndf2 = test[['Country_Region','Province_State','Lat','Long']].copy()\ngeo = pd.concat([df1,df2], axis=0)\ngeo = geo.groupby(['Country_Region','Province_State'])[['Lat','Long']].max().reset_index()\n\nle_country = LabelEncoder().fit( geo.Country_Region )\nle_state   = LabelEncoder().fit( geo.Province_State )\n\ntrain['country'] = le_country.transform( train.Country_Region)\ntrain['state']   = le_state.transform( train.Province_State)\n\ntest['country'] = le_country.transform( test.Country_Region)\ntest['state']   = le_state.transform( test.Province_State)\n\nprint('done', train.shape)\n\n# Probability Models\ntrain['wg']     = 0\ntrain['wga']    = 0\ntrain['wgr']    = 0\ntrain['wgc']    = 0\ntrain['wghcnb'] = 0\n\ntest['wg']     = 0\ntest['wga']    = 0\ntest['wgr']    = 0\ntest['wgc']    = 0\ntest['wghcnb'] = 0\n\ntest['SARIMAX'] = 0\ntest['ARIMA']   = 0\n\ncountries = ['Afghanistan']\n\nfor country in train.Country_Region.unique():\n    bool1 = train.Country_Region == country\n    print(country)\n    for state in train[bool1].Province_State.unique():\n        bool2 = bool1 & (train.Province_State == state) & (train.ConfirmedCases>0)\n        pop = np.max( train[bool2]['pop'] )\n        \n        data = train[ bool2 ].copy().reset_index()\n        data['ts'] = data.index+1\n        dfj   = data.iloc[0]['day_from_jan_first']\n        \n        cdata = data['ConfirmedCases'].values\n        ts = data['ts'].values\n\n        ## set up test stuffs\n        boolt = (test.Country_Region==country) & (test.Province_State== state)\n        datat = test[boolt].copy().reset_index()\n        datat['ts'] = datat.day_from_jan_first - dfj\n        datat.loc[datat.ts<=0,'ts'] = 1\n        \n        ## FIT WG\n        x0 = [1343, 5.440110881178935, 2.188935325131958, 0.9897823619555628]\n        sol  = minimize(wg_func, x0, constraints = cons)\n        a,r,c,hcnb = sol.x[0], sol.x[1], sol.x[2], sol.x[3]\n        if np.isnan(sol.x[0]):\n            train.loc[bool2, 'wg']    = np.NaN\n            train.loc[bool2,'wga']    = np.NaN\n            train.loc[bool2,'wgr']    = np.NaN\n            train.loc[bool2,'wgc']    = np.NaN\n            train.loc[bool2,'wghcnb'] = np.NaN\n            \n            test.loc[boolt, 'wg']    = np.NaN\n            test.loc[boolt,'wga']    = np.NaN\n            test.loc[boolt,'wgr']    = np.NaN\n            test.loc[boolt,'wgc']    = np.NaN\n            test.loc[boolt,'wghcnb'] = np.NaN\n        else:\n            datat['wg'] = datat.ts.apply(lambda x: (1-(a/(a+x**c))**r)*pop*(1-hcnb)).values\n            data['wg']  = data.ts.apply(lambda x: (1-(a/(a+x**c))**r)*pop*(1-hcnb)).values\n            train.loc[bool2, 'wg'] = data['wg'].values\n            train.loc[bool2,'wga']    = a\n            train.loc[bool2,'wgr']    = r\n            train.loc[bool2,'wgc']    = c\n            train.loc[bool2,'wghcnb'] = hcnb\n            \n            test.loc[boolt, 'wg'] = datat.wg.values\n            test.loc[boolt,'wga']    = a\n            test.loc[boolt,'wgr']    = r\n            test.loc[boolt,'wgc']    = c\n            test.loc[boolt,'wghcnb'] = hcnb\n        \n        ## Calculate Arima and SARIMAX\n        incr = [cdata[i] if i == 0 else cdata[i] - cdata[i-1] for i,item in enumerate(cdata)]\n        \n        ## get the last known cum count\n        cc = np.max( data[data.day_from_jan_first <=np.min(datat.day_from_jan_first)]['ConfirmedCases'])\n        try:\n            model_arima = ARIMA( incr, order=(1,1,0)).fit()\n            preds = [item if item >=0 else 0 for item in model_arima.predict(datat.ts[0].item() , datat.ts[-1:].item() )]\n            cum_sum = cc\n            preds_cc = []\n            for item in preds:\n                cum_sum = cum_sum + item\n                preds_cc.append(cum_sum)\n            test.loc[boolt, 'ARIMA'] = pd.Series(preds_cc).values\n        except:\n            test.loc[boolt, 'ARIMA']= np.NaN\n        \n        try:\n            model_SARIMAX = SARIMAX(incr, order=(1,1,0), seasonal_order=(1,1,0,12),enforce_stationarity=False).fit()\n            preds = [item if item >=0 else 0 for item in model_SARIMAX.predict(datat.ts[0].item() , datat.ts[-1:].item() )]\n            cum_sum = cc\n            preds_cc = []\n            for item in preds:\n                cum_sum = cum_sum + item\n                preds_cc.append(cum_sum)\n            \n            test.loc[boolt, 'SARIMAX']= pd.Series(preds_cc).values\n        except:\n            test.loc[boolt, 'SARIMAX']= np.NaN\n\nprint('done', datetime.now())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.loc[train.wg<0,'wg'] = 0\ntest.loc[test.wg<0,'wg'] = 0\n\ntrain.loc[train.wg.isnull(),'wg'] = 0\ntest.loc[test.wg.isnull(),'wg'] = 0\n\ntrain.loc[np.isinf(train.wg),'wg'] = 0\ntest.loc[np.isinf(test.wg),'wg'] = 0\nprint('done', datetime.now())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['wg2'] = 0\ntest['wg2'] = 0\ntrain['SARIMAX2'] = 0\ntrain['SARIMAX2'] = 0\n\ndata = train.groupby(['day_from_jan_first'])['ConfirmedCases','pop'].sum().reset_index()\n\npop = np.max( data['pop'] )\ndata['ts'] = data.index+1\ndfj   = data.iloc[0]['day_from_jan_first']\ncdata = data['ConfirmedCases'].values\nts = data['ts'].values\n\n## FIT WG\nx0 = [1343, 5.440110881178935, 2.188935325131958]\nsol  = minimize(wg_func2, x0, constraints = cons)\na,r,c = sol.x[0], sol.x[1], sol.x[2]\n\nprint(a,r,c)\n\nfor country in train.Country_Region.unique():\n    bool1 = train.Country_Region == country\n    for state in train[bool1].Province_State.unique():\n        bool2 = bool1 & (train.Province_State == state)\n        pop = np.max( train[bool2]['pop'] )\n        \n        data = train[ bool2 ].copy().reset_index()\n        data['ts'] = data.index+1\n        dfj   = data.iloc[0]['day_from_jan_first']\n        \n        cdata = data['ConfirmedCases'].values\n        ts = data['ts'].values\n\n        ## set up test stuffs\n        boolt = (test.Country_Region==country) & (test.Province_State== state)\n        datat = test[boolt].copy().reset_index()\n        datat['ts'] = datat.day_from_jan_first - dfj\n        datat.loc[datat.ts<=0,'ts'] = 1\n        \n        ## FIT WG\n        x0 = [1343, 5.440110881178935, 2.188935325131958, 0.9897823619555628]\n        sol  = minimize(wg_func, x0, constraints = cons)\n        a,r,c,hcnb = sol.x[0], sol.x[1], sol.x[2], sol.x[3]\n        if not np.isnan(sol.x[0]):\n            datat['wg2'] = datat.ts.apply(lambda x: (1-(a/(a+x**c))**r)*pop*(1-hcnb)).values\n            data[ 'wg2']  = data.ts.apply(lambda x: (1-(a/(a+x**c))**r)*pop*(1-hcnb)).values\n            train.loc[bool2, 'wg2'] = data['wg2'].values\n            test.loc[boolt, 'wg2']  = datat.wg2.values\n            \n        try:\n            model_SARIMAX = SARIMAX(data.ConfirmedCases, order=(1,1,0), seasonal_order=(1,1,0,12),enforce_stationarity=False).fit()\n            preds = [item if item >=0 else 0 for item in model_SARIMAX.predict(datat.ts[0].item() , datat.ts[-1:].item() )]\n            preds2 = [item if item >=0 else 0 for item in model_SARIMAX.predict(data.ts[0].item() , data.ts[-1:].item() )]\n            train.loc[bool2, 'SARIMAX2']= pd.Series(preds2).values\n            test.loc[boolt, 'SARIMAX2']= pd.Series(preds).values\n        except:\n            test.loc[boolt, 'SARIMAX2']= np.NaN\nprint('done', datetime.now())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = train.groupby(['day_from_jan_first'])['ConfirmedCases','pop'].sum().reset_index()\n\ntrain['wg3'] = 0\ntest['wg3'] = 0\n\npop = np.max( data['pop'] )\ndata['ts'] = data.index+1\ndfj   = data.iloc[0]['day_from_jan_first']\ncdata = data['ConfirmedCases'].values\nts = data['ts'].values\n\n## FIT WG\nx0 = [1343, 5.440110881178935, 2.188935325131958]\nsol  = minimize(wg_func2, x0, constraints = cons)\na,r,c = sol.x[0], sol.x[1], sol.x[2]\n\n\nfor country in train.Country_Region.unique():\n    bool1 = train.Country_Region == country\n    for state in train[bool1].Province_State.unique():\n        bool2 = bool1 & (train.Province_State == state)\n        pop = np.max( train[bool2]['pop'] )\n        \n        data = train[ bool2 ].copy().reset_index()\n        data['ts'] = data.index+1\n        dfj   = data.iloc[0]['day_from_jan_first']\n        \n        cdata = data['ConfirmedCases'].values\n        ts = data['ts'].values\n\n        ## set up test stuffs\n        boolt = (test.Country_Region==country) & (test.Province_State== state)\n        datat = test[boolt].copy().reset_index()\n        datat['ts'] = datat.day_from_jan_first - dfj\n        datat.loc[datat.ts<=0,'ts'] = 1\n        \n        datat['wg3'] = datat.ts.apply(lambda x: (1-(a/(a+x**c))**r)*pop*(1-hcnb)).values\n        data[ 'wg3']  = data.ts.apply(lambda x: (1-(a/(a+x**c))**r)*pop*(1-hcnb)).values\n        train.loc[bool2, 'wg3'] = data['wg3'].values\n        test.loc[boolt, 'wg3']  = datat.wg3.values\nprint('done', datetime.now())\nprint(a,r,c)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.loc[train.wg2<0,'wg2'] = 0\ntest.loc[test.wg2<0,'wg2'] = 0\n\ntrain.loc[train.wg2.isnull(),'wg2'] = 0\ntest.loc[test.wg2.isnull(),'wg2']   = 0\n\ntrain.loc[np.isinf(train.wg2),'wg2'] = 0\ntest.loc[ np.isinf(test.wg2),'wg2'] = 0\n\ntrain.loc[train.wg3<0,'wg3'] = 0\ntest.loc[test.wg3<0,'wg3'] = 0\n\ntrain.loc[train.wg3.isnull(),'wg3'] = 0\ntest.loc[test.wg3.isnull(),'wg3']   = 0\n\ntrain.loc[np.isinf(train.wg3),'wg3'] = 0\ntest.loc[ np.isinf(test.wg3),'wg3'] = 0\nprint('done', datetime.now())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(mean_squared_log_error(train.ConfirmedCases, train.wg))\nprint(mean_squared_log_error(train.ConfirmedCases, train.wg2))\nprint(mean_squared_log_error(train.ConfirmedCases, train.wg3))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for country in train.Country_Region.unique():\n    bool1 = train.Country_Region == country\n    for state in train[bool1].Province_State.unique():\n        bool2 = bool1 & (train.Province_State == state)\n        data = train[ bool2 ].copy().reset_index()\n        \n        wg1 = np.round(mean_squared_log_error(data.ConfirmedCases, data.wg),2)\n        wg2 = np.round(mean_squared_log_error(data.ConfirmedCases, data.wg2),2)\n        wg3 = np.round(mean_squared_log_error(data.ConfirmedCases, data.wg3),2)\n        \n        ## set up test stuffs\n        boolt = (test.Country_Region==country) & (test.Province_State== state)\n        datat = test[boolt].copy().reset_index()\n        \n        if wg1<wg2 and wg1<wg3:\n            data['best']  = data.wg\n            datat['best'] = datat.wg\n        elif wg2<wg1 and wg2 < wg3:\n            data['best']  = data.wg2\n            datat['best'] = datat.wg2\n        else:\n            data['best']  = data.wg3\n            datat['best'] = datat.wg3\n        \n        train.loc[bool2, 'wg'] = data['best'].values\n        test.loc[boolt, 'wg']  = datat['best'].values\nprint('done', datetime.now())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(mean_squared_log_error(train.ConfirmedCases, train.wg))\n\ntrain.drop('wg2', inplace=True, axis=1)\ntrain.drop('wg3', inplace=True, axis=1)\n\ntest.drop('wg2', inplace=True, axis=1)\ntest.drop('wg3', inplace=True, axis=1)\nprint('done', datetime.now())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"###############################################################################\n## logistic\n###############################################################################\n\nfrom scipy.optimize import curve_fit\n\n\ntrain_data = train.copy()\ntrain_df = train_data\ntrain_df['area'] = [str(i)+str(' - ')+str(j) for i,j in zip(train_data['Country_Region'], train_data['Province_State'])]\ntrain_df['Date'] = pd.to_datetime(train_df['Date'])\nfull_data = train_df\n\ntoday = full_data['Date'].max()+timedelta(days=1) \n\ndef get_country_data(train_df, area, metric):\n    country_data = train_df[train_df['area']==area]\n    country_data = country_data.drop(['Id','Province_State', 'Country_Region', 'Lat','Long'], axis=1)\n    country_data = pd.pivot_table(country_data, values=['ConfirmedCases','Fatalities'], index=['Date'], aggfunc=np.sum) \n    country_data = country_data[country_data[metric]!=0]\n    return country_data      \n\narea_info = pd.DataFrame(columns=['area', 'cases_start_date', 'deaths_start_date', 'init_ConfirmedCases', 'init_Fatalities'])\nfor i in range(len(train_df['area'].unique())):\n    area = train_df['area'].unique()[i]\n    area_cases_data = get_country_data(train_df, area, 'ConfirmedCases')\n    area_deaths_data = get_country_data(train_df, area, 'Fatalities')\n    cases_start_date = area_cases_data.index.min()\n    deaths_start_date = area_deaths_data.index.min()\n    if len(area_cases_data) > 0:\n        confirmed_cases = max(area_cases_data['ConfirmedCases'])\n    else:\n        confirmed_cases = 0\n    if len(area_deaths_data) > 0:\n        fatalities = max(area_deaths_data['Fatalities'])\n    else:\n        fatalities = 0\n    area_info.loc[i] = [area, cases_start_date, deaths_start_date, confirmed_cases, fatalities]\n\narea_info = area_info.fillna(pd.to_datetime(today))\narea_info['init_cases_day_no'] = pd.to_datetime(today)-area_info['cases_start_date']\narea_info['init_cases_day_no'] = area_info['init_cases_day_no'].dt.days.fillna(0).astype(int)\narea_info['init_deaths_day_no'] = pd.to_datetime(today)-area_info['deaths_start_date']\narea_info['init_deaths_day_no'] = area_info['init_deaths_day_no'].dt.days.fillna(0).astype(int)\narea_info.head()\n\n\ndef log_curve(x, k, x_0, ymax):\n    return ymax / (1 + np.exp(-k*(x-x_0)))\n    \ndef log_fit(train_df, area, metric):\n    area_data = get_country_data(train_df, area, metric)\n    x_data = range(len(area_data.index))\n    y_data = area_data[metric]\n    if len(y_data) < 5:\n        estimated_k = -1  \n        estimated_x_0 = -1 \n        ymax = -1\n    elif max(y_data) == 0:\n        estimated_k = -1  \n        estimated_x_0 = -1 \n        ymax = -1\n    else:\n        try:\n            popt, pcov = curve_fit(log_curve, x_data, y_data, bounds=([0,0,0],np.inf), p0=[0.3,100,10000], maxfev=1000000)\n            estimated_k, estimated_x_0, ymax = popt\n        except RuntimeError:\n            print(area)\n            print(\"Error - curve_fit failed\") \n            estimated_k = -1  \n            estimated_x_0 = -1 \n            ymax = -1\n    estimated_parameters = pd.DataFrame(np.array([[area, estimated_k, estimated_x_0, ymax]]), columns=['area', 'k', 'x_0', 'ymax'])\n    return estimated_parameters\n\ndef get_parameters(metric):\n    parameters = pd.DataFrame(columns=['area', 'k', 'x_0', 'ymax'], dtype=np.float)\n    for area in train_df['area'].unique():\n        estimated_parameters = log_fit(train_df, area, metric)\n        parameters = parameters.append(estimated_parameters)\n    parameters['k'] = pd.to_numeric(parameters['k'], downcast=\"float\")\n    parameters['x_0'] = pd.to_numeric(parameters['x_0'], downcast=\"float\")\n    parameters['ymax'] = pd.to_numeric(parameters['ymax'], downcast=\"float\")\n    parameters = parameters.replace({'k': {-1: parameters[parameters['ymax']>0].median()[0]}, \n                                     'x_0': {-1: parameters[parameters['ymax']>0].median()[1]}, \n                                     'ymax': {-1: parameters[parameters['ymax']>0].median()[2]}})\n    return parameters\n\n\ncases_parameters = get_parameters('ConfirmedCases')\ncases_parameters.head(20)\n\ndeaths_parameters = get_parameters('Fatalities')\ndeaths_parameters.head(20)\n\nfit_df = area_info.merge(cases_parameters, on='area', how='left')\nfit_df = fit_df.rename(columns={\"k\": \"cases_k\", \"x_0\": \"cases_x_0\", \"ymax\": \"cases_ymax\"})\nfit_df = fit_df.merge(deaths_parameters, on='area', how='left')\nfit_df = fit_df.rename(columns={\"k\": \"deaths_k\", \"x_0\": \"deaths_x_0\", \"ymax\": \"deaths_ymax\"})\nfit_df['init_ConfirmedCases_fit'] = log_curve(fit_df['init_cases_day_no'], fit_df['cases_k'], fit_df['cases_x_0'], fit_df['cases_ymax'])\nfit_df['init_Fatalities_fit'] = log_curve(fit_df['init_deaths_day_no'], fit_df['deaths_k'], fit_df['deaths_x_0'], fit_df['deaths_ymax'])\nfit_df['ConfirmedCases_error'] = fit_df['init_ConfirmedCases']-fit_df['init_ConfirmedCases_fit']\nfit_df['Fatalities_error'] = fit_df['init_Fatalities']-fit_df['init_Fatalities_fit']\nfit_df.head()\n\ntest_data = test.copy()\ntest_df = test_data\ntest_df['area'] = [str(i)+str(' - ')+str(j) for i,j in zip(test_data['Country_Region'], test_data['Province_State'])]\ntest_df = test_df.merge(fit_df, on='area', how='left')\ntest_df['Date'] = pd.to_datetime(test_df['Date'])\n\ntest_df['cases_start_date'] = pd.to_datetime(test_df['cases_start_date'])\ntest_df['deaths_start_date'] = pd.to_datetime(test_df['deaths_start_date'])\ntest_df['cases_day_no'] = test_df['Date']-test_df['cases_start_date']\ntest_df['cases_day_no'] = test_df['cases_day_no'].dt.days.fillna(0).astype(int)\ntest_df['deaths_day_no'] = test_df['Date']-test_df['deaths_start_date']\ntest_df['deaths_day_no'] = test_df['deaths_day_no'].dt.days.fillna(0).astype(int)\ntest_df['ConfirmedCases_fit'] = log_curve(test_df['cases_day_no'], test_df['cases_k'], test_df['cases_x_0'], test_df['cases_ymax'])\ntest_df['Fatalities_fit'] = log_curve(test_df['deaths_day_no'], test_df['deaths_k'], test_df['deaths_x_0'], test_df['deaths_ymax'])\ntest_df['ConfirmedCases_pred'] = round(test_df['ConfirmedCases_fit']+test_df['ConfirmedCases_error'])\ntest_df['Fatalities_pred'] = round(test_df['Fatalities_fit']+test_df['Fatalities_error'])\ntest_df.head()\n\ntrain_df = train.copy()\ntrain_df['area'] = [str(i)+str(' - ')+str(j) for i,j in zip(train_df['Country_Region'], train_df['Province_State'])]\ntrain_df         = train_df.merge(fit_df, on='area', how='left')\ntrain_df['Date'] = pd.to_datetime(train_df['Date'])\ntrain_df['cases_start_date'] = pd.to_datetime(train_df['cases_start_date'])\ntrain_df['deaths_start_date'] = pd.to_datetime(train_df['deaths_start_date'])\ntrain_df['cases_day_no'] = train_df['Date']-train_df['cases_start_date']\ntrain_df['cases_day_no'] = train_df['cases_day_no'].dt.days.fillna(0).astype(int)\ntrain_df['deaths_day_no'] = train_df['Date']-train_df['deaths_start_date']\ntrain_df['deaths_day_no'] = train_df['deaths_day_no'].dt.days.fillna(0).astype(int)\ntrain_df['ConfirmedCases_fit'] = log_curve(train_df['cases_day_no'], train_df['cases_k'], train_df['cases_x_0'], train_df['cases_ymax'])\ntrain_df['Fatalities_fit'] = log_curve(train_df['deaths_day_no'], train_df['deaths_k'], train_df['deaths_x_0'], train_df['deaths_ymax'])\ntrain_df['ConfirmedCases_pred'] = round(train_df['ConfirmedCases_fit']+train_df['ConfirmedCases_error'])\ntrain_df['Fatalities_pred'] = round(train_df['Fatalities_fit']+train_df['Fatalities_error'])\n\ntrain_df.head()\nprint('done', datetime.now())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['y_hat_log'  ]                = test_df.ConfirmedCases_fit\ntest['y_hat_log'  ]                = test_df.ConfirmedCases_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"###############################################################################\n## Modeling\n###############################################################################\ndropcols = ['Date', 'date', 'ConfirmedCases', 'Id', 'ForecastId', 'Fatalities']\ndropcols = dropcols + ['Country_Region','Province_State']\ndropcols = dropcols + ['eg', 'egr', 'mae_eg', 'wga', 'wgr', 'wgc','wghcnb', 'mae_wg', 'SARIMAX', 'ARIMA',\n           'cc_es',]\n\nprint('\\nModeling...')\nfeatures = [f for f in train.columns if f not in dropcols + \n            ['shift4w', 'shift6w', 'dist'\n             , 'dayofyear','year','quarter','hour','month','dayofmonth','dayofweek','weekofyear', 'Lat', 'Long']]\n\nprint(features)\n\nX_train = train[features].copy()\nX_test  = test[features].copy()\n\nX_train.reset_index(drop=True, inplace=True)\nX_test.reset_index(drop=True, inplace=True)\n\ny_train    = train[\"Fatalities\"]\ny_train_cc = train[\"ConfirmedCases\"]\n\nprint('done', datetime.now())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"isTraining = False\nparams_xgb = {}\nparams_xgb['n_estimators']       = 1100\nparams_xgb['max_depth']          = 10\nparams_xgb['seed']               = 2020\nparams_xgb['colsample_bylevel']  = 1\nparams_xgb['colsample_bytree']   = 1\nparams_xgb['learning_rate']      = 0.3\nparams_xgb['reg_alpha']          = 0\nparams_xgb['reg_lambda']         = 1\nparams_xgb['subsample']          = 1\n\n\nif isTraining:\n    X_TRAIN = X_train[features].values\n\n    kf      = KFold(n_splits = 5, shuffle = True, random_state=2020)\n    acc     = []\n\n    for tr_idx, val_idx in kf.split(X_TRAIN, y_train_cc):\n        ## Set up XY train/validation\n        X_tr, X_vl = X_TRAIN[tr_idx], X_TRAIN[val_idx, :]\n        y_tr, y_vl = y_train_cc[tr_idx], y_train_cc[val_idx]\n        print(X_tr.shape)\n\n        model_xgb_cc = xgb.XGBRegressor(**params_xgb)\n        model_xgb_cc.fit(X_tr, y_tr, verbose=True)\n        y_hat = model_xgb_cc.predict(X_vl)\n\n        print('xgb mae :', mean_absolute_error(  y_vl, y_hat) )\n        acc.append(mean_absolute_error( y_vl, y_hat) )\n\n\n    print('done', np.mean(acc))\nprint('done', datetime.now())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Fit confirmed cases with out wg\ndropcols = ['Date', 'date', 'ConfirmedCases', 'Id', 'ForecastId', 'Fatalities']\ndropcols = dropcols + ['Country_Region','Province_State']\ndropcols = dropcols + ['eg', 'egr', 'mae_eg', 'wg', 'wga', 'wgr', 'wg_xgb', 'wgc','wghcnb', 'mae_wg', 'SARIMAX', 'ARIMA',\n           'cc_es',]\n\nprint('\\nModeling...')\nfeatures = [f for f in train.columns if f not in dropcols + \n            ['shift4w', 'shift6w', 'dist'\n             , 'dayofyear','year','quarter','hour','month','dayofmonth','dayofweek','weekofyear']]\n\nprint(features)\n\nX_train = train[features].copy()\nX_test  = test[features].copy()\n\nX_train.reset_index(drop=True, inplace=True)\nX_test.reset_index(drop=True, inplace=True)\n\nX_train.head()\nprint('done', datetime.now())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params_xgb = {}\nparams_xgb['n_estimators']       = 1100\nparams_xgb['max_depth']          = 10\nparams_xgb['seed']               = 2020\nparams_xgb['colsample_bylevel']  = 1\nparams_xgb['colsample_bytree']   = 1\nparams_xgb['learning_rate']      = 0.3\nparams_xgb['reg_alpha']          = 0\nparams_xgb['reg_lambda']         = 1                                          \nparams_xgb['subsample']          = 1\n\n\nisTraining = True\n\nX_train.reset_index(drop=True, inplace=True)\n\nif isTraining:\n    print('Evaluating model...')\n    booll = X_train.day_from_jan_first < 86\n    X_tr, X_vl = X_train[booll][features        ], X_train[~booll][features]\n    y_tr, y_vl = train[booll]['ConfirmedCases'], train[~booll]['ConfirmedCases']\n\n    model_xgb_cc = xgb.XGBRegressor(**params_xgb)\n    model_xgb_cc.fit(X_tr, y_tr, verbose=True)\n    y_hat = model_xgb_cc.predict(X_vl)\n    y_hat[y_hat<0] = 0\n    print('xgb mae :', mean_absolute_error(  y_vl, y_hat), mean_squared_log_error(y_vl, y_hat), X_tr.shape, X_vl.shape ) \n\n    \nprint('done', datetime.now())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train    = train[\"Fatalities\"]\ny_train_cc = train[\"ConfirmedCases\"]\n\nparams_xgb = {}\nparams_xgb['n_estimators']       = 1100\nparams_xgb['max_depth']          = 10\nparams_xgb['seed']               = 2020\nparams_xgb['colsample_bylevel']  = 1\nparams_xgb['colsample_bytree']   = 1\nparams_xgb['learning_rate']      = 0.3\nparams_xgb['reg_alpha']          = 0\nparams_xgb['reg_lambda']         = 1\nparams_xgb['subsample']          = 1\n\nmodel_xgb_cc = xgb.XGBRegressor(**params_xgb).fit(X_train[features], y_train_cc, verbose=True)\ny_hat_xgb_c  = model_xgb_cc.predict(X_test[features])\nprint('done', datetime.now())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"###############################################################################\n## Feature Importantce\n###############################################################################\n\nplot = plot_importance(model_xgb_cc, height=0.9, max_num_features=20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Fit fatalities\nparams_xgb = {}\nparams_xgb['n_estimators']       = 1100\nparams_xgb['max_depth']          = 10\nparams_xgb['seed']               = 2020\nparams_xgb['colsample_bylevel']  = 1\nparams_xgb['colsample_bytree']   = 1\nparams_xgb['learning_rate']      = 0.300000012\nparams_xgb['reg_alpha']          = 0\nparams_xgb['reg_lambda']         = 1\nparams_xgb['subsample']          = 1\n\nmodel_xgb_f = xgb.XGBRegressor(**params_xgb).fit(X_train[features], y_train, verbose=True)\ny_hat_xgb_f = model_xgb_f.predict(X_test[features])\nprint(np.mean(y_hat_xgb_f))\nprint('done', datetime.now())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features2 = [f for f in features if f not in ['SARIMAX2', 'y_hat_log']]\n\nprint(features2)\nX_train2 = train[features2].copy()\nX_test2  = test[features2].copy()\n\nX_train2.reset_index(drop=True, inplace=True)\nX_test2.reset_index(drop=True, inplace=True)\n\nmodel_xgb_cc2 = xgb.XGBRegressor(**params_xgb).fit(X_train2, y_train_cc, verbose=True)\ny_hat_xgb_c2  = model_xgb_cc2.predict(X_test2)\nprint('done', datetime.now())\n\nplot = plot_importance(model_xgb_cc2, height=0.9, max_num_features=20)\nprint('done', datetime.now())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['y_hat_xgb_c']                = y_hat_xgb_c\ntest['y_hat_xgb_f']                = y_hat_xgb_f\ntest['y_hat_xgb_c2']               = y_hat_xgb_c2\n\n## Fix negative numbers\nprint('Fixing Negative Predictions:'\n     , np.sum(test.y_hat_xgb_c < 0)\n      , np.sum(test.y_hat_xgb_c2 < 0)\n     , np.sum(test.y_hat_xgb_f< 0)\n     , np.sum(test.wg< 0)\n     , np.sum(test.ARIMA< 0)\n     , np.sum(test.SARIMAX< 0)\n     , np.sum(test.y_hat_log< 0))\ntest.loc[test.y_hat_xgb_c < 0, 'y_hat_xgb_c'] = 0\ntest.loc[test.y_hat_xgb_c2 < 0, 'y_hat_xgb_c2'] = 0\ntest.loc[test.y_hat_xgb_f < 0, 'y_hat_xgb_f'] = 0\ntest.loc[test.wg < 0, 'wg']               = 0\ntest.loc[test.ARIMA < 0, 'ARIMA']         = 0\ntest.loc[test.SARIMAX < 0, 'SARIMAX']     = 0\ntest.loc[test.y_hat_log < 0, 'y_hat_log'] = 0\n\nprint('done', datetime.now())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Fixing Inf Predictions:'\n     , np.sum(test.y_hat_xgb_c.isnull())\n     , np.sum(test.y_hat_xgb_c2.isnull())\n     , np.sum(test.y_hat_xgb_f.isnull())\n     , np.sum(test.wg.isnull())\n     , np.sum(test.ARIMA.isnull())\n     , np.sum(test.SARIMAX.isnull())\n     , np.sum(test.y_hat_log.isnull()))\n\nbooll   = (test['SARIMAX'].isnull())\ntest.loc[booll, 'SARIMAX'] = test[booll]['y_hat_log']\n\nbooll   = (test['ARIMA'].isnull())\ntest.loc[booll, 'ARIMA'] = test[booll]['y_hat_log']\n\nprint('Fixing Inf Predictions:'\n     , np.sum(test.y_hat_xgb_c.isnull())\n     , np.sum(test.y_hat_xgb_c2.isnull())\n     , np.sum(test.y_hat_xgb_f.isnull())\n     , np.sum(test.wg.isnull())\n     , np.sum(test.ARIMA.isnull())\n     , np.sum(test.SARIMAX.isnull())\n     , np.sum(test.y_hat_log.isnull()))\nprint('done', datetime.now())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Fixing Unrealistic Predictions:',\n      np.sum( test['pop'] * (700/3100) < test.y_hat_xgb_c),\n      np.sum( test['pop'] * (700/3100) < test.y_hat_xgb_c2),\n      np.sum( test['pop'] * (700/3100) < test.y_hat_xgb_f),\n      np.sum( test['pop'] * (700/3100) < test.wg),\n      np.sum( test['pop'] * (700/3100) < test.ARIMA),\n      np.sum( test['pop'] * (700/3100) < test.SARIMAX),\n      np.sum( test['pop'] * (700/3100) < test.y_hat_log),\n           )\n\nbooll   = (test['pop'] * (700/3100) < test.y_hat_xgb_c)\ntest.loc[booll, 'y_hat_xgb_c'] = test[booll]['pop'] * (700/3100) # from cruise ships\n\nbooll   = (test['pop'] * (700/3100) < test.y_hat_xgb_c2)\ntest.loc[booll, 'y_hat_xgb_c2'] = test[booll]['pop'] * (700/3100)\n\nbooll   = (test['pop'] * (700/3100) < test.y_hat_xgb_f)\ntest.loc[booll, 'y_hat_xgb_f'] = test[booll]['pop'] * (700/3100)\n\nbooll   = (test['pop'] * (700/3100) < test.wg)\ntest.loc[booll, 'wg'] = test[booll]['pop'] * (700/3100)\n\nbooll   = (test['pop'] * (700/3100) < test.ARIMA)\ntest.loc[booll, 'ARIMA'] = test[booll]['pop'] * (700/3100)\n\nbooll   = (test['pop'] * (700/3100) < test.SARIMAX)\ntest.loc[booll, 'SARIMAX'] = test[booll]['pop'] * (700/3100) \n\nbooll   = (test['pop'] * (700/3100) < test.y_hat_log)\ntest.loc[booll, 'y_hat_log'] = test[booll]['pop'] * (700/3100)\n\nprint('Fixed Unrealistic Predictions:',\n      np.sum( test['pop'] * (700/3100) < test.y_hat_xgb_c),\n      np.sum( test['pop'] * (700/3100) < test.y_hat_xgb_f),\n      np.sum( test['pop'] * (700/3100) < test.wg),\n      np.sum( test['pop'] * (700/3100) < test.ARIMA),\n      np.sum( test['pop'] * (700/3100) < test.SARIMAX),\n      np.sum( test['pop'] * (700/3100) < test.y_hat_log),\n           )\nprint('done', datetime.now())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\n    (train.Province_State=='')&\n    (train.ConfirmedCases>0)&\n    (train.Country_Region=='France')][['Date','ConfirmedCases', 'wg']].tail(25)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['y_hat_ens']  = test.y_hat_xgb_c *.15   + test.wg *.05 + test.y_hat_log *.30  + test['SARIMAX'] * .03 + test.y_hat_xgb_c2 *.47\ntest[(test.Province_State=='')&(test.Country_Region=='France')][['Date','y_hat_xgb_c','y_hat_xgb_c2', 'wg','SARIMAX', 'SARIMAX2','y_hat_log', 'y_hat_ens']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# In case I missed anything\nprint('Empty Predictions?', np.sum(test.y_hat_ens.isnull()))\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\n    (train.Province_State=='')&\n    (train.ConfirmedCases>0)&\n    (train.Country_Region=='Turkey')][['Date','ConfirmedCases', 'wg', 'day_from_jan_first']].tail(25)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fixing 'bad' models\nnarf = train[train.day_from_jan_first== 79].copy()\nnarf = narf.merge(test[test.day_from_jan_first== 79], on=['Country_Region','Province_State','day_from_jan_first'])\n\nnarf['err'] = np.abs((narf.wg_y - narf.ConfirmedCases)/(1+narf.ConfirmedCases))\n\nprint(mean_squared_log_error(narf.ConfirmedCases,narf.y_hat_ens))\nprint('wg', mean_squared_log_error(narf.ConfirmedCases,narf.wg_y))\nprint('SARIMAX', mean_squared_log_error(narf.ConfirmedCases,narf.SARIMAX))\nprint('y_hat_log', mean_squared_log_error(narf.ConfirmedCases,narf.y_hat_log_y))\nprint('y_hat_xgb_c', mean_squared_log_error(narf.ConfirmedCases,narf.y_hat_xgb_c))\n\nnarf = narf[narf.err>100].copy()\nif narf.shape[0]> 0:\n    print(mean_squared_log_error(narf.ConfirmedCases,narf.y_hat_ens))\nelse:\n    print('no fixes for wg_y')\n    \nfor index, row in narf.iterrows():\n    country = row['Country_Region']\n    state   = row['Province_State']\n    booll   = (test.Country_Region==country) & (test.Province_State==state)\n    test.loc[booll, 'wg'] = (test[booll].y_hat_log) * .3 + (test[booll].y_hat_xgb_c) * .7\n\nprint('done', datetime.now())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\n    (train.Province_State=='')&\n    (train.ConfirmedCases>0)&\n    (train.Country_Region=='France')][['Date','ConfirmedCases', 'wg', 'day_from_jan_first']].tail(25)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['y_hat_ens']  = test.y_hat_xgb_c *.15   + test.wg *.05 + test.y_hat_log *.30  + test['SARIMAX'] * .03 + test.y_hat_xgb_c2 *.47\ntest[(test.Province_State=='')&(test.Country_Region=='France')][['Date','y_hat_xgb_c','y_hat_xgb_c2', 'wg', 'ARIMA','SARIMAX','y_hat_log', 'y_hat_ens']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fixing 'bad' models\nnarf = train[train.day_from_jan_first== 79].copy()\nnarf = narf.merge(test[test.day_from_jan_first== 79], on=['Country_Region','Province_State','day_from_jan_first'])\n\nnarf['err'] = np.abs((narf.y_hat_log_y - narf.ConfirmedCases)/(1+narf.ConfirmedCases))\n\nprint(mean_squared_log_error(narf.ConfirmedCases,narf.y_hat_ens))\nprint('wg', mean_squared_log_error(narf.ConfirmedCases,narf.wg_y))\nprint('SARIMAX', mean_squared_log_error(narf.ConfirmedCases,narf.SARIMAX))\nprint('y_hat_log', mean_squared_log_error(narf.ConfirmedCases,narf.y_hat_log_y))\nprint('y_hat_xgb_c', mean_squared_log_error(narf.ConfirmedCases,narf.y_hat_xgb_c))\n\nnarf = narf[narf.err>10].copy()\nif narf.shape[0]> 0:\n    print(mean_squared_log_error(narf.ConfirmedCases,narf.y_hat_ens))\nelse:\n    print('no fixes for y_hat_log')\n    \nfor index, row in narf.iterrows():\n    country = row['Country_Region']\n    state   = row['Province_State']\n    booll   = (test.Country_Region==country) & (test.Province_State==state)\n    #test.loc[booll, 'wg'] = (test[booll].y_hat_log) * .3 + (test[booll].y_hat_xgb_c) * .7\nprint('done', datetime.now())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\n    (train.Province_State=='')&\n    (train.ConfirmedCases>0)&\n    (train.Country_Region=='Turkey')][['Date','ConfirmedCases', 'wg', 'day_from_jan_first']].tail(25)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test[test.y_hat_ens==1521225]\ntest[(test.Province_State=='')&(test.Country_Region=='Turkey')][['Date','y_hat_xgb_c', 'wg', 'ARIMA','SARIMAX','y_hat_log', 'y_hat_ens']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['y_hat_ens'] = test.y_hat_ens.astype(int)\nprint(np.max( test['y_hat_ens']  ))\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"###############################################################################\n## Submision\n###############################################################################\n\nsubmissionOrig = pd.read_csv(\"../input/covid19-global-forecasting-week-2/submission.csv\")\nsubmissionOrig[\"ConfirmedCases\"]= pd.Series( test.y_hat_ens)\nsubmissionOrig[\"Fatalities\"]    = pd.Series( test.y_hat_xgb_f)\nsubmissionOrig.to_csv('submission.csv',index=False)\nsubmissionOrig.head(25)\nprint('done', datetime.now())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}