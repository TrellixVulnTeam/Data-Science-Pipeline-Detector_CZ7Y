{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Wed Apr  1 19:58:18 2020\n\n@author: Jaehoon Cha \n@email: chajaehoon79@gmail.com\n\"\"\"\nimport pandas as pd\nimport numpy as np\nimport os\n\n'''\ncall data\n'''\nmother_given_path = \"/kaggle/input/covid19-global-forecasting-week-2\"\n\ntrain_given_path = os.path.join(mother_given_path, \"train.csv\")\ntest_given_path = os.path.join(mother_given_path, \"test.csv\")\n\ntrain = pd.read_csv(train_given_path)\ntest = pd.read_csv(test_given_path)\n\n\ntrain.columns.values[2] = 'Country'\ntrain.columns.values[1] = 'Subplace'\ntrain.fillna(\"\", inplace = True)\n\ntest.columns.values[2] = 'Country'\ntest.columns.values[1] = 'Subplace'\ntest.fillna(\"\", inplace = True)\n\n\nCountries = train.Country.unique()\n\n\n'''\nSet locations and population columns\n'''\nimport pickle\nmother_path = \"/kaggle/input/covid19week2\"\nloc2pop_path = os.path.join(mother_path, \"loc2popv2.pickle\")\nwith open(loc2pop_path, 'rb') as f:\n    loc2pop = pickle.load(f)\n\n\ndef make_data():   \n    train.reset_index(drop = True, inplace = True)    \n    \n    \n    train['Location'] = train[['Subplace', 'Country']].apply(lambda x: '_'.join(x), axis=1)\n    for i in range(len(train)):\n        if i%100 == 0:\n            print(i/len(train))\n        if train.loc[i, 'Subplace'] == \"\":\n            train.loc[i, 'Location'] = train.loc[i, 'Country']\n        elif train.loc[i, 'Subplace'] == train.loc[i, 'Country']:\n            train.loc[i, 'Location'] = train.loc[i, 'Country']\n        else:\n            train.loc[i, 'Location'] = '_'.join([train.loc[i, 'Country'], train.loc[i, 'Subplace']])\n        train.loc[i, 'Population'] = loc2pop[train.loc[i, 'Location']]\n    \n    \n    test.reset_index(drop = True, inplace = True)    \n    \n    \n    test['Location'] = test[['Subplace', 'Country']].apply(lambda x: '_'.join(x), axis=1)\n    for i in range(len(test)):\n        if i%100 == 0:\n            print(i/len(test))\n        if test.loc[i, 'Subplace'] == \"\":\n            test.loc[i, 'Location'] = test.loc[i, 'Country']\n        elif test.loc[i, 'Subplace'] == test.loc[i, 'Country']:\n            test.loc[i, 'Location'] = test.loc[i, 'Country']\n        else:\n            test.loc[i, 'Location'] = '_'.join([test.loc[i, 'Country'], test.loc[i, 'Subplace']])\n        test.loc[i, 'Population'] = loc2pop[test.loc[i, 'Location']]\n    \n    with open('train_dfv2.csv', 'wb') as f:\n        pickle.dump(train, f)\n    \n    with open('test_dfv2.csv', 'wb') as f:\n        pickle.dump(test, f)   \n\n\n#make_data()\n\n\n'''\nCall dataset\n'''\nmother_path = \"/kaggle/input/covid19week2\"\ntraindf2_path = os.path.join(mother_path, \"train_dfv2.csv\")\ntestdf2_path = os.path.join(mother_path, \"test_dfv2.csv\")\n\nwith open(traindf2_path, 'rb') as f:\n    train = pickle.load(f)\n\nwith open(testdf2_path, 'rb') as f:\n    test = pickle.load(f)    \n    \n    \nLocations = train.Location.unique()\n\n'''\nCheck implausible data\n'''\ndic = {}\nfor loc in Locations:\n    indexNames = train[train.Location == loc].index\n    dic[loc] = np.array(train.loc[indexNames, \"ConfirmedCases\"])\n    tmp = np.diff(dic[loc]) < 0\n    if len(np.where(tmp == True)[0]) > 0:\n        print(loc)    \n    \ndic = {}\nfor loc in Locations:\n    indexNames = train[train.Location == loc].index\n    dic[loc] = np.array(train.loc[indexNames, \"Fatalities\"])\n    tmp = np.diff(dic[loc]) < 0\n    if len(np.where(tmp == True)[0]) > 0:\n        print(loc)\n\n'''\nMake dataset again by replacing implausible data\n'''\n\n\ndef make_data():   \n    train.reset_index(drop = True, inplace = True)    \n    \n    \n    train['Location'] = train[['Subplace', 'Country']].apply(lambda x: '_'.join(x), axis=1)\n    for i in range(len(train)):\n        if i%100 == 0:\n            print(i/len(train))\n        if train.loc[i, 'Subplace'] == \"\":\n            train.loc[i, 'Location'] = train.loc[i, 'Country']\n        elif train.loc[i, 'Subplace'] == train.loc[i, 'Country']:\n            train.loc[i, 'Location'] = train.loc[i, 'Country']\n        else:\n            train.loc[i, 'Location'] = '_'.join([train.loc[i, 'Country'], train.loc[i, 'Subplace']])\n        train.loc[i, 'Population'] = loc2pop[train.loc[i, 'Location']]\n    \n    \n    test.reset_index(drop = True, inplace = True)    \n    \n    \n    test['Location'] = test[['Subplace', 'Country']].apply(lambda x: '_'.join(x), axis=1)\n    for i in range(len(test)):\n        if i%100 == 0:\n            print(i/len(test))\n        if test.loc[i, 'Subplace'] == \"\":\n            test.loc[i, 'Location'] = test.loc[i, 'Country']\n        elif test.loc[i, 'Subplace'] == test.loc[i, 'Country']:\n            test.loc[i, 'Location'] = test.loc[i, 'Country']\n        else:\n            test.loc[i, 'Location'] = '_'.join([test.loc[i, 'Country'], test.loc[i, 'Subplace']])\n        test.loc[i, 'Population'] = loc2pop[test.loc[i, 'Location']]\n    \n   \n    for d in [6, 7, 8, 9]:\n        train.loc[(train.Location == 'Australia_Northern Territory') & (train.Date == '2020-03-{:02}'.format(d)), 'ConfirmedCases'] = 1.0   \n    for d in [31]:\n        train.loc[(train.Location == 'Australia_Queensland') & (train.Date == '2020-01-{:02}'.format(d)), 'ConfirmedCases'] = 3.0   \n    for d in [2, 3]:\n        train.loc[(train.Location == 'Australia_Queensland') & (train.Date == '2020-02-{:02}'.format(d)), 'ConfirmedCases'] = 3.0  \n    for d in [25]:\n        train.loc[(train.Location == 'Canada_Alberta') & (train.Date == '2020-03-{:02}'.format(d)), 'ConfirmedCases'] = 433.0  \n    for d in [17]:\n        train.loc[(train.Location == 'China_Guizhou') & (train.Date == '2020-03-{:02}'.format(d)), 'ConfirmedCases'] = 146.0  \n    for d in [9, 10, 11, 12, 13, 14, 15]:\n        train.loc[(train.Location == 'France_Saint Barthelemy') & (train.Date == '2020-03-{:02}'.format(d)), 'ConfirmedCases'] = 3.0  \n    for d in [17, 18, 19, 20, 21, 22, 23, 24, 25,26]:\n        train.loc[(train.Location == 'Guyana') & (train.Date == '2020-03-{:02}'.format(d)), 'ConfirmedCases'] = 5.0  \n    for d in [14]:\n        train.loc[(train.Location == 'US_Alaska') & (train.Date == '2020-03-{:02}'.format(d)), 'ConfirmedCases'] = 1.0  \n    for d in [18]:\n        train.loc[(train.Location == 'US_Nevada') & (train.Date == '2020-03-{:02}'.format(d)), 'ConfirmedCases'] = 76.0  \n    for d in [20]:\n        train.loc[(train.Location == 'US_Utah') & (train.Date == '2020-03-{:02}'.format(d)), 'ConfirmedCases'] = 108.0  \n    for d in [20, 21]:\n        train.loc[(train.Location == 'US_Virgin Islands') & (train.Date == '2020-03-{:02}'.format(d)), 'ConfirmedCases'] = 3.0  \n    for d in [29, 30]:\n        train.loc[(train.Location == 'US_Virgin Islands') & (train.Date == '2020-03-{:02}'.format(d)), 'ConfirmedCases'] = 22.0  \n    for d in [18]:\n        train.loc[(train.Location == 'US_Washington') & (train.Date == '2020-03-{:02}'.format(d)), 'ConfirmedCases'] = 1226.0  \n    for d in [21]:\n        train.loc[(train.Location == 'Canada_Quebec') & (train.Date == '2020-03-{:02}'.format(d)), 'Fatalities'] = 4.0  \n    for d in [15]:\n        train.loc[(train.Location == 'Iceland') & (train.Date == '2020-03-{:02}'.format(d)), 'Fatalities'] = 0.0  \n    for d in [20]:\n        train.loc[(train.Location == 'Iceland') & (train.Date == '2020-03-{:02}'.format(d)), 'Fatalities'] = 1.0  \n    for d in [20]:\n        train.loc[(train.Location == 'India') & (train.Date == '2020-03-{:02}'.format(d)), 'Fatalities'] = 4.0  \n    for d in [20]:\n        train.loc[(train.Location == 'Kazakhstan') & (train.Date == '2020-03-{:02}'.format(d)), 'Fatalities'] = 0.0  \n    for d in [18]:\n        train.loc[(train.Location == 'Philippines') & (train.Date == '2020-03-{:02}'.format(d)), 'Fatalities'] = 14.0  \n    for d in [18, 19, 20, 21]:\n        train.loc[(train.Location == 'Slovakia') & (train.Date == '2020-03-{:02}'.format(d)), 'Fatalities'] = 0.0  \n    for d in [24]:\n        train.loc[(train.Location == 'US_Hawaii') & (train.Date == '2020-03-{:02}'.format(d)), 'Fatalities'] = 0.0  \n    for d in [26, 27]:\n        train.loc[(train.Location == 'Serbia') & (train.Date == '2020-03-{:02}'.format(d)), 'Fatalities'] = 4.0  \n    for d in [30]:\n        train.loc[(train.Location == 'US_Virginia') & (train.Date == '2020-03-{:02}'.format(d)), 'Fatalities'] = 25.0  \n       \n    with open('train_dfv2.csv', 'wb') as f:\n        pickle.dump(train, f)\n    \n    with open('test_dfv2.csv', 'wb') as f:\n        pickle.dump(test, f)   \n\n\n#make_data()\n\n\n\n'''\nCall data\n'''\n\nmother_path = \"/kaggle/input/covid19week2\"\n\ntrain_path = os.path.join(mother_path, \"train_dfv2.csv\")\ntest_path = os.path.join(mother_path, \"test_dfv2.csv\")\nloc_path = os.path.join(mother_path, \"loc2popv2.pickle\")\n\nwith open(train_path, 'rb') as f:\n    train = pickle.load(f)\n\ntrain = train[train.Location != 'Diamond Princess']\n\nwith open(test_path, 'rb') as f:\n    test = pickle.load(f)    \n    \nwith open(loc_path, 'rb') as f:\n    loc2pop = pickle.load(f)\n    \n\nLocations = train.Location.unique()\n\n\n'''\nDesign NN\n'''\n\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Input, Dense, Concatenate\nfrom tensorflow.keras.models import Model\n\n\ntf.random.set_seed(0)\ntf.keras.backend.set_floatx('float32')\n\n      \nclass FC(Model):\n    def __init__(self, input_dims):\n        super(FC, self).__init__()\n        self.input_dims = input_dims\n        \n        self.inputs = Input(shape=self.input_dims)\n        self.dense1 = Dense(200, activation = 'softplus', name = 'dense1')\n        self.subdense1 = Dense(50, activation = 'softplus', name = 'sub_dense1')\n        self.subdense2 = Dense(1, activation = 'softplus', name = 'super_class')\n        self.subdense3 = Dense(50, activation = 'softplus', name = 'sub_dense3')\n        \n        self.dense2 = Dense(50, activation = 'softplus', name = 'dense2')\n       \n        self.logit = Dense(1, activation ='softplus', name = 'logit')\n        \n\n    def build(self):\n        x = self.inputs\n        f = self.dense1(x)\n        s  = self.subdense1(f)\n        self.lat = self.subdense2(s)\n        s = self.subdense3(self.lat)\n        f = self.dense2(f)\n        x = Concatenate()([f, s])\n        logit = self.logit(x)\n        return tf.keras.Model(inputs=self.inputs, outputs= [logit, self.lat])\n        \ndef loss_fn(X, LOGITS):\n    y, s = X\n    y_logit, s_logit  = LOGITS\n    y_loss = tf.keras.losses.MeanSquaredError()(y, y_logit)\n    s_loss = tf.keras.losses.MeanSquaredError()(s, s_logit)\n    return tf.reduce_mean(y_loss + s_loss)\n\n\n\n\nperiod = 21\n\n'''\nmake train dataset\n'''\ndic = {}\nfor loc in Locations:\n    indexNames = train[train.Location == loc].index\n    dic[loc] = np.array(train.loc[indexNames, \"ConfirmedCases\"])\n\ntrainset = []\ntestset = []\n\ndef allround(n):\n    if n != 0:\n        integer, decimal = int(np.log10(n)), np.log10(n) - int(np.log10(n))\n        return int(10**(decimal)) * 10**integer\n    else:\n        return 0\n    \nweight_pop = 10000\ndef extract_feature(sig, pop):\n    diff = np.diff(sig)\n    diff = np.array([allround(n) for n in diff]).astype(np.float32)\n    diff_ratio = weight_pop*diff/pop\n    return diff_ratio\n    \ndef target(sig, pop):\n    diff = np.diff(sig)\n    diff = np.array([allround(n) for n in diff]).astype(np.float32)\n    diff_ratio = weight_pop*diff/pop\n    return diff_ratio[0]\n    \n  \n    \nfor i, loc in enumerate(Locations):\n    if loc in ['Korea, South', 'China_Hubei']:\n        loc_df =  dic[loc].copy()\n        pop = loc2pop[loc]\n        pop = allround(pop)\n        try:\n            k = np.where(loc_df != 0)[0][0]\n        except:\n            k = len(indexNames)-1\n        try:\n            critical_point = np.where(loc_df/pop>1e-07)[0][0]\n        except:\n            critical_point = len(indexNames)-1\n        #Think here\n        if k + period < len(indexNames)-1:\n            # print(k)\n            while k+period < len(indexNames)-1:\n                tmp = loc_df[k:k+period+1]\n                diff = extract_feature(tmp[:-1], pop)\n                y_tmp = target(tmp[-2:], pop)\n                days = 1/(np.maximum(1, k - critical_point-14))\n                trainset.append([diff, days, y_tmp])\n                k+=1\n            tmp = loc_df[k:k+period+1]\n            diff = extract_feature(tmp[:-1], pop)\n            y_tmp = target(tmp[-2:], pop)\n            days = 1/(np.maximum(1, k - critical_point-14))\n            testset.append([diff, days, y_tmp])\n            # print(k, feature4)\n    \n   \n\n\ntrain_x = np.stack([trainset[i][0] for i in range(len(trainset))], axis = 0).astype(np.float32)\ntrain_s = np.stack([trainset[i][1] for i in range(len(trainset))], axis = 0).astype(np.float32)\ntrain_y = np.array([trainset[i][2] for i in range(len(trainset))]).astype(np.float32)\ntest_x = np.stack([testset[i][0] for i in range(len(testset))], axis = 0).astype(np.float32)\ntest_s = np.stack([testset[i][1] for i in range(len(testset))], axis = 0).astype(np.float32)\ntest_y = np.array([testset[i][2] for i in range(len(testset))]).astype(np.float32)\n\nn_train_samples = len(train_x)\nn_test_samples = len(test_x)\n\ntrain_ds = tf.data.Dataset.from_tensor_slices(\n    (train_x, train_y, train_s)).shuffle(n_train_samples).batch(32)\ntest_ds = tf.data.Dataset.from_tensor_slices((test_x, test_y, test_s)).batch(n_test_samples)\n\n    \nfolder_name = os.path.join(mother_path, \"mlp_p_{}\".format(period))\ntry:\n    os.mkdir(folder_name)\nexcept OSError:\n    pass  \n\n\n\nlr = 0.01\n\nmodel= FC(train_x.shape[1:]).build()    \n            \ncompute_loss = loss_fn\n\noptimizer = tf.keras.optimizers.SGD(lr=lr, decay=1e-6, momentum=0.9, nesterov=True)\ntrain_loss = tf.keras.metrics.Mean(name='train_loss')\ntest_loss = tf.keras.metrics.Mean(name='test_loss')\n\n\ndef train_step(inputs):\n    X, Y, S = inputs\n    with tf.GradientTape() as tape:\n        logits = model(X)\n        loss = compute_loss([Y, S], logits)\n    gradients = tape.gradient(loss, model.trainable_variables)\n    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n    train_loss(loss)  \n\ndef test_step(inputs):\n    X, Y, S = inputs\n    logits = model(X)\n    loss = compute_loss([Y, S], logits)\n    test_loss(loss)\n    \n    \n\ndef runs(log_freq = 1):\n    for epoch in range(5000):\n        for epoch_x, epoch_y, epoch_s in train_ds:\n            inputs = [epoch_x, epoch_y, epoch_s]\n            train_step(inputs)\n\n        for epoch_x, epoch_y, epoch_s in test_ds:\n            test_step([epoch_x, epoch_y, epoch_s])\n        template = 'epoch: {}, train_loss: {}, test_loss: {}'\n        print(template.format(epoch+1,\n                                  train_loss.result(),\n                                  test_loss.result()))    \n        \n# runs()\n\n\ntf.keras.backend.clear_session()\n\n'''\ncall confirmed forcast\n'''\ntrain_con_for_path = os.path.join(mother_path, \"train_for_dfv2.csv\")\n\nwith open(train_con_for_path, 'rb') as f:\n    train_for = pickle.load(f)\n\ndic_for = {}\nfor loc in Locations:\n    indexNames = train_for[train_for.Location == loc].index\n    dic_for[loc] = np.array(train_for.loc[indexNames, \"c\"])\n\ndic_fat = {}\nfor loc in Locations:\n    indexNames = train[train.Location == loc].index\n    dic_fat[loc] = np.array(train.loc[indexNames, \"Fatalities\"])\n\n\ntrainset = []\ntestset = []\n    \n    \n\n        \n    \nweight_con = 100\ndef extract_fat_feature(fat, con):\n    diff = np.diff(fat)\n    ratio = diff/con[:-1]\n    ratio[np.where(con[:-1] == 0)[0]] = 0\n    return weight_con*ratio\n   \ndef target_fat(fat, con):\n    diff = np.diff(fat)\n    ratio = diff/con[:-1]\n    ratio[np.where(con[:-1] == 0)[0]] = 0\n    return weight_con*ratio[0]\n    \n  \n    \nfor i, loc in enumerate(Locations):\n    if loc in ['Korea, South', 'China_Hubei']:\n        loc_df =  dic[loc].copy()\n        loc_fat_df = dic_fat[loc].copy()\n        pop = loc2pop[loc]\n        pop = allround(pop)\n        try:\n            k = np.where(loc_df != 0)[0][0]\n        except:\n            k = len(indexNames)-1\n        try:\n            critical_point = np.where(loc_df/pop>1e-07)[0][0]\n        except:\n            critical_point = len(indexNames)-1\n        #Think here\n        if k + period < len(indexNames)-1:\n            # print(k)\n            while k+period < len(indexNames)-1:\n                tmp_fat = loc_fat_df[k:k+period+1]\n                tmp = loc_df[k:k+period+1]\n                diff = extract_fat_feature(tmp_fat[:-1], tmp[:-1])\n                y_tmp = target_fat(tmp_fat[-2:],tmp[-2:])\n                days = 1/(np.maximum(1, k - critical_point-14))\n                trainset.append([diff, days, y_tmp])\n                k+=1\n            tmp_fat = loc_fat_df[k:k+period+1]\n            tmp = loc_df[k:k+period+1]\n            diff = extract_fat_feature(tmp_fat[:-1], tmp[:-1])\n            y_tmp = target_fat(tmp_fat[-2:],tmp[-2:])\n            days = 1/(np.maximum(1, k - critical_point-14))\n            testset.append([diff, days, y_tmp])\n            # print(k, feature4)\n    \n   \n\n\ntrain_x = np.stack([trainset[i][0] for i in range(len(trainset))], axis = 0).astype(np.float32)\ntrain_s = np.stack([trainset[i][1] for i in range(len(trainset))], axis = 0).astype(np.float32)\ntrain_y = np.array([trainset[i][2] for i in range(len(trainset))]).astype(np.float32)\ntest_x = np.stack([testset[i][0] for i in range(len(testset))], axis = 0).astype(np.float32)\ntest_s = np.stack([testset[i][1] for i in range(len(testset))], axis = 0).astype(np.float32)\ntest_y = np.array([testset[i][2] for i in range(len(testset))]).astype(np.float32)\n\n\nn_train_samples = len(train_x)\nn_test_samples = len(test_x)\n\n\ntrain_ds = tf.data.Dataset.from_tensor_slices(\n    (train_x, train_y, train_s)).shuffle(n_train_samples).batch(32)\ntest_ds = tf.data.Dataset.from_tensor_slices((test_x, test_y, test_s)).batch(n_test_samples)\n\n    \n\nfolder_name = os.path.join(mother_path, \"mlp_fat_p_{}\".format(period))\ntry:\n    os.mkdir(folder_name)\nexcept OSError:\n    pass  \n\n\n\nlr = 0.005\nmodel= FC(train_x.shape[1:]).build()    \n            \ncompute_loss = loss_fn\n\noptimizer = tf.keras.optimizers.SGD(lr=lr, decay=1e-6, momentum=0.9, nesterov=True)\n# optimizer = tf.keras.optimizers.Adam(lr=0.01)\ntrain_loss = tf.keras.metrics.Mean(name='train_loss')\ntest_loss = tf.keras.metrics.Mean(name='test_loss')\n\n\ndef train_fat_step(inputs):\n    X, Y, S = inputs\n    with tf.GradientTape() as tape:\n        logits = model(X)\n        loss = compute_loss([Y, S], logits)\n    gradients = tape.gradient(loss, model.trainable_variables)\n    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n    train_loss(loss)  \n\ndef test_fat_step(inputs):\n    X, Y, S = inputs\n    logits = model(X)\n    loss = compute_loss([Y, S], logits)\n    test_loss(loss)\n    \n    \n\n\ndef runs_fat(log_freq = 1):\n    for epoch in range(2):\n        for epoch_x, epoch_y, epoch_s in train_ds:\n            inputs = [epoch_x, epoch_y, epoch_s]\n            train_fat_step(inputs)\n\n        for epoch_x, epoch_y, epoch_s in test_ds:\n            test_fat_step([epoch_x, epoch_y, epoch_s])\n        template = 'epoch: {}, train_loss: {}, test_loss: {}'\n        print(template.format(epoch+1,\n                                  train_loss.result(),\n                                  test_loss.result()))    \n        \n\n        \nruns_fat()\n\nmother_sub_path = \"/kaggle/input/submission\"\ntrain_sub_path = os.path.join(mother_sub_path, \"submission.csv\")\nsubmit = pd.read_csv(train_sub_path)\n\n\nprint(submit)\nsubmit.to_csv('/kaggle/working/submission.csv',index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}