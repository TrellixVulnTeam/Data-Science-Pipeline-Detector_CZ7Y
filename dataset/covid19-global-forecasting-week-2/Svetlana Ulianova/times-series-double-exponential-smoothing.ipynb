{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\ntrain_df = pd.read_csv(\"/kaggle/input/covid19-global-forecasting-week-2/train.csv\", index_col = 'Id')\ntest_df = pd.read_csv(\"/kaggle/input/covid19-global-forecasting-week-2/test.csv\", index_col = 'ForecastId')\npd.set_option('display.max_columns', 150)\npd.set_option('display.max_rows', 150)\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove columns we do not need\ncols = ['Date','Fatalities']\ntimes_series_cntr_pr = train_df.drop(cols, axis=1).fillna('NA')\n\n#Double exponential smoothing for Confirmed cases\n\ncountries = train_df['Country_Region'].unique()\ndays_to_predict = len([x for x in train_df.Date.unique() if x not in test_df.Date.unique()]) + test_df.Date.nunique() - train_df.Date.nunique() \n\ndef double_exponential_smoothing(df, alpha, beta):\n    \"\"\"\n        series - dataset with timeseries\n        alpha - float [0.0, 1.0], smoothing parameter for level\n        beta - float [0.0, 1.0], smoothing parameter for trend\n    \"\"\"\n    result =[]\n    cntr = []\n    prov=[]\n    for c in countries:\n        for p in df.loc[df['Country_Region'] == c]['Province_State'].unique():\n            if p is not np.nan :\n                series = df.loc[(df['Province_State'] == p) & (df['Country_Region'] == c)].ConfirmedCases\n                series = list(series)\n                #result.append(series[0])\n                for n in range(1, len(series)+days_to_predict +1):\n                    if n == 1:\n                        level, trend = series[0], series[1] - series[0]\n                    if n >= len(series): # forecasting\n                        value = result[-1]\n                    else:\n                        value = series[n]\n                    last_level, level = level, alpha*value + (1-alpha)*(level+trend)\n                    trend = beta*(level-last_level) + (1-beta)*trend\n                    result.append(int(level+trend))\n                    prov.append(p)\n                    cntr.append(c)\n\n    return result, cntr, prov","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t = double_exponential_smoothing(times_series_cntr_pr,0.15, 0.9)\nfull_cc = pd.DataFrame([t[0],t[1],t[2]], index = ['ConfirmedCases','Country_Region','Province_State'], columns= np.arange(1, len(t[0]) + 1)).T\nfull_cc.loc[(full_cc['ConfirmedCases'] < 0) ,'ConfirmedCases'] = 0\nfull_cc = full_cc.sort_values(['Country_Region','ConfirmedCases','Province_State'])\nfull_cc.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove training data\n\ntotal_days = len([x for x in train_df.Date.unique() if x not in test_df.Date.unique()]) + test_df.Date.nunique() #100\nindeces = []\nfor j in range(0,294):\n    for i in range(1,len([x for x in train_df.Date.unique() if x not in test_df.Date.unique()]) + 1): # 57 + 1\n        indeces.append((i+j*total_days))\n\npred_cc = full_cc.drop(indeces).reset_index().ConfirmedCases","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_cc.shape[0] ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove columns we do not need\ncols = ['Date','ConfirmedCases']\ntimes_series_cntr_f = train_df.drop(cols, axis=1).fillna('NA')\n\n#Double exponential smoothing for Fatalities\n\ncountries = train_df['Country_Region'].unique()\ndays_to_predict = len([x for x in train_df.Date.unique() if x not in test_df.Date.unique()]) + test_df.Date.nunique() - train_df.Date.nunique() \n\ndef double_exponential_smoothing_ft(df, alpha, beta):\n    \"\"\"\n        series - dataset with timeseries\n        alpha - float [0.0, 1.0], smoothing parameter for level\n        beta - float [0.0, 1.0], smoothing parameter for trend\n    \"\"\"\n    result =[]\n    cntr = []\n    prov=[]\n    for c in countries:\n        for p in df.loc[df['Country_Region'] == c]['Province_State'].unique():\n            if p is not np.nan :\n                series = df.loc[(df['Province_State'] == p) & (df['Country_Region'] == c)].Fatalities\n                series = list(series)\n                #result.append(series[0])\n                for n in range(1, len(series)+days_to_predict +1):\n                    if n == 1:\n                        level, trend = series[0], series[1] - series[0]\n                    if n >= len(series): # forecasting\n                        value = result[-1]\n                    else:\n                        value = series[n]\n                    last_level, level = level, alpha*value + (1-alpha)*(level+trend)\n                    trend = beta*(level-last_level) + (1-beta)*trend\n                    result.append(int(level+trend))\n                    prov.append(p)\n                    cntr.append(c)\n\n    return result, cntr, prov","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f = double_exponential_smoothing_ft(times_series_cntr_f,0.15, 0.9)\nfull_f = pd.DataFrame([f[0],f[1],f[2]], index = ['Fatalities','Country_Region','Province_State'], columns= np.arange(1, len(f[0]) + 1)).T\nfull_f.loc[(full_f['Fatalities'] < 0) ,'Fatalities'] = 0\n\nfull_f = full_f.sort_values(['Country_Region','Fatalities','Province_State'])\npred_f = full_f.drop(indeces).reset_index().Fatalities","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predicted_df = pd.DataFrame([pred_cc, pred_f], index = ['ConfirmedCases','Fatalities']).T\npredicted_df.index += 1 \npredicted_df.to_csv('submission.csv', index_label = \"ForecastId\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}