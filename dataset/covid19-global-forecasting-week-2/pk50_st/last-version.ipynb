{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\n“I confirm that this is my own work, except where clearly indicated.”\n\"\"\"\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.ensemble import RandomForestClassifier\nimport pandas as pd\nimport os\nimport numpy as np \nfrom scipy.stats import norm \nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nimport plotly.express as px\nimport plotly.graph_objs as go\nfrom plotly.subplots import make_subplots\nimport plotly\nplotly.offline.init_notebook_mode() # For not show up chart error\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nfrom IPython.display import HTML\n%matplotlib inline\nfrom tqdm import tqdm\n\npd.set_option('display.max_columns', 100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Loading the data\ntrain = pd.read_csv('../input/train-3/train_3.csv')\ntest = pd.read_csv('../input/test-3/test_3.csv')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Before proceeding with the analysis, let's first make sure that there are no duplicate observations."},{"metadata":{},"cell_type":"markdown","source":"## Data at first sight"},{"metadata":{},"cell_type":"markdown","source":"Here is an excerpt of the the data description for the competition:\n\n* Values of -1 indicate that the feature was missing from the observation.\n* We are after 2 outputs from the model we are going to build: ConfirmedCases and Fatalities. These columns are labeld as inter later on\n\nOk, that's important information to get us started. Let's have a quick look at the first and last rows to confirm all of this.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Projecting the first 5 rows\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking the dimensions\ntrain.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's see if there are any duplicate observations in the training set."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dropping duplicates\ntrain.drop_duplicates()\n\n# Checking the dimensions again\ntrain.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"No duplicates. Let's make sure that test set has the dimensions that is suppose to have"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking \ntest.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We are missing 2 variables but these are the columns that we are after, so we are good. Next, let's take a first look at the data types in the training set."},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are quite a few interval data as denoted by the data type **int64** and **float64**. There are also some categorical as denoted by the dtype **object**. This implies that later on we shall create dummy variables as we will see below. But first, let's turn object variables to category to let python know that these are categorical variables. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Setting categorical variables for training set\ntrain['continent'] = train['continent'].astype('category')\ntrain['country_code'] = train['country_code'].astype('category')\ntrain['Country_Region'] = train['Country_Region'].astype('category')\n\n# Setting categorical variables for test set\ntest['continent'] = test['continent'].astype('category')\ntest['country_code'] = test['country_code'].astype('category')\ntest['Country_Region'] = test['Country_Region'].astype('category')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Management\n\nTo facilitate the data management, we'll store meta-information about the variables in a DataFrame. The method for the preparation of meta-data is mainly inspired from Bert Careman's kernel from another competition https://www.kaggle.com/bertcarremans/data-preparation-exploration. It's great how we can learn new things via participating in competitions such as Kaggle. \n\nSo all kudos for the technique of data management seen here go to Bert."},{"metadata":{},"cell_type":"markdown","source":"As for the the meta data, the structure is will be as follows:\n\n**role**: input, ID, ConfirmedCases, Fatalities\n\n**level**: nominal, interval, ordinal keep: True or False dtype: int, float, str\n\n**keep**: True or False\n\n**dtype**: int, float, str"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating the meta data\n\n\n## Something to store information\ndata = []\n\n## Creating a loop\nfor f in train.columns:\n    \n    # Defining the role for each variable\n    if f == 'ConfirmedCases':\n        role = 'ConfirmedCases'\n    elif f == 'Fatalities':\n        role = 'Fatalities'\n    elif f == 'Id':\n        role = 'Id'\n    elif f == 'Date':\n        role = 'Date'\n    else:\n        role = 'input'\n         \n    # Defining the level\n    if 'int' in f or f == 'ConfirmedCases':\n        level = 'inter'\n    elif 'int' in f or f == 'Fatalities':\n        level = 'inter'\n    elif 'int' in f or f == 'population':\n        level = 'interval'\n    elif 'cat' in f or f == 'continent':\n        level = 'nominal'\n    elif 'cat' in f or f == 'country_code':\n        level = 'nominal'\n    elif 'cat' in f or f == 'Country_Region':\n        level = 'nominal'\n    elif 'cat' in f or f == 'Id':\n        level = 'nominal'\n    elif train[f].dtype == float:\n        level = 'interval'\n    elif train[f].dtype == int:\n        level = 'interval'\n    elif train[f].dtype == 'object':\n        level = 'ordinal'\n\n        \n    # Initialize keep to True for all variables except for id\n    keep = True\n    if f == 'Id':\n        keep = False\n    \n    # Defining the data type \n    dtype = train[f].dtype\n    \n    # Creating a Dict that contains all the metadata for the variable\n    f_dict = {\n        'varname': f,\n        'role': role,\n        'level': level,\n        'keep': keep,\n        'dtype': dtype\n    }\n    data.append(f_dict)\n    \n#Saving the meta-train data\nmeta = pd.DataFrame(data, columns=['varname', 'role', 'level', 'keep', 'dtype'])\nmeta.set_index('varname', inplace=True)\n\n# Saving the meta-test data\nmeta_test = pd.DataFrame(data, columns=['varname', 'role', 'level', 'keep', 'dtype'])\nmeta_test.set_index('varname', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Below the number of variables per role and level are displayed."},{"metadata":{"trusted":true},"cell_type":"code","source":" meta","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here is also a more consise version of the data types in the data set"},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame({'count' : meta.groupby(['role', 'level'])['role'].size()}).reset_index()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's also look at the test set. This also helps verify that everything is okay with the test set as well."},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame({'count' : meta_test.groupby(['role', 'level'])['role'].size()}).reset_index()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Great. Everything seem to be working fine. Let's proceed with the analysis"},{"metadata":{},"cell_type":"markdown","source":"## Descriptive statistics"},{"metadata":{},"cell_type":"markdown","source":"### Interval Data"},{"metadata":{},"cell_type":"markdown","source":"Let's start via looking at the distribution of the interval data first."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calling the interval data\nv = meta[(meta.level == 'interval') & (meta.keep)].index\ntrain[v].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"First, it seems that the distributions of the variables in the dataset differ quite significantly. More precisely, the mean and the  standard deviation differs by large across variables. Also min and max are quite volatile as well. This suggests the need to scale the variables later on. \n\nIt is also clear that we have missing values (denoted by -1) in quite a few of the columns in the dataset. \n\nLet's see the quality of the data, how many values are missing from each variable."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Initiating an empty vector to store information\nvars_with_missing = []\n\n# Going through every column in the interval data and calculating how many missing values we have\nfor f in train.columns:\n    missings = train[train[f] == -1][f].count()\n    if missings > 0:\n        vars_with_missing.append(f)\n        missings_perc = missings/train.shape[0]\n        \n        print('Variable {} has {} records ({:.2%}) with missing values'.format(f, missings, missings_perc))\n        \nprint('In total, there are {} variables with missing values'.format(len(vars_with_missing)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have approximately 1% of observations missing from each column. Since I was the one who compiled information from various sources, I knew this already. One of the countries/regions in the given sets is the cruise ship \"Diamond Princess\" which obviously doesn't have any demographic information as a region as the rest of the countries/regions. There are also a couple other countries mainly from Africa for which the WHO did not have any available demographic information however, these observations do not constitute a large number of the observations. Hence, I decide to leave these observations untouched for now."},{"metadata":{},"cell_type":"markdown","source":"### Nominal Data\n\nLet's look at the nominal data next. Let's start with the cardinality.¶\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calling the nominal data\nv = meta[(meta.level == 'nominal') & (meta.keep)].index\ntrain[v].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Quick observations:\n\n* 188 distinct _**Country_Code**_ values\n* 173 distinct _**Country_Region**_ values\n* 5 distinct _**Continent**_ values\n* America seems to be the continent with the most observations in the dataset. More precisely, America seems to capture appx 33% of the total observations.\n\nThe fact that we have fewer **Country_Region** values steams from that we have some Provinces included in country_code and have their own distinct country code."},{"metadata":{},"cell_type":"markdown","source":"## Exploratory Data Visualization"},{"metadata":{},"cell_type":"markdown","source":"### Nominal\n\nLet's visualise the Confirmed cases and Fatalities per Country/Region. \n\nThe code for this visualization comes from the Kee's kernel found in this link https://www.kaggle.com/keedong/covid19-exponential-model2-kee\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_now = train.groupby(['Date','Country_Region']).sum().sort_values(['Country_Region','Date']).reset_index()\ndf_now['New Cases'] = df_now['ConfirmedCases'].diff()\ndf_now['New Fatalities'] = df_now['Fatalities'].diff()\ndf_now = df_now.groupby('Country_Region').apply(lambda group: group.iloc[-1:]).reset_index(drop = True)\n\n\ndf_now = df_now.sort_values('ConfirmedCases', ascending = False)\nfig = make_subplots(rows = 2, cols = 2)\nfig.add_bar(x=df_now['Country_Region'].head(10), y = df_now['ConfirmedCases'].head(10), row=1, col=1, name = 'Total cases')\n\ndf_now = df_now.sort_values('Fatalities', ascending=False)\nfig.add_bar(x=df_now['Country_Region'].head(10), y = df_now['Fatalities'].head(10), row=1, col=2, name = 'Total Fatalities')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Total cases in the US are most than any other country in the world. In fatalities however, Italy has the most followed by Spain."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calling the nominal data\nv = meta[(meta.level == 'nominal') & (meta.keep)].index\n\nfor f in v:\n    plt.figure()\n    fig, ax = plt.subplots(figsize=(20,10))\n    \n     # Calculate the Fatalities per category value\n    cat_perc = train[[f, 'Fatalities']].groupby([f],as_index=False).mean()\n    cat_perc.sort_values(by='Fatalities', ascending=False, inplace=True)\n    \n    # Bar plot\n    # Order the bars descending on target mean\n    sns.barplot(ax=ax, x=f, y='Fatalities', data=cat_perc, order=cat_perc[f])\n    plt.ylabel('Fatalities', fontsize=18)\n    plt.xlabel(f, fontsize=18)\n    plt.tick_params(axis='both', which='major', labelsize=18)\n    plt.show();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It appears that Confirmed cases in Asia top the confirmed cases anywhere else but the Fatalities are more severe in Europe. As we saw above Italy and Spain play a major role to that."},{"metadata":{},"cell_type":"markdown","source":"### Interval"},{"metadata":{"trusted":true},"cell_type":"code","source":"v = meta[(meta.level == 'interval') & (meta.keep)].index\n\nfor f in v:\n    plt.figure()\n    fig, ax = plt.subplots(figsize=(20,10))\n    \n    # Calculate the percentage of target=1 per category value\n    cat_perc = train[[f, 'ConfirmedCases']].groupby([f],as_index=False).mean()\n    cat_perc.sort_values(by='ConfirmedCases', ascending=False, inplace=True)\n    \n    # Bar plot\n    # Order the bars descending on target mean\n    sns.barplot(ax=ax, x=f, y='ConfirmedCases', data=cat_perc, order=cat_perc[f])\n    plt.ylabel('ConfirmedCases', fontsize=18)\n    plt.xlabel(f, fontsize=18)\n    plt.tick_params(axis='both', which='major', labelsize=18)\n    plt.show();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We confirm our observation from the descriptive statistics part above that interval variables' distribution vary substantially across the board. We can also see here that the data seem to be right skewed, meaning we have some high potive values. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Correlation matrix\ncorrmat = train.corr() \n  \n# Creating the plot\ncg = sns.clustermap(corrmat, cmap =\"YlGnBu\", linewidths = 0.1); \nplt.setp(cg.ax_heatmap.yaxis.get_majorticklabels(), rotation = 0) \n  \ncg ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ConfirmedCases correlation matrix \n# k : number of variables for heatmap \nk = 30\n  \ncols = corrmat.nlargest(k, 'ConfirmedCases')['ConfirmedCases'].index \n  \ncm = np.corrcoef(train[cols].values.T) \nf, ax = plt.subplots(figsize =(12, 10)) \n  \nsns.heatmap(cm, ax = ax, cmap =\"binary\", \n            linewidths = 0.1, yticklabels = cols.values,  \n                              xticklabels = cols.values) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Clearly, there are high correlations amongst some variables. Will let the algorithm further below handle this when we are dealing with feature selection. "},{"metadata":{},"cell_type":"markdown","source":" ## Feature Engineering"},{"metadata":{},"cell_type":"markdown","source":"### Creating dummy variables¶"},{"metadata":{},"cell_type":"markdown","source":"The values of the categorical variables do not represent any order or magnitude. For instance, category 2 is not twice the value of category 1. Therefore we can create dummy variables to deal with that. We drop the first dummy variable as this information can be derived from the other dummy variables generated for the categories of the original variable."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calling the nominal data\nv = meta[(meta.level == 'nominal') & (meta.keep)].index\nprint('Before dummification we have {} variables in train'.format(train.shape[1]))\ntrain = pd.get_dummies(train, columns=v, drop_first=True)\nprint('After dummification we have {} variables in train'.format(train.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Doing the same thing for test set."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calling the nominal data\nv = meta_test[(meta_test.level == 'nominal') & (meta_test.keep)].index\nprint('Before dummification we have {} variables in train'.format(test.shape[1]))\ntest = pd.get_dummies(test, columns=v, drop_first=True)\nprint('After dummification we have {} variables in train'.format(test.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next, we raise the interval variables to **polynomial degree=2** and create interactions between variables. Thanks to the get_feature_names method we can assign column names to these new variables."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calling the interval data\nv = meta[(meta.level == 'interval') & (meta.keep)].index\npoly = PolynomialFeatures(degree=2, interaction_only=False, include_bias=False)\n\n# Creating the df with the interactions\ninteractions = pd.DataFrame(data=poly.fit_transform(train[v]), columns=poly.get_feature_names(v))\ninteractions.drop(v, axis=1, inplace=True)  # Remove the original columns\n\n# Concat the interaction variables to the train data\nprint('Before creating interactions we have {} variables in train'.format(train.shape[1]))\ntrain = pd.concat([train, interactions], axis=1)\nprint('After creating interactions we have {} variables in train'.format(train.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Applying the same technique to the test set."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calling the interval data\nv = meta_test[(meta_test.level == 'interval') & (meta_test.keep)].index\npoly = PolynomialFeatures(degree=2, interaction_only=False, include_bias=False)\n\n# Creating the df with the interactions\ninteractions = pd.DataFrame(data=poly.fit_transform(test[v]), columns=poly.get_feature_names(v))\ninteractions.drop(v, axis=1, inplace=True)  # Remove the original columns\n\n# Concat the interaction variables to the train data\nprint('Before creating interactions we have {} variables in train'.format(test.shape[1]))\ntest = pd.concat([test, interactions], axis=1)\nprint('After creating interactions we have {} variables in train'.format(test.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Making sure that the dataset contains no NA values."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dropping NA values\ntrain = train.dropna()\n\n# Verifying that no N/A values exist\ntrain.isnull().sum().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"No NA values. Let's move on with Feature selection."},{"metadata":{},"cell_type":"markdown","source":"## Feature selection¶\n\nPersonally, I prefer to let the classifier algorithm chose which features to keep as i find it more robust. Here we use RandomForest to do the job. But there is one thing that we can do ourselves. That is removing features with no or a very low variance. Sklearn has a handy method to do that; VarianceThreshold"},{"metadata":{},"cell_type":"markdown","source":"### VarianceThreshold\n\nBy default it removes features with zero variance. This will be really helpful here as we will see below that there are quite a few zero-variance variables. If we choose to remove features with less than 1% variance, we remove 346 variables as seen below."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Setting the variance threshold\nselector = VarianceThreshold(threshold=.01)\nselector.fit(train.drop(['Id', 'ConfirmedCases','Fatalities','Date'], axis=1)) # Fit to train without the variables we need for submitting\n\nf = np.vectorize(lambda x : not x) # Function to toggle boolean array elements\n\n# finding variables with lower variance than threshold\nv = train.drop(['Id', 'ConfirmedCases','Fatalities','Date'], axis=1).columns[f(selector.get_support())]\nprint('{} variables have too low variance.'.format(len(v)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Training the RandomForest."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Getting the train and labels\nX_train = train.drop(['Id', 'ConfirmedCases','Fatalities','Date'], axis=1)\ny_train = train['Fatalities']\n\n# Getting the columns\nfeat_labels = X_train.columns\n\n# Fitting a Random Forest Classifier\nrf = RandomForestClassifier(n_estimators=1000, random_state=0, n_jobs=-1)\n\nrf.fit(X_train, y_train)\n\n# Getting the importances calculated from the RFC\nimportances = rf.feature_importances_\n\n# Sorting the variables by importance\nindices = np.argsort(rf.feature_importances_)[::-1]\n\n# Creating a loop that is going to show the importances per variable ranked from most important to less important\nfor f in range(20):\n    print(\"%2d) %-*s %f\" % (f + 1, 30,feat_labels[indices[f]], importances[indices[f]]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Setting the threshold for which variables to keep based on their variance contribution\nsfm = SelectFromModel(rf, threshold='median', prefit=True)\nprint('Number of features before selection: {}'.format(X_train.shape[1]))\n\n# Throwing away all the variables which fall below the threshold level specified above\nn_features = sfm.transform(X_train).shape[1]\nprint('Number of features after selection: {}'.format(n_features))\n\n# Creating a list with the selected variables\nselected_vars = list(feat_labels[sfm.get_support()])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Forming the final training set based on the feature selection \ntrain = train[selected_vars + ['ConfirmedCases','Fatalities','Date']]\n\n# Applying the selected variables to the test set as well\ntest = test[selected_vars + ['Date']]\n\ntrain_copy = train\ntest_copy = test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature normalization\n\nIn the last step prior to fitting a model, there are 2 things that remain to be done:\n1. Encode the **Date** variable\n2. Scale all numerical variables\n\nThe problem with the former is that the training set and the test set have a different number of observations and different dates in each set. Thus, if we try to apply the **OneHotEncoder** this leads to different size of columns for the two sets which wouldn't work for modelling since we want both datasets to have the same exact columns to be able to predict. A get around technique is applied to make the 2 column sets equal."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating a copy of the training and test sets\ntrain_unscaled = train_copy\ntest_unscaled = test_copy","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Before proceeding with encoding, we split the training set to training and validation sets using **Stratified Sampling** based on the population column which as we saw earlier is skewed. "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import StratifiedShuffleSplit\n\nsplit = StratifiedShuffleSplit( n_splits = 1, test_size = 0.2)\nfor train_index, test_index in split.split(train_unscaled, train_unscaled[\"population\"]):\n    train_strat = train_unscaled.loc[train_index]\n    valid_strat = train_unscaled.loc[test_index]\n\ny_train = train_strat[['ConfirmedCases','Fatalities']]\nx_train = train_strat.drop(['ConfirmedCases','Fatalities'], axis=1)\n\ny_valid = valid_strat[['ConfirmedCases','Fatalities']]\nx_valid = valid_strat.drop(['ConfirmedCases','Fatalities'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating copies of the datasets\ntrain_1 = x_train\ntest_1 = test_unscaled\nvalid1 = x_valid","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Start encoding\n\n## Assigning distinct numbers to every set\ntrain_1['train_1']=2\nvalid1['train_1']=1\ntest_1['train_1']=0\n\n## Combining the 3 sets\ncombined = pd.concat([train_1, valid1, test_1])\n\n# Getting dummies from the combined dataset\ndf = pd.get_dummies(combined['Date'])\n\n# Concatinating the dummy set with the combined set\ncombined = pd.concat([combined,df], axis = 1)\n\n## Forming the 3 sets using the distinct numbers that we initially set.\ntrain_df = combined[combined[\"train_1\"]== 2]\nvalid_df = combined[combined[\"train_1\"]== 1]\ntest_df = combined[combined[\"train_1\"]==0]\n\n# Forming the end sets\ntrain_df.drop([\"train_1\"], axis = 1, inplace = True)\nvalid_df.drop([\"train_1\"], axis = 1, inplace = True)\ntest_df.drop([\"train_1\"], axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Okay, now we are ready to encode the categorical variables (**Date**) and standardise the data. Doing both at the same time for all three sets (x_train, x_valid, test) using the handy tool called **pipeline** ."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import OneHotEncoder\n\n# Splitting between numerical and other variables\ntrain_num = train_df.select_dtypes(include=[\"number\"])\ntrain_cat = train_df.select_dtypes(exclude=[\"number\"])\n\n# Creating a pipeline\nnum_pipeline = Pipeline([\n    ('std_scaler', StandardScaler()),\n])\n\n## Getting the numerical and categorical variables\nnum_attribs = list(train_num)\ncat_attribs = list(train_cat)\n\nfull_pipeline = ColumnTransformer([\n    (\"num\", num_pipeline, num_attribs),\n])\n\n# Applying the transformation\nx_train = full_pipeline.fit_transform(train_df)\nx_valid = full_pipeline.fit_transform(valid_df)\ntest_pip = full_pipeline.fit_transform(test_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_pip.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Fitting\n\n## Model 1 - Decision Tree Regressor\n\nStarting the model fitting part wih a **DecisionTreeRegressor**. DecisionTreeRegressor is one the most powerful algorithms there are, mainly because of its ability to fit both parametric and non-parametric data. \n\nWhile fitting the model, will use 10-fold cross-validation with 3 repeats. As a performance metric, I use **Root Mean Squared Log Error** as required from the competition."},{"metadata":{"trusted":true},"cell_type":"code","source":"from numpy import absolute\nfrom numpy import mean\nfrom sklearn.metrics import mean_squared_log_error\nfrom numpy import std\nfrom sklearn.datasets import make_regression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedKFold\nfrom sklearn.metrics import  make_scorer\n\nnp.random.seed(9)\n# Creating the mean squared log error metric to let Scikit library use it in cross-validation\nscorer = make_scorer(mean_squared_log_error, greater_is_better=False)\n\n# define model\nmodel = DecisionTreeRegressor()\n\n# evaluate model\ncv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\nn_scores = cross_val_score(model, x_train, y_train, scoring= scorer, cv=cv, n_jobs=1)\n\n# summarize performance\nn_scores = absolute(n_scores)\nn_scores = np.sqrt(n_scores)\nprint('Result: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The model performed well. Let's evaluate it on the validation set."},{"metadata":{"trusted":true},"cell_type":"code","source":"np.random.seed(15)\n\n# Fitting the model on the training set\nmodel.fit(x_train,y_train)\n\n# Getting the predictions\ny_pred = model.predict(x_valid)\n\n# Calculating the loss\nloss = np.sqrt(mean_squared_log_error( y_valid, y_pred ))\nprint(loss)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model 2 - Deep Neural Network using Dropout\n\nA Neural network model can be a good option for the purposes of this competition. The algorithm is extremely useful in finding patterns that are too complex for being manually extracted and taught to recognize to the machine. So let’s fit a simple DNN with a small dropout rate.\n\nFor activation function in the hidden layers, **selu** is being used in order to avoid the **vanishing/exploding gradients** problem. For further explanation about the vanishing/exploding gradients problem feel free to see to this article https://www.semanticscholar.org/paper/Understanding-the-exploding-gradient-problem-Pascanu-Mikolov/c5145b1d15fea9340840cc8bb6f0e46e8934827f. \n\nUsing selu as activation functions also leads to self-regularization which is good. Since we are using selu activation, one of the conditions for selu to work is to use **LeCun initialization**. For further information about selu activation function feel free to read this article https://arxiv.org/pdf/1804.02763.pdf\n\nAs for the output layer, **Relu** is being used in order ensure we only get positive values. Lastly, I am adding a **Learning Scheduler**, namely **ReduceOnPlateu**, to help improve the learning rate of the algorithm when the validation loss does not improve after 5 rounds."},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow import keras\nfrom functools import partial\nfrom sklearn.model_selection import KFold\nfrom tensorflow import keras\nimport tensorflow as tf\nimport pandas as pd\n\n\n# Adding early stopping rules, checkpoint rules and Learning scheduling to improve the learning rate.\ncheckpoint_cb = keras.callbacks.ModelCheckpoint(\"keras_model_assign_2.h5\",save_best_only = True) # making sure the model is saved at every epoch and we are saving the best weights\nearly_stopping_cb = keras.callbacks.EarlyStopping( patience = 10, restore_best_weights=True) # Early stopping rule while preserving best weights\nlr_scheduler = keras.callbacks.ReduceLROnPlateau(factor = 0.5, patience=5) # reduces the learning rate by 0.5 when the validation score doesn't improve for 5 rounds\n\noptimizer = keras.optimizers.SGD(lr= 0.001, momentum = 0.9, nesterov=True) #adding an optimization parameter to improve learning rate\n\n\n####################\n###################\nseed = 7\ncvscores = []\n\n# Converting the train set to array for indexing\nX = np.array(x_train)\nY = np.array(y_train)\n\nX_valid = np.array(x_valid)\nY_valid = np.array(y_valid)\n\nnp.random.seed(seed)\n# define 5-fold cross validation test harness\nkfold = KFold(5, True, 1)\n\nfor train, test in kfold.split(X, Y):\n  # create model\n    model2 = tf.keras.models.Sequential([\n    keras.layers.Flatten(input_shape = x_train.shape[1:]),\n    keras.layers.Dense(100, activation = \"selu\", kernel_initializer = \"lecun_normal\"),\n    keras.layers.Dense(100, activation = \"selu\", kernel_initializer = \"lecun_normal\"),\n    keras.layers.Dense(100,activation = \"selu\", kernel_initializer = \"lecun_normal\"),\n    keras.layers.Dense(50, activation = \"selu\", kernel_initializer = \"lecun_normal\"),\n    keras.layers.Dropout(rate = 0.1),\n    keras.layers.Dense(2, activation = \"relu\", kernel_initializer = \"he_normal\")\n    ])\n    \n    # Compiling the model\n    model2.compile(loss=\"mean_squared_logarithmic_error\",\n             optimizer = optimizer)\n    \n    # Fitting the model\n    history_2 = model2.fit(X[train], Y[train],epochs=200, verbose=0,\n                        validation_data = (X_valid, Y_valid),\n                        callbacks = [checkpoint_cb, early_stopping_cb, lr_scheduler])\n    \n    # Evaluating the model\n    y_pred = model2.predict(X[test])\n    loss = np.sqrt(mean_squared_log_error( Y[test], y_pred ))\n    cvscores.append(loss)\n    \nprint(\"Scores:\",cvscores)\nprint(\"Mean:\",np.mean(cvscores))\nprint(\"Standard Deviation:\",np.std(cvscores))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looking at the scores from every iteration, it appears that as the trainng set is shuffled in each iteration, the performance of the model deteriorates."},{"metadata":{},"cell_type":"markdown","source":"# Model 3 - DNN using MC Dropout\n\nInstead of using Dropout, let's try and use Monte Carlo Dropout. MC Dropout attempts to mitigate the problem of representing model uncertainty without sacrificing either computational complexity or test accuracy so let's give it a try."},{"metadata":{"trusted":true},"cell_type":"code","source":"checkpoint_cb_2 = keras.callbacks.ModelCheckpoint(\"keras_model2_assign_2.h5\",\n                                               save_best_only = True) # making sure the model is saved at every epoch\n\n\n# defining Monte Carlo Dropout layers\nclass MCDropout(keras.layers.Dropout):\n    def call (self,inputs):\n        return super().call(inputs, training = True)\n\n################\n################\nseed = 7\ncvscores = []\nnp.random.seed(seed)\n\n# define 5-fold cross validation test harness\nkfold = KFold(5, True, 1)\n\n# Initiating K-Fold Cross-Validation while fitting the model\nfor train, test in kfold.split(X, Y):\n  # create model\n    model3 = tf.keras.models.Sequential([\n    keras.layers.Flatten(input_shape = x_train.shape[1:]),\n    keras.layers.Dense(100, activation = \"selu\", kernel_initializer = \"lecun_normal\"),\n    keras.layers.Dense(100, activation = \"selu\", kernel_initializer = \"lecun_normal\"),\n    keras.layers.Dense(100,activation = \"selu\", kernel_initializer = \"lecun_normal\"),\n    keras.layers.Dense(50, activation = \"selu\", kernel_initializer = \"lecun_normal\"),\n    MCDropout(rate =0.1),\n    keras.layers.Dense(2, activation = \"relu\", kernel_initializer = \"he_normal\")\n    ])\n    \n    # Compiling the model\n    model3.compile(loss=\"mean_squared_logarithmic_error\",\n             optimizer = optimizer)\n    \n    # Fitting the model\n    history_3 = model3.fit(X[train], Y[train],epochs=200,verbose=0, \n                        validation_data = (X_valid, Y_valid),\n                        callbacks = [early_stopping_cb,checkpoint_cb_2, lr_scheduler])\n    \n    # Evaluating the model\n    y_pred = model3.predict(X[test])\n    loss = np.sqrt(mean_squared_log_error( Y[test], y_pred ))\n    cvscores.append(loss)\n    \nprint(\"Scores:\",cvscores)\nprint(\"Mean:\",np.mean(cvscores))\nprint(\"Standard Deviation:\",np.std(cvscores))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Adding a MCDropout layer improved the model's performance however the same problem with the first model persists. Let's fit a model that requires less tuning of hypermarameters and see how it performs."},{"metadata":{},"cell_type":"markdown","source":"# Model 4 - DNN using Batch Normalisation\n\nBatch Normalization makes the networks much less sensitive to the weight initialization. The drawback is that we are adding extra computation at each layer which makes the model slower to converge and predict. Also, I choose to retain the Monte Carlo Dropout layer since it appears to improve the performance."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating a new checkpoint for a new model\ncheckpoint_3_cb = keras.callbacks.ModelCheckpoint(\"keras_model3_assign_2.h5\",\n                                               save_best_only = True) # making sure the model is saved at every epoch\n\n# Setting the model\n################\n################\nseed = 7\ncvscores = []\nnp.random.seed(seed)\n\n# define 5-fold cross validation test harness\nkfold = KFold(5, True, 1)\n\n# Initiating K-Fold Cross-Validation while fitting the model\nfor train, test in kfold.split(X, Y):\n  # create model\n    model_4 = keras.models.Sequential([\n    keras.layers.Flatten(input_shape = x_train.shape[1:]),\n    keras.layers.BatchNormalization(),\n    keras.layers.Dense(100, activation = \"selu\", kernel_initializer = \"lecun_normal\"),\n    keras.layers.BatchNormalization(),\n    keras.layers.Activation(\"selu\"),\n    keras.layers.Dense(100, activation = \"selu\", kernel_initializer = \"lecun_normal\"),\n    keras.layers.BatchNormalization(),\n    keras.layers.Activation(\"selu\"),\n    keras.layers.Dense(100, activation = \"selu\", kernel_initializer = \"lecun_normal\"),\n    keras.layers.BatchNormalization(),\n    keras.layers.Activation(\"selu\"),\n    keras.layers.Dense(50, activation = \"selu\", kernel_initializer = \"lecun_normal\"),\n    keras.layers.BatchNormalization(),\n    keras.layers.Activation(\"selu\"),\n    MCDropout(rate =0.15),\n    keras.layers.Dense(2, activation = \"relu\", kernel_initializer = \"he_normal\")\n])\n\n    # Compiling the model\n    model_4.compile(loss=\"mean_squared_logarithmic_error\",\n             optimizer = optimizer)\n    \n    # Fit the model\n    history_4 = model_4.fit(X[train], Y[train],epochs=200, verbose=0,\n                        validation_data = (X_valid, Y_valid),\n                        callbacks = [early_stopping_cb, checkpoint_3_cb, lr_scheduler])\n    \n    # evaluate the model\n    y_pred = model_4.predict(X[test])\n    loss = np.sqrt(mean_squared_log_error( Y[test], y_pred ))\n    cvscores.append(loss)\n    \nprint(\"Scores:\",cvscores)\nprint(\"Mean:\",np.mean(cvscores))\nprint(\"Standard Deviation:\",np.std(cvscores))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Fine Tuning\n\nThe better two models in terms of performance on the validation set are the **DecisionTreeClassifier** and the **DNN with Dropout layer** (model2). To decide which one to use, let's optimize the Decision Tree Classifier using **RandomizedSearchCV** and test them both again on the validation set. \n\nThrough trial and error, I found that **sample_split** above 50 leads to overfit so I limit this variable to 50. I also set **max_depth** to 90:155 again because through trial and error values between 90:155 lead to smaller generalization error."},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.stats import reciprocal \nfrom sklearn.model_selection import RandomizedSearchCV\n\n\nsamples_split = range(25,50)\nmax_depth = range(90,155)\n\nparameters={'min_samples_split': samples_split,\n            'max_depth': max_depth}\nseed = 7\nrnd_search_cv = RandomizedSearchCV(model, \n                                   parameters, \n                                   n_iter = 100, \n                                   cv=3, \n                                   scoring = scorer, \n                                   random_state=0)\n\nrnd_search_cv.fit(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's see which values were trialled during Randomized Search CV."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Collecting the results\ncvres = rnd_search_cv.cv_results_\n\n# Creating a loop that goes through the values tested and their associated scores\nfor mean_score, params in zip(-cvres[\"mean_test_score\"], cvres[\"params\"]):\n    print(mean_score,params)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Best parameters from optimization\nrnd_search_cv.best_params_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's compare."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Forming the final model\noptimized_dtc = rnd_search_cv.best_estimator_\n\n# Getting predictions\nopti_dtc_final_predictions = optimized_dtc.predict(x_valid)\nmodel2_final_predictions = model2.predict(x_valid)\n\ndtc_loss = np.sqrt(mean_squared_log_error( y_valid, opti_dtc_final_predictions ))\nmodel2_loss = np.sqrt(mean_squared_log_error( y_valid, model2_final_predictions ))\n    \nprint(\"DTC Score:\",dtc_loss)\nprint(\"DNN Score:\",model2_loss)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The DNN model performs better, hence this will be the final model."},{"metadata":{},"cell_type":"markdown","source":"## Output\n\nAll done, creating the output file."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Getting predictions\nfinal_predictions = model2_loss.predict(test_pip)\n\n# Creating the final submission file\nsub = pd.DataFrame(final_predictions)\nsub[\"ConfirmedCases\"] = sub[0].astype(int)\nsub[\"Fatalities\"] = sub[1].astype(int)\ncols = [0,1]\nsub.drop(sub.columns[cols],axis=1,inplace=True)\nsub.round() \nsub['ForecastId'] = range(1, len(sub) + 1)\nsub = sub[['ForecastId', 'ConfirmedCases','Fatalities']]\nsub.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"}},"nbformat":4,"nbformat_minor":4}