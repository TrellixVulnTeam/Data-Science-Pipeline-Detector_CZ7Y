{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# Loading submission file generated below as time was up\n# and I had not enough time to run this before competition\n# ended\nimport pandas as pd\nsubmission = pd.read_csv('/kaggle/input/resultscov19week2/submission (2).csv')\nsubmission","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv('submission.csv', index=False)\n\n# Everything below this generates the submission file above.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Imports\n# import pandas as pd\n# import numpy as np\n# import matplotlib.pyplot as plt\n# %matplotlib inline\n# plt.style.use('fivethirtyeight')\n# import warnings\n# warnings.filterwarnings(\"ignore\")\n# from tensorflow.keras.models import Sequential\n# from tensorflow.keras.layers import Dense, LSTM\n# from tensorflow.keras.callbacks import EarlyStopping\n# from lightgbm import LGBMRegressor\n# import time\n# from sklearn.model_selection import cross_val_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Loading data\n# df_train = pd.read_csv('/kaggle/input/covid19-global-forecasting-week-2/train.csv')\n# df_test = pd.read_csv('/kaggle/input/covid19-global-forecasting-week-2/test.csv')\n# submission = pd.read_csv('/kaggle/input/covid19-global-forecasting-week-2/submission.csv')\n# df_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Making Date become timestamp\n# from datetime import datetime\n# df_train['Date'] = pd.to_datetime(df_train['Date'])\n# df_test['Date'] = pd.to_datetime(df_test['Date'])\n\n# df_train['Date'] = df_train['Date'].apply(lambda s: time.mktime(s.timetuple()))\n# df_test['Date'] = df_test['Date'].apply(lambda s: time.mktime(s.timetuple()))\n\n# min_timestamp = np.min(df_train['Date'])\n# df_train['Date'] = df_train['Date'].apply(lambda s: (s - min_timestamp) / 86400.0)\n# df_test['Date'] = df_test['Date'].apply(lambda s: (s - min_timestamp) / 86400.0)\n# df_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# df_intersection = df_test[df_test['Date'] <= np.max(df_train['Date'])]\n# df_intersection","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Following the idea at\n# # https://www.kaggle.com/ranjithks/25-lines-of-code-results-better-score#Fill-NaN-from-State-feature\n# # Filling NaN states with the Country\n\n# EMPTY_VAL = \"EMPTY_VAL\"\n\n# def fillState(state, country):\n#     if state == EMPTY_VAL: return country\n#     return state\n\n# def replaceGeorgiaState (state, country):\n#     if (state == 'Georgia') and (country == 'US'):\n#         return 'Georgia_State'\n#     else:\n#         return state\n\n# df_train['Province_State'].fillna(EMPTY_VAL, inplace=True)\n# df_train['Province_State'] = df_train.loc[:, ['Province_State', 'Country_Region']].apply(lambda x : fillState(x['Province_State'], x['Country_Region']), axis=1)\n# df_train['Province_State'] = df_train.loc[:, ['Province_State', 'Country_Region']].apply(lambda x : replaceGeorgiaState(x['Province_State'], x['Country_Region']), axis=1)\n\n# df_test['Province_State'].fillna(EMPTY_VAL, inplace=True)\n# df_test['Province_State'] = df_test.loc[:, ['Province_State', 'Country_Region']].apply(lambda x : fillState(x['Province_State'], x['Country_Region']), axis=1)\n# df_test['Province_State'] = df_test.loc[:, ['Province_State', 'Country_Region']].apply(lambda x : replaceGeorgiaState(x['Province_State'], x['Country_Region']), axis=1)\n\n# df_intersection['Province_State'].fillna(EMPTY_VAL, inplace=True)\n# df_intersection['Province_State'] = df_intersection.loc[:, ['Province_State', 'Country_Region']].apply(lambda x : fillState(x['Province_State'], x['Country_Region']), axis=1)\n# df_intersection['Province_State'] = df_intersection.loc[:, ['Province_State', 'Country_Region']].apply(lambda x : replaceGeorgiaState(x['Province_State'], x['Country_Region']), axis=1)\n\n# df_train[df_train['Province_State'] == 'Georgia_State']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from sklearn.preprocessing import LabelEncoder\n# le = LabelEncoder()\n# df_train['country_code'] = le.fit_transform(df_train['Country_Region'])\n# df_test ['country_code'] = le.transform(df_test['Country_Region'])\n# df_intersection ['country_code'] = le.transform(df_intersection['Country_Region'])\n\n# le = LabelEncoder()\n# df_train['province_code'] = le.fit_transform(df_train['Province_State'])\n# df_test ['province_code'] = le.transform(df_test['Province_State'])\n# df_intersection ['province_code'] = le.transform(df_intersection['Province_State'])\n\n# df_train[df_train['Province_State'] == 'Georgia_State']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Adding validation data into the Intersection DF\n# states = sorted(set(df_intersection['Province_State']))\n# df_intersection['ConfirmedCases'] = float('NaN')\n# df_intersection['Fatalities'] = float('NaN')\n\n# for state in states:\n#     dates = sorted(set(df_intersection[df_intersection['Province_State'] == state]['Date']))\n#     min_date = np.min(dates)\n#     max_date = np.max(dates)\n#     idx = df_intersection[df_intersection['Province_State'] == state].index\n#     values = df_train[(df_train['Province_State'] == state) & (df_train['Date'] >= min_date) & (df_train['Date'] <= max_date)][['ConfirmedCases', 'Fatalities']].values\n#     values = pd.DataFrame(values, index = list(idx), columns=['ConfirmedCases', 'Fatalities'])\n#     df_intersection['ConfirmedCases'].loc[idx] = values['ConfirmedCases']\n#     df_intersection['Fatalities'].loc[idx] = values['Fatalities']\n# df_intersection","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Check if any Province_State value on test dataset isn't on train dataset\n# # If nothing prints, everything is okay\n# for a in set(df_test['Province_State']):\n#     if a not in set(df_train['Province_State']):\n#         print (a)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from sklearn.pipeline import make_pipeline\n# from sklearn.preprocessing import PolynomialFeatures, LabelEncoder\n# from sklearn.linear_model import LinearRegression, BayesianRidge, Lasso\n# from sklearn.metrics import mean_squared_log_error\n\n# from sklearn.ensemble.weight_boosting import AdaBoostRegressor\n# from sklearn.linear_model.base import LinearRegression\n# from sklearn.linear_model.passive_aggressive import PassiveAggressiveRegressor\n# from sklearn.linear_model.theil_sen import TheilSenRegressor\n\n# def handle_predictions (predictions, lowest = 0):\n#     #predictions = np.round(predictions, 0)\n#     # Predictions can't be negative\n#     predictions[predictions < 0] = 0\n#     # Predictions can't decrease from greatest value on train dataset\n#     predictions[predictions < lowest] = lowest\n#     # Predictions can't decrease over time\n#     for i in range(1, len(predictions)):\n#         if predictions[i] < predictions[i - 1]:\n#             predictions[i] = predictions[i - 1]\n#     #return predictions.astype(int)\n#     return predictions\n\n# def fillSubmission (state, column, values,):\n#     idx = df_test[df_test['Province_State'] == state].index\n#     values = pd.DataFrame(np.array(values), index = list(idx), columns=[column])\n#     submission[column].loc[idx] = values[column]\n#     return submission\n\n# def avg_rmsle():\n#     idx = df_intersection.index\n#     my_sub = df_test.loc[idx][['ConfirmedCases', 'Fatalities']]\n#     cases_pred = my_sub['ConfirmedCases'].values\n#     fatal_pred = my_sub['Fatalities'].values\n#     cases_targ = df_intersection.loc[idx]['ConfirmedCases'].values\n#     fatal_targ = df_intersection.loc[idx]['Fatalities'].values\n#     cases_score = np.sqrt(mean_squared_log_error( cases_targ, cases_pred ))\n#     fatal_score = np.sqrt(mean_squared_log_error( fatal_targ, fatal_pred ))\n#     score = (cases_score + fatal_score)/2\n#     return score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# def checkState (state):\n#     idx = df_test[df_test['Province_State'] == state].index\n#     return df_test.loc[idx]\n\n# def plotStatus (states):\n#     if type(states) == list:\n#         for state in states:\n#             plt.figure(figsize=(14,8))\n#             plt.title('COVID-19 cases on {}'.format(states))\n#             df = df_train[df_train['Province_State'] == state]\n#             test = df_test[df_test['Province_State'] == state]\n#             intersection = df_intersection[df_intersection['Province_State'] == state]\n#             idx = df_test[df_test['Province_State'] == state].index\n#             legend = []\n#             plt.xlabel('#Days since dataset')\n#             plt.ylabel('Number')\n#             plt.plot(df['Date'], df['ConfirmedCases'])\n#             plt.plot(test['Date'], test['ConfirmedCases'])\n#             #plt.plot(intersection['Date'], intersection['ConfirmedCases'])\n#             legend.append('{} confirmed cases'.format(state))\n#             legend.append('{} predicted cases'.format(state))\n#             #legend.append('{} actual cases'.format(state))\n#             plt.legend(legend)\n#             plt.show()\n#             legend = []\n#             plt.figure(figsize=(14,8))\n#             plt.title('COVID-19 fatalities on {}'.format(states))\n#             plt.xlabel('#Days since dataset')\n#             plt.ylabel('Number')\n#             plt.plot(df['Date'], df['Fatalities'])\n#             plt.plot(test['Date'], test['Fatalities'])\n#             #plt.plot(intersection['Date'], intersection['Fatalities'])\n#             legend.append('{} fatalities'.format(state))\n#             legend.append('{} predicted fatalities'.format(state))\n#             #legend.append('{} actual fatalities'.format(state))\n#             plt.show()\n#     else:\n#         state = states\n#         df = df_train[df_train['Province_State'] == state]\n#         plt.figure(figsize=(14,8))\n#         plt.xlabel('#Days since dataset')\n#         plt.ylabel('Number')\n#         plt.plot(df['Date'], df['ConfirmedCases'])\n#         plt.plot(df['Date'], df['Fatalities'])\n#         plt.legend(['Confirmed cases', 'Fatalities'])\n#     plt.show()\n\n# raw_train = pd.read_csv('/kaggle/input/covid19-global-forecasting-week-2/train.csv')\n# raw_test  = pd.read_csv('/kaggle/input/covid19-global-forecasting-week-2/test.csv')\n# raw_train['Date'] = pd.to_datetime(raw_train['Date'], infer_datetime_format=True)\n# raw_test['Date']  = pd.to_datetime(raw_test['Date'], infer_datetime_format=True)\n\n# def rmsle (state):\n#     idx = df_intersection[df_intersection['Province_State'] == state].index\n#     my_sub = df_test.loc[idx][['ConfirmedCases', 'Fatalities']]\n#     cases_pred = my_sub['ConfirmedCases'].values\n#     fatal_pred = my_sub['Fatalities'].values\n#     cases_targ = df_intersection.loc[idx]['ConfirmedCases'].values\n#     fatal_targ = df_intersection.loc[idx]['Fatalities'].values\n#     cases = np.sqrt(mean_squared_log_error( cases_targ, cases_pred ))\n#     fatal = np.sqrt(mean_squared_log_error( fatal_targ, fatal_pred ))\n#     return cases, fatal","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from pandas.plotting import autocorrelation_plot\n\n# plt.figure(figsize=(14,8))\n# autocorrelation_plot(df_train[ df_train['Province_State'] == 'Brazil' ]['ConfirmedCases'])\n# plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import time\n# from tqdm import tqdm\n\n# start_time = time.time()\n\n# lag_range = np.arange(1,8,1)\n\n# with tqdm(total = len(list(states))) as pbar:\n#     for state in states:\n#         for d in df_train['Date'].drop_duplicates():\n#             mask = (df_train['Date'] == d) & (df_train['Province_State'] == state)\n#             for lag in lag_range:\n#                 mask_org = (df_train['Date'] == (d - lag)) & (df_train['Province_State'] == state)\n#                 try:\n#                     df_train.loc[mask, 'ConfirmedCases_' + str(lag)] = df_train.loc[mask_org, 'ConfirmedCases'].values\n#                 except:\n#                     df_train.loc[mask, 'ConfirmedCases_' + str(lag)] = 0\n#                 try:\n#                     df_train.loc[mask, 'Fatalities_' + str(lag)] = df_train.loc[mask_org, 'Fatalities'].values\n#                 except:\n#                     df_train.loc[mask, 'Fatalities_' + str(lag)] = 0\n#         pbar.update(1)\n# print('Time spent for building features is {} minutes'.format(round((time.time()-start_time)/60,1)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from sklearn.preprocessing import MinMaxScaler, StandardScaler\n# from tensorflow.keras.layers import Dropout\n# import keras.backend as K\n\n# def root_mean_squared_log_error(y_true, y_pred):\n#     return K.sqrt(K.mean(K.square(K.log(y_pred + 1) - K.log(y_true + 1)))) \n\n# cases_columns = ['country_code', 'province_code', 'Date', 'ConfirmedCases_1', 'ConfirmedCases_2', 'ConfirmedCases_3', 'ConfirmedCases_4', 'ConfirmedCases_5', 'ConfirmedCases_6', 'ConfirmedCases_7']\n# fatal_columns = set(['country_code', 'province_code', 'Date', 'Fatalities_1', 'Fatalities_2', 'Fatalities_3', 'Fatalities_4', 'Fatalities_5', 'Fatalities_6', 'Fatalities_7'] + cases_columns)\n\n# X_cases_scaler = StandardScaler()\n# X_fatal_scaler = StandardScaler()\n# # Maybe by scaling y from 0 to something lower than 1,\n# # it makes it possible for predicting things greater\n# # than the greatest value on y more accurately.\n# y_cases_scaler = MinMaxScaler(feature_range=(0, .7)) # The max number of confirmed cases on a single state/province has already reached next to its' greatest\n# y_fatal_scaler = MinMaxScaler(feature_range=(0, .3)) # The number of fatalities maybe not\n\n# # Setting patience for training\n# es = EarlyStopping(monitor='loss', mode='min', verbose=2, patience=25)\n\n# # Getting datasets\n# X_cases = df_train[cases_columns].values\n# X_fatal = df_train[fatal_columns].values\n# y_cases = df_train['ConfirmedCases'].values.reshape(-1, 1)\n# y_fatal = df_train['Fatalities'].values.reshape(-1, 1)\n\n# # Scaling datasets\n# X_cases = X_cases_scaler.fit_transform(X_cases)\n# X_fatal = X_fatal_scaler.fit_transform(X_fatal)\n# y_cases = y_cases_scaler.fit_transform(y_cases)\n# y_fatal = y_fatal_scaler.fit_transform(y_fatal)\n\n# # Fixing shapes\n# X_cases = X_cases.reshape(X_cases.shape[0], 1, X_cases.shape[1])\n# X_fatal = X_fatal.reshape(X_fatal.shape[0], 1, X_fatal.shape[1])\n\n# # # 0.029632697546336188\n# # # Average = 0.1760559035529777, 0.21325053819565856\n# # # Modeling for cases\n# # model_cases = Sequential()\n# # model_cases.add(LSTM(60, return_sequences=True, input_shape=(1, len(cases_columns)), activation='softplus'))\n# # model_cases.add(LSTM(60, activation='relu'))\n# # model_cases.add(Dropout(0.2))\n# # model_cases.add(Dense(50, activation='sigmoid'))\n# # model_cases.add(Dense(1, activation='sigmoid'))\n# # model_cases.compile(loss=root_mean_squared_log_error, optimizer='adam')\n\n# # # Modeling for fatal\n# # model_fatal = Sequential()\n# # model_fatal.add(LSTM(60, return_sequences=True, input_shape=(1, len(fatal_columns)), activation='softplus'))\n# # model_fatal.add(LSTM(60, activation='relu'))\n# # model_fatal.add(Dropout(0.2))\n# # model_fatal.add(Dense(50, activation='sigmoid'))\n# # model_fatal.add(Dense(1, activation='sigmoid'))\n# # model_fatal.compile(loss=root_mean_squared_log_error, optimizer='adam')\n\n# # # 0.03529411203400082\n# # # Average = 0.1711087062044468, 0.3030005671812312\n# # # Modeling for cases\n# # model_cases = Sequential()\n# # model_cases.add(LSTM(60, return_sequences=True, input_shape=(1, len(cases_columns)), activation='sigmoid'))\n# # model_cases.add(Dropout(0.2))\n# # model_cases.add(LSTM(60, activation='sigmoid'))\n# # model_cases.add(Dropout(0.2))\n# # model_cases.add(Dense(50, activation='sigmoid'))\n# # model_cases.add(Dense(1, activation='sigmoid'))\n# # model_cases.compile(loss=root_mean_squared_log_error, optimizer='adam')\n\n# # # Modeling for fatal\n# # model_fatal = Sequential()\n# # model_fatal.add(LSTM(60, return_sequences=True, input_shape=(1, len(fatal_columns)), activation='softplus'))\n# # model_fatal.add(LSTM(30, activation='relu'))\n# # model_fatal.add(Dropout(0.2))\n# # model_fatal.add(Dense(10, activation='sigmoid'))\n# # model_fatal.add(Dense(1, activation='sigmoid'))\n# # model_fatal.compile(loss=root_mean_squared_log_error, optimizer='adam')\n\n# # # 0.03700217774421334\n# # # Average = 0.17584334824320144, 0.2999503126896585\n# # # Modeling for cases\n# # model_cases = Sequential()\n# # model_cases.add(LSTM(750, return_sequences=False, input_shape=(1, len(cases_columns)), activation='softplus'))\n# # model_cases.add(Dropout(0.2))\n# # model_cases.add(Dense(1, activation='sigmoid'))\n# # model_cases.compile(loss=root_mean_squared_log_error, optimizer='adam')\n\n# # # Modeling for fatal\n# # model_fatal = Sequential()\n# # model_fatal.add(LSTM(60, return_sequences=True, input_shape=(1, len(fatal_columns)), activation='relu'))\n# # model_fatal.add(Dropout(0.1))\n# # model_fatal.add(LSTM(60, activation='relu'))\n# # model_fatal.add(Dropout(0.1))\n# # model_fatal.add(Dense(10, activation='sigmoid'))\n# # model_fatal.add(Dense(1, activation='sigmoid'))\n# # model_fatal.compile(loss=root_mean_squared_log_error, optimizer='adam')\n\n# # # 0.027998083663296477\n# # # Average = 0.1431641824933455, 0.2134293608876186\n# # # Modeling for cases\n# # model_cases = Sequential()\n# # model_cases.add(LSTM(750, return_sequences=False, input_shape=(1, len(cases_columns)), activation='softplus'))\n# # model_cases.add(Dropout(0.2))\n# # model_cases.add(Dense(20, activation='sigmoid'))\n# # model_cases.add(Dense(1, activation='sigmoid'))\n# # model_cases.compile(loss=root_mean_squared_log_error, optimizer='adam')\n\n# # # Modeling for fatal\n# # model_cases = Sequential()\n# # model_cases.add(LSTM(60, return_sequences=True, input_shape=(1, len(cases_columns)), activation='sigmoid'))\n# # model_cases.add(Dropout(0.2))\n# # model_cases.add(LSTM(60, activation='sigmoid'))\n# # model_cases.add(Dropout(0.2))\n# # model_cases.add(Dense(50, activation='sigmoid'))\n# # model_cases.add(Dense(1, activation='softplus'))\n# # model_cases.compile(loss=root_mean_squared_log_error, optimizer='adam')\n\n# # # 0.07864703262912029\n# # # Average = 0.3382936857372058, 0.5043676839090568\n# # # Modeling for cases\n# # model_cases = Sequential()\n# # model_cases.add(LSTM(30, activation='softplus'))\n# # model_cases.add(Dropout(0.2))\n# # model_cases.add(Dense(15, activation='relu'))\n# # model_cases.add(Dense(3, activation='sigmoid'))\n# # model_cases.add(Dense(1, activation='sigmoid'))\n# # model_cases.compile(loss=root_mean_squared_log_error, optimizer='adam')\n\n# # # Modeling for fatal\n# # model_fatal = Sequential()\n# # model_fatal.add(LSTM(30, return_sequences=True, input_shape=(1, len(fatal_columns)), activation='softplus'))\n# # model_fatal.add(LSTM(20, return_sequences=True, activation='relu'))\n# # model_fatal.add(LSTM(10, activation='sigmoid'))\n# # model_fatal.add(Dropout(0.2))\n# # model_fatal.add(Dense(5, activation='sigmoid'))\n# # model_fatal.add(Dense(1, activation='sigmoid'))\n# # model_fatal.compile(loss=root_mean_squared_log_error, optimizer='adam')\n\n# # selu seems pretty OK\n# # softplus goes low on losses, pessimistic\n# # relu goes lower than selu on losses, but TOO SAFE.No rescaling solved. TOO SAFE\n# # tanh seems safe too\n# # sigmoid performs flawlessly, but on Italy it becomes too safe.\n# # linear can go to infinite\n\n# # 0.04121125879913207\n# # Average = 0.142672518218051, 0.3815508620346949\n# # Modeling for cases\n# model_cases = Sequential()\n# model_cases.add(LSTM(60, return_sequences=True, input_shape=(1, len(cases_columns)), activation='sigmoid'))\n# model_cases.add(Dropout(0.2))\n# model_cases.add(LSTM(60, activation='sigmoid'))\n# model_cases.add(Dropout(0.2))\n# model_cases.add(Dense(50, activation='sigmoid'))\n# model_cases.add(Dense(1, activation='softplus'))\n# model_cases.compile(loss=root_mean_squared_log_error, optimizer='adam')\n\n# # Modeling for fatal\n# model_fatal = Sequential()\n# model_fatal.add(LSTM(60, return_sequences=True, input_shape=(1, len(fatal_columns)), activation='sigmoid'))\n# model_fatal.add(Dropout(0.2))\n# model_fatal.add(LSTM(60, activation='sigmoid'))\n# model_fatal.add(Dropout(0.2))\n# model_fatal.add(Dense(50, activation='sigmoid'))\n# model_fatal.add(Dense(1, activation='softplus'))\n# model_fatal.compile(loss=root_mean_squared_log_error, optimizer='adam')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Fitting cases\n# model_cases.fit(X_cases, y_cases, batch_size=128, epochs=5000, callbacks=[es])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Fitting fatal\n# model_fatal.fit(X_fatal, y_fatal, batch_size=128, epochs=5000, callbacks=[es])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# input_cols = list(set(cases_columns + list(fatal_columns)))\n# output_cols = ['ConfirmedCases', 'Fatalities']\n# adj_input_cols = [e for e in input_cols if e not in ('province_code', 'country_code', 'Date')]\n# lag_range = np.arange(1,8,1)\n# pred_dt_range = range(int(df_test['Date'].min()), int(df_test['Date'].max()) + 1)\n\n# # Making a random set of 10 states in order to validate models\n# import random\n# random_validation_set = ['Brazil', 'New York', 'Afghanistan', 'Zhejiang', 'Italy']#random.sample(states, 10)\n# print (\"The random validation set is {}\".format(random_validation_set))\n\n# # Filling data for intersection\n# for col in (adj_input_cols + output_cols):\n#     df_test[col] = float('NaN')\n# test_intersection_mask = (df_test['Date'] <= df_train['Date'].max())\n# train_intersection_mask = (df_train['Date'] >= df_test['Date'].min())\n# df_test.loc[test_intersection_mask, input_cols + output_cols] = df_train.loc[train_intersection_mask, input_cols + output_cols].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# use_predictions = False\n\n# start_time = time.time()\n# with tqdm(total = len(list(states))) as pbar:\n#     for state in states:\n#         for d in pred_dt_range:\n#             mask = (df_test['Date'] == d) & (df_test['Province_State'] == state)\n#             if (d > df_train['Date'].max()):\n#                 for lag in lag_range:\n#                     mask_org = (df_test['Date'] == (d - lag)) & (df_test['Province_State'] == state)\n#                     try:\n#                         df_test.loc[mask, 'ConfirmedCases_' + str(lag)] = df_test.loc[mask_org, 'ConfirmedCases'].values\n#                     except:\n#                         df_test.loc[mask, 'ConfirmedCases_' + str(lag)] = 0\n#                     try:\n#                         df_test.loc[mask, 'Fatalities_' + str(lag)] = df_test.loc[mask_org, 'Fatalities'].values\n#                     except:\n#                         df_test.loc[mask, 'Fatalities_' + str(lag)] = 0\n#             X_test  = df_test.loc[mask, input_cols]\n#             # Cases\n#             X_test_cases = X_test[cases_columns].values\n#             X_test_cases = X_cases_scaler.transform(X_test_cases)\n#             X_test_cases.reshape(X_test_cases.shape[0], 1, X_test_cases.shape[1])\n#             next_cases = model_cases.predict(np.array([X_test_cases]))\n#             next_cases_scaled = y_cases_scaler.inverse_transform(next_cases)\n#             # Fatal\n#             X_test_fatal = X_test[fatal_columns].values\n#             X_test_fatal = X_fatal_scaler.transform(X_test_fatal)\n#             X_test_fatal.reshape(X_test_fatal.shape[0], 1, X_test_fatal.shape[1])\n#             next_fatal = model_fatal.predict(np.array([X_test_fatal]))\n#             next_fatal_scaled = y_fatal_scaler.inverse_transform(next_fatal)\n#             # Update df_test\n#             if (d > np.max(df_train['Date'].values)):\n#                 if (next_cases_scaled[0][0] < 0):\n#                     next_cases_scaled[0][0] = 0\n#                 if (next_cases_scaled[0][0] < X_test['ConfirmedCases_1'].values[0]):\n#                     next_cases_scaled[0][0] = X_test['ConfirmedCases_1'].values[0]\n#                 df_test.loc[mask, 'ConfirmedCases'] = next_cases_scaled\n#                 if (next_fatal_scaled[0][0] < 0):\n#                     next_fatal_scaled[0][0] = 0\n#                 if (next_fatal_scaled[0][0] < X_test['Fatalities_1'].values[0]):\n#                     next_fatal_scaled[0][0] = X_test['Fatalities_1'].values[0]\n#                 df_test.loc[mask, 'Fatalities'] = next_fatal_scaled\n#             else:\n#                 if use_predictions:\n#                     if (next_cases_scaled[0][0] < 0):\n#                         next_cases_scaled[0][0] = 0\n#                     if (next_cases_scaled[0][0] < X_test['ConfirmedCases_1'].values[0]):\n#                         next_cases_scaled[0][0] = X_test['ConfirmedCases_1'].values[0]\n#                     df_test.loc[mask, 'ConfirmedCases'] = next_cases_scaled\n#                     if (next_fatal_scaled[0][0] < 0):\n#                         next_fatal_scaled[0][0] = 0\n#                     if (next_fatal_scaled[0][0] < X_test['Fatalities_1'].values[0]):\n#                         next_fatal_scaled[0][0] = X_test['Fatalities_1'].values[0]\n#                     df_test.loc[mask, 'Fatalities'] = next_fatal_scaled\n#         # Fill cases\n#         lowest_pred = np.max(df_train[df_train['Province_State'] == state]['ConfirmedCases'].values)\n#         cases = handle_predictions (df_test[df_test['Province_State'] == state]['ConfirmedCases'].values, lowest_pred)\n#         submission = fillSubmission (state, 'ConfirmedCases', cases)\n#         # Fill fatal\n#         lowest_pred = np.max(df_train[df_train['Province_State'] == state]['Fatalities'].values)\n#         cases = handle_predictions (df_test[df_test['Province_State'] == state]['Fatalities'].values, lowest_pred)\n#         submission = fillSubmission (state, 'Fatalities', cases)\n#         # Update progress bar\n#         pbar.update(1)\n        \n# print('Time spent for predicting everything was {} minutes'.format(round((time.time()-start_time)/60,1)))\n# avg_rmsle()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# cases = []\n# fatal = []\n# for a in random_validation_set:\n#     score = rmsle(a)\n#     cases.append(score[0])\n#     fatal.append(score[1])\n#     print(score)\n# print (avg_rmsle())\n# print (\"Average = {}, {}\".format(np.average(cases), np.average(fatal)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for a in random_validation_set:\n#     plotStatus([a])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# submission = df_test[['ForecastId'] + output_cols]\n# submission","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# submission.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}