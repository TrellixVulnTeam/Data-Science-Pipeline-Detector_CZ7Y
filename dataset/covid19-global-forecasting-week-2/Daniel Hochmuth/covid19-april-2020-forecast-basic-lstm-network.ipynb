{"cells":[{"metadata":{},"cell_type":"markdown","source":"# COVID19 - April 2020 Forecast\n### A simple LSTM to predict time series\n\nDisclaimer: The purpose of this notebook is to implement a basic LSTM network on the COVID19 dataset as a basis for further analysis. Since I'm not using any external dataset, the predictions will most likely be inaccurate.\n\nFor the multivariate type of LSTM the credits go to this very useful blogpost: https://machinelearningmastery.com/how-to-develop-lstm-models-for-time-series-forecasting"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport datetime\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.preprocessing import MinMaxScaler\n\nfrom keras.models import Sequential\nfrom keras.layers import LSTM\nfrom keras.layers import Dense\nfrom keras.layers import Dropout\nfrom keras.layers import RepeatVector\nfrom keras.layers import TimeDistributed\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Exploration"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_df = pd.read_csv('/kaggle/input/covid19-global-forecasting-week-2/train.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"display(train_df.head())\ndisplay(train_df.info())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('We have', len(train_df.Country_Region.unique()), 'countries/regions in the dataset.')\nprint('We have', len(train_df.Province_State.unique()), 'provinces/states in the dataset.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"timetrend_sick = sns.lineplot(train_df['Date'], train_df['ConfirmedCases'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"timetrend_deceased = sns.lineplot(train_df['Date'], train_df['Fatalities'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Add a new column to be able to uniquely distinguish countries/regions\n\ntrain_df['UniqueRegion'] = np.where(train_df['Province_State'].isna(), train_df['Country_Region'], train_df['Country_Region'] + ' - ' + train_df['Province_State'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Show trends for 10 countries with most confirmed cases\n\ntop10_most_cases = train_df.loc[train_df['Date'] == train_df['Date'].max()][['UniqueRegion','ConfirmedCases']].sort_values(by='ConfirmedCases', ascending=False).head(10)\ntop10_most_cases_df = train_df.loc[train_df['UniqueRegion'].isin(top10_most_cases['UniqueRegion'].values)]\n\nmain_df = pd.DataFrame()\n\nfor i, top10_country in enumerate (top10_most_cases_df['UniqueRegion'].unique()):\n    if i == 0:\n        main_df = top10_most_cases_df.loc[top10_most_cases_df['UniqueRegion'] == top10_country][['Date', 'ConfirmedCases']].sort_values(by='Date')\n        main_df = main_df.rename({'ConfirmedCases': top10_country}, axis='columns')\n\n    else:\n        temp_df = top10_most_cases_df.loc[top10_most_cases_df['UniqueRegion'] == top10_country][['Date', 'ConfirmedCases']]\n        temp_df = temp_df.rename({'ConfirmedCases': top10_country}, axis='columns')\n        main_df = pd.merge(main_df, temp_df, on=['Date'])\n\nmain_df = main_df.set_index('Date')\nmain_df.plot(figsize=(20,10))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Transformation & Pre-processing"},{"metadata":{},"cell_type":"markdown","source":"The original plan was to use the number of sick per day as one of the input features in the model. I ended up abandoning this idea due to time constraints but I'm leaving the code here for possible future reference."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate number of new sick per day\n\nunique_regions = np.sort(train_df['UniqueRegion'].unique())\ntrain_df['SickPerDay'] = 0\n\nbaseline_length = len(train_df.loc[train_df['UniqueRegion'] == 'Afghanistan']) # Country chosen arbitrarily\n\nfor unique_region in unique_regions:\n    len_country = len(train_df.loc[train_df['UniqueRegion'] == unique_region])\n    len_diffs = len(train_df.loc[train_df['UniqueRegion'] == unique_region]['ConfirmedCases'].diff())\n    if len_country > baseline_length or len_diffs > baseline_length:\n        raise NameError('Too many rows for country {}'.format(unique_region))\n    train_df['SickPerDay'].loc[(train_df['UniqueRegion'] == unique_region)] = train_df.loc[train_df['UniqueRegion'] == unique_region]['ConfirmedCases'].diff()\n    \ntrain_df['SickPerDay'] = train_df['SickPerDay'].fillna(0)\n\n# Show an example\ndisplay(train_df.loc[train_df['UniqueRegion'] == 'Czechia'].tail())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The idea is to train the LSTM on all countries at once, so we need to transform the dataset a bit and create a separate column for each country/region."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Transform main data into a horizontal dataframe\n\ndef transform_horizontally(input_df, value_column):\n\n    horizontal_df = pd.DataFrame()\n\n    for i, uniqueRegion in enumerate (unique_regions):\n        if i == 0:\n            horizontal_df = input_df.loc[input_df['UniqueRegion'] == uniqueRegion][['Date', value_column]].sort_values(by='Date')\n            horizontal_df = horizontal_df.rename({value_column: uniqueRegion}, axis='columns')\n\n        else:\n            temp_df = input_df.loc[train_df['UniqueRegion'] == uniqueRegion][['Date', value_column]]\n            temp_df = temp_df.rename({value_column: uniqueRegion}, axis='columns')\n            horizontal_df = pd.merge(horizontal_df, temp_df, on=['Date'])\n            \n    return horizontal_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"confirmed_horizontal_df = transform_horizontally(train_df, 'ConfirmedCases').sort_values(by='Date')\nfatalities_horizontal_df = transform_horizontally(train_df, 'Fatalities').sort_values(by='Date')\n\n\ndisplay(confirmed_horizontal_df.head())\ndisplay(confirmed_horizontal_df.shape)\n\ndisplay(fatalities_horizontal_df.head())\ndisplay(fatalities_horizontal_df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Convert dataframes into numpy arrays\n\nnp_confirmed = confirmed_horizontal_df.drop(columns=['Date']).to_numpy()\nnp_fatalities = fatalities_horizontal_df.drop(columns=['Date']).to_numpy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Scale the values (better performance of LSTM)\n\nscaler = MinMaxScaler(feature_range = (0, 1))\n\nnp_confirmed_scaled = scaler.fit_transform(np_confirmed)\nnp_fatalities_scaled = scaler.fit_transform(np_confirmed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Split a multivariate sequence into samples\n# Credits to: https://machinelearningmastery.com/how-to-develop-lstm-models-for-time-series-forecasting\n\ndef split_sequences(sequences, n_steps_in, n_steps_out):\n    X, y = list(), list()\n    for i in range(len(sequences)):\n        # find the end of this pattern\n        end_ix = i + n_steps_in\n        out_end_ix = end_ix + n_steps_out\n        # check if we are beyond the dataset\n        if out_end_ix > len(sequences):\n            break\n        # gather input and output parts of the pattern\n        seq_x, seq_y = sequences[i:end_ix, :], sequences[end_ix:out_end_ix, :]\n        X.append(seq_x)\n        y.append(seq_y)\n    return np.array(X), np.array(y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_steps_in = 30\nn_steps_out = 1\n\nX_confirmed, y_confirmed = split_sequences(np_confirmed_scaled, n_steps_in, n_steps_out)\nX_fatalities, y_fatalities = split_sequences(np_fatalities_scaled, n_steps_in, n_steps_out)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"assert X_confirmed.shape == X_fatalities.shape\nassert y_confirmed.shape == y_fatalities.shape\n\nn_features = X_confirmed.shape[2]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training the LSTM Model"},{"metadata":{},"cell_type":"markdown","source":"### Confirmed Cases"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define model for confirmed cases\n\nmodel_confirmed = Sequential()\nmodel_confirmed.add(LSTM(1000, activation='relu', input_shape=(n_steps_in, n_features)))\nmodel_confirmed.add(RepeatVector(n_steps_out))\nmodel_confirmed.add(LSTM(2000, activation='relu', return_sequences=True))\nmodel_confirmed.add(Dropout(0.2))\nmodel_confirmed.add(LSTM(2000, activation='relu', return_sequences=True))\nmodel_confirmed.add(Dropout(0.2))\nmodel_confirmed.add(LSTM(2000, activation='relu', return_sequences=True))\nmodel_confirmed.add(Dropout(0.2))\nmodel_confirmed.add(LSTM(1000, activation='relu', return_sequences=True))\nmodel_confirmed.add(TimeDistributed(Dense(n_features)))\nmodel_confirmed.compile(optimizer='adam', loss='mse')\n\nmodel_confirmed.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history_confirmed = model_confirmed.fit(X_confirmed, y_confirmed, epochs=200)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(history_confirmed.history['loss'][30:])\nplt.title('Confirmed cases loss')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_confirmed_pred = np_confirmed_scaled[-n_steps_in-1:-n_steps_out].reshape((1, n_steps_in, n_features))\ny_confirmed_pred = model_confirmed.predict(X_confirmed_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Display targets and predictions side by side\n\ncomparison_df = pd.DataFrame()\ncomparison_df['Target'] = list(np_confirmed[-1])\ncomparison_df['Prediction'] = [int(x) for x in scaler.inverse_transform(y_confirmed_pred[0])[0].astype(int)]\ncomparison_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Fatalities"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define model for confirmed cases\n\nmodel_fatalities = Sequential()\nmodel_fatalities.add(LSTM(50, activation='relu', input_shape=(n_steps_in, n_features)))\nmodel_fatalities.add(RepeatVector(n_steps_out))\nmodel_fatalities.add(LSTM(100, activation='relu', return_sequences=True))\nmodel_fatalities.add(Dropout(0.2))\nmodel_fatalities.add(LSTM(100, activation='relu', return_sequences=True))\nmodel_fatalities.add(Dropout(0.2))\nmodel_fatalities.add(LSTM(100, activation='relu', return_sequences=True))\nmodel_fatalities.add(Dropout(0.2))\nmodel_fatalities.add(LSTM(100, activation='relu', return_sequences=True))\nmodel_fatalities.add(TimeDistributed(Dense(n_features)))\nmodel_fatalities.compile(optimizer='adam', loss='mse')\n\nmodel_fatalities.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history_fatalities = model_fatalities.fit(X_fatalities, y_fatalities, epochs=100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(history_fatalities.history['loss'])\nplt.title('Confirmed cases loss')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_fatalities_pred = np_fatalities_scaled[-n_steps_in-1:-n_steps_out].reshape((1, n_steps_in, n_features))\ny_fatalities_pred = model_fatalities.predict(X_fatalities_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Display targets and predictions side by side\n\ncomparison_df = pd.DataFrame()\ncomparison_df['Target'] = list(np_fatalities[-1])\ncomparison_df['Prediction'] = [int(x) for x in scaler.inverse_transform(y_fatalities_pred[0])[0].astype(int)]\ncomparison_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Read test set\n\ntest_df = pd.read_csv('/kaggle/input/covid19-global-forecasting-week-2/test.csv')\ntest_df['UniqueRegion'] = np.where(test_df['Province_State'].isna(), test_df['Country_Region'], test_df['Country_Region'] + ' - ' + test_df['Province_State'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predict for the necessary number of days\n\nnum_days_to_predict = datetime.datetime.strptime(test_df['Date'].max(), '%Y-%m-%d') - datetime.datetime.strptime(train_df['Date'].max(), '%Y-%m-%d')\nnum_days_to_predict.days\n\n# Copy the train set for the purposes of prediction\nX_test_confirmed = np_confirmed_scaled.copy()\nX_test_fatalities = np_fatalities_scaled.copy()\n\ndef predict_for_test_set(model, X_test):\n    for day in range(num_days_to_predict.days):\n        X_pred_temp = X_test[-n_steps_in:].reshape((1, n_steps_in, n_features))\n        y_pred_temp = model.predict(X_pred_temp)\n        X_test = np.append(X_test, y_pred_temp[0], axis=0)\n    return X_test\n\nX_test_confirmed = predict_for_test_set(model_confirmed, X_test_confirmed)\nX_test_fatalities = predict_for_test_set(model_fatalities, X_test_fatalities)\n\nassert X_test_confirmed.shape == X_test_fatalities.shape\nprint('We have', X_test_confirmed.shape[0], 'days after predicting.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Copy predicted values into test dataframe\n\ntest_final = pd.merge(test_df, train_df, how='left', on=['Date', 'UniqueRegion'])\nX_test_confirmed_inversed = scaler.inverse_transform(X_test_confirmed)\nX_test_fatalities_inversed = scaler.inverse_transform(X_test_fatalities)\ndummy_df = pd.DataFrame(test_final.loc[(test_final['UniqueRegion'] == unique_regions[0]) & ((test_final['ConfirmedCases'].isna()) | (test_final['Fatalities'].isna()))].sort_values(by='Date')['Date'])\n\nfor i, unique_region in enumerate(unique_regions):\n    df_temp = dummy_df.copy()\n    assert len(X_test_confirmed_inversed[-num_days_to_predict.days:,i]) == len(df_temp)\n    assert len(X_test_fatalities_inversed[-num_days_to_predict.days:,i]) == len(df_temp)\n    df_temp['ConfirmedCasesTemp'] = X_test_confirmed_inversed[-num_days_to_predict.days:,i]\n    df_temp['FatalitiesTemp'] = X_test_fatalities_inversed[-num_days_to_predict.days:,i]\n    df_temp['UniqueRegion'] = unique_region\n    test_final = pd.merge(test_final, df_temp, how='left', on=['Date', 'UniqueRegion'])\n    try:\n        test_final['ConfirmedCases'] = np.where((test_final['UniqueRegion'] == unique_region) & test_final['ConfirmedCases'].isna(), test_final['ConfirmedCasesTemp'], test_final['ConfirmedCases'])\n        test_final['Fatalities'] = np.where((test_final['UniqueRegion'] == unique_region) & test_final['Fatalities'].isna(), test_final['FatalitiesTemp'], test_final['Fatalities'])\n    except:\n        test_final['ConfirmedCases'] = np.where((test_final['UniqueRegion'] == unique_region) & test_final['ConfirmedCases'].isna(), None, test_final['ConfirmedCases'])\n        test_final['Fatalities'] = np.where((test_final['UniqueRegion'] == unique_region) & test_final['Fatalities'].isna(), None, test_final['Fatalities'])\n        \n    #display(test_final.head(50))\n    if 'ConfirmedCasesTemp' in test_final.columns:\n        test_final = test_final.drop(columns=['ConfirmedCasesTemp'])\n    if 'FatalitiesTemp' in test_final.columns:\n        test_final = test_final.drop(columns=['FatalitiesTemp'])\n        \n\n\nassert not test_final['ConfirmedCases'].isnull().values.any()\nassert not test_final['Fatalities'].isnull().values.any()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_final['ConfirmedCases'] = test_final['ConfirmedCases'].astype(int)\ntest_final['Fatalities'] = test_final['Fatalities'].astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_final.loc[test_final['UniqueRegion'] == 'Czechia']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_sample_df = pd.read_csv('/kaggle/input/covid19-global-forecasting-week-2/submission.csv')\nsub_sample_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_df = test_final[['ForecastId', 'ConfirmedCases', 'Fatalities']]\nsubmission_df.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}