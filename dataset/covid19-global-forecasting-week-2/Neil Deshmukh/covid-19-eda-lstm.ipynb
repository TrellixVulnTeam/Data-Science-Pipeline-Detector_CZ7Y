{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nimport multiprocessing\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nmem_bytes = os.sysconf('SC_PAGE_SIZE') * os.sysconf('SC_PHYS_PAGES')  # e.g. 4015976448\nmem_gib = mem_bytes/(1024.**3)  # e.g. 3.74\nprint(\"RAM: %f GB\" % mem_gib)\nprint(\"CORES: %d\" % multiprocessing.cpu_count())\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import plotly.graph_objects as go\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nimport time\nfrom datetime import datetime\nfrom pathlib import Path\nfrom sklearn import preprocessing\nimport keras.backend as K\nfrom keras.models import Sequential\nfrom keras.layers import Dense, LSTM, Dropout\nfrom keras.callbacks import EarlyStopping\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/covid19-global-forecasting-week-2/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/covid19-global-forecasting-week-2/test.csv\")\ntrain.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Basic Statistics & Visualization"},{"metadata":{"trusted":true},"cell_type":"code","source":"mask = train['Date'].max()\nworld_cum_confirmed = sum(train[train['Date'] == mask].ConfirmedCases)\nworld_cum_fatal = sum(train[train['Date'] == mask].Fatalities)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Number of Countires are: ', len(train['Country_Region'].unique()))\nprint('Training dataset ends at: ', mask)\nprint('Number of cumulative confirmed cases worldwide are: ', world_cum_confirmed)\nprint('Number of cumulative fatal cases worldwide are: ', world_cum_fatal)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# top 10 countires that have most servere situation\ncum_per_country = train[train['Date'] == mask].groupby(['Date','Country_Region']).sum().sort_values(['ConfirmedCases'], ascending=False)\ncum_per_country[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot growing curve for top 5 most servere countries except China\n#TODO: optimize code\ndate = train['Date'].unique()\ncc_us = train[train['Country_Region'] == 'US'].groupby(['Date']).sum().ConfirmedCases\nft_us = train[train['Country_Region'] == 'US'].groupby(['Date']).sum().Fatalities\ncc_ity = train[train['Country_Region'] == 'Italy'].groupby(['Date']).sum().ConfirmedCases\nft_ity = train[train['Country_Region'] == 'Italy'].groupby(['Date']).sum().Fatalities\ncc_spn = train[train['Country_Region'] == 'Spain'].groupby(['Date']).sum().ConfirmedCases\nft_spn = train[train['Country_Region'] == 'Spain'].groupby(['Date']).sum().Fatalities\ncc_gmn = train[train['Country_Region'] == 'Germany'].groupby(['Date']).sum().ConfirmedCases\nft_gmn = train[train['Country_Region'] == 'Germany'].groupby(['Date']).sum().Fatalities\ncc_frc = train[train['Country_Region'] == 'France'].groupby(['Date']).sum().ConfirmedCases\nft_frc = train[train['Country_Region'] == 'France'].groupby(['Date']).sum().Fatalities\n\nfig = go.Figure()\n# add traces\nfig.add_trace(go.Scatter(x=date, y=cc_us, name='US'))\nfig.add_trace(go.Scatter(x=date, y=cc_ity, name='Italy'))\nfig.add_trace(go.Scatter(x=date, y=cc_spn, name='Spain'))\nfig.add_trace(go.Scatter(x=date, y=cc_gmn, name='Germany'))\nfig.add_trace(go.Scatter(x=date, y=cc_frc, name='France'))\nfig.update_layout(title=\"Plot of Cumulative Cases for Top 5 countires (except China)\",\n    xaxis_title=\"Date\",\n    yaxis_title=\"Cases\")\nfig.update_xaxes(nticks=30)\n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = go.Figure()\n# add traces\nfig.add_trace(go.Scatter(x=date, y=ft_us, name='US'))\nfig.add_trace(go.Scatter(x=date, y=ft_ity, name='Italy'))\nfig.add_trace(go.Scatter(x=date, y=ft_spn, name='Spain'))\nfig.add_trace(go.Scatter(x=date, y=ft_gmn, name='Germany'))\nfig.add_trace(go.Scatter(x=date, y=ft_frc, name='France'))\nfig.update_layout(title=\"Plot of Fatal Cases for Top 5 countires (except China)\",\n    xaxis_title=\"Date\",\n    yaxis_title=\"Cases\")\nfig.update_xaxes(nticks=30)\n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Build Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"#TODO: check duplicates,missing numeric, string, typo.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['Date'] = pd.to_datetime(train['Date'])\ntest['Date'] = pd.to_datetime(test['Date'])\ntrain['Country_Region'] = train['Country_Region'].astype(str)\n# train['Province_State'] = train['Province_State'].astype(str)\ntest['Country_Region'] = test['Country_Region'].astype(str)\n# test['Province_State'] = test['Province_State'].astype(str)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"EMPTY_VAL = \"EMPTY_VAL\"\n\ndef fillState(state, country):\n    if state == EMPTY_VAL: return country\n    return state\n\n\ntrain['Province_State'].fillna(EMPTY_VAL, inplace=True)\ntrain['Province_State'] = train.loc[:, ['Province_State', 'Country_Region']].apply(lambda x : fillState(x['Province_State'], x['Country_Region']), axis=1)\n\ntest['Province_State'].fillna(EMPTY_VAL, inplace=True)\ntest['Province_State'] = test.loc[:, ['Province_State', 'Country_Region']].apply(lambda x : fillState(x['Province_State'], x['Country_Region']), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"le = preprocessing.LabelEncoder()\ntrain['country_encoder'] = le.fit_transform(train['Country_Region'])\ntrain['date_int'] = train['Date'].apply(lambda x: datetime.strftime(x, '%m%d')).astype(int)\n\ntest['country_encoder'] = le.transform(test['Country_Region'])\ntest['date_int'] = test['Date'].apply(lambda x: datetime.strftime(x, '%m%d')).astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"le = preprocessing.LabelEncoder()\ntrain['province_encoder'] = le.fit_transform(train['Province_State'])\ntest['province_encoder'] = le.transform(test['Province_State'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# #TODO: takes 44m ish, consider multi-processing, multi-cores, run in GPU\n# #TODO: create data_generate func\nstart_time = time.time()\n\ncountry = train['Country_Region'].drop_duplicates()\ntrain_df = train.copy()\ntrain_df.rename(columns={'Date': 'date', 'ConfirmedCases': 'cc_cases', 'Fatalities': 'ft_cases', 'Country_Region': 'country', 'Province_State': 'province'}, inplace=True)\nlags = np.arange(1,8,1)  # lag of 1 to 7\n\nwith tqdm(total = len(list(train_df['date'].unique()))) as pbar:\n    for d in train_df['date'].drop_duplicates():\n        for i in country:\n            province = train_df[train_df['country'] == i]['province'].drop_duplicates()\n            for j in province:\n                mask = (train_df['date'] == d) & (train_df['country'] == i) & (train_df['province'] == j)            \n                for lag in lags:\n                    mask_org = (train_df['date'] == (d - pd.Timedelta(days=lag))) & (train_df['country'] == i) & (train_df['province'] == j)\n                    try:\n                        train_df.loc[mask, 'cc_cases_' + str(lag)] = train_df.loc[mask_org, 'cc_cases'].values\n                    except:\n                        train_df.loc[mask, 'cc_cases_' + str(lag)] = 0\n\n                    try:\n                        train_df.loc[mask, 'ft_cases_' + str(lag)] = train_df.loc[mask_org, 'ft_cases'].values\n                    except:\n                        train_df.loc[mask, 'ft_cases_' + str(lag)] = 0\n        pbar.update(1)\nprint('Time spent for building features is {} minutes'.format(round((time.time()-start_time)/60,1)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.to_csv(Path('/kaggle/working', 'train_df.csv')) \n# saved locally, reload it\n#train_df = pd.read_csv(Path('/kaggle/input/covid19-global-forecasting-week-2', 'train.csv'), index_col = 0, parse_dates = ['date'])\ntrain_df[train_df['country'] == 'Italy'].tail()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### LSTM Modelling"},{"metadata":{"trusted":true},"cell_type":"code","source":"#TODO: walk forward validation\ndef split_train_val(df, val_ratio):\n    val_len = int(len(df) * val_ratio)\n    train_set =  df[:-val_len]\n    val_set = df[-val_len:]\n    return train_set, val_set","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_fixed_cols = ['ForecastId', 'Province_State', 'Country_Region', 'Date']\nfixed_cols = ['Id', 'province', 'country', 'date']\noutput_cols = ['cc_cases', 'ft_cases']\ninput_cols = list(set(train_df.columns.to_list()) - set(fixed_cols) - set(output_cols))\nprint('output columns are ', output_cols)\nprint('input columns are ', input_cols)\nX = train_df[input_cols]\ny = train_df[output_cols]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# split to cumulative and fatal features and build 2 separate models\n# split to train and validation set\ncc_input = ['cc_cases_1', 'cc_cases_2', 'cc_cases_3', 'cc_cases_4', 'cc_cases_5', 'cc_cases_6', 'cc_cases_7', 'country_encoder', 'province_encoder', 'date_int']\nft_input = ['ft_cases_1', 'ft_cases_2', 'ft_cases_3', 'ft_cases_4', 'ft_cases_5', 'ft_cases_6', 'ft_cases_7', 'country_encoder', 'province_encoder', 'date_int']\ncc_output = ['cc_cases']\nft_output = ['ft_cases']\nX_cc = X[cc_input]\nX_ft = X[ft_input]\ny_cc = y[cc_output]\ny_ft = y[ft_output]\ntrain_X_cc, val_X_cc = split_train_val(df = X_cc, val_ratio = 0.1)\ntrain_y_cc, val_y_cc = split_train_val(df = y_cc, val_ratio = 0.1)\ntrain_X_ft, val_X_ft = split_train_val(df = X_ft, val_ratio = 0.1)\ntrain_y_ft, val_y_ft = split_train_val(df = y_ft, val_ratio = 0.1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"idx = np.random.RandomState(seed=42).permutation(train_X_cc.index)\ntrain_X_cc = train_X_cc.reindex(idx)\ntrain_y_cc = train_y_cc.reindex(idx)\ntrain_X_ft = train_X_ft.reindex(idx)\ntrain_y_ft = train_y_ft.reindex(idx)\n# train_y_cc.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # normalization\nX_scaler_cc = MinMaxScaler()\nX_train_cc = X_scaler_cc.fit_transform(train_X_cc)\nX_val_cc =  X_scaler_cc.transform(val_X_cc) # intput/output 2D array-like\n\ny_scaler_cc = MinMaxScaler()\ny_train_cc = y_scaler_cc.fit_transform(train_y_cc)\ny_val_cc = y_scaler_cc.transform(val_y_cc) # array-like","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_scaler_ft = MinMaxScaler()\nX_train_ft = X_scaler_ft.fit_transform(train_X_ft)\nX_val_ft =  X_scaler_ft.transform(val_X_ft) # intput/output 2D array-like\n\ny_scaler_ft = MinMaxScaler()\ny_train_ft = y_scaler_ft.fit_transform(train_y_ft)\ny_val_ft = y_scaler_ft.transform(val_y_ft) # array-like","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Validate if train and test is splited correctly for 2 cases: ')\nprint('cumulative cases training has shape ', X_train_cc.shape, y_train_cc.shape)\nprint('fatal cases training has shape ', X_train_ft.shape, y_train_ft.shape)\nprint('cumulative cases valid has shape ', X_val_cc.shape, y_val_cc.shape)\nprint('fatal cases valid has shape ', X_val_ft.shape, y_val_ft.shape)\n# #TODO\nprint('Validate if train and test contains np.nan, np.inf, -np.inf after standardization: ')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''# if choose to not apply normalization, however it generates NaN in output...\nX_train_cc = train_X_cc.to_numpy()  \nX_val_cc = val_X_cc.to_numpy()\nX_train_ft = train_X_ft.to_numpy()\nX_val_ft = val_X_ft.to_numpy()\n\ny_train_cc = train_y_cc.to_numpy()\ny_val_cc = val_y_cc.to_numpy()\ny_train_ft = train_y_ft.to_numpy()\ny_val_ft = val_y_ft.to_numpy()'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for LSTM, intput.shape = (n_samples, 1, n_features)\nX_train_cc = X_train_cc.reshape(X_train_cc.shape[0], 1, X_train_cc.shape[1])\nX_val_cc = X_val_cc.reshape(X_val_cc.shape[0], 1, X_val_cc.shape[1])\n\nX_train_ft = X_train_ft.reshape(X_train_ft.shape[0], 1, X_train_ft.shape[1])\nX_val_ft = X_val_ft.reshape(X_val_ft.shape[0], 1, X_val_ft.shape[1])\nprint(X_train_cc.shape, X_val_cc.shape, X_train_ft.shape, X_val_ft.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# customize loss function which is aligned with kaggle evaluation\ndef root_mean_squared_log_error(y_true, y_pred):\n    return K.sqrt(K.mean(K.square(K.log(y_pred + 1) - K.log(y_true + 1)))) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#declaring only one model\ndef LSTM_model(n_1, input_dim, output_dim):\n    model = Sequential()\n    model.add(LSTM(n_1,input_shape=(1, input_dim), activation='relu'))\n    model.add(Dropout(0.1))\n    # model.add(LSTM(n_2, activation='relu'))\n    model.add(Dense(output_dim, activation='relu'))\n    model.compile(loss=root_mean_squared_log_error, optimizer='adam')\n    print(model.summary())\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#TODO: debug sometimes it's getting inf. Suspect bad input\nmodel_cc = LSTM_model(4, X_train_cc.shape[-1], y_train_cc.shape[-1])\nmodel_ft = LSTM_model(4, X_train_ft.shape[-1], y_train_ft.shape[-1])\nearly_stop = EarlyStopping(monitor='loss', patience=5, verbose=0, mode='min')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Start model training')\nstart_time = time.time()\nhistory_cc = model_cc.fit(X_train_cc, y_train_cc, epochs = 100,validation_data = (X_val_cc, y_val_cc), verbose = 2, callbacks=[early_stop])\nmodel_cc.save(\"model_cc.h5\")\nprint('Time spent for model training is {} minutes'.format(round((time.time()-start_time)/60,1)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot training & validation loss values\nplt.figure(figsize=(8,5))\nplt.plot(history_cc.history['loss'])\nplt.plot(history_cc.history['val_loss'])\nplt.title('CC Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Start model training')\nstart_time = time.time()\nhistory_ft = model_ft.fit(X_train_ft, y_train_ft, epochs = 100,validation_data = (X_val_ft, y_val_ft), verbose = 2, callbacks=[early_stop])\nmodel_ft.save(\"model_ft.h5\")\nprint('Time spent for model training is {} minutes'.format(round((time.time()-start_time)/60,1)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot training & validation loss values\nplt.figure(figsize=(8,5))\nplt.plot(history_ft.history['loss'])\nplt.plot(history_ft.history['val_loss'])\nplt.title('FT Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Validate if output makes sense\nyhat_val_cc = model_cc.predict(X_val_cc)\nprint(yhat_val_cc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(val_y_cc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Validate if output makes sense\nyhat_val_ft = model_cc.predict(X_val_ft)\nprint(yhat_val_ft)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(val_y_ft)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Make Prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"#TODO: takes 14m ish, consider multi-processing, multi-cores, run in GPU\n#TODO: create data_generate func\nstart_time = time.time()\ntest['Country_Region'] = test['Country_Region'].astype(str)\ntest['Province_State'] = test['Province_State'].astype(str)\ncountry = test['Country_Region'].drop_duplicates()\nadj_input_cols = [e for e in input_cols if e not in ('province_encoder', 'country_encoder', 'date_int')]\n# fill data for overlapped days\ntest_df = test.copy().join(pd.DataFrame(columns = adj_input_cols + output_cols))\ntest_df.rename(columns={'Date': 'date', 'Country_Region': 'country', 'Province_State': 'province'}, inplace=True)\nlags = np.arange(1,8,1)  # lag of 1 to 7\ntest_overlap_mask = (test_df['date'] <= train_df['date'].max())\ntrain_overlap_mask = (train_df['date'] >= test_df['date'].min())\ntest_df.loc[test_overlap_mask, input_cols + output_cols] = train_df.loc[train_overlap_mask, input_cols + output_cols].values\n\n# predict data for forward days\npred_dt_range = pd.date_range(start = train_df['date'].max() + pd.Timedelta(days=1), end = test_df['date'].max(), freq = '1D') # test_df['date'].max()\nwith tqdm(total = len(pred_dt_range)) as pbar:\n    for d in pred_dt_range:\n        \n        for i in country:\n            \n            province = test_df[test_df['country'] == i]['province'].drop_duplicates()\n            \n            for j in province:\n                \n                mask = (test_df['date'] == d) & (test_df['country'] == i) & (test_df['province'] == j)\n                \n                \n                # update input features for the predicted day\n                for lag in lags:\n                    mask_org = (test_df['date'] == (d - pd.Timedelta(days=lag))) & (test_df['country'] == i) & (test_df['province'] == j)\n                    try:\n                        test_df.loc[mask, 'cc_cases_' + str(lag)] = test_df.loc[mask_org, 'cc_cases'].values\n                    except:\n                        test_df.loc[mask, 'cc_cases_' + str(lag)] = 0\n\n                    try:\n                        test_df.loc[mask, 'ft_cases_' + str(lag)] = test_df.loc[mask_org, 'ft_cases'].values\n                    except:\n                        test_df.loc[mask, 'ft_cases_' + str(lag)] = 0\n                \n                test_X  = test_df.loc[mask, input_cols]\n            \n                # predict for comfirmed cases\n                test_X_cc = test_X[cc_input]\n                X_test_cc= test_X_cc\n                # X_test_cc =  X_scaler_cc.transform(test_X_cc) # intput/output 2D array-like\n                # X_test_cc = X_test_cc.reshape(X_test_cc.shape[0], 1, X_test_cc.shape[1])\n                X_test_cc = X_test_cc.to_numpy().reshape(X_test_cc.shape[0], 1, X_test_cc.shape[1])\n                next_cc = model_cc.predict(X_test_cc)\n                # next_cc_scaled = y_scaler_cc.inverse_transform(next_cc)\n                next_cc_scaled = next_cc\n                \n                # predict for fatal cases\n                test_X_ft = test_X[ft_input]\n                X_test_ft = test_X_ft\n                # X_test_ft =  X_scaler_ft.transform(test_X_ft) # intput/output 2D array-like\n                # X_test_ft = X_test_ft.reshape(X_test_ft.shape[0], 1, X_test_ft.shape[1])\n                X_test_ft = X_test_ft.to_numpy().reshape(X_test_ft.shape[0], 1, X_test_ft.shape[1])\n                next_ft = model_cc.predict(X_test_ft)\n                # next_ft_scaled = y_scaler_cc.inverse_transform(next_ft)\n                next_ft_scaled = next_ft\n                # print(d, ' - ', i, ' - ', j,  ' - Predicted Confirmed Cases are ', next_cc_scaled, ' - Predicted Fatal Cases are ', next_ft_scaled)\n                \n                # update yhat for next day\n                test_df.loc[mask, 'cc_cases'] = next_cc_scaled\n                test_df.loc[mask, 'ft_cases'] = next_ft_scaled\n                        \n        pbar.update(1)\n        \nprint('Time spent for building features is {} minutes'.format(round((time.time()-start_time)/60,1)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame()\nsubmission['ForecastId'] = test_df['ForecastId']\nsubmission['ConfirmedCases'] = test_df['cc_cases']\nsubmission['Fatalities'] = test_df['ft_cases']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv(\"submission.csv\",index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission[:20]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Thoughts Next\n\na0) Debug model (Done). \n\na) Try MinMaxscaler; add early_stopper, tuning, save model. inform myself if LSTM can have both two cases as predicted targets\n\nb) Add weather data, temp + humidity\n\nc) Cluster countires, apply categorical outputs as input features"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### "}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}