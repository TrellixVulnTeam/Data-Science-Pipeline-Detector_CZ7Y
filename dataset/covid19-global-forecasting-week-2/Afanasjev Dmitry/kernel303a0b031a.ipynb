{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport csv\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"with open('/kaggle/input/covid2-submissions/days30_normal.csv', 'rt') as infile:\n    with open('submission.csv', 'wt') as outfile:\n        writer = csv.writer(outfile)\n        for line in csv.reader(infile):\n            writer.writerow(line)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import defaultdict\nimport collections\nimport csv\nfrom datetime import datetime, timedelta\nimport glob\nimport logging\nfrom math import *\nimport numpy as np\nimport optparse\nimport os\nimport time\nimport sys\nimport copy\nfrom typing import Any, Dict, List, Optional, Tuple, Union\nimport xgboost as xgb\nfrom catboost import Pool, CatBoostRegressor\nfrom xml.etree import ElementTree\nfrom scipy.optimize import curve_fit\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"RUN = False\nPROD = True\nPROD_DAYS = 30\nEVAL_DAYS = 7","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"POINTS = {}\n\nPIVOTS = []\nPIVOTS.append([43.0, 12.0]) #italy\nPIVOTS.append([32.0, 53.0]) #iran\nPIVOTS.append([30.9756, 112.2707]) #hubei\nPIVOTS.append([36.1162, -119.6816]) #california\nPIVOTS.append([47.4009, -121.4905]) #seattle, WA\nPIVOTS.append([42.1657, -74.9481]) #NY\n\nPoint = collections.namedtuple('Point', ['latitude', 'longitude'])\nPointName = collections.namedtuple('PointName', ['country', 'state'])\nPointData = collections.namedtuple('PointData', ['cases', 'fatalities', 'recovered'])\nCountryData = collections.namedtuple('CountryData', ['population', 'area', 'density', 'coastline', 'migration', 'infant_mortality', 'gdp', 'literacy', 'phones', 'arable', 'crops', 'other', 'climate', 'birthrate', 'deathrate', 'agriculture', 'industry', 'service'])\nCountryData.__new__.__defaults__ = (-1,) * len(CountryData._fields)\n\nFEATURE_NAME_PREFIX = 'F_'\nDATE_FORMAT = '%Y-%m-%d'\n\nUSELESS_FEATURES = set([line.rstrip('\\n') for line in open('/kaggle/input/covidshared/useless_features', 'rt')])\n\nCoordinates = {}\n\nwith open('/kaggle/input/covidshared/points.csv') as f:\n    for line in csv.DictReader(f):\n        key = (line['Province/State'], line['Country/Region'])\n        latitude = float(line['Lat']) if line['Lat'] != '' else 0.0\n        longitude = float(line['Long']) if line['Long'] != '' else 0.0\n        Coordinates[key] = (latitude, longitude)\n        \ndef div(a, b):\n    return -1 if abs(b) < 1e-5 else a / b\n\ndef feature_name(s: str) -> str:\n    return FEATURE_NAME_PREFIX + s\n\ndef is_feature_name(s: str) -> bool:\n    return s.startswith(FEATURE_NAME_PREFIX)\n\ndef date_add_days(date: str, num_days: int) -> str:\n    return (datetime.strptime(date, DATE_FORMAT) + timedelta(days=num_days)).strftime(DATE_FORMAT)\n\ndef date_days_diff(date_start: str, date_end: str):\n    delta = datetime.strptime(date_end, DATE_FORMAT) - datetime.strptime(date_start, DATE_FORMAT)\n    return delta.days\n\ndef in_range(x: Any, full_range: Tuple[Any, Any]) -> bool:\n    return full_range[0] <= x <= full_range[1]\n\ndef in_left_range(x: Any, left_range: Tuple[Any, Any]) -> bool:\n    return left_range[0] <= x < left_range[1]\n\ndef normalize_string(s):\n    return s.strip().lower()\n\ndef haversine(lat1, lon1, lat2, lon2):\n    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n    dlon = lon2 - lon1 \n    dlat = lat2 - lat1 \n    tmp = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n    return asin(sqrt(tmp))\n\nclass CountryMetaData(object):\n    TOTAL_KEY = 'TOTAL'\n    US = 'us'\n    UNK_REGION = 'UNK'\n    CRUISE_SHIP = 'cruise ship'\n\n    def __init__(self) -> None:\n        # Map from country name to \n        self.country_data = defaultdict(CountryData)\n        self.us_data = defaultdict(CountryData)\n        self.region_id = {}\n        self.country_to_region_id = {}\n        self.lockdown = {}\n        self.tests = {}\n        self.not_found = 0\n\n        self.read_world_stats('/kaggle/input/covidshared/world_stats.csv')\n        self.read_us_stats('/kaggle/input/covidshared/us_stats.csv')\n        self.read_lockdown_data('/kaggle/input/covidshared/lockdown.csv')\n        self.read_tests('/kaggle/input/covidshared/tests.html')\n\n    def map_name(self, name: str) -> str:\n        if name == 'the gambia':\n            return normalize_string('Gambia, The')\n        if name == 'trinidad and tobago':\n            return normalize_string('Trinidad & Tobago')\n        if name == 'the bahamas':\n            return normalize_string('Bahamas, The')\n        if name == 'taiwan*':\n            return normalize_string('Taiwan')\n        if name == 'republic of the congo' or name == 'congo (kinshasa)' or name == 'congo (brazzaville)':\n            return normalize_string('Congo, Repub. of the')\n        if name == 'czechia':\n            return normalize_string('Czech Republic')\n        if name == 'central african republic':\n            return normalize_string('Central African Rep.')\n        if name == 'bosnia and herzegovina':\n            return normalize_string('Bosnia & Herzegovina')\n        if name == 'antigua and barbuda':\n            return normalize_string('Antigua & Barbuda')\n        if name == 'north macedonia':\n            return normalize_string('Macedonia')\n        return name\n\n    def read_world_stats(self, file_name) -> None:\n        with open(file_name) as f:\n            reader = csv.DictReader(f)\n            for row in reader:\n                country = normalize_string(row['Country'])\n                population = float(row['Population'])\n                region = normalize_string(row['Region'])\n                area = float(row['Area (sq. mi.)'] or -1)\n                density = population / area\n                coastline = float(row['Coastline (coast/area ratio)'].replace(',','.') or -1)\n                migration = float(row['Net migration'].replace(',','.') or -1)\n                infant_mortality = float(row['Infant mortality (per 1000 births)'].replace(',','.') or -1)\n                gdp = float(row['GDP ($ per capita)'] or -1)\n                literacy = float(row['Literacy (%)'].replace(',','.') or -1)\n                phones = float(row['Phones (per 1000)'].replace(',','.') or -1)\n                arable = float(row['Arable (%)'].replace(',','.') or -1)\n                crops = float(row['Crops (%)'].replace(',','.') or -1)\n                other = float(row['Other (%)'].replace(',','.') or -1)\n                climate = float(row['Climate'].replace(',','.') or -1)\n                birthrate = float(row['Birthrate'].replace(',','.') or -1)\n                deathrate = float(row['Deathrate'].replace(',','.') or -1)\n                agriculture = float(row['Agriculture'].replace(',','.') or -1)\n                industry = float(row['Industry'].replace(',','.') or -1)\n                service = float(row['Service'].replace(',','.') or -1)\n\n                self.country_data[country] = CountryData(\n                  population=population,\n                  area=area, \n                  density=density,\n                  coastline=coastline,\n                  migration=migration,\n                  infant_mortality=infant_mortality,\n                  gdp=gdp,\n                  literacy=literacy,\n                  phones=phones,\n                  arable=arable,\n                  crops=crops,\n                  other=other,\n                  climate=climate,\n                  birthrate=birthrate,\n                  deathrate=deathrate,\n                  agriculture=agriculture,\n                  industry=industry,\n                  service=service)\n\n                total = self.country_data[self.TOTAL_KEY]\n                self.country_data[self.TOTAL_KEY] = CountryData(population=population + total.population)\n\n                if region not in self.region_id:\n                    self.region_id[region] = len(self.region_id) + 1\n                self.country_to_region_id[country] = self.region_id[region]\n\n    def read_us_stats(self, file_name) -> None:\n        us_data = defaultdict(CountryData)\n        with open(file_name) as f:\n            reader = csv.DictReader(f)\n            for row in reader:\n                state = normalize_string(row['Location'])\n                population = float(row['Total'])\n\n                us_data[state] = CountryData(population=population)\n\n        us_stats = self.country_data[self.US]\n        for state, stats in us_data.items():\n            area = us_stats.area / len(us_data)\n            self.us_data[state] = CountryData(\n                population=stats.population,\n                area=area,\n                density = stats.population / area,\n                migration=us_stats.migration,\n                infant_mortality=us_stats.infant_mortality,\n                gdp=us_stats.gdp,\n                literacy=us_stats.literacy,\n                phones=us_stats.phones,\n                arable=us_stats.arable,\n                crops=us_stats.crops,\n                other=us_stats.other,\n                climate=us_stats.climate,\n                birthrate=us_stats.birthrate,\n                deathrate=us_stats.deathrate,\n                agriculture=us_stats.agriculture,\n                industry=us_stats.industry,\n                service=us_stats.service)\n\n    def read_lockdown_data(self, file_name) -> None:\n        us_data = defaultdict(CountryData)\n        with open(file_name) as f:\n            reader = csv.DictReader(f)\n            for row in reader:\n                country = normalize_string(row['country'])\n                state = normalize_string(row['state'])\n                lockdown = row['lockdown_date']\n                self.lockdown[PointName(country=country, state=state)] = lockdown\n\n    def read_tests(self, file_name):\n        with open(file_name) as f:\n            html = f.read().strip().replace('<br>', '')\n\n            trs = ElementTree.XML(html)[0]\n            headers = [td.text for td in trs[0]]\n\n            for i, tr in enumerate(trs):\n                if i == 0 or tr[1].text is None:\n                    continue\n\n                values = [td.text for td in tr]\n                row = dict(zip(headers, values))\n\n                country = normalize_string(row['Country or territory']).split('-')[0].strip()\n                date = datetime.strptime(row['Date'], '%d %b %Y').strftime(DATE_FORMAT)\n                tests = int(row['Total tests'].replace(',', ''))\n\n                if country == 'united states':\n                    country = 'us'\n\n                if country in self.tests:\n                    prev = self.tests[country]\n                    self.tests[country] = (max(date, prev.date), tests + prev.tests)\n                else:\n                    self.tests[country] = (date, tests)\n\n    def _get_data_impl(self, country, state):\n        if country == self.CRUISE_SHIP:\n            if state == 'diamond princess':\n                return CountryData(population=3711)\n\n        if country == self.US:\n            if state in self.country_data:\n                return self.country_data[state]\n\n        if state in self.us_data:\n            return self.us_data[state]\n\n        if country in self.country_data:\n            return self.country_data[country]\n\n        return None\n\n    def get_data(self, country, state):\n        country = self.map_name(normalize_string(country))\n        state = self.map_name(normalize_string(state))\n\n        data = self._get_data_impl(country, state)\n        if data:\n            return data\n\n        self.not_found += 1\n\n        if country == self.US:\n            return CountryData(population=self.us_data[self.US].population / len(self.us_data))\n\n        return CountryData(population=self.country_data[self.TOTAL_KEY].population / len(self.country_data))\n\n    def get_region(self, country):\n        country = self.map_name(normalize_string(country))\n        if country not in self.country_to_region_id:\n            self.region_id[country] = len(self.region_id) + 1\n            self.country_to_region_id[country] = self.region_id[country]\n\n        return self.country_to_region_id[country]\n\n    def _get_days_since_lockdown(self, lockdown: str, today: str) -> int:\n        days_since_lockdown = date_days_diff(lockdown, today)\n        return days_since_lockdown if days_since_lockdown >= 0 else -1\n\n    def get_days_since_lockdown(self, country: str, state: str, today: str) -> int:\n        country = normalize_string(country)\n        state = normalize_string(state)\n        country_state_key = PointName(country=country, state=state)\n        country_key = PointName(country=country, state='')\n        if country_state_key in self.lockdown:\n            return self._get_days_since_lockdown(self.lockdown[country_state_key], today)\n        if country_key in self.lockdown:\n            return self._get_days_since_lockdown(self.lockdown[country_key], today)\n        return -1\n\n    def get_tests_data(self, country: str) -> Optional[Tuple[str, int]]:\n        country = normalize_string(country)\n        return self.tests[country] if country  in self.tests else None\n    \nclass Example(object):\n    HEADER = ['key', 'state', 'country', 'latitude', 'longitude', 'date', 'cases', 'fatalities']\n\n    def __init__(\n      self,\n      key: int,\n      state: str,\n      country: str,\n      latitude: float,\n      longitude: float,\n      date: str,\n      cases: float,\n      fatalities: float) -> None:\n        self.key = key\n        self.state = state\n        self.country = country\n        self.latitude = latitude\n        self.longitude = longitude\n        self.date = date\n        self.cases = cases\n        self.fatalities = fatalities\n        self.recovered = None\n        self.features = {}\n\n    @classmethod\n    def from_dict(cls, row: Dict[str, str]) -> 'Example':\n        if 'Id' in row or 'ForecastId' in row:\n            key = (row['Province_State'], row['Country_Region'])\n            cur_lat, cur_long = Coordinates[key] # if key in Coordinates else (0.0, 0.0)\n            return Example(\n                key=int(row['Id']) if 'Id' in row else int(row['ForecastId']),\n                state=row['Province_State'],\n                country=row['Country_Region'],\n                latitude=cur_lat,\n                longitude=cur_long,\n                date=row['Date'],\n                cases=float(row['ConfirmedCases']) if 'ConfirmedCases' in row else None,\n                fatalities=float(row['Fatalities']) if 'Fatalities' in row else None)\n        elif 'key' in row:\n            e = Example(\n                key=int(row['key']),\n                state=row['state'],\n                country=row['country'],\n                latitude=float(row['latitude']),\n                longitude=float(row['longitude']),\n                date=row['date'],\n                cases=float(row['cases']) if row['cases'] else None,\n                fatalities=float(row['fatalities']) if row['fatalities'] else None)\n\n            for key in row:\n                if is_feature_name(key):\n                    e.features[key] = float(row[key])\n            return e\n        assert False, row\n\n    def to_row(self) -> List[Union[int, float, str]]:\n        row = []\n        for h in self.HEADER:\n            row.append(getattr(self, h))\n\n        for f in self.features:\n            row.append(self.features[f])\n\n        return row\n\n    @property\n    def point(self) -> Point:\n        return Point(latitude=self.latitude, longitude=self.longitude)\n\n    @property\n    def parsed_date(self):\n        return datetime.strptime(self.date, DATE_FORMAT)\n\n    @property\n    def point_name(self) -> PointName:\n        return PointName(country=self.country, state=self.state)\n\n    @property\n    def point_data(self) -> PointData:\n        return PointData(cases=self.cases, fatalities=self.fatalities, recovered=self.recovered)\n\n    def set_feature(self, name: str, value: float) -> None:\n        name = feature_name(name)\n        if name in USELESS_FEATURES:\n            return\n        assert name not in self.features\n        self.features[name] = value\n\n    def get_feature(self, name: str) -> float:\n        return self.features[feature_name(name)]\n\n\nclass PointDataSeries(object):\n    def __init__(self, dataset: List[Example]) -> None:\n        self.point_data_series = collections.defaultdict(dict)\n        self.locations = {}\n        for e in dataset:\n            assert e.date not in self.point_data_series[e.point_name], (e.point_name, e.date)\n            if e.point_name not in self.locations:\n                self.locations[e.point_name] = len(self.locations)\n            self.point_data_series[e.point_name][e.date] = e.point_data\n\n    @property\n    def series_len(self) -> int:\n        res = None\n        for series in self.point_data_series.values():\n            if res:\n                assert res == len(series)\n            else:\n                res = len(series)\n        return res\n\n    @property\n    def date_range(self) -> Tuple[str, str]:\n        res = None\n        for point_name in self.point_data_series:\n            dates = self.point_data_series[point_name].keys()\n            if res:\n                assert res == (min(dates), max(dates))\n            else:\n                res = (min(dates), max(dates))\n        return res\n\n    def get_full_series(self, point_name: PointName, start_date: str) -> List[PointData]:\n        return self.get_series(point_name, start_date, self.series_len)\n\n    def get_series(self, point_name: PointName, start_date: str, num: int) -> List[PointData]:\n        res = []\n        for i in range(num):\n            date = date_add_days(start_date, -i)\n            if date in self.point_data_series[point_name]:\n                res.append(self.point_data_series[point_name][date])\n            else:\n                res.append(None)\n        return res\n\n\nclass GeoZones(object):\n    # A B\n    # C D\n\n    LATITUDE_RANGE = (-90.0, 90.0)\n    LONGITUDE_RANGE = (-180.0, 180.0)\n\n    def __init__(self, points: Dict[PointName, Point], max_zone_points: int) -> None:\n        self.max_zone_points = max_zone_points\n        self.geo_zones = {}\n        self.leaf_geohashes = []\n\n        self.build(points)\n\n        self.point_geohash = {}\n        for geohash in self.leaf_geohashes:\n            for p_name, p in self.geo_zones[geohash].items():\n                self.point_geohash[p_name] = geohash\n        assert set(points.keys()) == set(self.point_geohash.keys())\n\n    def build(self, \n      points: Dict[PointName, Point],\n      geohash: str = '',\n      latitude_range: Tuple[float, float] = LATITUDE_RANGE,\n      longitude_range: Tuple[float, float] = LONGITUDE_RANGE) -> None:\n        for p in points.values():\n            assert in_left_range(p.latitude, latitude_range), (p, latitude_range)\n            assert in_left_range(p.longitude, longitude_range), (p, longitude_range)\n\n        self.geo_zones[geohash] = points\n        if len(points) <= self.max_zone_points:\n            self.leaf_geohashes.append(geohash)\n            return\n\n        quadrants = self._get_quadrants(latitude_range, longitude_range)\n        for c, params in quadrants.items():\n            self.build(\n                points={\n                  p_name: p\n                  for p_name, p in points.items()\n                  if in_left_range(p.latitude, params['latitude_range']) \n                  and in_left_range(p.longitude, params['longitude_range'])\n                },\n                geohash=geohash + c,\n                latitude_range=params['latitude_range'],\n                longitude_range=params['longitude_range'])\n\n    def _get_center_point(\n      self,\n      latitude_range: Tuple[float, float],\n      longitude_range: Tuple[float, float]) -> Point:\n        return Point(\n          latitude=(latitude_range[0] + latitude_range[1]) / 2.0,\n          longitude=(longitude_range[0] + longitude_range[1]) / 2.0)\n\n    def _get_quadrants(\n      self,\n      latitude_range: Tuple[float, float],\n      longitude_range: Tuple[float, float]) -> Dict[str, Dict[str, Tuple[float, float]]]:\n        center = self._get_center_point(latitude_range, longitude_range)\n        return {\n          'A': {\n            'latitude_range': (latitude_range[0], center.latitude),\n            'longitude_range': (longitude_range[0], center.longitude),\n          },\n          'B': {\n            'latitude_range': (center.latitude, latitude_range[1]),\n            'longitude_range': (longitude_range[0], center.longitude),\n          },\n          'C': {\n            'latitude_range': (latitude_range[0], center.latitude),\n            'longitude_range': (center.longitude, longitude_range[1]),\n          },\n          'D': {\n            'latitude_range': (center.latitude, latitude_range[1]),\n            'longitude_range': (center.longitude, longitude_range[1]),\n          },\n        }\n\n    def _get_same_zone_points_impl(\n      self,\n      p: Point,\n      geohash: str = '',\n      latitude_range: Tuple[float, float] = LATITUDE_RANGE,\n      longitude_range: Tuple[float, float] = LONGITUDE_RANGE) -> Dict[PointName, Point]:\n        assert geohash in self.geo_zones\n\n        quadrants = self._get_quadrants(latitude_range, longitude_range)\n        for c, params in quadrants.items():\n            if in_left_range(p.latitude, params['latitude_range']) and in_left_range(p.longitude, params['longitude_range']):\n                if geohash + c in self.geo_zones:\n                    return self._get_same_zone_points_impl(\n                        p=p,\n                        geohash=geohash + c,\n                        latitude_range=params['latitude_range'],\n                        longitude_range=params['longitude_range'])\n                else:\n                    return self.geo_zones[geohash]\n\n        assert False\n\n\ndef read_dataset(path: str) -> List[Example]:\n    dataset = []\n    with open(path) as f:\n        reader = csv.DictReader(f, quotechar='\"', delimiter=',')\n        for row in reader:\n            e = Example.from_dict(row)\n            dataset.append(e)\n            if e.point_name in POINTS:\n                assert POINTS[e.point_name] == e.point\n            else:\n                POINTS[e.point_name] = e.point\n\n    return dataset\n\ndef write_features(path: str, dataset: List[Example]) -> None:\n    feature_names = []\n    for e in dataset:\n        if feature_names:\n            assert feature_names == list(e.features.keys())\n        else:\n            feature_names = list(e.features.keys())\n\n    with open(path, 'w') as f:\n        writer = csv.writer(f, lineterminator='\\n', delimiter=',')\n        writer.writerow(Example.HEADER + feature_names)\n        for e in dataset:\n            writer.writerow(e.to_row())\n\ndef first_n_infected(element, pds, n, predict_window):\n    idx = predict_window\n    pds = pds.point_data_series\n\n    while True:\n        cur_date = date_add_days(element.date, -idx)\n        if element.point_name not in pds: return -1\n        e = pds[element.point_name]\n        if cur_date not in e: return -1\n        if e[cur_date].cases < n: return idx\n        idx += 1\n    \n\ndef prev(element, pds, n, predict_window):\n    pds = pds.point_data_series\n    result = []\n    date = element.date\n    for i in range(n):\n        cur_date = date_add_days(date, -(i + predict_window))\n        cases, fatalities, recovered = 0, 0, 0\n        if element.point_name in pds and cur_date in pds[element.point_name]:\n            e = pds[element.point_name][cur_date]\n            if e.cases:\n                cases = e.cases\n            if e.fatalities:\n                fatalities = e.fatalities\n            if e.recovered:\n                recovered = e.recovered\n\n        result.append(PointData(cases=cases, fatalities=fatalities, recovered=recovered))\n    return result\n\ndef fit_and_predict(cases, predict_window):\n    cases = cases[::-1]\n    x, y, logy = [], [], []\n    for i in range(len(cases)):\n        x.append(i+1)\n        y.append(cases[i])\n        logy.append(log(1 + cases[i]))\n    x, y, logy = map(np.asarray, [x, y, logy])\n    z = np.poly1d(np.polyfit(x, y, 3))\n    logz = np.poly1d(np.polyfit(x, logy, 3))\n    return z(x[-1] + predict_window), logz(x[-1] + predict_window)\n\ndef fit_delta(cases, predict_window):\n    cases = [log(1 + x) for x in cases[::-1]]\n    x, y = [], []\n    for i in range(len(cases) - 1):\n        x.append(i+1)\n        y.append(cases[i + 1] - cases[i])\n    x, y = map(np.asarray, [x, y])\n    z3 = np.poly1d(np.polyfit(x, y, 3))\n    z1 = np.poly1d(np.polyfit(x, y, 1))\n    return sum([z3(x[-1] + i + 1) for i in range(predict_window)]), sum([z1(x[-1] + i + 1) for i in range(predict_window)])\n\ndef delta_embedding(example, pds, predict_window, size, use_cases):\n    pds = pds.point_data_series\n    dates = [date_add_days(example.date, -predict_window)]\n    for i in range(size):\n        dates.append(date_add_days(dates[-1], -1))\n    dates = dates[::-1]\n    emb = []\n    for d in dates:\n        if example.point_name in pds and d in pds[example.point_name]:\n            e = pds[example.point_name][d]\n            emb.append(log(1 + (e.cases if use_cases else e.fatalities)))\n        else:\n            emb.append(0)\n    return [emb[i + 1] - emb[i] for i in range(len(emb) - 1)]\n\ndef cluster_distance(emb, cluster_center):\n    return sqrt(sum([(emb[i] - cluster_center[i]) ** 2 for i in range(len(emb))]))\n\ndef build_emb_bayes(dataset, pds, predict_window):\n    emb_bayes = defaultdict(list)\n    for e in dataset:\n        emb_cases = delta_embedding(e, pds, predict_window, 5, True)\n        prev_value = prev(e, pds, 1, predict_window)[0]\n        delta_diff = (log(1.0 + e.cases) - log(1.0 + prev_value.cases), log(1.0 + e.fatalities) - log(1.0 + prev_value.fatalities))\n        emb_bayes[e.date].append((emb_cases, e.point_data, delta_diff, e.country))\n    return emb_bayes\n\ndef build_features(dataset: List[Example], country_data: CountryMetaData, pds: PointDataSeries, predict_window: int, emb_bayes) -> List[Example]:\n    for e in dataset:\n        e_emb_cases = delta_embedding(e, pds, predict_window, 5, True)\n\n        best_cases = None\n        best_delta = None\n        best_dist_cases = None\n    \n        for i in range(10):\n            date = date_add_days(e.date, -predict_window - i)\n            assert date < e.date\n            if date in emb_bayes:\n                for e2_emb_cases, point_data, delta_diff, country in emb_bayes[date]:\n                    dist_cases = cluster_distance(e_emb_cases, e2_emb_cases)\n                    if best_dist_cases is None or best_dist_cases > dist_cases:\n                        best_dist_cases = dist_cases\n                        best_cases = point_data\n                        best_delta = delta_diff\n           \n        prev_values = prev(e, pds, 1, predict_window)\n        if best_cases is None:\n            e.set_feature('emb_bayes_cases_cases', -1)\n            e.set_feature('emb_bayes_cases_fatal', -1)\n            e.set_feature('emb_bayes_cases_cases_delta', -1)\n            e.set_feature('emb_bayes_cases_fatal_delta', -1)\n            e.set_feature('emb_bayes_cases_cases_delta_diff', 0)\n            e.set_feature('emb_bayes_cases_fatal_delta_diff', 0)\n        else:\n            e.set_feature('emb_bayes_cases_cases', best_cases.cases)\n            e.set_feature('emb_bayes_cases_fatal', best_cases.fatalities)\n            e.set_feature('emb_bayes_cases_cases_delta', log(1 + best_cases.cases) - log(1 + prev_values[0].cases))\n            e.set_feature('emb_bayes_cases_fatal_delta', log(1 + best_cases.fatalities) - log(1 + prev_values[0].fatalities))\n            e.set_feature('emb_bayes_cases_cases_delta_diff', best_delta[0])\n            e.set_feature('emb_bayes_cases_fatal_delta_diff', best_delta[1])\n     \n    for e in dataset:\n        location_index = pds.locations[e.point_name]\n        for i in range(294):\n            e.set_feature('L' + str(i), 1 if i == location_index else 0)\n        for i in range(32):\n            e.set_feature('predict_window_' + str(i), 1 if (i + 1) == predict_window else 0)\n        e.set_feature('state_len', len(e.state))\n        e.set_feature('country_len', len(e.country))\n        e.set_feature('latitude', e.latitude / 90.0)\n        e.set_feature('longitude', e.longitude / 180.0)\n\n        data = country_data.get_data(e.country, e.state)\n        e.set_feature('country_population', data.population)\n        # e.set_feature('country_population_log', log(data.population) / 25.0)\n        # e.set_feature('country_area', data.area)\n        # e.set_feature('country_density', data.density)\n        # e.set_feature('country_coastline', data.coastline)\n        # e.set_feature('country_migration', data.migration)\n        # e.set_feature('country_infant_mortality', data.infant_mortality)\n        # e.set_feature('country_gdp', data.gdp)\n        # e.set_feature('country_literacy', data.literacy)\n        # e.set_feature('country_phones', data.phones)\n        # e.set_feature('country_arable', data.arable)\n        # e.set_feature('country_crops', data.crops)\n        # e.set_feature('country_other', data.other)\n        # e.set_feature('country_climate', data.climate)\n        # e.set_feature('country_birthrate', data.birthrate)\n        # e.set_feature('country_deathrate', data.deathrate)\n        # e.set_feature('country_agriculture', data.agriculture)\n        # e.set_feature('country_industry', data.industry)\n        # e.set_feature('country_service', data.service)\n\n        #cluster_info = point2cluster[e.point_name]\n        #e.set_feature('clusterid', cluster_info[0])\n        #for i in range(len(cluster_info[1])):\n        #  e.set_feature('cluster_distance_' + str(i), cluster_info[1][i])\n\n        #pivots\n        #for i in range(len(PIVOTS)):\n        #  dist = haversine(PIVOTS[i][0], PIVOTS[i][1], e.latitude, e.longitude)\n        #  e.set_feature('pivots_dist_' + str(i), dist)\n\n        tests_data = country_data.get_tests_data(e.country)\n\n        weekday = e.parsed_date.weekday()\n        assert 0 <= weekday < 7\n        for i in range(7):\n            e.set_feature('day_' + str(i), int(i == weekday))\n\n        #for n in [1, 10, 100, 1000]:\n        #  e.set_feature('first_' + str(n), first_n_infected(e, pds, n, predict_window))\n\n        prev_values = prev(e, pds, 50, predict_window)\n        for i in [10, 20, 50]:\n            fit = fit_and_predict([x.cases for x in prev_values[:i]], predict_window)\n            e.set_feature('fit_' + str(i), fit[0])\n            e.set_feature('fitlog_' + str(i), fit[1])\n            fit = fit_and_predict([x.fatalities for x in prev_values[:i]], predict_window)\n            e.set_feature('fit_fatal_' + str(i), fit[0])\n            e.set_feature('fitlog_fatal_' + str(i), fit[1])\n            fit = fit_delta([x.cases for x in prev_values[:i]], predict_window)\n            e.set_feature('fit_delta3_cases_' + str(i), fit[0])\n            e.set_feature('fit_delta1_cases_' + str(i), fit[1])\n            fit = fit_delta([x.fatalities for x in prev_values[:i]], predict_window)\n            e.set_feature('fit_delta3_fatal_' + str(i), fit[0])\n            e.set_feature('fit_delta1_fatal_' + str(i), fit[1])\n        \n        #res = fit_exp([x.cases for x in prev_values], predict_window, e.country, e.date)\n        #fit_gauss([x.cases for x in prev_values], [x.fatalities for x in prev_values], predict_window, e.country, e.date)\n        #e.set_feature('fitexp', res[0])\n        #e.set_feature('fitexp_b', res[1])\n        #e.set_feature('fitexp_e', res[2])\n\n        e.set_feature('infected_share', div(prev_values[0].cases, data.population))\n        e.set_feature('mortality_share', div(prev_values[0].fatalities, prev_values[0].cases))\n        # e.set_feature('mortality_to_recovered_share', div(prev_values[0].fatalities, prev_values[0].recovered))\n\n        e.set_feature('recovered', log(1 + prev_values[0].recovered))\n        e.set_feature('recovered_share', div(prev_values[0].recovered, prev_values[0].cases))\n        e.set_feature('headroom', prev_values[0].cases - prev_values[0].fatalities - prev_values[0].recovered)\n        # e.set_feature('headroom_share', div(prev_values[0].cases - prev_values[0].fatalities - prev_values[0].recovered, prev_values[0].cases))\n\n        for i in range(20):\n            if i > 0:\n                e.set_feature('prev_cases_speed_' + str(i+1), -log(1 + prev_values[i].cases) + log(1 + prev_values[0].cases))\n                e.set_feature('prev_cases_speed_window_' + str(i+1), (log(1 + prev_values[0].cases) - log(1 + prev_values[i].cases)) / i * predict_window)\n                e.set_feature('prev_fatal_speed_' + str(i+1), -log(1 + prev_values[i].fatalities) + log(1 + prev_values[0].fatalities))\n                e.set_feature('prev_fatal_speed_window_' + str(i+1), (log(1 + prev_values[0].fatalities) - log(1 + prev_values[i].fatalities)) / i * predict_window)\n            if i < 10:\n                e.set_feature('prev_cases_' + str(i+1), log(1 + prev_values[i].cases))\n                e.set_feature('prev_cases_accum_' + str(i+1), prev_values[i].cases)\n                e.set_feature('prev_fatal_' + str(i+1), log(1 + prev_values[i].fatalities))\n                e.set_feature('prev_fatal_accum_' + str(i+1), prev_values[i].fatalities)\n\n        e.set_feature('region', country_data.get_region(e.country))\n        e.set_feature('days_since_lockdown', country_data.get_days_since_lockdown(e.country, e.state, date_add_days(e.date, -predict_window)))\n\n    return dataset\n\ndef build_geohash_features(dataset: List[Example], gz: GeoZones) -> List[Example]:\n    F_PREFIXES = [feature_name('prev_cases_'), feature_name('prev_fatal_'), feature_name('infected_share')]\n\n    feature_lists = defaultdict(lambda: defaultdict(list))\n    for e in dataset:\n        gh = gz.point_geohash[e.point_name]\n        for fp in F_PREFIXES:\n            for f_name, f_value in e.features.items():\n                if f_name.startswith(fp):\n                    feature_lists[gh][f_name].append(f_value)\n\n    feature_stats = defaultdict(dict)\n    for gh, feature_values in feature_lists.items():\n        for f_name, f_values in feature_values.items():\n            feature_stats[gh][f_name] = {\n                'min': min(f_values),\n                'max': max(f_values),\n                'sum': 1.0 * sum(f_values) / len(f_values),\n                'cnt': len(f_values),\n              }\n\n    for e in dataset:\n        gh = gz.point_geohash[e.point_name]\n        e.set_feature('geohash', hash(gh))\n        for f_name, f_stats in feature_stats[gh].items():\n            e.set_feature('geozone_min_' + f_name, f_stats['min'])\n            e.set_feature('geozone_max_' + f_name, f_stats['max'])\n            e.set_feature('geozone_sum_' + f_name, f_stats['sum'])\n            e.set_feature('geozone_cnt_' + f_name, f_stats['cnt'])\n    return dataset\n\ndef main_FeaturesStage(opts):\n    start = time.time()\n    raw_train_set = read_dataset(opts.train_csv)\n    raw_test_set = read_dataset(opts.test_csv)\n    country_data = CountryMetaData()\n    #reports = read_reports()\n  \n    pds = PointDataSeries(raw_train_set)\n    data_date_range = pds.date_range\n  \n    gz12 = GeoZones(points=POINTS, max_zone_points=12)\n  \n    test_date_range = (date_add_days(opts.start_date, 1 - opts.num_days), opts.start_date)\n    train_date_range = (data_date_range[0], date_add_days(test_date_range[0], -1))\n    no_data_test_date_range = (date_add_days(data_date_range[1], 1), test_date_range[1])\n\n    train_set = [e for e in raw_train_set if in_range(e.date, train_date_range)]\n    test_set = [e for e in raw_train_set if in_range(e.date, test_date_range) and date_add_days(e.date, -opts.predict_window) == date_add_days(opts.start_date, -opts.num_days)]\n    if no_data_test_date_range[0] <= no_data_test_date_range[1]:\n        test_set += [e for e in raw_test_set if in_range(e.date, no_data_test_date_range) and date_add_days(e.date, -opts.predict_window) == date_add_days(opts.start_date, -opts.num_days)]\n\n    if opts.last_n_days != -1:\n        train_set = train_set[-opts.last_n_days:]\n        test_set = test_set[-opts.last_n_days:]\n    # point2cluster = kmeans(pds, opts.predict_window, [x for x in raw_train_set if x.date == date_add_days(opts.start_date, -opts.num_days)])\n\n    emb_bayes = build_emb_bayes(raw_train_set, pds, opts.predict_window)\n\n    #import_reports_data(train_set, reports)\n\n    train_features = build_features(train_set, country_data, pds, opts.predict_window, emb_bayes)\n    train_features = build_geohash_features(train_features, gz12)\n    write_features(opts.train_features_csv, train_features)\n    train_features = []\n  \n    test_features = build_features(test_set, country_data, pds, opts.predict_window, emb_bayes)\n    test_features = build_geohash_features(test_features, gz12)\n    write_features(opts.test_features_csv, test_features)\n    end = time.time()\n    print('Took {} seconds'.format(end - start))\n    if opts.verbose:\n        print('Info:')\n        print('  Data series len: {}'.format(pds.series_len))\n        print('  Data date range: {}'.format(data_date_range))\n        print('  Num geo-points: {}'.format(len(POINTS)))\n        print('  Num geo-zones (up to 10 points): {}'.format(len(gz12.leaf_geohashes)))\n        print('  Num features: {}'.format(len(train_set[0].features)))\n        print('  Train date range: {}'.format(train_date_range))\n        print('  Test date range: {}'.format(test_date_range))\n        print('  Num train examples: {}'.format(len(train_features)))\n        print('  Num test examples: {}'.format(len(test_features)))\n        if no_data_test_date_range[0] <= no_data_test_date_range[1]:\n            print('  Test date range (no data): {}'.format(no_data_test_date_range))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Learning\nimport math\ndef read_learn_datasets(path, days):\n    examples = []\n    for i in range(days):\n        examples.extend(read_dataset(path + '_' + str(i+1) + '.csv'))\n    return examples\n\ndef get_pool_data(mode, data, prefix):\n    def get(e):\n        return e if e else 0.0\n    x, delta_y, abs_y, fit, prev = [], [], [], [], []\n    for e in data[prefix + 'examples']:\n        if mode == 'cases':\n            cur_prev, cur_fit, cur_y = e.features['F_prev_cases_1'], e.features['F_fit_20'], get(e.cases)\n        else:\n            cur_prev, cur_fit, cur_y = e.features['F_prev_fatal_1'], e.features['F_fit_fatal_20'], get(e.fatalities)\n        cur_y = math.log(1 + cur_y)\n        abs_y.append(cur_y)\n        delta_y.append(cur_y - cur_prev)\n        x.append(list(e.features.values()))\n        fit.append(cur_fit)\n        prev.append(cur_prev)\n        data['names'] = list(e.features.keys())\n    data[prefix + 'x'] = np.asarray(x)\n    data[prefix + 'delta_y'] = np.asarray(delta_y)\n    data[prefix + 'abs_y'] = np.asarray(abs_y)\n    data[prefix + 'fit'] = fit\n    data[prefix + 'prev'] = prev\n\ndef read_everything(opts):\n    data = {}  \n    data['train_examples'] = read_learn_datasets(opts.train_features_csv, opts.days)\n    data['test_examples'] = read_learn_datasets(opts.test_features_csv, opts.days)\n    get_pool_data(opts.mode, data, 'train_')\n    get_pool_data(opts.mode, data, 'test_')\n    return data\n\ndef get_pool(data, prefix, is_delta, is_catboost):\n    y_key = prefix + 'delta_y' if is_delta else prefix + 'abs_y'\n    if is_catboost:\n        return Pool(data[prefix + 'x'], data[y_key])\n    return xgb.DMatrix(data[prefix + 'x'], label=data[y_key])\n\ndef write_predictions(path, data, mode, prefix, predicted, is_delta, name):\n    with open('_'.join([path, name, mode]) + '.csv', 'w') as f:\n        writer = csv.writer(f, lineterminator='\\n', delimiter=',')\n        writer.writerow(['key', 'state', 'country', 'date', 'y', 'prediction', 'fit20', 'prev'])\n        examples, y, fit, prev = data[prefix + 'examples'], data[prefix + 'abs_y'], data[prefix + 'fit'], data[prefix + 'prev']\n        for i in range(len(examples)):\n            e = examples[i]\n            pred = predicted[i]\n            if is_delta:\n                pred += prev[i]\n            writer.writerow([e.key, e.state, e.country, e.date, y[i], max(pred, 0), fit[i], prev[i]])\n\ndef model_name(is_delta, is_linear, is_catboost):\n    prefix = 'delta' if is_delta else 'abs'\n    if is_linear:   return prefix + '_linear'\n    if is_catboost: return prefix + '_catboost'\n    return prefix + '_trees'\n\ndef do_learn(data, opts, is_delta, is_linear, is_catboost, iterations):\n    train = get_pool(data, 'train_', is_delta, is_catboost)\n    test  = get_pool(data, 'test_', is_delta, is_catboost)\n  \n    if is_catboost:\n        #model = CatBoostRegressor(iterations=opts.iterations, depth=opts.depth, learning_rate=opts.eta, loss_function='RMSE', logging_level='Verbose')\n        #model.fit(train, eval_set=test)\n        return\n    else:\n        param = {'nthread' : 16, 'objective' : 'reg:squarederror', 'seed' : opts.seed, 'eta' : opts.eta}\n        if is_linear:\n            param['booster'] = 'gblinear'\n            param['alpha'] = 0.00001\n        else:\n            param['max_depth'] = opts.depth\n            param['subsample'] = 0.8\n        evallist = [(test, 'eval'), (train, 'train')] if opts.verbose else []\n        booster = xgb.train(param, train, 2 if opts.super_quick else iterations, evallist)\n        train_predicted, test_predicted = booster.predict(train), booster.predict(test)\n        #if opts.dump:\n            #dump_model('data/model.txt', data['names'], booster)\n  \n    write_predictions(opts.train_predicted_csv, data, opts.mode, 'train_', train_predicted, is_delta, model_name(is_delta, is_linear, is_catboost))\n    write_predictions(opts.test_predicted_csv,  data, opts.mode, 'test_',  test_predicted,  is_delta, model_name(is_delta, is_linear, is_catboost))\n    #if not is_linear and not is_catboost: print_importance(opts.importance, booster, data['names'])\n\ndef main_LearningStage(opts):\n    data = read_everything(opts)\n\n    iterations = {}\n    iterations[('delta', 'trees', 'cases')] = 170\n    iterations[('delta', 'trees', 'fatal')] = 260\n    iterations[('abs', 'trees', 'cases')] = 210\n    iterations[('abs', 'trees', 'fatal')] = 260\n    iterations[('delta', 'linear', 'cases')] = 4\n    iterations[('delta', 'linear', 'fatal')] = 10\n    iterations[('abs', 'linear', 'cases')] = 40\n    iterations[('abs', 'linear', 'fatal')] = 10\n    \n    for is_linear in [False, True]:\n        for is_delta in [True, False]:\n            key = ('delta' if is_delta else 'abs', 'linear' if is_linear else 'trees', opts.mode)\n            print('__'.join(['Key'] + list(key)))\n            do_learn(data, opts, is_delta, is_linear, False, iterations[key])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_final_score(dt, at, dl, al, weights):\n    prev = float(dt['prev'])\n    dt, at, dl, al = map(lambda x: float(x['prediction']), [dt, at, dl, al])\n    pred = 0\n    pred += weights['wdt'] * dt\n    pred += weights['wat'] * at\n    pred += weights['wdl'] * dl\n    pred += weights['wal'] * al\n    return max(pred, prev)\n\ndef coeff():\n    return [0.0, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.5, 0.75, 0.85, 0.9, 1.0]\n\ndef ordered_loc_data(loc_data):\n    cur = [[k, loc_data[k]] for k in loc_data.keys()]\n    cur.sort(key=lambda x: x[0])\n    return [x[1] for x in cur]\n\ndef get_for_country(weights, values):\n    error, n = 0.0, 0\n    for v in values:\n        sc, sf = v['scores_cases'], v['scores_fatal']\n        cases = sc[0] * weights['wdt'] + sc[1] * weights['wat'] + sc[2] * weights['wdl'] + sc[3] * weights['wal']\n        fatal = sf[0] * weights['wdt'] + sf[1] * weights['wat'] + sf[2] * weights['wdl'] + sf[3] * weights['wal']\n        n += 2\n        error += (v['y_cases'] - cases) ** 2\n        fatal += (v['y_fatal'] - fatal) ** 2\n    return error\n\ndef prepare_index(examples_delta_trees, examples_abs_trees, examples_delta_linear, examples_abs_linear, opts, weights, country_weights):\n    index = {}\n    for e_dt, e_at, e_dl, e_al in zip(examples_delta_trees, examples_abs_trees, examples_delta_linear, examples_abs_linear):\n        key = '__'.join(map(str, [e_dt['country'], e_dt['state']]))\n        if country_weights is None:\n            pred = get_final_score(e_dt, e_at, e_dl, e_al, weights)\n        else:\n            pred = get_final_score(e_dt, e_at, e_dl, e_al, country_weights[key])\n        date = e_dt['date']\n        if key not in index: index[key] = {}\n        loc_index = index[key]\n        if date not in loc_index: loc_index[date] = {}\n        dct = loc_index[date]\n        dct['pred_' + e_dt['mode']] = pred\n        dct['y_'    + e_dt['mode']] = float(e_dt['y'])\n        dct['obj_'  + e_dt['mode']] = e_dt\n        dct['prev_' + e_dt['mode']] = float(e_dt['prev'])\n        dct['scores_' + e_dt['mode']] = list(map(lambda x: float(x['prediction']), [e_dt, e_at, e_dl, e_al]))\n\n    for key in index.keys():\n        index[key] = ordered_loc_data(index[key])\n    return index\n\ndef apply_rules(loc_data, min_mortality, max_mortality):\n    prev_cases = []\n    prev_fatalities = []\n    \n    for v in loc_data:\n        cases, fatalities = v['pred_cases'], v['pred_fatal']\n      \n        if len(prev_cases) > 0 and cases < prev_cases[-1]:\n            cases = prev_cases[-1]\n      \n        if len(prev_fatalities) > 0 and fatalities < prev_fatalities[-1]:\n            fatalities = prev_fatalities[-1]\n      \n        if cases - fatalities > min_mortality:\n            fatalities = cases - min_mortality\n        if cases - fatalities < max_mortality and cases > 5:\n            fatalities = cases - max_mortality\n        v['final_cases'] = max(cases, v['prev_cases'])\n        v['final_fatal'] = max(fatalities, v['prev_fatal'])\n        prev_cases.append(v['final_cases'])\n        prev_fatalities.append(v['final_fatal'])\n\ndef copy_and_set_prediction(e, pred, lst):\n    tmp = copy.deepcopy(e)\n    tmp['prediction'] = pred\n    lst.append(tmp)\n\ndef calc_error(examples_delta_trees, examples_abs_trees, examples_delta_linear, examples_abs_linear, opts, weights, country_weights):\n    index = prepare_index(examples_delta_trees, examples_abs_trees, examples_delta_linear, examples_abs_linear, opts, weights, country_weights)\n    for loc_data in index.values():\n        apply_rules(loc_data, math.log(opts.min_mortality), math.log(opts.max_mortality))\n    \n    error = 0\n    cases_list, fatalities_list = [], []\n    n = 0\n    for loc_data in index.values():\n        for v in loc_data:\n            if not opts.only_fatal:\n                n += 1\n                error += (v['final_cases'] - v['y_cases']) ** 2\n            if not opts.only_cases:\n                n += 1\n                error += (v['final_fatal'] - v['y_fatal']) ** 2\n            copy_and_set_prediction(v['obj_cases'], v['final_cases'], cases_list)\n            copy_and_set_prediction(v['obj_fatal'], v['final_fatal'], fatalities_list)\n  \n    return (error / n) ** 0.5, cases_list, fatalities_list\n\ndef read_examples(pattern, mode):\n    result = []\n    with open('{}_{}.csv'.format(pattern, mode), 'r') as f:\n        reader = csv.DictReader(f, quotechar='\"', delimiter=',')\n        for row in reader:\n            row['mode'] = mode\n            result.append(row)\n    return result\n\ndef read_examples_for_model(opts, model_name):\n    test_examples = []\n    test_examples.extend(read_examples(opts.predicted_pattern + model_name, \"cases\"))\n    test_examples.extend(read_examples(opts.predicted_pattern + model_name, \"fatal\"))\n    return test_examples\n\ndef write_final_predictions(path, lst):\n    with open(path, 'wt') as f:\n        w = csv.writer(f)\n        w.writerow(lst[0].keys())\n        for e in lst:\n            w.writerow([e[k] for k in e.keys()])\n\ndef print_error(error, opts, w, win):\n    s = 'Test error = {}, Win = {}. min_mortality={}, max_mortality={}, formula = {} * DT + {} * AT + {} * DL + {} * AL'\n    return s.format(error, win, opts.min_mortality, opts.max_mortality, w['wdt'], w['wat'], w['wdl'], w['wal'])\n\ndef main_FinalizingStage(opts):\n    examples_delta_trees  = read_examples_for_model(opts, '_delta_trees')\n    examples_abs_trees    = read_examples_for_model(opts, '_abs_trees')\n    examples_delta_linear = read_examples_for_model(opts, '_delta_linear')\n    examples_abs_linear   = read_examples_for_model(opts, '_abs_linear')\n\n    country_weights = None #read_country_weights(opts.country_weights)\n    weights = {'wdt' : opts.wdt, 'wat' : opts.wat, 'wdl' : opts.wdl, 'wal' : opts.wal}\n    prod_error, cases_list, fatalities_list = calc_error(examples_delta_trees, examples_abs_trees, examples_delta_linear, examples_abs_linear, opts, weights, country_weights)\n    print(prod_error)\n    write_final_predictions(opts.predicted_pattern + '_cases.csv', cases_list)\n    write_final_predictions(opts.predicted_pattern + '_fatalities.csv', fatalities_list)\n  \n    #if opts.optimize:\n    #errors = []\n    #for wdt in coeff():\n    #  for wat in coeff():\n    #    if wdt + wat > 1: break\n    #    for wdl in coeff():\n    #      wal = 1.0 - wdt - wat - wdl\n    #      if wal < 0: break\n    #      weights = {'wdt' : wdt, 'wat' : wat, 'wdl' : wdl, 'wal' : wal}\n    #      test_error, _, __ = calc_error(examples_delta_trees, examples_abs_trees, examples_delta_linear, examples_abs_linear, opts, weights, country_weights)\n    #      errors.append([test_error, print_error(test_error, opts, weights, prod_error - test_error)])\n    #errors.sort(key=lambda x: x[0])\n    #for e in errors:\n    #  print(e[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Run week2.py\ndef main_Week2Adjustment(opts):\n    data = {}\n    with open(opts.train, 'r') as f:\n        reader = csv.DictReader(f, quotechar='\"', delimiter=',')\n        for row in reader:\n            key = '__'.join([row['Province_State'], row['Country_Region']])\n            if key not in data: data[key] = [-1, -1, -1, -1]\n            if row['Date'] == '2020-03-27':\n                data[key][0] = math.log(1 + float(row['ConfirmedCases']))\n                data[key][1] = math.log(1 + float(row['Fatalities']))\n            if row['Date'] == '2020-03-31':\n                data[key][2] = math.log(1 + float(row['ConfirmedCases']))\n                data[key][3] = math.log(1 + float(row['Fatalities']))\n        hack = {}\n        with open(opts.cases_predicted_csv, 'r') as f:\n            writer = csv.writer(open(opts.modified_cases, 'wt'))\n            reader = csv.DictReader(f, quotechar='\"', delimiter=',')\n            last_country = 'None'\n            writer.writerow(['key','state','country','date','prediction', 'fit20'])\n            for row in reader:\n                key = '__'.join([row['state'], row['country']])\n                val = data[key]\n                if last_country != key:\n                    last_country = key\n                    last_value = data[key][2]\n                    delta = (val[2] - val[0]) / 4\n                    delta = max(0, delta)\n                else:\n                    delta *= 0.925\n                pred = float(row['prediction'])\n                at_most = last_value + delta\n                pred = min(pred, at_most)\n                last_value = pred\n                hack['__'.join([row['state'], row['country'], row['date']])] = pred\n                writer.writerow([row['key'], row['state'], row['country'], row['date'], pred, 0])\n\n        with open(opts.fatal_predicted_csv, 'r') as f:\n            writer = csv.writer(open(opts.modified_fatal, 'wt'))\n            reader = csv.DictReader(f, quotechar='\"', delimiter=',')\n            last_country = 'None'\n            writer.writerow(['key','state','country','date','prediction', 'fit20'])\n            for row in reader:\n                key = '__'.join([row['state'], row['country']])\n                val = data[key]\n                if last_country != key:\n                    last_country = key\n                    last_value = data[key][3]\n                    delta = (val[2] - val[0]) / 4 # not a bug\n                    delta = max(0, delta)\n                else:\n                    delta *= 0.93\n                pred = float(row['prediction'])\n                at_most = last_value + delta\n                pred = min(pred, at_most)\n                cases_value = hack['__'.join([row['state'], row['country'], row['date']])]\n                if pred < cases_value - opts.max_mortality_log:\n                    pred = cases_value - opts.max_mortality_log\n                last_value = pred\n                writer.writerow([row['key'], row['state'], row['country'], row['date'], pred, 0])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def read_one(path):\n    res = []\n    with open(path, 'r') as f:\n        for row in csv.DictReader(f, quotechar='\"', delimiter=','):\n            res.append(row)\n    return res\n\ndef main_Submission(opts):\n    submission = read_one(opts.submission)\n    cases = read_one(opts.predicted_cases)\n    fatal = read_one(opts.predicted_fatalities)\n  \n    cases_index = {}\n    for case in cases:\n        cases_index[case['state'] + '__' + case['country'] + '__' + case['date']] = math.exp(float(case['prediction'])) - 1\n    fatal_index = {}\n    for case in fatal:\n        fatal_index[case['state'] + '__' + case['country'] + '__' + case['date']] = math.exp(float(case['prediction'])) - 1\n    n, found = 0, 0\n    with open(opts.output, 'w') as f:\n        writer = csv.writer(f, lineterminator='\\n', delimiter=',')\n        writer.writerow(['ForecastId', 'ConfirmedCases', 'Fatalities'])\n        for rec in submission:\n            key = rec['Province_State'] + '__' + rec['Country_Region'] + '__' + rec['Date']\n            cases_answer, fatal_answer = 0, 0\n            n += 1\n            if key in cases_index:\n                found += 1\n                cases_answer, fatal_answer = cases_index[key], fatal_index[key]\n            writer.writerow([rec['ForecastId'], cases_answer, fatal_answer])\n    print('N={}'.format(n))\n    print('Found={}'.format(found))\n    print('Share={}'.format(float(found) / n))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Run all.py\nclass FeaturesOpts:\n    def __init__(self, num_days, predict_window, verbose, start_date, last_n_days = -1):\n        self.train_csv = '/kaggle/input/covid19-global-forecasting-week-2/train.csv'\n        self.test_csv = '/kaggle/input/covid19-global-forecasting-week-2/test.csv'\n        self.train_features_csv = 'train_features_{}.csv'.format(predict_window)\n        self.test_features_csv = 'test_features_{}.csv'.format(predict_window)\n        self.num_days = num_days\n        self.predict_window = predict_window\n        self.verbose = verbose\n        self.start_date = start_date\n        self.last_n_days = last_n_days\n        \nstart = time.time()\nif RUN:\n    if PROD:\n        for i in range(PROD_DAYS):\n            print('Day ' + str(i + 1))\n            main_FeaturesStage(FeaturesOpts(PROD_DAYS, i + 1, False, '2020-04-02'))\n    else:\n        for i in range(EVAL_DAYS):\n            print('Day ' + str(i + 1))\n            main_FeaturesStage(FeaturesOpts(EVAL_DAYS, i + 1, False, '2020-03-31'))\nend = time.time()\nprint('Overall took {} minutes'.format((end - start) / 60))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Run all.py\nclass LearningOpts:\n    def __init__(self, num_days, mode, super_quick = False):\n        self.train_features_csv = 'train_features'\n        self.test_features_csv = 'test_features'\n        self.train_predicted_csv = 'train_predicted'\n        self.test_predicted_csv = 'test_predicted'\n        self.mode = mode\n        self.days = num_days\n        self.verbose = True\n        self.eta = 0.03\n        self.seed = 0\n        self.depth = 6\n        self.super_quick = super_quick\n        \nstart = time.time()\nif RUN:\n    if PROD:\n        main_LearningStage(LearningOpts(PROD_DAYS, 'cases'))\n        main_LearningStage(LearningOpts(PROD_DAYS, 'fatal'))\n    else:\n        main_LearningStage(LearningOpts(EVAL_DAYS, 'cases', False))\n        main_LearningStage(LearningOpts(EVAL_DAYS, 'fatal'))\nend = time.time()\nprint('Overall took {} minutes'.format((end - start) / 60))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Run stat.py\nclass FinalizingOpts:\n    def __init__(self):\n        self.predicted_pattern = 'test_predicted'\n        self.verbose = True\n        self.only_cases = False\n        self.only_fatal = False\n        self.wdt = 0.5\n        self.wat = 0.3\n        self.wdl = 0.1\n        self.wal = 0.1\n        self.min_mortality = 250.0\n        self.max_mortality = 1.0\n        self.optimize = False\n        \nif RUN:\n    main_FinalizingStage(FinalizingOpts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Run week2.py\nclass Week2Opts:\n    def __init__(self):\n        self.train = '/kaggle/input/covid19-global-forecasting-week-2/train.csv'\n        self.cases_predicted_csv = 'test_predicted_cases.csv'\n        self.fatal_predicted_csv = 'test_predicted_fatalities.csv'\n        self.modified_cases = 'modified_cases.csv'\n        self.modified_fatal = 'modified_fatal.csv'\n        self.max_mortality_log = 4.7\n        \nif RUN and PROD:\n    main_Week2Adjustment(Week2Opts())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Run make_submission.py\nclass SubmissionOpts:\n    def __init__(self, use_modified = True):\n        self.submission = '/kaggle/input/covid19-global-forecasting-week-2/test.csv'\n        self.predicted_cases = 'modified_cases.csv' if use_modified else 'test_predicted_cases.csv'\n        self.predicted_fatalities = 'modified_fatal.csv' if use_modified else 'test_predicted_fatalities.csv'\n        self.output = 'submission.csv'\n        self.verbose = True\n        \nif RUN and PROD:\n    main_Submission(SubmissionOpts(True))\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}