{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\npd.options.display.max_rows=1000","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#train = pd.read_csv('/kaggle/input/covid19-global-forecasting-week-2/train.csv')\n#train = pd.read_csv('/kaggle/input/train-week2-doctored/trainDoctored.csv')\ntrain = pd.read_csv('/kaggle/input/train-proc/train_proc.csv')\n#test = pd.read_csv('/kaggle/input/covid19-global-forecasting-week-2/test.csv')\ntest = pd.read_csv('/kaggle/input/test-proc/test_proc.csv')\nsubs = pd.read_csv('/kaggle/input/covid19-global-forecasting-week-2/submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cow = pd.read_csv('/kaggle/input/countries-of-the-world/countries of the world.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cow.rename(columns={'Country':'Country_Region'},inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cow['Pop. Density (per sq. mi.)'] = cow['Pop. Density (per sq. mi.)'].str.replace(',', '.').astype(float)\ncow['Coastline (coast/area ratio)'] = cow['Coastline (coast/area ratio)'].str.replace(',', '.').astype(float)\ncow['Net migration'] = cow['Net migration'].str.replace(',', '.').astype(float)\ncow['Infant mortality (per 1000 births)'] = cow['Infant mortality (per 1000 births)'].str.replace(',', '.').astype(float)\ncow['Literacy (%)'] = cow['Literacy (%)'].str.replace(',', '.').astype(float)\ncow['Phones (per 1000)'] = cow['Phones (per 1000)'].str.replace(',', '.').astype(float)\ncow['Arable (%)'] = cow['Arable (%)'].str.replace(',', '.').astype(float)\ncow['Crops (%)'] = cow['Crops (%)'].str.replace(',', '.').astype(float)\ncow['Other (%)'] = cow['Other (%)'].str.replace(',', '.').astype(float)\ncow['Climate'] = cow['Climate'].str.replace(',', '.').astype(float)\ncow['Birthrate'] = cow['Birthrate'].str.replace(',', '.').astype(float)\ncow['Deathrate'] = cow['Deathrate'].str.replace(',', '.').astype(float)\ncow['Agriculture'] = cow['Agriculture'].str.replace(',', '.').astype(float)\ncow['Industry'] = cow['Industry'].str.replace(',', '.').astype(float)\ncow['Service'] = cow['Service'].str.replace(',', '.').astype(float)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# !pip install geopy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train['Province_State'].fillna('',inplace=True)\n# train_groupped = train.groupby(['Province_State','Country_Region']).max().reset_index()\n# #train_groupped['Probab'] = train_groupped['Fatalities']/train_groupped['ConfirmedCases']\n# train_groupped = train_groupped.sort_values(by='Country_Region')\n# train_groupped","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train_groupped['Lat']=None\n# train_groupped['Long']=None\n# from geopy.geocoders import Nominatim\n# geolocator = Nominatim(user_agent=\"specify_your_app_name_here\")\n# for i in range(len(train_groupped)):\n#     if train_groupped['Province_State'].iloc[i]=='':\n#         loc = train_groupped['Country_Region'].iloc[i]\n#     else:\n#         loc = train_groupped['Province_State'].iloc[i] + ', ' + train_groupped['Country_Region'].iloc[i]\n#     location = geolocator.geocode(loc)\n#     try:\n#         train_groupped['Lat'].iloc[i] = location.latitude\n#         train_groupped['Long'].iloc[i] =  location.longitude\n#     except:\n#         loc = train_groupped['Country_Region'].iloc[i]\n#         location = geolocator.geocode(loc)\n#         train_groupped['Lat'].iloc[i] = location.latitude\n#         train_groupped['Long'].iloc[i] =  location.longitude\n        \n# bcg_countries = ['Armenia','Azerbaijan','Belarus','Estonia','Georgia','Kazakhstan','Kyrgyzstan','Latvia','Lithuania',\n#                  'Moldova','Russia','Tajikistan','Turkmenistan','Ukraine','Uzbekistan',\n#                 'United Kingdom','India','Brazil','Germany','Bulgaria','Hungary', 'Poland','Romania','Slovakia',\n#                  'Malta','France','Norway','Greece','Singapore','Malaysia', 'Korea, South', 'Taiwan','Japan','Thailand','Sri Lanka','South Africa',\n#                 'Philippines','Mongolia','Hong Kong','Pakistan','Ecuador','Argentina','Bolivia','Columbia','Chile','Paraguay','Peru','Uruguay','Venezuela']\n\n# train_groupped['BCG'] = [i in  bcg_countries for i in train_groupped['Country_Region']]\n# train = pd.merge(train,train_groupped[['Province_State','Country_Region','Lat','Long','BCG']])\n# train = train.sort_values(by='Date').reset_index(drop=True)\n# train.to_csv('train_proc.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cow[cow['Country_Region']=='United States']['Country_Region']='US '\ncow.fillna(0,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['Province_State'].fillna(train['Country_Region'],inplace=True)\ntest['Province_State'].fillna(test['Country_Region'],inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['Date'] = pd.to_datetime(train['Date'])\ntest['Date'] = pd.to_datetime(test['Date'])\ntrain['Date'] = train['Date'].astype('int64')\ntest['Date'] = test['Date'].astype('int64')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['Country_Region'] = train['Country_Region'].apply(lambda x: str(x)+' ')\ntest['Country_Region'] = test['Country_Region'].apply(lambda x: str(x)+' ')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# test['Province_State'].fillna('',inplace=True)\n# test_groupped = test.groupby(['Province_State','Country_Region']).max().reset_index()\n\n# test_groupped['Lat']=None\n# test_groupped['Long']=None\n# from geopy.geocoders import Nominatim\n# geolocator = Nominatim(user_agent=\"specify_your_app_name_here\")\n# for i in range(len(test_groupped)):\n#     if test_groupped['Province_State'].iloc[i]=='':\n#         loc = test_groupped['Country_Region'].iloc[i]\n#     else:\n#         loc = test_groupped['Province_State'].iloc[i] + ', ' + test_groupped['Country_Region'].iloc[i]\n#     location = geolocator.geocode(loc)\n#     try:\n#         test_groupped['Lat'].iloc[i] = location.latitude\n#         test_groupped['Long'].iloc[i] =  location.longitude\n#     except:\n#         loc = test_groupped['Country_Region'].iloc[i]\n#         location = geolocator.geocode(loc)\n#         test_groupped['Lat'].iloc[i] = location.latitude\n#         test_groupped['Long'].iloc[i] =  location.longitude\n        \n# bcg_countries = ['Armenia','Azerbaijan','Belarus','Estonia','Georgia','Kazakhstan','Kyrgyzstan','Latvia','Lithuania',\n#                  'Moldova','Russia','Tajikistan','Turkmenistan','Ukraine','Uzbekistan',\n#                 'United Kingdom','India','Brazil','Germany','Bulgaria','Hungary', 'Poland','Romania','Slovakia',\n#                  'Malta','France','Norway','Greece','Singapore','Malaysia', 'Korea, South', 'Taiwan','Japan','Thailand','Sri Lanka','South Africa',\n#                 'Philippines','Mongolia','Hong Kong','Pakistan','Ecuador','Argentina','Bolivia','Columbia','Chile','Paraguay','Peru','Uruguay','Venezuela']\n\n# test_groupped['BCG'] = [i in  bcg_countries for i in test_groupped['Country_Region']]\n# test = pd.merge(test,test_groupped[['Province_State','Country_Region','Lat','Long','BCG']])\n\n# test.drop('ForecastId',axis=1,inplace=True)\n\n# test.to_csv('test_proc.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.merge(train,cow,how='left')\ntest = pd.merge(test,cow,how='left')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.fillna(0,inplace=True)\ntest.fillna(0,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ndef LbelEnc(df):\n    le.fit(df['Province_State'].astype(str))\n    df['Province_State'] = le.transform(df['Province_State'].astype(str))\n    le.fit(df['Country_Region'].astype(str))\n    df['Country_Region'] = le.transform(df['Country_Region'].astype(str))\n    le.fit(df['BCG'].astype(str))\n    df['BCG'] = le.transform(df['BCG'].astype(str))\n    le.fit(df['Region'].astype(str))\n    df['Region'] = le.transform(df['Region'].astype(str))\n    return df\n\ntrain = LbelEnc(train)\ntest = LbelEnc(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"try:\n    test.drop('Unnamed: 0',axis=1,inplace=True)\nexcept:\n    1\ntry:\n    train.drop('Unnamed: 0',axis=1,inplace=True)\nexcept:\n    1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import lightgbm as LGB\n# from sklearn.model_selection import TimeSeriesSplit,GridSearchCV\n\n# #import sklearn\n# #sklearn.metrics.SCORERS.keys()\n\n# cvt = TimeSeriesSplit(n_splits=100) \n# params = {'num_leaves':[31,10,15,20],'max_depth':[-1,15,20],'max_features':['sqrt','log2','auto',None]}\n# lgb = LGB.LGBMRegressor(boosting_type='gbdt',learning_rate=1, n_estimators=500)\n# model = GridSearchCV(estimator=lgb, param_grid=params,cv=cvt,scoring='neg_root_mean_squared_error') #'neg_median_absolute_error', 'neg_mean_absolute_error', 'neg_mean_squared_error', 'neg_mean_squared_log_error', 'neg_root_mean_squared_error',","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = train[['Province_State', 'Country_Region', 'Date',\n       'Lat', 'Long', 'Region',\n       'Population', 'Area (sq. mi.)', 'Pop. Density (per sq. mi.)',\n        'Climate', 'Birthrate', 'Deathrate']]\ntest = test[['Province_State', 'Country_Region', 'Date',\n       'Lat', 'Long', 'Region',\n       'Population', 'Area (sq. mi.)', 'Pop. Density (per sq. mi.)',\n        'Climate', 'Birthrate', 'Deathrate']]\nY_train = train[['ConfirmedCases','Fatalities']]\nY_train1 = train['ConfirmedCases']\nY_train2 = train['Fatalities']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# model.fit(X_train,Y_train1);\n# print(model.best_params_)\n# print('best_score on train : '+str(model.best_score_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# model.fit(X_train,Y_train2)\n# print(model.best_params_)\n# print('best_score on train for Fatalities: '+str(model.best_score_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nfrom catboost import Pool, CatBoostRegressor\n\n# initialize data\ntrain_data = X_train\ntrain_label = Y_train1\ntest_data = test\n\n# initialize Pool\ntrain_pool = Pool(train_data, \n                  train_label, \n                  cat_features=[0,1,5])\n\ntest_pool = Pool(test_data, \n                 cat_features=[0,1,5]) \n\n# specify the training parameters \nmodel = CatBoostRegressor(iterations=10000, \n                          depth=8, \n                          learning_rate=1, \n                          loss_function='RMSE')\n#train the model\nmodel.fit(train_pool)\n\n# make the prediction using the resulting model\npreds1 = model.predict(test_pool)\nprint(preds1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# initialize data\ntrain_data = X_train\ntrain_label = Y_train2\ntest_data = test\n\n# initialize Pool\ntrain_pool = Pool(train_data, \n                  train_label, \n                  cat_features=[0,1,5])\n\ntest_pool = Pool(test_data, \n                 cat_features=[0,1,5]) \n\n# specify the training parameters \nmodel = CatBoostRegressor(iterations=10000, \n                          depth=8, \n                          learning_rate=1, \n                          loss_function='RMSE')\n#train the model\nmodel.fit(train_pool)\n\n# make the prediction using the resulting model\npreds2 = model.predict(test_pool)\nprint(preds2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"outputFile = pd.DataFrame({\"ForecastId\": test.index+1,\n                           \"ConfirmedCases\": (np.abs(preds1)+0.5).astype('int'),\n                           \"Fatalities\": (np.abs(preds2)+0.5).astype('int')})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"outputFile.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}