{"cells":[{"metadata":{},"cell_type":"markdown","source":"> # RNN啊\n我们要学习的模型是LSTM,是RNN的升华版\n这是人工智能的范畴了\n为什么我这次notebook要写在kaggle上呢\n因为我电脑配置的是pytorch，当我尝试配置tensorflow发现一个问题\n各位电脑可能不支持cuda\n一个加速显卡优化计算的东西\n即使可以\n我们要统一tensoflow版本，然后各位电脑配置也很难算的快\n所以我们干脆就在kaggle上面跑了，各位也方便\n我们搭最简单的模型就够了，很深的网络是为了抽取很抽象的特征，我们的特征其实已经挺抽象了","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport geopandas as gpd\nfrom shapely.geometry import Point\nimport os\nimport tensorflow as tf\nfrom tqdm import tqdm\nfrom sklearn.utils import shuffle\nfrom sklearn.metrics import mean_squared_log_error\n\nfrom datetime import datetime\nfrom datetime import timedelta\n\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import Input\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\n\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 看一下数据到底是什么样","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#######################读取最基本的训练数据\n##############################################为了和之前训练的模型相匹配，我们就根据3.24前的数据训练","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = gpd.read_file(\"/kaggle/input/covid19-global-forecasting-week-4/train.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()\n#####################为什么地区需要有两个数据列，是冗余的部分，可以校正","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = gpd.read_file(\"/kaggle/input/covid19-global-forecasting-week-4/train.csv\")\ntrain_df[\"ConfirmedCases\"] = train_df[\"ConfirmedCases\"].astype(\"float\")\ntrain_df[\"Fatalities\"] = train_df[\"Fatalities\"].astype(\"float\")\ntrain_df[\"Country_Region\"] = [ row.Country_Region.replace(\"'\",\"\").strip(\" \") if row.Province_State==\"\" else str(row.Country_Region+\"_\"+row.Province_State).replace(\"'\",\"\").strip(\" \") for idx,row in train_df.iterrows()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df[train_df[\"Country_Region\"]==\"China\"] #################国家和地区在一起了","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#调用上传的丰富数据库，含有其他团队整理的信息\nextra_data_df = gpd.read_file(\"/kaggle/input/covid19-enriched-dataset-week-2/enriched_covid_19_week_2.csv\")\nextra_data_df[\"Country_Region\"] = [country_name.replace(\"'\",\"\") for country_name in extra_data_df[\"Country_Region\"]]\n############################类型转换，所有数据使用前都进行一次数据转换，防止奇奇怪怪的类型错误\nextra_data_df[\"restrictions\"] = extra_data_df[\"restrictions\"].astype(\"int\")\nextra_data_df[\"quarantine\"] = extra_data_df[\"quarantine\"].astype(\"int\")\nextra_data_df[\"schools\"] = extra_data_df[\"schools\"].astype(\"int\")\nextra_data_df[\"total_pop\"] = extra_data_df[\"total_pop\"].astype(\"float\")\nextra_data_df[\"density\"] = extra_data_df[\"density\"].astype(\"float\")\nextra_data_df[\"hospibed\"] = extra_data_df[\"hospibed\"].astype(\"float\")\nextra_data_df[\"lung\"] = extra_data_df[\"lung\"].astype(\"float\")\n############################将每一个数据列与该数据列的最大值进行一个比值，做一个正则化处理\nextra_data_df[\"total_pop\"] = extra_data_df[\"total_pop\"]/max(extra_data_df[\"total_pop\"])\nextra_data_df[\"density\"] = extra_data_df[\"density\"]/max(extra_data_df[\"density\"])\nextra_data_df[\"hospibed\"] = extra_data_df[\"hospibed\"]/max(extra_data_df[\"hospibed\"])\nextra_data_df[\"lung\"] = extra_data_df[\"lung\"]/max(extra_data_df[\"lung\"])\nextra_data_df[\"age_100+\"] = extra_data_df[\"age_100+\"].astype(\"float\")\nextra_data_df[\"age_100+\"] = extra_data_df[\"age_100+\"]/max(extra_data_df[\"age_100+\"])\n###########################################################只提取必要的属性列\nextra_data_df = extra_data_df[[\"Country_Region\",\"Date\",\"restrictions\",\"quarantine\",\"schools\",\"hospibed\",\"lung\",\"total_pop\",\"density\",\"age_100+\"]]\nextra_data_df.head()\n############################################约束措施，隔离措施，收治率，肺病率，总人口，密度，百岁老人比例\n#####################之前美国欧洲不都是养老院集体失守","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"extra_data_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ntrain_df = train_df.merge(extra_data_df, how=\"left\", on=['Country_Region','Date']).drop_duplicates()\ntrain_df.head()\n######################合并总的数据","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"####################新的数据并不完整","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"##########进一步处理\n######对每一个国家分别做处理\n######因为额外数据只提供到3月25号之前，我们需要填补缺失值","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for country_region in train_df.Country_Region.unique():\n    query_df = train_df.query(\"Country_Region=='\"+country_region+\"' and Date=='2020-03-25'\")\n    train_df.loc[(train_df[\"Country_Region\"]==country_region) & (train_df[\"Date\"]>\"2020-03-25\"),\"total_pop\"] = query_df.total_pop.values[0]\n    train_df.loc[(train_df[\"Country_Region\"]==country_region) & (train_df[\"Date\"]>\"2020-03-25\"),\"hospibed\"] = query_df.hospibed.values[0]\n    train_df.loc[(train_df[\"Country_Region\"]==country_region) & (train_df[\"Date\"]>\"2020-03-25\"),\"density\"] = query_df.density.values[0]\n    train_df.loc[(train_df[\"Country_Region\"]==country_region) & (train_df[\"Date\"]>\"2020-03-25\"),\"lung\"] = query_df.lung.values[0]\n    train_df.loc[(train_df[\"Country_Region\"]==country_region) & (train_df[\"Date\"]>\"2020-03-25\"),\"age_100+\"] = query_df[\"age_100+\"].values[0]\n    train_df.loc[(train_df[\"Country_Region\"]==country_region) & (train_df[\"Date\"]>\"2020-03-25\"),\"restrictions\"] = query_df.restrictions.values[0]\n    train_df.loc[(train_df[\"Country_Region\"]==country_region) & (train_df[\"Date\"]>\"2020-03-25\"),\"quarantine\"] = query_df.quarantine.values[0]\n    train_df.loc[(train_df[\"Country_Region\"]==country_region) & (train_df[\"Date\"]>\"2020-03-25\"),\"schools\"] = query_df.schools.values[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#####增补数据中有很多国家有空缺，针对空缺的国家我们用世界的中位数去替代","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"############################这里在计算世界的均值\nmedian_pop = np.median(extra_data_df.total_pop)\nmedian_hospibed = np.median(extra_data_df.hospibed)\nmedian_density = np.median(extra_data_df.density)\nmedian_lung = np.median(extra_data_df.lung)\nmedian_centenarian_pop = np.median(extra_data_df[\"age_100+\"])\n#######################################额外的数据库中哪里有缺失值\nprint(\"The missing countries/region are:\")\n#########################################遍布每一个原始数据集自己的数据\n#########################################在额外数据中寻找是否存在\nfor country_region in train_df.Country_Region.unique():\n    if extra_data_df.query(\"Country_Region=='\"+country_region+\"'\").empty:\n        print(country_region)\n        train_df.loc[train_df[\"Country_Region\"]==country_region,\"total_pop\"] = median_pop\n        train_df.loc[train_df[\"Country_Region\"]==country_region,\"hospibed\"] = median_hospibed\n        train_df.loc[train_df[\"Country_Region\"]==country_region,\"density\"] = median_density\n        train_df.loc[train_df[\"Country_Region\"]==country_region,\"lung\"] = median_lung\n        train_df.loc[train_df[\"Country_Region\"]==country_region,\"age_100+\"] = median_centenarian_pop\n        train_df.loc[train_df[\"Country_Region\"]==country_region,\"restrictions\"] = 0\n        train_df.loc[train_df[\"Country_Region\"]==country_region,\"quarantine\"] = 0\n        train_df.loc[train_df[\"Country_Region\"]==country_region,\"schools\"] = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#####采取和之前lightGBM一样的方法，构造属性列，趋势数据，还记得吗\n#####当时人为间隔了数据列，人为模拟控制了爆发期ar\n#####当时如果能理解这个应该很简单","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trend_df = pd.DataFrame(columns={\"infection_trend\",\"fatality_trend\",\"quarantine_trend\",\"school_trend\",\"total_population\",\"expected_cases\",\"expected_fatalities\"})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.info","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# # # *注意啊，我们调整了训练时间，不仅模型变化了，我们的训练范围也变化了*************","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#丢掉了第一天的数据，因为这样刚好就能是7天的一个周期了\n#根据过去三周的趋势以及基础设施条件去预测最后一天的情况\n#跟新啊：为了配合第一周的预测\n#我们选用的训练数据是1.22到3.25\n#我们的周期选用的是7天\n\n\nmodel_train_df = train_df.query(\"Date>'2020-01-22'and Date<'2020-03-24'\")\ndays_in_sequence = 5\n\ntrend_list = []\n\nwith tqdm(total=len(list(model_train_df.Country_Region.unique()))) as pbar:\n    for country in model_train_df.Country_Region.unique():\n        for province in model_train_df.query(f\"Country_Region=='{country}'\").Province_State.unique():\n            province_df = model_train_df.query(f\"Country_Region=='{country}' and Province_State=='{province}'\")\n            \n            for i in range(0,len(province_df),int(days_in_sequence/3)):\n                if i+days_in_sequence<=len(province_df):\n                    ###########################这一步很重要\n                    ###########################在做什么呢？\n                    ###########################在做一个滑动的序列，以天为单位滑动\n                    ##########################每次滚动一周\n                    infection_trend = [float(x) for x in province_df[i:i+days_in_sequence-1].ConfirmedCases.values]\n                    fatality_trend = [float(x) for x in province_df[i:i+days_in_sequence-1].Fatalities.values]\n                    restriction_trend = [float(x) for x in province_df[i:i+days_in_sequence-1].restrictions.values]\n                    quarantine_trend = [float(x) for x in province_df[i:i+days_in_sequence-1].quarantine.values]\n                    school_trend = [float(x) for x in province_df[i:i+days_in_sequence-1].schools.values]\n                    \n                    \n                    #########################下面就不是时间序列了，不需要滚\n                    total_population = float(province_df.iloc[i].total_pop)\n                    density = float(province_df.iloc[i].density)\n                    hospibed = float(province_df.iloc[i].hospibed)\n                    lung = float(province_df.iloc[i].lung)\n                    centenarian_pop = float(province_df.iloc[i][\"age_100+\"])\n\n                    \n                    #######################滚动每个时间点预计的死亡确诊人数\n                    expected_cases = float(province_df.iloc[i+days_in_sequence-1].ConfirmedCases)\n                    expected_fatalities = float(province_df.iloc[i+days_in_sequence-1].Fatalities)\n\n                    ##########################\n                    trend_list.append({\"infection_trend\":infection_trend,\n                                     \"fatality_trend\":fatality_trend,\n                                     \"restriction_trend\":restriction_trend,\n                                     \"quarantine_trend\":quarantine_trend,\n                                     \"school_trend\":school_trend,\n                                     \"demographic_inputs\":[total_population,density,hospibed,lung,centenarian_pop],\n                                     \"expected_cases\":expected_cases,\n                                     \"expected_fatalities\":expected_fatalities})\n        pbar.update(1)  #########################这条代码就是显示下面的进度\ntrend_df = pd.DataFrame(trend_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"###############################接下来我们要做一组显示效果的测试集数据","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#丢掉了第一天的数据，因为这样刚好就能是7天的一个周期了\n#根据过去三周的趋势以及基础设施条件去预测最后一天的情况\n#跟新啊：为了配合第一周的预测\n#我们选用的训练数据是1.22到3.25\n#我们的周期选用的是7天\n\n\nmodel_test_df = train_df.query(\"Date>'2020-03-26'and Date<'2020-04-14' and Country_Region =='Afghanistan'\")\ndays_in_sequence = 5\n\ntest_trend_list = []\n\nwith tqdm(total=len(list(model_test_df.Country_Region.unique()))) as pbar:\n    for country in model_test_df.Country_Region.unique():\n        for province in model_test_df.query(f\"Country_Region=='{country}'\").Province_State.unique():\n            province_df = model_test_df.query(f\"Country_Region=='{country}' and Province_State=='{province}'\")\n            \n            for i in range(0,len(province_df),int(days_in_sequence/3)):\n                if i+days_in_sequence<=len(province_df):\n                    ###########################这一步很重要\n                    ###########################在做什么呢？\n                    ###########################在做一个滑动的序列，以天为单位滑动\n                    ##########################每次滚动一周\n                    infection_trend = [float(x) for x in province_df[i:i+days_in_sequence-1].ConfirmedCases.values]\n                    fatality_trend = [float(x) for x in province_df[i:i+days_in_sequence-1].Fatalities.values]\n                    restriction_trend = [float(x) for x in province_df[i:i+days_in_sequence-1].restrictions.values]\n                    quarantine_trend = [float(x) for x in province_df[i:i+days_in_sequence-1].quarantine.values]\n                    school_trend = [float(x) for x in province_df[i:i+days_in_sequence-1].schools.values]\n                    \n                    \n                    #########################下面就不是时间序列了，不需要滚\n                    total_population = float(province_df.iloc[i].total_pop)\n                    density = float(province_df.iloc[i].density)\n                    hospibed = float(province_df.iloc[i].hospibed)\n                    lung = float(province_df.iloc[i].lung)\n                    centenarian_pop = float(province_df.iloc[i][\"age_100+\"])\n\n                    \n                    #######################滚动每个时间点预计的死亡确诊人数\n                    expected_cases = float(province_df.iloc[i+days_in_sequence-1].ConfirmedCases)\n                    expected_fatalities = float(province_df.iloc[i+days_in_sequence-1].Fatalities)\n\n                    ##########################\n                    test_trend_list.append({\"infection_trend\":infection_trend,\n                                     \"fatality_trend\":fatality_trend,\n                                     \"restriction_trend\":restriction_trend,\n                                     \"quarantine_trend\":quarantine_trend,\n                                     \"school_trend\":school_trend,\n                                     \"demographic_inputs\":[total_population,density,hospibed,lung,centenarian_pop],\n                                     \"expected_cases\":expected_cases,\n                                     \"expected_fatalities\":expected_fatalities})\n        pbar.update(1)  #########################这条代码就是显示下面的进度\ntest_trend_df = pd.DataFrame(test_trend_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_test_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"############################每一个trend都是对接下来21天发展的估计，demographic_inputs是我们的其他变量\n############################","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(model_test_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(test_trend_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"        ##################################得到的具体分组数据，有趋势，有硬件条件\n                #########################很关键啊，我们这一份的数据每一行都是当日确诊人数以及感染确诊趋势，以及硬性条件趋势\n            #########################################前面的趋势，是连续13天的趋势，最后是第十四天的确诊以及致死病例\n            #########################################那这就存在一个很好的预测形式","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trend_df[\"temporal_inputs\"] = [np.asarray([trends[\"infection_trend\"],trends[\"fatality_trend\"],trends[\"restriction_trend\"],trends[\"quarantine_trend\"],trends[\"school_trend\"]]) for idx,trends in trend_df.iterrows()]  \n#########################################当然要打乱顺序，数据之间不能有联系","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trend_df = shuffle(trend_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_trend_df[\"temporal_inputs\"] = [np.asarray([trends[\"infection_trend\"],trends[\"fatality_trend\"],trends[\"restriction_trend\"],trends[\"quarantine_trend\"],trends[\"school_trend\"]]) for idx,trends in test_trend_df.iterrows()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_trend_df             ######################这是我们的测试集合","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#######################temporal_inputs是所有输入的集合啊，所有趋势数据的集合","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"######################################全是0的数据我们就不需要了，趋势是0，我们选择保留5份","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# ######################重要，我们要转变数据输入形似","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sequence_length = 4","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training_item_count = int(len(trend_df))\n# validation_item_count = len(trend_df)-int(len(trend_df)*training_percentage)\ntraining_df = trend_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_temporal_train = np.asarray(np.transpose(np.reshape(np.asarray([np.asarray(x) for x in training_df[\"temporal_inputs\"].values]),(training_item_count,5,sequence_length)),(0,2,1) )).astype(np.float32)\nX_demographic_train = np.asarray([np.asarray(x) for x in training_df[\"demographic_inputs\"]]).astype(np.float32)\nY_cases_train = np.asarray([np.asarray(x) for x in training_df[\"expected_cases\"]]).astype(np.float32)\nY_fatalities_train = np.asarray([np.asarray(x) for x in training_df[\"expected_fatalities\"]]).astype(np.float32)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"########################################比如我们调用","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_trend_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_item_count = int(len(test_trend_df))\n# validation_item_count = len(trend_df)-int(len(trend_df)*training_percentage)\ntest_df = test_trend_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_temporal_test = np.asarray(np.transpose(np.reshape(np.asarray([np.asarray(x) for x in test_df[\"temporal_inputs\"].values]),(test_item_count,5,sequence_length)),(0,2,1) )).astype(np.float32)\nX_demographic_test = np.asarray([np.asarray(x) for x in test_df[\"demographic_inputs\"]]).astype(np.float32)\nY_cases_test = np.asarray([np.asarray(x) for x in test_df[\"expected_cases\"]]).astype(np.float32)\nY_fatalities_test = np.asarray([np.asarray(x) for x in test_df[\"expected_fatalities\"]]).astype(np.float32)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# ###########################下面这几段无视","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# training_item_count = int(len(trend_df)*training_percentage)\n# validation_item_count = len(trend_df)-int(len(trend_df)*training_percentage)\n# training_df = trend_df[:training_item_count]\n# validation_df = trend_df[training_item_count:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# X_temporal_train = np.asarray(np.transpose(np.reshape(np.asarray([np.asarray(x) for x in training_df[\"temporal_inputs\"].values]),(training_item_count,5,sequence_length)),(0,2,1) )).astype(np.float32)\n# X_demographic_train = np.asarray([np.asarray(x) for x in training_df[\"demographic_inputs\"]]).astype(np.float32)\n# Y_cases_train = np.asarray([np.asarray(x) for x in training_df[\"expected_cases\"]]).astype(np.float32)\n# # Y_fatalities_train = np.asarray([np.asarray(x) for x in training_df[\"expected_fatalities\"]]).astype(np.float32)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#############################################################################注意啊注意，我们最终的效果不是源自这个测试集","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# X_temporal_test = np.asarray(np.transpose(np.reshape(np.asarray([np.asarray(x) for x in validation_df[\"temporal_inputs\"]]),(validation_item_count,5,sequence_length)),(0,2,1)) ).astype(np.float32)\n# X_demographic_test = np.asarray([np.asarray(x) for x in validation_df[\"demographic_inputs\"]]).astype(np.float32)\n# Y_cases_test = np.asarray([np.asarray(x) for x in validation_df[\"expected_cases\"]]).astype(np.float32)\n# Y_fatalities_test = np.asarray([np.asarray(x) for x in validation_df[\"expected_fatalities\"]]).astype(np.float32)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. 训练模型","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"这就是我们的模型\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"###########################假装这里有张图片","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#temporal input branch\n###############时间序列数据输入分支\n##################逐个详解\n\n##########################这是我们的输入,看上面我们的输入形似，序列数据的输入形式\n # 这里是作为Sequential模型的第一层所以指定input_shape参数，后面加的层不需要这个\n    # 这里的input_shape是两个元素的，第一个代表每个输入的样本序列长度，第二个元素代表\n    # 每个序列里面的1个元素具有多少个输入数据。例如，LSTM处理的序列长度为20，每个时间\n    # 步即序列的元素是由两个维度组成，那么这个参数设置为(20, 5)\n\n    # 输入是固定格式。无论怎么改网络结构我们都是这样子输入\n# temporal_input_layer = Input(shape=(sequence_length,5))\n####################################输出的单元数目\n####################################return_sequences=True返回LSTM中间状态\n\n####################################这和pytorch中的dropout应该一样，都是短接网络，防止过拟合\n\ntemporal_input_layer=Input(shape=(sequence_length,5))\ntemporal_dense=layers.Dense(16)(temporal_input_layer)\nmain_rnn_layer = layers.LSTM(64, return_sequences=True, recurrent_dropout=0.2)(temporal_dense)\n\n\n\n# main_rnn_layer = layers.LSTM(64, return_sequences=True, recurrent_dropout=0.2)(temporal_input_layer)\n\n\n#硬性条件的全连接层\n#####################输入层的属性是5\n#####################第一层输出16个属性\n########################################dropout同上\n\ndemographic_input_layer = Input(shape=(5))\ndemographic_first_dense=layers.Dense(16)(demographic_input_layer)\ndemographic_dense = layers.Dense(16)(demographic_first_dense)\ndemographic_dropout = layers.Dropout(0.2)(demographic_dense)\n\n#这是我们的确诊分支\n#连接之前的那一层，有32个单元输出\nrnn_c = layers.LSTM(32)(main_rnn_layer)\n##################################################并行连接\nmerge_c = layers.Concatenate(axis=-1)([rnn_c,demographic_dropout])\n########################同样是一个全连接层\n########################dropot防止过拟合，回忆一下pytorch\ndense_c = layers.Dense(128)(merge_c)\ndropout_c = layers.Dropout(0.3)(dense_c)\n##############################################对执行结果进行激活\n##############################################alpha是斜率\n#############################################\ncases = layers.Dense(1, activation=layers.LeakyReLU(alpha=0.1),name=\"cases\")(dropout_c)\n\n#同理是死亡分支\nrnn_f = layers.LSTM(32)(main_rnn_layer)\nmerge_f = layers.Concatenate(axis=-1)([rnn_f,demographic_dropout])\ndense_f = layers.Dense(128)(merge_f)\ndropout_f = layers.Dropout(0.3)(dense_f)\nfatalities = layers.Dense(1, activation=layers.LeakyReLU(alpha=0.1), name=\"fatalities\")(dropout_f)\n\n########################我们的最终模型\nmodel = Model([temporal_input_layer,demographic_input_layer], [cases,fatalities])\n\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"callbacks = [ReduceLROnPlateau(monitor='val_loss', patience=4, verbose=1, factor=0.6),\n             EarlyStopping(monitor='val_loss', patience=20),\n             ModelCheckpoint(filepath='best_model.h5', monitor='val_loss', save_best_only=True)]\nmodel.compile(loss=[tf.keras.losses.MeanSquaredLogarithmicError(),tf.keras.losses.MeanSquaredLogarithmicError()], optimizer=\"adam\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit([X_temporal_train,X_demographic_train], [Y_cases_train, Y_fatalities_train], \n          epochs = 250, \n          batch_size = 16, \n          validation_data=([X_temporal_test,X_demographic_test],  [Y_cases_test, Y_fatalities_test]), \n          callbacks=callbacks)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4. Performance during training","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#################################从图像上看，结果并不好","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5. 下面这部分最重要！！！！！！！！！！！！！！！！！开始预测！","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"经历了艰难我们训练出一个模型","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model.load_weights(\"best_model.h5\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = model.predict([X_temporal_test,X_demographic_test])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#########################################没什么好说的。一如既往，我们的输出是一个数组，只有一个元素的数组\n#########################################这是我们大概的预测结果\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"display_limit = 30\nfor inputs, pred_cases, exp_cases, pred_fatalities, exp_fatalities in zip(X_temporal_test,predictions[0][:display_limit], Y_cases_test[:display_limit], predictions[1][:display_limit], Y_fatalities_test[:display_limit]):\n    print(\"================================================\")\n    print(inputs)\n    print(\"Expected cases:\", exp_cases, \" Prediction:\", pred_cases[0], \"Expected fatalities:\", exp_fatalities, \" Prediction:\", pred_fatalities[0] )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 6. 预测模型趋势","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"我们的预测值可以作为下一次预测的输入变量，不断向下滚动\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 9.已经无法提交了，缩小了训练集，分数太低了","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df = gpd.read_file(\"/kaggle/input/covid19-global-forecasting-week-4/test.csv\")\n#The country_region got modifying in the enriched dataset by @optimo, \n# so we have to apply the same change to the test Dataframe.\ntest_df[\"Country_Region\"] = [ row.Country_Region if row.Province_State==\"\" else row.Country_Region+\"_\"+row.Province_State for idx,row in test_df.iterrows() ]\ntest_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Just need to do this little trick to extract the relevant date and the forecastId and add that to the submission file.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}