{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#importing requiste libraries\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn import linear_model\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import Lasso\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn import metrics\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nfrom tensorflow.python.keras.layers import Dense\nfrom tensorflow.python.keras import Sequential\nfrom tensorflow.python.keras.layers import LSTM\nfrom sklearn.ensemble import ExtraTreesRegressor,RandomForestRegressor,AdaBoostRegressor\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. Importing Data"},{"metadata":{},"cell_type":"markdown","source":"Weather data source: https://www.kaggle.com/winterpierre91/covid19-global-weather-data<br>\nPopulation density and socio-economic data source: https://www.kaggle.com/fernandol/countries-of-the-world"},{"metadata":{"trusted":true},"cell_type":"code","source":"class dataImport():\n    \n    def __init__(self,filename,type, df=None):\n        self.df = df\n    \n    def import_and_describe_data(self,filename,type):\n      if type=='csv':\n        self.df=pd.read_csv(filename)\n        print('------------------------------------DF Head-----------------------------------------')\n        print(self.df.head())\n        print('-------------------------------------Shape------------------------------------------')\n        print(self.df.shape)\n        print('--------------------------------------Info------------------------------------------')\n        print(self.df.info())\n        print('------------------------------DataSet Description-----------------------------------')\n        print(self.df.describe())\n        return self\n    \n    #Drop columns with single value.\n    def drop_single_value_columns(self):\n        drop_cols = list(filter(lambda x : len(self.df[x].unique()) < 2, self.df.columns))        \n        print('------------------------------Single Value coulmns----------------------------------')\n        print('Columns dropped: ',drop_cols)\n        self.df.drop(drop_cols,axis=1, inplace=True)   \n        return self\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ntrain=(dataImport(f'/kaggle/input/covid19-global-forecasting-week-2/train.csv','csv')\n.import_and_describe_data(f'/kaggle/input/covid19-global-forecasting-week-2/train.csv','csv')\n.drop_single_value_columns()).df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test=(dataImport(f'/kaggle/input/covid19-global-forecasting-week-2/test.csv','csv')\n.import_and_describe_data(f'/kaggle/input/covid19-global-forecasting-week-2/test.csv','csv')\n.drop_single_value_columns()).df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"temperature_date=(dataImport(f'/kaggle/input/covid19-global-weather-data/temperature_dataframe.csv','csv')\n.import_and_describe_data(f'/kaggle/input/covid19-global-weather-data/temperature_dataframe.csv','csv')\n.drop_single_value_columns()).df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"GDP_Density_data=(dataImport(f'/kaggle/input/countries-of-the-world/countries of the world.csv','csv')\n.import_and_describe_data(f'/kaggle/input/countries-of-the-world/countries of the world.csv','csv')\n.drop_single_value_columns()).df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. Data Cleansing"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#removing spaces in string columns and coverting all textual data to lower case\ntrain=train.apply(lambda x: x.astype(str).str.lower().replace(' ','', regex=True))\ntest=test.apply(lambda x: x.astype(str).str.lower().replace(' ','', regex=True))\ntemperature_date=temperature_date.apply(lambda x: x.astype(str).str.lower().replace(' ','', regex=True))\nGDP_Density_data=GDP_Density_data.apply(lambda x: x.astype(str).str.lower().replace(' ','', regex=True))\n#Province is more or less have Nan values replacing with country's value\ntrain['Province_State']=np.where(train.Province_State=='nan', train.Country_Region, train.Province_State)\ntest['Province_State']=np.where(test.Province_State=='nan', test.Country_Region, test.Province_State)\ntemperature_date['province']=np.where(temperature_date.province=='nan', temperature_date.country, temperature_date.province)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#for 177 rows temparature is null replacing with 0\ntemperature_date=temperature_date.fillna(0)\ntemperature_date.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#3-4 countries/rows have none values will be replaced with 0's.\n# Will go for machine learning driven imputation for now will proceed with replacing 0's\nGDP_Density_data=GDP_Density_data.fillna(0)\nGDP_Density_data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# there are few countries which are represented differently in temparature and covid data set changing them in temparature data set\ntemperature_date['country']=temperature_date.country.replace(\"usa\",\"us\")\ntemperature_date['country']=temperature_date.country.replace(\"uk\",\"unitedkingdom\")\ntemperature_date['country']=temperature_date.country.replace(\"taiwan\",\"taiwan*\")\ntemperature_date['country']=temperature_date.country.replace(\"korea\",\"korea,south\")\ntemperature_date['country']=temperature_date.country.replace(\"uae\",\"unitedarabemirates\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#changing temparature field to float \ntemperature_date.tempC=temperature_date.tempC.astype(float)\n#taking mean of temparature for every country\ntemperature_date=temperature_date.groupby(temperature_date['country'])['tempC'].mean().reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#mereging temperature data set with train/test\ntrain = pd.merge(train,temperature_date[['tempC','country']], how='left',  left_on=['Country_Region'], right_on=['country'])\ntest = pd.merge(test,temperature_date[['tempC','country']], how='left',  left_on=['Country_Region'], right_on=['country'])\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#few african countries don't have tempartures and simple google search inidcates temp around 20\n# as said earlier will replace with ML imputation technique after looking how this metric influences the model\ntrain.tempC.fillna(20,inplace=True)\ntest.tempC.fillna(20,inplace=True)\ntrain.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#correcting country names in Density data set\nGDP_Density_data['Country']=GDP_Density_data.Country.replace(\"unitedstates\",\"us\")\nGDP_Density_data['Country']=GDP_Density_data.Country.replace(\"taiwan\",\"taiwan*\")\nGDP_Density_data['Country']=GDP_Density_data.Country.replace(\"korea\",\"korea,south\")\nGDP_Density_data['Country']=GDP_Density_data.Country.replace(\"uae\",\"unitedarabemirates\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.merge(train,GDP_Density_data, how='left',  left_on=['Country_Region'], right_on=['Country'])\ntest = pd.merge(test,GDP_Density_data, how='left',  left_on=['Country_Region'], right_on=['Country'])\ntrain.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#imputing 0's where data is not there\ntrain.fillna(0,inplace=True)\ntest.fillna(0,inplace=True)\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#dropping unwanted columns\ntrain=train.drop(['country','Country','Region'], axis = 1)\ntest=test.drop(['country','Country','Region'], axis = 1)\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#identifying numerical data and converting to float values\ncols=['ConfirmedCases','Fatalities','Population','Area (sq. mi.)','Pop. Density (per sq. mi.)'\n      ,'Coastline (coast/area ratio)','Net migration','Infant mortality (per 1000 births)',\n     'GDP ($ per capita)','Literacy (%)','Phones (per 1000)','Climate','Birthrate','Deathrate']\ncols_test=['Population','Area (sq. mi.)','Pop. Density (per sq. mi.)'\n      ,'Coastline (coast/area ratio)','Net migration','Infant mortality (per 1000 births)',\n     'GDP ($ per capita)','Literacy (%)','Phones (per 1000)','Climate','Birthrate','Deathrate']\n#from locale import atof\ntrain[cols]=train[cols].astype(str).apply(lambda x: x.str.replace(',', '').astype(float), axis=1)\ntest[cols_test]=test[cols_test].astype(str).apply(lambda x: x.str.replace(',', '').astype(float), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#dropping columns which not be required\ntrain=train.drop(['Area (sq. mi.)','Coastline (coast/area ratio)','Net migration','Industry','Phones (per 1000)','Arable (%)','Crops (%)','Other (%)','Climate','Agriculture','Service'], axis = 1)\ntest=test.drop(['Area (sq. mi.)','Coastline (coast/area ratio)','Net migration','Industry','Phones (per 1000)','Arable (%)','Crops (%)','Other (%)','Climate','Agriculture','Service'], axis = 1)\n\ntrain.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#imputing most commonly applied values\ndef fillna(col):\n    col.fillna(col.value_counts().index[0], inplace=True)\n    return col\ntrain=train.apply(lambda col:fillna(col))\ntest=test.apply(lambda col:fillna(col))\ntrain.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#will bin temparature into 9 bins to see if any impact on confirmed cases/fatalities\n\ntrain['quan']=pd.qcut(train['tempC'], q=[0,.1, .25, .5, .75,.80,.85,.90,.95,.99], labels=[1,2,3,4,5,6,7,8,9])\np=train[['tempC','quan','ConfirmedCases','Fatalities','Pop. Density (per sq. mi.)','Population']][(train.ConfirmedCases>0)]\nlists=['Fatalities','Pop. Density (per sq. mi.)']\n\np.groupby(p['quan'])['tempC'].mean()#.reset_index()\n\np=p[['quan','ConfirmedCases','tempC','Fatalities','Pop. Density (per sq. mi.)','Population']].groupby(p['quan'])['ConfirmedCases','tempC','Fatalities','Pop. Density (per sq. mi.)','Population'].max().reset_index()\np=pd.DataFrame(p)\n#sns.lineplot(x='tempC',y=lists, markers=True,data=p)\nax = p.plot(x=\"tempC\", y=\"ConfirmedCases\", legend=False,color='b')\nax2 = ax.twinx()\np.plot(x=\"tempC\", y=[\"Pop. Density (per sq. mi.)\",\"Fatalities\"],ax=ax2, legend=False,color=['r','y'])\nax.figure.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we see above there seems to more reported cases where temparature is less than 25 degress<br>\nALso we see density of population one more factor with tempearature cause for confirmed cases<br>\nWill have GDP and other social indicators also considered as in near future that may determine how things moves in another 2-3 months.<br>\nThere are 2 reasons to capture social/economic indicators like GDP and birth/mortality rate:<br>\na. Countries with better socio economic indicators may have more people travelling which may be the reson for spike in covid cases.<br>\nb. Poorer countries may have less travllers, but lacking proper testing and lack of health care may be a deterent, time will tell how this will progress in next 2-3 months.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#dropping quan\ntrain=train.drop(['quan'], axis = 1)\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#checing for correlation between different I/P parameters\ntrain.corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#'tempC' removing for now once ml imputer is implemented will bring this back\ntrain=train.drop(['tempC'], axis = 1)\ntest=test.drop(['tempC'], axis = 1)\n'''\nX=train[['Province_State', 'Country_Region', 'Date',  'Population', 'Pop. Density (per sq. mi.)',\n       'Infant mortality (per 1000 births)', 'GDP ($ per capita)',\n       'Literacy (%)', 'Birthrate', 'Deathrate']]\nX1=test[['Province_State', 'Country_Region', 'Date',  'Population', 'Pop. Density (per sq. mi.)',\n       'Infant mortality (per 1000 births)', 'GDP ($ per capita)',\n       'Literacy (%)', 'Birthrate', 'Deathrate']]   \n'''\nX=train[['Province_State', 'Country_Region', 'Date']]\nX1=test[['Province_State', 'Country_Region', 'Date']]   \n\nY_ConfirmedCases=train['ConfirmedCases']\nY_Fatalities=train['Fatalities']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler,LabelEncoder,StandardScaler\nautoscaler = LabelEncoder()\nX['Province_State']=autoscaler.fit_transform(X['Province_State'])\nX['Country_Region']=autoscaler.fit_transform(X['Country_Region'])\nX['Date']=autoscaler.fit_transform(X['Date'])\nX1['Province_State']=autoscaler.fit_transform(X1['Province_State'])\nX1['Country_Region']=autoscaler.fit_transform(X1['Country_Region'])\nX1['Date']=autoscaler.fit_transform(X1['Date'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nminmaxscale=StandardScaler()\n\nminmaxscale.fit(pd.concat([X,X1]))\n                              \ntrain_X=minmaxscale.fit_transform(X)\ntest_X=minmaxscale.fit_transform(X1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ntuned_models = [\n               RandomForestRegressor(n_estimators= 8,\n                               criterion= 'mse',\n                               max_features = 'log2',#log2\n                               min_samples_split = 60,\n                               random_state = 40)]  \ntuned_parameters = {    'base_estimator':tuned_models,\n                        'loss' : ['exponential']#exponential\n                        ,'random_state' : [43]\n                        ,'learning_rate' : [0.1]\n                         }\n#exponential\nclf_ConfirmedCases = GridSearchCV(AdaBoostRegressor(), tuned_parameters, cv=4)\nclf_ConfirmedCases.fit(train_X,Y_ConfirmedCases)\n\nclf_Fatalities = GridSearchCV(AdaBoostRegressor(), tuned_parameters, cv=4)\nclf_Fatalities.fit(train_X,Y_Fatalities)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_xgbrf_ConfirmedCases = clf_ConfirmedCases.predict(train_X)\nmetrics.r2_score(Y_ConfirmedCases,pred_xgbrf_ConfirmedCases)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_xgbrf_Fatalities = clf_Fatalities.predict(train_X)\nmetrics.r2_score(Y_Fatalities,pred_xgbrf_Fatalities)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_X1=train\ntrain_X1=pd.concat([pd.DataFrame(pred_xgbrf_ConfirmedCases,columns=['ConfirmedCases_Predicted']),train_X1],axis=1)\ntrain_X1=pd.concat([pd.DataFrame(pred_xgbrf_Fatalities,columns=['Fatalities_Predicted']),train_X1],axis=1)\ntrain_X1[train_X1.Country_Region=='india']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nclf_ConfirmedCases_pred_test=clf_ConfirmedCases.predict(test_X)\n\nclf_Fatalities_pred_test=clf_Fatalities.predict(test_X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ntest_X1=test\ntest_X1=pd.concat([pd.DataFrame(clf_ConfirmedCases_pred_test,columns=['ConfirmedCases_Predicted']),test_X1],axis=1)\ntest_X1=pd.concat([pd.DataFrame(clf_Fatalities_pred_test,columns=['Fatalities_Predicted']),test_X1],axis=1)\ntest_X1\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_X1[test_X1.Country_Region=='spain']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"output=test_X1[['ForecastId','ConfirmedCases_Predicted','Fatalities_Predicted']].astype('int')\noutput.columns=['ForecastId','ConfirmedCases_Predicted','Fatalities_Predicted']\noutput.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Working on few time series forecasting method like var, vecm and lag model will update the note book once done.<br>\nAlso will include few more analysis on univariate, bi-variate and multi variate by considering other related data sets like climate and socio- economic data.<br>\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}