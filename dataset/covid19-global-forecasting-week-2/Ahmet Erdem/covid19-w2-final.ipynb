{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom scipy.optimize import curve_fit\n\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\nimport matplotlib.pyplot as plt\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import log_loss\nfrom sklearn.preprocessing import OneHotEncoder\n\nimport xgboost as xgb\n\nfrom tensorflow.keras.optimizers import Nadam\nfrom sklearn.metrics import mean_squared_error\nimport tensorflow as tf\nimport tensorflow.keras.layers as KL\nfrom datetime import timedelta\nimport numpy as np\nimport pandas as pd","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def giba_model():\n    def exponential(x, a, k, b):\n        return a*np.exp(x*k) + b\n\n    def rmse( yt, yp ):\n        return np.sqrt( np.mean( (yt-yp)**2 ) )  \n\n    train = pd.read_csv('../input/covid19-global-forecasting-week-2/train.csv')\n    train['Date'] = pd.to_datetime( train['Date'] )\n    train['Province_State'] = train['Province_State'].fillna('')\n\n    test  = pd.read_csv('../input/covid19-global-forecasting-week-2/test.csv')\n    test['Date'] = pd.to_datetime( test['Date'] )\n    test['Province_State'] = test['Province_State'].fillna('')\n    test['Id'] = -1\n    test['ConfirmedCases'] = 0\n    test['Fatalities'] = 0\n\n    publictest = test.loc[ test.Date > train.Date.max() ].copy()\n    train = pd.concat( (train, publictest ) )\n\n    train['ForecastId'] = pd.merge( train, test, on=['Country_Region','Province_State','Date'], how='left' )['ForecastId_y'].values\n\n    train.sort_values( ['Country_Region','Province_State','Date'], inplace=True )\n    train = train.reset_index(drop=True)\n\n    train['cid'] = train['Country_Region'] + '_' + train['Province_State']\n\n\n    train['log0'] = np.log1p( train['ConfirmedCases'] )\n    train['log1'] = np.log1p( train['Fatalities'] )\n\n    train['log0'] = train.groupby('cid')['log0'].cummax()\n    train['log1'] = train.groupby('cid')['log1'].cummax()\n\n    train = train.loc[ (train.log0 > 0) | (train.ForecastId.notnull()) ].copy()\n    train = train.reset_index(drop=True)\n\n    train['day'] = train.groupby('cid')['Id'].cumcount()\n\n\n    def create_features( df, traindate, lag=1 ):\n        df['lag0_1'] = df.groupby('cid')['target0'].shift(lag)\n        df['lag1_1'] = df.groupby('cid')['target1'].shift(lag)\n        df['lag0_1'] = df.groupby('cid')['lag0_1'].fillna( method='bfill' )\n        df['lag1_1'] = df.groupby('cid')['lag1_1'].fillna( method='bfill' )\n\n        df['m0'] = df.groupby('cid')['lag0_1'].rolling(2).mean().values\n        df['m1'] = df.groupby('cid')['lag0_1'].rolling(3).mean().values\n        df['m2'] = df.groupby('cid')['lag0_1'].rolling(4).mean().values\n        df['m3'] = df.groupby('cid')['lag0_1'].rolling(5).mean().values\n        df['m4'] = df.groupby('cid')['lag0_1'].rolling(7).mean().values\n        df['m5'] = df.groupby('cid')['lag0_1'].rolling(10).mean().values\n        df['m6'] = df.groupby('cid')['lag0_1'].rolling(12).mean().values\n        df['m7'] = df.groupby('cid')['lag0_1'].rolling(16).mean().values\n        df['m8'] = df.groupby('cid')['lag0_1'].rolling(20).mean().values\n\n        df['n0'] = df.groupby('cid')['lag1_1'].rolling(2).mean().values\n        df['n1'] = df.groupby('cid')['lag1_1'].rolling(3).mean().values\n        df['n2'] = df.groupby('cid')['lag1_1'].rolling(4).mean().values\n        df['n3'] = df.groupby('cid')['lag1_1'].rolling(5).mean().values\n        df['n4'] = df.groupby('cid')['lag1_1'].rolling(7).mean().values\n        df['n5'] = df.groupby('cid')['lag1_1'].rolling(10).mean().values\n        df['n6'] = df.groupby('cid')['lag1_1'].rolling(12).mean().values\n        df['n7'] = df.groupby('cid')['lag1_1'].rolling(16).mean().values\n        df['n8'] = df.groupby('cid')['lag1_1'].rolling(20).mean().values\n\n        df['m0'] = df.groupby('cid')['m0'].fillna( method='bfill' )\n        df['m1'] = df.groupby('cid')['m1'].fillna( method='bfill' )\n        df['m2'] = df.groupby('cid')['m2'].fillna( method='bfill' )\n        df['m3'] = df.groupby('cid')['m3'].fillna( method='bfill' )\n        df['m4'] = df.groupby('cid')['m4'].fillna( method='bfill' )\n        df['m5'] = df.groupby('cid')['m5'].fillna( method='bfill' )\n        df['m6'] = df.groupby('cid')['m6'].fillna( method='bfill' )\n        df['m7'] = df.groupby('cid')['m7'].fillna( method='bfill' )\n        df['m8'] = df.groupby('cid')['m8'].fillna( method='bfill' )\n\n        df['n0'] = df.groupby('cid')['n0'].fillna( method='bfill' )\n        df['n1'] = df.groupby('cid')['n1'].fillna( method='bfill' )\n        df['n2'] = df.groupby('cid')['n2'].fillna( method='bfill' )\n        df['n3'] = df.groupby('cid')['n3'].fillna( method='bfill' )\n        df['n4'] = df.groupby('cid')['n4'].fillna( method='bfill' )\n        df['n5'] = df.groupby('cid')['n5'].fillna( method='bfill' )\n        df['n6'] = df.groupby('cid')['n6'].fillna( method='bfill' )\n        df['n7'] = df.groupby('cid')['n7'].fillna( method='bfill' )\n        df['n8'] = df.groupby('cid')['n8'].fillna( method='bfill' )\n\n        df['flag_China'] = 1*(df['Country_Region'] == 'China')\n        df['flag_Italy'] = 1*(df['Country_Region'] == 'Italy')\n        df['flag_Spain'] = 1*(df['Country_Region'] == 'Spain')\n        df['flag_US']    = 1*(df['Country_Region'] == 'US')\n        df['flag_Brazil']= 1*(df['Country_Region'] == 'Brazil')\n\n    #     ohe = OneHotEncoder(sparse=False)\n    #     country_ohe = ohe.fit_transform( df[['cid']] )\n    #     country_ohe = pd.DataFrame( country_ohe )\n    #     country_ohe.columns = df['cid'].unique().tolist()\n\n    #     df = pd.concat( ( df, country_ohe ), axis=1, sort=False )\n\n        tr = df.loc[ df.Date  < traindate ].copy()\n        vl = df.loc[ df.Date == traindate ].copy()\n\n        tr = tr.loc[ tr.lag0_1 > 0 ]\n\n        return tr, vl    \n    \n    def train_period( \n                    train, \n                    valid_days = ['2020-03-13'],\n                    lag = 1,\n                    seed = 1,\n                    ):\n\n        train['target0'] = np.log1p( train['ConfirmedCases'] )\n        train['target1'] = np.log1p( train['Fatalities'] )\n\n        param = {\n            'subsample': 0.80,\n            'colsample_bytree': 0.85,\n            'max_depth': 7,\n            'gamma': 0.000,\n            'learning_rate': 0.01,\n            'min_child_weight': 5.00,\n            'reg_alpha': 0.000,\n            'reg_lambda': 0.400,\n            'silent':1,\n            'objective':'reg:squarederror',\n            #'booster':'dart',\n            #'tree_method': 'gpu_hist',\n            'nthread': -1,\n            'seed': seed,\n            }\n\n        tr, vl = create_features( train.copy(), valid_days[0] , lag=lag )\n        features = [f for f in tr.columns if f not in [\n            'Id',\n            'ConfirmedCases',\n            'Fatalities',\n            'log0',\n            'log1',\n            'target0',\n            'target1',\n            'Province_State',\n            'Country_Region',\n            'Date',\n            'ForecastId',\n            'cid',\n            #'day',\n        ] ]\n\n        dtrain = xgb.DMatrix( tr[features], tr['target0'] )\n        dvalid = xgb.DMatrix( vl[features], vl['target0'] )\n        watchlist = [(dvalid, 'eval')]\n        model0 = xgb.train( param, dtrain, 767, watchlist , verbose_eval=0 )#, early_stopping_rounds=25 )\n\n        dtrain = xgb.DMatrix( tr[features], tr['target1'] )\n        dvalid = xgb.DMatrix( vl[features], vl['target1'] )\n        watchlist = [(dvalid, 'eval')]\n        model1 = xgb.train( param, dtrain, 767, watchlist , verbose_eval=0 )#, early_stopping_rounds=25 )\n\n        ypred0 = model0.predict( dvalid )\n        ypred1 = model1.predict( dvalid )\n        vl['ypred0'] = ypred0\n        vl['ypred1'] = ypred1\n\n        #walkforwarding scoring all dates\n        feats = ['Province_State','Country_Region','Date']\n        for day in valid_days:\n            tr, vl = create_features( train.copy(), day, lag=2 )\n            dvalid = xgb.DMatrix( vl[features] )\n            ypred0 = model0.predict( dvalid )\n            ypred1 = model1.predict( dvalid )\n            vl['ypred0'] = ypred0\n            vl['ypred1'] = ypred1\n\n            train[ 'ypred0' ] = pd.merge( train[feats], vl[feats+['ypred0']], on=feats, how='left' )['ypred0'].values\n            train.loc[ train.ypred0<0, 'ypred0'] = 0\n            train.loc[ train.ypred0.notnull(), 'target0'] = train.loc[ train.ypred0.notnull() , 'ypred0']\n\n            train[ 'ypred1' ] = pd.merge( train[feats], vl[feats+['ypred1']], on=feats, how='left' )['ypred1'].values\n            train.loc[ train.ypred1<0, 'ypred1'] = 0\n            train.loc[ train.ypred1.notnull(), 'target1'] = train.loc[ train.ypred1.notnull() , 'ypred1']\n\n            px = np.where( (train.Date==day ) )[0]\n            print( day, rmse( train['log0'].iloc[px], train['target0'].iloc[px] ), rmse( train['log1'].iloc[px], train['target1'].iloc[px] )  )\n\n        VALID = train.loc[ (train.Date>=valid_days[0])&(train.Date<=valid_days[-1]) ].copy() \n        del VALID['ypred0'],VALID['ypred1']\n\n        sc0 = rmse( VALID['log0'], VALID['target0'] )\n        sc1 = rmse( VALID['log1'], VALID['target1'] )\n        print( sc0, sc1, (sc0+sc1)/2 )\n\n        return VALID.copy()    \n    \n    \n    VALID0 = train_period( train, \n                          valid_days = ['2020-03-13','2020-03-14','2020-03-15','2020-03-16','2020-03-17','2020-03-18','2020-03-19','2020-03-20','2020-03-21','2020-03-22','2020-03-23','2020-03-24','2020-03-25','2020-03-26','2020-03-27','2020-03-28','2020-03-29','2020-03-30','2020-03-31'],\n                          lag = 1,\n                          seed = 1 )    \n    \n    VALID1 = train_period( train, \n                          valid_days = ['2020-03-16','2020-03-17','2020-03-18','2020-03-19','2020-03-20','2020-03-21','2020-03-22','2020-03-23','2020-03-24','2020-03-25','2020-03-26','2020-03-27','2020-03-28','2020-03-29','2020-03-30','2020-03-31'],\n                          lag = 1,\n                          seed = 1 )    \n    \n    VALID2 = train_period( train, \n                          valid_days = ['2020-03-19','2020-03-20','2020-03-21','2020-03-22','2020-03-23','2020-03-24','2020-03-25','2020-03-26','2020-03-27','2020-03-28','2020-03-29','2020-03-30','2020-03-31'],\n                          lag = 1,\n                          seed = 1 )    \n\n    VALID3 = train_period( train, \n                          valid_days = ['2020-03-22','2020-03-23','2020-03-24','2020-03-25','2020-03-26','2020-03-27','2020-03-28','2020-03-29','2020-03-30','2020-03-31'],\n                          lag = 1,\n                          seed = 1 )   \n        \n    sa0 = rmse( VALID0['log0'], VALID0['target0'] )\n    sa1 = rmse( VALID1['log0'], VALID1['target0'] )\n    sa2 = rmse( VALID2['log0'], VALID2['target0'] )\n    sa3 = rmse( VALID3['log0'], VALID3['target0'] )\n\n    sb0 = rmse( VALID0['log1'], VALID0['target1'] )\n    sb1 = rmse( VALID1['log1'], VALID1['target1'] )\n    sb2 = rmse( VALID2['log1'], VALID2['target1'] )\n    sb3 = rmse( VALID3['log1'], VALID3['target1'] )\n\n\n    print('13-31: ' + str(sa0)[:6] + ', ' + str(sb0)[:6] + ' = ' + str(0.5*sa0+0.5*sb0)[:6]  )\n    print('16-31: ' + str(sa1)[:6] + ', ' + str(sb1)[:6] + ' = ' + str(0.5*sa1+0.5*sb1)[:6]  )\n    print('19-31: ' + str(sa2)[:6] + ', ' + str(sb2)[:6] + ' = ' + str(0.5*sa2+0.5*sb2)[:6]  )\n    print('22-31: ' + str(sa3)[:6] + ', ' + str(sb3)[:6] + ' = ' + str(0.5*sa3+0.5*sb3)[:6]  )\n\n    print( 'Avg: ',  (sa0+sb0+sa1+sb1+sa2+sb2+sa3+sb3) / 8 )\n    \n    TEST  = train_period( train, \n                          valid_days = ['2020-04-01','2020-04-02','2020-04-03','2020-04-04','2020-04-05','2020-04-06','2020-04-07','2020-04-08','2020-04-09','2020-04-10',\n                                        '2020-04-11','2020-04-12','2020-04-13','2020-04-14','2020-04-15','2020-04-16','2020-04-17','2020-04-18','2020-04-19','2020-04-20',\n                                        '2020-04-21','2020-04-22','2020-04-23','2020-04-24','2020-04-25','2020-04-26','2020-04-27','2020-04-28','2020-04-29','2020-04-30'],\n                          lag = 1,\n                          seed = 1 )    \n    \n    VALID2_sub = VALID2.copy()\n    VALID2_sub['target0'] = np.log1p( VALID2_sub['ConfirmedCases']  )\n    VALID2_sub['target1'] = np.log1p( VALID2_sub['Fatalities']  )\n    sub = pd.concat( (VALID2_sub,TEST.loc[ TEST.Date>='2020-04-01' ] ) )\n\n    sub = sub[['ForecastId','target0','target1']]\n    sub.columns = ['ForecastId','ConfirmedCases','Fatalities']\n    sub['ForecastId'] = sub['ForecastId'].astype( np.int )\n    sub['ConfirmedCases'] = np.expm1( sub['ConfirmedCases'] )\n    sub['Fatalities'] = np.expm1( sub['Fatalities'] )\n    print( sub.describe()  )\n    \n    # :21 / 22:31\n    # :18 / 19:31\n    # :15 / 16:31\n    # :12 / 13:31\n    VALID0.to_csv('fold-13-31.csv', index=False)\n    VALID1.to_csv('fold-16-31.csv', index=False)\n    VALID2.to_csv('fold-19-31.csv', index=False)\n    VALID3.to_csv('fold-22-31.csv', index=False)\n    TEST.to_csv('fold-submission.csv', index=False)    \n    \n    return sub\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_nn_sub():\n    df = pd.read_csv(\"../input/covid19-global-forecasting-week-2/train.csv\")\n    sub_df = pd.read_csv(\"../input/covid19-global-forecasting-week-2/test.csv\")\n\n    coo_df = pd.read_csv(\"../input/covid19week1/train.csv\").rename(columns={\"Country/Region\": \"Country_Region\"})\n    coo_df = coo_df.groupby(\"Country_Region\")[[\"Lat\", \"Long\"]].mean().reset_index()\n    coo_df = coo_df[coo_df[\"Country_Region\"].notnull()]\n\n    loc_group = [\"Province_State\", \"Country_Region\"]\n\n\n    def preprocess(df):\n        df[\"Date\"] = df[\"Date\"].astype(\"datetime64[ms]\")\n        df[\"days\"] = (df[\"Date\"] - pd.to_datetime(\"2020-01-01\")).dt.days\n        df[\"weekend\"] = df[\"Date\"].dt.dayofweek//5\n\n        df = df.merge(coo_df, how=\"left\", on=\"Country_Region\")\n        df[\"Lat\"] = (df[\"Lat\"] // 30).astype(np.float32).fillna(0)\n        df[\"Long\"] = (df[\"Long\"] // 60).astype(np.float32).fillna(0)\n\n        for col in loc_group:\n            df[col].fillna(\"none\", inplace=True)\n        return df\n\n    df = preprocess(df)\n    sub_df = preprocess(sub_df)\n\n    print(df.shape)\n\n    TARGETS = [\"ConfirmedCases\", \"Fatalities\"]\n\n    for col in TARGETS:\n        df[col] = np.log1p(df[col])\n\n    NUM_SHIFT = 5\n\n    features = [\"Lat\", \"Long\"]\n\n    for s in range(1, NUM_SHIFT+1):\n        for col in TARGETS:\n            df[\"prev_{}_{}\".format(col, s)] = df.groupby(loc_group)[col].shift(s)\n            features.append(\"prev_{}_{}\".format(col, s))\n\n    df = df[df[\"Date\"] >= df[\"Date\"].min() + timedelta(days=NUM_SHIFT)].copy()\n\n    TEST_FIRST = sub_df[\"Date\"].min() # pd.to_datetime(\"2020-03-13\") #\n    TEST_DAYS = (df[\"Date\"].max() - TEST_FIRST).days + 1\n\n    dev_df, test_df = df[df[\"Date\"] < TEST_FIRST].copy(), df[df[\"Date\"] >= TEST_FIRST].copy()\n\n    def nn_block(input_layer, size, dropout_rate, activation):\n        out_layer = KL.Dense(size, activation=None)(input_layer)\n        #out_layer = KL.BatchNormalization()(out_layer)\n        out_layer = KL.Activation(activation)(out_layer)\n        out_layer = KL.Dropout(dropout_rate)(out_layer)\n        return out_layer\n\n\n    def get_model():\n        inp = KL.Input(shape=(len(features),))\n\n        hidden_layer = nn_block(inp, 64, 0.0, \"relu\")\n        gate_layer = nn_block(hidden_layer, 32, 0.0, \"sigmoid\")\n        hidden_layer = nn_block(hidden_layer, 32, 0.0, \"relu\")\n        hidden_layer = KL.multiply([hidden_layer, gate_layer])\n\n        out = KL.Dense(len(TARGETS), activation=\"linear\")(hidden_layer)\n\n        model = tf.keras.models.Model(inputs=[inp], outputs=out)\n        return model\n\n    get_model().summary()\n\n    def get_input(df):\n        return [df[features]]\n\n    NUM_MODELS = 10\n\n\n    def train_models(df, save=False):\n        models = []\n        for i in range(NUM_MODELS):\n            model = get_model()\n            model.compile(loss=\"mean_squared_error\", optimizer=Nadam(lr=1e-4))\n            hist = model.fit(get_input(df), df[TARGETS],\n                             batch_size=2048, epochs=500, verbose=0, shuffle=True)\n            if save:\n                model.save_weights(\"model{}.h5\".format(i))\n            models.append(model)\n        return models\n\n    models = train_models(dev_df)\n\n\n    prev_targets = ['prev_ConfirmedCases_1', 'prev_Fatalities_1']\n\n    def predict_one(df, models):\n        pred = np.zeros((df.shape[0], 2))\n        for model in models:\n            pred += model.predict(get_input(df))/len(models)\n        pred = np.maximum(pred, df[prev_targets].values)\n        pred[:, 0] = np.log1p(np.expm1(pred[:, 0]) + 0.1)\n        pred[:, 1] = np.log1p(np.expm1(pred[:, 1]) + 0.01)\n        return np.clip(pred, None, 15)\n\n    print([mean_squared_error(dev_df[TARGETS[i]], predict_one(dev_df, models)[:, i]) for i in range(len(TARGETS))])\n\n\n    def rmse(y_true, y_pred):\n        return np.sqrt(mean_squared_error(y_true, y_pred))\n\n    def evaluate(df):\n        error = 0\n        for col in TARGETS:\n            error += rmse(df[col].values, df[\"pred_{}\".format(col)].values)\n        return np.round(error/len(TARGETS), 5)\n\n\n    def predict(test_df, first_day, num_days, models, val=False):\n        temp_df = test_df.loc[test_df[\"Date\"] == first_day].copy()\n        y_pred = predict_one(temp_df, models)\n\n        for i, col in enumerate(TARGETS):\n            test_df[\"pred_{}\".format(col)] = 0\n            test_df.loc[test_df[\"Date\"] == first_day, \"pred_{}\".format(col)] = y_pred[:, i]\n\n        print(first_day, np.isnan(y_pred).sum(), y_pred.min(), y_pred.max())\n        if val:\n            print(evaluate(test_df[test_df[\"Date\"] == first_day]))\n\n\n        y_prevs = [None]*NUM_SHIFT\n\n        for i in range(1, NUM_SHIFT):\n            y_prevs[i] = temp_df[['prev_ConfirmedCases_{}'.format(i), 'prev_Fatalities_{}'.format(i)]].values\n\n        for d in range(1, num_days):\n            date = first_day + timedelta(days=d)\n            print(date, np.isnan(y_pred).sum(), y_pred.min(), y_pred.max())\n\n            temp_df = test_df.loc[test_df[\"Date\"] == date].copy()\n            temp_df[prev_targets] = y_pred\n            for i in range(2, NUM_SHIFT+1):\n                temp_df[['prev_ConfirmedCases_{}'.format(i), 'prev_Fatalities_{}'.format(i)]] = y_prevs[i-1]\n\n            y_pred, y_prevs = predict_one(temp_df, models), [None, y_pred] + y_prevs[1:-1]\n\n\n            for i, col in enumerate(TARGETS):\n                test_df.loc[test_df[\"Date\"] == date, \"pred_{}\".format(col)] = y_pred[:, i]\n\n            if val:\n                print(evaluate(test_df[test_df[\"Date\"] == date]))\n\n        return test_df\n\n    test_df = predict(test_df, TEST_FIRST, TEST_DAYS, models, val=True)\n    print(evaluate(test_df))\n\n    for col in TARGETS:\n        test_df[col] = np.expm1(test_df[col])\n        test_df[\"pred_{}\".format(col)] = np.expm1(test_df[\"pred_{}\".format(col)])\n\n    models = train_models(df, save=True)\n\n    sub_df_public = sub_df[sub_df[\"Date\"] <= df[\"Date\"].max()].copy()\n    sub_df_private = sub_df[sub_df[\"Date\"] > df[\"Date\"].max()].copy()\n\n    pred_cols = [\"pred_{}\".format(col) for col in TARGETS]\n    #sub_df_public = sub_df_public.merge(test_df[[\"Date\"] + loc_group + pred_cols].rename(columns={col: col[5:] for col in pred_cols}), \n    #                                    how=\"left\", on=[\"Date\"] + loc_group)\n    sub_df_public = sub_df_public.merge(test_df[[\"Date\"] + loc_group + TARGETS], how=\"left\", on=[\"Date\"] + loc_group)\n\n    SUB_FIRST = sub_df_private[\"Date\"].min()\n    SUB_DAYS = (sub_df_private[\"Date\"].max() - sub_df_private[\"Date\"].min()).days + 1\n\n    sub_df_private = df.append(sub_df_private, sort=False)\n\n    for s in range(1, NUM_SHIFT+1):\n        for col in TARGETS:\n            sub_df_private[\"prev_{}_{}\".format(col, s)] = sub_df_private.groupby(loc_group)[col].shift(s)\n\n    sub_df_private = sub_df_private[sub_df_private[\"Date\"] >= SUB_FIRST].copy()\n\n    sub_df_private = predict(sub_df_private, SUB_FIRST, SUB_DAYS, models)\n\n    for col in TARGETS:\n        sub_df_private[col] = np.expm1(sub_df_private[\"pred_{}\".format(col)])\n\n    sub_df = sub_df_public.append(sub_df_private, sort=False)\n    sub_df[\"ForecastId\"] = sub_df[\"ForecastId\"].astype(np.int16)\n\n    return sub_df[[\"ForecastId\"] + TARGETS]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub1 = giba_model()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub2 = get_nn_sub()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub1.sort_values(\"ForecastId\", inplace=True)\nsub2.sort_values(\"ForecastId\", inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\n\nTARGETS = [\"ConfirmedCases\", \"Fatalities\"]\n\n[np.sqrt(mean_squared_error(np.log1p(sub1[t].values), np.log1p(sub2[t].values))) for t in TARGETS]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df = sub1.copy()\nfor t in TARGETS:\n    sub_df[t] = np.expm1(np.log1p(sub1[t].values)*0.5 + np.log1p(sub2[t].values)*0.5)\n    \nsub_df.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"}},"nbformat":4,"nbformat_minor":4}