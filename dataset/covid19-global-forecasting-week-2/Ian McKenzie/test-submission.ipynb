{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport sklearn as sk\nimport matplotlib.pyplot as plt\nimport datetime as dt","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data exploration"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"../input/covid19-global-forecasting-week-2/train.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = train\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"countries = df[\"Country_Region\"].unique()\nprint(f\"There are {len(countries)} countries in the dataset\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[pd.notna(df[\"Province_State\"])][\"Country_Region\"].unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So the data for these 8 countries is further subdivided into regions."},{"metadata":{"trusted":true},"cell_type":"code","source":"df[df[\"Country_Region\"] == \"United Kingdom\"][\"Province_State\"].unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"But that's because they have discontiguous territories. Is that the case for all Province_State pairs?"},{"metadata":{"trusted":true},"cell_type":"code","source":"df[df[\"Country_Region\"] == \"US\"][\"Province_State\"].unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Nope, the US is divided into states. I presume it's just the smallest blocks they have data for."},{"metadata":{},"cell_type":"markdown","source":"What are we trying to predict? Let's look at the test and submission files"},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv(\"../input/covid19-global-forecasting-week-2/train.csv\")\nsub = pd.read_csv(\"../input/covid19-global-forecasting-week-2/submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We want to predict the (cumulative) number of confirmed cases and fatalities in all those countries (and more specifically province/state if available) for given days in April"},{"metadata":{},"cell_type":"markdown","source":"# Data preparation\n\nFirst thing to try is combining the country and province columns. That way we will just have a \"region\" column and there won't be some with two levels and some with only one. We can then one-hot encode the whole thing, which will make a huge dataframe but hopefully it's worth it."},{"metadata":{"trusted":true},"cell_type":"code","source":"# function to add an underscore to non-na values and to replace nas with empty trying\ndef process_province(x):\n    if not pd.isna(x):\n        x = \"_\" + x\n    else:\n        x = ''\n    return x\n\ndf[\"Province_State\"] = df[\"Province_State\"].apply(process_province)\ndf[\"Region\"] = df[\"Country_Region\"] + df[\"Province_State\"]\n# saving unique regions for later\nregions = df[\"Region\"].unique()\ndf = df.drop([\"Province_State\", \"Country_Region\"], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we import the lockdown data and preprocess it to match. For a first approximation we'll pretend that all lockdowns are equal and drop the \"Type\" column"},{"metadata":{"trusted":true},"cell_type":"code","source":"lockdown = pd.read_csv(\"../input/covid19-lockdown-dates-by-country/countryLockdowndates.csv\")\nlockdown.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# dropping reference and type columns\nlockdown = lockdown.drop([\"Type\", \"Reference\"], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lockdown[\"Province\"] = lockdown[\"Province\"].apply(process_province)\nlockdown[\"Region\"] = lockdown[\"Country/Region\"] + lockdown[\"Province\"]\nlockdown = lockdown.drop([\"Province\", \"Country/Region\"], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# preparing the date column to be readable\n# lockdownlockdown[\"Date\"].fillna()\nlockdown[\"Date\"] = pd.to_datetime(lockdown[\"Date\"], dayfirst=True)\nlockdown = lockdown.rename({\"Region\": \"Region\",\"Date\": \"Lockdown_Date\"}, axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lockdown = lockdown.set_index(\"Region\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.assign(Lockdown_Date = '')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.set_index(\"Region\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.update(lockdown)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"Lockdown_Date\"] = pd.to_datetime(df[\"Lockdown_Date\"])\ndf[\"Date\"] = pd.to_datetime(df[\"Date\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature engineering\n\nLockdown seems to have a roughly two week lag in affecting the death rate/*confirmed* cases, so I've added that as a feature"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.assign(Two_Weeks_Lockdown = (df[\"Date\"] - df[\"Lockdown_Date\"] >= dt.timedelta(days=14)).astype(int))\ndf = df.drop(\"Lockdown_Date\", axis=1)\n\n# after all this mess we reset the index\ndf = df.reset_index()\ndf = df.set_index(\"Id\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"Two_Weeks_Lockdown\"].value_counts()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# seems to be working as italy entered lockdown 2020-3-11\ndf[df[\"Region\"] == \"Italy\"][50:]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will use autoregression, where we build a series of previous steps to use to predict the next, and extend this into the future. This requires some more set up of the data matrix though."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Establishing the number of 'lags' to keep track of \nP = 10","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def lag_dataset(df_orig, y, p=P):\n    \"\"\"\n    Function to lag this specific dataset, doesn't work in general.\n    INPUT\n    df_orig : the pandas dataframe of cases etc. (pd.DataFrame)\n    y : which column we are interested in (str)\n    p : how many shifted columns to make (int)\n    OUTPUT\n    lagged_df : the lagged dataframe (pd.DataFrame)\n    \"\"\"\n    df = df_orig.copy()\n    df = df.reset_index().set_index([\"Id\",\"Date\"])\n    # we have to treat each region individually\n    types = df[\"Region\"].unique()\n    lagged_df = pd.DataFrame()\n    \n    for t in types:\n        values = pd.DataFrame(df[df[\"Region\"]==t][y])\n        lagged_y = pd.concat([values.shift(s) for s in range(p+1)], axis=1)\n        lagged_y.columns = ['t']+['t-'+str(s) for s in range(1,p+1)]\n        lagged_y = lagged_y[p:]\n        lagged_y = lagged_y.assign(Region=t)\n        lagged_df = pd.concat([lagged_df, lagged_y])\n    # merging in stages\n    lagged_df = lagged_df.reset_index()\n    lagged_df = lagged_df.merge(df.drop([\"ConfirmedCases\", \"Fatalities\"], axis=1), left_on=[\"Region\",\"Date\"], right_on=[\"Region\", \"Date\"],suffixes=(None, None))\n    lagged_df = lagged_df.set_index([\"Id\"])\n    # done merging\n    return lagged_df\nfatalities_lagged = lag_dataset(df, \"Fatalities\", p=P)\ncases_lagged = lag_dataset(df, \"ConfirmedCases\", p=P)\nfatalities_lagged[fatalities_lagged[\"Region\"] == \"Italy\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# now we one-hot encode the \"Region\" column, which will give us 293 new columns. Yay.\nfatalities_df = pd.get_dummies(fatalities_lagged, \"Region\")\ncases_df = pd.get_dummies(cases_lagged, \"Region\")\nfatalities_df.info()\nfatalities_df.head()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Evaluation\nThe kaggle competition uses root mean squared logarithmic error (RMSLE).\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# rmsle where p is predicted and a is actual value\n# INPUT\n# p : 1d list or array of predictions\n# a : 1d list or array of actual values\ndef rmsle(p, a):\n    n = len(p)\n    sigma = [(np.log(p[i] + 1) - np.log(a[i] + 1))**2 for i in range(n)]\n    return np.sqrt(1/n * np.sum(sigma))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predicting the future for a certain region\ndef predict_region(reg_df, target, clf, p=P):\n    # adding rows onto the end until we get to the target\n    while reg_df.iloc[-1][\"Date\"] < target:\n        last_row = reg_df.iloc[-1].copy()\n        last_row[\"Date\"] += pd.Timedelta('1 day')\n        # moving the ts back and predicting the next one\n        for i in range(p, 0, -1):\n            if i > 1:\n                first = \"t-\" + str(i)\n                second = \"t-\" + str(i-1)\n            else:\n                first = \"t-\" + str(i)\n                second = \"t\"\n            last_row[first] = last_row[second]\n        last_row[\"t\"] = clf.predict(last_row.drop([\"t\", \"Date\"]).to_numpy().reshape(1,-1))[0]\n        # we can work inplace because this is a copy\n        reg_df = reg_df.append(last_row)\n    return reg_df\n\n# using PRE TRAINED clf, predict the fatalities or cases between start and stop\ndef predict_region_between(df, region, start, stop, clf, p=P):\n    # evaluating only on one region between start and stop\n    reg_df = df[df[f\"Region_{region}\"] == 1]\n    reg_df = reg_df[reg_df[\"Date\"] < start].copy()\n    return predict_region(reg_df, stop, clf, p)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neural_network import MLPRegressor\n\n# We train two separate mlps:\n# one on data up to 19/3 for predicting to the end of March and\n# one on ALL the data for predicting April\nstart_march = np.datetime64('2020-03-19')\nend_march = np.datetime64('2020-03-31')\nend_april = np.datetime64('2020-04-30')\n\n# FATALITIES MLPS AND TRAINING SETS\nmlp_some_f = MLPRegressor(verbose=True)\nmlp_all_f = MLPRegressor(verbose=True)\n\n# training data is ALL regions with dates before start\nX_some_f = fatalities_df[fatalities_df[\"Date\"] < start_march].drop([\"t\", \"Date\"], axis=1).to_numpy()\ny_some_f = fatalities_df[fatalities_df[\"Date\"] < start_march][\"t\"]\n\nX_all_f = fatalities_df.drop([\"t\", \"Date\"], axis=1).to_numpy()\ny_all_f = fatalities_df[\"t\"]\n\nmlp_some_f.fit(X_some_f, y_some_f)\nmlp_all_f.fit(X_all_f, y_all_f)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# CASES MLPS AND TRAINING SETS\nmlp_some_c = MLPRegressor(verbose=True)\nmlp_all_c = MLPRegressor(verbose=True)\n\n# training data is ALL regions with dates before start\nX_some_c = cases_df[cases_df[\"Date\"] < start_march].drop([\"t\", \"Date\"], axis=1).to_numpy()\ny_some_c = cases_df[cases_df[\"Date\"] < start_march][\"t\"]\n\nX_all_c = cases_df.drop([\"t\", \"Date\"], axis=1).to_numpy()\ny_all_c = cases_df[\"t\"]\n\nmlp_some_c.fit(X_some_c, y_some_c)\nmlp_all_c.fit(X_all_c, y_all_c)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def graph_region_between(df, region, start, stop, clf, p=P, title=None):\n    pred_df = predict_region_between(df, region, start, stop, clf)\n    reg_preds = pred_df[pred_df[\"Date\"]>=start]\n    reg_true = df[df[f\"Region_{region}\"]==1]\n    reg_true = reg_true[reg_true[\"Date\"] >= start]\n    \n    plt.scatter(reg_true[\"t\"], reg_preds[\"t\"])\n    plt.xlabel(f\"True # cases in {region}\")\n    plt.ylabel(f\"Predicted # cases in {region}\")\n    if title is None:\n        plt.title(f\"{region} from {start} to {stop}\")\n    else:\n        plt.title(title)\n    plt.plot(range(int(max(max(reg_true[\"t\"]), max(reg_preds[\"t\"])))))\n    plt.show()\n\ngraph_region_between(cases_df, \"Italy\" ,start_march, end_march, mlp_some_c, p=P, title=\"Cases 19-31/3\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# evaluating on the whole test data set\ny_hat_f = pd.DataFrame()\nfor region in regions:\n    print(region)\n    # predicting to end of march and to end of april separately\n    f_some_df = predict_region_between(fatalities_df, region, start_march, end_march, mlp_some_f)\n    f_some_preds = f_some_df[f_some_df[\"Date\"]>=start_march][\"t\"]\n    \n    # predicting to end of march and to end of april separately\n    f_all_df = predict_region_between(fatalities_df, region, end_march, end_april, mlp_all_f)\n    f_all_preds = f_all_df[f_all_df[\"Date\"]>end_march][\"t\"]\n\n    y_hat_f = pd.concat([y_hat_f, f_some_preds, f_all_preds])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# evaluating on the whole test data set\ny_hat_c = pd.DataFrame()\nfor region in regions:\n    print(region)\n    # predicting to end of march and to end of april separately\n    c_some_df = predict_region_between(cases_df, region, start_march, end_march, mlp_some_c)\n    c_some_preds = c_some_df[c_some_df[\"Date\"]>=start_march][\"t\"]\n    \n    # predicting to end of march and to end of april separately\n    c_all_df = predict_region_between(cases_df, region, end_march, end_april, mlp_all_c)\n    c_all_preds = c_all_df[c_all_df[\"Date\"]>end_march][\"t\"]\n\n    y_hat_c = pd.concat([y_hat_c, c_some_preds, c_all_preds])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def scatter_y_yhat(y, yhat):\n    plt.scatter(y, yhat)\n    plt.xlabel(f\"Actual values\")\n    plt.ylabel(f\"Predicted values\")\n    plt.title(f\"Scatter of predictions vs true values, centre line is perfect prediction\")\n    plt.plot(range(int(max(max(y), max(yhat)))))\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_hat_f.columns = [\"Fatalities\"]\ny_hat_c.columns = [\"ConfirmedCases\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_h_f  = y_hat_f.copy().reset_index()\ny_h_c  = y_hat_c.copy().reset_index()\n\nprint(len(y_h_f), len(sub))\nsub.update(y_h_f)\nsub.update(y_h_c)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.to_csv(\"submission.csv\", index=False)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{},"version_major":2,"version_minor":0}}},"nbformat":4,"nbformat_minor":4}