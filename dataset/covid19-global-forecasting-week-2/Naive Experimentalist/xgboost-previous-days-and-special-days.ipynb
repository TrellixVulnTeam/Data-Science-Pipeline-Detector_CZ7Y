{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nimport seaborn as sns\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\nfrom os import listdir\nfrom os.path import isfile, join\n\nbase_folder = '/kaggle/input/'\n\n# loading day-by-day data (based on hopkins datasets) prepared by Kaggle\n# (it contains 'ConfirmedCases' and 'Fatalities', but doesn't contain 'Recovered')\ndata_base = base_folder + 'covid19-global-forecasting-week-2/'\ndf = pd.read_csv(data_base + 'train.csv')\ndf.rename(columns={'Province_State': 'Province/State', 'Country_Region': 'Country/Region'}, inplace=True)\ndf['Province/State'].fillna('entire country', inplace=True)\n\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def add_extra_features_from_previous_days(data_fr, tail_size=5):\n    cols_tmp = []\n    col_prefix = 'PreviousDay'\n    for i in range (0, tail_size):\n        col_cc = '{}-{}ConfirmedCases'.format(col_prefix, i)\n        col_f  = '{}-{}Fatalities'.format(col_prefix, i)\n\n        data_fr[col_cc] = data_fr.groupby(['Country/Region', 'Province/State'])['ConfirmedCases'].shift(periods=i+1, fill_value=0)\n        data_fr[col_f] = data_fr.groupby(['Country/Region', 'Province/State'])['Fatalities'].shift(periods=i+1, fill_value=0)\n        data_fr[col_cc + 'Delta'] = data_fr.groupby(['Country/Region', 'Province/State'])[col_cc].diff().fillna(0)\n        data_fr[col_f + 'Delta'] = data_fr.groupby(['Country/Region', 'Province/State'])[col_f].diff().fillna(0)\n        cols_tmp += [col_cc, col_f, col_cc + 'Delta', col_f + 'Delta']\n    # df['PreviousDay-0ConfirmedCases'] = df.groupby(['Country/Region', 'Province/State'])['ConfirmedCases'].shift(periods=1, fill_value=0)\n    return  cols_tmp\n\n# creating extra features from the history: previous day, previous day -1, previous day -2 ...\nTAIL = 28\nprevious_days_cols = add_extra_features_from_previous_days(df, TAIL)\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['PreviousDay-0ConfirmedCases'].max()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# calculating day \"zero\" for every country\nday_zero = datetime.strptime(min(df['Date']), '%Y-%m-%d')\ndf['DayNum'] = (df['Date'].astype('datetime64[ns]') - day_zero).apply(lambda x: int(x.days))\ndf['DayZero'] = df.where((df['ConfirmedCases'] > 0) & ((df['PreviousDay-0ConfirmedCases'] == 0)|(df['Date'] == day_zero)))['DayNum']\ndf['DayZero'] = df.groupby(['Country/Region', 'Province/State'])['DayZero'].ffill()\ndf['DayZero'] = df.groupby(['Country/Region', 'Province/State'])['DayZero'].bfill()\n# calculating real DayNum counted from the day \"zero\"\nreal_day_num = df['DayNum'] - df['DayZero'] + 1\ndf['RealDayNum'] = real_day_num - real_day_num.where(real_day_num<0).fillna(0)\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# loading population data \ndf_population = pd.read_csv(base_folder + 'world-populaton/all_population.csv', delimiter=';', decimal=',', na_values='N.A.')\n# urban population: NaNs with 100% (it's a good estimation!)\ndf_population['Urban Pop'] = df_population['Urban Pop'].fillna(100.0)\n# OHE for a continent\ndf_population = pd.get_dummies(df_population, columns=['Continent'])\n# let's remember new columns for continents\ncontinent_columns = []\nfor c in df_population.columns:\n    if 'Continent_' in c:\n        continent_columns.append(c)\ndf_population","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Countries names map between World By Map and Hopking datasets\ncountries_to_replace = [\n    ('Czech Republic', 'Czechia'),\n    ('United States of America', 'US'),\n    ('Côte d\\'Ivoire (Ivory Coast)', 'Côte d\\'Ivoire'),\n    ('Korea (South)', 'Korea, South'),\n    ('Swaziland', 'Eswatini'),\n    ('Gambia', 'The Gambia'),\n    ('Myanmar (Burma)', 'Myanmar'),\n    ('East Timor', 'Timor-Leste'),\n    ('Macedonia', 'North Macedonia'),\n    ('Cape Verde', 'Cabo Verde'),\n    ('Congo (Republic)', 'Congo (Brazzaville)'),\n    ('Congo (Democratic Republic)', 'Congo (Kinshasa)'),\n    ('Palestinian Territories', 'State of Palestine'),\n    ('Bahamas', 'The Bahamas'),\n    ('United Kingdom of Great Britain and Northern Ireland', 'United Kingdom'),\n    ('Vatican City', 'Holy See')\n]\n# loading different datasets from World By Map\ncsv_dir = base_folder + 'worldbymap/'\nfiles = [\n    'labor_force',\n    'death_rate',\n    'air_traffic_passengers',\n    'hospital_bed_density',\n    'obesity',\n    'old_people',\n    'physicians_density'\n]\nwbm = {}\nfor f in files:\n    wbm[f] = pd.read_csv(csv_dir + f + '.csv', delimiter=';', decimal=',', na_values='N.A.')\n    for ctr in countries_to_replace:\n        wbm[f] = wbm[f].replace(ctr[0], ctr[1])\nwbm[files[0]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_add = pd.DataFrame()\nfor dataset in wbm.keys():\n    if df_add.shape == (0, 0):\n        df_add = wbm[dataset].copy()\n    else:\n        df_add = df_add.merge(wbm[dataset], on='Country', how='left')\ndf_add.rename(columns={\"Country\": \"Country/Region\"}, inplace=True)\ndf_add","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_external = pd.merge(df_population, df_add, on='Country/Region', how='left')\n# df_external = df_population.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# merging covid dataset with additional external data\ndf_pop = pd.merge(df, df_external, on=['Country/Region', 'Province/State'], how='left')\ndf_pop","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error, mean_squared_log_error\nfrom sklearn.preprocessing import MinMaxScaler\nfrom scipy import stats\n\next_cols = ['LaborForceTotal', 'LaborForcePerCapita ', 'DeathRate', 'AirTrafficPassengersTotal',\n            'AirTrafficPassengersPerCapita', 'HospitalBedDensity', 'Obesity', 'OldPeople',\n            'PhysiciansDensity']\n# ext_cols = []\npop_cols = ['DayNum', 'RealDayNum', 'Yearly change', 'Density', 'Land Area', 'Med. Age', 'Urban Pop', 'Population']\npop_cols = ['DayNum', 'RealDayNum', 'Med. Age', 'Urban Pop']\n\nmodel_x_columns_without_dummies = pop_cols + ext_cols + previous_days_cols\nmodel_x_columns = model_x_columns_without_dummies + continent_columns\n\n# let's define an evaluation metric\ndef rmsle(ytrue, ypred):\n    return np.sqrt(mean_squared_log_error(ytrue, ypred))\n\ndef mae(ytrue, ypred):\n    return mean_absolute_error(ytrue, ypred)\n\n# checking how the model predicts test data\ndef analyse(data_y_test, data_y_pred):\n    chart_data = pd.DataFrame({'x1': data_y_test.flatten(),\n                               'x2': data_y_pred.flatten(),\n                               'y': np.abs(data_y_test.flatten()-data_y_pred.flatten()).flatten()})\n    sns.scatterplot(x='x1', y='y', data=chart_data, color='black')\n    sns.scatterplot(x='x2', y='y', data=chart_data, color='red')\n    print('RMSLE: {}'.format(round(rmsle(data_y_test, data_y_pred), 6)))\n\ndef analyse2(tr_y, tr_pred, data_y_test, data_y_pred):\n    chart_data0 = pd.DataFrame({\n        'x00': tr_y.flatten(),\n        'x01': tr_pred.flatten(),\n        'y0': np.abs(tr_y.flatten()-tr_pred.flatten()).flatten()})\n\n    chart_data1 = pd.DataFrame({\n        'x10': data_y_test.flatten(),\n        'x11': data_y_pred.flatten(),\n        'y1': np.abs(data_y_test.flatten()-data_y_pred.flatten()).flatten()})\n    \n    fig, ax =plt.subplots(1,2)\n    sns.scatterplot(x='x00', y='y0', data=chart_data0, color='blue', ax=ax[0])\n    sns.scatterplot(x='x01', y='y0', data=chart_data0, color='yellow', ax=ax[0])\n    sns.scatterplot(x='x10', y='y1', data=chart_data1, color='black', ax=ax[1])\n    sns.scatterplot(x='x11', y='y1', data=chart_data1, color='red', ax=ax[1])\n    \n    print('RMSLE train: {}'.format(round(rmsle(tr_y, tr_pred), 6)))\n    print('RMSLE test:  {}'.format(round(rmsle(data_y_test, data_y_pred), 6)))\n\n\ndef prepare_data(df, what_to_predict, test_size=0.3, dropna=False):\n    df_tmp = df.copy()\n    \n    if dropna:\n        df_tmp.dropna(inplace=True)\n        \n    # preparing X and y datasets for output model training\n    data_X = df_tmp[model_x_columns]\n    data_y = np.log1p(df_tmp[[what_to_predict]].values.flatten())\n    # splitting data to train and test\n    return train_test_split(data_X, data_y, test_size=test_size, random_state=42)\n    \ndef predict_output(input_data, model):\n    df_final = input_data[model_x_columns].copy()\n    y_pred = model.predict(df_final)\n    return y_pred\n\ndef expm1_relu(y):\n    tmp = np.expm1(y)\n    tmp[tmp<0]=0    \n    return np.around(tmp)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# xgboost\n# n_estimators: 336\n# max_depth: 5\n# min_child_weight: 1\n# gamma: 0.16\n# subsample: 0.98\n# colsample_bytree: 0.86\n\nfrom xgboost.sklearn import XGBRegressor\n\ndata_X_tr, data_X_test, data_y_tr, data_y_test = prepare_data(df_pop, 'Fatalities', test_size=0.7, dropna=True)\n# data_X_val, data_X_test, data_y_val, data_y_test = train_test_split(data_X_rest, data_y_rest, test_size=0.5, random_state=111)\n\nmodel_f = XGBRegressor(learning_rate=0.01, n_estimators=880, max_depth=3, min_child_weight=0.0, gamma=0.0,\n                       subsample=0.8, colsample_bytree=0.7, reg_alpha=0.0, reg_lambda=0.0,\n                       objective='reg:squarederror', scale_pos_weight=1, seed=37)\n\nmodel_f.fit(data_X_tr, data_y_tr)\n\ntr_pred = predict_output(data_X_tr, model_f)\n\ndata_y_pred = predict_output(data_X_test, model_f)\nanalyse2(expm1_relu(data_y_tr), expm1_relu(tr_pred), expm1_relu(data_y_test), expm1_relu(data_y_pred))","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"model_cc = XGBRegressor(learning_rate=0.1, n_estimators=100, max_depth=5, min_child_weight=0, gamma=0,\n                        subsample=0.7, colsample_bytree=0.8, reg_alpha=0, reg_lambda=0,\n                        objective='reg:squaredlogerror', scale_pos_weight=1, seed=37, colsample_bynode=0.5)\n\ndata_X_tr, data_X_test, data_y_tr, data_y_test = prepare_data(df_pop, 'ConfirmedCases', test_size=0.7, dropna=True)\n# data_X_val, data_X_test, data_y_val, data_y_test = train_test_split(data_X_rest, data_y_rest, test_size=0.5, random_state=111)\n\nhist = model_cc.fit(data_X_tr, data_y_tr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tr_pred = predict_output(data_X_tr, model_cc)\n\ndata_y_pred = predict_output(data_X_test, model_cc)\nanalyse2(expm1_relu(data_y_tr), expm1_relu(tr_pred), expm1_relu(data_y_test), expm1_relu(data_y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# prepare test data\ndf_test = pd.read_csv(data_base + 'test.csv')\ndf_test.rename(columns={'Province_State': 'Province/State', 'Country_Region': 'Country/Region'}, inplace=True)\n\n# replace empty province\ndf_test['Province/State'].fillna('entire country', inplace=True)\n# calculate daynum based on the date of report\ndf_test['DayNum'] = (df_test['Date'].astype('datetime64[ns]') - day_zero).apply(lambda x: int(x.days))\n# get countries' zero days from df train dataset and join them with the test dataset\nzero_days = pd.DataFrame(df.groupby(['Country/Region', 'Province/State', 'DayZero']).size().reset_index()[['Country/Region', 'Province/State', 'DayZero']])\nzero_days.drop_duplicates(subset=['Country/Region', 'Province/State'], keep='last', inplace=True)\ndf_test = df_test.merge(zero_days, on=['Country/Region', 'Province/State'], how='left')\n\ndf_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# calculating RealDayNum based on DayZero\nreal_day_num = df_test['DayNum'] - df_test['DayZero'] + 1\ndf_test['RealDayNum'] = real_day_num - real_day_num.where(real_day_num<0).fillna(0)\n\n# merging df_test with population data\ndf_test_pop = pd.merge(df_test, df_external, on=['Country/Region', 'Province/State'], how='left')\ndf_test_pop.fillna(df_test_pop.mean(), inplace=True)\ndf_test_pop","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"output_columns = ['ConfirmedCases', 'Fatalities']\ntmp_output_columns = ['ConfirmedCases_y', 'Fatalities_y']\n\n# let's take available data a from training dataset (overlap with test dataset)\nlast_training_day = df['DayNum'].max()\nfirst_test_day = df_test['DayNum'].min()\ntrain_test_keys = ['Country/Region', 'Province/State', 'DayNum']\ndf_test_pop = pd.merge(df_test_pop, df[df['DayNum']>=first_test_day][train_test_keys+output_columns], on=train_test_keys, how='left')\ndf_test_pop[output_columns] = df_test_pop[output_columns].fillna(0).copy()\ndf_test_pop","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# And now comes a very importsnt part of the prediction.\n# As we need to use previous prediction in the next prediction we have to predict day by day\nlast_test_day = df_test['DayNum'].max()\nfor day in range(last_training_day+1, last_test_day+1):\n    print('predicting day {} ({} to go)'.format(day, last_test_day-day))\n    up_to_current_day = df_test_pop.where(df_test['DayNum']<=day).dropna(subset=['Country/Region'])\n    # calculate columns for previous days\n    previous_days_columns = add_extra_features_from_previous_days(up_to_current_day, TAIL)\n    # predict output for current day\n    up_to_current_day['ConfirmedCases'] = expm1_relu(predict_output(up_to_current_day[model_x_columns], model_cc))\n    up_to_current_day['Fatalities'] = expm1_relu(predict_output(up_to_current_day[model_x_columns], model_f))\n    # fill df_test with current day predictions\n    tmp_dataset = up_to_current_day[up_to_current_day['DayNum']==day][train_test_keys+output_columns]\n    df_test_pop = pd.merge(df_test_pop, tmp_dataset, on=train_test_keys, how='left', suffixes=('', '_y'))\n    df_test_pop[tmp_output_columns] = df_test_pop[tmp_output_columns].fillna(0).copy()\n    df_test_pop['ConfirmedCases'] += df_test_pop['ConfirmedCases_y']\n    df_test_pop['Fatalities'] += df_test_pop['Fatalities_y']\n    df_test_pop.drop(columns=tmp_output_columns, inplace=True)\n# up_to_current_day_scaled[model_x_columns]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"up_to_current_day[up_to_current_day.isnull().any(axis=1)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_columns = ['ForecastId', 'ConfirmedCases', 'Fatalities']\ndf_test_pop[submission_columns].to_csv('submission.csv', index=False)\ndf_test_pop","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test_pop['ConfirmedCases'].max()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test_pop['Fatalities'].max()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"}},"nbformat":4,"nbformat_minor":4}