{"cells":[{"metadata":{},"cell_type":"markdown","source":"Aggregates Statistics are COVID-19 cases dominate the news, they dominate the conversation, and they dominate Kaggle. One of the most watch Aggregates Statistics is the number of Confirmed Cases in each country.  The challenge we face when thinking about the number of Confirmed Cases is whether these figures are accurate.  Some countries have taken heroic steps to testing, and others remain slow to roll-out tests.  This begs the question: what can we rely on?  While the number of confirmed may vary greatly between countries based on testing policy, the number of fatalities I expect to be far more faithful.  The problem with comparing these figures is the 1. most people don't die of the disease, and 2. countries can observe fatalities at a lag to their number of Confirmed Cases.  The question then remains: can we use Fatalities to verify the consistency of Confirmed Cases, and if the factors driving them do differ, why?"},{"metadata":{},"cell_type":"markdown","source":"# Contents\n1. [Data](#Data)  \n2. [%Δ ConfirmedCases Model](#%Δ-ConfirmedCases-Model)  \n3. [%Δ Fatalities Model](#%Δ-Fatalities-Model)  \n4. [Model Comparison](#Model-Comparison)  \n5. [Conclusion](#Conclusion)"},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"! apt install libgeos-dev\n! pip uninstall -y shapely; pip install --no-binary :all: shapely==1.6.4\n! pip uninstall -y cartopy; pip install --no-binary :all: cartopy==0.17.0\n! pip install geoviews==1.6.6 hvplot==0.5.2 panel==0.8.0 bokeh==1.4.0","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport os\nfrom operator import add, mul\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport hvplot.pandas\nimport holoviews as hv\nimport cartopy.crs as ccrs\nimport geopandas as gpd\nfrom toolz.curried import map, partial, pipe, reduce\nfrom statsmodels.regression.linear_model import OLS\nfrom statsmodels.tools.tools import add_constant\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \n# Any results you write to the current directory are saved as output.\nhv.extension('bokeh')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data"},{"metadata":{},"cell_type":"markdown","source":"I chose to draw on a number of data sources on not only COVID cases but country indicators on GDP, infant mortality, etc., as well as data on population estimates and land size.  In order to better control for the variance in Fatalities and Confirmed Cases as a result of country sizes, I opted to look at Fatalities and Confirmed Cases per Capita.  As, at this stage in the virus, the pandemic is still dominated by the exponential growth in new cases, I opted to analyze the relationship between the percent change in Fatalities or Confirmed Cases per Capita, against percent changes in our factors.  Two interesting exceptions to this were variables representing the weeks since the first case in the country and the first case death, where I included both the log of the weeks since this event, to represent percent change, and the original value. This is used to model any effects relating to the logarithmic flattening of the curve late in the infection in a given country.  "},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"countries = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres')).replace('United States of America', 'US')\ncovid = pd.read_csv('/kaggle/input/covid19-global-forecasting-week-2/train.csv', parse_dates=['Date'], index_col='Id')\nindicators = pd.read_csv('/kaggle/input/countries-of-the-world/countries of the world.csv', decimal=',').replace('United States', 'US')\n\ncountry_indicators = (countries.assign(name = lambda df: df.name.astype(str).str.strip())\n                     .merge(indicators.assign(Country = lambda df: df.Country.astype(str).str.strip()), \n                            left_on = 'name', right_on='Country', how='inner'))\nweeks = (covid\n         .assign(dayofweek = lambda df: df.Date.dt.dayofweek)\n         .set_index('Date')\n         .drop(columns=['Province_State'])\n         .groupby(['Country_Region', pd.Grouper(freq='W')]).agg({'ConfirmedCases':'sum', 'Fatalities':'sum', 'dayofweek':'max'})\n         .reset_index()\n         .where(lambda df: df.ConfirmedCases > 0)\n         .dropna(0)\n         .groupby('Country_Region')\n         .apply(lambda df: (df\n                            .sort_values('Date')\n                            .assign(week_of_infection = lambda df: pd.np.arange(df.shape[0]))))\n         .where(lambda df: df.dayofweek >= 6)\n         .drop(columns=['dayofweek'])\n         .dropna(0)\n         .reset_index(drop=True)\n         .merge(country_indicators, left_on='Country_Region', right_on='name', how='inner')\n         .pipe(lambda df: gpd.GeoDataFrame(df, geometry='geometry'))\n         .assign(ConfirmedCases_per_capita = lambda df: (df.ConfirmedCases / df.pop_est),\n                 Fatalities_per_capita= lambda df: (df.Fatalities / df.pop_est),\n                 land_area = lambda df: df.area.astype('float'),\n                 week_of_infection_exp = lambda df: df.week_of_infection.apply(np.exp))\n         .groupby('Country_Region')\n         .apply(lambda df: (df\n                            .assign(week_since_first_death = lambda x: (x.week_of_infection - x.where(lambda y: y.Fatalities > 0)\n                                                                        .week_of_infection.min())\n                                                                        .clip(lower=0)\n                                                                        .fillna(0))))\n         .assign(week_since_first_death_exp = lambda df: df.week_since_first_death.apply(np.exp))\n         .drop(columns = 'gdp_md_est'))\nweeks","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To construct our design matrix for our experiment, we opted to include all our numeric columns and, to account for country-specific effects, we opted to include dummy variables for countries.  This design matrix is reused in both models, to estimate covariates for %Δ Cases/capita and %Δ Fatalities/capita.  "},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"X, y_cases = (weeks\n     .select_dtypes(include=['number'])\n     .drop(columns=['ConfirmedCases', 'Fatalities', 'ConfirmedCases_per_capita', 'Fatalities_per_capita'])\n     .replace(0, 1e-8)# add jitter\n     .transform(np.log)\n     .pipe(lambda df: df.fillna(df.mean()))\n     .rename(columns = lambda name: '%Δ ' + name)\n     .rename(columns = {'%Δ week_of_infection_exp': 'week_of_infection'})\n     .rename(columns = {'%Δ week_since_first_death_exp': 'week_since_first_death'})\n     .pipe(lambda df: pd.concat([df, pd.get_dummies(weeks.name, drop_first=True).rename(columns =lambda s: 'is_'+s)], axis=1))\n     .assign(const = 1),\n        \n    weeks\n    .loc[:, ['ConfirmedCases_per_capita']]\n    .rename(columns={'ConfirmedCases_per_capita': 'Cases/capita'})\n    .replace(0, 1e-8)# add jitter\n    .transform(np.log)  \n    .rename(columns = lambda name: '%Δ ' + name)\n    )\n\nX.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# %Δ ConfirmedCases Model"},{"metadata":{},"cell_type":"markdown","source":"Our final response variable for %Δ Cases/capita looks approximately symmetric, which should make our assumption of conditional normality in our models better motivated. "},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"y_cases.hvplot.kde(title='Kernel Density Estimation of %Δ Confirmed Cases Response')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\nTo perform feature selection, we opted to use forward-backwards stepwise feature selection, with an input threshold of 0.015 and removal threshold at the 5% level of significance.  In order to ensure this procedure was not bias to the order of the columns, the columns were randomly shuffles, and the selection procedure was rerun multiple times. "},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"def stepwise_selection(X, y, \n                       initial_list=[], \n                       threshold_in=0.015, \n                       threshold_out = 0.05, \n                       verbose=True):\n    \"\"\" Perform a forward-backward feature selection \n    based on p-value from statsmodels.api.OLS\n    Arguments:\n        X - pandas.DataFrame with candidate features\n        y - list-like with the target\n        initial_list - list of features to start with (column names of X)\n        threshold_in - include a feature if its p-value < threshold_in\n        threshold_out - exclude a feature if its p-value > threshold_out\n        verbose - whether to print the sequence of inclusions and exclusions\n    Returns: list of selected features \n    Always set threshold_in < threshold_out to avoid infinite looping.\n    See https://en.wikipedia.org/wiki/Stepwise_regression for the details\n    \"\"\"\n    included = list(initial_list)\n    while True:\n        changed=False\n        # forward step\n        excluded = list(set(X.columns)-set(included))\n        new_pval = pd.Series(index=excluded)\n        for new_column in excluded:\n            model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included+[new_column]]))).fit()\n            new_pval[new_column] = model.pvalues[new_column]\n        best_pval = new_pval.min()\n        if best_pval < threshold_in:\n            best_feature = new_pval.idxmin()\n            included.append(best_feature)\n            changed=True\n            if verbose:\n                print('Add  {:30} with p-value {:.6}'.format(best_feature, best_pval))\n\n        # backward step\n        model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included]))).fit()\n        # use all coefs except intercept\n        pvalues = model.pvalues.iloc[1:]\n        worst_pval = pvalues.max() # null if pvalues is empty\n        if worst_pval > threshold_out:\n            changed=True\n            worst_feature = pvalues.argmax()\n            included.remove(worst_feature)\n            if verbose:\n                print('Drop {:30} with p-value {:.6}'.format(worst_feature, worst_pval))\n        if not changed:\n            break\n    return included\n\nparams_cases = stepwise_selection(X.loc[:, np.random.permutation(X.columns)], y_cases, threshold_in=0.015)\n\nmodel_cases = OLS(y_cases, X.loc[:, params_cases])\nresults_cases = model_cases.fit()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Regression Estimates**  \nOur final model includes many of our country-specific effects, which may be interesting to analyze. What is fascinating to investigate in our model is the inclusion of significant %Δ Industry and %Δ Agriculture features.  This may suggest that countries with largely service-based economies have lower growth-rates in infection controlling for our other variables.  "},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"results_cases.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Influence + Leverage against Squared Residuals + QQ-plot**  \nOur final model includes many of our country-specific effects, which may be interesting to analyze. What is fascinating\nA major concern for our analysis is the clear structure in our leverage and residuals, suggesting there may be an omitted variable not included in our design matrix or by our selection procedure.  Despite this structure, the distribution of our errors appears to strongly follow our assumptions of normality, which is promising for the later tests on our model.  g to investigate in our model is the inclusion of significant %Δ Industry and %Δ Agriculture features.  This may suggest that countries with largely service-based economies have lower growth-rates in infection controlling for our other variables.  "},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(17, 5))\nfig = sm.graphics.influence_plot(results_cases, ax=axes[0], criterion=\"cooks\")\nfig = sm.graphics.plot_leverage_resid2(results_cases, ax=axes[1])\nres = results_cases.resid # residuals\nfig = sm.qqplot(res, ax=axes[2])\nfig.tight_layout()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Partial Regression Plots**  \nWhat should raise concern is the correlation many variables have with the errors, and the presence of heteroskedasticity in our data, which may be a function of the number of the transformations on our data or omitted variables. "},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"fig = plt.figure(figsize=(30,60))\nsm.graphics.plot_partregress_grid(results_cases, fig=fig)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# %Δ Fatalities Model"},{"metadata":{},"cell_type":"markdown","source":"For our second model, we will investigate the covariates on %Δ Fatalities Response.  Looking at the Kernel Density Estimation of our response variable, we can see clearly a mixture of two- possibly three- symmetric distributions, which may have interesting covariates in our data. "},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"y_fatalities = (weeks\n    .loc[:, ['Fatalities_per_capita']]\n    .rename(columns={'Fatalities_per_capita': 'Fatalities/capita'})\n    .replace(0, 1e-8)# add jitter\n    .transform(np.log)  \n    .rename(columns = lambda name: '%Δ ' + name))\n\ny_fatalities.hvplot.kde(title='Kernel Density Estimation of %Δ Fatalities Response')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Similar to our first model, we chose a forward-backwards stepwise method of feature selection, but with a threshold of 0.025 for variables entering our model. This higher value was chosen after analyzing our models under a number of different hyperparameters and comparing the variables entering the model against our %Δ Cases/capita model. "},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"params_fatalities = stepwise_selection(X.loc[:, np.random.permutation(X.columns)], y_fatalities,  threshold_in=0.025)\n\nmodel_fatalities = OLS(y_fatalities, X.loc[:, params_fatalities])\nresults_fatalities = model_fatalities.fit()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Regression Estimates**  \nOur estimated model appears to have far fewer features included and a far lower Adjusted R-squared. While it may be difficult to explain why our %Δ Fatalities/capita model is explained worse by its covariates than the %Δ Cases/capita model, this may be due to the fact that many countries are too early on in their infection rate to recognise deaths, making estimation more challenging. "},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"results_fatalities.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Influence + Leverage against Squared Residuals + QQ-plot**  \nThis model appears far more dominated by points of high leverage, and our residuals seem to exhibit much fatter tails to the model. "},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(17, 5))\nfig = sm.graphics.influence_plot(results_fatalities, ax=axes[0], criterion=\"cooks\")\nfig = sm.graphics.plot_leverage_resid2(results_fatalities, ax=axes[1])\nres = results_fatalities.resid # residuals\nfig = sm.qqplot(res, ax=axes[2])\nfig.tight_layout()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Partial Regression Plots**  \nYet again, our model does suffer from string correlation against our residuals which may be a function of either omitted variables or poor transformations of our feature-space.  "},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(30,60))\nsm.graphics.plot_partregress_grid(results_fatalities, fig=fig)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Comparison"},{"metadata":{},"cell_type":"markdown","source":"\nThe main aim of this analysis has been to compare the coefficients across our two models to identify where and why they differ. The conjecture I present in this notebook is that if these coefficients differ this may be an indication that either Fatalities are driven by other factors which do not influence the number of Confirmed Cases, or that the number of Confirmed Cases is a function of factors which lead to better testing and thus higher rates of Confirmed Cases.  What is interesting here is to observe where these may be either omitted variables or change in the sign of a coefficient between the two models. "},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"(pd.concat([results_cases.params.to_frame(name='Coefficient').assign(Response = '%Δ Cases/capita'),\n            results_fatalities.params.to_frame(name='Coefficient').assign(Response = '%Δ Fatalities/capita')], axis=0)\n .drop(index=['const'])\n .reset_index().rename(columns={'index': 'Covariate'})\n .where(lambda s: ~s.Covariate.str.startswith('is_')).dropna().set_index('Covariate')\n .hvplot.bar(title='COVID-19: Coefficients on (%Δ) Covariate against (%Δ) Response', by='Response', rot=90)\n .opts(width=1200, height=400))","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"coefficients = (results_cases.params.to_frame('Cases')\n                .join(results_fatalities.params.to_frame('Fatalities'), how='outer')\n                .fillna(0))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In order to extend on our visual analysis of these coefficients, we can test if the coefficients of one of our models are statistically different from the estimated of the model, under the t-distribution. This is different than identifying whether these coefficients are statistically non-zero, as many of these coefficients we are comparing against can take on positive and negative values.  "},{"metadata":{},"cell_type":"markdown","source":"Firstly we will check if estimates for our %Δ Fatalitlies/capita model the same as those for %Δ ConfirmedCases per capita model.   "},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"formula = (coefficients\n .Cases\n .loc[results_fatalities.params.index]\n .reset_index()\n .rename(columns={'index':'Name'})\n .assign(formula = lambda df: df.Name.astype(str) + ' = ' + df.Cases.astype(str) + ' ,')\n .formula\n .sum())[:-1]\n\nT_test = results_fatalities.t_test(formula)\nT_test.summary_frame().assign(names = model_fatalities.exog_names).set_index('names').round(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Secondly, we will check if our estimates for our %Δ ConfirmedCases/capita model is the same as those for our %Δ Fatalities/capita model.  "},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"formula = (coefficients\n .Fatalities\n .loc[results_cases.params.index]\n .reset_index()\n .rename(columns={'index':'Name'})\n .assign(formula = lambda df: df.Name.astype(str) + ' = ' + df.Fatalities.astype(str) + ' ,')\n .formula\n .sum())[:-1]\n\nT_test = results_cases.t_test(formula)\nT_test.summary_frame().assign(names = model_cases.exog_names).set_index('names').round(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conclusion\nWhat appears interesting from our analysis, is that apart from some country-specific estimtes. The estimates of our coefficients do seem to differ between our two models. The structure of the economies of countries appears a credible factor to investigate why these estimates vary so much which may have far reaching implications as the virus spreads. "},{"metadata":{"trusted":true},"cell_type":"markdown","source":"I would love to hear your feedback on this notebook and any suggestions on how I may improve the analysis in anyway by included new data sources or new methodologies.  Please, if you liked this kernel, please give it a vote and check our some of my other intesting kernels on COVID-19 Survival Analysis.  "}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}