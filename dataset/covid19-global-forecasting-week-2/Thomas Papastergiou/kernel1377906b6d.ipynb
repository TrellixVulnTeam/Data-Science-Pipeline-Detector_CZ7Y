{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nfiles = [None]*3\ni=0\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        files[i] = os.path.join(dirname, filename)\n        i = i + 1\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(3):\n    f = files[i].find('train')\n    if f!=-1:\n        train_dt = pd.read_csv(files[i])\nfor i in range(3):\n    f = files[i].find('test')\n    if f!=-1:\n        test_dt = pd.read_csv(files[i])\nfor i in range(3):\n    f = files[i].find('submission')\n    if f!=-1:\n        submi_dt = pd.read_csv(files[i])\nend_train = train_dt.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from pandas.api.types import CategoricalDtype\n\n# concatenate train_and test sets\n\ntest_train_dt = train_dt.append(test_dt, sort=False)\n\n# test_train_dt[19698-5:19698+5]\n# test_train_dt.shape\n\n# make Date Categorical\nuniqueDates = list(test_train_dt.Date.unique())\ncat_type_date = CategoricalDtype(categories = uniqueDates , ordered=True)\n# test_train_dt.Date = \ntest_train_dt.Date = test_train_dt.Date.astype(cat_type_date).cat.codes.astype(float)\n# Province_State categorical NaN = -1.0\ntest_train_dt.Province_State = test_train_dt.Province_State.astype(\"category\").cat.codes.astype(float)\n# Country_Region categorical\ntest_train_dt.Country_Region = test_train_dt.Country_Region.astype(\"category\").cat.codes.astype(float)\ntest_train_dt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# test_train_dt['SinceTheOutbreak'] = 0\n# test_train_dt\n\n# nrOfRows = test_train_dt.shape[0]\n# # nrOfRows\n\n# sinceTheOutbreak = np.zeros((nrOfRows,1))\n# # for i in range(nrOfRows):\n\n# test_train_dt[test_train_dt['Country_Region']==0]\n# sinceTheOutbreak[test_train_dt['Country_Region']==0,:] = 0\n# test_train_dt['Country_Region']==0 & test_train_dt['Country_region']==0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_train_dt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_set = test_train_dt[:end_train]\ntest_set = test_train_dt[end_train:]\n\ntime_span = 7;\nnuberOfFeatures = 2*time_span +1\n\ntrain_set.shape[0]\nnrOfTrainDates = train_set.Date.unique().shape[0]\n# find the number of countries without provices\n\nnrOfCountriesNoRegion = train_set[train_set.Province_State==-1].Country_Region.unique().shape[0]\n\n# find the number of countries with provinces\n\nnrOfTrainProvinces = train_set[train_set.Province_State!=-1].Province_State.unique().shape[0]\n# find the number of different countries/states\n\nnrOfCountries_ALL = nrOfTrainProvinces + nrOfCountriesNoRegion\n# nrOfCountries_ALL\nnrOfTrainData = nrOfCountries_ALL*(nrOfTrainDates-time_span) #+1\n# nrOfTrainData\nX = np.zeros((nrOfTrainData,nuberOfFeatures))\nY = np.zeros((nrOfTrainData,2))\n# X.shape\n# calcuate the labels for the train set\n\ninst_count = 0;\n#max country ID + 1\nmax_country_ID = train_set.Country_Region.unique().max()+1\n\ntrain_set['Province_State'] = train_set['Province_State']+max_country_ID\ntest_set['Province_State'] = test_set['Province_State']+max_country_ID\n# fix greenland\n\n\n\n# train_set[train_set[\"Province_State\"]>-1] + max_country_ID\n\nfor i in range(nrOfCountriesNoRegion):\n\n\n\n    country = train_set[train_set['Country_Region']==i]\n    if country.Province_State.unique().shape[0] == 1:\n\n        nrOfdaysPerCountry = country.shape[0]\n        for j in range(nrOfdaysPerCountry-time_span):\n            X[inst_count:inst_count+1, 0:time_span] = country.ConfirmedCases[j:time_span+j].values\n            X[inst_count:inst_count+1, time_span:2*time_span] = country.Fatalities[j:time_span+j].values\n            X[inst_count:inst_count+1, 2*time_span:2*time_span+1] = country.Country_Region[j:j+1].values\n            Y[inst_count:inst_count+1,0:1] = country.ConfirmedCases[time_span+j:time_span+j+1].values\n            Y[inst_count:inst_count+1,1:2] = country.Fatalities[time_span+j:time_span+j+1].values\n    #         print(country.Date[time_span+j:time_span+j+1].values)\n            inst_count = inst_count + 1\n\n    else:\n        nrOfProvinces = country.Province_State.unique().shape[0]\n    #     print(nrOfProvinces)\n    #     print(country.Province_State.unique())\n        for ii in country.Province_State.unique():\n    #         train_set[train_set['Country_Region']==i]\n            province = country[country['Province_State']==ii]\n\n            nrOfdaysPerProvince = province.shape[0]\n            for j in range(nrOfdaysPerProvince-time_span):\n                print(j)\n                X[inst_count:inst_count+1, 0:time_span] = province.ConfirmedCases[j:time_span+j].values\n                X[inst_count:inst_count+1, time_span:2*time_span] = province.Fatalities[j:time_span+j].values\n                X[inst_count:inst_count+1, 2*time_span:2*time_span+1] = province.Province_State[j:j+1].values\n                Y[inst_count:inst_count+1,0:1] = province.ConfirmedCases[time_span+j:time_span+j+1].values\n                Y[inst_count:inst_count+1,1:2] = province.Fatalities[time_span+j:time_span+j+1].values\n        #         print(country.Date[time_span+j:time_span+j+1].values)\n                inst_count = inst_count + 1\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from keras.datasets import mnist\nimport keras.utils.np_utils as ku\nimport keras.models as models\nimport keras.layers as layers\nfrom keras import regularizers\nfrom keras.optimizers import rmsprop\nfrom keras.constraints import nonneg\n\nimport numpy as np\nimport numpy.random as nr\n# from tensorflow import set_random_seed\nimport matplotlib.pyplot as plt\n\n\nfrom keras.layers import Dropout, LeakyReLU","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_loss(history):\n    train_loss = history.history['loss']\n#     test_loss = history.history['val_loss']\n    x = list(range(1, len(train_loss) + 1))\n#     plt.plot(x, test_loss, color = 'red', label = 'test loss')\n    plt.plot(x, train_loss, label = 'traning loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.title('Loss vs. Epoch')\n    plt.legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# kernel_regularizer=regularizers.l2(0.1)\nnn = models.Sequential()\nnn.add(layers.Dense(128, activation = 'linear', input_shape = (nuberOfFeatures, ), \n                    kernel_regularizer=regularizers.l2(0.01) )) #,\n#                     kernel_constraint=nonneg()))\n# nn.add(Dropout(rate = 0.2))\nnn.add(layers.Dense(64, activation = 'linear',\n                    kernel_regularizer=regularizers.l2(0.01) )) #,\n#                     kernel_constraint=nonneg()))\n# nn.add(Dropout(rate = 0.2))\nnn.add(layers.Dense(32, activation = 'relu', \n                    kernel_regularizer=regularizers.l2(0.01) )) #,\n#                     kernel_constraint=nonneg()))\n# nn.add(Dropout(rate = 0.2))\n\nnn.add(layers.Dense(2, activation = 'linear'))\nnn.summary()\n\n# kernel_regularizer=regularizers.l1(10.0)\n\n\nnn.compile(optimizer = 'rmsprop', loss = 'mean_squared_logarithmic_error', \n                metrics = ['mean_squared_logarithmic_error'])\nhistory = nn.fit(X, Y, \n                  epochs = 200, batch_size = nrOfdaysPerCountry, verbose = 0) #,validation_data = (X, Y))\n\n\n\nplot_loss(history)\n\n\n\nnrOfTestInst = test_set.shape[0]\n\nX_test = np.zeros((nrOfTestInst, nuberOfFeatures))\n\n# the train dates\ntestDates = test_set.Date.unique()\nfirstTestDay = testDates.min()\nfor i in range(nrOfTestInst):\n    # i=3999\n    # starts from 0!!!!!!\n    # i=353\n    country_index = test_set.Country_Region[i:i+1].values[0]\n    province_index = test_set.Province_State[i:i+1].values[0]\n    # province_index = 172 means NaN\n    # is a country or region\n    # test_set['Country_Region']==0\n\n    test_instance = test_set[i:i+1]\n    test_date = test_instance.Date.values[0]\n\n\n    # if test_set[test_set['Country_Region'] == country_index].Province_State.unique().shape[0] == 1:\n    # print('# is a country')\n    # how many days we need\n    start_previous_days = test_date - time_span\n    end_previous_days = test_date - 1\n    # how many days from test set\n\n\n    only_test_set_day = test_date-firstTestDay\n    if only_test_set_day>=time_span:\n        test_set_days = time_span\n    else:\n        test_set_days = test_date - firstTestDay   \n    # how many days from the train set\n    train_set_days = time_span - test_set_days\n\n\n\n    #from train set \n    #     print(train_set[train_set['Country_Region']==country_index and train_set[Date]>=start_previous_days])\n    #     train_set['Country_Region']==country_index and train_set['Date']>=start_previous_days\n    # from train set\n    if test_set[test_set['Country_Region'] == country_index].Province_State.unique().shape[0] == 1:\n        tmp_count = train_set[train_set['Country_Region']==country_index]\n    else:\n    #     print('state')\n        if province_index != 172:\n            tmp_count = train_set[train_set['Province_State']==province_index]\n        else:\n            tmp_count_0 = train_set[train_set['Country_Region']==country_index]\n            tmp_count = tmp_count_0[tmp_count_0['Province_State']==province_index]\n    tmp_count1 = tmp_count[tmp_count['Date']>=start_previous_days]\n    tmp_train_set_days = tmp_count1[tmp_count1['Date']<=start_previous_days+train_set_days-1]\n    # print('train')\n    # print(tmp_train_set_days)\n    # from test set\n    test_set_start_day = test_date - test_set_days\n    test_set_end_day = test_date - 1\n    #     print(test_set_end_day)\n    if test_set[test_set['Country_Region'] == country_index].Province_State.unique().shape[0] == 1:\n        tmp_test = test_set[test_set.Country_Region==country_index]\n    else:\n    #     print('state')\n        if province_index != 172:\n            tmp_test = test_set[test_set.Province_State==province_index]\n        else:\n    #         tmp_test_0 = test_set[test_set.Province_State==province_index]\n            tmp_test_0 = test_set[test_set.Country_Region==country_index]\n            tmp_test = tmp_test_0[tmp_test_0.Province_State==province_index]\n\n    tmp_test1 = tmp_test[tmp_test.Date>=test_set_start_day]\n    tmp_test_set_days = tmp_test1[tmp_test1.Date<=test_set_end_day]\n    # print('test')\n    # print(tmp_test_set_days)\n    features_df = tmp_train_set_days.append(tmp_test_set_days)\n    #     print(features_df)\n    X_test[i:i+1, 0:time_span] = features_df.ConfirmedCases[:].values\n    X_test[i:i+1, time_span:2*time_span] = features_df.Fatalities[:].values\n    X_test[i:i+1, 2*time_span:2*time_span+1] = features_df.Country_Region[0:1].values\n    prediction = nn.predict(X_test[i:i+1])\n    #     print(prediction)\n#     if prediction[0,0] < 0:\n#         prediction[0,0] = 0\n#     if prediction[0,1] < 0:\n#         prediction[0,1] = 0\n    test_set.set_value(i, 'ConfirmedCases', round(prediction[0,0]))\n    test_set.set_value(i, 'Fatalities',round(prediction[0,1]))\n\n# find the overlap between test and train\ncountry_index = 0\ntrainDates = train_set[train_set['Country_Region']==0].Date\ntestDates = test_set[test_set['Country_Region']==0].Date\ncommon_Dates = np.intersect1d(trainDates, testDates)\n\nnrOfCommon_Dates = common_Dates.shape[0]\n\nS = 0\nn = 0\n\nfor i in range(nrOfCommon_Dates):\n    # common_Dates[i]\n    tmp_train = train_set[train_set['Date']==common_Dates[i]].ConfirmedCases + 1\n    tmp_log_train = tmp_train.apply(np.log)\n    # tmp_log_train\n    tmp_test = test_set[test_set['Date']==common_Dates[i]].ConfirmedCases + 1\n    tmp_log_test = tmp_test.apply(np.log)\n    dif = (tmp_log_train.values - tmp_log_test.values)\n    squ = dif*dif\n    S = S + squ.sum()\n    n = n + squ.shape[0]\n    # test_set[test_set['Date']==common_Dates[i]]\n    # tmp_log_test\nRMSLE = np.sqrt((1/n)*S)\nprint('RMSLE:')\nprint(RMSLE)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ind = test_dt[test_dt['Country_Region']=='Italy'].ForecastId.index\n# ind = test_dt[test_dt['Province_State']=='Hubei'].ForecastId.index\nconfCases = test_set[ind.min():ind.max()]['ConfirmedCases']\nplt.plot(confCases, label = 'Confirmed Cases')\nplt.xlabel('Predictions Dates since 19/03')\nplt.ylabel('Confirmed Cases')\nplt.title('Confirmed Cases')\n# ConfirmedCases\nfatal = test_set[ind.min():ind.max()]['Fatalities']\nplt.figure()\nplt.plot(fatal, label = 'Fatalities')\nplt.xlabel('Predictions Dates since 19/03')\nplt.ylabel('Fatalities')\nplt.title('Fatalities')\n\n# Fatalities\n# test_dt[test_dt['Country_Region']=='Greece']\n# test_dt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submi_dt['ConfirmedCases'] = test_set.ConfirmedCases\nsubmi_dt['Fatalities'] = test_set.Fatalities\nsubmi_dt.to_csv('submission.csv', index = False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}